{"id": "36181169", "url": "https://en.wikipedia.org/wiki?curid=36181169", "title": "(308242) 2005 GO21", "text": "(308242) 2005 GO21\n\nBased on an absolute magnitude of 16.4, the asteroid has an estimated diameter of 1.6 km (within a factor of two). is the largest potentially hazardous asteroid (PHA) discovered in 2005. On 21 June 2012 it passed Earth at a distance of . The 2012 passage was studied with radar using Goldstone and Arecibo.\n"}
{"id": "42156921", "url": "https://en.wikipedia.org/wiki?curid=42156921", "title": "(388188) 2006 DP14", "text": "(388188) 2006 DP14\n\n, provisional designation \"\", is a sub-kilometer sized, peanut-shaped asteroid on a highly eccentric orbit, classified as near-Earth object and potentially hazardous asteroid of the Apollo group. This contact binary was discovered on 23 February 2006, by astronomers of the LINEAR program at the Lincoln Laboratory's Experimental Test Site near Socorro, New Mexico, in the United States. On 10 February 2014, it passed 6.25 lunar distances from Earth. The asteroid is approximately 400 meters in diameter and has a rotation period of 5.77 hours.\n\n\"\" is a member of the Apollo group, which are Earth-crossing asteroids and the largest group of near-Earth asteroids.\n\nIt orbits the Sun at a distance of 0.3–2.4 AU once every 19 months (583 days; semi-major axis of 1.37 AU). Its orbit has a high eccentricity of 0.78 and an inclination of 12° with respect to the ecliptic. As no precoveries were taken, the body's observation arc begins with its discovery observation in 2006.\n\nThe asteroid has an Earth minimum orbit intersection distance (MOID) of , which corresponds to 6.4 lunar distances (LD). On 10 February 2014, it passed Earth close to this theoretical minimum distance at 6.25 LD, or . This makes is a potentially hazardous asteroid (PHA), a body with a threatening close approach to the Earth, due to its low MOID and large size (absolute magnitude of 18.9). PHAs are defined as objects with an absolute magnitude of 22 or brighter – which generically corresponds to a diameter of approximately 140 meters – and a MOID that is smaller than 0.05 AU or 19.5 LD. \n\n\" is an assumed stony S-type asteroid. This asteroid is a typical contact binary, with two distinctive lobes on either end that appear to be in contact, giving it a peanut-like shape. \n\nOn the night of 11 February 2014, NASA scientists conducted a radar imaging session using the 70-meter dish at Goldstone Observatory. These observations, using delay-Doppler radar imaging, revealed a 400 × 200 meters sized body, while the \"Collaborative Asteroid Lightcurve Link\" calculates a diameter of almost 500 meters, based on an assumed standard albedo for stony asteroids of 0.20 and an absolute magnitude of 18.9.\n\nAmateur and professional astronomers helped track \"\" in the preceding days, so they would know just where to point the large antenna.\n\nGoldstone's radiometric observations also gave a rotation period of approximately 6 hours. Photometric follow-up observations led to two light-curves that gave a refined period of 5.77 and 5.78 hours with a high brightness variation of 1.05 and 0.9, respectively (). Its high brightness amplitude is also indicative for its elongated shape. \n\nAs of 2018, this minor planet remains unnamed.\n\n"}
{"id": "1393046", "url": "https://en.wikipedia.org/wiki?curid=1393046", "title": "Agarwood", "text": "Agarwood\n\nAgarwood, aloeswood or gharuwood is a fragrant dark resinous wood used in incense, perfume, and small carvings. It is formed in the heartwood of aquilaria trees when they become infected with a type of mould (\"Phialophora parasitica\"). Prior to infection, the heartwood is odourless, relatively light and pale coloured; however, as the infection progresses, the tree produces a dark aromatic resin, called aloes or agar(not to be confused with the edible, algae-derived agar used in microbiology, medicine and desserts - (as well as \"gaharu\", \"jinko\", \"oud\", or \"oodh\"; not to be confused with bukhoor), in response to the attack, which results in a very dense, dark, resin embedded heartwood. The resin embedded wood is valued in many cultures for its distinctive fragrance, and thus is used for incense and perfumes. The aromatic qualities of agarwood are influenced by the species, geographic location, its branch, trunk and root origin, length of time since infection, and methods of harvesting and processing \n\nOne of the main reasons for the relative rarity and high cost of agarwood is the depletion of the wild resource. Since 1995, \"Aquilaria malaccensis\", the primary source, has been listed in Appendix II (potentially threatened species) by the Convention on International Trade in Endangered Species of Wild Fauna and Flora. In 2004, all \"Aquilaria\" species were listed in Appendix II; however, a number of countries have outstanding reservations regarding that listing.\n\nFirst-grade agarwood is one of the most expensive natural raw materials in the world, with 2010 prices for superior pure material as high as US$100,000/kg, although in practice adulteration of the wood and oil is common, allowing for prices as low as US$100/kg. A whole range of qualities and products are on the market, varying in quality with geographical location, botanical species, the age of the specific tree, cultural deposition and the section of the tree where the piece of agarwood stems from. Oud oil is distilled from agarwood, and fetches high prices depending on the oil's purity. The current global market for agarwood is estimated to be in the range of US$6 – 8 billion and is growing rapidly.\n\nThe odour of agarwood is complex and pleasing, with few or no similar natural analogues. In the perfume state, the scent is mainly distinguished by a combination of \"oriental-woody\" and \"very soft fruity-floral\" notes. The incense smoke is also characterized by a \"sweet-balsamic\" note and \"shades of vanilla and musk\" and amber (not to be confused with ambergris). As a result, agarwood and its essential oil gained great cultural and religious significance in ancient civilizations around the world, being described as a fragrant product as early as 1400 BCE in one of the world's oldest written texts – the Sanskrit Vedas from India. Agarwood is highly revered in the seminal texts of Hinduism, Christianity, Buddhism, and Islam .\n\nDioscorides in his book Materia Medica (65 CE) described several medical qualities of agarwood (Áγαλλοχου) andmentioned its use as an incense. Even though Dioscorides describes agarwood as having an astringent and bitter taste, it was used to freshen the breath when chewed or as a decoction held in the mouth. He also writes that a root extract was used to treat stomach complaints and dysentery as well as pains of the lungs and liver. Agarwood’s use as a medicinal product was also recorded in the Sahih Muslim, which dates back to approximately the eighth century, and in the Ayurvedic medicinal text the Susruta Samhita.\n\nAs early as the third century CE in ancient Viet Nam, the chronicle \"Nan zhou yi wu zhi\" (Lĩnh nam chích quái) (Strange things from the South) written by Wa Zhen of the Eastern Wu Dynasty mentioned agarwood produced in the Rinan commandery, now Central Vietnam, and how people collected it in the mountains.\n\nDuring the sixth century CE in Japan, in the recordings of the \"Nihon Shoki\" (The Chronicles of Japan) the second oldest book of classical Japanese history, mention is made of a large piece of fragrant wood identified as agarwood. The source for this piece of wood is claimed to be from Pursat, Cambodia (based on the smell of the wood). The famous piece of wood still remains in Japan today and is showcased less than 10 times per century at the Nara National Museum.\n\nStarting in 1580 after Nguyễn Hoàng took control over the central provinces of modern Vietnam, he encouraged trade with other countries, specifically China and Japan. Agarwood was exported in three varieties: Calambac (kỳ nam in Vietnamese), trầm hương (very similar but slightly harder and slightly more abundant), and agarwood proper. A pound of Calambac bought in Hội An for 15 taels could be sold in Nagasaki for 600 taels. The Nguyễn Lords soon established a Royal Monopoly over the sale of Calambac. This monopoly helped fund the Nguyễn state finances during the early years of the Nguyen rule.\n\nXuanzang's travelogues and the \"Harshacharita\", written in seventh century AD in Northern India, mentions use of agarwood products such as 'Xasipat' (writing-material) and 'aloe-oil' in ancient Assam (Kamarupa). The tradition of making writing materials from its bark still exists in Assam.\n\nAgarwood is known under many names in different cultures:\n\nThere are seventeen species in the genus \"Aquilaria\", large evergreens native to southeast Asia, and nine are known to produce agar wood. In theory agarwood can be produced from all members; however, until recently it was primarily produced from \"A. malaccensis\". \"A. agallocha\" and \"A. secundaria\" are synonyms for \"A. malaccensis\". \"A. crassna\" and \"A. sinensis\" are the other two members of the genus that are usually harvested. The gyrinops tree can also produce agarwood.\n\nFormation of agar wood occurs in the trunk and roots of trees that have been penetrated by an insect feeding on wood and oily resin, the Ambrosia beetle (\"Dinoplatypus chevrolati\" by</ref> Gen.Sc.Prof. Stephan-Alexander Ebu Can Peter, 2017). A mold infection may then occur, and in response, the tree produces a salutary self-defense material to conceal damages or infections. While the unaffected wood of the tree is relatively light in color, the resin dramatically increases the mass and density of the affected wood, changing its color from a pale beige to yellow, orange, red, dark brown or black. In natural forests, only about 7 out of 100 Aquilaria trees of same species are infected and produce aloes/agar wood. A common method in artificial forestry is to inoculate trees with the fungus. It produces a \"damage sap\" and is referred to as \"fake\" aloes/agar wood. Gen.Sc.Prof. Stephan-Alexander Ebu Can Peter, 2017\n\nOud oil can be distilled from agar wood using steam; the total yield of oil for 70 kg of wood will not exceed 20 ml.\n\nThe composition of agarwood oil is exceedingly complex with more than 150 chemical compounds identified. At least 70 of these are terpenoids which come in the form of sesquiterpenes and chromones; no monoterpenes have been detected at all. Other common classes of compounds include agarofurans, cadinanes, eudesmanes, valencanes and eremophilanes, guaianes, prezizanes, vetispiranes, simple volatile aromatic compounds as well as a range of miscellaneous compounds. The exact balance of these materials will vary depending on the age and species of tree as well as the exact details of the oil extraction process.\n\nThe following species of \"Aquilaria\" produce agarwood:\nSri Lankan \"agarwood\" is known as Wala Patta and is of the Gyrinops walla species.\n\nOverharvesting and habitat loss threatens some populations of agarwood-producing species. Concern over the impact of the global demand for agarwood has thus led to the inclusion of the main taxa on CITES Appendix II, which requires that international trade in agarwood be monitored. Monitoring is conducted by London-based TRAFFIC (a joint WWF and IUCN programme). CITES also provides that international trade in agarwood be subject to controls designed to ensure that harvest and exports are not to the detriment of the survival of the species in the wild.\n\nIn addition, agarwood plantations have been established in a number of countries, and reintroduced into countries such as Malaysia and Sri Lanka as commercial plantation crops. The success of these plantations depends on the stimulation of agarwood production in the trees. Numerous inoculation techniques have been developed, with varying degrees of success.\n\n\n"}
{"id": "60779", "url": "https://en.wikipedia.org/wiki?curid=60779", "title": "Amphitrite", "text": "Amphitrite\n\nIn ancient Greek mythology, Amphitrite (; ) was a sea goddess and wife of Poseidon and the queen of the sea. Under the influence of the Olympian pantheon, she became merely the consort of Poseidon and was further diminished by poets to a symbolic representation of the sea. In Roman mythology, the consort of Neptune, a comparatively minor figure, was Salacia, the goddess of saltwater.\"\n\nAmphitrite was a daughter of Nereus and Doris (and thus a Nereid), according to Hesiod's \"Theogony\", but of Oceanus and Tethys (and thus an Oceanid), according to the \"Bibliotheca\", which actually lists her among both the Nereids \"and\" the Oceanids. Others called her the personification of the sea itself (saltwater). Amphitrite's offspring included seals and dolphins. Poseidon and Amphitrite had a son, Triton who was a merman, and a daughter, Rhodos (if this Rhodos was not actually fathered by Poseidon on Halia or was not the daughter of Asopus as others claim). \"Bibliotheca\" (3.15.4) also mentions a daughter of Poseidon and Amphitrite named Benthesikyme.\nAmphitrite is not fully personified in the Homeric epics: \"out on the open sea, in Amphitrite's breakers\" (\"Odyssey\" iii.101), \"moaning Amphitrite\" nourishes fishes \"in numbers past all counting\" (\"Odyssey\" xii.119). She shares her Homeric epithet \"Halosydne\" (\"sea-nourished\") with Thetis in some sense the sea-nymphs are doublets.\n\nThough Amphitrite does not figure in Greek \"cultus\", at an archaic stage she was of outstanding importance, for in the Homeric Hymn to Delian Apollo, she appears at the birthing of Apollo among, in Hugh G. Evelyn-White's translation, \"all the chiefest of the goddesses, Dione and Rhea and Ichnaea and Themis and loud-moaning Amphitrite;\" more recent translators are unanimous in rendering \"Ichnaean Themis\" rather than treating \"Ichnae\" as a separate identity. Theseus in the submarine halls of his father Poseidon saw the daughters of Nereus dancing with liquid feet, and \"august, ox-eyed Amphitrite\", who wreathed him with her wedding wreath, according to a fragment of Bacchylides. Jane Ellen Harrison recognized in the poetic treatment an authentic echo of Amphitrite's early importance: \"It would have been much simpler for Poseidon to recognize his own son... the myth belongs to that early stratum of mythology when Poseidon was not yet god of the sea, or, at least, no-wise supreme there—Amphitrite and the Nereids ruled there, with their servants the Tritons. Even so late as the \"Iliad\" Amphitrite is not yet 'Neptuni uxor'\" [Neptune's wife]\".\nAmphitrite, \"the third one who encircles [the sea]\", was so entirely confined in her authority to the sea and the creatures in it that she was almost never associated with her husband, either for purposes of worship or in works of art, except when he was to be distinctly regarded as the god who controlled the sea. An exception may be the cult image of Amphitrite that Pausanias saw in the temple of Poseidon at the Isthmus of Corinth (ii.1.7).\n\nPindar, in his sixth Olympian Ode, recognized Poseidon's role as \"great god of the sea, husband of Amphitrite, goddess of the golden spindle.\" For later poets, Amphitrite became simply a metaphor for the sea: Euripides, in \"Cyclops\" (702) and Ovid, \"Metamorphoses\", (i.14).\n\nEustathius said that Poseidon first saw her dancing at Naxos among the other Nereids, and carried her off. But in another version of the myth, she fled from his advances to Atlas, at the farthest ends of the sea; there the dolphin of Poseidon sought her through the islands of the sea, and finding her, spoke persuasively on behalf of Poseidon, if we may believe Hyginus and was rewarded by being placed among the stars as the constellation Delphinus.\nIn the arts of vase-painting and mosaic, Amphitrite was distinguishable from the other Nereids only by her queenly attributes. In works of art, both ancient ones and post-Renaissance paintings, Amphitrite is represented either enthroned beside Poseidon or driving with him in a chariot drawn by sea-horses (\"hippocamps\") or other fabulous creatures of the deep, and attended by Tritons and Nereids. She is dressed in queenly robes and has nets in her hair. The pincers of a crab are sometimes shown attached to her temples.\n\n\n\n\n"}
{"id": "2803219", "url": "https://en.wikipedia.org/wiki?curid=2803219", "title": "Calabrian (stage)", "text": "Calabrian (stage)\n\nCalabrian is a subdivision of the Pleistocene Epoch of the geologic time scale, defined as ~1.8 Ma.—781,000 years ago ± 5,000 years, a period of ~.\n\nThe end of the stage is defined by the last magnetic pole reversal (781 ± 5 Ka) and plunge into an ice age and global drying possibly colder and drier than the late Miocene (Messinian) through early Pliocene (Zanclean) cold period. Originally the Calabrian was a European faunal stage primarily based on mollusk fossils. It has become the second geologic age in the Early Pleistocene. Many of the mammalian faunal assemblages of the Early Pleistocene start in the Gelasian. For example, the Platygonus and other Blancan fauna appear first in the Gelasian.\n\nBecause sea shells are much more abundant as fossils, 19th- and early-20th-century geo-scientists used the plentiful and well-differentiatable Mollusca (mollusks) and Brachiopods to identify stratigraphic boundaries. Thus the Calabrian was originally defined as an assemblage of mollusk fossils, most brachiopods being extinct by then. Efforts were then made to find the best representation of that assemblage in a stratigraphic section. By 1948 scientists used the initial appearance of cool-water (northern) invertebrate faunas in Mediterranean marine sediments as the beginning marker for the Calabrian. The 18th International Geological Congress in London (1948) placed the base of the Pleistocene at the base of the marine strata of the Calabrian Faunal Stage and denominated a type section in southern Italy. However, it was discovered that the original type section was discontinuous at that point and that the base of the Calabrian Stage as defined by fauna assemblages extended to earlier levels within the Pleistocene. A new type section was chosen, several miles from the original one, at Vrica, 4 km south of Crotone in Calabria, southern Italy. Analysis of strontium and oxygen isotopes as well as of planktonic foraminifera has confirmed the viability of the current type section. The 27th International Geological Congress in Moscow in 1984 formally ratified the type section. The starting date was originally thought to be about 1.65 million years ago, but has been recalculated as 1.806 Mya.\n\nThe Global Boundary Stratotype Section and Point, GSSP, for the former start of the Pleistocene is in a reference section at Vrica, 4 km south of Crotone in Calabria, Southern Italy, a location whose exact dating has recently been confirmed by analysis of strontium and oxygen isotopes as well as by planktonic foraminifera.\n\nThe beginning of the Calabrian hence is defined as: Just above top of magnetic polarity chronozone C2n (Olduvai) and the extinction level of calcareous nannofossil \"Discoaster brouweri\" (base Zone CN13). Above the boundary are the lowest occurrence of calcareous nannofossil medium \"Gephyrocapsa spp.\" and the extinction level of the planktonic foraminifer \"Globigerinoides extremus\".\n\nThe end of the Calabrian is defined as the Brunhes–Matuyama magnetic reversal event.\n\n"}
{"id": "1743372", "url": "https://en.wikipedia.org/wiki?curid=1743372", "title": "Carnarvon xeric shrublands", "text": "Carnarvon xeric shrublands\n\nThe Carnarvon xeric shrublands is a deserts and xeric shrublands ecoregion of Western Australia, contiguous with the Carnarvon Interim Biogeographic Regionalisation for Australia (IBRA) bioregion.\n\nThe ecoregion covers an area of 90,500 square kilometers (34,900 square miles) from the bounded by the Indian Ocean to the west from the Peron Peninsula in Shark Bay up to the North West Cape. The Pilbara shrublands lie to the northeast, the Western Australian Mulga shrublands to the east, and the Southwest Australia savanna to the south. The region is named for the coastal town of Carnarvon and includes a number of coastal towns and tourist resorts. \n\nThe terrain is generally low, and the vegetation varies with the underlying geology, which consists mostly of recent alluvial, aeolian, and marine sediments over cretaceous strata. This is a very dry region with less than 250mm of rainfall per year.\n\nLow samphire and saltbush shrublands cover the saline alluvial plains, snakewood scrublands cover the clay flats, Bowgada low woodland covers sandy ridges and plains, red sand dune fields are interspersed or overlain with tree to shrub steppe over hummock grasslands, and \"Acacia startii/A. bivenosa\" shrublands cover limestone outcrops in the north. Other trees in the area include limestone wattle (\"Acacia sclerosperma\") with an undergorwth of dead finish (\"Acacia tetragonophylla\"). The sheltered embayments and extensive tidal flats along the coast support mangroves.\n\nWildlife of the area includes birds such as the thick-billed grasswren and the red-tailed black cockatoo. This is also the area where it is possible that there may be a surviving population of the lesser stick-nest rat which is thought to be extinct.\n\n"}
{"id": "14528806", "url": "https://en.wikipedia.org/wiki?curid=14528806", "title": "Cedar Creek (Michigan)", "text": "Cedar Creek (Michigan)\n\nCedar Creek may refer to several small streams in the U.S. state of Michigan:\n\n"}
{"id": "45311423", "url": "https://en.wikipedia.org/wiki?curid=45311423", "title": "Central Anatolian deciduous forests", "text": "Central Anatolian deciduous forests\n\nThe Central Anatolian deciduous forests is a Palearctic ecoregion of the Temperate broadleaf and mixed forest Biome. It is located in central Anatolia, Asian Turkey.\n"}
{"id": "56128143", "url": "https://en.wikipedia.org/wiki?curid=56128143", "title": "Cupola (cave formation)", "text": "Cupola (cave formation)\n\nA cupola is a recess, indentation or cavity in the ceiling of a lava tube, a kind of cave formation. Cupolas may originate from partial collapse of the ceiling, inflation of the roof by gas or lava pressure, or from a roofed-over site of a former tube overflow. They can be situated anywhere in a lava tube but are most common in lower-level passages.\n"}
{"id": "8432385", "url": "https://en.wikipedia.org/wiki?curid=8432385", "title": "Dispatchable generation", "text": "Dispatchable generation\n\nDispatchable generation refers to sources of electricity that can be used \"on demand\" and dispatched at the request of power grid operators, according to market needs. Dispatchable generators can be turned on or off, or can adjust their power output according to an order. \nThis is in contrast with non-dispatchable renewable energy sources such as wind power and solar PV power which cannot be controlled by operators. \nThe only types of renewable energy that are dispatchable without separate energy storage are biomass, geothermal and ocean thermal energy conversion.\n\nDispatchable plants have different speed at which they can be dispatched.\nThe fastest plants to dispatch are hydroelectric power plants and natural gas power plants. \nFor example, the 1,728 MW Dinorwig pumped storage power plant can reach full output in 16 seconds.\nAlthough theoretically dispatchable, certain thermal plants such as nuclear or coal are designed to run as base load power plants and may take hours or sometimes days to cycle off and then back on again. \n\nThe attractiveness of utility-scale energy storage is that it can compensate for the indeterminacy of wind power and solar PV power. \nDuring 2017, solar thermal storage power has become cheaper and a bulk dispatchable source. \nEarlier, affordable large-scale storage technologies other than hydro were not available. \n\nThe main reasons why dispatchable power plants are needed are:\nUse cases for dispatchable generators comprise:\n\n"}
{"id": "5121209", "url": "https://en.wikipedia.org/wiki?curid=5121209", "title": "Eco-sufficiency", "text": "Eco-sufficiency\n\nEco-sufficiency requires a reduction in the consumption of energy and natural resources and in the generation of waste (among which GHG emissions) in absolute terms.\n\nThis goes beyond the goal of increasing eco-efficiency, which focuses on increasing output per unit of input to foster decoupling. Given a very loose relation in rich countries between GDP and human well-being, pursuing eco-sufficiency may be possible without reducing the latter by, for example, increasing leisure and communal activities.\n\nIn 1993, Wolfgang Sachs summarised sufficiency with \"four Ds\":\n\nThere are potentials to substantially reduce the use of natural resources without compromising human wealth. However, many studies show that economic growth “eats up” these gains at least partially; if the economy grows by 2% per annum, we need an increase in resource productivity of 4% in order to achieve a 2% reduction in absolute resource use. While eco-efficiency aims to improve the effectiveness of material, energy, and land use, eco-sufficiency aims at reducing negative environmental consequences through a reduction of the demand for consumer goods.\n\nEco-sufficiency is closely to related issues of quality of life and work-life balance. It can to some extent be achieved on the individual level, e.g. through energy savings, reduction of transport and change in diets (less meat). However, it also requires a change in social frameworks, provided on the national or European level, including measures such as environmental taxes, environmental planning and new concepts of labour.\n\nEco-sufficiency is largely associated with the idea of altruism – the sober lifestyle adopted by its advocates is a manifestation of their concern for the well-being of future generations – and thus to date is endorsed by only a small minority of members of industrialised societies. A wide spread of eco-sufficiency will only occur, if people find the new lifestyles sufficiently attractive. It needs to be shown that less material wealth can bring increased well-being or happiness to individuals and societies. For example, it has been shown that downsizing or relocating consumers can be motivated by purely selfish reasons such as improving one’s health, avoiding stress, the nostalgia for a “good old time” and so on.\nAn important aspect here is related to the so-called work-and-spend cycle. If leisure and forms of work other than employment (incl. self-employment), such as caring, do it yourself and community work gain relevance, income - and therefore consumption - would be reduced.\n"}
{"id": "32026909", "url": "https://en.wikipedia.org/wiki?curid=32026909", "title": "Enamtila", "text": "Enamtila\n\nEnamtila () is a Sumerian term meaning \"house of life\" or possibly \"house of creation\". It was a sanctuary dedicated to Enlil, likely to have been located within the Ekur at Nippur during the Akkadian Empire. It also referred to various other temples including those to later versions of Enlil; Marduk and Bel as well as one to Ea. It was likely another name for Ehursag, a temple dedicated to Shulgi in Ur. A hymn to Nanna suggests the link \"To Ehursag, the house of the king (we go), to the Enamtila of prince Shulgi we go!\" Another reference in the Inanna - Dunmuzi text translated by Samuel Noah Kramer references the king's palace by this name and possibly makes references to the \"sacred marriage\": \"In the Enamtila, the house of the king, his wife dwelt with him in joy, in the Enamtila, the house of the king, Inanna dwelt with him in joy. Inanna, rejoicing in his house ...\". A fire is reported to have broken out next to the Enamtila in a Babylonian astronomical diary dated to the third century BC. The Enamtila is also referred to as a palace of Ibbi-Sin at Ur in the Lament for Sumer and Ur, \"Its king sat immobilised in his own palace. Ibbi-Suen was sitting in anguish in his own palace. In E-namtila, his place of delight, he wept bitterly. The flood dashing a hoe on the ground was levelling everything.\"\n\n"}
{"id": "33343936", "url": "https://en.wikipedia.org/wiki?curid=33343936", "title": "Energy in Jordan", "text": "Energy in Jordan\n\nEnergy in Jordan describes energy and electricity production, consumption and import in Jordan. Jordan is among the highest in the world in dependency on foreign energy sources, with 96% of the country's energy needs coming from imported oil and natural gas from neighboring Middle Eastern countries. This complete reliance on foreign oil imports consumes a significant amount of Jordan's GDP. This led the country to plan investments of $15 billion in renewable and nuclear energy. To further address these problems, the National Energy Strategy for 2007-2020 was created which projects to boost reliance on domestic energy sources from 4 per cent to 40 per cent by the end of the decade.\n\nMoreover, multiple attacks on the Arab Gas Pipeline from 2011-2014 which supplies 88% of the country's electricity generation needs - forced the country’s power plants onto diesel and heavy fuel oil, costing the treasury millions of dinars and pushing the national energy bill to record highs, over JD4 billion.\n\nPrimary energy use in Jordan was, in 2009, 87 TWh and 15 TWh per million persons and, in 2008, 82 TWh and 14 TWh/million persons.\n\nNatural gas is increasingly being used to fulfill the country’s domestic energy needs, especially with regard to electricity generation. Jordan was estimated to have only modest natural gas reserves (about 6 billion cubic meters in 2002), but new estimates suggest a much higher total. In 2003 the country produced and consumed an estimated 390 million cubic meters of natural gas. The primary source is located in the eastern portion of the country at the Risha gas field. In the past, the country imported the bulk of its natural gas via the Arab Gas Pipeline that stretches from the Al Arish terminal in Egypt underwater to Al Aqabah and then to northern Jordan, where it links to two major power stations. This Egypt–Jordan pipeline supplied Jordan with approximately 1 billion cubic meters (BCM) of natural gas per year.\n\nJordan has developed one gas field, at Risha in the eastern desert near the border with Iraq. The current output of around 30 million cubic feet per day (Mmcf/d) from the Risha field is used to fuel one nearby power plant, which generates about 10% of Jordan's electricity.\n\nIn May 2001, a 30-year agreement had been concluded with Egypt for gas sales to begin at a rate of 100 Mmcf/d beginning in 2003. Construction of the section of the pipeline in Egypt began in late 2001, starting from the existing pipeline terminus at El-Arish in Sinai. This section was completed in mid-2003, allowing deliveries to begin to one power plant at Aqaba.\n\nIn August 2003, Jordan began imports of natural gas from Egypt. The second phase of the project, which connected to the Rihab power plant approximately 70 km north of the capital Amman, was completed in early 2006. The plant site is approximately 835 meters above sea level and located within a rural area surrounded by extensive agricultural land. The Rihab power plant comprises 2 simple cycle gas turbines which are nominally rated at 30 MW gross and a 297 MW combined-cycle gas turbine which comprises 2 gas turbines with 100 MW and 1 steam turbine with 97 MW.\n\nGas supplies from Egypt were halted in 2013 due to insurgent activities in the Sinai and domestic gas shortages in Egypt. In light of this, a liquified natural gas terminal was built in the Port of Aqaba to facilitate gas imports. In 2017, a low-capacity gas pipeline from Israel was completed which supplies the Arab Potash factories near the Dead Sea. As of 2018, a large capacity pipeline from Israel is under contstruction in northern Jordan which is expected to begin operating by 2020 and will supply the kingdom with 3 BCM of gas per year, thereby satisfying the bulk of Jordan’s natural gas consumption needs.\n\nOil shale represents a significant potential resource in Jordan. Oil shale deposits underlie more than 60% of Jordanian territory, and are estimated at 40 to 70 billion tonnes of oil shale. The deposits include a high quality marinite oil shale of Late Cretaceous to early Tertiary age. The most important and investigated deposits are located in west-central Jordan, where they occur at the surface and close to developed infrastructure.\n\nAlthough oil shale was utilized in northern Jordan prior to and during World War I, intensive exploration and studies of Jordan's oil shale resource potential started in the 1970s and 1980s, being motivated by higher oil prices, modern technology and better economic potential. As of 2011, no oil shale industry exists in Jordan, but several companies are considering both shale oil extraction and oil shale combustion for thermal power generation.\n\nA series of projects is set to make available 37,000 tonnes of shale oil in the central region of Attarat by 2015, 15,000 tonnes in Karak by 2017, and develop thousands of tonnes in potential shale reserves along the Jordanian-Iraqi border.\n\n \nJordan has signed memoranda of understanding with the United States, United Kingdom, Canada, France, Japan, China, Russia, Spain, South Korea, Argentina, Romania, and Turkey.\n\nPlans are in place to construct two 1,000MW reactors, nearly doubling the Kingdom's electricity generation capacity, by 2022. Jordan plans to get 60% of its energy needs from nuclear energy by 2035. According to the JAEC, all evaluations took into account the highest safety requirements, including lessons from the Fukushima incident. The plants will be used for electricity generation and desalination.\n\nThe government first chose a site 25 kilometers south of the Red Sea port of Aqaba but shifted the tentative location to the Mafraq area, 40 kilometers northeast of Amman, citing the proximity to the Khirbet Al Samra power plant for using its wastewater to cool the reactor. The decision to relocate the site was taken by the Belgian contractor, Tractabel, which has concluded that the seismic padding required to build on the original site near Aqaba would have led to additional costs of about 15 percent according to JAEC officials.\n\nIn December 2009, Jordan Atomic Energy Commission (JAEC) in cooperation with a consortium headed by the Korean Atomic Energy Research Institute signed an agreement with Daewoo Heavy Industries to build a its first 5 MW research reactor by 2015 at the Jordan University of Science and Technology.\n\nThe National Energy Strategy includes ambitious targets to increase the contribution of renewable energy sources to the national energy supply. The share of renewable energy in the total energy mix is anticipated to reach 7% by 2015 and 10% by 2020. Plans are in place for up to 2,000 MW in solar and wind energy projects by the end of the decade, with 64 international firms expressing interest in various projects across the country. The government is looking to generate 30–50 MW of biomass by 2020. The passage of the \"Renewable Energy and Energy Efficiency Law\", which aims to streamline investment procedures and paves the way for citizens to sell electricity back to the national grid, would go a long way into breathing life into the alternative energy sector. By November 2014 Jordan had 10 MW of installed capacity from renewable energy, and had over 15 renewable energy power plants in progress to be completed by the end of 2015, raising the installed capacity to 500 MW, representing 14% of the overall installed capacity.\n\nJordan lies within the solar belt of the world with average solar radiation ranging between 5 and 7 kilowatt-hour (kWh) per square metre. Decentralized generation from photovoltaic systems in rural and remote villages is currently used for lighting, water pumping and other social services of up to 1000 kW of peak capacity. In addition, about 15% of all households are equipped with solar water heating systems. In May 2012, a 280 kW solar electricity system was inaugurated to be used at El Hassan Science City.\n\nAs per the Energy Master Plan, 30 percent of all households are expected to be equipped with solar water heating system by the year 2020. The government is hoping to construct the first concentrated solar power (CSP) demonstration project in the short to medium term and is considering Aqaba and the south-eastern region for this purpose. It is also planning to have solar desalination plant. According to the national strategy the planned installed capacity will amount to 300–600 MW consisting of CSP, PV and hybrid systems by 2020.\n\nSeveral projects with a total capacity of 400 MW were already allocated in two 200-megawatt tender rounds. First Solar signed a Build-Operate-Maintain contract with the Jordanian government for the 52.5 MW Shams Ma'an Solar PV power plant, with a 20-year power purchase agreement (PPA). Construction of the plant is expected to start in early 2015 and finish in 2016. The \"Shams Ma'an project\" was tendered in the first round and granted a tariff of US 14.8¢ per kilowatt-hour, while the second round drew record-low tariffs of six and seven cents per kilowatt hour for each of the four 50-megawatt projects (US 6.13¢, 6.49¢, 6.91¢ and 7.67¢ per kWh). These tariffs belong to the worldwide lowest so far ever allocated and are not much above the world record tariff of US ¢5.89 per kWh tendered in early 2015 for the second phase of the Mohammed bin Rashid Al Maktoum Solar Park in the United Arab Emirates.\n\nA plan to put solar panels at all 6000 mosques in the country was announced in February 2015.\n\nJordan inaugurated its first solar-powered charging station for electric cars in February 2012. Located at El Hassan Science City (EHSC), the station is considered the first step towards promoting solar-powered vehicles and building more solar-charging facilities on the streets of Jordan.\n\nThe Sahara Forest Project, a Norwegian endeavour to create oases in hot, arid and uninhabited lands, is currently being implemented in the southern city of Aqaba, with the cooperation of the Aqaba Special Economic Zone Authority, to address its food, water and energy challenges. The objective of the project is to enable restorative growth, revegetation, and creation of green jobs through the profitable production of food, freshwater, bio fuels and electricity. The three core components of the Sahara Forest Project are saltwater-cooled greenhouses, concentrated solar power (CSP) for electricity and heat generation, and technologies for desert revegetation.\n\nIn October 2016, Jordan signed a power purchase agreement with Masdar, a clean energy developer based in Abu Dhabi, UAE to build the biggest single solar installation in the country, Baynouna Solar Power Plant, once complete with a 200 MW capacity. The project is expected to be operational by 2018 and deliver 110,000 local households with their annual power needs.\n\nJordan currently operates three wind power plants at Ibrahimyah, Hofa and Tafila. The Ibrahimyah plant, located approximately 80 km north of Amman, consists of 4 wind turbines with capacity 0.08 MW for each. The Hofa plant, located approximately 92 km north of Amman, consists of 5 wind turbines with capacity 0.225 MW for each. The Tafila Wind Farm is located in Tafilah Governorate in south-west Jordan. The Tafila Wind Farm has a capacity of 117 MW and produces 390 gigawatt-hours annually powering 83,000 homes.\n\n"}
{"id": "29770725", "url": "https://en.wikipedia.org/wiki?curid=29770725", "title": "Eric Halsall", "text": "Eric Halsall\n\nEric Halsall (18 March 1920 – 21 October 1996) was an English countryman, author and television presenter. Best known for his fourteen-year run as a commentator on BBC TV series \"One Man and His Dog\".\n\nHalsall was born and brought up in Burnley, Lancashire, attending Burnley Grammar School. He was a keen footballer and had a trial at Burnley F.C.. Turned down by the RAF, he joined the Burnley Express, initially in advertising and then as a writer. He married Rita Greenwood in 1942.\n\nHalsall worked from the 1950s until 1985 as a farms manager for the National Coal Board. However he was best known as presenter of the long-running TV series \"One Man and His Dog\", in which he commentated on sheepdog trials for 14 years from 1975 to 1989. He was also the course director for the series, in charge of laying out the trial fields.\n\nHalsall also served for 21 years as a director of the International Sheep Dog Society, and was a winner of its Wilkinson Sword Trophy Award.\n\nHalsall lived in the Cliviger area, close to Burnley. He was president of the Burnley branch of the Guide Dogs for the Blind Association.\n\n"}
{"id": "9249117", "url": "https://en.wikipedia.org/wiki?curid=9249117", "title": "Frank Bursley Taylor", "text": "Frank Bursley Taylor\n\nFrank Bursley Taylor (1860 – 1938) was an American geologist, the son of a lawyer in Fort Wayne, Indiana. He was a Harvard dropout who studied privately financed in large part by his wealthy father. He became a specialist in the glacial geology of the Great Lakes, and proposed to the Geological Society of America on December 29, 1908 that the continents moved on the Earth's surface, that a shallow region in the Atlantic marks where Africa and South America were once joined, and that the collisions of continents could uplift mountains. His ideas were based on his studies on mountain ranges as the Andes, Rockies, Alps and Himalayas, concluding that these mountains could have been formed only as a result of titanic lateral pressures that thrust the earth's surface upward. \nHis theory was either ignored or opposed by other scientists of his time. He wrote a total ten papers on the subject of continental drift Taylor's ideas about continental drift were independently discovered by Alfred Wegener in Germany three years later, in January 1912, and the theory of continental drift is historically often referred to as the \"Taylor-Wegener hypothesis,\" although Taylor himself disapproved of the hyphenated name. But even with Wegener's extensive extra research the idea did not achieve acceptance until the 1960s when a vast weight of evidence had accrued via Harry Hess, Fred Vine and Drummond Matthews.\n\nIn a later paper he proposed that this occurred by their being dragged towards the equator by tidal forces during the hypothesized capture of the moon, resulting in \"general crustal creep\" toward the equator. The initial key to his proposal, the complementary shapes of the continental masses, had been observed as early as the 16th century by Abraham Ortelius, but had lacked a credible driving force. His own proposition was that the moon was captured by the Earth's gravity during the Cretaceous period 100 million years ago, and came so close to the earth that its tidal pull dragged the continents toward the Equator. This lacked evidence, thus undermining the credibility of the continental drift observation. He had proposed that the continents ploughed through the ocean floors towards the equator, wrinkling their Equator-facing fronts to produce the Himalayas and Alps. Although his proposed mechanism was wrong, he was the first to come to the insight that one of the effects of continental motion would be the formation of mountains due to the collision of continental plates.\n"}
{"id": "41222", "url": "https://en.wikipedia.org/wiki?curid=41222", "title": "Group delay and phase delay", "text": "Group delay and phase delay\n\nIn signal processing, group delay is the time delay of the amplitude envelopes of the various sinusoidal components of a signal through a device under test, and is a function of frequency for each component. Phase delay, in contrast, is the time delay of the \"phase\" as opposed to the time delay of the \"amplitude envelope\".\n\nAll frequency components of a signal are delayed when passed through a device such as an amplifier, a loudspeaker, or propagating through space or a medium, such as air. This signal delay will be different for the various frequencies unless the device has the property of being linear phase (\"Linear phase\" and \"minimum phase\" are often used interchangeably, but are actually quite different.) The delay variation means that signals consisting of multiple frequency components will suffer distortion because these components are not delayed by the same amount of time at the output of the device. This changes the shape of the signal in addition to any constant delay or scale change. A sufficiently large delay variation can cause problems such as poor fidelity in audio or intersymbol interference (ISI) in the demodulation of digital information from an analog carrier signal. High speed modems use adaptive equalizers to compensate for non-constant group delay.\n\nGroup delay is a useful measure of time distortion, and is calculated by differentiating, with respect to frequency, the phase response of the device under test (DUT): the group delay is a measure of the slope of the phase response at any given frequency. Variations in group delay cause signal distortion, just as deviations from linear phase cause distortion.\n\nIn linear time-invariant (LTI) system theory, control theory, and in digital or analog signal processing, the relationship between the input signal, formula_1, to output signal, formula_2, of an LTI system is governed by a convolution operation:\n\nOr, in the frequency domain,\n\nwhere\n\nand\n\nHere formula_8 is the time-domain impulse response of the LTI system and formula_9, formula_10, formula_11, are the Laplace transforms of the input formula_1, output formula_2, and impulse response formula_8, respectively. formula_11 is called the transfer function of the LTI system and, like the impulse response formula_8, \"fully\" defines the input-output characteristics of the LTI system.\n\nSuppose that such a system is driven by a quasi-sinusoidal signal, that is, a sinusoid whose amplitude envelope formula_17 is slowly-changing relative to the frequency formula_18 of the sinusoid. Mathematically, this means that the driving signal has the form\n\nsubject to the assumption\n\nThen the output of such an LTI system is very well approximated as\n\nHere formula_22 and formula_23, the group delay and phase delay respectively, are given by the expressions below (and potentially are functions of the angular frequency formula_18).\n\nIn a linear phase system (with non-inverting gain), both formula_22 and formula_23 are constant (i.e. independent of formula_18) and equal, and their common value equals the overall delay of the system; and the unwrapped phase shift of the system (namely formula_28) is negative, with magnitude increasing linearly with frequency formula_18.\n\nMore generally, it can be shown that for an LTI system with transfer function formula_11 driven by a complex sinusoid of unit amplitude,\n\nthe output is\n\nwhere the phase shift formula_33 is\n\nAdditionally, it can be shown that the group delay, formula_22, and phase delay, formula_23, are frequency-dependent, and they can be computed from the phase shift formula_33 by\n\nIn physics, and in particular in optics, the term group delay has the following meanings:\n\nIt is often desirable for the group delay to be constant across all frequencies; otherwise there is temporal smearing of the signal. Because group delay is formula_45, as defined in (1), it therefore follows that a constant group delay can be achieved if the transfer function of the device or medium has a linear phase response (i.e., formula_46 where the group delay formula_47 is a constant).\nThe degree of nonlinearity of the phase indicates the deviation of the group delay from a constant.\n\nGroup delay has some importance in the audio field and especially in the sound reproduction field. Many components of an audio reproduction chain, notably loudspeakers and multiway loudspeaker crossover networks, introduce group delay in the audio signal. It is therefore important to know the threshold of audibility of group delay with respect to frequency, especially if the audio chain is supposed to provide high fidelity reproduction. The best thresholds of audibility table has been provided by .\n\nFlanagan, Moore and Stone conclude that at 1, 2 and 4 kHz, a group delay of about 1.6 ms is audible with headphones in a non-reverberant condition.\n\nA transmitting apparatus is said to have true time delay (TTD) if the time delay is independent of the frequency of the electrical signal. TTD is an important characteristic of lossless and low-loss, dispersion free, transmission lines. TTD allows for a wide instantaneous signal bandwidth with virtually no signal distortion such as pulse broadening during pulsed operation.\n\n\n"}
{"id": "14098", "url": "https://en.wikipedia.org/wiki?curid=14098", "title": "History of the Americas", "text": "History of the Americas\n\nThe prehistory of the Americas (North, South, and Central America, and the Caribbean) begins with people migrating to these areas from Asia during the height of an Ice Age. These groups are generally believed to have been isolated from peoples of the \"Old World\" until the coming of Europeans in the 10th century from Norway and with the voyages of Christopher Columbus in 1492.\n\nThe ancestors of today's American Indigenous peoples were the Paleo-Indians; they were hunter-gatherers who migrated into North America. The most popular theory asserts that migrants came to the Americas via Beringia, the land mass now covered by the ocean waters of the Bering Strait. Small lithic stage peoples followed megafauna like bison, mammoth (now extinct), and caribou, thus gaining the modern nickname \"big-game hunters.\" Groups of people may also have traveled into North America on shelf or sheet ice along the northern Pacific coast.\n\nCultural traits brought by the first immigrants later evolved and spawned such cultures as Iroquois on North America and Pirahã of South America. These cultures later developed into civilizations. In many cases, these cultures expanded at a later date than their Old World counterparts. Cultures that may be considered advanced or civilized include Norte Chico, Cahokia, Zapotec, Toltec, Olmec, Maya, Aztec, Chimor, Mixtec, Moche, Mississippian, Puebloan, Totonac, Teotihuacan, Huastec people, Purépecha, Izapa, Mazatec, Muisca, and the Inca.After the voyages of Christopher Columbus in 1492, Spanish, Portuguese and later English, French and Dutch colonial expeditions arrived in the New World, conquering and settling the discovered lands, which led to a transformation of the cultural and physical landscape in the Americas. Spain colonized most of the Americas from present-day Southwestern United States, Florida and the Caribbean to the southern tip of South America. Portugal settled in what is mostly present-day Brazil while England established colonies on the Eastern coast of the United States, as well as the North Pacific coast and in most of Canada. France settled in Quebec and other parts of Eastern Canada and claimed an area in what is today the central United States. The Netherlands settled New Netherland (administrative centre New Amsterdam - now New York), some Caribbean islands and parts of Northern South America.\n\nEuropean colonization of the Americas led to the rise of new cultures, civilizations and eventually states, which resulted from the fusion of Native American and European traditions, peoples and institutions. The transformation of American cultures through colonization is evident in architecture, religion, gastronomy, the arts and particularly languages, the most widespread being Spanish (376 million speakers), English (348 million) and Portuguese (201 million). The colonial period lasted approximately three centuries, from the early 16th to the early 19th centuries, when Brazil and the larger Hispanic American nations declared independence. The United States obtained independence from England much earlier, in 1776, while Canada formed a federal dominion in 1867. Others remained attached to their European parent state until the end of the 19th century, such as Cuba and Puerto Rico which were linked to Spain until 1898. Smaller territories such as Guyana obtained independence in the mid-20th century, while certain Caribbean islands and French Guiana remain part of a European power to this day.\n\n \nThe specifics of Paleo-Indian migration to and throughout the Americas, including the exact dates and routes traveled, are subject to ongoing research and discussion. The traditional theory has been that these early migrants moved into the Beringia land bridge between eastern Siberia and present-day Alaska around 40,000 – 17,000 years ago, when sea levels were significantly lowered due to the Quaternary glaciation. These people are believed to have followed herds of now-extinct Pleistocene megafauna along \"ice-free corridors\" that stretched between the Laurentide and Cordilleran ice sheets. Another route proposed is that, either on foot or using primitive boats, they migrated down the Pacific Northwest coast to South America. Evidence of the latter would since have been covered by a sea level rise of a hundred meters following the last ice age.\n\nArchaeologists contend that the Paleo-Indian migration out of Beringia (eastern Alaska), ranges from 40,000 to around 16,500 years ago. This time range is a hot source of debate. The few agreements achieved to date are the origin from Central Asia, with widespread habitation of the Americas during the end of the last glacial period, or more specifically what is known as the late glacial maximum, around 16,000 – 13,000 years before present.\n\nThe American Journal of Human Genetics released an article in 2007 stating \"Here we show, by using 86 complete mitochondrial genomes, that all Indigenous American haplogroups, including Haplogroup X (mtDNA), were part of a single founding population.\" Amerindian groups in the Bering Strait region exhibit perhaps the strongest DNA or mitochondrial DNA relations to Siberian peoples. The genetic diversity of Amerindian indigenous groups increase with distance from the assumed entry point into the Americas. Certain genetic diversity patterns from West to East suggest, particularly in South America, that migration proceeded first down the west coast, and then proceeded eastward. Geneticists have variously estimated that peoples of Asia and the Americas were part of the same population from 42,000 to 21,000 years ago.\n\nNew studies shed light on the founding population of indigenous Americans, suggesting that their ancestry traced to both east Asian and western Eurasians who migrated to North America directly from Siberia. A 2013 study in the journal Nature reported that DNA found in the 24,000-year-old remains of a young boy in Mal’ta Siberia suggest that up to one-third of the indigenous Americans may have ancestry that can be traced back to western Eurasians, who may have \"had a more north-easterly distribution 24,000 years ago than commonly thought\" Professor Kelly Graf said that \"Our findings are significant at two levels. First, it shows that Upper Paleolithic Siberians came from a cosmopolitan population of early modern humans that spread out of Africa to Europe and Central and South Asia. Second, Paleoindian skeletons with phenotypic traits atypical of modern-day Native Americans can be explained as having a direct historical connection to Upper Paleolithic Siberia.\" A route through Beringia is seen as more likely than the Solutrean hypothesis.\n\nOn October 3, 2014, the Oregon cave where the oldest DNA evidence of human habitation in North America was found was added to the National Register of Historic Places. The DNA, radiocarbon dated to 14,300 years ago, was found in fossilized human coprolites uncovered in the Paisley Five Mile Point Caves in south central Oregon.\n\nThe Lithic stage or \"Paleo-Indian period\", is the earliest classification term referring to the first stage of human habitation in the Americas, covering the Late Pleistocene epoch. The time period derives its name from the appearance of \"Lithic flaked\" stone tools. Stone tools, particularly projectile points and scrapers, are the primary evidence of the earliest well known human activity in the Americas. Lithic reduction stone tools are used by archaeologists and anthropologists to classify cultural periods.\n\nSeveral thousand years after the first migrations, the first complex civilizations arose as hunter-gatherers settled into semi-agricultural communities. Identifiable sedentary settlements began to emerge in the so-called Middle Archaic period around 6000 BCE. Particular archaeological cultures can be identified and easily classified throughout the Archaic period.\n\nIn the late Archaic, on the north-central coastal region of Peru, a complex civilization arose which has been termed the Norte Chico civilization, also known as Caral-Supe. It is the oldest known civilization in the Americas and one of the five sites where civilization originated independently and indigenously in the ancient world, flourishing between the 30th and 18th centuries BC. It pre-dated the Mesoamerican Olmec civilization by nearly two millennia. It was contemporaneous with the Egypt following the unification of its kingdom under Narmer and the emergence of the first Egyptian hieroglyphics.\n\nMonumental architecture, including earthwork platform mounds and sunken plazas have been identified as part of the civilization. Archaeological evidence points to the use of textile technology and the worship of common god symbols. Government, possibly in the form of theocracy, is assumed to have been required to manage the region. However, numerous questions remain about its organization. In archaeological nomenclature, the culture was pre-ceramic culture of the pre-Columbian Late Archaic period. It appears to have lacked ceramics and art.\n\nOngoing scholarly debate persists over the extent to which the flourishing of Norte Chico resulted from its abundant maritime food resources, and the relationship that these resources would suggest between coastal and inland sites.\n\nThe role of seafood in the Norte Chico diet has been a subject of scholarly debate. In 1973, examining the Aspero region of Norte Chico, Michael E. Moseley contended that a maritime subsistence (seafood) economy had been the basis of society and its early flourishing. This theory, later termed \"maritime foundation of Andean Civilization\" was at odds with the general scholarly consensus that civilization arose as a result of intensive grain-based agriculture, as had been the case in the emergence of civilizations in northeast Africa (Egypt) and southwest Asia (Mesopotamia).\n\nWhile earlier research pointed to edible domestic plants such as squash, beans, lucuma, guava, pacay, and camote at Caral, publications by Haas and colleagues have added avocado, achira, and corn (Zea Mays) to the list of foods consumed in the region. In 2013, Haas and colleagues reported that maize was a primary component of the diet throughout the period of 3000 to 1800 BC.\n\nCotton was another widespread crop in Norte Chico, essential to the production of fishing nets and textiles. Jonathan Haas noted a mutual dependency, whereby \"The prehistoric residents of the Norte Chico needed the fish resources for their protein and the fishermen needed the cotton to make the nets to catch the fish.\"\n\nIn the 2005 book \"\", journalist Charles C. Mann surveyed the literature at the time, reporting a date \"sometime before 3200 BC, and possibly before 3500 BC\" as the beginning date for the formation of Norte Chico. He notes that the earliest date securely associated with a city is 3500 BC, at Huaricanga in the (inland) Fortaleza area.\n\nThe Norte Chico civilization began to decline around 1800 BC as more powerful centers appeared to the south and north along its coast, and to the east within the Andes Mountains.\n\nAfter the decline of the Norte Chico civilization, several large, centralized civilizations developed in the Western Hemisphere: Chavin, Nazca, Moche, Huari, Quitus, Cañaris, Chimu, Pachacamac, Tiahuanaco, Aymara and Inca in the Central Andes (Ecuador, Peru and Bolivia); Muisca in Colombia ; Taínos in Dominican Republic (Hispaniola, Española) and part of Caribbean; and the Olmecs, Maya, Toltecs, Mixtecs, Zapotecs, Aztecs and Purepecha in southern North America (Mexico, Guatemala).\n\nThe Olmec civilization was the first Mesoamerican civilization, beginning around 1600-1400 BC and ending around 400 BC. Mesoamerica is considered one of the six sites around the globe in which civilization developed independently and indigenously. This civilization is considered the mother culture of the Mesoamerican civilizations. The Mesoamerican calendar, numeral system, writing, and much of the Mesoamerican pantheon seem to have begun with the Olmec.\n\nSome elements of agriculture seem to have been practiced in Mesoamerica quite early. The domestication of maize is thought to have begun around 7,500 to 12,000 years ago. The earliest record of lowland maize cultivation dates to around 5100 BC. Agriculture continued to be mixed with a hunting-gathering-fishing lifestyle until quite late compared to other regions, but by 2700 BC, Mesoamericans were relying on maize, and living mostly in villages. Temple mounds and classes started to appear. By 1300/ 1200 BC, small centres coalesced into the Olmec civilization, which seems to have been a set of city-states, united in religious and commercial concerns. The Olmec cities had ceremonial complexes with earth/clay pyramids, palaces, stone monuments, aqueducts and walled plazas. The first of these centers was at San Lorenzo (until 900 bc). La Venta was the last great Olmec centre. Olmec artisans sculpted jade and clay figurines of Jaguars and humans. Their iconic giant heads - believed to be of Olmec rulers - stood in every major city.\n\nThe Olmec civilization ended in 400 BC, with the defacing and destruction of San Lorenzo and La Venta, two of the major cities. It nevertheless spawned many other states, most notably the Mayan civilization, whose first cities began appearing around 700-600 BC. Olmec influences continued to appear in many later Mesoamerican civilizations.\n\nCities of the Aztecs, Mayas, and Incas were as large and organized as the largest in the Old World, with an estimated population of 200,000 to 350,000 in Tenochtitlan, the capital of the Aztec empire. The market established in the city was said to have been the largest ever seen by the conquistadors when they arrived. The capital of the Cahokians, Cahokia, located near modern East St. Louis, Illinois, may have reached a population of over 20,000. At its peak, between the 12th and 13th centuries, Cahokia may have been the most populous city in North America. Monk's Mound, the major ceremonial center of Cahokia, remains the largest earthen construction of the prehistoric New World.\n\nThese civilizations developed agriculture as well, breeding maize (corn) from having ears 2–5 cm in length to perhaps 10–15 cm in length. Potatoes, tomatoes, pumpkins, beans, avocados, and chocolate are now the most popular of the pre-Columbian agricultural products. The civilizations did not develop extensive livestock as there were few suitable species, although alpacas and llamas were domesticated for use as beasts of burden and sources of wool and meat in the Andes. By the 15th century, maize was being farmed in the Mississippi River Valley after introduction from Mexico. The course of further agricultural development was greatly altered by the arrival of Europeans.\n\n\nCahokia was a major regional chiefdom, with trade and tributary chiefdoms located in a range of areas from bordering the Great Lakes to the Gulf of Mexico.\n\n\nThe Iroquois League of Nations or \"People of the Long House\", based in present-day upstate and western New York, had a confederacy model from the mid-15th century. It has been suggested that their culture contributed to political thinking during the development of the later United States government. Their system of affiliation was a kind of federation, different from the strong, centralized European monarchies.\n\nLeadership was restricted to a group of 50 sachem chiefs, each representing one clan within a tribe; the Oneida and Mohawk people had nine seats each; the Onondagas held fourteen; the Cayuga had ten seats; and the Seneca had eight. Representation was not based on population numbers, as the Seneca tribe greatly outnumbered the others. When a sachem chief died, his successor was chosen by the senior woman of his tribe in consultation with other female members of the clan; property and hereditary leadership were passed matrilineally. Decisions were not made through voting but through consensus decision making, with each sachem chief holding theoretical veto power. The Onondaga were the \"firekeepers\", responsible for raising topics to be discussed. They occupied one side of a three-sided fire (the Mohawk and Seneca sat on one side of the fire, the Oneida and Cayuga sat on the third side.)\n\nElizabeth Tooker, an anthropologist, has said that it was unlikely the US founding fathers were inspired by the confederacy, as it bears little resemblance to the system of governance adopted in the United States. For example, it is based on inherited rather than elected leadership, selected by female members of the tribes, consensus decision-making regardless of population size of the tribes, and a single group capable of bringing matters before the legislative body.\n\nLong-distance trading did not prevent warfare and displacement among the indigenous peoples, and their oral histories tell of numerous migrations to the historic territories where Europeans encountered them. The Iroquois invaded and attacked tribes in the Ohio River area of present-day Kentucky and claimed the hunting grounds. Historians have placed these events as occurring as early as the 13th century, or in the 17th century Beaver Wars.\n\nThrough warfare, the Iroquois drove several tribes to migrate west to what became known as their historically traditional lands west of the Mississippi River. Tribes originating in the Ohio Valley who moved west included the Osage, Kaw, Ponca and Omaha people. By the mid-17th century, they had resettled in their historical lands in present-day Kansas, Nebraska, Arkansas and Oklahoma. The Osage warred with Caddo-speaking Native Americans, displacing them in turn by the mid-18th century and dominating their new historical territories.\n\n\nThe Pueblo people of what is now the Southwestern United States and northern Mexico, living conditions were that of large stone apartment like adobe structures. They live in Arizona, New Mexico, Utah, Colorado, and possibly surrounding areas.\n\nChichimeca was the name that the Mexica (Aztecs) generically applied to a wide range of semi-nomadic peoples who inhabited the north of modern-day Mexico, and carried the same sense as the European term \"barbarian\". The name was adopted with a pejorative tone by the Spaniards when referring especially to the semi-nomadic hunter-gatherer peoples of northern Mexico.\n\nThe Zapotec emerged around 1500 years BCE. Their writing system influenced the later Olmec. They left behind the great city Monte Alban.\n\nThe Olmec civilization emerged around 1200 BCE in Mesoamerica and ended around 400 BCE. Olmec art and concepts influenced surrounding cultures after their downfall. This civilization was thought to be the first in America to develop a writing system. After the Olmecs abandoned their cities for unknown reasons, the Maya, Zapotec and Teotihuacan arose.\n\nThe Purepecha civilization emerged around 1000 CE in Mesoamerica . They flourished from 1100 CE to 1530 CE. They continue to live on in the state of Michoacán. Fierce warriors, they were never conquered and in their glory years, successfully sealed off huge areas from Aztec domination.\n\n\nMaya history spans 3,000 years. The Classic Maya may have collapsed due to changing climate in the end of the 10th century.\n\nThe Toltec were a nomadic people, dating from the 10th - 12th century, whose language was also spoken by the Aztecs.\n\nTeotihuacan (4th century BCE - 7/8th century CE) was both a city, and an empire of the same name, which, at its zenith between 150 and the 5th century, covered most of Mesoamerica.\n\nThe Aztec having started to build their empire around 14th century found their civilization abruptly ended by the Spanish conquistadors. They lived in Mesoamerica, and surrounding lands. Their capital city Tenochtitlan was one of the largest cities of all time.\n\nThe oldest known civilization of the Americas was established in the Norte Chico region of modern Peru. Complex society emerged in the group of coastal valleys, between 3000 and 1800 BCE. The Quipu, a distinctive recording device among Andean civilizations, apparently dates from the era of Norte Chico's prominence.\n\nThe Chavín established a trade network and developed agriculture by as early as (or late compared to the Old World) 900 BCE according to some estimates and archaeological finds. Artifacts were found at a site called Chavín in modern Peru at an elevation of 3,177 meters. Chavín civilization spanned from 900 BCE to 300 BCE.\n\nHolding their capital at the great city of Cusco, the Inca civilization dominated the Andes region from 1438 to 1533.\nKnown as \"Tahuantinsuyu\", or \"the land of the four regions\", in Quechua, the Inca culture was highly distinct and developed. Cities were built with precise, unmatched stonework, constructed over many levels of mountain terrain. Terrace farming was a useful form of agriculture. There is evidence of excellent metalwork and even successful trepanation of the skull in Inca civilization.\n\nAround 1000, the Vikings established a short-lived settlement in Newfoundland, now known as L'Anse aux Meadows. Speculations exist about other Old World discoveries of the New World, but none of these are generally or completely accepted by most scholars.\n\nSpain sponsored a major exploration led by Italian explorer Christopher Columbus in 1492; it quickly led to extensive European colonization of the Americas. The Europeans brought Old World diseases which are thought to have caused catastrophic epidemics and a huge decrease of the native population. Columbus came at a time in which many technical developments in sailing techniques and communication made it possible to report his voyages easily and to spread word of them throughout Europe. It was also a time of growing religious, imperial and economic rivalries that led to a competition for the establishment of colonies.\n\n15th to 19th century colonies in the New World:\n\nThe formation of sovereign states in the New World began with the United States Declaration of Independence of 1776. The American Revolutionary War lasted through the period of the Siege of Yorktown — its last major campaign — in the early autumn of 1781, with peace being achieved in 1783.\nThe Spanish colonies won their independence in the first quarter of the 19th century, in the Spanish American wars of independence. Simón Bolívar and José de San Martín, among others, led their independence struggle. Although Bolivar attempted to keep the Spanish-speaking parts of Latin America politically allied, they rapidly became independent of one another as well, and several further wars were fought, such as the Paraguayan War and the War of the Pacific. (See Latin American integration.) In the Portuguese colony Dom Pedro I (also Pedro IV of Portugal), son of the Portuguese king Dom João VI, proclaimed the country's independence in 1822 and became Brazil's first Emperor. This was peacefully accepted by the crown in Portugal, upon compensation.\n\nSlavery has had a significant role in the economic development of the New World after the colonization of the Americas by the Europeans. The cotton, tobacco, and sugar cane harvested by slaves became important exports for the United States and the Caribbean countries.\n\nAs a part of the British Empire, Canada immediately entered World War I when it broke out in 1914. Canada bore the brunt of several major battles during the early stages of the war, including the use of poison gas attacks at Ypres. Losses became grave, and the government eventually brought in conscription, despite the fact this was against the wishes of the majority of French Canadians. In the ensuing Conscription Crisis of 1917, riots broke out on the streets of Montreal. In neighboring Newfoundland, the new dominion suffered a devastating loss on July 1, 1916, the First day on the Somme.\n\nThe United States stayed out of the conflict until 1917, when it joined the Entente powers. The United States was then able to play a crucial role at the Paris Peace Conference of 1919 that shaped interwar Europe. Mexico was not part of the war, as the country was embroiled in the Mexican Revolution at the time.\n\nThe 1920s brought an age of great prosperity in the United States, and to a lesser degree Canada. But the Wall Street Crash of 1929 combined with drought ushered in a period of economic hardship in the United States and Canada. From 1936 to 1949, there was a popular uprising against the anti-Catholic Mexican government of the time, set off specifically by the anti-clerical provisions of the Mexican Constitution of 1917.\n\nOnce again, Canada found itself at war before its neighbors, however even Canadian contributions were slight before the Japanese attack on Pearl Harbor. The entry of the United States into the war helped to tip the balance in favour of the allies. Two Mexican tankers, transporting oil to the United States, were attacked and sunk by the Germans in the Gulf of Mexico waters, in 1942. The incident happened in spite of Mexico's neutrality at that time. This led Mexico to enter the conflict with a declaration of war on the Axis nations. The destruction of Europe wrought by the war vaulted all North American countries to more important roles in world affairs, especially the United States, which emerged as a \"superpower\".\n\nThe early Cold War era saw the United States as the most powerful nation in a Western coalition of which Mexico and Canada were also a part. In Canada, Quebec was transformed by the Quiet Revolution and the emergence of Quebec nationalism. Mexico experienced an era of huge economic growth after World War II, a heavy industrialization process and a growth of its middle class, a period known in Mexican history as \"El Milagro Mexicano\" (the Mexican miracle). The Caribbean saw the beginnings of decolonization, while on the largest island the Cuban Revolution introduced Cold War rivalries into Latin America.\n\nThe civil rights movement in the U.S. ended Jim Crow and empowered black voters in the 1960s, which allowed black citizens to move into high government offices for the first time since Reconstruction. However, the dominant New Deal coalition collapsed in the mid 1960s in disputes over race and the Vietnam War, and the conservative movement began its rise to power, as the once dominant liberalism weakened and collapsed. Canada during this era was dominated by the leadership of Pierre Elliot Trudeau. In 1982, at the end of his tenure, Canada enshrined a new constitution.\n\nCanada's Brian Mulroney not only ran on a similar platform but also favored closer trade ties with the United States. This led to the Canada-United States Free Trade Agreement in January 1989. Mexican presidents Miguel de la Madrid, in the early 1980s and Carlos Salinas de Gortari in the late 1980s, started implementing liberal economic strategies that were seen as a good move. However, Mexico experienced a strong economic recession in 1982 and the Mexican peso suffered a devaluation. In the United States president Ronald Reagan attempted to move the United States back towards a hard anti-communist line in foreign affairs, in what his supporters saw as an attempt to assert moral leadership (compared to the Soviet Union) in the world community. Domestically, Reagan attempted to bring in a package of privatization and regulation to stimulate the economy.\n\nThe end of the Cold War and the beginning of the era of sustained economic expansion coincided during the 1990s. On January 1, 1994, Canada, Mexico and the United States signed the North American Free Trade Agreement, creating the world's largest free trade area. In 2000, Vicente Fox became the first non-PRI candidate to win the Mexican presidency in over 70 years. The optimism of the 1990s was shattered by the 9/11 attacks of 2001 on the United States, which prompted military intervention in Afghanistan, which also involved Canada. Canada did not support the United States' later move to invade Iraq, however.\n\nIn the U.S. the Reagan Era of conservative national policies, deregulation and tax cuts took control with the election of Ronald Reagan in 1980. By 2010, political scientists were debating whether the election of Barack Obama in 2008 represented an end of the Reagan Era, or was only a reaction against the bubble economy of the 2000s (decade), which burst in 2008 and became the Late-2000s recession with prolonged unemployment.\n\nDespite the failure of a lasting political union, the concept of Central American reunification, though lacking enthusiasm from the leaders of the individual countries, rises from time to time. In 1856–1857 the region successfully established a military coalition to repel an invasion by United States adventurer William Walker. Today, all five nations fly flags that retain the old federal motif of two outer blue bands bounding an inner white stripe. (Costa Rica, traditionally the least committed of the five to regional integration, modified its flag significantly in 1848 by darkening the blue and adding a double-wide inner red band, in honor of the French tricolor).\n\nIn 1907, a Central American Court of Justice was created. On December 13, 1960, Guatemala, El Salvador, Honduras, and Nicaragua established the Central American Common Market (\"CACM\"). Costa Rica, because of its relative economic prosperity and political stability, chose not to participate in the CACM. The goals for the CACM were to create greater political unification and success of import substitution industrialization policies. The project was an immediate economic success, but was abandoned after the 1969 \"Football War\" between El Salvador and Honduras. A Central American Parliament has operated, as a purely advisory body, since 1991. Costa Rica has repeatedly declined invitations to join the regional parliament, which seats deputies from the four other former members of the Union, as well as from Panama and the Dominican Republic.\n\nIn the 1960s and 1970s, the governments of Argentina, Brazil, Chile, and Uruguay were overthrown or displaced by U.S.-aligned military dictatorships. These dictatorships detained tens of thousands of political prisoners, many of whom were tortured and/or killed (on inter-state collaboration, see Operation Condor). Economically, they began a transition to neoliberal economic policies. They placed their own actions within the United States Cold War doctrine of \"National Security\" against internal subversion. Throughout the 1980s and 1990s, Peru suffered from an internal conflict (see Túpac Amaru Revolutionary Movement and Shining Path). Revolutionary movements and right-wing military dictatorships have been common, but starting in the 1980s a wave of democratization came through the continent, and democratic rule is widespread now. Allegations of corruption remain common, and several nations have seen crises which have forced the resignation of their presidents, although normal civilian succession has continued.\n\nInternational indebtedness became a notable problem, as most recently illustrated by Argentina's default in the early 21st century. In recent years, South American governments have drifted to the left, with socialist leaders being elected in Chile, Bolivia, Brazil, Venezuela, and a leftist president in Argentina and Uruguay. Despite the move to the left, South America is still largely capitalist. With the founding of the Union of South American Nations, South America has started down the road of economic integration, with plans for political integration in the European Union style.\n\n"}
{"id": "20145137", "url": "https://en.wikipedia.org/wiki?curid=20145137", "title": "Houtu", "text": "Houtu\n\nHòutǔ () or Hòutǔshén (), also Hòutǔ Niángniáng (in Chinese either or ), otherwise called Dimǔ () or Dimǔ Niángniáng (), is the deity of deep earth and soil in Chinese religion and mythology. Houtu is of ambiguous gender. Houtu is the consort or female form of Tudigong (\"Lord of Local Land\").\n\nHoutu was worshiped by Emperor Wu of Han in 113 BC.\n\nHoutu is featured in some versions of the myth of the Great Flood of China: Yu did not do such a great job of channeling the Yellow River into the sea, dredging the wrong way. Sacred Mother Houtu then made the Yellow River Map, and sent one of her divine messenger birds to tell Yu what to do; specifically, that he should open a channel to the east, to allow the right drainage.\n\n\n"}
{"id": "426456", "url": "https://en.wikipedia.org/wiki?curid=426456", "title": "Ice core", "text": "Ice core\n\nAn ice core is a core sample that is typically removed from an ice sheet or a high mountain glacier. Since the ice forms from the incremental buildup of annual layers of snow, lower layers are older than upper, and an ice core contains ice formed over a range of years. Cores are drilled with hand augers (for shallow holes) or powered drills; they can reach depths of over two miles (3.2 km), and contain ice up to 800,000 years old.\n\nThe physical properties of the ice and of material trapped in it can be used to reconstruct the climate over the age range of the core. The proportions of different oxygen and hydrogen isotopes provide information about ancient temperatures, and the air trapped in tiny bubbles can be analysed to determine the level of atmospheric gases such as carbon dioxide. Since heat flow in a large ice sheet is very slow, the borehole temperature is another indicator of temperature in the past. These data can be combined to find the climate model that best fits all the available data.\n\nImpurities in ice cores may depend on location. Coastal areas are more likely to include material of marine origin, such as sea salt ions. Greenland ice cores contain layers of wind-blown dust that correlate with cold, dry periods in the past, when cold deserts were scoured by wind. Radioactive elements, either of natural origin or created by nuclear testing, can be used to date the layers of ice. Some volcanic events that were sufficiently powerful to send material around the globe have left a signature in many different cores that can be used to synchronise their time scales.\n\nIce cores have been studied since the early 20th century, and several cores were drilled as a result of the International Geophysical Year (1957–1958). Depths of over 400 m were reached, a record which was extended in the 1960s to 2164 m at Byrd Station in Antarctica. Soviet ice drilling projects in Antarctica include decades of work at Vostok Station, with the deepest core reaching 3769 m. Numerous other deep cores in the Antarctic have been completed over the years, including the West Antarctic Ice Sheet project, and cores managed by the British Antarctic Survey and the International Trans-Antarctic Scientific Expedition. In Greenland, a sequence of collaborative projects began in the 1970s with the Greenland Ice Sheet Project; there have been multiple follow-up projects, with the most recent, the East Greenland Ice-Core Project, expected to complete a deep core in east Greenland in 2020.\n\nAn ice core is a vertical column through a glacier, sampling the layers that formed through an annual cycle of snowfall and melt. As snow accumulates, each layer presses on lower layers, making them denser until they turn into firn. Firn is not dense enough to prevent air from escaping; but at a density of about 830 kg/m it turns to ice, and the air within is sealed into bubbles that capture the composition of the atmosphere at the time the ice formed. The depth at which this occurs varies with location, but in Greenland and the Antarctic it ranges from 64 m to 115 m. Because the rate of snowfall varies from site to site, the age of the firn when it turns to ice varies a great deal. At Summit Camp in Greenland, the depth is 77 m and the ice is 230 years old; at Dome C in Antarctica the depth is 95 m and the age 2500 years. As further layers build up, the pressure increases, and at about 1500 m the crystal structure of the ice changes from hexagonal to cubic, allowing air molecules to move into the cubic crystals and form a clathrate. The bubbles disappear and the ice becomes more transparent.\n\nTwo or three feet of snow may turn into less than a foot of ice. The weight above makes deeper layers of ice thin and flow outwards. Ice is lost at the edges of the glacier to icebergs, or to summer melting, and the overall shape of the glacier does not change much with time. The outward flow can distort the layers, so it is desirable to drill deep ice cores at places where there is very little flow. These can be located using maps of the flow lines.\n\nImpurities in the ice provide information on the environment from when they were deposited. These include soot, ash, and other types of particle from forest fires and volcanoes; isotopes such as beryllium-10 created by cosmic rays;\nmicrometeorites; and pollen. The lowest layer of a glacier, called basal ice, is frequently formed of subglacial meltwater that has refrozen. It can be up to about 20 m thick, and though it has scientific value (for example, it may contain subglacial microbial populations), it often does not retain stratigraphic information.\n\nCores are often drilled in areas such as Antarctica and central Greenland where the temperature is almost never warm enough to cause melting, but the summer sun can still alter the snow. In polar areas, the sun is visible day and night during the local summer and invisible all winter. It can make some snow sublimate, leaving the top inch or so less dense. When the sun approaches its lowest point in the sky, the temperature drops and hoar frost forms on the top layer. Buried under the snow of following years, the coarse-grained hoar frost compresses into lighter layers than the winter snow. As a result, alternating bands of lighter and darker ice can be seen in an ice core.\n\nIce cores are collected by cutting around a cylinder of ice in a way that enables it to be brought to the surface. Early cores were often collected with hand augers and they are still used for short holes. A design for ice core augers was patented in 1932 and they have changed little since. An auger is essentially a cylinder with helical metal ribs (known as flights) wrapped around the outside, at the lower end of which are cutting blades. Hand augers can be rotated by a T handle or a brace handle, and some can be attached to handheld electric drills to power the rotation. With the aid of a tripod for lowering and raising the auger, cores up to 50 m deep can be retrieved, but the practical limit is about 30 m for engine-powered augers, and less for hand augers. Below this depth, electromechanical or thermal drills are used.\n\nThe cutting apparatus of a drill is on the bottom end of a drill barrel, the tube that surrounds the core as the drill cuts downward. The cuttings (chips of ice cut away by the drill) must be drawn up the hole and disposed of or they will reduce the cutting efficiency of the drill. They can be removed by compacting them into the walls of the hole or into the core, by air circulation (dry drilling), or by the use of a drilling fluid (wet drilling). Dry drilling is limited to about 400 m depth, since below that point a hole would close up as the ice deforms from the weight of the ice above.\n\nDrilling fluids are chosen to balance the pressure so that the hole remains stable. The fluid must have a low kinematic viscosity to reduce tripping time (the time taken to pull the drilling equipment out of the hole and return it to the bottom of the hole). Since retrieval of each segment of core requires tripping, a slower speed of travel through the drilling fluid could add significant time to a project—a year or more for a deep hole. The fluid must contaminate the ice as little as possible; it must have low toxicity, for safety and to minimize the effect on the environment; it must be available at a reasonable cost; and it must be relatively easy to transport. Historically, there have been three main types of ice drilling fluids: two-component fluids based on kerosene-like products mixed with fluorocarbons to increase density; alcohol compounds, including aqueous ethylene glycol and ethanol solutions; and esters, including n-butyl acetate. Newer fluids have been proposed, including new ester-based fluids, low-molecular weight dimethyl siloxane oils, fatty-acid esters, and kerosene-based fluids mixed with foam-expansion agents.\n\nRotary drilling is the main method of drilling for minerals and it has also been used for ice drilling. It uses a string of drill pipe rotated from the top, and drilling fluid is pumped down through the pipe and back up around it. The cuttings are removed from the fluid at the top of the hole and the fluid is then pumped back down. This approach requires long trip times, since the entire drill string must be hoisted out of the hole, and each length of pipe must be separately disconnected, and then reconnected when the drill string is reinserted. Along with the logistical difficulties associated with bringing heavy equipment to ice sheets, this makes traditional rotary drills unattractive. In contrast, wireline drills allow the removal of the core barrel from the drill assembly while it is still at the bottom of the borehole. The core barrel is hoisted to the surface, and the core removed; the barrel is lowered again and reconnected to the drill assembly. Another alternative is flexible drill-stem rigs, in which the drill string is flexible enough to be coiled when at the surface. This eliminates the need to disconnect and reconnect the pipes during a trip.The need for a string of drillpipe that extends from the surface to the bottom of the borehole can be eliminated by suspending the entire downhole assembly on an armoured cable that conveys power to the downhole motor. These cable-suspended drills can be used for both shallow and deep holes; they require an anti-torque device, such as leaf-springs that press against the borehole, to prevent the drill assembly rotating around the drillhead as it cuts the core. The drilling fluid is usually circulated down around the outside of the drill and back up between the core and core barrel; the cuttings are stored in the downhole assembly, in a chamber above the core. When the core is retrieved, the cuttings chamber is emptied for the next run. Some drills have been designed to retrieve a second annular core outside the central core, and in these drills the space between the two cores can be used for circulation. Cable-suspended drills have proved to be the most reliable design for deep ice drilling.\n\nThermal drills, which cut ice by electrically heating the drill head, can also be used, but they have some disadvantages. Some have been designed for working in cold ice; they have high power consumption and the heat they produce can degrade the quality of the retrieved ice core. Early thermal drills, designed for use without drilling fluid, were limited in depth as a result; later versions were modified to work in fluid-filled holes but this slowed down trip times, and these drills retained the problems of the earlier models. In addition, thermal drills are typically bulky and can be impractical to use in areas where there are logistical difficulties. More recent modifications include the use of antifreeze, which eliminates the need for heating the drill assembly and hence reduces the power needs of the drill. Hot-water drills use jets of hot water at the drill head to melt the water around the core. The drawbacks are that it is difficult to accurately control the dimensions of the borehole, the core cannot easily be kept sterile, and the heat may cause thermal shock to the core.\n\nWhen drilling in temperate ice, thermal drills have an advantage over electromechanical (EM) drills: ice melted by pressure can refreeze on EM drill bits, reducing cutting efficiency, and can clog other parts of the mechanism. EM drills are also more likely to fracture ice cores where the ice is under high stress.\n\nWhen drilling deep holes, which require drilling fluid, the hole must be cased (fitted with a cylindrical lining), since otherwise the drilling fluid will be absorbed by the snow and firn. The casing has to reach down to the impermeable ice layers. To install casing a shallow auger can be used to create a pilot hole, which is then reamed (expanded) until it is wide enough to accept the casing; a large diameter auger can also be used, avoiding the need for reaming. An alternative to casing is to use water in the borehole to saturate the porous snow and firn; the water eventually turns to ice.\n\nIce cores from different depths are not all equally in demand by scientific investigators, which can lead to a shortage of ice cores at certain depths. To address this, work has been done on technology to drill replicate cores: additional cores, retrieved by drilling into the sidewall of the borehole, at depths of particular interest. Replicate cores were successfully retrieved at WAIS divide in the 2012-2013 drilling season, at four different depths.\n\nThe logistics of any coring project are complex because the locations are usually difficult to reach, and may be at high altitude. The largest projects require years of planning and years to execute, and are usually run as international consortiums. The EastGRIP project, for example, which as of 2017 is drilling in eastern Greenland, is run by the Centre for Ice and Climate, in Denmark, and includes representatives from 12 countries on its steering committee. Over the course of a drilling season, scores of people work at the camp, and logistics support includes airlift capabilities provided by the US Air National Guard, using Hercules transport planes owned by the National Science Foundation. In 2015 the EastGRIP team moved the camp facilities from NEEM, a previous Greenland ice core drilling site, to the EastGRIP site. Drilling is expected to continue until at least 2020.\n\nWith some variation between projects, the following steps must occur between drilling and final storage of the ice core.\n\nThe drill removes an annulus of ice around the core but does not cut under it. A spring-loaded lever arm called a core dog can break off the core and hold it in place while it is brought to the surface. The core is then extracted from the drill barrel, usually by laying it out flat so that the core can slide out onto a prepared surface. The core must be cleaned of drilling fluid as it is slid out; for the WAIS Divide coring project, a vacuuming system was set up to facilitate this. The surface that receives the core should be aligned as accurately as possible with the drill barrel to minimise mechanical stress on the core, which can easily break. The ambient temperature is kept well below freezing to avoid thermal shock.\n\nA log is kept with information about the core, including its length and the depth it was retrieved from, and the core may be marked to show its orientation. It is usually cut into shorter sections, the standard length in the US being one metre. The cores are then stored on site, usually in a space below snow level to simplify temperature maintenance, though additional refrigeration can be used. If more drilling fluid must be removed, air may be blown over the cores. Any samples needed for preliminary analysis are taken. The core is then bagged, often in polythene, and stored for shipment. Additional packing, including padding material, is added. When the cores are flown from the drilling site, the aircraft's flight deck is unheated to help maintain a low temperature; when they are transported by ship they must be kept in a refrigeration unit.\n\nThere are several locations around the world that store ice cores, such as the National Ice Core Laboratory in the US. These locations make samples available for testing. A substantial fraction of each core is archived for future analyses.\n\nOver a depth range known as the brittle ice zone, bubbles of air are trapped in the ice under great pressure. When the core is brought to the surface, the bubbles can exert a stress that exceeds the tensile strength of the ice, resulting in cracks and spall. At greater depths, the air disappears into clathrates and the ice becomes stable again. At the WAIS Divide site, the brittle ice zone was from 520 m to 1340 m depth.\n\nThe brittle ice zone typically returns poorer quality samples than for the rest of the core. Some steps can be taken to alleviate the problem. Liners can be placed inside the drill barrel to enclose the core before it is brought to the surface, but this makes it difficult to clean off the drilling fluid. In mineral drilling, special machinery can bring core samples to the surface at bottom-hole pressure, but this is too expensive for the inaccessible locations of most drilling sites. Keeping the processing facilities at very low temperatures limits thermal shocks. Cores are most brittle at the surface, so another approach is to break them into 1 m lengths in the hole. Extruding the core from the drill barrel into a net helps keep it together if it shatters. Brittle cores are also often allowed to rest in storage at the drill site for some time, up to a full year between drilling seasons, to let the ice gradually relax.\n\nMany different kinds of analysis are performed on ice cores, including visual layer counting, tests for electrical conductivity and physical properties, and assays for inclusion of gases, particles, radionuclides, and various molecular species. For the results of these tests to be useful in the reconstruction of palaeoenvironments, there has to be a way to determine the relationship between depth and age of the ice. The simplest approach is to count layers of ice that correspond to the original annual layers of snow, but this is not always possible. An alternative is to model the ice accumulation and flow to predict how long it takes a given snowfall to reach a particular depth. Another method is to correlate radionuclides or trace atmospheric gases with other timescales such as periodicities in the earth's orbital parameters.\n\nA difficulty in ice core dating is that gases can diffuse through firn, so the ice at a given depth may be substantially older than the gases trapped in it. As a result, there are two chronologies for a given ice core: one for the ice, and one for the trapped gases. To determine the relationship between the two, models have been developed for the depth at which gases are trapped for a given location, but their predictions have not always proved reliable. At locations with very low snowfall, such as Vostok, the uncertainty in the difference between ages of ice and gas can be over 1,000 years.\n\nThe density and size of the bubbles trapped in ice provide an indication of crystal size at the time they formed. The size of a crystal is related to its growth rate, which in turn depends on the temperature, so the properties of the bubbles can be combined with information on accumulation rates and firn density to calculate the temperature when the firn formed.\n\nRadiocarbon dating can be used on the carbon in trapped . In the polar ice sheets there is about 15–20 µg of carbon in the form of in each kilogram of ice, and there may also be carbonate particles from wind-blown dust (loess). The can be isolated by subliming the ice in a vacuum, keeping the temperature low enough to avoid the loess giving up any carbon. The results have to be corrected for the presence of produced directly in the ice by cosmic rays, and the amount of correction depends strongly on the location of the ice core. Corrections for produced by nuclear testing have much less impact on the results. Carbon in particulates can also be dated by separating and testing the water-insoluble organic components of dust. The very small quantities typically found require at least 300 g of ice to be used, limiting the ability of the technique to precisely assign an age to core depths.\n\nTimescales for ice cores from the same hemisphere can usually be synchronised using layers that include material from volcanic events. It is more difficult to connect the timescales in different hemispheres. The Laschamp event, a geomagnetic reversal about 40,000 years ago, can be identified in cores; away from that point, measurements of gases such as (methane) can be used to connect the chronology of a Greenland core (for example) with an Antarctic core. In cases where volcanic tephra is interspersed with ice, it can be dated using argon/argon dating and hence provide fixed points for dating the ice. Uranium decay has also been used to date ice cores. Another approach is to use Bayesian probability techniques to find the optimal combination of multiple independent records. This approach was developed in 2010 and has since been turned into a software tool, DatIce.\n\nThe boundary between the Pleistocene and the Holocene, about 11,700 years ago, is now formally defined with reference to data on Greenland ice cores. Formal definitions of stratigraphic boundaries allow scientists in different locations to correlate their findings. These often involve fossil records, which are not present in ice cores, but cores have extremely precise palaeoclimatic information that can be correlated with other climate proxies.\n\nThe dating of ice sheets has proved to be a key element in providing dates for palaeoclimatic records. According to Richard Alley, \"In many ways, ice cores are the ‘rosetta stones’ that allow development of a global network of accurately dated paleoclimatic records using the best ages determined anywhere on the planet\".\n\nCores show visible layers, which correspond to annual snowfall at the core site. If a pair of pits is dug in fresh snow with a thin wall between them and one of the pits is roofed over, an observer in the roofed pit will see the layers revealed by sunlight shining through. A six-foot pit may show anything from less than a year of snow to several years of snow, depending on the location. Poles left in the snow from year to year show the amount of accumulated snow each year, and this can be used to verify that the visible layer in a snow pit corresponds to a single year's snowfall.\n\nIn central Greenland a typical year might produce two or three feet of winter snow, plus a few inches of summer snow. When this turns to ice, the two layers will make up no more than a foot of ice. The layers corresponding to the summer snow will contain bigger bubbles than the winter layers, so the alternating layers remain visible, which makes it possible to count down a core and determine the age of each layer. As the depth increases to the point where the ice structure changes to a clathrate, the bubbles are no longer visible, and the layers can no longer be seen. Dust layers may now become visible. Ice from Greenland cores contains dust carried by wind; the dust appears most strongly in late winter, and appears as cloudy grey layers. These layers are stronger and easier to see at times in the past when the earth's climate was cold, dry, and windy.\n\nAny method of counting layers eventually runs into difficulties as the flow of the ice causes the layers to become thinner and harder to see with increasing depth. The problem is more acute at locations where accumulation is high; low accumulation sites, such as central Antarctica, must be dated by other methods. For example, at Vostok, layer counting is only possible down to an age of 55,000 years.\n\nWhen there is summer melting, the melted snow refreezes lower in the snow and firn, and the resulting layer of ice has very few bubbles so is easy to recognise in a visual examination of a core. Identification of these layers, both visually and by measuring density of the core against depth, allows the calculation of a melt-feature percentage (MF): an MF of 100% would mean that every year's deposit of snow showed evidence of melting. MF calculations are averaged over multiple sites or long time periods in order to smooth the data. Plots of MF data over time reveal variations in the climate, and have shown that since the late 20th century melting rates have been increasing.\n\nIn addition to manual inspection and logging of features identified in a visual inspection, cores can be optically scanned so that a digital visual record is available. This requires the core to be cut lengthwise, so that a flat surface is created.\n\nThe isotopic composition of the oxygen in a core can be used to model the temperature history of the ice sheet. Oxygen has three stable isotopes, , and . The ratio between and indicates the temperature when the snow fell. Because is lighter than , water containing is slightly more likely to turn into vapour, and water containing is slightly more likely to condense from vapour into rain or snow crystals. At lower temperatures, the difference is more pronounced. The standard method of recording the / ratio is to subtract the ratio in a standard known as standard mean ocean water (SMOW):\n\nformula_1\n\nwhere the ‰ sign indicates parts per thousand. A sample with the same / ratio as SMOW has a of 0‰; a sample that is depleted in has a negative . Combining the measurements of an ice core sample with the borehole temperature at the depth it came from provides additional information, in some cases leading to significant corrections to the temperatures deduced from the data. Not all boreholes can be used in these analyses. If the site has experienced significant melting in the past, the borehole will no longer preserve an accurate temperature record.\n\nHydrogen ratios can also be used to calculate a temperature history. Deuterium (, or D) is heavier than hydrogen () and makes water more likely to condense and less likely to evaporate. A ratio can be defined in the same way as . There is a linear relationship between and :\n\nformula_2\n\nwhere d is the deuterium excess. It was once thought that this meant it was unnecessary to measure both ratios in a given core, but in 1979 Merlivat and Jouzel showed that the deuterium excess reflects the temperature, relative humidity, and wind speed of the ocean where the moisture originated. Since then it has been customary to measure both.\n\nWater isotope records, analyzed in cores from Camp Century and Dye 3 in Greenland, were instrumental in the discovery of Dansgaard-Oeschger events—rapid warming at the onset of an interglacial, followed by slower cooling. Other isotopic ratios have been studied, for example, the ratio between and can provide information about past changes in the carbon cycle. Combining this information with records of carbon dioxide levels, also obtained from ice cores, provides information about the mechanisms behind changes in over time.\n\nIt was understood in the 1960s that analyzing the air trapped in ice cores would provide useful information on the paleoatmosphere, but it was not until the late 1970s that a reliable extraction method was developed. Early results included a demonstration that the concentration was 30% less at the last glacial maximum than just before the start of the industrial age. Further research has demonstrated a reliable correlation between levels and the temperature calculated from ice isotope data.\n\nBecause (methane) is produced in lakes and wetlands, the amount in the atmosphere is correlated with the strength of monsoons, which are in turn correlated with the strength of low-latitude summer insolation. Since insolation depends on orbital cycles, for which a timescale is available from other sources, can be used to determine the relationship between core depth and age. (nitrous oxide) levels are also correlated with glacial cycles, though at low temperatures the graph differs somewhat from the and graphs. Similarly, the ratio between (nitrogen) and (oxygen) can be used to date ice cores: as air is gradually trapped by the snow turning to firn and then ice, is lost more easily than , and the relative amount of correlates with the strength of local summer insolation. This means that the trapped air retains, in the ratio of to , a record of the summer insolation, and hence combining this data with orbital cycle data establishes an ice core dating scheme.\n\nDiffusion within the firn layer causes other changes that can be measured. Gravity causes heavier molecules to be enriched at the bottom of a gas column, with the amount of enrichment depending on the difference in mass between the molecules. Colder temperatures cause heavier molecules to be more enriched at the bottom of a column. These fractionation processes in trapped air, determined by the measurement of the / ratio and of neon, krypton and xenon, have been used to infer the thickness of the firn layer, and determine other palaeoclimatic information such as past mean ocean temperatures. Some gases such as helium can rapidly diffuse through ice, so it may be necessary to test for these \"fugitive gases\" within minutes of the core being retrieved to obtain accurate data. Chlorofluorocarbons (CFCs), which contribute to the greenhouse effect and also cause ozone loss in the stratosphere, can be detected in ice cores after about 1950; almost all CFCs in the atmosphere were created by human activity.\n\nSummer snow in Greenland contains some sea salt, blown from the surrounding waters; there is less of it in winter, when much of the sea surface is covered by pack ice. Similarly, hydrogen peroxide appears only in summer snow because its production in the atmosphere requires sunlight. These seasonal changes can be detected because they lead to changes in the electrical conductivity of the ice. Placing two electrodes with a high voltage between them on the surface of the ice core gives a measurement of the conductivity at that point. Dragging them down the length of the core, and recording the conductivity at each point, gives a graph that shows an annual periodicity. Such graphs also identify chemical changes caused by non-seasonal events such as forest fires and major volcanic eruptions. When a known volcanic event, such as the eruption of Laki in Iceland in 1783, can be identified in the ice core record, it provides a cross-check on the age determined by layer counting. Material from Laki can be identified in Greenland ice cores, but did not spread as far as Antarctica; the 1815 eruption of Tambora in Indonesia injected material into the stratosphere, and can be identified in both Greenland and Antarctic ice cores. If the date of the eruption is not known, but it can be identified in multiple cores, then dating the ice can in turn give a date for the eruption, which can then be used as a reference layer. This was done, for example, in an analysis of the climate for the period from 535 to 550 AD, which was thought to be influenced by an otherwise unknown tropical eruption in about 533 AD; but which turned out to be caused by two eruptions, one in 535 or early 536 AD, and a second one in 539 or 540 AD. There are also more ancient reference points, such as the eruption of Toba about 72,000 years ago.\n\nMany other elements and molecules have been detected in ice cores. In 1969, it was discovered that lead levels in Greenland ice had increased by a factor of over 200 since pre-industrial times, and increases in other elements produced by industrial processes, such as copper, cadmium, and zinc, have also been recorded. The presence of nitric and sulfuric acid ( and ) in precipitation can be shown to correlate with increasing fuel combustion over time. Methanesulfonate (MSA) () is produced in the atmosphere by marine organisms, so ice core records of MSA provide information on the history of the oceanic environment. Both hydrogen peroxide () and formaldehyde () have been studied, along with organic molecules such as carbon black that are linked to vegetation emissions and forest fires. Some species, such as calcium and ammonium, show strong seasonal variation. In some cases there are contributions from more than one source to a given species: for example, Ca comes from dust as well as from marine sources; the marine input is much greater than the dust input and so although the two sources peak at different times of the year, the overall signal shows a peak in the winter, when the marine input is at a maximum. Seasonal signals can be erased at sites where the accumulation is low, by surface winds; in these cases it is not possible to date individual layers of ice between two reference layers.\n\nSome of the deposited chemical species may interact with the ice, so what is detected in an ice core is not necessarily what was originally deposited. Examples include HCHO and . Another complication is that in areas with low accumulation rates, deposition from fog can increase the concentration in the snow, sometimes to the point where the atmospheric concentration could be overestimated by a factor of two.\nGalactic cosmic rays produce in the atmosphere at a rate that depends on the solar magnetic field. The strength of the field is related to the intensity of solar radiation, so the level of in the atmosphere is a proxy for climate. Accelerator mass spectrometry can detect the low levels of in ice cores, about 10,000 atoms in a gram of ice, and these can be used to provide long-term records of solar activity. Tritium (), created by nuclear weapons testing in the 1950s and 1960s, has been identified in ice cores, and both Cl and have been found in ice cores in Antarctica and Greenland. Chlorine-36, which has a half-life of 301,000 years, has been used to date cores, as have krypton (, with a half-life of 11 years), lead (, 22 years), and silicon (, 172 years).\n\nMeteorites and micrometeorites that land on polar ice are sometimes concentrated by local environmental processes. For example, there are places in Antarctica where winds evaporate surface ice, concentrating the solids that are left behind, including meteorites. Meltwater ponds can also contain meteorites. At the South Pole Station, ice in a well is melted to provide a water supply, leaving micrometeorites behind. These have been collected by a robotic \"vacuum cleaner\" and examined, leading to improved estimates of their flux and mass distribution. The well is not an ice core, but the age of the ice that was melted is known, so the age of the recovered particles can be determined. The well becomes about 10 m deeper each year, so micrometeorites collected in a given year are about 100 years older than those from the previous year. Pollen, an important component of sediment cores, can also be found in ice cores. It provides information on changes in vegetation.\n\nIn addition to the impurities in a core and the isotopic composition of the water, the physical properties of the ice are examined. Features such as crystal size and axis orientation can reveal the history of ice flow patterns in the ice sheet. The crystal size can also be used to determine dates, though only in shallow cores.\n\nIn 1841 and 1842, Louis Agassiz drilled holes in the Unteraargletscher in the Alps; these were drilled with iron rods and did not produce cores. The deepest hole achieved was 60 m. On Erich von Drygalski's Antarctic expedition in 1902 and 1903, 30 m holes were drilled in an iceberg south of the Kerguelen Islands and temperature readings were taken. The first scientist to create a snow sampling tool was James E. Church, described by Pavel Talalay as \"the father of modern snow surveying\". In the winter of 1908–1909, Church constructed steel tubes with slots and cutting heads to retrieve cores of snow up to 3 m long. Similar devices are in use today, modified to allow sampling to a depth of about 9 m. They are simply pushed into the snow and rotated by hand.\n\nThe first systematic study of snow and firn layers was by Ernst Sorge, who was part of the Alfred Wegener Expedition to central Greenland in 1930–1931. Sorge dug a 15 m pit to examine the snow layers, and his results were later formalized into Sorge's Law of Densification by Henri Bader, who went on to do additional coring work in northwest Greenland in 1933. In the early 1950s, a SIPRE expedition took pit samples over much of the Greenland ice sheet, obtaining early oxygen isotope ratio data. Three other expeditions in the 1950s began ice coring work: a joint Norwegian-British-Swedish Antarctic Expedition (NBSAE), in Queen Maud Land in Antarctica; the Juneau Ice Field Research Project (JIRP), in Alaska; and Expéditions Polaires Françaises, in central Greenland. Core quality was poor, but some scientific work was done on the retrieved ice.\n\nThe International Geophysical Year (1957–1958) saw increased glaciology research around the world, with one of the high priority research targets being deep cores in polar regions. SIPRE conducted pilot drilling trials in 1956 (to 305 m) and 1957 (to 411 m) at Site 2 in Greenland; the second core, with the benefit of the previous year's drilling experience, was retrieved in much better condition, with fewer gaps. In Antarctica, a 307 m core was drilled at Byrd Station in 1957–1958, and a 264 m core at Little America V, on the Ross Ice Shelf, the following year. The success of the IGY core drilling led to increased interest in improving ice coring capabilities, and was followed by a CRREL project at Camp Century, where in the early 1960s three holes were drilled, the deepest reaching the base of the ice sheet at 1387 m in July 1966. The drill used at Camp Century then went to Byrd Station, where a 2164 m hole was drilled to bedrock before the drill was frozen into the borehole by sub-ice meltwater and had to be abandoned.\n\nFrench, Australian and Canadian projects from the 1960s and 1970s include a 905 m core at Dome C in Antarctica, drilled by CNRS; cores at Law Dome drilled by ANARE, starting in 1969 with a 382 m core; and Devon Ice Cap cores recovered by a Canadian team in the 1970s.\n\nSoviet ice drilling projects began in the 1950s, in Franz Josef Land, the Urals, Novaya Zemlya, and at Mirny and Vostok in the Antarctic; not all these early holes retrieved cores. Over the following decades work continued at multiple locations in Asia. Drilling in the Antarctic focused mostly on Mirny and Vostok, with a series of deep holes at Vostok begun in 1970. The first deep hole at Vostok reached 506.9 m in April 1970; by 1973 a depth of 952 m had been reached. A subsequent hole, Vostok 2, drilled from 1971 to 1976, reached 450 m, and Vostok 3 reached 2202 m in 1985 after six drilling seasons. Vostok 3 was the first core to retrieve ice from the previous glacial period, 150,000 years ago. Drilling was interrupted by a fire at the camp in 1982, but further drilling began in 1984, eventually reaching 2546 m in 1989. A fifth Vostok core was begun in 1990, reached 3661 m in 2007, and was later extended to 3769 m. The estimated age of the ice is 420,000 years at 3310 m depth; below that point it is difficult to interpret the data reliably because of mixing of the ice.\n\nEPICA, a European ice coring collaboration, was formed in the 1990s, and two holes were drilled in East Antarctica: one at Dome C, which reached 2871 m in only two seasons of drilling, but which took another four years to reach bedrock at 3260 m; and one at Kohnen Station, which reached bedrock at 2760 m in 2006. The Dome C core had very low accumulation rates, which mean that the climate record extended a long way; by the end of the project the usable data extended to 800,000 years ago.\n\nOther deep Antarctic cores included a Japanese project at Dome F, which reached 2503 m in 1996, with an estimated age of 330,000 years for the bottom of the core; and a subsequent hole at the same site which reached 3035 m in 2006, estimated to reach ice 720,000 years old. US teams drilled at McMurdo Station in the 1990s, and at Taylor Dome (554 m in 1994) and Siple Dome (1004 m in 1999), with both cores reaching ice from the last glacial period. The West Antarctic Ice Sheet (WAIS) project, completed in 2011, reached 3405 m; the site has high snow accumulation so the ice only extends back 62,000 years, but as a consequence, the core provides high resolution data for the period it covers. A 948 m core was drilled at Berkner Island by a project managed by the British Antarctic Survey from 2002 to 2005, extending into the last glacial period; and an Italian-managed ITASE project completed a 1620 m core at Talos Dome in 2007.\n\nIn 2016, cores were retrieved from the Allan Hills in Antarctica in an area where old ice lay near the surface. The cores were dated by potassium-argon dating; traditional ice core dating is not possible as not all layers were present. The oldest core was found to include ice from 2.7 million years ago—by far the oldest ice yet dated from a core.\n\nIn 1970, scientific discussions began which resulted in the Greenland Ice Sheet Project (GISP), a multinational investigation into the Greenland ice sheet that lasted until 1981. Years of field work were required to determine the ideal location for a deep core; the field work included several intermediate-depth cores, at Dye 3 (372 m in 1971), Milcent (398 m in 1973) and Crete (405 m in 1974), among others. A location in north-central Greenland was selected as ideal, but financial constraints forced the group to drill at Dye 3 instead, beginning in 1979. The hole reached bedrock at 2037 m, in 1981. Two holes, 30 km apart, were eventually drilled at the north-central location in the early 1990s by two groups: GRIP, a European consortium, and GISP-2, a group of US universities. GRIP reached bedrock at 3029 m in 1992, and GISP-2 reached bedrock at 3053 m the following year. Both cores were limited to about 100,000 years of climatic information, and since this was thought to be connected to the topography of the rock underlying the ice sheet at the drill sites, a new site was selected 200 km north of GRIP, and a new project, NorthGRIP, was launched as an international consortium led by Denmark. Drilling began in 1996; the first hole had to be abandoned at 1400 m in 1997, and a new hole was begun in 1999, reaching 3085 m in 2003. The hole did not reach bedrock, but terminated at a subglacial river. The core provided climatic data back to 123,000 years ago, which covered part of the last interglacial period. The subsequent North Greenland Eemian (NEEM) project retrieved a 2537 m core in 2010 from a site further north, extending the climatic record to 128,500 years ago; NEEM was followed by EastGRIP, which began in 2015 in east Greenland and is expected to be complete in 2020.\n\nIce cores have been drilled at locations away from the poles, notably in the Himalayas and the Andes. Some of these cores reach back to the last glacial period, but they are more important as records of El Niño events and of monsoon seasons in south Asia. Cores have also been drilled on Mount Kilimanjaro, in the Alps, and in Indonesia, New Zealand, Iceland, Scandinavia, Canada, and the US.\n\nIPICS (International Partnerships in Ice Core Sciences) has produced a series of white papers outlining future challenges and scientific goals for the ice core science community. These include plans to:\n\n\n\n"}
{"id": "41319353", "url": "https://en.wikipedia.org/wiki?curid=41319353", "title": "International Facility for Food Irradiation Technology", "text": "International Facility for Food Irradiation Technology\n\nThe International Facility for Food Irradiation Technology (IFFIT) was a research and training centre at the Institute of Atomic Research in Agriculture in Wageningen, Netherlands, sponsored by the Food and Agriculture Organization (FAO) of the United Nations, the International Atomic Energy Agency (IAEA) and the Dutch Ministry of Agriculture and Fisheries.\n\nThe organisation's aim was to address food loss and food safety in developing countries by speeding up the practical introduction of the food irradiation process. They achieved this by training initiatives, research and feasibility studies.\n\nIt was founded in 1978 and was operational until 1990, and during those twelve years over four hundred key personnel from over fifty countries were trained in aspects of food irradiation, making a significant contribution to the development and use of the radiation process. The Facility also co-ordinated research into the technology, economics and implementation of food irradiation, assisted in the assessment of the feasibility of using radiation to preserve foodstuffs, and evaluated trial shipments of irradiated material.\n\nThe Facility had a pilot plant with a cobalt-60 source whose activity was , which was stored underwater. Drums or boxes containing products were placed on rotating tables or conveyor belts, and irradiation took place by raising the source out of the pool.\n\nDuring IFFIT's first five years of operation, 109 scientists from 40 countries attended six training courses, five of them being general training courses on food irradiation and the sixth being a specialised course on public health aspects. IFFIT also evaluated shipments of irradiated mangoes, spices, avocado, shrimp, onions and garlic, and produced 46 reports.\n\nOne trainee noted that Professor D. A. A. Mossel (1918–2004) assisted with the training courses with what he described as \"remarkably suggestive lectures and his phenomenal foreign language abilities\". From 1988 onwards, Ari Brynjolfsson was director of IFFIT.\n\nReports produced by the International Facility for Food Irradiation Technology include:\n"}
{"id": "9626727", "url": "https://en.wikipedia.org/wiki?curid=9626727", "title": "Joseph Pitty Couthouy", "text": "Joseph Pitty Couthouy\n\nJoseph Pitty Couthouy (6 January 1808 – 4 April 1864) was an American naval officer, conchologist, and invertebrate palaeontologist. Born in Boston, Massachusetts, he entered the Boston Latin School in 1820. He married Mary Greenwood Wild on 9 March 1832.\n\nCouthouy applied to President Andrew Jackson for a position on the Scientific Corps of the U.S. Navy's Exploring Expedition of 1838.\n\nHe sailed with the expedition on 18 August 1838, but was sent to the Sandwich Islands for sick leave. Eventually, he dismissed according to Charles Wilkes for attempting to \"promote dissension, bring me into disrepute, and destroy the harmony and efficiency of the Squadron.\" \n\nAlthough he meticulously labeled all of his specimens from the expedition, Dall recounts how \"The authorities in Washington had appointed a reverened gentleman who knew nothing of science, with a fat salary, to unpack and take care of the specimens sent home by the expedition.\" This gentleman then separated the specimens from the tags thus rendering many of them useless. Couthouy returned to Washington and tried to work up what he could of the collection and was then informed, \"to crown all of his misfortunes\", that his pay was to be reduced by forty-four percent. He then returned to his profession as a master in the merchant marine, visiting South America and the Pacific. \n\nIn 1854, he took command of an expedition to the Bay of Cumaná, where he spent three unsuccessful years in search of the wreck of the Spanish treasure ship \"San Pedro\", lost there in the early part of the century. \n\nA good linguist, he spoke fluent Spanish, French, Italian, and Portuguese, and had mastered several dialects used in the Pacific Islands. \n\nIn the American Civil War, Couthouy was ordered to command on 31 December 1862, which was wrecked, and Couthouy made prisoner. He later commanded .\n\nFinally, he commanded during the Red River Campaign. On 2 April 1864, he was shot by a sniper and died the following day.\n\n\n"}
{"id": "49772024", "url": "https://en.wikipedia.org/wiki?curid=49772024", "title": "Journey to the Center of the Earth (1993 TV film)", "text": "Journey to the Center of the Earth (1993 TV film)\n\nThe 1993 Journey to the Center of the Earth is a TV film first aired on NBC. It stars Carel Struycken, Tim Russ, and Jeffrey Nordling, John Neville, F. Murray Abraham, Fabiana Udenio, and Kim Miyori. A TV series was originally planned after its release but it was cancelled.\n\nA team of explorers sets on a voyage to the earth's core, following an earlier attempt years before. Their ship, \"Avenger\", enters the lava chamber of an active volcano and uses an energy ray called a \"sonic blaster\" to blast through the flow. They enter in a subterranean world over 100 kilometers below the Earth's surface. The place is filled with many strange creatures. As they explore deeper into the underground caverns they encounter a yeti which the crew named Dallas that serves as their guide. Meanwhile, an unknown malevolent entity is attempting to recover the missing pieces of an Atlantean artifact known as the \"book of knowledge\" one of which a crew member of the Avenger brought with him, that will supposedly give massive powers to whomever possesses it.\n\n\n"}
{"id": "5904871", "url": "https://en.wikipedia.org/wiki?curid=5904871", "title": "Kyr", "text": "Kyr\n\nThe abbreviation kyr means \"thousand years\".\n\nKyr was formerly common in some English language works, especially in geology and astronomy, for the unit of 1,000 years or millennium. The \"k\" is the unit prefix for kilo- or thousand with the suffix \"yr\" simply an abbreviation for \"year\".\n\nOccasionally, the \"k\" is shown in upper case, as in \"100 Kyr\"; this is an incorrect usage. \"kyr\" itself is often considered incorrect, with some preferring to use \"ky\".\n\nISO 80000-3 recommends usage of ka (for kiloannum), which avoids the implicit English bias of \"year\" by using a Latin root.\n\n\n"}
{"id": "28814089", "url": "https://en.wikipedia.org/wiki?curid=28814089", "title": "Larry Persily", "text": "Larry Persily\n\nLarry Persily was Federal Coordinator (director) of the Alaska Natural Gas Transportation Projects 2010-2015. The Federal Coordinator is nominated with advice and consent of the Senate by the President of the United States. He was nominated by Barack Obama on December 9, 2009, and was confirmed by the United States Senate on March 10, 2010.\n\nPersily criticized his former employer, then-Governor Sarah Palin, as John McCain's choice for vice president. Until June 2008, he had worked as the Washington, DC liaison for the state government of Alaska. \"She's not qualified, she doesn't have the judgment, to be next in line to the president of the United States,\" said Persily in August 2008.\n\nIn December 2009, Democratic Senator Mark Begich recommended Persily for the Obama administration post.\n\nPersily was also a \"key aide\" to Alaska house member Mike Hawker.\n\n\n"}
{"id": "46769273", "url": "https://en.wikipedia.org/wiki?curid=46769273", "title": "List of Bermuda hurricanes", "text": "List of Bermuda hurricanes\n\nThe British Overseas Territory of Bermuda has a long history of encounters with Atlantic tropical cyclones, many of which inflicted significant damage and influenced the territory's development. A small archipelago comprising about 138 islands and islets, Bermuda occupies in the North Atlantic Ocean, roughly east of Cape Hatteras, North Carolina. The islands are situated far outside the main development region for Atlantic hurricanes, but within the typical belt of recurving tropical cyclones. Most storms form in the central Atlantic or western Caribbean Sea before approaching Bermuda from the southwest; storms forming north of 28°N are unlikely to impact the territory.\n\nAccording to the Bermuda Weather Service, the islands of Bermuda experience a damaging tropical cyclone once every six to seven years, on average. Due to the small area of the island chain, landfalls and direct hits are rare. Strictly speaking, only nine landfalls have occurred during years included in the official Atlantic hurricane database, starting in 1851. When hurricanes Fay and Gonzalo struck Bermuda just days apart in October 2014, that season became the first to produce two landfalls. Two damaging storms impacted Bermuda in September 1899, but the center of the first storm narrowly missed the islands. Tropical cyclones, and their antecedent or remnant weather systems, have affected the territory in all seasons, most frequently in the late summer months. A study of recorded storms from 1609 to 1996 found that direct hits from hurricanes were most common in early September and late October, with an intervening relative lull creating two distinct 'seasons'.\n\nHurricanes late in the year are often in the process of undergoing extratropical transition and receiving baroclinic enhancement. Bermuda is less likely to be impacted during years when the Caribbean, Gulf of Mexico, and southeastern United States are favored targets. Even in intense hurricanes, the islands tend to fare relatively well; ever since a cyclone in 1712 destroyed many wooden buildings, most structures have been built with stone walls and roofs, and are able to withstand severe winds. As a result, hurricane-related deaths have been uncommon since the early 18th century. Ten storms have collectively caused 129 fatalities; 110 of them, or 85%, were the result of shipwrecks along the shore in Hurricane \"Ten\" of 1926. Hurricane Fabian in 2003 was the only system in the weather satellite era to cause storm-related deaths.\n\nIn total, 186 events are listed, with widely varying degrees of damage. A hurricane in 1609 was responsible for the first permanent settlement on Bermuda: in late July, the Jamestown-bound, British ship \"Sea Venture\" nearly foundered in the storm and sought refuge on the islands, which the passengers found surprisingly hospitable. Hurricane Fabian was the most intense storm to impact the territory in modern times, though officially it did not make landfall, and was the only storm to have its name retired for effects in Bermuda. The costliest storms were Fabian and Gonzalo, which caused about $300 million and $200–400 million in damage respectively (2003 and 2014 USD). Accounting for inflation and continued development, Fabian would have likely wrought around $650 million in damage had it struck in 2014. The most recent tropical cyclone to affect the islands was Tropical Storm Chris in July 2018. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe following is a list of hurricanes with known deaths in the territory.\n\n\n\n"}
{"id": "3615473", "url": "https://en.wikipedia.org/wiki?curid=3615473", "title": "List of Lepidoptera that feed on Achillea", "text": "List of Lepidoptera that feed on Achillea\n\nAchillea (yarrow) species are used as food plants by the caterpillars of a number of Lepidoptera species, primarily moths:\n\n\n\n\n\n"}
{"id": "5872765", "url": "https://en.wikipedia.org/wiki?curid=5872765", "title": "List of Sites of Special Scientific Interest in Norfolk", "text": "List of Sites of Special Scientific Interest in Norfolk\n\nNorfolk is a county in East Anglia. It has an area of and a population as of mid-2017 of 898,400. The top level of local government is Norfolk County Council with seven second tier councils: Breckland District Council, Broadland District Council, Great Yarmouth Borough Council, King's Lynn and West Norfolk Borough Council, North Norfolk District Council, Norwich City Council and South Norfolk District Council. The county is bounded by Cambridgeshire, Suffolk, Lincolnshire and the North Sea.\n\nIn England, Sites of Special Scientific Interest (SSSIs) are designated by Natural England, a non-departmental public body which is responsible for protecting England's natural environment. Designation as an SSSI gives legal protection to the most important wildlife and geological sites. As of November 2018 there are one hundred and sixty-three SSSIs in Norfolk, out of which one hundred and twenty-three are biological, twenty-five geological and fifteen are both biological and geological.\n\nSixty-one sites are Special Areas of Conservation, forty-four are Special Protection Areas, thirty-two are Ramsar sites, forty are Geological Conservation Review sites, thirty-five are Nature Conservation Review sites, eighteen are National Nature Reserves, ten are Local Nature Reserves, twenty-eight are in Areas of Outstanding Natural Beauty, one is on the Register of Historic Parks and Gardens and three contain Scheduled Monuments. Twenty-two sites are managed by the Norfolk Wildlife Trust, one by the Suffolk Wildlife Trust, three by the National Trust, one by the Royal Society for the Protection of Birds and one by the Wildfowl & Wetlands Trust.\n\n\n"}
{"id": "34413435", "url": "https://en.wikipedia.org/wiki?curid=34413435", "title": "List of Streptomyces species", "text": "List of Streptomyces species\n\nThere are roughly 550 species of \"Streptomyces\" bacteria recognized at present, but new ones are still being discovered (e.g., \"S. boninensis\" and \"S. fuscigenes\", both described in 2018). Many species are named after their colorful hyphae and/or spores.\n\n\"Streptomyces\" species include:\n"}
{"id": "53767281", "url": "https://en.wikipedia.org/wiki?curid=53767281", "title": "List of bolides", "text": "List of bolides\n\nThe following is a list of bolides and fireballs seen on Earth in recent times. These are small asteroids (known as meteoroids) that regularly impact the Earth. Although most are so small that they burn up in the atmosphere before reaching the surface, some larger objects may reach the surface as fragments, known as meteorites. A few of these are detected by NASA's Sentry system, in which case the impact is predicted in advance.\n\nMeteors with a recorded absolute magnitude are then converted to an asteroidal absolute magnitude under (M = M + 31) and then converted to a diameter assuming that meteoroids brighten by approximate 10 magnitudes when entering the atmosphere. Objects reported only to the American Meteor Society are only listed if observed by at least 100 people, and are cross-referenced with https://fireballs.ndc.nasa.gov if possible, to determine further physical characteristics. The fourth and third to last parameters are calculated from http://convertalot.com/asteroid_impact_calculator.html , assuming a density of 1.5 g/cm^3, an impact angle of 45°, and a velocity of 17 km/s (if not provided). The actual values for both may vary by as much as the value itself, so be aware that these values are only estimates.\n\nThe bolide on 2015/09/07 is the 2015 Thailand bolide, and the bolide on 2015/02/26 May be related to the 2015 Kerala meteorite.\n"}
{"id": "48960169", "url": "https://en.wikipedia.org/wiki?curid=48960169", "title": "List of earthquakes in the Azores", "text": "List of earthquakes in the Azores\n\nThe following is a list of the prominent or destructive earthquakes occurring in the Azores, or affecting the populace of the archipelago:\n\n\n\n\n\n\n"}
{"id": "45312767", "url": "https://en.wikipedia.org/wiki?curid=45312767", "title": "List of elm trees", "text": "List of elm trees\n\nMany elm (\"Ulmus\") trees of various kinds have attained great size or otherwise become particularly noteworthy; among these are the following.\n\nMost of North America's notable elms are \"Ulmus americana\", a fast-growing and long-lived species capable of attaining great size in a few centuries, especially when open-grown.\n\n\n\n\n\n\n\n"}
{"id": "9019611", "url": "https://en.wikipedia.org/wiki?curid=9019611", "title": "List of grape diseases", "text": "List of grape diseases\n\nThis is a list of diseases of grapes (\"Vitis\" spp.).\n\n"}
{"id": "525091", "url": "https://en.wikipedia.org/wiki?curid=525091", "title": "List of planetary nebulae", "text": "List of planetary nebulae\n\nThe following is an incomplete list of known planetary nebulae.\n\nData values are sourced from the individual articles, where available, for each nebula. Please see the article of the nebula in question for data references.\n\nMany distances are estimates (denoted by the \"approx.\" suffix), as true distances are difficult to infer for planetary nebulae. Please see main article, Planetary nebulae, for more details on distance estimates.\n"}
{"id": "28579938", "url": "https://en.wikipedia.org/wiki?curid=28579938", "title": "List of rivers of Iran", "text": "List of rivers of Iran\n\nThis is a list of the rivers wholly or partly in Iran, arranged geographically by river basin from west to east.\n\n\n\n\n\n\n\n\n"}
{"id": "22493284", "url": "https://en.wikipedia.org/wiki?curid=22493284", "title": "List of rivers of Malawi", "text": "List of rivers of Malawi\n\nThis is a list of rivers in Malawi. This list is arranged by drainage basin, with respective tributaries indented under each larger stream's name.\n\n\n\n"}
{"id": "53044634", "url": "https://en.wikipedia.org/wiki?curid=53044634", "title": "Manglot Wildlife Park", "text": "Manglot Wildlife Park\n\nManglot Wildlife Park was established in 1990 in Khyber Pakhtunkhwa, Pakistan; the park is provide a natural sanctuary for wildlife. The area of the park is spread over on 1,756 acres including hilltops and plain area near the Indus River in Nizampur, Nowshera.\n\nThe flora of the Manglot Wildlife Park include scrub forest primarily consisting of the olive trees. Acacia modesta, zizyphus nummelaria, olea cuspidate, deodonia viscose and monothica boxifolia are the predominantly Vegetation of this park.\n\nThe animals includes the Chinkara, hog deer, common leopard, wolf, wild boar, jackal, porcupine and hare.\n\nIncluding different sort of snakes and wild lizards are present in the park.\n\nIncluding Chukar, three varieties of partridges, rock pigeon, dove and several sparrows from different species.\n\n"}
{"id": "29489291", "url": "https://en.wikipedia.org/wiki?curid=29489291", "title": "Meldungen aus Norwegen", "text": "Meldungen aus Norwegen\n\nMeldungen aus Norwegen (Reports from Norway) is a series of reports on the situation in occupied Norway during World War II, by the Oslo department of the German Sicherheitspolizei (Sipo) and Sicherheitsdienst (SD). The reports were edited by Georg Wolff and distributed to German military leaders in Norway and Germany. They were typically structured with a section on the general situation (), a section on the resistance movement (), and other details ().\n"}
{"id": "81684", "url": "https://en.wikipedia.org/wiki?curid=81684", "title": "Minthe", "text": "Minthe\n\nIn Greek mythology, Minthe (also Menthe, Mintha or Mentha; or Μένθη) was a naiad associated with the river Cocytus. \n\nThe \"-nth-\" element in \"menthe\" is characteristic of a class of words borrowed from a Pre-Greek language: compare \"acanthus\", labyrinth, Corinth, etc.\n\nMinthe was dazzled by Hades and made an attempt to seduce him, but Queen Persephone intervened and metamorphosed Minthe, in the words of Strabo's account, \"into the garden mint, which some call \"hedyosmon\" (lit. 'sweet-smelling')\". \"Mint (Mintha), men say, was once a maid beneath the earth, a Nymphe of Kokytos (Cocytus), and she lay in the bed of Aidoneus [Hades]; but when he raped the maid Persephone from the Aitnaian hill [Mount Etna in Sicily], then she complained loudly with overweening words and raved foolishly for jealousy, and Demeter in anger trampled upon her with her feet and destroyed her. For she had said that she was nobler of form and more excellent in beauty than dark-eyed Persephone and she boasted that Aidoneus would return to her and banish the other from his halls : such infatuation leapt upon her tongue. And from the earth spray the weak herb that bears her name.\"\n\nIn ancient Greece, mint was used in funerary rites, together with rosemary and myrtle, and not simply to offset the smell of decay; mint was an element in the fermented barley drink called the \"kykeon\" that was an essential preparatory entheogen for participants in the Eleusinian mysteries, which offered hope in the afterlife for initiates.\n\n\n"}
{"id": "23913353", "url": "https://en.wikipedia.org/wiki?curid=23913353", "title": "Multiphase heat transfer", "text": "Multiphase heat transfer\n\nA multiphase flow system is one characterized by the simultaneous presence of several phases, the two-phase system being the simplest case. The term ‘two-component’ is sometimes used to describe flows in which the phases consist of different chemical substances. For example, steam-water flows are two-phase, while air-water flows are two-component. Some two-component flows (mostly liquid-liquid) technically consist of a single phase but are identified as two-phase flows in which the term “phase” is applied to each of the components. Since the same mathematics describes two-phase and two-component flows, the two expressions can be treated as synonymous.\n\nThe analysis of multiphase systems can include consideration of multiphase flow and multiphase heat transfer. When all of the phases in a multiphase system exist at the same temperature, multiphase flow is the only concern. However, when the temperatures of the individual phases are different, interphase heat transfer also occurs. \n\nIf different phases of the same pure substance are present in a multiphase system, interphase heat transfer will result in a change of phase, which is always accompanied by interphase mass transfer. The combination of heat transfer with mass transfer during phase change makes multiphase systems distinctly more challenging than simpler systems. Based on the phases that are involved in the system, phase change problems can be classified as: (1) solid–liquid phase change (melting and solidification), (2) solid–vapor phase change (sublimation and deposition), and (3) liquid–vapor phase change (boiling/evaporation and condensation). Melting and sublimation are also referred to as fluidification because both liquid and vapor are regarded as fluids.\n\nFaghri, A., and Zhang, Y., 2006, Transport Phenomena in Multiphase Systems, , Elsevier, Burlington, MA.\n\nLock, G.S.H., 1994, Latent Heat Transfer, Oxford Science Publications, Oxford University, Oxford, UK.\n"}
{"id": "82841", "url": "https://en.wikipedia.org/wiki?curid=82841", "title": "Naiad", "text": "Naiad\n\nIn Greek mythology, the Naiads (; Greek: Ναϊάδες) are a type of female spirit, or nymph, presiding over fountains, wells, springs, streams, brooks and other bodies of fresh water.\n\nThey are distinct from river gods, who embodied rivers, and the very ancient spirits that inhabited the still waters of marshes, ponds and lagoon-lakes, such as pre-Mycenaean Lerna in the Argolis.\n\nNaiads were associated with fresh water, as the Oceanids were with saltwater and the Nereids specifically with the Mediterranean, but because the ancient Greeks thought of the world's waters as all one system, which percolated in from the sea in deep cavernous spaces within the earth, there was some overlap. Arethusa, the nymph of a spring, could make her way through subterranean flows from the Peloponnesus, to surface on the island of Sicily.\n\nThe Greek word is Ναϊάς (\"Naiás\", ), plural Ναϊάδες (\"Naiades\", ) It derives from νάειν (\"náein\"), \"to flow\", or νᾶμα (\"nãma\"), \"running water\". \"Naiad\" has several English pronunciations: , , , .\n\nThey were often the object of archaic local cults, worshipped as essential to humans. Boys and girls at coming-of-age ceremonies dedicated their childish locks to the local naiad of the spring. In places like Lerna their waters' ritual cleansings were credited with magical medical properties. Animals were ritually drowned there. Oracles might be situated by ancient springs.\n\nNaiads could be dangerous: Hylas of the \"Argo\"'s crew was lost when he was taken by naiads fascinated by his beauty (\"see illustration\"). The naiads were also known to exhibit jealous tendencies. Theocritus' story of naiad jealousy was that of a shepherd, Daphnis, who was the lover of Nomia or Echenais; Daphnis had on several occasions been unfaithful to Nomia and as revenge she permanently blinded him. Salmacis forced the youth Hermaphroditus into a carnal embrace and, when he sought to get away, fused with him.\n\nThe water nymph associated with particular springs was known all through Europe in places with no direct connection with Greece, surviving in the Celtic wells of northwest Europe that have been rededicated to Saints, and in the medieval Melusine.\n\nWalter Burkert points out, \"When in the \"Iliad\" [xx.4–9] Zeus calls the gods into assembly on Mount Olympus, it is not only the well-known Olympians who come along, but also all the nymphs and all the rivers; Okeanos alone remains at his station\", Greek hearers recognized this impossibility as the poet's hyperbole, which proclaimed the universal power of Zeus over the ancient natural world: \"the worship of these deities,\" Burkert confirms, \"is limited only by the fact that they are inseparably identified with a specific locality.\"\n\nRobert Graves offered a sociopolitical reading of the common myth-type in which a mythic king is credited with marrying a naiad and founding a city: it was the newly arrived Hellenes justifying their presence. The loves and rapes of Zeus, according to Graves' readings, record the supplanting of ancient local cults by Olympian ones (Graves 1955, \"passim\"). \n\nSo, in the back-story of the myth of Aristaeus, Hypseus, a king of the Lapiths, married Chlidanope, a naiad, who bore him Cyrene. Aristaeus had more than ordinary mortal experience with the naiads: when his bees died in Thessaly, he went to consult them. His aunt Arethusa invited him below the water's surface, where he was washed with water from a perpetual spring and given advice.\n\nSt. Charles Avenue in New Orleans was formerly known as Nyades Street, and is parallel to Dryades Street.\n\n\n"}
{"id": "18181603", "url": "https://en.wikipedia.org/wiki?curid=18181603", "title": "Petroleum Training Institute", "text": "Petroleum Training Institute\n\nPetroleum Training Institute (P.T.I.) in Effurun, Delta State was established in 1973 by the federal government of Nigeria as a prerequisite for the membership in the Organization of Petroleum Exporting Countries (OPEC) to train indigenous middle-level manpower to meet the labour force demands of the oil and gas industry in Nigeria and the West African subregion. It awards General Welding Certificates, ND (National Diploma) and HND (Higher National Diploma) certificates.\n\nThe Institute is headed by the Principal and CEO Professor Sunny Iyuke, who last served at the university of Witswatersrand (Johannesburg), before returning to Nigeria to head the institute. He was appointed following the retirement of the Institutes immediate past Principal/CEO - Mr A.j. Orukele.\n\n\n\n1. Professor Sunny Iyuke Retrieved 2017-01-16 \n"}
{"id": "43709", "url": "https://en.wikipedia.org/wiki?curid=43709", "title": "Radiation pressure", "text": "Radiation pressure\n\nRadiation pressure is the pressure exerted upon any surface due to the exchange of momentum between the object and the electromagnetic field. This includes the momentum of light or electromagnetic radiation of any wavelength which is absorbed, reflected, or otherwise emitted (e.g. black body radiation) by matter on any scale (from macroscopic objects to dust particles to gas molecules).\n\nThe forces generated by radiation pressure are generally too small to be noticed under everyday circumstances; however, they are important in some physical processes. This particularly includes objects in outer space where it is usually the main force acting on objects besides gravity, and where the net effect of a tiny force may have a large cumulative effect over long periods of time. For example, had the effects of the sun's radiation pressure on the spacecraft of the Viking program been ignored, the spacecraft would have missed Mars orbit by about . Radiation pressure from starlight is crucial in a number of astrophysical processes as well. The significance of radiation pressure increases rapidly at extremely high temperatures, and can sometimes dwarf the usual gas pressure, for instance in stellar interiors and thermonuclear weapons.\n\nRadiation pressure can equally well be accounted for by considering the momentum of a classical electromagnetic field or in terms of the momenta of photons, particles of light. The interaction of electromagnetic waves or photons with matter may involve an exchange of momentum. Due to the law of conservation of momentum, any change in the total momentum of the waves or photons must involve an equal and opposite change in the momentum of the matter it interacted with (Newton's third law of motion), as is illustrated in the accompanying figure for the case of light being perfectly reflected by a surface. This transfer of momentum is the general explanation for what we term radiation pressure.\n\nJohannes Kepler put forward the concept of radiation pressure back in 1619 to explain the observation that a tail of a comet always points away from the Sun.\n\nThe assertion that light, as electromagnetic radiation, has the property of momentum and thus exerts a pressure upon any surface it is exposed to was published by James Clerk Maxwell in 1862, and proven experimentally by Russian physicist Pyotr Lebedev in 1900 and by Ernest Fox Nichols and Gordon Ferrie Hull in 1901. The pressure is very feeble, but can be detected by allowing the radiation to fall upon a delicately poised vane of reflective metal in a Nichols radiometer (this should not be confused with the Crookes radiometer, whose characteristic motion is \"not\" caused by radiation pressure but by impacting gas molecules).\n\nRadiation pressure can be viewed as a consequence of the conservation of momentum given the momentum attributed to electromagnetic radiation. That momentum can be equally well calculated on the basis of electromagnetic theory or from the combined momenta of a stream of photons, giving identical results as is shown below.\n\nAccording to Maxwell's theory of electromagnetism, an electromagnetic wave carries momentum, which will be transferred to an opaque surface it strikes.\n\nThe energy flux (irradiance) of a plane wave is calculated using the Poynting vector formula_1, whose magnitude we denote by S. S divided by the speed of light is the density of the linear momentum per unit area (pressure) of the electromagnetic field. That pressure is experienced as radiation pressure on the surface:\n\nwhere formula_3 is pressure (usually in Pascals), formula_4 is the incident irradiance (usually in W/m) and formula_5 is the speed of light in vacuum.\n\nIf the surface is planar at an angle α to the incident wave, the intensity across the surface will be geometrically reduced by the cosine of that angle and the component of the radiation force against the surface will also be reduced by the cosine of α, resulting in a pressure:\n\nThe momentum from the incident wave is in the same direction of that wave. But only the component of that momentum normal to the surface contributes to the pressure on the surface, as given above. The component of that force tangent to the surface is not called pressure.\n\nThe above treatment for an incident wave accounts for the radiation pressure experienced by a black (totally absorbing) body. If the wave is specularly reflected, then the recoil due to the reflected wave will further contribute to the radiation pressure. In the case of a perfect reflector, this pressure will be identical to the pressure caused by the incident wave:\n\nthus \"doubling\" the net radiation pressure on the surface:\n\nFor a partially reflective surface, the second term must be multiplied by the reflectivity (also known as reflection coefficient of intensity), so that the increase is less than double. For a diffusely reflective surface, the details of the reflection and geometry must be taken into account, again resulting in an increased net radiation pressure of less than double.\n\nJust as a wave reflected from a body contributes to the net radiation pressure experienced, a body that emits radiation of its own (rather than reflected) obtains a radiation pressure again given by the irradiance of that emission \"in the direction normal to the surface\" \"I\":\n\nThe emission can be from black body radiation or any other radiative mechanism. Since all materials emit black body radiation (unless they are totally reflective or at absolute zero), this source for radiation pressure is ubiquitous but usually very tiny. However, because black body radiation increases rapidly with temperature (according to the fourth power of temperature as given by the Stefan–Boltzmann law), radiation pressure due to the temperature of a very hot object (or due to incoming black body radiation from similarly hot surroundings) can become very significant. This becomes important in stellar interiors which are at millions of degrees.\n\nElectromagnetic radiation can be viewed in terms of particles rather than waves; these particles are known as photons. Photons do not have a rest-mass, however photons are never at rest (they move at the speed of light) and acquire a momentum nonetheless which is given by:\nwhere \"p\" is momentum, \"h\" is Planck's constant, λ is wavelength, and \"c\" is speed of light in vacuum. And \"E\" is the energy of a single photon given by: \n\nThe radiation pressure again can be seen as the transfer of each photon's momentum to the opaque surface, plus the momentum due to a (possible) recoil photon for a (partially) reflecting surface. Since an incident wave of irradiance \"I\" over an area \"A\" has a power of \"IA\", this implies a flux of \"I/E\" photons per second per unit area striking the surface. Combining this with the above expression for the momentum of a single photon, results in the same relationships between irradiance and radiation pressure described above using classical electromagnetics. And again, reflected or otherwise emitted photons will contribute to the net radiation pressure identically.\n\nIn general, the pressure of electromagnetic waves can be obtained from the vanishing of the trace of the electromagnetic stress tensor: Since this trace equals 3\"P\" - \"u\", we get\nwhere \"u\" is the radiation density per unit volume. \n\nThis can also be shown in the specific case of the pressure exerted on surfaces of a body in thermal equilibrium with its surroundings, at a temperature \"T\": The body will be surrounded by a uniform radiation field described by the Planck black body radiation law, and will experience a compressive pressure due to that impinging radiation, its reflection, and its own black body emission. From that it can be shown that the resulting pressure is equal to one third of the total radiant energy per unit volume in the surrounding space.\n\nBy using Stefan–Boltzmann law, this can be expressed as\n\nwhere formula_14 is the Stefan–Boltzmann constant.\n\nSolar radiation pressure is due to the sun's radiation at closer distances, thus especially within the solar system. While it acts on all objects, its net effect is generally greater on smaller bodies since they have a larger ratio of surface area to mass. All spacecraft experience such a pressure except when they are behind the shadow of a larger orbiting body.\n\nSolar radiation pressure on objects near the earth may be calculated using the sun's irradiance at 1 AU, known as the solar constant or \"G\", whose value is set at 1361 W/m as of 2011.\n\nAll stars have a spectral energy distribution that depends on their surface temperature. The distribution is approximately that of black-body radiation. This distribution must be taken into account when calculating the radiation pressure or identifying reflector materials for optimizing a solar sail for instance.\n\nSolar radiation pressure at the earth's distance from the sun, may be calculated by dividing the solar constant \"G\" (above) by the speed of light c. For an absorbing sheet facing the sun, this is simply:\n\nThis result is in the S.I. unit Pascals, equivalent to N/m (Newtons per square meter). For a sheet at an angle α to the sun, the effective area \"A\" of a sheet is reduced by a geometrical factor resulting in a force \"in the direction of the sunlight\" of:\n\nTo find the component of this force normal to the surface, another cosine factor must be applied resulting in a pressure \"P\" on the surface of:\n\nNote, however, that in order to account for the net effect of solar radiation on a spacecraft for instance, one would need to consider the \"total\" force (in the direction away from the sun) given by the preceding equation, rather than just the component normal to the surface that we identify as \"pressure\".\n\nThe solar constant is defined for the sun's radiation at the distance to the earth, also known as one astronomical unit (AU). Consequently, at a distance of \"R\" astronomical units (\"R\" thus being dimensionless), applying the inverse square law, we would find:\n\nFinally, considering not an absorbing but a perfectly reflecting surface, the pressure is \"doubled\" due to the reflected wave, resulting in:\n\nNote that unlike the case of an absorbing material, the resulting force on a reflecting body is given exactly by this pressure acting normal to the surface, with the tangential forces from the incident and reflecting waves canceling each other. In practice, materials are neither totally reflecting nor totally absorbing, so the resulting force will be a weighted average of the forces calculated using these formulae.\n\nSolar radiation pressure is a source of orbital perturbations. It significantly affects the orbits and trajectories of small bodies including all spacecraft.\n\nSolar radiation pressure affects bodies throughout much of the Solar System. Small bodies are more affected than large because of their lower mass relative to their surface area. Spacecraft are affected along with natural bodies (comets, asteroids, dust grains, gas molecules).\n\nThe radiation pressure results in forces and torques on the bodies that can change their translational and rotational motions. Translational changes affect the orbits of the bodies. Rotational rates may increase or decrease. Loosely aggregated bodies may break apart under high rotation rates. Dust grains can either leave the Solar System or spiral into the Sun.\n\nA whole body is typically composed of numerous surfaces that have different orientations on the body. The facets may be flat or curved. They will have different areas. They may have optical properties differing from other aspects.\n\nAt any particular time, some facets will be exposed to the Sun and some will be in shadow. Each surface exposed to the Sun will be reflecting, absorbing, and emitting radiation. Facets in shadow will be emitting radiation. The summation of pressures across all of the facets will define the net force and torque on the body. These can be calculated using the equations in the preceding sections.\n\nThe Yarkovsky effect affects the translation of a small body. It results from a face leaving solar exposure being at a higher temperature than a face approaching solar exposure. The radiation emitted from the warmer face will be more intense than that of the opposite face, resulting in a net force on the body that will affect its motion.\n\nThe YORP effect is a collection of effects expanding upon the earlier concept of the Yarkovsky effect, but of a similar nature. It affects the spin properties of bodies.\n\nThe Poynting–Robertson effect applies to grain-size particles. From the perspective of a grain of dust circling the Sun, the Sun's radiation appears to be coming from a slightly forward direction (aberration of light). Therefore, the absorption of this radiation leads to a force with a component against the direction of movement. (The angle of aberration is tiny since the radiation is moving at the speed of light while the dust grain is moving many orders of magnitude slower than that.) The result is a gradual spiral of dust grains into the Sun. Over long periods of time, this effect cleans out much of the dust in the Solar System.\n\nWhile rather small in comparison to other forces, the radiation pressure force is inexorable. Over long periods of time, the net effect of the force is substantial. Such feeble pressures can produce marked effects upon minute particles like gas ions and electrons, and are essential in the theory of electron emission from the Sun, of cometary material, and so on.\n\nBecause the ratio of surface area to volume (and thus mass) increases with decreasing particle size, dusty (micrometre-size) particles are susceptible to radiation pressure even in the outer solar system. For example, the evolution of the outer rings of Saturn is significantly influenced by radiation pressure.\n\nAs a consequence of light pressure, Einstein in 1909 predicted the existence of \"radiation friction\" which would oppose the movement of matter. He wrote, “radiation will exert pressure on both sides of the plate. The forces of pressure exerted on the two sides are equal if the plate is at rest. However, if it is in motion, more radiation will be reflected on the surface that is ahead during the motion (front surface) than on the back surface. The backward acting force of pressure exerted on the front surface is thus larger than the force of pressure acting on the back. Hence, as the resultant of the two forces, there remains a force that counteracts the motion of the plate and that increases with the velocity of the plate. We will call this resultant 'radiation friction' in brief.”\n\nSolar sailing, an experimental method of spacecraft propulsion, uses radiation pressure from the Sun as a motive force. The idea of interplanetary travel by light was mentioned by Jules Verne in \"From the Earth to the Moon\".\n\nA sail reflects about 90% of the incident radiation. The 10% that is absorbed is radiated away from both surfaces, with the proportion emitted from the unlit surface depending on the thermal conductivity of the sail. A sail has curvature, surface irregularities, and other minor factors that affect its performance.\n\nThe Japan Aerospace Exploration Agency (JAXA) has successfully unfurled a solar sail in space which has already succeeded in propelling its payload with the IKAROS project.\n\nRadiation pressure has had a major effect on the development of the cosmos, from the birth of the universe to ongoing formation of stars and shaping of clouds of dust and gasses on a wide range of scales.\n\nThe photon epoch is a phase when the energy of the universe was dominated by photons, between 10 seconds and 380,000 years after the Big Bang.\n\nThe process of galaxy formation and evolution began early in the history of the cosmos. Observations of the early universe strongly suggest that objects grew from bottom-up (i.e., smaller objects merging to form larger ones). As stars are thereby formed and become sources of electromagnetic radiation, radiation pressure from the stars becomes a factor in the dynamics of remaining circumstellar material.\n\nThe gravitational compression of clouds of dust and gases is strongly influenced by radiation pressure, especially when the condensations lead to star births. The larger young stars forming within the compressed clouds emit intense levels of radiation that shift the clouds, causing either dispersion or condensations in nearby regions, which influences birth rates in those nearby regions.\n\nStars predominantly form in regions of large clouds of dust and gases, giving rise to star clusters. Radiation pressure from the member stars eventually disperses the clouds, which can have a profound effect on the evolution of the cluster.\n\nMany open clusters are inherently unstable, with a small enough mass that the escape velocity of the system is lower than the average velocity of the constituent stars. These clusters will rapidly disperse within a few million years. In many cases, the stripping away of the gas from which the cluster formed by the radiation pressure of the hot young stars reduces the cluster mass enough to allow rapid dispersal.\n\nStar formation is the process by which dense regions within molecular clouds in interstellar space collapse to form stars. As a branch of astronomy, star formation includes the study of the interstellar medium and giant molecular clouds (GMC) as precursors to the star formation process, and the study of protostars and young stellar objects as its immediate products. Star formation theory, as well as accounting for the formation of a single star, must also account for the statistics of binary stars and the initial mass function.\n\nPlanetary systems are generally believed to form as part of the same process that results in star formation. A protoplanetary disk forms by gravitational collapse of a molecular cloud, called a solar nebula, and then evolves into a planetary system by collisions and gravitational capture. Radiation pressure can clear a region in the immediate vicinity of the star. As the formation process continues, radiation pressure continues to play a role in affecting the distribution of matter. In particular, dust and grains can spiral into the star or escape the stellar system under the action of radiation pressure.\n\nIn stellar interiors the temperatures are very high. Stellar models predict a temperature of 15 MK in the center of the Sun, and at the cores of supergiant stars the temperature may exceed 1 GK. As the radiation pressure scales as the fourth power of the temperature, it becomes important at these high temperatures. In the Sun, radiation pressure is still quite small when compared to the gas pressure. In the heaviest non-degenerate stars, radiation pressure is the dominant pressure component.\n\nSolar radiation pressure strongly affects comet tails. Solar heating causes gases to be released from the comet nucleus, which also carry away dust grains. Radiation pressure and solar wind then drive the dust and gases away from the Sun's direction. The gases form a generally straight tail, while slower moving dust particles create a broader, curving tail.\n\nLasers can be used as a source of monochromatic light with wavelength formula_20. With a set of lenses, one can focus the laser beam to a point that is formula_20 in diameter (or formula_22). \n\nThe radiation pressure of a 30 mW laser of 1064 nm can therefore be computed as follows: \n\nformula_23\n\nformula_24\n\nformula_25\n\nThis is used in optical tweezers. \n\nLaser cooling is applied to cooling materials very close to absolute zero. Atoms traveling towards a laser light source perceive a doppler effect tuned to the absorption frequency of the target element. The radiation pressure on the atom slows movement in a particular direction until the Doppler effect moves out of the frequency range of the element, causing an overall cooling effect.\n\nLarge lasers operating in space have been suggested as a means of propelling sail craft in beam-powered propulsion.\n\nThe reflection of a laser pulse from the surface of an elastic solid gives rise to various types of elastic waves that propagate inside the solid. The weakest waves are generally those that are generated by the radiation pressure acting during the reflection of the light. Recently, such light-pressure-induced elastic waves were observed inside an ultrahigh-reflectivity dielectric mirror. These waves are the most basic fingerprint of a light-solid matter interaction on the macroscopic scale.\n\n\n"}
{"id": "438766", "url": "https://en.wikipedia.org/wiki?curid=438766", "title": "Raised beach", "text": "Raised beach\n\nA raised beach, coastal terrace, or perched coastline is a relatively flat, horizontal or gently inclined surface of marine origin, mostly an old abrasion platform which has been lifted out of the sphere of wave activity (sometimes called \"tread\"). Thus, it lies above or under the current sea level, depending on the time of its formation. It is bounded by a steeper ascending slope on the landward side and a steeper descending slope on the seaward side (sometimes called \"riser\"). Due to its generally flat shape it is often used for anthropogenic structures such as settlements and infrastructure.\n\nA raised beach is an emergent coastal landform. Raised beaches and marine terraces are beaches or wave-cut platforms raised above the shoreline by a relative fall in the sea level.\n\nAround the world, a combination of tectonic coastal uplift and Quaternary sea-level fluctuations has resulted in the formation of marine terrace sequences, most of which were formed during separate interglacial highstands that can be correlated to marine isotope stages (MIS).\n\nA marine terrace commonly retains a shoreline angle or inner edge, the slope inflection between the marine abrasion platform and the associated paleo sea-cliff. The shoreline angle represents the maximum shoreline of a transgression and therefore a paleo-sea level.\n\nThe platform of a marine terrace usually has a gradient between 1°5° depending on the former tidal range with, commonly, a linear to concave profile. The width is quite variable, reaching up to , and seems to differ between the northern and southern hemispheres. The cliff faces that delimit the platform can vary in steepness depending on the relative roles of marine and subaerial processes. At the intersection of the former shore (wave-cut/abrasion-) platform and the rising cliff face the platform commonly retains a shoreline angle or inner edge (notch) that indicates the location of the shoreline at the time of maximum sea ingression and therefore a paleo-sea level. Sub-horizontal platforms usually terminate in a low tide cliff, and it is believed that the occurrence of these platforms depends on tidal activity. Marine terraces can extend for several tens of kilometers parallel to the coast.\n\nOlder terraces are covered by marine and/or alluvial or colluvial materials while the uppermost terrace levels usually are less well preserved. While marine terraces in areas of relatively rapid uplift rates (> 1 mm/year) can often be correlated to individual interglacial periods or stages, those in areas of slower uplift rates may have a polycyclic origin with stages of returning sea levels following periods of exposure to weathering.\n\nMarine terraces can be covered by a wide variety of soils with complex histories and different ages. In protected areas, allochtonous sandy parent materials from tsunami deposits may be found. Common soil types found on marine terraces include planosols and solonetz.\n\nIt is now widely thought that marine terraces are formed during the separated highstands of interglacial stages correlated to marine isotope stages (MIS).\n\nThe formation of marine terraces is controlled by changes in environmental conditions and by tectonic activity during recent geological times. Changes in climatic conditions have led to eustatic sea-level oscillations and isostatic movements of the Earth’s crust, especially with the changes between glacial and interglacial periods.\n\nProcesses of eustasy lead to glacioeustatic sea level fluctuations due to changes of the water volume in the oceans and hence to regressions and transgressions of the shoreline. At times of maximum glacial extent during the last glacial period, the sea level was about lower compared to today. Eustatic sea level changes can also be caused by changes in the void volume of the oceans, either through sedimento-eustasy or tectono-eustasy.\n\nProcesses of isostasy involve the uplift of continental crusts along with their shorelines. Today, the process of glacial isostatic adjustment mainly applies to Pleistocene glaciated areas. In Scandinavia, for instance, the present rate of uplift reaches up to /year.\n\nIn general, eustatic marine terraces were formed during separate sea level highstands of interglacial stages and can be correlated to marine oxygene isotopic stages (MIS). Glacioisostatic marine terraces were mainly created during stillstands of the isostatic uplift. When eustasy was the main factor for the formation of marine terraces, derived sea level fluctuations can indicate former climate changes. This conclusion has to be treated with care, as isostatic adjustments and tectonic activities can be extensively overcompensated by a eustatic sea level rise. Thus, in areas of both eustatic and isostatic or tectonic influences, the course of the relative sea level curve can be complicated. Hence, most of today's marine terrace sequences were formed by a combination of tectonic coastal uplift and Quaternary sea level fluctuations.\n\nJerky tectonic uplifts can also lead to marked terrace steps while smooth relative sea level changes may not result in obvious terraces, and their formations are often not referred to as marine terraces.\n\nMarine terraces often result from marine erosion along rocky coast lines in temperate regions due to wave attack and sediment carried in the waves. Erosion also takes place in connection with weathering and cavitation. The speed of erosion is highly dependent on the shoreline material (hardness of rock), the bathymetry, and the bedrock properties and can be between only a few millimeters per year for granitic rocks and more than per year for volcanic ejecta. The retreat of the sea cliff generates a shore (wave-cut/abrasion-) platform through the process of abrasion. A relative change of the sea level leads to regressions or transgressions and eventually forms another terrace (marine-cut terrace) at a different altitude while notches in the cliff face indicate short stillstands.\n\nIt is believed that the terrace gradient increases with tidal range and decreases with rock resistance. In addition, the relationship between terrace width and the strength of the rock is inverse, and higher rates of uplift and subsidence as well as a higher slope of the hinterland increases the number of terraces formed during a certain time.\n\nFurthermore, shore platforms are formed by denudation and marine-built terraces arise from accumulations of materials removed by shore erosion. Thus a marine terrace can be formed by both erosion and accumulation. However, there is an ongoing debate about the roles of wave erosion and weathering in the formation of shore platforms.\n\nReef flats or uplifted coral reefs are another kind of marine terrace found in intertropical regions. They are a result of biological activity, shoreline advance and accumulation of reef materials.\n\nWhile a terrace sequence can date back hundreds of thousands of years, its degradation is a rather fast process. On the one hand a deeper transgression of cliffs into the shoreline may completely destroy previous terraces; on the other hand older terraces might be decayed or covered by deposits, colluvia or alluvial fans. Erosion and backwearing of slopes caused by incisive streams play another important role in this degradation process.\n\nThe total displacement of the shoreline relative to the age of the associated interglacial stage allows calculation of a mean uplift rate or the calculation of eustatic level at a particular time if the uplift is known.\n\nIn order to estimate vertical uplift, the eustatic position of the considered paleo sea levels relative to the present one must be known as precisely as possible. Our chronology relies principally on relative dating based on geomorphologic criteria but in all cases we associated the shoreline angle of the marine terraces with numerical ages. The best-represented terrace worldwide is the one correlated to the last interglacial maximum (MISS 5e) (Hearty and Kindler, 1995; Johnson and Libbey, 1997, Pedoja et al., 2006 a, b, c). Age of MISS 5e is arbitrarily fixed to range from 130 to 116 ka (Kukla et al., 2002) but is demonstrated to range from 134 to 113 ka in Hawaii and Barbados (Muhs et al., 2002) with a peak from 128 to 116 ka on tectonically stable coastlines (Muhs, 2002). Older marine terraces well represented in worldwide sequences are those related to MIS 9 (~303-339 ka) and 11 (~362-423 ka) (Imbrie et al., 1984). Compilations show that sea level was 3 ± 3 meters higher during MISS 5e, MIS 9 and 11 than during the present one and –1 ± 1 m to the present one during MIS 7 (Hearty and Kindler, 1995, Zazo, 1999). Consequently, MIS 7 (~180-240 ka; Imbrie et al., 1984) marine terraces are less pronounced and sometimes absent (Zazo, 1999). When the elevations of these terraces are higher than the uncertainties in paleo-eustatic sea level mentioned for the Holocene and Late Pleistocene, these uncertainties have no effect on overall interpretation.\n\nSequence can also occurs where the accumulation of ice sheets have depressed the land so that when the ice sheets melts the land readjusts with time thus raising the height of the beaches (glacio-isostatic rebound)and in places where co-seismic uplift occur. In the latter case, the terrace are not correlated with sea level highstand even if co-seismic terrace are known only for the Holocene.\n\nFor exact interpretations of the morphology extensive datings, surveying and mapping of marine terraces is applied. This includes stereoscopic aerial photographic interpretation (ca. 1 : 10,000 - 25,000), on-site inspections with topographic maps (ca. 1 : 10,000) and analysis of eroded and accumulated material. Moreover, the exact altitude can be determined with an aneroid barometer or preferably with a levelling instrument mounted on a tripod. It should be measured with the accuracy of and at about every , depending on the topography. In remote areas technics of photogrammetry and tacheometry can be applied.\n\nDifferent methods for dating and correlation of marine terraces can be used and combined.\n\nThe morphostratigraphic approach focuses especially in regions of marine regression on the altitude as the most important criterion to distinguish coast lines of different ages. Moreover, individual marine terraces can be correlated based on their size and continuity. Also paleo-soils as well as glacial, fluvial, eolian and periglacial landforms and sediments may be used to find correlations between terraces. On New Zealand’s North Island, for instance, tephra and loess were used to date and correlate marine terraces. At the terminus advance of former glaciers marine terraces can be correlated by their size, as their width decreases with age due to the slowly thawing glaciers along the coast line.\n\nThe lithostratigraphic approach uses typical sequences of sediment and rock strata to prove sea level fluctuations on the basis of an alternation of terrestrial and marine sediments or littoral and shallow marine sediments. Those strata show typical layers of transgressive and regressive patterns. However, an unconformity in the sediment sequence might make this analysis difficult.\n\nThe biostratigraphic approach uses remains of organisms which can indicate the age of a marine terrace. For that often mollusc shells, foraminifera or pollen are used. Especially Mollusca can show specific properties depending on their depth of sedimentation. Thus they can be used to estimate former water depths.\n\nMarine terraces are often correlated to marine oxygene isotopic stages (MIS) (e.g. Johnson, M. E.; Libbey, L. K. 1997) and can also be roughly dated using their stratigraphic position.\n\nThere are various methods for the direct dating of marine terraces and their related materials including C radiocarbon dating, which is the most common one. E.g. this method has been used on the North Island of New Zealand to date several marine terraces. It utilizes terrestrial biogenic materials in coastal sediments such as mollusc shells analyzing the C isotope. In some cases dating based on the Th/U ratio was applied though in case of detrital contamination or low uranium concentrations a high resolution dating was found to be difficult. In a study in southern Italy paleomagnetism was used to carry out paleomagnetic datings and luminescence dating (OSL) was used in different studies on the San Andreas Fault and on the Quaternary Eupcheon Fault in South Korea. In the last decennia, the dating of marine terraces has been enhanced since the arrival of terrestrial cosmogenic nuclides method, and particularly through the use of Be and Al cosmogenic isotopes produced in-situ. These isotopes record the duration of surface exposure to cosmic rays. and this exposure age reflects the age of abandonment of a marine terrace by the sea.\n\nIn order to calculate the eustatic sea level for each dated terrace it is assumed that the eustatic sea-level position corresponding to at least one marine terrace is known and that the uplift rate has remained essentially constant in each section.\n\nMarine terraces play an important role in the research on tectonics and earthquakes. They may show patterns and rates of tectonic uplift and thus may be used to estimate the tectonic activity in a certain region. In some cases the exposed secondary landforms can be correlated with known seismic events such as the 1855 Wairarapa earthquake on the Wairarapa Fault near Wellington, New Zealand which produced a uplift. This figure can be estimated from the vertical offset between raised shorelines in the area.\n\nFurthermore, with the knowledge of eustatic sea level fluctuations the speed of isostatic uplift can be estimated and eventually the change of relative sea levels for certain regions can be reconstructed. Thus marine terraces also provide information for the research on climate change and trends in future sea level changes.\n\nWhen analyzing the morphology of marine terraces it must be considered, that both eustasy and isostasy can have an influence on the formation process. This way can be assessed, whether there were changes in sea level or whether tectonic activities took place.\n\nRaised beaches are found in a wide variety of coast and geodynamical background such as subduction on the pacific coast of South America (Pedoja et al., 2006), of North America, passive margin of the Atlantic coast of South America (Rostami et al., 2000), collision context on the Pacific coast of Kamchatka (Pedoja et al., 2006), Papua New Guinea, New Zealand, Japan (Ota and Yamaguchi, 2004), passive margin of the South China sea coast (Pedoja et al., in press), on west-facing Atlantic coasts, such as Donegal Bay, County Cork and County Kerry in Ireland; Bude, Widemouth Bay, Crackington Haven, Tintagel, Perranporth and St Ives in Cornwall, the Vale of Glamorgan, Gower Peninsula, Pembrokeshire and Cardigan Bay in Wales, the Isle of Jura and Isle of Arran in Scotland, Finistère in Brittany and Galicia in Northern Spain and at Squally Point in Eatonville, Nova Scotia within the Cape Chignecto Provincial Park.\n\nOther important sites include various coasts of New Zealand, e.g. Turakirae Head near Wellington being one of the world’s best and most thoroughly studied examples. Also along the Cook Strait in New Zealand there is a well-defined sequence of uplifted marine terraces from the late Quaternary at Tongue Point. It features a well preserved lower terrace from the last interglacial, a widely eroded higher terrace from the penultimate interglacial and another still higher terrace, which is nearly completely decayed. Furthermore, on New Zealand’s North Island at the eastern Bay of Plenty a sequence of seven marine terraces has been studied.\n\nAlong many coasts of mainland and islands around the Pacific, marine terraces are typical coastal features. An especially prominent marine terraced coastline can be found north of Santa Cruz, near Davenport, California, where terraces probably have been raised by repeated slip earthquakes on the San Andreas Fault. Hans Jenny (pedologist) famously researched the pygmy forests of the Mendocino and Sonoma county marine terraces. The marine terrace's \"ecological staircase\" of Salt Point State Park is also bound by the San Andreas Fault.\n\nAlong the coasts of South America marine terraces are present, where the highest ones are situated where plate margins lie above subducted oceanic ridges and the highest and most rapid rates of uplift occur. At Cape Laundi, Sumba Island, Indonesia an ancient patch reef can be found at above sea level as part of a sequence of coral reef terraces with eleven terraces being wider than . The coral marine terraces at Huon Peninsula, New Guinea, which extend over and rise over above present sea level are currently on UNESCO’s tentative list for world heritage sites under the name \"Houn Terraces - Stairway to the Past.\"\n\nOther considerable examples include marine terraces rising up to on some Philippine Islands and along the Mediterranean Coast of North Africa, especially in Tunisia, rising up to .\n\nUplift can also be registered through tidal notch sequences. Notches are often portrayed as lying at sea level; however notch types actually form a continuum from wave notches formed in quiet conditions at sea level to surf notches formed in more turbulent conditions and as much as above sea level (Pirazzoli et al., 1996 in Rust and Kershaw, 2000). As stated above, there was at least one higher sea level during the Holocene, so that some notches may not contain a tectonic component in their formation.\n\n\n"}
{"id": "99742", "url": "https://en.wikipedia.org/wiki?curid=99742", "title": "Rudra", "text": "Rudra\n\n(could be a hurricane or tempest) or \"the most frightening one\". The \"Shri Rudram\" hymn from the Yajurveda is dedicated to Rudra, and is important in the Saivism sect. In it Rudra is referred as \"God of Gods\".\n\nThe Hindu god Shiva shares several features with the Rudra: the theonym \"Shiva\" originated as an epithet of Rudra, the adjective \"shiva\" (\"kind\") being used euphemistically of Rudra, who also carries the epithet \"Aghora, Abhayankar\" (\" non terrifying\"). Usage of the epithet came to exceed the original theonym by the post-Vedic period (in the Sanskrit Epics), and the name \"Rudra\" has been taken as a synonym for the god Shiva and the two names are used interchangeably.\n\nThe etymology of the theonym \"Rudra\" is somewhat uncertain. It is usually derived from the root \"rud-\" which means \"to cry, howl.\" According to this etymology, the name Rudra has been translated as \"the roarer\". A Rigvedic verse \"rukh draavayathi, iti rudraha\" where \"rukh\" means “sorrow/misery”, \"draavayathi\" means “drive out/eliminate” and \"iti\" means “that which” (or “the one who”) implies that \"Rudra\" is the eliminator of evil and usherer of peace. An alternative etymology suggested by Prof. Pischel derives \"Rudra\" as the “red one”, the “brilliant one” from a lost root \"rud-\", “red” or “ruddy”, or alternatively (according to Grassman) “shining”.\n\nStella Kramrisch notes a different etymology connected with the adjectival form \"raudra\", which means “wild”, i.e. of \"rude\" (untamed) nature, and translates the name \"Rudra\" as “the wild one” or “the fierce god”. R. K. Sharma follows this alternate etymology and translates the name as “the terrible” in his glossary for the Shiva Sahasranama. The commentator suggests six possible derivations for \"rudra\". However, another reference states that Sayana suggested ten derivations.\n\nThe adjective \"shivam\" in the sense of “propitious” or “kind” is applied to the name \"Rudra\" in RV 10.92.9. \n\nRudra is called “the archer” (Sanskrit: \"\") and the arrow is an essential attribute of Rudra. This name appears in the Shiva Sahasranama, and R. K. Sharma notes that it is used as a name of Shiva often in later languages. The word is derived from the Sanskrit root \"-\" which means \"to injure\" or \"to kill\" and Sharma uses that general sense in his interpretive translation of the name as \"One who can kill the forces of darkness\". The names (\"bowman\") and (“archer”, literally “Armed with a hand-full of arrows”) also refer to archery.\n\nIn other contexts the word \"rudra\" can simply mean “the number eleven”. The word \"rudraksha\" (Sanskrit: ' = \"rudra\" and ' “eye”), or “eye of Rudra”, is used as a name both for the berry of the Rudraksha tree, and a name for a string of the prayer beads made from those seeds.\n\nThe earliest mentions of Rudra occur in the Rigveda, where three entire hymns are devoted to him. There are about seventy-five references to Rudra in the Rigveda overall.\n\nIn the Rigveda Rudra's role as a frightening god is apparent in references to him as \"ghora\" (\"extremely terrifying\"), or simply as \"asau devam\" (\"that god\"). He is \"fierce like a formidable wild beast\" (RV 2.33.11). Chakravarti sums up the perception of Rudra by saying: \"Rudra is thus regarded with a kind of cringing fear, as a deity whose wrath is to be deprecated and whose favor curried.\"\n\nRV 1.114 is an appeal to Rudra for mercy, where he is referred to as \"mighty Rudra, the god with braided hair.\"\n\nIn RV 7.46, Rudra is described as armed with a bow and fast-flying arrows. As quoted by R. G. Bhandarkar, the hymn says Rudra discharges \"brilliant shafts which run about the heaven and the earth\" (RV 7.46.3), which may be a reference to lightning.\n\nRudra was believed to cure diseases, and when people recovered from them or were free of them, that too was attributed to the agency of Rudra. He is asked not to afflict children with disease (RV 7.46.2) and to keep villages free of illness (RV 1.114.1). He is said to have healing remedies (RV 1.43.4), as the best physician of physicians (RV 2.33.4), and as possessed of a thousand medicines (RV 7.46.3). This is described in Shiva's alternative name Vaidyanatha (Lord of Remedies).\n\nA verse from the Rig Veda (RV 2.33.9) calls Rudra \"The Lord or Sovereign of the Universe\" (\"īśānādasya bhuvanasya\"):\n<poem style=\"margin-left:2em; float:left;\">\n\"sthirebhiraṅghaiḥ pururūpa ughro babhruḥ śukrebhiḥ pipiśehiraṇyaiḥ\"\n\"īśānādasya bhuvanasya bhūrerna vā u yoṣad rudrādasuryam\" (RV 2.33.9)\n</poem>\n\n<poem style=\"margin-left:2em; float:left;\">\nWith firm limbs, multiform, the strong, the tawny adorns himself with bright gold decorations:\nThe strength of Godhead never departs from Rudra, him who is Sovereign of this world, the mighty.\n</poem>\n\nA verse of Śrī Rudram (= Yajurveda 16.18) speaks of Rudra as Lord of the Universe: <br>\nAnother verse (Yajurveda 16.46) locates Rudra in the heart of the gods, showing that he is the inner Self of all, even the gods. <br>\nIn a verse popularly known as the Mahamrityunjaya Mantra, both Rig Veda (7.59.12) and Yajur Veda (3.60) recommend worshipping Rudra to attain moksha (liberation):\n\nIn the Taittiriya Aranyaka of Yajur Veda(10.24.1) Rudra is identified as the universal existent (\"all this\") and thus as the Purusha (Supreme Person or inner Self) of the Vedas:\n\nThe Taittiriya Aranyaka of Yajur Veda 1.10.1 identifies Rudra and Brihaspati as Sons of Bhumi (Earth) and Heaven:\n\n[The translations below need to be cleaned up; the transliteration standardized; the so-called \"modern translation\" should be removed, because it is not necessary or helpful. Do these lines constitute a single verse, or are they separate verses drawn from different places in the text? That needs to be made clear.]\n\nRudra is used both as a name of Shiva and collectively (\"the Rudras\") as the name for the Maruts. Maruts are \"storm gods\", associated with the atmosphere. They are a group of gods, whose number varies from two to sixty, sometimes also rendered as eleven, thirty-three or a hundred and eighty in number (i. e. three times sixty, see RV 8.96.8.).\n\nThe Rudras are sometimes referred to as \"the sons of Rudra\", whereas Rudra is referred to as \"Father of the Maruts\" (RV 2.33.1).\n\nRudra is mentioned along with a litany of other deities in RV 7.40.5. Here is the reference to Rudra, whose name appears as one of many gods who are called upon:\n\nOne scholiast interpretation of the Sanskrit word ', meaning \"ramifications\" or \"branches\", is that all other deities are, as it were branches of Vishnu, but Ralph T. H. Griffith cites Ludwig as saying \"This [...] gives no satisfactory interpretation\" and cites other views which suggest that the text is corrupt at that point.\n\nIn the various recensions of the Yajurveda is included a litany of stanzas praising Rudra: (\"Maitrāyaṇī-Saṃhitā\" 2.9.2, \"Kāṭhaka-Saṃhitā\" 17.11, \"Taittirīya-Saṃhitā\" 4.5.1, and \"Vājasaneyi-Saṃhitā\" 16.1–14). This litany is subsequently referred to variously as the \"Śatarudriyam\", the \"Namakam\" (because many of the verses commence with the word \"namaḥ\" [`homage`]), or simply the \"Rudram\". This litany was recited during the \"Agnicayana\" ritual (\"the piling of Agni\"), and it later became a standard element in Rudra liturgy.\n\nA selection of these stanzas, augmented with others, is included in the \"Paippalāda-Saṃhitā\" of the Atharvaveda (PS 14.3—4). This selection, with further PS additions at the end, circulated more widely as the \"Nīlarudram\" (or \"Nīlarudra Upaniṣad\").\n\nThe President of the Ramakrishna Mission, at Chennai, in commentating on the foreword to Swami Amritananda's translation of \"Sri Rudram and Purushasuktam\", stated that \"Rudra to whom these prayers are addressed is not a sectarian deity, but the Supreme Being who is omnipresent and manifests Himself in a myriad forms for the sake of the diverse spiritual aspirants.\" Sri Rudram occurs in the fourth Kanda of the Taittirya Samhita in the Yajur Veda.\nIt is a preeminent Vedic hymn to Lord Shiva as the God of dissolution, chanted daily in Shiva temples throughout India.\"\n\nThe prayer depicts the diverse aspects of the Almighty. The Shri Rudram hymn is unique in that it shows the presence of divinity throughout the entire universe. We cannot confine the qualities of the divine to those that are favorable to us. The Lord is both garden and graveyard, the slayer and the most benevolent one. The Almighty is impartial and ubiquitous.\n\nIn it Rudra is described as the most dreaded terroriser (frightening).Sri Rudram describes Rudra the vedic deity as the personification of 'terror'. Rudra comes from 'Ru' meaning '\"Roar or Howl\" (the words 'dreaded' or 'fearsome' could only be used as adjectives to Rudra and not as Rudra, because Rudra is the personification of terror); 'dra' is a superlative meaning 'the most'. So Rudra, depending on the poetic situation, can be meant as 'the most severe roarer/howler' - could be a hurricane or tempest - or 'the most frightening one'.\n\nShiva as we know him today shares many features with Rudra, and Shiva and Rudra are viewed as the same personality in Hindu scriptures. The two names are used synonymously. Rudra, the god of the roaring storm, is usually portrayed in accordance with the element he represents as a fierce, destructive deity.\n\nThe oldest surviving text of Hinduism is the Rig Veda, which is dated to between 1700 and 1100 BC based on linguistic and philological evidence. A god named Rudra is mentioned in the Rig Veda. The name Rudra is still used as a name for Shiva. In RV 2.33, he is described as the \"Father of the Rudras\", a group of storm gods.\n\nThe hymn 10.92 of the Rigveda states that deity Rudra has two natures, one wild and cruel (rudra), another that is kind and tranquil (shiva). The Vedic texts do not mention bull or any animal as the transport vehicle (\"vahana\") of Rudra or other deities. However, post-Vedic texts such as the Mahabharata and the Puranas state the Nandi bull, the Indian zebu, in particular, as the vehicle of Rudra and of Shiva, thereby unmistakably linking them as same.\n\nThe 10th Sikh Guru, Guru Gobind Singh describes the incarnation of Rudra in his book the Dasam Granth, the canto is titled Rudra Avatar.\n\n\n\n \n"}
{"id": "36966162", "url": "https://en.wikipedia.org/wiki?curid=36966162", "title": "San Joaquin Wildlife Sanctuary", "text": "San Joaquin Wildlife Sanctuary\n\nThe San Joaquin Wildlife Sanctuary is a constructed wetland in Irvine, California, in the flood plain of San Diego Creek just above its outlet into the Upper Newport Bay.\n\nThe site is owned by the Irvine Ranch Water District; it was used for farmland in the 1950s and 1960s, and (prior to its reconstruction) as a duck hunting range. Restoration of the wetlands began in 1988 and was completed in 2000.\nNow, the site serves a dual purpose of removing nitrates from the creek water and providing a bird habitat. The water district also operates an adjacent wastewater treatment facility but the treated wastewater does not enter the wildlife sanctuary.\n\nWithin the sanctuary, water from the creek percolates through a system of ponds, constructed in 1997 and ringed with bulrushes; the ponds are periodically drained and re-seeded, and the surrounding land is covered with native plants. A small hill at one edge of the site serves as an arboretum for non-native trees, planted for Earth Day in 1990. The landscaping has been designed to attract birds, and nesting boxes for the birds have been provided. Monthly censuses have found over 120 species of birds including hawks, swallows, roadrunners, hummingbirds, herons, egrets, pelicans, sandpipers, ducks, geese, and kingfishers.\n\nThe sanctuary is open to the public daily during the daytime, and has over of wheelchair-accessible hiking trails. The facilities also include free parking, restrooms, benches, and trail maps. The Duck Club, a building that was moved to the site in the 1940s and was until 1988 the base for two hunting clubs, serves as a free meeting facility for non-profit organizations. The Audubon Society maintains a chapter office in another building, the former bunkhouse of the Duck Club.\n\n"}
{"id": "13364000", "url": "https://en.wikipedia.org/wiki?curid=13364000", "title": "Soil health", "text": "Soil health\n\nSoil health is a state of a soil meeting its range of ecosystem functions as appropriate to its environment. Soil health testing is an assessment of this status. Soil health depends on soil biodiversity (with a robust soil biota), and it can be improved via soil conditioning (soil amendment).\n\nThe term soil health is used to describe the state of a soil in: \n\nSoil Health has partly if not largely replaced the expression \"Soil Quality\" that was extant in the 1990s. The primary difference between the two expressions is that soil quality was focused on individual traits within a functional group, as in \"quality of soil for maize production\" or \"quality of soil for roadbed preparation\" and so on. The addition of the word \"health\" shifted the perception to be integrative, holistic and systematic. The two expressions still overlap considerably.\n\nThe underlying principle in the use of the term “soil health” is that soil is not just an inert, lifeless growing medium, which modern farming tends to represent, rather it is a living, dynamic and ever-so-subtly changing whole environment. It turns out that soils highly fertile from the point of view of crop productivity are also lively from a biological point of view. It is now commonly recognized that soil microbial biomass is large: in temperate grassland soil the bacterial and fungal biomass have been documented to be 1–/hectare and 2–/ha, respectively. \nSome microbiologists now believe that 80% of soil nutrient functions are essentially controlled by microbes.\nIf this is consistently true, than the prevailing Liebig nutrient theory model, and Mitscherlich modifications of it, all which exclude biology, are perhaps dangerously incorrect for managing soil fertility sustainably for the future.\n\nUsing the human health analogy, a healthy soil can be categorized as one:\n\nSoil health is the condition of the soil in a defined space and at a defined scale relative to a set of benchmarks that encompass healthy functioning. It would not be appropriate to refer to soil health for soil-roadbed preparation, as in the analogy of soil quality in a functional class. \nThe definition of soil health may vary between users of the term as alternative users may place differing priorities upon the multiple functions of a soil.\nTherefore, the term soil health can only be understood within the context of the user of the term, and their aspirations of a soil, as well as by the boundary definition of the soil at issue.\n\nDifferent soils will have different benchmarks of health depending on the “inherited” qualities, and on the geographic circumstance of the soil.\nThe generic aspects defining a healthy soil can be considered as follows:\n\nThis translates to:\n\nAn unhealthy soil thus is the simple converse of the above.\n\nOn the basis of the above, soil health will be measured in terms of individual ecosystem services provided relative to the benchmark. Specific benchmarks used to evaluate soil health include CO release, humus levels, microbial activity, and available calcium.\n\nSoil health testing is spreading in the United States, Australia and South Africa.\nCornell University, a land-grant college in NY State, has had a Soil Health Test since 2006. Woods End Laboratories, a private soil lab founded in Maine in 1975, has offered a soil quality package since 1985 that contains physical (aggregate stability) chemical (mineral balance) and biology (CO respiration) which today are prerequisites for soil health testing. in the United States, six commercial soil labs are registered for \"Soil Health\" testing under the NRCS website. The approach of all the labs is to add into common chemical nutrient testing a biological set of factors not normally included in routine soil testing. The best example is adding biological soil respiration (\"CO-Burst\") as a test procedure; this has already been adapted to modern commercial labs .\n\nThere is resistance among soil testing labs and university scientists to adding new biological tests, primarily since interpretation of soil fertility is based on models from \"crop response\" studies which match yield to test levels of specific chemical nutrients. These soil test methods have evolved slowly over the past 40 years and are backed by considerable effort. However, in this same time USA soils have also lost up to 75% of their carbon (humus), causing biological fertility to drop drastically. Many critics of the current system say this is sufficient evidence that the old soil testing models have failed us, and need to be replaced with new approaches. These older models have stressed \"maximum yield\" and \" yield calibration\" to such an extent that related factors have been overlooked. Thus, surface and groundwater pollution with excess nutrients (nitrates and phosphates) has grown enormously, and is reported presently (in the United States) to be the worst it has been since the 1970s, before the advent of environmental consciousness.\n\n\n"}
{"id": "2549543", "url": "https://en.wikipedia.org/wiki?curid=2549543", "title": "Thermal power station", "text": "Thermal power station\n\nA thermal power station is a power station in which heat energy is converted to electric power. In most of the places in the world the turbine is steam-driven. Water is heated, turns into steam and spins a steam turbine which drives an electrical generator. After it passes through the turbine, the steam is condensed in a condenser and recycled to where it was heated; this is known as a Rankine cycle. The greatest variation in the design of thermal power stations is due to the different heat sources; fossil fuel dominates here, although nuclear heat energy and solar heat energy are also used. Some prefer to use the term \"energy center\" because such facilities convert forms of heat energy into electrical energy. Certain thermal power stations are also designed to produce heat energy for industrial purposes, or district heating, or desalination of water, in addition to generating electrical power.\n\nAlmost all coal, petroleum, nuclear, geothermal, solar thermal electric, and waste incineration plants, as well as many natural gas power stations are thermal. Natural gas is frequently combusted in gas turbines as well as boilers. The waste heat from a gas turbine, in the form of hot exhaust gas, can be used to raise steam, by passing this gas through a heat recovery steam generator (HRSG) the steam is then used to drive a steam turbine in a combined cycle plant that improves overall efficiency. Power stations burning coal, fuel oil, or natural gas are often called \"fossil fuel power stations\". Some biomass-fueled thermal power stations have appeared also. Non-nuclear thermal power stations, particularly fossil-fueled plants, which do not use cogeneration are sometimes referred to as \"conventional power stations\".\n\nCommercial electric utility power stations are usually constructed on a large scale and designed for continuous operation. Virtually all Electric power stations use three-phase electrical generators to produce alternating current (AC) electric power at a frequency of 50 Hz or 60 Hz. Large companies or institutions may have their own power stations to supply heating or electricity to their facilities, especially if steam is created anyway for other purposes. Steam-driven power stations have been used to drive most ships in most of the 20th century until recently. Steam power stations are now only used in large nuclear naval ships. Shipboard power stations usually directly couple the turbine to the ship's propellers through gearboxes. Power stations in such ships also provide steam to smaller turbines driving electric generators to supply electricity. Nuclear marine propulsion is, with few exceptions, used only in naval vessels. There have been many turbo-electric ships in which a steam-driven turbine drives an electric generator which powers an electric motor for propulsion.\n\nCogeneration plants, often called combined heat and power (CH&P) facilities, produce both electric power and heat for process heat or space heating, such as steam and hot water.\n\nThe initially developed reciprocating steam engine has been used to produce mechanical power since the 18th Century, with notable improvements being made by James Watt. When the first commercially developed central electrical power stations were established in 1882 at Pearl Street Station in New York and Holborn Viaduct power station in London, reciprocating steam engines were used. The development of the steam turbine in 1884 provided larger and more efficient machine designs for central generating stations. By 1892 the turbine was considered a better alternative to reciprocating engines; turbines offered higher speeds, more compact machinery, and stable speed regulation allowing for parallel synchronous operation of generators on a common bus. After about 1905, turbines entirely replaced reciprocating engines in large central power stations.\n\nThe largest reciprocating engine-generator sets ever built were completed in 1901 for the Manhattan Elevated Railway. Each of seventeen units weighed about 500 tons and was rated 6000 kilowatts; a contemporary turbine set of similar rating would have weighed about 20% as much.\n\nThe energy efficiency of a conventional thermal power station, considered salable energy produced as a percent of the heating value of the fuel consumed, is typically 33% to 48%. As with all heat engines, their efficiency is limited, and governed by the laws of thermodynamics. Other types of power stations are subject to different efficiency limitations, most hydropower stations in the United States are about 90 percent efficient in converting the energy of falling water into electricity while the efficiency of a wind turbine is limited by Betz's law, to about 59.3%.\n\nThe energy of a thermal power station not utilized in power production must leave the plant in the form of heat to the environment. This waste heat can go through a condenser and be disposed of with cooling water or in cooling towers. If the waste heat is instead utilized for district heating, it is called cogeneration. An important class of thermal power station are associated with desalination facilities; these are typically found in desert countries with large supplies of natural gas and in these plants, freshwater production and electricity are equally important co-products.\n\nThe Carnot efficiency dictates that higher efficiencies can be attained by increasing the temperature of the steam. Sub-critical fossil fuel power stations can achieve 36–40% efficiency. Supercritical designs have efficiencies in the low to mid 40% range, with new \"ultra critical\" designs using pressures of 4400 psi (30.3 MPa) and multiple stage reheat reaching about 48% efficiency. Above the critical point for water of and 3212 psi (22.06 MPa), there is no phase transition from water to steam, but only a gradual decrease in density.\n\nCurrently most of the nuclear power stations must operate below the temperatures and pressures that coal-fired plants do, in order to provide more conservative safety margins within the systems that remove heat from the nuclear fuel rods. This, in turn, limits their thermodynamic efficiency to 30–32%. Some advanced reactor designs being studied, such as the very-high-temperature reactor, Advanced Gas-cooled Reactor, and supercritical water reactor, would operate at temperatures and pressures similar to current coal plants, producing comparable thermodynamic efficiency.\n\nThe direct cost of electric energy produced by a thermal power station is the result of cost of fuel, capital cost for the plant, operator labour, maintenance, and such factors as ash handling and disposal. Indirect, social or environmental costs such as the economic value of environmental impacts, or environmental and health effects of the complete fuel cycle and plant decommissioning, are not usually assigned to generation costs for thermal stations in utility practice, but may form part of an environmental impact assessment.\n\nFor units over about 200 MW capacity, redundancy of key components is provided by installing duplicates of the forced and induced draft fans, air preheaters, and fly ash collectors. On some units of about 60 MW, two boilers per unit may instead be provided. The list of coal power stations has the 200 largest power stations ranging in size from 2,000MW to 5,500MW.\n\nIn the nuclear plant field, \"steam generator\" refers to a specific type of large heat exchanger used in a pressurized water reactor (PWR) to thermally connect the primary (reactor plant) and secondary (steam plant) systems, which generates steam. In a nuclear reactor called a boiling water reactor (BWR), water is boiled to generate steam directly in the reactor itself and there are no units called steam generators.\n\nIn some industrial settings, there can also be steam-producing heat exchangers called \"heat recovery steam generators\" (HRSG) which utilize heat from some industrial process, most commonly utilizing hot exhaust from a gas turbine. The steam generating boiler has to produce steam at the high purity, pressure and temperature required for the steam turbine that drives the electrical generator.\n\nGeothermal plants do not need boilers because they use naturally occurring steam sources. Heat exchangers may be used where the geothermal steam is very corrosive or contains excessive suspended solids.\n\nA fossil fuel steam generator includes an economizer, a steam drum, and the furnace with its steam generating tubes and superheater coils. Necessary safety valves are located at suitable points to relieve excessive boiler pressure. The air and flue gas path equipment include: forced draft (FD) fan, air preheater (AP), boiler furnace, induced draft (ID) fan, fly ash collectors (electrostatic precipitator or baghouse), and the flue-gas stack.\n\nThe boiler feedwater used in the steam boiler is a means of transferring heat energy from the burning fuel to the mechanical energy of the spinning steam turbine. The total feed water consists of recirculated \"condensate\" water and purified \"makeup water\". Because the metallic materials it contacts are subject to corrosion at high temperatures and pressures, the makeup water is highly purified before use. A system of water softeners and ion exchange demineralizers produces water so pure that it coincidentally becomes an electrical insulator, with conductivity in the range of 0.3–1.0 microsiemens per centimeter. The makeup water in a 500 MWe plant amounts to perhaps 120 US gallons per minute (7.6 L/s) to replace water drawn off from the boiler drums for water purity management, and to also offset the small losses from steam leaks in the system.\n\nThe feed water cycle begins with condensate water being pumped out of the condenser after traveling through the steam turbines. The condensate flow rate at full load in a 500 MW plant is about 6,000 US gallons per minute (400 L/s).\n\nThe water is pressurized in two stages, and flows through a series of six or seven intermediate feed water heaters, heated up at each point with steam extracted from an appropriate duct on the turbines and gaining temperature at each stage. Typically, in the middle of this series of feedwater heaters, and before the second stage of pressurization, the condensate plus the makeup water flows through a deaerator that removes dissolved air from the water, further purifying and reducing its corrosiveness. The water may be dosed following this point with hydrazine, a chemical that removes the remaining oxygen in the water to below 5 parts per billion (ppb). It is also dosed with pH control agents such as ammonia or morpholine to keep the residual acidity low and thus non-corrosive.\n\nThe boiler is a rectangular furnace about on a side and tall. Its walls are made of a web of high pressure steel tubes about in diameter.\n\nPulverized coal is air-blown into the furnace through burners located at the four corners, or along one wall, or two opposite walls, and it is ignited to rapidly burn, forming a large fireball at the center. The thermal radiation of the fireball heats the water that circulates through the boiler tubes near the boiler perimeter. The water circulation rate in the boiler is three to four times the throughput. As the water in the boiler circulates it absorbs heat and changes into steam. It is separated from the water inside a drum at the top of the furnace. The saturated steam is introduced into superheat pendant tubes that hang in the hottest part of the combustion gases as they exit the furnace. Here the steam is superheated to to prepare it for the turbine.\n\nPlants designed for lignite (brown coal) are increasingly used in locations as varied as Germany, Victoria, Australia, and North Dakota. Lignite is a much younger form of coal than black coal. It has a lower energy density than black coal and requires a much larger furnace for equivalent heat output. Such coals may contain up to 70% water and ash, yielding lower furnace temperatures and requiring larger induced-draft fans. The firing systems also differ from black coal and typically draw hot gas from the furnace-exit level and mix it with the incoming coal in fan-type mills that inject the pulverized coal and hot gas mixture into the boiler.\n\nPlants that use gas turbines to heat the water for conversion into steam use boilers known as heat recovery steam generators (HRSG). The exhaust heat from the gas turbines is used to make superheated steam that is then used in a conventional water-steam generation cycle, as described in the gas turbine combined-cycle plants section.\n\nThe water enters the boiler through a section in the convection pass called the economizer. From the economizer it passes to the steam drum and from there it goes through downcomers to inlet headers at the bottom of the water walls. From these headers the water rises through the water walls of the furnace where some of it is turned into steam and the mixture of water and steam then re-enters the steam drum. This process may be driven purely by natural circulation (because the water is the downcomers is denser than the water/steam mixture in the water walls) or assisted by pumps. In the steam drum, the water is returned to the downcomers and the steam is passed through a series of steam separators and dryers that remove water droplets from the steam. The dry steam then flows into the superheater coils.\n\nThe boiler furnace auxiliary equipment includes coal feed nozzles and igniter guns, soot blowers, water lancing, and observation ports (in the furnace walls) for observation of the furnace interior. Furnace explosions due to any accumulation of combustible gases after a trip-out are avoided by flushing out such gases from the combustion zone before igniting the coal.\n\nThe steam drum (as well as the superheater coils and headers) have air vents and drains needed for initial start up.\n\nFossil fuel power stations often have a superheater section in the steam generating furnace. The steam passes through drying equipment inside the steam drum on to the superheater, a set of tubes in the furnace. Here the steam picks up more energy from hot flue gases outside the tubing, and its temperature is now superheated above the saturation temperature. The superheated steam is then piped through the main steam lines to the valves before the high-pressure turbine.\n\nNuclear-powered steam plants do not have such sections but produce steam at essentially saturated conditions. Experimental nuclear plants were equipped with fossil-fired superheaters in an attempt to improve overall plant operating cost.\n\nThe condenser condenses the steam from the exhaust of the turbine into liquid to allow it to be pumped. If the condenser can be made cooler, the pressure of the exhaust steam is reduced and efficiency of the cycle increases.\n\nThe surface condenser is a shell and tube heat exchanger in which cooling water is circulated through the tubes. The exhaust steam from the low-pressure turbine enters the shell, where it is cooled and converted to condensate (water) by flowing over the tubes as shown in the adjacent diagram. Such condensers use steam ejectors or rotary motor-driven exhausts for continuous removal of air and gases from the steam side to maintain vacuum.\n\nFor best efficiency, the temperature in the condenser must be kept as low as practical in order to achieve the lowest possible pressure in the condensing steam. Since the condenser temperature can almost always be kept significantly below 100 °C where the vapor pressure of water is much less than atmospheric pressure, the condenser generally works under vacuum. Thus leaks of non-condensible air into the closed loop must be prevented.\n\nTypically the cooling water causes the steam to condense at a temperature of about and that creates an absolute pressure in the condenser of about , i.e. a vacuum of about relative to atmospheric pressure. The large decrease in volume that occurs when water vapor condenses to liquid creates the low vacuum that helps pull steam through and increase the efficiency of the turbines.\n\nThe limiting factor is the temperature of the cooling water and that, in turn, is limited by the prevailing average climatic conditions at the power station's location (it may be possible to lower the temperature beyond the turbine limits during winter, causing excessive condensation in the turbine). Plants operating in hot climates may have to reduce output if their source of condenser cooling water becomes warmer; unfortunately this usually coincides with periods of high electrical demand for air conditioning.\n\nThe condenser generally uses either circulating cooling water from a cooling tower to reject waste heat to the atmosphere, or once-through water from a river, lake or ocean.\nThe heat absorbed by the circulating cooling water in the condenser tubes must also be removed to maintain the ability of the water to cool as it circulates. This is done by pumping the warm water from the condenser through either natural draft, forced draft or induced draft cooling towers (as seen in the adjacent image) that reduce the temperature of the water by evaporation, by about 11 to 17 °C (20 to 30 °F)—expelling waste heat to the atmosphere. The circulation flow rate of the cooling water in a 500 MW unit is about 14.2 m³/s (500 ft³/s or 225,000 US gal/min) at full load.\n\nThe condenser tubes are made of brass or stainless steel to resist corrosion from either side. Nevertheless, they may become internally fouled during operation by bacteria or algae in the cooling water or by mineral scaling, all of which inhibit heat transfer and reduce thermodynamic efficiency. Many plants include an automatic cleaning system that circulates sponge rubber balls through the tubes to scrub them clean without the need to take the system off-line.\n\nThe cooling water used to condense the steam in the condenser returns to its source without having been changed other than having been warmed. If the water returns to a local water body (rather than a circulating cooling tower), it is often tempered with cool 'raw' water to prevent thermal shock when discharged into that body of water.\n\nAnother form of condensing system is the air-cooled condenser. The process is similar to that of a radiator and fan. Exhaust heat from the low-pressure section of a steam turbine runs through the condensing tubes, the tubes are usually finned and ambient air is pushed through the fins with the help of a large fan. The steam condenses to water to be reused in the water-steam cycle. Air-cooled condensers typically operate at a higher temperature than water-cooled versions. While saving water, the efficiency of the cycle is reduced (resulting in more carbon dioxide per megawatt-hour of electricity).\n\nFrom the bottom of the condenser, powerful condensate pumps recycle the condensed steam (water) back to the water/steam cycle.\n\nPower station furnaces may have a reheater section containing tubes heated by hot flue gases outside the tubes. Exhaust steam from the high-pressure turbine is passed through these heated tubes to collect more energy before driving the intermediate and then low-pressure turbines.\n\nExternal fans are provided to give sufficient air for combustion. The Primary air fan takes air from the atmosphere and, first warms the air in the air preheater for better economy. Primary air then passes through the coal pulverizers, and carries the coal dust to the burners for injection into the furnace. The Secondary air fan takes air from the atmosphere and, first warms the air in the air preheater for better economy. Secondary air is mixed with the coal/primary air flow in the burners.\n\nThe induced draft fan assists the FD fan by drawing out combustible gases from the furnace, maintaining a slightly negative pressure in the furnace to avoid leakage of combustion products from the boiler casing.\n\nThe turbine generator consists of a series of steam turbines interconnected to each other and a generator on a common shaft. There is usually a high-pressure turbine at one end, followed by an intermediate-pressure turbine, and finally one, two, or three low-pressure turbines, and the generator. As steam moves through the system and loses pressure and thermal energy, it expands in volume, requiring increasing diameter and longer blades at each succeeding stage to extract the remaining energy. The entire rotating mass may be over 200 metric tons and long. It is so heavy that it must be kept turning slowly even when shut down (at 3 rpm) so that the shaft will not bow even slightly and become unbalanced. This is so important that it is one of only six functions of blackout emergency power batteries on site. (The other five being emergency lighting, communication, station alarms, generator hydrogen seal system, and turbogenerator lube oil.)\n\nFor a typical late 20th-century power station, superheated steam from the boiler is delivered through diameter piping at and to the high-pressure turbine, where it falls in pressure to and to in temperature through the stage. It exits via diameter cold reheat lines and passes back into the boiler, where the steam is reheated in special reheat pendant tubes back to . The hot reheat steam is conducted to the intermediate pressure turbine, where it falls in both temperature and pressure and exits directly to the long-bladed low-pressure turbines and finally exits to the condenser.\n\nThe generator, typically about long and in diameter, contains a stationary stator and a spinning rotor, each containing miles of heavy copper conductor. There is generally no permanent magnet, thus preventing black starts. In operation it generates up to 21,000 amperes at 24,000 volts AC (504 MWe) as it spins at either 3,000 or 3,600 rpm, synchronized to the power grid. The rotor spins in a sealed chamber cooled with hydrogen gas, selected because it has the highest known heat transfer coefficient of any gas and for its low viscosity, which reduces windage losses. This system requires special handling during startup, with air in the chamber first displaced by carbon dioxide before filling with hydrogen. This ensures that a highly explosive hydrogen–oxygen environment is not created.\n\nThe power grid frequency is 60 Hz across North America and 50 Hz in Europe, Oceania, Asia (Korea and parts of Japan are notable exceptions), and parts of Africa. The desired frequency affects the design of large turbines, since they are highly optimized for one particular speed.\n\nThe electricity flows to a distribution yard where transformers increase the voltage for transmission to its destination.\n\nThe steam turbine-driven generators have auxiliary systems enabling them to work satisfactorily and safely. The steam turbine generator, being rotating equipment, generally has a heavy, large-diameter shaft. The shaft therefore requires not only supports but also has to be kept in position while running. To minimize the frictional resistance to the rotation, the shaft has a number of bearings. The bearing shells, in which the shaft rotates, are lined with a low-friction material like Babbitt metal. Oil lubrication is provided to further reduce the friction between shaft and bearing surface and to limit the heat generated.\n\nAs the combustion flue gas exits the boiler it is routed through a rotating flat basket of metal mesh which picks up heat and returns it to incoming fresh air as the basket rotates. This is called the air preheater. The gas exiting the boiler is laden with fly ash, which are tiny spherical ash particles. The flue gas contains nitrogen along with combustion products carbon dioxide, sulfur dioxide, and nitrogen oxides. The fly ash is removed by fabric bag filters in baghouses or electrostatic precipitators. Once removed, the fly ash byproduct can sometimes be used in the manufacturing of concrete. This cleaning up of flue gases, however, only occurs in plants that are fitted with the appropriate technology. Still, the majority of coal-fired power stations in the world do not have these facilities. Legislation in Europe has been efficient to reduce flue gas pollution. Japan has been using flue gas cleaning technology for over 30 years and the US has been doing the same for over 25 years. China is now beginning to grapple with the pollution caused by coal-fired power stations.\n\nWhere required by law, the sulfur and nitrogen oxide pollutants are removed by stack gas scrubbers which use a pulverized limestone or other alkaline wet slurry to remove those pollutants from the exit stack gas. Other devices use catalysts to remove nitrous oxide compounds from the flue-gas stream. The gas travelling up the flue-gas stack may by this time have dropped to about . A typical flue-gas stack may be tall to disperse the remaining flue gas components in the atmosphere. The tallest flue-gas stack in the world is tall at the Ekibastuz GRES-2 Power Station in Kazakhstan.\n\nIn the United States and a number of other countries, atmospheric dispersion modeling studies are required to determine the flue-gas stack height needed to comply with the local air pollution regulations. The United States also requires the height of a flue-gas stack to comply with what is known as the \"good engineering practice\" (GEP) stack height. In the case of existing flue gas stacks that exceed the GEP stack height, any air pollution dispersion modeling studies for such stacks must use the GEP stack height rather than the actual stack height.\n\nFly ash is captured and removed from the flue gas by electrostatic precipitators or fabric bag filters (or sometimes both) located at the outlet of the furnace and before the induced draft fan. The fly ash is periodically removed from the collection hoppers below the precipitators or bag filters. Generally, the fly ash is pneumatically transported to storage silos for subsequent transport by trucks or railroad cars.\n\nAt the bottom of the furnace, there is a hopper for collection of bottom ash. This hopper is kept filled with water to quench the ash and clinkers falling down from the furnace. Arrangements are included to crush the clinkers and convey the crushed clinkers and bottom ash to a storage site. Ash extractors are used to discharge ash from municipal solid waste–fired boilers.\n\nSince there is continuous withdrawal of steam and continuous return of condensate to the boiler, losses due to blowdown and leakages have to be made up to maintain a desired water level in the boiler steam drum. For this, continuous make-up water is added to the boiler water system. Impurities in the raw water input to the plant generally consist of calcium and magnesium salts which impart hardness to the water. Hardness in the make-up water to the boiler will form deposits on the tube water surfaces which will lead to overheating and failure of the tubes. Thus, the salts have to be removed from the water, and that is done by a water demineralising treatment plant (DM). A DM plant generally consists of cation, anion, and mixed bed exchangers. Any ions in the final water from this process consist essentially of hydrogen ions and hydroxide ions, which recombine to form pure water. Very pure DM water becomes highly corrosive once it absorbs oxygen from the atmosphere because of its very high affinity for oxygen.\n\nThe capacity of the DM plant is dictated by the type and quantity of salts in the raw water input. However, some storage is essential as the DM plant may be down for maintenance. For this purpose, a storage tank is installed from which DM water is continuously withdrawn for boiler make-up. The storage tank for DM water is made from materials not affected by corrosive water, such as PVC. The piping and valves are generally of stainless steel. Sometimes, a steam blanketing arrangement or stainless steel doughnut float is provided on top of the water in the tank to avoid contact with air. DM water make-up is generally added at the steam space of the surface condenser (i.e., the vacuum side). This arrangement not only sprays the water but also DM water gets deaerated, with the dissolved gases being removed by a de-aerator through an ejector attached to the condenser.\n\nIn coal-fired power stations, the raw feed coal from the coal storage area is first crushed into small pieces and then conveyed to the coal feed hoppers at the boilers. The coal is next pulverized into a very fine powder. The pulverizers may be ball mills, rotating drum grinders, or other types of grinders.\n\nSome power stations burn fuel oil rather than coal. The oil must kept warm (above its pour point) in the fuel oil storage tanks to prevent the oil from congealing and becoming unpumpable. The oil is usually heated to about 100 °C before being pumped through the furnace fuel oil spray nozzles.\n\nBoilers in some power stations use processed natural gas as their main fuel. Other power stations may use processed natural gas as auxiliary fuel in the event that their main fuel supply (coal or oil) is interrupted. In such cases, separate gas burners are provided on the boiler furnaces.\n\nBarring gear (or \"turning gear\") is the mechanism provided to rotate the turbine generator shaft at a very low speed after unit stoppages. Once the unit is \"tripped\" (i.e., the steam inlet valve is closed), the turbine coasts down towards standstill. When it stops completely, there is a tendency for the turbine shaft to deflect or bend if allowed to remain in one position too long. This is because the heat inside the turbine casing tends to concentrate in the top half of the casing, making the top half portion of the shaft hotter than the bottom half. The shaft therefore could warp or bend by millionths of inches.\n\nThis small shaft deflection, only detectable by eccentricity meters, would be enough to cause damaging vibrations to the entire steam turbine generator unit when it is restarted. The shaft is therefore automatically turned at low speed (about one percent rated speed) by the barring gear until it has cooled sufficiently to permit a complete stop.\n\nAn auxiliary oil system pump is used to supply oil at the start-up of the steam turbine generator. It supplies the hydraulic oil system required for steam turbine's main inlet steam stop valve, the governing control valves, the bearing and seal oil systems, the relevant hydraulic relays and other mechanisms.\n\nAt a preset speed of the turbine during start-ups, a pump driven by the turbine main shaft takes over the functions of the auxiliary system.\n\nWhile small generators may be cooled by air drawn through filters at the inlet, larger units generally require special cooling arrangements. Hydrogen gas cooling, in an oil-sealed casing, is used because it has the highest known heat transfer coefficient of any gas and for its low viscosity which reduces windage losses. This system requires special handling during start-up, with air in the generator enclosure first displaced by carbon dioxide before filling with hydrogen. This ensures that the highly flammable hydrogen does not mix with oxygen in the air.\n\nThe hydrogen pressure inside the casing is maintained slightly higher than atmospheric pressure to avoid outside air ingress. The hydrogen must be sealed against outward leakage where the shaft emerges from the casing. Mechanical seals around the shaft are installed with a very small annular gap to avoid rubbing between the shaft and the seals. Seal oil is used to prevent the hydrogen gas leakage to atmosphere.\n\nThe generator also uses water cooling. Since the generator coils are at a potential of about 22 kV, an insulating barrier such as Teflon is used to interconnect the water line and the generator high-voltage windings. Demineralized water of low conductivity is used.\n\nThe generator voltage for modern utility-connected generators ranges from in smaller units to in larger units. The generator high-voltage leads are normally large aluminium channels because of their high current as compared to the cables used in smaller machines. They are enclosed in well-grounded aluminium bus ducts and are supported on suitable insulators. The generator high-voltage leads are connected to step-up transformers for connecting to a high-voltage electrical substation (usually in the range of 115 kV to 765 kV) for further transmission by the local power grid.\n\nThe necessary protection and metering devices are included for the high-voltage leads. Thus, the steam turbine generator and the transformer form one unit. Smaller units may share a common generator step-up transformer with individual circuit breakers to connect the generators to a common bus.\n\nMost of the power station operational controls are automatic. However, at times, manual intervention may be required. Thus, the plant is provided with monitors and alarm systems that alert the plant operators when certain operating parameters are seriously deviating from their normal range.\n\nA central battery system consisting of lead–acid cell units is provided to supply emergency electric power, when needed, to essential items such as the power station's control systems, communication systems, generator hydrogen seal system, turbine lube oil pumps, and emergency lighting. This is essential for a safe, damage-free shutdown of the units in an emergency situation.\n\nTo dissipate the thermal load of main turbine exhaust steam, condensate from gland steam condenser, and condensate from Low Pressure Heater by providing a continuous supply of cooling water to the main condenser thereby leading to condensation.\n\nThe consumption of cooling water by inland power stations is estimated to reduce power availability for the majority of thermal power stations by 2040–2069.\n\nMost thermal stations use coal as the main fuel. Raw coal is transported from coal mines to a power station site by trucks, barges, bulk cargo ships, or railway cars. Generally, when shipped by railway, the coal cars are sent as a full train of cars. The coal received at site may be of different sizes. The railway cars are unloaded at site by rotary dumpers or side tilt dumpers to tip over onto conveyor belts below. The coal is generally conveyed to crushers which crush the coal to about size. The crushed coal is then sent by belt conveyors to a storage pile. Normally, the crushed coal is compacted by bulldozers, as compacting of highly volatile coal avoids spontaneous ignition.\n\nThe crushed coal is conveyed from the storage pile to silos or hoppers at the boilers by another belt conveyor system.\n\n\n"}
{"id": "41659140", "url": "https://en.wikipedia.org/wiki?curid=41659140", "title": "Ukrainian Optical Facilities for Near-Earth Space Surveillance Network", "text": "Ukrainian Optical Facilities for Near-Earth Space Surveillance Network\n\nUkrainian Optical Facilities for Near-Earth Space Surveillance Network, the UMOS (from Ukrainian acronym: УМОС = \"Українська мережа оптичних станцій дослідження навколоземного космічного простору\") is an alliance of Ukrainian institutional research observatories and optical facilities of State Space Agency. Its strategic tasks include near-Earth space\nresearch (from LEO to HEO), and the studies of motion of selected objects by development and improvement of theory, models and algorithms. Also it is aimed at addressing tactical problems like\nassistance in Ukrainian or international space launches. The UMOS's field of view covers large area of\nspace, its facilities arrangement allows to carry out the observations of 3-4 consecutive passes of LEO satellites, as well as synchronous and complementing observations.\n\nThe UMOS has conducted the ground-based surveillance of Near-Earth objects since the establishment of the network. The results of data processing earned positive feedbacks from customers in Ukraine: the State Space Agency, as well as our partners abroad. In particular, since 2011 the UMOS (some of the observatories since 2005) has conducted maintenance the first circuits after launching into orbit (Sun-synchronous) with Dnepr to identify and/or refine orbits of space objects (e.g. RapidEye, EgyptSat‑1, CryoSat‑2, , corresponding R/B of launcher).\n\nAlso the UMOS carries out optical observations of active satellites and space debris on a single joint list to produce independent TLE catalog and to estimate the probabilities of satellite orbital conjunction for priority satellites, which meets the requirements of national Control Systems and Space Environment Analysis (i.e. similar to Space Situational Awareness). During just 2012 over 130,000 of position estimates for 194 LEO space objects (including series RapidEye, Orbcomm, CubeSat), 37 MEO objects, 29 GEO and GSO objects and also HEO space observatory Radioastron were obtained. As a result, the observation-based database was created, which had comprised 1509 sets of TLE for 183 space objects until the end of 2012.\n\nRecently, due to the increase in number of space debris and solar activity maximum, the operational failures at satellites are much more frequent. Photometric observations can aid in satellite in-orbit status analysis and provide with necessary data for the assessment of feasibility of their recovery.\n\nThe photometric observations and data analysis for five tumbling satellites have been made since 2010. Among these five satellites were: two launch failures - Express-AM4 (5,775 kg mass at altitude of 1,000 km), Phobos-Grunt (13,200 kg mass on the low reference orbit of 250 km); as well as three accidents of satellite malfunction, when the appropriate control center has lost the connection - CBERS-2B (1,450 kg at altitude of 780 km), EgyptSat-1 (160 kg at altitude of 670 km), (170 kg at altitude of 670 km). All these satellites, except Phobos-Grunt, are non-resolved objects, thus photometry has been the only source of in-orbit status information. Over 50,000 photometric brightness estimates were obtained for the satellites both in the integral light and broadband BVR Johnson system filters. The UMOS is proven capable to run the whole study, from primary observations to analysis of the light curves and further determination of satellite orientation variations and origin of its possible damage.\n\n"}
{"id": "95254", "url": "https://en.wikipedia.org/wiki?curid=95254", "title": "Ungud", "text": "Ungud\n\nIn Australian Aboriginal mythology, Ungud is a snake god who is sometimes male and sometimes female. He is associated with rainbows and the fertility and erections of the tribe's shamans.\n\n"}
{"id": "25685247", "url": "https://en.wikipedia.org/wiki?curid=25685247", "title": "Walters Shoals", "text": "Walters Shoals\n\nThe Walters Shoals is a group of submerged mountains off the coast of Madagascar. Despite being from the coast, the tips of some of the mountains are only below the surface. The Walters Shoals is home to many species of fish, crustaceans, and mollusks.\nIt was discovered in 1963 by the South African Hydrographic Frigate SAS \"Natal\" captained by Cmdr Walters. When found it had a huge population of Galápagos sharks but they have since been fished out.\n"}
