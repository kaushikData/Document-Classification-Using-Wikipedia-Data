{"id": "217706", "url": "https://en.wikipedia.org/wiki?curid=217706", "title": "Aldo Leopold", "text": "Aldo Leopold\n\nAldo Leopold (January 11, 1887 – April 21, 1948) was an American author, philosopher, scientist, ecologist, forester, conservationist, and environmentalist. He was a professor at the University of Wisconsin and is best known for his book \"A Sand County Almanac\" (1949), which has sold more than two million copies.\n\nLeopold was influential in the development of modern environmental ethics and in the movement for wilderness conservation. His ethics of nature and wildlife preservation had a profound impact on the environmental movement, with his ecocentric or holistic ethics regarding land. He emphasized biodiversity and ecology and was a founder of the science of wildlife management.\n\nRand Aldo Leopold was born in Burlington, Iowa, on January 11, 1887. His father, Carl Leopold, was a businessman who made walnut desks and was first cousin to his wife, Clara Starker. Charles Starker, father of Clara and uncle of Carl, was a German immigrant, educated in engineering and architecture. Rand Aldo was named after two of his father's business partners—C. W. Rand and Aldo Sommers—although he eventually dropped the use of \"Rand\". The Leopold family included younger siblings Mary Luize, Carl Starker, and Frederic. Leopold's first language was German, although he mastered English at an early age.\n\nAldo Leopold's early life was highlighted by the outdoors. Carl would take his children on excursions into the woods and taught his oldest son woodcraft and hunting. Aldo showed an aptitude for observation, spending hours counting and cataloging birds near his home. Mary would later say of her older brother, \"He was very much an outdoorsman, even in his extreme youth. He was always out climbing around the bluffs, or going down to the river, or going across the river into the woods.\" He attended Prospect Hill Elementary, where he ranked at the top of his class, and then, the overcrowded Burlington High School. Every August, the family vacationed in Michigan at the forested Les Cheneaux Islands in Lake Huron, which the children took to exploring.\n\nIn 1900, Gifford Pinchot, who oversaw the newly implemented Division of Forestry in the Department of Agriculture, donated money to Yale University to begin one of the nation's first forestry schools. Hearing of this development, the teenaged Leopold decided on forestry as a vocation. His parents agreed to let him attend The Lawrenceville School, a preparatory college in New Jersey, to improve his chances of admission to Yale. The Burlington High School principal wrote in a reference letter to the headmaster at Lawrenceville that Leopold was \"as earnest a boy as we have in school... painstaking in his work... Moral character above reproach.\" He arrived at his new school in January 1904, shortly before he turned 17. He was considered an attentive student, although he was again drawn to the outdoors. Lawrenceville was suitably rural, and Leopold spent much time mapping the area and studying its wildlife. Leopold studied at the Lawrenceville School for a year, during which time he was accepted to Yale. Because the Yale School of Forestry granted only graduate degrees, he first enrolled in Sheffield Scientific School's preparatory forestry courses for his undergraduate studies. While Leopold was able to explore the woods and fields of Lawrenceville daily, sometimes to the detriment of his studying, at Yale he had little opportunity to do so; his studies and social life engagements made his outdoor trips few and far between.\n\nIn 1909, Leopold was assigned to the Forest Service's District 3 in the Arizona and New Mexico territories. At first, he was a forest assistant at the Apache National Forest in the Arizona Territory. In 1911, he was transferred to the Carson National Forest in northern New Mexico. Leopold's career, which kept him in New Mexico until 1924, included developing the first comprehensive management plan for the Grand Canyon, writing the Forest Service's first game and fish handbook, and proposing Gila Wilderness Area, the first national wilderness area in the Forest Service system.\n\nOn April 5, 1923, he was elected an associate member (now called \"professional member\") of the Boone and Crockett Club, a wildlife conservation organization founded by Theodore Roosevelt and George Bird Grinnell.\n\nIn 1924, he accepted transfer to the U.S. Forest Products Laboratory in Madison, Wisconsin, and became an associate director.\n\nIn 1933, he was appointed Professor of Game Management in the Agricultural Economics Department at the University of Wisconsin, the first such professorship of wildlife management.\n\nHe married Estella Bergere in northern New Mexico in 1912 and they had five children together. They lived in a modest two-story home close to the UW–Madison campus. His children followed in his footsteps as teachers and naturalists: Aldo Starker Leopold (1913–1983) was a wildlife biologist and professor at UC Berkeley; Luna B. Leopold (1915–2006) became a hydrologist and geology professor at UC Berkeley; Nina Leopold Bradley (1917–2011) was a researcher and naturalist; Aldo Carl Leopold (1919–2009) was a plant physiologist, who taught at Purdue University for 25 years; and daughter Estella Leopold (b. 1927) is a noted botanist and conservationist who is a professor \"emeritus\" at the University of Washington. Today, Leopold's home is an official landmark of the city of Madison.\n\nHe purchased 80 acres in the sand country of central Wisconsin. The once-forested region had been logged, swept by repeated fires, overgrazed by dairy cows, and left barren. There, he put his theories to work in the field and eventually wrote his best-selling \"A Sand County Almanac\" (1949), finished just prior to his death. Leopold died of a heart attack while battling a wild fire on a neighbor's property.\n\nEarly on, Leopold was assigned to hunt and kill bears, wolves, and mountain lions in New Mexico. Local ranchers hated these predators because of livestock losses, but Leopold came to respect the animals. He developed an ecological ethic that replaced the earlier wilderness ethic that stressed the need for human dominance. His rethinking the importance of predators in the balance of nature has resulted in the return of bears and mountain lions to New Mexico wilderness areas.\n\nBy the early 1920s, Leopold had concluded that a particular kind of preservation should be embraced in the national forests of the American West. He was prompted to this by the rampant building of roads to accommodate the \"proliferation of the automobile\" and the related increasingly heavy recreational demands placed on public lands. He was the first to employ the term \"wilderness\" to describe such preservation. Over the next two decades, he added ethical and scientific rationales to his defense of the wilderness concept. In one essay, he rhetorically asked, \"Of what avail are forty freedoms without a blank spot on the map?\" Leopold saw a progress of ethical sensitivity from interpersonal relationships, to relationships to society as a whole, to relationships with the land, leading to a steady diminution of actions based on expediency, conquest, and self-interest. Leopold thus rejected the utilitarianism of conservationists such as Theodore Roosevelt.\n\nBy the 1930s, Leopold was the nation's foremost expert on wildlife management. He advocated the scientific management of wildlife habitats by both public and private landholders rather than a reliance on game refuges, hunting laws, and other methods intended to protect specific species of desired game. In his 1933 book \"Game Management\", Leopold defined the science of wildlife management as \"the art of making land produce sustained annual crops of wild game for recreational use.\" But, as Curt Meine has pointed out, he also considered it to be a technique for restoring and maintaining diversity in the environment.\n\nThe concept of \"wilderness\" also took on a new meaning; Leopold no longer saw it as a hunting or recreational ground, but as an arena for a healthy biotic community, including wolves and mountain lions. In 1935, he helped found the Wilderness Society, dedicated to expanding and protecting the nation's wilderness areas. He regarded the society as \"one of the focal points of a new attitude—an intelligent humility toward Man's place in nature.\" Science writer Connie Barlow says Leopold wrote eloquently from a perspective that today would be called Religious Naturalism.\nLeopold's nature writing is notable for its simple directness. His portrayals of various natural environments through which he had moved, or had known for many years, displayed impressive intimacy with what exists and happens in nature. He offered frank criticism of the harm he believed was frequently done to natural systems (such as land) out of a sense of a culture or society's sovereign ownership over the land base – eclipsing any sense of a community of life to which humans belong. He felt the security and prosperity resulting from \"mechanization\" now gives people the time to reflect on the preciousness of nature and to learn more about what happens there; however, he also wrote, \"Theoretically, the mechanization of farming ought to cut the farmer's chains, but whether it really does is debatable.\"\n\nThe book was published in 1949, shortly after Leopold's death. One of the well-known quotes from the book which clarifies his land ethic is,\n\nA thing is right when it tends to preserve the integrity, stability, and beauty of the biotic community. It is wrong when it tends otherwise. (p.262) \n\nThe concept of a trophic cascade is put forth in the chapter, \"Thinking Like a Mountain\", wherein Leopold realizes that killing a predator wolf carries serious implications for the rest of the ecosystem — a conclusion that found sympathetic appreciation generations later:\n\nIn \"The Land Ethic\", a chapter in \"A Sand County Almanac\", Leopold delves into conservation in \"The Ecological Conscience\" section. He wrote: \"Conservation is a state of harmony between men and land.\" He noted that conservation guidelines at the time boiled down to: \"obey the law, vote right, join some organizations, and practice what conservation is profitable on your own land; the government will do the rest.\" (p. 243–244)\n\nLeopold explained:\n\nThe Aldo Leopold Foundation of Baraboo, Wisconsin, was founded in 1982 by Aldo and Estella Leopold's five children as a 501(c)3 not-for-profit conservation organization whose mission is \"to foster the land ethic through the legacy of Aldo Leopold.\" The Aldo Leopold Foundation owns and manages the original Aldo Leopold Shack and Farm and 300 surrounding acres, in addition to several other parcels. Its headquarters is at the green-built Leopold Center, where it conducts educational and land stewardship programs. The foundation also acts as the executor of Leopold's literary estate, encourages scholarship on Leopold, and serves as a clearinghouse for information regarding Leopold, his work, and his ideas. It provides interpretive resources and tours for thousands of visitors annually, distributes a curriculum about how to use Leopold's writing and ideas in environmental education, and maintains a robust website and numerous print resources. In 2012, in collaboration with the United States Forest Service, the foundation released the first high-definition, full-length film about Leopold, entitled \"Green Fire: Aldo Leopold and a Land Ethic for Our Time.\" The film aired on public television stations across the nation and won a Midwest regional Emmy award in the documentary category.\nThe Aldo Leopold Wilderness in New Mexico's Gila National Forest was named after him in 1980.\n\nThe Leopold Center for Sustainable Agriculture was established in 1987 at Iowa State University in Ames. It was named in honor of Leopold. Since its founding, it has pioneered new forms of sustainable agriculture practices.\n\nThe U.S. Forest Service established the Aldo Leopold Wilderness Research Institute at the University of Montana, Missoula in 1993. It is \"the only Federal research group in the United States dedicated to the development and dissemination of knowledge needed to improve management of wilderness, parks, and similarly protected areas.\"\n\nThe Aldo Leopold Legacy Trail System, a system of 42 state trails in Wisconsin, was created by the state in 2007.\n\nAn organization, the Leopold Heritage Group, is \"dedicated to promoting the global legacy of Aldo Leopold in his hometown of Burlington, Iowa.\"\n\n\n\n\n\n"}
{"id": "11573895", "url": "https://en.wikipedia.org/wiki?curid=11573895", "title": "Alien from L.A.", "text": "Alien from L.A.\n\nAlien From L.A. is a 1988 science fiction film that stars Kathy Ireland as a young woman who visits the underground civilization of Atlantis. The film was featured on \"Mystery Science Theater 3000\".\n\nWanda Saknussemm (Ireland) is a nerdy social misfit with large glasses and an intolerable squeaky voice who lives in Los Angeles and works at a diner. After being dumped by her boyfriend for \"not having a sense of adventure\", Wanda is informed by a letter that her father, an archaeologist, has died. She flies to North Africa and while going through her father's belongings, she finds his notes about Atlantis, apparently an alien ship that crashed millennia ago and sank into the center of the Earth. Wanda comes across a chamber beneath her father's apartment and accidentally sets off a chain of events that ultimately cause her to fall into a deep hole.\n\nAn unharmed Wanda wakes up deep within the Earth to find Gus (William R. Moses), a miner whom she protects from being slain by two people. Gus, who has a very inconsistent Australian accent, agrees to help Wanda find her father, whom she believes is alive and trapped underground. Wanda soon discovers that both she and her father are believed to be spies planning an invasion of Atlantis. During her adventures, Wanda's appearance changes from nerdy to attractive (by removing her glasses and using a steam vent to clean her skin). People from the surface world are referred to as \"aliens\" by Atlanteans, who appear virtually identical to surface dwellers, and when Wanda is overheard talking about Malibu Beach by a low-life informant (Janie Du Plessis), she soon becomes a hunted woman and must dodge efforts at capture, both from the mysterious \"Government House\" and from thugs in the pay of the crime lord Mambino (Deep Roy). During these sequences, many references to Wanda's \"big bones\" are made, as though it were a trait by which she could be identified; however, no obvious physical distinction between Wanda and the Atlanteans is noticeable.\n\nWanda's efforts at escape are aided by Charmin (Thom Matthews), a handsome rogue who (briefly) assists her flight and falls for Wanda. She is ultimately captured by the evil General Pykov (Du Plessis again), who wants to kill both Wanda and her incarcerated father. The Atlantean leader decides to free Wanda and her father, provided they remain quiet about Atlantis. Gus shows up and helps the duo escape while fighting off General Pykov and her soldiers. Wanda and her father board a ship that takes them back to the surface and the film ends with Wanda on the beach, wearing a bikini and a sarong. She refuses the advances of her ex-boyfriend and is soon reunited with Charmin, who inexplicably appears on a motorcycle.\n\n\nThe film was mostly shot in Johannesburg, at producer Avi Lerner's studio, plus additional shooting in Durban, South Africa and Swakopmund, Namibia. Locations ranged from South Africa's deep digging mines and gold fields both on the outskirts of Johannesburg. There was one additional day of shooting at a safari complex near Pretoria. Most of the Namibia shoot took place in and around the old German colonial town of Swakopmund, with additional scenes also shot along Namibia's famed Skeleton Coast.\n\nThe character of Arnold Saknussemm is a reference to Arne Saknussemm from Jules Verne's \"Journey to the Center of the Earth\", an Icelandic explorer who had gone missing long before the narrative of the story had begun.\n\nThe film was featured in a 1993 episode of \"Mystery Science Theater 3000\". This episode was released in March 2013 by Shout! Factory.\n\n"}
{"id": "3073175", "url": "https://en.wikipedia.org/wiki?curid=3073175", "title": "Amphinome", "text": "Amphinome\n\nIn Greek mythology, the name Amphinome (Ancient Greek: Ἀμφινόμη) may refer to:\n"}
{"id": "19810421", "url": "https://en.wikipedia.org/wiki?curid=19810421", "title": "Astra 19.2°E", "text": "Astra 19.2°E\n\nAstra 19.2°E is the name for the group of Astra communications satellites co-located at the 19.2°East orbital position in the Clarke Belt that are owned and operated by SES based in Betzdorf, Luxembourg.\n\nAstra 19.2°E used to be commonly known as Astra 1, as it was the first orbital position used by Astra and the craft positioned there all have the Astra 1x name, but this was changed by SES to Astra 19.2°E in 2008, to avoid confusion with other Astra orbital positions that now include Astra 1x craft originally positioned at 19.2°East.\n\nThe Astra satellites at 19.2°East provide for services downlinking in the 10.70 GHz-12.70 GHz range of the K band.\n\nAstra 19.2°E is one of the major TV satellite positions serving Europe, transmitting over 1,150 TV, radio and interactive channels to more than 93 million direct-to-home (DTH) and cable homes in 35 countries (the other major satellite positions being at 13° East, 28.2° East, 23.5° East, and 5° East).\n\nThere are more than 40 high definition television (HDTV) channels broadcast by the satellites at 19.2°E, using five HDTV platforms. SES was instrumental in introducing satellite HDTV broadcasting in Europe, using the Astra 19.2°E satellites, and helped establish the HD ready specifications for TVs to view HDTV broadcasts. A subsidiary of SES, HD+ operates the HD+ free-to-view platform of German channels from Astra 19.2°E.\n\nAstra 19.2°E was one of the last satellite position to carry numerous analogue channels, until April 30, 2012 when the switch-off of German analogue broadcasts was completed. It is also the only position to have carried radio stations in the proprietary Astra Digital Radio format, although that technology was superseded by DVB-S radio as the analogue transponders that carried the service switched to digital.\n\n\n\nThe satellites at the Astra 19.2°E position primarily provide digital TV, digital radio and multimedia services to Europe and North Africa, principally to Algeria, Austria, Belgium, France, Germany, Morocco, the Netherlands, Poland, Spain, Switzerland, and Tunisia.\n\nAstra 19.2°E provides both free-to-air and a number of pay-TV services in networks such as ARD Digital, ArenaSat, CanalDigitaal, CanalSat, Canal+, ORF Digital, Sky Germany, ProSieben, Sat.1, UPC Direct, and ZDF, and is the market leader for DTH and communal dish reception in Austria, Belgium, France, Germany, the Netherlands, Spain and Switzerland.\n\nThe relatively close proximity of Astra 19.2°E to one of SES' other orbital positions, Astra 23.5°E, allows the use in target countries of a single small dish fitted with a monoblock Duo LNB to receive channels from both positions.\n\nAs of the end of 2016, the Astra satellites at 19.2° east broadcast 913 channels (525 in SD, 381 in HD, 7 in UHD) to 116 million households\n\nLaunched in 1988, Astra 1A was the first satellite in the Astra 19.2°E group. With 16 transponders, Astra 1A was the first satellite intended for DTH reception of satellite TV across Europe. From the start of transmissions in 1989, Astra 1A carried four channels for Sky Television plc, the world's first commercial multi-channel DTH service, on transponders leased before the satellite was completed.\n\nEarly channels broadcasting from 19.2°East included those primarily intended for the UK, Germany, the Benelux countries, and Scandinavia, and so-called pan-European channels such as MTV Europe, CNN International, and National Geographic Channel.\n\nAstra 1A was joined at 19.2°East by Astra 1B in 1991 and subsequently by Astra 1C in 1993, establishing SES' principles of co-locating satellites for the provision of transparent backup by each satellite for the others in the group.\n\nThe first three satellites at Astra 19.2°E carried only analogue channels in PAL and D2-MAC. The fourth satellite, Astra 1D launched in 1994, was originally intended to carry the first European digital TV channels but the rapid expansion of satellite television across Europe and demand for analogue TV capacity meant that it was primarily used for analogue signals.\n\nAstra 1E (1995) was dedicated to digital satellite TV services for Europe and subsequent satellites launched to Astra 19.2°E were also all-digital in the traffic they carried.\n\nHand-in-hand with the switchover to digital transmission of TV by satellite came a shift to encryption and the targeting of channels to individual countries or regions. The demand for digital TV capacity was so great that SES opened up additional orbital positions to provide for new digital networks aimed at specific countries, starting with Astra 28.2°E for the UK and Ireland, in 1998. That became the home of Sky Digital, and the last Sky analogue channels left Astra 19.2°E in 2001.\n\nMost Scandinavian broadcasters have migrated from Astra 19.2°E to 1°West and Astra 5°E, and SES has also opened orbital positions of Astra 23.5°E and Astra 31.5°E to cope with the ever-increasing demands for digital capacity and the expanding markets of Eastern Europe, Africa and Asia that are now served by Astra satellites.\n\n\n"}
{"id": "322643", "url": "https://en.wikipedia.org/wiki?curid=322643", "title": "Australian Alps montane grasslands", "text": "Australian Alps montane grasslands\n\nThe Australian Alps montane grasslands is a montane grassland ecoregion of south-eastern Australia, restricted to the montane regions above 1300 metres (the upper altitudinal limit of \"Eucalyptus pauciflora\").\nThe Australian Alps occupy less than 0.3% of the Australian landmass and run for 600 km from the Brindabella Ranges near Canberra along the borders of the Australian Capital Territory, New South Wales and Victoria almost as far as Melbourne. The highest point is Mount Kosciuszko (2,228 m) in the Snowy Mountains. On the Australian mainland these are the south-eastern section of the country's Great Dividing Range while there are also significant elements of montane grassland in Tasmania. These mountain areas are notable in Australia, which is mostly flat and dry, and the Alps receive almost a quarter of the country's rainfall and are the water source for almost half of the population. Winters are dark, cold and windy with snow on the ground, with Mount Kosciuszko for example only having 10 frost-free days per year.\n\nOn the mainland, the Australian Alps montane grasslands are surrounded at lower elevations by the Southeast Australia temperate forests ecoregion.\n\nThe montane grasslands are a mixed habitat of grassland, heath and bog that is home to a rich collection of Alpine and other plants adapted to the cold climate, snow and harsh dry winters. The ecoregion can be sub-categorised in to montane (between 1,100 m and 1,400 m), subalpine (between 1,400 m and 1,850 m), and alpine (normally above 1,850 m) bands. At lower elevations a number of different types of eucalyptus tree including mountain ash \"'(Eucalyptus regnans)\" grow on the rich soils of the mountain valleys while the trees of the subalpine elevations are snow gum\" (Eucalyptus pauciflora)\"and black sallee\" (Eucalyptus stellulatea)\" with a ground cover of heath shrubs. The tree line is between 1600 and 1800 m and above that the alpine flora consists predominantly of species of\" Poa \"(snow grass), usually associated with closed and open shrublands of orites, \"Grevillea, Prostanthera\", and \"Hovea\". At the highest alpine elevations, these mosaics may give way to a feldmark or, in zones where snow lies into the summer months, to a snow patch community. Sphagnum bog communities of \"Sphagnum cristatum\" and \"Empodisma minus\" (spreading rope-rush) occur in stream beds or other low-lying areas.\n\nThe occurrence of grasslands represents an ecological climax condition, the culmination of a cycle of colonisation of bare ground by woody shrubs which provide protection for seedlings of grass species. The shrubs senesce after 40 to 50 years, leaving a closed canopy.\n\nAlthough this is a harsh environment there is much endemic wildlife in the Alps including the chameleon-like Alpine thermocolor grasshopper, mountain pygmy possum (\"Burramys parvus\") and the corroboree frog (\"Pseudophryne corroboree\"). One particularly restricted range species is the Baw Baw frog (\"Philoria frosti\") which only lives on the Baw Baw Plateau in Victoria. The larger mammals of the lower elevations include red-necked wallaby \"(Macropus rufogriseus)\", swamp wallaby \"(Wallabia bicolor)\", common wombat \"(Vombatus ursinus)\", tiger quoll \"(Dasyurus maculatus)\", short-beaked echidna \"(Tachyglossus aculeatus)\" and platypus \"(Ornithorhynchus anatinus)\".\n\nThis is an ecoregion generally thought to have experienced long periods of minimal disturbance prior to European settlement. Since the nineteenth century, grazing and an increased incidence of fire resulted in a reduction in range condition, and extensive damage to bog communities, from which plant communities have been slow to recover. However the Alps have long been protected as national parkland in order to preserve the water sources and most of the region is now contained in large contiguous National Parks. These include Brindabella and Namadgi National Park and Bimberi and Scabby Range Nature Reserves near Canberra, Kosciuszko in New South Wales, and Alpine and Snowy River National Parks and the Avon Wilderness Park in Victoria. This parkland which does suffer litter, trampling of wildlife and other damage associated with tourism including the clearance of parkland to create ski resorts.\n"}
{"id": "1062350", "url": "https://en.wikipedia.org/wiki?curid=1062350", "title": "Bhagirathi River", "text": "Bhagirathi River\n\nThe Bhāgīrathī (Pron:/ˌbʌgɪˈɹɑːθɪ/) is a turbulent Himalayan river in the Indian state of Uttarakhand, and one of the two headstreams (or source) of the Ganges, the major river of Northern India and considered holy in Hinduism. In Hindu faith and culture, the Bhagirathi is considered the source stream of the Ganges. However, in hydrology, the other headstream, Alaknanda, is considered the source stream on account of its great length and discharge.\n\nThe word \"Bhagirathi\" (Sanskrit, literally, \"caused by Bhagiratha\") refers to a mythological Sagar Dynasty prince who, to gain the release of his 60,000 great-uncles from the curse of saint Kapila, brought the goddess \"Ganga\" in the form of the river Ganges, from the heavens to the earth. Hence, Ganges considered as the daughter of Bhagiratha and Ganges also called Baghirathi. Bhagiratha was the king of Kosala, a kingdom in ancient India. He was a descendant of the great King Sagara of the Suryavanshi, or Surya Dynasty. He was one of the forefathers of Lord Rama, of the Ramayana, the epic in which Bhagiratha's tale is primarily recounted.The story of Bhagiratha explained in Balakhanda of Ramayana.Shiva brought Ganga river to Bindu Sarovar on request of Bhagiratha.\n\nThe headwaters of the Bhagirathi are formed at Gaumukh (elevation ), at the foot of the Gangotri glacier and Khatling glaciers in the Garhwal Himalaya. It is then joined by its tributaries; these are, in order from the source: \n\nThe \"Bhilangna\" itself rises at the foot of the Khatling Glacier (elevation ) approximately south of Gaumukh.\n\nThe river flows from its source for before meeting the Alaknanda River at an elevation of in the town of Devprayag. Downstream of this confluence, considered holy by Hindus, the river is known as the Ganga Ji, or Ganges River by westerners. The controversial Tehri dam lies at the confluence of the Bhāgirathi and the Bhilangna, at , near Tehri.Chaukhamba I is the highest point of the Bhagirathi basin.\n\nThere are 18 dams along the Bhāgirathi River, either in operation, under construction or planned. These are, in order from the source:\n\n"}
{"id": "1065970", "url": "https://en.wikipedia.org/wiki?curid=1065970", "title": "Brain-to-body mass ratio", "text": "Brain-to-body mass ratio\n\nBrain-to-body mass ratio, also known as the brain-to-body weight ratio, is the ratio of brain mass to body mass, which is hypothesized to be a rough estimate of the intelligence of an animal, although fairly inaccurate in many cases. A more complex measurement, encephalization quotient, takes into account allometric effects of widely divergent body sizes across several taxa. The raw brain-to-body mass ratio is however simpler to come by, and is still a useful tool for comparing encephalization within species or between fairly closely related species.\n\nBrain size usually increases with body size in animals (i.e. large animals usually have larger brains than smaller animals); the relationship is not, however, linear. Small mammals such as mice may have a brain/body ratio similar to humans, while elephants have a comparatively lower brain/body ratio.\n\nIn animals, it is thought that the larger the brain, the more brain weight will be available for more complex cognitive tasks. However, large animals need more neurons to represent their own bodies and control specific muscles; thus, relative rather than absolute brain size makes for a ranking of animals that better coincides with the observed complexity of animal behaviour. The relationship between brain-to-body mass ratio and complexity of behaviour is not perfect as other factors also influence intelligence, like the evolution of the recent cerebral cortex and different degrees of brain folding, which increase the surface of the cortex, which is positively correlated in humans to intelligence. The noted exception to this, of course, is swelling of the brain which, while resulting in greater surface area, does not alter the intelligence of those suffering from it.\n\nThe relationship between brain weight and body weight of all living vertebrates follows two completely separate linear functions for cold-blooded and warm-blooded animals. Cold-blooded vertebrates have much smaller brains than warm-blooded vertebrates of the same size. However, if brain metabolism is taken into account, the brain-to-body relationship of both warm and cold-blooded vertebrates becomes similar, with most using between 2 and 8 percent of their basal metabolism for the brain and spinal cord.\n\nDolphins have the highest brain-to-body weight ratio of all cetaceans. Monitor lizards, tegus and anoles and some tortoise species have the largest among reptiles. Among birds, the highest brain-to-body ratios are found among parrots, crows, magpies, jays and ravens. Among amphibians, the studies are still limited. Either octopuses or jumping spiders have some of the highest for an invertebrate, although some ant species have 14%-15% of their mass in their brains, the highest value known for any animal. Sharks have one of the highest for fish alongside manta rays (although the electrogenic elephantfish has a ratio nearly 80 times higher - about 1/32, which is slightly higher than that for humans). Treeshrews have a higher brain to body mass ratio than any other mammal, including humans. Shrews hold about 10% of their body mass in their brain.\n\nIt is a trend that the larger the animal gets, the smaller the brain-to-body mass ratio is. Large whales have very small brains compared to their weight, and small rodents like mice have a relatively large brain, giving a brain-to-body mass ratio similar to humans. One explanation could be that as an animal's brain gets larger, the size of the neural cells remains the same, and more nerve cells will cause the brain to increase in size to a lesser degree than the rest of the body. This phenomenon can be described by an equation of the form E = CS, where \"E\" and \"S\" are brain and body weights, \"r\" a constant that depends on animal family (but close to 2/3 in many vertebrates), and \"C\" is the cephalization factor. It has been argued that the animal's ecological niche, rather than its evolutionary family, is the main determinant of its encephalization factor \"C\".\n\nIn the essay \"Bligh's Bounty\", Stephen Jay Gould noted that if one looks at vertebrates with very low encephalization quotient, their brains are slightly less massive than their spinal cords. Theoretically, intelligence might correlate with the absolute amount of brain an animal has after subtracting the weight of the spinal cord from the brain. This formula is useless for invertebrates because they do not have spinal cords, or in some cases, central nervous systems.\n\nRecent research indicates that, in non-human primates, whole brain size is a better measure of cognitive abilities than brain-to-body mass ratio. The total weight of the species is greater than the predicted sample only if the frontal lobe is adjusted for spatial relation. The brain-to-body mass ratio was however found to be an excellent predictor of variation in problem solving abilities among carnivoran mammals.\n\nIn humans, the brain to body weight ratio can vary greatly from person to person; it would be much higher in an underweight person than an overweight person, and higher in infants than adults. The same problem is encountered when dealing with marine mammals, which may have considerable body fat masses. Some researchers therefore prefer lean body weight to brain mass as a better predictor.\n\n\n"}
{"id": "1621913", "url": "https://en.wikipedia.org/wiki?curid=1621913", "title": "Bulk Richardson number", "text": "Bulk Richardson number\n\nThe Bulk Richardson Number (BRN) is an approximation of the Gradient Richardson number. The BRN is a dimensionless ratio in meteorology related to the consumption of turbulence divided by the shear production (the generation of turbulence kinetic energy caused by wind shear) of turbulence. It is used to show dynamic stability and the formation of turbulence.\n\nThe BRN is used frequently in meteorology due to widely available rawinsonde (frequently called radiosonde) data and numerical weather forecasts that supply wind and temperature measurements at discrete points in space.\n\nBelow is the formula for the BRN. Where g is gravitational acceleration, \"T\" is absolute virtual temperature, Δθ is the virtual potential temperature difference across a layer of thickness Δ\"z\" (vertical depth), and Δ\"U\" and Δ\"V\" are the changes in horizontal wind components across that same layer.\n\nHigh values indicate unstable and/or weakly-sheared environments; low values indicate weak instability and/or strong vertical shear. Generally, values in the range of around 10 to 50 suggest environmental conditions favorable for supercell development.\n\nIn the limit of layer thickness becoming small, the Bulk Richardson number approaches the Gradient Richardson number, for which a critical Richardson number is roughly Ri= 0.25. Numbers less than this critical value are dynamically unstable and likely to become or remain turbulent.\n\nThe critical value of 0.25 applies only for local gradients, not for finite differences across thick layers. The thicker the layer is the more likely we are to average out large gradients that occur within small sub-regions of the layer of interest. This results in uncertainty of our prediction of the occurrence of turbulence, and now one must use an artificially large value of the critical Richardson number to give reasonable results using our smoothed gradients. This means that the thinner the layer, the closer the value to the theory.\n\n\n"}
{"id": "7797716", "url": "https://en.wikipedia.org/wiki?curid=7797716", "title": "Cafetite", "text": "Cafetite\n\nCafetite is a rare titanium oxide mineral with formula (Ca,Mg)(Fe,Al)TiO·4(HO). It is named for its composition, Ca-Fe-Ti.\n\nIt was first described in 1959 for an occurrence in the Afrikanda Massif, Afrikanda, Kola Peninsula, Murmanskaja Oblast', Northern Region, Russia. It is also reported from the Khibiny and Kovdor massifs of the Kola Peninsula and from Meagher County, Montana, US.\n\nIt occurs in pegmatites in a pyroxenite intrusion as crystals in miarolitic cavities. It occurs associated with ilmenite, titaniferous magnetite, titanite, anatase, perovskite, baddeleyite, phlogopite, clinochlore and kassite.\n"}
{"id": "16685442", "url": "https://en.wikipedia.org/wiki?curid=16685442", "title": "Cauca Valley dry forests", "text": "Cauca Valley dry forests\n\nThe Cauca Valley dry forests is a tropical dry broadleaf forest ecoregion in Colombia.\n\nThe Cauca Valley dry forests occupies an area of , extending in a long, narrow strip along the Cauca River. The Cauca Valley is nestled between the Cordillera Occidental and Cordillera Central in the northern Andes. These ranges create a rain shadow, which makes the Cauca Valley drier than the surrounding forests. The Cauca Valley dry forests lie below 1000 meters elevation; the higher slopes are occupied by the distinct Cauca Valley montane forests. Most of these forests have been cleared for agriculture over the years making it one of the most critically endangered ecoregions in Colombia. Laguna de Sonso Nature Reserve has a small area of protected forest.\n\nThe ecoregion is part of the Tumbesian-Andean Valleys Dry Forests global ecoregion, which holds six terrestrial ecoregions: Tumbes-Piura dry forests, Ecuadorian dry forests, Patía Valley dry forests, Magdalena Valley dry forests, Cauca Valley dry forests and Marañón dry forests. \nThe fauna and flora of the global ecoregion have high levels of endemism.\n\nThe main plant communities are open woodland, deciduous dry forest, evergreen dry forest, riparian forest, arid scrub and wetlands. The natural vegetation has been almost completely displaced by human activities, chiefly agriculture.\n\nThree endemic or near-endemic birds found in the ecoregion are the white-chested swift \"(Cypseloides lemosi)\", grayish piculet \"(Picumnus granadensis)\", apical flycatcher \"(Myiarchus apicalis)\".\n\nThe Cauca Valley dry forests have been mostly converted to agricultural fields. The city of Cali lies in the ecoregion.\n\n"}
{"id": "10484531", "url": "https://en.wikipedia.org/wiki?curid=10484531", "title": "Channahon State Park", "text": "Channahon State Park\n\nChannahon State Park is an Illinois state park in Will County, Illinois, United States. The park was named after a Native American word meaning \"the meeting of the waters\". It lies adjacent to the confluence of the Dupage, Des Plaines, and Kankakee Rivers.\n\nThe park is near the municipality of Channahon, Illinois. It is served by U.S. Highway 6.\n\n\n\n"}
{"id": "2104998", "url": "https://en.wikipedia.org/wiki?curid=2104998", "title": "DXing", "text": "DXing\n\nDXing is the hobby of receiving and identifying distant radio or television signals, or making two way radio contact with distant stations in amateur radio, citizens' band radio or other two way radio communications. Many DXers also attempt to obtain written verifications of reception or contact, sometimes referred to as \"QSLs\" or \"veries\". The name of the hobby comes from DX, telegraphic shorthand for \"distance\" or \"distant\".\n\nThe practice of DXing arose during the early days of radio broadcasting. Listeners would mail \"reception reports\" to radio broadcasting stations in hopes of getting a written acknowledgement or a QSL card that served to officially verify they had heard a distant station. Collecting these cards became popular with radio listeners in the 1920s and 1930s, and reception reports were often used by early broadcasters to gauge the effectiveness of their transmissions. Although international shortwave broadcasts are on the decline, DXing remains popular among dedicated shortwave listeners. The pursuit of two-way contact between distant amateur radio operators is also a significant activity within the amateur radio hobby.\n\nEarly radio listeners, often using home made crystal sets and long wire antennas, found radio stations few and far between. With the broadcast bands uncrowded, signals of the most powerful stations could be heard over hundreds of miles, but weaker signals required more precise tuning or better receiving gear.\n\nBy the 1950s, and continuing through the mid-1970s, many of the most powerful North American \"clear channel\" stations such as KDKA, WLW, CKLW, CHUM, WABC, WJR, WLS, WKBW, KFI, KAAY, KSL and a host of border blasters from Mexico pumped out Top 40 music played by popular disc jockeys. As most smaller, local AM radio stations had to sign off at night, the big 50 kW stations had loyal listeners hundreds of miles away.\n\nThe popularity of DXing the medium-wave band has diminished as the popular music formats quickly migrated to the clearer, though less propagating, FM radio beginning in the 1970s. Meanwhile, the MW band in the United States was getting more and more crowded with new stations and existing stations receiving FCC authorization to operate at night. In Canada, just the opposite occurred as AM stations began moving to FM beginning in the 1980s and continuing through today.\n\nOutside of the Americas and Australia, most AM radio broadcasting was in the form of synchronous networks of government-operated stations, operating with hundreds, even thousands of kilowatts of power. Still, the lower powered stations and occasional trans-oceanic signal were popular DX targets.\n\nEspecially during wartime and times of conflict, reception of international broadcasters, whose signals propagate around the world on the shortwave bands has been popular with both casual listeners and DXing hobbyists.\n\nWith the rise in popularity of streaming audio over the internet, many international broadcasters (including the BBC and Voice of America) have cut back on their shortwave broadcasts. Missionary Religious broadcasters still make extensive use of shortwave radio to reach less developed countries around the world.\n\nIn addition to international broadcasters, the shortwave bands also are home to military communications, RTTY, amateur radio, pirate radio, and the mysterious broadcasts of numbers stations. Many of these signals are transmitted in single side band mode, which requires the use of specialized receivers more suitable to DXing than to casual listening.\n\nThough sporadic in nature, signals on the FM broadcast and VHF television bands - especially those stations at the lower end of these bands - can \"skip\" for hundreds, even thousands of miles. North American FM stations have been received in Western Europe, and European TV signals have been received on the West Coast of the U.S.\n\nPolice, fire, and military communications on the VHF bands are also DX'ed to some extent on multi-band radio scanners, though they are mainly listened to strictly on a local basis. One difficulty is in identifying the exact origins of communications of this nature, as opposed to commercial broadcasters which must identify themselves at the top of each hour, and can often be identified through mentions of sponsors, slogans, etc. throughout their programming.\n\nAmateur radio operators who specialize in making two way radio contact with other amateurs in distant countries are also referred to as \"DXers\". On the HF (also known as shortwave) amateur bands, DX stations are those in foreign countries. On the VHF/UHF amateur bands, DX stations can be within the same country or continent, since making a long-distance VHF contact, without the help of a satellite, can be very difficult. DXers collect QSL cards as proof of contact and can earn special certificates and awards from amateur radio organizations. \n\nIn addition, many clubs offer awards for communicating with a certain number of DX stations. For example, the ARRL offers the DX Century Club award, or DXCC. The basic certificate is awarded for working and confirming at least 100 entities on the ARRL DXCC List. For award purposes, other areas than just political countries can be classified as \"DX countries\". For example, the French territory of Reunion Island in the Indian Ocean is counted as a DX country, even though it is a region of France. The rules for determining what is a DX country can be quite complex and to avoid potential confusion, radio amateurs often use the term \"entity\" instead of country. In addition to entities, some awards are based on island groups in the world's oceans. On the VHF/UHF bands, many radio amateurs pursue awards based on Maidenhead grid locators. \n\nIn order to give other amateurs a chance to confirm contacts at new or exotic locations, amateurs have mounted DXpeditions to countries or regions that have no permanent base of amateur radio operators. There are also frequent contests where radio amateurs operate their stations on certain dates for a fixed period of time to try to communicate with as many DX stations as possible.\n\nMany radio enthusiasts are members of DX clubs. There are many DX clubs in many countries around the world. They are useful places to find information about up-to-date news relating to international radio. Many people also enjoy social events, which can form a large part of the enjoyment that people can get out of the radio hobby.\n\nOne of the interesting sides of DXing as a hobby is collecting QSL cards (acknowledgement cards from the broadcaster) confirming the listener's reception report (sometimes called SINPO report, see next section).\n\nUsually a QSL card will have a picture on one side and the reception data on the other. Most of the broadcasters will use pictures and messages indicating their country's culture or technological life.\n\nSINPO stands for the following qualities, graded on a scale of 1 to 5, where '1' means the quality was very bad and '5' very good.\n\nS - Signal strength\nI - Interference with other stations or broadcasters\nN - Noise ratio in the received signal\nP - Propagation (ups and downs of the reception)\nO - Overall merit\n\nAlthough this is a subjective measure, with practise the grading becomes more consistent, and a particular broadcast may be assessed by several listeners from the same area, in which case the broadcaster could assess correspondence between reports.\n\nAfter listening to a broadcast, the listener writes a report with SINPO values, typically including his geographical location (called QTH in amateur radio terminology) in longitude and latitude, the types of receiver and antennae used, the frequency the transmission was heard on, a brief description of the programme listened to, their opinion about it, suggestions if any, and so on.\n\nThe listener can send the report to the broadcaster either by post or email, and request verification (QSL) from them.\n\nVariants of this report are:\na) the SIO report which omits the Noise and Propagation,\nb) grading on a scale of 1 to 3 (instead of 1 to 5) and\nc) the SINFO report where the F stands for fading.\n\nDX communication is communication over large or relatively uncommon distances. On the UHF or VHF bands which are typically used for short range or line of sight communications, DX may represent communication with stations 50 or 100 miles away. The UHF and microwave bands have also been used to accomplish Earth–Moon–Earth communication between stations worldwide. On the low frequency bands (30 to 300 kHz), contacts between stations separated by more than 100 miles are often considered DX.\n\nAmong amateur radio operators and shortwave listeners, most traditional DX communication occurs on the HF bands, where the ionosphere is used to refract the transmitted radio beam. The beam returns to the Earth's surface, and may then be reflected back into the ionosphere for a second bounce. Ionospheric refraction is generally only feasible for frequencies below about 50 MHz, and is highly dependent upon atmospheric conditions, the time of day, and the eleven-year sunspot cycle. It is also affected by solar storms and some other solar events, which can alter the Earth's ionosphere by ejecting a shower of charged particles.\n\nThe angle of refraction places a minimum on the distance at which the refracted beam will first return to Earth. This distance increases with frequency. As a result, any station employing DX will be surrounded by an annular \"dead zone\" where they can't hear other stations or be heard by them.\n\nThis is the phenomenon that allows short wave radio reception to occur beyond the limits of line of sight. It is utilized by amateur radio enthusiasts (hams), shortwave broadcast stations (such as BBC and Voice of America) and others, and is what allows one to hear AM (MW) stations from areas far from their location. It is one of the backups to failure of long distance communication by satellites, when their operation is affected by electromagnetic storms from the sun.\n\nFor example, in clear ionosphere conditions, one can hear France Inter on 711 kHz, far into the UK and as far as Reading, Berkshire. It is also possible to hear Radio Australia from Melbourne as far away as Lansing, Michigan, a distance of some 9835 miles (15,827 kilometers).\n\nRadio equipment used in DXing ranges from inexpensive portable receivers to deluxe equipment costing thousands of dollars. Using just a simple AM radio, one can easily hear signals from the most powerful stations propagating hundreds of miles at night. Even inexpensive shortwave radio receivers can receive signals emanating from several countries during any time of day.\n\nSerious hobbyists use more elaborate receivers designed specifically for pulling in distant signals, and often build their own antennas designed for a specific frequency band. There is much discussion and debate in the hobby about the relative merits of lesser priced shortwave receivers vs. their multi-thousand dollar \"big brother\" radios. In general, a good desktop or \"PC Radio\" will be able to \"hear\" just about what a very expensive high-performance receiver can receive. The difference between the two types comes into play during difficult band or reception conditions. The expensive receiver will have more filtering options and usually better adjacent channel interference blocking, sometimes resulting in the difference of being able to receive or not receive a signal under poor conditions. Reception of international broadcasting seldom shows a noticeable difference between the two radios. Car radios are also used for DXing the broadcast bands.\n\nAnother recent trend is for the hobbyist to employ multiple radios and antennas connected to a personal computer. Through advanced radio control software, the radios can be automatically ganged together, so that tuning one radio can tune all the others in the group. This DXing technique is sometimes referred to as diversity reception and facilitates easy \"A to B\" comparison of different antennas and receivers for a given signal. For more details on \"PC Radios\" or computer controlled shortwave receivers see the discussion in Shortwave listening.\n\nHaving a minimum of two Dipole antennas at right angles to each other (for example, one running North-South and one running East-West) can produce dramatically different reception patterns. These simple antennas can be made for a few dollars worth of wire and a couple of insulators.\n\n\n"}
{"id": "52689782", "url": "https://en.wikipedia.org/wiki?curid=52689782", "title": "Desert Camp Conservation Park", "text": "Desert Camp Conservation Park\n\nDesert Camp Conservation Park (formerly Desert Camp National Parks Reserve and Desert Camp National Park) is a protected area in the Australian state of South Australia located in the state’s Limestone Coast region in the gazetted locality of Marcollat about south of the town centre in Keith.\n\nThe conservation park occupies land in sections 87 and 105 of the cadastral unit of the Hundred of Marcollat on the northern side of Rowney Road which is also known as the Kingston - Keith road. The land originally gained protected area status on 27 July 1967, when section 87 was gazetted under the \"National Parks Act 1966\" as the \"Desert Camp National Park\". Its name was amended to \"Desert Camp National Parks Reserve\" on 9 November 1967. Section 105 came in existence about 14 months later after work to Rowney Road resulted in it being “severed … from an adjoining lease.” Section 105 was subsequently added ion 21 November 1968 to the national parks reserve. In 1972, the national parks reserve was reconstituted as the \"Desert Camp Conservation Park\" upon the proclamation of the \"National Parks and Wildlife Act 1972\".\n\nIn 1992, the conservation park was described as follows:(It) is located in the Angle Rock Environmental Association… This association is characterised by interdunal plains with occasional low narrow dune ridges and isolated granite outcrops. Soils are moderately deep, alkaline, sandy, pedal, mottled-yellow duplex soils… These soils support an open woodland of pink gum \"(Eucalyptus fasciculosa)\" over a heath understorey of mallee honey-myrtle \"(Melaleuca brevifolia)\", broombush \"(M. uncinata)\", austral grass tree \"(Xanthorrhoea australis)\", slaty sheoak \"(Allocasuarina muelleriana)\" and desert hakea \"(Hakea muelleriana)\".\n\nAs of 1992, there was “limited visitation” with the main visitor groups being “bird observers and field naturalists”. \n\nThe conservation park is classified as an IUCN Category III protected area. In 1980, it was listed on the now-defunct Register of the National Estate.\n\n\n"}
{"id": "8724", "url": "https://en.wikipedia.org/wiki?curid=8724", "title": "Doppler effect", "text": "Doppler effect\n\nThe Doppler effect (or the Doppler shift) is the change in frequency or wavelength of a wave in relation to an observer who is moving relative to the wave source. It is named after the Austrian physicist Christian Doppler, who described the phenomenon in 1842.\n\nA common example of Doppler shift is the change of pitch heard when a vehicle sounding a horn approaches and recedes from an observer. Compared to the emitted frequency, the received frequency is higher during the approach, identical at the instant of passing by, and lower during the recession.\n\nThe reason for the Doppler effect is that when the source of the waves is moving towards the observer, each successive wave crest is emitted from a position closer to the observer than the previous wave. Therefore, each wave takes slightly less time to reach the observer than the previous wave. Hence, the time between the arrival of successive wave crests at the observer is reduced, causing an increase in the frequency. While they are traveling, the distance between successive wave fronts is reduced, so the waves \"bunch together\". Conversely, if the source of waves is moving away from the observer, each wave is emitted from a position farther from the observer than the previous wave, so the arrival time between successive waves is increased, reducing the frequency. The distance between successive wave fronts is then increased, so the waves \"spread out\".\n\nFor waves that propagate in a medium, such as sound waves, the velocity of the observer and of the source are relative to the medium in which the waves are transmitted. The total Doppler effect may therefore result from motion of the source, motion of the observer, or motion of the medium. Each of these effects is analyzed separately. For waves which do not require a medium, such as light or gravity in general relativity, only the relative difference in velocity between the observer and the source needs to be considered.\n\nDoppler first proposed this effect in 1842 in his treatise \"Über das farbige Licht der Doppelsterne und einiger anderer Gestirne des Himmels\" (On the coloured light of the binary stars and some other stars of the heavens). The hypothesis was tested for sound waves by Buys Ballot in 1845. He confirmed that the sound's pitch was higher than the emitted frequency when the sound source approached him, and lower than the emitted frequency when the sound source receded from him. Hippolyte Fizeau discovered independently the same phenomenon on electromagnetic waves in 1848 (in France, the effect is sometimes called \"effet Doppler-Fizeau\" but that name was not adopted by the rest of the world as Fizeau's discovery was six years after Doppler's proposal). In Britain, John Scott Russell made an experimental study of the Doppler effect (1848).\n\nIn classical physics, where the speeds of source and the receiver relative to the medium are lower than the velocity of waves in the medium, the relationship between observed frequency formula_1 and emitted frequency formula_2 is given by:\n\nThe frequency is decreased if either is moving away from the other.\n\nThe above formula assumes that the source is either directly approaching or receding from the observer. If the source approaches the observer at an angle (but still with a constant velocity), the observed frequency that is first heard is higher than the object's emitted frequency. Thereafter, there is a monotonic decrease in the observed frequency as it gets closer to the observer, through equality when it is coming from a direction perpendicular to the relative motion (and was emitted at the point of closest approach; but when the wave is received, the source and observer will no longer be at their closest), and a continued monotonic decrease as it recedes from the observer. When the observer is very close to the path of the object, the transition from high to low frequency is very abrupt. When the observer is far from the path of the object, the transition from high to low frequency is gradual.\n\nIf the speeds formula_6 and formula_5 are small compared to the speed of the wave, the relationship between observed frequency formula_1 and emitted frequency formula_2 is approximately\nGiven formula_13\n\nwe divide for formula_14\n\nformula_15\n\nSince formula_16 we can substitute the geometric expansion:\n\nformula_17\n\nTo understand what happens, consider the following analogy. Someone throws one ball every second at a man. Assume that balls travel with constant velocity. If the thrower is stationary, the man will receive one ball every second. However, if the thrower is moving towards the man, he will receive balls more frequently because the balls will be less spaced out. The inverse is true if the thrower is moving away from the man. So it is actually the \"wavelength\" which is affected; as a consequence, the received frequency is also affected. It may also be said that the velocity of the wave remains constant whereas wavelength changes; hence frequency also changes.\n\nWith an observer stationary relative to the medium, if a moving source is emitting waves with an actual frequency formula_2 (in this case, the wavelength is changed, the transmission velocity of the wave keeps constant formula_19 note that the \"transmission velocity\" of the wave does not depend on the \"velocity of the source\"), then the observer detects waves with a frequency formula_1 given by\n\nA similar analysis for a moving \"observer\" and a stationary source (in this case, the wavelength keeps constant, but due to the motion, the rate at which the observer receives waves formula_19 and hence the \"transmission velocity\" of the wave [with respect to the observer] formula_19 is changed) yields the observed frequency:\n\nThese can be generalized into the equation that was presented in the previous section.\n\nAn interesting effect was predicted by Lord Rayleigh in his classic book on sound: if the source is moving toward the observer at twice the speed of sound, a musical piece emitted by that source would be heard in correct time and tune, but \"backwards\". The Doppler effect with sound is only clearly heard with objects moving at high speed, as change in frequency of musical tone involves a speed of around 40 meters per second, and smaller changes in frequency can easily be confused by changes in the amplitude of the sounds from moving emitters. Neil A Downie has demonstrated how the Doppler effect can be made much more easily audible by using an ultrasonic (e.g. 40 kHz) emitter on the moving object. The observer then uses a heterodyne frequency converter, as used in many bat detectors, to listen to a band around 40 kHz. In this case, with the bat detector tuned to give frequency for the stationary emitter of 2000 Hz, the observer will perceive a frequency shift of a whole tone, 240 Hz, if the emitter travels at 2 meters per second.\n\nThe siren on a passing emergency vehicle will start out higher than its stationary pitch, slide down as it passes, and continue lower than its stationary pitch as it recedes from the observer. Astronomer John Dobson explained the effect thus:\n\nIn other words, if the siren approached the observer directly, the pitch would remain constant, at a higher than stationary pitch, until the vehicle hit him, and then immediately jump to a new lower pitch. Because the vehicle passes by the observer, the radial velocity does not remain constant, but instead varies as a function of the angle between his line of sight and the siren's velocity:\n\nwhere formula_27 is the angle between the object's forward velocity and the line of sight from the object to the observer.\n\nThe Doppler effect for electromagnetic waves such as light is of great use in astronomy and results in either a so-called redshift or blueshift. It has been used to measure the speed at which stars and galaxies are approaching or receding from us; that is, their radial velocities. This may be used to detect if an apparently single star is, in reality, a close binary, to measure the rotational speed of stars and galaxies, or to detect exoplanets. This redshift and blueshift happens on a very small scale, if an object is moving toward earth, there would not be a noticeable difference in visible light \n\nNote that redshift is also used to measure the expansion of space, but that this is not truly a Doppler effect. Rather, redshifting due to the expansion of space is known as cosmological redshift, which can be derived purely from the Robertson-Walker metric under the formalism of General Relativity. Having said this, it also happens that there \"are\" detectable Doppler effects on cosmological scales, which, if incorrectly interpreted as cosmological in origin, lead to the observation of redshift-space distortions.\n\nThe use of the Doppler effect for light in astronomy depends on our knowledge that the spectra of stars are not homogeneous. They exhibit absorption lines at well defined frequencies that are correlated with the energies required to excite electrons in various elements from one level to another. The Doppler effect is recognizable in the fact that the absorption lines are not always at the frequencies that are obtained from the spectrum of a stationary light source. Since blue light has a higher frequency than red light, the spectral lines of an approaching astronomical light source exhibit a blueshift and those of a receding astronomical light source exhibit a redshift.\n\nAmong the nearby stars, the largest radial velocities with respect to the Sun are +308 km/s (BD-15°4041, also known as LHS 52, 81.7 light-years away) and −260 km/s (Woolley 9722, also known as Wolf 1106 and LHS 64, 78.2 light-years away). Positive radial velocity means the star is receding from the Sun, negative that it is approaching.\n\nThe Doppler effect is used in some types of radar, to measure the velocity of detected objects. A radar beam is fired at a moving target — e.g. a motor car, as police use radar to detect speeding motorists — as it approaches or recedes from the radar source. Each successive radar wave has to travel farther to reach the car, before being reflected and re-detected near the source. As each wave has to move farther, the gap between each wave increases, increasing the wavelength. In some situations, the radar beam is fired at the moving car as it approaches, in which case each successive wave travels a lesser distance, decreasing the wavelength. In either situation, calculations from the Doppler effect accurately determine the car's velocity. Moreover, the proximity fuze, developed during World War II, relies upon Doppler radar to detonate explosives at the correct time, height, distance, etc.\n\nBecause the doppler shift affects the wave incident upon the target as well as the wave reflected back to the radar, the change in frequency observed by a radar due to a target moving at relative velocity formula_28 is twice that from the same target emitting a wave:\n\nAn echocardiogram can, within certain limits, produce an accurate assessment of the direction of blood flow and the velocity of blood and cardiac tissue at any arbitrary point using the Doppler effect. One of the limitations is that the ultrasound beam should be as parallel to the blood flow as possible. Velocity measurements allow assessment of cardiac valve areas and function, abnormal communications between the left and right side of the heart, leaking of blood through the valves (valvular regurgitation), and calculation of the cardiac output. Contrast-enhanced ultrasound using gas-filled microbubble contrast media can be used to improve velocity or other flow-related medical measurements.\n\nAlthough \"Doppler\" has become synonymous with \"velocity measurement\" in medical imaging, in many cases it is not the frequency shift (Doppler shift) of the received signal that is measured, but the phase shift (\"when\" the received signal arrives).\n\nVelocity measurements of blood flow are also used in other fields of medical ultrasonography, such as obstetric ultrasonography and neurology. Velocity measurement of blood flow in arteries and veins based on Doppler effect is an effective tool for diagnosis of vascular problems like stenosis.\n\nInstruments such as the laser Doppler velocimeter (LDV), and acoustic Doppler velocimeter (ADV) have been developed to measure velocities in a fluid flow. The LDV emits a light beam and the ADV emits an ultrasonic acoustic burst, and measure the Doppler shift in wavelengths of reflections from particles moving with the flow. The actual flow is computed as a function of the water velocity and phase. This technique allows non-intrusive flow measurements, at high precision and high frequency.\n\nDeveloped originally for velocity measurements in medical applications (blood flow), Ultrasonic Doppler Velocimetry (UDV) can measure in real time complete velocity profile in almost any liquids containing particles in suspension such as dust, gas bubbles, emulsions. Flows can be pulsating, oscillating, laminar or turbulent, stationary or transient. This technique is fully non-invasive.\n\nFast moving satellites can have a Doppler shift of dozens of kilohertz relative to a ground station. The speed, thus magnitude of Doppler effect, changes due to earth curvature. Dynamic Doppler compensation, where the frequency of a signal is changed progressively during transmission, is used so the satellite receives a constant frequency signal.\n\nThe Leslie speaker, most commonly associated with and predominantly used with the famous Hammond organ, takes advantage of the Doppler effect by using an electric motor to rotate an acoustic horn around a loudspeaker, sending its sound in a circle. This results at the listener's ear in rapidly fluctuating frequencies of a keyboard note.\n\nA laser Doppler vibrometer (LDV) is a non-contact instrument for measuring vibration. The laser beam from the LDV is directed at the surface of interest, and the vibration amplitude and frequency are extracted from the Doppler shift of the laser beam frequency due to the motion of the surface.\n\nDuring the segmentation of vertebrate embryos, waves of gene expression sweep across the presomitic mesoderm, the tissue from which the precursors of the vertebrae (somites) are formed. A new somite is formed upon arrival of a wave at the anterior end of the presomitic mesoderm. In zebrafish, it has been shown that the shortening of the presomitic mesoderm during segmentation leads to a Doppler effect as the anterior end of the tissue moves into the waves. This Doppler effect contributes to the period of segmentation.\n\nSince 1968 scientists such as Victor Veselago have speculated about the possibility of an inverse Doppler effect. The size of the doppler shift depends on the refractive index of the medium a wave is traveling through. But some materials are capable of negative refraction, which should lead to a Doppler shift that works in a direction opposite that of a conventional Doppler shift. The experiment that claimed to have detected this effect was conducted by Nigel Seddon and Trevor Bearpark in Bristol, United Kingdom in 2003.\n\n\n"}
{"id": "14184871", "url": "https://en.wikipedia.org/wiki?curid=14184871", "title": "Eastern Himalayan alpine shrub and meadows", "text": "Eastern Himalayan alpine shrub and meadows\n\nThe Eastern Himalayan alpine shrub and meadows is a montane grasslands and shrublands ecoregion of Bhutan, China, India, Myanmar, and Nepal, which lies between the tree line and snow line in the eastern portion of the Himalaya Range.\n\nThe Eastern Himalayan alpine shrub and meadows covers an area of , extending along the north and south faces of the Himalaya Range from the Kali Gandaki Gorge in central Nepal eastwards through Tibet and India's Sikkim state, Bhutan, India's Arunachal Pradesh state, and northernmost Myanmar.\n\nThe alpine shrub and meadows lie between approximately elevation. Permanent ice and snow lies above . The Eastern Himalayan subalpine conifer forests lie below along the southern slopes of the range, from Central Nepal to Bhutan. The Northeastern Himalayan subalpine conifer forests lie south of the range in Arunachal Pradesh, extending north of the range into the lower valley of the Brahmaputra River and its tributaries. The Northern Triangle temperate forests lie to the south of the alpine shrub and meadows in northern Myanmar, and the Nujiang Langcang Gorge alpine conifer and mixed forests lie to the east in the gorges of the upper Irrawaddy and Salween rivers.\n\nThe Yarlung Tsangpo arid steppe lies in the upper Brahmaputra Valley of Tibet, north of the Eastern Himalayan alpine shrub and meadows.\n\nAlpine shrublands, characterized by rhododendrons, predominate at lower elevations, close to the treeline. The rhododendron flora of the ecoregion is quite varied, with species composition changing as one moves from west to east along the range.\n\nAbove the shrublands are alpine meadows which support a variety of herbaceous plants, including species of \"Alchemilla, Androsace, Anemone, Diapensia, Draba, Gentiana, Impatiens, Leontopodium, Meconopsis, Pedicularis, Potentilla, Primula, Rhododendron, Saussurea, Saxifraga, Sedum\", and \"Viola\". In the spring and summer, the alpine meadows are covered with brightly colored flowers.\n\nOn the upper slopes, low grasses and cushion plants grow among the boulders and scree.\n\nLarge mammals include the snow leopard (\"Uncia uncia\"), bharal or Himalayan blue sheep (\"Pseudois nayaur\"), Himalayan tahr (\"Hemitragus jemlahicus\"), takin (\"Budorcas taxicolor\"), Himalayan musk deer (\"Moschus chrysogaster\"), Himalayan goral (\"Nemorhaedus baileyi\"), and Himalayan serow (\"Capricornis thar\"). Smaller mammals include Himalayan marmots (\"Marmota himalayana\"), weasels, and pikas.\n\nSeveral protected areas lie within or partly within the ecoregion, including:\n\n\n\n"}
{"id": "53528300", "url": "https://en.wikipedia.org/wiki?curid=53528300", "title": "Environmental aspects of the electric car", "text": "Environmental aspects of the electric car\n\nElectric cars can have several environmental benefits over conventional internal combustion engine automobiles, such as \nFurthermore, the carbon dioxide emitted for the manufacturing should be taken into account.\n\nElectric cars also have disadvantages, such as:\n\nElectric cars usually also show significantly reduced greenhouse gas emissions, depending on the method used for electricity generation to charge the batteries. For example, some battery electric vehicles do not produce CO emissions at all, but only if their energy is obtained from sources such as solar, wind, nuclear, or hydropower.\n\nEven when the power is generated using fossil fuels, electric vehicles usually, compared to gasoline vehicles, show significant reductions in overall well-wheel global carbon emissions due to the highly carbon-intensive production in mining, pumping, refining, transportation and the efficiencies obtained with gasoline. Researchers in Germany have claimed that while there is some technical superiority of electric propulsion compared with conventional technology that in many countries the effect of electrification of vehicles' fleet emissions will predominantly be due to regulation rather than technology. Indeed, electricity production is submitted to emission quotas, while vehicles' fuel propulsion is not, thus electrification shifts demand from a non-capped sector to a capped sector. This means that the emissions of electrical grids can be expected to improve over time as more wind and solar generation is deployed.\n\nMany countries are introducing average emissions targets across all cars sold by a manufacturer, with financial penalties on manufacturers that fail to meet these targets. This has created an incentive for manufacturers, especially those selling many heavy or high-performance cars, to introduce electric cars as a means of reducing average fleet emissions.\n\nElectric cars have several benefits over conventional internal combustion engine automobiles, including a significant reduction of local air pollution, especially in cities, as they do not emit harmful tailpipe pollutants such as particulates (soot), volatile organic compounds, hydrocarbons, carbon monoxide, ozone, lead, and various oxides of nitrogen. The clean air benefit may only be local because, depending on the source of the electricity used to recharge the batteries, air pollutant emissions may be shifted to the location of the generation plants. This is referred to as the long tailpipe of electric vehicles. The amount of carbon dioxide emitted depends on the emission intensity of the power sources used to charge the vehicle, the efficiency of the said vehicle and the energy wasted in the charging process. For mains electricity the emission intensity varies significantly per country and within a particular country, and on the demand, the availability of renewable sources and the efficiency of the fossil fuel-based generation used at a given time.\n\nCharging a vehicle using renewable energy (e.g., wind power or solar panels) yields very low carbon footprint-only that to produce and install the generation system (see Energy Returned On Energy Invested.) Even on a fossil-fueled grid, it's quite feasible for a household with a solar panel to produce enough energy to account for their electric car usage, thus (on average) cancelling out the emissions of charging the vehicle, whether or not the panel directly charges it. Even when using exclusively grid electricity, introducing EVs comes with a major environmental benefits in most (EU) countries, except those relying on old coal fired power plants. So for example the part of electricity, which is produced with renewable energy is (2014) in Norway 99 percent and in Germany 30 percent.\n\nThe following table compares tailpipe and upstream emissions estimated by the U.S. Environmental Protection Agency for all series production model year 2014 all-electric passenger vehicles available in the U.S. market. Since all-electric cars do not produce tailpipe emissions, for comparison purposes the two most fuel efficient plug-in hybrids and the typical gasoline-powered car are included in the table. Total emissions include the emissions associated with the production and distribution of electricity used to charge the vehicle, and for plug-in hybrid electric vehicles, it also includes emissions associated with tailpipe emissions produced from the internal combustion engine. These figures were published by the EPA in October in its 2014 report \"\"Light-Duty Automotive Technology, Carbon Dioxide Emissions, and Fuel Economy Trends\".\"\n\nTo account for the upstream emissions associated with the production and distribution of electricity, and since electricity production in the United States varies significantly from region to region, the EPA considered three scenarios/ranges with the low end scenario corresponding to the California powerplant emissions factor, the middle of the range represented by the national average powerplant emissions factor, and the upper end of the range corresponding to the powerplant emissions factor for the Rocky Mountains. The EPA estimates that the electricity GHG emission factors for various regions of the country vary from 346 g /kWh in California to 986 g /kWh in the Rockies, with a national average of 648 g /kWh. In the case of plug-in hybrids, and since their all-electric range depends on the size of the battery pack, the analysis introduced a utility factor as a projection of the share of miles that will be driven using electricity by an average driver.\n\nThe Union of Concerned Scientists (UCS) published in 2012, a report with an assessment of average greenhouse gas emissions resulting from charging plug-in car batteries considering the full life-cycle (well-to-wheel analysis) and the fuel used to generate electric power by region in the U.S. The study used the Nissan Leaf all-electric car to establish the analysis's baseline. The UCS study expressed the results in terms of miles per gallon instead of the conventional unit of grams of carbon dioxide emissions per year. The study found that in areas where electricity is generated from natural gas, nuclear, or renewable resources such as hydroelectric, the potential of plug-in electric cars to reduce greenhouse emissions is significant. On the other hand, in regions where a high proportion of power is generated from coal, hybrid electric cars produce less emissions than plug-in electric cars, and the best fuel efficient gasoline-powered subcompact car produces slightly less emissions than a plug-in car. In the worst-case scenario, the study estimated that for a region where all energy is generated from coal, a plug-in electric car would emit greenhouse gas emissions equivalent to a gasoline car rated at a combined city/highway fuel economy of . In contrast, in a region that is completely reliant on natural gas, the plug-in would be equivalent to a gasoline-powered car rated at combined.\n\nThe study found that for 45% of the U.S. population, a plug-in electric car will generate lower emissions than a gasoline-powered car capable of a combined fuel economy of , such as the Toyota Prius. Cities in this group included Portland, Oregon, San Francisco, Los Angeles, New York City, and Salt Lake City, and the cleanest cities achieved well-to-wheel emissions equivalent to a fuel economy of . The study also found that for 37% of the population, the electric car emissions will fall in the range of a gasoline-powered car rated at a combined fuel economy between , such as the Honda Civic Hybrid and the Lexus CT200h. Cities in this group include Phoenix, Arizona, Houston, Miami, Columbus, Ohio and Atlanta, Georgia. An 18% of the population lives in areas where the power supply is more dependent on burning carbon, and emissions will be equivalent to a car rated at a combined fuel economy between , such as the Chevrolet Cruze and Ford Focus. This group includes Denver, Minneapolis, Saint Louis, Missouri, Detroit, and Oklahoma City. The study found that there are no regions in the U.S. where plug-in electric cars will have higher greenhouse gas emissions than the average new compact gasoline engine automobile, and the area with the dirtiest power supply produces emissions equivalent to a gasoline-powered car rated .\n\nIn September 2014, the UCS published an updated analysis of its 2012 report. The 2014 analysis found that 60% of Americans, up from 45% in 2009, live in regions where an all-electric car produce fewer equivalent emissions per mile than the most efficient hybrid. The UCS study found two reasons for the improvement. First, electric utilities have adopted cleaner sources of electricity to their mix between the two analysis. Second, electric vehicles have become more efficient, as the average 2013 all-electric vehicle used , representing a 5% improvement over 2011 models. Also, some new models are cleaner than the average, such as the BMW i3, which is rated at 0.27 kWh by the EPA. In states with a cleaner mix generation, the gains were larger. The average all-electric car in California went up to equivalent from in the 2012 study. States with dirtier generation that rely heavily on coal still lag, such as Colorado, where the average BEV only achieves the same emissions as a gasoline-powered car. The author of the 2014 analysis noted that the benefits are not distributed evenly across the U.S. because electric car adoptions is concentrated in the states with cleaner power.\n\nImproving on the UCS analysis and several others, an analysis by economists affiliated with the National Bureau of Economic Research (NBER), published in November 2014, estimated marginal emissions of electricity demand that vary by location and time of day across the United States. The marginal analysis, applied to plug-in electric vehicles, found that the emissions of charging PEVs vary by region and hours of the day. In some regions, such as the Western U.S. and Texas, emissions per mile from driving PEVs are less than those from driving a hybrid car. However, in other regions, such as the Upper Midwest, charging during the recommended hours of midnight to 4 a.m. implies that PEVs generate more emissions per mile than the average car currently on the road.\n\nThe results show a tension between electricity load management and environmental goals as the hours when electricity is the least expensive to produce tend to be the hours with the greatest emissions. This occurs because coal-fired plants, which have higher emission rates, are most commonly used to meet base-level and off-peak electricity demand; while natural gas plants, which have relatively low emissions rates, are often brought online to meet peak demand.\n\nIn November 2015, the Union of Concerned Scientists published a new report comparing two battery electric vehicles (BEVs) with similar gasoline vehicles by examining their global warming emissions over their full life-cycle, craddle-to-grave analysis. The two BEVs modeled, midsize and full-size, are based on the two most popular BEV models sold in the United States in 2015, the Nissan LEAF and the Tesla Model S. The study found that all-electric cars representative of those sold today, on average produce less than half the global warming emissions of comparable gasoline-powered vehicles, despite higher emissions of manufacture. Considering the regions where the two most popular electric cars are being sold, excess manufacturing emissions are offset within 6 to 16 months of average driving. The study also concluded that driving an average EV results in lower global warming emissions than driving a gasoline car that gets in regions covering two-thirds of the U.S. population, up from 45% in 2009. Based on where EVs are sold in the U.S. in 2015, the average EV produces global warming emissions equal to a gasoline vehicle with a fuel economy rating. The authors identified two main reasons for this reduction since the 2012 study. Electricity generation has become cleaner, as coal-fired generation has declined while lower-carbon alternatives have increased. In addition, electric cars are becoming more efficient. For example, the Nissan Leaf and the Chevrolet Volt, have undergone efficiency improvements to the original 2010 models, and other, more efficient BEV models, such as the most lightweight and efficient BMW i3, have entered the market.\n\nA study made in the UK in 2008, concluded that electric vehicles had the potential to reduce carbon dioxide and greenhouse gas emissions by at least 40%, even taking into account the emissions due to current electricity generation in the UK and emissions relating to the production and disposal of electric vehicles. The savings are questionable relative to hybrid or diesel cars. (According to official British government testing, the most efficient European market cars are well below 115 grams of per kilometer driven, although a study in Scotland gave 149.5 g/km as the average for new cars in the UK.) But because UK consumers can select their energy suppliers, it also depends on how 'green' their chosen supplier is in providing energy into the grid. Unlike other countries, in the UK a stable proportion of the electricity is produced by nuclear, coal and gas plants. Therefore, there are only minor differences in the environmental impact over the year.\n\nIn a worst-case scenario where incremental electricity demand would be met exclusively with coal, a 2009 study conducted by the World Wide Fund for Nature and IZES found that a mid-size EV would emit roughly , compared with an average of for a gasoline-powered compact car. This study concluded that introducing 1 million EV cars to Germany would, in the best-case scenario, only reduce emissions by 0.1%, if nothing is done to upgrade the electricity infrastructure or manage demand. A more reasonable estimate, relaxing the coal assumption, was provided by Massiani and Weinmann taking into account that the source of energy used for electricity generation would be determined based on the temporal pattern of the additional electricity demand (in other words an increase in electricity consumption at peak hour will activate the marginal technology, while an off peak increase would typically activate other technologies). Their conclusion is that natural gas will provide most of the energy used to reload EV, while renewable energy will not represent more than a few percent of the energy used.\n\nVolkswagen conducted a life-cycle assessment of its electric vehicles certified by an independent inspection agency. The study found that emissions during the use phase of its all-electric VW e-Golf are 99% lower than those of the Golf 1.2 TSI when powers comes from exclusively hydroelectricity generated in Germany, Austria and Switzerland. Accounting for the electric car entire life-cycle, the e-Golf reduces emissions by 61%. When the actual EU-27 electricity mix is considered, the e-Golf emissions are still 26% lower than those of the conventional Golf 1.2 TSI. In 2014 in Germany, 28 percent of whole electricity was renewable energy produced in Germany.\n\nIn France and Belgium, which have many nuclear power plants, emissions from electric car use would be about . Because of the stable nuclear production, the timing of charging electric cars has almost no impact on their environmental footprint.\n\nElectric cars also have impacts arising from the manufacturing of the vehicle. Since battery packs are heavy, manufacturers work to lighten the rest of the vehicle. As a result, electric car components contain many lightweight materials that require a lot of energy to produce and process, such as aluminium and carbon-fiber-reinforced polymers. Electric motors and batteries add to the energy of electric-car manufacture. Also, the magnets in the motors of many electric vehicles contain rare-earth metals. In a study released in 2012, a group of MIT researchers calculated that global mining of two rare-earth metals, neodymium and dysprosium, would need to increase 700% and 2600%, respectively, over the next 25 years to keep pace with various green-tech plans. Substitute strategies introduce trade-offs in efficiency and cost. The same MIT study noted that the materials used in batteries are harmful to the environment. Mining and processing of metals such as lithium, copper, and nickel uses energy and can release toxic compounds. In regions with poor legislature, mineral exploitation can increase risks further. The local population may be exposed to toxic substances through air and groundwater contamination.. \n\nA paper published in the Journal of Industrial Ecology named \"Comparative environmental life cycle assessment of conventional and electric vehicles\" begins by stating that it is important to address concerns of problem-shifting. The study highlighted in particular the toxicity of the electric car's manufacturing process compared to conventional petrol/diesel cars. It concludes that the global warming potential of the process used to make electric cars is twice that of conventional cars. The study also finds that electric cars do not make sense if the electricity they consume is produced predominately by coal-fired power plants. However, the study was later corrected by the authors due to overstating the environmental damage of electric vehicles; many of the electric vehicle components had been incorrectly modelled, and the European power grids were cleaner in many respects than the paper had assumed.\n\nSeveral reports have found that hybrid electric vehicles, plug-in hybrids and all-electric cars generate more carbon emissions during their production than current conventional vehicles. Study of electric car production in Malaysia estimated a compact electric car production release 5,791 kg per unit against conventional vehicles 4,166 kg , but still have a lower overall carbon footprint over the full life cycle. The initial higher carbon footprint is due mainly to battery production. As an example, the Ricardo study estimated that 43% of production emissions for a mid-size electric car are from the battery production.\n\nIn February 2014, the Automotive Science Group (ASG) published the result of a study conducted to assess the life-cycle of over 1,300 automobiles across nine categories sold in North America. The study found that among advanced automotive technologies, the Nissan Leaf holds the smallest life-cycle environmental footprint of any model year 2014 automobile available in the North American market with at least four-person occupancy. The study concluded that the increased environmental impacts of manufacturing the battery electric technology is more than offset with increased environmental performance during operational life. For the assessment, the study used the average electricity mix of the U.S. grid in 2014.\n\nIn 2017, a report made by IVL Swedish Environmental Research Institute also calculated that the CO2 emissions of lithium-ion batteries (present in many electric cars today) are in the order of 150-200 kilos of carbon dioxide equivalents per kilowatt-hour battery. Half of the CO2 emissions (50%) comes from cell manufacturing, whereas mining and refining contributes only a small part of the CO2 emissions. In practice, emissions in the order of 150-200 kilos of carbon dioxide equivalents per kilowatt-hour means that an electric car with a 100kWh battery will thus have emitted 15-20 tons of carbon dioxide even before the vehicle ignition is turned on. One of the authors, Mats-Ola Larsson, reportedly said that this is the same amount of emissions as driving a gasoline car for 8,2 years. However, Popular Mechanics calculates that even if the 15-20 tons estimate is correct, it would only take 2.4 years of driving for the electric car with a 100kWh battery to recover the greenhouse emissions from the battery manufacturing. It however does not calculate in the emissions from battery replacements (battery replacement of regular lithium-ion batteries needs to be done every 2–3 years; lithium-ion batteries from cars typically last longer though). Furthermore, two other studies suggest a 100kWh battery would generate about 6-6.4 tons of CO2 emissions, so significantly less than what the IVL study claims.\n\n\n\nClimate Summit 2014, September 2014\n"}
{"id": "2502378", "url": "https://en.wikipedia.org/wiki?curid=2502378", "title": "Epsilon radiation", "text": "Epsilon radiation\n\nEpsilon radiation is tertiary radiation caused by secondary radiation (\"e.g.\", delta radiation). Epsilon rays are a form of particle radiation and are composed of electrons. The term was coined by J. J. Thomson, but is very rarely used today.\n\n"}
{"id": "18859701", "url": "https://en.wikipedia.org/wiki?curid=18859701", "title": "FIVB Beach Volleyball World Rankings", "text": "FIVB Beach Volleyball World Rankings\n\nThe FIVB Beach Volleyball World Rankings is a ranking system for men's and women's national teams in beach volleyball. The teams of the member nations of FIVB, beach volleyball and volleyball's world governing body, are ranked based on their competitions results with the most successful teams being ranked highest.\n\nThe rankings are used in international competitions to define the seeded teams and arrange them in pools. Specific procedures for seeding and pooling are established by the FIVB in each competition's formula.\n\n"}
{"id": "337386", "url": "https://en.wikipedia.org/wiki?curid=337386", "title": "Foreign language influences in English", "text": "Foreign language influences in English\n\nThe core of English language descends from Old English, the language brought with the Angles, Saxon, and Jutish settlers to what was to be called England in and after the 500s. The bulk of the language in spoken and written texts is from this source. As a statistical rule, around 70 percent of words in any text are Anglo-Saxon. Moreover, the grammar is largely Anglo-Saxon.\n\nA significant portion of the English vocabulary comes from Romance and Latinate sources. Estimates of native words (derived from Old English) range from 20%–33%, with the rest made up of outside borrowings. A portion of these borrowings come directly from Latin, or through one of the Romance languages, particularly Anglo-Norman and French, but some also from Italian, Portuguese, and Spanish; or from other languages (such as Gothic, Frankish or Greek) into Latin and then into English. The influence of Latin in English, therefore, is primarily lexical in nature, being confined mainly to words derived from Latin roots.\n\nWhile some new words enter English as slang, most do not. Some words are adopted from other languages; some are mixtures of existing words (portmanteau words), and some are new creations made of roots from dead languages: e.g., thanatopsis.\n\nA computerized survey of about 80,000 words in the old \"Shorter Oxford Dictionary\" (3rd ed.) was published in \"Ordered Profusion\" by Thomas Finkenstaedt and Dieter Wolff (1973) that estimated the origin of English words as follows:\n\n\nA survey by Joseph M. Williams in \"Origins of the English Language\" of 10,000 words taken from several thousand business letters gave this set of statistics:\n\nHere is a list of the most common foreign language influences in English, where other languages have influenced or contributed words to English.\n\nWords are almost absent, except for dialectal words, such as the Yan Tan Tethera system of counting sheep. However, hypotheses have been made that English syntax was influenced by Celtic languages, such as the system of continuous tenses was a cliché of similar Celtic phrasal structures; this is controversial, as the system has clear native English and other Germanic developments.\n\nThe French contributed legal, military, technological, and political terminology. Their language also contributed common words, such as the names of meats: \"veal\", \"mutton\", \"beef\", \"pork\", and how food was prepared: \"boil\", \"broil\", \"fry\", \"roast\", and \"stew\"; as well as words related to the nobility: \"prince\", \"duke\", \"marquess\", \"viscount\", \"baron\", and their feminine equivalents. Nearly 40 percent of English words (in an 80,000 word dictionary) may be of French origin.\n\nScientific and technical words, medical terminology, academic and legal terminology.\n\nScientific and medical terminology (for instance -phobias and -ologies), Christian theological terminology.\n\n\"Castle\", \"cauldron\", \"kennel\", \"catch\", \"cater\" are among Norman words introduced into English. The Norman language also introduced (or reinforced) words of Norse origin such as \"mug\".\n\nThere are many ways through which Dutch words have entered the English language: via trade and navigation, such as \"skipper\" (from \"schipper\"), \"freebooter\" (from \"vrijbuiter\"), \"keelhauling\" (from \"kielhalen\"); via painting, such as \"landscape\" (from \"landschap\"), \"easel\" (from \"ezel\"), \"still life\" (from \"stilleven\"); warfare, such as \"forlorn hope (from \"verloren hoop\"), \"beleaguer (from \"beleger\"), \"to bicker\" (from \"bicken\"); via civil engineering, such as \"dam\", \"polder\", \"dune\" (from \"duin\"); via the New Netherland settlements in North America, such as \"cookie\" (from \"koekie\"), \"boss\" from \"baas\", \"Santa Claus\" (from \"Sinterklaas\"); via Dutch/Afrikaans speakers with English speakers in South Africa, such as \"wildebeest\", \"apartheid\", \"boer\"; via French words of Dutch/Flemish origin that have subsequently been adopted into English, such as \"boulevard\" (from \"bolwerk\"), \"mannequin\" (from \"manneken\"), \"buoy\" (from \"boei\").\n\nWords relating to warfare and tactics, for instance \"flotilla\" and \"guerrilla\"; or related to science and culture, whether created in Arabic, originated in Amerindian civilizations (Cariban: \"cannibal\", \"hurricane\"; Mescalero: \"apache\"; Nahuatl: \"tomato\", \"coyote\", \"chocolate\"; Quechua: \"potato\"; Taíno: \"tobacco\"), or Iberian Romance languages (\"aficionado\", \"albino\", \"alligator\", \"cargo\", \"cigar\", \"embargo\", \"guitar\", \"jade\", \"mesa\", \"paella\", \"platinum\", \"plaza\", \"renegade\", \"rodeo\", \"salsa\", \"savvy\", \"sierra\", \"siesta\", \"tilde\", \"tornado\", \"vanilla\" etc.).\n\nWords relating to some music, piano, \"fortissimo\". Or Italian culture, such as \"piazza\", \"pizza\", \"gondola\", \"balcony\", \"fascism\". The English word \"umbrella\" comes from Italian \"ombrello\".\n\nWords relating to culture, originating from the colonial era. Many of these words are of Persian origin rather than Hindi because Persian was the official language of the Mughal courts. e.g., \"pyjamas\", \"bungalow\", \"verandah\", \"jungle\", \"curry\", \"shampoo\", \"khaki\".\n\nGerman words relating to World War I and World War II found their way into the English language, words such as \"Blitzkrieg\", \"Führer\" and \"Lebensraum\"; food terms, such as \"bratwurst\", \"hamburger\" and \"frankfurter\"; words related to psychology and philosophy, such a \"gestalt\", \"Übermensch\", \"zeitgeist\" and \"realpolitik\". From German origin are also: \"wanderlust\", \"schadenfreude\", \"kaputt\", \"kindergarten\", \"autobahn\", \"rucksack\".\n\nWords of Old Norse origin have entered English primarily from the contact between Old Norse and Old English during colonisation of eastern and northern England between the mid 9th to the 11th centuries (see also Danelaw). Many of these words are part of English core vocabulary, such as \"egg\", \"sky\" or \"knife\".\n\nWords used in religious contexts, like Sabath, kosher, hallelujah, amen, and jubilee or words that have become slang like \"schmuck\", \"shmooze\", \"nosh\", \"oy vey\", and \"schmutz\".\n\nTrade items such as \"borax\", \"coffee\", \"cotton\", \"hashish\", \"henna\", \"mohair\", \"muslin\", \"saffron\"; Islamic religious terms such as \"jihad\", \"hadith\" and \"sharia\"; scientific vocabulary borrowed into Latin in the 12th and 13th centuries (\"alcohol\", \"alkali\", \"algebra\", \"azimuth\", \"cipher\", \"nadir\"); plants or plant products originating in tropical Asia and introduced to medieval Europe through Arabic intermediation (\"camphor\", \"jasmine\", \"lacquer\", \"lemon\", \"orange\", \"sugar\"); Middle Eastern cuisine words (\"couscous\", \"falafel\", \"hummus\", \"kebab\", \"tahini\").\n\nCardinal numbering in English follows two models, Germanic and Italic. The basic numbers are zero through ten. The numbers eleven through nineteen follow native Germanic style, as do twenty, thirty, forty, fifty, sixty, seventy, eighty, and ninety.\n\nStandard English, especially in very conservative formal contexts, continued to use native Germanic style as late as World War I for intermediate numbers greater than 20, viz., \"one-and-twenty,\" \"five-and-thirty,\" \"seven-and-ninety,\" and so. But with the advent of the Industrial Revolution, the Latin tradition of counting as \"twenty-one,\" \"thirty-five,\" \"ninety-seven,\" etc., which is easier to say and was already common in non-standard regional dialects, gradually replaced the traditional Germanic style to become the dominant style by the end of nineteenth century.\n\nLinguistic purism in the English language is the belief that words of native origin should be used instead of foreign-derived ones (which are mainly Romantic, Latin and Greek). \"Native\" can mean \"Anglo-Saxon\" or it can be widened to include all Germanic words. In its mild form, it merely means using existing native words instead of foreign-derived ones (such as using \"begin\" instead of \"commence\"). In its more extreme form, it involves reviving native words that are no longer widely used (such as \"ettle\" for \"intend\") and/or coining new words from Germanic roots (such as word stock for vocabulary). This dates at least to the inkhorn term debate of the 16th and 17th century, where some authors rejected the foreign influence, and has continued to this day, being most prominent in Plain English advocacy to avoid Latinate terms if a simple native alternative exists.\n\n\n"}
{"id": "11037", "url": "https://en.wikipedia.org/wiki?curid=11037", "title": "Freyr", "text": "Freyr\n\nFreyr (Old Norse: Lord), sometimes anglicized as Frey, is a widely attested god associated with sacral kingship, virility and prosperity, with sunshine and fair weather, and pictured as a phallic fertility god in Norse mythology. Freyr is said to \"bestow peace and pleasure on mortals\". Freyr, sometimes referred to as Yngvi-Freyr, was especially associated with Sweden and seen as an ancestor of the Swedish royal house.\n\nIn the Icelandic books the \"Poetic Edda\" and the \"Prose Edda\", Freyr is presented as one of the Vanir, the son of the sea god Njörðr, as well as the twin brother of the goddess Freyja. The gods gave him Álfheimr, the realm of the Elves, as a teething present. He rides the shining dwarf-made boar Gullinbursti and possesses the ship Skíðblaðnir which always has a favorable breeze and can be folded together and carried in a pouch when it is not being used. He has the servants Skírnir, Byggvir and Beyla.\n\nThe most extensive surviving Freyr myth relates Freyr's falling in love with the female jötunn Gerðr. Eventually, she becomes his wife but first Freyr has to give away his sword which fights on its own \"if wise be he who wields it.\" Although deprived of this weapon, Freyr defeats the jötunn Beli with an antler. However, lacking his sword, Freyr will be killed by the fire jötunn Surtr during the events of Ragnarök.\n\nLike other Germanic deities, veneration of Freyr is revived in the modern period in Heathenry movement.\n\nWritten around 1080, one of the oldest written sources on pre-Christian Scandinavian religious practices is Adam of Bremen's \"Gesta Hammaburgensis ecclesiae pontificum\". Adam claimed to have access to first-hand accounts on pagan practices in Sweden. He refers to Freyr with the Latinized name Fricco and mentions that an image of him at Skara was destroyed by the Christian missionary, Bishop Egino. His description of the Temple at Uppsala gives some details on the god.\n\nLater in the account Adam states that when a marriage is performed a libation is made to the image of Fricco.\n\nHistorians are divided on the reliability of Adam's account. While he is close in time to the events he describes he has a clear agenda to emphasize the role of the Archbishopric of Hamburg-Bremen in the Christianization of Scandinavia. His timeframe for the Christianization of Sweden conflicts with other sources, such as runic inscriptions and archaeological evidence does not confirm the presence of a large temple at Uppsala. On the other hand, the existence of phallic idols was confirmed in 1904 with a find at Rällinge in Södermanland, Sweden.\n\nWhen Snorri Sturluson was writing in 13th century Iceland, the indigenous Germanic gods were still remembered although they had not been openly worshiped for more than two centuries.\n\nIn the \"Gylfaginning\" section of his \"Prose Edda\", Snorri introduces Freyr as one of the major gods.\n\nThis description has similarities to the older account by Adam of Bremen but the differences are interesting. Adam assigns control of the weather and produce of the fields to Thor but Snorri says that Freyr rules over those areas. Snorri also omits any explicitly sexual references in Freyr's description. Those discrepancies can be explained in several ways. It is possible that the Norse gods did not have exactly the same roles in Icelandic and Swedish paganism but it must also be remembered that Adam and Snorri were writing with different goals in mind. Either Snorri or Adam may also have had distorted information.\n\nThe only extended myth related about Freyr in the \"Prose Edda\" is the story of his marriage.\n\nThe woman is Gerðr, a beautiful giantess. Freyr immediately falls in love with her and becomes depressed and taciturn. After a period of brooding, he consents to talk to Skírnir, his foot-page. He tells Skírnir that he has fallen in love with a beautiful woman and thinks he will die if he cannot have her. He asks Skírnir to go and woo her for him.\n\nThe loss of Freyr's sword has consequences. According to the \"Prose Edda\", Freyr had to fight Beli without his sword and slew him with an antler. But the result at Ragnarök, the end of the world, will be much more serious. Freyr is fated to fight the fire-giant Surtr and since he does not have his sword he will be defeated.\n\nEven after the loss of his weapon Freyr still has two magical artifacts, both of them dwarf-made. One is the ship Skíðblaðnir, which will have favoring breeze wherever its owner wants to go and can also be folded together like a napkin and carried in a pouch. The other is the boar Gullinbursti whose mane glows to illuminate the way for his owner. No myths involving Skíðblaðnir have come down to us but Snorri relates that Freyr rode to Baldr's funeral in a wagon pulled by Gullinbursti.\n\nFreyr is referred to several times in skaldic poetry. In \"Húsdrápa\", partially preserved in the Prose Edda, he is said to ride a boar to Baldr's funeral.\n\nIn a poem by Egill Skalla-Grímsson, Freyr is called upon along with Njörðr to drive Eric Bloodaxe from Norway. The same skald mentions in \"Arinbjarnarkviða\" that his friend has been blessed by the two gods.\n\nIn \"Nafnaþulur\" Freyr is said to ride the horse Blóðughófi (\"Bloody Hoof\").\n\nFreyr is mentioned in several of the poems in the \"Poetic Edda\". The information there is largely consistent with that of the \"Prose Edda\" while each collection has some details not found in the other.\n\n\"Völuspá\", the best known of the Eddic poems, describes the final confrontation between Freyr and Surtr during Ragnarök.\n\nSome scholars have preferred a slightly different translation, in which the sun shines \"from the sword of the gods\". The idea is that the sword which Surtr slays Freyr with is the \"sword of the gods\" which Freyr had earlier bargained away for Gerðr. This would add a further layer of tragedy to the myth. Sigurður Nordal argued for this view but the possibility represented by Ursula Dronke's translation above is equally possible.\n\n\"Grímnismál\", a poem which largely consists of miscellaneous information about the gods, mentions Freyr's abode.\n\nA tooth-gift was a gift given to an infant on the cutting of the first tooth. Since \"Alfheimr\" or \"Álfheimr\" means \"World of Álfar (Elves)\" the fact that Freyr should own it is one of the indications of a connection between the Vanir and the obscure Álfar. \"Grímnismál\" also mentions that the sons of Ívaldi made Skíðblaðnir for Freyr and that it is the best of ships.\n\nIn the poem \"Lokasenna\", Loki accuses the gods of various misdeeds. He criticizes the Vanir for incest, saying that Njörðr had Freyr with his sister. He also states that the gods discovered Freyr and Freyja having sex together. The god Týr speaks up in Freyr's defense.\n\n\"Lokasenna\" also mentions that Freyr has servants called Byggvir and Beyla. They seem to have been associated with the making of bread.\n\nThe courtship of Freyr and Gerðr is dealt with extensively in the poem \"Skírnismál\".\nFreyr is depressed after seeing Gerðr. Njörðr and Skaði ask Skírnir to go and talk with him. Freyr reveals the cause of his grief and asks Skírnir to go to Jötunheimr to woo Gerðr for him. Freyr gives Skírnir a steed and his magical sword for the journey.\n\nWhen Skírnir finds Gerðr he starts by offering her treasures if she will marry Freyr. When she declines he gets her consent by threatening her with destructive magic.\nSnorri Sturluson starts his epic history of the kings of Norway with \"Ynglinga saga\", a euhemerized account of the Norse gods. Here Odin and the Æsir are men from Asia who gain power through their prowess in war and Odin's skills. But when Odin attacks the Vanir he bites off more than he can chew and peace is negotiated after the destructive and indecisive Æsir-Vanir War. Hostages are exchanged to seal the peace deal and the Vanir send Freyr and Njörðr to live with the Æsir. At this point the saga, like \"Lokasenna\", mentions that incest was practised among the Vanir.\n\nOdin makes Njörðr and Freyr priests of sacrifices and they become influential leaders. Odin goes on to conquer the North and settles in Sweden where he rules as king, collects taxes and maintains sacrifices. After Odin's death, Njörðr takes the throne. During his rule there is peace and good harvest and the Swedes come to believe that Njörðr controls these things. Eventually Njörðr falls ill and dies.\n\nFreyr had a son named Fjölnir, who succeeds him as king and rules during the continuing period of peace and good seasons. Fjölnir's descendants are enumerated in \"Ynglingatal\" which describes the mythological kings of Sweden.\n\nThe 14th century Icelandic \"Ögmundar þáttr dytts\" contains a tradition of how Freyr was transported in a wagon and administered by a priestess, in Sweden. Freyr's role as a fertility god needed a female counterpart in a divine couple (McKinnell's translation 1987):\n\nIn this short story, a man named Gunnar was suspected of manslaughter and escaped to Sweden, where Gunnar became acquainted with this young priestess. He helped her drive Freyr's wagon with the god effigy in it, but the god did not appreciate Gunnar and so attacked him and would have killed Gunnar if he had not promised himself to return to the Christian faith if he would make it back to Norway. When Gunnar had promised this, a demon jumped out of the god effigy and so Freyr was nothing but a piece of wood. Gunnar destroyed the wooden idol and dressed himself as Freyr, then Gunnar and the priestess travelled across Sweden where people were happy to see the god visiting them. After a while he made the priestess pregnant, but this was seen by the Swedes as confirmation that Freyr was truly a fertility god and not a scam. Finally, Gunnar had to flee back to Norway with his young bride and had her baptized at the court of Olaf Tryggvason.\n\nWorship of Freyr is alluded to in several Icelanders' sagas.\n\nThe protagonist of \"Hrafnkels saga\" is a priest of Freyr. He dedicates a horse to the god and kills a man for riding it, setting in motion a chain of fateful events.\n\nIn \"Gísla saga\" a chieftain named Þorgrímr Freysgoði is an ardent worshipper of Freyr. When he dies he is buried in a howe.\n\n\"Hallfreðar saga\", \"Víga-Glúms saga\" and \"Vatnsdœla saga\" also mention Freyr.\n\nOther Icelandic sources referring to Freyr include \"Íslendingabók\", \"Landnámabók\", and \"Hervarar saga\".\n\n\"Íslendingabók\", written around 1125, is the oldest Icelandic source to mention Freyr, including him in a genealogy of Swedish kings. \"Landnámabók\" includes a heathen oath to be sworn at an assembly where Freyr, Njörðr, and \"the almighty \"áss\"\" are invoked. \"Hervarar saga\" mentions a Yuletide sacrifice of a boar to Freyr.\n\nThe 12th Century Danish \"Gesta Danorum\" describes Freyr, under the name Frø, as the \"viceroy of the gods\".\n\nThat Freyr had a cult at Uppsala is well confirmed from other sources. The reference to the change in sacrificial ritual may also reflect some historical memory. There is archaeological evidence for an increase in human sacrifices in the late Viking Age though among the Norse gods human sacrifice is most often linked to Odin. Another reference to Frø and sacrifices is found earlier in the work, where the beginning of an annual \"blót\" to him is related. King Hadingus is cursed after killing a divine being and atones for his crime with a sacrifice.\n\nThe sacrifice of dark-coloured victims to Freyr has a parallel in Ancient Greek religion where the chthonic fertility deities preferred dark-coloured victims to white ones.\n\nIn book 9, Saxo identifies Frø as the \"king of Sweden\" (\"rex Suetiae\"):\n\nThe reference to public prostitution may be a memory of fertility cult practices. Such a memory may also be the source of a description in book 6 of the stay of Starcatherus, a follower of Odin, in Sweden.\n\nA strophe of the Anglo-Saxon rune poem (c. 1100) records that:\n\nThis may refer to the origins of the worship of Ingui in the tribal areas that Tacitus mentions in his \"Germania\" as being populated by the Inguieonnic tribes. A later Danish chronicler lists Ingui was one of three brothers that the Danish tribes descended from. The strophe also states that \"then he (Ingui) went back over the waves, his wagon behind him\" which could connect Ingui to earlier conceptions of the wagon processions of Nerthus and the later Scandinavian conceptions of Freyr's wagon journeys.\n\nIngui is mentioned also in some later Anglo-Saxon literature under varying forms of his name, such as \"For what doth Ingeld have to do with Christ\" and the variants used in Beowulf to designate the kings as 'leader of the friends of Ing'. The compound Ingui-Frea (OE) and Yngvi-Freyr (ON) likely refer to the connection between the god and the Germanic kings' role as priests during the sacrifices in the pagan period, as \"Frea\" and \"Freyr\" are titles meaning 'Lord'.\n\nThe Swedish royal dynasty was known as the Ynglings from their descent from Yngvi-Freyr. This is supported by Tacitus, who wrote about the Germans: \"In their ancient songs, their only way of remembering or recording the past they celebrate an earth-born god Tuisco, and his son Mannus, as the origin of their race, as their founders. To Mannus they assign three sons, from whose names, they say, the coast tribes are called Ingaevones; those of the interior, Herminones; all the rest, Istaevones\".\n\nIn 1904, a Viking Age statuette identified as a depiction of Freyr was discovered on the farm Rällinge in Lunda, Södermanland parish in the province of Södermanland, Sweden. The depiction features a cross-legged seated, bearded male with an erect penis. He is wearing a pointed cap and stroking his triangular beard. The statue is 9 centimeters tall and is displayed at the Swedish Museum of National Antiquities.\n\nA part of the Swedish Skog tapestry depicts three figures that has been interpreted as allusions to Odin, Thor, and Freyr, but also as the three Scandinavian holy kings Canute, Eric and Olaf. The figures coincide with 11th century descriptions of statue arrangements recorded by Adam of Bremen at the Temple at Uppsala and written accounts of the gods during the late Viking Age. The tapestry is originally from Hälsingland, Sweden but is now housed at the Swedish Museum of National Antiquities.\n\nSmall pieces of gold foil featuring engravings dating from the Migration Period into the early Viking Age (known as \"gullgubber\") have been discovered in various locations in Scandinavia, at one site almost 2,500. The foil pieces have been found largely on the sites of buildings, only rarely in graves. The figures are sometimes single, occasionally an animal, sometimes a man and a woman with a leafy bough between them, facing or embracing one another. The human figures are almost always clothed and are sometimes depicted with their knees bent. Scholar Hilda Ellis Davidson says that it has been suggested that the figures are taking part in a dance, and that they may have been connected with weddings, as well as linked to the Vanir group of gods, representing the notion of a divine marriage, such as in the \"Poetic Edda\" poem \"Skírnismál\"; the coming together of Gerðr and Freyr.\n\nNorway\nSweden\nDenmark\nNetherlands\nBelgium\n\n\n\n \n"}
{"id": "4313931", "url": "https://en.wikipedia.org/wiki?curid=4313931", "title": "Fuel economy in automobiles", "text": "Fuel economy in automobiles\n\nThe fuel economy of an automobile relates distance traveled by a vehicle and the amount of fuel consumed. Consumption can be expressed in terms of volume of fuel to travel a distance, or the distance travelled per unit volume of fuel consumed. Since fuel consumption of vehicles is a significant factor in air pollution, and since importation of motor fuel can be a large part of a nation's foreign trade, many countries impose requirements for fuel economy. Different methods are used to approximate the actual performance of the vehicle. The energy in fuel is required to overcome various losses (wind resistance, tire drag, and others) encountered while propelling the vehicle, and in providing power to vehicle systems such as ignition or air conditioning. Various strategies can be employed to reduce losses at each of the conversions between the chemical energy in the fuel and the kinetic energy of the vehicle. Driver behavior can affect fuel economy; maneuvers such as sudden acceleration and heavy braking waste energy.\n\nElectric cars do not directly burn fuel, and so do not have fuel economy per se, but equivalence measures, such as miles per gallon gasoline equivalent have been created to attempt to compare them.\n\nFuel economy is the relationship between the distance traveled and fuel consumed.\n\nFuel economy can be expressed in two ways:\n\n\n\nConversions of units:\nNote that when expressed as \"units of fuel per fixed distance\" (L/100km etc.), a lower number means more efficient, while a higher number means less efficient; whereas, using \"Units of distance per fixed fuel unit\" (mpg, km/L etc.), a higher number means more efficient, while a lower number means less efficient.\n\nWhile the thermal efficiency (mechanical output to chemical energy in fuel) of petroleum engines has increased since the beginning of the automotive era to a current maximum of 36.4% this is not the only factor in fuel economy. The design of automobile as a whole and usage pattern affects the fuel economy. Published fuel economy is subject to variation between jurisdiction due to variations in testing protocols.\n\nOne of the first studies to determine fuel economy in the United States was the Mobil Economy Run, which was an event that took place every year from 1936 (except during World War II) to 1968. It was designed to provide real fuel efficiency numbers during a coast to coast test on real roads and with regular traffic and weather conditions. The Mobil Oil Corporation sponsored it and the United States Auto Club (USAC) sanctioned and operated the run. In more recent studies, the average fuel economy for new passenger car in the United States improved from 17 mpg (13.8 L/100 km) in 1978 to more than 22 mpg (10.7 L/100 km) in 1982.\nThe average fuel economy in 2008 for new cars, light trucks and SUVs in the United States was 26.4 mpg (8.9 L/100 km). 2008 model year cars classified as \"midsize\" by the US EPA ranged from 11 to 46 mpg(21 to 5 L/100 km) However, due to environmental concerns caused by CO emissions, new EU regulations are being introduced to reduce the average emissions of cars sold beginning in 2012, to 130 g/km of CO, equivalent to 4.5 L/100 km (52 mpg, 63 mpg) for a diesel-fueled car, and 5.0 L/100 km (47 mpg, 56 mpg) for a gasoline (petrol)-fueled car.\n\nThe average consumption across the fleet is not immediately affected by the \"new vehicle\" fuel economy: for example, Australia's car fleet average in 2004 was 11.5 L/100 km (20.5 mpg), compared with the average new car consumption in the same year of 9.3 L/100 km (25.3 mpg)\n\nFuel economy at steady speeds with selected vehicles was studied in 2010. The most recent study indicates greater fuel efficiency at higher speeds than earlier studies; for example, some vehicles achieve better fuel economy at rather than at , although not their best economy, such as the 1994 Oldsmobile Cutlass Ciera with the LN2 2.2L engine, which has its best economy at (), and gets better economy at than at ( vs ). The proportion of driving on high speed roadways varies from 4% in Ireland to 41% in the Netherlands.\n\nWhen the US National Maximum Speed Law's speed limit was mandated, there were complaints that fuel economy could decrease instead of increase. The 1997 Toyota Celica got better fuel-efficiency at than it did at ( vs ), although even better at than at ( vs ), and its best economy () at only . Other vehicles tested had from 1.4 to 20.2% better fuel-efficiency at vs. . Their best economy was reached at speeds of (see graph).\n\nOfficials hoped that the limit, combined with a ban on ornamental lighting, no gasoline sales on Sunday, and a 15% cut in gasoline production, would reduce total gas consumption by 200,000 barrels a day, representing a 2.2% drop from annualized 1973 gasoline consumption levels. This was partly based on a belief that cars achieve maximum efficiency between and that trucks and buses were most efficient at .\n\nIn 1998, the U.S. Transportation Research Board footnoted an estimate that the 1974 National Maximum Speed Limit (NMSL) reduced fuel consumption by 0.2 to 1.0 percent. Rural interstates, the roads most visibly affected by the NMSL, accounted for 9.5% of the U.S' vehicle-miles-traveled in 1973, but such free-flowing roads typically provide more fuel-efficient travel than conventional roads.\n\nIdentical vehicles can have varying fuel consumption figures listed depending upon the testing methods of the jurisdiction.\n\nLexus IS 250 – petrol 2.5 L \"4GR-FSE\" V6, 204 hp (153 kW), 6 speed automatic, rear wheel drive\n\nSince the total force opposing the vehicle's motion (at constant speed) multiplied by the distance through which the vehicle travels represents the work that the vehicle's engine must perform, the study of fuel economy (the amount of energy consumed per unit of distance traveled) requires a detailed analysis of the forces that oppose a vehicle's motion. In terms of physics, Force = rate at which the amount of work generated (energy delivered) varies with the distance traveled, or:\n\nNote: The amount of work generated by the vehicle's power source (energy delivered by the engine) would be exactly proportional to the amount of fuel energy consumed by the engine if the engine's efficiency is the same regardless of power output, but this is not necessarily the case due to the operating characteristics of the internal combustion engine.\n\nFor a vehicle whose source of power is a heat engine (an engine that uses heat to perform useful work), the amount of fuel energy that a vehicle consumes per unit of distance (level road) depends upon:\n\nIdeally, a car traveling at a constant velocity on level ground in a vacuum with frictionless wheels could travel at any speed without consuming any energy beyond what is needed to get the car up to speed. Less ideally, any vehicle must expend energy on overcoming road load forces, which consist of aerodynamic drag, tire rolling resistance, and inertial energy that is lost when the vehicle is decelerated by friction brakes. With ideal regenerative braking, the inertial energy could be completely recovered, but there are few options for reducing aerodynamic drag or rolling resistance other than optimizing the vehicle's shape and the tire design. Road load energy, or the energy demanded at the wheels, can be calculated by evaluating the vehicle equation of motion over a specific driving cycle. The vehicle powertrain must then provide this minimum energy in order to move the vehicle, and will lose a large amount of additional energy in the process of converting fuel energy into work and transmitting it to the wheels. Overall, the sources of energy loss in moving a vehicle may be summarized as follows:\n\nFuel-efficiency decreases from electrical loads are most pronounced at lower speeds because most electrical loads are constant while engine load increases with speed. So at a lower speed a higher proportion of engine horsepower is used by electrical loads. Hybrid cars see the greatest effect on fuel-efficiency from electrical loads because of this proportional effect.\n\nTechnologies that may improve fuel efficiency, but are not yet on the market, include:\nMany aftermarket consumer products exist that are purported to increase fuel economy; many of these claims have been discredited. In the United States, the Environmental Protection Agency maintains a list of devices that have been tested by independent laboratories and makes the test results available to the public.\n\nThe mandatory publication of the fuel consumption by the manufacturer led some to use dubious practices to reach better values in the past. If the test is on a test stand, the vehicle may detect open doors and adapt the engine control. Also when driven according to the test regime, the parameters may adapt automatically. Test laboratories use a \"golden car\" that is tested in each one to check that each lab produces the same set of measurements for a given drive cycle.\n\nTire pressures and lubricants have to be as recommended by the manufacturer (Higher tire pressures are required on a particular dynamometer type, but this is to compensate for the different rolling resistance of the dynamometer, not to produce an unrealistic load on the vehicle). Normally the quoted figures a manufacturer publishes have to be proved by the relevant authority witnessing vehicle/engine tests. Some jurisdictions independently test emissions of vehicles in service, and as a final measure can force a recall of all of a particular type of vehicle if the customer vehicles do not fulfill manufacturers' claims within reasonable limits. The expense and bad publicity from such a recall encourages manufacturers to publish realistic figures. The US Federal government retests 10–15% of models), to make sure that the manufacturer's tests are accurate.\n\nReal world fuel consumption can vary greatly as they can be affected by many factors that have little to do with the vehicle. Driving conditions – weather, traffic, temperature; driving style – hard braking, jack rabbit starts, and speeding; road conditions – paved vs gravel, smooth vs potholes; and things like carrying excess weight, roof racks, and fuel quality can all combine to dramatically increase fuel consumption. Expecting to consistently perform in the face of so many variables is impossible as is the expectation for one set of numbers to encompass every driver and their personal circumstances.\n\nThe ratings are meant to provide a comparison, and are not a promise of actual performance.\n\nFor many years critics had claimed that EPA (U.S. \"Environmental Protection Agency\") estimated fuel economy figures had been misleading. The primary arguments of the EPA detractors were focused on the lack of real world testing, and the very limited scale (i.e., city or highway).\n\nPartly as a response to these criticisms, the EPA changed their fuel economy rating system in 2008 in an attempt to more adequately address these concerns. Instead of testing simply in two presumed modes, the testing now covers:\n\nWhile the new EPA standards may represent an improvement, real world user data may still be the best way to gather and collect accurate fuel economy information. As such the EPA has also set up a http://www.fueleconomy.gov/mpg/MPG.do?action=browseList website where drivers can enter and track their own real-world fuel economy numbers.\n\nThere are also a number of websites that attempt to track and report individual user fuel economy data through real-life driving. Sites or publications such as Consumer Reports, Edmunds.com, Consumer Guide and TrueDelta.com offer this service and claim more accurate numbers than those listed by the EPA.\n\nGovernments, various environmentalist organizations, and companies like Toyota and Shell Oil Company have historically urged drivers to maintain adequate air pressure in tires and careful acceleration/deceleration habits. Keeping track of fuel efficiency stimulates fuel economy-maximizing behavior.\n\nA five-year partnership between Michelin and Anglian Water shows that 60,000 liters of fuel can be saved on tire pressure. The Anglian Water fleet of 4,000 vans and cars are now lasting their full lifetime. This shows the impact that tire pressures have on the fuel efficiency.\n\nEnvironmental management systems EMAS as well as good fleet management includes record keeping of the fleet fuel consumption. Quality management uses those figures to steer the measures acting on the fleets. This is a way to check whether procurement, driving, and maintenance in total have contributed to changes in the fleet's overall consumption.\n\n<nowiki>* highway ** combined</nowiki>\n\nFrom October 2008, all new cars had to be sold with a sticker on the windscreen showing the fuel consumption and the CO emissions. Fuel consumption figures are expressed as \"urban\", \"extra urban\" and \"combined\", measured according to ECE Regulations 83 and 101 – which are the based on the European driving cycle; previously, only the \"combined\" number was given.\n\nAustralia also uses a star rating system, from one to five stars, that combines greenhouse gases with pollution, rating each from 0 to 10 with ten being best. To get 5 stars a combined score of 16 or better is needed, so a car with a 10 for economy (greenhouse) and a 6 for emission or 6 for economy and 10 for emission, or anything in between would get the highest 5 star rating. The lowest rated car is the Ssangyong Korrando with automatic transmission, with one star, while the highest rated was the Toyota Prius hybrid. The Fiat 500, Fiat Punto and Fiat Ritmo as well as the Citroen C3 also received 5 stars. The greenhouse rating depends on the fuel economy and the type of fuel used. A greenhouse rating of 10 requires 60 or less grams of CO per km, while a rating of zero is more than 440 g/km CO. The highest greenhouse rating of any 2009 car listed is the Toyota Prius, with 106 g/km CO and . Several other cars also received the same rating of 8.5 for greenhouse. The lowest rated was the Ferrari 575 at 499 g/km CO and . The Bentley also received a zero rating, at 465 g/km CO. The best fuel economy of any year is the 2004–2005 Honda Insight, at .\n\nVehicle manufacturers follow a controlled laboratory testing procedure to generate the fuel consumption data that they submit to the Government of Canada. This controlled method of fuel consumption testing, including the use of standardized fuels, test cycles and calculations, is used instead of on-road driving to ensure that all vehicles are tested under identical conditions and that the results are consistent and repeatable.\n\nSelected test vehicles are “run in” for about 6,000 km before testing. The vehicle is then mounted on a chassis dynamometer programmed to take into account the aerodynamic efficiency, weight and rolling resistance of the vehicle. A trained driver runs the vehicle through standardized driving cycles that simulate trips in the city and on the highway. Fuel consumption ratings are derived from the emissions generated during the driving cycles.\n\nTHE 5 CYCLE TEST:\n\n1. The city test simulates urban driving in stop-and-go traffic with an average speed of 34 km/h and a top speed of 90 km/h. The test runs for approximately 31 minutes and includes 23 stops. The test begins from a cold engine start, which is similar to starting a vehicle after it has been parked overnight during the summer. The final phase of the test repeats the first eight minutes of the cycle but with a hot engine start. This simulates restarting a vehicle after it has been warmed up, driven and then stopped for a short time. Over five minutes of test time are spent idling, to represent waiting at traffic lights. The ambient temperature of the test cell starts at 20 °C and ends at 30 °C.\n\n2. The highway test simulates a mixture of open highway and rural road driving, with an average speed of 78 km/h and a top speed of 97 km/h. The test runs for approximately 13 minutes and does not include any stops. The test begins from a hot engine start. The ambient temperature of the test cell starts at 20 °C and ends at 30 °C.\n\n3. In the cold temperature operation test, the same driving cycle is used as in the standard city test, except that the ambient temperature of the test cell is set to −7 °C.\n\n4. In the air conditioning test, the ambient temperature of the test cell is raised to 35 °C. The vehicle's climate control system is then used to lower the internal cabin temperature. Starting with a warm engine, the test averages 35 km/h and reaches a maximum speed of 88 km/h. Five stops are included, with idling occurring 19% of the time.\n\n5. The high speed/quick acceleration test averages 78 km/h and reaches a top speed of 129 km/h. Four stops are included and brisk acceleration maximizes at a rate of 13.6 km/h per second. The engine begins warm and air conditioning is not used. The ambient temperature of the test cell is constantly 25 °C.\n\nTests 1, 3, 4 & 5 are averaged to create the city driving fuel consumption rate.\n\nTests 2, 4 & 5 are averaged to create the highway driving fuel consumption rate.\n\nIn the European Union, passenger vehicles are commonly tested using two drive cycles, and corresponding fuel economies are reported as 'urban' and 'extra-urban', in litres per 100 km and (in the UK) in miles per imperial gallon.\n\nThe urban economy is measured using the test cycle known as ECE-15, first introduced in 1970 by EC Directive 70/220/EWG and finalized by EEC Directive 90/C81/01 in 1999. It simulates a 4,052 m (2.518 mile) urban trip at an average speed of 18.7 km/h (11.6 mph) and at a maximum speed of 50 km/h (31 mph).\n\nThe extra-urban driving cycle or EUDC lasts 400 seconds (6 minutes 40 seconds) at an average speed 62.6 km/h (39 mph) and a top speed of 120 km/h (74.6 mph).\n\nEU fuel consumption numbers are often considerably lower than corresponding US EPA test results for the same vehicle. For example, the 2011 Honda CR-Z with a six-speed manual transmission is rated 6.1/4.4 L/100 km in Europe and 7.6/6.4 L/100 km (31/37 mpg ) in the United States.\n\nIn the European Union advertising has to show Carbon dioxide (CO)-emission and fuel consumption data in a clear way as described in the UK Statutory Instrument 2004 No 1661. Since September 2005 a colour-coded \"Green Rating\" sticker has been available in the UK, which rates fuel economy by CO emissions: A: <= 100 g/km, B: 100–120, C: 121–150, D: 151–165, E: 166–185, F: 186–225, and G: 226+. Depending on the type of fuel used, for gasoline A corresponds to about and G about . Ireland has a very similar label, but the ranges are slightly different, with A: <= 120 g/km, B: 121–140, C: 141–155, D: 156–170, E: 171–190, F: 191–225, and G: 226+.\n\nIn the UK the ASA (Advertising standards agency) have claimed that fuel consumption figures are misleading. Often the case with European vehicles as the MPG (miles per gallon) figures that can be advertised are often not the same as 'real world' driving.\n\nThe ASA have said that Car manufacturers can use ‘cheats’ to prepare their vehicles for their compulsory fuel efficiency and emissions tests in a way set out to make themselves look as ‘clean’ as possible. This practice is common in petrol and diesel vehicle tests, but hybrid and electric vehicles are not immune as manufacturers apply these techniques to fuel efficiency.\n\nCar experts also assert that the \"official\" MPG figures given by manufacturers do not represent the \"true\" MPG values from real-world driving. Websites have been set up to show the real-world MPG figures, based on crowd-sourced data from real users, vs the official MPG figures.\n\nThe major loopholes in the current EU tests allow car manufacturers a number of ‘cheats’ to improve results. Car manufacturers can:\n\nAccording to the results of a 2014 study by the International Council on Clean Transportation (ICCT), the gap between official and real-world fuel-economy figures in Europe has risen to about 38% in 2013 from 10% in 2001. The analysis found that for private cars, the difference between on-road and official values rose from around 8% in 2001 to 31% in 2013, and 45% for company cars in 2013. The report is based on data from more than half a million private and company vehicles across Europe. The analysis was prepared by the ICCT together with the Netherlands Organisation for Applied Scientific Research (TNO), and the German Institut für Energie- und Umweltforschung Heidelberg (IFEU).\n\nThe evaluation criteria used in Japan reflects driving conditions commonly found, as the typical Japanese driver doesn't drive as fast as other regions internationally (Speed limits in Japan)\n\nThe 10–15 mode driving cycle test is the official fuel economy and emission certification test for new light duty vehicles in Japan. Fuel economy is expressed in km/L (kilometers per litre) and emissions are expressed in g/km. The test is carried out on a dynamometer and consist of 25 tests which cover idling, acceleration, steady running and deceleration, and simulate typical Japanese urban and/or expressway driving conditions. The running pattern begins with a warm start, lasts for 660 seconds (11 minutes) and runs at speeds up to . The distance of the cycle is , average speed of , and duration 892 seconds (14.9 minutes), including the initial 15 mode segment.\n\nA new more demanding test, called the JC08, was established in December 2006 for Japan’s new standard that goes into effect in 2015, but it is already being used by several car manufacturers for new cars. The JC08 test is significantly longer and more rigorous than the 10–15 mode test. The running pattern with JC08 stretches out to 1200 seconds (20 minutes), and there are both cold and warm start measurements and top speed is . The economy ratings of the JC08 are lower than the 10–15 mode cycle, but they are expected to be more real world. The Toyota Prius became the first car to meet Japan’s new 2015 Fuel Economy Standards measured under the JC08 test.\n\nStarting on 7 April 2008 all cars of up to 3.5 tonnes GVW sold other than private sale need to have a fuel economy sticker applied (if available) that shows the rating from one half star to six stars with the most economic cars having the most stars and the more fuel hungry cars the least, along with the fuel economy in L/100 km and the estimated annual fuel cost for driving 14,000 km (at present fuel prices). The stickers must also appear on vehicles to be leased for more than 4 months. All new cars currently rated range from to and received respectively from 4.5 to 5.5 stars.\n\nThe Energy Tax Act of 1978 in the US established a gas guzzler tax on the sale of new model year vehicles whose fuel economy fails to meet certain statutory levels. The tax applies only to cars (not trucks) and is collected by the IRS. Its purpose is to discourage the production and purchase of fuel-inefficient vehicles. The tax was phased in over ten years with rates increasing over time. It applies only to manufacturers and importers of vehicles, although presumably some or all of the tax is passed along to automobile consumers in the form of higher prices. Only new vehicles are subject to the tax, so no tax is imposed on used car sales. The tax is graduated to apply a higher tax rate for less-fuel-efficient vehicles. To determine the tax rate, manufacturers test all the vehicles at their laboratories for fuel economy. The US Environmental Protection Agency confirms a portion of those tests at an EPA lab.\n\nIn some cases, this tax may apply only to certain variants of a given model; for example, the 2004–2006 Pontiac GTO (captive import version of the Holden Monaro) did incur the tax when ordered with the four-speed automatic transmission, but did not incur the tax when ordered with the six-speed manual transmission.\n\nTwo separate fuel economy tests simulate city driving and highway driving: the \"city\" driving program or Urban Dynamometer Driving Schedule or (UDDS) or FTP-72 is defined in and consists of starting with a cold engine and making 23 stops over a period of 31 minutes for an average speed of 20 mph (32 km/h) and with a top speed of 56 mph (90 km/h).\n\nThe \"highway\" program or Highway Fuel Economy Driving Schedule (HWFET) is defined in and uses a warmed-up engine and makes no stops, averaging 48 mph (77 km/h) with a top speed of 60 mph (97 km/h) over a distance. The measurements are then adjusted downward by 10% (city) and 22% (highway) to more accurately reflect real-world results. A weighted average of city (55%) and highway (45%) fuel economies is used to determine the guzzler tax.\n\nThe procedure has been updated to FTP-75, adding a \"hot start\" cycle which repeats the \"cold start\" cycle after a 10-minute pause.\n\nBecause EPA figures had almost always indicated better efficiency than real-world fuel-efficiency, the EPA has modified the method starting with 2008. Updated estimates are available for vehicles back to the 1985 model year.\n\nUS EPA altered the testing procedure effective MY2008 which adds three new Supplemental Federal Test Procedure (SFTP) tests to include the influence of higher driving speed, harder acceleration, colder temperature and air conditioning use.\n\nSFTP US06 is a high speed/quick acceleration loop that lasts 10 minutes, covers , averages and reaches a top speed of . Four stops are included, and brisk acceleration maximizes at a rate of per second. The engine begins warm and air conditioning is not used. Ambient temperature varies between to .\n\nSFTO SC03 is the air conditioning test, which raises ambient temperatures to , and puts the vehicle's climate control system to use. Lasting 9.9 minutes, the loop averages and maximizes at a rate of . Five stops are included, idling occurs 19 percent of the time and acceleration of 5.1 mph/sec is achieved. Engine temperatures begin warm.\n\nLastly, a cold temperature cycle uses the same parameters as the current city loop, except that ambient temperature is set to .\n\nEPA tests for fuel economy do not include electrical load tests beyond climate control, which may account for some of the discrepancy between EPA and real world fuel-efficiency. A 200 W electrical load can produce a 0.4 km/L (0.94 mpg) reduction in efficiency on the FTP 75 cycle test.\n\nFollowing the efficiency claims made for vehicles such as Chevrolet Volt and Nissan Leaf, the National Renewable Energy Laboratory recommended to use EPA's new vehicle fuel efficiency formula that gives different values depending on fuel used. In November 2010 the EPA introduced the first fuel economy ratings in the Monroney stickers for plug-in electric vehicles.\n\nFor the fuel economy label of the Chevy Volt plug-in hybrid EPA rated the car separately for all-electric mode expressed in miles per gallon gasoline equivalent (MPG-e) and for gasoline-only mode expressed in conventional miles per gallon. EPA also estimated an overall combined city/highway gas-electricity fuel economy rating expressed in miles per gallon gasoline equivalent (MPG-e). The label also includes a table showing fuel economy and electricity consumed for five different scenarios: , , and driven between a full charge, and a never charge scenario. This information was included in order to make the consumers aware of the variability of the fuel economy outcome depending on miles driven between charges. Also the fuel economy for a gasoline-only scenario (never charge) was included. For electric-only mode the energy consumption estimated in kWh per is also shown.\nFor the fuel economy label of the Nissan Leaf electric car EPA rated the combined fuel economy in terms of miles per gallon gasoline equivalent, with a separate rating for city and highway driving. This fuel economy equivalence is based on the energy consumption estimated in kWh per 100 miles, and also shown in the Monroney label.\n\nIn May 2011, the National Highway Traffic Safety Administration (NHTSA) and EPA issued a joint final rule establishing new requirements for a fuel economy and environment label that is mandatory for all new passenger cars and trucks starting with model year 2013, and voluntary for 2012 models. The ruling includes new labels for alternative fuel and alternative propulsion vehicles available in the US market, such as plug-in hybrids, electric vehicles, flexible-fuel vehicles, hydrogen fuel cell vehicle, and natural gas vehicles. The common fuel economy metric adopted to allow the comparison of alternative fuel and advanced technology vehicles with conventional internal combustion engine vehicles is miles per gallon of gasoline equivalent (MPGe). A gallon of gasoline equivalent means the number of kilowatt-hours of electricity, cubic feet of compressed natural gas (CNG), or kilograms of hydrogen that is equal to the energy in a gallon of gasoline.\n\nThe new labels also include for the first time an estimate of how much fuel or electricity it takes to drive , providing US consumers with fuel consumption per distance traveled, the metric commonly used in many other countries. EPA explained that the objective is to avoid the traditional miles per gallon metric that can be potentially misleading when consumers compare fuel economy improvements, and known as the \"MPG illusion\" – this illusion arises because the reciprocal (i.e. non-linear) relationship between cost (equivalently, volume of fuel consumed) per unit distance driven and MPG value means that \"differences\" in MPG values are not directly meaningful – only ratios are (in mathematical terms, the reciprocal function does not commute with addition and subtraction; in general, a difference in reciprocal values is not equal to the reciprocal of their difference). It has been claimed that many consumers are unaware of this, and therefore compare MPG values by subtracting them, which can give a misleading picture of relative differences in fuel economy between different pairs of vehicles – for instance, an increase from 10 to 20 MPG corresponds to a 100% improvement in fuel economy, whereas an increase from 50 to 60 MPG is only a 20% improvement, although in both cases the difference is 10 MPG. The EPA explained that the new gallons-per-100-miles metric provides a more accurate measure of fuel efficiency – notably, it is equivalent to the normal metric measurement of fuel economy, liters per 100 kilometers (L/100 km).\n\nThe Corporate Average Fuel Economy (CAFE) regulations in the United States, first enacted by Congress in 1975, are federal regulations intended to improve the average fuel economy of cars and light trucks (trucks, vans and sport utility vehicles) sold in the US in the wake of the 1973 Arab Oil Embargo. Historically, it is the sales-weighted average fuel economy of a manufacturer's fleet of current model year passenger cars or light trucks, manufactured for sale in the United States. Under Truck CAFE standards 2008–2011 this changes to a \"footprint\" model where larger trucks are allowed to consume more fuel. The standards were limited to vehicles under a certain weight, but those weight classes were expanded in 2011.\n\nThe Clean Air Act of 1970 prohibited states from establishing their own air pollution standards. However, the legislation authorized the EPA to grant a waiver to California, allowing the state to set higher standards. The law provides a “piggybacking” provision that allows other states to adopt vehicle emission limits that are the same as California’s. California’s waivers were routinely granted until 2007, when the Bush administration rejected the state’s bid to adopt global warming pollution limits for cars and light trucks. California and 15 other states that were trying to put in place the same emissions standards sued in response. The case was tied up in court until the administration of Barack Obama, which in 2009 reversed the Bush administration’s decision by granting the waiver.\n\nIn April 2018, EPA Administrator Scott Pruitt announced that the Trump administration planned to roll back the federal fuel economy standards put in place in 2012 and that it would also seek to curb California’s authority to set its own standards. However, the Trump administration is reportedly also in talks with state officials to develop a compromise that would allow the state and national standards to stay in place.\n\n\n\n\n"}
{"id": "3720264", "url": "https://en.wikipedia.org/wiki?curid=3720264", "title": "Govardhan Hill", "text": "Govardhan Hill\n\nGovardhana Hill (), also called Mount Govardhana, Giri Raj and Royal Hill, is a sacred Hindu site in the Mathura district of Uttar Pradesh, India on an 8 km long hill located in the area of Govardhan and Radha Kund, which is about from Vrindavan.\n\nKnown as Govardhan or Giriraj it is the sacred center of Braj and is identified as a natural form of the Lord Krishna himself (Govardhana sila).\n\nThe name 'Govardhana' has two primary translations. In the literal meaning, 'Go' translates to 'cows', and 'vardhana' translates to 'nourishment'. Another meaning of 'Go' is 'the senses' and 'vardhana' can also mean 'to increase' - thus the name is also translated by devotees of Krishna as 'that which increases the senses' in their attraction to Krishna. In this connection, it is believed that the personality of Govardhan blesses the devotee by increasing his devotion (bhakti). Thus, by residing in the foothills of Govardhan Hill, all the senses and the respective duties of a soul attain divinity and are more inclined to perform service to Krishna.\n\nGovardhan Hill, stretching from Radha Kund to south of Govardhan, is a long ridge that, at its highest, stands a mere above the surrounding land, belying artistic depictions of it as a steep hill. At the southern end of the hill is the village of Punchari, while at the crest stand the villages of Aanyor and Jatipura.\n\nGovardhan Hill is considered a sacred site because it is the setting for many legends relating to the life of Lord Krishna, the deity believed to be embodied in the earth of the hill. Krishna and his brother Balaram are said to have spent many happy hours roaming among its shady groves, pools, caves and lush cow-pastures. An Eden-like sanctuary, the area's waterfalls, garden-grove (\"van\"), arbour (\"nikunj\"), water tank (\"kund\"), and flora are depicted in scenes of Krishna's adventures and raas with Radha.\n\nThe buildings and other structures on the Hill date from the sixteenth century. , there is no known archaeological evidence of any remains of greater age.\n\nA few of the sites include:\n\nThere are legends of Krishna’s saving the hill from a flood, “dalliances with gopis (cow-herdresses)’’ and interactions with demons and gods. Artwork has been created of the Hill represented as a bull and a peacock, Krishna in a cave, the Hill as a mountain of food Annakut, depicted in the floods brought on by Indra, and with the Yamuna River.\n\nGovardhan Puja is celebrated on the day after Diwali. It is the day upon which Lord Krishna defeated Indra, the deity of thunder and rain. As per the story, Krishna saw huge preparations for the annual offering to Indra and questions his father Nanda about it. He debated with the villagers about what their 'dharma' truly was. They were farmers, they should do their duty and concentrate on farming and protection of their cattle. He continued to say that all human beings should merely do their 'dharma', to the best of their ability and not pray or conduct sacrifices for natural phenomenon. The villagers were convinced by Krishna, and did not proceed with the special puja (prayer). Indra was then angered, and flooded the village. Krishna then lifted Mt Govardhan and held it up as protection to his people and cattle from the rain. Indra finally accepted defeat and recognized Krishna as supreme. This aspect of Krishna's life is mostly glossed over - but it actually set up on the basis of the 'karma' philosophy later detailed in the Bhagavad Gita.\n\nAccording to ancient Vaishnava legends the Vedic Deva (demigod akin to guardian angel), Indra (god of rain & lightning) was feared by human beings because he would either give the people no rain or flood them if he was not satisfied with their worship. When Krishna found out, he opposed the performance of sacrificial worship for Indra. He emphasized the importance of karma and doing ones duty. This made Indra angry at the boy Krishna (one of the incarnations of the Supreme God ).\n\nIndra thus invoked many clouds to appear in the sky and schemed to flood the region with rains lasting for seven days and seven nights. Krishna in reply then lifted Govardhan Hill, under which all the animals and people of the region took shelter, safe from the rains of Indra's fury. Ultimately, Indra accepted defeat. He offered his prayers and left to his heavenly kingdom.\n\nIt also represents the downfall of Indra, and a new beginning in Hindu philosophy, from a more sacrificial/ appeasement oriented worship, to a more spiritual plane of thought. This evolution of thought in Hinduism was brought about by Krishna, and therefore he has been the most important Hindu deity since then - considered an 'avatar' of the supreme.\n\nIn 2018, the Uttar Pradesh Chief Minister Yogi Adityanath declared Govardhan as a pilgrimage centre (teerth sthal) along with Mathura, Baldev, Nandgaon, Radhakund and Gokul ..Yogi Adityanath Government is also Plan ned to Rejuvenate Govardhan Parvat With Dwapar Yug Flora like kadamb, karoli, tamal, pakkad and tilkan\n\n\n"}
{"id": "18179220", "url": "https://en.wikipedia.org/wiki?curid=18179220", "title": "History of electric power transmission", "text": "History of electric power transmission\n\nThe tools and means of moving electricity far from where it is generated date back to the late 19th century. They include the movement of electricity in bulk (formally called \"transmission\") and the delivery of electricity to individual customers (\"distribution\"). In the beginning, the two terms were used interchangeably.\n\nPrior to electricity, various systems have been used for transmission of power across large distances. Chief among them were telodynamic (cable in motion), pneumatic (pressurized air), and hydraulic (pressurized fluid) transmission. Cable cars were the most frequent example of telodynamic transmission, whose lines could extend for several miles for a single section. Pneumatic transmission was used for city power transmission systems in Paris, Birmingham, Rixdorf, Offenbach, Dresden and Buenos Aires at the beginning of the twentieth century. Cities in the 19th century also used hydraulic transmission using high pressure water mains to deliver power to factory motors. London's system delivered 7000 hp ( 5 megawatts) over a network of pipes carrying water at 800 psi. These systems were replaced by cheaper and more versatile electrical systems, but by the end of the 19th century, city planners and financiers were well aware of the benefits, economics, and process of establishing power transmission systems.\n\nIn the early days of electric power usage, widespread transmission of electric power had two obstacles. First, devices requiring different voltages required specialized generators with their own separate lines. Street lights, electric motors in factories, power for streetcars and lights in homes are examples of the diversity of devices with voltages requiring separate systems. Secondly, generators had to be relatively near their loads (a mile or less for low voltage devices). It was known that long distance transmission was possible the higher the voltage was raised, so both problems could be solved if transforming voltages could be cheaply performed from a single universal power line.\n\n Much of early electricity was direct current, which could not easily be increased or decreased in voltage either for long-distance transmission or for sharing a common line to be used with multiple types of electric devices. Companies simply ran different lines for the different classes of loads their inventions required, for example, Charles Brush's New York arc lamp systems required up to 10 kV for many lamps in a series circuit, Edison's incandescent lights used 110 V, streetcars built by Siemens or Sprague required large motors in the 500 volt range, whereas industrial motors in factories used still other voltages. Due to this specialization of lines, and because transmission was so inefficient, it seemed at the time that the industry would develop into what is now known as a distributed generation system with large numbers of small generators located near their loads.\n\nHigh voltage was of interest to early researchers working on the problem of transmission over distance. They knew from elementary electricity principle that the same amount of power could be transferred on a cable by doubling the voltage and halving the current. Due to Joule's Law, they also knew that the capacity of a wire is proportional to the square of the current traveling on it, regardless the voltage, and so by doubling the voltage, the same cable would be capable of transmitting the same amount of power four times the distance.\n\nAt the Paris Exposition of 1878, electric arc lighting had been installed along the Avenue de l'Opera and the Place de l'Opera, using electric Yablochkov arc lamps, powered by Zénobe Gramme alternating current dynamos. Yablochkov candles required high voltage, and it was not long before experimenters reported that the arc lamps could be powered on a circuit. Within a decade scores of cities would have lighting systems using a central power plant that provided electricity to multiple customers via electrical transmission lines. These systems were in direct competition with the dominant gaslight utilities of the period.\n\nThe idea of investing in a central plant and a network to deliver energy produced to customers who pay a recurring fee for service was familiar business model for investors: it was identical to the lucrative gaslight business, or the hydraulic and pneumatic power transmission systems. The only difference was the commodity being delivered was electricity, not gas, and the \"pipes\" used for delivering were more flexible.\n\nThe California Electric Company (now PG&E) in San Francisco in 1879 used two direct current generators from Charles Brush's company to supply multiple customers with power for their arc lamps. This San Francisco system was the first case of a utility selling electricity from a central plant to multiple customers via transmission lines. CEC soon opened a second plant with 4 additional generators. Service charges for light from sundown to midnight was $10 per lamp per week.\n\nGrand Rapids Electric Light & Power Company, established in March 1880 by William T. Powers and others, began operation of the world’s first commercial central station hydroelectric power plant, Saturday, July 24, 1880, getting power from Wolverine Chair and Furniture Company’s water turbine. It operated a 16-light Brush electric dynamo lighting several storefronts in Grand Rapids, Michigan. It is the earliest predecessor of Consumers Energy of Jackson, Michigan. \n\nIn December 1880, Brush Electric Company set up a central station to supply a length of Broadway with arc lighting. By the end of 1881, New York, Boston, Philadelphia, Baltimore, Montreal, Buffalo, San Francisco, Cleveland and other cities had Brush arc lamp systems, producing public light well into the 20th century. By 1893 there were 1500 arc lamps illuminating New York streets.\n\nExtremely bright arc lights were too bright, and with the high voltages and sparking/fire hazard, too dangerous to use indoors. In 1878 inventor Thomas Edison saw a market for a system that could bring electric lighting directly into a customer's business or home, a niche not served by arc lighting systems. After devising a commercially viable incandescent light bulb in 1879, Edison went on to develop the first large scale investor-owned electric illumination \"utility\" in lower Manhattan, eventually serving one square mile with 6 \"jumbo dynamos\" housed at Pearl Street Station. When service began in September 1882, there were 85 customers with 400 light bulbs. Each dynamo produced 100 kW2—enough for 1200 incandescent lights, and transmission was at 110 V via underground conduits. The system cost $300,000 to build with installation of the of underground conduits one of the most expensive parts of the project. Operating expenses exceeded income in the first two years and fire destroyed the plant in 1890. Further, Edison had a three wire system so that either 110 V or 220 V could be supplied to power some motors.\n\nAvailability of large amounts of power from diverse locations would become possible after Charles Parsons' production of turbogenerators beginning 1889. Turbogenerator output quickly jumped from 100 kW to 25 megawatts in two decades. Prior to efficient turbogenerators, hydroelectric projects were a significant source of large amounts of power requiring transmission infrastructure.\n\nWhen George Westinghouse became interested in electricity, he quickly and correctly concluded that Edison's low voltages were too inefficient to be scaled up for transmission needed for large systems. He further understood that long-distance transmission needed high voltage and that inexpensive conversion technology only existed for alternating current. Transformers would play the decisive role in the victory of alternating current over direct current for transmission and distribution systems. In 1876, Pavel Yablochkov patented his mechanism of using induction coils to serve as a step up transformer prior to the Paris Exposition demonstrating his arc lamps. In 1881, Lucien Gaulard and John Dixon Gibbs developed a more efficient device which they dubbed the secondary generator, namely an early transformer provided with 1:1 turn ratio and open magnetic circuit.\nThe first demonstrative long distance (34 km, i.e. 21 mi) AC line was built for the 1884 International Exhibition of Turin, Italy. It was powered by a 2-kV, 130-Hz Siemens & Halske alternator and featured several Gaulard secondary generators with their primary windings connected in series, which fed incandescent lamps. The system proved the feasibility of AC electric power transmission on long distances. After this success, between 1884 and 1885, Hungarian engineers Zipernowsky, Bláthy, and Déri from the Ganz company in Budapest created the efficient \"Z.B.D.\" closed-core coils, as well as the modern electric distribution system. The three had discovered that all former coreless or open-core devices were incapable of regulating voltage, and were therefore impractical. Their joint patent described two versions of a design with no poles: the \"closed-core transformer\" and the \"shell-core transformer\". Ottó Bláthy suggested the use of closed-cores, Károly Zipernowsky the use of shunt connections, and Miksa Déri performed the experiments.\n\nIn the closed-core transformer the iron core is a closed ring around which the two coils are wound. In the shell type transformer, the windings are passed through the core. In both designs, the magnetic flux linking the primary and secondary windings travels almost entirely within the iron core, with no intentional path through air. The core consists of iron strands or sheets. These revolutionary design elements would finally make it technically and economically feasible to provide electric power for lighting in homes, businesses and public spaces. Zipernowsky, Bláthy and Déri also discovered the transformer formula, Vs/Vp = Ns/Np. Electrical and electronic systems the world over rely on the principles of the original Ganz transformers. The inventors are also credited with the first use of the word \"transformer\" to describe a device for altering the EMF of an electric current.\n\nA very first operative AC line was put into service in 1885 in via dei Cerchi, Rome, Italy, for public lighting. It was powered by two Siemens & Halske alternators rated 30 hp (22 kW), 2 kV at 120 Hz and used 200 series-connected Gaulard 2-kV/20-V step-down transformers provided with a closed magnetic circuit, one for each lamp. Few months later it was followed by the first British AC system, which was put into service at the Grosvenor Gallery, London. It also featured Siemens alternators and 2.4-kV/100-V step-down transformers, one per user, with shunt-connected primaries.\n\nThe concept that is the basis of modern transmission using inexpensive step up and step down transformers was first implemented by Westinghouse, William Stanley, Jr. and Franklin Leonard Pope in 1886 in Great Barrington, Massachusetts, resorting also to European technology. In 1888 Westinghouse also licensed Nikola Tesla's induction motor patent giving AC a much needed usable motor. This system was developed into the modern 3-phase system by Mikhail Dolivo-Dobrovolsky and Allgemeine Elektricitäts-Gesellschaft and Charles Eugene Lancelot Brown in Europe, starting in 1889. The simplicity of polyphase generators and motors meant that besides their efficiency they could be manufactured cheaply, compactly and would require little attention to maintain. Simple economics would drive the expensive, bulky and mechanically complex DC dynamos to their ultimate extinction. As it turned out, the deciding factor in the War of Currents was the availability of low cost step up and step down transformers that meant that all customers regardless of their specialized voltage requirements could be served at minimal cost of conversion. This \"universal system\" is today regarded as one of the most influential innovations for the use of electricity.\n\nThe case for alternating current was not clear at the turn of the century and high voltage direct current transmission systems were successfully installed without the benefit of transformers. Rene Thury, who had spent six months at Edison's Menlo Park facility, understood his problem with transmission and was convinced that moving electricity over great distances was possible using direct current. He was familiar with the work of Marcel Deprez, who did early work on high voltage transmission after being inspired by the capability of arc lamp generators to support lights over great distances. Deprez avoided transformers by placing generators and loads in series as arc lamp systems of Charles F. Brush did. Thury developed this idea into the first commercial system for high-voltage DC transmission. Like Brush's dynamos, current is kept constant, and when increasing load demands more pressure, voltage is increased. The Thury System was successfully used on several DC transmission projects from Hydro generators. The first in 1885 was a low voltage system in Bözingen, and the first high voltage system went into service in 1889 in Genoa, Italy, by the \"Acquedotto de Ferrari-Galliera\" company. This system transmitted 630 kW at 14 kV DC over a circuit 120 km long. The largest Thury System was the Lyon Moutiers project that was 230 km in length, eventually delivering 20 megawatts, at 125 kV.\n\nUltimately, the versatility of the Thury system was hampered by the fragility of series distribution, and the lack of a reliable DC conversion technology that would not show up until the 1940s with improvements in mercury arc valves. The AC \"universal system\" won by force of numbers, proliferating systems with transformers both to couple generators to high-voltage transmission lines, and to connect transmission to local distribution circuits. By a suitable choice of utility frequency, both lighting and motor loads could be served. Rotary converters and later mercury-arc valves and other rectifier equipment allowed DC load to be served by local conversion where needed. Even generating stations and loads using different frequencies could also be interconnected using rotary converters. By using common generating plants for every type of load, important economies of scale were achieved, lower overall capital investment was required, load factor on each plant was increased allowing for higher efficiency, allowing for a lower cost of energy to the consumer and increased overall use of electric power.\n\nBy allowing multiple generating plants to be interconnected over a wide area, electricity production cost was reduced. The most efficient available plants could be used to supply the varying loads during the day. Reliability was improved and capital investment cost was reduced, since stand-by generating capacity could be shared over many more customers and a wider geographic area. Remote and low-cost sources of energy, such as hydroelectric power or mine-mouth coal, could be exploited to lower energy production cost.\n\nThe first transmission of three-phase alternating current using high voltage took place in 1891 during the international electricity exhibition in Frankfurt. A 15 kV transmission line connected Lauffen on the Neckar and Frankfurt am Main, on a 175 km long distance.\n\nInitially transmission lines were supported by porcelain pin-and-sleeve insulators similar to those used for telegraphs and telephone lines. However, these had a practical limit of 40 kV. In 1907, the invention of the disc insulator by Harold W. Buck of the Niagara Falls Power Corporation and Edward M. Hewlett of General Electric allowed practical insulators of any length to be constructed for higher voltages.\n\nThe first large scale hydroelectric generators in the USA were installed in 1895 at Niagara Falls and provided electricity to Buffalo, New York, via power transmission lines. A statue of Nikola Tesla stands at, Goat Island Niagara Falls New York, today in tribute to his contributions.\n\nVoltages used for electric power transmission increased throughout the 20th century.\nThe first \"high voltage\" AC power station, rated 4-MW 10-kV 85-Hz, was put into service in 1889 by Sebastian Ziani de Ferranti at Deptford, London. The first electric power transmission line in North America operated at 4000 V. It went online on June 3, 1889, with the lines between the generating station at Willamette Falls in Oregon City, Oregon, and Chapman Square in downtown Portland, Oregon stretching about 13 miles. By 1914 fifty-five transmission systems operating at more than 70,000 V were in service, and the highest voltage then used was 150 kV. The first three-phase alternating current power transmission at 110 kV took place in 1907 between Croton and Grand Rapids, Michigan. Voltages of 100 kV and more were not established technology until around 5 years later, with for example the first 110 kV line in Europe between Lauchhammer and Riesa, Germany, in 1912.\n\nIn the early 1920s the Pit River – Cottonwood – Vaca-Dixon line was built for 220 kV transporting power from hydroelectric plants in the Sierra Nevada to the San Francisco Bay Area, at the same time the Big Creek – Los Angeles lines were upgraded to the same voltage. Both of those systems entered commercial service in 1923. On April 17, 1929 the first 220 kV line in Germany was completed, running from Brauweiler near Cologne, over Kelsterbach near Frankfurt, Rheinau near Mannheim, Ludwigsburg–Hoheneck near Austria. This line comprises the North-South interconnect, at the time one of the world's largest power systems. The masts of this line were designed for eventual upgrade to 380 kV. However the first transmission at 380 kV in Germany was on October 5, 1957 between the substations in Rommerskirchen and Ludwigsburg–Hoheneck.\n\nThe world's first 380 kV power line was built in Sweden, the 952 km Harsprånget – Hallsberg line in 1952. In 1965, the first extra-high-voltage transmission at 735 kV took place on a Hydro-Québec transmission line. In 1982 the first transmission at 1200 kV was in the Soviet Union.\n\nThe rapid industrialization in the 20th century made electrical transmission lines and grids a critical part of the economic infrastructure in most industrialized nations. Interconnection of local generation plants and small distribution networks was greatly spurred by the requirements of World War I, where large electrical generating plants were built by governments to provide power to munitions factories; later these plants were connected to supply civil load through long-distance transmission.\n\nSmall municipal electrical utilities did not necessarily desire to reduce the cost of each unit of electricity sold; to some extent, especially during the period 1880–1890, electrical lighting was considered a luxury product and electric power was not substituted for steam power. Engineers such as Samuel Insull in the United States and Sebastian Z. De Ferranti in the United Kingdom were instrumental in overcoming technical, economic, regulatory and political difficulties in development of long-distance electric power transmission. By introduction of electric power transmission networks, in the city of London the cost of a kilowatt-hour was reduced to one-third in a ten-year period.\n\nIn 1926 electrical networks in the United Kingdom began to be interconnected in the National Grid, initially operating at 132 kV.\n"}
{"id": "1243767", "url": "https://en.wikipedia.org/wiki?curid=1243767", "title": "Horn (anatomy)", "text": "Horn (anatomy)\n\nA horn is a permanent pointed projection on the head of various animals consisting of a covering of keratin and other proteins surrounding a core of live bone. Horns are distinct from antlers, which are not permanent. In mammals, true horns are found mainly among the ruminant artiodactyls, in the families Antilocapridae (pronghorn) and Bovidae (cattle, goats, antelope etc.).\n\nOne pair of horns is usual; however, two or more pairs occur in a few wild species and domesticated breeds of sheep. Polycerate (multi-horned) sheep breeds include the Hebridean, Icelandic, Jacob, Manx Loaghtan, and the Navajo-Churro.\n\nHorns usually have a curved or spiral shape, often with ridges or fluting. In many species only males have horns. Horns start to grow soon after birth, and continue to grow throughout the life of the animal (except in pronghorns, which shed the outer layer annually, but retain the bony core). Partial or deformed horns in livestock are called \"scurs\". Similar growths on other parts of the body are not usually called horns, but spurs, claws or hoofs depending on the part of the body on which they occur.\n\nThe term \"horn\" is also popularly applied to other hard and pointed features attached to the head of animals in various other families:\n\nMany mammal species in various families have tusks, which often serve the same functions as horns, but are in fact oversized teeth. These include the Moschidae (Musk deer, which are ruminants), Suidae (Wild Boars), Proboscidea (Elephants), Monodontidae (Narwhals) and Odobenidae (Walruses).\nPolled animals or \"pollards\" are those of normally-horned (mainly domesticated) species whose horns have been removed, or which have not grown. In some cases such animals have small horny growths in the skin where their horns would be – these are known as \"scurs\".\n\nCutaneous horns are the only examples of horns growing on people. They are most often benign growths and can be removed by a razor.\n\nCases of people growing horns have been historically described, sometimes with mythical status. Researchers have not however discovered photographic evidence of the phenomenon. There are human cadaveric specimens that show outgrowings, but these are instead classified as osteomas or other excrescences.\n\nThe phenomenon of humans with horns has been observed in countries lacking advanced medicine. There are living people, several in China, with cases of cutaneous horns, most common in the elderly.\n\nSome people, notably The Enigma, have horn implants; that is, they have implanted silicone beneath the skin as a form of body modification.\n\nThe erect penis is sometimes referred to in slang use as a \"horn\", but it normally contains no keratin. However, a cutaneous horn can grow \"on\" the penis, which may indicate cancer.\n\nAnimals have a variety of uses for horns and antlers, including defending themselves from predators and fighting members of their own species (\"horn fighting\") for territory, dominance or mating priority. Horns are usually present only in males but in some species, females too may possess horns. It has been theorized by researchers that taller species living in the open are more visible from longer distances and more likely to benefit from horns to defend themselves against predators. Female bovids that are not hidden from predators due to their large size or open savannah like habitat are more likely to bear horns than small or camouflaged species.\n\nIn addition, horns may be used to root in the soil or strip bark from trees. In animal courtship many use horns in displays. For example, the male blue wildebeest reams the bark and branches of trees to impress the female and lure her into his territory. Some animals with true horns use them for cooling. The blood vessels in the bony core allow the horns to function as a radiator.\n\nAfter the death of a horned animal, the keratin may be consumed by the larvae of the Horn Moth.\n\n\n\n\n"}
{"id": "36114202", "url": "https://en.wikipedia.org/wiki?curid=36114202", "title": "Kalla kadal", "text": "Kalla kadal\n\nKallakadal is Malayalam word having literal meaning of 'sea thief'. The word kalla kadal, which is used by local fishermen was adopted by UNESCO. Geographically, it refers to phenomenon when the sea water creeps in during good weather like a thief arriving unannounced. Kallakkadal event is considered as one of the major societal problems along the Indian coasts and these are caused by high swell waves, without any sign in the local winds, sometimes cause severe flooding. \n\nA recent research focused on the causes of Kallakkadal/swell surge has established the link between North Indian Ocean high swell events and the meteorological conditions in the Southern Ocean using a combination of ocean wave observations and numerical model simulations. The study confirms that Kallakkadal events are caused by swells propagating from the Southern Indian Ocean of 30°S, from the region between Africa and Australia. Swell waves are generated by distant weather systems, where wind blows for a duration of time over a large fetch.\nThe long period swell waves (>18 s) seen during Kallakkadal/Swell surge events are generated in the southern Indian ocean by severe low pressure system existed 3-5 days prior to the Kallakkadal events. The quasi-stationary nature of the southern ocean low pressure systems provides strong (~25 ms-1) and long duration (~3 days) surface winds over a large fetch; essential conditions for the generation of long period swells. The intense winds associated with these severe low pressure systems in the Southern Indian Ocean trigger the generation of high waves which propagate to Indian Coastal regions as swells. Furthermore these swells affect coastal areas which are dependent on the local topography, angle of incidence and tidal conditions, cause high wave activity and Kallakkadal along the Indian coastal regions. The study shows that Kallakkadal events along NIO coasts can be effectively monitored and forecasted at least 2 days in advance if the meteorological conditions of the Southern Ocean are properly monitored. INCOIS has been now successfully giving advanced warning to the coastal community about the Kallakkadal events.\nCharacteristics of “Kallakkadal” as follows:\n\n"}
{"id": "2236123", "url": "https://en.wikipedia.org/wiki?curid=2236123", "title": "Kamchatka Current", "text": "Kamchatka Current\n\nThe Kamchatka Current is a cold-water current flowing south-westward from the Bering Strait, along the Siberian Pacific coast and the Kamchatka Peninsula. A portion of this current then becomes the Oyashio Current while the remainder joins the warmer North Pacific Current.\n\n"}
{"id": "203902", "url": "https://en.wikipedia.org/wiki?curid=203902", "title": "Khun Borom", "text": "Khun Borom\n\nKhun Borom (, ) or Khoun Bourôm (, ) is a legendary progenitor of the Tai-speaking peoples, considered by the Lao to be the father of their race.\n\nAccording to the myth of Khoun Borôm, a myth commonly related among Tai-speaking peoples, in ancient times people were wicked and crude. A great deity destroyed them with a flood, leaving only three worthy chiefs who were preserved in heaven to be the founders and guides for a new race of people. The deity sent the three chiefs back to the earth with a buffalo to help them till the land.\n\nThe chiefs and the buffalo arrived in the legendary land of Muang Thèn, located at today's Điện Biên Phủ, Vietnam. Once the land had been prepared for rice cultivation, the buffalo died and a bitter gourd vine grew from his nostril. From the gourds on the vine, the new human race emerged. Relatively dark-skinned aboriginal peoples emerging from gourds cut open with a hot skewer and the lighter skinned Tai peoples emerging from cuts made with a chisel.\n\nThe gods then taught the Tai peoples how to build houses and cultivate rice. They were instructed in proper rituals and behavior, and grew prosperous. As their population grew, they needed aid in governing their relations and resolving disputes. Phagna Thèn, the king of the gods, sent his son, Khoun Borôm, to be the ruler of the Tai people. Khoun Borôm ruled the Tai people for 25 years, teaching them to use new tools and other arts.\n\nAfter this quarter-century span, Khoun Borôm divided the kingdom among his seven sons, giving each one of them a portion of the kingdom to rule.\n\nKhoun Borôm is sometimes erroneously identified, apparently without any evidence at all, as having been Piluoge, a historical ruler of the Nanzhao (Ai Lao) Empire. It is claimed that this Khoun Borôm, ruler of Nanzhao, had nine sons, and seven of them became kings in different kingdoms in \"Lèmthong\":\n\n\nThere were 19 kings after Khoun Lo who ruled Muang Swa. The last one was Khoun Vang, who was then succeeded by:\n\n\nBoth King Mangrai of Chiang Mai and Uthong of Ayutthaya are said to have been descendants of Khum Borom's younger sons.\n\nSome interpreters of the story of Khoun Borôm believe that it describes Tai-speaking peoples arriving in Southeast Asia from China (mythically identified with heaven, from which the Tai chiefs emerge after the flood). The system of dividing and expanding a kingdom in order to provide for the sons of a ruler agrees in general with the apparent organization and succession practices of ancient Tai village groups was called \"mueang\".\n\nScholar David K. Wyatt believes that the Khoun Borôm myth may provide insight into the early history of the Tai people in Southeast Asia. Versions of the Khoun Borôm myth occur as early as 698 CE in Xiang Khouang, and identify Tai-speaking kingdoms that would be formally established years later. This may indicate the early geographical spread of Tai-speaking peoples, and provides a mythological explanation for why modern Tai-speaking peoples are found in such widespread pockets.\n\nLinguistic analysis indicates that the division of the early Tai speakers into the language groups that gave rise to modern Thai, Lao and other languages occurred sometime between the 7th and 11th centuries CE. This split proceeded along geographic lines very similar to the division given in the Khoun Borôm legend.\n\n\n"}
{"id": "4449080", "url": "https://en.wikipedia.org/wiki?curid=4449080", "title": "List of New Jersey hurricanes", "text": "List of New Jersey hurricanes\n\n<onlyinclude>\nThere have been 114 hurricane or tropical storms that affected the U.S. state of New Jersey.</onlyinclude> Due to its location, few hurricanes have hit the state directly, though numerous hurricanes have passed near or through New Jersey in its history. About every 10 years, hurricanes approach the coastline close enough to send waves over barrier islands' dunes and into back bays. According to an estimate by meteorologist George Prouflis, the chances for a direct hit by a hurricane on the Jersey shore each year is 1 in 200.\n\nNew Jersey has seen the remnants of several once-powerful hurricanes, some resulting in heavy damage. Nine storms dropped over of rainfall in the state, including a hurricane in 1940 that interacted with a cold front and dropped of rainfall in Ewan. Numerous hurricanes that remained offshore have each drowned small numbers of swimmers.\n\nMost of the following are tropical cyclones that passed through the state after weakening from their peak.\n\nIn the 19th century, two hurricanes struck the coastline, each in 1804 and in 1821; both caused minor damage. The most significant storm of the century was the Gale of 1878, which produced hurricane-force winds across western New Jersey. The hurricane caused severe damage and 11 deaths.\n<onlyinclude>\n\nHurricane activity was above average during this time period. A hurricane in 1903 hit near Atlantic City, causing heavy damage near the shore. The most severe hurricane in the time period was the 1944 Great Atlantic Hurricane. Though it did not make landfall, it brought strong winds and waves to the coastline, destroying hundreds of homes.\n\nSeveral tropical cyclones affected the state during the time period, though Hurricane Donna was the most severe. Paralleling the coastline offshore, the hurricane caused heavy damage near the coastline from high waves and winds. In addition, Hurricane Belle was predicted to strike the state, though it passed to the east with only minor effects.\n\n\nThe 1980s were a relatively active decade, with 11 tropical cyclones affecting the state. The most notable storm of the decade was Hurricane Gloria in 1985, which was originally predicted to strike the state. The hurricane caused minor damage throughout the state.\n\n\nThirteen tropical cyclones affected New Jersey during the 1990s. The 1991 Perfect Storm eroded beaches severely along the coast, while Hurricane Floyd in 1999 produced severe flooding in northern New Jersey, killing six.\n\n\n\nHurricane Sandy was the most destructive hurricane ever recorded in New Jersey. The fourth-costliest hurricane in U.S. history caused widespread, devastating damage and left millions of New Jersey residents without electricity, some lasting as long as three weeks. \n\nTropical cyclones affect New Jersey the most during the month of September, though the state has experienced tropical cyclones throughout the hurricane season, excluding November. Storms affect the state most in September due to peak warmth in water temperatures. No recorded storm has affected the state between November and May.\n\nMost tropical cyclones that impact New Jersey only cause rainfall or strong waves, though a few have caused deaths in the state, including the following:\n\nThe following storms have caused hurricane-force winds in New Jersey:\n\n"}
{"id": "36815031", "url": "https://en.wikipedia.org/wiki?curid=36815031", "title": "List of Sites of Special Scientific Interest in Flintshire", "text": "List of Sites of Special Scientific Interest in Flintshire\n\nThis is a list of the Sites of Special Scientific Interest (SSSIs) in the Flintshire Area of Search (AoS).\n"}
{"id": "6218889", "url": "https://en.wikipedia.org/wiki?curid=6218889", "title": "List of Sites of Special Scientific Interest in South Perth", "text": "List of Sites of Special Scientific Interest in South Perth\n\nThe following is a list of Sites of Special Scientific Interest in the South Perth Area of Search, in Scotland. For other areas, see List of SSSIs by Area of Search.\n\n"}
{"id": "3381929", "url": "https://en.wikipedia.org/wiki?curid=3381929", "title": "List of reptiles", "text": "List of reptiles\n\nList of reptiles lists the vertebrate class of reptiles by family, spanning three subclasses.\n\n\n\n\nSuperorder Crocodylomorpha\n\n\n\n"}
{"id": "8591462", "url": "https://en.wikipedia.org/wiki?curid=8591462", "title": "List of stars in Crater", "text": "List of stars in Crater\n\nThis is the list of notable stars in the constellation Crater, sorted by decreasing brightness.\n\n"}
{"id": "27600021", "url": "https://en.wikipedia.org/wiki?curid=27600021", "title": "Marine (Scotland) Act 2010", "text": "Marine (Scotland) Act 2010\n\nOn 10 March 2010, Scotland's Marine Bill received Royal Assent, making it the Marine (Scotland) Act 2010.\n\nThe Marine (Scotland) Act is an Act of the Scottish Parliament which provides a framework which will help balance competing demands on Scotland's seas. It introduces a duty to protect and enhance the marine environment and includes measures to help boost economic investment and growth in areas such as marine renewables.\n\nThe main measures include:\n\n\nWildlife organisations, such as the Royal Society for the Protection of Birds, welcomed the new laws.\n\n\n"}
{"id": "7901833", "url": "https://en.wikipedia.org/wiki?curid=7901833", "title": "Ministry of Economy, Energy and Tourism (Bulgaria)", "text": "Ministry of Economy, Energy and Tourism (Bulgaria)\n\nThe Ministry of Economy and Energy (, \"Ministerstvo na ikonomikata i energetikata\") of Bulgaria is the ministry charged with regulating the economy and energy policy of the country. The Ministry of Economy was founded in December 1999 through the merger of the Ministry of Industry and the Ministry of Commerce and Tourism. In August 2005 was formed the current Ministry of Economy and Energy through the merger of the former Ministry of Economy and Ministry of Energy and Energy Resources.\n\nIn August 2005, the Minister of Economy and Energy was Rumen Ovcharov of the Bulgarian Socialist Party, with Deputy Ministers Lachezar Borisov, Valentin Ivanov, Yordan Dimov, Nina Radeva, Korneliya Ninova, Galina Tosheva and Anna Yaneva.\n\nIn March 2012, the Minister of Economy, Energy, and Tourism switched to Delyan Dobrev.\n\n"}
{"id": "54776381", "url": "https://en.wikipedia.org/wiki?curid=54776381", "title": "Ministry of Energy (Pakistan)", "text": "Ministry of Energy (Pakistan)\n\nThe Ministry of Energy, Power division is a Pakistan Government's level ministry created on 4 August 2017 after merging of the Ministry of Petroleum and Natural Resources with the power division of the Ministry of Water and Power (now renamed Ministry of Water Resources), respectively. The ministry has two division - petroleum and power. The Petroleum Division is headed by the Petroleum Secretary and the Power Division is headed by the Power Secretary. It is currently headed by Omar Ayub Khan as of 11 September 2018.\n\n\n\n\n \n"}
{"id": "8159430", "url": "https://en.wikipedia.org/wiki?curid=8159430", "title": "Ministry of Power (India)", "text": "Ministry of Power (India)\n\nThe Ministry of Power is an Indian government ministry. The current Union Minister of State (Independent charge) is R.K. Singh.\nThe ministry is charged with overseeing electricity production and infrastructure development, including generation, transmission, and delivery, as well as maintenance projects. India faces challenges in electrical supply and delivery, and is often unable to meet demand, even in very large cities.\n\nThe ministry acts as a liaison between the central government and state electricity operations, as well as with the private sector. The ministry also oversees rural electrification projects.\n\nThe Ministry of Power became a ministry on July 2, 1992 during the P. V. Narasimha Rao government. Prior to that time it had been a department (the Department of Power) in the Ministry of Power, Coal and Non-Conventional Energy Sources. That ministry was split into the Ministry of Power, Ministry of Coal, and Ministry of Non-Conventional Energy Sources (renamed the Ministry of New and Renewable Energy in 2006).\n\nIn 2012, the Ministry of Power inaugurated the \"Smart Grid\" project in Puducherry.\n\n"}
{"id": "10001676", "url": "https://en.wikipedia.org/wiki?curid=10001676", "title": "Multi-Application Survivable Tether", "text": "Multi-Application Survivable Tether\n\nThe Multi-Application Survivable Tether (MAST) experiment was an in-space investigation designed to use CubeSat spacecraft connected by tethers to better understand the survivability of tethers in space. It was launched as a secondary payload on a Dnepr rocket on 17 April 2007 into a 98°, 647 x 782 km orbit. The MAST payload incorporated three picosatellites, named \"Ralph,\" \"Ted,\" and \"Gadget,\" which were intended to separate and deploy a tether. The experiment hardware was designed under a NASA Small Business Technology Transfer (STTR) collaboration between Tethers Unlimited, Inc. (TUI) and Stanford University, with TUI developing the tether, tether deployer, tether inspection subsystem, satellite avionics, and software, and Stanford students developing the satellite structures and assisting with the avionics design.\n\nThe experiment is currently on-orbit. After launch, as of 25 April 2007, TUI had made contact with the \"Gadget\" picosatellite, but not with \"Ted\", the tether-deployer picosatellite, or \"Ralph,\" the end mass.\n\nThe MAST experiment consists of three CubeSats (3U) launched together as a stack. The entire stack was about the size of a loaf of bread.\nThe middle satellite in the stack, called \"Gadget\", is the tether inspector. Gadget was designed to slowly crawl up and down the tether after deployment, acquiring images as it moves. As of 9 May 2007, the MAST team has downloaded over 1 MB of data from Gadget. Gadget's GPS receiver has acquired an almanac from the GPS satellites, but apparently has not yet achieved a trajectory solution.\n\"Ted\", the tether deployer satellite, is at one end of the stack. Researchers were unable to establish contact with Ted, and remain uncertain of its status.\n\"Ralph\" is at the other end of the stack, and is described as simply a \"tether endmass\". Its design did include a radio, but the groundstation has not received any signals from Ralph. They think Ralph's battery charge has dropped below the level needed to sustain radio operation.\n\nThe experimenter team made contact with the \"Gadget\" picosatellite, but not with \"Ted\", the tether-deployer pico satellite, because the Ted pico satellite was powered only by a primary battery, which had depleted by the time the team gained access to the ground station. While the system was designed so that the satellites would separate even if communications were not established to the tether deployer, the system did not fully deploy. Radar measurements show the tether initially deployed just 1 meter, The mission experienced communications challenges due to limited availability of the ground station, which resulted in the team establishing contact with only one of the three pico satellites. The team operated the \"Gadget\" pico satellite for nearly two months before terminating the experiment.\n\n"}
{"id": "25070974", "url": "https://en.wikipedia.org/wiki?curid=25070974", "title": "Nyatoh", "text": "Nyatoh\n\nNyatoh is a trade name for wood of a number of hardwood species of the genera \"Palaquium\" and \"Payena\" growing in rainforest environments in southeast Asia, especially in Indonesia and the Philippines. Nyatoh wood is reddish and most species are easy to work with and takes to stain and polish well. It has a tight straight grain that resembles cherry wood. The surface is dark brown/red in color.\n\nNyatoh is generally perceived as a sustainable resource. However, several species within the related genera of \"Palaquium\" and \"Payena\", and some \"Palaquium\" species, are on the IUCN Red List due to overexploitation and alarming reductions in their habitats.\n\nThe harvesting and sales of nyatoh has been criticized by some environmental groups, who have won agreements in the United States to not resell the wood from various furniture and home-improvement chains.\n"}
{"id": "53048910", "url": "https://en.wikipedia.org/wiki?curid=53048910", "title": "P Leonis", "text": "P Leonis\n\nThe Bayer designation p Leonis (p Leo) is shared by five star systems in the constellation Leo:\n\nNot to be confused with:\n"}
{"id": "51767165", "url": "https://en.wikipedia.org/wiki?curid=51767165", "title": "Plouto (Oceanid)", "text": "Plouto (Oceanid)\n\nIn Greek mythology, Plouto or Pluto (Πλουτώ \"Wealth\") was, according to the late 8th–early 7th century BC Greek poet Hesiod, and the probably nearly as old \"Homeric Hymn 2 to Demeter\", one of the many Oceanid daughters of Oceanus and Tethys. Hesiod calls her \"soft eyed\", and the \"Homeric Hymn\" has her as one of the \"deep-bosomed daughters of Oceanus\" who were the playmates of Persephone when she was abducted by Hades. She may be the same with another Pluto, the mother of Tantalus.\n\n"}
{"id": "12035366", "url": "https://en.wikipedia.org/wiki?curid=12035366", "title": "Podilski Tovtry National Nature Park", "text": "Podilski Tovtry National Nature Park\n\nNational Nature Park \"Podilski Tovtry\" () is a national park, located in the Horodok, Kamianets-Podilskyi, and the Chemerivtsi Raions (districts) of Khmelnytskyi Oblast (province) in southern region of the western Ukraine. It is the biggest nature conservation area in Ukraine.\n\nThe National Environmental Park \"Podilski Tovtry\" was created on the decree #476/96 of the President of Ukraine, then Leonid Kuchma, on July 27, 1996, in order to maintain and protect the natural landscape of the Podillia region. The park is a nature-conservational, recreational, culturally enlightening, and scientifically researching institution of national importance. According to the Law of Ukraine of September 21, 2000 #1989-III, the National nature park \"Podilski Tovtry\" belongs to the National Ecological Network of Ukraine of the Podillya region (Podillya).\n\nThe park's area covers 2,613.16 km² (645,725.9 acre). There are 1,700 different types of flora, 60 of which are included in the Red Book of Ukraine. Also there are 217 different types of fauna, 29 of which are included in the Red Book of Ukraine as well. In total there are 127 objects of nature conservation which contain about 3,000 different types, forms, and sorts of plants.\n\nThe park is divided into several functional zones: sacred zone, regulated recreation zone, stationary recreation zone, and managed zone. In those zones are located 21 stationary recreation institutions (such as sanatoriums, profilactoriums, tourist resorts, holiday homes, etc.), some 160 industrial companies, collective and individual farming that cause harm to nature. Therefore, the main goal of the National nature park is the preservation of natural diversity, creation of the organized zones of recreation and wellness.\n\nThe parks natural borders serve Zbruch River to the west (south from the Sataniv town), Dniester to the south towards the mouth of Ushytsia River, administrative borders of the Novo Ushytsia and Dunaivtsi Raions to the east, the Dymytrov, Ordzhonikidze, Vatutin collective farms (Horodok Raion) to the north.\n\nThe park includes some 14% of state forest fund, over 13% of collective farm forests, and other territories of region. The total forest fund of the park accounts for some .\n\n\n\n\n"}
{"id": "712617", "url": "https://en.wikipedia.org/wiki?curid=712617", "title": "Power rating", "text": "Power rating\n\nIn electrical engineering and mechanical engineering, the power rating of equipment is the highest power input allowed to flow through particular equipment. According to the particular discipline, the term \"power\" may refer to the electrical or mechanical power. A power rating can also involve average and maximum power, which may vary depending on the kind of equipment and its application.\n\nPower rating limits are usually set as a guideline by the manufacturers, protecting the equipment and simplifying the design of larger systems, by providing a level of operation under which the equipment will not be damaged while allowing for a certain safety margin.\n\nIn equipment which primarily dissipate electric power or convert it into mechanical power, such as resistors, and speakers, the power rating given is usually the maximum power that can be safely dissipated by the equipment. The usual reason for this limit is heat, although in certain electromechanical devices, particularly speakers, it is to prevent mechanical damage. When heat is the limiting factor, the power rating is easily calculated. First, the amount of heat that can be safely dissipated by the device, formula_1, must be calculated. This is related to the maximum safe operating temperature, the ambient temperature or temperature range in which the device will be operated, and the method of cooling. If formula_2 is the maximum safe operating temperature of the device, formula_3 is the ambient temperature, and formula_4 is the total thermal resistance between the device and ambient, then the maximum heat dissipation is given by\nIf all power in a device is dissipated as heat, then this is also the power rating.\n\nEquipment is generally rated by the power they will deliver, for example, at the shaft of an electric or hydraulic motor. The power input to the equipment will be greater owing to the less than 100% efficiency of the device. Efficiency of a device is often defined as the ratio of output power to the sum of output power and losses. In some types of equipment it is possible to measure or calculate losses directly. This allows efficiency to be calculated with greater precision than the quotient of input power over output power, where relatively small measurement uncertainty will greatly affect the resulting calculated efficiency.\n\nIn devices that primarily convert between different forms of electric power, such as transformers, or transport it from one location to another, such as transmission lines, the power rating almost always refers to the maximum power flow through the device, not dissipation within it. The usual reason for the limit is heat, and the maximum heat dissipation is calculated as above.\n\nPower ratings are usually given in watts for real power and volt-amperes for apparent power, although for devices intended for use in large power systems, both may be given in a per-unit system. Cables are usually rated by giving their maximum voltage and their ampacity. As the power rating depends on the method of cooling, different ratings may be specified for air cooling, water cooling, etc.\n\nFor AC-operated devices (e.g. coaxial cable, loudspeakers), there may be even be two power ratings, a maximum (peak) power rating and an average power rating. For such devices, the peak power rating usually specifies the low frequency or pulse energy, while the average power rating limits high-frequency operation. Average power calculation rating depends on some assumptions how the device is going to be used. For example, the EIA rating method for loudspeakers uses a shaped noise signal that simulates music and allows peak excursion of 6 dB, so an EIA rating of 50 Watts corresponds to 200 Watts peak rating.\n\n\"Maximum continuous rating (MCR)\" is defined as the maximum output (MW) that an electric power generating station is capable of producing continuously under normal conditions over a year. Under ideal conditions, the actual output could be higher than the MCR.\n\nWithin shipping, ships usually operates at the nominal continuous rating (NCR) which is 85% of the 90% of MCR. The 90% MCR is usually the contractual output for which the propeller is designed. Thus, the usual output at which ships are operated is around 75% to 77% of MCR.\n\nIn some fields of engineering, even a more complex set of power ratings is used. For example, helicopter engines are rated for continuous power (which does not have a time constraint), takeoff and hover power rating (defined as half to one hour operation), maximum contingency power (which can be sustained for two-three minutes), and emergency (half a minute) power rating.\nFor electrical motors, a similar kind of information is conveyed by the \"service factor\", which is a multiplier that, when applied to the rated output power, gives the power level a motor can sustain for shorter periods of time. The service factor is typically in the 1.15-1.4 range, with the figure being lower for higher-power motors. For every hour of operation at the service-factor-adjusted power rating, a motor loses two to three hours of life at nominal power, i.e. its service life is reduced to less than half for continued operation at this level. The service factor is defined in the ANSI/NEMA MG 1 standard, and is generally used in the United States. There is no IEC standard for the service factor.\n\nExceeding the power rating of a device by more than the margin of safety set by the manufacturer usually does damage to the device by causing its operating temperature to exceed safe levels. In semiconductors, irreparable damage can occur very quickly. Exceeding the power rating of most devices for a very short period of time is not harmful, although doing so regularly can sometimes cause cumulative damage.\n\nPower ratings for electrical apparatus and transmission lines are a function of the duration of the proposed load and the ambient temperature; a transmission line or transformer, for example, can carry significantly more load in cold weather than in hot weather. Momentary overloads, causing high temperatures and deterioration of insulation, may be considered an acceptable trade-off in emergency situations. The power rating of switching devices varies depending on the circuit voltage as well as the current. In certain aerospace or military applications, a device may carry a much higher rating than would be accepted in devices intended to operate for long service life.\n\nAudio amplifier power ratings are typically established by driving the device under test to the onset of clipping, to a predetermined distortion level, variable per manufacturer or per product line. Driving an amplifier to 1% distortion levels will yield a higher rating than driving it to 0.01% distortion levels. Similarly, testing an amplifier at a single mid-range frequency, or testing just one channel of a two-channel amplifier, will yield a higher rating than if it is tested throughout its intended frequency range with both channels working. Manufacturers can use these methods to market amplifiers whose published maximum power output includes some amount of clipping in order to show higher numbers.\n\nFor instance, the Federal Trade Commission (FTC) established an amplifier rating system in which the device is tested with both channels driven throughout its advertised frequency range, at no more than its published distortion level. The Electronic Industries Association (EIA) rating system, however, determines amplifier power by measuring a single channel at 1,000 Hz, with a 1% distortion level—1% clipping. Using the EIA method rates an amplifier 10 to 20% higher than the FTC method.\n\nThe nominal power of a photovoltaic module is determined by measuring current and voltage while varying resistance under defined illumination. The conditions are specified in standards such as IEC 61215, IEC 61646 and UL 1703; specifically the light intensity is 1000 W/m, with a spectrum similar to sunlight hitting the earth's surface at latitude 35° N in the summer (airmass 1.5) and temperature of the cells at 25 °C. The power is measured while varying the resistive load on the module between open and closed circuit.\n\nThe maximum power measured is the nominal power of the module in Watts. Colloquially, this is also written as \"W\"; this format is colloquial as it is outside the standard by adding suffixes to standardized units. The nominal power divided by the light power that falls on the module (area x 1000 W/m) is the \"efficiency\".\n\n"}
{"id": "15354848", "url": "https://en.wikipedia.org/wiki?curid=15354848", "title": "Radio Research Laboratory (Harvard)", "text": "Radio Research Laboratory (Harvard)\n\nThe Radio Research Laboratory (RRL), located on the campus of Harvard University, was an 800-person secret research laboratory during World War II. Under the U.S. Office of Scientific Research and Development (OSRD), it was a spinoff of the Radiation Laboratory (Rad Lab) at MIT, and set up to develop electronic countermeasures to enemy radars and communications, as well as electronic counter-countermeasures (ECCM) to circumvent enemy ECM. The RRL was directed by Frederick E. Terman and operated between 1941 and 1945.\n\nThe RRL was engaged in both analysis and hardware development. They made significant contributions to the basic understanding of methods, theories, and circuits at very-high and ultra-high frequencies for radio systems, particularly in signals intelligence gear and statistical communications techniques. However, unlike the Rad Lab, the RRL never released significant details on its accomplishments; ECM and ECCM have always been closely guarded secrets by all nations.\n\nThe RRL conducted considerable work on chaff, light-weight aluminum strips dropped in clouds from aircraft to confuse enemy radars. Fred L. Whipple, an astronomer, made detailed analytical studies of this and devised a formula giving radar cross-section at a given wavelength per kilogram of chaff.\n\nAn example of RRL hardware development was Tuba (aka 'Project Tuba'), a giant ECM system generating continuous 80-kW signals in the range of 300-600 MHz to jam German Lichtenstein radars. The power tube (called Resnatron) for Tuba was developed at the RRL by David H. Shone and Lauritsen C. Marshall. Tuba used a horn antenna built of mesh wire 150 feet long and driven through 22- by 6- inch waveguides, possibly the largest ever built. Tuba was placed in operation in mid-1944 on the south coast of England. The radiated energy was such that it lighted fluorescent bulbs a mile away and jammed radars throughout Europe.\n\nThe RRL staff did document some of the general theory that they developed. After the war, this was released in a two-volume publication \"Very High-Frequency Techniques\", edited by Herbert J. Reich (McGraw-Hill, 1947).\n\n"}
{"id": "337279", "url": "https://en.wikipedia.org/wiki?curid=337279", "title": "Self-ionization of water", "text": "Self-ionization of water\n\nThe self-ionization of water (also autoionization of water, and autodissociation of water) is an ionization reaction in pure water or in an aqueous solution, in which a water molecule, HO, deprotonates (loses the nucleus of one of its hydrogen atoms) to become a hydroxide ion, OH. The hydrogen nucleus, H, immediately protonates another water molecule to form hydronium, HO. It is an example of autoprotolysis, and exemplifies the amphoteric nature of water.\n\nChemically pure water has an electrical conductivity of 0.055 µS/cm. According to the theories of Svante Arrhenius, this must be due to the presence of ions. The ions are produced by the self-ionization reaction\nThis equilibrium applies to pure water and any aqueous solution.\n\nExpressed with chemical activities , instead of concentrations, the thermodynamic equilibrium constant for the water ionization reaction is:\n\nwhich is numerically equal to the more traditional thermodynamic equilibrium constant written as:\n\nunder the assumption that the sum of the chemical potentials of H and HO is formally equal to twice the chemical potential of HO at the same temperature and pressure.\n\nBecause most acid–base solutions are typically very dilute, the activity of water is generally approximated as being equal to unity, which allows the ionic product of water to be expressed as:\n\nIn dilute aqueous solutions, the activities of the solute particles are approximately equal to their concentrations. Thus, the \"ionization constant\", \"dissociation constant\", \"self-ionization constant\", or \"ionic product\" of water, symbolized by \"K\", may be given by:\nwhere [HO] is the molarity (≈molar concentration) of hydrogen or hydronium ion, and [OH] is the concentration of hydroxide ion. When the equilibrium constant is written as a product of concentrations (as opposed to activities) it is necessary to make corrections to the value of formula_5 depending on ionic strength and other factors (see below).\n\nAt 25 °C and zero ionic strength, \"K\" is equal to . Note that as with all equilibrium constants, the result is dimensionless because the concentration is in fact a concentration relative to the standard state, which for H and OH are both defined to be 1 molal (or molar). For most practical purposes, the molal and molar concentrations are equal near the ambient temperature and pressure. The molal concentration scale results in concentration values which account for density changes with temperature or pressure changes; therefore, it is the scale used in precise or nonambient applications, e.g., for seawater, or at elevated temperatures, like those in thermal power plants.\n\nWe can also define p\"K\" formula_6 −log \"K\" (which is approximately 14 at 25 °C). This is analogous to the notations pH and p\"K\" for an acid dissociation constant, where the symbol p denotes a cologarithm. The logarithmic form of the equilibrium constant equation is p\"K\" = pH + pOH.\n\nThe dependence of the water ionization on temperature and pressure has been investigated thoroughly. The value of p\"K\" decreases as temperature increases from the melting point of ice to a minimum at c. 250 °C, after which it increases up to the critical point of water c. 374 °C. It decreases with increasing pressure.\n\nWith electrolyte solutions, the value of p\"K\" is dependent on ionic strength of the electrolyte. Values for sodium chloride are typical for a 1:1 electrolyte. With 1:2 electrolytes, MX, p\"K\" decreases with increasing ionic strength.\n\nThe value of \"K\" is usually of interest in the liquid phase. Example values for superheated steam (gas) and supercritical water fluid are given in the table.\n\nHeavy water, DO, self-ionizes less than normal water, HO;\nThis is attributed to oxygen forming a slightly stronger bond to deuterium because the larger mass of deuterium results in a lower zero-point energy, a quantum mechanical effect analogous to the kinetic isotope effect.\n\nExpressed with activities \"a\", instead of concentrations, the thermodynamic equilibrium constant for the heavy water ionization reaction is:\n\nAssuming the activity of the DO to be 1, and assuming that the activities of the DO and OD are closely approximated by their concentrations\nThe following table compares the values of p\"K\" for HO and DO.\n\nIn water–heavy water mixtures equilibria several species are involved: HO, HDO, DO, HO, DO, HDO, HDO, HO, DO.\n\nThe rate of reaction for the ionization reaction\ndepends on the activation energy, Δ\"E\". According to the Boltzmann distribution the proportion of water molecules that have sufficient energy, due to thermal population, is given by\n\nwhere \"k\" is the Boltzmann constant. Thus some dissociation can occur because sufficient thermal energy is available. The following sequence of events has been proposed on the basis of electric field fluctuations in liquid water. Random fluctuations in molecular motions occasionally (about once every 10 hours per water molecule) produce an electric field strong enough to break an oxygen–hydrogen bond, resulting in a hydroxide (OH) and hydronium ion (HO); the hydrogen nucleus of the hydronium ion travels along water molecules by the Grotthuss mechanism and a change in the hydrogen bond network in the solvent isolates the two ions, which are stabilized by solvation. Within 1 picosecond, however, a second reorganization of the hydrogen bond network allows rapid proton transfer down the electric potential difference and subsequent recombination of the ions. This timescale is consistent with the time it takes for hydrogen bonds to reorientate themselves in water.\n\nThe inverse recombination reaction\nis among the fastest chemical reactions known, with a reaction rate constant of at room temperature. Such a rapid rate is characteristic of a diffusion-controlled reaction, in which the rate is limited by the speed of molecular diffusion.\n\nWater molecules dissociate into equal amounts of HO and OH, so their concentrations are equal to at 25 °C. A solution in which the HO and OH concentrations equal each other is considered a neutral solution. In general, the pH of the neutral point is numerically equal to p\"K\".\n\nPure water is neutral, but most water samples contain impurities. If an impurity is an acid or base, this will affect the concentrations of hydronium ion and hydroxide ion. Water samples that are exposed to air will absorb some acidic carbon dioxide to form carbonic acid (HCO) and the concentration of HO will increase due to the reaction HCO + HO = HCO + HO. The concentration of OH will decrease in such a way that the product [HO][OH] remains constant for fixed temperature and pressure. Thus these water samples will be slightly acidic. If a pH of exactly 7.0 is required, it must be maintained with an appropriate buffer solution.\n\n\n"}
{"id": "44519182", "url": "https://en.wikipedia.org/wiki?curid=44519182", "title": "Temperate rainforests of the Russian Far East", "text": "Temperate rainforests of the Russian Far East\n\nThe temperate rainforests of the Russian Far East are found within the Russian states of Primorsky Krai and Khabarovski Krai and contains the Sikhote-Alin mountain range. Found within the Russian Federation, this area is one of the most productive and diverse forests in the world and also contains one of the highest endangered species densities on Earth. While most temperate rainforests around the world have retained only a fraction of their historical range, these forests maintain the majority of their former range and almost all of their historical biodiversity. The region is also notable for having what has become the last remaining large tract of viable habitat for the critically endangered Amur tiger and Amur leopard.\n\nThe Russian Federation contains one sixth of the world’s land mass and around half of the country’s territory is what could be considered continuous wilderness, with large expanses of boreal forests providing relatively untouched habitat for an extremely wide diversity of species. The temperate rainforests are located at the intersection of the Pacific tectonic plate and the Eurasian continent’s plate in far southeastern Russia, forming an area with a unique set of characteristics seen no where else in the world. During the last glacial maximum, the area was not glaciated, allowing for the development of a complex ecosystem containing species with origins in Siberia’s boreal forest and Manchuria’s subtropical forests. Historically these forests ranged from the southeastern Pacific coast of Russia, North Korea, and into northern China, however large amounts of human development (in China especially) has limited the forest to its current range in the Russian Far East. In 2001, UNESCO recognized a 1.5 million hectare area of forest in the central part of the Sikhote-Alin mountains as a World Heritage Site in Russia, citing the area as one of the most unique and valuable areas of intact forest in the world \n\nThese forests are found dispersed throughout the Sikhote-Alin mountain range, which extend over 1000 km along the west coast of the Sea of Japan. The mountains have a maximum height of 1850 m, while the average elevation within the range is around 1000 m. The major rivers include the Ussuri, which acts as the western border with China and empties into the Amur river, which in turn flows northeast to empties into the Sea of Okhotsk. The Bikin and Iman rivers act as tributaries to the Ussuri, flowing west from the Sikhote-Alin mountain range. \n\nKnown as the \"Ussuri taiga,\" this region of Russia has long, cold winters and fairly mild summers to go along with a mean precipitation of 800–1000 mm per year. During the summer and fall, a monsoonal influence brings tropical storms and typhoons coming from the southeast, resulting in substantial rainfall. Fog occurs as a result of difference in temperatures between the continent and the ocean, resulting in more moderate temperatures, higher humidities; helping to prevent substantial water loss from occurring due to evapotranspiration. Continental winds coming in from Siberia bring cooler, dry air during winter, resulting in lighter precipitation (30–50 cm of snow). Winters can be long and bitterly cold with January mean temperatures ranging from -15 to -20 °C and snow covering the forest floor from October to April. The driest parts of the year are from April to June and September to October, which also happen to coincide with the greatest threat of forest fire.\n\nThe forests fall in the transition zone between two biomes: the southern Asian hardwood forest and the northern coniferous forest. There have been around 2500 species of vascular plants and 300 vertebrate species recorded in Primorsky Krai.\n\nThe forest is a mix of broadleaf and conifer, with the forest type becoming more conifer dominated at higher elevations and more broad-leaf-conifer mixed at the lower elevations and within valleys. The most common species include the Korean pine (\"Pinus koraiensis\") and needle fir (\"Abies holophylla\") at the lowest elevations and coastlines. Jezo spruce (\"Picea jezoensis\") and Manchurian fir (\"Abies nephrolepis\") are common species to be found from 700–1400 meters. Other tree species include Mongolian oak (\"Quercus mongolica\"), white birch (\"Betula platyphylla\"), Scots pine (\"Pinus sylvestris\"), trembling aspen (\"Populus tremula\"), Dahurian birch (\"Betula davurica\"), and Gmelin larch (\"Larix gemlinii\"), a deciduous evergreen tree common throughout, but dominant in the northern reaches of the forest\nThe Amur region of Russia holds the last remaining habitats of the critically endangered Amur tiger, Amur leopard, and Manchurian sika deer. It has been estimated that there are less the 600 tigers. and around 40 leopards left in the wild. The area also contains populations of Asiatic black bears and Kamchatka brown bears, as the Russian Far East is the only place in the world where tigers, brown bears, and leopards coexist. This region also happens to be some of the last of habitat of the Blakiston’s fish owl (Bubo blakistoni); along with being the world’s largest owl, it is unique in the way that it eats fish (primarily Masu salmon) and relies on old growth forests along river banks to feed, nest, and breed. Common ungulates include red deer, roe deer, wild boar, Manchurian moose, and musk deer.\n\nFire and logging constitute the primary disturbances in this area, both of which are occurring at an increasing frequency. Illegal logging practices, along with a political climate more favorable to multinational logging corporations has drastically increased the amount of timber taken per year. The occurrence of Korean pine in dominant forests has been declining as more common and more intense wildfires start to take their toll on the species, limiting its ability to recover and changing areas that were once dominated by old growth Korean pine to Mongolian oak and birch forests. Above 700–800 meters in altitude, the forest type transitions from broadleaf to coniferous, dominated by Jezo spruce and Manchurian fir. The further to the north, the altitudinal gradient decreases with the increase in latitude, resulting in coniferous forests at sea level at 47° N latitude.\n\nUnder the USSR control, conservation strategies focused on the creation and a network of strictly protected nature reserves, called zapovednik and partially protected wildlife refuges, called zakaznik. These conservation strategies were largely successful and well maintained, as 85 zapovedniks and 26 national parks remained within Russian control after the fall of the Soviet Union in December 1991. Following the break up, a period of economic hardship has ensued in Russia, resulting in conservation funding cuts and increases in illegal logging and poaching, casting serious doubt on the viabilities of the protected areas and the many rare and endemic species that they contain.\n\nConservation policies have been falling from federal control down to a regional level, which creates funding and enforcement challenges, but also allows for a more grass-roots, bottom up approach to conservation, which despite the challenges has seen some success. Much of the recent funding for conservation studies and implementation of legislation comes from donors and organizations in the United States, where investing in conserving the biodiversity in the Russian Far East is seen as a good economic investment. At a species level, Korean pine is a highly valued type of timber, and although the logging of the species is restricted, illegal logging practices thrive due to a lack of enforcement. It should be noted that Korean pine forests also receive the most interest when it comes to conservationists, as they have been found to have the highest densities of tigers and their prey. Because of this, it appears that the longevity of the temperate rain forests in the Russian Far East will most likely depend on the success of the habitat conservation efforts for the endangered species found there, particularly those for the Amur tiger.\n\n"}
{"id": "57546169", "url": "https://en.wikipedia.org/wiki?curid=57546169", "title": "Thomas Pearce (priest)", "text": "Thomas Pearce (priest)\n\nThomas Pearce (1820–1885) was an English clergyman, known under the pseudonym \"Idstone\" as an author on dogs.\n\nHe was born at Hatford, the son of Francis Joseph Pearce, a clergyman who died when he was aged two, and his wife Mary Ann Rickards. He then moved to be with her, in Leicestershire.\n\nThomas Pearce matriculated at Lincoln College, Oxford in June 1838, as did his elder brother Francis Joseph at Exeter College. The family moved to Oxford at this period. Thomas graduated B.A. in 1843, and M.A. in 1849.\n\nOrdained in the Church of England in 1845, Pearce then held a number of curacies in southern England. He was in 1845 at Goldenhill; in 1847 at Highcliffe; in 1851 at Waterperry; and in 1852 in Sparsholt, which was at that time in Berkshire.\n\nPearce became vicar of Morden, Dorset in 1853. He was also rector of Charborough from 1871. He died on 24 September 1885. His interests including breeding champion setters, shooting snipe, and collecting birds.\n\nPearce wrote articles for \"The Field\", from 1865. His works included:\n\n\nThe compilation \"The Dogs of the British Islands\" (1866) was mostly from Pearce's work. The pseudonym Idstone, from a hamlet now in Oxfordshire, is connected to Pearce family properties that lay in the civil parish of Ashbury, Oxfordshire. It was a pen name, published in lists of related sporting authors.\n\nPearce married on 14 July 1852 Fanny Georgina Blake. Her father Charles Henry Blake (1794–1872), born in Calcutta and in early life an indigo planter, became a property speculator in London. They had three sons and a daughter:\n\n"}
{"id": "7525878", "url": "https://en.wikipedia.org/wiki?curid=7525878", "title": "Titanowodginite", "text": "Titanowodginite\n\nTitanowodginite is a mineral with the chemical formula MnTiTaO. Titanowodginite has a Mohs hardness of 5.5 and a vitreous luster. It is an iridescent dark brown to black crystal that commonly forms in a matrix of smoky quartz or white beryl in a complex zoned pegmatite.\n\nIt was first described in 1992 for an occurrence in the Tanco Mine located in southern Manitoba, Canada. It was named because it is a titanium bearing member of the wodginite group.\n"}
{"id": "14566980", "url": "https://en.wikipedia.org/wiki?curid=14566980", "title": "Water sky", "text": "Water sky\n\nWater sky is a phenomenon that is closely related to ice blink. It forms in regions with large areas of ice and low-lying clouds and so is limited mostly to the extreme northern and southern sections of earth, in Antarctica and in the Arctic.\n\nWhen light hits the blue oceans or seas, some of it bounces back and enables the observer to physically see the water. However, some of the light also is reflected back up on to the bottoms of low-lying clouds and causes a dark spot to appear underneath some clouds. These clouds may be visible when the seas are not and can show alert and knowledgeable travelers the general direction of water. The dark clouds over open water have long been used by polar explorers and scientists to navigate in sea ice. For example, Arctic explorer Fridtjof Nansen and his assistant Hjalmar Johansen used the phenomenon to find lanes of water in their failed expedition to the North Pole as did Bernacchi and Sir Douglas Mawson in the Antarctica.\n\n\n"}
{"id": "6592617", "url": "https://en.wikipedia.org/wiki?curid=6592617", "title": "William Napier (astronomer)", "text": "William Napier (astronomer)\n\nWilliam M. Napier or Bill Napier (born 29 June 1940 in Perth, Scotland) is the author of five high tech thriller novels and a number of nonfiction science books.\n\nHe received his Bachelor of Science degree in 1963 and his Doctor of Philosophy degree in 1966, both from the University of Glasgow.\n\nNapier is a professional astronomer who has worked at the Royal Observatory in Edinburgh, the University of Oxford and Armagh Observatory. He is currently an honorary professor of Astrobiology in the Center for Astrobiology at Cardiff University, which describes him as \"a leading figure in the dynamics and physics of comets, and a pioneer of the modern versions of catastrophism.\" And honorary professor at the Buckingham Centre for Astrobiology, University of Buckingham, which describes him as, \"a pioneer of modern studies of the impact hazard due to asteroids and comets,\" and also as having, \"carried out an investigation of long-running claims of anomalous QSO/galaxy associations.\" His research work focuses on comets and cosmology. The result of his collaboration with Victor Clube and others on the role of giant comets in Earth history is known as \"coherent catastrophism.\"\n\nAccording to Napier, 13,000 years ago the earth was affected by a major, rapid cooling event that caused the extinction of a large number of species and a major disruption of paleoindian cultures. Previously thought to have been caused by an enormous asteroid crashing into the planet, Professor presented evidence that the cooling even was caused collision with \"a dense trail of material from a large disintegrating comet.\"\n\n\n\n"}
{"id": "7827316", "url": "https://en.wikipedia.org/wiki?curid=7827316", "title": "World Administrative Radio Conference", "text": "World Administrative Radio Conference\n\nThe World Administrative Radio Conference (WARC) was a technical conference of the International Telecommunication Union (ITU) where delegates from member nations of the ITU met to revise or amend the entire international Radio Regulations pertaining to all telecommunication services throughout the world. The conference was held in Geneva, Switzerland, with preparatory conferences held in Panama City, Panama.\n\nIn 1992 at an \"Additional Plenipotentiary Conference\" in Geneva the ITU was restructured and as a result from 1993 the conference became known as the World Radiocommunication Conference or WRC. \n\n\n"}
