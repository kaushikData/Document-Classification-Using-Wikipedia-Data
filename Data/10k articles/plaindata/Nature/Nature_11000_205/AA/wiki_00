{"id": "41408217", "url": "https://en.wikipedia.org/wiki?curid=41408217", "title": "A Gardener's Year", "text": "A Gardener's Year\n\nA Gardener's Year is a 1905 non-fiction book from H. Rider Haggard.\n\n"}
{"id": "6300090", "url": "https://en.wikipedia.org/wiki?curid=6300090", "title": "Absorption heat pump", "text": "Absorption heat pump\n\nAn absorption heat pump is an air-source heat pump driven not by electricity, but by a heat source such as solar-heated water, or geothermal-heated water. Absorption refrigerators also work on the same principle, but are not reversible and cannot serve as a heat source.\nSingle, double or triple iterative absorption cooling cycles are used in different solar-thermal-cooling system designs. The more cycles, the more efficient they are.\n\nIn the late 19th century, the most common phase change refrigerant material for absorption cooling was a solution of ammonia and water. Today, the combination of lithium bromide and water is also in common use. One end of the system of expansion/condensation pipes is heated, and the other end gets cold enough to make ice. Originally, natural gas was used as a heat source in the late 19th century. Today, propane is used in recreational vehicle absorption chiller refrigerators. Innovative hot water solar thermal energy collectors can also be used as the modern \"free energy\" heat source.\n\nEfficient absorption chillers require water of at least 190 °F (88 °C). Common, inexpensive flat-plate solar thermal collectors only produce about 160 °F (70 °C) water, but several successful commercial projects in the US, Asia and Europe have shown that flat plate solar collectors specially developed for temperatures over 200 °F (93 °C) (featuring double glazing, increased backside insulation, etc.) can be effective and cost efficient. Evacuated-tube solar panels can be used as well. Concentrating solar collectors required for absorption chillers are less effective in hot humid, cloudy environments, especially where the overnight low temperature and relative humidity are uncomfortably high. Where water can be heated well above 190 °F (88+ °C), it can be stored and used when the sun is not shining.\n\nFor more than 150 years, absorption chillers have been used to make ice. This ice can be stored and used as an \"ice battery\" for cooling when the sun is not shining, as it was in the 1995 Hotel New Otani Tokyo in Japan. Mathematical models are available in the public domain for ice-based thermal energy storage performance calculations.\n\nBy using a fuel cell as opposed to a burner to create heat, it would be theoretically possible to create an air-conditioner which converted approximately 55% of the fuel (assuming a methane fuel cell) to electricity and the rest to driving an air-conditioner.\n\n\n"}
{"id": "261690", "url": "https://en.wikipedia.org/wiki?curid=261690", "title": "Air mass", "text": "Air mass\n\nIn meteorology, an air mass is a volume of air defined by its temperature and water vapor content. Air masses cover many hundreds or thousands of miles, and adapt to the characteristics of the surface below them. They are classified according to latitude and their continental or maritime source regions. Colder air masses are termed polar or arctic, while warmer air masses are deemed tropical. Continental and superior air masses are dry while maritime and monsoon air masses are moist. Weather fronts separate air masses with different density (temperature and/or moisture) characteristics. Once an air mass moves away from its source region, underlying vegetation and water bodies can quickly modify its character. Classification schemes tackle an air mass' characteristics, as well as modification.\n\nThe Bergeron classification is the most widely accepted form of air mass classification, though others have produced more refined versions of this scheme over different regions of the globe. Air mass classification involves three letters. The first letter describes its moisture properties, with c used for continental air masses (dry) and m for maritime air masses (moist). Its source region: T for Tropical, P for Polar, A for arctic or Antarctic, M for monsoon, E for Equatorial, and S for superior air (an adiabatically drying and warming air formed by significant downward motion in the atmosphere). For instance, an air mass originating over the desert southwest of the United States in summer may be designated \"cT\". An air mass originating over northern Siberia in winter may be indicated as \"cA\".\n\nThe stability of an air mass may be shown using a third letter, either \"k\" (air mass colder than the surface below it) or \"w\" (air mass warmer than the surface below it). An example of this might be a polar air mass blowing over the Gulf Stream, denoted as \"cPk\". Occasionally, one may also encounter the use of an apostrophe or \"degree tick\" denoting that a given air mass having the same notation as another it is replacing is colder than the replaced air mass (usually for polar air masses). For example, a series of fronts over the Pacific might show an air mass denoted mPk followed by another denoted mPk'.\n\nAnother convention utilizing these symbols is the indication of modification or transformation of one type to another. For instance, an Arctic air mass blowing out over the Gulf of Alaska may be shown as \"cA-mPk\". Yet another convention indicates the layering of air masses in certain situations. For instance, the overrunning of a polar air mass by an air mass from the Gulf of Mexico over the Central United States might be shown with the notation \"mT/cP\" (sometimes using a horizontal line as in fraction notation).\n\nTropical and equatorial air masses are hot as they develop over lower latitudes. Those that develop over land (continental) are drier and hotter than those that develop over oceans, and travel poleward on the western periphery of the subtropical ridge. Maritime tropical air masses are sometimes referred to as trade air masses. Maritime tropical air masses that effect the United States originate in the Caribbean Sea, southern Gulf of Mexico, and tropical Atlantic east of Florida through the Bahamas. Monsoon air masses are moist and unstable. Superior air masses are dry, and rarely reach the ground. They normally reside over maritime tropical air masses, forming a warmer and drier layer over the more moderate moist air mass below, forming what is known as a trade wind inversion over the maritime tropical air mass. \n\nContinental Polar air masses (cP) are air masses that are cold and dry due to their continental source region. Continental polar air masses that affect North America form over interior Canada. Continental Tropical air masses (cT) are a type of tropical air produced by the subtropical ridge over large areas of land and typically originate from low-latitude deserts such as the Sahara Desert in northern Africa, which is the major source of these air masses. Other less important sources producing cT air masses are the Arabian Peninsula, the central arid/semi-arid part of Australia and deserts lying in the Southwestern United States. Continental tropical air masses are extremely hot and dry. \n\nArctic, Antarctic, and polar air masses are cold. The qualities of arctic air are developed over ice and snow-covered ground. Arctic air is deeply cold, colder than polar air masses. Arctic air can be shallow in the summer, and rapidly modify as it moves equatorward. Polar air masses develop over higher latitudes over the land or ocean, are very stable, and generally shallower than arctic air. Polar air over the ocean (maritime) loses its stability as it gains moisture over warmer ocean waters.\n\nA weather front is a boundary separating two masses of air of different densities, and is the principal cause of meteorological phenomena. In surface weather analyses, fronts are depicted using various colored lines and symbols, depending on the type of front. The air masses separated by a front usually differ in temperature and humidity.\nCold fronts may feature narrow bands of thunderstorms and severe weather, and may on occasion be preceded by squall lines or dry lines. Warm fronts are usually preceded by stratiform precipitation and fog. The weather usually clears quickly after a front's passage. Some fronts produce no precipitation and little cloudiness, although there is invariably a wind shift. \n\nCold fronts and occluded fronts generally move from west to east, while warm fronts move poleward. Because of the greater density of air in their wake, cold fronts and cold occlusions move faster than warm fronts and warm occlusions. Mountains and warm bodies of water can slow the movement of fronts. When a front becomes stationary, and the density contrast across the frontal boundary vanishes, the front can degenerate into a line which separates regions of differing wind velocity, known as a shearline. This is most common over the open ocean.\n\nAir masses can be modified in a variety of ways. Surface flux from underlying vegetation, such as forest, acts to moisten the overlying air mass. Heat from underlying warmer waters can significantly modify an air mass over distances as short as to . For example, southwest of extratropical cyclones, curved cyclonic flow bringing cold air across the relatively warm water bodies can lead to narrow lake-effect snow bands. Those bands bring strong localized precipitation since large water bodies such as lakes efficiently store heat that results in significant temperature differences (larger than 13 °C or 23 °F) between the water surface and the air above. Because of this temperature difference, warmth and moisture are transported upward, condensing into vertically oriented clouds (see satellite picture) which produce snow showers. The temperature decrease with height and cloud depth are directly affected by both the water temperature and the large-scale environment. The stronger the temperature decrease with height, the deeper the clouds get, and the greater the precipitation rate becomes.\n\n"}
{"id": "16039777", "url": "https://en.wikipedia.org/wiki?curid=16039777", "title": "Arrigetch Peaks", "text": "Arrigetch Peaks\n\nThe Arrigetch Peaks are a cluster of rugged granite spires in the Endicott Mountains of the central Brooks Range in northern Alaska. The name \"Arrigetch\" means 'fingers of the outstretched hand' in the Inupiat language. The peaks ring the glacial cirques at the head of the Kobuk River and 2 tributaries of the Alatna River: Arrigetch Creek and Aiyagomahala Creek (Creek 4662). They are located at latitude 67 degrees 24' N and longitude 154 degrees 10' W. All of the summits of the peaks are around 6,000 ft, 1825 m elevation. The Arrigetch Peaks area was designated as a National Natural Landmark in 1968 for its spectacular geography.\n\nThe earliest recorded visit was in 1911 by Philip Smith, a geologist. The renowned conservationist Robert Marshall traveled through the area in the 1930s. These trips were described in his 1933 book \"Arctic Village\", and posthumous \"Alaska Wilderness: Exploring the Central Brooks Range\". A British climbing party completed the first successful rock climbing expedition to the peaks in 1964. The peaks have been visited by a number of rock climbing expeditions since then.\n\n"}
{"id": "41158834", "url": "https://en.wikipedia.org/wiki?curid=41158834", "title": "Beach meadow", "text": "Beach meadow\n\nBeach Meadows are coastal meadows influenced by the presence of the nearby sea.\n\nUnder this definition, the salinity of the air and wind is usually high and the meadows are often flooded during and after stormy weather. These conditions implies that the flora is dominated by salt-tolerant species. But that alone does not make a meadow. To be categorized as a meadow in the first place, the plantgrowth has to be low in height, and normally this can only be achieved from wear by general traffic or grazing of the landscape, either artificially or by livestock. Beach meadows are therefore usually thought of as cultural landscapes or biotopes, requiring some degree of intervention and not being able to sustain itself on its own. If left to their own, beach meadows would usually transform into a socalled transitional meadow and eventually a shrubby or bushy seashore habitat.\n\nAs explained, beach meadows are fundamentally an unstable nature type and this condition have an important influence on the flora and fauna found here. Beach meadows are characterized by a number of plants and animals, that could not have thrived, if the habitat was left to a natural development. The grazing (or traffic) hinders bushes, trees and shrubs to get the upper hand and allows low-growth plants to emerge and dominate. This again attracts and supports a special fauna that would change character, if the beach meadow were left to its own. The specific flora and fauna is of course determined by the general climate and geography of the beach meadow.\n\nBeach meadows offers a variety of opportunities to many birds, primarily waders, ducks and gulls. Many species are resting here during tides and they are feeding in the small ponds. A number of bird species use them as breeding grounds, especially on islands without predators like fox, \"mustelidae\" (raccoon dogs, wolverines, polecats, etc.) or the like. Introducing even small numbers of predators to beach meadow habitats, can wreak havoc on the bird populations, as they have unrestricted access to eggs and birds.\n\nDenmark have several beach meadows. Large and well-developed meadows can primarily be found in the Wadden Sea, Limfjord and Isefjord areas and on the islands of Læsø and Lolland, but there are many other smaller beach meadows throughout this lowlying country. On the Faroese Islands and in Greenland 'fell meadows' can be found. They are usually not flooded by seawater, so it could be argued if they are truly beach meadows.\n\nIceland, Norway and some parts of Sweden, have extensive 'fell meadows'.\n\nIn the Baltic region, there are several beach meadows. Sweden can present true beach meadows on Öland and in some parts of Gotland fx. and Estonia has large beach meadows of international importance to migratory birds.\n\nUnited Kingdom have several beach meadows throughout. They can be found in Cornwall and the country of Wales have several. Here the coast often consists of high cliffs, so the meadows are usually not flooded by seawater and strictly speaking they would be classified as 'coastal meadows' or 'fell meadows'. Examples can be found on the Llŷn Peninsula, where cattle are grazing. Scotland have many fell meadows.\n\nIreland have large areas of fell meadows with grazing cattle.\n\nOn the American continent, Canada can present extensive fell meadows on Nova Scotia fx..\n\nAs the practices of cattle grazing are diminishing in many places - especially the industrialized parts -, the stress of overgrowth are endangering them as meadow habitats.\n\n\n"}
{"id": "7270721", "url": "https://en.wikipedia.org/wiki?curid=7270721", "title": "Biological monitoring working party", "text": "Biological monitoring working party\n\nThe biological monitoring working party (BMWP) is a procedure for measuring water quality using families of macroinvertebrates as biological indicators.\n\nThe method is based on the principle that different aquatic invertebrates have different tolerances to pollutants. In the case of BMWP, this is based on the sensitivity/tolerance to organic pollution (i.e. nutrient enrichment that can affect the availability of dissolved oxygen). It is important to recognise that the ranking of sensitivity/tolerance will vary for different kinds of pollution. In the case of BMWP/Organic pollution rankings, the presence of mayflies or stoneflies for instance indicate the cleanest waterways and are given a tolerance score of 10. The lowest scoring invertebrates are worms (\"Oligochaeta\") which score 1. The number of different macroinvertebrates is also an important factor, because a better quality water is assumed to contain fewer pollutants that would exclude \"sensitive\" species - resulting in a higher diversity.\n\nKick sampling, where a net is placed downstream from the sampler and the river bed is agitated with the foot for a given period of time (the standard is 3 minutes), is employed. Any macroinvertebrates caught in the net are stored and preserved with an alcohol solution, and identified to the family level, this can be done with the live organisms as well.\n\nThe BMWP score equals the sum of the tolerance scores of all macroinvertebrate families in the sample. A higher BMWP score is considered to reflect a better water quality. Alternatively, also the Average Score Per Taxon (ASPT) score is calculated. The ASPT equals the average of the tolerance scores of all macroinvertebrate families found, and ranges from 0 to 10. The main difference between both indices is that ASPT does not depend on the family richness. Once BMWP and ASPT are calculated, the Lincoln Quality Index (LQI) is used to assess the water quality in the Anglian Water Authority area.\n\nOther indices that can be used to assess water quality are the Chandler Score, the Trent Biotic Index and the \"Rapid Bioassessment Protocols\".\n\n"}
{"id": "3755949", "url": "https://en.wikipedia.org/wiki?curid=3755949", "title": "Chuya Steppe", "text": "Chuya Steppe\n\nThe Chuya Steppe () in the Siberian Altai Mountains is a depression formed by tectonic movement of major faults in the Earth's crust. Its name comes from the large river which runs through the steppe, the Chuya River.\n\nKosh-Agach is a major village in the north of the steppe. Other large settlements include Chaganuzun and Beltir.\n\nThe Chuya Steppe is filled with Cenozoic sediments, derived from the surrounding mountains of the Chuya Range-Chuya Alps.\n\nThe 7.3 Altai earthquake shook South Central Siberia with a maximum Mercalli intensity of X (\"Extreme\"), causing $10.6–33 million in damage, three deaths, and five injuries.\n"}
{"id": "5346", "url": "https://en.wikipedia.org/wiki?curid=5346", "title": "Colloid", "text": "Colloid\n\nIn chemistry, a colloid is a mixture in which one substance of microscopically dispersed insoluble particles is suspended throughout another substance. Sometimes the dispersed substance alone is called the colloid; the term colloidal suspension refers unambiguously to the overall mixture (although a narrower sense of the word \"suspension\" is distinguished from colloids by larger particle size). Unlike a solution, whose solute and solvent constitute only one phase, a colloid has a dispersed phase (the suspended particles) and a continuous phase (the medium of suspension). To qualify as a colloid, the mixture must be one that does not settle or would take a very long time to settle appreciably.\n\nThe dispersed-phase particles have a diameter between approximately 1 and 1000 nanometers. Such particles are normally easily visible in an optical microscope, although at the smaller size range (), an ultramicroscope or an electron microscope may be required. Homogeneous mixtures with a dispersed phase in this size range may be called \"colloidal aerosols\", \"colloidal emulsions\", \"colloidal foams\", \"colloidal dispersions\", or \"hydrosols\". The dispersed-phase particles or droplets are affected largely by the surface chemistry present in the colloid.\n\nSome colloids are translucent because of the Tyndall effect, which is the scattering of light by particles in the colloid. Other colloids may be opaque or have a slight color.\n\nColloidal suspensions are the subject of interface and colloid science. This field of study was introduced in 1845 by Italian chemist Francesco Selmi and further investigated since 1861 by Scottish scientist Thomas Graham.\n\nBecause the size of the dispersed phase may be difficult to measure, and because colloids have the appearance of solutions, colloids are sometimes identified and characterized by their physico-chemical and transport properties. For example, if a colloid consists of a solid phase dispersed in a liquid, the solid particles will not diffuse through a membrane, whereas with a true solution the dissolved ions or molecules will diffuse through a membrane. Because of the size exclusion, the colloidal particles are unable to pass through the pores of an ultrafiltration membrane with a size smaller than their own dimension. The smaller the size of the pore of the ultrafiltration membrane, the lower the concentration of the dispersed colloidal particles remaining in the ultrafiltered liquid. The measured value of the concentration of a truly dissolved species will thus depend on the experimental conditions applied to separate it from the colloidal particles also dispersed in the liquid. This is particularly important for solubility studies of readily hydrolyzed species such as Al, Eu, Am, Cm, or organic matter complexing these species.\nColloids can be classified as follows:\nBased on the nature of interaction between the dispersed phase and the dispersion medium, colloids can be classified as: Hydrophilic colloids: The colloid particles are attracted toward water. They are also called reversible sols. Hydrophobic colloids: These are opposite in nature to hydrophilic colloids. The colloid particles are repelled by water. They are also called irreversible sols.\n\nIn some cases, a colloid suspension can be considered a homogeneous mixture. This is because the distinction between \"dissolved\" and \"particulate\" matter can be sometimes a matter of approach, which affects whether or not it is homogeneous or heterogeneous.\n\nThe following forces play an important role in the interaction of colloid particles:\n\nThere are two principal ways of preparation of colloids:\n\nThe stability of a colloidal system is defined by particles remaining suspended in solution at equilibrium.\n\nStability is hindered by aggregation and sedimentation phenomena, which are driven by the colloid's tendency to reduce surface energy. Reducing the interfacial tension will stabilize the colloidal system by reducing this driving force. \nAggregation is due to the sum of the interaction forces between particles. If attractive forces (such as van der Waals forces) prevail over the repulsive ones (such as the electrostatic ones) particles aggregate in clusters.\n\nElectrostatic stabilization and steric stabilization are the two main mechanisms for stabilization against aggregation.\nA combination of the two mechanisms is also possible (electrosteric stabilization). All the above-mentioned mechanisms for minimizing particle aggregation rely on the enhancement of the repulsive interaction forces.\n\nElectrostatic and steric stabilization do not directly address the sedimentation/floating problem.\n\nParticle sedimentation (and also floating, although this phenomenon is less common) arises from a difference in the density of the dispersed and of the continuous phase. The higher the difference in densities, the faster the particle settling.\nThe method consists in adding to the colloidal suspension a polymer able to form a gel network and characterized by shear thinning properties. Examples of such substances are xanthan and guar gum.\n\nParticle settling is hindered by the stiffness of the polymeric matrix where particles are trapped. In addition, the long polymeric chains can provide a steric or electrosteric stabilization to dispersed particles.\n\nThe rheological shear thinning properties find beneficial in the preparation of the suspensions and in their use, as the reduced viscosity at high shear rates facilitates deagglomeration, mixing and in general the flow of the suspensions.\n\nUnstable colloidal dispersions can form flocs as the particles aggregate due to interparticle attractions. In this way photonic glasses can be grown. This can be accomplished by a number of different methods:\n\nUnstable colloidal suspensions of low-volume fraction form clustered liquid suspensions, wherein individual clusters of particles fall to the bottom of the suspension (or float to the top if the particles are less dense than the suspending medium) once the clusters are of sufficient size for the Brownian forces that work to keep the particles in suspension to be overcome by gravitational forces. However, colloidal suspensions of higher-volume fraction form colloidal gels with viscoelastic properties. Viscoelastic colloidal gels, such as bentonite and toothpaste, flow like liquids under shear, but maintain their shape when shear is removed. It is for this reason that toothpaste can be squeezed from a toothpaste tube, but stays on the toothbrush after it is applied.\n\nMultiple light scattering coupled with vertical scanning is the most widely used technique to monitor the dispersion state of a product, hence identifying and quantifying destabilisation phenomena. It works on concentrated dispersions without dilution. When light is sent through the sample, it is backscattered by the particles / droplets. The backscattering intensity is directly proportional to the size and volume fraction of the dispersed phase. Therefore, local changes in concentration (\"e.g.\"Creaming and Sedimentation) and global changes in size (\"e.g.\" flocculation, coalescence) are detected and monitored.\n\nThe kinetic process of destabilisation can be rather long (up to several months or even years for some products) and it is often required for the formulator to use further accelerating methods in order to reach reasonable development time for new product design. Thermal methods are the most commonly used and consists in increasing temperature to accelerate destabilisation (below critical temperatures of phase inversion or chemical degradation). Temperature affects not only the viscosity, but also interfacial tension in the case of non-ionic surfactants or more generally interactions forces inside the system. Storing a dispersion at high temperatures enables to simulate real life conditions for a product (e.g. tube of sunscreen cream in a car in the summer), but also to accelerate destabilisation processes up to 200 times.\nMechanical acceleration including vibration, centrifugation and agitation are sometimes used. They subject the product to different forces that pushes the particles / droplets against one another, hence helping in the film drainage. However, some emulsions would never coalesce in normal gravity, while they do under artificial gravity. Moreover, segregation of different populations of particles have been highlighted when using centrifugation and vibration.\n\nIn physics, colloids are an interesting model system for atoms. Micrometre-scale colloidal particles are large enough to be observed by optical techniques such as confocal microscopy. Many of the forces that govern the structure and behavior of matter, such as excluded volume interactions or electrostatic forces, govern the structure and behavior of colloidal suspensions. For example, the same techniques used to model ideal gases can be applied to model the behavior of a hard sphere colloidal suspension. In addition, phase transitions in colloidal suspensions can be studied in real time using optical techniques, and are analogous to phase transitions in liquids. In many interesting cases optical fluidity is used to control colloid suspensions.\n\nA colloidal crystal is a highly ordered array of particles that can be formed over a very long range (typically on the order of a few millimeters to one centimeter) and that appear analogous to their atomic or molecular counterparts. One of the finest natural examples of this ordering phenomenon can be found in precious opal, in which brilliant regions of pure spectral color result from close-packed domains of amorphous colloidal spheres of silicon dioxide (or silica, SiO). These spherical particles precipitate in highly siliceous pools in Australia and elsewhere, and form these highly ordered arrays after years of sedimentation and compression under hydrostatic and gravitational forces. The periodic arrays of submicrometre spherical particles provide similar arrays of interstitial voids, which act as a natural diffraction grating for visible light waves, particularly when the interstitial spacing is of the same order of magnitude as the incident lightwave.\n\nThus, it has been known for many years that, due to repulsive Coulombic interactions, electrically charged macromolecules in an aqueous environment can exhibit long-range crystal-like correlations with interparticle separation distances, often being considerably greater than the individual particle diameter. In all of these cases in nature, the same brilliant iridescence (or play of colors) can be attributed to the diffraction and constructive interference of visible lightwaves that satisfy Bragg’s law, in a matter analogous to the scattering of X-rays in crystalline solids.\n\nThe large number of experiments exploring the physics and chemistry of these so-called \"colloidal crystals\" has emerged as a result of the relatively simple methods that have evolved in the last 20 years for preparing synthetic monodisperse colloids (both polymer and mineral) and, through various mechanisms, implementing and preserving their long-range order formation.\n\nIn the early 20th century, before enzymology was well understood, colloids were thought to be the key to the operation of enzymes; i.e., the addition of small quantities of an enzyme to a quantity of water would, in some fashion yet to be specified, subtly alter the properties of the water so that it would break down the enzyme's specific substrate, such as a solution of ATPase breaking down ATP. Furthermore, life itself was explainable in terms of the aggregate properties of all the colloidal substances that make up an organism. As more detailed knowledge of biology and biochemistry developed, the colloidal theory was replaced by the macromolecular theory, which explains enzymes as a collection of identical huge molecules that act as very tiny machines, freely moving about between the water molecules of the solution and individually operating on the substrate, no more mysterious than a factory full of machinery. The properties of the water in the solution are not altered, other than the simple osmotic changes that would be caused by the presence of any solute. In humans, both the thyroid gland and the intermediate lobe (\"pars intermedia\") of the pituitary gland contain colloid follicles.\n\nColloidal particles can also serve as transport vector\nof diverse contaminants in the surface water (sea water, lakes, rivers, fresh water bodies) and in underground water circulating in fissured rocks\n(e.g. limestone, sandstone, granite). Radionuclides and heavy metals easily sorb onto colloids suspended in water. Various types of colloids are recognised: inorganic colloids (e.g. clay particles, silicates, iron oxy-hydroxides), organic colloids (humic and fulvic substances). When heavy metals or radionuclides form their own pure colloids, the term \"eigencolloid\" is used to designate pure phases, i.e., pure Tc(OH), U(OH), or Am(OH). Colloids have been suspected for the long-range transport of plutonium on the Nevada Nuclear Test Site. They have been the subject of detailed studies for many years. However, the mobility of inorganic colloids is very low in compacted bentonites and in deep clay formations\nbecause of the process of ultrafiltration occurring in dense clay membrane.\nThe question is less clear for small organic colloids often mixed in porewater with truly dissolved organic molecules.\n\nColloid solutions used in intravenous therapy belong to a major group of volume expanders, and can be used for intravenous fluid replacement. Colloids preserve a high colloid osmotic pressure in the blood, and therefore, they should theoretically preferentially increase the intravascular volume, whereas other types of volume expanders called crystalloids also increase the interstitial volume and intracellular volume. However, there is still controversy to the actual difference in efficacy by this difference, and much of the research related to this use of colloids is based on fraudulent research by Joachim Boldt. Another difference is that crystalloids generally are much cheaper than colloids.\n"}
{"id": "4303426", "url": "https://en.wikipedia.org/wiki?curid=4303426", "title": "Cumulonimbus calvus", "text": "Cumulonimbus calvus\n\nCumulonimbus calvus is a moderately tall cumulonimbus cloud which is capable of precipitation, but has not yet reached the tropopause, which is the height of stratospheric stability where it forms into a cumulonimbus capillatus (fibrous-top) or cumulonimbus incus (anvil-top). Cumulonimbus calvus clouds develop from cumulus congestus, and its further development under auspicious conditions will result in cumulonimbus incus.\n\nThis cloud consists mainly of water droplets. By definition of cumulonimbus cloud, at its top water droplets are transformed into ice crystals. But for cumulonimbus calvus, content of ice crystals are meager and polar are in early stage, so cloud tops still look round and puffy.\n\nCumulonimbus calvus is characterized by distinctive (between other types of cumulonimbus cloud) rounded shape and relatively sharp edges of its top area, unlike cumulonimbus incus or cumulonimbus capillatus, which have cirriform tops. Developing cumulonimbus calvus lose sharp outlines of the top as more water droplets transform into ice crystals. Strong updrafts may form pileus or thin vertical stripes protruding upwards out of the cloud. When upper parts of the cloud freeze to greater extent and clearly visible cirriforms appears, cumulonimbus calvus metamorphose another species of cumulonimbus.\n\nCumulonimbus calvus arcus is a sub-form of cumulonimbus calvus, which has arcus cloud ahead of cloud's front.\n\nLike other cumulonimbus clouds, cumulonimbus calvus can cause severe weather conditions including: \n"}
{"id": "27879377", "url": "https://en.wikipedia.org/wiki?curid=27879377", "title": "Cyprus Museum of Natural History", "text": "Cyprus Museum of Natural History\n\nThe Cyprus Museum of Natural History is a natural history museum on the outskirts of Nicosia, Cyprus.\n\nThe museum was founded by the Photos Photiades Charity, Scientific and Cultural Foundation, and is the largest museum of its kind in Cyprus. Its exhibits include stuffed animals of various sorts, fossils, shells, rocks, and minerals.\n\n"}
{"id": "43773629", "url": "https://en.wikipedia.org/wiki?curid=43773629", "title": "EPS Service Parts Act of 2014", "text": "EPS Service Parts Act of 2014\n\nThe EPS Service Parts Act of 2014 () is a bill that would exempt certain external power supplies from complying with standards set forth in a final rule published by the United States Department of Energy in February 2014. The United States House Committee on Energy and Commerce describes the bill as a bill that \"provides regulatory relief by making a simple technical correction to the 2007 Energy Independence and Security Act to exempt certain power supply (EPS) service and spare parts from federal efficiency standards.\"\n\nThe bill was introduced into the United States House of Representatives during the 113th United States Congress.\n\nThe Energy Independence and Security Act of 2007 (Pub.L. 110-140 originally named the \"Clean Energy Act of 2007\") is an Act of Congress concerning the energy policy of the United States. As part of the Democratic Party's 100-Hour Plan during the 110th Congress, it was introduced in the United States House of Representatives by Representative Nick Rahall of West Virginia, along with 198 cosponsors. A revised bill passed both houses on December 18, 2007 and President Bush, a Republican, signed it into law on December 19, 2007, in response to his \"Twenty in Ten\" challenge to reduce gasoline consumption by 20% in 10 years.\n\nThe stated purpose of the act is “to move the United States toward greater energy independence and security, to increase the production of clean renewable fuels, to protect consumers, to increase the efficiency of products, buildings, and vehicles, to promote research on and deploy greenhouse gas capture and storage options, and to improve the energy performance of the Federal Government, and for other purposes.”.\n\n\"This summary is based largely on the summary provided by the Congressional Budget Office, as ordered reported by the House Committee on Energy and Commerce on July 15, 2014. This is a public domain source.\"\n\nThe United States Department of Energy (DOE) prescribes energy conservation standards for more than 50 categories of appliances and equipment. H.R. 5057 would exempt certain external power supplies from complying with standards set forth in a final rule published in February 2014. (An external power supply is a hardware component that converts household electric current into lower-voltage current used to operate devices such as laptops and smartphones.) The bill would authorize DOE to limit the applicability of that exemption if the Secretary finds that the exemption would result in a significant reduction in energy savings that would otherwise result if the February 2014 rule were fully implemented.\n\nThe Congressional Budget Office (CBO) estimates that enacting H.R. 5057 would not significantly affect the federal budget. Based on information from DOE, we estimate that any costs incurred by the agency to carry out the bill’s provisions would total less than $500,000 annually and would be subject to the availability of appropriated funds. H.R. 5057 would not affect direct spending or revenues; therefore, pay-as-you-go procedures do not apply.\n\nH.R. 5057 contains no intergovernmental or private-sector mandates as defined in the Unfunded Mandates Reform Act and would not affect the budgets of state, local, or tribal governments.\n\nThe EPS Service Parts Act of 2014 is a bill that was introduced into the United States House of Representatives on July 10, 2014 by Rep. Cory Gardner (R, CO-4). The bill was referred to the United States House Committee on Energy and Commerce and the United States House Energy Subcommittee on Energy and Power. The committee voted in favor of the bill on July 15, 2014 in a unanimous vote.\n\nThe bill had the support of the Alliance to Save Energy (ASE), the American Council for an Energy-Efficient Economy (ACEEE), the Association of Home Appliance Manufacturers (AHAM), the Consumer Electronics Association (CEA), the National Association of Manufacturers (NAM), the National Electrical Manufacturers Association (NEMA), and the Natural Resources Defense Council (NRDC).\n\nAccording to supporters of the bill, \"without this change in law, manufacturers would be required to redesign and qualify new service and spare EPS for existing products that are no longer in production at significant expense for both the companies and consumers.\" The Energy Independence and Security Act of 2007 did originally include an exemption for such devices through 2015, but manufactures and others are finding that a deadline of 2015 is too soon because consumers are still using the older devices even if they are no longer being produced.\n\n\n"}
{"id": "37297145", "url": "https://en.wikipedia.org/wiki?curid=37297145", "title": "Eclogitization", "text": "Eclogitization\n\nEclogitization is the tectonic process in which the high-pressure, metamorphic facies, eclogite (a very dense rock), is formed. This leads to an increase in the density of regions of Earth's crust, which leads to changes in plate motion at convergent boundaries (where rock sinks beneath other rock).\n\nThere is the argument that collision between two continents should slow down because of continental buoyancy, and that for convergence to continue, it should do so at a new subduction zone where oceanic crust can be consumed. Certain areas such as the Alps, Zagros, and Himalayas (where continental collisions have continued for tens of millions of years, in the middle of land, creating mountain ranges) contradict this argument, and have led geologists to propose a \"continental undertow\" that continues subduction. This \"continental undertow\" is explained by the slab pull concept. Slab pull is the concept that plate motion is driven by the weight of cool, dense plates and that heavier plates will begin to subduct. Once a descending slab is disconnected there must be a force that continues subduction. Eclogitization is the mechanism for continuing subduction after slab detachment in a subduction zone.\n\nEclogitization typically occurs at two locations in a collisional fold mountain (fig 2), in the subduction of crust and at the base of the crustal root of the overriding crust. At these zones high pressures are reached, as well as medium to high temperatures, and eclogitization commences. Metamorphic re-crystallization during burial can lead to a significant density increase (up to 10% in the case of eclogitization), meaning approximately 300–600 kg/m of crustal rocks and continental lower crust and oceanic crust reach higher density than the mantle.\n\nThis density increase acts as the main driver in the convection of Earth's mantle. It also explains the disconnection of a tectonic unit from the descending lithosphere, subsequent continuation of subduction, and the exhumation following subduction.\n\nEclogitization is difficult to study because the rocks are rare: eclogites constitute only a very minor volume of continental basement exposed today at Earth's surface. The few areas that are available to study eclogitization and view eclogites include garnet peridotites in Greenland and in other ophiolite complexes. Examples are also known in Saxony, Bavaria, Carinthia, Norway and Newfoundland. A few eclogites also occur in the northwest highlands of Scotland and the Massif Central of France. Glaucophane-eclogites occur in Italy and the Pennine Alps. Occurrences exist in western North America, including the southwest and the Franciscan Formation of the California Coast Ranges. Transitional Granulite-Eclogite facies granitoid, felsic volcanics, mafic rocks and granulites occur in the Musgrave Block of the Petermann Orogeny, central Australia. Recently, coesite- and glaucophane-bearing eclogites have been found in the northwestern Himalaya. Although limited localities are available to study, these areas provide the crucial samples to understand exhumation as well as continued subduction by continental \"undertow.\"\n\nFluids, rather than pressure and temperature conditions, are the key thing that makes the process of eclogitization, and the delamination (falling away) of crustal roots, in collisional orogens (fold mountains), possible. Partially eclogitized amphibolites, gabbros, and\ngranulites from the Western Gneiss Region of Norway, the Marun-Keu Complex in the polar Ural Mountains, and the Dabie-Sulu belt in China demonstrate that fluid is required for complete eclogitization. In these locations, eclogite occurs alongside unreacted rocks subjected to the same temperatures and pressures, with the eclogite forming where fluid can reach, for example along fractures.\n\nAn influx of fluids into the subduction zone or from the underlying mantle is vital for these metamorphic reactions to continue – fluids play a much more significant role in eclogite metamorphism than either temperature or pressure. Without HO, reactions will not proceed to completion, leaving metamorphic rocks metastable (stuck in an incomplete state) at unexpectedly high temperatures and pressures. Without the metamorphosis of less dense rocks to eclogite, which is eclogitization, continental \"undertow\" may be hindered, and subduction may be slowed down, or even eventually stop.\n\n"}
{"id": "55160898", "url": "https://en.wikipedia.org/wiki?curid=55160898", "title": "Electronic properties of graphene", "text": "Electronic properties of graphene\n\nGraphene is a zero-gap semiconductor, because its conduction and valence bands meet at the Dirac points, which are six locations in momentum space, on the edge of the Brillouin zone, divided into two non-equivalent sets of three points. The two sets are labeled K and K'. The sets give graphene a valley degeneracy of . By contrast, for traditional semiconductors the primary point of interest is generally Γ, where momentum is zero. Four electronic properties separate it from other condensed matter systems.\n\nHowever, if the in-plane direction is confined, in which case it is referred to as a nanoribbon, its electronic structure is different. If it is \"zig-zag\", the bandgap is zero. If it is \"armchair\", the bandgap is non-zero (see figure).\n\nElectrons propagating through graphene's honeycomb lattice effectively lose their mass, producing quasi-particles that are described by a 2D analogue of the Dirac equation rather than the Schrödinger equation for spin- particles.\n\nWhen atoms are placed onto the graphene hexagonal lattice, the overlap between the \"p\"(π) orbitals and the \"s\" or the \"p\" and \"p\" orbitals is zero by symmetry. The \"p\" electrons forming the π bands in graphene can be treated independently. Within this π-band approximation, using a conventional tight-binding model, the dispersion relation (restricted to first-nearest-neighbor interactions only) that produces energy of the electrons with wave vector \"k\" is\n\nwith the nearest-neighbor (π orbitals) hopping energy \"γ\" ≈ and the lattice constant . The conduction and valence bands, respectively, correspond to the different signs. With one \"p\" electron per atom in this model the valence band is fully occupied, while the conduction band is vacant. The two bands touch at the zone corners (the \"K\" point in the Brillouin zone), where there is a zero density of states but no band gap. The graphene sheet thus displays a semimetallic (or zero-gap semiconductor) character, although not if rolled into a carbon nanotube, due to its curvature. Two of the six Dirac points are independent, while the rest are equivalent by symmetry. In the vicinity of the \"K\"-points the energy depends \"linearly\" on the wave vector, similar to a relativistic particle. Since an elementary cell of the lattice has a basis of two atoms, the wave function has an effective 2-spinor structure.\n\nAs a consequence, at low energies, even neglecting the true spin, the electrons can be described by an equation that is formally equivalent to the massless Dirac equation. Hence, the electrons and holes are called Dirac fermions. This pseudo-relativistic description is restricted to the chiral limit, i.e., to vanishing rest mass \"M\", which leads to additional features:\n\nHere \"v\" ~ (.003 c) is the Fermi velocity in graphene, which replaces the velocity of light in the Dirac theory; formula_3 is the vector of the Pauli matrices; formula_4 is the two-component wave function of the electrons and \"E\" is their energy.\n\nThe equation describing the electrons' linear dispersion relation is\n\nwhere the wavevector \"k\" is measured from the Dirac points (the zero of energy is chosen here to coincide with the Dirac points). The equation uses a pseudospin matrix formula that describes two sublattices of the honeycomb lattice.\n\nGraphene's unit cell has two identical carbon atoms and two zero-energy states: one in which the electron resides on atom A, the other in which the electron resides on atom B. However, if the two atoms in the unit cell are not identical, the situation changes. Hunt et al. showed that placing hexagonal boron nitride (h-BN) in contact with graphene can alter the potential felt at atom A versus atom B enough that the electrons develop a mass and accompanying band gap of about 30 meV [0.03 Electron Volt(eV)].\n\nThe mass can be positive or negative. An arrangement that slightly raises the energy of an electron on atom A relative to atom B gives it a positive mass, while an arrangement that raises the energy of atom B produces a negative electron mass. The two versions behave alike and are indistinguishable via optical spectroscopy. An electron traveling from a positive-mass region to a negative-mass region must cross an intermediate region where its mass once again becomes zero. This region is gapless and therefore metallic. Metallic modes bounding semiconducting regions of opposite-sign mass is a hallmark of a topological phase and display much the same physics as topological insulators.\n\nIf the mass in graphene can be controlled, electrons can be confined to massless regions by surrounding them with massive regions, allowing the patterning of quantum dots, wires and other mesoscopic structures. It also produces one-dimensional conductors along the boundary. These wires would be protected against backscattering and could carry currents without dissipation.\n\nElectron waves in graphene propagate within a single-atom layer, making them sensitive to the proximity of other materials such as high-κ dielectrics, superconductors and ferromagnetics.\n\nGraphene displays remarkable electron mobility at room temperature, with reported values in excess of . Hole and electron mobilities were expected to be nearly identical. The mobility is nearly independent of temperature between and , which implies that the dominant scattering mechanism is defect scattering. Scattering by graphene's acoustic phonons intrinsically limits room temperature mobility to at a carrier density of , times greater than copper.\n\nThe corresponding resistivity of graphene sheets would be . This is less than the resistivity of silver, the lowest otherwise known at room temperature. However, on substrates, scattering of electrons by optical phonons of the substrate is a larger effect than scattering by graphene’s own phonons. This limits mobility to .\n\nCharge transport is affected by adsorption of contaminants such as water and oxygen molecules. This leads to non-repetitive and large hysteresis I-V characteristics. Researchers must carry out electrical measurements in vacuum. Graphene surfaces can be protected by a coating with materials such as SiN, PMMA and h-BN. In January 2015, the first stable graphene device operation in air over several weeks was reported, for graphene whose surface was protected by aluminum oxide. In 2015 lithium-coated graphene was observed to exhibit superconductivity and in 2017 evidence for unconventional superconductivity was demonstrated in single layer graphene placed on the electron-doped (non-chiral) \"d\"-wave superconductor PrCeCuO (PCCO).\n\nElectrical resistance in 40-nanometer-wide nanoribbons of epitaxial graphene changes in discrete steps. The ribbons' conductance exceeds predictions by a factor of 10. The ribbons can act more like optical waveguides or quantum dots, allowing electrons to flow smoothly along the ribbon edges. In copper, resistance increases in proportion to length as electrons encounter impurities.\n\nTransport is dominated by two modes. One is ballistic and temperature independent, while the other is thermally activated. Ballistic electrons resemble those in cylindrical carbon nanotubes. At room temperature, resistance increases abruptly at a particular length—the ballistic mode at 16 micrometres and the other at 160 nanometres.\n\nGraphene electrons can cover micrometer distances without scattering, even at room temperature.\n\nDespite zero carrier density near the Dirac points, graphene exhibits a minimum conductivity on the order of formula_6. The origin of this minimum conductivity is unclear. However, rippling of the graphene sheet or ionized impurities in the substrate may lead to local puddles of carriers that allow conduction. Several theories suggest that the minimum conductivity should be formula_7; however, most measurements are of order formula_6 or greater and depend on impurity concentration.\n\nNear zero carrier density graphene exhibits positive photoconductivity and negative photoconductivity at high carrier density. This is governed by the interplay between photoinduced changes of both the Drude weight and the carrier scattering rate.\n\nGraphene doped with various gaseous species (both acceptors and donors) can be returned to an undoped state by gentle heating in vacuum. Even for dopant concentrations in excess of 10 cm carrier mobility exhibits no observable change. Graphene doped with potassium in ultra-high vacuum at low temperature can reduce mobility 20-fold. The mobility reduction is reversible on removing the potassium.\n\nDue to graphene's two dimensions, charge fractionalization (where the apparent charge of individual pseudoparticles in low-dimensional systems is less than a single quantum) is thought to occur. It may therefore be a suitable material for constructing quantum computers using anyonic circuits.\n\nIn 2018, superconductivity was reported in twisted bilayer graphene.\n\nFirst-principle calculations with quasiparticle corrections and many-body effects explore the electronic and optical properties of graphene-based materials. The approach is described as three stages. With GW calculation, the properties of graphene-based materials are accurately investigated, including bulk graphene, nanoribbons, edge and surface functionalized armchair oribbons, hydrogen saturated armchair ribbons, Josephson effect in graphene SNS junctions with single localized defect and armchair ribbon scaling properties.\n\nIn 2014 researchers magnetized graphene by placing it on an atomically smooth layer of magnetic yttrium iron garnet. The graphene's electronic properties were unaffected. Prior approaches involved doping. The dopant's presence negatively affected its electronic properties.\n\nIn magnetic fields of ≈10 tesla, additional plateaus of Hall conductivity at formula_9 with formula_10 are observed. The observation of a plateau at formula_11 and the fractional quantum Hall effect at formula_12 were reported.\n\nThese observations with formula_13 indicate that the four-fold degeneracy (two valley and two spin degrees of freedom) of the Landau energy levels is partially or completely lifted. One hypothesis is that the magnetic catalysis of symmetry breaking is responsible for lifting the degeneracy.\n\nGraphene is claimed to be an ideal material for spintronics due to its small spin-orbit interaction and the near absence of nuclear magnetic moments in carbon (as well as a weak hyperfine interaction). Electrical spin current injection and detection has been demonstrated up to room temperature. Spin coherence length above 1 micrometre at room temperature was observed, and control of the spin current polarity with an electrical gate was observed at low temperature.\n\nSpintronic and magnetic properties can be present in graphene simultaneously. Low-defect graphene nanomeshes manufactured using a non-lithographic method exhibit large-amplitude ferromagnetism even at room temperature. Additionally a spin pumping effect is found for fields applied in parallel with the planes of few-layer ferromagnetic nanomeshes, while a magnetoresistance hysteresis loop is observed under perpendicular fields.\n\nCharged particles in high-purity graphene behave as a strongly interacting, quasi-relativistic plasma. The particles move in a fluid-like manner, traveling along a single path and interacting with high frequency. The behavior was observed in a graphene sheet faced on both sides with a h-BN crystal sheet.\n\nThe quantum Hall effect is a quantum mechanical version of the Hall effect, which is the production of transverse (perpendicular to the main current) conductivity in the presence of a magnetic field. The quantization of the Hall effect formula_14 at integer multiples (the \"Landau level\") of the basic quantity formula_15 (where \"e\" is the elementary electric charge and \"h\" is Planck's constant) It can usually be observed only in very clean silicon or gallium arsenide solids at temperatures around and high magnetic fields.\n\nGraphene shows the quantum Hall effect with respect to conductivity quantization: the effect is \"anomalous\" in that the sequence of steps is shifted by 1/2 with respect to the standard sequence and with an additional factor of 4. Graphene's Hall conductivity is formula_16, where \"N\" is the Landau level and the double valley and double spin degeneracies give the factor of 4. These anomalies are present at room temperature, i.e. at roughly .\n\nThis behavior is a direct result of graphene's massless Dirac electrons. In a magnetic field, their spectrum has a Landau level with energy precisely at the Dirac point. This level is a consequence of the Atiyah–Singer index theorem and is half-filled in neutral graphene, leading to the \"+1/2\" in the Hall conductivity. Bilayer graphene also shows the quantum Hall effect, but with only one of the two anomalies (i.e. formula_17). In the second anomaly, the first plateau at \"N=0\" is absent, indicating that bilayer graphene stays metallic at the neutrality point.\n\nUnlike normal metals, graphene's longitudinal resistance shows maxima rather than minima for integral values of the Landau filling factor in measurements of the Shubnikov–de Haas oscillations, whereby the term \"integral\" quantum Hall effect. These oscillations show a phase shift of π, known as Berry’s phase. Berry’s phase arises due to the zero effective carrier mass near the Dirac points. The temperature dependence of the oscillations reveals that the carriers have a non-zero cyclotron mass, despite their zero effective mass.\n\nGraphene samples prepared on nickel films, and on both the silicon face and carbon face of silicon carbide, show the anomalous effect directly in electrical measurements. Graphitic layers on the carbon face of silicon carbide show a clear Dirac spectrum in angle-resolved photoemission experiments. The effect is observed in cyclotron resonance and tunneling experiments.\n\nThe Casimir effect is an interaction between disjoint neutral bodies provoked by the fluctuations of the electrodynamical vacuum. Mathematically it can be explained by considering the normal modes of electromagnetic fields, which explicitly depend on the boundary (or matching) conditions on the interacting bodies' surfaces. Since graphene/electromagnetic field interaction is strong for a one-atom-thick material, the Casimir effect is of interest.\n\nThe Van der Waals force (or dispersion force) is also unusual, obeying an inverse cubic, asymptotic power law in contrast to the usual inverse quartic.\n\nThe electronic properties of graphene are significantly influenced by the supporting substrate. The Si(100)/H surface does not perturb graphene's electronic properties, whereas the interaction between it and the clean Si(100) surface changes its electronic states significantly. This effect results from the covalent bonding between C and surface Si atoms, modifying the π-orbital network of the graphene layer. The local density of states shows that the bonded C and Si surface states are highly disturbed near the Fermi energy.\n"}
{"id": "33442335", "url": "https://en.wikipedia.org/wiki?curid=33442335", "title": "Erminio Sipari", "text": "Erminio Sipari\n\nErminio Sipari (1 December 1879 – 28 January 1968) was an Italian politician and naturalist, author of studies on the preservation of nature and founder of Parco Nazionale d'Abruzzo, which he chaired from 1922 to 1933.\n\nSipari was born in Alvito, Lazio. His family, well known in southern Italy, had many properties in Terra di Lavoro, Abruzzo and Apulia. His father was the brother of Luisa Sipari, the mother of Benedetto Croce, and his mother, Cristina Cappelli, came from a noble family. Erminio studied in Rome and graduated in civil engineering at the University of Turin, after which he specialized in electrical engineering in Liège. Back in Italy in 1905, he established his own engineering firm, based in Rome and Pescasseroli.\n\nIn 1913, when Sipari was first elected in the Italian Parliament, he took the opportunity to draw attention to the real threat of extinction of important species, such as the Abruzzo chamois and the Marsican brown bear, and proposed the creation of a national park in the area of Marsica. In 1921 he was appointed as Undersecretary of State to the Italian Navy. In the same year, he founded the \"Ente Autonomo of the Parco Nazionale d'Abruzzo\", who leased 5 square kilometres of land to be protected in the district of Opi. In September 1922 Sipari opened the Park in Pescasseroli. After, the park was recognized by Italian law (January 1923).\n\nAs its president, Sipari argued that the Abruzzo National Park must protect fauna and flora, but also allow the birth of tourism. He was able to create and manage a park based on sustainable development. He wrote numerous articles on the birth of the National Park and also a comprehensive report, called \"Relazione Sipari\" (1926), which at the time was considered the main promoter of nature conservation in Italy.\n\n"}
{"id": "9070153", "url": "https://en.wikipedia.org/wiki?curid=9070153", "title": "Fernando Villaamil", "text": "Fernando Villaamil\n\nFernando Villaamil Fernández-Cueto (November 23, 1845 – July 3, 1898) was a Spanish naval officer, remembered for his internationally recognized professionalism, for being the designer of the first destroyer warship in history and for his heroic death in the naval Battle of Santiago de Cuba of the Spanish–American War, being the highest ranking Spanish officer to suffer this fate in that event.\n\nFernando Villaamil was born in Serantes, near Castropol (Asturias), in the north of Spain, less than a mile from the Bay of Biscay coastline. He descended from a family of respected noblemen and landowners, but his father found himself almost completely ruined, and had to sell all his property, including the family ancestral home. It seems that this event produced a strong mixed feeling of both affection and anger about his native region in Fernando, which would last for the rest of his life.\n\nIn 1861 he entered the Spanish Navy \"Colegio Naval de San Fernando\", and one year later he went, as midshipman, aboard the frigate \"Esperanza\", the first of the long series of warships on which he would serve his nation until the final \"Furor\".\n\nHe then served in the Philippines and Cuba, the last remains of the Spanish Empire, and in 1873 he was back in Spain and was nominated as a teacher in the Naval School that the Spanish Navy held aboard a frigate anchored in the naval base of Ferrol. Along the following years Villaamil took advantage of the studying and writing opportunities presented by his new job, becoming one of the best known and respected Spanish Navy officers.\n\nIn 1884, Villaamil was appointed Second Officer in the Ministry of the Navy. As such, he took the initiative of studying and designing a new class of warship intended to fight the then-new torpedo boats.\n\nOnce he reached his conclusions on the subject, he obtained the agreement of the Minister of the Navy, Manuel Pezuela, and selected the British shipyards of James & George Thompson, in Clydebank to build the new vessel, beginning in late 1885.\n\nVillaamil was assigned to Great Britain to supervise the works and study the operating procedures of the British naval dockyards, as well as the new Engineers corps. On January 19, 1887, the Destructor, the first torpedo boat destroyer, was formally handed over to the Spanish Navy, with great expectations from the European naval community.\n\nOn the 24 the ship, which had reached in the trials, weighed anchor in Falmouth, bounded for Vigo, with Villaamil in command. Twenty-four hours later, she reached the Spanish coast, making through a stormy Bay of Biscay.\n\nIn one day the doubts about the ship seaworthiness were answered forever, and her designer and commander had every reason to feel proud. As a consequence of the success of the \"Destructor\", Villaamil's professional reputation grew, both in Spain and abroad.\n\nVillaamil was a strong advocate of oceanic sailing as the best training for the young Navy officers and in 1892, being appointed commander of the corvette \"Nautilus\" he took advantage of the celebrations of the fourth centenary of America's discovery to get approval for an instruction cruise around the world, rounding the three Great Capes.\n\nOn November 30, 1892\nthe \"Nautilus\" left Ferrol, in the northwest end of Spain. She rounded Cape Agulhas and Cape Leeuwin, passed through Bass and Cook straits, rounded Cape Horn, went to New York City and eventually, after sailing forty thousand miles, came back to Spain on a shining Sunday, July 16, 1894, in San Sebastián.\n\nHere the sailors suddenly realized that an approaching launch hoisted the royal pennant. It was the Regent Queen and her son, the child King Alfonso XIII, coming to welcome Villaamil and all the \"Nautilus\" crew.\n\nDuring the following years, Villaamil and some other forward-thinking colleagues tried to make the Spanish public aware of the critical deficiencies of the Navy.\n\nThen, events rushed ahead: tension with the United States was rising quickly, and on February 16, 1898—the day following the sudden explosion of the USS \"Maine\" in Havana—Villaamil was appointed Chief of the First Division of torpedo boats and destroyers.\n\nMeanwhile, a totally unrealistic feeling of unbeatable naval power spread over Spain, and the Government decided that a whole fleet, commanded by Admiral Pascual Cervera, should be sent across the Atlantic, contrary to Cervera's and the Spanish Navy's own advice.\n\nVillaamil and his First Division left Cadiz on March 13, and on April 18 they gathered with Admiral Cervera's fleet in the Cape Verde islands.\n\nA month later, on April 24, the United States declared war on Spain, and Cervera received the order to go to the Antilles. Villaamil's Division was split, its destroyers integrated in Cervera's fleet, and the torpedo boats sent back to Spain.\n\nThus, Villaamil was left with no very specific responsibilities. He could have returned to Spain, but he chose to go forward with his fellows, even though he was totally aware of the disaster the ill-prepared fleet was headed for.\n\nHe was always in disagreement with both the Spanish Government's shaky war direction and Cervera's rather passive strategy. Instead, he advocated trying to offset the superiority of the American forces by scattering the fleet and taking the initiative through quick and dispersed daring actions; and he even volunteered to lead an audacious diversionary attack to New York with his destroyers, but his proposals were not accepted.\n\nTherefore, Villaamil had to resign himself unwillingly to be shut with all the fleet in the bay of Santiago de Cuba.\n\nIn the end, on July 3 the whole Spanish fleet came out through the narrow mouth of the bay, ship by ship, almost like a funeral cortege, to be easily destroyed by the waiting American fleet in an eerie kind of target shooting exercise.\n\nVillaamil was killed on board one of his destroyers, the \"Furor\". Francisco Arderius, officer in the ship, reported Fernando Villaamil's end:\n\n"}
{"id": "7726257", "url": "https://en.wikipedia.org/wiki?curid=7726257", "title": "Hesco bastion", "text": "Hesco bastion\n\nThe HESCO MIL is a modern gabion primarily used for flood control and military fortifications. It is made of a collapsible wire mesh container and heavy duty fabric liner, and used as a temporary to semi-permanent levee or blast wall against explosions or small-arms. It has seen considerable use in Iraq and Afghanistan. It was developed in the late 1980s by a British company of the same name.\n\nOriginally designed for use on beaches and marshes for erosion and flood control, the HESCO MIL quickly became a popular security device in the 1990s. HESCO barriers continue to be used for their original purpose. They were used in 2005 to reinforce levees around New Orleans in the few days between Hurricane Katrina and Hurricane Rita. During the June 2008 Midwest floods of HESCO barrier wall were shipped to Iowa. In late March, 2009, of HESCO barrier were delivered to Fargo, North Dakota to protect against floods. In late September, 2016, 10 miles of HESCO barriers were used in Cedar Rapids, Iowa, for the fall flood of 2016.\n\nSpecifically, the brand name for the barrier is \"Concertainer\" (a portmanteau of \"concertina\" and \"container\"), with HESCO Bastion being the company that produces it.\n\nThe HESCO MIL was originally developed by Jimi Heselden, a British entrepreneur and ex-coal miner, who founded HESCO Bastion Ltd. in 1989 to manufacture his invention.\n\nAssembling the HESCO MIL units entails unfolding it and filling it with sand, soil or gravel, usually using a front end loader. The placement of the barrier is generally very similar to the placement of a sandbag barrier or earth berm except that room must generally be allowed for the equipment used to fill the barrier. The main advantage of HESCO barriers, strongly contributing to their popularity with troops and flood fighters, is the quick and easy setup. Previously, people had to fill sandbags, a slow undertaking, with one worker filling about 20 sandbags per hour. Workers using HESCO barriers and a front end loader can do ten times the work of those using sandbags.\n\nThe HESCO barriers come in a variety of sizes. Most of the barriers can also be stacked, and they are shipped collapsed in compact sets. Example dimensions of typical configurations are to .\n\nA new system of HESCO MIL Concertainer developed specially for military use is deployed from a container, which is dragged along the line of ground where the barrier is to be formed, unfolding up to several hundred metres of barrier ready for filling within minutes.\n\n\n"}
{"id": "26172399", "url": "https://en.wikipedia.org/wiki?curid=26172399", "title": "Honshū alpine conifer forests", "text": "Honshū alpine conifer forests\n\nThe Honshū alpine conifer forests ecoregion covers in the high-elevation mountains of central Honshū and the Oshima Peninsula of Hokkaidō, Japan. It is a temperate coniferous forest ecoregion in the Palearctic ecozone.\n\nNorthern Japanese Hemlock grow with species of \"Rhododendron\" and \"Menziesia\". Maries' Fir, Veitch's Fir, and Jezo Spruce grow in forests with plentiful herbs and ferns in their understories. \"Sasa\" grass is very dense in places.\n\nSika deer and Asian black bear inhabit this ecoregion. Significant birds include the rock ptarmigan and the golden eagle.\n\n"}
{"id": "1094615", "url": "https://en.wikipedia.org/wiki?curid=1094615", "title": "Hough function", "text": "Hough function\n\nIn applied mathematics, the Hough functions are the eigenfunctions of Laplace's tidal equations which govern fluid motion on a rotating sphere. As such, they are relevant in geophysics and meteorology where they form part of the solutions for atmospheric and ocean waves. These functions are named in honour of Sydney Samuel Hough.\n\nEach Hough mode is a function of latitude and may be expressed as an infinite sum of associated Legendre polynomials; the functions are orthogonal over the sphere in the continuous case. Thus they can also be thought of as a generalized Fourier series in which the basis functions are the normal modes of an atmosphere at rest.\n\n"}
{"id": "3780225", "url": "https://en.wikipedia.org/wiki?curid=3780225", "title": "Inter Press Service", "text": "Inter Press Service\n\nInter Press Service (IPS) is a global news agency. Its main focus is the production of news and analysis about events and processes affecting economic, social and political development. The agency largely covers news on the Global South, civil society, and globalization.\n\nInter Press Service was set up in 1964 as a non-profit international cooperative of journalists. Its founders were the Italian journalist Roberto Savio and the Argentine political scientist Pablo Piacentini. Initially, the primary objective of IPS was to fill the information gap between Europe and Latin America after the political turbulence following the Cuban revolution of 1959 (Giffard in Salwen and Garrison, 1991).\n\nLater, the network expanded to include all continents, beginning with a Latin American base in Costa Rica in 1982, and extended its editorial focus. In 1994, IPS changed its legal status to that of a \"public-benefit organization for development cooperation\".\n\nIn 1996 IPS had permanent offices and correspondents in 41 countries, covering 108 nations. It had as subscribers over 600 print media, around 80 news agencies and database services, and 65 broadcast media, in addition to over 500 NGOs and institutions.\n\nIPS’s stated aims are to give prominence to the voices of marginalized and vulnerable people and groups, report from the perspectives of developing countries, and to reflect the views of civil society. The mainstreaming of gender in reporting and the assessment of the impacts of globalization are a priority.\n\nIPS may be unique in its concentration on developing countries and the strong relationships with civil society. For this reason, IPS has even been termed the probably \"largest and most credible of all 'alternatives' in the world of news agencies\" (Boyd-Barrett and Rantanen, 1998: 174/5), being the \"first and only independent and professional news agency which provides on a daily basis information with a Third World focus and point of view\" (Boyd-Barrett and Thussu, 1992: 94; cf. Giffard, 1998: 191; Fenby, 1986).\n\nDespite all the laudable aims, it is, however, important to see that IPS has never possessed the resources to be a major player in the international media landscape. Because of its focus on longer background pieces focusses on development issues impacting the lives of people in the South instead of concise news, it is yet to be a news provider for mainstream media in developed countries. In fairness to IPS, mainstream media often rely on their own fly in and out journalists from where IPS reports.\n\nIPS is registered as an international not-for-profit association. It has 'general' NGO consultative status with ECOSOC at the United Nations, and the OECD status of \"ODA eligible international organization\".\n\nFive editorial desks coordinate the network of journalists around the world: Montevideo (regional bureau for Latin America), Berlin-London (Europe and the Mediterranean), Bangkok (Asia and the Pacific), New York (North America and the Caribbean) and Johannesburg (Africa). Most of IPS's journalists and editors are native to the country or region in which they are working.\n\nIPS receives funding from various sources: through its subscribers and media clients, as beneficiary of multilateral and national development cooperation programmes, and as recipient of project financing from foundations. It is not, as most other agencies, financed by a country or a group of newspapers. Hence, the agency’s budget is comparatively small. Still it manages to be \"roughly the sixth largest international news-gathering organization\" (Rauch, 2003: 89).\n\nThe actual role of IPS in the international mediascape is hard to assess. Clipping services are expensive, and do not exist in many countries where IPS is strong. Additionally, in some countries news agencies are not credited in bylines. One study by the UN's Food and Agriculture Organization for media coverage of the FAO in 1991 found that of the nearly 3000 clippings with news agency bylines, 13% credited IPS, making it the third-most cited agency. IPS reports were collected from 138 different publications in 39 countries - more countries than any other agency. IPS was particularly strong in Latin America - 72% of clippings from Latin America with news agency bylines came from IPS.\n\n\n\n\n\n"}
{"id": "972312", "url": "https://en.wikipedia.org/wiki?curid=972312", "title": "Liquid air", "text": "Liquid air\n\nLiquid air is air that has been cooled to very low temperatures (cryogenic temperatures), so that it has condensed into a pale blue mobile liquid. To thermally insulate it from room temperature, it is stored in specialized containers (Vacuum insulated flasks are often used). Liquid air can absorb heat rapidly and revert to its gaseous state. It is often used for condensing other substances into liquid and/or solidifying them, and as an industrial source of nitrogen, oxygen, argon, and other inert gases through a process called air separation.\n\nLiquid air has a density of approximately 870 kg/m (0.87 g/cm), though the density may vary depending on the elemental composition of the air. Since dry gaseous air contains approximately 78% nitrogen, 21% oxygen, and 1% argon, the density of liquid air at standard composition is calculated by the percentage of the components and their respective liquid densities (see liquid nitrogen and liquid oxygen). Although air contains trace amounts of carbon dioxide (about 0.040%), this gas sublimes (transfers directly between gas and solid, and therefore does not exist as a liquid) at pressures less than 5.1 atmospheres.\n\nThe boiling point of liquid air is -194.35 °C, intermediate between the boiling points of liquid nitrogen and liquid oxygen. However, it can be difficult to keep at a stable temperature as the liquid boils, since the nitrogen will boil off first, leaving the mixture oxygen-rich and changing the boiling point. This may also occur in some circumstances due to the liquid air condensing oxygen out of the atmosphere.\n\nLiquid air freezes at approximately 58 K (-215 °C)(-355 F), also at standard atmospheric pressure.\n\nThe constituents of air were once known as \"permanent gases\", as they could not be liquified solely by compression at room temperature. A compression process will raise the temperature of the gas. This heat is removed by cooling to the ambient temperature in a heat exchanger, and then expanding by venting into a chamber. The expansion causes a lowering of the temperature, and by counter-flow heat exchange of the expanded air, the pressurized air entering the expander is further cooled. With sufficient compression, flow, and heat removal, eventually droplets of liquid air will form, which may then be employed directly for low temperature demonstrations.\n\nThe main constituents of air were liquefied for the first time by Polish scientists Zygmunt Florenty Wróblewski and Karol Olszewski in 1883.\n\nDevices for the production of liquid air are simple enough to be fabricated by the experimenter using commonly available materials.\n\nThe most common process for the preparation of liquid air is the two-column Hampson–Linde cycle using the Joule–Thomson effect. Air is fed at high pressure (>60 psig, or 520 kPa) into the lower column, in which it is separated into pure nitrogen and oxygen-rich liquid. The rich liquid and some of the nitrogen are fed as reflux into the upper column, which operates at low pressure (<10 psig, or 170 kPa), where the final separation into pure nitrogen and oxygen occurs. A raw argon product can be removed from the middle of the upper column for further purification.\n\nIn manufacturing processes, the liquid air product is fractionated into its constituent gases in either liquid or gaseous form, as the oxygen is especially useful for fuel gas welding and cutting, and the argon is useful as an oxygen-excluding shielding gas in gas tungsten arc welding. Liquid nitrogen is useful in various low-temperature applications, being nonreactive at normal temperatures (unlike oxygen), and boiling at .\n\nBetween 1899 and 1902, the automobile Liquid Air was produced and demonstrated by a joint American/English company, with the claim that they could construct a car that would run a hundred miles on liquid air.\n\nOn 2 October 2012, the Institution of Mechanical Engineers said liquid air could be used as a means of storing energy. This was based on a technology that was developed by Peter Dearman, a garage inventor in Hertfordshire, England to power vehicles.\n\n\n"}
{"id": "16407742", "url": "https://en.wikipedia.org/wiki?curid=16407742", "title": "List of Dungeons &amp; Dragons monsters (1977–99)", "text": "List of Dungeons &amp; Dragons monsters (1977–99)\n\nThe following is a list of monsters that appeared in various books and supplements for the \"Basic\" version of \"Dungeons & Dragons\" from the release of the first \"Basic Set\" in 1977 until the end of the line in 1994.\n\nThe \"Dungeons & Dragons Basic Set\", edited by John Eric Holmes, was first released in 1977, as an introductory set for new players. While players were intended to go onto \"Advanced Dungeons & Dragons\" once the play options in the \"Basic Set\" were exhausted, as stated in the book, many players found they preferred the simpler play style of the basic game. The \"Basic Set\" was based upon the original work published in 1974 and three of the supplementary booklets to the original set. Much like the first edition \"Advanced Dungeons & Dragons\" \"Monster Manual\", also released in 1977, this book collected many of the monsters from the previous D&D supplements, and included the stat lines on the same page as the monsters' descriptions. The monster descriptions are found on pages 22–33 of the book. Most descriptions do not feature illustrations.\n\nThe second version of the \"D&D Basic Set\", edited by Tom Moldvay, was first printed in 1981, and is generally known as the \"red book\".This revision solidified the split between the \"Basic\" and \"Advanced\" lines of the game. The page numbers are prefixed with B; creature descriptions are found on pages B30 through B44. Few of the descriptions include illustrations, and most descriptions are limited to one paragraph.\n\nThe \"D&D Expert Rules\" were first printed in 1981, and is generally known as the \"blue book\". The new rules allowed for outdoor, wilderness-based adventures (over and above dungeon adventures), and for characters up to 14th level. The page numbers are prefixed with X; creature descriptions are found on pages X28 through X42. Again, few of the descriptions include illustrations, and most descriptions are limited to one paragraph. ().\n\nA fifth edition of the Basic Rules was printed in 1983. This version split the rules into two books, the Player's Manual and the Dungeon Master's Rulebook, and was sold as a boxed set. Creature descriptions are provided on pages 25–39 of the Dungeon Master's Rulebook; with some minor changes to order and naming conventions and some removals, they duplicate the entries found in the 1981 edition. Once again the descriptions are generally limited to one paragraph, with few illustrations.\n\nA second edition of the Expert Rules was printed in 1983, and again was sold as a boxed set. Creature descriptions are provided on pages 46–57 of the Expert Rulebook; with some minor changes to order and naming conventions and several removals, they duplicate the entries found in the 1981 edition. Once again the descriptions are generally limited to one paragraph, with few illustrations.\n\nThe Dungeon Master's Companion divides creatures into two groups: those from the Prime Plane and those from Other Planes. These are listed separately below. The creature descriptions are found on pages 28–42 of the book; again, most descriptions do not include illustrations. Notably, the first list of creatures is not presented in alphabetical order. Neither the Companion Set nor its individual books (Player's and Dungeon Master's Companions) have an ISBN.\n\nThe Master DM's Book divides creatures into two groups: again, those from the Prime Plane and those from Other Planes. These are listed separately below. The creature descriptions are found on pages 24–41 of the book; again, most descriptions do not include illustrations. Neither the Master Set nor its individual books (Master Player's and Master DM's Books) have an ISBN.\n\nThe Immortals DM's Book provides a single section containing new or previously published creatures, listed below. The creature descriptions are found on pages 28–50 of the book; again, most descriptions do not include illustrations. Neither the Immortals Set nor its individual books (Immortals Player's and Immortals DM's Books) have an ISBN.\n\n\"The Palace of the Silver Princess\" was the only D&D module ever published by TSR to be recalled immediately after distribution, ostensibly for editorial issues and questionable artwork. Two versions of the module exist: the extremely rare orange-cover original printing (available for download as a PDF from Wizards of the Coast), and the more common green-cover revised edition. The original version added 13 new creatures; two of these survived in the revised edition, but the majority were removed. The original version of the module was issued as . (Page numbers provided below are from the Wizards of the Coast PDF download; page numbers in parentheses refer to the revised edition.) The updated green-cover version of the module contained only 3 new creatures, and was issued with the same ISBN.\n\n\"The Lost City\" module, published in 1982, adds 4 new creatures, on pages 25–27 of the module booklet, with limited illustrations and short descriptions.\n\n\"Horror on the Hill\", published in 1983, added three new creatures, on pages 31 and 32 of the module booklet. The descriptions included illustrations of all three new monsters.\n\n\"The Veiled Society\" module, published in 1984, added a single new creature, with illustration.\n\n\"Rahasia\", published in 1984, added three new or previously published creatures on page 28, and three additional NPC characters on page 29. The three additional creatures are listed below.\n\n\"Journey to the Rock\", published in 1984, added seven new or previously published creatures on pages 31 and 32.\n\n\"Night's Dark Terror\", published in 1986, was billed as a special Basic/Expert transition module from the United Kingdom. It added nine new or previously published creatures, with illustrations, on pages 54–56.\n\n\"In Search of Adventure\", published in 1987, was an abridged 160-page compilation module containing the majority of elements from modules B1 through B9. \"New\" monsters, previously published in modules B1 through B9 or in the D&D Expert Rules, were included in a section on pages 124–128.\n\n\"King's Festival\", published in 1989, was billed as an introductory module specifically designed for inexperienced and new players. It incorporated two previously published creatures on page 31.\n\n\"Queen's Harvest\", published in 1989, was billed as an introductory module specifically designed for inexperienced and new players. It incorporated three new or previously published creatures on page 32.\n\n\"The Isle of Dread\" was the first module published for use with the D&D Expert Rules, in 1981. It contained a New Monsters section on pages 28–30, containing 15 previously unpublished creatures. This module was included in the Expert Rules boxed set. It was reprinted in 1983, and issued in the boxed set with the 1983 version of the Expert Rules. The 1983 version included all the creatures from the original 1981 edition (on pages 30–32), and added one more. The creatures from both versions are listed in the table below. Page numbers in () are those for the 1983 edition.\n\n. (1981 edition)\n\n. (1983 edition)\n\nCastle Amber was the second module published for use with the D&D Expert Rules, in 1981. It contained a New Monsters section on pages 24–27, adding 17 new monsters either previously unseen or published in D&D Expert Module X1. .\n\n\"Curse of Xanathon\", published in 1982, added a single new creature on page 30.\n\nThe first of a two-part series, \"Master of the Desert Nomads\", was published in 1983 and included a New Monsters section on pages 29–31 of the module, containing five new creature descriptions with illustrations.\n\nThe conclusion to the series started with module X4, \"Temple of Death\" was published in 1983 and included a New Monsters section on pages 27–29, containing five previously unpublished creatures, with illustrations.\n\n\"Quagmire!\", published in 1984, added six new or previously published creatures on page 29.\n\n\"The War Rafts of Kron\" was published in 1984 and included a New Monsters section on pages 29–31, containing 14 new or previously published creatures.\n\n\"Drums on Fire Mountain\" was published in 1984 and included a New Monsters section on pages 27–28, containing 5 previously unpublished creatures.\n\n\"The Savage Coast\" was published in 1985 and included a New Monsters section on pages 27–29, containing 13 new or previously published creatures, with illustrations.\n\n\"Red Arrow, Black Shield\" was published in 1985 and included a New Monsters section on page 40, containing 2 previously published creatures with illustrations.\n\n\"Saga of the Shadow Lord\" was published in 1986 and included a New Monsters section on page 61, containing 3 previously unpublished creatures.\n\n\"Skarda's Mirror\" was published in 1987 and included a single previously unpublished creature on page 44.\n\n\"Crown of Ancient Glory\" was published in 1986 and included a New Monsters section on page 62-63, containing 5 new or previously published creatures.\n\n\"Quest for the Heartstone\" was published in 1984 and included a New Monsters section on pages 31–32, containing 4 previously unpublished creatures, with illustrations.\n\n\"Thunderdelve Mountain\" was a solo adventure published in 1985; it included a New Monsters section on page 38, containing 3 previously unpublished creatures.\n\n\"Blizzard Pass\" was a Basic Set solo D&D module published in 1983 which included a New Monsters section on page 31, containing two previously unpublished creatures with illustrations.\n\n\"Maze of the Riddling Minotaur\" was an Expert Set solo D&D module published in 1983 which included a New Monsters section on the interior cover, containing 5 previously unpublished creatures with illustrations.\n\n\"Blade of Vengeance\" was a one-on-one adventure published in 1984; it included a New Monsters section on pages 27 and 28, containing 4 previously unpublished creatures.\n\n\"Death's Ride\" was published in 1984 and included a New Monsters section on page 26-27, containing one previously unpublished creature with illustration.\n\n\"Sabre River\" was published in 1984 and included a New Monsters section on page 32, containing one previously unpublished creature.\n\n\"Mystery of the Snow Pearls\" was published in 1985 and included a New Monsters section on page 32, containing two previously unpublished creatures with illustrations.\n\n\"Where Chaos Reigns\" was published in 1985 and included a New Monsters section on a removable sheet, containing 3 previously unpublished creatures with illustrations.\n\n\"The Endless Stair\" was published in 1987 and included a New Monsters section on pages 28–30, containing 4 previously unpublished creatures with illustrations.\n\n\"Legacy of Blood\" was published in 1987 and included a New Monsters section on pages 27–28 containing 5 previously unpublished creatures with illustrations.\n\n\"Into the Maelstrom\" was published in 1985 and included a New Monsters section on page 29, containing one previously unpublished creature.\n\n\"Vengeance of Alphaks\" was published in 1986 and included a New Monsters section on page 30, containing two previously unpublished creatures.\n\n\"Talons of Night\" was published in 1987 and included a New Monsters section on pages 42–44 containing 5 new or previously published creatures with illustrations.\n\n\"Creature Catalogue\" was the first full book of monsters for the basic edition of the Dungeons & Dragons game, published in 1986 after the Basic, Expert, Companion, and Master sets. In this book is collected all the creatures first presented in the official Dungeons & Dragons adventure modules to that time, plus many new creatures. Pages 3–7 contain an Introduction, pages 8–10 contains a comprehensive index of all D&D monsters found in this book, and the Basic, Expert, Companion and Master rule sets, pages 11–89 contain the monster descriptions, pages 90–91 contain a creature \"type\" index, and pages 92–96 contain an index to creatures by habitat. The monsters are divided into six sections, arranged by creature type: Animals (pages 11–21), Conjurations (pages 22–31), Humanoids (pages 32–48), Lowlifes (pages 49–58), Monsters (pages 59–81), and Undead (pages 82–87).\n\nThe Dungeons & Dragons Rules Cyclopedia was published in 1991. It was intended as an encyclopedia of all the major rules for the basic Dungeons & Dragons game up to that point, including most of the information appearing in the previous boxed set. Chapter 14 of the book, pages 152–218, presents most of the monsters used in the basic D&D game once again. Several pages of that chapter describe how to use the statistics, as well as providing suggestions on how to use the monsters. The actual monster descriptions are found on pages 156–213 of the book.\n"}
{"id": "5856482", "url": "https://en.wikipedia.org/wiki?curid=5856482", "title": "List of Sites of Special Scientific Interest by Area of Search", "text": "List of Sites of Special Scientific Interest by Area of Search\n\nThe following is a list of Sites of Special Scientific Interest (SSSI) by Area of Search, in the United Kingdom. SSSIs are areas of conservation, consisting of protected areas, recognised for their biological or geological significance. In Northern Ireland an SSSI is called an Area of Special Scientific Interest (ASSI).\n\nThe English counties were revised under the 1974 reorganisation of local government. Until the 2010s, Natural England, which maintains the database of English SSSIs, kept the listing of counties as it was in 1974, but by 2015 they had updated their lists to reflect later changes. For example, Avon was abolished in 1996 and is now divided by Natural England between Somerset, Gloucestershire and the \"County of Bristol\"; Hereford and Worcester was abolished in 1998 and divided into the two historic counties. The list of English counties below has not been updated to reflect these changes and is still based on the 1974 counties. \n\nNatural England lists of SSSIs for each Area of Search in England:\n"}
{"id": "10969924", "url": "https://en.wikipedia.org/wiki?curid=10969924", "title": "List of Superfund sites in Delaware", "text": "List of Superfund sites in Delaware\n\nThis is a list of Superfund sites in Delaware designated under the Comprehensive Environmental Response, Compensation, and Liability Act (CERCLA) environmental law. The CERCLA federal law of 1980 authorized the United States Environmental Protection Agency (EPA) to create a list of polluted locations requiring a long-term response to clean up hazardous material contaminations. These locations are known as Superfund sites, and are placed on the National Priorities List (NPL). \n\nThe NPL guides the EPA in \"determining which sites warrant further investigation\" for environmental remediation. As of December 16, 2010, there were 14 Superfund sites on the National Priorities List in Delaware. One additional site has been proposed for entry on the list. Six sites have been cleaned up and removed from the list.\n\n\n"}
{"id": "970757", "url": "https://en.wikipedia.org/wiki?curid=970757", "title": "List of ecoregions in Australia", "text": "List of ecoregions in Australia\n\nEcoregions in Australia are geographically distinct plant and animal communities, defined by the World Wide Fund for Nature based on geology, soils, climate, and predominant vegetation.\n\nThe World Wide Fund for Nature (WWF) identified 825 terrestrial ecoregions that cover the Earth's land surface, 40 of which cover Australia and its dependent islands. The WWF ecoregions are classified by biome type (Tropical and subtropical moist broadleaf forests, Temperate grasslands, savannas, and shrublands, tundra, etc.), and into one of eight terrestrial ecozones. Australia, together with New Zealand, New Guinea and neighboring island groups, is part of the Australasia ecozone. The IBRA bioregions informed the delineation of the WWF ecoregions for Australia, and the WWF ecoregions generally follow the same ecoregion boundaries, while often clustering two or more similar bioregions into a larger ecoregion. The ecoregion articles in Wikipedia generally follow the WWF scheme.\n\nThe WWF ecoregions are based heavily upon the Interim Biogeographic Regionalisation for Australia (IBRA) regionalisation. Like the IBRA, it was developed for use as a planning tool for conservation science, with the goal of establishing a system of nature reserves in each of the ecoregions or bioregions sufficient to preserve biodiversity. Both systems also have a prioritization system for establishing preserves; the WWF designated its Global 200 ecoregions as priorities for conservation, and the Department of Environment and Heritage ranks its bioregions high, medium, or low priority, based on \"the potential value land reservation in those regions would add to the development of a comprehensive, adequate and representative reserve system for Australia.\"\n\nTropical and subtropical moist broadleaf forests\nTemperate broadleaf and mixed forests\nTropical and subtropical grasslands, savannas, and shrublands\nTemperate grasslands, savannas, and shrublands\nMontane grasslands and shrublands\nTundra\nMediterranean forests, woodlands, and scrub\nDeserts and xeric shrublands\n\n"}
{"id": "40271292", "url": "https://en.wikipedia.org/wiki?curid=40271292", "title": "List of extreme temperatures in Vatican City", "text": "List of extreme temperatures in Vatican City\n\nThe following list shows the readings of the maximum and minimum temperatures for each year from 1862 to the present, recorded in the weather station of the Collegio Romano in Rome, established in 1788. The station, actually located in the Italian territory, was opened when Rome was part of the Papal States. The first weather station in the Vatican state was opened only in 2009, and is placed in the Palace of the Governorate of Vatican City.\n\n"}
{"id": "43220172", "url": "https://en.wikipedia.org/wiki?curid=43220172", "title": "List of largest cosmic structures", "text": "List of largest cosmic structures\n\nThis is a list of the largest cosmic structures so far discovered. The unit of measurement used is the light-year (distance traveled by light in one Julian year; approximately 9.46 trillion kilometres).\n\nThis list includes superclusters, galaxy filaments and large quasar groups (LQGs). The list characterizes each structure based on its longest dimension.\n\nNote that this list refers only to coupling of matter with defined limits, and not the coupling of matter in general (as per example the cosmic microwave background, which fills the entire universe). All structures in this list are defined as to whether their presiding limits have been identified.\n\nThere are some speculations about this list:\n\n\nVoids are immense spaces between galaxy filaments and other large-scale structures. Technically they are not structures. They are vast spaces which contain very few, or no galaxies. They are theorized to be caused by quantum fluctuations during the early formation of the universe.\n\nA list of the largest voids so far discovered is below. Each is ranked according to its longest dimension.\n"}
{"id": "44436435", "url": "https://en.wikipedia.org/wiki?curid=44436435", "title": "List of natural gas and oil production accidents in the United States", "text": "List of natural gas and oil production accidents in the United States\n\nThis article is intended as a companion to the Wikipedia article List of pipeline accidents in the United States. Large accidents, reported in the Wikipedia article, Industrial Disasters, are included. The production process encompasses all parts of the process from drilling for fuels to refining or processing to the final product. It also includes storage and disposal of waste. Unless otherwise stated, all accidents are associated with production wells.\n\nSince many accidents involve transport of raw materials, several states included in this list have little or no fossil fuel production.\n\nUseful web sites include:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n– Major crude oil train derailment in Mosier, a small town in the Columbia gorge spilled and burst into flames June 2016. The accident was due to railway dysfunction and bad parts. As of now, groundwater contamination is found at the site impacting wildlife.\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "27574421", "url": "https://en.wikipedia.org/wiki?curid=27574421", "title": "List of planetary features with Māori names", "text": "List of planetary features with Māori names\n\nIn the International Astronomical Union's system of unique feature names for topographic and albedo features on planets and moons in the Solar System, many are named in the language of the indigenous Māori people of New Zealand. These names are primarily from Māori mythology.\n\nFeatures on the planet Venus are named for women and goddesses.\n\"Colles\" are small hills or knobs, named after sea goddesses. \n\"Coronae\" are oval features of uncertain origin, named for fertility and earth goddesses.\n\"Planitiae\", low plains, are mythological heroines. \n\"Tholi\", small dome-like hills, are named for (miscellaneous) goddesses. \n\"Valles\", valleys, are named according to their length: if more than 400 km, after the word for the planet in various world languages, otherwise after river goddesses.\n\n\nCraters on Venus that have diameters less than 20 km are named with common female first names:\n\n\n\nThe innermost moon of Jupiter, Io is a world emblazoned with the reds, yellows, whites and blacks of sulphur chemistry and the scars of active volcanism. Its features have a wide variety of naming themes: gods, goddesses and heroes associated with fire, sun, thunder, and volcanoes; mythical blacksmiths; people associated with the myth of Io; or people from Dante's Inferno. Names may also be derived from a nearby, more prominent feature; this is a situation where the same name could be used for both a volcano and an adjacent valley.\n\n\"Montes\", mountains, can be named in the Dantean or Greek categories, or for associated features.\n\"Paterae\", volcanic pits with flat floors and steep walls, are named from these first two groups, and include names from their associated eruptive center.\n\"Valles\", valleys, are named for their associated feature.\n\nThe largest moon of Saturn is a world in its own right: Titan has a dense atmosphere and complex weather system, with liquid-carved river networks and sizable seas.\n\n\"Maria\", seas, are large expanses of dark materials thought to be liquid hydrocarbons; they are named for sea creatures from myth and literature. \"Virgae\", streaks or stripes of colour, are named for rain gods and goddesses.\n\nNeptune's largest moon is thought to be a captured Kuiper belt object, an interloper from further out in the Solar System. Triton's features are given aquatic names, excluding Roman and Greek references.\n"}
{"id": "1469833", "url": "https://en.wikipedia.org/wiki?curid=1469833", "title": "List of rivers of Luxembourg", "text": "List of rivers of Luxembourg\n\nThese are the main rivers of Luxembourg.\n\nAll of Luxembourg's rivers are drained into the North Sea, most via the Moselle River, except in the extreme south-west of the country, which are drained by the Chiers. Rivers that flow into the sea are sorted alphabetically. Rivers that flow into other rivers are sorted by the proximity of their points of confluence to the sea. Some rivers (e.g. Meuse, Rhine) do not flow through Luxembourg themselves, but they are mentioned for having tributaries from Luxembourg. They are given in \"italics\". For an alphabetical list of rivers of Luxembourg see .\n"}
{"id": "4315644", "url": "https://en.wikipedia.org/wiki?curid=4315644", "title": "List of rivers of Pakistan", "text": "List of rivers of Pakistan\n\nThis is a list of rivers wholly or partly in Pakistan, organised geographically by river basin, from west to east. Tributaries are listed from the mouth to the source. The longest and the largest river in Pakistan is the Indus River. Around two-thirds of water supplied for irrigation and in homes come from the Indus and its associated rivers.\n\n\"Some of these rivers flow only during the rainy season, so for part of the year the water may or may not reach the sea.\"\n\n\n\n"}
{"id": "8591690", "url": "https://en.wikipedia.org/wiki?curid=8591690", "title": "List of stars in Hydra", "text": "List of stars in Hydra\n\nThis is the list of notable stars in the constellation Hydra, sorted by decreasing brightness.\n\n\n"}
{"id": "33445477", "url": "https://en.wikipedia.org/wiki?curid=33445477", "title": "List of tallest mountains in the Solar System", "text": "List of tallest mountains in the Solar System\n\nThis is a list of the tallest mountains in the Solar System. The tallest peak or peaks on worlds where significant mountains have been measured are given; in some cases, the tallest peaks of different classes on a world are also listed. At 21.9 km, the enormous shield volcano Olympus Mons on Mars is the tallest mountain on any planet in the Solar System. For 40 years, following its discovery in 1971, it was the tallest mountain known in the Solar System. However, in 2011, the central peak of the crater Rheasilvia on the asteroid and protoplanet Vesta was found to be of comparable height.\n\nThe heights are given from base to peak, because there is no nonarbitrary equivalent to height above sea level on other worlds.\n\nThe following images are shown in order of decreasing base-to-peak height.\n\n"}
{"id": "49546069", "url": "https://en.wikipedia.org/wiki?curid=49546069", "title": "List of the most prominent summits of Canada", "text": "List of the most prominent summits of Canada\n\nThe following sortable table comprises the 150 most topographically prominent mountain peaks of Canada.\n\nThe summit of a mountain or hill may be measured in three principal ways:\n\nMount Logan exceeds of topographic prominence. Five peaks of Canada exceed , 11 exceed , 41 exceed , and 143 ultra-prominent summits exceed of topographic prominence.\n\nOf these 150 most prominent summits of Canada, 108 are located in British Columbia, 20 in Yukon, 16 in Nunavut, ten in Alberta, and one in the Northwest Territories. Three of these summits lie on the British Columbia-Alberta border and two lie on the British Columbia-Yukon border. Four of these summits lie on the international Yukon-Alaska border and four lie on the international British Columbia-Alaska border.\n\n\n"}
{"id": "2342780", "url": "https://en.wikipedia.org/wiki?curid=2342780", "title": "Little Rann of Kutch", "text": "Little Rann of Kutch\n\nLittle desert of Kutch is a salt marsh which is part of Rann of Kutch in Kutch district, Gujarat, India.\n\nIt is famous as the world's last refuge of the Indian wild ass (khur) for the conservation of which it has been declared as the Indian Wild Ass Sanctuary. Though a bleak landscape it is rich in biodiversity and is an ecologically important area for wildlife and many local and migratory waterbirds like cranes, ducks, pelicans, flamingos and land birds like sandgrouse, francolins and the Indian bustards.It is also home to various unique mammals apart from wild ass such as the Indian wolf (\"Canis lupus pallipes\"), desert fox (\"Vulpes vulpes pusilla\") and nilgai (\"Boselaphus tragocamelus\").\n\nIt is nominated by the forest department to be a biosphere reserve which are areas of terrestrial and coastal ecosystems internationally recognized within the framework of UNESCO's Man and Biosphere (MAB) programme. it will focus on conserving biological diversity, research, monitoring and providing sustainable development models, the proposal has been sent to and listed at UNESCO.\n\nThere is traditional commercial salt panning activity in the region which the state of Gujarat's Forest Department wants to discourage as it is considered a threat to ecology of the region, wildlife and to the endangered Indian wild ass.\n\nThe area is also being used to farm shrimp which is more profitable then salt panning, again this activity is discouraged by the forest department.\n\nGujarat government is studying to dam the 1.26 km stretch of Hadakiya Creek of Rann of Kutch area so that impounded monsoon water is used for recreational and fisheries purposes.\n\nFrom the city of Bhuj various ecologically rich and wildlife conservation areas of the Kutch / Kachchh district can be visited such as Indian Wild Ass Sanctuary, Kutch Desert Wildlife Sanctuary, Narayan Sarovar Sanctuary, Kutch Bustard Sanctuary, Banni Grasslands Reserve and Chari-Dhand Wetland Conservation Reserve etc.\n\n\n\n"}
{"id": "1112787", "url": "https://en.wikipedia.org/wiki?curid=1112787", "title": "Mean high water spring", "text": "Mean high water spring\n\nThe mean high water springs (MHWS) is the highest level that spring tides reach on the average over a period of time (often 19 years). The height of mean high water springs is the average throughout the year (when the average maximum declination of the moon is 23.5°) of two successive high waters during those periods of 24 hours when the range of the tide is at its greatest. \n\nThis level is generally close to being the \"high water mark\" where debris accumulates on the shore annually.\n\n"}
{"id": "12321716", "url": "https://en.wikipedia.org/wiki?curid=12321716", "title": "Oil shale industry", "text": "Oil shale industry\n\nThe oil shale industry is an industry of mining and processing of oil shale—a fine-grained sedimentary rock, containing significant amounts of kerogen (a solid mixture of organic chemical compounds), from which liquid hydrocarbons can be manufactured. The industry has developed in Brazil, China, Estonia and to some extent in Germany and Russia. Several other countries are currently conducting research on their oil shale reserves and production methods to improve efficiency and recovery. Estonia accounted for about 70% of the world's oil shale production in a study published in 2005.\n\nOil shale has been used for industrial purposes since the early 17th century, when it was mined for its minerals. Since the late 19th century, shale oil has also been used for its oil content and as a low grade fuel for power generation. However, barring countries having significant oil shale deposits, its use for power generation is not particularly widespread. Similarly, oil shale is a source for production of synthetic crude oil and it is seen as a solution towards increasing domestic production of oil in countries that are reliant on imports.\n\nOil shale has been used since ancient times. Modern industrial oil shale mining began in 1837 at the Autun mines in France, followed by Britain, Germany and several other countries. The oil shale industry started growing just before World War I because of the mass production of automobiles and trucks and the supposed shortage of gasoline for transportation needs. In 1924, the Tallinn Power Plant was the first power plant in the world to switch to oil shale firing.\n\nFollowing the end of World War II, the oil shale industry declined due to the discovery of large supplies of easily accessible and cheaper crude oil. Oil shale production however, continued to grow in Estonia, Russia and China.\n\nFollowing the 1973 oil crisis, the oil shale industry was restarted in several countries, but in the 1980s, when oil prices fell, many industries faced closure. The global oil shale industry has grown again from the mid-1990s. In 2003, the oil shale development program was initiated in the United States, and in 2005, the commercial leasing program for oil shale and tar sands was introduced.\n\nAs of May 2007, Estonia is actively engaged in exploitation of oil shale on a significant scale and accounts for 70% of the world's processed oil shale. Estonia is unique in that its oil shale deposit account for just 17% of total deposits in European Union but it generates 90% of its power from oil shale. Oil shale industry in Estonia employs 7,500 people, which is about 1% of national employment, accounting for 4% of its gross domestic product.\n\nOil shale is mined either by traditional underground mining or surface mining techniques. There are several mining methods available, but the common aim of all these methods is to fragment the oil shale deposits in order to enable the transport of shale fragments to a power plant or retorting facility. The main methods of surface mining are \"open pit mining\" and \"strip mining\". An important method of sub-surface mining is the \"room-and-pillar method\". In this method, the material is extracted across a horizontal plane while leaving \"pillars\" of untouched material to support the roof. These pillars reduce the likelihood of a collapse. Oil shale can also be obtained as a by-product of coal mining.\n\nThe largest oil shale mine in the world is the Estonia Mine, operated by Enefit Kaevandused. In 2005, Estonia mined 14.8 million tonnes of oil shale. During the same period, mining permits were issued for almost 24 million tonnes, with applications being received for mining an additional 26 million tonnes. In 2008, the Estonian Parliament approved the \"National Development Plan for the Use of Oil Shale 2008-2015\", which limits the annual extraction of oil shale to 20 million tonnes.\n\nOil shale can be used as a fuel in thermal power plants, wherein oil shale is burnt like coal to drive the steam turbines. As of 2012, there are oil shale-fired power plants in Estonia with a generating capacity of 2,967 megawatts (MW), China, and Germany.\nAlso Israel, Romania and Russia have run oil shale-fired power plants, but have shut them down or switched to other fuels like natural gas. Jordan and Egypt have announced their plans to construct oil shale-fired power plants, while Canada and Turkey plan to burn oil shale at the power plants along with coal.\n\nThermal power plants which use oil shale as a fuel mostly employ two types of combustion methods. The traditional method is \"Pulverized combustion\" (PC) which is used in the older units of oil shale-fired power plants in Estonia, while the more advanced method is \"Fluidized bed combustion\" (FBC), which is used in the Holcim cement factory in Dotternhausen, Germany, and was used in the Mishor Rotem power plant in Israel. The main FBC technologies are \"Bubbling fluidized bed combustion\" (BFBC) and \"Circulating fluidized bed combustion\" (CFBC).\n\nThere are more than 60 power plants around the world, which are using CFBC technology for combustion of coal and lignite, but only two new units at Narva Power Plants in Estonia, and one at Huadian Power Plant in China use CFBC technology for combustion of oil shale. The most advanced and efficient oil shale combustion technology is \"Pressurized fluidized-bed combustion\" (PFBC). However, this technology is still premature and is in its nascent stage.\n\nThe major shale oil producers are China and Estonia, with Brazil a distant third, while Australia, USA, Canada and Jordan have planned to set up or restart shale oil production. According to the World Energy Council, in 2008 the total production of shale oil from oil shale was 930,000 tonnes, equal to , of which China produced 375,000 tonnes, Estonia 355,000 tonnes, and Brazil 200 tonnes. In comparison, production of the conventional oil and natural gas liquids in 2008 amounted 3.95 billion tonnes or .\n\nAlthough there are several oil shale retorting technologies, only four technologies are currently in commercial use. These are Kiviter, Galoter, Fushun, and Petrosix. The two main methods of extracting oil from shale are \"ex-situ\" and \"in-situ\". In \"ex-situ\" method, the oil shale is mined and transported to the retort facility in order to extract the oil. The \"in-situ\" method converts the kerogen while it is still in the form of an oil shale deposit, and then extracts it via a well, where it rises up as normal petroleum.\n\nOil shale is used for cement production by Kunda Nordic Cement in Estonia, by Holcim in Germany, and by Fushun cement factory in China. Oil shale can also be used for production of different chemical products, construction materials, and pharmaceutical products, e.g. ammonium bituminosulfonate. However, use of oil shale for production of these products is still very rare and in experimental stages only.\n\nSome oil shales are suitable source for sulfur, ammonia, alumina, soda ash, and nahcolite which occur as shale oil extraction byproducts. Some oil shales can also be used for uranium and other rare chemical element production. During 1946–1952, a marine variety of Dictyonema shale was used for uranium production in Sillamäe, Estonia, and during 1950–1989 alum shale was used in Sweden for the same purpose. Oil shale gas can also be used as a substitute for natural gas. After World War II, Estonian-produced oil shale gas was used in Leningrad and the cities in North Estonia. However, at the current price level of natural gas, this is not economically feasible.\n\nThe amount of economically recoverable oil shale is unknown. The various attempts to develop oil shale deposits have succeeded only when the cost of shale-oil production in a given region comes in below the price of crude oil or its other substitutes.\nAccording to a survey conducted by the RAND Corporation, the cost of producing a barrel of shale oil at a hypothetical surface retorting complex in the United States (comprising a mine, retorting plant, upgrading plant, supporting utilities, and spent shale reclamation), would range between US$70–95 ($440–600/m), adjusted to 2005 values. Assuming a gradual increase in output after the start of commercial production, the analysis projects a gradual reduction in processing costs to $30–40 per barrel ($190–250/m) after achieving the milestone of . Royal Dutch Shell has announced that its Shell ICP technology would realize a profit when crude oil prices are higher than $30 per barrel ($190/m), while some technologies at full-scale production assert profitability at oil prices even lower than $20 per barrel ($130/m).\n\nTo increase the efficiency of oil shale retorting and by this the viability of the shale oil production, researchers have proposed and tested several co-pyrolysis processes, in which other materials such as biomass, peat, waste bitumen, or rubber and plastic wastes are retorted along with the oil shale. Some modified technologies propose combining a fluidized bed retort with a circulated fluidized bed furnace for burning the by-products of pyrolysis (char and oil shale gas) and thereby improving oil yield, increasing throughput, and decreasing retorting time.\n\nIn a 1972 publication by the journal \"Pétrole Informations\" (ISSN 0755-561X), shale oil production was unfavorably compared to the coal liquefaction. The article stated that coal liquefaction was less expensive, generated more oil, and created fewer environmental impacts than oil shale extraction. It cited a conversion ratio of of oil per one tonne of coal, as against of shale oil per one tonne of oil shale.\n\nA critical measure of the viability of oil shale as an energy source lies in the ratio of the energy produced by the shale to the energy used in its mining and processing, a ratio known as \"Energy Returned on Energy Invested\" (EROEI). A 1984 study estimated the EROEI of the various known oil-shale deposits as varying between 0.7–13.3\nalthough known oil-shale extraction development projects assert an EROEI between 3 and 10. According to the World Energy Outlook 2010, the EROEI of \"ex-situ\" processing is typically 4 to 5 while of \"in-situ\" processing it may be even as low as 2. However, according to the IEA most of used energy can be provided by burning the spent shale or oil-shale gas.\n\nThe water needed in the oil shale retorting process offers an additional economic consideration: this may pose a problem in areas with water scarcity.\n\nMining oil shale involves a number of environmental impacts, more pronounced in surface mining than in underground mining. These include acid drainage induced by the sudden rapid exposure and subsequent oxidation of formerly buried materials, the introduction of metals including mercury into surface-water and groundwater, increased erosion, sulfur-gas emissions, and air pollution caused by the production of particulates during processing, transport, and support activities. In 2002, about 97% of air pollution, 86% of total waste and 23% of water pollution in Estonia came from the power industry, which uses oil shale as the main resource for its power production.\n\nOil-shale extraction can damage the biological and recreational value of land and the ecosystem in the mining area. Combustion and thermal processing generate waste material. In addition, the atmospheric emissions from oil shale processing and combustion include carbon dioxide, a greenhouse gas. Environmentalists oppose production and usage of oil shale, as it creates even more greenhouse gases than conventional fossil fuels. Experimental \"in situ\" conversion processes and carbon capture and storage technologies may reduce some of these concerns in the future, but at the same time they may cause other problems, including groundwater pollution. Among the water contaminants commonly associated with oil shale processing are oxygen and nitrogen heterocyclic hydrocarbons. Commonly detected examples include quinoline derivatives, pyridine, and various alkyl homologues of pyridine (picoline, lutidine).\n\nWater concerns are sensitive issues in arid regions, such as the western US and Israel's Negev Desert, where plans exist to expand oil-shale extraction despite a water shortage. Depending on technology, above-ground retorting uses between one and five barrels of water per barrel of produced shale-oil. A 2008 programmatic environmental impact statement issued by the US Bureau of Land Management stated that surface mining and retort operations produce of waste water per of processed oil shale. \"In situ\" processing, according to one estimate, uses about one-tenth as much water.\n\nEnvironmental activists, including members of Greenpeace, have organized strong protests against the oil shale industry. In one result, Queensland Energy Resources put the proposed Stuart Oil Shale Project in Australia on hold in 2004.\n\n"}
{"id": "30782694", "url": "https://en.wikipedia.org/wiki?curid=30782694", "title": "Peat energy in Finland", "text": "Peat energy in Finland\n\nPeat energy in Finland describes peat energy use in Finland. Peat has high global warming emissions and high environmental concerns. It may be compared to brown coal (lignite) or worse than this lowest rank of coal. Peat is the most harmful energy source for global warming in Finland. (Hard coal, which is more harmful, is only used as an emergency back-up source for energy, and not in every year.) According to IEA the Finnish subsidies for peat in 2007-2010 undermined the goal to reduce emissions and counteracted other environmental policies and The European Union emissions trading scheme.\n\nAccording to the national energy statistics the energy use of peat grew in Finland in the 1980s: 1975: 0.5 TWh and 1980 4,7 TWh. The share of peat energy was 19 TWh in 2005 and in the peak year 2007 28.4 TWh. In 2006 the peat energy provided 25.3 TWh which gave 6.2 TWh electricity, 6.1 TWh warming and 4.7 TWh industry heat.\n\nAccording to the U.S. Geological Survey, Finland and Ireland are the two main peat fuel users in the world. The regional shares of peat extraction in 2005 for gardening, agricultural and fuel use was : Finland 34%, Ireland 20%, Russia 8%, Belarus 7%, Canada 6%, Ukraine 4%, Sweden 5, Latvia 3%, Estonia 3 %and USA 3%. In the Americas peat was used only for soil improvement and in Finland 90% as fuel.\n\nFinland’s environmental administration has a new project studying water pollution from the peat collection areas. Local people have complained of water pollution. The new studies show 20-fold differences between samples. The EIA counts only the largest particles that are only about 10% of total. The heavy rain showers have been previously ignored. According to evaluations one rain shower could bring as much particles as the rest of the summer.\n\nThe accidental large water emission from peat collection area in 1981 in Lestijärvi was an environmental wakeup.\n\nVapo Oy is the major peat producer in Finland. In 1984 Vapo was 100% owned by Finnish state. Finnforest part of Metsä Group owned 33% of Vapo in 2002-2005 and 49.9% in 2005-2009. In 2009 ownership of Vapo was reconstructed to Suomen Energiavarat Oy, owned by EPV Energy (ex Etelä-Pohjanmaan Voima Oy) and several peat energy using companies. In June 2009 Metsäliitto (Finnforest) sold its share to Suomen Energiavarat Oy. In 2012 responsiples include several municipal energy companies e.g. Helsinki, Vantaa, Oulu, Rauma, Seinäjoki, Vaasa and Lahti.\n\nVapo has ongoing 100 applications to new peat collection areas.\n\nEnvironmental counsellor Antti Ylitalo has written public appeals to protect endangered natural bogs (moss, moor). Vapo made a disqualification appeal against Ylitalo claiming that his statements formed a conflict of interests in the official evaluations of Vapo’s applications. Ylitalo felt that the disqualification appeal by Vapo infringed his freedom of speech.\n\nAccording to VTT (2005) the major producers were Vapo Oy (78%) sekä Turveruukki Oy (10%) followed by Fortum Power and Heat, Kuopion Energia, Alholmens Kraft and Vaskiluodon Voima.\n\nAfter the peat production Suo Oy, 100% owned by Vapo, cultivates reed canary grass in the bog. Suo owns 6500 ha agricultural land. Suo Oy received 673 000 € agricultural support in 2010. This was the fifth biggest agricultural support in Finland. The company received support for energy plant cultivation. Vapo (Suo Oy) has received in total €10 million public financing in ten years for the reed canary grass cultivation used as energy with wood and peat.\n\nVTT used to be a state financed research organization but today it gets a third of its funds from the state and two thirds from the companies. The payer may at least in some cases influence the publication of results.\n\nIn the summer 2010, one researcher specialist from VTT expressed his/her critics about the reliability of the electricity market report for the parliament. VTT gave him/her a warning since according to VTT the statement was given in the name of VTT and she/he was not qualified for it. This special researcher claims that he acted privately and revealed in general the contradictions of the electricity market report.\n\nThe Finnish government ordered an energy evaluation report from VTT for energy tax reform which was published in 2010. VTT report studies the carbon tax for oil, gas, coal and peat. VTT did not give tax recommendations for peat based on its actual carbon load. The responsible director was Satu Helynen. The close relationships of Ms. Helynen and the peat industry associations was revealed.\n\nSatu Helynen denied a VTT scientist to write about peat in autumn 2010. According to the scientist the leaders of VTT tried to prevent them to express in public dissentient opinions about the peat energy tax. Helynen considered the writing onesided and demanded an other perspective. The issue came public in August 2010 in a STT (Finnish News Service) interview, when a group of VTT scientists claimed anonyme VTT of pressure.\n\nSatu Helynen was a member of Enas board during 2005-2009: a company owned by VTT and two peat energy companies: Vapo and Jyväskylän Energia. Satu Helynen was the chairman of Finbio ry Association until 2009. Finbio was claimed to cooperate with the “Association of Finnish Peat Industries“ and “Peat Producers Association” and lobbing peat for the members of parliament.\n\nSatu Helynen was a member of the Executive Board of the International Peat Society (IPS) 2008-2012. IPS is a global non-profit organisation for all interests related to peat and peatlands, having scientific and corporate members in 42 countries worldwide and promoting Wise Use of peatlands. The Executive Board of the IPS consists of both university and company directors and is elected by an annual assembly of national member associations. In Finland, Suoseura acts as national committee of IPS. In addition to approximately 400 individual members from universities and other organisation; their corporate members are VTT, Vapo, Turveruukki and the Bioenergy Association of Finland. \n\nVTT does not publish or control any economic or board membership commitments of its scientists as at least the members of board of public companies have to do. In medicine publications and conferences the statements of commitments of scientists are normal. Mr. Kari Larjava, e.g. VTT expert in biogas and head of Ms. Helynen, did not know IPS but considered its membership of no relevant problem in regard to commitments.\n\nIn August 2010 VTT published new principles of its employees' freedom to make public statements. According to Olli Mäenpää, Professor of Administration Law at Helsinki University, the new principles of VTT are in conflict with Finnish constitutional freedom to express one's opinions.\n\n\"The new VTT rules deny the right to make any private statements, verbal or written, without permission from the leadership of VTT in the line of activities of VTT. The permission for statements will only be allowed when there is no conflict.\" In order to guarantee the consistence of research and reliability of the content, the scientists should prevent all criticism of the content of VTT publications publicly after the publications. The ethical rules oblige the research organisation to prevent activities that will mislead the public.\n\n\"Finnish law guarantees everyone the freedom of expression as private persons that no organization or person is allowed to restrict. Democratic society has no need to restrict free communication.\" This freedom covers peaceful communication excluding agitation in crimes or mental troublemaking. In practice, VTT and some other employers including Sanoma Oy (Helsingin Sanomat) have given outsiders the impression of willingness to restrict this freedom included in the human rights declaration of the United Nations.\n\nIt \"makes your hair stand on end\", said Professor Mäenpää. In legal proceedings every employer tends to lose the case. \"In practice the attempts to restrict the basic human rights will label these organizations with a negative reputation that is strongly disapproved in public.\" The ability to question facts and assumptions and the evaluation of report argumentations are cornerstones of scientific discoveries, innovative ideas and sustainable development of society including its politics. Freedom of expression is a cornerstone of democracy and a cornerstone of international human rights declarations.\n\nAccording to the Finnish Ministry of Trade and Industry (KTM) peat is determined in Finland as \"slowly renewable biomass fuel\". The definition is from the report: Patrick Crill (USA), Ken Hargreaves (UK) and Mr. Atte Korhola (FIN) Turpeen asema Suomen kasvihuonekaasutaseissa (2000) ordered by KTM. This report has been criticised by several scientists. For example, environment experts and university academics Mr. Raimo Heikkilä (Oulu), Mr. Tapio Lindholm (Helsinki) and Mr. Heikki Simola (Joensuu) wrote an opponent review in 2007. In short: The use of peat fuel releases carbon that was bound long before the industrial revolution. The renewal of peat layers in thousands of years is insignificant compared to the carbon emissions of the peat fuel in regard to the climate change. It is highly questionable to claim that the human created problem would be compensated by a natural process elsewhere. For the greenhouse gas inventories the natural peat lands are basically neutral since storage of carbon equals release of methane. The agricultural land use will never restore the carbon permanently in the ecosystem equal to the original peat layers. There is no difference in the burning of peat or coal in respect to the climate warming.\n\nProfessor Atte Korhola University of Helsinki is married to Eija-Riitta Korhola, conservative member of parliament (Kokoomus) and Member of the European Parliament since 2004. Ms. Eija-Riitta Korhola received campaign financing from the nuclear power industry in 2004 for the European Parliament election, 2004 (Finland) campaign. Many Finnish nuclear power companies are also peat industry companies.\n\nThe United Nations Environment and Sustainable Development World Commission Finland expressed in 1989 that Finland should not increase peat production in the 1990s based on its global warming emissions. Finland should discontinue promoting peat energy use and the draining of peat bogs abroad, for example in Indonesia, Brunei and Uganda.\n\nPeat and hard coal are the most harmful energy sources for global warming in Finland. According to VTT studies peat is often the most harmful one.\n\nThe storage of carbon in Finland is: bogs 6000 million tonnes, forest soil 1300 million tonnes and trees 800 million tonnes. According to senior lecturer Heikki Simola (University of East Finland) the carbon load from the ditches of bogs is 6000 tonnes pro person annually. In the south of Finland the majority of swamps have been ditched for the forest production. Kyoto protocol do not consider emissions from nature and the emissions from the land use changes.\n\n25% of Finnish plants grow on bogs and 1/3 of Finnish birds live at some stage around the bog areas. Biologically there are 60-70 types of bogs. The Finnish language distinguish the differences: ”letto, neva, räme, korpi, palsa”. More than half of the Finnish bog area had been drained in 2008. Peat collection is the most violent attack for the wild nature. In peat collection bog is drained and peat layer is collected for several meters. Forest is often planted in the area after the peat collection. Regeneration is possible, but the complete regeneration of the bog will take tens of thousands of years. In the USA it was calculated in 2002 (Science 297 950 2002) that wetlands are economically more valuable than agricultural fields. The increase of bog biodiversity support from $6.5 billion to $45 billion can give $4400–5200 billion. economical benefit (in the world).\n\nPeat is the most popular energy source in Finland for new energy investments 2005-2015. The new energy plants in Finland starting 2005-2015 have as energy source: peat 36% and hard coal 11%: combined: 47%. The major carbon dioxide emitting peat plants during 2005-15 are/will be (CO2 kt): PVO 2700 kt, Jyväskylän Energia 561 kt, Etelä-Pohjanmaan Voima Oy (EPV Energia) 374 kt, Kuopion Energia 186 kt, UPM Kymmene 135 kt and Vapo 69 kt. EPV Energy is partner in TVO nuclear plants and Jyväskylän and Kuopion Energia partners in Fennovoima nuclear plants in Finland.\n\nAccording to IEA country report the Finnish subsidies for peat undermine the goal to reduce CO emissions and counteracts other environmental policies and The European Union emissions trading scheme. IEA recommends to adhere to the timetable to phase out the peat subsidies in 2010. \"To encourage sustained production of peat in the face of negative incentives from the European Union’s emissions trading scheme for greenhouse gases, Finland has put in place a premium tariff scheme to subsidise peat. The premium tariff is designed to directly counter the effect of the European Union’s emissions trading scheme.\"\n\n"}
{"id": "35886478", "url": "https://en.wikipedia.org/wiki?curid=35886478", "title": "Protected areas of Libya", "text": "Protected areas of Libya\n\nProtected areas of Libya include any geographical area protected for a specific use.\n\nMost protected areas are intended for the conservation of flora and fauna. Libya's national parks and nature reserves are maintained by the \"Technical Committee of Wildlife and National Parks\" which was created in 1990, as part of the General Secretariat of Agricultural Reclamation and Land Reform.\n\nAreas may also be protected for their value and importance as historical, cultural heritage or scientific sites. More information on these can be found in the list of heritage sites in Libya.\n\nThere were no national parks or protected areas in Libya prior to the Libyan coup of 1969. The new revolutionary government began to designate national parks and nature reserves in the 1970s. By 2009 seven national parks, five nature reserves and twenty-four other protected areas had been established, mostly along the coast of the Mediterranean Sea. Among the most visited of these areas are Karaboli National Park, El-Kouf National Park, Benghazi Nature Reserve, and Tripoli Nature Reserve along the coast, and Zellaf Nature Reserve in the southwestern desert. \n\nThe Benghazi Nature Reserve is located north of the city of Benghazi about 14 km on the Mediterranean coast. Known for its migratory waterbirds, the reserve includes a stretch of beach dunes and salt marshes, as well as the Ayn Zayanah Lagoon. The lagoon is fed by a number of freshwater springs and flamingos winter over there. Despite its protected status, the area is also used for fishing and recreation. In a number of places garbage has been illegally dumped in the reserve.\n\nEl Kouf National Park is located near and to the west of the city of Bayda, in Jabal al Akhdar District in the forested hills of the Jebel Akhdar or \"Green Mountains\". The park is only about 15 km wide from east and west, but it streaches from the coast for a 160 km to the south, following the course of Wadi el-Kouf. There are three major ecozones, beach dunes give way to maquis shrubland at higher elevations, with a fluvial microhabitat along the course of the wadi. The park protects both marine and land wildlife. Noted species include bottlenose dolphins, golden wolves, striped hyenas and small-spotted genets. The landscape includes cliffs and caves.\n\n\n\n"}
{"id": "7625061", "url": "https://en.wikipedia.org/wiki?curid=7625061", "title": "Rosenfeld's law", "text": "Rosenfeld's law\n\nRosenfeld's law is an axiom relating physics to economics, that states that the amount of energy required to produce one dollar of GDP has decreased by about one percent per year since 1845.\n\nThe original quote by Arthur H. Rosenfeld is:\nFrom 1845 to the present, the amount of energy required to produce the same amount of gross national product has steadily decreased at the rate of about 1 percent per year. This is not quite as spectacular as Moore's Law of integrated circuits, but it has been tested over a longer period of time. One percent per year yields a factor of 2.7 when compounded over 100 years. It took 56 BTUs (59,000 joules) of energy consumption to produce one (1992) dollar of GDP in 1845. By 1998, the same dollar required only 12.5 BTUs (13,200 joules).\n"}
{"id": "34184830", "url": "https://en.wikipedia.org/wiki?curid=34184830", "title": "Roy Gordon Grainger", "text": "Roy Gordon Grainger\n\nRoy Gordon Grainger (born 19 February 1962) is a New Zealand physicist. He is head of the Earth Observation Data Group in the Atmospheric, Oceanic and Planetary Physics Sub-Department at the University of Oxford and a Tutorial Fellow in Physics at St Hugh's College, Oxford.\n\nGrainger was educated at Auckland Grammar School before attending the University of Auckland to read physics. He gained a Doctorate in Atmospheric Physics on the subject of remote sensing of cloud properties. He worked for a short time in UV research at the New Zealand Meteorological Service before taking up a post-doctoral position in the Physics Department in Oxford, where his research was focused on measurement of stratospheric aerosols using the ISAMS satellite instrument designed at Oxford.\n\nIn 1998, he returned to New Zealand to accept a Lectureship at the University of Canterbury, where he conducted new instrumental work to measure aerosol properties.\n\nGrainger is the Principal Investigator of the Optimal Retrieval of Aerosol and Cloud (ORAC) project, which is a community code to optimally estimate aerosol and cloud properties from satellite imagery, and was principally responsible for starting this project.\n\nThe Earth Observation Data Group which he heads is primarily interested in atmospheric trace gases and clouds (especially with regard to processes which control climate), and conducts satellite studies of atmospheric aerosols.\n\nGrainger lives in Oxford, England, and is the great-grandson of Alfred Henry Grainger, after whom Grainger Falls is named.\n\n"}
{"id": "9523363", "url": "https://en.wikipedia.org/wiki?curid=9523363", "title": "Snow tire", "text": "Snow tire\n\nSnow tires—also called winter tires—are tires designed for use on snow and ice. Snow tires have a tread design with larger gaps than those on summer tires, increasing traction on snow and ice. Such tires that have passed a specific winter traction performance test are entitled to display a \"Three-Peak Mountain Snow Flake\" symbol on their sidewalls. Tires designed for winter conditions are optimized to drive at temperatures below . Some snow tires have metal or ceramic studs that protrude from the tire to increase traction on hard-packed snow or ice. Studs abrade dry pavement, causing dust and creating wear in the wheel path. Regulations that require the use of snow tires or permit the use of studs vary by country in Asia and Europe, and by state or province in North America.\n\nRelated to snow tires are those with an M+S rating, which denotes an \"all-season\" capability—quieter on clear roads, but less capable on snow or ice than a winter tire.\n\nSnow tires operate on a variety of surfaces, including pavement (wet or dry), mud, ice, or snow. The tread design of snow tires is adapted primarily to allow penetration of the snow into the tread, where it compacts and provides resistance against slippage. The snow strength developed by compaction depends on the properties of the snow, which depend on its temperature and water content—wetter, warmer snow compacts better than dry, colder snow up to a point where the snow is so wet that it lubricates the tire-road interface. New and powder snow have densities of . Compacted snow may have densities of .\n\nSnow or ice-covered roadways present lower braking and cornering friction, compared to dry conditions. The roadway friction properties of snow, in particular, are a function of temperature. At temperatures below , snow crystals are harder and generate more friction as a tire passes over them than at warmer conditions with snow or ice on the road surface. However, as temperatures rise above , the presence of free water increasingly lubricates the snow or ice and diminishes tire friction. Hydrophilic rubber compounds help create friction in the presence of water or ice.\n\nAttributes that can distinguish snow tires from \"all-season\" and summer tires include:\nSnow tires are designed to encounter many of the same conditions that summer tires encounter, as well, so they incorporate siping for wet surfaces. Wet-film conditions on hard-compacted snow or ice require studs or chains.\n\nMany jurisdictions in Asia, Europe, and North America seasonally allow snow tires with metal or ceramic studs to improve grip on packed snow or ice. Such tires are prohibited in other jurisdictions or during warmer months because of the damage they may cause to road surfaces. The metal studs are fabricated by encapsulating a hard pin in a softer material base, sometimes called the \"jacket\". The pin is often made of tungsten carbide, a very hard high performance ceramic. The softer base is the part that anchors the stud in the rubber of the tire. As the tire wears with use, the softer base wears so that its surface is at about the same level as the rubber, whereas the hard pin wears so that it continues to protrude from the tire. The pin should protrude at least for the tire to function properly. Snow tires do not eliminate skidding on ice and snow, but they greatly reduce risks.\n\nStuddable tires are manufactured with molded holes on the rubber tire tread. Usually, there are 80 to 100 molded holes per tire for stud insertion. The insertion is done by using a special tool that spreads the rubber hole so that a stud jacket can be inserted and the flange at the bottom of the jacket can be fitted nicely to the bottom of the hole. The metal studs come in specific heights to match the depths of the holes molded into the tire tread based on the tread depths. For this reason, stud metals can only be inserted when the tires have not been driven on. A proper stud insertion results in the metal jacket that is flush with the surface of the tire tread having only the pin part that protrudes.\n\nWhen studs come into contact with pavements they abrade the asphalt or concrete surface. This can result in creating polluting dust and wear in the wheel path that prevents proper drainage. For this reason, studded tires are banned, at least seasonally, in many jurisdictions.\n\nThe compacted snow develops strength against slippage along a shear plane parallel to the contact area of the tire on the ground. At the same time, the bottom of the tire treads compress the snow on which they are bearing, also creating friction. The process of compacting snow within the treads requires it to be expelled in time for the tread to compact snow anew on the next rotation. The compaction/contact process works both in the direction of travel for propulsion and braking, but also laterally for cornering.\n\nThe deeper the snow that the tire rolls through, the higher the resistance encountered by the tire, as it compacts the snow it encounters and plows some of it to either side. At some point on a given angle of uphill pitch, this resistance becomes greater than the resistance to slippage achieved by the tread's contact with the snow and the tires with power begin to slip and spin. Deeper snow means that climbing a hill without spinning the powered wheels becomes more difficult. However, the plowing/compaction effect aids in braking to the extent that it creates rolling resistance.\n\n\nASTM International (American Society for Testing and Materials International) is an international standards organization that develops and publishes voluntary consensus technical standards for a wide range of materials, products, systems, and services. The pertinent standard for snow tires is ASTM F1805 – 16, \"Standard Test Method for Single Wheel Driving Traction in a Straight Line on Snow- and Ice-Covered Surfaces\", which assess tire performance on snow and ice. It measures the traction of tires under acceleration in the rolling direction. Tires that pass this test are entitled to display the \"3PMSF\" (Three-Peak Mountain Snow Flake) symbol.\n\nIn Russia light vehicles and buses must be equipped with snow M+S or \"3PMSF\" tires on all axles from December through February and have a minimum tread depth of .\n\nAll prefectures of Japan, except for the southernmost prefecture of Okinawa, require the motorized vehicles to be fitted with winter tires or tire chains if the road is icy or slushy. If tread grooves of snow tires are worn off for more than 50% of its original depth, tires must be replaced to meet the legal requirements. Drivers will be fined for failing to comply with the snow tire or tire chains requirements. Nationwide studded tire restrictions for passenger vehicles came in effect in April 1991, followed by restrictions for commercial trucks in 1993. Studded tires are technically still legal in Japan, but the usage is restricted by environmental law and it is a criminal offence to operate a vehicle fitted with studded tire on dry asphalt or concrete.\n\nAs of 2016, European regulations pertaining to snow tires varied by country. The principal aspects of regulations were whether use was mandatory and whether studded tires were permitted.\n\n\nThe U.S. National Highway Traffic Safety Administration (NHTSA) and Transport Canada allow display of a \"3PMSF\" symbol to indicate that the tire has exceeded the industry requirement from a reference (non-snow) tire. As of 2016, snow tires were 3.6% of the US market and 35% of the Canadian market.\n\nUS states and Canadian provinces control the use of snow tires. No state or province (except for Quebec) requires snow tires. They may require snow tires or chains only in certain areas during the winter:\n\nThe use of studded tires is regulated in the United States and Canada by individual states and provinces, as follows:\n"}
{"id": "44855959", "url": "https://en.wikipedia.org/wiki?curid=44855959", "title": "Solona River", "text": "Solona River\n\nSolona River is a name of several rivers in the Eastern Europe, especially in Ukraine. In the Ukrainian language, the name means a salty (saline) river.\n\n\n"}
{"id": "43691183", "url": "https://en.wikipedia.org/wiki?curid=43691183", "title": "Structures for lossless ion manipulations", "text": "Structures for lossless ion manipulations\n\nStructures for lossless ion manipulations (SLIM) are a form of ion optics to which various radio frequency and dc electric potentials can be applied and used to enable a broad range of ion manipulations, such as separations based upon ion mobility spectrometry, reactions (unimolecular, ion-molecule, and ion-ion), and storage (i.e. ion trapping). SLIM was developed by Richard D. Smith and coworkers at PNNL and are generally fabricated from arrays of electrodes on evenly spaced planar surfaces.\n\nIn SLIM, ions move in the space between the two surfaces, in directions controlled using electric fields, and also moved between different of multi-level SLIM, as can be constructed from a stack of PCBs. The lossless nature of SLIM is derived from the use of rf electric fields, and particularly the pseudo potential derived from the inhomogeneous electric fields resulting from rf of appropriate frequency applied to multiple adjacent electrodes, and that serves to prevent ions from closely approaching the electrodes and surface where loss would conventionally be expected. SLIM are generally used in conjunction with mass spectrometry for analytical applications.\n\nThe first SLIM were fabricated using printed circuit board technology (PCB), and used to demonstrate a range of simple ion manipulations in gases at low pressures (a few torr). This SLIM technology has conceptual similarities with integrated electronic circuits, but instead of moving electrons we use electric fields to create pathways, switches, etc. to manipulate ions in the gas phase. \n\nSLIM devices can enable complex sequences of ion separations, transfers and trapping to occur in the space between two surfaces positioned (e.g., ~4 mm apart) and each patterned with conductive electrodes. The SLIM devices use the inhomogeneous electric fields created by arrays of closely spaced electrodes to which readily generated peak-to-peak RF voltages (e.g., Vp-p ~ 100 V; ~ 1 MHz) are applied with opposite polarity on adjacent electrodes to create effective potential fields that prevent ions from approaching the surfaces. \n\nThe confinement functions over a range of pressures (<0.1 torr to ~50 torr), and over an adjustable mass-to-charge ratio (\"m/z\") range (e.g., \"m/z\" 200 to >2000). This effective potential works in conjunction with DC potentials applied to side electrodes to prevent ion losses, and allows creating ion traps and conduits in the gap between the two surfaces for the effectively lossless storage and movement of ions as a result of any gradient in the applied DC fields. \n\nThe operating pressure for SLIM devices has initially been reported to be in the 1-10 torr range, and a range well suited for applications of ion mobility separations.\n"}
{"id": "51614357", "url": "https://en.wikipedia.org/wiki?curid=51614357", "title": "Tarangire Ecosystem", "text": "Tarangire Ecosystem\n\nThe Tarangire Ecosystem () is a geographical region in Africa. It is located in northern Tanzania and extends between 2.5 and 5.5 degrees south latitudes and between 35.5 and 37 degrees east longitudes.\n\nThe Tarangire Ecosystem hosts the second-largest population of migratory ungulates in East Africa and the largest population of elephants in northern Tanzania.\n\nThe Tarangire Ecosystem is defined by the long distance migratory movements of eastern white-bearded wildebeest and plains zebra. It includes the dry season wildlife concentration area near the Tarangire River in Tarangire National Park, and the wet-season dispersal and calving grounds to the north in the Northern Plains and to the east in Simanjiro Plains, spanning in total approximately 20,500 km (7,900 sq mi). Migratory animals must have access to both the dry-season water source in the park, and the nutrient-rich forage available only on the calving grounds outside the park to successfully raise their calves and maintain their high abundance.\n\nThe Tarangire Ecosystem is also known as the Masai Steppe, or the Tarangire-Manyara Ecosystem.\n\nTarangire has approximately 500 species of birds, and more than 60 species of larger mammal.\n\nThe area falls within the eastern branch of the East African Rift Valley which has widened and the valley floor fallen over the past few million years. About 250,000 years ago Lake Manyara and Lake Burunge were part of a larger lake called Proto-Manyara, a basin of internal drainage that lost water through evaporation and deep percolation. Subsequent rises in the Rift Valley floor changed drainage patterns and the lake was reduced in size and divided into the two shallow, alkali lakes currently seen. Topography is now mainly low ridges of gneiss and pre-Cambrian rocks covered with well-drained, medium textured, stony soils. Large areas of valley bottoms are montmorillonite black cotton soils. Ancient lake sediments produced clay soils in the Proto-Manyara area. Minjingu Hill and Vilima Vitatu were islands in Proto-Manyara Lake and their phosphate deposits there are derived from accumulated waterbird feces. Volcanic ash deposits produce rich soils on the Northern Plains and Simanjiro Plains where migratory wildebeest and zebra find forage with the nutrients necessary for lactation and healthy calf growth.\n\nThe current western boundary is the rift valley escarpment, the northern boundary is the Kenyan border near Lake Natron, the southern and eastern boundaries are not defined by any strict geographic features. Elevation ranges from about 1000 m in the southwest to 2660 m in the northeast.\n\nTarangire has a bimodal rainfall averaging 650 mm per annum, with short rains from November to February, long rains from March to May, and dry season from June to October. The rains, particularly the short rains, are very unreliable and often fail. Rainfall varies inter-annually, the standard deviation of the annual rainfall is equal to 37% of the mean annual rainfall. The inter-annual variation of monthly rainfall varies even more markedly, the standard deviation of monthly rainfall is 72% of the mean. This high variability in rainfall is also reflected in a high inter-annual variation of the length of the wet season. For example, the wet season lasted 38 days in 1983/1984 and 200 days in 1987/1988.\n\nThe oldest known elephant to give birth to twins is found in Tarangire. A recent birth of elephant twins in the Tarangire National Park of Tanzania is a great example of how the birth of these two healthy and thriving twins can beat the odds.\n\nBetween the sixteenth and nineteenth centuries, the Maa-speaking pastoral people expanded into the area, replacing other pastoral groups like Nilotes and farming Bantu groups. By 1880 the Maasai reached their greatest extent. Around 1900 they suffered pleuro-pneumonia and small pox diseases that killed many. At the same time the outbreak of the rinderpest decimated Maasai livestock and wildlife.\n\nThe colonial period of 1880s to 1950s saw the displacement of Maasai from lands with high potential for agricultural development by the European farmers/settlers. Many game parks were created at the same time, often evicting pastoralists from key dry season grazing areas and watering points. Because of the abundant water and pasture in the Tarangire ecosystem, it had a reputation as one of the best pastoral areas in Tanzania. Many herders who were evicted from the Serengeti National Park in the 1950s relocated to this area.\n\nTanzania became independent in 1961 and most of the British colonial administration/legislation was adopted by the new government. In the 1960s and 1970s the rinderpest that had previously killed many wildlife and livestock species was controlled. The control of rinderpest resulted in the increased numbers of wildebeest in the Serengeti ecosystem. This pushed Maasai into Tarangire area to avoid contact with wildebeest calving areas in the short grass-plains. Such areas are associated with the spread of Malignant Catarrhal Fever that affects cattle. Between 1962 and 1963 the worst drought in 50 years hit most parts of the country including Tarangire area and killed many wildlife and livestock.\n\nIn 1967 agriculture was promoted as the backbone of the national economy. Large-scale farms like the Lolkisale bean farms were established in Tarangire to produce crops for export as well as for national reserves during droughts and food shortage. Human population increased in Tarangire area due both to natural increase and immigration of agriculturists from nearby regions of Kilimanjaro and Arusha. This displaced Maasai pastoralists and wildlife from the best rangelands into more marginal areas.\n\nIn 1970, the Tarangire Game Reserve was upgraded to become Tarangire National Park. By the mid 1980s the movement of commercial interests and farmers into the area had expanded, blocking many traditional migratory routes for wildlife.\n\nIn 2001, the Tanzanian government turned over the National Ranching Company land at Manyrara Ranch to the Tanzania Land Conservation Trust to help conserve the wildlife migration corridor between Tarangire National Park and the calving grounds to the north on the Gelai Plains. Conservation easements are being used as conservation tools on the calving grounds east of Tarangire National Park on the Simanjiro Plains. Land-use planning informed by wildlife survey data is being tried to help conserve pastoral rangelands, wildlife migration routes, and calving grounds in the Northern Plains.\n\n"}
{"id": "5089687", "url": "https://en.wikipedia.org/wiki?curid=5089687", "title": "U.S. Standard Atmosphere", "text": "U.S. Standard Atmosphere\n\nThe U.S. Standard Atmosphere is an atmospheric model of how the pressure, temperature, density, and viscosity of the Earth's atmosphere change over a wide range of altitudes or elevations. The model, based on an existing international standard, was first published in 1958 by the U.S. Committee on Extension to the Standard Atmosphere, and was updated in 1962, 1966, and 1976. It is largely consistent in methodology with the International Standard Atmosphere, differing mainly in the assumed temperature distribution at higher altitudes.\n\nThe USSA mathematical model divides the atmosphere into layers with an assumed linear distribution of absolute temperature \"T\" against geopotential altitude \"h\". The other two values (pressure \"P\" and density \"ρ\") are computed by simultaneously solving the equations resulting from:\nat each geopotential altitude, where \"g\" is the standard acceleration of gravity, and \"R\" is the specific gas constant for dry air.\n\nAir density must be calculated in order to solve for the pressure, and is used in calculating dynamic pressure for moving vehicles. Dynamic viscosity is an empirical function of temperature, and kinematic viscosity is calculated by dividing dynamic viscosity by the density.\n\nThus the standard consists of a tabulation of values at various altitudes, plus some formulas by which those values were derived.\n\nTo allow modeling conditions below mean sea level, the troposphere is actually extended to , where the temperature is , pressure is , and density is .\n\nThe basic assumptions made for the 1962 version were:\n\n\nThis is the most recent version and differs from previous versions only above 32 km:\n\n\n\n"}
{"id": "1920027", "url": "https://en.wikipedia.org/wiki?curid=1920027", "title": "Vertical seismic profile", "text": "Vertical seismic profile\n\nIn geophysics, vertical seismic profile (VSP) is a technique of seismic measurements used for correlation with surface seismic data. The defining characteristic of a VSP (of which there are many types) is that either the energy source, or the detectors (or sometimes both) are in a borehole. In the most common type of VSP, Hydrophones, or more often geophones or accelerometers, in the borehole record reflected seismic energy originating from a seismic source at the surface.\n\nThere are numerous methods for acquiring a vertical seismic profile (VSP). Zero-offset VSPs (A) have sources close to the wellbore directly above receivers. Offset VSPs (B) have sources some distance from the receivers in the wellbore. Walkaway VSPs (C) feature a source that is moved to progressively farther offset and receivers held in a fixed location. Walk-above VSPs (D) accommodate the recording geometry of a deviated well, having each receiver in a different lateral position and the source directly above the receiver. Salt-proximity VSPs (E) are reflection surveys to help define a salt-sediment interface near a wellbore by using a source on top of a salt dome away from the drilling rig. Drill-noise VSPs (F), also known as seismic-while-drilling (SWD) VSPs, use the noise of the drill bit as the source and receivers laid out along the ground. Multi-offset VSPs (G) involve a source some distance from numerous receivers in the wellbore.\n\n"}
{"id": "53588968", "url": "https://en.wikipedia.org/wiki?curid=53588968", "title": "Vilma Rose Hunt", "text": "Vilma Rose Hunt\n\nVilma Rose Hunt (November 15, 1926 – December 29, 2012) was a scientist noted for research into radiation and workplace safety for women. After beginning a dentistry career in Australia and New Zealand, Hunt traveled to the United States where she earned her A.M. in Physical Anthropology at Radcliffe College and began researching public health and radiation biology. In 1964, Hunt discovered that polonium 210 is a natural contaminant of tobacco, providing additional evidence for the link between smoking and bronchial cancer. In 1974, she wrote a 121-page report on workplace hazards for pregnant women, which made the front page of the \"New York Times\". She published a book, \"Work and the Health of Women\", in 1979. From 1979 to 1981, Hunt served as an administrator for the United States Environmental Protection Agency, enacting public health solutions to environmental contamination at sites like Love Canal, New York, and Three Mile Island Nuclear Generating Station, Pennsylvania. Hunt retired in Gloucester in 1985, though she served as an environmental consultant and visiting lecturer until her death.\n\nVilma Hunt was born in Sydney on November 15, 1926. She is the daughter of Margaret Rose (Lynch) Dalton-Webb and William Dalton-Webb, an electrician. When Vilma was seven years old, the Dalton-Webb family moved to Kempsey, New South Wales. She attended public school and graduated from high school in 1942. After graduation, during World War II, she enlisted in the women's auxiliary branch of the Royal Australian Air Force.\n\nIn 1952, Vilma travelled to Boston to study dentistry. She met Edward Eyre Hunt, Jr. at the Forsyth Dental Infirmary. They married in 1952, moved to Gloucester, Massachusetts, and had four children: Margaret, William, Louise, Catherine and Martine (a foster daughter). Margaret would later become the chair of the women and gender studies department at Amherst College. Vilma and Edward retired in 1985.\n\nVilma Hunt began her career as a dentist. She earned her Bachelor of Dental Surgery at the University of Sydney in 1950, and served as a Junior Dental Officer with the New Zealand Department of Health from 1950–1952. She accepted a scholarship to study dentistry at Harvard University in 1952.\n\nAfter studying, interning, and lecturing on dentistry for several years, Hunt began studying anthropology. She earned her A.M. in Physical Anthropology from Radcliffe College in 1958 and began to conduct research with the Radcliffe Institute for Independent Study, from 1961–1963, and the Harvard School of Public Health, from 1962–1966. She received additional training in radiation biology from the Argonne National Laboratory, in Illinois, in the summer of 1963.\n\nWhile at HSPH, Hunt tested a cigarette butt for radiation, on a hunch. She discovered high levels of Polonium-201, a radioactive element, and launched an investigation alongside colleague Edward P. Radford. The pair published their findings, titled \"Polonium-210: A Volatile Radioelement in Cigarettes\" in \"Science\". The article came out on January 17, 1964––just six days after the surgeon general's report on the dangers of smoking was released. In 1964, Hunt was elected to Phi Beta Kappa.\n\nHunt taught and lectured for over three decades. From 1967–1969, she served as Assistant Professor of Environmental Health at the Yale University School of Medicine. From 1969–1972, she served as Assistant and Associate Professor of Environmental Health at Pennsylvania State University, where she was granted tenure in 1972. She would return to become Professor of Environmental Health from 1982–1985 before leaving to become a Mellon Research Fellow at the Massachusetts Institute of Technology in 1984. She received the National Endowment for the Humanities award in 1985.\n\nOutside of the classroom, Hunt conducted research into chemical hazards in the workplace, particularly in relation to reproductive health. On April 30, 1974, she published a 121-page report for the Department of Health, Education, and Welfare titled \"Occupational Problems of Pregnant Women.\" Within the report, she wrote \"We are all responsible for the health of future generations and we can no longer ignore a fact of life––reproduction and work are women's lot.\" Elsewhere, she details the poor working conditions and hazardous exposures present in factories and shows their correlation to high infant mortality rates, birth defects, and miscarriages. Differing from previous research on the subject, she also includes male fertility in her research, demonstrating that high levels of radiation negatively affect male sperm count. Her report was read across the nation, and on March 14, 1976, it made the front page of New York Times.\n\nA few years later, in 1979, Hunt published her first (and only completed) novel, \"A Brief History of Women Workers and Hazards in the Workplace\". In it, she discussed changing attitudes towards women's' health concerns and the treatment of women in factories, and called for labor regulations to protect women from benzene, a toxic chemical. She commented on how quickly the presence of women in lead factories declined, after it was exposed that lead had disastrous effects on children and the reproductive system. Hunt attributed the slow-changing pace of laws around benzene factories to the lack of definitive research on whether benzene affects the fetus. Hunt criticized the government's slow action and called for the improvement of factory conditions, rather than the removal of women from the industry.\n\nHunt also worked in conservation. She served on the Science Advisory Board of the United States Environmental Protection Agency from 1978–1979, where she was tasked with investigating and explaining chemical poisoning and radiation at environmental contamination sites, including the now-infamous Love Canal, NY, and Three Mile, PA.\n\nAfter she retired in 1985, Hunt became active in Gloucester local government, and served as a curator at the Magnolia Historical Society. She researched the history of uranium, and with the help of Harvard student Melissa Inouye, she began a book on the topic. At the time of Hunt's death in 2012, the text was incomplete.\n"}
{"id": "52509417", "url": "https://en.wikipedia.org/wiki?curid=52509417", "title": "Volcanic ash aggregation", "text": "Volcanic ash aggregation\n\nVolcanic ash aggregation occurs when particles of volcanic ash collide and stick together during transport. This process modifies the size distribution of airborne particles, which affects both atmospheric dispersal and fallout patterns on the ground. Aggregation also impacts the dynamics of volcanic plumes, pyroclastic density currents, and their associated hazards. \n\nThere are two main approaches to include the effects of ash aggregation in numerical models of ash injection and dispersal. One is to initialize the model with an aggregated grain size distribution, by moving fractions of the erupted mass into larger size bins (for example, the Cornell model). A second approach is a full theoretical description of aggregate growth through time, based on the Smoluchowski coagulation equation. Several methods exist to deal with this equation, including continuous and discrete methods. Continuous methods use the method of moments to track the evolution of a continuous grain size distribution, typically represented by a mean and standard deviation. In contrast, discrete methods use a discretized (binned) grain size distribution. Areas of uncertainty in the numerical schemes include parameterization of the particle sticking efficiency, timescales over which aggregation occurs, and the fraction of erupted mass participating in the aggregation process. \n"}
{"id": "48644551", "url": "https://en.wikipedia.org/wiki?curid=48644551", "title": "World Hereford Council", "text": "World Hereford Council\n\nThe World Hereford Council is the international breeding authority on Hereford cattle, existing for the promotion of breed excellence and the cultivation of global trade in Hereford genetics. Originally based in England, at the Hereford Cattle Society, the Council was established in 1951 as a response to the increasingly widespread international export and breeding of Hereford cattle, and the need to maintain a global authority connecting Hereford cattle to the original English herd book.\n\nThe Headquarters of the Council are based in the country of residence of the Secretary General (Uruguay, as of 2015), and move with each new election.\n\nThe World Hereford Council aims to advance the mutual interests of pedigree Hereford breeders on a worldwide scale, to act as an international breed authority, and to act as mediator in any disagreements or conflicts that may occur between any of the member countries. Structurally, the Council is led by a Secretary General, voted in by the twenty member countries (each nominating two representatives). \n\n"}
