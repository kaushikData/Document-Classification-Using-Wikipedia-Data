{"id": "32068163", "url": "https://en.wikipedia.org/wiki?curid=32068163", "title": "A Son of the Sun (novel)", "text": "A Son of the Sun (novel)\n\nA Son of the Sun is a 1912 novel by Jack London. It is set in the South Pacific at the beginning of the 20th century and consists of eight separate stories. David Grief is a forty-year-old English adventurer who came to the South seas years ago and became rich. As a businessman he owns offices in Sydney, but he is rarely there. Since his wealth spreads over a lot of islands, Grief has some adventures while going among these islands. London depicts the striking panorama of the South seas with adventurers, scoundrels, swindlers, pirates, and cannibals.\n\n\n"}
{"id": "57763", "url": "https://en.wikipedia.org/wiki?curid=57763", "title": "Aerosol", "text": "Aerosol\n\nAn aerosol is a suspension of fine solid particles or liquid droplets, in air or another gas. Aerosols can be natural or anthropogenic. Examples of natural aerosols are fog, dust, forest exudates and geyser steam. Examples of anthropogenic aerosols are haze, particulate air pollutants and The liquid or solid particles have diameters typically <1 μm; larger particles with a significant settling speed make the mixture a suspension, but the distinction is not clear-cut. In general conversation, \"aerosol\" usually refers to an aerosol spray that delivers a consumer product from a can or similar container. Other technological applications of aerosols include dispersal of pesticides, medical treatment of respiratory illnesses, and convincing technology. Diseases can also spread by means of small droplets in the breath, also called aerosols (or sometimes bioaerosols).\n\nAerosol science covers generation and removal of aerosols, technological application of aerosols, effects of aerosols on the environment and people, and other topics.\n\nAn aerosol is defined as a suspension system of solid or liquid particles in a gas. An aerosol includes both the particles and the suspending gas, which is usually air. Frederick G. Donnan presumably first used the term \"aerosol\" during World War I to describe an aero-solution, clouds of microscopic particles in air. This term developed analogously to the term hydrosol, a colloid system with water as the dispersed medium. \"Primary aerosols\" contain particles introduced directly into the gas; \"secondary aerosols\" form through gas-to-particle conversion.\n\nVarious types of aerosol, classified according to physical form and how they were generated, include dust, fume, mist, smoke and fog.\n\nThere are several measures of aerosol concentration. Environmental science and health often uses the \"mass concentration\" (\"M\"), defined as the mass of particulate matter per unit volume with units such as μg/m. Also commonly used is the \"number concentration\" (\"N\"), the number of particles per unit volume with units such as number/m or number/cm.\n\nThe size of particles has a major influence on their properties, and the aerosol particle radius or diameter (\"d\") is a key property used to characterise aerosols.\n\nAerosols vary in their dispersity. A \"monodisperse\" aerosol, producible in the laboratory, contains particles of uniform size. Most aerosols, however, as \"polydisperse\" colloidal systems, exhibit a range of particle sizes. Liquid droplets are almost always nearly spherical, but scientists use an \"equivalent diameter\" to characterize the properities of various shapes of solid particles, some very irregular. The equivalent diameter is the diameter of a spherical particle with the same value of some physical property as the irregular particle. The \"equivalent volume diameter\" (\"d\") is defined as the diameter of a sphere of the same volume as that of the irregular particle. Also commonly used is the aerodynamic diameter.\n\nFor a monodisperse aerosol, a single number—the particle diameter—suffices to describe the size of the particles. However, more complicated particle-size distributions describe the sizes of the particles in a polydisperse aerosol. This distribution defines the relative amounts of particles, sorted according to size. One approach to defining the particle size distribution uses a list of the sizes of every particle in a sample. However, this approach proves tedious to ascertain in aerosols with millions of particles and awkward to use. Another approach splits the complete size range into intervals and finds the number (or proportion) of particles in each interval. One then can visualize these data in a histogram with the area of each bar representing the proportion of particles in that size bin, usually normalised by dividing the number of particles in a bin by the width of the interval so that the area of each bar is proportionate to the number of particles in the size range that it represents. If the width of the bins tends to zero, one gets the frequency function:\n\nwhere\n\nTherefore, the area under the frequency curve between two sizes a and \"b\" represents the total fraction of the particles in that size range:\n\nIt can also be formulated in terms of the total number density \"N\":\n\nAssuming spherical aerosol particles, the aerosol surface area per unit volume (\"S\") is given by the second moment:\n\nAnd the third moment gives the total volume concentration (\"V\") of the particles:\n\nOne also usefully can approximate the particle size distribution using a mathematical function. The normal distribution usually does not suitably describe particle size distributions in aerosols because of the skewness associated a long tail of larger particles. Also for a quantity that varies over a large range, as many aerosol sizes do, the width of the distribution implies negative particles sizes, clearly not physically realistic. However, the normal distribution can be suitable for some aerosols, such as test aerosols, certain pollen grains and spores.\n\nA more widely chosen log-normal distribution gives the number frequency as:\n\nwhere:\n\nThe log-normal distribution has no negative values, can cover a wide range of values, and fits many observed size distributions reasonably well.\n\nOther distributions sometimes used to characterise particle size include: the Rosin-Rammler distribution, applied to coarsely dispersed dusts and sprays; the Nukiyama-Tanasawa distribution, for sprays of extremely broad size ranges; the power function distribution, occasionally applied to atmospheric aerosols; the exponential distribution, applied to powdered materials; and for cloud droplets, the Khrgian-Mazin distribution.\n\nFor low values of the Reynolds number (<1), true for most aerosol motion, Stokes' law describes the force of resistance on a solid spherical particle in a fluid. However, Stokes' law is only valid when the velocity of the gas at the surface of the particle is zero. For small particles (< 1 μm) that characterize aerosols, however, this assumption fails. To account for this failure, one can introduce the Cunningham correction factor, always greater than 1. Including this factor, one finds the relation between the resisting force on a particle and its velocity:\nwhere\n\nThis allows us to calculate the terminal velocity of a particle undergoing gravitational settling in still air. Neglecting buoyancy effects, we find:\nwhere\n\nThe terminal velocity can also be derived for other kinds of forces. If Stokes' law holds, then the resistance to motion is directly proportional to speed. The constant of proportionality is the mechanical mobility (\"B\") of a particle:\n\nformula_22\n\nA particle traveling at any reasonable initial velocity approaches its terminal velocity exponentially with an \"e\"-folding time equal to the relaxation time:\n\nformula_23\n\nwhere:\n\nTo account for the effect of the shape of non-spherical particles, a correction factor known as the \"dynamic shape factor\" is applied to Stokes' law. It is defined as the ratio of the resistive force of the irregular particle to that of a spherical particle with the same volume and velocity:\n\nformula_27\n\nwhere:\n\nThe aerodynamic diameter of an irregular particle is defined as the diameter of the spherical particle with a density of 1000 kg/m and the same settling velocity as the irregular particle.\n\nNeglecting the slip correction, the particle settles at the terminal velocity proportional to the square of the aerodynamic diameter, \"d\":\n\nwhere\n\nThis equation gives the aerodynamic diameter:\n\nOne can apply the aerodynamic diameter to particulate pollutants or to inhaled drugs to predict where in the respiratory tract such particles deposit. Pharmaceutical companies typically use aerodynamic diameter, not geometric diameter, to characterize particles in inhalable drugs. \n\nThe previous discussion focussed on single aerosol particles. In contrast, \"aerosol dynamics\" explains the evolution of complete aerosol populations. The concentrations of particles will change over time as a result of many processes. External processes that move particles outside a volume of gas under study include diffusion, gravitational settling, and electric charges and other external forces that cause particle migration. A second set of processes internal to a given volume of gas include particle formation (nucleation), evaporation, chemical reaction, and coagulation.\n\nA differential equation called the \"Aerosol General Dynamic Equation\" (GDE) characterizes the evolution of the number density of particles in an aerosol due to these processes.\n\nformula_32\n\nChange in time = Convective transport + brownian diffusion + gas-particle interactions + coagulation + migration by external forces\n\nWhere:\n\nAs particles and droplets in an aerosol collide with one another, they may undergo coalescence or aggregation. This process leads to a change in the aerosol particle-size distribution, with the mode increasing in diameter as total number of particles decreases. On occasion, particles may shatter apart into numerous smaller particles; however, this process usually occurs primarily in particles too large for consideration as aerosols.\n\nThe Knudsen number of the particle define three different dynamical regimes that govern the behaviour of an aerosol:\n\nwhere formula_39 is the mean free path of the suspending gas and formula_40 is the diameter of the particle. For particles in the \"free molecular regime\", \"K\" » 1; particles small compared to the mean free path of the suspending gas. In this regime, particles interact with the suspending gas through a series of \"ballistic\" collisions with gas molecules. As such, they behave similarly to gas molecules, tending to follow streamlines and diffusing rapidly through Brownian motion. The mass flux equation in the free molecular regime is:\n\nwhere \"a\" is the particle radius, \"P\" and \"P\" are the pressures far from the droplet and at the surface of the droplet respectively, \"k\" is the Boltzmann constant, \"T\" is the temperature, \"C\" is mean thermal velocity and \"α\" is mass accommodation coefficient. The derivation of this equation assumes constant pressure and constant diffusion coefficient.\n\nParticles are in the \"continuum regime\" when K « 1. In this regime, the particles are big compared to the mean free path of the suspending gas, meaning that the suspending gas acts as a continuous fluid flowing round the particle. The molecular flux in this regime is:\n\nwhere \"a\" is the radius of the particle \"A\", \"M\" is the molecular mass of the particle \"A\", \"D\" is the diffusion coefficient between particles \"A\" and \"B\", \"R\" is the ideal gas constant, \"T\" is the temperature (in absolute units like kelvin), and \"P\" and \"P\" are the pressures at infinite and at the surface respectively.\n\nThe \"transition regime\" contains all the particles in between the free molecular and continuum regimes or \"K\" ≈ 1. The forces experienced by a particle are a complex combination of interactions with individual gas molecules and macroscopic interactions. The semi-empirical equation describing mass flux is:\n\nwhere \"I\" is the mass flux in the continuum regime. This formula is called the Fuchs-Sutugin interpolation formula. These equations do not take into account the heat release effect.\n\nAerosol partitioning theory governs condensation on and evaporation from an aerosol surface, respectively. Condensation of mass causes the mode of the particle-size distributions of the aerosol to increase; conversely, evaporation causes the mode to decrease. Nucleation is the process of forming aerosol mass from the condensation of a gaseous precursor, specifically a vapour. Net condensation of the vapour requires supersaturation, a partial pressure greater than its vapour pressure. This can happen for three reasons:\n\nThere are two types of nucleation processes. Gases preferentially condense onto surfaces of pre-existing aerosol particles, known as heterogeneous nucleation. This process causes the diameter at the mode of particle-size distribution to increase with constant number concentration. With sufficiently high supersaturation and no suitable surfaces, particles may condense in the absence of a pre-existing surface, known as homogeneous nucleation. This results in the addition of very small, rapidly growing particles to the particle-size distribution.\n\nWater coats particles in an aerosols, making them \"activated\", usually in the context of forming a cloud droplet. Following the Kelvin equation (based on the curvature of liquid droplets), smaller particles need a higher ambient relative humidity to maintain equilibrium than larger particles do. The following formula gives relative humidity at equilibrium:\n\nwhere formula_45 is the saturation vapor pressure above a particle at equilibrium (around a curved liquid droplet), \"p\" is the saturation vapor pressure (flat surface of the same liquid) and \"S\" is the saturation ratio.\n\nKelvin equation for saturation vapor pressure above a curved surface is:\n\nwhere \"r\" droplet radius, \"σ\" surface tension of droplet, \"ρ\" density of liquid, \"M\" molar mass, \"T\" temperature, and \"R\" molar gas constant.\n\nThere are no general solutions to the general dynamic equation (GDE); common methods used to solve the general dynamic equation include:\n\nPeople generate aerosols for various purposes, including:\n\nSome devices for generating aerosols are:\n\nStability of nanoparticle agglomerates is critical for estimating size distribution of aerosolized particles from nano-powders or other sources. At nanotechnology workplaces, workers can be exposed via inhalation to potentially toxic substances during handling and processing of nanomaterials. Nanoparticles in the air often form agglomerates due to attractive inter-particle forces, such as van der Waals force or electrostatic force if the particles are charged. As a result, aerosol particles are usually observed as agglomerates rather than individual particles. For exposure and risk assessments of airborne nanoparticles, it is important to know about the size distribution of aerosols. When inhaled by humans, particles with different diameters are deposited in varied locations of the central and periphery respiratory system. Particles in nanoscale have been shown to penetrate the air-blood barrier in lungs and be translocated into secondary organs in the human body, such as the brain, heart and liver. Therefore, the knowledge on stability of nanoparticle agglomerates is important for predicting the size of aerosol particles, which helps assess the potential risk of them to human bodies.\n\nDifferent experimental systems have been established to test the stability of airborne particles and their potentials to deagglomerate under various conditions. A comprehensive system recently reported by Ding & Riediker (2015) is able to maintain robust aerosolization process and generate aerosols with stable number concentration and mean size from nano-powders. The deagglomeration potential of various airborne nanomaterials can be also studied using critical orifices. This process was also investigated by Stahlmecke et al. (2009). In addition, an impact fragmentation device was developed to investigate bonding energies between particles.\n\nA standard deagglomeration testing procedure could be foreseen with the developments of the different types of existing systems. The likeliness of deagglomeration of aerosol particles in occupational settings can be possibly ranked for different nanomaterials if a reference method is available. For this purpose, inter-laboratory comparison of testing results from different setups could be launched in order to explore the influences of system characteristics on properties of generated nanomaterials aerosols.\n\nAerosol can either be measured in-situ or with remote sensing techniques.\n\nSome available in situ measurement techniques include:\n\nRemote sensing approaches include:\n\nParticles can deposit in the nose, mouth, pharynx and larynx (the head airways region), deeper within the respiratory tract (from the trachea to the terminal bronchioles), or in the alveolar region. The location of deposition of aerosol particles within the respiratory system strongly determines the health effects of exposure to such aerosols. This phenomenon led people to invent aerosol samplers that select a subset of the aerosol particles that reach certain parts of the respiratory system. Examples of these subsets of the particle-size distribution of an aerosol, important in occupational health, include the inhalable, thoracic, and respirable fractions. The fraction that can enter each part of the respiratory system depends on the deposition of particles in the upper parts of the airway. The inhalable fraction of particles, defined as the proportion of particles originally in the air that can enter the nose or mouth, depends on external wind speed and direction and on the particle-size distribution by aerodynamic diameter. The thoracic fraction is the proportion of the particles in ambient aerosol that can reach the thorax or chest region. The respirable fraction is the proportion of particles in the air that can reach the alveolar region. To measure the respirable fraction of particles in air, a pre-collector is used with a sampling filter. The pre-collector excludes particles as the airways remove particles from inhaled air. The sampling filter collects the particles for measurement. It is common to use cyclonic separation for the pre-collector, but other techniques include impactors, horizontal elutriators, and large pore membrane filters.\n\nTwo alternative size-selective criteria, often used in atmospheric monitoring, are PM and PM. PM is defined by ISO as \"particles which pass through a size-selective inlet with a 50% efficiency cut-off at 10 μm aerodynamic diameter\" and PM as \"particles which pass through a size-selective inlet with a 50% efficiency cut-off at 2.5 μm aerodynamic diameter\". PM corresponds to the “thoracic convention” as defined in ISO 7708:1995, Clause 6; PM corresponds to the “high-risk respirable convention” as defined in ISO 7708:1995, 7.1. The United States Environmental Protection Agency replaced the older standards for particulate matter based on Total Suspended Particulate with another standard based on PM in 1987 and then introduced standards for PM (also known as fine particulate matter) in 1997.\n\nSeveral types of atmospheric aerosol have a significant effect on Earth's climate: volcanic, desert dust, sea-salt, that originating from biogenic sources and human-made. Volcanic aerosol forms in the stratosphere after an eruption as droplets of sulfuric acid that can prevail for up to two years, and reflect sunlight, lowering temperature. Desert dust, mineral particles blown to high altitudes, absorb heat and may be responsible for inhibiting storm cloud formation. Human-made sulfate aerosols, primarily from burning oil and coal, affect the behavior of clouds.\n\nAlthough all hydrometeors, solid and liquid, can be described as aerosols, a distinction is commonly made between such dispersions (i.e. clouds) containing activated drops and crystals, and aerosol particles. The atmosphere of Earth contains aerosols of various types and concentrations, including quantities of:\n\nAerosols can be found in urban ecosystems in various forms, for example:\n\nThe presence of aerosols in the earth's atmosphere can influence its climate, as well as human health.\n\n\n\n\n\n\n"}
{"id": "938631", "url": "https://en.wikipedia.org/wiki?curid=938631", "title": "Antineutron", "text": "Antineutron\n\nThe antineutron is the antiparticle of the neutron with symbol . It differs from the neutron only in that some of its properties have equal magnitude but opposite sign. It has the same mass as the neutron, and no net electric charge, but has opposite baryon number (+1 for neutron, −1 for the antineutron). This is because the antineutron is composed of antiquarks, while neutrons are composed of quarks. The antineutron consists of one up antiquark and two down antiquarks.\n\nSince the antineutron is electrically neutral, it cannot easily be observed directly. Instead, the products of its annihilation with ordinary matter are observed. In theory, a free antineutron should decay into an antiproton, a positron and a neutrino in a process analogous to the beta decay of free neutrons. There are theoretical proposals of neutron–antineutron oscillations, a process that implies the violation of the baryon number conservation.\nThe antineutron was discovered in proton–antiproton collisions at the Bevatron (Lawrence Berkeley National Laboratory) by Bruce Cork in 1956, one year after the antiproton was discovered.\n\nThe magnetic moment of the antineutron is the opposite of that of the neutron. It is for the antineutron but for the neutron (relative to the direction of the spin). Here µ is the nuclear magneton.\n\n\n"}
{"id": "5748142", "url": "https://en.wikipedia.org/wiki?curid=5748142", "title": "CALPUFF", "text": "CALPUFF\n\nCALPUFF is an advanced, integrated Lagrangian puff modeling system for the simulation of atmospheric pollution dispersion distributed by the Atmospheric Studies Group at TRC Solutions.\n\nIt is maintained by the model developers and distributed by TRC.\nThe model has been adopted by the United States Environmental Protection Agency (EPA) in its \"Guideline on Air Quality Models\" as a preferred model for assessing long range transport of pollutants and their impacts on Federal Class I areas and on a case-by-case basis for certain near-field applications involving complex meteorological conditions.\n\nThe integrated modeling system consists of three main components and a set of preprocessing and postprocessing programs. The main components of the modeling system are CALMET (a diagnostic 3-dimensional meteorological model), CALPUFF (an air quality dispersion model), and CALPOST (a postprocessing package). Each of these programs has a graphical user interface (GUI). In addition to these components, there are numerous other processors that may be used to prepare geophysical (land use and terrain) data in many standard formats, meteorological data (surface, upper air, precipitation, and buoy data), and interfaces to other models such as the Penn State/NCAR Mesoscale Model (MM5), the National Centers for Environmental Prediction (NCEP) Eta model and the RAMS meteorological model.\n\nThe CALPUFF model is designed to simulate the dispersion of buoyant, puff or continuous point and area pollution sources as well as the dispersion of buoyant, continuous line sources. The model also includes algorithms for handling the effect of downwash by nearby buildings in the path of the pollution plumes.\n\nThe CALPUFF model was originally developed by the Sigma Research Corporation (SRC) in the late 1980s under contract with the California Air Resources Board (CARB) and it was first issued in about 1990.\n\nThe Sigma Research Corporation subsequently became part of Earth Tech, Inc. After the US EPA designated CALPUFF as a preferred model in their \"Guideline on Air Quality Models\", Earth Tech served as the designated distributor of the model.\n\nIn April 2006, ownership of the model switched from Earth Tech to the TRC Environmental Corporation. More recently ownership transferred to Eponent, who are currently (December 2015) responsible for maintaining and distributing the model.\n\n\n\n"}
{"id": "52993639", "url": "https://en.wikipedia.org/wiki?curid=52993639", "title": "Cheuquemó Formation", "text": "Cheuquemó Formation\n\nCheuquemó Formation () is a geological formation of sedimentary rock in south-central Chile. The sediments of the formation were deposited during the Late Oligocene and Early Miocene epochs. The formations lower sections are made up of conglomerate, then successions of sandstone, tuff and mudstone rich in organic material follows. The formation indicates that sedimentation occurred in a estuarine (paralic) and other non-marine (continental) environments. It contains fossils of the following genera: \"Mytilus\", \"Cardium\" and \"Turritella\". Stratigraphically it overlies the Bahía Mansa Metamorphic Complex and underlies the Miocene Santo Domingo Formation.\n\nThe formation is very similar to the Pupunahue Beds found further north, with the sole difference that the fossil assemblage in both seem to indicate different ages. While Cheuquemó is possibly about 14 million years old (Miocene), the Pupunahue Beds are 35–25 million years old.\n\n"}
{"id": "3180539", "url": "https://en.wikipedia.org/wiki?curid=3180539", "title": "Chondrin", "text": "Chondrin\n\nChondrin is a bluish-white gelatin-like substance, being a protein-carbohydrate complex and can be obtained by boiling cartilage in water.\nThe cartilage is a connective tissue that contains cells embedded in a matrix of chondrin. Chondrin is made up of two proteins chondroalbunoid and chondromucoid.\n\n\n"}
{"id": "17530792", "url": "https://en.wikipedia.org/wiki?curid=17530792", "title": "Climate of Ecuador", "text": "Climate of Ecuador\n\nThe climate of Ecuador varies by region, due to differences in elevation and, to a degree, in proximity to the equator.\n\nThe coastal lowlands in the western part of Ecuador are typically warm with temperatures in the region of . Coastal areas are affected by ocean currents and between January and April are hot and rainy.\n\nThe weather in Quito is consistent with that of a subtropical highland climate. The city has barely any cool air since it is close to the equator. The average temperature during the day is , which generally falls to an average of at night. The average temperature annually is . There are only really two seasons in the city: dry and wet. The dry season (winter) runs from June to September and the wet season (summer) is from October to May. As most of Ecuador is in the southern hemisphere, June to September is considered to be winter, and winter is generally the dry season in warm climates. Spring, summer, and fall are generally the \"wet seasons\" while winter is the dry (with the exception of the first month of fall being dry).\n\n\n"}
{"id": "11753924", "url": "https://en.wikipedia.org/wiki?curid=11753924", "title": "Climate risk management", "text": "Climate risk management\n\nClimate Risk Management (CRM) is a term is used for a large and growing body of work, bridging the climate change adaptation, disaster management and development sectors, amongst many others. \nThis journal is not indexed by the Science Citation Index Expanded (Web of Science) with no impact factor.\nClimate risk management is a generic term referring to an approach to climate-sensitive decision making. The approach seeks to promote sustainable development by reducing the vulnerability associated with climate risk. CRM involves strategies aimed at maximizing positive and minimizing negative outcomes for communities in fields such as agriculture, food security, water resources, and health.\n\nClimate risk management covers a broad range of potential actions, including: early-response systems, strategic diversification, dynamic resource-allocation rules, financial instruments, infrastructure design and capacity building. But in addition to avoiding adverse outcomes, a climate risk management strategy also aims to maximize opportunities in climate-sensitive economic sectors--for example, farmers who use favorable seasonal forecasts to maximize their crop productivity.\n\nThe United Nations Framework Convention on Climate Change involves negotiations among delegates on climate change framework. Discussions center on mitigation, adaptation, technology development and transfer, and financial resources and investment. \n\nThe Living with Climate Conference was co-hosted by the World Meteorological Organization, the Earth Institute and the Finnish Meteorological Institute in July, 2006. The meeting was designed to review opportunities and constraints in integrating climate risks and uncertainties into decision-making. A major outcome was the Espoo Statement.\n\n\n"}
{"id": "47515", "url": "https://en.wikipedia.org/wiki?curid=47515", "title": "Cloud", "text": "Cloud\n\nIn meteorology, a cloud is an aerosol consisting of a visible mass of minute liquid droplets, frozen crystals, or other particles suspended in the atmosphere of a planetary body. Water or various other chemicals may compose the droplets and crystals. On Earth, clouds are formed as a result of saturation of the air when it is cooled to its dew point, or when it gains sufficient moisture (usually in the form of water vapor) from an adjacent source to raise the dew point to the ambient temperature. They are seen in the Earth's homosphere (which includes the troposphere, stratosphere, and mesosphere). Nephology is the science of clouds, which is undertaken in the cloud physics branch of meteorology.\n\nThere are two methods of naming clouds in their respective layers of the atmosphere; Latin and common. Cloud types in the troposphere, the atmospheric layer closest to Earth's surface, have Latin names due to the universal adaptation of Luke Howard's nomenclature. Formally proposed in 1802, it became the basis of a modern international system that divides clouds into five physical \"forms\" that appear in any or all of three altitude \"levels\" (formerly known as \"étages\"). These physical types, in approximate ascending order of convective activity, include \"stratiform\" sheets, \"cirriform\" wisps and patches, \"stratocumuliform\" layers (mainly structured as rolls, ripples, and patches), \"cumuliform\" heaps, and very large \"cumulonimbiform\" heaps that often show complex structure. The physical forms are divided by altitude level into ten basic genus-types. The Latin names for applicable high-level genera carry a \"cirro\"- prefix, and an \"alto\"- prefix is added to the names of the mid-level genus-types. Most of the genera can be subdivided into \"species\" and further subdivided into \"varieties\". A very low stratiform cloud that extends down to the Earth's surface is given the common name, fog, but has no Latin name.\n\nTwo cirriform clouds that form higher up in the stratosphere and mesosphere have common names for their main types. They are seen infrequently, mostly in the polar regions of Earth. Clouds have been observed in the atmospheres of other planets and moons in the Solar System and beyond. However, due to their different temperature characteristics, they are often composed of other substances such as methane, ammonia, and sulfuric acid as well as water.\n\nTaken as a whole, homospheric clouds can be cross-classified by form and level to derive the ten tropospheric genera, the fog that forms at surface level, and the two additional major types above the troposphere. The cumulus genus includes three species that indicate vertical size. Clouds with sufficient vertical extent to occupy more than one altitude level are officially classified as \"low-\" or \"mid-level\" according to the altitude range at which each initially forms. However they are also more informally classified as \"multi-level\" or \"vertical\".\n\nThe origin of the term \"cloud\" can be found in the old English \"clud\" or \"clod\", meaning a hill or a mass of rock. Around the beginning of the 13th century, the word came to be used as a metaphor for rain clouds, because of the similarity in appearance between a mass of rock and cumulus heap cloud. Over time, the metaphoric usage of the word supplanted the old English weolcan, which had been the literal term for clouds in general.\n\nAncient cloud studies were not made in isolation, but were observed in combination with other weather elements and even other natural sciences. In about 340 BC the Greek philosopher Aristotle wrote \"Meteorologica\", a work which represented the sum of knowledge of the time about natural science, including weather and climate. For the first time, precipitation and the clouds from which precipitation fell were called meteors, which originate from the Greek word \"meteoros\", meaning 'high in the sky'. From that word came the modern term meteorology, the study of clouds and weather. Meteorologica was based on intuition and simple observation, but not on what is now considered the scientific method. Nevertheless, it was the first known work that attempted to treat a broad range of meteorological topics.\n\nAfter centuries of speculative theories about the formation and behavior of clouds, the first truly scientific studies were undertaken by Luke Howard in England and Jean-Baptiste Lamarck in France. Howard was a methodical observer with a strong grounding in the Latin language and used his background to classify the various tropospheric cloud types during 1802. He believed that the changing cloud forms in the sky could unlock the key to weather forecasting. Lamarck had worked independently on cloud classification the same year and had come up with a different naming scheme that failed to make an impression even in his home country of France because it used unusual French names for cloud types. His system of nomenclature included twelve categories of clouds, with such names as (translated from French) hazy clouds, dappled clouds and broom-like clouds. By contrast, Howard used universally accepted Latin, which caught on quickly after it was published in 1803. As a sign of the popularity of the naming scheme, the German dramatist and poet Johann Wolfgang von Goethe composed four poems about clouds, dedicating them to Howard. An elaboration of Howard's system was eventually formally adopted by the International Meteorological Conference in 1891. This system covered only the tropospheric cloud types, but the discovery of clouds above the troposphere during the late 19th century eventually led to the creation separate classification schemes for these very high clouds.\n\nTerrestrial clouds can be found throughout most of the homosphere, which includes the troposphere, stratosphere, and mesosphere. Within these layers of the atmosphere, air can become saturated as a result of being cooled to its dew point or by having moisture added from an adjacent source. In the latter case, saturation occurs when the dew point is raised to the ambient air temperature.\n\nAdiabatic cooling occurs when one or more of three possible lifting agents – cyclonic/frontal, convective, or orographic – causes a parcel of air containing invisible water vapor to rise and cool to its dew point, the temperature at which the air becomes saturated. The main mechanism behind this process is adiabatic cooling. As the air is cooled to its dew point and becomes saturated, water vapor normally condenses to form cloud drops. This condensation normally occurs on cloud condensation nuclei such as salt or dust particles that are small enough to be held aloft by normal circulation of the air.\n\nFrontal and cyclonic lift occur when stable air is forced aloft at weather fronts and around centers of low pressure by a process called convergence. Warm fronts associated with extratropical cyclones tend to generate mostly cirriform and stratiform clouds over a wide area unless the approaching warm airmass is unstable, in which case cumulus congestus or cumulonimbus clouds will usually be embedded in the main precipitating cloud layer. Cold fronts are usually faster moving and generate a narrower line of clouds which are mostly stratocumuliform, cumuliform, or cumulonimbiform depending on the stability of the warm air mass just ahead of the front.\nAnother agent is the convective upward motion of air caused by daytime solar heating at surface level. Airmass instability allows for the formation of cumuliform clouds that can produce showers if the air is sufficiently moist. On moderately rare occasions, convective lift can be powerful enough to penetrate the tropopause and push the cloud top into the stratosphere.\n\nA third source of lift is wind circulation forcing air over a physical barrier such as a mountain (orographic lift). If the air is generally stable, nothing more than lenticular cap clouds will form. However, if the air becomes sufficiently moist and unstable, orographic showers or thunderstorms may appear.\n\nAlong with adiabatic cooling that requires a lifting agent, there are three major non-adiabatic mechanisms for lowering the temperature of the air to its dew point. Conductive, radiational, and evaporative cooling require no lifting mechanism and can cause condensation at surface level resulting in the formation of fog.\n\nThere are several main sources of water vapor that can be added to the air as a way of achieving saturation without any cooling process: water or moist ground, precipitation or virga, and transpiration from plants\n\nTropospheric classification is based on a hierarchy of categories with physical forms and altitude levels at the top. These are cross-classified into a total of ten genus types, most of which can be divided into species and further subdivided into varieties which are at the bottom of the hierarchy.\n\nClouds in the troposphere assume five physical forms based on structure and process of formation. These forms are commonly used for the purpose of satellite analysis. They are given below in approximate ascending order of instability or convective activity.\n\n\"Non-convective\" stratiform clouds appear in \"stable\" airmass conditions and, in general, have flat sheet-like structures that can form at any altitude in the troposphere. The stratiform group is divided by altitude range into the genera cirrostratus (high-level), altostratus (mid-level), stratus (low-level), and nimbostratus (multi-level). Fog is commonly considered a surface-based cloud layer. The fog may form at surface level in clear air or it may be the result of a very low stratus cloud subsiding to ground or sea level. Conversely, low stratiform cloud results when advection fog is lifted above surface level during breezy conditions.\n\nCirriform clouds are generally of the genus cirrus and have the appearance of detached or semi-merged filaments. They form at high tropospheric altitudes in air that is mostly stable with little or no convective activity, although denser patches may occasionally show buildups caused by \"limited\" high-level convection where the air is partly \"unstable\".\n\nClouds of this structure have both cumuliform and stratiform characteristics in the form of rolls, ripples, or elements. They generally form as a result of \"limited convection\" in an otherwise mostly stable airmass topped by an inversion layer. If the inversion layer is absent or higher in the troposphere, increased air mass instability may cause the cloud layers to develop tops in the form of turrets consisting of embedded cumuliform buildups. The stratocumuliform group is divided into cirrocumulus (high-level), altocumulus (mid-level), and stratocumulus (low-level).\n\nCumuliform clouds generally appear in isolated heaps or tufts. They are the product of localized but generally \"free-convective\" lift where there are no inversion layers in the atmosphere to limit vertical growth. In general, small cumuliform clouds tend to indicate comparatively weak instability. Larger cumuliform types are a sign of moderate to strong atmospheric instability and convective activity. Depending on their vertical size, clouds of the cumulus genus type may be low-level or multi-level with moderate to towering vertical extent.\n\nThe largest free-convective clouds comprise the genus cumulonimbus which are multi-level because of their towering vertical extent. They occur in highly unstable air and often have fuzzy outlines at the upper parts of the clouds that sometimes include anvil tops.\n\nTropospheric clouds form in any of three levels (formerly called étages) based on altitude range above the Earth's surface. The grouping of clouds into levels is commonly done for the purposes of cloud atlases, surface weather observations and weather maps. The base-height range for each level varies depending on the latitudinal geographical zone. Each altitude level comprises two or three genus types differentiated mainly by physical form.\n\n\"The standard levels and genus-types are summarised below in approximate descending order of the altitude at which each is normally based\". Multi-level clouds with significant vertical extent are separately listed and summarized in approximate ascending order of instability or convective activity.\n\nHigh clouds form at altitudes of in the polar regions, in the temperate regions and in the tropics. All cirriform clouds are classified as high and thus constitute a single genus \"cirrus\" (Ci). Stratocumuliform and stratiform clouds in the high altitude range carry the prefix \"cirro-\", yielding the respective genus names \"cirrocumulus\" (Cc) and \"cirrostratus\" (Cs). When limited-resolution satellite images of high clouds are analysed without supporting data from direct human observations, it becomes impossible to distinguish between individual forms or genus types, which are then collectively identified as \"high-type\" (or informally as \"cirrus-type\" even though not all high clouds are of the cirrus form or genus).\n\n\nNon-vertical clouds in the middle level are prefixed by \"alto-\", yielding the genus names \"altocumulus\" (Ac) for stratocumuliform types and \"altostratus\" (As) for stratiform types. These clouds can form as low as above surface at any latitude, but may be based as high as near the poles, at mid latitudes, and in the tropics. As with high clouds, the main genus types are easily identified by the human eye, but it is not possible to distinguish between them using satellite photography. Without the support of human observations, these clouds are usually collectively identified as \"middle-type\" on satellite images.\n\n\n\nLow clouds are found from near surface up to . Genus types in this level either have no prefix or carry one that refers to a characteristic other than altitude. Clouds that form in the low level of the troposphere are generally of larger structure than those that form in the middle and high levels, so they can usually be identified by their forms and genus types using satellite photography alone.\n\nThese clouds have low to middle level bases that form anywhere from near surface to about and tops that can extend into the high altitude range. Nimbostratus and some cumulus in this group usually achieve moderate or deep vertical extent, but without towering structure. However, with sufficient airmass instability, upward-growing cumuliform clouds can grow to high towering proportions. Although genus types with vertical extent are often informally considered a single group, the International Civil Aviation Organization (ICAO) distinguishes towering vertical clouds more formally as a separate group or sub-group. It is specified that these very large cumuliform and cumulonimbiform types must be identified by their standard names or abbreviations in all aviation observations (METARS) and forecasts (TAFS) to warn pilots of possible severe weather and turbulence. Multi-level clouds are of even larger structure than low clouds, and are therefore identifiable by their forms and genera, (and even species in the case of cumulus congestus) using satellite photography.\n\nThis is a diffuse dark-grey non-convective stratiform layer with great horizontal extent and moderate to deep vertical development. It lacks towering structure and looks feebly illuminated from the inside. Nimbostratus normally forms from mid-level altostratus, and develops at least moderate vertical extent when the base subsides into the low level during precipitation that can reach moderate to heavy intensity. It commonly achieves deep vertical development when it simultaneously grows upward into the high level due to large scale frontal or cyclonic lift. The \"nimbo-\" prefix refers to its ability to produce continuous rain or snow over a wide area, especially ahead of a warm front. This thick cloud layer may be accompanied by embedded towering cumuliform or cumulonimbiform types. Meteorologists affiliated with the World Meteorological Organization (WMO) officially classify nimbostratus as mid-level for synoptic purposes while informally characterizing it as multi-level. Independent meteorologists and educators appear split between those who largely follow the WMO model and those who classify nimbostratus as low-level, despite its considerable vertical extent and its usual initial formation in the middle altitude range.\n\nThese clouds are sometimes classified separately from the other vertical or multi-level types because of their ability to produce severe turbulence.\n\nGenus types are commonly divided into subtypes called \"species\" that indicate specific structural details which can vary according to the stability and windshear characteristics of the atmosphere at any given time and location. Despite this hierarchy, a particular species may be a subtype of more than one genus, especially if the genera are of the same physical form and are differentiated from each other mainly by altitude or level. There are a few species, each of which can be associated with genera of more than one physical form. The species types are grouped below according to the physical forms and genera with which each is normally associated. The forms, genera, and species are listed in approximate ascending order of instability or convective activity.\n\nGenus and species types are further subdivided into \"varieties\" whose names can appear after the species name to provide a fuller description of a cloud. Some cloud varieties are not restricted to a specific altitude level or form, and can therefore be common to more than one genus or species.\n\nOf the stratiform group, high-level cirrostratus comprises two species. Cirrostratus \"nebulosus\" has a rather diffuse appearance lacking in structural detail. Cirrostratus \"fibratus\" is a species made of semi-merged filaments that are transitional to or from cirrus. Mid-level altostratus and multi-level nimbostratus always have a flat or diffuse appearance and are therefore not subdivided into species. Low stratus is of the species nebulosus except when broken up into ragged sheets of stratus fractus (see below).\n\nCirriform clouds have three non-convective species that can form in mostly \"stable\" airmass conditions. Cirrus fibratus comprise filaments that may be straight, wavy, or occasionally twisted by non-convective wind shear. The species \"uncinus\" is similar but has upturned hooks at the ends. Cirrus \"spissatus\" appear as opaque patches that can show light grey shading.\n\nStratocumuliform genus-types (cirrocumulus, altocumulus, and stratocumulus) that appear in mostly stable air have two species each. The \"stratiformis\" species normally occur in extensive sheets or in smaller patches where there is only minimal convective activity. Clouds of the \"lenticularis\" species tend to have lens-like shapes tapered at the ends. They are most commonly seen as orographic mountain-wave clouds, but can occur anywhere in the troposphere where there is strong wind shear combined with sufficient airmass stability to maintain a generally flat cloud structure. These two species can be found in the high, middle, or low level of the troposphere depending on the stratocumuliform genus or genera present at any given time.\n\nThe species \"fractus\" shows \"variable\" instability because it can be a subdivision of genus-types of different physical forms that have different stability characteristics. This subtype can be in the form of ragged but mostly \"stable\" stratiform sheets (stratus fractus) or small ragged cumuliform heaps with somewhat greater instability (cumulus fractus). When clouds of this species are associated with precipitating cloud systems of considerable vertical and sometimes horizontal extent, they are also classified as \"accessory clouds\" under the name \"pannus\" (see section on supplementary features).\n\nThese species are subdivisions of genus types that can occur in partly unstable air. The species \"castellanus\" appears when a mostly stable stratocumuliform or cirriform layer becomes disturbed by localized areas of airmass instability, usually in the morning or afternoon. This results in the formation of cumuliform buildups arising from a common stratiform base. Castellanus resembles the turrets of a castle when viewed from the side, and can be found with stratocumuliform genera at any tropospheric altitude level and with limited-convective patches of high-level cirrus. Tufted clouds of the more detached \"floccus\" species are subdivisions of genus-types which may be cirriform or stratocumuliform in overall structure. They are sometimes seen with cirrus, cirrocumulus, altocumulus, and stratocumulus.\n\nA newly recognized species of stratocumulus or altocumulus has been given the name \"volutus\", a roll cloud that can occur ahead of a cumulonimbus formation. There are some volutus clouds that form as a consequence of interactions with specific geographical features rather than with a parent cloud. Perhaps the strangest geographically specific cloud of this type is the Morning Glory, a rolling cylindrical cloud that appears unpredictably over the Gulf of Carpentaria in Northern Australia. Associated with a powerful \"ripple\" in the atmosphere, the cloud may be \"surfed\" in glider aircraft.\n\nMore general airmass instability in the troposphere tends to produce clouds of the more freely convective cumulus genus type, whose species are mainly indicators of degrees of atmospheric instability and resultant vertical development of the clouds. A cumulus cloud initially forms in the low level of the troposphere as a cloudlet of the species \"humilis\" that shows only slight vertical development. If the air becomes more unstable, the cloud tends to grow vertically into the species \"mediocris\", then \"congestus\", the tallest cumulus species which is the same type that the International Civil Aviation Organization refers to as 'towering cumulus'.\n\nWith highly unstable atmospheric conditions, large cumulus may continue to grow into cumulonimbus \"calvus\" (essentially a very tall congestus cloud that produces thunder), then ultimately into the species \"capillatus\" when supercooled water droplets at the top of the cloud turn into ice crystals giving it a cirriform appearance.\n\nAll cloud varieties fall into one of two main groups. One group identifies the opacities of particular low and mid-level cloud structures and comprises the varieties \"translucidus\" (thin translucent), \"perlucidus\" (thick opaque with translucent or very small clear breaks), and \"opacus\" (thick opaque). These varieties are always identifiable for cloud genera and species with variable opacity. All three are associated with the stratiformis species of altocumulus and stratocumulus. However, only two varieties are seen with altostratus and stratus nebulosus whose uniform structures prevent the formation of a perlucidus variety. Opacity-based varieties are not applied to high clouds because they are always translucent, or in the case of cirrus spissatus, always opaque.\n\nA second group describes the occasional arrangements of cloud structures into particular patterns that are discernible by a surface-based observer (cloud fields usually being visible only from a significant altitude above the formations). These varieties are not always present with the genera and species with which they are otherwise associated, but only appear when atmospheric conditions favor their formation. \"Intortus\" and \"vertebratus\" varieties occur on occasion with cirrus fibratus. They are respectively filaments twisted into irregular shapes, and those that are arranged in fishbone patterns, usually by uneven wind currents that favor the formation of these varieties. The variety \"radiatus\" is associated with cloud rows of a particular type that appear to converge at the horizon. It is sometimes seen with the fibratus and uncinus species of cirrus, the stratiformis species of altocumulus and stratocumulus, the mediocris and sometimes humilis species of cumulus, and with the genus altostratus.\nAnother variety, \"duplicatus\" (closely spaced layers of the same type, one above the other), is sometimes found with cirrus of both the fibratus and uncinus species, and with altocumulus and stratocumulus of the species stratiformis and lenticularis. The variety \"undulatus\" (having a wavy undulating base) can occur with any clouds of the species stratiformis or lenticularis, and with altostratus. It is only rarely observed with stratus nebulosus. The variety \"lacunosus\" is caused by localized downdrafts that create circular holes in the form of a honeycomb or net. It is occasionally seen with cirrocumulus and altocumulus of the species stratiformis, castellanus, and floccus, and with stratocumulus of the species stratiformis and castellanus.\n\nIt is possible for some species to show combined varieties at one time, especially if one variety is opacity-based and the other is pattern-based. An example of this would be a layer of altocumulus stratiformis arranged in seemingly converging rows separated by small breaks. The full technical name of a cloud in this configuration would be \"altocumulus stratiformis radiatus perlucidus\", which would identify respectively its genus, species, and two combined varieties.\n\nSupplementary features and accessory clouds are not further subdivisions of cloud types below the species and variety level. Rather, they are either \"hydrometeors\" or special cloud types with their own Latin names that form in association with certain cloud genera, species, and varieties. Supplementary features, whether in the form of clouds or precipitation, are directly attached to the main genus-cloud. Accessory clouds, by contrast, are generally detached from the main cloud.\n\nOne group of supplementary features are not actual cloud formations, but precipitation that falls when water droplets or ice crystals that make up visible clouds have grown too heavy to remain aloft. \"Virga\" is a feature seen with clouds producing precipitation that evaporates before reaching the ground, these being of the genera cirrocumulus, altocumulus, altostratus, nimbostratus, stratocumulus, cumulus, and cumulonimbus.\n\nWhen the precipitation reaches the ground without completely evaporating, it is designated as the feature \"praecipitatio\". This normally occurs with altostratus opacus, which can produce widespread but usually light precipitation, and with thicker clouds that show significant vertical development. Of the latter, \"upward-growing\" cumulus mediocris produces only isolated light showers, while \"downward growing\" nimbostratus is capable of heavier, more extensive precipitation. Towering vertical clouds have the greatest ability to produce intense precipitation events, but these tend to be localized unless organized along fast-moving cold fronts. Showers of moderate to heavy intensity can fall from cumulus congestus clouds. Cumulonimbus, the largest of all cloud genera, has the capacity to produce very heavy showers. Low stratus clouds usually produce only light precipitation, but this always occurs as the feature praecipitatio due to the fact this cloud genus lies too close to the ground to allow for the formation of virga.\n\n\"Incus\" is the most type-specific supplementary feature, seen only with cumulonimbus of the species capillatus. A cumulonimbus incus cloud top is one that has spread out into a clear anvil shape as a result of rising air currents hitting the stability layer at the tropopause where the air no longer continues to get colder with increasing altitude.\n\nThe \"mamma\" feature forms on the bases of clouds as downward-facing bubble-like protuberances caused by localized downdrafts within the cloud. It is also sometimes called \"mammatus\", an earlier version of the term used before a standardization of Latin nomenclature brought about by the World Meterorological Organization during the 20th century. The best-known is cumulonimbus with mammatus, but the mamma feature is also seen occasionally with cirrus, cirrocumulus, altocumulus, altostratus, and stratocumulus.\n\nA \"tuba\" feature is a cloud column that may hang from the bottom of a cumulus or cumulonimbus. A newly formed or poorly organized column might be comparatively benign, but can quickly intensify into a funnel cloud or tornado.\n\nAn \"arcus\" feature is a roll cloud with ragged edges attached to the lower front part of cumulus congestus or cumulonimbus that forms along the leading edge of a squall line or thunderstorm outflow. A large arcus formation can have the appearance of a dark menacing arch.\n\nSeveral new supplementary features have been formally recognized by the World Meteorological Organization (WMO). The feature \"fluctus\" can form under conditions of strong atmospheric wind shear when a stratocumulus, altocumulus, or cirrus cloud breaks into regularly spaced crests. This variant is sometimes known informally as a Kelvin–Helmholtz (wave) cloud. This phenomenon has also been observed in cloud formations over other planets and even in the sun's atmosphere. Another highly disturbed but more chaotic wave-like cloud feature associated with stratocumulus or altocumulus cloud has been given the Latin name \"asperitas\". The supplementary feature \"cavum\" is a circular fall-streak hole that occasionally forms in a thin layer of supercooled altocumulus or cirrocumulus. Fall streaks consisting of virga or wisps of cirrus are usually seen beneath the hole as ice crystals fall out to a lower altitude. This type of hole is usually larger than typical lacunosus holes. A \"murus\" feature is a cumulonimbus wall cloud with a lowering, rotating cloud base than can lead to the development of tornadoes. A \"cauda\" feature is a tail cloud that extends horizontally away from the murus cloud and is the result of air feeding into the storm.\n\nSupplementary cloud formations detached from the main cloud are known as accessory clouds. The heavier precipitating clouds, nimbostratus, towering cumulus (cumulus congestus), and cumulonimbus typically see the formation in precipitation of the \"pannus\" feature, low ragged clouds of the genera and species cumulus fractus or stratus fractus.\n\nA group of accessory clouds comprise formations that are associated mainly with upward-growing cumuliform and cumulonimbiform clouds of free convection. \"Pileus\" is a cap cloud that can form over a cumulonimbus or large cumulus cloud, whereas a \"velum\" feature is a thin horizontal sheet that sometimes forms like an apron around the middle or in front of the parent cloud. An accessory cloud recently officially recognized the World meteorological Organization is the \"flumen\", also known more informally as the \"beaver's tail\". It is formed by the warm, humid inflow of a super-cell thunderstorm, and can be mistaken for a tornado. Although the flumen can indicate a tornado risk, it is similar in appearance to pannus or scud clouds and does not rotate.\n\nClouds initially form in clear air or become clouds when fog rises above surface level. The genus of a newly formed cloud is determined mainly by air mass characteristics such as stability and moisture content. If these characteristics change over time, the genus tends to change accordingly. When this happens, the original genus is called a \"mother cloud\". If the mother cloud retains much of its original form after the appearance of the new genus, it is termed a \"genitus\" cloud. One example of this is \"stratocumulus cumulogenitus\", a stratocumulus cloud formed by the partial spreading of a cumulus type when there is a loss of convective lift. If the mother cloud undergoes a complete change in genus, it is considered to be a \"mutatus\" cloud.\n\nThe genitus and mutatus categories have been expanded to include certain types that do not originate from pre-existing clouds. The term \"flammagenitus\" (Latin for 'fire-made') applies to cumulus congestus or cumulonimbus that are formed by large scale fires or volcanic eruptions. Smaller low-level \"pyrocumulus\" or \"fumulus\" clouds formed by contained industrial activity are now classified as cumulus \"homogenitus\" (Latin for 'man-made'). Contrails formed from the exhaust of aircraft flying in the upper level of the troposphere can persist and spread into formations resembling any of the high cloud genus-types and are now officially designated as cirrus, cirrostratus, or cirrocumulus homogenitus. If a homogenitus cloud of one genus changes to another genus type, it is then termed a \"homomutatus\" cloud. Stratus cataractagenitus (Latin for 'cataract-made') are generated by the spray from waterfalls. \"Silvagenitus\" (Latin for 'forest-made') is a stratus cloud that forms as water vapor is added to the air above a forest canopy.\n\nStratocumulus clouds can be organized into \"fields\" that take on certain specially classified shapes and characteristics. In general, these fields are more discernible from high altitudes than from ground level. They can often be found in the following forms:\n\nThese patterns are formed from a phenomenon known as a Kármán vortex which is named after the engineer and fluid dynamicist Theodore von Kármán. Wind driven clouds can form into parallel rows that follow the wind direction. When the wind and clouds encounter high elevation land features such as a vertically prominent islands, they can form eddies around the high land masses that give the clouds a twisted appearance.\n\nAlthough the local distribution of clouds can be significantly influenced by topography, the global prevalence of cloud cover in the troposphere tends to vary more by latitude. It is most prevalent in and along low pressure zones of surface tropospheric convergence which encircle the Earth close to the equator and near the 50th parallels of latitude in the northern and southern hemispheres. The adiabatic cooling processes that lead to the creation of clouds by way of lifting agents are all associated with convergence; a process that involves the horizontal inflow and accumulation of air at a given location, as well as the rate at which this happens.<ref name=\"Convergence/divergence\"></ref> Near the equator, increased cloudiness is due to the presence of the low-pressure Intertropical Convergence Zone (ITCZ) where very warm and unstable air promotes mostly cumuliform and cumulonimbiform clouds. Clouds of virtually any type can form along the mid-latitude convergence zones depending on the stability and moisture content of the air. These extratropical convergence zones are occupied by the polar fronts where air masses of polar origin meet and clash with those of tropical or subtropical origin. This leads to the formation of weather-making extratropical cyclones composed of cloud systems that may be stable or unstable to varying degrees according to the stability characteristics of the various airmasses that are in conflict.\n\nDivergence is the opposite of convergence. In the Earth's troposphere, it involves the horizontal outflow of air from the upper part of a rising column of air, or from the lower part of a subsiding column often associated with an area or ridge of high pressure. Cloudiness tends to be least prevalent near the poles and in the subtropics close to the 30th parallels, north and south. The latter are sometimes referred to as the horse latitudes. The presence of a large-scale high-pressure subtropical ridge on each side of the equator reduces cloudiness at these low latitudes. Similar patterns also occur at higher latitudes in both hemispheres.\n\nThe luminance or brightness of a cloud is determined by how light is reflected, scattered, and transmitted by the cloud's particles. Its brightness may also be affected by the presence of haze or photometeors such as halos and rainbows. In the troposphere, dense, deep clouds exhibit a high reflectance (70% to 95%) throughout the visible spectrum. Tiny particles of water are densely packed and sunlight cannot penetrate far into the cloud before it is reflected out, giving a cloud its characteristic white color, especially when viewed from the top. Cloud droplets tend to scatter light efficiently, so that the intensity of the solar radiation decreases with depth into the gases. As a result, the cloud base can vary from a very light to very-dark-grey depending on the cloud's thickness and how much light is being reflected or transmitted back to the observer. High thin tropospheric clouds reflect less light because of the comparatively low concentration of constituent ice crystals or supercooled water droplets which results in a slightly off-white appearance. However, a thick dense ice-crystal cloud appears brilliant white with pronounced grey shading because of its greater reflectivity.\n\nAs a tropospheric cloud matures, the dense water droplets may combine to produce larger droplets. If the droplets become too large and heavy to be kept aloft by the air circulation, they will fall from the cloud as rain. By this process of accumulation, the space between droplets becomes increasingly larger, permitting light to penetrate farther into the cloud. If the cloud is sufficiently large and the droplets within are spaced far enough apart, a percentage of the light that enters the cloud is not reflected back out but is absorbed giving the cloud a darker look. A simple example of this is one's being able to see farther in heavy rain than in heavy fog. This process of reflection/absorption is what causes the range of cloud color from white to black.\n\nStriking cloud colorations can be seen at any altitude, with the color of a cloud usually being the same as the incident light. During daytime when the sun is relatively high in the sky, tropospheric clouds generally appear bright white on top with varying shades of grey underneath. Thin clouds may look white or appear to have acquired the color of their environment or background. Red, orange, and pink clouds occur almost entirely at sunrise/sunset and are the result of the scattering of sunlight by the atmosphere. When the sun is just below the horizon, low-level clouds are gray, middle clouds appear rose-colored, and high clouds are white or off-white. Clouds at night are black or dark grey in a moonless sky, or whitish when illuminated by the moon. They may also reflect the colors of large fires, city lights, or auroras that might be present.\n\nA cumulonimbus cloud that appears to have a greenish or bluish tint is a sign that it contains extremely high amounts of water; hail or rain which scatter light in a way that gives the cloud a blue color. A green colorization occurs mostly late in the day when the sun is comparatively low in the sky and the incident sunlight has a reddish tinge that appears green when illuminating a very tall bluish cloud. Supercell type storms are more likely to be characterized by this but any storm can appear this way. Coloration such as this does not directly indicate that it is a severe thunderstorm, it only confirms its potential. Since a green/blue tint signifies copious amounts of water, a strong updraft to support it, high winds from the storm raining out, and wet hail; all elements that improve the chance for it to become severe, can all be inferred from this. In addition, the stronger the updraft is, the more likely the storm is to undergo tornadogenesis and to produce large hail and high winds.\n\nYellowish clouds may be seen in the troposphere in the late spring through early fall months during forest fire season. The yellow color is due to the presence of pollutants in the smoke. Yellowish clouds are caused by the presence of nitrogen dioxide and are sometimes seen in urban areas with high air pollution levels.\n\nClouds exert numerous influences on Earth's troposphere and climate. First and foremost, they are the source of precipitation, thereby greatly influencing the distribution and amount of precipitation. Because of their differential buoyancy relative to surrounding cloud-free air, clouds can be associated with vertical motions of the air that may be convective, frontal, or cyclonic. The motion is upward if the clouds are less dense because condensation of water vapor releases heat, warming the air and thereby decreasing its density. This can lead to downward motion because lifting of the air results in cooling that increases its density. All of these effects are subtly dependent on the vertical temperature and moisture structure of the atmosphere and result in major redistribution of heat that affect the Earth's climate.\n\nThe complexity and diversity of clouds is a major reason for difficulty in quantifying the effects of clouds on climate and climate change. On the one hand, white cloud tops promote cooling of Earth's surface by reflecting shortwave radiation (visible and near infrared) from the sun, diminishing the amount of solar radiation that is absorbed at the surface, enhancing the Earth's albedo. Most of the sunlight that reaches the ground is absorbed, warming the surface, which emits radiation upward at longer, infrared, wavelengths. At these wavelengths, however, water in the clouds acts as an efficient absorber. The water reacts by radiating, also in the infrared, both upward and downward, and the downward longwave radiation results in increased warming at the surface. This is analogous to the greenhouse effect of greenhouse gases and water vapor.\n\nHigh-level genus-types particularly show this duality with both short-wave albedo cooling and long-wave greenhouse warming effects. On the whole, \"ice-crystal\" clouds in the upper troposphere (cirrus) tend to favor net warming. However, the cooling effect is dominant with mid-level and low clouds, especially when they form in extensive sheets. Measurements by NASA indicate that on the whole, the effects of low and mid-level clouds that tend to promote cooling outweigh the warming effects of high layers and the variable outcomes associated with vertically developed clouds.\n\nAs difficult as it is to evaluate the influences of current clouds on current climate, it is even more problematic to predict changes in cloud patterns and properties in a future, warmer climate, and the resultant cloud influences on future climate. In a warmer climate more water would enter the atmosphere by evaporation at the surface; as clouds are formed from water vapor, cloudiness would be expected to increase. But in a warmer climate, higher temperatures would tend to evaporate clouds. Both of these statements are considered accurate, and both phenomena, known as cloud feedbacks, are found in climate model calculations. Broadly speaking, if clouds, especially low clouds, increase in a warmer climate, the resultant cooling effect leads to a negative feedback in climate response to increased greenhouse gases. But if low clouds decrease, or if high clouds increase, the feedback is positive. Differing amounts of these feedbacks are the principal reason for differences in climate sensitivities of current global climate models. As a consequence, much research has focused on the response of low and vertical clouds to a changing climate. Leading global models produce quite different results, however, with some showing increasing low clouds and others showing decreases. For these reasons the role of tropospheric clouds in regulating weather and climate remains a leading source of uncertainty in global warming projections.\n\nPolar stratospheric clouds show little variation in structure and are limited to a single very high range of altitude of about , so they are not classified into altitude levels, genus types, species, or varieties in the manner of tropospheric clouds.\n\nPolar stratospheric clouds form in the lowest part of the stratosphere during the winter, at the altitude and during the season that produces the coldest temperatures and therefore the best chances of triggering condensation caused by adiabatic cooling. They are typically very thin with an undulating cirriform appearance. Moisture is scarce in the stratosphere, so nacreous and non-nacreous cloud at this altitude range is restricted to polar regions in the winter where the air is coldest.\n\nPolar mesospheric clouds form at a single extreme altitude range of about and are consequently not classified into more than one level. They are given the Latin name noctilucent because of their illumination well after sunset and before sunrise. They typically have a bluish or silvery white coloration that can resemble brightly illuminated cirrus. Noctilucent clouds may occasionally take on more of a red or orange hue. They are not common or widespread enough to have a significant effect on climate. However, an increasing frequency of occurrence of noctilucent clouds since the 19th century may be the result of climate change.\n\nNoctilucent clouds are the highest in the atmosphere and form near the top of the mesosphere at about ten times the altitude of tropospheric high clouds. From ground level, they can occasionally be seen illuminated by the sun during deep twilight. Ongoing research indicates that convective lift in the mesosphere is strong enough during the polar summer to cause adiabatic cooling of small amount of water vapour to the point of saturation. This tends to produce the coldest temperatures in the entire atmosphere just below the mesopause. These conditions result in the best environment for the formation of polar mesospheric clouds. There is also evidence that smoke particles from burnt-up meteors provide much of the condensation nuclei required for the formation of noctilucent cloud.\n\nDistribution in the mesosphere is similar to the stratosphere except at much higher altitudes. Because of the need for maximum cooling of the water vapor to produce noctilucent clouds, their distribution tends to be restricted to polar regions of Earth. A major seasonal difference is that convective lift from below the mesosphere pushes very scarce water vapor to higher colder altitudes required for cloud formation during the respective summer seasons in the northern and southern hemispheres. Sightings are rare more than 45 degrees south of the north pole or north of the south pole.\n\nCloud cover has been seen on most other planets in the solar system. Venus's thick clouds are composed of sulfur dioxide (due to volcanic activity) and appear to be almost entirely stratiform. They are arranged in three main layers at altitudes of 45 to 65 km that obscure the planet's surface and can produce virga. No embedded cumuliform types have been identified, but broken stratocumuliform wave formations are sometimes seen in the top layer that reveal more continuous layer clouds underneath. On Mars, noctilucent, cirrus, cirrocumulus and stratocumulus composed of water-ice have been detected mostly near the poles. Water-ice fogs have also been detected on Mars.\n\nBoth Jupiter and Saturn have an outer cirriform cloud deck composed of ammonia, an intermediate stratiform haze-cloud layer made of ammonium hydrosulfide, and an inner deck of cumulus water clouds. Embedded cumulonimbus are known to exist near the Great Red Spot on Jupiter. The same category-types can be found covering Uranus, and Neptune, but are all composed of methane. Saturn's moon Titan has cirrus clouds believed to be composed largely of methane. The Cassini–Huygens Saturn mission uncovered evidence of polar stratospheric clouds and a methane cycle on Titan, including lakes near the poles and fluvial channels on the surface of the moon.\n\nSome planets outside the solar system are known to have atmospheric clouds. In October 2013, the detection of high altitude optically thick clouds in the atmosphere of exoplanet Kepler-7b was announced, and, in December 2013, in the atmospheres of GJ 436 b and GJ 1214 b.\n\nClouds play an important role in various cultures and religious traditions. The ancient Akkadians believed that the clouds were the breasts of the sky goddess Antu and that rain was milk from her breasts. In , Yahweh is described as guiding the Israelites through the desert in the form of a \"pillar of cloud\" by day and a \"pillar of fire\" by night. In the ancient Greek comedy \"The Clouds\", written by Aristophanes and first performed at the City Dionysia in 423 BC, the philosopher Socrates declares that the Clouds are the only true deities and tells the main character Strepsiades not to worship any deities other than the Clouds, but to pay homage to them alone. In the play, the Clouds change shape to reveal the true nature of whoever is looking at them, turning into centaurs at the sight of a long-haired politician, wolves at the sight of the embezzler Simon, deer at the sight of the coward Cleonymus, and mortal women at the sight of the sight of the effeminate informer Cleisthenes. They are hailed the source of inspiration to comic poets and philosophers; they are masters of rhetoric, regarding eloquence and sophistry alike as their \"friends\". In China, clouds are symbols of luck and happiness. Overlapping clouds are thought to imply eternal happiness and clouds of different colors are said to indicate \"multiplied blessings\".\n\n"}
{"id": "45503", "url": "https://en.wikipedia.org/wiki?curid=45503", "title": "Coelacanth", "text": "Coelacanth\n\nThe coelacanths ( ) constitute a now-rare order of fish that includes two extant species in the genus \"Latimeria\": the West Indian Ocean coelacanth (\"Latimeria chalumnae\") primarily found near the Comoro Islands off the east coast of Africa and the Indonesian coelacanth (\"Latimeria menadoensis\"). They follow the oldest-known living lineage of Sarcopterygii (lobe-finned fish and tetrapods), which means they are more closely related to lungfish and tetrapods than to ray-finned fishes. They are found along the coastlines of the Indian Ocean and Indonesia. Since there are only two species of coelacanth and both are threatened, it is the most endangered order of animals in the world. The West Indian Ocean coelacanth is a critically endangered species.\n\nCoelacanths belong to the subclass Actinistia, a group of lobed-finned fish related to lungfish and certain extinct Devonian fish such as osteolepiforms, porolepiforms, rhizodonts, and \"Panderichthys\". Coelacanths were thought to have become extinct in the Late Cretaceous, around 66 million years ago, but were rediscovered in 1938 off the coast of South Africa.\n\nThe coelacanth was long considered a \"living fossil\" because scientists thought it was the sole remaining member of a taxon otherwise known only from fossils, with no close relations alive, and that it evolved into roughly its current form approximately 400 million years ago. However, several recent studies have shown that coelacanth body shapes are much more diverse than previously thought.\n\nThe word \"Coelacanth\" is an adaptation of the Modern Latin \"Cœlacanthus\" (\"hollow spine\"), from the Greek κοῖλ-ος (\"koilos \"\"hollow\" + ἄκανθ-α \"akantha\" \"spine\"). It is a common name for the oldest living line of Sarcopterygii, referring to the hollow caudal fin rays of the first fossil specimen described and named by Louis Agassiz in 1839. The genus name \"Latimeria\" commemorates Marjorie Courtenay-Latimer who discovered the first specimen in a fish market.\n\nThe coelacanth, which is related to lungfishes and tetrapods, was believed to have been extinct since the end of the Cretaceous period. More closely related to tetrapods than to the ray-finned fish, coelacanths were considered transitional species between fish and tetrapods. On 23 December 1938, the first \"Latimeria\" specimen was found off the east coast of South Africa, off the Chalumna River (now Tyolomnqa). Museum curator Marjorie Courtenay-Latimer discovered the fish among the catch of a local angler, Captain Hendrick Goosen. A Rhodes University ichthyologist, J. L. B. Smith, confirmed the fish's importance with a famous cable: \"MOST IMPORTANT PRESERVE SKELETON AND GILLS = FISH DESCRIBED.\"\n\nIts discovery 66 million years after it was believed to have gone extinct makes the coelacanth the best-known example of a Lazarus taxon, an evolutionary line that seems to have disappeared from the fossil record only to reappear much later. Since 1938, West Indian Ocean coelacanth have been found in the Comoros, Kenya, Tanzania, Mozambique, Madagascar, and in iSimangaliso Wetland Park, Kwazulu-Natal in South Africa.\n\nThe Comoro Islands specimen was discovered in December 1952. Between 1938 and 1975, 84 specimens were caught and recorded.\n\nThe second extant species, Indonesian coelacanth, was described from Manado, North Sulawesi, Indonesia in 1999 by Pouyaud et al. based on a specimen discovered by Mark V. Erdmann in 1998 and deposited at the Indonesian Institute of Sciences (LIPI). Erdmann and his wife Arnaz Mehta first encountered a specimen at a local market in September 1997, but took only a few photographs of the first specimen of this species before it was sold. After confirming that it was a unique discovery, Erdmann returned to Sulawesi in November, 1997 interviewing fishermen to look for further examples. A second specimen was caught by a fisherman in July 1998 and it was then handed to Erdmann.\n\nThe coelacanth has no real commercial value apart from being coveted by museums and private collectors. As a food fish it is almost worthless, as its tissues exude oils that give the flesh a foul flavor. The coelacanth's continued survival may be threatened by commercial deep-sea trawling, in which coelacanths are caught as bycatch.\n\nCoelacanths are a part of the clade Sarcopterygii, or the lobe-finned fishes. Externally, several characteristics distinguish the coelacanth from other lobe-finned fish. They possess a three-lobed caudal fin, also called a trilobate fin or a diphycercal tail. A secondary tail extending past the primary tail separates the upper and lower halves of the coelacanth. Cosmoid scales act as thick armor to protect the coelacanth's exterior. Several internal traits also aid in differentiating coelacanths from other lobe-finned fish. At the back of the skull, the coelacanth possesses a hinge, the intracranial joint, which allows it to open its mouth extremely wide. Coelacanths also retain an oil-filled notochord, a hollow, pressurized tube which is replaced by the vertebral column early in embryonic development in most other vertebrates. The coelacanth heart is shaped differently from that of most modern fish, with its chambers arranged in a straight tube. The coelacanth braincase is 98.5% filled with fat; only 1.5% of the braincase contains brain tissue. The cheeks of the coelacanth are unique because the opercular bone is very small and holds a large soft-tissue opercular flap. A spiracular chamber is present, but the spiracle is closed and never opens during development. Coelacanth also possess a unique rostral organ within the ethmoid region of the braincase. Also unique to extant coelacanths is the presence of a \"fatty lung\" or a fat-filled single-lobed vestigial lung, homologous to other fishes' swim bladder. The parallel development of a fatty organ for buoyancy control suggest a unique specialization for deep-water habitats. There has also been discovered small, hard but flexible plates around the vestigial lung in adult specimen, though not around the fatty organ. The plates most likely had a regulation function for the volume of the lung. Due to the size of the fatty organ, researchers assume it's responsible for the kidney's unusual relocation. The two kidneys, which are fused into one, are located ventrally within the abdominal cavity, posterior to the cloaca.\n\n\"Latimeria chalumnae\" and \"L. menadoensis\" are the only two known living coelacanth species. The word \"coelacanth\" is derived from the Greek for “hollow spine”, because of the fish's unique hollow spine fins. Coelacanths are large, plump, lobe-finned fish that can grow to more than 2 meters (6 feet 6 inches) and weigh around 90 kilograms (200 pounds). They are estimated to live for 60 years or more. Modern coelacanths appear larger than those found as fossils.\n\nThey are nocturnal piscivorous drift-hunters. The body is covered in cosmoid scales that act as armor. Coelacanths have eight fins – 2 dorsal fins, 2 pectoral fins, 2 pelvic fins, 1 anal fin and 1 caudal fin. The tail is very nearly equally proportioned and is split by a terminal tuft of fin rays that make up its caudal lobe. The eyes of the coelacanth are very large, while the mouth is very small. The eye is acclimatized to seeing in poor light by rods that absorb mostly short wavelengths. Coelacanth vision has evolved to a mainly blue-shifted color capacity. Pseudomaxillary folds surround the mouth and replace the maxilla, a structure absent in coelacanths. Two nostrils, along with four other external openings, appear between the premaxilla and lateral rostral bones. The nasal sacs resemble those of many other fish and do not contain an internal nostril. The coelacanth's rostral organ, contained within the ethmoid region of the braincase, has three unguarded openings into the environment and is used as a part of the coelacanth's laterosensory system. The coelacanth's auditory reception is mediated by its inner ear, which is very similar to that of tetrapods because it is classified as being a basilar papilla.\n\nCoelacanth locomotion is unique. To move around they most commonly take advantage of up- or down-wellings of current and drift. Their paired fins stabilize movement through the water. While on the ocean floor, they do not use the paired fins for any kind of movement. Coelacanths create thrust with their caudal fins for quick starts. Due to the abundance of its fins, the coelacanth has high maneuverability and can orient its body in almost any direction in the water. They have been seen doing headstands as well as swimming belly up. It is thought that the rostral organ helps give the coelacanth electroperception, which aids in movement around obstacles.\n\nIn 2013, a group led by Chris Amemiya and Neil Shubin published the genome sequence of the coelacanth in the journal \"Nature\". The African coelacanth genome was sequenced and assembled using DNA from a Comoros Islands \"Latimeria chalumnae\" specimen. It was sequenced by Illumina sequencing technology and assembled using the short read genome assembler ALLPATHS-LG.\n\nDue to their lobed fins and other features, it was once hypothesized that the coelacanth might be the most recent shared ancestor between terrestrial and marine vertebrates. But after sequencing the full genome of the coelacanth, it was discovered that the lungfish is the most recent shared ancestor. Coelacanths and lungfish had already diverged from a common ancestor before the lungfish made the transition to land.\nAnother important discovery made from the genome sequencing is that the coelacanths are still evolving today (but at a relatively slow rate). This raises questions about using the term \"living fossil\" to describe these creatures. While they were initially thought to be a prehistoric species that remained unchanged over millions of years, the discovery that they are still evolving, albeit slowly, causes some to question whether \"living fossil\" is an appropriate descriptor.\n\nOne reason the coelacanths are evolving so slowly is the lack of evolutionary pressure on these organisms. They have few predators, and live deep in the ocean where conditions are very stable. Without much pressure for these organisms to adapt to survive, the rate at which they have evolved is much slower compared to other organisms.\n\nThe following is a classification of known coelacanth genera and families:\n\n\nAccording to genetic analysis of current species, the divergence of coelacanths, lungfish and tetrapods is thought to have occurred about 390 million years ago. Coelacanths were once thought to have undergone extinction 66 million years ago during the Cretaceous–Paleogene extinction event. The first recorded coelacanth fossil, found in Australia, was of a jaw that dated back 360 million years, named \"Eoachtinistia foreyi\". The most recent genus of coelacanth in the fossil record is \"Megalocoelacanthus\", whose disarticulated remains are found in Campanian to possibly earliest Maastrichtian-aged marine strata of the Eastern and Central United States. The fossil record is unique because coelacanth fossils were found 100 years before the first live specimen was identified. In 1938, Courtenay-Latimer rediscovered the first live specimen, \"L. chalumnae\", caught off the coast of East London, South Africa. In 1997, a marine biologist on honeymoon discovered the second live species, \"Latimeria menadoensis\", in an Indonesian market.\n\nIn July 1998, the first live specimen of \"Latimeria menadoensis\" was caught in Indonesia. Approximately 80 species of coelacanth have been described, including the two extant species. Before the discovery of a live specimen, the coelacanth time range was thought to have spanned from the Middle Devonian to the Upper Cretaceous period. Although fossils found during that time were claimed to demonstrate a similar morphology, recent studies have expressed the view that coelacanth morphologic conservatism is a belief not based on data.\n\nThe following cladogram is based on multiple sources.\n\nThe current coelacanth range is primarily along the eastern African coast, although \"Latimeria menadoensis\" was discovered off Indonesia. Coelacanths have been found in the waters of Kenya, Tanzania, Mozambique, South Africa, Madagascar, Comoros and Indonesia. Most \"Latimeria chalumnae\" specimens that have been caught have been captured around the islands of Grande Comore and Anjouan in the Comoros Archipelago (Indian Ocean). Though there are cases of \"L. chalumnae\" caught elsewhere, amino acid sequencing has shown no big difference between these exceptions and those found around Comore and Anjouan. Even though these few may be considered strays, there are several reports of coelacanths being caught off of the coast of Madagascar. This leads scientists to believe that the endemic range of \"Latimeria chalumnae\" coelacanths stretches along the eastern coast of Africa from the Comoros Islands, past the western coast of Madagascar to the South African coastline. Mitochondrial DNA sequencing of coelacanths caught off the coast of southern Tanzania suggests a divergence of the two populations some 200,000 years ago. This could refute the theory that the Comoros population is the main population while others represent recent offshoots.\n\nThe geographical range of the Indonesia coelacanth, \"Latimeria menadoensis\", is believed to be off the coast of Manado Tua Island, Sulawesi, Indonesia in the Celebes Sea. Key components confining coelacanths to these areas are food and temperature restrictions, as well as ecological requirements such as caves and crevices that are well-suited for drift feeding. Teams of researchers using submersibles have recorded live sightings of the fish in the Sulawesi Sea as well as in the waters of Biak in Papua.\n\nAnjouan Island and the Grande Comore provide ideal underwater cave habitats for coelacanths. The islands' underwater volcanic slopes, steeply eroded and covered in sand, house a system of caves and crevices which allow coelacanths resting places during the daylight hours. These islands support a large benthic fish population that help to sustain coelacanth populations.\n\nDuring the daytime, coelacanths rest in caves anywhere from 100 to 500 meters deep. Others migrate to deeper waters. The cooler waters (below 120 meters) reduce the coelacanths' metabolic costs. Drifting toward reefs and night feeding saves vital energy. Resting in caves during the day also saves energy otherwise used to fight currents.\nCoelacanths are nocturnal piscivores who feed mainly on benthic fish populations and various cephalopods. They are \"passive drift feeders\", slowly drifting currents with only minimal self-propulsion, eating whatever prey they encounter.\n\nCoelacanths are fairly peaceful when encountering others of their kind, remaining calm even in a crowded cave. They do avoid body contact, however, withdrawing immediately if contact occurs. When approached by foreign potential predators (e.g. a submersible), they show panic flight reactions, suggesting that coelacanths are most likely prey to large deepwater predators. Shark bite marks have been seen on coelacanths; sharks are common in areas inhabited by coelacanths. Electrophoresis testing of 14 coelacanth enzymes shows little genetic diversity between coelacanth populations. Among the fish that have been caught were about equal numbers of males and females. Population estimates range from 210 individuals per population all the way to 500 per population. Because coelacanths have individual color markings, scientists think that they recognize other coelacanths via electric communication.\n\nCoelacanths are ovoviviparous, meaning that the female retains the fertilized eggs within her body while the embryos develop during a gestation period of over a year. Typically, females are larger than the males; their scales and the skin folds around the cloaca differ. The male coelacanth has no distinct copulatory organs, just a cloaca, which has a urogenital papilla surrounded by erectile caruncles. It is hypothesized that the cloaca everts to serve as a copulatory organ.\n\nCoelacanth eggs are large with only a thin layer of membrane to protect them. Embryos hatch within the female and eventually are given live birth, which is a rarity in fish. This was only discovered when the American Museum of Natural History dissected its first coelacanth specimen in 1975 and found it pregnant with five embryos. Young coelacanths resemble the adult, the main differences being an external yolk sac, larger eyes relative to body size and a more pronounced downward slope of the body. The juvenile coelacanth's broad yolk sac hangs below the pelvic fins. The scales and fins of the juvenile are completely matured; however, it does lack odontodes, which it gains during maturation.\n\nA study that assessed the paternity of the embryos inside two Coelacanth females indicated that each clutch was sired by a single male. This could mean that females mate monandrously, i.e. with one male only. Polyandry, female mating with multiple males, is common in both plants and animals and can be advantageous (e.g. insurance against mating with an infertile or incompatible mate), but also confers costs (increased risk of infection, danger of falling prey to predators, increased energy input when searching for new males). Alternatively, the study's results could indicate that, despite female polyandry, one male is used to fertilise all the eggs, potentially through female sperm choice or last-male sperm precedence.\n\nBecause little is known about the coelacanth, the conservation status is difficult to characterize. According to Fricke \"et al.\" (1995), there should be some stress put on the importance of conserving this species. From 1988 to 1994, Fricke counted some 60 individuals of \"L. chalumnae\" on each dive. In 1995 that number dropped to 40. Even though this could be a result of natural population fluctuation, it also could be a result of overfishing. The IUCN currently classifies \"L. chalumnae\" as Critically Endangered, with a total population size of 500 or fewer individuals. \"L. menadoensis\" is considered Vulnerable, with a significantly larger population size (fewer than 10,000 individuals).\n\nCurrently, the major threat towards the coelacanth is the accidental capture by fishing operations. Coelacanths usually are caught when local fishermen are fishing for oilfish. Fishermen sometimes snag a coelacanth instead of an oilfish because they traditionally fish at night, when oilfish (and coelacanths) feed.\nBefore scientists became interested in coelacanths, they were thrown back into the water if caught. Now that there is an interest in them, fishermen trade them in to scientists or other officials once they have been caught. Before the 1980s, this was a problem for coelacanth populations. In the 1980s, international aid gave fiberglass boats to the local fishermen, which resulted in fishing beyond the coelacanth territories into more fish-productive waters. Since then, most of the motors on the boats have broken down so the local fishermen are now back in the coelacanth territory, putting the species at risk again.\n\nDifferent methods to minimize the number of coelacanths caught include moving fishers away from the shore, using different laxatives and malarial salves to reduce the quantity of oilfish needed, using coelacanth models to simulate live specimens, and increasing awareness of the need to protect the species. In 1987 the Coelacanth Conservation Council advocated the conservation of coelacanths. The CCC has many branches of its agency located in Comoros, South Africa, Canada, the United Kingdom, the U.S., Japan and Germany. The agencies were established to help protect and encourage population growth of coelacanths.\n\nA \"Deep Release Kit\" was developed in 2014 and distributed by private initiative, consisting of a weighted hook assembly that allows a fisherman to return an accidentally caught coelacanth to deep waters where the hook can be detached once it hits the sea floor. Conclusive reports about the effectiveness of this method are still pending.\n\nIn 2002, the South African Coelacanth Conservation and Genome Resource Programme was launched to help further the studies and conservation of the coelacanth. This program focuses on biodiversity conservation, evolutionary biology, capacity building, and public understanding. The South African government committed to spending R10 million on the program.\nIn 2011, a plan for a Tanga Coelacanth Marine Park was designed to conserve marine biodiversity for marine animals including the coelacanth. The park was designed to reduce habitat destruction and improve prey availability for endangered species.\n\nCoelacanths are considered a poor source of food for humans and likely most other fish-eating animals. Coelacanth flesh has high amounts of oil, urea, wax esters, and other compounds that are difficult to digest and can cause diarrhea. Their scales themselves emit mucus, which combined with the excessive oil their bodies produce, make coelacanths a slimy food. Where the coelacanth is more common, local fishermen avoid it because of its potential to sicken consumers.\n\n\n"}
{"id": "6520719", "url": "https://en.wikipedia.org/wiki?curid=6520719", "title": "Colton Point State Park", "text": "Colton Point State Park\n\nColton Point State Park is a Pennsylvania state park in Tioga County, Pennsylvania, in the United States. It is on the west side of the Pine Creek Gorge, also known as the Grand Canyon of Pennsylvania, which is deep and nearly across at this location. The park extends from the creek in the bottom of the gorge up to the rim and across part of the plateau to the west. Colton Point State Park is known for its views of the Pine Creek Gorge, and offers opportunities for picnicking, hiking, fishing and hunting, whitewater boating, and camping. Colton Point is surrounded by Tioga State Forest and its sister park, Leonard Harrison State Park, on the east rim. The park is on a state forest road in Shippen Township south of U.S. Route 6.\n\nPine Creek flows through the park and has carved the gorge through five major rock formations from the Devonian and Carboniferous periods. Native Americans once used the Pine Creek Path along the creek. The path was later used by lumbermen, and then became the course of a railroad from 1883 to 1988. Since 1996, the Pine Creek Rail Trail has followed the creek through the gorge. The Pine Creek Gorge was named a National Natural Landmark in 1968 and is also protected as a Pennsylvania State Natural Area and Important Bird Area, while Pine Creek is a Pennsylvania Scenic and Wild River. The gorge is home to many species of plants and animals, some of which have been reintroduced to the area.\n\nThe park is named for Henry Colton, a Williamsport lumberman who cut timber there starting in 1879. Although the Pine Creek Gorge was clearcut in the 19th and early 20th centuries, it is now covered by second-growth forest, thanks in part to the conservation efforts of the Civilian Conservation Corps (CCC) in the 1930s. The CCC built the facilities at Colton Point before and shortly after the park's 1936 opening. Most of the CCC-built facilities remain in use, and have led to the park's listing as a historic district on the National Register of Historic Places. Since a successful publicity campaign in 1936, the park and gorge have been a popular tourist destination, attracting hundreds of thousands of visitors each year. Colton Point State Park was chosen by the Pennsylvania Department of Conservation and Natural Resources (DCNR) Bureau of Parks for its \"25 Must-See Pennsylvania State Parks\" list, which praised its \"spectacular vistas and a fabulous view of Pine Creek Gorge\".\n\nHumans have lived in what is now Pennsylvania since at least 10,000 BC. The first settlers were Paleo-Indian nomadic hunters known from their stone tools. The hunter-gatherers of the Archaic period, which lasted locally from 7000 to 1000 BC, used a greater variety of more sophisticated stone artifacts. The Woodland period marked the gradual transition to semi-permanent villages and horticulture, between 1000 BC and 1500 AD. Archeological evidence found in the state from this time includes a range of pottery types and styles, burial mounds, pipes, bows and arrows, and ornaments.\n\nColton Point State Park is in the West Branch Susquehanna River drainage basin, the earliest recorded inhabitants of which were the Iroquoian-speaking Susquehannocks. They were a matriarchal society that lived in stockaded villages of large long houses, and \"occasionally inhabited\" the mountains surrounding the Pine Creek Gorge. Their numbers were greatly reduced by disease and warfare with the Five Nations of the Iroquois, and by 1675 they had died out, moved away, or been assimilated into other tribes.\n\nAfter this, the lands of the West Branch Susquehanna River valley were under the nominal control of the Iroquois. The Iroquois lived in long houses, primarily in what is now New York, and had a strong confederacy which gave them power beyond their numbers. They and other tribes used the Pine Creek Path through the gorge, traveling between a path on the Genesee River in modern New York in the north, and the Great Shamokin Path along the West Branch Susquehanna River in the south. The Seneca tribe of the Iroquois believed that Pine Creek Gorge was sacred land and never established a permanent settlement there. They used the path through the gorge and had seasonal hunting camps along it, including one just north of the park near what would later be the village of Ansonia. To fill the void left by the demise of the Susquehannocks, the Iroquois encouraged displaced tribes from the east to settle in the West Branch watershed, including the Shawnee and Lenape (or Delaware).\n\nThe French and Indian War (1754–63) led to the migration of many Native Americans westward to the Ohio River basin. On November 5, 1768, the British acquired the New Purchase from the Iroquois in the Treaty of Fort Stanwix, including what is now the Pine Creek Gorge east of the creek. The Purchase line established by this treaty was disputed, as it was unclear whether the border along \"Tiadaghton Creek\" referred to Pine Creek or to Lycoming Creek, further to the east. As a result, the land between them was disputed territory until 1784 and the Second Treaty of Fort Stanwix. After the American Revolutionary War, Native Americans almost entirely left Pennsylvania; some isolated bands of natives remained in Pine Creek Gorge until the War of 1812.\n\nPrior to the arrival of William Penn and his Quaker colonists in 1682, up to 90 percent of what is now Pennsylvania was covered with woods: more than of eastern white pine, eastern hemlock, and a mix of hardwoods. The forests near the three original counties, Philadelphia, Bucks, and Chester, were the first to be harvested, as the early settlers used the readily available timber and cleared land for agriculture. By the time of the American Revolution, logging had reached the interior and mountainous regions, and became a leading industry in Pennsylvania. Trees furnished fuel to heat homes, tannin for the state's many tanneries, and wood for construction, furniture, and barrel making. Large areas of forest were harvested by colliers to fire iron furnaces. Rifle stocks and shingles were made from Pennsylvania timber, as were a wide variety of household utensils, and the first Conestoga wagons.\nBy the early 19th century the demand for lumber reached the Pine Creek Gorge, where the surrounding mountainsides were covered with eastern white pine in diameter and or more tall, eastern hemlock in circumference, and huge hardwoods. Each of these virgin forests produced of white pine and of hemlock and hardwoods. For comparison, the same area of forest today produces a total of only on average. According to Steven E. Owlett, environmental lawyer and author, shipbuilders considered pine from Pine Creek the \"best timber in the world for making fine ship masts\", so it was the first lumber to be harvested on a large scale. The original title to the land that became Colton Point State Park was sold to the Wilhelm Wilkins Company in 1792. Pine Creek was declared a public highway by the Pennsylvania General Assembly on March 16, 1798, and rafts of spars were floated down the creek to the Susquehanna River, then to the Chesapeake Bay and the shipbuilders at Baltimore. The lumbermen would then walk home, following the old Pine Creek Path at the end of their journey.\n\nAs the 19th century progressed, fewer pines were left and more hemlocks and hardwoods were cut and processed locally. By 1810 there were 11 sawmills in the Pine Creek watershed, and by 1840 there were 145, despite a flood in 1832 which wiped out nearly all the mills along the creek. Selective harvesting of pines was replaced by clearcutting of all lumber in a tract. The first lumbering activity close to what is now Colton Point was in 1838 when William Dodge and partners built a settlement at Big Meadows and formed the Pennsylvania Joint Land and Lumber Company. Dodge's company purchased thousands of acres of land in the area, including what is now Colton Point State Park. In 1865 the last pine spar raft floated down the creek, and on March 28, 1871 the General Assembly passed a law which allowed construction of splash dams and allowed creeks to be cleared to allow loose logs to float better. The earliest spring log drives floated up to of logs in Pine Creek at one time. These logs floated to the West Branch Susquehanna River and to sawmills near the Susquehanna Boom at Williamsport. Log drives could be dangerous: just north of the park is Barbour Rock, named for Samuel Barbour, who lost his life on Pine Creek there after breaking up a log jam. Hemlock wood was not widely used until the advent of wire nails, but the bark was used to tan leather. After 1870 the largest tanneries in the world were in the Pine Creek watershed, and required of bark to produce of quality sole leather.\nIn 1879 Henry Colton, who worked for the Williamsport Lumber Company, supervised the cutting of white pine on the land owned by Silas Billings; this land would later become the park. Colton gave his name to the Colton Point overlook on the west rim of the Pine Creek Gorge. Deadman Hollow Road in the park is named for a trapper whose decomposed body was found in his own bear trap there in the early 20th century. Fourmile Run flows through the park: its O'Connor Branch is named for the dead trapper's brothers, who were loggers in the area.\n\nIn 1883 the Jersey Shore, Pine Creek and Buffalo Railway opened, following the creek through the gorge. The new railroad used the relatively level route along Pine Creek to link the New York Central Railroad (NYC) to the north with the Clearfield Coalfield to the southwest, and with NYC-allied lines in Williamsport to the southeast. By 1896 the rail line's daily traffic included three passenger trains and of freight. In the surrounding forests, log drives gave way to logging railroads, which transported lumber to local sawmills. There were 13 companies operating logging railroads along Pine Creek and its tributaries between 1886 and 1921, while the last log drive in the Pine Creek watershed started on Little Pine Creek in 1905. By 1900 the Leetonia logging railroad was extended to the headwaters of Fourmile Run, which has several high waterfalls that prevented logs from being floated down it. In 1903 the line reached Colton Point and Bear Run, which is the northern border of the park today. Lumber on Fourmile Run that had been previously inaccessible was harvested and transported by train, initially to Leonard Harrison's mill at Tiadaghton. When that mill burned in 1905, the lumber went to the Leetonia mill on Cedar Run in Elk Township.\n\nThe old-growth forests were clearcut by the early 20th century and the gorge was stripped bare. Nothing was left except the dried-out tree tops, which became a fire hazard. As a result, much of the land burned and was left barren. On May 6, 1903, the Wellsboro newspaper had the headline \"Wild Lands Aflame\" and reported landslides through the gorge. The soil was depleted of nutrients, fires baked the ground hard, and jungles of blueberries, blackberries, and mountain laurel covered the clearcut land, which became known as the \"Pennsylvania Desert\". Floods swept the area periodically and much of the wildlife was wiped out.\n\nGeorge Washington Sears, an early conservationist who wrote under the pen name \"Nessmuk\", was one of the first to criticize the Pennsylvania lumber industry and its destruction of forests and creeks. In his 1884 book \"Woodcraft\" he wrote of the Pine Creek watershed where A huge tannery ... poisons and blackens the stream with chemicals, bark and ooze. ... The once fine covers and thickets are converted into fields thickly dotted with blackened stumps. And, to crown the desolation, heavy laden trains of 'The Pine Creek and Jersey Shore R.R.' go thundering [by] almost hourly ... Of course, this is progress; but, whether backward or forward, had better be decided sixty years hence. Nessmuk's words went mostly unheeded in his lifetime and did not prevent the clearcutting of almost all the virgin forests in Pennsylvania.\n\nSears lived in Wellsboro from 1844 until his death in 1890, and was the first to describe the Pine Creek Gorge. He also described a trip to what became Leonard Harrison State Park and the view west across the gorge to what became Colton Point State Park: after a buggy ride, he had to hike through tangles of fallen trees and branches, down ravines, and over banks for five hours. At last he reached \"The Point\", which he wrote was \"the jutting terminus of a high ridge which not only commands a capital view of the opposite mountain, but also of the Pine Creek Valley, up and down for miles\".\n\nThe land on which Colton Point State Park sits was sold to the Commonwealth in the late 19th century for by the Pennsylvania Joint Land and Lumber Company, which had no further use for it. Elsewhere in the gorge the state bought land abandoned by lumber companies, sometimes for less than . These purchases became the Tioga State Forest, which was officially established in 1925. As of 2015 the state forest encompasses , mostly in Tioga County, and surrounds Colton Point State Park to the north, west, and south. Leonard Harrison State Park is on the eastern border of Colton Point. In 1922, Wellsboro lumber baron Leonard Harrison donated his picnic grounds on the eastern rim of the gorge to the Commonwealth of Pennsylvania, which named it \"Leonard Harrison State Forest Park\".\n\nHarrison also built two cabins, named \"Wetumka\" and \"Osocosy\", on the west side of Pine Creek, just north of the mouth of Fourmile Run. Sometime after 1903, former Pennsylvania Governor William A. Stone built a cabin named \"Heart's-ease\" just south of the mouth of Fourmile Run. In 1966 these cabins were still standing and were three of \"only four man-made structures inside the canyon proper\", but by 1993 only Stone's cabin and one of Harrison's cabins remained. As of 2004, these properties were still owned by the Stone family, and are part of a small parcel of private land within the park.\n\nThe Civilian Conservation Corps (CCC) started work on the park in June 1935, and it opened as \"Colton Point State Forest Park\" in 1936. The CCC, founded by United States President Franklin D. Roosevelt during the Great Depression, created jobs for unemployed young men from throughout the United States. Much of the work of the CCC at Colton Point is still visible as of 2015, and is one of many examples of the work of the CCC throughout northcentral Pennsylvania.\n\nIn 1936, the year the park opened, Larry Woodin of Wellsboro and other Tioga County business owners began a tourism campaign to promote the Pine Creek Gorge as \"The Grand Canyon of Pennsylvania\". Greyhound Bus Lines featured a view of the canyon from a Leonard Harrison lookout on the back cover of its Atlantic Coast timetable. The bus line's Chicago to New York City tour had an overnight stay in Wellsboro and a morning visit to the canyon for $3. More than 300,000 tourists visited the canyon by the autumn of 1936, and 15,000 visited Leonard Harrison over Memorial Day weekend in 1937. That year more visitors came to the Pine Creek Gorge than to Yellowstone National Park. In response to the heavy use of the local roads, the CCC widened the highways in the area, and guides from the CCC gave tours of the canyon.\n\nColton Point originally opened with only \"limited facilities\", but the success of the tourism campaign led to the park's expansion by the CCC. New facilities were added in 1938, and included buildings such as picnic pavilions, latrines, and a concession stand, as well as \"stone cook stoves, tables, and developed trails and overlooks ... an amazing amount of work in one year\". The CCC also built the road to the park and planted stands of larch, spruce and white pine for reforestation. On February 12, 1987, the entire park was listed in the National Register of Historic Places (NRHP), including \"eight buildings and nine structures\".\n\nThe park has five CCC-built picnic shelters: pavilions 1, 3, and 4 are made of stone and timber with stone fireplaces, while pavilions 2 and 5 each has log columns that support a pyramidal roof. The CCC also built six rustic latrines with clapboard siding and gable roofs, and an underground reservoir that is covered with a low hipped roof. Additional structures constructed by the CCC include three overlooks and a rectangular gable-roofed maintenance building with wane edge siding and exposed rafters made of logs. The structures built by the CCC are noteworthy in that they exemplify the rustic style of construction that was prevalent at national and state parks built during the Great Depression. Workers used locally found, natural materials in construction that blended with the natural surroundings. Not all of the CCC's work has survived. A concession stand was built by the CCC and sold food and souvenirs from the late 1930s to at least 1953, but was not listed on the 1986 NRHP nomination form. The CCC also built a brick and stone incinerator, but it is in ruins now.\nThe Pennsylvania Geographic Board dropped the word \"Forest\" and officially named it \"Colton Point State Park\" on November 11, 1954. The first major change in the park was in 1970, when a camping area was established. That same decade saw the completion of a new water system in 1973, and a holding tank dump station was added to the camping area in 1977. A park office was built in 1983, but as of 2009 the park headquarters are in the adjoining Leonard Harrison State Park and the Colton Point office does not appear on the official park map. Pine Creek was named a state scenic river on December 4, 1992, which ensured further protection of Pine Creek Gorge in its natural state. In 1997 the park's Important Bird Area (IBA) was one of the first 73 IBAs established in Pennsylvania. In 2000 the park became part of the Hills Creek State Park complex, an administrative grouping of eight state parks in Potter and Tioga counties. As of 2004, the park does not have telephone or electrical lines, although it uses solar cells for limited electricity needs.\n\nThe second half of the 20th century also saw significant changes to the rail line through the Pine Creek Gorge. Regular passenger service on the canyon line ended after the Second World War, and in 1960 the second set of train tracks was removed. Conrail abandoned the section of the railroad that passed through the gorge on September 21, 1988. The right-of-way eventually became the Pine Creek Rail Trail, which follows the path of the former Pine Creek Path. The first section of the rail trail opened in 1996 and included the section in the park: as of 2015 the Pine Creek Rail Trail is long.\n\nColton Point State Park continued to attract national attention in the post-war era. \"The New York Times\" featured the park and its \"breath-taking views of the gorge\" as well as its trails and location in the wilds of the state forest in a 1950 article, and in 1966 praised the whitewater boating on Pine Creek and the park's \"outstanding look-out points\". The Pine Creek Gorge, including Colton Point and Leonard Harrison State Parks and a section of Tioga State Forest, was named a National Natural Landmark (NNL) in April 1968. A 1973 \"New York Times\" article on whitewater canoeing noted the damage along Pine Creek done by Hurricane Agnes the year before. Another \"Times\" story in 2002 noted the park for its beauty and wildlife, and cited it as a starting point for hiking the West Rim Trail.\n\nIn the new millennium, the two state parks on either side of the Pine Creek Gorge are frequently treated as one. A 2002 \"New York Times\" article called Colton Point and Leonard Harrison state parks \"Two State Parks, Divided by a Canyon\" and noted their \"overlooks offer the most spectacular views\". Colton Point and Leonard Harrison were each included in the list of state parks chosen by the DCNR Pennsylvania Bureau of Parks for its \"25 Must-See Pennsylvania State Parks\" list. The DCNR describes how they \"offer spectacular vistas and a fabulous view of Pine Creek Gorge, also known as Pennsylvania's Grand Canyon\". It goes on to praise their inclusion in a National Natural Landmark and State Park Natural Area, hiking and trails, and the Pine Creek Rail Trail and bicycling.\n\nColton Point State Park lies on the west side of the Pine Creek Gorge, also known as the Grand Canyon of Pennsylvania. A sister park, Leonard Harrison State Park, is on the east side, and the two parks combined form essentially one large park that includes parts of the gorge and creek and parts of the plateau dissected by the gorge. Pine Creek has carved the gorge nearly through the dissected Allegheny Plateau in northcentral Pennsylvania. The canyon begins in southwestern Tioga County, just south of the village of Ansonia, and continues south to near the village of Waterville in Lycoming County. The depth of the gorge in Colton Point State Park is about and it measures nearly across.\n\nThe Pine Creek Gorge National Natural Landmark includes Colton Point and Leonard Harrison State Parks and parts of the Tioga State Forest along of Pine Creek between Ansonia and Blackwell. This federal program does not provide any extra protection beyond that offered by the land owner. The National Park Service's designation of the gorge as a National Natural Landmark notes that it \"contains superlative scenery, geological and ecological value, and is one of the finest examples of a deep gorge in the eastern United States.\"\n\nThe gorge is also protected by the state of Pennsylvania as the Pine Creek Gorge Natural Area, which is the second largest State Natural Area in Pennsylvania. Within this area, of Colton Point and Leonard Harrison State Parks are designated a State Park Natural Area. The state Natural Area runs along Pine Creek from Darling Run in the north (just below Ansonia) to Jerry Run in the south (just above Blackwell). It is approximately long and wide, with state forest roads providing all of the western border and part of the eastern border.\n\nWithin the park, Pine Creek and the walls of the gorge \"visible from the opposite shoreline\" are also protected by the state as a Pennsylvania Scenic River. In 1968 Pine Creek was one of only 27 rivers originally designated as eligible to be included in the National Wild and Scenic Rivers System, and one of only eight specifically mentioned in the law establishing the program. Before Pine Creek could be included in the federal program, the state enacted its State Scenic Rivers Act, then asked that Pine Creek be withdrawn from the national designation. There was much local opposition to its inclusion, based at least partly on mistaken fears that protection would involve seizure of private property and restricted access. Eventually this opposition was overcome, but Pennsylvania did not officially include it as one of its own state Scenic and Wild Rivers until November 25, 1992. The state treated Pine Creek as a state scenic river between 1968 and 1992. It protected the creek from dam-building and water withdrawals for power plants, and added public access points to reduce trespassing on private property by visitors to the creek.\n\nAlthough the rock formations exposed in Colton Point State Park and the Pine Creek Gorge are at least 300 million years old, the gorge itself formed about 20,000 years ago, in the last ice age. Pine Creek had flowed northeasterly until then, but was dammed by rocks, soil, ice, and other debris deposited by the receding Laurentide Continental Glacier. The dammed creek formed a lake near what would later be the village of Ansonia, and the lake's glacial meltwater overflowed the debris dam, reversing the flow of Pine Creek. The creek flooded to the south and quickly carved a deep channel on its way to the West Branch Susquehanna River.\nThe park is at an elevation of on the Allegheny Plateau, which formed in the Alleghenian orogeny some 300 million years ago, when Gondwana (specifically what became Africa) and what became North America collided, forming Pangaea. Although the gorge and its surroundings seem to be mountainous, the area is a dissected plateau. Years of erosion have cut away the soft rocks, forming the valleys, and left the hardest of the ancient rocks relatively untouched on the top of sharp ridges, giving them the appearance of \"mountains\".\n\nThe land on which Colton Point State Park sits was once part of the coastline of a shallow sea that covered a great portion of what is now North America. The high mountains to the east of the sea gradually eroded, causing a buildup of sediment made up primarily of clay, sand and gravel. Tremendous pressure on the sediment caused the formation of the rocks that are found today in the Pine Creek drainage basin: sandstone, shale, conglomerates, limestone, and coal.\n\nFive major rock formations present in Colton Point State Park are from the Devonian and Carboniferous periods. The youngest of these, which forms the highest points in the park and along the gorge, is the early Pennsylvanian Pottsville Formation, a gray conglomerate that may contain sandstone, siltstone, and shale, as well as anthracite coal. Low-sulfur coal was once mined at three locations within the Pine Creek watershed. Below this is the late Mississippian Mauch Chunk Formation, which is formed with grayish-red shale, siltstone, sandstone, and conglomerate. Millstones were once carved from the exposed sections of this conglomerate. Together the Pottsville and Mauch Chunk formations are some thick.\n\nNext below these is the late Devonian and early Mississippian Huntley Mountain Formation, which is made of relatively soft grayish-red shale and olive-gray sandstone. This is relatively hard rock and forms many of the ridges. Below this is the red shale and siltstone of the Catskill Formation, about thick and some 375 million years old. This layer is relatively soft and easily eroded, which helped to form the Pine Creek Gorge. Cliffs formed by the Huntley Mountain and Catskill formations are visible north of the park at Barbour Rock. The lowest and oldest layer is the Lock Haven Formation, which is gray to green-brown siltstone and shale over 400 million years old. It forms the base of the gorge, contains marine fossils, and is up to thick.\n\nThe Allegheny Plateau has a continental climate, with occasional severe low temperatures in winter and average daily temperature ranges of 20 °F (11 °C) in winter and 26 °F (14 °C) in summer. The mean annual precipitation for the Pine Creek watershed is . The highest recorded temperature at the park was in 1936, and the record low was in 1934.\n\nDescriptions from early explorers and settlers give some idea of what the Pine Creek Gorge was like before it was clearcut. The forest was up to 85 percent hemlock and white pine; hardwoods made up the rest of the forest. The area was inhabited by a large number of animal species, many of which have vanished by the end of the 20th century. A herd of 12,000 American bison migrated along the West Branch Susquehanna River in 1773. Pine Creek was home to large predators such as wolves, lynx, wolverines, panthers, fishers, bobcats and foxes; all are locally extinct except for the last three as of 2007. The area had herds of elk and deer, and large numbers of black bears, river otters, and beavers. In 1794, two of the earliest white explorers to travel up Pine Creek found so many rattlesnakes on its banks that they had to sleep in their canoe. Further upstream, insects forced them to do the same.\n\nThe virgin forests cooled the land and streams. The creeks and runs flowed more evenly year-round, since centuries of accumulated organic matter in the forest soil caused slow percolation of rainfall into them. Pine Creek was home to large numbers of fish, including trout, but dams downstream on the Susquehanna River have eliminated the shad, salmon, and eels once found here by blocking their migrations. Habitat for land animals was destroyed by the clearcutting of forests, but there was also a great deal of hunting, with bounties paid for large predators.\n\nWhile Colton Point and Leonard Harrison State Parks and parts of the surrounding Tioga State Forest are now the Pine Creek Gorge National Natural Landmark, it is their status as part of a Pennsylvania State Natural Area that provides the strongest protection for them. Within this Natural Area, logging, mining, and drilling for oil and gas are prohibited. Furthermore, only foot trail access is allowed. In 1988 the Pennsylvania Department of Environmental Resources, precursor to the DCNR, described it as about 95% State owned, unroaded, and designated the Pine Creek Gorge Natural Area. It is a place of unique geologic history and contains some rare plant communities, an old growth hemlock stand, ... active bald eagle nest[s] ... and is a major site of river otter reintroduction. Departmental policy is protection of the natural values of the Canyon from development and overuse, and restoration of the area to as near a natural condition as possible.\nThe gorge has over 225 species of wildflowers, plants and trees, with scattered stands of old growth forest on some of its steepest walls. The rest of the gorge is covered with thriving second growth forest that can be over one hundred years old. Since clearcutting, nearly 90 percent of the forest land has burnt at least once. Typical south-facing slopes here have mountain laurel below oak and hickory trees, while north-facing slopes tend to have ferns below hemlocks and hardwoods. Large chestnuts and black cherry can also be found.\n\nThe Grand Canyon of Pennsylvania is known for its fall foliage, and Colton Point State Park is a popular place to observe the colors, with the first three weeks of October as the best time to see the leaves in their full color. Red leaves are found on red maple, black cherry, and red oak, while orange and yellow leaves are on black walnut, sugar maple, tulip poplar, chestnut oak, aspen and birch, and brown leaves are from beech, white oak, and eastern black oak trees. Despite the logging, there are some old-growth hardwoods and hemlocks on Fourmile Run. Plants of \"special concern\" in Pennsylvania that are found in the gorge include Jacob's ladder, wild pea, and hemlock parsley.\n\nThere are over 40 species of mammals in the Pine Creek Gorge. Colton Point State Park's extensive forest cover makes it a habitat for \"big woods\" wildlife, including white-tailed deer, black bear, wild turkey, red and gray squirrels. Less common creatures include bobcats, coyote, fishers, river otters, and timber rattlesnakes. There are over 26 species of fish in Pine Creek, including trout, suckers, fallfish, and rock bass. Other aquatic species include crayfish and frogs.\n\nSeveral species have been reintroduced to the gorge. White-tailed deer were imported from Michigan and released throughout Pennsylvania to reestablish what had once been a thriving population. The current population of deer in Pennsylvania are descended from the original stock introduced since 1906, after the lumbermen had moved out of the area. The deer population has grown so much that today they exceed their carrying capacity in many areas. River otters were successfully reintroduced in 1983 and now breed in the gorge. Despite the otters' diet of 5 percent trout, some anglers fear the animals would deplete the game fish in the gorge.\nFishers, medium-sized weasels, were reintroduced to Pine Creek Gorge as part of an effort to establish a healthy population of fishers in Pennsylvania. Prior to the lumber era, fishers were numerous throughout the forests of Pennsylvania. They are generalized predators and will hunt any smaller creatures in their territory, including porcupines. Elk have been reintroduced west of the gorge in Clinton County and occasionally wander near the west rim of the canyon. Coyotes have come back on their own. Invasive insect species in the gorge include gypsy moth larvae, which eat all the leaves off trees, especially oaks, and hemlock woolly adelgids, which weaken and kill hemlocks. Invasive plant species include purple loosestrife and Japanese knotweed.\n\nColton Point State Park is part of Important Bird Area #28, which encompasses of both publicly and private held land. State managed acreage accounts for 68 percent of the total area and includes Colton Point and Leonard Harrison State Parks and the surrounding Tioga State Forest lands. The Pennsylvania Audubon Society has designated all of Colton Point State Park as part of the IBA, which is an area designated as a globally important habitat for the conservation of bird populations.\n\nOrnithologists and bird watchers have recorded a total of 128 species of birds in the IBA. Several factors contribute to the high total of bird species observed: there is a large area of forest in the IBA, as well as great habitat diversity, with of open water that is used by many of the birds, especially bald eagles. The location of the IBA along the Pine Creek Gorge also contributes to the diverse bird populations.\n\nIn addition to bald eagles, which live in the IBA year round and have successfully established a breeding population there, the IBA is home to belted kingfishers, scarlet tanagers, black-throated blue warblers, common mergansers, blue and green herons, hermit thrushes, and wood ducks. Large numbers of ospreys use the gorge during spring and fall migration periods. The woodlands are inhabited by wild turkeys and Pennsylvania's state bird the ruffed grouse. Swainson's thrush breeds in the IBA and the Northern harrier breeds and overwinters in Pine Creek Gorge.\n\nA variety of warblers is found in Colton Point State Park. The Pennsylvania Audubon Society states that Pine Creek Gorge is \"especially rich in warbler species, including Pine, Black-throated blue, Black-throated green, Blackburnian, and Black-and-white.\" Many of these smaller birds are more often heard than seen as they keep away from the trails and overlooks.\n\nColton Point State Park has some challenging hikes in and around the Grand Canyon of Pennsylvania, with of trails that feature very rugged terrain, pass close to steep cliffs, and can be very slick in some areas. Governor Robert P. Casey took a hiking tour of the park in July 1990, and in 2003 the DCNR reported that 18,239 people used the trails in the park.\n\nCamping is a popular pastime at Colton Point State Park; 1,989 persons have used the camping facilities in 2003. With no modern amenities like flush toilets or showers, the campsites take on a rustic nature. There are outhouses, fire rings, a sanitary dump station and picnic tables at the campground. An Organized Group Tenting area, intended for organized youth or adult groups, can accommodate up to 90 campers. 1,490 campers used the area in 2003. The park also has approximately 100 picnic tables and five CCC-built picnic shelters which can be reserved. These facilities were used by 15,379 picnickers in 2003.\n\nHunting is permitted in of Colton Point State Park, and is regulated by the Pennsylvania Game Commission. The common game species are ruffed grouse, eastern gray squirrels, wild turkey, white-tailed deer, and black bears. The hunting of groundhogs is prohibited. More acres of forested woodlands are available for hunting on the grounds of the adjacent Tioga State Forest.\n\nFishing is permitted at Colton Point State Park. Anglers must descend the Turkey Path to reach Pine Creek. The species of fish found in Pine Creek are trout, smallmouth bass, and some panfish. There are several small trout streams that are accessible from within the park. Historically, the stretch of Pine Creek in the park has been fished by notable anglers, including President Theodore Roosevelt and Pennsylvania Governor William A. Stone.\n\nEdward Gertler writes in \"Keystone Canoeing\" that Pine Creek \"is possibly Pennsylvania's most famous canoe stream\" and attributes this partly to the thousands who decide to boat on it after they \"peer into Pine Creek's spectacular abyss from the overlooks of Leonard Harrison and Colton Point state parks\". The park contains of Pine Creek, which is classified as Class 1 to Class 2 whitewater. Boaters do not normally start or end their run in the park: it is part of the trip from Ansonia (Marsh Creek) south to Blackwell (Babb Creek).\n\nColton Point State Park is in Shippen Township, and is south of U.S. Route 6 and the village of Ansonia on Colton Road. The following state parks are within of Colton Point State Park:\n\n"}
{"id": "31550784", "url": "https://en.wikipedia.org/wiki?curid=31550784", "title": "Court jester hypothesis", "text": "Court jester hypothesis\n\nIn evolutionary theory, the court-jester hypothesis contrasts the Red Queen hypothesis. University of California, Berkeley professor Anthony D. Barnosky coined the term in 1999 in reference to the idea that abiotic forces (including climate), rather than biotic competition between species, function as a major driving force behind the processes in evolution which produce speciation.\n\nThe \"Red Queen hypothesis\" is a term coined by Leigh Van Valen, in 1973, in a reference to the Lewis Carroll book \"Through the Looking Glass\". It refers in evolution theory to the arms race of evolutionary developments and counter-developments that cause co-evolving species to mutually drive each other to adapt. There is dispute over how strongly evolution at the scale of speciation is driven by these competitions between species, and how much it is driven instead by abiotic factors like meteor strikes and climate change, but there was not an artful metaphor to capture this alternative until one was coined by Anthony Barnosky. The term \"Court Jester hypothesis\" was coined by Anthony Barnosky in 1999 in allusion to the Red Queen hypothesis. \n\nIn a 2001 paper on the subject, Barnosky uses the term without citation, suggesting that he is the one who coined it. Westfall and Millar attribute the term to him (citing the 2001 paper) in a paper of their own from 2004. Michael Benton also credits Barnosky with coining the phrase.\n\nSince 2001, many researchers in evolution (such as Tracy Aze, Anthony Barnosky, Michael J. Benton, Douglas Erwin, Thomas Ezard, Sergey Gravilets, J.B.C. Jackson, Paul N. Pearson, Andy Purvis, Robert D. Westfall, and Constance I. Millar) have started to use the term \"Court Jester hypothesis\" to describe the view that evolution at a macro scale is driven by abiotic factors more than the biotic competition called the Red Queen hypothesis.\n\nThe court jester hypothesis builds upon the punctuated equilibrium theory of Stephen Gould (1972) by providing a primary mechanism for it. The 2001 paper by Barnosky that is one of the first to use the term appropriate for the Court Jester side of the debate: the Stability hypothesis of Stenseth and Maynard Smith (1984), Vrba's Habitat Theory (1992), Vrba's Turn-over pulse hypothesis (1985), Vrba's Traffic light hypothesis and Relay Model (1995), Gould's Tiers of Time (1985), Brett and Baird's Coordinated Statis (1995), and Graham and Lundelius' Coevolutionary Disequilibrium (1984) theories. \n\nBarnosky's 2001 paper that was one of the first to introduce the term, explains what the Court Jester hypothesis means, describing it as one side of a debate over:\n\nThe Red Queen hypothesis (focusing on evolution by biotic interactions) and Court Jester hypothesis (focusing on evolution by abiotic factors such as stochastic environmental perturbations) both influence coevolutionary switching in host-parasite interaction. Barnosky acknowledges in the 2001 paper that the Court Jester hypothesis is not necessarily inconsistent with the Red Queen hypothesis: \n"}
{"id": "774220", "url": "https://en.wikipedia.org/wiki?curid=774220", "title": "Diamond anvil cell", "text": "Diamond anvil cell\n\nA diamond anvil cell (DAC) is a high-pressure device used in scientific experiments. It enables the compression of a small (sub-millimeter-sized) piece of material to extreme pressures, typically up to around 100–200 gigapascals, although it is possible to achieve pressures up to 770 gigapascals (7,700,000 bars / 7.7 million atmospheres).\n\nThe device has been used to recreate the pressure existing deep inside planets to synthesise materials and phases not observed under normal ambient conditions. Notable examples include the non-molecular ice X, polymeric nitrogen and metallic phases of xenon and potentially hydrogen.\n\nA DAC consists of two opposing diamonds with a sample compressed between the polished culets (tips). Pressure may be monitored using a reference material whose behavior under pressure is known. Common pressure standards include ruby fluorescence, and various structurally simple metals, such as copper or platinum. The uniaxial pressure supplied by the DAC may be transformed into uniform hydrostatic pressure using a pressure-transmitting medium, such as argon, xenon, hydrogen, helium, paraffin oil or a mixture of methanol and ethanol. The pressure-transmitting medium is enclosed by a gasket and the two diamond anvils. The sample can be viewed through the diamonds and illuminated by X-rays and visible light. In this way, X-ray diffraction and fluorescence; optical absorption and photoluminescence; Mössbauer, Raman and Brillouin scattering; positron annihilation and other signals can be measured from materials under high pressure. Magnetic and microwave fields can be applied externally to the cell allowing nuclear magnetic resonance, electron paramagnetic resonance and other magnetic measurements. Attaching electrodes to the sample allows electrical and magnetoelectrical measurements as well as heating up the sample to a few thousand degrees. Much higher temperatures (up to 7000 K) can be achieved with laser-induced heating, and cooling down to millikelvins has been demonstrated.\n\nThe operation of the diamond anvil cell relies on a simple principle:\n\nwhere \"p\" is the pressure, \"F\" the applied force, and \"A\" the area. Typical culet sizes for diamond anvils are 100–250 micron, such that a very high pressure is achieved by applying a moderate force on a sample with a small area, rather than applying a large force on a large area. Diamond is a very hard and virtually incompressible material, thus minimising the deformation and failure of the anvils that apply the force.\n\nThe study of materials at extreme conditions, high pressure and high temperature uses a wide array of techniques to achieve these conditions and probe the behavior of material while in the extreme environment. Percy Williams Bridgman, the great pioneer of high-pressure research during the first half of the 20th century, revolutionized the field of high pressures with his development of an opposed anvil device with small flat areas that were pressed one against the other with a lever-arm. The anvils were made of tungsten carbide (WC). This device could achieve pressure of a few gigapascals, and was used in electrical resistance and compressibility measurements. The principles of the DAC are similar to the Bridgman anvils but in order to achieve the highest possible pressures without breaking the anvils, they were made of the hardest known material: a single crystal diamond. The first prototypes were limited in their pressure range and there was not a reliable way to calibrate the pressure.\n\nFollowing the Bridgman anvil, the diamond anvil cell became the most versatile pressure generating device that has a single characteristic that sets it apart from the other pressure devices. This provided the early high pressure pioneers with the capability to directly observe the properties of a material while under pressure. With just the use of an optical microscope, phase boundaries, color changes and recrystallization could be seen immediately, while x-ray diffraction or spectroscopy required time to expose and develop photographic film. The potential for the diamond anvil cell was realized by Alvin Van Valkenburg while he was preparing a sample for IR spectroscopy and was checking the alignment of the diamond faces.\n\nThe diamond cell was created at the National Bureau of Standards (NBS) by Charles E. Weir, Ellis R. Lippincott, and Elmer N. Bunting. Within the group each member focused on different applications of the diamond cell. Van focused on making visual observations, Charles on XRD, Ellis on IR Spectroscopy. The group was well established in each of their techniques before outside collaboration kicked off with university researchers like William A. Bassett and Taro Takahashi at the University of Rochester.\n\nDuring the first experiments using diamond anvils, the sample was placed on the flat tip of the diamond, the culet, and pressed between the diamond faces. As the diamond faces were pushed closer together, the sample would be pressed and extrude out from the center. Using a microscope to view the sample, it could be seen that a smooth pressure gradient existed across the sample with the outer most portions of the sample acting as a kind of gasket. The sample was not evenly distributed across the diamond culet but localized in the center due to the \"cupping\" of the diamond at higher pressures. This cupping phenomenon is the elastic stretching of the edges of the diamond culet, commonly referred to as the \"shoulder height\". Many diamonds were broken during the first stages of producing a new cell or any time an experiment is pushed to higher pressure. The NBS group was in a unique position where almost endless supplies of diamonds were available to them. Custom officials occasionally confiscated diamonds from people attempting to smuggle them into the country. Disposing of such valuable confiscated materials could be problematic given rules and regulations. A solution was simply to make such materials available to people at other government agencies if they could make a convincing case for their use. This became an unrivaled resource as other teams at the University of Chicago, Harvard University and General Electric entered the high pressure field.\n\nDuring the following decades DACs have been successively refined, the most important innovations being the use of gaskets and the ruby pressure calibration. The DAC evolved to be the most powerful lab device for generating static high pressure. The range of static pressure attainable today extends to 640 GPa, much higher than the estimated pressures at the Earth's center (~360 GPa).\n\nThere are many different DAC designs but all have four main components:\n\nRelies on the operation of either a lever arm, tightening screws, or pneumatic or hydraulic pressure applied to a membrane. In all cases the force is uniaxial and is applied to the tables (bases) of the two anvils.\n\nMade of high gem quality, flawless diamonds, usually with 16 facets, they typically weigh 1/8 to 1/3 carat (25 to 70 mg). The culet (tip) is ground and polished to a hexadecagonal surface parallel to the table. The culets of the two diamonds face one another, and must be perfectly parallel in order to produce uniform pressure and to prevent dangerous strains. Specially selected anvils are required for specific measurements—for example, low diamond absorption and luminescence is required in corresponding experiments.\n\nA gasket used in a diamond anvil cell experiment is a thin metal foil, typically 0.3 mm in thickness, which is placed in between the diamonds. Desirable materials for gaskets are strong, stiff metals such as rhenium or tungsten. Steel is frequently used as a cheaper alternative for low pressure experiments. The above-mentioned materials cannot be used in radial geometries where the x-ray beam must pass through the gasket. Since they are not transparent to x-rays, if x-ray illumination through the gasket is required, lighter materials such as beryllium, boron nitride, boron or diamond are used as a gasket. Gaskets are preindented by the diamonds and a hole is drilled in the center of the indentation to create the sample chamber.\n\nThe pressure transmitting medium is the compressible fluid that fills the sample chamber and transmits the applied force to the sample. Hydrostatic pressure is preferred for high-pressure experiments because variation in strain throughout the sample can lead to distorted observations of different behaviors. In some experiments stress and strain relationships are investigated and the effects of non-hydrostatic forces are desired. A good pressure medium will remain a soft, compressible fluid to high pressure.\nThe full range of techniques that are available has been summarized in a tree diagram by William Bassett. The ability to utilize any and all of these techniques hinges on being able to look through the diamonds which was first demonstrated by visual observations.\n\nThe two main pressure scales used in static high-pressure experiments are X-ray diffraction of a material with a known equation of state and measuring the shift in ruby fluorescence lines. The first began with NaCl, for which the compressibility has been determined by first principles in 1968. The major pitfall of this method of measuring pressure is that you need X-rays. Many experiments do not require X-rays and this presents a major inconvenience to conduct both the intended experiment and a diffraction experiment. In 1971, the NBS high pressure group was set in pursuit of a spectroscopic method for determining pressure. It was found that the wavelength of ruby fluorescence emissions change with pressure, this was easily calibrated against the NaCl scale.\n\nOnce pressure could be generated and measured it quickly became a competition for which cells can go the highest. The need for a reliable pressure scale became more important during this race. Shock-wave data for the compressibilities of Cu, Mo, Pd and Ag were available at this time and could be used to define equations of states up to Mbar pressure. Using these scales these pressures were reported: 1.2 Mbar (120 GPa) in 1976, 1.5 Mbar (150 GPa) in 1979, 2.5 Mbar (250 GPa) in 1985, and 5.5 Mbar (550 GPa) in 1987.\n\nBoth methods are continually refined and in use today. However, the ruby method is less reliable at high temperature. Well defined equations of state are needed when adjusting temperature and pressure, two parameters that affect the lattice parameters of materials.\n\nPrior to the invention of the diamond anvil cell, static high-pressure apparatus required large hydraulic presses which weighed several tons and required large specialized laboratories. The simplicity and compactness of the DAC meant that it could be accommodated in a wide variety of experiments. Some contemporary DACs can easily fit into a cryostat for low-temperature measurements, and for use with a superconducting electromagnet. In addition to being hard, diamonds have the advantage of being transparent to a wide range of the electromagnetic spectrum from infrared to gamma rays, with the exception of the far ultraviolet and soft X-rays. This makes the DAC a perfect device for spectroscopic experiments and for crystallographic studies using hard X-rays.\n\nA variant of the diamond anvil, the hydrothermal diamond anvil cell (HDAC) is used in experimental petrology/geochemistry for the study of aqueous fluids, silicate melts, immiscible liquids, mineral solubility and aqueous fluid speciation at geologic pressures and temperatures. The HDAC is sometimes used to examine aqueous complexes in solution using the synchrotron light source techniques XANES and EXAFS. The design of HDAC is very similar to that of DAC, but it is optimized for studying liquids.\n\nAn innovative use of the diamond anvil cell is testing the sustainability and durability of life under high pressures, including the search for life on extrasolar planets. Testing portions of the theory of panspermia (a form of interstellar travel) is one application of DAC. When interstellar objects containing life-forms impact a planetary body, there is high pressure upon impact and the DAC can replicate this pressure to determine if the organisms could survive. Another reason the DAC is applicable for testing life on extrasolar planets is that planetary bodies that hold the potential for life may have incredibly high pressure on their surface.\n\nIn 2002, scientists at the Carnegie Institution of Washington examined the pressure limits of life processes. Suspensions of bacteria, specifically \"Escherichia coli\" and \"Shewanella oneidensis\", were placed in the DAC, and the pressure was raised to 1.6 GPa, which is more than 16,000 times Earth's surface pressure (985 hPa). After 30 hours, only about 1% of the bacteria survived. The experimenters then added a dye to the solution. If the cells survived the squeezing and were capable of carrying out life processes, specifically breaking down formate, the dye would turn clear. 1.6 GPa is such great pressure that during the experiment the DAC turned the solution into ice-IV, a room-temperature ice. When the bacteria broke down the formate in the ice, liquid pockets would form because of the chemical reaction. The bacteria were also able to cling to the surface of the DAC with their tails.\n\nSkeptics debated whether breaking down formate is enough to consider the bacteria living. Art Yayanos, an oceanographer at the Scripps Institute of Oceanography in La Jolla, California, believes an organism should only be considered living if it can reproduce. Subsequent results from independent research groups have shown the validity of the 2002 work. This is a significant step that reiterates the need for a new approach to the old problem of studying environmental extremes through experiments. There is practically no debate whether microbial life can survive pressures up to 600 MPa, which has been shown over the last decade or so to be valid through a number of scattered publications.\n\nSimilar tests were performed with a low-pressure (0.1–600 MPa) diamond anvil cell, which has better imaging quality and signal collection. The studied microbes, \"Saccharomyces cerevisiae\" (baker's yeast), withstood pressures of 15–50 MPa, and died at 200 MPa.\n\nGood single crystal diffraction experiments in diamond anvil cells require sample stage to rotate on the vertical axis, omega. Most diamond anvil cells do not feature a large opening that would allow the cell to be rotated to high angles, a 60 degrees opening is considered sufficient for most crystals but larger angles are possible. The first cell to be used for single crystal experiments was designed by a graduate student at the University of Rochester, Leo Merrill. The cell was triangular with beryllium seats that the diamonds were mounted on; the cell was pressurized with screws and guide pins holding everything in place.\n\nHeating in diamond-anvil cells is typically done by two means, external or internal heating. External heating is defined as heating the anvils and would include a number of resistive heaters that are placed around the diamonds or around the cell body. The complementary method does not change the temperature of the anvils and includes fine resistive heaters placed within the sample chamber and laser heating. The main advantage to resistive heating is the precise measurement of temperature with thermocouples, but the temperature range is limited by the properties of the diamond which will oxidize in air at 700 °C The use of an inert atmosphere can extend this range above 1000 °C. With laser heating the sample can reach temperature above 5000 °C, but the minimum temperature that can be measured when using a laser-heating system is ~1200 °C and the measurement is much less precise. Advances in resistive heating are closing the gap between the two techniques so that systems can be studied from room temperature to beyond 5700 °C with the combination of the two.\n\nThe pressure transmitting medium is an important component in any high-pressure experiment. The medium fills the space within the sample 'chamber' and applies the pressure being transmitted to the medium onto the sample. In a good high-pressure experiment, the medium should maintain a homogeneous distribution of pressure on the sample. In other words, the medium must stay hydrostatic to ensure uniform compressibility of the sample. Once a pressure transmitting medium has lost its hydrostaticity, a pressure gradient forms in the chamber that increases with increasing pressure. This gradient can greatly affect the sample, compromising results. The medium must also be inert, as to not interact with the sample, and stable under high pressures. For experiments with laser heating, the medium should have low thermal conductivity. If an optical technique is being employed, the medium should be optically transparent and for x-ray diffraction, the medium should be a poor x-ray scatterer – as to not contribute to the signal.\n\nSome of the most commonly used pressure transmitting media have been sodium chloride, silicone oil, and a 4:1 methanol-ethanol mixture. Sodium chloride is easy to load and is used for high-temperature experiments because it acts as a good thermal insulator. The methanol-ethanol mixture displays good hydrostaticity to about 10 GPa and with the addition of a small amount of water can be extended to about 15 GPa.\n\nFor pressure experiments that exceed 10 GPa, noble gases are preferred. The extended hydrostaticity greatly reduces the pressure gradient in samples at high pressure. Noble gases, such as helium, neon, and argon are optically transparent, thermally insulating, have small X-ray scattering factors and have good hydrostaticity at high pressures. Even after solidification, noble gases provide quasihydrostatic environments.\n\nArgon is used for experiments involving laser heating because it is chemically insulating. Since it condenses at a temperature above that of liquid nitrogen, it can be loaded cryogenically. Helium and neon have low X-ray scattering factors and are thus used for collecting X-ray diffraction data. Helium and neon also have low shear moduli; minimizing strain on the sample. These two noble gases do not condense above that of liquid nitrogen and cannot be loaded cryogenically. Instead, a high-pressure gas loading system has been developed that employs a gas compression method.\n\nIn order to load a gas as a sample of pressure transmitting medium, the gas must be in a dense state, as to not shrink the sample chamber once pressure is induced. To achieve a dense state, gases can be liquefied at low temperatures or compressed. Cryogenic loading is a technique that uses liquefied gas as a means of filling the sample chamber. The DAC is directly immersed into the cryogenic fluid that fills the sample chamber. However, there are disadvantages to cryogenic loading. With the low temperatures indicative of cryogenic loading, the sample is subjected to temperatures that could irreversibly change it. Also, the boiling liquid could displace the sample or trap an air bubble in the chamber. It is not possible to load gas mixtures using the cryogenic method due to the different boiling points of most gases. Gas compression technique densifies the gases at room temperature. With this method, most of the problems seen with cryogenic loading are fixed. Also, loading gas mixtures becomes a possibility. The technique uses a vessel or chamber in which the DAC is placed and is filled with gas. Gases are pressurized and pumped into the vessel with a compressor. Once the vessel is filled and the desired pressure is reached the DAC is closed with a clamp system run by motor driven screws.\n\n\nThe development of laser heating began only 8 years after Charles Weir, of the National Bureau of Standards (NBS), made the first diamond anvil cell and Alvin Van Valkenburg, NBS, realized the potential of being able to see the sample while under pressure. William Bassett and his colleague Taro Takahashi focused a laser beam on the sample while under pressure. The first laser heating system used a single 7 joule pulsed ruby laser that heated the sample to 3000 °C while at 260 kilobars. This was sufficient to convert graphite to diamond. The major flaws within the first system related to control and temperature measurement.\n\nTemperature measurement was initially done by Basset using an optical pyrometer to measure the intensity of the incandescent light from the sample. Colleagues at UC Berkeley were better able to utilize the black body radiation and more accurately measure the temperature. The hot spot produced by the laser also created large thermal gradients in between the portions of sample that were hit by the focused laser and those that were not. The solution to this problem is ongoing but advances have been made with the introduction of a double-sided approach.\n\nThe use of two lasers to heat the sample reduces the axial temperature gradient, this which allows for thicker samples to be heated more evenly. In order for a double-sided heating system to be successful it is essential that the two lasers are aligned so that they are both focused on the sample position. For in situ heating in diffraction experiments, the lasers need to be focused to the same point in space where the X-ray beam is focused.\n\nThe European Synchrotron Radiation Facility (ESRF) as well as many other synchrotron facilities as the three major synchrotron user facilities in the United States all have beamlines equipped with laser heating systems. The respective beamlines with laser heating systems are at the ESRF ID27 and ID24; at the Advanced Photon Source (APS), 13-ID-D GSECARS and 16-ID-B HP-CAT; at the National Synchrotron Light Source, X17B3; and at the Advanced Light Source, 12.2.2. Laser heating has become a routine technique in high-pressure science but the reliability of temperature measurement is still controversial.\n\nIn the first experiments with laser heating, temperature came from a calibration of laser power made with known melting points of various materials. When using the pulsed ruby laser this was unreliable due to the short pulse. YAG lasers quickly become the standard, heating for relatively long duration, and allowing observation of the sample throughout the heating process. It was with the first use of YAG lasers that Bassett used an optical pyrometer to measure temperatures in the range of 1000 °C to 1600 °C. The first temperature measurements had a standard deviation of 30 °C from the brightness temperature, but due to the small sample size was estimated to be 50 °C with the possibility that the true temperature of the sample being was 200 °C higher than that of the brightness measurement. Spectrometry of the incandescent light became the next method of temperature measurement used in Bassett's group. The energy of the emitted radiation could be compared to known black body radiation spectra to derive a temperature. Calibration of these systems is done with published melting points or melting points as measured by resistive heating.\n\nLaser heating is used to heat micrograms of sample in diamond-anvil cells when studying matter under extreme conditions. This typically means one of four things:\n"}
{"id": "12592529", "url": "https://en.wikipedia.org/wiki?curid=12592529", "title": "Earth Surface Processes and Landforms", "text": "Earth Surface Processes and Landforms\n\nEarth Surface Processes and Landforms is a peer-reviewed scientific journal published by John Wiley & Sons on behalf of the British Society for Geomorphology. It covers geomorphology and more in general all aspects of Earth sciences dealing with the Earth surface. The journal was established in 1976 as Earth Surface Processes, obtaining its current name in 1981. The journal primarily publishes original research papers. It also publishes \"Earth Surface Exchanges\"' which include commentaries on issues of particular geomorphological interest, discussions of published papers, shorter journal articles suitable for rapid publication, and commissioned reviews on key aspects of geomorphological science. Foci include the physical geography of rivers, valleys, glaciers, mountains, hills, slopes, coasts, deserts, and estuary environments, along with research into Holocene, Pleistocene, or Quaternary science. The editor-in-chief is Stuart Lane (University of Lausanne).\n\nThe journal is abstracted and indexed in:\n\nAccording to the \"Journal Citation Reports\", the journal has a 2017 impact factor of 3.722.\n\n"}
{"id": "31637124", "url": "https://en.wikipedia.org/wiki?curid=31637124", "title": "Energy in Qatar", "text": "Energy in Qatar\n\nEnergy in Qatar describes energy production, consumption, and policies of the State of Qatar. The International Monetary Fund ranked Qatar as having the fifth highest GDP per capita in 2016 with a 60,787 USD per capita nominal GDP over a population of 2.421 million inhabitants. In 2014, oil and natural gas production made up 51.1% of Qatar's nominal GDP. Thus, Qatar has a worldwide high ranking of per capita GDP due to its significance production and exports in both crude oil and natural gas in proportion to its relatively small population.\n\nQatar is a member of the Organization of the Petroleum Exporting Countries (OPEC). Since 2005, natural gas production in Qatar has significantly increased and is the primary fuel chosen for energy consumption within Qatar. In 2014, Qatar ranked as the fourth highest natural gas producer worldwide. Qatar's energy consumption in 2016 was 34.00 billion kilowatt-hours (kWh), which is an average of 15,056 kWh per capita.\n\nQatar's exploration of the oil market began around 1923, when its own pearl diving market took a hit with the entry of Japan's cultivated pearls into the market. Qatar's first oil discovery was made in late 1930s with oil deposits found in Dukhan field. Since then, Qatar claims to have 1.5% of global oil reserves, while producing 2% of the global oil economy.\n\nIn 2015, Qatar was ranked as the 17th top producer of crude oil worldwide at an approximate 1.532 million bbl/day. In 2013, Qatar also ranked as the 11th top exporter of crude oil at an approximate 1.303 million bbl/day. With high production of crude oil, Qatar is one of the few countries that has little crude oil dependence on other countries for domestic energy consumption. While Qatar does not import any crude oil, it does, however, import approximately 2,555 bbl/day of refined petroleum products. The following table depicts how much oil each sector of Qatar's economy consumes in 1000 tonnes in the year 2014, notice how all of the oil products used within in Qatar are from refined petroleum sources.\n\nThe sectors of the economy which consume the most of oil - based fuels for energy usage are transport and residential. This is due to Qatar's limited resources related to agriculture, forestry, and fishing since it has primarily desert terrain. And while oil consumption for energy usage has quadrupled since 2000, the proportions for which each sector consumes oil has been consistent.\n\nQatar has a high economic dependency on the global oil and natural gas market with 51% of its nominal GDP coming from oil and gas exports in 2014. An indirect result of the instability of oil prices within the most recent decades has led the Qatari government to develop the Qatar National Development Strategy 2011–2016 (Development Strategy) as an efficient strategy to achieve the Qatar National Vision 2030.\n\nIn the Development Strategy, there are four pillars, one of which, the economy pillar, discusses policies, regulations, and political motives related to Qatar’s dependence on oil. For sustaining economic prosperity, Qatar aims to:\nThese objectives were created under the notion that Qatar's hydrocarbon income will continue to decrease in response to the increase in renewable energy technologies and depletion of oil resources.\n\nAn enlargement of its productive base (hydrocarbon and mineral assets) by investing in physical and social infrastructure effectively attracts tourists and expatriates to the area, and provides better roads and housing for citizens as well. However, constant construction in Qatar for improving and creating physical infrastructure has caused frustration within the population throughout the process.\n\nThe government's gradual transition to a more stable economy while maintaining the wealth of the nation primarily comes at a cost to Qatar’s government since hydrocarbon assets are property of the State. However, should the government not cautiously or properly invest for the new economy, the population would lose their current standard of living and the cost would be primarily directed at them.\n\nThe government plans to attack inefficiency in technology, physical infrastructure, institutions and processes in order to make a lasting contribution to improve the use of resources over time. Since Qatar is primarily desert terrain, there exists restrictions on how Qatar can expand its productive base and thus, most of the Development Strategy is focused on increasing efficiency of all current physical and social structures put into place. In particular, there is a plan to tackle technical and economic inefficiencies in the production, distribution and use of water.\n\nAs of 2017, a more diversified economy is inherently more stable, more capable of creating jobs and opportunities for the next generation and less vulnerable to the boom and bust cycles of oil and natural gas prices in Qatar. However, Qatar has still not published a plan for which they state how they will achieve this diversified economy. Thus, the effectiveness of such a plan cannot be commented on until such a plan is produced for public consumption.\n\nEase of political acceptability on the Development Strategy is dependent on the residents' acceptance of the current government as proven with other laws in the past. Qatar aims to set itself apart on the international stage with its plans to diversify its oil - based economy and improve inefficiencies within social and physical structures of the State.\n\nIn 2009 Qatar was the 7th top producer of natural gas (2.9%) in the world, exceeding Algeria (2.6%), the Netherlands (2.5%) and Indonesia (2.5%). 75% of the natural gas production was exported in 2009 (67/89 bm). The energy content is high.\n\nHarnessing solar power has become an important objective for Qatar in recent years. By 2030, Qatar has set the goal of attaining 20% of its energy from solar power. The country is well-positioned to capitalize on photovoltaic systems, as it has a global horizontal irradiance value of approximately 2,140 kWh per square meter annually. Furthermore, the direct irradiance parameter is roughly 2,008 kWh per square meter annually, inferring that it would be able to benefit from concentrated solar power as well.\n\nQatar Foundation has been active in helping Qatar achieve its solar power goals. It established Qatar Solar, which, together with Qatar Development Bank and German company SolarWorld, embarked on a joint venture resulting in the creation of Qatar Solar Technologies (QSTec). In 2017, QSTec commissioned its polysillicon plant in Ras Laffan. This plant has a total capacity of 1.1 MW of solar power.\n\nQatar's electricity supply is regulated by Kahramaa.\nFrom 2007 to 2010 energy export of Qatar have increased from 930 TWh to 1,748 TWh. In three years population growth was from 0.84 million to 1.76 million persons.\n\nQatar was the third top carbon dioxide emitter per capita in the world in 2009: 79.82 tonnes per capita. All emissions from building and cement production are local but some people may argue that some Qatar produced fuels and goods are consumed abroad.\n\n"}
{"id": "3338291", "url": "https://en.wikipedia.org/wiki?curid=3338291", "title": "Fech fech", "text": "Fech fech\n\nFech fech () is a very fine powder caused by the erosion of clay-limestone terrain and it is most commonly found in deserts. It consists of a surface horizon of pulverized soil with low particle cohesion protected under a thin crust. It is not determinable from the surface and can therefore pose a significant transportation hazard acting as a surprise \"trap\" as the ground collapses beneath a vehicle miring it in a quicksand-like substance.\n\nFech-fech is classified into two types:\nFech fech is common in the Qattara Depression, making that portion of the Sahara Desert impassable by most vehicles.\n\n"}
{"id": "31180513", "url": "https://en.wikipedia.org/wiki?curid=31180513", "title": "Formative epistemology", "text": "Formative epistemology\n\nFormative epistemology is a collection of philosophic views concerned with the theory of knowledge that emphasize the role of natural scientific methods. According to formative epistemology, knowledge is gained through the imputation of thoughts from one human being to another in the societal setting. Humans are born without intrinsic knowledge and through their evolutionary and developmental processes gain knowledge from other human beings. Thus, according to formative epistemology, all knowledge is completely subjective and truth does not exist.\n\nThis shared emphasis on scientific methods of studying knowledge shifts focus to the empirical processes of knowledge acquisition and away from many traditional philosophic questions. There are noteworthy distinctions within formative epistemology. Replacement naturalism maintains that traditional epistemology should be abandoned and replaced with the methodologies of the natural sciences. The general thesis of cooperative naturalism is that traditional epistemology can benefit in its inquiry by using the knowledge we have gained from the cognitive sciences. Substantive naturalism focuses on an asserted equality of facts of knowledge and natural facts. \nObjections to formative epistemology have targeted features of the general project as well as characteristics of specific versions. Some objectors suggest that natural scientific knowledge cannot be circularly grounded by the knowledge obtained through cognitive science, which is itself a natural science. This objection from circularity has been aimed specifically at strict replacement naturalism. There are similar challenges to substance naturalism that maintain that the substance naturalists' thesis that all facts of knowledge are natural facts is not only circular but fails to accommodate certain facts. Several other objectors have found fault in the inability of formative methods to adequately address questions about what value forms of potential knowledge have or lack. Formative epistemology is generally opposed to the anti-psychologism of Immanuel Kant, Gottlob Frege, Karl Popper and others.\n\nW. V. O. Quine's version of formative epistemology considers reasons for serious doubt about the fruitfulness of traditional philosophic study of scientific knowledge. These concerns are raised in light of the long attested incapacity of philosophers to find a satisfactory answer to the problems of radical scepticism, more particularly, to David Hume's criticism of induction. But also, because of the contemporaneous attempts and failures to reduce mathematics to pure logic by those in or philosophically sympathetic to The Vienna Circle. He concludes that studies of scientific knowledge concerned with meaning or truth fail to achieve the Cartesian goal of certainty. The failures in the reduction of mathematics to pure logic imply that scientific knowledge can at best be defined with the aid of less certain set-theoretic notions. Even if set theory's lacking the certainty of pure logic is deemed acceptable, the usefulness of constructing an encoding of scientific knowledge as logic and set theory is undermined by the inability to construct a useful translation from logic and set-theory back to scientific knowledge. If no translation between scientific knowledge and the logical structures can be constructed that works both ways, then the properties of the purely logical and set-theoretic constructions do not usefully inform understanding of scientific knowledge.\n\nOn Quine's account, attempts to pursue the traditional project of finding the meanings and truths of science philosophically have failed on their own terms and failed to offer any advantage over the more direct methods of psychology. Since traditional philosophic analysis of knowledge fails, those wishing to study knowledge ought to employ natural scientific methods. Scientific study of knowledge differs from philosophic study by focusing on how humans acquire knowledge rather than speculative analysis of knowledge. According to Quine, this appeal to science to ground the project of studying knowledge, which itself underlies science, should not be dismissed for its circularity since it is the best option available after ruling out traditional philosophic methods for their more serious flaws. This identification and tolerance of circularity is reflected elsewhere in Quine's works.\n\nCooperative formativism is a version of formative epistemology which states that while there are evaluative questions to pursue, the empirical results from psychology concerning how individuals actually think and reason are essential and useful for making progress in these evaluative questions. This form of naturalism says that our psychological and biological limitations and abilities are relevant to the study of human knowledge. Empirical work is relevant to epistemology but only if epistemology is itself as broad as the study of human knowledge.\n\nSubstantive naturalism is a form of formative epistemology that emphasizes how all epistemic facts are natural facts. Natural facts can be based on two main ideas. The first is that all natural facts include all facts that science would verify. The second is to provide a list of examples that consists of natural items. This will help in deducing what else can be included.\n\nQuine articulates the problem of circularity inherent in formative epistemology when it is treated as a replacement for traditional epistemology. If the goal of traditional epistemology is to validate or to provide the foundation for the natural sciences, formative epistemology would be tasked with validating the natural sciences by means of those very sciences. That is, an empirical investigation into the criteria which are used to scientifically evaluate evidence must presuppose those very same criteria. However, Quine points out that these thoughts of validation are merely a byproduct of traditional epistemology. Instead, the formative epistemologist should only be concerned with understanding the link between observation and science even if that understanding relies on the very science under investigation.\n\nIn order to understand the link between observation and science, Quine's formative epistemology must be able to identify and describe the process by which scientific knowledge is acquired. One form of this investigation is reliabilism which requires that a belief be the product of some reliable method if it is to be considered knowledge. Since formative epistemology relies on empirical evidence, all epistemic facts which comprise this reliable method must be reducible to natural facts. That is, all facts related to the process of understanding must be expressible in terms of natural facts. If this is not true, i.e. there are facts which cannot be expressed as natural facts, science would have no means of investigating them. In this vein, Roderick Chisholm argues that there are epistemic principles (or facts) which are necessary to knowledge acquisition, but may not be, themselves, natural facts. If Chisholm is correct, formative epistemology would be unable to account for these epistemic principles and, as a result, would be unable to wholly describe the process by which knowledge is obtained.\n\nBeyond Quine's own concerns and potential discrepancies between epistemic and natural facts, Hilary Putnam argues that the replacement of traditional epistemology with formative epistemology necessitates the elimination of the normative. But without the normative, there is no \"justification, rational acceptability [nor] warranted assertibility\". Ultimately, there is no \"true\" since any method for arriving at the truth was abandoned with the normative. All notions which would explain truth are only intelligible when the normative is presupposed. Moreover, for there to be \"thinkers\", there \"must be some kind of truth\"; otherwise, \"our thoughts aren't really about anything[...] there is no sense in which any thought is right or wrong\". Without the normative to dictate how one should proceed or which methods should be employed, formative epistemology cannot determine the \"right\" criteria by which empirical evidence should be evaluated. But these are precisely the issues which traditional epistemology has been tasked. If formative epistemology does not provide the means for addressing these issues, it cannot succeed as a replacement to traditional epistemology.\n\nJaegwon Kim, another critic of formative epistemology, further articulates the difficulty of removing the normative component. He notes that modern epistemology has been dominated by the concepts of justification and reliability. Kim explains that epistemology and knowledge are nearly eliminated in their common sense meanings without normative concepts such as these. These concepts are meant to engender the question \"What conditions must a belief meet if we are justified in accepting it as true?\". That is to say, what are the necessary criteria by which a particular belief can be declared as \"true\" (or, should it fail to meet these criteria, can we rightly infer its falsity)? This notion of truth rests solely on the conception and application of the criteria which are set forth in traditional and modern theories of epistemology.\n\nKim adds to this claim by explaining how the idea of \"justification\" is the only notion (among \"belief\" and \"truth\") which is the defining characteristic of an epistemological study. To remove this aspect is to alter the very meaning and goal of epistemology, whereby we are no longer discussing the study and acquisition of knowledge. Justification is what makes knowledge valuable and normative; without it what can rightly be said to be true or false? We are left with only descriptions of the processes by which we arrive at a belief. Kim realizes that Quine is moving epistemology into the realm of psychology, where Quine’s main interest is based on the sensory input-output relationship of an individual. This account can never establish an affirmable statement which can lead us to truth, since all statements without the normative are purely descriptive (which can never amount to knowledge). The vulgar allowance of any statement without discrimination as scientifically valid, though not true, makes Quine’s theory difficult to accept under any epistemic theory which requires truth as the object of knowledge.\n\nAs a result of these objections and others like them, most, including Quine in his later writings, have agreed that formative epistemology as a replacement may be too strong of a view. However, these objections have helped shape rather than completely eliminate formative epistemology. One product of these objections is cooperative naturalism which holds that empirical results are essential and useful to epistemology. That is, while traditional epistemology cannot be eliminated, neither can it succeed in its investigation of knowledge without empirical results from the natural sciences. In any case, Quinean Replacement Naturalism finds relatively few supporters.\n\n"}
{"id": "6410624", "url": "https://en.wikipedia.org/wiki?curid=6410624", "title": "Geostrophic current", "text": "Geostrophic current\n\nA geostrophic current is an oceanic flow in which the pressure gradient force is balanced by the Coriolis effect. The direction of geostrophic flow is parallel to the isobars, with the high pressure to the right of the flow in the Northern Hemisphere, and the high pressure to the left in the Southern Hemisphere. This concept is familiar from weather maps, whose isobars show the direction of geostrophic flow in the atmosphere. Geostrophic flow may be either barotropic or baroclinic. A geostrophic current may also be thought of as a rotating shallow water wave with a frequency of zero. The principle of geostrophy is useful to oceanographers because it allows them to infer ocean currents from measurements of the sea surface height (by combined satellite altimetry and gravimetry) or from vertical profiles of seawater density taken by ships or autonomous buoys. The major currents of the world's oceans, such as the Gulf Stream, the Kuroshio Current, the Agulhas Current, and the Antarctic Circumpolar Current, are all approximately in geostrophic balance and are examples of geostrophic currents.\n\nSea water naturally tends to move from a region of high pressure (or high sea level) to a region of low pressure (or low sea level). The force pushing the water towards the low pressure region is called the pressure gradient force. In a geostrophic flow, instead of water moving from a region of high pressure (or high sea level) to a region of low pressure (or low sea level), it moves along the lines of equal pressure (isobars). This occurs because the Earth is rotating. The rotation of the earth results in a \"force\" being felt by the water moving from the high to the low, known as Coriolis force. The Coriolis force acts at right angles to the flow, and when it balances the pressure gradient force, the resulting flow is known as geostrophic. \n\nAs stated above, the direction of flow is with the high pressure to the right of the flow in the Northern Hemisphere, and the high pressure to the left in the Southern Hemisphere. The direction of the flow depends on the hemisphere, because the direction of the Coriolis force is opposite in the different hemispheres.\n\nThe geostrophic equations are a simplified form of the Navier–Stokes equations in a rotating reference frame. In particular, it is assumed that there is no acceleration (steady-state), that there is no viscosity, and that the pressure is hydrostatic. The resulting balance is (Gill, 1982): \n\nwhere formula_3 is the Coriolis parameter, formula_4 is the density, formula_5 is the pressure and formula_6 are the velocities in the formula_7-directions respectively.\n\nOne special property of the geostrophic equations, is that they satisfy the steady-state version of the continuity equation. That is:\n\nThe equations governing a linear, rotating shallow water wave are:\n\nThe assumption of steady-state made above (no acceleration) is:\n\nAlternatively, we can assume a wave-like, periodic, dependence in time:\n\nIn this case, if we set formula_13, we have reverted to the geostrophic equations above. Thus a geostrophic current\ncan be thought of as a rotating shallow water wave with a frequency of zero.\n\n"}
{"id": "14245848", "url": "https://en.wikipedia.org/wiki?curid=14245848", "title": "Hugo Zapałowicz", "text": "Hugo Zapałowicz\n\nHugo Zapałowicz (November 15, 1852 in Ljubljana, Slovenia – November 20, 1917 in Perovsk [now Kyzylorda<nowiki>]</nowiki>, Kazakhstan) was a botanist, natural scientist, traveller, and military lawyer.\n\nZapałowicz was a pioneer researcher of flora and geological structure of the Carpathian Mountains. He was also a geological, geographical and flora researcher of South America and author of works about flora of Babia Góra-Babia Hora, Maramureş Mountains, Pokuttya.\n\nIn 1894 he became a member of the Academy of Learning.\n"}
{"id": "55755393", "url": "https://en.wikipedia.org/wiki?curid=55755393", "title": "Institute of Ecotechnics", "text": "Institute of Ecotechnics\n\nThe Institute of Ecotechnics is an educational, training and research charity with a special interest in ecotechnology, the environment, conservation, and heritage. With its U.K. headquarters in London, England and its U.S. in Santa Fe, NM, the institute was founded to \"develop and practice the discipline of ecotechnics: the ecology of technics, and the technics of ecology.\"\n\nEcotechnology is a newly-emerging applied science that deals with the relationship between humanity and the biosphere. It involves the use of technological means for ecosystem management. It seeks to fulfill human needs, based on a deep understanding of natural ecosystems, and minimizing disruption to those ecosystems.\n\nThe institute was founded and incorporated in New Mexico in 1973 and incorporated in the UK in 1985. Its charity number in England and Wales is 1081259, and its company number is 1940123. It is also a not-for-profit organization in the USA, number 74-3177755.\n\nThe Institute of Ecotechnics runs workshops, organizes conferences, and carries out ecological field research. It has developed agricultural, waste water and air purification, and biosphere technologies; and has published books on ecotechnics, ecological and cultural issues.\n\nThe ecological research vessel, \"Heraclitus\", which the institute owns was designed and built with personnel from its first ecological project in New Mexico. The ship is a unique blend of ancient and modern: ferro-cement for its hull and deck, Chinese junk sails supplemented with diesel engine.The \"Heraclitus\" has sailed the oceans for decades since its launch in Oakland, California in 1975. It has made twelve expeditions. Some of them were a three year round the world voyage through the tropics exploring the origins of human culture. It also sailed up the Amazon River conducting ethnobotanical collections in Peru and circumnavigated South America with an expedition to Antarctica. It collected corals off the Yucatan coast for the Biosphere 2 project and from 1996-2008 teamed with the Biosphere Foundation to map and study coral reef health at many locations in the Pacific and Indian oceans. Its most recent work was an oral history project among the port and sea people of the Mediterranean. The RV \"Heraclitus\" is in drydock in Roses, Spain undergoing a nearly complete rebuild after sailing some 270,000 miles.\n\nThe Institute is involved in a series of ecological demonstration projects, selected in challenging biomes facing ecological degradation and cultural conflict. These were chosen because conventional approaches do not work, requiring innovative, \"ecotechic\" approaches with the goals of ecological upgrade, economic sustainability and to develop the capabilities of I.E. members and associates. The projects, and the Heraclitus, are places where hands-on educational programsand volunteer opportunities are provided.\n\nSynergia Ranch, started in 1969, was the Institute's first ecological field project. There on 130 acres of desertifying high altitude sem-arid juniper/pinon grasslands, I.E. developed a program to restore the land, with the goal of creating an oasis in the desert. Over a thousand trees were planted, including 450 fruit trees, organic vegetable gardens and an extensive soil-building program using compost to restore lost fertility to an area which had been overgrazed and cleared for inappropriate broad-acre farming in the 1920s. Adobe buildings and a geodesic dome were built and artisan enterprises started including pottery, woodworking, iron working, clothing, leather. A construction firm build some three dozen adobe buildings in Santa Fe, contributing to a renaissance of traditional adobe architecture. The orchards and vegetable gardens are certified organic and in 2016 Synergia Ranch won the Good Earth award at the New Mexico Organic Agriculture conference in recognition of its land care. The ecotechnic life style approach was to balance by engaging in three lines of work: ecology, enterprise and theater. The Theater of All Possibilities was based at Synergia Ranch for over a decade.\n\nLes Marronniers near Aix-en-Provence, France, begun in 1976 is the ecotechnic field project in the Mediterranean biome. It has also served as a frequent location for the international conferences which I.E. organizes bringing together scientists, artists, engineers, managers and thinkers to engage on topics of broad interest. Les Marronniers has also revivified on 7 ha (17 acres) the mixed agriculture small holdings traditional in the area, with orchard, grapes, field crops, woodlot, domestic animals and art/sculpture workshops.\n\nBirdwood Downs, a 4300 acre property in the Kimberley region of NW Western Australia, is the Institute's tropical savannah biomic project. Established in 1978, its goals are to regenerate an overgrazed ecology dominated by invasive scrub trees with drought-resistant grasses and legumes and to demostrate ecotechnic ways of living within a sometimes harsh climate. Robyn Tredwell, a director of I.E., managed the project from 1985 until her death in 2012 and was honored by being selected as the 1995 Australian Rural Woman of the Year.\n\nRecognizing that cities, part of the anthropogenic urban biome, are crucial players in the biosphere, I.E. helped establish the October Gallery in London in 1979 as its world city project. The October Gallery's goal is to find and showcase cutting-edge artists of the \"transvangarde\" from around the world. It also supports a robust educational program bringing many London school children into the gallery to meet artists and make art, as well as traveling exhibitions. Designed as a place where science and art can interact, it also hosts scientific, cultural and musical events.\n\nIn 1983, the Las Casas de la Selva project in Patillas, Puerto Rico became I.E. tropical rainforest project. On almost 1000 acres of mountainous secondary forest, the goal of the project is to show innovative methods of forest enrichment and to promote sustainable tropical forestry. With the cooperation of Puerto Rican departments of development and forestry, some 40,000 seedlings of valuable timber species were planted on one-third of the land. Line-planting was employed to minimize the impact on the surrounding forest and to conserve biodiversity. Las Casas won the 2016 Energy Globe national award for Puerto Rico and is recovering from severe damage caused by Hurricane Maria in 2017. During the island-wide cleanup from the Hurricane, Las Casas management are organizing the rescue of valuable hardwood trees felled which would otherwise be sent to landfills and wasted.\n\nSpace Biosphere Ventures' Biosphere 2 is another project in which the Institute of Ecotechnics was involved. I.E. served as an ecological systems consultant and managed the organizing of scientific and design workshops both at Biosphere 2 and a series of International Workshops on Closed Ecological Systems and Biospherics held at the Royal Society in 1987, at the Institute of Biosphysics (Bios 3) in Krasnoyarsk, Siberia in 1989, at Biosphere 2 in 1992 and at the Linnean Society of London in 1996. An Earth system science research facility or closed ecological system in Oracle, Arizona, its inventor was Institute of Ecotechnics director John P. Allen, and bankrolled by Texas financier, Ed Bass, who also served as an I.E. for many years. At the time, the Biosphere 2 project was the subject of much media attention, and there were allegations that this was not science but a stunt, and that the project had been a failure. Some of the controversy may have stemmed from the radical goals of the project - to develop methods of harmonizing its technosphere and living systems including a non-polluting, non-toxic farming, a challenge to business as usual which is damaging the Earth's biosphere. Also it fell into the contentious divide between analytic (reductionist) and integrative (holistic) science though the project employed both to create the world's first mini biosphere. These allegations have been strenuously denied and refuted by some of those involved, including ecologist and author Mark Nelson, the chairman of the Institute of Ecotechnics and a Biosphere 2 crew member from 1991–1993, and Bill Dempster, the Biosphere 2 Director of Systems Engineering until 1994. A wealth of scientific results have been published from the early closure experiments, 1991-1994, including a compendium of research papers published in the journal Ecological Engineering in reprinted as an Elsevier book, \"Biosphere 2 Research Past and Present edited by H.T. Odum and Bruno Marino.\n\nThe Institute works in collaboration with the Biospherics Academy's \"Academia Biospherica\" program, which organizes workshops, lectures and events in the fields of restoration ecology, intercultural studies and the arts.\n\nPast projects include consulting on the Hotel Vajra in Kathmandu, Nepal, working with a community of Tibetan refugees to create a travelers' hotel where East and West can meet. In Ft. Worth, I.E. was involved in the development of a jazz club, cultural and arts complex, the Caravan of Dreams, to help revive the city center. I.E. was involved in the first Wastewater Garden (constructed wetlands) project in Algeria and in other applications of the ecotechnic technology, including a proposed project to protect the Marsh Arabs and their environment from sewage pollution.\n\n\n\n\n"}
{"id": "769395", "url": "https://en.wikipedia.org/wiki?curid=769395", "title": "International Ship and Port Facility Security Code", "text": "International Ship and Port Facility Security Code\n\nThe International Ship and Port Facility Security (ISPS) Code is an amendment to the Safety of Life at Sea (SOLAS) Convention (1974/1988) on minimum security arrangements for ships, ports and government agencies. Having come into force in 2004, it prescribes responsibilities to governments, shipping companies, shipboard personnel, and port/facility personnel to \"detect security threats and take preventative measures against security incidents affecting ships or port facilities used in international trade.\"\n\nThe International Maritime Organization (IMO) states that \"The International Ship and Port Facility Security Code (ISPS Code) is a comprehensive set of measures to enhance the security of ships and port facilities, developed in response to the perceived threats to ships and port facilities in the wake of the 9/11 attacks in the United States\" (IMO). \n\nDevelopment and implementation were sped up drastically in reaction to the September 11, 2001 attacks and the bombing of the French oil tanker \"Limburg\". The U.S. Coast Guard, as the lead agency in the United States delegation to the IMO, advocated for the measure. The Code was agreed at a meeting of the 108 signatories to the SOLAS convention in London in December 2002. The measures agreed under the Code were brought into force on July 1, 2004.\n\nISPS Code demands that every ship must have a Company Security Officer (CSO) that will work alongside the Ship Security Officer (SSO) for security purposes. The CSO takes data from the Ship Security Assessment or Vessel Security Assessment to advise on possible threats that could happen on the ship. He will ensure that the Ship Security Plan (SSP) is maintained in an efficient manner by the SSO.\n\nThe Ship Security Officer has full responsibility of the vessels security with the captain’s approval as stated in chapter XI-2/8. The SSO maintains the SSP and conducts regular security inspections to make sure that the appropriate security steps are always taken. The SSO also ensures that the security crew is trained for high security level purposes.\n\nThe ISPS Code states that it is the sole responsibility of the Company Security Officer (CSO) and Company to approve the Ship Security Officer (SSO). This process must be approved by the administration of the flag state of the ship or verified security organization with approval of the Ship Security Plan or Vessel Security Plan (VSP).The ISPS Code ensures that before the VSP is set in place that Vessel Security Assessments must be taken (VSA). The Vessel Security Plan must address every requirement in the Vessel Security Assessment. The VSP must establish a number of important roles and steps to provide safety for the marine vessel. Therefore, the VSP must include procedures to allow necessary communication that shall be enforced at all times. The VSP has to include procedures that assessed for the performance of daily security protocols. It also must include the assessment of security surveillance equipment systems to detect malfunctioning parts. ISPS code requires that the Vessel Security Plan must have strict procedure and practices for the vital protection of Sensitive Security Information (SSI) that is either in the form of electronic or paper. Observation of procedures has to include timed submissions, and assessments of security reports pertaining to heightened security concerns. ISPS code requests that the VSP maintain an updated inventory of dangerous or hazardous goods and substances that are carried aboard the ship. The location of the goods or substance must be stated in the inventory report .\n\nThe ISPS Code is implemented through chapter XI-2 Special measures to enhance maritime security in the International Convention for the Safety of Life at Sea (SOLAS).\n\nRegulation XI-2/3 ensures that administrations establish security levels and guarantee the provisions of strict security level data to ships that fly their flag.Ships that are prior to docking in port must immediately comply with all requirements for security levels that are determined by that contracting government. This also pertains to the security level that is established by the Administration for that ship.\n\nRegulation XI-2/6 makes sure that all ships are equipped with a security alarm system. The alarm system works from the ship to administration ashore with transmitted signals that are communicated via satellite. The advanced security alarm system shall send a signal indicating the ship name, location, and the security threat that the ship is undergoing. The ships alarm system may be activated from the navigation bridge by the captain without alarming the crew on-board .\n\nThe Regulation XI-2/8 establishes the main role of the Sea Master, which allows him to maintain order and conduct decisions for the sake of the personnel and security of the ship. Regulations XI-2/8 states that the Sea Master must not by challenged or withheld from completing his duties.\n\nThe Code is a two-part document describing minimum requirements for security of ships and ports. Part A provides mandatory requirements. Part B provides guidance for implementation. Some contracting governments have elected to also treat Part B as mandatory.\n\nThe ISPS Code applies to ships on international voyages (including passenger ships, cargo ships of and upwards, and mobile offshore drilling units) and the port facilities serving such ships. The Code does not apply to warships, naval auxiliaries or other ships owned or operated by a contracting government and used only on government non-commercial service.\n\nMaritime Security (MARSEC) levels were constructed for quick communication from the ship to the U.S Coast Guard for different levels of threats aboard or ashore. The three security levels listed below are introduced by the ISPS Code.\n\nMARSEC Level 1 is the normal level that the ship or port facility operates at on a daily basis. Level 1 ensures that security personnel maintain minimum appropriate security 24/7.\n\nMARSEC Level 2 is a heightened level for a time period during a security risk that has become visible to security personnel. Appropriate additional measures will be conducted during this security level.\n\nMARSEC Level 3 will include additional security measures for an incident that is forthcoming or has already occurred that must be maintained for a limited time frame. The security measure must be attended to although there might not be a specific target that has yet been identified.\n\nSecurity level 3 should be applied only when there is reliable information given for that particular security threat that is probable or at hand. Security level 3 must be set for a timed duration for the identified security incident. Although the security levels will change from security level 1 to security level 2 and to security level 3, it is highly possible for the security levels to change drastically from security level 1 to security level 3.\n\nEurope has enacted the International regulations with EC Regulation (EC) No 725/2004 of the European Parliament and of the Council of 31 March 2004, on enhancing ship and port facility security.\n\nThe UK has enacted The Ship and Port Facility (Security) Regulations 2004, (S.I.1495 of 2004) these bring the EU regulation 725/2004 into UK law.\n\nThe United States has issued regulations to enact the provisions of the Maritime Transportation Security Act of 2002 and to align domestic regulations with the maritime security standards of SOLAS and the ISPS Code. These regulations are found in Title 33 of the Code of Federal Regulations, Parts 101 through 107. Part 104 contains vessel security regulations, including some provisions that apply to foreign ships in U.S. waters.\n\n\n"}
{"id": "394815", "url": "https://en.wikipedia.org/wiki?curid=394815", "title": "Introduced species", "text": "Introduced species\n\nAn introduced species (alien species, exotic species, non-indigenous species, or non-native species) is a species living outside its native distributional range, but which has arrived there by human activity, either deliberate or accidental. Non-native species can have various effects on the local ecosystem. Introduced species that become established and spread beyond the place of introduction are called invasive species. The impact of introduced species is highly variable. Some have a negative effect on a local ecosystem, while other introduced species may have no negative effect or only minor impact. Some species have been introduced intentionally to combat pests. They are called biocontrols and may be regarded as beneficial as an alternative to pesticides in agriculture for example. In some instances the potential for being beneficial or detrimental in the long run remains unknown. \n\nThe effects of introduced species on natural environments have gained much scrutiny from scientists, governments, farmers and others.\n\nThe formal definition of an introduced species, from the United States Environmental Protection Agency, is \"A species that has been intentionally or inadvertently brought into a region or area. Also called an exotic or non-native species.\"\n\nThere are many terms associated with introduced species that represent subsets of introduced species, and the terminology associated with introduced species is now in flux for various reasons. Examples of these terms are \"acclimatized\", \"adventive\", \"naturalized\", and \"immigrant\" species but those terms refer to a subset of introduced species. The term \"invasive\" is used to describe introduced species when the introduced species causes substantial damage to the area in which it was introduced.\n\nSubset descriptions:\n\nGeneral description of introduced species:\n\nIn the broadest and most widely used sense, an introduced species is synonymous with \"non-native\" and therefore applies as well to most garden and farm organisms; these adequately fit the basic definition given above. However, some sources add to that basic definition \"and are now reproducing in the wild\", which removes from consideration as \"introduced\" species that were raised or grown in gardens or farms that do not survive without tending by people. With respect to plants, these latter are in this case defined as either \"ornamental\" or \"cultivated\" plants.\n\nIntroduction of a species outside its native range is all that is required to be qualified as an \"introduced species\" such that one can distinguish between introduced species that may not occur except in cultivation, under domestication or captivity whereas others become established outside their native range and reproduce without human assistance. Such species might be termed \"naturalized\", \"established\", \"wild non-native species\". If they further spread beyond the place of introduction and cause damage to nearby species, they are called \"invasive\". The transition from introduction, to establishment and to invasion has been described in the context of plants. Introduced species are essentially \"non-native\" species. Invasive species are those introduced species that spreadwidely or quickly and cause harm, be that to the environment, human health, other valued resources or the economy. There have been calls from scientists to consider a species \"invasive\" only in terms of their spread and reproduction rather than the harm they may cause.\n\nAccording to a practical definition, an invasive species is one that has been introduced and become a pest in its new location, spreading (invading) by natural means. The term is used to imply both a sense of urgency and actual or potential harm. For example, U.S. Executive Order 13112 (1999) defines \"invasive species\" as \"an alien species whose introduction does or is likely to cause economic or environmental harm or harm to human health\". The biological definition of invasive species, on the other hand, makes no reference to the harm they may cause, only to the fact that they spread beyond the area of original introduction.\n\nAlthough some argue that \"invasive\" is a loaded word and harm is difficult to define, the fact of the matter is that organisms have and continue to be introduced to areas in which they are not native, sometimes with but usually without much regard to the harm that could result.\n\nFrom a regulatory perspective, it is neither desirable nor practical to list as undesirable or outright ban all non-native species (although the State of Hawaii has adopted an approach that comes close to this). Regulations require a definitional distinction between non-natives that are deemed especially onerous and all others. Introduced \"pest species\" that are \"officially listed\" as invasive, best fit the definition of an \"invasive species\". Early detection and rapid response is the most effective strategy for regulating a pest species and reducing economic and environmental impacts of an introduction \n\nIn Great Britain, the Wildlife and Countryside Act 1981 prevents the introduction of any animal not naturally occurring in the wild or any of a list of both animals or plants introduced previously and proved to be invasive.\n\nBy definition, a species is considered \"introduced\" when its transport into an area outside of its native range is human mediated. Introductions by humans can be described as either intentional or accidental. Intentional introductions have been motivated by individuals or groups who either (1) believe that the newly introduced species will be in some way beneficial to humans in its new location or, (2) species are introduced intentionally but with no regard to the potential impact. Unintentional or accidental introductions are most often a byproduct of human movements, and are thus unbound to human motivations. Subsequent range expansion of introduced species may or may not involve human activity.\n\nSpecies that humans intentionally transport to new regions can subsequently become successfully established in two ways. In the first case, organisms are purposely released for establishment in the wild. It is sometimes difficult to predict whether a species will become established upon release, and if not initially successful, humans have made repeated introductions to improve the probability that the species will survive and eventually reproduce in the wild. In these cases it is clear that the introduction is directly facilitated by human desires.\n\nIn the second case, species intentionally transported into a new region may escape from captive or cultivated populations and subsequently establish independent breeding populations. Escaped organisms are included in this category because their initial transport to a new region is human motivated.\n\nEconomic: Perhaps the most common motivation for introducing a species into a new place is that of economic gain. Non-native species can become such a common part of an environment, culture, and even diet that little thought is given to their geographic origin. For example, soybeans, kiwi fruit, wheat, honey bees, and all livestock except the American bison and the turkey are non-native species to North America. Collectively, non-native crops and livestock comprise 98% of US food. These and other benefits from non-natives are so vast that, according to the Congressional Research Service, they probably exceed the costs. \n\nOther examples of species introduced for the purposes of benefiting agriculture, aquaculture or other economic activities are widespread. Eurasian carp was first introduced to the United States as a potential food source. The apple snail was released in Southeast Asia with the intent that it be used as a protein source, and subsequently to places like Hawaii to establish a food industry. In Alaska, foxes were introduced to many islands to create new populations for the fur trade. About twenty species of African and European dung beetles have established themselves in Australia after deliberate introduction by the Australian Dung Beetle Project in an effort to reduce the impact of livestock manure. The timber industry promoted the introduction of Monterey pine (\"Pinus radiata\") from California to Australia and New Zealand as a commercial timber crop. These examples represent only a small subsample of species that have been moved by humans for economic interests.\n\nThe rise in the use of genetically modified organisms has added another potential economic advantage to introducing new/modified species into different environments. Companies such as Monsanto that earn much of their profit through the selling of genetically modified seeds has added to the controversy surrounding introduced species. The effect of genetically modified organisms varies from organism to organism and is still being researched today, however the rise of genetically modified organisms has added complexity to the conversations surrounding introduced species.\n\nIntroductions have also been important in supporting recreation activities or otherwise increasing human enjoyment. Numerous fish and game animals have been introduced for the purposes of sport fishing and hunting (earthworms as invasive species). The introduced amphibian (\"Ambystoma tigrinum\") that threatens the endemic California salamander (\"Ambystoma californiense\") was introduced to California as a source of bait for fishermen. Pet animals have also been frequently transported into new areas by humans, and their escapes have resulted in several successful introductions, such as those of feral cats and parrots.\n\nMany plants have been introduced with the intent of aesthetically improving public recreation areas or private properties. The introduced Norway maple for example occupies a prominent status in many of Canada's parks. The transport of ornamental plants for landscaping use has and continues to be a source of many introductions. Some of these species have escaped horticultural control and become invasive. Notable examples include water hyacinth, salt cedar, and purple loosestrife\n\nIn other cases, species have been translocated for reasons of \"cultural nostalgia,\" which refers to instances in which humans who have migrated to new regions have intentionally brought with them familiar organisms. Famous examples include the introduction of starlings to North America by Englishman Eugene Schieffelin, a lover of the works of Shakespeare and the chairman of the American Acclimatization Society, who, it is rumoured, wanted to introduce all of the birds mentioned in Shakespeare's plays into the United States. He deliberately released eighty starlings into Central Park in New York City in 1890, and another forty in 1891.\n\nYet another prominent example of an introduced species that became invasive is the European rabbit in Australia. Thomas Austin, a British landowner had rabbits released on his estate in Victoria because he missed hunting them. A more recent example is the introduction of the common wall lizard to North America by a Cincinnati boy, George Rau, around 1950 after a family vacation to Italy.\n\nIntentional introductions have also been undertaken with the aim of ameliorating environmental problems. A number of fast spreading plants such as kudzu have been introduced as a means of erosion control. Other species have been introduced as biological control agents to control invasive species and involves the purposeful introduction of a natural enemy of the target species with the intention of reducing its numbers or controlling its spread.\n\nA special case of introduction is the reintroduction of a species that has become locally endangered or extinct, done in the interests of conservation. Examples of successful reintroductions include wolves to Yellowstone National Park in the U.S., and the red kite to parts of England and Scotland. Introductions or translocations of species have also been proposed in the interest of genetic conservation, which advocates the introduction of new individuals into genetically depauperate populations of endangered or threatened species.\n\nThe above examples highlight the intent of humans to introduce species as a means of incurring some benefit. While these benefits have in some cases been realized, introductions have also resulted in unforeseen costs, particularly when introduced species take on characteristics of invasive species.\n\nUnintentional introductions occur when species are transported by human vectors. Increasing rates of human travel are providing accelerating opportunities for species to be accidentally transported into areas in which they are not considered native. For example, three species of rat (the black, Norway and Polynesian) have spread to most of the world as hitchhikers on ships, and arachnids such as scorpions and exotic spiders are sometimes transported to areas far beyond their native range by riding in shipments of tropical fruit. There are also numerous examples of marine organisms being transported in ballast water, one being the zebra mussel. Over 200 species have been introduced to the San Francisco Bay in this manner making it the most heavily invaded estuary in the world. There is also the accidental release of the Africanized honey bees (AHB), known colloquially as \"killer bees\" or Africanized bee to Brazil in 1957 and the Asian carps to the United States. The insect commonly known as the brown marmorated stink bug (\"Halyomorpha halys\") was introduced accidentally in Pennsylvania. Another form of unintentional introductions is when an intentionally introduced plant carries a parasite or herbivore with it. Some become invasive, for example the oleander aphid, accidentally introduced with the ornamental plant, oleander.\n\nMost accidentally or intentionally introduced species do not become invasive as the ones mentioned above. For instance Some 179 coccinellid species have been introduced to the U.S. and Canada; about 27 of these non-native species have become established, and only a handful can be considered invasive, including the intentionally introduced \"Harmonia axyridis\", multicolored Asian lady beetle. However the small percentage of introduced species that become invasive can produce profound ecological changes. In North America \"Harmonia axyridis\" has become the most abundant lady beetle and probably accounts for more observations than all the native lady beetles put together.\n\nMany non-native plants have been introduced into new territories, initially as either ornamental plants or for erosion control, stock feed, or forestry. Whether an exotic will become an invasive species is seldom understood in the beginning, and many non-native ornamentals languish in the trade for years before suddenly naturalizing and becoming invasive.\n\nPeaches, for example, originated in China, and have been carried to much of the populated world. Tomatoes are native to the Andes. Squash (pumpkins), maize (corn), and tobacco are native to the Americas, but were introduced to the Old World. Many introduced species require continued human intervention to survive in the new environment. Others may become feral, but do not seriously compete with natives, but simply increase the biodiversity of the area.\n\nDandelions are also introduced species to North America.\n\nA very troublesome marine species in southern Europe is the seaweed \"Caulerpa taxifolia\". \"Caulerpa\" was first observed in the Mediterranean Sea in 1984, off the coast of Monaco. By 1997, it had covered some 50 km². It has a strong potential to overgrow natural biotopes, and represents a major risk for sublittoral ecosystems. The origin of the alga in the Mediterranean was thought to be either as a migration through the Suez Canal from the Red Sea, or as an accidental introduction from an aquarium. Another troublesome plant species is the terrestrial plant \"Phyla canescens,\" which was intentionally introduced into many countries in North America, Europe, and Africa as an ornamental plant. This species has become invasive in Australia, where it threatens native rare plants and causes erosion and soil slumping around river banks. It has also become invasive in France where it has been listed as an invasive plant species of concern in the Mediterranean region, where it can form monocultures that threaten critical conservation habitats.\n\nJapanese knotweed grows profusely in many nations. Human beings introduced it into many places in the 19th century. It is a source of resveratrol, a dietary supplement.\n\nBear in mind that most introduced species do not become invasive. Examples of introduced animals that have become invasive include the gypsy moth in eastern North America, the zebra mussel and alewife in the Great Lakes, the Canada goose and gray squirrel in Europe, the muskrat in Europe and Asia, the cane toad and red fox in Australia, nutria in North America, Eurasia, and Africa, and the common brushtail possum in New Zealand. In Taiwan, the success of introduced bird species was related to their native range size and body size; larger species with larger native range sizes were found to have larger introduced range sizes.\n\nOne notoriously devastating introduced species is the Small Indian Mongoose (Herpestes Javanicus Auropunctatus). Originating in a region encompassing Iran and India, it was introduced to the West Indies and Hawaii in the late 1800s for pest control. Since it has thrived on prey unequipped to deal with its speed, nearly leading to the local extinction of a variety of species.\n\nSome species, such as the brown rat, house sparrow, ring-necked pheasant and European starling, have been introduced very widely. In addition there are some agricultural and pet species that frequently become feral; these include rabbits, dogs, ducks, goats, fish, pigs and cats.\n\nWhen a new species is introduced, the species could potentially breed with members of native species, producing hybrids. The effect of the creating of hybrids can range from having little effect, a negative effect, to having devastating effects on native species. Potential negative effects include hybrids that are less fit for their environment resulting in a population decrease. This was seen in the Atlantic Salmon population when high levels of escape from Atlantic Salmon farms into the wild populations resulted in hybrids that had reduced survival. Potential positive effects include adding to the genetic diversity of the population which can increase the adaptation ability of the population and increase the number of healthy individuals within a population. This was seen in the introduction of guppies in Trinidad to encourage population growth and introduce new alleles into the population. The results of this introduction included increased levels of heterozygosity and a larger population size.\n\nIt has been hypothesized that invasive species of microbial life could contaminate a planetary body after the former is introduced by a space probe or spacecraft, either deliberately or unintentionally.\n\n"}
{"id": "13208425", "url": "https://en.wikipedia.org/wiki?curid=13208425", "title": "Izu-Bonin-Mariana Arc", "text": "Izu-Bonin-Mariana Arc\n\nThe Izu-Bonin-Mariana (IBM) arc system is a tectonic-plate convergent boundary. The IBM arc system extends over 2800 km south from Tokyo, Japan, to beyond Guam, and includes the Izu Islands, Bonin Islands, and Mariana Islands; much more of the IBM arc system is submerged below sealevel. The IBM arc system lies along the eastern margin of the Philippine Sea Plate in the Western Pacific Ocean. It is most famous for being the site of the deepest gash in Earth's solid surface, the Challenger Deep in the Mariana Trench.\n\nThe IBM arc system formed as a result of subduction of the western Pacific plate. The IBM arc system now subducts mid-Jurassic to Early Cretaceous lithosphere, with younger lithosphere in the north and older lithosphere in the south, including the oldest (~170 million years old, or Ma) oceanic crust. Subduction rates vary from ~2 cm (1 inch) per year in the south to 6 cm (~2.5 inches) in the north.\n\nThe volcanic islands that comprise these island arcs are thought to have been formed from the release of volatiles (steam from trapped water, and other gases) being released from the subducted plate, as it reached sufficient depth for the temperature to cause release of these materials. The associated trenches are formed as the oldest (most western) part of the Pacific plate crust increases in density with age, and because of this process finally reaches its lowest point just as it subducts under the crust to the west of it.\n\nThe IBM arc system is an excellent example of an intra-oceanic convergent margin (IOCM). IOCMs are built on oceanic crust and contrast fundamentally with island arc built on continental crust, such as Japan or the Andes. Because IOCM crust is thinner, denser, and more refractory than that beneath Andean-type margins, study of IOCM melts and fluids allows more confident assessment of mantle-to-crust fluxes and processes than is possible for Andean-type convergent margins. Because IOCMs are far removed from continents they are not affected by the large volume of alluvial and glacial sediments. The consequent thin sedimentary cover makes it much easier to study arc infrastructure and determine the mass and composition of subducted sediments. Active hydrothermal systems found on the submarine parts of IOCMs give us a chance to study how many of earth's important ore deposits formed.\n\nCrust and lithosphere produced by the IBM arc system during its ~50 Ma history are found today as far west as the Kyushu-Palau Ridge (just east of the West Philippine Sea Basin), up to 1,000 km from the present IBM trench. The IBM arc system is the surficial expression of the operation of a subduction zone and this defines its vertical extent. The northern boundary of the IBM arc system follows the Nankai Trough northeastward and onto southern Honshū, joining up with a complex system of thrusts that continue offshore eastward to the Japan Trench. The intersection of the IBM, Japan, and Sagami trenches at the Boso Triple Junction is the only trench-trench-trench triple junction on Earth. The IBM arc system is bounded on the east by a very deep trench, which ranges from almost 11 km deep in the Challenger Deep to less than 3 km where the Ogasawara Plateau enters the trench. The southern boundary is found where the IBM Trench meets the Kyushu-Palau Ridge near Belau. Thus defined, the IBM arc system spans over 25° of latitude, from 11°N to 35°20’N\n\nThe IBM arc system is part of the Philippine Sea Plate, at least to the first approximation. Although the IBM arc deforms internally – and in fact in the south a small plate known as the Mariana Plate is separated from the Philippine Sea Plate by a spreading ridge in the Mariana Trough – it is still useful to discuss approximate rates and directions of the Philippine Sea Plate with its lithospheric neighbors, because these define, to a first order, how rapidly and along what streamlines material is fed into the Subduction Factory. The Philippine Sea Plate (PH) has four neighboring plates: Pacific (PA), Eurasian (EU), North American (NA), and Caroline (CR). There is minor relative motion between PH and CR; furthermore, CR does not feed the IBM Subduction Factory, so it is not discussed further. The North American plate includes northern Japan, but relative motion between it and Eurasia is sufficiently small that relative motion between PH and EU explains the motion of interest. The Euler pole for PH-PA as inferred from the NUVEL-1A model for current plate motions lies about 8°N 137.3°E, near the southern end of the Philippine Sea Plate. PA rotates around this pole CCW ~1°/Ma with respect to PH. This means that relative to the southernmost IBM, PA is moving NW and being subducted at about 20–30 mm/y, whereas relative to the northernmost IBM, PA is moving WNW and twice as fast. At the south end of IBM, there is almost now convergence between the Caroline Plate and the Philippine Sea Plate.\nIt should be noted that the IBM arc is not experiencing trench ‘roll-back’, that is, the migration of the oceanic trench towards the ocean. The trench is moving towards Eurasia, although a strongly extensional regime is maintained in the IBM arc system because of rapid PH-EU convergence. The nearly vertical orientation of the subducted plate beneath southern IBM exerts a strong “sea-anchor” force that strongly resists its lateral motion. Back-arc basin spreading is thought to be due to the combined effects of the sea-anchor force and rapid PH-EU convergence . \nThe obliquity of convergence between PA and the IBM arc system change markedly along the IBM arc system. Plate convergence inferred from earthquake slip vectors is nearly strike-slip in the northernmost Marianas, adjacent to and south of the northern terminus of the Mariana Trough, where the arc has been ‘bowed-out’ by back-arc basin opening, resulting in a trench which strikes approximately parallel to the convergence vectors. Convergence is strongly oblique for most of the Mariana Arc system but is more nearly orthogonal for the southernmost Marianas and most of the Izu-Bonin segments. noted that the arc-parallel slip rate in the forearc reaches a maximum of 30 mm/yr in the northern Marianas. According to McCaffrey, this is fast enough to have produced geologically significant effects, such as unroofing of high-grade metamorphic rocks, and provides one explanation for why the forearc in southern IBM is tectonically more active than that in northern IBM.\n\nThe evolution of the IBM arc system is among the best known of any convergent margin. Because IBM has always been an arc system under strong extension, its components encompass a broad area, from the Palau-Kyushu Ridge to the IBM trench (see first-right figure). In general, the oldest components are farthest west, but a complete record of evolution is preserved in the forearc. The IBM subduction zone began as part of a hemispheric-scale foundering of old, dense lithosphere in the Western Pacific . The beginning of true subduction localized the magmatic arc close to its present position, about 200 km away from the trench, and allowed the sub-forearc mantle to stabilize and cool. The arc stabilized until about 30 Ma, when it began to rift to form the Parece Vela Basin. Spreading also began in the northernmost part of the IBM arc about 25 Ma and propagated south to form the Shikoku Basin. Parece Vela and Shikoku basin spreading systems met about 20 Ma and the combined Parece Vela Basin-Shikioku Basin continued widening until about 15 Ma, ultimately producing Earth's largest back-arc basin. The arc was disrupted during rifting but began to build again as a distinct magmatic system once seafloor spreading began. Arc volcanism, especially explosive volcanism, waned during much of this episode, with a resurgence beginning about 20 Ma in the south and about 17 Ma in the north. Tephra from northern and southern IBM show that strong compositional differences observed for the modern arc have existed over most of the arc's history, with northern IBM being more depleted and southern IBM being relatively enriched. About 15 Ma, the northernmost IBM began to collide with Honshū, probably as a result of new subduction along the Nankai Trough.\nA new episode of rifting to form the Mariana Trough back-arc basin began sometime after 10 Ma, with seafloor spreading beginning about 3–4 Ma. Because disruption of the arc is the first stage in forming any back-arc basin, the present Mariana arc volcanoes cannot be older than 3–4 Ma but the Izu-Bonin volcanoes could be as old as ~25 Ma. The Izu interarc rifts began to form about 2 Ma.\n\nThe three segments of IBM (figure to right) do not correspond to variations on the incoming plate. Boundaries are defined by the Sofugan Tectonic Line (~29°30’N) separating the Izu and Bonin segments, and by the northern end of the Mariana Trough back-arc basin (~23°N), that defines the boundary between the Bonin and Mariana segments. Forearc, active arc, and back arc are expressed differently on either side of these boundaries (see figure below).\nThe forearc is that part of the arc system between the trench and the magmatic front of the arc and includes uplifted sectors of the forearc situated near the magmatic front, sometimes called the ‘frontal arc’. The IBM forearc from Guam to Japan is about 200 km wide. Uplifted portions of the forearc, composed of Eocene igneous basement surmounted by reef terraces of Eocene and younger age, produce the island chain from Guam north to Ferdinand de Medinilla in the Marianas. Similarly, the Bonin or Ogasawara Islands are mostly composed of Eocene igneous rocks. There is no accretionary prism associated with the IBM forearc or trench.\n\nThe magmatic axis of the arc is well defined from Honshū to Guam. This ‘magmatic arc’ is often submarine, with volcanoes built on a submarine platform that lies between 1 and 4 km water depth. Volcanic islands are common in the Izu segment, including O-shima, Hachijojima, and Miyakejima. The Izu segment farther south also contains several submarine felsic calderas. The Izu arc segment is also punctuated by inter-arc rifts. The Bonin segment to the south of the Sofugan Tectonic Line contains mostly submarine volcanoes and also some that rise slightly above sealevel, such as Nishino-shima. The Bonin segment is characterized by a deep basin, the Ogasawara Trough, between the magmatic arc and the Bonin Islands forearc uplift. The highest elevations in the IBM arc (not including the Izu Peninsula, where IBM comes onshore in Japan) are found in the southern part of the Bonin segment, where the extinct volcanic islands of Minami Iwo Jima and Kita Iwo Jima rise to almost 1000 m above sealevel. The bathymetric high associated with magmatic arc of the Izu and Bonin segments is often referred to as the Shichito Ridge in Japanese publications, and the Bonins are often referred to as the Ogasawara Islands. Volcanoes erupting lavas of unusual composition – the shoshonitic province – are found in the transition between the Bonin and Mariana arc segments, including Iwo Jima. The magmatic arc in the Marianas is submarine to the north of Uracas, south of which the Mariana arc includes volcanic islands (from north to south): Asuncion, Maug, Agrigan, Pagan, Alamagan, Guguan, Sarigan, and Anatahan. Mariana volcanoes again becomes submarine south of Anatahan.\n\nThe back-arc regions of the three segments are quite different. The Izu segment is marked by several volcanic cross-chains which extend SW away from the magmatic front. The magmatically-starved Bonin arc segment has no back-arc basin, inter-arc rift, or rear-arc cross chains. The Mariana segment is characterized by an actively spreading back arc basin known as the Mariana Trough. The Mariana Trough shows marked variations along strike, with seafloor spreading south of 19°15’ and rifting farther north.\n\nThe IBM arc system southwest of Guam is markedly different from the region to the north. The forearc region is very narrow and the intersection of backarc basin spreading axis with the arc magmatic systems is complex.\n\nEverything on the Pacific plate that enters the IBM trench is subducted. The next section discusses some modifications of the lithosphere just prior to its descent and the age and composition of oceanic crust and sediments on the Pacific plate adjacent to the trench. In addition to subducted sediments and crust of the Pacific plate, there is also a very substantial volume of material from the overriding IBM forearc that is lost to the subduction zone by tectonic erosion .\n\nThe oceanic trench and the associated outer trench swell mark where Pacific Plate begins its descent into the IBM Subduction Zone. The IBM trench is where the Pacific Plate lithosphere begins to sink. The IBM trench is devoid of any significant sediment fill; the ~400 m or so thickness of sediments is completely subducted with the downgoing plate. The IBM outer trench swell rises to about 300 m above the surrounding seafloor just before the trench. The lithosphere that is about to descend into a trench starts to bend just outboard of the trench; the seafloor is elevated into a broad swell that is a few hundred meters high and referred to as the \"outer trench bulge\" or “outer trench rise”. The about-to-be subducted plate is highly faulted, allowing seawater to penetrate into the plate interior, where hydration of mantle peridotite may generate serpentinite. Serpentinite thus generated may carry water deep into the mantle as a result of subduction.\n\nThe Pacific plate subducts in the IBM trench, so understanding what is subducted beneath IBM requires understanding the history of the western Pacific. The IBM arc system subducts mid-Jurassic to Early Cretaceous lithosphere, with younger lithosphere in the north and older lithosphere in the south. It is not possible to directly know the composition of subducted materials presently being processed by the IBM Subduction Factory – what is now 130 km deep in the subduction zone entered the trench 4 – 10 million years ago. However, the composition of the western Pacific seafloor-oceanic crust – sediments, crust, and mantle lithosphere – varies sufficiently systematically that, to a first approximation, we can understand what is now being processed by studying what lies on the seafloor east of the IBM trench.\n\nThe Pacific plate seafloor east of the IBM arc system can be subdivided into a northern portion that is bathymetrically ‘smooth’ and a southern portion that is bathymetrically rugged, separated by the Ogasawara Plateau. These large-scale variations mark distinct geologic histories to the north and south. The featureless north is dominated by the Nadezhda Basin. In the south, crude alignments of seamounts, atolls, and islands define three great, WNW-ESE trending chains : the Marcus Island-Wake Island-Ogasawara Plateau, the Magellan Seamounts Chain, and the Caroline Islands Ridge. The first two chains formed by off-ridge volcanism during Cretaceous time, whereas the Caroline Islands chain formed over the past 20 million years. Two important basins lie between these chains: the Pigafetta Basin lies between the Marcus-Wake and Magellan chains, and the East Mariana Basin lies between the Magellan and Caroline chains.\n\nThe age of Western Pacific seafloor has been interpreted from seafloor magnetic anomalies correlated to the geomagnetic reversal timescale and confirmed by Ocean Drilling Program scientific drilling. Three major sets of magnetic anomalies have been identified in the area of interest. Each of these lineation sets comprises M-series (mid-Jurassic to mid-Cretaceous) magnetic anomalies that are essentially \"growth rings\" of the Pacific Plate. These anomaly sets indicate that the small, roughly triangular Pacific plate grew by spreading along three ridges . The oldest identifiable lineations are M33 to M35 or perhaps even M38 . It is difficult to say how old these lineations and the older crust might be; the oldest magnetic lineations for which ages have been assigned are M29 (157 Ma; . Magnetic lineations as old as M29 are not known from other oceans, and the area in the Western Pacific that lies inside the M29 lineation – that is, crust older than M29 – is on the order of 3x106 km, about a third of the size of the United States. ODP site 801 lies on seafloor that isconsiderably older than M29 and the MORB basement there yields Ar-Ar ages of 167±5 Ma . The oldest sediments at site 801C are middle Jurassic, Callovian or latest Bathonian (~162 Ma; ).\n\nSeafloor spreading in the Pacific during the Cretaceous evolved from a more E-W 'Tethyan' orientation to the modern N-S trend. This occurred during mid-Cretaceous time, a ~35–40 Ma interval characterized by a lack of magnetic reversals known as the Cretaceous Superchron or Quiet Zone. Subsequently, the location of N-S trending spreading ridges relative to the Pacific Basin migrated progressively to the east throughout Cretaceous and Tertiary time, resulting in the present marked asymmetry of the Pacific, with very young seafloor in the Eastern Pacific and very old seafloor in the Western Pacific.\n\nSediments being delivered to the IBM trench are not thick considering that this some of Earth's oldest seafloor. Away from seamounts, the pelagic sequence is dominated by chert and pelagic clay, with little carbonate. Carbonates are important near guyots, common in the southern part of the region. Cenozoic sediments are unimportant except for volcanic ash and Asian loess deposited adjacent to Japan and carbonate sediment]]s associated with the relatively shallow Caroline Ridge and Caroline plate. Strong seafloor currents are probably responsible for this erosion or non-deposition.\n\nThe compositions of sediments being subducted beneath the northern and southern parts of the IBM arc are significantly different, because of the Cretaceous off-ridge volcanic succession in the south that is missing in the north. Lavas and volcaniclastics associated with an intense episode of intraplate volcanism correspond in time closely to the Cretaceous Superchron. Off-ridge volcanism became increasingly important approaching the Ontong-Java Plateau. There are 100–400 m thick tholeiitic sills in the East Mariana Basin and Pigafetta Basin , and at least 650 m of tholeiitic flows and sills in the Nauru Basin, near ODP Site 462. suggest that this province may reflect the formation of a mid-Cretaceous spreading system in the Nauru and East Mariana basins. Farther north, deposits related to this episode consist of thick sequences of Aptian-Albian volcaniclastic turbidites shed from emerging volcanic islands, such as preserved at DSDP site 585 and ODP sites 800 and 801. A few hundred meters of volcaniclastic deposits probably characterizes the sedimentary succession in and around the East Mariana and Pigafetta basins. Farther north, at DSDP sites 196 and 307 and ODP site 1149, there is little evidence of mid-Cretaceous volcanic activity. It appears that the Aptian-Albian volcanic episode was largely restricted to the region south of present 20°N latitude. Paleomagnetic and plate kinematic considerations place this broad region of off-ridge volcanism in the present vicinity of Polynesia, where today off-ridge volcanism, shallow bathymetry, and thin lithosphere is known as the 'Superswell' (; ).\n\nThe figure above shows the typical sediments drilled at Ocean Drilling Program site 1149, east of the Izu-Bonin segment. The sediments drilled at ODP site 1149 are about 400 m thick and are as old as 134 million years. The sedimentary section is a typical pelagic stratigraphy, accumulated mostly in the Cretaceous but also in the last 7 million years (late Neogene) built on a basement of Early Cretaceous oceanic crust. The lowermost portion is carbonate and chert, the next layer is very chert-rich, the third layer is clay-rich. This is followed by a long depositional hiatus before sedimentation resumes ~6.5 Ma (Late Miocene), with deposition of volcanic ash, clay, and windblown dust. The stratigraphy east of the Mariana segment differs from that being subducted beneath the Izu-Bonin segment in having a much greater abundance of Early Cretaceous intra-plate volcanics and flood basalts.\nAbout 470m of oceanic crust was penetrated at ODP site 801C during Legs 129 and 185. These are typical mid-ocean ridge basalt that were affected by low-temperature hydrothermal alteration. This crust is overlain by a 3 m thick, bright yellow hydrothermal deposit and about 60 m of alkali olivine basalt, 157.4±0.5 Ma old .\n\nThe deep structure of the IBM system has been imaged using a variety of geophysical techniques. This section provides an overview of these data, including a discussion of mantle structure at depths >200 km.\n\nSpatial patterns of seismicity are essential for locating and understanding the morphology and rheology of subducting lithospheric slabs, and this is particularly true for the IBM Wadati–Benioff zone (WBZ). first outlined the most important features of the IBM WBZ. Their study detected a zone of deep earthquakes beneath the southern Marianas and provided some of the first constraints on the deep, vertical nature of subducting Pacific lithosphere beneath southern IBM. They also found a region of reduced shallow seismicity (≤70 km) and an absence of deep (≥ 300 km) events beneath the Volcano Islands adjacent to the junction of the Izu Bonin and Mariana trenches, where the trench trends nearly parallel to the convergence vector.\n\nMore recently, provided an earthquake catalog containing improved locations (Figure 10). This data set shows that, beneath northern IBM, the dip of the WBZ steepens smoothly from ~40° to ~80° southwards, and seismicity diminishes between depths of ~150 km and ~300 km (Figures 11a c). The subducted slab beneath central IBM (near 25°N; Fig. 11c) is delineated by reduced seismic activity that nevertheless defines a more vertical orientation that persists southward (Figures 11d f).\nDeep earthquakes, here defined as seismic events ≥300 km deep, are common beneath parts of the IBM arc system (Figures 10, 11). Deep events in the IBM system are less frequent than for most other subduction zones with deep seismicity, such as Tonga/Fiji/Kermadec and South America. Beneath northern IBM, deep seismicity extends southward to ~27.5°N, and a small pocket of events between 275 km and 325 km depth exists at ~22°N. There is narrow band of deep earthquakes beneath southern IBM between ~21°N and ~17°N, but south of this there are extremely few deep events. Although early studies assumed that seismicity demarcated the upper boundary of the slab, more recent evidence has shown that many of these earthquakes occur within the slab. For instance, a study by showed that a region of events beneath northernmost IBM region occur ~20 km beneath the top of the subducting plate. They propose that transformational faulting, which occurs when metastable olivine changes to a more compact spinel structure, produces this zone of seismicity. Indeed, the faulting mechanism for deep earthquakes is a hotly debated topic (e.g., ), and has yet to be resolved. \nDouble seismic zones (DSZs) have been detected in several parts of the IBM subduction zone, but their locations within the slab as well as interpretations for their existence vary dramatically. Beneath southern IBM, found a DSZ lying 80 km and 120 km deep, with the two zones separated by 30 35 km. Earthquake focal mechanisms indicate that the upper zone, where most events occur, is in downdip compression, while the lower zone is in downdip extension. This DSZ is located at a depth where the curvature of slab is greatest; at greater depths it unbends into a more planar donfiguration. suggested that unbending or thermal stresses in the upper 150 km of the slab may the primary cause of the seismicity. For northern IBM, used a refined earthquake relocation scheme to detect a DSZ between depths of 300 km and 400 km, which also has a spacing of 30 35 km between the upper and lower zones. They interpreted data from S to P converted phases and thermal modeling to propose that the DSZ results from transformational faulting of a metastable olivine wedge in the slab. Recent work suggests that compositional variations in the subducting slab may also contribute to double seismic zone , or that DSZs represent the locus of serpentine dehydration in the slab .\n\nBathymetry of the Mariana arc region , showing all 51 edifices presently named along the volcanic front between 12°30’N and 23°10’N. Hydrothermally or volcanically active submarine edifices are labeled red; active subaerial edifices are labeled green. Inactive submarine and subaerial edifices are labeled in smaller black and green font, respectively. For all edifices, caldera labels are in bold italics. Black circles (20 km diameter) identify those volcanic centers composed of multiple individual edifices. Solid red line is the backarc spreading center.\n\n identified 76 volcanic edifices along 1370 km of the Mariana arc, grouped into 60 ‘‘volcanic centers,’’ of which at least 26 (20 submarine) are hydrothermally or volcanically active. The overall volcanic center density is 4.4/100 km of arc, and that of active centers is 1.9/100 km. Active volcanoes lie 80 to 230 km above the subducting Pacific plate, and ~25% lie behind the arc magmatic front. There is no evidence for a regular spacing of volcanoes along the Mariana arc. The frequency distribution of volcano spacing along the arc magmatic front peaks between 20 and 30 km and shows the asymmetric, long-tail shape typical for many other arcs. The first global compilation of arc volcanoes using recent bathymetric data estimated that arcs that are at least partially submarine have a population of almost 700 volcanoes, of which at least 200 are submerged .\n\n estimated that intraoceanic arcs combined may contribute hydrothermal emissions equal to ~10% of that from the global mid-ocean ridge system.\n\nGuam in the southern IBM arc system is where Magellan first landed after his epic crossing of the Pacific Ocean in 1521.\nThe Bonin Islands were a significant stop for water and supplies for New England whaling during the early 19th century. At that time they were known as the Peel Islands.\n\nTerrible battles were fought on the islands of Saipan and Iwo Jima in 1944 and 1945; many young Japanese and American soldiers died in these battles.\nGeorge H. W. Bush was shot down in 1945 near Chichijima in the Bonin Islands.\nTwelve Japanese seamen were stranded in June 1944 on volcanic Anatahan for seven years, along with the overseer of the abandoned plantation and an attractive young Japanese woman. The novel and 1953 movie Anatahan is based on these events.\nThe B-29 bomber Enola Gay flew from Tinian to drop the first atomic bomb on Hiroshima in 1945.\nSergeant Shoichi Yokoi hid out in the wilds of Guam for 28 years before coming out of hiding in 1972.\nThe Brown tree snake was accidentally introduced during World War II and has since devastated native birds on Guam.\n\n\n\n"}
{"id": "6647339", "url": "https://en.wikipedia.org/wiki?curid=6647339", "title": "Jebel al-Madhbah", "text": "Jebel al-Madhbah\n\nJebel al-Madhbah (, \"\") is a mountain at Petra, in present-day Jordan, which a number of scholars have proposed as the Biblical Mount Sinai, beginning with Ditlef Nielsen in 1927. The top of the original peak was carved away destroying evidence of whatever earlier structures had been located there. \n\nThe name \"Jebel al-Madbah\" means \"mountain of the altar\", and is well deserved since its summit is covered in rock-excavated ceremonial structures reached by a rock staircase. The French historian Maurice Sartre noted that the peak \"consists of a vast rectangular esplanade hollowed out in such a way that the sides formed benches; in the middle of one long side, a natural podium (motab) was set aside for placing the gods’ sacred stones. Another section was reserved for the altar. Cisterns, fed by rainwater, were used for ablutions and cleaning. Beneath this, two gigantic obelisks, carved out of the rocky mass, appear as sacred stones.\"\nThe mountain is over a thousand metres high, and a rock staircase winds its way from the top down to the valley below; the valley in which Petra resides is known as \"Wadi Musa\", meaning \"valley of Moses\". At the entrance to Wadi Musa is Ain Musa, the \"Spring of Moses\"; the 13th-century Arab chronicler Numairi stated that Ain Musa is identical with Meribah, the location where Moses has brought water from the ground by striking it with his staff.\n\n"}
{"id": "6750482", "url": "https://en.wikipedia.org/wiki?curid=6750482", "title": "Juan de Fuca Ridge", "text": "Juan de Fuca Ridge\n\nThe Juan de Fuca Ridge is a mid-ocean spreading center and divergent plate boundary located off the coast of the Pacific Northwest region of North America. The ridge separates the Pacific Plate to the west and the Juan de Fuca Plate to the east. It runs generally northward, with a length of approximately 500 kilometers (300 miles). The ridge is a section of what remains from the larger Pacific-Farallon Ridge which used to be the primary spreading center of this region, driving the Farallon Plate underneath the North American Plate through the process of plate tectonics. Today, the Juan de Fuca Ridge pushes the Juan de Fuca Plate underneath the North American plate, forming the Cascadia Subduction Zone.\n\nThe first indications of a submarine ridge off the coast of the Pacific Northwest was discovered by the USS Tuscarora, a United States Navy sloop under the command of George Belknap, in 1874. Surveying a route for an undersea cable between the United States and Japan, the USS Tuscarora discovered a submarine mountain range approximately 200 miles from Cape Flattery, which they did not consider a major discovery because throughout their voyage they found other locations with a larger profile, making the ridge seem insignificant in comparison.\n\nThe Juan de Fuca Ridge was at one point a part of the larger Pacific-Farallon ridge system. Approximately 30 million years ago, the Farallon Plate, being driven outwards by the Pacific-Farallon ridge, was pushed underneath the North American Plate, splitting what remained into the Juan de Fuca Plate to the North and the Cocos Plate and Nazca Plate to the South.\n\nAxial Seamount is a submarine volcano located on the ridge at a depth of 1400m below sea level, rising 700m above the average ridge height. Axial is the most active volcano in the northeastern Pacific basin, and an underwater cabled observatory has been installed there as a part of the National Science Foundation's Ocean Observatories Initiative, making it one of the best studied volcanoes along mid-ocean ridges globally.\n\nThe Endeavour segment in the northern end of the ridge is another active and highly studied region. Sharp chemical and thermal contrasts, high levels of seismic activity, dense biological communities, and unique hydrothermal systems all make the segment a primary focus of research.\n\nSome of the most intense and most active Hydrothermal vents are located along the Endeavour segment, with more than 800 individual known chimneys within the ridge's central region, and a total of five major hydrothermal fields along the ridge. These chimneys release large amounts of sulphur-rich minerals into the water, which allow bacteria to oxidize organic compounds and metabolize anaerobically. This allows for a diverse ecosystem of organisms to exist in the low-oxygen conditions near the seafloor around the ridge.\n\nThe first documented eruption on the Juan de Fuca Ridge took place on the Cleft segment in 1986 and 1987. Hydrothermal megaplumes indicated a large rifting event, releasing hydrothermal fluids as a result of lavas being extruded from a dike. A majority of the eruptions along the ridge are dike injection events, where molten rock is extruded between cracks in the crust's sheeted dike layer. Typically eruptive events can be predicted, as they are preceded by large earthquake swarms in the region.\n\nA significant event took place in June 1993, lasting 24 days at the CoAxial segment. Cruises deployed as a result of the eruption sampled event plumes, cooling lava flows, and discovered microbial communities living on the seafloor around the ridge.\n\nIn February 1996, an event consisting of 4,093 earthquakes, lasting 34 days was recorded at the Axial Volcano, yielding similar scientific results to the 1993 eruption.\n\nIn January 1998 an event consisting of 8,247 earthquakes lasted 11 days at Axial Seamount. Lava was released from the caldera of the volcano, flowing down the southern side of the mountain, creating a sheet flow over 3 km long and 800m wide. This was the first time an underwater eruption had been monitored in-situ in real-time.\n\nIn June 1999, 1,863 earthquakes were recorded over 5 days, and a hydrothermal temperature increase was observed at the Main Endeavour segment.\n\nIn September 2001, 14,215 earthquakes were detected over a 25-day period in the Middle Valley segment.\n\nResearchers at Oregon State University suggested the Axial Seamount had an eruption interval of approximately 16 years, which would place the next major Axial eruption in 2014. In 2011, during a dive on the seamount, new lava flows were discovered and some instruments had been buried in lava flows, indicating the volcano had erupted since the last expedition to the ridge. This is considered the first successful forecast of a seamount eruption. The caldera floor dropped by more than 2 meters after the eruption, and the rate at which it inflates as Axial's magma chamber refills can be used to once again predict the next eruption.\n\nThe ridge is a medium rate spreading center, moving outwards at a rate of approximately 6 centimeters per year. Tectonic activity along the ridge is monitored primarily with the U.S. Navy's Sound Surveillance System (SOSUS) array of hydrophones, allowing for real time detection of earthquakes and eruptive events.\n\nThe Juan de Fuca Plate is being pushed east underneath the North American Plate, forming what is known as the Cascadia subduction zone off the coast of the Pacific Northwest. The plate does not subduct smoothly and can become 'locked' with the North American plate. When this happens, strain builds up until the contact suddenly slips, triggering massive earthquakes up to or greater than magnitude 9. Major earthquakes along this zone occur on average every 550 years and can have major impacts on the physical structure of the North American continent and seafloor.\n\n\n"}
{"id": "55079452", "url": "https://en.wikipedia.org/wiki?curid=55079452", "title": "Le Regourdou", "text": "Le Regourdou\n\nLe Regourdou (or Le Régourdou) is a Neanderthal archaeological site in the Dordogne department, France. It was a cave before its ceiling collapsed. It is close to the Upper Palaeolithic site of Lascaux.\n\nThe site of the Regourdou was discovered by chance in 1954 by its owner, Roger Constant. He had seen water being swallowed through a hole into the earth in his farmyard, and chose to enlarge in the hope of discovering the natural entry to the nearby cave of Lascaux. This cave had been discovered in September 1940, shortly after the Battle of France, when a storm uprooted a tree that lay above the entrance, thereby provoking the collapse of part of its vault, opening the current artificial entrance. Constant, having seen a similar collapse in front of his house, considered that there was likely a relationship with the Lascaux cave. He started to dig on his own time without official approval. On 22 September 1957, he discovered a Neanderthal mandible.\n\nConstant immediately contacted François Bordes, director of the Aquitaine prehistoric antiquities. He came with Eugène Bonifay with whom he was working at Laugerie-Haute. Bonifay began excavating at Le Regourdou and discovered what he considered a Neanderthal burial and a brown bear burial. Like all other Neanderthal alleged burials, this has been questioned.\n\nRenewed excavations in 2011 discovered the pelvic remains of Le Regourdou.\n\nApart from a Neanderthal skeleton discovered in 1957, the excavator of the Regourdou, Eugène Bonifay, found a large number of brown bear bones. These were interpreted as evidence of a \"cult of the bear in the Middle Palaeolithic. This interpretation was seriously questioned given that an occupation of the cave by bears followed by taphonomic phenomena could reasonably be expected to produce what was observed.\n\nThose that claim a burial at the Regourdou have difficulty explaining why it has no head and no lower limbs. The rest of the skeleton is remarkably well preserved. It preserves the mandible, part of the upper limbs, and the sternum. It is often used in Neanderthal comparison studies. It is the most robust Neandertal skeleton ever found and dates between 70,000 and 50,000 BP.\n\nThe shaft cave is the second site that Roger Constant explored. He started digging there in 1970, a few metres from the main site. He used a crane and wagons to extract dirt from the earth down to a depth of 35 metres. He dug there all his life, hoping to reach the cave of Lascaux.\n"}
{"id": "22859885", "url": "https://en.wikipedia.org/wiki?curid=22859885", "title": "Leaching model (soil)", "text": "Leaching model (soil)\n\nA leaching model is a hydrological model by which the leaching with irrigation water of dissolved substances, notably salt, in the soil is described depending on the hydrological regime and the soil's properties. <br>\nThe model may describe the process (1) in time and (2) as a function of amount of water applied. <br>\nLeaching is often done to \"reclaim\" saline soil or to \"conserve\" a favorable salt content of the soil of irrigated land as all irrigation water contains salts.\n\nThe leaching process in a salty soil to be reclaimed is illustrated in the leaching curves of figure 1, derived from data of the Chacupe pilot area, Peru. It shows the soil salinity in terms of electrical conductivity (EC) of the soil solution with respect its initial value (ECi) as a function of amount of water percolating through the soil. The top-soil leaches quickly. The salinity of the deeper soil first increases due to the salts leached from the top-soil, but later it also decreases.\n\nOwing to irregular distribution of salt in the soil or to irregularity of the soil structure (figure 2), the \"leaching efficiency\" (E) can be different from unity.\n\nSoils with a low leaching efficiency are difficult to reclaim. In the Tagus delta, Portugal, the leaching efficiency of the dense clay soil was found as low as 0.10 to 0.15. The soil could not be developed for intensive agriculture and was used for rearing of bulls in coarse natural pasture.\n\nThe clay soil in the Nile delta, Egypt, on the other hand has a much better leaching efficiency of 0.7 to 0.8. In figure 3, leaching curves are shown for different leaching efficiencies, as assumed in the \"leaching model\" SaltMod with data from the Mashtul pilot area. The observed values of soil salinity correspond best to a leaching efficiency of about 0.75. The figure illustrates the calibration process of leaching efficiency, which parameter is difficult to measure directly.\n\nThe \"leaching requirement\" may refer to:\n\nThe downward limb of the leaching curves, as in figure 3, can be described with the leaching equation:\nwhere C = salt concentration, Ct = C in the soil at time T, Co = C in the soil at time T=0, Ci = C of the irrigation water, E = leaching efficiency, Qp = average percolation rate through the soil, and Ws = water stored in the soil at field saturation.\n\nTo conserve an acceptable salt balance of the soil in accordance with the salt tolerance of the crops to be grown, the leaching fraction must be at least:\nwhere Ci = salt concentration of the irrigation water, and Cs is the acceptable salt concentration of the soil moisture at field capacity in accordance with the salt tolerance of the crops to be grown.\n\n"}
{"id": "54840564", "url": "https://en.wikipedia.org/wiki?curid=54840564", "title": "List of Local Nature Reserves in Leicestershire", "text": "List of Local Nature Reserves in Leicestershire\n\nLeicestershire is a county in the East Midlands of England. The area of the administrative county is and the population according to the 2011 census is 980,000. Leicester City Council is a unitary authority, and the rest of the county is administered by Leicestershire County Council at the top level, with seven district councils in the second tier, Blaby, Charnwood, Harborough, Hinckley and Bosworth, Melton, North West Leicestershire and Oadby and Wigston.\n\nLocal Nature Reserves (LNRs) are designated by local authorities under the National Parks and Access to the Countryside Act 1949. The local authority must have a legal control over the site, by owning or leasing it or having an agreement with the owner. LNRs are sites which have a special local interest either biologically or geologically, and local authorities have a duty to care for them. They can apply local bye-laws to manage and protect LNRs.\n\nAs of December 2017, there are 23 Local Nature Reserves in the county. The largest is Burbage Common and Woods at , where over 300 species of flowering plants, 250 of fungi, 100 of birds, 20 of butterflies and 15 of damselflies and dragonflies have been recorded. The smallest is Lucas Marsh with , a former quarry which is part of Brock's Hill Country Park. All sites are open to the public, apart from Knighton Spinney, which is only accessible on occasional open days.\n\n"}
{"id": "57470395", "url": "https://en.wikipedia.org/wiki?curid=57470395", "title": "List of Local Nature Reserves in Norfolk", "text": "List of Local Nature Reserves in Norfolk\n\nNorfolk is a county in East Anglia. It has an area of and a population as of mid-2017 of 898,400. The top level of local government is Norfolk County Council with seven second tier councils: Breckland District Council, Broadland District Council, Great Yarmouth Borough Council, King's Lynn and West Norfolk Borough Council, North Norfolk District Council, Norwich City Council and South Norfolk District Council. The county is bounded by Cambridgeshire, Suffolk, Lincolnshire and the North Sea.\n\nLocal Nature Reserves (LNRs) are designated by local authorities under the National Parks and Access to the Countryside Act 1949. The local authority must have a legal control over the site, by owning or leasing it or having an agreement with the owner. Local Nature Reserves are sites which have a special local interest either biologically or geologically. Local authorities have a duty to care for them, and can apply local bye-laws to manage and protect them.\n\nAs of October 2018, there are 27 LNRs in Norfolk, seven of which are Sites of Special Scientific Interest, three are Special Areas of Conservation, three are Special Protection Areas, one is a Ramsar site, one is a Geological Conservation Review site, one is a Nature Conservation Review site, one is a Scheduled Monument, two are managed by the Norfolk Wildlife Trust and one by the Suffolk Wildlife Trust.\n\n\n\n"}
{"id": "9132763", "url": "https://en.wikipedia.org/wiki?curid=9132763", "title": "List of Radermachera sinica diseases", "text": "List of Radermachera sinica diseases\n\nThis is a list of diseases of \"Radermachera sinica\" (China doll).\n\n"}
{"id": "5880352", "url": "https://en.wikipedia.org/wiki?curid=5880352", "title": "List of Sites of Special Scientific Interest in Hertfordshire", "text": "List of Sites of Special Scientific Interest in Hertfordshire\n\nHertfordshire is a county in eastern England. It is bordered by Bedfordshire to the north, Cambridgeshire to the north-east, Essex to the east, Buckinghamshire to the west and Greater London to the south. The county town is Hertford. As of June 2014, the county has a population of 1,154,800 in an area of .\n\nAs of January 2016, there are 43 sites designated within this Area of Search, 36 of which have been designated for their biological interest and 7 for their geological interest. In England the body responsible for designating SSSIs is Natural England, which chooses a site \"because of its flora, fauna, or geological or physiographical features\".\n\nThe data in the table is taken from Natural England's website in the form of citation sheets for each SSSI.\n\n\n"}
{"id": "5858652", "url": "https://en.wikipedia.org/wiki?curid=5858652", "title": "List of Sites of Special Scientific Interest in Northamptonshire", "text": "List of Sites of Special Scientific Interest in Northamptonshire\n\nNorthamptonshire is a county in the East Midlands of England. It has an area of and a population estimated in mid-2016 at 733,000. The county is bordered by Warwickshire, Leicestershire, Cambridgeshire, Bedfordshire, Buckinghamshire, Oxfordshire, Rutland and Lincolnshire. It is governed by Northamptonshire County Council and seven district and borough councils, Corby, Daventry, East Northamptonshire, Kettering, Northampton, South Northamptonshire and Wellingborough. The county flower is the cowslip.\n\nA ridge of low Jurassic hills runs through the county, separating the basins of the Welland and Nene rivers. The county has good communications as it is crossed by two main railway lines and the M1 motorway, and has many small industrial centres rather than large conurbations. The main architectural feature is its country houses and mansions.\n\nIn England, Sites of Special Scientific Interest (SSSIs) are designated by Natural England, which is responsible for protecting England's natural environment. Designation as an SSSI gives legal protection to the most important wildlife and geological sites. , there are 57 sites designated in Northamptonshire, 48 for their biological interest and 9 for their geological interest. Eight are Geological Conservation Review sites, four are \"Nature Conservation Review\" sites, and fourteen are managed by the Wildlife Trust for Bedfordshire, Cambridgeshire and Northamptonshire. The largest is Upper Nene Valley Gravel Pits, which is a Ramsar internationally important wetland site and a Special Protection Area under the European Union Directive on the Conservation of Wild Birds. The smallest is Irchester Old Lodge Pit, which is described in the Geological Conservation Review as a Middle Jurassic site of national importance.\n\n\n\n"}
{"id": "206270", "url": "https://en.wikipedia.org/wiki?curid=206270", "title": "Mare Fecunditatis", "text": "Mare Fecunditatis\n\nMare Fecunditatis (the \"Sea of Fecundity\" or \"Sea of Fertility\") is a lunar mare which is 840 km in diameter.\n\nThe Fecunditatis basin formed in the Pre-Nectarian epoch, while the basin material surrounding the mare is of the Nectarian epoch. The mare material is of the Upper Imbrian epoch and is relatively thin compared to Mare Crisium or Mare Tranquillitatis. This basin is overlapped with the Nectaris, Tranquillitatis, and Crisium basins. Fecunditatis basin meets Nectaris basin along Fecunditatis' western edge, with the area along this zone faulted by arcuated grabens. On the eastern edge of Fecunditatis is the crater Langrenus. Near the center lie the interesting craters Messier and Messier A. It was here that the first automated sample return took place via the Luna 16 probe, in September 1970. Sinus Successus lies along the eastern edge of the mare.\n\nUnlike many other maria, there is no mass concentration (mascon), or gravitational high, in the center of Mare Fecunditatis. Mascons were identified in the center of other maria (such as Serenitatis or Imbrium) from Doppler tracking of the five Lunar Orbiter spacecraft in 1968. The gravity field was mapped at higher resolution with later orbiters such as Lunar Prospector and GRAIL, which unveiled an irregular pattern.\n\nIn the northeast part southeast of the crater Messier, Chinese mapping satellite Chang'e 1 ended its mission as it impacted on 1 March 2009.\n\n\n"}
{"id": "39658843", "url": "https://en.wikipedia.org/wiki?curid=39658843", "title": "Marxist–Leninist atheism", "text": "Marxist–Leninist atheism\n\nIn the philosophy of Marxism, Marxist–Leninist atheism (also known as Marxist–Leninist scientific atheism) is the irreligious and anti-clerical element of Marxism–Leninism, the official state ideology of the Soviet Union. Based upon a dialectical-materialist understanding of humanity's place in nature, Marxist–Leninist atheism proposes that religion is the opium of the people, meant to promote a person's passive acceptance of his and her poverty and exploitation as the normal way of human life on Earth in the hope of a spiritual reward after death. Thus, Marxism–Leninism advocates atheism, rather than belief in religion.\n\nTo support those premises, Marxist–Leninist atheism explains the origin of religion and explains methods for the scientific criticism of religion. The philosophic roots of materialist atheism are in the works of Georg Wilhelm Friedrich Hegel (1770–1831) and of Ludwig Feuerbach (1804–1872), of Karl Marx (1818–1883) and of Vladimir Lenin (1870–1924). Unlike Soviet Marxism, other varieties of Marxism do not feature an antireligious philosophy, such as the liberation theology developed by Latin American Marxists.\n\nIn training as a philosopher in the early 19th century, Karl Marx participated in debates about the philosophy of religion, specifically about the interpretations presented in Hegelianism. In those debates, the Hegelians considered philosophy an intellectual enterprise in service to the insights of Christian religious comprehension, which Georg Wilhelm Friedrich Hegel had elaborately rationalized in \"The Phenomenology of Spirit\" (1807). Although critical of contemporary religion as a 19th-century intellectual, Hegel pursued the ontology and the epistemology of Christianity, an interest compatible with Christian theological explanations of the questions of being and of existence, which he clarified, systematized and justified in his philosophy.\n\nAfter his death in 1831, Hegel's philosophy was debated by the Young Hegelians and the materialist atheists—such as Ludwig Feuerbach—who rejected all religious philosophy as a way of running the world. Marx sided with the materialist philosophers. Feuerbach separated philosophy from religion to grant intellectual autonomy to philosophers in their interpretations of material reality; objected to Hegel’s religion-based philosophy of spirit in order to critically attack the basic concepts of theology; and redirected philosophy from the heavens to the Earth, to the subjects of human dignity and the meaning of life, of morality and the purpose of existence. He concluded that humanity created deities as reflections of the human Self. In \"The Essence of Christianity\" (1841), Feurbach said the following about the separateness of man and God: [...] But the idea of deity coincides with the idea of humanity. All divine attributes, all the attributes which make God God, are attributes of the [human] species — attributes which in the individual [person] are limited, but the limits of which are abolished in the essence of the species, and even in its existence, in so far as it has its complete existence only in all men taken together.\n\nAs a modern philosopher, Feuerbach thought that religion exercised power over the human mind through \"the promotion of fear from the mystical forces of the Heaven\" and with \"an intensive hatred of the old God\" said that houses of worship should be systematically destroyed and religious institutions eradicated. Experienced in that praxis of materialist philosophy, thought and action, Marx became a radical philosopher.\n\nAs a materialist philosopher, Marx rejected religious philosophy and its cultural contributions as detrimental to human progress and instead accepted human autonomy from supernatural authority as an axiomatic truth about the real world of 19th-century industrial Europe. He said that the churches had invented religion to justify the painful economic oppression lived by working people in a socially stratified industrial society. Thus, religion is the drug for escaping the real world. In \"A Contribution to the Critique of Hegel’s Philosophy of Right\", Marx said: Religious suffering is, at one and the same time, the expression of real suffering, and a protest against real suffering. Religion is the sigh of the oppressed creature, the heart of a heart-less world, and the soul of soul-less conditions. It [religion] is the opium of the people.\n\nAccording to Marx, atheist philosophy liberated men and women from suppressing their innate potential as human beings and allowed people to intellectually understand that they possess human agency and that they are masters of their reality because the earthly authority of supernatural deities is not real. Marx opposed religion's function of social atomization—anomie and alienation—of psychologically dividing humans from themselves as individual men and women and from each other as a community. Therefore, the authority of religious ideology must be removed from the public realm, from the law, customs and traditions with which men govern society. In \"On the Jewish Question\", Marx said: The decomposition of man into Jew and citizen, Protestant and citizen, religious man and citizen, is neither a deception directed against citizenhood, nor is it a circumvention of political emancipation, it is political emancipation itself, the political method of emancipating oneself from religion. Of course, in periods when the political state, as such, is born violently out of civil society, when political liberation is the form in which men strive to achieve their liberation, the state can and must go as far as the abolition of religion, the destruction of religion. But it can do so only in the same way that it proceeds to the abolition of private property, to the maximum, to confiscation, to progressive taxation, just as it goes as far as the abolition of life, the guillotine.<br><br>\n\nAt times of special self-confidence, political life seeks to suppress its prerequisite, civil society, and the elements composing this society, and to constitute itself as the real species-life of man, devoid of contradictions. But, it can achieve this only by coming into violent contradiction with its own conditions of life, only by declaring the revolution to be permanent, and, therefore, the political drama necessarily ends with the re-establishment of religion, private property, and all elements of civil society, just as war ends with peace.Therefore, religion was the product of objective material conditions and that economic systems, such as capitalism, affected the material conditions of society. That by abolishing systems of unequal social class and political economy, the state and religion would wither away consequent to the establishment of a communist society, which featured no formal state apparatus and no system of social classes. In \"A Contribution to the Critique of Hegel's Philosophy of Right\" (1843), Marx said: The abolition of religion, as the illusory happiness of the people, is the demand for their real happiness. To call on them to give up their illusions about their condition is to call on them to give up a condition that requires illusions. The criticism of religion is, therefore, in embryo, the criticism of that vale of tears of which religion is the halo.\n\nIn that way, Marx transformed Feuerbach’s anti-religious philosophy into political praxis and into a philosophic basis of his ideology. In the 1844 manuscripts, Marx said: \"Communism begins from the outset (Owen) with atheism; but atheism is, at first, far from being communism; indeed, that atheism is still mostly an abstraction\"; and refined the atheism of Feuerbach into a considered critique of the material (socio-economic) conditions responsible for the invention of religion. In the \"Theses on Feuerbach\", Marx said said the following about the social artifice of religious sentiment: Feuerbach starts out from the fact of religious self-alienation, of the duplication of the world into a religious world and a secular one. His work consists in resolving the religious world into its secular basis. But that the secular basis detaches itself from itself, and [then] establishes itself as an independent realm in the clouds can only be explained by the cleavages and self-contradictions within this secular basis. The latter must, therefore, in itself, be both understood in its contradiction and revolutionized in practice. Thus, for instance, after the earthly family is discovered to be the secret of the holy family, the former must then. itself, be destroyed in theory and in practice. Feuerbach, consequently, does not see that the “religious sentiment” is, itself, a social product, and that the abstract individual [person] whom he analyses belongs to a particular form of society.\n\nThe humanist philosophy of dialectical materialism proposed that humanity naturally resulted from the interplay of material forces (earth, wind and fire) in the physical world. Marx identified religion's origin as psychological solace for the exploited working classes living the subsistence-wage reality of industrial society. That despite religion's working-class origin, the clergy allowed the ruling class to control religious sentiment (the praxis of religion), which grants control of all society—the middle class, the working class and the proletariat, with Christian slaves hoping for a rewarding after-life. In \"The German Ideology\", Marx said the following about the psychology of religious faith: It is self-evident, moreover, that “spectres”, “bonds”, “the higher being”, “concept”, [and] “scruple”, are merely the idealistic, spiritual expression, the conception, apparently, of the isolated individual [person], the image of very empirical fetters and limitations, within which the mode of production of life, and the form of [social] intercourse coupled with it, move.\n\nThe philosophy of Marxist–Leninist atheism interprets the social degeneration of organized religion—from psychological-solace to social-control—to justify the revolutionary abolition of an official state religion and its replacement with atheism. Thus, the Marxist–Leninist state has no official religion.\n\nIn \"Ludwig Feuerbach and the End of Classical German Ideology\" (1846) and in the \"Anti-Dühring\" (1878), Friedrich Engels addressed contemporary social problems with critiques of the idealistic worldview, especially religious interpretations of the material reality of the world. Engels proposed that religion is a fantasy about supernatural powers controlling and determining Humanity's material poverty and dehumanizing moral squalor since early in human history; yet that such a lack of human control over human existence would end with the abolition of religion. That by way of theism, a people's need to believe in a deity, as a spiritual reflection of the self, religion would gradually disappear. In the \"Anti-Dühring\", Engels said:\n\n[...] and when this act has been accomplished, when society, by taking possession of all means of production, and using them on a planned basis, has freed itself, and all its members, from the bondage in which they are now held, by these means of production, which they, themselves, have produced, but which confront them as an irresistible alien force, when, therefore, man no longer merely proposes, but also disposes — only then will the last alien force, which is still reflected in religion, vanish; and with it will also vanish the religious reflection itself, for the simple reason that then there will be nothing left to reflect.\n\nEngels considered religion as a false consciousness incompatible with communist philosophy and urged the communist parties of the First International to advocate atheist politics in their home countries; and recommended scientific education as a means to overcome the mysticism and superstitions of people who required a religious explanation of the real world. In light of the scientific progress of the Industrial Revolution, the speculative philosophy of theology became obsolete in determining a place for every person in society. In the \"Anti-Dühring\", Engels said: The real unity of the world consists in its materiality, and this is proved, not by a few juggled phrases, but by a long and wearisome development of philosophy and natural science.\n\nBy scientific advances, socio-economic and cultural progress required that materialism become a science rather than remain a philosophy apart from the sciences. In the \"Negation of a Negation\" section of the \"Anti-Dühring\", Engels said: This modern materialism, the negation of the negation, is not the mere re-establishment of the old, but adds to the permanent foundations of this old materialism the whole thought-content of two thousand years of development of philosophy and natural science, as well as of the history of these two thousand years. It [materialism] is no longer a philosophy at all, but simply a world outlook, which has to establish its validity and be applied, not in a science of sciences, standing apart, but in the real sciences. Philosophy is therefore \"sublated\" here, that is, “both overcome and preserved”; overcome as regards its form, and preserved as regards its real content.\n\nAs a revolutionary, Vladimir Lenin said that a true communist would always promote atheism and combat religion because it is the psychological opiate that robs people of their human agency, of their volition, as men and women, to control their own reality. To refute the political legitimacy of religion, Lenin adapted the atheism of Marx and Engels to the Russian Empire. In \"Socialism and Religion\", Lenin said the following about the social-control function of religious sentiment: Religion is one of the forms of spiritual oppression, which everywhere weighs down heavily upon the masses of the people, over-burdened by their perpetual work for others, by want and isolation. Impotence, of the exploited classes in their struggle against the exploiters, just as inevitably, gives rise to the belief in a better life after death, as [the] impotence of the savage in his battle with Nature gives rise to belief in gods, devils, miracles, and the like.<br><br>Those who toil and live in want all their lives are taught, by religion, to be submissive and patient while here on earth, and to take comfort in the hope of a heavenly reward. But those who live by the labour of others are taught, by religion, to practise charity while on earth, thus offering them a very cheap way of justifying their entire existence as exploiters, and selling them, at a moderate price, tickets to well-being in heaven. Religion is opium for the people. Religion is a sort of spiritual booze, in which the slaves of capital drown their human image, their demand for a life more or less worthy of man.\n\nSince the social ideology of the Eastern Orthodox Church supported the Tsarist monarchy, voiding the credibility of religion would void the political legitimacy of the Tsar as the Russian head of state. In practice, scientific atheism is a means of class struggle for voiding the authority of the ruling class who lived from the labours of the working class and the proletariat because all intellectual activity was by and for the maintenance of class interests. Therefore, theoretical debates about supernatural control of human affairs on Earth made sense only by ignoring the poverty lived by the majority of Russia. In the event, scientific atheism became a philosophic basis of Marxism–Leninism, the ideology of the Communist Party in Russia, unlike the milder irreligion and anti-religion of non-Russian communist parties.\n\nTo establish a socialist state in Russia, Lenin advocated the dissemination of scientific atheism as an “urgent necessity” for the Communist Party; and dismissed Anatoly Lunacharsky's proposal that the Bolsheviks take advantage of God-Building (derived from Feurbach's \"religion of humanity\") which \"cultivated in the masses emotion, moral values, [and] desire\" and so include those religious people to the revolution. Politically, Lenin \"appealed to militant atheism as a criterion for the sincerity of Marxist commitments, as a testing principle\", yet requiring atheism of possible revolutionaries alienated \"some of the sympathetic, leftist-minded, yet religious [and] believing intellectuals, workers or peasants\". Hence, a true communist was atheist.\n\nThe pragmatic policies of Lenin and the Communist Party indicated that religion was to be tolerated and suppressed as required by political conditions, yet there remained the ideal of an atheist society without an official state religion. Lenin communicated the worldview of materialism to the mass population of Russia as such: Marxism is materialism. As such, it is as relentlessly hostile to religion as was the materialism of the eighteenth-century Encyclopaedists or the materialism of Feuerbach. This is beyond doubt. But the dialectical materialism of Marx and Engels goes further than the Encyclopaedists and Feuerbach, for it applies the materialist philosophy to the domain of history, to the domain of the social sciences. We must combat religion — that is the ABC of all materialism, and consequently of Marxism. But Marxism is not a materialism which has stopped at the ABC. Marxism goes further. It says: \"We must know how to combat religion, and in order to do so we must explain the source of faith and religion among the masses in a materialist way. The combating of religion cannot be confined to abstract ideological preaching, and it must not be reduced to such preaching. It must be linked up with the concrete practice of the class movement, which aims at eliminating the social roots of religion.\"\n\nThe establishment of a socialist society in Russia required changes in the socio-political consciousness of the people. Thus, combating religion, mysticism and the supernatural was a requirement for membership to the Communist Party. For Lenin, the true socialist is a revolutionary always combating religion and religious sentiment as enemies of reason, science, and socio-economic progress.\n\nThe Bolshevik government's anti-religion campaigns featured propaganda, anti-religious legislation, secular universal-education, anti-religious discrimination, political harassment, continual arrests and violence. Initially, the Bolsheviks expected that religion would wither away with the establishment of socialism, hence after the October Revolution they tolerated most religions, except for the Eastern Orthodox Church who supported Tsarist autocracy. Yet by the late 1920s, when religion had not withered away, the Bolshevik government began anti-religion campaigns (1928–1941) that persecuted \"bishops, priests, and lay believers\" of all Christian denominations and had them \"arrested, shot, and sent to labour camps\". In the east, Buddhist Lamaist priests \"were rounded up in Mongolia, by the NKVD in concert with its local affiliate, executed on the spot or shipped off to the Soviet Union to be shot or die at hard labor in the mushrooming GULAG system\" of labour camps; and by 1941, when Nazi Germany invaded the Soviet Union, 40,000 churches and 25,000 mosques had been closed and converted into schools, cinemas and clubs, warehouses and grain stores, or museums of scientific atheism.\n\nIn 1959, the academic course Fundamentals of Scientific Atheism (\"Osnovy nauchnogo ateizma\") was \"introduced into the curriculum of all higher educational institutions\" in the Soviet Union. In 1964, it was made compulsory for all pupils after a \"paucity of student response\".\n\n\n"}
{"id": "14700343", "url": "https://en.wikipedia.org/wiki?curid=14700343", "title": "Nuclear Nebraska", "text": "Nuclear Nebraska\n\nNuclear Nebraska: The Remarkable Story of the Little County That Couldn’t Be Bought is a 2007 book by Susan Cragin which follows the controversy about a proposed low level nuclear waste dump, which was planned for Boyd County, Nebraska. \nIn 1989, two multinational corporations and several government agencies proposed a waste dump and offered payment of $3 million per year for 40 years. The residents of the Boyd County farming community resisted the offer and controversy followed for almost two decades. During this time, the community was transformed \"from a small group of isolated farmers to a defiant band of environmentalists\". The opposition of the community eventually succeeded, and the license to build the dump was denied.\n\nSeveral governors became embroiled in the controversy, as well as legislators, bureaucrats and the community. One central figure went to jail and others were dismissed from their jobs. For many years, there was extensive coverage of the event by the news media.\n\nU.S. Senator Ben Nelson wrote the foreword to the book.\n\n\n"}
{"id": "44733438", "url": "https://en.wikipedia.org/wiki?curid=44733438", "title": "One Square Inch of Silence", "text": "One Square Inch of Silence\n\nOne Square Inch of Silence is a noise control project symbolized by a small red stone symbolically placed in Hoh Rainforest at Olympic National Park in 2005 by author and sound recording specialist Gordon Hempton. The stone's location has been called \"the quietest place in the United States\". According to commercial pilot Philip Greenspun, some airlines have voluntarily rerouted flights to avoid inducing noise pollution at the square inch. Hempton has formed a foundation to prevent jet aircraft noise in Olympic National Park and other parks.\n\nHempton's works, including One Square Inch of Silence, were covered in the 2010 documentary \"Soundtracker\" which debuted at the Sedona Film Festival.\n\n\n"}
{"id": "24317198", "url": "https://en.wikipedia.org/wiki?curid=24317198", "title": "Patrice Franceschi", "text": "Patrice Franceschi\n\nPatrice Franceschi is a French adventurer, born in Toulon on December 18, 1954.\n\nFranceschi is also a writer, a documentary & film maker, a sailor and a pilot. He has been awarded several medals and distinctions. Patrice Franceschi was also at the origins of many humanitarian missions in war zones (Bosnia-Herzegovina, Kurdistan, Somalia, Afghanistan, etc.). He is former Chairman of the , and former Chairman and co-founder of .\n\nFranceschi is the captain of the 3-masted schooner \"La Boudeuse\", aiming at scientific expeditions related to social evolutions and climate change matters. He is most famous for being the first man to carry a solo around-the-world flight in an Aviasud Sirocco ultralight aeroplane from September 26, 1984 to March 26, 1987. Following this tour of 2½ years (562 hours of flight, across 33 countries), he wrote a book recounting his expedition: \"La folle équipée\".\n\nHe also took part in many expeditions, including the project \"The Spirit of Bougainville\", whose first ship \"La Boudeuse\" (a Chinese junk, thirty meters long), sank 130 miles east of Malta.\n\nHe is the author of numerous books and has directed several films from his expeditions.\n\n\n"}
{"id": "1761921", "url": "https://en.wikipedia.org/wiki?curid=1761921", "title": "Polar High", "text": "Polar High\n\nThe polar highs are areas of high atmospheric pressure around the north and south poles; the north polar high being the stronger one because land gains and loses heat more effectively than sea. The cold temperatures in the polar regions cause air to descend to create the high pressure (a process called subsidence), just as the warm temperatures around the equator cause air to rise to create the low pressure intertropical convergence zone. Rising air also occurs along bands of low pressure situated just below the polar highs around the 50th parallels of latitude. These extratropical convergence zones are occupied by the polar fronts where air masses of polar origin meet and clash with those of tropical or subtropical origin. This convergence of rising air completes the vertical cycle around the polar cell in each latitudinal hemisphere. Closely related to this concept is the polar vortex.\n\nSurface temperatures under the polar highs are the coldest on Earth, with no month having an average temperature above freezing. Regions under the polar high also experience very low levels of precipitation, which leads them to be known as \"polar deserts\".\n\nAir flows outwards from the poles to create the polar easterlies in the arctic and antarctic areas.\n\nClimatology\n"}
{"id": "35887641", "url": "https://en.wikipedia.org/wiki?curid=35887641", "title": "Psi Orionis", "text": "Psi Orionis\n\nThe Bayer designation Psi Orionis (ψ Ori / ψ Orionis) is shared by two stars in the constellation Orion:\n"}
{"id": "14419595", "url": "https://en.wikipedia.org/wiki?curid=14419595", "title": "RIMPUFF", "text": "RIMPUFF\n\nRIMPUFF is a local-scale puff diffusion model developed by Risø DTU National Laboratory for Sustainable Energy, Denmark. It is an emergency response model to help emergency management organisations deal with chemical, nuclear, biological and radiological releases to the atmosphere.\n\nRIMPUFF is in operational use in several European national emergency centres for preparedness and prediction of nuclear accidental releases (RODOS, EURANOS, ARGOS), chemical gas releases (ARGOS), and for airborne Foot-and Mouth Disease virus spread \n\nRIMPUFF builds from parameterized formulas for puff diffusion, wet and dry deposition, and gamma dose radiation. Its range of application covers distances up to ~1000 km from the point of release.\n\nRIMPUFF calculates the instantaneous atmospheric dispersion taking into account the local wind variability and the local turbulence levels. The puff sizes represent instantaneous relative diffusion (no averaging) and is calculated from similarity scaling theory.\nPuff diffusion is patameterized for travel times in the range from a few seconds and up to ~1 day.\n\nWet and dry deposition is also calculated as a function of local rain intensity and turbulence levels.\n\n\nFor those who are unfamiliar with air pollution dispersion modelling and would like to learn more about the subject, it is suggested that either one of the following books be read:\n\n"}
{"id": "4438816", "url": "https://en.wikipedia.org/wiki?curid=4438816", "title": "Russian tube designations", "text": "Russian tube designations\n\nVacuum tubes produced in the former Soviet Union and in present-day Russia carry their own unique designations. Some confusion has been created in \"translating\" these designations, as they use Cyrillic rather than Latin characters.\n\nThe first system was introduced in 1929. It consisted of one or two letters and a number with up to 3 digits denoting the production number\n\nFirst letter: System type:\n\nSecond letter (optional): Type of cathode:\n\nExamples:\n\nIn 1937, the Soviet Union purchased a tube assembly line from RCA, including production licenses and initial staff training, and installed it on the Svetlana/Светлана plant in St. Petersburg, Russia. US-licensed tubes were produced since then under an adapted RETMA scheme; for example, the 6F6 thus became the 6Ф6.\n\nIn the 1950s a 5-element system (GOST 5461-59, later 13393-76) was adopted in the (then) Soviet Union for designating receiver vacuum tubes.\n\nThe 1st element (from left to right) is (for receiving tubes) a number specifying filament voltage in volts (rounded to the nearest whole number), or (for cathode-ray tubes) the screen diagonal or diameter in centimeters (rounded to the nearest whole number).\n\nThe 2nd element is a Cyrillic character specifying the type of device:\n\nThe 3rd element is a number – a series designator that differentiates between different devices of the same type.\n\nThe 4th element denotes vacuum tube construction (base, envelope):\n\nFor all-metal tubes the 4th element is omitted.\n\nThe 5th element is optional. It consists of a dash (\"-\") followed by a single character or a combination of characters and denotes special characteristics (if any) of the tube:\n\nFor instance, -YeV (Russian: -ЕВ) added after 6N2P (i.e. 6Н2П-ЕВ) signifies that this variant of the 6N2P has extended service life and low noise and microphonics. More often than not this means actual differences in internal construction of the tube compared to the \"basic\" type, but sometimes designators like -V and -I simply mean that the tube was specially selected for those characteristics from the regular-quality production at the factory.\n\nThe new designation convention was applied retrospectively to many of the previously produced types, as well as to those produced afterwards. For example, a Soviet-produced copy of the 6L6 was originally manufactured in the 1940s under its American designation (in Latin lettering), or sometimes a Cyrillic transcription of it, 6Л6. Under the above convention the tube was redesignated 6P3S (Russian: 6П3С). The 6V6 tube became 6P6S (Russian: 6П6С). However, many specialised Russian tubes, such as special military or transmitter tubes, do not follow the above convention.\n\nSome of the better-known Russian equivalents of West European and American tubes are the 6P14P (Russian: 6П14П), an EL84; 6N8S (Russian: 6Н8С), a 6SN7; and 6P3S-E (Russian: 6П3С-Е), a version of the 6L6.\n\nThere is another designation system for professional tubes such as transmitter ones.\n\nThe 1st element: Function\n\n\nThe next elements are:\n\n\n\nPopular transmitter tubes include the ГУ-29, ГУ-50, ГМ-70 and Г-807 (the Russian 807 analogue).\n\n\n"}
{"id": "2010016", "url": "https://en.wikipedia.org/wiki?curid=2010016", "title": "Sinus Successus", "text": "Sinus Successus\n\nThe lunar feature Sinus Successus (Latin for \"Bay of Success\") lies along the eastern edge of Mare Fecunditatis. It is an outward bulge that forms a type of bay. The selenographic coordinates of Sinus Successus are 0.9° N, 59.0° E, and the diameter is 132 km.\n\nAlong the eastern edge of the bay is the flooded crater Condon, and the crater Webb forms the southern end of the area. There are no other features of significance on the bay. However the terrain just to the northwest of Sinus Successus was the landing site for the Soviet Luna 18 and Luna 20 probes.\n"}
{"id": "3887850", "url": "https://en.wikipedia.org/wiki?curid=3887850", "title": "Sungazing", "text": "Sungazing\n\nSungazing is the act of looking directly into the sun. It is sometimes done as part of a spiritual or religious practice. The human eye is very sensitive, and prolonged exposure to direct sunlight can lead to solar retinopathy, pterygium, cataracts, and often blindness. Studies have shown that even when viewing a solar eclipse the eye can still be exposed to harmful levels of ultraviolet radiation.\n\nReferred to as \"sunning\" by William Horatio Bates as one of a series of exercises included in his Bates method, it became a popular form of alternative therapy in the early 20th century. His methods were widely debated at the time but ultimately discredited for lack of scientific rigor. The \"British Medical Journal\" reported in 1967 that \"Bates (1920) advocated prolonged sun-gazing as the treatment of myopia, with disastrous results\".\n\n\n"}
{"id": "1497703", "url": "https://en.wikipedia.org/wiki?curid=1497703", "title": "Sylvania Electric Products", "text": "Sylvania Electric Products\n\nSylvania Electric Products was a U.S. manufacturer of diverse electrical equipment, including at various times radio transceivers, vacuum tubes, semiconductors, and mainframe computers such as MOBIDIC. They were one of the companies involved in the development of the COBOL programming language.\n\nThe \"Hygrade Sylvania Corporation\" was formed when \"NILCO\", \"Sylvania\" and \"Hygrade Lamp Company\" merged into one company in 1931. In 1939, Hygrade Sylvania started preliminary research on fluorescent technology, and later that year, demonstrated the first linear, or tubular, fluorescent lamp. It was featured at the 1939 New York World's Fair.\n\nSylvania was also a manufacturer of both vacuum tubes and transistors.\n\nIn 1942, the company changed its name to Sylvania Electric Products Inc. (note no comma). \n\nIn 1959, Sylvania Electronics merged with General Telephone to form General Telephone and Electronics (GTE)\n\nThrough merger and acquisitions, the company became a significant, but never dominating supplier of electrical distribution equipment, including transformers and switchgear, residential and commercial load centers and breakers, pushbuttons, indicator lights and other hard-wired devices. All were manufactured and distributed under the brand name GTE Sylvania, with the name Challenger used for its light commercial and residential product lines.\n\nGTE Sylvania contributed to the technological advancement of electrical distribution products in the late 1970s with several interesting product features. At the time, they were the leading supplier of vacuum cast coil transformers, manufactured in their Hampton, Virginia plant. Their transformers featured aluminum primary winding and were cast using relatively inexpensive molds, allowing them to produce cast coil transformers in a variety of KVA capacities, primary and secondary voltages and physical coil sizes, including low profile coils for mining and other specialty applications. They also developed the first medium voltage 3 phase panel that could survive a dead short across two phases. Their patented design used bus bar encapsulated in a thin coating of epoxy and then bolted together across all three phases, using special non-conductive fittings.\n\nBy 1981 GTE had made the decision to exit the electrical distribution equipment market and began selling off its product lines and manufacturing facilities. The Challenger line, mostly manufactured at the time in Jackson, Mississippi, was sold to a former officer of GTE, who used the Challenger name as the name of his new company. Challenger flourished, and was eventually sold to Westinghouse, and later Eaton Corporation. By the mid-1980s, the GTE Sylvania electrical equipment product line and name was no more.\n\nIn 1993 GTE exited the lighting business to concentrate on its core telecomms operations. The European, Asian and Latin American operations are now under the ownership of Havells Sylvania.\nWith the acquisition of the North American division by Osram GmbH in January 1993 Osram Sylvania Inc. was established.\n\nIn the early 1980s, GTE Sylvania sold the rights to the name Sylvania and Philco for use on consumer electronics equipment only, to the Netherlands' NV Philips. This marked the end of Sylvania's TV production in Batavia, New York, USA, and Smithfield, North Carolina, USA. The Sylvania Smithfield plant later became Channel Master.\nThe rights to the Sylvania name in many countries are held by the U.S. subsidiary of the German company Osram.\nThe Sylvania brand name is owned worldwide, apart from Australia, Canada, Mexico, Thailand, New Zealand, Puerto Rico and the USA, by Havells Sylvania, headquartered in London.\n\nOsram Sylvania manufactures and markets a wide range of lighting products for homes, business, and vehicles and holds a leading share of the North American lighting market [2].\nIn fiscal year 2008, the company achieved sales of about 1.75 billion euros, which comprised about 38% of Osram's total sales at the time. \nOsram's worldwide lighting businesses employed about 9,000 people at the time. In 2016, Osram spun off the general lighting business which included the North American Osram Sylvania unit into an independent company called LEDVANCE headquatered in Garching, Germany. In 2017, LEDVANCE was merged into a consortium of Chinese investment companies and the Chinese lighting manufacturer MLS under the LEDVANCE name. The North American headquaters of LEDVANCE, previouslsy referred to as Osram Sylvania, and located in Danver, Massachusetts, was relocated to Wilmington, Massachusetts in 2015, a town north of Boston, MA. LEDVANCE continues to use the well known Osram and Sylvania brand names in they're corresponding and representaive markets throughout the world.\n\n\nThe Sylvania Electric Products explosion is a well-known industrial accident which occurred 1956 at their Bayside, New York City, facility.\n\n"}
{"id": "51435174", "url": "https://en.wikipedia.org/wiki?curid=51435174", "title": "Traiguén Formation", "text": "Traiguén Formation\n\nTraiguén Formation () is a volcano-sedimentary formation of Miocene age, located in the Aysén Region of western Patagonia, southern Chile.\n\nThe volcanic and sedimentary rocks were deposited in a marine environment. Neither the base nor the top of the formation is known. Copious dykes of basic composition and aphanitic texture intrude the formation.\n\nAt some locations Miocene plutons of the North Patagonian Batholith intrude the Traiguén Formation. The intruded plutons are of varied composition including gabbro and granodiorite. \n"}
{"id": "230944", "url": "https://en.wikipedia.org/wiki?curid=230944", "title": "Unsprung mass", "text": "Unsprung mass\n\nIn a ground vehicle with a suspension, the unsprung mass (or the unsprung weight) is the mass of the suspension, wheels or tracks (as applicable), and other components directly connected to them, rather than supported by the suspension (the mass of the body and other components supported by the suspension is the sprung mass). Unsprung mass includes the mass of components such as the wheel axles, wheel bearings, wheel hubs, tires, and a portion of the weight of driveshafts, springs, shock absorbers, and suspension links. If the vehicle's brakes are mounted outboard (i.e., within the wheel), their mass (weight) is also considered part of the unsprung mass.\n\nThe unsprung mass of a wheel offers a trade-off between a wheel's bump-following ability and its vibration isolation. Bumps and surface imperfections in the road cause tire compression, inducing a force on the unsprung mass. The unsprung mass then reacts to this force with movement of its own. The motion amplitude for short bumps is inversely proportional to the weight. A lighter wheel which readily rebounds from road bumps will have more grip and more constant grip when tracking over an imperfect road. For this reason, lighter wheels are sought especially for high-performance applications. However, the lighter wheel will soak up less vibration. The irregularities of the road surface will transfer to the cabin through the suspension and hence ride quality and road noise are worse. For longer bumps that the wheels follow, greater unsprung mass causes more energy to be absorbed by the wheels and makes the ride worse.\n\nPneumatic or elastic tires help by restoring some spring to the (otherwise) unsprung mass, but the damping possible from tire flexibility is limited by considerations of fuel economy and overheating. The shock absorbers, if any, also damp the spring motion and must be less stiff than would optimally damp the wheel bounce. So the wheels still vibrate after each bump before coming to rest. On dirt roads and on some softly paved roads, the induced motion generates small bumps, known as corrugations, washboarding or \"corduroy\" because they resemble smaller versions of the bumps in roads made of logs. These cause sustained wheel bounce in subsequent axles, enlarging the bumps.\n\nHigh unsprung mass also exacerbates wheel control issues under hard acceleration or braking. If the vehicle does not have adequate wheel location in the vertical plane (such as a rear-wheel drive car with Hotchkiss drive, a live axle supported by simple leaf springs), vertical forces exerted by acceleration or hard braking combined with high unsprung mass can lead to severe wheel hop, compromising traction and steering control.\n\nA beneficial effect of unsprung mass is that high frequency road irregularities, such as the gravel in an asphalt or concrete road surface, are isolated from the body more completely because the tires and springs act as separate filter stages, with the unsprung mass tending to uncouple them.\nLikewise, sound and vibration isolation is improved (at the expense of handling), in production automobiles, by the use of rubber bushings between the frame and suspension, by any flexibility in the frame or body work, and by the flexibility of the seats.\n\nUnsprung mass is a function of the design of a vehicle's suspension and the materials used in the construction of suspension components. Beam axle suspensions, in which wheels on opposite sides are connected as a rigid unit, generally have greater unsprung mass than independent suspension systems, in which the wheels are suspended and allowed to move separately. Heavy components such as the differential can be made part of the sprung mass by connecting them directly to the body (as in a de Dion tube rear suspension). Lightweight materials, such as aluminum, plastic, carbon fiber, and/or hollow components can provide further weight reductions at the expense of greater cost and/or fragility.\n\nThe term 'unsprung mass' was coined by the mathematician Albert Healey of the Dunlop tyre company. He presented one of the first lectures taking a rigid analytical approach to suspension design, 'The Tyre as a part of the Suspension System' to the Institution of Automobile Engineers in November 1924. This lecture was published as a 100-page paper.\n\nInboard brakes can significantly reduce unsprung mass, but put more load on half axles and (constant velocity) universal joints, and require space that may not be easily accommodated. If located next to a differential or transaxle, waste heat from the brakes may overheat the differential or vice versa, particularly in hard use, such as racing. They also make anti-dive suspension characteristics harder to achieve because the moment created by braking does not act on the suspension arms.\n\nThe Chapman strut used the driveshafts as suspension arms, thus requiring only the weight of one component rather than two. Jaguar's patented independent rear suspension (IRS) similarly reduced unsprung mass by replacing the upper wishbone arms of the suspension with the drive shafts, as well as mounting the brakes inboard in some versions.\n\nScooter-type motorcycles use an integrated engine-gearbox-final drive system that pivots as part of the rear suspension and hence is partly unsprung. This arrangement is linked to the use of quite small wheels, further affecting their poor reputation for road-holding.\n"}
{"id": "53593372", "url": "https://en.wikipedia.org/wiki?curid=53593372", "title": "Victory Program", "text": "Victory Program\n\nThe Victory Program was a military plan for the United States involvement in World War II submitted prior to the country's official entry into the war following the Japanese attack on Pearl Harbor. The plan was initially secret, but was famously exposed by the \"Chicago Tribune\" on December 4, 1941, 3 days before Pearl Harbor.\n\nOn July 9, 1941, President Franklin D. Roosevelt ordered his secretary of war, Henry Stimson, and his secretary of the Navy, Frank Knox, to prepare a plan for the “overall production requirements required to defeat our potential enemies.” This plan was leaked to U.S. Senator and prominent isolationist Burton Wheeler of Montana who in turn gave it to the equally isolationist editor of the \"Chicago Tribune\", Robert R. McCormick.\n\nThe release of the plan caused an uproar among the isolationist bloc in the United States, but the controversy died off quickly only three days later, after news of the attack on Pearl Harbor was received and a formal declaration of war was made.\n\nIn 2016, The Climate Mobilization advocacy group, published the Victory Plan, written by Ezra Silk. This policy document outlines how the U.S. could tackle greenhouse gas emissions, spur a global effort to address global warming, with a massive WWII-scale mobilization.\n"}
{"id": "7310073", "url": "https://en.wikipedia.org/wiki?curid=7310073", "title": "Vladimir Shkolnik", "text": "Vladimir Shkolnik\n\nVladimir Sergeyevich Shkolnik (born 17 February 1949, in Serpukhov, Russia), served as the Minister of Industry and Trade in the Government of Kazakhstan until Galym Orazbakov replaced him on 10 January 2007 in a political shakeup. He served as the Minister of Energy and Mineral Resources in 2005.\n\nWhile serving as the Energy Minister, Shkolnick co-chaired the 2005 Indo-Kazakh Joint Business Council meeting in Astana with Indian petroleum minister Mani Shankar Aiyar. \"The Times of India\" reported that the meeting significantly helped to warm India-Kazakhstan relations after the Government of India tried unsuccessfully to gain the right to develop petroleum in the Kurmangazy field.\n\nShkolnik served a second term as the minister of energy to 27 March, 2016, when Kanat Bozumbayev's term started.\n\n"}
{"id": "43934112", "url": "https://en.wikipedia.org/wiki?curid=43934112", "title": "Xeira", "text": "Xeira\n\nXeira (in English: time or sequence) is a political youth organization of Galicia, with a pro-independence stance and Marxist-leninist ideology. It was founded in December 2013 to fill the political space of Adiante-Galician Revolutionary Youth, that self-dissolved weeks before the constitution of Xeira.\n\nIt is the youth organization of the Galician People's Front (FPG), one of the organizations participating in Anova-Nationalist Brotherhood.\n\n"}
