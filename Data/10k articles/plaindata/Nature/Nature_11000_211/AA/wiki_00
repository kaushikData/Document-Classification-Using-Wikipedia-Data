{"id": "54226702", "url": "https://en.wikipedia.org/wiki?curid=54226702", "title": "Acaste (Oceanid)", "text": "Acaste (Oceanid)\n\nIn Greek mythology, Acaste (; Ancient Greek: Ακαστη \"Akas\"tê \"unstable\" or \"irregular \"\";\" feminine form of Acastus) was one of the Oceanids, sea nymph daughters of the sea deities, Oceanus and Tethys.\n\nHesiod mentioned Acaste as one of the Oceanids:\n\nAcaste only appeared in one myth, along with her sisters, she was one of the companions of Persephone when the maiden she was abducted by Hades, the god of the Underworld. Persephone recounted her kidnapping to her mother Demeter in the following passage:\n\n"}
{"id": "166377", "url": "https://en.wikipedia.org/wiki?curid=166377", "title": "Advection", "text": "Advection\n\nIn the field of physics, engineering, and earth sciences, advection is the transport of a substance by bulk motion. The properties of that substance are carried with it. Generally the majority of the advected substance is a fluid. The properties that are carried with the advected substance are conserved properties such as energy. An example of advection is the transport of pollutants or silt in a river by bulk water flow downstream. Another commonly advected quantity is energy or enthalpy. Here the fluid may be any material that contains thermal energy, such as water or air. In general, any substance or conserved, extensive quantity can be advected by a fluid that can hold or contain the quantity or substance.\n\nDuring advection, a fluid transports some conserved quantity or material via bulk motion. The fluid's motion is described mathematically as a vector field, and the transported material is described by a scalar field showing its distribution over space. Advection requires currents in the fluid, and so cannot happen in rigid solids. It does not include transport of substances by molecular diffusion.\n\nAdvection is sometimes confused with the more encompassing process of convection which is the combination of advective transport and diffusive transport.\n\nIn meteorology and physical oceanography, advection often refers to the transport of some property of the atmosphere or ocean, such as heat, humidity (see moisture) or salinity. \n\nAdvection is important for the formation of orographic clouds and the precipitation of water from clouds, as part of the hydrological cycle.\n\nThe term \"advection\" often serves as a synonym for \"convection\", and this correspondence of terms is used in the literature. More technically, convection applies to the movement of a fluid (often due to density gradients created by thermal gradients), whereas advection is the movement of some material by the velocity of the fluid. Thus, somewhat confusingly, it is technically correct to think of momentum being advected by the velocity field in the Navier-Stokes equations, although the resulting motion would be considered to be convection. Because of the specific use of the term convection to indicate transport in association with thermal gradients, it is probably safer to use the term advection if one is uncertain about which terminology best describes their particular system.\nIn meteorology and physical oceanography, advection often refers to the horizontal transport of some property of the atmosphere or ocean, such as heat, humidity or salinity, and convection generally refers to vertical transport (vertical advection). Advection is important for the formation of orographic clouds (terrain-forced convection) and the precipitation of water from clouds, as part of the hydrological cycle.\n\nThe advection equation also applies if the quantity being advected is represented by a probability density function at each point, although accounting for diffusion is more difficult.\n\nThe advection equation is the partial differential equation that governs the motion of a conserved scalar field as it is advected by a known velocity vector field. It is derived using the scalar field's conservation law, together with Gauss's theorem, and taking the infinitesimal limit.\n\nOne easily visualized example of advection is the transport of ink dumped into a river. As the river flows, ink will move downstream in a \"pulse\" via advection, as the water's movement itself transports the ink. If added to a lake without significant bulk water flow, the ink would simply disperse outwards from its source in a diffusive manner, which is not advection. Note that as it moves downstream, the \"pulse\" of ink will also spread via diffusion. The sum of these processes is called convection.\n\nIn Cartesian coordinates the advection operator is\n\nwhere formula_2 is the velocity field, and formula_3 is the del operator (note that Cartesian coordinates are used here).\n\nThe advection equation for a conserved quantity described by a scalar field formula_4 is expressed mathematically by a continuity equation:\n\nwhere formula_5 is the divergence operator and again formula_6 is the velocity vector field. Frequently, it is assumed that the flow is incompressible, that is, the velocity field satisfies\n\nand formula_6 is said to be solenoidal. If this is so, the above equation can be rewritten as\n\nIn particular, if the flow is steady, then\n\nwhich shows that formula_4 is constant along a streamline. \nHence, formula_11 so formula_4 doesn't vary in time.\n\nIf a vector quantity formula_13 (such as a magnetic field) is being advected by the solenoidal velocity field formula_6, the advection equation above becomes:\n\nHere, formula_13 is a vector field instead of the scalar field formula_4.\n\nThe advection equation is not simple to solve numerically: the system is a hyperbolic partial differential equation, and interest typically centers on discontinuous \"shock\" solutions (which are notoriously difficult for numerical schemes to handle).\n\nEven with one space dimension and a constant velocity field, the system remains difficult to simulate. The equation becomes\n\nwhere formula_19 is the scalar field being advected \nand formula_20 is the formula_21 component of the vector formula_22.\n\nAccording to Zang, numerical simulation can be aided by considering the skew symmetric form for the advection operator.\n\nwhere\n\nand formula_6 is the same as above.\n\nSince skew symmetry implies only imaginary eigenvalues, this form reduces the \"blow up\" and \"spectral blocking\" often experienced in numerical solutions with sharp discontinuities (see Boyd).\n\nUsing vector calculus identities, these operators can also be expressed in other ways, available in more software packages for more coordinate systems.\n\nThis form also makes visible that the skew symmetric operator introduces error when the velocity field diverges. Solving the advection equation by numerical methods is very challenging and there is a large scientific literature about this.\n\n"}
{"id": "1643360", "url": "https://en.wikipedia.org/wiki?curid=1643360", "title": "Andesite line", "text": "Andesite line\n\nThe andesite line is the most significant regional geologic distinction in the Pacific Ocean basin. It separates the mafic basaltic volcanic rocks of the Central Pacific Basin from the partially submerged continental areas of more felsic andesitic volcanic rock on its margins. The andesite line parallels the subduction zones and deep oceanic trenches around the Pacific basin. It is the surface expression of melting within and above the plunging subducting slab. It follows the western edge of the islands off California and passes south of the Aleutian Arc, along the eastern edge of the Kamchatka Peninsula, the Kuril Islands, Japan, the Mariana Islands, the Solomon Islands, and New Zealand's North Island. The dissimilarity continues northeastward along the western edge of the Andes mountains of South America to Mexico, returning then to the islands off California. Indonesia, the Philippines, Japan, New Guinea, and New Zealand lie outside the andesite line.\n\nWithin the closed loop of the andesite line are most of the deep troughs, submerged volcanic mountains, and oceanic volcanic islands that characterize the Pacific basin. It is here that basaltic lavas gently flow out of rifts to build huge dome-shaped volcanic mountains whose eroded summits form island arcs, chains, and clusters. Outside the andesite line, volcanism is of the explosive type. The Pacific Ring of Fire runs parallel to the line and is the world's foremost belt of explosive volcanism.\n\nThe term \"andesite line\" predates the geologic understanding of plate tectonics. The term was first used in 1912 by New Zealand geologist Patrick Marshall to describe the distinct structural and volcanologic boundary extending from east of New Zealand to Fiji and north of the New Hebrides and the Solomon Islands.\n\n"}
{"id": "16077444", "url": "https://en.wikipedia.org/wiki?curid=16077444", "title": "Animal source foods", "text": "Animal source foods\n\nAnimal source foods (ASF) include many food item that comes from an animal source such as meat, milk, eggs, cheese and yogurt. Many individuals do not consume ASF or consume little ASF by either personal choice or necessity as ASF may not be accessible or available to these people.\n\nAside from performed vitamin A, vitamin B and vitamin D, all vitamins found in animal source foods may also be found in plant-derived foods. Examples are tofu to replace meat (both contain protein in sufficient amounts), and certain seaweeds and vegetables as respectively kombu and kale to replace dairy foods as milk (both contain calcium in sufficient amounts). There are some nutrients which are rare to find in sufficient density in plant based foods. One example would be zinc, the exception would be pumpkin seeds that have been soaked for improved digestion. The increased fiber in these foods can also make absorption difficult. Deficiencies are very possible in these nutrients if vegetarians are not very careful and willing to eat sufficient quantities of these exceptional plant based foods. A good way to find these foods would be to search for them on one of the online, nutrient analyzing databases. An example would be nutritiondata.com.\n\nMost humans eat an omnivorous diet (comprising animal source foods and plant source foods) though some civilisations have eaten only animal foods. Although a healthy diet containing all essential macro and micronutrients may be possible by only consuming a plant based diet (with vitamin B obtained from supplements if no animal sourced foods are consumed), some populations are unable to consume an adequate quantity or variety of these plant based items to obtain appropriate amounts of nutrients, particularly those that are found in high concentrations in ASF. Frequently, the most vulnerable populations to these micronutrient deficiencies are pregnant women, infants, and children in developing countries. In the 1980s the Nutrition Collaborative Research Support Program (NCRSP) found that six micronutrients were low in the mostly vegetarian diets of children in malnourished areas of Egypt, Mexico, and Kenya. These six micronutrients are vitamin A, vitamin B, riboflavin, calcium, iron and zinc. ASF are the only food source of Vitamin B. ASF also provide high biological value protein, energy, fat compared with plant food sources.\n\nAll six micronutrients richly found in ASF, vitamin A, vitamin B, riboflavin, calcium, iron and zinc play a critical role in the growth and development of children. Inadequate stores of these micronutrients, either resulting from inadequate intake or poor absorption, is associated with poor growth, anemias (iron deficiency anemia and macrocytic anemia), rickets, night blindness, impaired cognitive functioning, neuromuscular deficits, diminished work capacity, psychiatric disorders and death. Some of these effects, such as impaired cognitive development from an iron deficiency, are irreversible. However, ASF foods have been shown to raise the risk of heart disease, diabetes, cancer, obesity and other preventable diseases due to the high levels of saturated fat and cholesterol.\n\nMicronutrient deficiency is associated in poor early cognitive development. Programs designed to address these micronutrient deficiencies should be targeted to infants, children, and pregnant women. To address these significant mirconutrient deficiencies, some global health researchers and practitioners developed and piloted a snack program in Kenya school children. However, some communities are vegetarians for religious or cultural reasons. Efforts must be made to develop culturally appropriate interventions to address the micronutrient deficiencies in these populations, such as through food fortification.\n\nAccording to a 2006 United Nations initiative, the livestock industry sector emerges as one of the top two or three most significant contributors to the most serious environmental problems, at every scale from local to global.\" As such, using plant-derived foods is typically considered better for the interests of the environment. Despite this, the raising of certain animals can be more environmentally sound than others. According to the Farralones Institute's report from 1976, raising rabbits, and chickens (on a well-considered approach) for food can still be quite sustainable. As such, the production of meat and other produce, such as eggs, may still be considered environmentally friendly (if this is done in an industrial, high-efficiency manner). In addition, raising goats (for goat milk and meat) can also be environmentally quite friendly and has been favored by certain environmental activists, such as Mohandas Gandhi.\n\n"}
{"id": "30157651", "url": "https://en.wikipedia.org/wiki?curid=30157651", "title": "Asterodia", "text": "Asterodia\n\nIn Greek mythology, the name Asterodia, Asterodeia, or Asterodea (; Ancient Greek: Ἀστεροδεία, Ἀστεροδία) refers to:\n\n\n"}
{"id": "58281818", "url": "https://en.wikipedia.org/wiki?curid=58281818", "title": "Ayacara Formation", "text": "Ayacara Formation\n\nThe Ayacara Formation is a sedimentary formation made up of interbedded sand and siltstone cropping out around Hornopirén and Ayacara Peninsula in western Los Lagos Region, Chile. Less common rocks are tuff and conglomerate. The formation dates to the Early and Middle Miocene (no earlier than 21.8–17.6 million years ago) when it deposited during a marine transgression. \n\n"}
{"id": "1293882", "url": "https://en.wikipedia.org/wiki?curid=1293882", "title": "Baptism of the Lord", "text": "Baptism of the Lord\n\nThe Baptism of the Christ (or the Baptism of Christ) is the feast day commemorating the baptism of Jesus in the Jordan River by John the Baptist. Originally the baptism of Christ was celebrated on Epiphany, which commemorates the coming of the Magi, the baptism of Christ, and the wedding at Cana. Over time in the West, however, the celebration of the baptism of the Lord came to be commemorated as a distinct feast from Epiphany. It is celebrated in the Catholic Church as well as the Anglican and Lutheran Churches on the first Sunday following The Epiphany of Our Lord ( January 6).\n\nIn the Eastern Orthodox and the Eastern Catholic Churches, the Baptism of the Lord is celebrated as an integral part of the celebration on January 6, the Great Feast of the Theophany. For those churches which follow the traditional Julian Calendar, January 6 falls on January 19 of the modern Gregorian Calendar (see Epiphany (holiday) and Theophany for details).\n\nThe Baptism of the Lord is observed as a distinct feast in the Roman rite, although it was originally one of three Gospel events marked by the feast of the Epiphany. Long after the visit of the Magi had in the West overshadowed the other elements commemorated in the Epiphany, Pope Pius XII instituted in 1955 a separate liturgical commemoration of the Baptism.\n\nIn fact, the Tridentine Calendar has no feast of the Baptism of the Lord. It was almost four centuries later that the feast was instituted, under the denomination \"Commemoration of the Baptism of our Lord\", for celebration on 13 January as a major double, using for the Office and the Mass those previously said on the Octave of the Epiphany, which Pius XII abolished; but if the Commemoration of the Baptism of Our Lord occurred on Sunday, the Office and Mass were to be those of the Feast of the Holy Family without any commemoration.\n\nIn his revision of the calendar five years later, Pope John XXIII kept on 13 January the \"Commemoration of the Baptism of our Lord Jesus Christ\", with the rank of a second-class feast.\n\nA mere 14 years after the institution of the feast, Pope Paul VI set its date as the first Sunday after January 6 (as early as January 9 or as late as January 13) or, if in a particular country the Epiphany is celebrated on January 7 or 8, on the following Monday.\n\nPope John Paul II initiated a custom whereby on this feast the Pope baptizes babies in the Sistine Chapel.\n\nIn the Church of England, Epiphany may be observed on January 6 proper, or on the Sunday between January 2 and 8. If Epiphany is observed on a Sunday on January 6 or before, the Baptism of Christ is observed on the following Sunday. If the Epiphany is observed on January 7 or 8, the Baptism of Christ is observed on the following Monday. In the Church of England, Ordinary Time does not begin until the day after the Presentation of Christ in the Temple.\n\nIn the Episcopal Church [USA], Epiphany is always celebrated on January 6, and the Baptism of the Lord is always celebrated on the following Sunday. It is not clear as to whether or not the Feast of the Baptism of our Lord is the end of Christmastide for the Episcopal Church. On one hand, the Prayer Book refers to the \"Twelve Days of Christmas,\" and clearly distinguishes the Christmas and Epiphany seasons, the latter extending until Ash Wednesday. On the other hand, the Prayer Book allows for the continued use of Christmas prayers and readings on the weekdays following the Epiphany and leading up to the Baptism of our Lord. Further, the Epiphany and the Baptism of Christ are viewed as specially connected, allowing the interpretation that Christmastide does extend through and end with the Feast of our Lord's Baptism on the Sunday following the Epiphany.\n"}
{"id": "87743", "url": "https://en.wikipedia.org/wiki?curid=87743", "title": "Bergelmir", "text": "Bergelmir\n\nIn Norse mythology, Bergelmir ( ; Old Norse \"Mountain Yeller\" or \"Bear Yeller\")\nis a frost giant, the son of giant Þrúðgelmir and the grandson of Ymir (who was called Aurgelmir among giants), the first frost giant, according to stanza 29 of the poem \"Vafthrudnismal\" from the \"Poetic Edda\":\n\nAccording to the \"Gylfaginning\" section of the \"Prose Edda\" by Snorri Sturluson, Bergelmir and his wife alone among the giants were the only survivors of the enormous deluge of blood which flowed from Ymir's wounds when he was killed by Odin and his brothers Vili and Vé. They escaped the sanguinary flood by climbing onto an object and subsequently became the progenitors of a new race of frost giants.\n\nR. D. Fulk notes that Snorri's \"Prose Edda\" account \"conflicts with the poetic version, as the [\"Prose Edda\"] presents a Noah-like figure, while the latter has Bergelmir laid (\"lagiðr\") in the \"lúðr\", implying he is an infant, as in the Scyld story. But Snorri does add the crucial element not made in the explicit verses, that the \"lúðr\" is to serve as a floating vessel.\"\n\nFulk continues that \"the key word here is \"lúðr\", which ought to refer to a flour-bin. To be precise, the object is a box or wooden trough, perhaps on legs, in which the stones of a hand-mill sit [...]. It is true that most glossators assume some meaning other than 'flour-bin' in \"Vafþrúðnismál\" and \"Snorra edda\" [an alternate name for the \"Prose Edda\"], suggesting instead something in the range of 'coffin (or cradle), chest, ark (i.e. boat)'.\" Fulk details that \"the interpretation of 'ark' derives solely from the passage in \"Snorra Edda\", because of Bergelmir's resemblance to Noah, and the fact that [Old Icelandic] \"ǫrk\" [...] can refer to both Noah's ark and a chest or a sarcophagus.\"\n\nScholars John Lindow and Carolyne Larrington agree that the \"Prose Edda\" account of the flood borrowed from Judeo-Christian tradition of Noah's Ark.\n\n"}
{"id": "42943433", "url": "https://en.wikipedia.org/wiki?curid=42943433", "title": "Blue men of the Minch", "text": "Blue men of the Minch\n\nThe blue men of the Minch, also known as storm kelpies ( ), are mythological creatures inhabiting the stretch of water between the northern Outer Hebrides and mainland Scotland, looking for sailors to drown and stricken boats to sink. They appear to be localised to the Minch and surrounding areas, unknown in other parts of Scotland and without counterparts in the rest of the world.\n\nApart from their blue colour, the mythical creatures look much like humans, and are about the same size. They have the power to create storms, but when the weather is fine they float sleeping on or just below the surface of the water. The blue men swim with their torsos raised out of the sea, twisting and diving as porpoises do. They are able to speak, and when a group approaches a ship its chief may shout two lines of poetry to the master of the vessel and challenge him to complete the verse. If the skipper fails in that task then the blue men will attempt to overturn the ship and capsize it.\n\nSuggestions to explain the mythical blue men include that they may be a personification of the sea, or originate with the Picts, whose painted bodies may have given the impression of men raising themselves out of the water if they were seen crossing the sea in boats that might have resembled kayaks. The genesis of the blue men may alternatively lie with the North African slaves the Vikings took with them to Scotland, where they spent the winter months close to the Shiant Isles in the Minch.\n\nThe Minch, a strait that separates the northwest Highlands of Scotland and the northern Inner Hebrides from the northern Outer Hebrides, is home to the blue men. According to lexicographer Edward Dwelly, in the Scottish Gaelic terms for the blue men – \"na fir ghorma\", \"fear gorm\" and \"sruth nam fear gorm\" – the word \"gorm\" refers to any shade of blue and \"fear\" can be translated as \"man\", \"na fir\" representing the plural \"the men\".\n\nThe blue men are also styled as storm kelpies. The most common water spirits in Scottish folklore, kelpies are usually described as powerful horses, but the name is attributed to several different forms and fables throughout the country. The name \"kelpie\" may be derived from the Scottish Gaelic \"calpa\" or \"cailpeach\", meaning \"heifer\" or \"colt\".\n\nThe mythical blue men may have been part of a tribe of \"fallen angels\" that split into three; the first became the ground dwelling fairies, the second evolved to become the sea inhabiting blue men, and the remainder the \"Merry Dancers\" of the Northern Lights in the sky. The legendary creatures are the same size as humans but, as the name implies, blue in colour. Writer and journalist Lewis Spence thought they were the \"personifications of the sea itself\" as they took their blue colouration from the hue of the sea. Their faces are grey and long in shape and some have long arms, which are also grey, and they favour blue headgear; at least one account claims that they also have wings. The tempestuous water around the Shiant Isles to the north of Skye, an area subject to rapid tides in all weathers, flows beside the caves inhabited by the blue men, a stretch of water known as the Current of Destruction owing to the number of ships wrecked there.\n\nAlthough other storm kelpies are reported as inhabiting the Gulf of Corrievreckan, described by poet, writer and folklorist Alasdair Alpin MacGregor as \"the fiercest of the Highland storm kelpies\", the blue men are confined to a very restricted area. According to Donald A. Mackenzie they have no counterparts elsewhere in the world or even in other areas of Scotland; such limited range is rare for beliefs in spirits and demons. Folklorist and Tiree minister John Gregorson Campbell states that they were unknown in Argyll on the nearby coast of the mainland for instance, although Church of Scotland minister John Brand, who visited Quarff in Shetland in mid-1700, recounts a tale of what may have been a blue man in the waters around the island. In the form of a bearded old man it rose out of the water, terrifying the passengers and crew of a boat it was following.\n\nIn traditional tales the blue men have the power to create severe storms, but when the weather is fine they sleep or float just under the surface of the water. They swim with their torso from the waist upwards raised out of the sea, twisting and diving in a similar way to a porpoise. To amuse themselves the creatures play shinty when the skies are clear and bright at night. They are able to speak and converse with mariners and are especially vocal when soaking vessels with water spray, roaring with laughter as vessels capsize.\n\nWhen the blue men gather to attack passing vessels their chief, sometimes named as Shony, rises up out of the water and shouts two lines of poetry to the skipper, and if he cannot add two lines to complete the verse the blue men seize his boat. Mackenzie highlights the following exchange between the skipper of a boat and the chief of the blue men:\n\nThe quick responses took the blue chief by surprise; defeated and unable to do any damage to the vessel, the blue men returned to their underwater caves, allowing the vessel free passage through the strait. The blue men may alternatively board a passing vessel and demand tribute from its crew, threatening that if it is not forthcoming they will raise up a storm.\n\nNo surviving tales mention attempts to kill the demons, but a Gregorson Campbell story tells of the capture of a blue man. Sailors seize a blue man and tie him up on board their ship after he is discovered \"sleeping on the waters\". Two fellow blue men give chase, calling out to each other as they swim towards the ship:\n\nOn hearing his companions' voices the captured spirit breaks free of his bonds and jumps overboard as he answers:\n\nSailors thus believed that all blue men have names by which they address each other.\n\nMackenzie's explanation of the legend of the blue men was based partly on research into the Annals of Ireland and goes back to the times of Harald Fairhair, the first Norse king, and his battles against the Vikings. The Scottish Gaelic term \"fear gorm\", meaning \"blue men\", is the descriptor for a black man according to Dwelly. Thus \"sruth nam fear gorm\", one of the blue men's Gaelic names, literally translates as \"stream of the blue men\", or \"river, tide or stream of the black man\". Around the 9th century the Vikings took Moors they had captured and were using as slaves to Ireland. The Vikings spent winter months near the Shiant Isles, and Mackenzie attributes the story of the blue men to \"marooned foreign slaves\". He quotes an excerpt from historian Alan Orr Anderson's \"Early sources of Scottish history, A.D. 500 to 1286\":\n\nMore recent newspaper reports have repeated Mackenzie's hypothesis. Historian Malcolm Archibald agrees the legend originates from the days Norsemen had North African slaves, but speculates that the myth may have originated with the Tuareg people of Saharan Africa, who were known as the \"blue men of the desert\".\n\nThe origin of the blue men of the Minch may alternatively lie with \"tattooing people\" specifically the Picts, whose Latin name \"picti\" means \"painted people\". If they were seen crossing the water in boats resembling the kayaks of the Finn-men they may have given simple islanders and mariners the impression of the upper part of the body rising out of the water.\n\n\n"}
{"id": "21418432", "url": "https://en.wikipedia.org/wiki?curid=21418432", "title": "Criticism of democracy", "text": "Criticism of democracy\n\nCriticism of democracy is grounded in democracy's contested definition—its purpose, process, and outcomes. Since Classical antiquity and through the modern era, democracy has been associated with \"rule of the people,\" \"rule of the majority,\" and free selection or election either through direct participation or elected representation respectively, but has not been linked to a particular outcome.\n\nPolitical thinkers approach their critique of democracy from different perspectives. Many do not necessarily oppose democracy—\"rule of the people\"—but, rather, seek to expand or question its popular definition. In their work, they distinguish between democratic principles that are effectively implemented through undemocratic procedures; undemocratic principles that are implemented through democratic procedures; and variations of the same kind.\n\nFor instance, some critics of democracy would agree with Winston Churchill's famous remark, \"No one pretends that democracy is perfect or all-wise. Indeed, it has been said that democracy is the worst form of government except all those other forms that have been tried from time to time.\" While others, may be more prepared to describe existing democratic regimes as anything but \"rule of the people.\"\n\nCritics of democracy have tried to highlight democracy's inconsistencies, paradoxes, and limits by contrasting it with other forms of governments. They have characterized most modern democracies as democratic polyarchies and democratic aristocracies; they have identified fascist moments in modern democracies; they have termed the societies produced by modern democracies as neo-feudal; while, yet others, have contrasted democracy with Nazism, anarcho-capitalism, theocracy, and absolute monarchy.\n\nThe most widely known critics of democracy include Plato and the authors of the Federalist Papers, who were interested in establishing a representative democracy in America instead of a direct democracy.\n\nAdditional historical figures associated with the critique of democracy thought include Aristotle, Montesquieu, James Harrington, Jean-Jacques Rousseau, Martin Heidegger, Hubert Lagardelle, Charles Maurras, Friedrich Nietzsche, Carl Schmitt, Oswald Spengler, Nicolás Gómez Dávila, and Elazar Menachem Shach.\n\nLeading contemporary thinkers in critical democratic theory include Jürgen Habermas, Robert A. Dahl, Robert E. Goodin, Bernard Manin, Joseph Schumpeter, James S. Fishkin, Ian Shapiro, Jason Brennan, Hélène Landemore and Hans-Hermann Hoppe.\n\nOne such argument is that the benefits of a specialized society may be compromised by democracy. As ordinary citizens are encouraged to take part in the political life of the country, they have the power to directly influence the outcome of government policies through the democratic procedures of voting, campaigning and the use of press. The result is that government policies may be more influenced by non-specialist opinions and thereby the effectiveness compromised, especially if a policy is very technically sophisticated and/or the general public inadequately informed. For example, there is no guarantee that those who campaign about the government's economic policies are themselves professional economists or academically competent in this particular discipline, regardless of whether they were well-educated. Essentially this means that a democratic government may not be providing the most good for the largest number of people. However, some have argued that this should not even be the goal of democracies because the minority could be seriously mistreated under that purported goal.\n\nFriedrich Nietzsche, as an opponent of Christianity, saw western democracy as connected to it, claiming that \"the \"democratic\" movement is Christianity's heir\" and denounced the democratic man for being inherently unable to \"feel any shame for being unable to rise above\" his desire \"to satisfy a host of petty wants through the calculation of long-term self-interest\". Nietzsche claimed that in a democracy \"[w]hen the individual's highest and strongest instincts break forth with a passion, driving him far and above the average, beyond the lowlands of the herd conscience\", \"the moral perspective now considers how harmful or harmless an opinion, an emotional state, a will, a talent is to the community, to equality\". \"Exalted, self-directed spirituality, a will to solitude, even great powers of reason are felt as a danger\". \"Morality in Europe today is herd animal morality\".\n\nThe real difference between ancient democracies and modern republics lies, according to Madison, in \"\"the total exclusion of the people in their collective capacity\" from any share in the \"latter\", and not in the \"total exclusion of the representatives of the people\" from the administration of the \"former\".— Bernard Manin, p. 2 (See: Madison, \"Federalist 63,\" in \"The Federalist Papers,\" p. 387; Madison's emphasis.)Bernard Manin is interested in distinguishing modern representative republics, such as the United States, from ancient direct democracies, such as Athens. Manin believes that both aspire to \"rule of the people,\" but that the nature of modern representative republics lends them to \"rule of the aristocratic.\" Manin explains that in ancient democracies, virtually every citizen had the chance to be selected to populate the government but in modern republics, only elites have the chance of being elected. He does not defend this phenomenon but rather seeks to describe it.\n\nManin draws from James Harrington, Montesquieu, and Jean-Jacques Rousseau to suggest that the dominant form of government, representative as opposed to direct, is effectively aristocratic. He proposes that modern representative governments exercise political power through aristocratic elections which, in turn, brings into question democracy's \"rule of the people\" principle. As far as Montesquieu is concerned, elections favor the \"best\" citizens who Manin notes tend to be wealthy and upper-class. As far as Rousseau is concerned, elections favor the incumbent government officials or the citizens with the strongest personalities, which results in hereditary aristocracy. Manin further evinces the aristocratic nature of representative governments by contrasting them with the ancient style of selection by lot. Manin notes that Montesquieu believed that lotteries prevent jealousy and distribute offices equally (among citizens from different ranks), while Rousseau believed that lotteries choose indifferently, preventing self-interest and partiality from polluting the citizen's choice (and thus prevent hereditary aristocracy).\n\nHowever, Manin also provides criticism of direct democracy, or selection by lot. Manin reflects on Montesquieu's interrogation of the extent to which Athenian direct democracy was truly direct. Montesquieu finds that citizens who had reason to believe they would be accused as \"unworthy of selection\" commonly withheld their names from the lottery, thereby making selection by lot vulnerable to self-selection bias and, thus, aristocratic in nature. Manin does not dwell on direct democracy's potentially aristocratic elements, perhaps because he share's Montesquieu's belief that there is nothing alarming about the exclusion of citizens who may be incompetent; this exclusion may be inevitable in any method of selection.\n\nAdditionally, Manin is interested in explaining the discrepancy between 18th century American and French revolutionaries' \"declaration\" of the \"equality of all citizens\" and their \"enactment\" of (aristocratic) elections in their respective democratic experiments. Manin suggests that the discrepancy is explained by the revolutionaries' contemporary preoccupation with one form of equality over another. The revolutionaries prioritized gaining the equal right to consent to their choice of government (even a potentially aristocratic democracy), at the expense of seeking the equal right to be face of that democracy. And it is elections, not lots, that provide citizens with more opportunities to consent. In elections, citizens consent both to the procedure of elections and to the product of the elections (even if they produce the election of elites). In lotteries, citizens consent only to the procedure of lots, but not to the product of the lots (even if they produce election of the average person). That is, if the revolutionaries prioritized consent to be governed over equal opportunity to serve as the government, then their choice of elections over lotteries makes sense.\n\nA major scholarly attack on the basis of democracy was made by German-Italian political scientist Robert Michels who developed the mainstream political science theory of the iron law of oligarchy in 1911. Michels argued that oligarchy is inevitable as an \"iron law\" within any organization as part of the \"tactical and technical necessities\" of organization and on the topic of democracy, Michels stated: \"It is organization which gives birth to the dominion of the elected over the electors, of the mandataries over the mandators, of the delegates over the delegators. Who says organization, says oligarchy\" and went on to state \"Historical evolution mocks all the prophylactic measures that have been adopted for the prevention of oligarchy.\" Michels stated that the official goal of democracy of eliminating elite rule was impossible, that democracy is a façade legitimizing the rule of a particular elite, and that elite rule, that he refers to as oligarchy, is inevitable. Michels had formerly been a Marxist but became drawn to the syndicalism of Sorel, Eduoard Berth, Arturo Labriola, and Enrico Leone and had become strongly opposed parliamentarian, legalistic, and bureaucratic socialism of social democracy and in contrast supported an activist, voluntarist, anti-parliamentarian socialism. Michels would later become a supporter of fascism upon Mussolini's rise to power in 1922, viewing fascism's goal to destroy liberal democracy in a sympathetic manner.\n\nCharles Maurras, an FRS member of the \"Action française\" movement, stated in a famous dictum \"Democracy is evil, democracy is death.\" Maurras' concept of \"politique naturelle\" declared recognition of inescapable biological inequality and thereby natural hierarchies, and claimed that the individual is naturally subordinated to social collectivities such as the family, the society, and the state, which he claims are doomed to fail if based upon the \"myth of equality\" or \"abstract liberty\". Maurras criticized democracy as being a \"government by numbers\" in which quantity matters more over quality and prefers the worst over the best. Maurras denounced the principles of liberalism as described in \"The Social Contract\" by Jean-Jacques Rousseau and in \"Declaration of the Rights of Man and of the Citizen\" as based upon the false assumption of liberty and the false assumption of equality. He claimed that the parliamentary system subordinates the national interest, or common good, to private interests of a parliament's representatives where only short-sighted interests of individuals prevail.\n\nFrench revolutionary syndicalist Hubert Lagardelle claimed that French revolutionary syndicalism came to being as the result of \"the reaction of the proletariat against democracy,\" which he claimed was \"the \"popular form\" of bourgeois dominance.\" Lagardelle opposed democracy for its universalism, and believed in the necessity of class separation of the proletariat from the bourgeoisie, as democracy did not recognize the social differences between them.\n\nIsraeli politician Rabbi Elazar Menachem Shach promoted Judaic law to be the natural governance for Jews and condemned democracy, he claimed that \"Democracy as a machinery of lies, false notions, pursuit of narrow interests and deceit - as opposed to the Torah regime, which is based on seeking the ultimate truth.\" Shach criticized democracy for having no real goals, saying \"The whole point of democracy is money. The one does what the other asks him to do in pursuit of his own interest, so as to be given what he himself asks for, and the whole purpose of the transaction is that each would get what they want.\"\n\nMore recently, democracy is criticized for not offering enough political stability. As governments are frequently elected on and off there tend to be frequent changes in the policies of democratic countries both domestically and internationally. Even if a political party maintains power, vociferous, headline grabbing protests and harsh criticism from the mass media are often enough to force sudden, unexpected political change. Frequent policy changes with regard to business and immigration are likely to deter investment and so hinder economic growth. For this reason, many people have put forward the idea that democracy is undesirable for a developing country in which economic growth and the reduction of poverty are top priority. However, Anthony Downs argued that the political market works much the same way as the economic market, and that there could potentially be an equilibrium in the system because of democratic process. However, he eventually argued that imperfect knowledge in politicians and voters prevented the reaching of that equilibrium.\n\nDemocracy is also criticised for frequent elections due to the instability of coalition governments. Coalitions are frequently formed \"after\" the elections in many countries (for example India) and the basis of alliance is predominantly to enable a viable majority, not an ideological concurrence.\n\nThis opportunist alliance not only has the handicap of having to cater to too many ideologically opposing factions, but it is usually short lived since any perceived or actual imbalance in the treatment of coalition partners, or changes to leadership in the coalition partners themselves, can very easily result in the coalition partner withdrawing its support from the government.\n\nDemocratic institutions work on consensus to decide an issue, which usually takes longer than a unilateral decision.\n\nM. S. Golwalkar in his book Bunch of Thoughts describes democracy as, \"is to a very large extent only a myth in practice...The high-sounding concept of \"individual freedom\" only meant the freedom of those talented few to exploit the rest.\"\n\nThis is a simple form of appealing to the short term interests of the voters.\n\nAnother form is commonly called Pork barrel, where local areas or political sectors are given special benefits but whose costs are spread among all taxpayers.\n\nMere elections are just one aspect of the democratic process. Other tenets of democracy, like relative equality and freedom, are frequently absent in ostensibly democratic countries.\n\nMoreover, in many countries, democratic participation is less than 50% at times, and it can be argued that election of individual(s) instead of ideas disrupts democracy.\n\nThe new establishment of democratic institutions, in countries where the associated practices have as yet been uncommon or deemed culturally unacceptable, can result in institutions that are not sustainable in the long term. One circumstance supporting this outcome may be when it is part of the common perception among the populace that the institutions were established as a direct result of foreign pressure.\n\nSustained regular inspection from democratic countries, however effortful and well-meaning, are normally not sufficient in preventing the erosion of democratic practices. In the cases of several African countries, corruption still is rife in spite of democratically elected governments, as one of the most severe examples, Zimbabwe, is often perceived to have backfired into outright militarism.\n\nEconomist Donald Wittman has written numerous works attempting to counter criticisms of democracy common among his colleagues. He argues democracy is efficient based on the premise of rational voters, competitive elections, and relatively low political transactions costs. Economist Bryan Caplan argues that, while Wittman makes strong arguments for the latter two points, the first is vitiated by the insurmountable evidence for voter irrationality. The problem is not merely the voters' lack of information, but their poor judgement. For many voters, the difficulty of learning about a particular issue is too high compared to the likely costs of ignorance, but this ignorance does not lessen their enthusiasm for voting. Other economists, such as Meltzer and Richard, have added that as industrial activity in a democracy increases, so too do the people's demands for subsidies and support from the government. By the median voter theorem, only a few people actually hold the balance of power in the country, and many may be unhappy with their decisions. In this way, they argue, democracies are inefficient.\n\nSuch a system could result in a wealth disparity or racial discrimination. Fierlbeck (1998) points out that such a result is not necessarily due to a failing in the democratic process, but rather, \"because democracy is responsive to the desires of a large middle class increasingly willing to disregard the muted voices of economically marginalized groups within its own borders.\" The will of the democratic majority may not always be in the best interest of all citizens.\n\nVoters may not be educated enough to exercise their democratic rights prudently. Politicians may take advantage of voters' irrationality, and compete more in the field of public relations and tactics, than in ideology. While arguments against democracy are often taken by advocates of democracy as an attempt to maintain or revive traditional hierarchy and autocratic rule, many extensions have been made to develop the argument further. In Lipset's 1959 essay about the requirements for forming democracy, he found that almost all emerging democracies provided good education. However, education alone cannot sustain a democracy, though Caplan did note in 2005 that as people become educated, they think more like economists.\n\nPoliticians and special interests have attempted to manipulate public opinion for as long as recorded history − this has put into question the feasibility of democratic government. Critics claim that mass media actually shapes public opinion, and can therefore be used to \"control\" democracy. Opinion polls before the election are under special criticism. Furthermore, the disclosure of reputation damaging material shortly before elections may be used to significantly manipulate public opinion. In the United States the FBI was criticized for announced that the agency would examine potentially incriminating evidence against Hillary Clinton's use of a private email server just 11 days before the election. It has been said that misinformation − such as fake news − has become central to elections around the world. In December 2016 United States' intelligence agencies have concluded that Russia worked \"to undermine public faith in the U.S. democratic process, denigrate Secretary [Hillary] Clinton, and harm her electability and potential presidency\" − including passing material against the Democrats to WikiLeaks to discredit the election and favor Donald Trump. Social bots and other forms of online propaganda as well as search engine result algorithms may be used to alter the perception and opinion of voters. In 2016 Andrés Sepúlveda disclosed that he manipulated public opinion to rig elections in Latin America. According to him with a budget of $600,000 he led a team of hackers that stole campaign strategies, manipulated social media to create false waves of enthusiasm and derision, and installed spyware in opposition offices to help Enrique Peña Nieto, a right-of-center candidate, win the election.\n\nVarious reasons can be found for eliminating or suppressing political opponents. Methods such as false flags, counterterrorism-laws, planting or creating compromising material and perpetuation of public fear may be used to suppress dissent. After a failed coup d'état over 110,000 people have been purged and nearly 40,000 have been imprisoned in Turkey, which is or was considered to be a democratic nation, during the 2016 Turkish purges.\n\nFake parties, phantom political rivals and \"scarecrow\" opponents may be used to undermine the opposition.\n\nRobert A. Dahl defines democracies as systems of government that respond nearly fully to each and every one of their citizens. He then poses that no such, fully responsive system exists today. However, this does not mean that \"partially\" democratic regimes do not exist—they do. Thus, Dahl rejects a democracy dichotomy in favor of a democratization spectrum. To Dahl, the question is not whether a country is a democracy or not. The question is to what extent a country is experiencing democratization at a national level. Dahl measures this democratization in terms of the country's endorsement and reception of public contestation. And polyarchy, or \"rule of the many people,\" is the only existing form of democratizeable government; that is, it is within polyarchies that democratization can flourish. Countries do not immediately transform from hegemonies and competitive oligarchies into democracies. Instead, a country that adopts democracy as its form of government can only claim to have switched to polyarchy, which is conducive to, but does not guarantee, democratization. Dahl's polyarchy spectrum ends at the point in which a country becomes a full polyarchy at the national level and begins to democratize at the subnational level, among its social and private affairs. Dahl is not deeply concerned about the limits of his polyarchy spectrum because he believes that most countries today still have a long way before they reach full polyarchy status. For Dahl, whatever lies beyond full polyarchy is only possible, and thus only a concern, for advanced countries like the Western Europe.\n\nPlato's \"Republic\" presents a critical view of democracy through the narration of Socrates: \"Democracy, which is a charming form of government, full of variety and disorder, and dispensing a sort of equality to equals and unequaled alike.\" In his work, Plato lists 5 forms of government from best to worst. Assuming that \"the Republic\" was intended to be a serious critique of the political thought in Athens, Plato argues that only Kallipolis, an aristocracy led by the unwilling philosopher-kings (the wisest men), is a just form of government.\n\nPlato rejected Athenian democracy on the basis that such democracies were anarchic societies without internal unity, that they followed citizens' impulses rather than pursuing the common good, that democracies are unable to allow a sufficient number of their citizens to have their voices heard, and that such democracies were typically run by fools. Plato attacked Athenian democracies for mistaking anarchy for freedom. The lack of coherent unity in Athenian democracy made Plato conclude that such democracies were a mere collection of individuals occupying a common space rather than a form of political organization.\n\nAccording to Plato, other forms of government place too much focus on lesser virtues and degenerate into other forms from best to worst, starting with timocracy, which overvalues honour, then oligarchy, which overvalues wealth, which is followed by democracy. In democracy, the oligarchs, or merchant, are unable to wield their power effectively and the people take over, electing someone who plays on their wishes (for example, by throwing lavish festivals). However, the government grants the people too much freedom, and the state degenerates into the fourth form, tyranny, or mob rule.\n\nThe constitutions of many countries have parts of them that restrict the nature of the types of laws that legislatures\ncan pass. A fundamental idea behind some of these restrictions, is that the majority of a population and its elected legislature can often be the source of minority persecutions, such as with racial discrimination. For example, during the mid-1930s and mid-1970s in the democratic country of Sweden, the government forcibly sterilized thousands of innocent women. They were sterilized due to \"'mental defects', or simply because they were of mixed race.\" A second example is when, in 2014 in Pakistan, \"a Christian couple were burnt alive in a brick kiln by a mob for their alleged burning of the pages of Quran.\" This was followed by little police retaliation and a statement from the President of Pakistan stating that his government would protect the rights and interests of the Christian community. Some countries throughout the world have judiciaries where judges can serve for long periods of time, and often serve under appointed posts. This is often balanced, however, by the fact that some trials are decided by juries. While many, like Wittman, have argued that democracies work much the same way as the free market and that there is competition among parties to prevent oppression by the majority, others have argued that there is actually very little competition among political parties in democracies due to the high cost associated with campaigning.\n\nJohn T. Wenders, a professor of Economics at the University of Idaho, writes:\n\nAdditionally, some political scientists question the notion that majority rule is an \"uncontested good.\" If we base our critique on the definition of democracy as governance based on the will of the majority, there can be some foreseeable consequences to this form of rule. For example, Fierlbeck (1998: 12) points out that the middle class majority in a country may decide to redistribute wealth and resources into the hands of those that they feel are most capable of investing or increasing them. Of course this is only a critique of a subset of types of democracy that primarily use majority rule.\n\nUS President James Madison devoted the whole of Federalist No. 10 to a scathing critique of democracy and offered that republics are a far better solution, saying: \"...democracies have ever been spectacles of turbulence and contention; have ever been found incompatible with personal security or the rights of property; and have in general been as short in their lives as they have been violent in their deaths.\" Madison offered that republics were superior to democracies because republics safeguarded against tyranny of the majority, stating in Federalist No. 10: \"the same advantage which a republic has over a democracy, in controlling the effects of faction, is enjoyed by a large over a small republic\".\n\nThe Founding Fathers of the United States intended to address this criticism by combining democracy with republicanism. A constitution would limit the powers of what a simple majority can accomplish.\n\nMachiavelli put the idea that democracies will tend to cater to the whims of the people, who follow false ideas to entertain themselves, squander their reserves, and do not deal with potential threats to their rule until it is too late. He put forth a cyclical theory of government where monarchies tend to decay into aristocracies, which then decay into democracies, which subsequently decay into anarchy, then tyranny, then return to monarchy. An example is the timeline of France before, during, and after the Revolution until the last Bourbon Monarch.\n\nHowever Machiavelli's definition of democracy was narrower than the current one. He hypothesized that a hybrid system of government incorporating facets of all three major types (monarchy, aristocracy and democracy) could break this cycle. Many modern democracies that have separation of powers are claimed to represent these kinds of hybrid governments. However, in modern democracies there is usually no direct correlation with Machiavelli's idea, because of weakening of the separation of powers, or erosion of the original function of the various branches. For example, the modern United States executive branch has slowly accumulated more power from the legislative branch, and the Senate no longer functions as a quasi-aristocratic body as was originally intended, since senators are now democratically elected.\n\nSome have tried to argue that the Coase theorem applies to political markets as well. Daron Acemoglu, however, provides evidence to the contrary, claiming that the Coase Theorem is only valid while there are \"rules of the game,\" so to speak, that are being enforced by the government. But when there is nobody there to enforce the rules for the government itself, there is no way to guarantee that low transaction costs will lead to an efficient outcome in democracies.\n\n\n"}
{"id": "53421127", "url": "https://en.wikipedia.org/wiki?curid=53421127", "title": "Drohobych Petroleum and Gas College", "text": "Drohobych Petroleum and Gas College\n\nDrohobych Petroleum and Gas College (Ukrainian: \"\") is one of technical colleges in Drohobych, Ukraine. It was first established as Drohobych Petroleum Technical College in 1945, transforming into \"Petroleum and Gas College\" in 2012. It is the alma mater of many Ukrainian scientists, businessmen and politicians.\n\nDrohobych Petroleum and Gas College was founded as Drohobych Petroleum Technical College on January 20, 1945 (Decree №58 of the Ukrainian Government). Since its establishment the school trained over 20 thousand specialists in oil, gas and their refining industry.\n\nThe first graduation ceremony was held in 1948.\n\nGiven the increasing demand for industry specialists by Carpathian region companies, college opened a branch department in Boryslav in, 1953. Evening courses were dedicated to specialists training in the fields of \"Exploitation of oil and gas wells\" and \"The technology of oil and gas\".\n\nIn 1957 Drohobych Petroleum Technical College was merged with Lviv College of Petroleum Refining.\n\nIn 1958 college established distance-learning department, conducting training of specialists in the fields of \"Drilling oil and gas wells\", \"Exploitation of oil and gas wells\", \"Equipment of oil and gas industries\" and \"Chemical technology of oil and gas\".\n\nIn 1959 a new branch (evening school) was opened in Dolyna with following desiplines offered: \"Exploitation of oil and gas wells\", \"Equipment oil and gas industry\" and \"Oil and gas industry enterprises planning\".\n\nDue to the decrease in the number of applicants for evening departments, Boryslav and Dolyna branches ceased operations in 1973 and 1975 respectively.\n\nIn 1997 Petroleum Technical College received a license qualifying it for training junior specialists in the field of \"Banking\" and \"Gas and oil exploitation and storage management\".\n\nCurrently, the college prepares specialists in nearly all specialties related to oil and gas extraction and processing.\n\nIn December 2006, the Ministry of Education and Science of Ukraine (Decree №64) accredited Petroleum Technical College as a higher educational institution of the first level and issued its first certificate of accreditation (license serial no. AB482982) granting college the right to provide educational services related to obtaining higher education level qualification \"junior specialist\".\n\nIn August 2012 college was renamed to \"State Higher Educational Institution \"Drohobych Petroleum and Gas College\" (Decree №870 of the Ministry of Education of Ukraine).\n\nCollege training is based on primary and secondary education on a full-time, part-time and distance learning basis. Total student population: 1600. Annual college intake ranges from 330 to 400 students. 46 of 99 teaching staff are lecturers of the first category, 7 - teaching Methodists, 3 PhDs, 4 Candidates of Science.\n\nThe College operates in conjunction with the Ivano-Frankivsk National Technical University of Oil and Gas, Kiev Polytechnic Institute and Dnipro State Chemical Technology University. College is also represented as educational consulting office of Ivano-Frankivsk National Technical University of Oil and Gas where alumni may continue their education at university of IV-th accreditation level. The best college graduates can enroll in partner universities for shorter term programmes (3 years instead of 4).\n\nFull-time / part-time teaching is based on personal financing contracts and government scholarship schemes.\n\nList of specialties:\n\n\n\n\n"}
{"id": "37715947", "url": "https://en.wikipedia.org/wiki?curid=37715947", "title": "DubaiSat-2", "text": "DubaiSat-2\n\nDubaiSat-2 is an electro-optical Earth observation satellite built by the Emirates Institution for Advanced Science and Technology under an agreement with Satrec Initiative, a satellite manufacturing company in South Korea. EIAST’s objective with DubaiSat-2 is to provide electro-optical images, that can be commercialized, for users within the United Arab Emirates and beyond and to develop and implement new technologies not used in DubaiSat-1. EIAST also intends to continue manpower training for the UAE’s space program. 16 UAE engineers have been working on the design, development, testing and manufacturing of the satellite. The participation of the UAE engineers, who are currently working in South Korea, has increased by 100 per cent from the DubaiSat-1 project.\n\nThe space segment consists of a spacecraft bus and an electro-optical payload. The electro-optical payload is a push-broom camera with Time Delay Integration (TDI) sensors (1 panchromatic and 4 multi-spectral bands). DubaiSat-2 is designed for a sun-synchronous orbit of 600 km, with a spatial resolution of 1m PAN and 4m Multispectral (MS), and a Swath of 12.2 km.\n\nThe modules in the satellite use two CAN Bus networks to communicate with each other and it has the capacity to store approximately 17,000 km of image data. It also includes an experimental propulsion system for orbit correction and maintenance. The satellite’s expected lifetime is at least five years.\n\nDubaiSat-2 has a hexagonal shape. The mechanical bus consists of 2 decks and an upper sun shield to protect the cold propellants from solar and Earth radiation. The electronics are distributed on the decks and on the side panels. Four solar panels are attached to the sides of the satellite. Longerons and rails make up the bus structure frame. On the top, carbon-fiber-reinforced polymer (CFRP) struts hold the Sun shield at the baffle of the \"High Resolution Advanced Imaging System\" (HiRAIS). The High-Resolution Imaging System is attached to the bus at the internal deck. The mechanical configuration of the satellite is less than 2000 mm in height and less than 1500 mm in diameter. The total mass of the satellite is less than 300 kg.\n\nThe power system supplies and controls the required voltage levels and current essential for satellite operation during its mission. The power system uses a rechargeable Li-ion battery to provide power for the satellite’s payload and other subsystems. The system is divided into two stages, first is the charging stage and the second is the discharging stage.\nThe charging stage is made of a solar power generator and power regulator. DubaiSat-2 generates more than 450W of power using four solar panels. Each solar panel contains 6 arrays and each array consists of 26 cells. The solar panels charge the batteries. The battery charging process is handled and regulated by the \"Battery Charging Regulators\" modules. DubaiSat-2 has three battery charging regulators with hot redundancy configuration which seamlessly take over if the primary battery charging regulator fails. The spacecraft can function normally with only two battery charging regulators.\n\nThe \"High-Resolution Advanced Imaging System (HiRAIS)\" is the primary payload of Dubaisat-2. It is an advanced Earth observation Camera which captures high resolution images and is capable of other functions such as high-speed data transmission. The HiRAIS is a compact and light weight instrument; and is built from a composite material to obtain high strength while keeping its weight down. The camera is based on a Korsch optical design which uses a three-mirror configuration to reduce optical irregularities and provides a wide field of view.\n\nThe HiRAIS consists of three main units: the Electro-Optical Subsystem, the Solid-State Recorder Unit, and the Image Transmission Unit. The Electro-Optical Subsystem is composed of the telescope, an auxiliary camera module, and a Focal Plane Assembly which are all integrated into one system. It is a pushbroom type camera with a 1-meter Ground Sampling Distance (GSD) for panchromatic imagery and 4 meter GSD in four multispectral bands.\n\nThe Solid-State Recorder Unit handles the processing, storage and maintenance of image data and is capable of storing image data received from the Focal Plane Assembly during the imaging mode. It is also responsible for image data compressing, encrypting and encoding before sending it to the Image Transmission Unit during downloading mode. The unit has a storage capacity of 256 Gbit arranged in 4 identical storage boards.\n\nThe Image Transmission Unit provides transmission of X-band data at a rate of 160 Mbit/s. It is divided into two main components: X-band Transmitter Unit and a high gain X-band Antenna.\n\nDubaiSat-2 is propelled by a propulsion system called \"The Hall Effect Propulsion System\". It is an electrical propulsion system with Xenon gas fuel and microwave cathode. The system will be used for the satellite's orbit correction and maintenance.\n\nDubaiSat-2 ground system consists of the Main Mission Control Station, the Subsidiary Mission Control Station, the Main Image Receiving and Processing Station, Customer Image Receiving and Processing Station, and the Antenna system.\n\nThe ground station monitors and controls the satellite. From there satellite operation planning is conducted, which includes imaging and download scenarios, mission timelines, orbit maintenance operation, and image downloads. The ground station is located at EIAST’s premises in Dubai, United Arab Emirates.\n\nDubaiSat-2 was successfully launched by a Dnepr rocket operated by Kosmotras of the Russian Federation. It has been placed into a polar Sun Synchronous Orbit at an altitude of 600 km and local UAE time of descending node of 10:30 am. Initially set for the last quarter of 2012, the launch took place at 07:10:16 UTC on 21 November 2013. Thirty one other satellites were deployed by the rocekt, with an additional payload remaining attached to the upper stage. The revisit time for a ground location will be 8 days maximum. The tilting capability of DubaiSat-2 can go up to ±45° roll tilt, ±30° pitch tilt.\n\nIn 2018, DubaiSat-2 and its predecessor, DubaiSat-1, was joined and will work together with UAE's third satellite, KhalifaSat.\n\n\n \n"}
{"id": "10537022", "url": "https://en.wikipedia.org/wiki?curid=10537022", "title": "Dynamics Explorer", "text": "Dynamics Explorer\n\nDynamics Explorer was a NASA mission, launched on August 3, 1981 and terminated on February 28, 1991. It consisted of two unmanned satellites, DE-1 and DE-2, whose purpose was to investigate the interractions between plasmas in the magnetosphere and those in the ionosphere. The two satellites were launched together into polar coplanar orbits, which allowed them to simultaneously observe the upper and lower parts of the atmosphere.\n\nBoth spacecraft had a polygonal shape, and were approximately 137 cm in diameter and 115 cm high. Each also had a 200-cm radio antenna and two 6-meter booms which were needed to distance some of the equipment from the main body of the spacecraft. They were stacked on top of each other and launched aboard a Delta 3000 booster rocket. Upon reaching orbit, the two spacecraft departed from the booster and entered separate orbits. Dynamics Explorer 1 was placed into a high altitude elliptical orbit, while DE-2 was put into a lower orbit that was also more circular.\n\nThe main instrument aboard Dynamics Explorer 1 was the Plasma Wave Instrument (PWI). This instrument, designed and built by the Plasma Wave Group, measured auroral kilometric radiation, auroral hiss, Z-mode radiation, and narrow band electromagnetic emissions. Additional Instruments aboard the spacecraft included:\n\nThe Dynamics Explorer 2 carried the following instruments for data collection:\n\nAs a result of a malfunction in the Delta 3000 booster rocket in which its main engine shut off slightly early, DE-2 was placed into a slightly lower orbit than was anticipated. This was not a serious problem, however, and the spacecraft had lasted its expected lifespan when it re-entered the Earth's atmosphere on February 19, 1983. DE-1, being in a higher orbit, continued to collect data until 1991, when the mission was officially terminated.\n\n"}
{"id": "1323240", "url": "https://en.wikipedia.org/wiki?curid=1323240", "title": "Electrohydrodynamics", "text": "Electrohydrodynamics\n\nElectrohydrodynamics (EHD), also known as electro-fluid-dynamics (EFD) or electrokinetics, is the study of the dynamics of electrically charged fluids. It is the study of the motions of ionized particles or molecules and their interactions with electric fields and the surrounding fluid. The term may be considered to be synonymous with the rather elaborate electrostrictive hydrodynamics. ESHD covers the following types of particle and fluid transport mechanisms: electrophoresis, electrokinesis, dielectrophoresis, electro-osmosis, and electrorotation. In general, the phenomena relate to the direct conversion of electrical energy into kinetic energy, and \"vice versa\".\n\nIn the first instance, shaped electrostatic fields (ESF's) create hydrostatic pressure (HSP, or motion) in dielectric media. When such media are fluids, a flow is produced. If the dielectric is a vacuum or a solid, no flow is produced. Such flow can be directed against the electrodes, generally to move the electrodes. In such case, the moving structure acts as an electric motor. Practical fields of interest of EHD are the common air ioniser, electrohydrodynamic thrusters and EHD cooling systems.\n\nIn the second instance, the converse takes place. A powered flow of medium within a shaped electrostatic field adds energy to the system which is picked up as a potential difference by electrodes. In such case, the structure acts as an electrical generator.\n\nElectrokinesis is the particle or fluid transport produced by an electric field acting on a fluid having a net mobile charge. (See -kinesis for explanation and further uses of the -kinesis suffix.) \"Electrokinesis\" was first observed by Ferdinand Frederic Reuss during 1808, in the electrophoresis of clay particles The effect was also noticed and publicized in the 1920s by Thomas Townsend Brown which he called the Biefeld–Brown effect, although he seems to have miss-identified it as an electric field acting on gravity. The flow rate in such a mechanism is linear in the electric field. Electrokinesis is of considerable practical importance in microfluidics, because it offers a way to manipulate and convey fluids in microsystems using only electric fields, with no moving parts.\n\nThe force acting on the fluid, is given by the equation\n\nformula_1\n\nwhere, formula_2 is the resulting force, measured in newtons, formula_3 is the current, measured in amperes, formula_4 is the distance between electrodes, measured in metres, and formula_5\" \"is the ion mobility coefficient of the dielectric fluid, measured in m/(V·s).\n\nIf the electrodes are free to move within the fluid, while keeping their distance fixed from each other, then such a force will actually propel the electrodes with respect to the fluid.\n\n\"Electrokinesis\" has also been observed in biology, where it was found to cause physical damage to neurons by inciting movement in their membranes. It is discussed in R.J.Elul's \"Fixed charge in the cell membrane\" (1967).\n\nIn October 2003, Dr. Daniel Kwok, Dr. Larry Kostiuk and two graduate students from the University of Alberta discussed a method of hydrodynamic to electrical energy conversion by exploiting the natural electrokinetic properties of a liquid such as ordinary tap water, by pumping fluids through tiny micro-channels with a pressure difference. This technology could some day provide a practical and clean energy storage device, replacing today's batteries, for devices such as mobile phones or calculators which would be charged up by simply pumping water to high pressure. Pressure would then be released on demand, for fluid flow to take place over the micro-channels. When water travels, or streams over a surface, the ions of which water is made \"rub\" against the solid, leaving the surface slightly charged. Kinetic energy from the moving ions would thus be converted to electrical energy. Although the power generated from a single channel is extremely small, millions of parallel micro-channels can be used to increase the power output.\nThis streaming potential, water-flow phenomenon was discovered in 1859 by German physicist Georg Hermann Quincke. \n\nThe fluid flows in microfluidic and nanofluidic devices are often stable and strongly damped by viscous forces (with Reynolds numbers of order unity or smaller). However, heterogeneous ionic conductivity fields in the presence of applied electric fields can, under certain conditions, generate an unstable flow field owing to electrokinetic instabilities (EKI). Conductivity gradients are prevalent in on-chip electrokinetic processes such as preconcentration methods (e.g. field amplified sample stacking and isoelectric focusing), multidimensional assays, and systems with poorly specified sample chemistry. The dynamics and periodic morphology of \"electrokinetic instabilities\" are similar to other systems with Rayleigh–Taylor instabilities. The particular case of a plat plane geometry with homogeneous ions injection in the bottom side leads to a mathematical frame identical to the Rayleigh–Bénard convection.\n\nEKI's can be leveraged for rapid mixing or can cause undesirable dispersion in sample injection, separation and stacking. These instabilities are caused by a coupling of electric fields and ionic conductivity gradients that results in an electric body force. This coupling results in an electric body force in the bulk liquid, outside the electric double layer, that can generate temporal, convective, and absolute flow instabilities. Electrokinetic flows with conductivity gradients become unstable when the electroviscous stretching and folding of conductivity interfaces grows faster than the dissipative effect of molecular diffusion.\n\nSince these flows are characterized by low velocities and small length scales, the Reynolds number is below 0.01 and the flow is \"laminar\". The onset of instability in these flows is best described as an electric \"Rayleigh number\".\n\nLiquids can be printed at nanoscale by pyro-EHD.\n\n\n"}
{"id": "2495757", "url": "https://en.wikipedia.org/wiki?curid=2495757", "title": "Gas flare", "text": "Gas flare\n\nA gas flare, alternatively known as a flare stack, is a gas combustion device used in industrial plants such as petroleum refineries, chemical plants, natural gas processing plants as well as at oil or gas production sites having oil wells, gas wells, offshore oil and gas rigs and landfills. \n\nIn industrial plants, flare stacks are primarily used for burning off flammable gas released by pressure relief valves during unplanned over-pressuring of plant equipment. During plant or partial plant startups and shutdowns, flare stacks are also often used for the planned combustion of gases over relatively short periods.\n\nGas flaring at many oil and gas production sites protects against the dangers of over-pressuring industrial plant equipment. When petroleum crude oil is extracted and produced from onshore or offshore oil wells, raw natural gas associated with the oil is brought to the surface as well. Especially in areas of the world lacking pipelines and other gas transportation infrastructure, vast amounts of such associated gas are commonly flared as waste or unusable gas. The flaring of associated gas may occur at the top of a vertical flare stack (as in the adjacent photo) or it may occur in a ground-level flare in an earthen pit. Preferably, associated gas is reinjected into the reservoir, which saves it for future use while maintaining higher well pressure and crude oil producibility. \n\nWhen industrial plant equipment items are over-pressured, the pressure relief valve is an essential safety device that automatically release gases and sometimes liquids. Those pressure relief valves are required by industrial design codes and standards as well as by law.\n\nThe released gases and liquids are routed through large piping systems called \"flare headers\" to a vertical elevated flare. The released gases are burned as they exit the flare stacks. The size and brightness of the resulting flame depends upon the flammable material's flow rate in joules per hour (or btu per hour).\n\nMost industrial plant flares have a vapor-liquid separator (also known as a knockout drum) upstream of the flare to remove any large amounts of liquid that may accompany the relieved gases.\n\nSteam is very often injected into the flame to reduce the formation of black smoke. When too much steam is added, a condition known as \"over steaming\" can occur resulting in reduced combustion efficiency and higher emissions. To keep the flare system functional, a small amount of gas is continuously burned, like a pilot light, so that the system is always ready for its primary purpose as an over-pressure safety system.\n\nThe adjacent flow diagram depicts the typical components of an overall industrial flare stack system:\n\n\nImproperly operated flares may emit methane and other volatile organic compounds as well as sulfur dioxide and other sulfur compounds, which are known to exacerbate asthma and other respiratory problems. Other emissions from improperly operated flares may include, aromatic hydrocarbons (benzene, toluene, xylenes) and benzapyrene, which are known to be carcinogenic.\n\nFlaring can affect wildlife by attracting birds and insects to the flame. Approximately 7,500 migrating songbirds were attracted to and killed by the flare at the liquefied natural gas terminal in Saint John, New Brunswick, Canada on September 13, 2013. Similar incidents have occurred at flares on offshore oil and gas installations. Moths are known to be attracted to lights. A brochure published by the Secretariat of the Convention on Biological Diversity describing the Global Taxonomy Initiative describes a situation where \"a taxonomist working in a tropical forest noticed that a gas flare at an oil refinery was attracting and killing hundreds of these [hawk or sphinx] moths. Over the course of the months and years that the refinery was running a vast number of moths must have been killed, suggesting that plants could not be pollinated over a large area of forest\".\n\nAs of the end of 2011, 150 × 10 cubic meters (5.3 × 10 cubic feet) of associated gas are flared annually. That is equivalent to about 25 per cent of the annual natural gas consumption in the United States or about 30 per cent of the annual gas consumption in the European Union. If it were to reach market, this quantity of gas (at a nominal value of $5.62 per 1000 cubic feet) would be worth $29.8 billion USD.\n\nAlso as of the end of 2011, 10 countries accounted for 72 per cent of the flaring, and twenty for 86 per cent. The top ten leading contributors to world gas flaring at the end of 2011, were (in declining order): Russia (27%), Nigeria (11%), Iran (8%), Iraq (7%), United States (5%), Algeria (4%), Kazakhstan (3%), Angola (3%), Saudi Arabia (3%) and Venezuela (3%).\n\nThat amount of flaring and burning of associated gas from oil drilling sites is a significant source of carbon dioxide (CO) emissions. Coupled with fossil fuel combustion and cement production, flaring's carbon dioxide emissions in 2010 have tripled (1300 ± 110 GtCO2) compared to the last recording (years 1750-1970, 420 ± 35 GtCO had been emitted.) 2400 × 10 tons of carbon dioxide are emitted annually in this way and it amounts to about 1.2 per cent of the worldwide emissions of carbon dioxide. That may seem to be insignificant, but in perspective it is more than half of the \"Certified Emissions Reductions\" (a type of carbon credits) that have been issued under the rules and mechanisms of the Kyoto Protocol as of June 2011.\n\nSatellite data show that from 2005 to 2010, global gas flaring decreased by about 20%. The most significant reductions in terms of volume were made in Russia (down 40%) and Nigeria (down 29%).\n\nThe Obama administration implemented rules curbing flaring, and subsequently the Trump administration attempted to delay implementation of the rules. In October 2017 a federal magistrate judge vacated the Department of Interior's move to delay implementation.\n\nMethane's estimated global warming potential is 34 times greater than that of CO. Therefore, to the extent that gas flares convert methane to CO before it is released into the atmosphere, they reduce the amount of global warming that would otherwise occur.\n\n"}
{"id": "2199445", "url": "https://en.wikipedia.org/wiki?curid=2199445", "title": "Gravity train", "text": "Gravity train\n\nA gravity train is a theoretical means of transportation intended to go between two points on the surface of a sphere, following a straight tunnel that goes directly from one point to the other through the interior of the sphere.\n\nIn a large body such as a planet, this train could be left to accelerate using just the force of gravity, since during the first half of the trip (from the point of departure until the middle), the downward pull towards the center of gravity would pull it towards the destination. During the second half of the trip, the acceleration would be in the opposite direction relative to the trajectory, but (ignoring the effects of friction) the speed acquired before would be enough to cancel this deceleration exactly (so that the train would reach its destination with speed equal to zero).\n\nIn reality, there are two reasons gravity trains do not exist. First, a lengthy transit distance would pierce the Earth's mantle and traverse a region where rock is more fluid than solid. No materials are known that would withstand the tremendous heat and pressure in the inner core. Temperature is estimated as 5,700 K (5,430 °C; 9,800 °F), and pressure as high as about 330 to 360 gigapascals (3,300,000 to 3,600,000 atm). Secondly, frictional losses would be significant. Rolling friction losses could be reduced by using a magnetically levitated train. However, unless all air is evacuated from the tunnel, frictional losses due to air resistance would render the gravity train unusable. Evacuating the atmosphere to make it a vactrain would eliminate this drag but would require additional power. Such objections would not apply for solid planets and moons that do not have an atmosphere.\n\nIn the 17th century, British scientist Robert Hooke presented the idea of an object accelerating inside a planet in a letter to Isaac Newton. A gravity train project was seriously presented to the Paris Academy of Sciences in the 19th century. The same idea was proposed, without calculation, by Lewis Carroll in 1893 in \"Sylvie and Bruno Concluded\". The idea was rediscovered in the 1960s when physicist Paul Cooper published a paper in the \"American Journal of Physics\" suggesting that gravity trains be considered for a future transportation project.\n\nUnder the assumption of a spherical planet with uniform density, and ignoring relativistic effects as well as friction, a gravity train has the following properties:\n\nFor gravity trains between points which are not the antipodes of each other, the following hold:\n\nOn the planet Earth specifically, since a gravity train's movement is the projection of a very Low Earth Orbit satellite's movement onto a line, it has the following parameters:\n\nTo put some numbers in perspective, the deepest current bore hole is the Kola Superdeep Borehole with a true depth of 12,262 meters. While to cover a distance between London and Paris (350 km) via a hypocycloidical path would need the creation of a 55,704-metre-deep hole. This depth isn't only 4.5 times as deep; it will also already need a tunnel that passes inside the Earth's mantle.\n\nUsing the approximations that the Earth is perfectly\nspherical and of uniform density formula_1, and the fact that within a\nuniform hollow sphere there is no gravity, the gravitational acceleration formula_2\nexperienced by a body within the Earth is proportional to the\nratio of the distance from the center formula_3 to the Earth's radius formula_4.\nThis is because underground at distance formula_3 from the center is like\nbeing on the surface of a planet of radius formula_3, within a hollow sphere\nwhich contributes nothing.\n\nOn the surface, formula_8, so the gravitational acceleration is formula_9. Hence, the gravitational acceleration at formula_3 is\n\nIn the case of a straight line through the center of the Earth, the\nacceleration of the body is equal to that of gravity: it is falling freely\nstraight down. We start falling the surface, so at time formula_12\n(treating acceleration and velocity as positive downwards):\n\nDifferentiating twice: \n\nwhere formula_15. This class of problems, where there is a restoring force proportional to the displacement away from zero, has general solutions of the form formula_16, and describes simple harmonic motion such as in a spring or pendulum.\n\nIn this case formula_17 so that formula_18, we begin at the surface at time zero, and oscillate back and forth forever.\n\nThe travel time to the antipodes is half of one cycle of this oscillator, that is the time for the argument to formula_19 to sweep out formula_20 radians. Using simple approximations of formula_21 that time is\n\nFor the more general case of the straight line path between any two points on the surface of a sphere we calculate the acceleration of the body as it moves frictionlessly along its straight path.\n\nThe body travels along AOB, O being the midpoint of the path, and the closest point to the center of the Earth on this path. At distance formula_3 along this path, the force of gravity depends on distance formula_24 to the center of the Earth as above. Using the shorthand formula_25 for length OC:\n\nThe resulting acceleration on the body, because is it on a frictionless\ninclined surface, is formula_27:\n\nBut formula_29 is formula_30, so substituting:\n\nwhich is exactly the same for this new formula_3, distance along AOB away from O, as for the formula_3 in the diametric case along ACD. So the remaining analysis is the same, accommodating the initial condition that the maximal formula_3 is formula_35 the complete equation of motion is\n\nThe time constant formula_15 is the same as in the diametric case so the journey time is still 42 minutes; it's just that all the distances and speeds are scaled by the constant formula_38.\n\nThe time constant formula_39 depends only on formula_40 so if we expand that we get\n\nwhich depends only on the gravitational constant and formula_1 the density of the planet. The size of the planet is immaterial; the journey time is the same if the density is the same.\n\nThe 1914 book Tik-Tok of Oz has a tube, that passed from Oz, through the center of the earth, emerging in the country of the Great Jinjin, Tittiti-Hoochoo.\n\nIn the 2012 movie \"Total Recall\", a gravity train called \"The Fall\" goes through the center of the Earth to commute between Western Europe and Australia.\n\nIn the video game Super Mario Galaxy, there are various planets with holes that Mario can jump through to illustrate the gravity train effect.\n\n\n\n"}
{"id": "371197", "url": "https://en.wikipedia.org/wiki?curid=371197", "title": "Hardwood", "text": "Hardwood\n\nHardwood is wood from dicot trees. These are usually found in broad-leaved temperate and tropical forests. In temperate and boreal latitudes they are mostly deciduous, but in tropics and subtropics mostly evergreen. Hardwood contrasts with softwood (which is from gymnosperm trees).\n\nHardwoods are produced by angiosperm trees that reproduce by flowers, and have broad leaves. Many species are deciduous. Those of temperate regions lose their leaves every autumn as temperatures fall and are dormant in the winter, but those of tropical regions may shed their leaves in response to seasonal or sporadic periods of drought. Hardwood from deciduous species, such as oak, normally shows annual growth rings, but these may be absent in some tropical hardwoods.\n\nHardwoods have a more complex structure than softwoods and are often much slower growing as a result. The dominant feature separating \"hardwoods\" from softwoods is the presence of pores, or vessels. The vessels may show considerable variation in size, shape of perforation plates (simple, scalariform, reticulate, foraminate), and structure of cell wall, such as spiral thickenings.\n\nAs the name suggests, the wood from these trees is generally harder than that of softwoods, but there are significant exceptions. In both groups there is an enormous variation in actual wood hardness, with the range in density in hardwoods completely including that of softwoods; some hardwoods (\"e.g.\", balsa) are softer than most softwoods, while yew is an example of a hard softwood.\n\nHardwoods are employed in a large range of applications, including fuel, tools, construction, boat building, furniture making, musical instruments, flooring, cooking, barrels, and manufacture of charcoal. Solid hardwood joinery tends to be expensive compared to softwood. In the past, tropical hardwoods were easily available, but the supply of some species, such as Burma teak and mahogany, is now becoming scarce due to over-exploitation. Cheaper \"hardwood\" doors, for instance, now consist of a thin veneer bonded to a core of softwood, plywood or medium-density fibreboard (MDF). Hardwoods may be used in a variety of objects, but are most frequently seen in furniture or musical instruments because of their density which adds to durability, appearance, and performance. Different species of hardwood lend themselves to different end uses or construction processes. This is due to the variety of characteristics apparent in different timbers, including density, grain, pore size, growth and fibre pattern, flexibility and ability to be steam bent. For example, the interlocked grain of elm wood (\"Ulmus\" spp.) makes it suitable for the making of chair seats where the driving in of legs and other components can cause splitting in other woods. \n\nThere is a correlation between density and calories/volume. This makes the denser hardwoods like oak, cherry, and apple more suited for camp fires, cooking fires, and smoking meat, as they tend to burn hotter and longer than softwoods like pine or cedar whose low-density construction and highly-flammable sap make them burn quickly and without producing quite as much heat.\n\n\n\n"}
{"id": "22616026", "url": "https://en.wikipedia.org/wiki?curid=22616026", "title": "ITIES", "text": "ITIES\n\nIn electrochemistry, ITIES is an acronym for the \"\"i\"nterface between \"t\"wo \"i\"mmiscible \"e\"lectrolyte \"s\"olutions\". Usually, one electrolyte is an aqueous electrolyte composed of hydrophilic ions such as NaCl dissolved in water and the other electrolyte is a lipophilic salt such as tetrabutylammonium tetraphenylborate dissolved in an organic solvent immiscible with water such as nitrobenzene, or 1,2-dichloroethane.\n\nAn ITIES is an electrochemical interface that is either polarisable or polarised. An ITIES is said \"polarisable\" if one can change the Galvani potential difference, or in other words the difference of inner potentials between the two adjacent phases, without noticeably changing the chemical composition of the respective phases (i.e. without noticeable electrochemical reactions taking place at the interface). An ITIES system is said \"polarised\" if the distribution of the different charges and redox species between the two phases determines the Galvani potential difference.\n\nThree major classes of charge transfer reactions can be studied at an ITIES:\n\nThe Nernst equation for an ion transfer reaction reads\n\nwhere formula_2 is the standard transfer potential defined as the Gibbs energy of transfer expressed in a voltage scale.\n\nThe Nernst equation for a single heterogeneous electron transfer reaction reads\n\nwhere formula_5 is the standard redox potential for the interfacial transfer of electrons defined as the difference the standard redox potentials of the two redox couples but referred to the aqueous standard hydrogen electrode (SHE).\n\nTo study charge transfer reactions of an ITIES, a four-electrode cell is used.\n\nTwo reference electrodes are used to control the polarisation of the interface, and two counter electrodes made of noble metals are used to pass the current. The aqueous supporting electrolyte must be hydrophilic, such as LiCl, and the organic electrolyte must be lipophilic, such as tetraheptylammonium tetra-pentafluorophenyl borate.\n\nContrary to a neutral solute, the partition coefficient of an ion depends of the Galvani potential difference between the two phases:\n\nWhen a salt is distributed between two phases, the Galvani potential difference is called the distribution potential and is obtained from the respective Nernst equations for the cation C and the anion A to read\n\nwhere γ represents the activity coefficient.\n\n"}
{"id": "53069342", "url": "https://en.wikipedia.org/wiki?curid=53069342", "title": "Iota Fornacis", "text": "Iota Fornacis\n\nThe Bayer designation ι Fornacis (Iota Fornacis, ι For) is shared by two stars in the constellation Fornax:\n"}
{"id": "171616", "url": "https://en.wikipedia.org/wiki?curid=171616", "title": "Judith Wright", "text": "Judith Wright\n\nJudith Arundell Wright (31 May 191525 June 2000) was an Australian poet, environmentalist and campaigner for Aboriginal land rights.\n\nJudith Wright was born in Armidale, New South Wales. The eldest child of Phillip Wright and his first wife, Ethel, she spent most of her formative years in Brisbane and Sydney. Wright was of Cornish ancestry. After the early death of her mother, she lived with her aunt and then boarded at New England Girls' School after her father's remarriage in 1929. After graduating, Wright studied Philosophy, English, Psychology and History at the University of Sydney. At the beginning of World War II, she returned to her father's station to help during the shortage of labour caused by the war.\n\nWright's first book of poetry, \"The Moving Image\", was published in 1946 while she was working at the University of Queensland as a research officer. Then, she had also worked with Clem Christesen on the literary magazine \"Meanjin,\" the first edition of which was published in late 1947. In 1950 she moved to Mount Tamborine, Queensland, with the novelist and abstract philosopher Jack McKinney. Their daughter Meredith was born in the same year. They married in 1962, but Jack was to live only until 1966.\n\nIn 1966, she published \"The Nature of Love\", her first collection of short stories, through Sun Press, Melbourne. Set mainly in Queensland, they include 'The Ant-lion' ,'The Vineyard Woman', 'Eighty Acres', 'The Dugong', 'The Weeping Fig' and 'The Nature of Love', all first published in The Bulletin. Wright was nominated for the 1967 Nobel Prize for Literature.\n\nWith David Fleay, Kathleen McArthur and Brian Clouston, Wright was a founding member and, from 1964 to 1976, President, of the Wildlife Preservation Society of Queensland. She was the second Australian to receive the Queen's Gold Medal for Poetry, in 1991.\n\nFor the last three decades of her life, she lived near the New South Wales town of Braidwood. Allegedly, she had moved to the Braidwood area to be closer to H. C. Coombs, her lover of 25 years, who was based in Canberra.\n\nShe started to lose her hearing in her mid-20s, and she became completely deaf by 1992.\n\nJudith Wright died in Canberra on 25 June 2000, aged 85.\n\nJudith Wright was the author of several collections of poetry, including \"The Moving Image\", \"Woman to Man\", \"The Gateway\", \"The Two Fires\", \"Birds\", \"The Other Half\", \"Magpies\", \"Shadow\", \"Hunting Snake,\" among others.\n\nHer work is noted for a keen focus on the Australian environment, which began to gain prominence in Australian art in the years following World War II. She deals with the relationship between settlers, Indigenous Australians and the bush, among other themes. Wright's aesthetic centres on the relationship between mankind and the environment, which she views as the catalyst for poetic creation. Her images characteristically draw from the Australian flora and fauna, yet contain a mythic substrata that probes at the poetic process, limitations of language, and the correspondence between inner existence and objective reality.\n\nHer poems have been translated into several languages, including Italian, Japanese and Russian.\n\nIn 2003, the National Library of Australia published an expanded edition of Wright's collection titled \"Birds\". Most of these poems were written in the 1950s when she was living on Tamborine Mountain in southeast Queensland. Meredith McKinney, Wright's daughter, writes that they were written at \"a precious and dearly-won time of warmth and bounty to counterbalance at last what felt, in contrast, the chilly dearth and difficulty of her earlier years\". McKinney goes on to say that \"many of these poems have a newly relaxed, almost conversational tone and rhythm, an often humorous ease and an intimacy of voice that surely reflects the new intimacies and joys of her life\". Despite the joy reflected in the poems, however, they also acknowledge \"the experiences of cruelty, pain and death that are inseparable from the lives of birds as of humans ... and [turn] a sorrowing a clear-sighted gaze on the terrible damage we have done and continue to do to our world, even as we love it\".\n\nWright was well known for her campaigning in support of the conservation of the Great Barrier Reef and Fraser Island. With some friends, she helped found one of the earliest nature conservation movements.\n\nWright was also an impassioned advocate for the Aboriginal land rights movement. Tom Shapcott, reviewing \"With Love and Fury\", her posthumous collection of selected letters published in 2007, comments that her letter on this topic to the Australian Prime Minister John Howard was \"almost brutal in its scorn\". Shortly before her death, she attended a march in Canberra for reconciliation between non-indigenous Australians and the Aboriginal people.\n\nIn 2009 as part of the Q150 celebrations, Judith Wright was announced as one of the Q150 Icons of Queensland for her role as an \"Influential Artists\".\n\nIn June 2006 the Australian Electoral Commission (AEC) announced that the new federal electorate in Queensland, which was to be created at the 2007 federal election, would be named Wright in honor of her accomplishments as a \"poet and in the areas of arts, conservation and indigenous affairs in Queensland and Australia\". However, in September 2006 the AEC announced it would name the seat after John Flynn, the founder of the Royal Flying Doctor Service, due to numerous objections from people fearing the name Wright may be linked to disgraced former Queensland ALP MP Keith Wright. Under the 2009 redistribution of Queensland, a new seat in southeast Queensland was created and named in Wright's honour; it was first contested in 2010.\n\nThe Judith Wright Centre of Contemporary Arts in Fortitude Valley, Brisbane is named after her.\n\nOn 2 January 2008, it was announced that a future suburb in the district of Molonglo Valley, Canberra would be named \"Wright\". There is a street in the Canberra suburb of Franklin named after her, as well. Another of the Molonglo Valley suburbs is to be named after Wright's lover, \"Nugget\" Coombs.\n\n\n\n\n\n\nListed here are print references cited in the article.\n\n\n"}
{"id": "25511123", "url": "https://en.wikipedia.org/wiki?curid=25511123", "title": "Kunyu Quantu", "text": "Kunyu Quantu\n\nThe Kunyu Quantu (), or Full Map of the World, was a map of the world developed by Jesuit father Ferdinand Verbiest during his mission in China in 1674. A copy is in the Hunterian Museum.\n\nThe map follows the earlier works of Matteo Ricci, such as the \"Kunyu Wanguo Quantu\".\n\n\n"}
{"id": "10985761", "url": "https://en.wikipedia.org/wiki?curid=10985761", "title": "Köprülü Canyon", "text": "Köprülü Canyon\n\nKöprülü Canyon () is a canyon and a National Park in the Province of Antalya, Turkey. Covering an area of , it was established as a national park by the Ministry of Forest on December 12, 1973.\n\nThe canyon is deep in some places and stretches for along the Köprü River. There are fish restaurants at the rest area; fresh trout is a specialty. The Roman Oluklu Bridge (Olukluköprü) over a tributary gorge and the Bugrum Bridge over the Kocadere stream were engineering feats of their time.\n\nThe park, rich in flora and fauna and noted for its wild beauty, is from Antalya. Northeast of Antalya on the Side road, take the turnoff for Taşağıl and Beşkonak to reach Köprülü Canyon National Park. A circular scenic route criss-crosses, passing through extensive forests and over waterfalls.\n\n"}
{"id": "52598433", "url": "https://en.wikipedia.org/wiki?curid=52598433", "title": "Lillering Forest", "text": "Lillering Forest\n\nLillering Forest is a forest and protected Natura 2000 area in Denmark. It is located in peninsular Denmark in Aarhus Municipality, south of Harlev and some 10 km. west of Aarhus. It is a deciduous forest covering some 50 hectares and it is composed mainly of beech and European ash. It is privately owned but protected and partially managed by Aarhus Municipality and the Danish Nature Agency. The Natura 2000 site covers 135 hectares and also encompasses Tåstrup Bog, Tåstrup Lake and Stjær Forest, south of Lillering Forest. The southern section with Stjær Forest is within, and managed by, Skanderborg Municipality.\n\nLillering Forest and surroundings has been known as a botanically interesting locality for many years with the earliest description dating back to 1877 (Zahrtmann 1877). Some 400 vascular plants have been registered, about a third of Danish plants. A number of plants primarily exist in south-eastern Denmark, and further south, with the northern boundary being at or around Aarhus and some of these are well represented in Lillering Forest. The most important might be hornbeam which has a significant presence. Others are plants such as dead-nettles, white butterbur, bitter-cress, wall barley, yellow and blue anemone and the endangered multicolored viola. In addition a total of nine species of orchids have been registered although in later years only six has been noted - these are the broad-leaved helleborine, green-flowered helleborine, early-purple orchid, western marsh orchid, early marsh-orchid, eggleaf twayblade and the greater butterfly-orchid. The border zone between Tåstrup Bog and Lillering Forest contains woodland geranium and mountain melick.\nThe area contains several rare species of geometer moths associated with buckthorns that grow at the woodland edge or in Tåstrup Bog. The nearby meadows is home to a rare, small owl that lives on species of mouse-ear chickweed. The meadows in the forest has a number of moths such as the six-spot burnet which are endangered in Denmark. Some 6 species are registered of which 3 in Jutland are found only here and 4 are associated with hackberry at the woodland edge. The stream Bøgebakke Bæk has a large population of the uncommon mayfly siphlonurus aestivalis.\nThe Natura 2000 protected area includes Tåstrup Lake and adjoining wetlands (c. 20 ha.), Tåstrup Bog (c. 25 ha.), Lillering Forest (ca. 50 ha.) and Stjær Forest consisting of the forests Søskov and Stjær Stenskov (c. 20 ha). The four areas are not connected since former agricultural land separates Lillering Forest and Tåstrup Bog to the north and Tåstrup Lake and Stjær Forest to the south. \n"}
{"id": "80604", "url": "https://en.wikipedia.org/wiki?curid=80604", "title": "List of Latin names of rivers", "text": "List of Latin names of rivers\n\nFollowing is a list of rivers stating the Latin and equivalent English name.\n\n¹ - Latinized spelling of a Greek name.\n\n\n"}
{"id": "3225134", "url": "https://en.wikipedia.org/wiki?curid=3225134", "title": "List of Lepidoptera that feed on larches", "text": "List of Lepidoptera that feed on larches\n\nLarches (\"Larix\" species) are used as food plants by the caterpillars of a number of Lepidoptera species, including:\n\nSpecies which feed exclusively on \"Larix\"\n\n\nSpecies which feed on \"Larix\" and other plants\n\n\n"}
{"id": "15585719", "url": "https://en.wikipedia.org/wiki?curid=15585719", "title": "List of Ptilotus species", "text": "List of Ptilotus species\n\nThis is a list of the currently recognized species in the genus \"Ptilotus\". All species are native to continental Australia, mostly in the arid regions, though one species also occurs in Tasmania and another in Malesia.\n\n\n"}
{"id": "15566656", "url": "https://en.wikipedia.org/wiki?curid=15566656", "title": "List of books about energy issues", "text": "List of books about energy issues\n\nThis is a list of books about energy issues:\n\n"}
{"id": "46894465", "url": "https://en.wikipedia.org/wiki?curid=46894465", "title": "List of mountains in Uganda", "text": "List of mountains in Uganda\n\nThis is a list of mountains in Uganda.\n\n"}
{"id": "18356800", "url": "https://en.wikipedia.org/wiki?curid=18356800", "title": "List of parties to the Comprehensive Nuclear-Test-Ban Treaty", "text": "List of parties to the Comprehensive Nuclear-Test-Ban Treaty\n\nThe contracting states to the Comprehensive Nuclear-Test-Ban Treaty (CTBT) are the states that have signed and ratified the international agreement banning all nuclear explosions in all environments. Technically they will not be \"parties\" until the treaty enters into force, at which point these states will also be Member States of the Comprehensive Nuclear-Test-Ban Treaty Organization (CTBTO), which comes into existence upon entry into force of the treaty. Non-contracting states are also listed, including those that are signatories and those are not. States Signatories are Members of the CTBTO Preparatory Commission.\n\nOn September 24, 1996, the Comprehensive Nuclear-Test-Ban Treaty (CTBT) was opened for signature. All five nuclear weapons states recognized under the Nuclear Non-Proliferation Treaty (China, France, Russia, the United Kingdom, and the United States) signed the treaty, with 66 other states following that day. Fiji became the first state to ratify the treaty on October 10, 1996. As of September 2018, 184 states have signed and 167 states have ratified the treaty. Most recently, Tuvalu signed and Thailand ratified the treaty in September 2018.\n\nSignatures are received at the United Nations Headquarters in New York City by authorized representatives of the state. Ratification is achieved with the approval of either or both chamber of the legislature and executive of the state. The instrument of ratification serves as the document binding the state to the international treaty and can be accepted only with the validating signature of the head of state or other official with full powers to sign it. The instrument is deposited with the Secretary-General of the United Nations.\n\nUnder the CTBT, there are 195 Annex 1 states which include a subset of 44 Annex 2 states.\nEight Annex 2 states have not ratified the treaty: China, Egypt, Iran, Israel and the United States have already signed the Treaty, whereas India, North Korea and Pakistan have not signed it. The treaty will come into force only with the signature and ratification of the above Annex 2 states of the treaty, 180 days after they have all deposited their instruments of ratification.\n\nIn addition, the UN observer State of Palestine has neither signed nor acceded to the convention.\n\nIn 1998, India said it would only sign the treaty if the United States presented a schedule for eliminating its nuclear stockpile, a condition the United States rejected.\n\nIn 2016, Israeli Prime Minister Benjamin Netanyahu said that its ratification was dependent upon \"the regional context and the appropriate timing\".\n\nThe United States has signed the CTBT, but not ratified it; there is ongoing debate whether to ratify the CTBT.\n\nThe United States has stated that its ratification of the CTBT is conditional upon:\n\nProponents of ratification claim that it would:\n\n\nOpponents of ratification claim that:\n\nOn 13 October 1999, the United States Senate rejected ratification of the CTBT. During his 2008 presidential election campaign Barack Obama said that \"As president, I will reach out to the Senate to secure the ratification of the CTBT at the earliest practical date.\" In his speech in Prague on 5 April 2009, he announced that “[To] achieve a global ban on nuclear testing, my administration will immediately and aggressively pursue U.S. ratification of the Comprehensive Test Ban Treaty. After more than five decades of talks, it is time for the testing of nuclear weapons to finally be banned.”\n\nAn article in Bulletin of the Atomic Scientists describes how a North Korean underground nuclear test on 25 May 2009 was detected and the source located by GPS satellites. The authors suggest that the effectiveness of GPS satellites for detecting nuclear explosions enhances the ability to verify compliance with the Comprehensive Nuclear Test Ban Treaty, giving the United States more reason to ratify it.\n\n"}
{"id": "2163161", "url": "https://en.wikipedia.org/wiki?curid=2163161", "title": "List of plant communities in the British National Vegetation Classification", "text": "List of plant communities in the British National Vegetation Classification\n\nThe following is the list of the 286 plant communities which comprise the British National Vegetation Classification (NVC). These are grouped by major habitat category, as used in the five volumes of \"British Plant Communities\", the standard work describing the NVC.\n\nThe following 25 communities are described in Volume 1 of \"British Plant Communities\". For an article summarising these communities see Woodland and scrub communities in the British National Vegetation Classification system.\n\n\nThe following 38 communities are described in Volume 2 of \"British Plant Communities\". For an article summarising these communities see Mires in the British National Vegetation Classification system.\n\n\nThe following 22 communities are described in Volume 2 of \"British Plant Communities\". For an article summarising these communities see Heaths in the British National Vegetation Classification system.\n\n\nThe following 13 communities are described in Volume 3 of \"British Plant Communities\". For an article summarising these communities see Mesotrophic grasslands in the British National Vegetation Classification system.\n\n\nThe following 14 communities are described in Volume 3 of \"British Plant Communities\". For an article summarising these communities see Calcicolous grasslands in the British National Vegetation Classification system.\n\n\nThe following 21 communities are described in Volume 3 of \"British Plant Communities\". For an article summarising these communities see Calcifugous grasslands and montane communities in the British National Vegetation Classification system.\n\n\nThe following 24 communities are described in Volume 4 of \"British Plant Communities\". For an article summarising these communities see Aquatic communities in the British National Vegetation Classification system.\n\n\nThe following 28 communities are described in Volume 4 of \"British Plant Communities\". For an article summarising these communities see Swamps and tall-herb fens in the British National Vegetation Classification system.\n\n\nThe following 28 communities are described in Volume 5 of \"British Plant Communities\". For an article summarising these communities see Salt-marsh communities in the British National Vegetation Classification system.\n\n\nThe following 19 communities are described in Volume 5 of \"British Plant Communities\". For an article summarising these communities see Shingle, strandline and sand-dune communities in the British National Vegetation Classification system.\n\n\nThe following 12 communities are described in Volume 5 of \"British Plant Communities\". For an article summarising these communities see Maritime cliff communities in the British National Vegetation Classification system.\n\n\nThe following 42 communities are described in Volume 5 of \"British Plant Communities\". For an article summarising these communities see Vegetation of open habitats in the British National Vegetation Classification system.\n\n"}
{"id": "148571", "url": "https://en.wikipedia.org/wiki?curid=148571", "title": "List of satellites which have provided data on Earth's magnetosphere", "text": "List of satellites which have provided data on Earth's magnetosphere\n\nBelow is a list of satellites which have provided data on the Earth's magnetosphere.\n\n1γ = 10 oersted = Dynamic range of instrumentation\n"}
{"id": "43780773", "url": "https://en.wikipedia.org/wiki?curid=43780773", "title": "List of unclimbed mountains of Nepal", "text": "List of unclimbed mountains of Nepal\n\nThis is the comprehensive list of all unclimbed mountains of Nepal.\n"}
{"id": "23676013", "url": "https://en.wikipedia.org/wiki?curid=23676013", "title": "Marine energy", "text": "Marine energy\n\nMarine energy or marine power (also sometimes referred to as ocean energy, ocean power, or marine and hydrokinetic energy) refers to the energy carried by ocean waves, tides, salinity, and ocean temperature differences. The movement of water in the world’s oceans creates a vast store of kinetic energy, or energy in motion. Some of this energy can be harnessed to generate electricity to power homes, transport and industries.\n\nThe term marine energy encompasses both wave power i.e. power from surface waves, and tidal power i.e. obtained from the kinetic energy of large bodies of moving water. Offshore wind power is not a form of marine energy, as wind power is derived from the wind, even if the wind turbines are placed over water.\n\nThe oceans have a tremendous amount of energy and are close to many if not most concentrated populations. Ocean energy has the potential of providing a substantial amount of new renewable energy around the world.\n\nThere is the potential to develop 20,000–80,000 terawatt-hours per year (TWh/y) of electricity generated by changes in ocean temperatures, salt content, movements of tides, currents, waves and swells\n\nIndonesia as archipelagic country with three quarter of the area is ocean, has 49 GW recognized potential ocean energy and has 727 GW theoretical potential ocean energy.\n\nThe oceans represent a vast and largely untapped source of energy in the form of surface waves, fluid flow, salinity gradients, and thermal.\n\nMarine and Hydrokinetic (MHK) or marine energy development in U.S. and international waters includes projects using the following devices:\n\n\nStrong ocean currents are generated from a combination of temperature, wind, salinity, bathymetry, and the rotation of the Earth. The Sun acts as the primary driving force, causing winds and temperature differences. Because there are only small fluctuations in current speed and stream location with no changes in direction, ocean currents may be suitable locations for deploying energy extraction devices such as turbines.\n\nOcean currents are instrumental in determining the climate in many regions around the world. While little is known about the effects of removing ocean current energy, the impacts of removing current energy on the farfield environment may be a significant environmental concern. The typical turbine issues with blade strike, entanglement of marine organisms, and acoustic effects still exists; however, these may be magnified due to the presence of more diverse populations of marine organisms using ocean currents for migration purposes. Locations can be further offshore and therefore require longer power cables that could affect the marine environment with electromagnetic output.\n\nAt the mouth of rivers where fresh water mixes with salt water, energy associated with the salinity gradient can be harnessed using pressure-retarded reverse osmosis process and associated conversion technologies. Another system is based on using freshwater upwelling through a turbine immersed in seawater, and one involving electrochemical reactions is also in development.\n\nSignificant research took place from 1975 to 1985 and gave various results regarding the economy of PRO and RED plants. It is important to note that small-scale investigations into salinity power production take place in other countries like Japan, Israel, and the United States. In Europe the research is concentrated in Norway and the Netherlands, in both places small pilots are tested. Salinity gradient energy is the energy available from the difference in salt concentration between freshwater with saltwater. This energy source is not easy to understand, as it is not directly occurring in nature in the form of heat, waterfalls, wind, waves, or radiation.\n\nWater typically varies in temperature from the surface warmed by direct sunlight to greater depths where sunlight cannot penetrate. This differential is greatest in tropical waters, making this technology most applicable in water locations. A fluid is often vaporized to drive a turbine that may generate electricity or produce desalinized water. Systems may be either open-cycle, closed-cycle, or hybrid.\n\nThe energy from moving masses of water — a popular form of hydroelectric power generation. Tidal power generation comprises three main forms, namely: tidal stream power, tidal barrage power, and dynamic tidal power.\n\nSolar energy from the Sun creates temperature differentials that result in wind. The interaction between wind and the surface of water creates waves, which are larger when there is a greater distance for them to build up. Wave energy potential is greatest between 30° and 60° latitude in both hemispheres on the west coast because of the global direction of wind. When evaluating wave energy as a technology type, it is important to distinguish between the four most common approaches: point absorber buoys, surface attenuators, oscillating water columns, and overtopping devices.\n\nThe wave energy sector is reaching a significant milestone in the development of the industry, with positive steps towards commercial viability being taken. The more advanced device developers are now progressing beyond single unit demonstration devices and are proceeding to array development and multi-megawatt projects. The backing of major utility companies is now manifesting itself through partnerships within the development process, unlocking further investment and, in some cases, international co-operation.\n\nAt a simplified level, wave energy technology can be located near-shore and offshore. Wave energy converters can also be designed for operation in specific water depth conditions: deep water, intermediate water or shallow water. The fundamental device design will be dependent on the location of the device and the intended resource characteristics.\n\nPetroleum and natural gas beneath the ocean floor are also sometimes considered a form of ocean energy. An ocean engineer directs all phases of discovering, extracting, and delivering offshore petroleum (via oil tankers and pipelines,) a complex and demanding task. Also centrally important is the development of new methods to protect marine wildlife and coastal regions against the undesirable side effects of offshore oil extraction.\n\nThe UK is leading the way in wave and tidal (marine) power generation. The world's first marine energy test facility was established in 2003 to kick start the development of the marine energy industry in the UK. Based in Orkney, Scotland, the European Marine Energy Centre (EMEC) has supported the deployment of more wave and tidal energy devices than at any other single site in the world. The Centre was established with around £36 million of funding from the Scottish Government, Highlands and Islands Enterprise, the Carbon Trust, UK Government, Scottish Enterprise, the European Union and Orkney Islands Council, and is the only accredited wave and tidal test centre for marine renewable energy in the world, suitable for testing a number of full-scale devices simultaneously in some of the harshest weather conditions while producing electricity to the national grid.\n\nClients that have tested at the centre include Aquamarine Power, AW Energy, Pelamis Wave Power, Seatricity, ScottishPower Renewables and Wello on the wave site, and Alstom (formerly Tidal Generation Ltd), ANDRITZ HYDRO Hammerfest, Kawasaki Heavy Industries, Magallanes, Nautricity, Open Hydro, Scotrenewables Tidal Power, and Voith on the tidal site. \n\nLeading the €11m FORESEA (Funding Ocean Renewable Energy through Strategic European Action) project, which provides funding support to ocean energy technology developers to access Europe's world-leading ocean energy test facilities, EMEC will welcome a number of wave and tidal clients to their pipeline for testing on site. \n\nBeyond device testing, EMEC also provides a wide range of consultancy and research services, and is working closely with Marine Scotland to streamline the consenting process for marine energy developers. EMEC is at the forefront in the development of international standards for marine energy, and is forging alliances with other countries, exporting its knowledge around the world to stimulate the development of a global marine renewables industry.\n\nCommon environmental concerns associated with marine energy developments include:\n\n\nThe Tethys database provides access to scientific literature and general information on the potential environmental effects of marine energy.\n\n\n\n"}
{"id": "8263355", "url": "https://en.wikipedia.org/wiki?curid=8263355", "title": "Merope", "text": "Merope\n\nMerope (; Greek: Μερόπη) was originally the name of several, probably unrelated, characters in Greek mythology. The name may refer to:\n\n\n\n\n\n"}
{"id": "20431", "url": "https://en.wikipedia.org/wiki?curid=20431", "title": "Momentum", "text": "Momentum\n\nIn Newtonian mechanics, linear momentum, translational momentum, or simply momentum (pl. momenta) is the product of the mass and velocity of an object. It is a vector quantity, possessing a magnitude and a direction in three-dimensional space. If is an object's mass and is the velocity (also a vector), then the momentum is\nIn SI units, it is measured in kilogram meters per second (kg⋅m/s). Newton's second law of motion states that a body's rate of change in momentum is equal to the net force acting on it.\n\nMomentum depends on the frame of reference, but in any inertial frame it is a \"conserved\" quantity, meaning that if a closed system is not affected by external forces, its total linear momentum does not change. Momentum is also conserved in special relativity (with a modified formula) and, in a modified form, in electrodynamics, quantum mechanics, quantum field theory, and general relativity. It is an expression of one of the fundamental symmetries of space and time: translational symmetry.\n\nAdvanced formulations of classical mechanics, Lagrangian and Hamiltonian mechanics, allow one to choose coordinate systems that incorporate symmetries and constraints. In these systems the conserved quantity is \"generalized momentum\", and in general this is different from the \"kinetic\" momentum defined above. The concept of generalized momentum is carried over into quantum mechanics, where it becomes an operator on a wave function. The momentum and position operators are related by the Heisenberg uncertainty principle.\n\nIn continuous systems such as electromagnetic fields, fluids and deformable bodies, a momentum density can be defined, and a continuum version of the conservation of momentum leads to equations such as the Navier–Stokes equations for fluids or the Cauchy momentum equation for deformable solids or fluids.\n\nMomentum is a vector quantity: it has both magnitude and direction. Since momentum has a direction, it can be used to predict the resulting direction and speed of motion of objects after they collide. Below, the basic properties of momentum are described in one dimension. The vector equations are almost identical to the scalar equations (see multiple dimensions).\n\nThe momentum of a particle is conventionally represented by the letter . It is the product of two quantities, the particle's mass (represented by the letter ) and its velocity ():\n\nThe unit of momentum is the product of the units of mass and velocity. In SI units, if the mass is in kilograms and the velocity is in meters per second then the momentum is in kilogram meters per second (kg⋅m/s). In cgs units, if the mass is in grams and the velocity in centimeters per second, then the momentum is in gram centimeters per second (g⋅cm/s).\n\nBeing a vector, momentum has magnitude and direction. For example, a 1 kg model airplane, traveling due north at 1 m/s in straight and level flight, has a momentum of 1 kg⋅m/s due north measured with reference to the ground.\n\nThe momentum of a system of particles is the vector sum of their momenta. If two particles have respective masses and , and velocities and , the total momentum is\nThe momenta of more than two particles can be added more generally with the following:\n\nA system of particles has a center of mass, a point determined by the weighted sum of their positions:\n\nIf all the particles are moving, the center of mass will generally be moving as well (unless the system is in pure rotation around it). If the center of mass is moving at velocity , the momentum is:\nThis is known as Euler's first law.\n\nIf the net force applied to a particle is a constant , and is applied for a time interval , the momentum of the particle changes by an amount\n\nIn differential form, this is Newton's second law; the rate of change of the momentum of a particle is equal to the instantaneous force acting on it,\n\nIf the net force experienced by a particle changes as a function of time, , the change in momentum (or impulse ) between times and is\n\nImpulse is measured in the derived units of the newton second (1 N⋅s = 1 kg⋅m/s) or dyne second (1 dyne⋅s = 1 g⋅m/s)\n\nUnder the assumption of constant mass , it is equivalent to write\nhence the net force is equal to the mass of the particle times its acceleration.\n\n\"Example\": A model airplane of mass 1 kg accelerates from rest to a velocity of 6 m/s due north in 2 s. The net force required to produce this acceleration is 3 newtons due north. The change in momentum is 6 kg⋅m/s. The rate of change of momentum is 3 (kg⋅m/s)/s = 3 N.\n\nIn a closed system (one that does not exchange any matter with its surroundings and is not acted on by external forces) the total momentum is constant. This fact, known as the \"law of conservation of momentum\", is implied by Newton's laws of motion. Suppose, for example, that two particles interact. Because of the third law, the forces between them are equal and opposite. If the particles are numbered 1 and 2, the second law states that and . Therefore,\nwith the negative sign indicating that the forces oppose. Equivalently,\n\nIf the velocities of the particles are and before the interaction, and afterwards they are and , then\n\nThis law holds no matter how complicated the force is between particles. Similarly, if there are several particles, the momentum exchanged between each pair of particles adds up to zero, so the total change in momentum is zero. This conservation law applies to all interactions, including collisions and separations caused by explosive forces. It can also be generalized to situations where Newton's laws do not hold, for example in the theory of relativity and in electrodynamics.\n\nMomentum is a measurable quantity, and the measurement depends on the motion of the observer. For example: if an apple is sitting in a glass elevator that is descending, an outside observer, looking into the elevator, sees the apple moving, so, to that observer, the apple has a non-zero momentum. To someone inside the elevator, the apple does not move, so, it has zero momentum. The two observers each have a frame of reference, in which, they observe motions, and, if the elevator is descending steadily, they will see behavior that is consistent with those same physical laws.\n\nSuppose a particle has position in a stationary frame of reference. From the point of view of another frame of reference, moving at a uniform speed , the position (represented by a primed coordinate) changes with time as\nThis is called a Galilean transformation. If the particle is moving at speed in the first frame of reference, in the second, it is moving at speed\nSince does not change, the accelerations are the same:\nThus, momentum is conserved in both reference frames. Moreover, as long as the force has the same form, in both frames, Newton's second law is unchanged. Forces such as Newtonian gravity, which depend only on the scalar distance between objects, satisfy this criterion. This independence of reference frame is called Newtonian relativity or Galilean invariance.\n\nA change of reference frame, can, often, simplify calculations of motion. For example, in a collision of two particles, a reference frame can be chosen, where, one particle begins at rest. Another, commonly used reference frame, is the center of mass frame – one that is moving with the center of mass. In this frame,\nthe total momentum is zero.\n\nBy itself, the law of conservation of momentum is not enough to determine the motion of particles after a collision. Another property of the motion, kinetic energy, must be known. This is not necessarily conserved. If it is conserved, the collision is called an \"elastic collision\"; if not, it is an \"inelastic collision\".\n\nAn elastic collision is one in which no kinetic energy is absorbed in the collision. Perfectly elastic \"collisions\" can occur when the objects do not touch each other, as for example in atomic or nuclear scattering where electric repulsion keeps them apart. A slingshot maneuver of a satellite around a planet can also be viewed as a perfectly elastic collision. A collision between two pool balls is a good example of an \"almost\" totally elastic collision, due to their high rigidity, but when bodies come in contact there is always some dissipation.\n\nA head-on elastic collision between two bodies can be represented by velocities in one dimension, along a line passing through the bodies. If the velocities are and before the collision and and after, the equations expressing conservation of momentum and kinetic energy are:\n\nA change of reference frame can simplify analysis of a collision. For example, suppose there are two bodies of equal mass , one stationary and one approaching the other at a speed (as in the figure). The center of mass is moving at speed and both bodies are moving towards it at speed . Because of the symmetry, after the collision both must be moving away from the center of mass at the same speed. Adding the speed of the center of mass to both, we find that the body that was moving is now stopped and the other is moving away at speed . The bodies have exchanged their velocities. Regardless of the velocities of the bodies, a switch to the center of mass frame leads us to the same conclusion. Therefore, the final velocities are given by\n\nIn general, when the initial velocities are known, the final velocities are given by\nIf one body has much greater mass than the other, its velocity will be little affected by a collision while the other body will experience a large change.\n\nIn an inelastic collision, some of the kinetic energy of the colliding bodies is converted into other forms of energy (such as heat or sound). Examples include traffic collisions, in which the effect of lost kinetic energy can be seen in the damage to the vehicles; electrons losing some of their energy to atoms (as in the Franck–Hertz experiment); and particle accelerators in which the kinetic energy is converted into mass in the form of new particles.\n\nIn a perfectly inelastic collision (such as a bug hitting a windshield), both bodies have the same motion afterwards. If one body is motionless to begin with, the equation for conservation of momentum is\nso\nIn a frame of reference moving at the speed , the objects are brought to rest by the collision and 100% of the kinetic energy is converted to other forms of energy.\n\nOne measure of the inelasticity of the collision is the coefficient of restitution , defined as the ratio of relative velocity of separation to relative velocity of approach. In applying this measure to a ball bouncing from a solid surface, this can be easily measured using the following formula:\n\nThe momentum and energy equations also apply to the motions of objects that begin together and then move apart. For example, an explosion is the result of a chain reaction that transforms potential energy stored in chemical, mechanical, or nuclear form into kinetic energy, acoustic energy, and electromagnetic radiation. Rockets also make use of conservation of momentum: propellant is thrust outward, gaining momentum, and an equal and opposite momentum is imparted to the rocket.\n\nReal motion has both direction and velocity and must be represented by a vector. In a coordinate system with axes, velocity has components in the -direction, in the -direction, in the -direction. The vector is represented by a boldface symbol:\nSimilarly, the momentum is a vector quantity and is represented by a boldface symbol:\n\nThe equations in the previous sections, work in vector form if the scalars and are replaced by vectors and . Each vector equation represents three scalar equations. For example,\nrepresents three equations:\n\nThe kinetic energy equations are exceptions to the above replacement rule. The equations are still one-dimensional, but each scalar represents the magnitude of the vector, for example,\nEach vector equation represents three scalar equations. Often coordinates can be chosen so that only two components are needed, as in the figure. Each component can be obtained separately and the results combined to produce a vector result.\n\nA simple construction involving the center of mass frame can be used to show that if a stationary elastic sphere is struck by a moving sphere, the two will head off at right angles after the collision (as in the figure).\n\nThe concept of momentum plays a fundamental role in explaining the behavior of variable-mass objects such as a rocket ejecting fuel or a star accreting gas. In analyzing such an object, one treats the object's mass as a function that varies with time: . The momentum of the object at time is therefore . One might then try to invoke Newton's second law of motion by saying that the external force on the object is related to its momentum by , but this is incorrect, as is the related expression found by applying the product rule to :\n\nThis equation does not correctly describe the motion of variable-mass objects. The correct equation is\nwhere is the velocity of the ejected/accreted mass \"as seen in the object's rest frame\". This is distinct from , which is the velocity of the object itself as seen in an inertial frame.\n\nThis equation is derived by keeping track of both the momentum of the object as well as the momentum of the ejected/accreted mass (\"dm\"). When considered together, the object and the mass (\"dm\") constitute a closed system in which total momentum is conserved.\n\nNewtonian physics assumes that absolute time and space exist outside of any observer; this gives rise to Galilean invariance. It also results in a prediction that the speed of light can vary from one reference frame to another. This is contrary to observation. In the special theory of relativity, Einstein keeps the postulate that the equations of motion do not depend on the reference frame, but assumes that the speed of light is invariant. As a result, position and time in two reference frames are related by the Lorentz transformation instead of the Galilean transformation.\n\nConsider, for example, a reference frame moving relative to another at velocity in the direction. The Galilean transformation gives the coordinates of the moving frame as\nwhile the Lorentz transformation gives\nwhere is the Lorentz factor:\n\nNewton's second law, with mass fixed, is not invariant under a Lorentz transformation. However, it can be made invariant by making the \"inertial mass\" of an object a function of velocity:\n\nThe modified momentum,\nobeys Newton's second law:\n\nWithin the domain of classical mechanics, relativistic momentum closely approximates Newtonian momentum: at low velocity, is approximately equal to , the Newtonian expression for momentum.\n\nIn the theory of special relativity, physical quantities are expressed in terms of four-vectors that include time as a fourth coordinate along with the three space coordinates. These vectors are generally represented by capital letters, for example for position. The expression for the \"four-momentum\" depends on how the coordinates are expressed. Time may be given in its normal units or multiplied by the speed of light so that all the components of the four-vector have dimensions of length. If the latter scaling is used, an interval of proper time, , defined by\nis invariant under Lorentz transformations (in this expression and in what follows the metric signature has been used, different authors use different conventions). Mathematically this invariance can be ensured in one of two ways: by treating the four-vectors as Euclidean vectors and multiplying time by ; or by keeping time a real quantity and embedding the vectors in a Minkowski space. In a Minkowski space, the scalar product of two four-vectors and is defined as\n\nIn all the coordinate systems, the (contravariant) relativistic four-velocity is defined by\nand the (contravariant) four-momentum is\nwhere is the invariant mass. If (in Minkowski space), then\nUsing Einstein's mass-energy equivalence, , this can be rewritten as\nThus, conservation of four-momentum is Lorentz-invariant and implies conservation of both mass and energy.\n\nThe magnitude of the momentum four-vector is equal to :\nand is invariant across all reference frames.\n\nThe relativistic energy–momentum relationship holds even for massless particles such as photons; by setting it follows that\n\nIn a game of relativistic \"billiards\", if a stationary particle is hit by a moving particle in an elastic collision, the paths formed by the two afterwards will form an acute angle. This is unlike the non-relativistic case where they travel at right angles.\n\nThe four-momentum of a planar wave can be related to a wave four-vector\nFor a particle, the relationship between temporal components, , is the Planck–Einstein relation, and the relation between spatial components, , describes a de Broglie matter wave.\n\nNewton's laws can be difficult to apply to many kinds of motion because the motion is limited by \"constraints\". For example, a bead on an abacus is constrained to move along its wire and a pendulum bob is constrained to swing at a fixed distance from the pivot. Many such constraints can be incorporated by changing the normal Cartesian coordinates to a set of \"generalized coordinates\" that may be fewer in number. Refined mathematical methods have been developed for solving mechanics problems in generalized coordinates. They introduce a \"generalized momentum\", also known as the \"canonical\" or \"conjugate momentum\", that extends the concepts of both linear momentum and angular momentum. To distinguish it from generalized momentum, the product of mass and velocity is also referred to as \"mechanical\", \"kinetic\" or \"kinematic momentum\". The two main methods are described below.\n\nIn Lagrangian mechanics, a Lagrangian is defined as the difference between the kinetic energy and the potential energy :\n\nIf the generalized coordinates are represented as a vector and time differentiation is represented by a dot over the variable, then the equations of motion (known as the Lagrange or Euler–Lagrange equations) are a set of equations:\nIf a coordinate is not a Cartesian coordinate, the associated generalized momentum component does not necessarily have the dimensions of linear momentum. Even if is a Cartesian coordinate, will not be the same as the mechanical momentum if the potential depends on velocity. Some sources represent the kinematic momentum by the symbol .\n\nIn this mathematical framework, a generalized momentum is associated with the generalized coordinates. Its components are defined as\nEach component is said to be the \"conjugate momentum\" for the coordinate .\n\nNow if a given coordinate does not appear in the Lagrangian (although its time derivative might appear), then\nThis is the generalization of the conservation of momentum.\n\nEven if the generalized coordinates are just the ordinary spatial coordinates, the conjugate momenta are not necessarily the ordinary momentum coordinates. An example is found in the section on electromagnetism.\n\nIn Hamiltonian mechanics, the Lagrangian (a function of generalized coordinates and their derivatives) is replaced by a Hamiltonian that is a function of generalized coordinates and momentum. The Hamiltonian is defined as\nwhere the momentum is obtained by differentiating the Lagrangian as above. The Hamiltonian equations of motion are\nAs in Lagrangian mechanics, if a generalized coordinate does not appear in the Hamiltonian, its conjugate momentum component is conserved.\n\nConservation of momentum is a mathematical consequence of the homogeneity (shift symmetry) of space (position in space is the canonical conjugate quantity to momentum). That is, conservation of momentum is a consequence of the fact that the laws of physics do not depend on position; this is a special case of Noether's theorem.\n\nIn Maxwell's equations, the forces between particles are mediated by electric and magnetic fields. The electromagnetic force (\"Lorentz force\") on a particle with charge due to a combination of electric field and magnetic field is\n(in SI units).\nIt has an electric potential and magnetic vector potential .\nIn the non-relativistic regime, its generalized momentum is\nwhile in relativistic mechanics this becomes\n\nIn Newtonian mechanics, the law of conservation of momentum can be derived from the law of action and reaction, which states that every force has a reciprocating equal and opposite force. Under some circumstances, moving charged particles can exert forces on each other in non-opposite directions. Nevertheless, the combined momentum of the particles and the electromagnetic field is conserved.\n\nThe Lorentz force imparts a momentum to the particle, so by Newton's second law the particle must impart a momentum to the electromagnetic fields.\n\nIn a vacuum, the momentum per unit volume is\nwhere is the vacuum permeability and is the speed of light. The momentum density is proportional to the Poynting vector which gives the directional rate of energy transfer per unit area:\n\nIf momentum is to be conserved over the volume over a region , changes in the momentum of matter through the Lorentz force must be balanced by changes in the momentum of the electromagnetic field and outflow of momentum. If is the momentum of all the particles in , and the particles are treated as a continuum, then Newton's second law gives\nThe electromagnetic momentum is\nand the equation for conservation of each component of the momentum is\nThe term on the right is an integral over the surface area of the surface representing momentum flow into and out of the volume, and is a component of the surface normal of . The quantity is called the Maxwell stress tensor, defined as\n\nThe above results are for the \"microscopic\" Maxwell equations, applicable to electromagnetic forces in a vacuum (or on a very small scale in media). It is more difficult to define momentum density in media because the division into electromagnetic and mechanical is arbitrary. The definition of electromagnetic momentum density is modified to\nwhere the H-field is related to the B-field and the magnetization by\nThe electromagnetic stress tensor depends on the properties of the media.\n\nIn quantum mechanics, momentum is defined as a self-adjoint operator on the wave function. The Heisenberg uncertainty principle defines limits on how accurately the momentum and position of a single observable system can be known at once. In quantum mechanics, position and momentum are conjugate variables.\n\nFor a single particle described in the position basis the momentum operator can be written as\n\nwhere is the gradient operator, is the reduced Planck constant, and is the imaginary unit. This is a commonly encountered form of the momentum operator, though the momentum operator in other bases can take other forms. For example, in momentum space the momentum operator is represented as\n\nwhere the operator acting on a wave function yields that wave function multiplied by the value , in an analogous fashion to the way that the position operator acting on a wave function yields that wave function multiplied by the value \"x\".\n\nFor both massive and massless objects, relativistic momentum is related to the phase constant formula_66 by\nElectromagnetic radiation (including visible light, ultraviolet light, and radio waves) is carried by photons. Even though photons (the particle aspect of light) have no mass, they still carry momentum. This leads to applications such as the solar sail. The calculation of the momentum of light within dielectric media is somewhat controversial (see Abraham–Minkowski controversy).\n\nIn fields such as fluid dynamics and solid mechanics, it is not feasible to follow the motion of individual atoms or molecules. Instead, the materials must be approximated by a continuum in which there is a particle or fluid parcel at each point that is assigned the average of the properties of atoms in a small region nearby. In particular, it has a density and velocity that depend on time and position . The momentum per unit volume is .\n\nConsider a column of water in hydrostatic equilibrium. All the forces on the water are in balance and the water is motionless. On any given drop of water, two forces are balanced. The first is gravity, which acts directly on each atom and molecule inside. The gravitational force per unit volume is , where is the gravitational acceleration. The second force is the sum of all the forces exerted on its surface by the surrounding water. The force from below is greater than the force from above by just the amount needed to balance gravity. The normal force per unit area is the pressure . The average force per unit volume inside the droplet is the gradient of the pressure, so the force balance equation is\n\nIf the forces are not balanced, the droplet accelerates. This acceleration is not simply the partial derivative because the fluid in a given volume changes with time. Instead, the material derivative is needed:\nApplied to any physical quantity, the material derivative includes the rate of change at a point and the changes due to advection as fluid is carried past the point. Per unit volume, the rate of change in momentum is equal to . This is equal to the net force on the droplet.\n\nForces that can change the momentum of a droplet include the gradient of the pressure and gravity, as above. In addition, surface forces can deform the droplet. In the simplest case, a shear stress , exerted by a force parallel to the surface of the droplet, is proportional to the rate of deformation or strain rate. Such a shear stress occurs if the fluid has a velocity gradient because the fluid is moving faster on one side than another. If the speed in the direction varies with , the tangential force in direction per unit area normal to the direction is\nwhere is the viscosity. This is also a flux, or flow per unit area, of x-momentum through the surface.\n\nIncluding the effect of viscosity, the momentum balance equations for the incompressible flow of a Newtonian fluid are\nThese are known as the Navier–Stokes equations.\n\nThe momentum balance equations can be extended to more general materials, including solids. For each surface with normal in direction and force in direction , there is a stress component . The nine components make up the Cauchy stress tensor , which includes both pressure and shear. The local conservation of momentum is expressed by the Cauchy momentum equation:\nwhere is the body force.\n\nThe Cauchy momentum equation is broadly applicable to deformations of solids and liquids. The relationship between the stresses and the strain rate depends on the properties of the material (see Types of viscosity).\n\nA disturbance in a medium gives rise to oscillations, or waves, that propagate away from their source. In a fluid, small changes in pressure can often be described by the acoustic wave equation:\nwhere is the speed of sound. In a solid, similar equations can be obtained for propagation of pressure (P-waves) and shear (S-waves).\n\nThe flux, or transport per unit area, of a momentum component by a velocity is equal to . In the linear approximation that leads to the above acoustic equation, the time average of this flux is zero. However, nonlinear effects can give rise to a nonzero average. It is possible for momentum flux to occur even though the wave itself does not have a mean momentum.\n\nIn about 530 AD, working in Alexandria, Byzantine philosopher John Philoponus developed a concept of momentum in his commentary to Aristotle's \"Physics\". Aristotle claimed that everything that is moving must be kept moving by something. For example, a thrown ball must be kept moving by motions of the air. Most writers continued to accept Aristotle's theory until the time of Galileo, but a few were skeptical. Philoponus pointed out the absurdity in Aristotle's claim that motion of an object is promoted by the same air that is resisting its passage. He proposed instead that an impetus was imparted to the object in the act of throwing it. Ibn Sīnā (also known by his Latinized name Avicenna) read Philoponus and published his own theory of motion in \"The Book of Healing\" in 1020. He agreed that an impetus is imparted to a projectile by the thrower; but unlike Philoponus, who believed that it was a temporary virtue that would decline even in a vacuum, he viewed it as a persistent, requiring external forces such as air resistance to dissipate it.\nThe work of Philoponus, and possibly that of Ibn Sīnā, was read and refined by the European philosophers Peter Olivi and Jean Buridan. Buridan, who in about 1350 was made rector of the University of Paris, referred to impetus being proportional to the weight times the speed. Moreover, Buridan's theory was different from his predecessor's in that he did not consider impetus to be self-dissipating, asserting that a body would be arrested by the forces of air resistance and gravity which might be opposing its impetus.\n\nRené Descartes believed that the total \"quantity of motion\" () in the universe is conserved, where the quantity of motion is understood as the product of size and speed. This should not be read as a statement of the modern law of momentum, since he had no concept of mass as distinct from weight and size, and more importantly he believed that it is speed rather than velocity that is conserved. So for Descartes if a moving object were to bounce off a surface, changing its direction but not its speed, there would be no change in its quantity of motion. Galileo, in his \"Two New Sciences\", used the Italian word \"impeto\" to similarly describe Descartes' quantity of motion.\n\nLeibniz, in his \"Discourse on Metaphysics\", gave an argument against Descartes' construction of the conservation of the \"quantity of motion\" using an example of dropping blocks of different sizes different distances. He points out that force is conserved but quantity of motion, construed as the product of size and speed of an object, is not conserved.\n\nThe first correct statement of the law of conservation of momentum was by English mathematician John Wallis in his 1670 work, \"Mechanica sive De Motu, Tractatus Geometricus\": \"the initial state of the body, either of rest or of motion, will persist\" and \"If the force is greater than the resistance, motion will result\". Wallis uses \"momentum\" and \"vis\" for force. Newton's \"Philosophiæ Naturalis Principia Mathematica\", when it was first published in 1687, showed a similar casting around for words to use for the mathematical momentum. His Definition II defines \"quantitas motus\", \"quantity of motion\", as \"arising from the velocity and quantity of matter conjointly\", which identifies it as momentum. Thus when in Law II he refers to \"mutatio motus\", \"change of motion\", being proportional to the force impressed, he is generally taken to mean momentum and not motion. It remained only to assign a standard term to the quantity of motion. The first use of \"momentum\" in its proper mathematical sense is not clear but by the time of Jenning's \"Miscellanea\" in 1721, five years before the final edition of Newton's \"Principia Mathematica\", momentum or \"quantity of motion\" was being defined for students as \"a rectangle\", the product of and , where is \"quantity of material\" and is \"velocity\", .\n\n\n"}
{"id": "1506434", "url": "https://en.wikipedia.org/wiki?curid=1506434", "title": "Moraine-dammed lake", "text": "Moraine-dammed lake\n\nA moraine-dammed lake occurs when the terminal moraine has prevented some meltwater from leaving the valley. Its most common shape is that of a long ribbon (ribbon lake). \nExample of moraine dammed lakes include:\n\nIn the 19th century the Argentine explorer Francisco Perito Moreno suggested that many Patagonian lakes draining to the Pacific were in fact part of the Atlantic basin but had been moraine dammed during the quaternary glaciations changing their outlets to the west. He argued that as originally belonging to the Atlantic basin these lakes should be awarded to Argentina. Most of the lakes situated in the Himalaya of Nepal and Bhutan are also of the moraine dammed type. They may burst at any time. That is why the areas below such lakes have high risk of flooding.\n\n \n"}
{"id": "24703336", "url": "https://en.wikipedia.org/wiki?curid=24703336", "title": "Nokomis 3", "text": "Nokomis 3\n\nIn 1971, a group of scientists came together and formed a product called Nokomis 3. It is formulated to disperse oil that has been spilled into the ocean, lakes, bays, or rivers. This synergetic formulation of chemicals is water-based and non-flammable. This product has been used by the United States Navy, State of California Fish and Game and the United States Environmental Protection Agency (EPA). Marlin Supply Inc. is the main distributor of this product with laboratories in San Francisco, Texas, Florida and Mexico. Nokomis 3 is a highly effective mixture that, when it comes into contact with the particles, oils, or greases, breaks up those particles into millions more and speeds up the process of biodegradation.\n\nWhen the product is released out of an airplane or aerial applied, it has an increased viscosity, causing a better chance for it to reach the ground.\nThis product is designed specifically to clean up No. 2 Oil Fuel and is a base on the pH scale, and is completely water soluble.\n\nThis specific product of Nokomis 3 is a water based colloid. Nokomis 3-AA can be appied through spray nozzles on workboats or ships and for best use the propellor of the boats can be helpful with mixing the product into the water for the most effect.\n\nThe second design of Nokomis 3 is the Nokomis 3-F4. It is made with a higher viscosity, increasing the chance that the full product with reach the waters surface. It is not always applied by plane. It can also be sprayed through nozzles to the top of the surface. Depending on the situation and largeness of the oil spill. it can be applied either diluted or at full strength.\n\n\n"}
{"id": "1877084", "url": "https://en.wikipedia.org/wiki?curid=1877084", "title": "Névé", "text": "Névé\n\nNévé is a young, granular type of snow which has been partially melted, refrozen and compacted, yet precedes the form of ice. This type of snow is associated with glacier formation through the process of nivation. Névé that survives a full season of ablation turns into firn, which is both older and slightly denser. Firn eventually becomes glacial ice – the long-lived, compacted ice that glaciers are composed of. Glacier formation can take days to years depending on freeze-thaw factors. Névé is annually observed in skiing slopes, and is generally disliked as an icy falling zone.\n\nNévé has a minimum density of 500 kg/m³, which is roughly half of the density of liquid water at 1 atm.\n\nNévé can also refer to the alpine region in which snowfall accumulates, becomes névé, and feeds a glacier.\n"}
{"id": "6188564", "url": "https://en.wikipedia.org/wiki?curid=6188564", "title": "Ocean acoustic tomography", "text": "Ocean acoustic tomography\n\nOcean acoustic tomography is a technique used to measure temperatures and currents over large regions of the ocean. On ocean basin scales, this technique is also known as acoustic thermometry. The technique relies on precisely measuring the time it takes sound signals to travel between two instruments, one an acoustic source and one a receiver, separated by ranges of 100–5000 km. If the locations of the instruments are known precisely, the measurement of time-of-flight can be used to infer the speed of sound, averaged over the acoustic path. Changes in the speed of sound are primarily caused by changes in the temperature of the ocean, hence the measurement of the travel times is equivalent to a measurement of temperature. A 1 °C change in temperature corresponds to about 4 m/s change in sound speed. An oceanographic experiment employing tomography typically uses several source-receiver pairs in a moored array that measures an area of ocean.\n\nSeawater is an electrical conductor, so the oceans are opaque to electromagnetic energy (e.g., light or radar). The oceans are fairly transparent to low-frequency acoustics, however. The oceans conduct sound very efficiently, particularly sound at low frequencies, i.e., less than a few hundred hertz. These properties motivated Walter Munk and Carl Wunsch\n\nFrom its beginning, the idea of observations of the ocean by acoustics was married to estimation of the ocean's state using modern numerical ocean models and the techniques assimilating data into numerical models. As the observational technique has matured, so too have the methods of data assimilation and the computing power required to perform those calculations.\n\nOne of the intriguing aspects of tomography is that it exploits the fact that acoustic signals travel along a set of generally stable ray paths. From a single transmitted acoustic signal, this set of rays gives rise to multiple arrivals at the receiver, the travel time of each arrival corresponding to a particular ray path. The earliest arrivals correspond to the deeper-traveling rays, since these rays travel where sound speed is greatest. The ray paths are easily calculated using computers (\"ray tracing\"), and each ray path can generally be identified with a particular travel time. The multiple travel times measure the sound speed averaged over each of the multiple acoustic paths. These measurements make it possible to infer aspects of the structure of temperature or current variations as a function of depth. The solution for sound speed, hence temperature, from the acoustic travel times is an inverse problem.\n\nOcean acoustic tomography integrates temperature variations over large distances, that is, the measured travel times result from the accumulated effects of all the temperature variations along the acoustic path, hence measurements by the technique are inherently averaging. This is an important, unique property, since the ubiquitous small-scale turbulent and internal-wave features of the ocean usually dominate the signals in measurements at single points. For example, measurements by thermometers (i.e., moored thermistors or Argo drifting floats) have to contend with this 1-2 °C noise, so that large numbers of instruments are required to obtain an accurate measure of average temperature. For measuring the average temperature of ocean basins, therefore, the acoustic measurement is quite cost effective. Tomographic measurements also average variability over depth as well, since the ray paths cycle throughout the water column.\n\n\"Reciprocal tomography\" employs the simultaneous transmissions between two acoustic transceivers. A \"transceiver\" is an instrument incorporating both an acoustic source and a receiver. The slight differences in travel time between the reciprocally-traveling signals are used to measure ocean currents, since the reciprocal signals travel with and against the current. The average of these reciprocal travel times is the measure of temperature, with the small effects from ocean currents entirely removed. Ocean temperatures are inferred from the \"sum\" of reciprocal travel times, while the currents are inferred from the \"difference\" of reciprocal travel times. Generally, ocean currents (typically 10 cm/s) have a much smaller effect on travel times than sound speed variations (typically 5 m/s), so \"one-way\" tomography measures temperature to good approximation.\n\nIn the ocean, large-scale temperature changes can occur over time intervals from minutes (internal waves) to decades (oceanic climate change). Tomography has been employed to measure variability over this wide range of temporal scales and over a wide range of spatial scales. Indeed, tomography has been contemplated as a measurement of ocean climate using transmissions over antipodal distances.\n\nTomography has come to be a valuable method of ocean observation, exploiting the characteristics of long-range acoustic propagation to obtain synoptic measurements of average ocean temperature or current. One of the earliest applications of tomography in ocean observation occurred in 1988-9. A collaboration between groups at the Scripps Institution of Oceanography and the Woods Hole Oceanographic Institution deployed a six-element tomographic array in the abyssal plain of the Greenland Sea gyre to study deep water formation and the gyre circulation. Other applications include the measurement of ocean tides,\nand the estimation of ocean mesoscale dynamics by combining tomography, satellite altimetry, and\nin situ data with ocean dynamical models.\nIn addition to the decade-long measurements obtained in the North Pacific, acoustic thermometry has been employed to measure temperature changes of the upper layers of the Arctic Ocean basins, which continues to be an area of active interest.\nusing data from acoustic pulses sent from one end of the earth to the other.\nAcoustic thermometry is an idea to observe the world's ocean basins, and the ocean climate in particular, using trans-basin acoustic transmissions. \"Thermometry\", rather than \"tomography\", has been used to indicate basin-scale or global scale measurements. Prototype measurements of temperature have been made in the North Pacific Basin and across the Arctic Basin.\n\nStarting in 1983, John Spiesberger of the Woods Hole Oceanographic Institution, and Ted Birdsall and Kurt Metzger of the University of Michigan developed the use of sound to infer information about the ocean's large-scale temperatures, and in particular to attempt the detection of global warming in the ocean. This group transmitted sounds from Oahu that were recorded at about ten receivers stationed around the rim of the Pacific Ocean over distances of 4000 km.\nThese experiments demonstrated that changes in temperature could be measured with an accuracy of about 20 millidegrees. Spiesberger et al. did not detect global warming. Instead they discovered that other natural climatic fluctuations, such as El Nino, were responsible in part\nfor substantial fluctuations in temperature that may have masked any slower and smaller trends that may have occurred from global warming\nThe Acoustic Thermometry of Ocean Climate (ATOC) program was implemented in the North Pacific Ocean, with acoustic transmissions from 1996 through fall 2006. The measurements terminated when agreed-upon environmental protocols ended. The decade-long deployment of the acoustic source showed that the observations are sustainable on even a modest budget. The transmissions have been verified to provide an accurate measurement of ocean temperature on the acoustic paths, with uncertainties that are far smaller than any other approach to ocean temperature measurement.\n\nThe ATOC project was embroiled in issues concerning the effects of acoustics on marine mammals (e.g. whales, porpoises, sea lions, etc.). Public discussion was complicated by technical issues from a variety of disciplines (physical oceanography, acoustics, marine mammal biology, etc.) that makes understanding the effects of acoustics on marine mammals difficult for the experts, let alone the general public. Many of the issues concerning acoustics in the ocean and their effects on marine mammals were unknown. Finally, there were a variety of public misconceptions initially, such as a confusion of the definition of sound levels in air vs. sound levels in water. If a given number of decibels in water are interpreted as decibels in air, the sound level will seem to be orders of magnitude larger than it really is - at one point the ATOC sound levels were erroneously interpreted as \"louder than 10,000 747 airplanes\". In fact, the sound powers employed, 250 W, were comparable those made by blue or fin whales, although those whales vocalize at much lower frequencies. The ocean carries sound so efficiently that sounds do not have to be that loud to cross ocean basins. Other factors in the controversy were the extensive history of activism where marine mammals are concerned, stemming from the ongoing whaling conflict, and the sympathy that much of the public feels toward marine mammals.\n\nAs a result of this controversy, the ATOC program conducted a $6 million study of the effects of the acoustic transmissions on a variety of marine mammals. After six years of study the official, formal conclusion from this study was that the ATOC transmissions have \"no significant biological impact\".\n\nOther acoustics activities in the ocean may not be so benign insofar as marine mammals are concerned. Various types of man-made sounds have been studied as potential threats to marine mammals, such as airgun shots for geophysical surveys, or transmissions by the U.S. Navy for various purposes. The actual threat depends on a variety of factors beyond noise levels: sound frequency, frequency and duration of transmissions, the nature of the acoustic signal (e.g., a sudden pulse, or coded sequence), depth of the sound source, directionality of the sound source, water depth and local topography, reverberation, etc.\n\nIn the case of the ATOC, the source was mounted on the bottom about a half mile deep, hence marine mammals, which are bound to the surface, were generally further than a half mile from the source. This fact, combined with the modest source level, the infrequent 2% duty cycle (the sound is on only 2% of the day), and other such factors, made the sound transmissions benign in its effect on marine life.\n\nTomographic transmissions consist of long coded signals (e.g., \"m-sequences\") lasting 30 seconds or more. The frequencies employed range from 50 to 1000 Hz and source powers range from 100 to 250 W, depending on the particular goals of the measurements. With precise timing such as from GPS, travel times can be measured to a nominal accuracy of 1 millisecond. While these transmissions are audible near the source, beyond a range of several kilometers the signals are usually below ambient noise levels, requiring sophisticated spread-spectrum signal processing techniques to recover them.\n\n\n\n"}
{"id": "6956352", "url": "https://en.wikipedia.org/wiki?curid=6956352", "title": "Outline of air pollution dispersion", "text": "Outline of air pollution dispersion\n\n\nAir pollution dispersion – distribution of air pollution into the atmosphere. Air pollution is the introduction of particulates, biological molecules, or other harmful materials into Earth's atmosphere, causing disease, death to humans, damage to other living organisms such as food crops, or the natural or built environment. Air pollution may come from anthropogenic or natural sources. Dispersion refers to what happens to the pollution during and after its introduction; understanding this may help in identifying and controlling it. Air pollution dispersion has become the focus of environmental conservationists and governmental environmental protection agencies (local, state, province and national) of many countries (which have adopted and used much of the terminology of this field in their laws and regulations) regarding air pollution control.\n\nAir pollution emission plume – flow of pollutant in the form of vapor or smoke released into the air. Plumes are of considerable importance in the atmospheric dispersion modelling of air pollution. There are three primary types of air pollution emission plumes:\n\nThere are five types of air pollution dispersion models, as well as some hybrids of the five types:\n\n\n\nEffect of turbulence on dispersion – turbulence increases the entrainment and mixing of unpolluted air into the plume and thereby acts to reduce the concentration of pollutants in the plume (i.e., enhances the plume dispersion). It is therefore important to categorize the amount of atmospheric turbulence present at any given time. This type of disperson is scale dependent. Such that, for flows where the cloud of pollutant is smaller than the largest eddies present, there will be mixing. There is no limit on the size on mixing motions in the atmosphere and therefore bigger clouds will experience larger and stronger mixing motions. And hence, this type of dispersion is scale dependent.\n\nPasquill atmospheric stability classes – oldest and, for a great many years, the most commonly used method of categorizing the amount of atmospheric turbulence present was the method developed by Pasquill in 1961. \nHe categorized the atmospheric turbulence into six stability classes named A, B, C, D, E and F with class A being the most unstable or most turbulent class, and class F the most stable or least turbulent class. \n\nTable 1: The Pasquill stability classes\n\nTable 2: Meteorological conditions that define the Pasquill stability classes\nIncoming solar radiation is based on the following: strong (> 700 W m), moderate (350-700 W m), slight (< 350 W m)\n\nThe stability class can be defined also by using the\n\n\nAdvanced air pollution dispersion models – they do not categorize atmospheric turbulence by using the simple meteorological parameters commonly used in defining the six Pasquill classes as shown in Table 2 above. The more advanced models use some form of Monin-Obukhov similarity theory. Some examples include:\n\n\n\n\n\n"}
{"id": "10225184", "url": "https://en.wikipedia.org/wiki?curid=10225184", "title": "Oxygen evolution", "text": "Oxygen evolution\n\nOxygen evolution is the process of generating molecular oxygen (O) by a chemical reaction, usually from water. Oxygen evolution from water is effected by oxygenic photosynthesis, electrolysis of water, and thermal decomposition of various oxides. The biological process supports aerobic life. When relatively pure oxygen is required industrially, it is isolated by distillation of liquified air.\n\nPhotosynthetic oxygen evolution is the fundamental process by which oxygen is generated in earth's biosphere. The reaction is part of the light-dependent reactions of photosynthesis in cyanobacteria and the chloroplasts of green algae and plants. It utilizes the energy of light to split a water molecule into its protons and electrons for photosynthesis. Free oxygen, generated as a by-product of this reaction, is released into the atmosphere.\n\nWater oxidation is catalyzed by a manganese-containing cofactor contained in photosystem II known as the oxygen-evolving complex (OEC) or water-splitting complex. Manganese is an important cofactor, and calcium and chloride are also required for the reaction to occur.The stoichiometry this reaction follows:\nThe protons are released into the thylakoid lumen, thus contributing to the generation of a proton gradient across the thylakoid membrane. This proton gradient is the driving force for ATP synthesis via photophosphorylation and coupling the absorption of light energy and oxidation of water to the creation of chemical energy during photosynthesis.\n\nIt was not until the end of the 18th century that Joseph Priestley discovered by accident the ability of plants to \"restore\" air that had been \"injured\" by the burning of a candle. He followed up on the experiment by showing that air \"restored\" by vegetation was \"not at all inconvenient to a mouse.\" He was later awarded a medal for his discoveries that: \"...no vegetable grows in vain... but cleanses and purifies our atmosphere.\" Priestley's experiments were followed up by Jan Ingenhousz, a Dutch physician, who showed that \"restoration\" of air only worked in the presence of light and green plant parts.\n\nIngenhousz suggested in 1796 that CO (carbon dioxide) is split during photosynthesis to release oxygen, while the carbon combined with water to form carbohydrates. While this hypothesis was attractive and reasonable and thus widely accepted for a long time, it was later proven incorrect. Graduate student C.B. Van Niel at Stanford University found that purple sulfur bacteria reduce carbon to carbohydrates, but accumulate sulfur instead of releasing oxygen. He boldly proposed that, in analogy to the sulfur bacteria's forming elemental sulfur from HS (hydrogen sulfide), plants would form oxygen from HO (water). In 1937, this hypothesis was corroborated by the discovery that plants are capable of producing oxygen in the absence of CO. This discovery was made by Robin Hill, and subsequently the light-driven release of oxygen in the absence of CO was called the \"Hill reaction\". Our current knowledge of the mechanism of oxygen evolution during photosynthesis was further established in experiments tracing isotopes of oxygen from water to oxygen gas.\n\nTogether with hydrogen (H), oxygen is evolved by electrolysis of water. \nElectrons (e) are transferred from the cathode to protons to form hydrogen gas. The half reaction, balanced with acid, is:\n\nAt the positively charged anode, an oxidation reaction occurs, generating oxygen gas and releasing electrons to the anode to complete the circuit:\n\nCombining either half reaction pair yields the same overall decomposition of water into oxygen and hydrogen:\n\nChemical oxygen generators consist of chemical compounds that release O upon some stimulation, usually heat. They are used in submarines and commercial aircraft, providing emergency oxygen. Oxygen is generated by high-temperature decomposition of sodium chlorate:\nPotassium permanganate also releases oxygen upon heating, but the yield is modest.\n\n\n"}
{"id": "11770045", "url": "https://en.wikipedia.org/wiki?curid=11770045", "title": "Proven reserves", "text": "Proven reserves\n\nProven reserves, also called measured reserves, 1P, and reserves, are industry specific terms regarding fossil fuel energy sources. They are defined as a \"Quantity of energy sources estimated with reasonable certainty, from the analysis of geologic and engineering data, to be recoverable from well established or known reservoirs with the existing equipment and under the existing operating conditions.\" A reserve is considered a proven reserve if it is probable that 90% or more of the resource is recoverable while being economically profitable. These terms relate to common fossil fuel reserves such as oil reserves, natural gas reserves, or coal reserves. \n\nOperating conditions are taken into account when determining if a reserve is classified as \"proven\". Operating conditions include operational break-even price, regulatory and contractual approvals, without which the reserve cannot be classified as proven. Price changes therefore can have a large impact on the classification of proven reserves. Regulatory and contractual conditions may change, and also affect the amount of proven reserves. If a reserve's resources can be recovered using current technology but is not economically profitable it is considered \"technically recoverable\" but cannot be considered a proven reserve. Reserves less than 90% recoverable but more than 50% are considered \"probable reserves\" and below 50% are \"possible reserves\".\n\nThe \"engineering\" term P90 refers to 90 percent engineering probability, is a commonly accepted specific definition by Society of Petroleum Engineers, it does not take into account anything except technical concerns. Therefore, it is different from the business term which does take into account current break-even profitability, and regulatory and contractual approval, but is considered a very rough equivalent. The definition is certainly not universal. Energy Watch Group uses a different definition, P95.\n\nDisregarding economics, the proper \"engineering\" term for the total technologically extractable amount is the Producible fraction, which is easily confused with the \"business term\" \"proven reserves\". However, the purely engineering term is also misleading in that squeezing the last bits of fossil fuel out follows the diminishing returns and at some point is so costly that it becomes highly impractical, as seen on a bell curve, which is why measures like P90 and P95 were created. The term \"proven reserves\" is further subdivided into \"proved developed reserves\" and \"proved undeveloped reserves\". Note that it does not include \"Unproven reserves\", which is broken down into probable reserves as well as possible reserves .\n\nThese reserve categories are totaled up by the measures 1P, 2P, and 3P, which are inclusive of all reserves types:\n\nNew proven reserves are commonly added by new field discoveries. Reserves growth also commonly occurs in previously existing fields, as the characteristics of the reservoir become better understood, as fields are extended laterally, or new oil and gas reservoirs are found in existing fields. Reserve growth may also take place due to technological and economic changes.\n\nIn Russia, reserves categories A, B, and C correspond roughly to developed producing reserves, undeveloped reserves (development is approved), and discovered resources without a firm plan to develop yet, respectively; the designation ABC corresponds to discovered resources. Better references pending. \n\nOil companies employ specialist reserve valuation consultants - such as Gaffney, Cline & Associates, Sproule, Miller and Lents, Ltd., DeGolyer and MacNaughton, Ryder Scott, Netherland, Sewell & Associates Inc. (NSAI), Evolution Resources, Cawley, Gillespie & Associates Inc. (CG&A) and others - to provide third party reports as part of Securities and Exchange Commission (SEC) SEC filings. On December 30 2009, recognising advances in exploration and valuation technology, the SEC allowed 2P probable and 3P possible reserves to be reported, along with 1P proved reserves, though oil companies also have to verify the independence of third party consultants. Since investors view 1P reserves with much greater importance than 2P or 3P reserves, oil companies seek to convert 2P and 3P reserves into 1P reserves.\n\n"}
{"id": "877912", "url": "https://en.wikipedia.org/wiki?curid=877912", "title": "Rangeland", "text": "Rangeland\n\nRangelands are grasslands, shrublands, woodlands, wetlands, and deserts that are grazed by domestic livestock or wild animals. Types of rangelands include tallgrass and shortgrass prairies, desert grasslands and shrublands, woodlands, savannas, chaparrals, steppes, and tundras. Rangelands do not include forests lacking grazable understory vegetation, barren desert, farmland, or land covered by solid rock, concrete and/or glaciers.\n\nRangelands are distinguished from pasture lands because they grow primarily native vegetation, rather than plants established by humans. Rangelands are also managed principally with practices such as managed livestock grazing and prescribed fire rather than more intensive agricultural practices of seeding, irrigation, and the use of fertilizers.\n\nGrazing is an important use of rangelands but the term \"rangeland\" is not synonymous with \"grazinglands\". Livestock grazing can be used to manage rangelands by harvesting forage to produce livestock, changing plant composition or reducing fuel loads.\n\nFire is also an important regulator of range vegetation, whether set by humans or resulting from lightning. Fires tend to reduce the abundance of woody plants and promote herbaceous plants including grasses, forbs, and grass-like plants. The suppression or reduction of periodic wildfires from desert shrublands, savannas, or woodlands frequently invites the dominance of trees and shrubs to the near exclusion of grasses and forbs.\n\nThe United States Environmental Protection Agency defines rangeland as \"lands on which the native vegetation (climax or natural potential plant community) is predominantly grasses, grass-like plants, forbs, or shrubs suitable for grazing or browsing use.\" The EPA classifies natural grassland and savannas as rangeland, and in some cases includes wetlands, deserts, tundra, and \"certain forb and shrub communities.\" The primary difference between rangeland and pasture is management; rangelands tend to have natural vegetation along with a few introduced plant species, but all managed by grazing, while pastures have forage that is adapted for livestock and managed, by seeding, mowing, fertilization and irrigation.\n\nPrairies are considered part of the temperate grasslands, savannas and shrublands biome by ecologists, based on similar temperate climates, moderate rainfall, and grasses, herbs, and shrubs, rather than trees, as the dominant vegetation type. Temperate grassland regions include the Pampas of Argentina, and the steppes of Eurasia.\n\nGrasslands are areas where the vegetation is dominated by grasses (Poaceae) and other herbaceous (non-woody) plants (forbs). However, sedge (Cyperaceae) and rush (Juncaceae) families can also be found. Grasslands occur naturally on all continents except Antarctica. In temperate latitudes, such as northwest Europe and the Great Plains and California in North America, native grasslands are dominated by perennial bunch grass species, whereas in warmer climates annual species form a greater component of the vegetation.\n\nSteppe, in physical geography, refers to a biome region characterized by grassland plain without trees apart from those near rivers and lakes. The prairie (especially the shortgrass and mixed prairie) is an example of a steppe, though it is not usually called such. It may be semi-desert, or covered with grass or shrubs or both, depending on the season and latitude. The term is also used to denote the climate encountered in regions too dry to support a forest, but not dry enough to be a desert.\n\nPampas are the fertile South American lowlands that include the Argentine provinces of Buenos Aires, La Pampa, Santa Fe, Entre Ríos and Córdoba, most of Uruguay, and the State of Rio Grande do Sul, in the southernmost end of Brazil covering more than . These vast plains are only interrupted by the low Ventana and Tandil hills near Bahía Blanca and Tandil (Argentina), with a height of and respectively. The climate is mild, with precipitation of to , more or less evenly distributed through the year, making the soils appropriate for agriculture. This area is also one of the distinct physiography provinces of the larger Paraná-Paraguay Plain division. These plains contain unique wildlife because of the different terrains around it. Some of this wildlife includes the rhea, the badger, and the prairie chicken.\n\nShrubland is a plant community characterized by vegetation dominated by shrubs, often also including grasses, herbs, and geophytes. Shrubland may either occur naturally or be the result of human activity. It may be the mature vegetation type in a particular region and remain stable over time, or a transitional community that occurs temporarily as the result of a disturbance, such as fire. A stable state may be maintained by regular natural disturbance such as fire or browsing. Shrubland may be unsuitable for human habitation because of the danger of fire. The term \"shrubland\" was first coined in 1903.\n\nWoodland is a low-density forest forming open habitats with plenty of sunlight and limited shade. Woodlands may support an understory of shrubs and herbaceous plants including grasses. Woodland may form a transition to shrubland under drier conditions or during early stages of primary or secondary succession. Higher densities and areas of trees, with largely closed canopy, provide extensive and nearly continuous shade are referred to as forest.\n\nSavanna is a grassland ecosystem characterized by the trees being sufficiently small or widely spaced so that the canopy does not close. The open canopy allows sufficient light to reach the ground to support an unbroken herbaceous layer consisting primarily of C4 grasses.\n\nDesert is a landscape or region that receives an extremely low amount of precipitation, defined as areas with an average annual precipitation of less than per year, or as areas where more water is lost by evapotranspiration than falls as precipitation. In the Köppen climate classification system, deserts are classed as \"BWh\" (hot desert) or \"BWk\" (temperate desert). In the Thornthwaite climate classification system, deserts would be classified as arid megathermal climates.\n\nTundra is a biome where the tree growth is hindered by low temperatures and short growing seasons. The term \"tundra\" comes through Russian тундра from the Kildin Sami word \"tūndâr\" \"uplands,\" \"treeless mountain tract.\" There are three types of tundra: Arctic tundra, alpine tundra, and Antarctic tundra In tundra, the vegetation is composed of dwarf shrubs, sedges and grasses, mosses, and lichens. Scattered trees grow in some tundra. The ecotone (or ecological boundary region) between the tundra and the forest is known as the tree line or timberline.\n\nRangeland is a prominent feature of rural Canada. A provincial jurisdiction, administration and policy regarding range use varies across the country. As in many other Commonwealth countries, public tenures on crown land for the purpose of range activities are common in geographically compatible areas. Reconciling the economic needs of ranchers and the need for environmental conservation is one of the primary themes in modern range discourse.\n\nIn western Canada, both grassland and forested range are significant. In British Columbia, 70 percent of grassland range is privately owned and 60 percent of the total annual livestock forage requirement is provided by grazing on Crown rangeland (34 million hectares), 80 percent of which is forested range. Grassland range predominates in much of the prairie provinces’ ranching area; however, forested range is particularly important in the boreal region.\n\nCertain rangelands are preserved as provincially-protected areas similar to parks, others are managed as community resources. For example, in Alberta since 2003 there has been legislation allowing the creation of \"Heritage Rangelands\" within the parks system. As of 2012 there were 2 heritage rangelands and 6 proposed future heritage rangelands run by Alberta Parks. There are also 32 provincial grazing reserves located throughout Alberta administered as public lands by Alberta Sustainable Resource Development. The federal government has administered several \"Community Pastures\" in Western Canada that were reclaimed lands suffering erosion during the 1930s. In 2012, it was announced that this federal involvement would be phased out over a six-year period.\n\nOf the land within the United States borders, 36% is considered rangeland. The western side of the United States is 53% rangeland. Around 399 million acres (1,610,000 km²) of rangeland are privately owned. The Bureau of Land Management manages about 167 million acres (676,000 km²) of publicly owned rangeland, with the United States Forest Service managing approximately 95 million acres (380,000 km²) more. Ranchers may lease portions of this public rangeland and pay a fee based on the number and type of livestock and the period for which they are on the land.\n\nHistorically much of the land in the western United States was used for grazing and much of some states still is. In many of those states, such as Arizona, an open-range law applies which requires a land owner to fence cattle out rather than in; thus cattle are theoretically allowed to roam free. In modern times open-range laws can conflict with urban development as occasional stray cows, bulls, or even herds wander into subdivisions or onto highways.\n\n\n\nThe different types of rangeland come together to form about 70% (excluding Antarctica) of the Earth's surface.\n\nAbout 75% of Australia’s land mass is Rangeland. 53 of Australia's 85 bioregions have rangelands. In Western Australia, rangelands cover about 87% of the state’s 2.5 million square kilometres. Australian Rangelands support significant parts of the nation's economy, including Australia's valuable mining industry ($12 billion/yr), tourism ($2 billion/yr), pastoralism ($5.5 billion/yr – cattle $4.4 billion & sheep $1 billion). Australia's rangelands include a diverse group of relatively undisturbed ecosysterms such as; tropical savannas, woodlands, shrublands, grasslands and deserts. Rangelands in Australia cover low rainfall and variable climates which include; arid semi arid and seasonally high rainfall areas.\n\nAustralian rangelands are important in; biodiversity, income, social and cultural heritage, sub-artesian water sources and major river systems, clean and green food and fiber production, and carbon storage. Rangelands contain a wealth of biodiversity including a total of 1800 types of plants and 605 vertebrate animals currently identified. Rangelands are managed by Australia's Department of Agriculture, Fisheries and Forestry (DAFF) and the ESRM Programs on the local level \n\nRangelands in South America are located in regions with climate ranging from arid to sub-humid. Annual precipitation in these areas ranges from approximately 150 to 1500 mm (6–60 inches). Within South America, rangelands cover about 33% of the total land area. South American rangelands include; grasslands, shrublands, savannas, and hot and cold deserts. Rangelands in South America exclude hyperarid deserts. Examples of the South American rangelands include the; Patagonian Steppe, the Monte, the Pampas, the \"Lianos\" or \"Cerrado,\" the \"Chaco\" and the \"Caatinga.\"\nThe change in the intensity and location of tropical thunderstorms and other weather patterns is the driving force in the climates of southern South America.\n\nIn Kenya, rangelands make up for 85% of the land surface area, and are largely inhabited by nomadic pastoralists who are largely dependent on livestock. This movement often brings along an incursion of different diseases with the common one being the rinderpest virus in the Kenyan wildlife population from the Somali ecosystem.\n\nIn the past, rangelands in western China supported a pastoral economy and large wildlife populations. Now the rangelands have shrunk due to population growth, economic, government, and social factors. Rangeland types in China include; Semi-desert, Dry Alpine Grasslands, Alpine Dwarf Shrub, Wetland types.\n\nRangelands produce a wide variety of goods and services desired by society, including livestock forage (Grazing), wildlife habitat, water, mineral resources, wood products, wildland recreation, open space and natural beauty. The geographic extent and many important resources of rangelands make their proper use and management vitally important to people everywhere.\n\n"}
{"id": "2811351", "url": "https://en.wikipedia.org/wiki?curid=2811351", "title": "Rucbah", "text": "Rucbah\n\nThe traditional star name Rucbah can refer to three different stars:\n"}
{"id": "3143801", "url": "https://en.wikipedia.org/wiki?curid=3143801", "title": "Sierra Juárez and San Pedro Mártir pine-oak forests", "text": "Sierra Juárez and San Pedro Mártir pine-oak forests\n\nThe Sierra Juárez and San Pedro Mártir pine-oak forests is an ecoregion, in the Temperate coniferous forests biome, that covers the higher elevations of the Sierra Juárez and Sierra San Pedro Mártir ranges, of the Peninsular Ranges, in the northern Baja California Peninsula of Mexico, near the border with California (United States).\n\nThe ecoregion covers an area of . It lies at the southeastern extent of the Mediterranean climate region that covers much of California and the northwestern corner of Baja California, and the climate is temperate with winter rains.\n\nThe pine-oak forests are bounded by the southern extent of the California chaparral and woodlands to the west, by the Baja California Desert to the southwest, and by the Sonoran Desert to the east.\n\nThese forests are predominantly pine, juniper, fir, and oak. Ten pine species can be found in the ranges, including Tamarack Pine (\"Pinus contorta\" subsp. \"murrayana\"), Sugar Pine (\"Pinus lambertiana\"), Parry Pinyon \"(Pinus quadrifolia)\", along with White Fir (\"Abies concolor\" subsp. \"lowiana\"), and California Incense Cedar (\"Calocedrus decurrens\"). Oak species include Coast Live Oak (\"Quercus agrifolia\"), Engelmann Oak (\"Quercus engelmannii\"), Canyon Live Oak (\"Quercus chrysolepis\"), Baja Oak (\"Quercus peninsularis\")., and Island Oak (\"Quercus tomentella\"). There are also several isolated strands of aspens (Populus tremuloides) on the higher altitudes.\n\nTecate Cypress (\"Cupressus forbesii\") and San Pedro Martir Cypress (\"Cupressus arizonica\" subsp. \"Montana\") are found in scattered groves across the range. The Sierra Juárez and San Pedro Mártir pine-oak forests are near the southern limit of the distribution of the California Fan Palm (\"Washingtonia filifera\"). The higher portions of these Peninsular Ranges harbor many rare and endemic species.\n"}
{"id": "41301792", "url": "https://en.wikipedia.org/wiki?curid=41301792", "title": "Space for Life", "text": "Space for Life\n\nSpace for Life () is a museum district in Montreal, Quebec, Canada. It brings together the city's four most prominent natural museums: the Montreal Biodome and the Rio Tinto Alcan Planetarium, situated in Montreal's Olympic Park, and the Montreal Botanical Garden and Montreal Insectarium, in the adjacent Maisonneuve Park.\n\nSpace for Life was established in 2011 as a successor body to Montreal Nature Museums. It describes itself as the largest natural sciences complex in Canada. As of 2013, its executive director is Charles-Mathieu Brunelle and Montreal executive committee member Manon Gauthier is responsible for its political oversight.\n\nThe Montreal Biodome, Insectarium, Botanical Garden and Rio Tinto Alcan Planetarium invite us to rethink the ties between human beings and nature, cultivating a new way of living. Together, the four prestigious institutions form a place where nature and science are honoured. They have positioned themselves as a Space for Life and are dedicated to sharing their vast heritage and knowledge with you.\n\nSpace for Life is a place that brings together the Montreal Biodome, Insectarium, Botanical Garden and Planetarium, but it is also much more. It’s a participatory movement and a commitment to biodiversity. It is a vast project based on citizen participation and co-creation with visitors. Just like nature belongs to everyone, it is everyone’s movement. It’s a state of mind, a way of experiencing nature. It is a space we visit where we can exchange, collaborate and learn.\n\nThrough its efforts in communication, conservation, education and research, Space for Life guides humans to better experience nature.\n\nThe Space for Life comprises the Montreal Biodome,Botanical Garden, Insectarium and Rio Tinto Alcan Planetarium. The institutions are interdependent, and designed to inspire visitors to adopt a new way of experiencing nature. They are connected by the Grande Place, a space that inspires new ways of coming together, enjoying the site, playing outside, building, interacting and experiencing everyday life.\n\nSpace for Life is the largest natural science museum complex in Canada, one of the leading tourist sites in Montréal and all of Quebec and a place with immense potential to impress and thrill visitors through nature, explain nature and encourage behaviour that is respectful of nature.\n\nSpace for Life is also committed to increasing awareness of our planet’s biodiversity and encouraging people to better protect it. In fact, our four institutions have created a sustainable development charter.\n\nBy offering visitors immersive experiences combining science and emotion, the Biodôme, Botanical Garden, Insectarium and Planetarium invite us all to look at nature differently.\n\nWe have a collective commitment to nature, but also a commitment to achievement, meaningfulness and mobilization.\n\nWith their participatory, unifying approach that is authentic, inventive, committed and open to the world, our four institutions have joined forces to create a movement, a Space for Life; a place where people come together to create, shaped by Montrealers and visitors from around the world.\n\nSpace for Life has initiated a movement that aims to help people better understand the concept of interdependence underlying biodiversity, become aware of the services provided by nature and gradually change the way they live. Our institutions invite you to join the movement, by taking an active part and spreading the message yourself.\n\nThe Space for Life is a collective initiative encouraging Montrealers and local stakeholders to become involved and make it their own.\n\nA movement to get closer to nature\nA participatory, creative, historic movement\n\nAt Space for Life, our approach to sustainable development informs all our decisions and actions, encouraging us to consider the inextricable connections between society, ethics, the economy and the environment.\n\nWith this goal in mind, we are committed to integrating sustainable development principles in all our activities. This includes our charter of commitment and 11 areas of focus:\n\n\nMontréal Space for Life is the largest natural science museum complex in Canada. By 2019, four major projects will have been developed, creating constantly evolving and changing spaces for life.\n\nThe Insectarium Metamorphosis, the Biodôme Migration and two other major Space for Life projects are a legacy for Montrealers, for the planet and for future generations.\n\nAt a time when the issues the planet is facing, especially those related to the loss of biodiversity, raise the question of the relevance of our modern lifestyles, Space for Life, which is rooted in these unique institutions whose reputation and credibility are recognized both locally and internationally, has a fundamental role to play.\n\nThese two projects – characterized by their uniqueness, both in terms of architecture and design and the memorable and distinctive experiences they offer visitors – will let Space for Life truly play its role by inviting citizens to reconnect with nature and invent new ways of living. Reflecting a multidisciplinary vision wherein the architectural gesture emerges from a global creative approach, a bold architectural design will create living spaces that are permeable, ecological and evolving, while meeting the highest green building standards.\n\nKuehn Malvezzi, Pelletier De Fontenay, Jodoin Lamarre Pratte, Dupras Ledoux and NCK\n\nA unique experience\n\nThe Insectarium will become a true biotope in which insects, plants and people can interact and take an interest in one another. In an architectural space that is both a landscape and an organism, the underground and closed spaces, water, shadow and daylight follow and play off one another along a route that plunges visitors into an immersive, sensory experience in the heart of the world of insects.\n\n\nKANVA + NEUF architect(e)s, Bouthillette Parizeau + NCK\n\nA renewed experience, re-thinking the deeper meaning of the ecosystems. \n\nThe Migration project will revamp the Biodôme and its scenic design, in most areas accessible to the public, to offer an innovative, participatory, immersive experience. The architects compare the Biodôme to a living organism and their concept echoes notions of cellular biology. The cell, the basic building block of life, becomes a model for structuring and shaping the building blocks of nature – ecosystems. The design team is proposing to reorganize the spaces, open up the centre of the Biodôme, and offer bold, complementary experiences.\n\nThe overall project budget is $22 million. This includes professional and project management fees, studies, construction costs, restoration, acquisition and relocation of live collections, museology, furniture, various contingencies, etc.\n\n\nProgrammation informations can be found on http://espacepourlavie.ca/en \n\n"}
{"id": "69181", "url": "https://en.wikipedia.org/wiki?curid=69181", "title": "Sycamine", "text": "Sycamine\n\nThe sycamine tree ( \"sykaminοs\") is a tree mentioned in Luke 17:6 of the Bible. It is rendered by Luther as \"mulberry tree\", which is most probably the correct rendering. It is found in two species, the black mulberry (\"Morus nigra\") and the white mulberry (\"Morus alba\"), which are common in Palestine. It is in the same family as the fig-tree. Some contend, however, that this name denotes the sycamore fig ( \"sykomorea\") of Luke 19:4.\n"}
{"id": "35886715", "url": "https://en.wikipedia.org/wiki?curid=35886715", "title": "Theta Crucis", "text": "Theta Crucis\n\nThe Bayer designation Theta Crucis (θ Cru / θ Crucis) is shared by two star systems, in the constellation Crux:\n"}
{"id": "143689", "url": "https://en.wikipedia.org/wiki?curid=143689", "title": "Vacuum flask", "text": "Vacuum flask\n\nA vacuum flask (also known as a Dewar flask, Dewar bottle or thermos) is an insulating storage vessel that greatly lengthens the time over which its contents remain hotter or cooler than the flask's surroundings. Invented by Sir James Dewar in 1892, the vacuum flask consists of two flasks, placed one within the other and joined at the neck. The gap between the two flasks is partially evacuated of air, creating a near-vacuum which significantly reduces heat transfer by conduction or convection.\n\nVacuum flasks are used domestically to keep beverages hot or cold for extended periods of time and for many purposes in industry.\n\nThe vacuum flask was designed and invented by Scottish scientist Sir James Dewar in 1892 as a result of his research in the field of cryogenics and is sometimes called a Dewar flask in his honour. While performing experiments in determining the specific heat of the element palladium, Dewar made a brass chamber that he enclosed in another chamber to keep the palladium at its desired temperature. He evacuated the air between the two chambers, creating a partial vacuum to keep the temperature of the contents stable. Through the need for this insulated container James Dewar created the vacuum flask, which became a significant tool for chemical experiments and also became a common household item. The flask was later developed using new materials such as glass and aluminum; however, Dewar refused to patent his invention.\n\nDewar's design was quickly transformed into a commercial item in 1904 as two German glassblowers, Reinhold Burger and Albert Aschenbrenner, discovered that it could be used to keep cold drinks cold and warm drinks warm. The Dewar flask design had never been patented but the German men who discovered the commercial use for the product renamed it \"Thermos,\" and subsequently claimed both the rights to the commercial product and the trademark to the name. In his subsequent attempt to claim the rights to the invention, Dewar instead lost a court case to the company. The manufacturing and performance of the Thermos bottle was significantly improved and refined by the Viennese inventor and merchant Gustav Robert Paalen, who designed various types for domestic use, which he also patented, and distributed widely, through his Thermos Bottle Companies in the United States and Canada. The name later became a genericized trademark after the term \"thermos\" became the household name for such a liquid container. The vacuum flask went on to be used for many different types of scientific experiments and the commercial “Thermos” was transformed into a common item. \"Thermos\" remains a registered trademark in some countries, but it was declared a genericized trademark by court action in the United States in 1963, since it had become colloquially synonymous with vacuum flasks in general. However, there are other vacuum flasks.\n\nAfter the German glassblowers determined the commercial uses for the Dewar flask, the technology was sold to the Thermos company, who used it to mass-produce vacuum flasks for at-home use. Over time, the company expanded the size, shapes and materials of these consumer products, primarily used for carrying coffee on the go and carrying liquids on camping trips to keep them either hot or cold. Eventually other manufacturers produced similar products for consumer use.\n\nThe vacuum flask consists of two vessels, one placed within the other and joined at the neck. The gap between the two vessels is partially evacuated of air, creating a partial-vacuum which reduces heat conduction or convection. Heat transfer by thermal radiation may be minimized by silvering flask surfaces facing the gap but can become problematic if the flask's contents or surroundings are very hot; hence vacuum flasks usually hold contents below the boiling point of water. Most heat transfer occurs through the neck and opening of the flask, where there is no vacuum. Vacuum flasks are usually made of metal, borosilicate glass, foam or plastic and have their opening stoppered with cork or polyethylene plastic. Vacuum flasks are often used as insulated shipping containers.\n\nExtremely large or long vacuum flasks sometimes cannot fully support the inner flask from the neck alone, so additional support is provided by \"spacers\" between the interior and exterior shell. These spacers act as a thermal bridge and partially reduce the insulating properties of the flask around the area where the spacer contacts the interior surface.\n\nSeveral technological applications, such as NMR and MRI machines, rely on the use of double vacuum flasks. These flasks have two vacuum sections. The inner flask contains liquid helium and the outer flask contains liquid nitrogen, with one vacuum section in between. The loss of precious helium is limited in this way.\n\nOther improvements to the vacuum flask include the \"vapour-cooled radiation shield\" and the \"vapour-cooled neck\", both of which help to reduce evaporation from the flask.\n\nIn laboratories and industry, vacuum flasks are often used to hold liquefied gases (often LN2) for flash freezing, sample preparation and other processes where maintaining an extreme low temperature is desired. Larger vacuum flasks store liquids that become gaseous at well below ambient temperature, such as oxygen and nitrogen; in this case the leakage of heat into the extremely cold interior of the bottle results in a slow boiling-off of the liquid so that a narrow unstoppered opening, or a stoppered opening protected by a pressure relief valve, is necessary to prevent pressure from building up and eventually shattering the flask. The insulation of the vacuum flask results in a very slow \"boil\" and thus the contents remain liquid for long periods without refrigeration equipment.\n\nVacuum flasks have been used to house standard cells and ovenized Zener diodes, along with their printed circuit board, in precision voltage-regulating devices used as electrical standards. The flask helped with controlling the Zener temperature over a long time span and was used to reduce variations of the output voltage of the Zener standard owing to temperature fluctuation to within a few parts per million.\n\nOne notable use was by Guildline Instruments, of Canada, in their Transvolt, model 9154B, saturated standard cell, which is an electrical voltage standard. Here a silvered vacuum flask was encased in foam insulation and, using a large glass vacuum plug, held the saturated cell. The output of the device was 1.018 volts and was held to within a few parts per million.\n\nThe principle of the vacuum flask makes it ideal for storing certain types of rocket fuel, and NASA used it extensively in the propellant tanks of the Saturn launch vehicles in the 1960s and 1970s.\n\nThe design and shape of the Dewar flask was used as a model for optical experiments based on the idea that the shape of the two compartments with the space in between is similar to the way the light hits the eye. The vacuum flask has also been part of experiments using it as the capacitor of different chemicals in order to keep them at a consistent temperature.\n\nThe industrial Dewar flask is the base for a device used to passively insulate medical shipments. Most vaccines are sensitive to heat and require a cold chain system to keep them at stable, near freezing temperatures. The Arktek device uses eight one-litre ice blocks to hold vaccines at under 10 °C.\n\nVacuum flasks are at risk of implosion hazard, and glass vessels under vacuum, in particular, may shatter unexpectedly. Chips, scratches or cracks can be a starting point for dangerous vessel failure, especially when the vessel temperature changes rapidly (when hot or cold liquid is added). Proper preparation of the Dewar vacuum flask by tempering prior to use is advised to maintain and optimize the functioning of the unit. Glass vacuum flasks are usually fitted into a metal base with the cylinder contained in or coated with mesh, aluminum or plastic to aid in handling, protect it from physical damage, and contain fragments should they break.\n\nIn addition, cryogenic storage dewars are usually pressurized, and they may explode if pressure relief valves are not used.\n\nThe rate of heat (energy) loss through a vacuum flask can be analyzed thermodynamically, starting from the second TdS relation:\n\nformula_1\n\nformula_2\n\nAssuming constant pressure throughout the process,\n\nformula_3\n\nRearranging the equation in terms of the temperature of the outside surface of the vacuum flask's inner wall,\n\nformula_4\n\nWhere\n\n\nNow consider the general expression for heat loss due to radiation:\n\nformula_5\n\nIn the case of the vacuum flask,\n\nformula_6\n\nSubstituting our earlier expression for T,\n\nformula_7\n\nWhere\n\nAssuming that the outer surface of the inner wall and the inner surface of the outer wall of the vacuum flask are coated with polished silver to minimize heat loss due to radiation, we can say that the rate of heat absorption by the inner surface of the outer wall is equal to the absorptivity of polished silver times the heat radiated by the outer surface of the inner wall,\n\nformula_8\n\nIn order for energy balance to be maintained, the heat lost through the outer surface of the outer wall must be equal to the heat absorbed by the inner surface of the outer wall,\n\nformula_9\n\nSince the absorptivity of polished silver is the same as its emissivity, we can write\n\nformula_10\n\nWe must also consider the rate of heat loss through the lid of the vacuum flask (assuming it is made of polypropylene, a common plastic) where there is no vacuum inside the material. In this area, the three heat transfer modes of conduction, convection, and radiation are present. Therefore, the rate of heat loss through the lid is,\n\nformula_11\n\nformula_12\n\nWhere\n\nNow we have an expression for the total rate of heat loss, which is the sum of the rate of heat loss through the walls of the vacuum flask and the rate of heat loss through the lid,\n\nformula_13\n\nwhere we substitute each of the expressions for each component into the equation.\n\nThe rate of entropy generation of this process can also be calculated, starting from entropy balance:\n\nformula_14\n\nWritten in rate form,\n\nformula_15\n\nAssuming a steady-state process,\n\nformula_16\n\nformula_17\n\nSince there is no heat added to the system,\n\nformula_18\n\n\n"}
{"id": "90213", "url": "https://en.wikipedia.org/wiki?curid=90213", "title": "Xōchipilli", "text": "Xōchipilli\n\n' is the god of art, games, beauty, dance, flowers, and song in Aztec mythology. His name contains the Nahuatl words (\"flower\") and (either \"prince\" or \"child\"), and hence means \"flower prince\". As the patron of writing and painting, he was called ' the \"Seven-flower,\" but he could also be referred to as \"Five-flower.\" His wife is the human girl , and his twin sister is . As one of the gods responsible for fertility and agricultural produce, he is also associated with (god of rain), and (god of maize). may correspond to the Tonsured Maize God among the Classic Mayas.\n\nIn the mid-19th century, a 16th-century Aztec statue of Xochipilli was unearthed on the side of the volcano Popocatépetl near Tlalmanalco. The statue is of a single figure seated upon a temple-like base. Both the statue and the base upon which it sits are covered in carvings of sacred and psychoactive organisms including mushrooms (\"Psilocybe aztecorum\"), tobacco (\"Nicotiana tabacum\"), \"Ololiúqui\" (\"Turbina corymbosa\"), \"sinicuichi\" (\"Heimia salicifolia\"), possibly \"cacahuaxochitl\" (\"Quararibea funebris\"), and one unidentified flower. \n\"The texts always use the flower in an entirely spiritual sense, and the aim of the religious colleges was to cause the flower of the body to bloom: This flower can be no other than the soul. The association\nof the flower with the sun is also evident. One of the hieroglyphs for the sun is a four-petalled flower, and the feasts of the ninth month, dedicated to Huitzilopochtliupo, were entirely given over to flower offerings.\"\n\n- Paul Pettennude, Ph.D.\n\nThe figure himself sits on the base, head tilted up, eyes open, jaw tensed, with his mouth half open and his arms opened to the heavens. The statue is currently housed in the Aztec hall of the Museo Nacional de Antropología in Mexico City.\n\nIn the popular reality television show, \"Survivor\", a statue of Xochipilli was used as the Immunity Idol for the .\n\nIt has been suggested by Wasson, Schultes, and Hofmann that the statue of Xochipilli represents a figure in the throes of entheogenic ecstasy. The position and expression of the body, in combination with the very clear representations of hallucinogenic plants which are known to have been used in sacred contexts by the Aztec support this interpretation.\n\nWasson says that in the statue's depiction Xochipilli \"is absorbed by \"temicxoch\", 'dream flowers', as the Nahua say describing the awesome experience that follows the ingestion of an entheogen. I can think of nothing like it in the long and rich history of European art: Xochipilli absorbed in \"temicxoch\".\"\n\n\n"}
