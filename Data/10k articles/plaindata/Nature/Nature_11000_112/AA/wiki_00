{"id": "4450928", "url": "https://en.wikipedia.org/wiki?curid=4450928", "title": "6DJ8", "text": "6DJ8\n\nThe 6DJ8 is a miniature nine-pin medium gain dual triode vacuum tube. It is distinguished by its very high transconductance, mostly the result of its frame grid construction. Versions of the tube are named ECC88, E88CC, 6922, 6N23P, and 6N11.\n\nDeveloped by Amperex, the American division of Philips in 1958. It was developed by Philips in the Netherlands under European designation ECC88. The series-string version 7DJ8 / PCC88 was also introduced at the same time. The 6DJ8 was designed for use as a low-noise amplifier in VHF and UHF TV tuners, an improved successor to the usual 6BK7, but due to its high gain it was used in much high-end test equipment. Most high-quality oscilloscopes from the 1950s to the 1960s have many 6DJ8 tubes.\n\nAlthough not originally designed for the purpose, it became popular in hi-fi audio amplifiers. European-produced version of the tube is designated ECC88. An industrial (improved/higher ratings) version of the tube was designated 6922 / E88CC between 1959 and 1961. Siemens introduced their own super ECC88 / 6DJ8 at the same time under CCa type. New Old Stock (NOS) 6DJ8s and ECC88s produced in the past by major American or West European vacuum tube manufacturers (such as Philips or Amperex) remain extremely popular with and highly sought by audiophiles. A version (direct equivalent) of this tube was also produced in the Soviet Union as 6N23P (Russian: 6Н23П) and in China as 6N11.\n\n\n - 6DJ8 tube data at DuncanAmps.\n\n"}
{"id": "36303662", "url": "https://en.wikipedia.org/wiki?curid=36303662", "title": "Adriatic Question", "text": "Adriatic Question\n\nIn the aftermath of the First World War, the Adriatic Question or Adriatic Problem concerned the fate of the territories along the eastern coast of the Adriatic Sea that formerly belonged to the Austro-Hungarian Empire. The roots of the dispute lay in the secret Treaty of London, signed during the war (26 April 1915), and in growing nationalism, especially Italian irredentism and Yugoslavism, which led ultimately to the creation of Yugoslavia. The question was a major barrier to agreement at the Paris Peace Conference, but was partially resolved by the Treaty of Rapallo between Italy and Yugoslavia on 12 November 1920.\n\nAustria-Hungary exited the war on 3 November 1918, when it ordered its troops to cease fighting. The Armistice of Villa Giusti, signed with Italy that day, took effect on 4 November, and on 13 November the Armistice of Belgrade was signed with Italy's allies on the Balkan front. Italy began immediately to occupy territories ceded to it by the treaty of 1915, while simultaneously the South Slavs formed local governments in opposition to both Italian expansion and Austro-Hungarian authority. A National Council of Slovenes, Croats and Serbs was set up in Zagreb on 5–6 October, and the State of Slovenes, Croats and Serbs was proclaimed there on 29 October and that same day the \"Sabor\", the legitimate parliament of Croatia-Slavonia, declared independence from Austria-Hungary. On 1 December the Kingdom of Serbs, Croats and Slovenes (later Yugoslavia) was formed in Belgrade by union of this new state with Serbia and Montenegro.\n\nThe main argument presented by Yugoslavia was that the territories under consideration contained about seven million Slavs, almost the entire population. \nOf the Dalmatian islands, only Lošinj (Lussin) had an Italian majority. When the hinterland of Fiume was included along with its suburbs the Yugoslav majority increased further. The Italian claim on Gorizia and Gradisca was generally recognised, as was its claim on the Slavic settlements around Friuli.\n\nAt the Conference of Oppressed Nationalities in Rome (8–10 April 1918), Italy lent official support to the Declaration of Corfu (20 July 1917), a Yugoslavist document supported by Britain and France that expressed the need to unite the South Slavs politically.\n\nThe Italians argued that the natural geographic boundary of Italy included the Julian and Dinaric Alps, and that therefore the Austro-Hungarian littoral lay within geographic Italy. The strategic importance of the geography of the eastern coast of the Adriatic has been generally accepted. The Chief of the Division of Boundary Geography with the American delegation to the Paris Peace Conference, Douglas Wilson Johnson, wrote at the time, \"Any naval power on the [Adriatic] eastern coast must find itself possessing immense advantages over Italy.\" Johnson went on to note that the offer of Pula (Pola), Vlorë (Valona) and a central Dalmatian island group to the Italians effectively settled the strategic problem and balanced the two Adriatic powers.\n\nOn 29 October 1918 the Austro-Hungarians evacuated Fiume, and its Croatian mayor, Rikard Lenac, proclaimed the town's adherence to the National Council of the State of Slovenes, Croats and Serbs. On 30 October the Italian community set up a \"national council\" and proclaimed Fiume's union with Italy. The next day the local governor handed over power to the Italians, as did the governor of Trieste. On 4 November an Italian naval detachment under Admiral Guglielmo Rainer aboard the battleship arrived in the harbour of Fiume. Rainer declared the local government dissolved and, on 14 November Italian Prime Minister Vittorio Orlando, acting on the advice of Chief of Staff Armando Diaz, order Rainer to land a troop of marines. The next day (15 November) Diaz requested Allied troops take part in the occupation of Fiume. The commander of British forces on the Italian Front, the Earl of Cavan, was ordered to send a company \"to show that the occupation is allied, and to keep order\". That same day 2,000 Serbian soldiers arrived on the outskirts of Fiume. \n\nFrance and the United States also sent naval contingents to Fiume, and on 17 November some American, British and French naval officers met with Mayor Lenac and Ivan Lorković, a delegate from the Slovene–Croat–Serb state, in the abandoned governor's palace. It was agreed that the Serb troops should evacuate the area that afternoon and that the Italian marines should not be landed for another three days, pending orders from the Supreme Council of the Paris Peace Conference. Although Rainer agreed, he was countermanded by his superior, on the grounds that he had acted without instruction, and an Italian troops force eighty strong entered the city that afternoon. They ordered Lenac's government out of the palace and removed all Serbian flags on public display. On 18 November the National Council of Slovenes, Croats and Serbs officially protested to the allied commanders. The arrival of an American battalion on 19 November prevented any outbreak of violence, but the Italians eventually posted 12,000 troops in the city—\"an Italian military occupation in overwhelming force, for political reasons\" wrote Admiral Edward Kiddle, and Commodore Howard Kelly, commanding the British Adriatic Force, could remark on 22 November that \"the Italian occupation has all the appearance of an annexation to the Italian Crown.\"\n\nWhen Italy first began occupying land east of the Tagliamento, the former boundary between Italy and Austria-Hungary, it employed troops of the 332nd Infantry Regiment of the United States (which were under Italian command on the Italian Front), with the goal of appearing to be an international occupation force. Although this misuse of American troops led the United States War Department to order them withdrawn, President Woodrow Wilson countermanded the order to prevent a breakdown in negotiations over the Adriatic Question in Paris. Thereafter, as a result, the international control for the Adriatic was a naval responsibility.\n\nOn 16 November 1918 representatives of the Allied navies—American, British, French and Italian—met in Venice to establish the Naval Commission (or Committee) for the Adriatic. At several sessions held in Rome from 26 November, the commission decided the fate of the Austro-Hungarian fleet and of its coastline. The fleet was divided in control between the Americans, French and Italians pending a final political decision regarding its fate, while the coast was divided into three zones of control, an Italian in the north (mainly corresponding to the Austrian Littoral), an American in the middle (mainly Dalmatia) and a French in the south (mainly Albania). This arrangement eventually received political ratification in Paris. Josephus Daniels, United States Secretary of the Navy, wrote to his Chief of Naval Operations, William S. Benson, also American naval advisor to the Paris Peace Commission, that \"due to possible Adriatic developments and [American] desire to show sympathy with [the] Slavonic government being formed in the late Austro-Hungarian Empire, consider it desirable to send flag officer ... immediately into the Adriatic.\" Benson later wrote to Admiral William H. G. Bullard, commanding American naval forces in their zone, that \"the general principles laid down by the President\", i.e. the Fourteen Points, which stressed national self-determination, should be furthered, implying active American support for Yugoslav nationhood.\n\nThe Naval Commission began its work in Fiume in early December, but in January the Italian representative, Admiral Vittorio Mola, resigned in protest. At the demands of the American, British and French admirals, Italy appointed Rear Admiral Ugo Rombo to replace him on 1 February 1919. At a meeting in Venice on 8 February Rombo told Admiral Albert Parker Niblack that the Americans did not understand the Adriatic problem, leading to a breakdown in negotiations between the Italians and the rest. In the end the Commission's report was not publicised.\n\nWhen the Commission moved to Split, the admirals tasked the Yugoslavs with maintaining order, but on 24 February some Yugoslav (mostly Croat) citizens attacked some Italian officers meeting with local Italianophiles. Rombo demanded the Allies patrol the city, but Niblack and Benson vetoed it.\n\nOn 8 October 1918, in response to the recent opening of the National Council in Zagreb, the \"Reichsrat\", one of the two parliaments of Austria-Hungary, freed 348 of the 379 sailors still in custody after the mutiny of 1–3 February in Kotor (Cattaro). On 22–24 October officers conducted talks with sailors in their native languages on all ships explaining Emperor Charles I's plan to federalise Cisleithania, but it was too late to restore morale or loyalty to the crown. On 26 October the forbidden call to mutiny, \"Hurra-Rufe\", was heard on the in Kotor, and two days later on the battleships in Pula. The sailors organised councils but did not openly rebel or riot. On 31 October 1918 the Austro-Hungarian Navy, with all its ships and other craft, supplies, stores and facilities (ports, arsenals, etc.) was handed over to the National Council of Slovenes, Croats and Serbs by Admiral Miklós Horthy, acting under orders received from the emperor the previous day. All non-Yugoslav sailors were furloughed and all officers were given the option to continue in service to the new nation. The emperor's order contained a proviso that in the future all the \"nations\" of Austria-Hungary should have a right to claim compensation for their share of the value of the fleet. The formal handover at Kotor took place on 1 November, the same day the Italians sank the battleship at Pula, whether yet aware it was a Yugoslav ship or not. Admiral William Sims, commander of the United States Navy in Europe, ordered Admiral Bullard to go to Pula aboard the and escort the former Austro-Hungarian ships that had been taken over by Yugoslavia to Corfu under white flags.\n\nBy the end of the day, 1 November, Chief of the Naval Staff Paolo Emilio Thaon di Revel had informed the government that \"the entire Austrian fleet or at least a great part of it has pulled down the Austrian flag and raised the Yugoslav. The fleet or a good part of it is already in the hands of the Yugoslav National Committee.\" This did not effect negotiations at Villa Giusti, and the armistice was signed on behalf of the navy by Prince Johann of Liechtenstein of the Austrian Navy League and Captain Georg Zwierkowski of the Danube Flotilla. The treaty promised the Italians a share of the Austro-Hungarian fleet (although this had in fact ceased to exist some days before):\nShortly before the armistice came into effect at 3:00 p.m. on 4 November, the Italian navy occupied Vis (Lissa). This was done \"to exorcise the old demons of 1866 [i.e., the Austro-Prussian War<nowiki>]</nowiki> and restore the honour of the Italian navy while the war at least technically was still underway.\" Before the end of the day Fiume, Trieste and Pula had been occupied, and on 5 November Šibenik (Sebenico) followed. The Yugoslavs abandoned all the vessels they could not man to the Italians, and on 7 November the sailed out of Pula for the unoccupied port of Bakar (Buccari). On 9 November the Italian flag was raised on all remaining vessels in Pula. That day, cruisers of the British, French and Italian navies sailed into the Bay of Kotor (Bocche di Cattaro) and the last unoccupied Austro-Hungarian port. The sailors there cut up their Austro-Hungarian ensigns—to prevent them from being taken by the Italians as trophies—and distributed them as souvenirs. That same day, at an inter-Allied conference on Corfu, the Italian diplomat Ugo Conz retorted to his British colleagues, \"There can be no fleet where there is no state. There can be no Yugoslav fleet as long as such a state has not yet been founded or as long as peace has not been definitively concluded.\"\n\nIn Dalmatia, the American occupation zone, the citizens had elected a provisional assembly and a governor, and both supported the nascent State of Slovenes, Croats and Serbs. Order was kept by the local militia, which was often unreliable, and by Serbian troops which had begun to garrison the area in small numbers. This arrangement was supported by the Americans, but \"[b]y early 1919, disturbed local conditions forced the [Naval] Committee for the Adriatic to circulate four armed inter-allied patrols night and day throughout the area under the command of the American admiral.\" After a few months of this the Admiral Bullard supplanted the inter-allied patrol with a patrol composed entirely of Americans to assist the local police in maintaining order for a few hours each day.\n\nSeveral times Italian troops crossed the frontier into the American zone of occupation, but in each case they were turned back, either by a warning or, in one case, by the arrival of American warships and landing of American forces. In several towns the Americans posted notice that they would protect the lives and property of Dalmatians against any Italian injustice. This latter sort of propaganda was the most effective means of keeping the peace, since the American admiral had to rely on Serbian troops for garrisoning the interior. His own garrison was minuscule, and reinforcements were only landed in emergencies. One Yugoslav mayor from the Italian zone sent a letter of thanks to the American admiral for protecting his people from the Italians.\n\nThe first effort to bring American naval forces in the Adriatic home took place in December 1919. In February 1920 the Italians requested four Austro-Hungarian ships allocated to Italy by the Allied Military Committee of Versailles and which were being guarded at Split by American forces. The Italian ambition to receive these vessels led to the decision to retain American ships in the Adriatic longer. The American occupation ended only after the Italian forces had evacuated their zone and the International Committee for the Destruction of Enemy Warships had allocated all the formerly Austro-Hungarian vessels under its command to the Italian Navy. The last American troops left aboard the on 29 September 1921, after the Treaty of Rapallo had been signed.\nThe negotiations attending the Adriatic Question at the Paris Peace Conference may be divided into three periods based on the dominant Italian personality of the time: January–June 1919, the Baron Sonnino period; June–September 1919, the Tittoni period; and 12 September – 9 December 1919, the d'Annunzio period.\n\nFrom January until June 1919 negotiations were dominated by Baron Sidney Sonnino, the Italian Foreign Minister, who divided and conquered his allies, forcing Britain and France to acquiesce in the Treaty of London and endeavouring to negotiate directly with America from a position of strength. At the same time he whipped up the Italian people with nationalistic propaganda. When the government of Orlando was replaced by that of Francesco Saverio Nitti on 23 June, Sonnino was replaced by Tommaso Tittoni. The nationalist fervour he had stoked, however, broke into open violence in Fiume, where, on 6 July 1919, an element of the Italian population massacred some of the occupying French soldiers. \n\nOn 15 April President Wilson issued a memorandum proposing a line, the so-called \"Wilson Line\", dividing the Istrian peninsula between Italy and Yugoslavia. Trieste and Pula, with the railway connecting them, lay on the Italian side; Fiume and Ljubljana (Laibach), with the railway connecting them, on the Yugoslav. Učka (Monte Maggiore) was to be Italian, but the Wilson Line ran further west of Fiume than that of the Treaty of London. Italy would have none of the rights in northern Dalmatia granted it by that treaty, but it would receive the islands of Vis (Lissa) and Lošinj (Lussin). The Yugoslav fleet, inherited from Austria-Hungary, was to be reduced, and the area of the frontier demilitarised generally. The Italians alleged that the Wilson line did not give sufficient protection to the Trieste–Pula railway.\n\nTittoni altered the course of negotiations by abandoning the Treaty of London and strengthening the Franco-Italian alliance, but he did not accept President Wilson's proposed \"line\". The French diplomat André Tardieu worked as an intermediary between Tittoni and the Americans, and he first suggested the creation of a buffer state out of a strip of land around Fiume, the future Free State of Fiume. The main American objection at the time was that the buffer state denied its inhabitants the right of self-determination.\n\nOn 12 September 1919, Gabriele d'Annunzio led a band of disaffected soldiers of the \"Granatieri di Sardegna\" in a march on Fiume—the so-called \"impresa di Fiume\" (\"Fiume enterprise\")—defying the international commission and the governments of Italy and Yugoslavia. Tittoni petitioned the international community, represented by French Prime Minister Georges Clemenceau after the American and British heads of government had both gone home, and the Yugoslav government for time to allow Italy to rein in d'Annunzio. In October he proposed that Fiume itself and a coastal strip leading to it should be conceded to Italy, that besides Lošinj and Vis she should possess the islands of Cres (Cherso) and Lastovo (Lagosta) and that the city of Zadar (Zara) should be a free city under Italian protection.\n\nAfter the departure of the president, Frank Polk headed up the American Commission to Negotiate Peace in Paris. He was the driving force behind the memorandum of 9 December, signed by American, British and French delegates. This statement denied the Italians Fiume and most Yugoslav islands and even restricted their mandate over Albania. This memorandum was quickly abandoned by the British and French, whose prime ministers signed a compromise with their Italian counterpart on 14 January 1920 without American participation. Therein it was agreed to concede Fiume and a coastal strip to Italy and in exchange hive off the northern part of Albania and give it to Yugoslavia. To gain the latter's acceptance the signatories of the new compromise threatened to otherwise enforce the Treaty of London.\n\nThis last compromise aroused the anger of President Wilson, who, in a statement issued 10 February, denounced it as \"a positive denial of the principles for which America entered the war\". He threatened to withdraw the United States from the Treaty of Versailles and from the Franco-American agreement of the past June. On 26 February, Clemenceau and British Prime Minister David Lloyd George published a note offering to disavow the January compromise and suggesting that the memorandumg of December be similarly sidelined. They expressed their wish to see Yugoslavia and Italy negotiate directly, and, contrary to Wilson's desire, stood by their intention to enforce the Treaty of London if all else failed. It was suggested by some at the time that Lloyd George had personally frustrated the efforts of Wilson by a private agreement with Nitti whereby the latter would support Britain against France on the Eastern (i.e., Russian and Turkish) Question.\n\nIn a presidential note published in \"The New York Times\" on 4 March 1920, Woodrow Wilson affirmed that \"he cannot approve any plan which assigns to Jugo-Slavia in the Northern district of Albania territorial compensation for what she is deprived of elsewhere,\" thus forestalling the concession of Shkodër (Scutari) to Yugoslavia in exchange for Yugoslav recognition of Italian rights to Fiume. In an internal memorandum dated 9 December 1919, the delegates of America, Britain and France recognised the borders of the Principality of Albania as laid down in 1913. The Yugoslav delegation, in a memo dated 14 January 1920, was in favour of an independent Albania free of foreign influence, but if that should not be feasible the delegates favoured territorial concessions to Yugoslavia in the north. The Allies were already content to recognise an Italian mandate over central Albania and Yugoslav rights of transit through northern Albania, with the attendant right to build and operate railroads on its territory. The Albanian representatives at the Congress of Lushnjë (27–31 January) voted against any Italian mandate, despite the fact that as a solution it was made to counter Italy's designs on Vlorë (Valona).\n\nAfter the collapse of Montenegro's army in 1916, the government and the court went into exile in France. The Allied governments gave King Nicholas I of Montenegro an income, but by the end of 1916 the British and the French had become convinced that he was responsible for the surrender of his army and were refusing to countenance an independent Montenegro after the war. Until the war's end the king and his government continued to receive financial support and diplomatic recognition by the Allies—\"an inexplicable state of affairs\". The Ally with the most interest in Montenegro was Italy, which had trade relations with her and which saw here as a part of the same sphere of influence as Albania. Italian policy favoured the restoration of Nicholas and immediately after the Austro-Hungarian armistice army and navy troops were moved up the Montenegrin coast in order to keep order pending the return of Montenegrin (royal) self-government. The French refused to allow Nicholas to return, and lent their support to the Montenegrin Yugoslavists. The Italians allowed some of Nicholas's supporters to enter Montenegro with them and gave their tacit support to the royalist Christmas Uprising of December 1918 against the Yugoslavists.\n\nAt Paris, Baron Sonnino defended Montenegrin independence, argued for her presence at the negotiating table and attacked irregularities in the election of members for the Podgorica Assembly. Queen Elena of Italy, a daughter of the king of Montenegro, even interceded with Woodrow Wilson on her father's behalf. Although the king was prepared to accept an autonomous Montenegro in a federal Yugoslavia, the Allies believed that any autonomy given to that state would only furnish grounds for continuing Italian intervention. On 7 July 1919, Foreign Minister Tittoni approached Major Johnson, the American geographical expert at Paris, with a proposal: in exchange for sovereignty over the Montenegrin Bay of Kotor (Boka Kotorska) and Mount Lovćen Italy would cede all her rights in northern Dalmatia and certain islands. He also conveyed his preference for continued Montenegrin independence, which Johnson (and the British delegate Sir Eyre Crowe) considered a regional danger, and asked Johnson to forward his proposals to the Yugoslav Foreign Minister, Ante Trumbić. After getting Secretary Lansing's approval the next day (8 July), Johnson informed a surprised Trumbić. That same evening the Yugoslav delegation unanimously rejected Italy's \"big shot\" proposal. After this rebuff, the Italians began negotiating directly with the Yugoslavs through their delegates in the Allied Shipping Commission in London, although the Montenegrin question was mostly avoided. Despite \"the preservation of an independent Montenegro\" being \"among the major postulates of her foreign policy\", in Prime Minister Nitti's words, it was felt that in the long term Yugoslav–Montenegrin unification was inevitable and being alone in pressing for her independence would be unseemly in Paris.\n\nBy the middle of 1919 the Bay of Kotor, as well as the towns of Bar (Antivari), Budva (Budua) and Ulcinj (Dulcigno) were occupied by army and navy personnel of Britain, France, Italy and Yugoslavia. Clashes were frequent between the last two. The Italians armed and transported the royalist rebels, and spread propaganda about Serb actions in Montenegro. On 20 July 1919 the Yugoslav army attacked some Montenegrin royalists seeking Italian protection in Kotor and several were killed. The Italian government clamped down on its commander in the Balkans, General Settimio Piacentini. In a secret cable of 29 July Tittoni claimed that the Yugoslavs were willing to recognise an Italian potectorate in Albania if Italy would give up any claim in Montenegro.\n\nIn debate in the House of Lords on 11 March 1920, Foreign Secretary Lord Curzon affirmed that the Allies did not recognise the legitimacy of the Podgorica Assembly or its deposition of King Nicholas and proclaimed union with Serbia. On 12 February, the Leader of the House of Commons, Bonar Law, urged that \"the real point at issue is ... whether what is being done, or what will be done, is in accordance with the wishes of the majority of the Montenegrin people.\" In April 1919, the Count de Salis, former ambassador to Montenegro (1911–16) at Cetinje, was dispatched there to report on the state of the union with Serbia. The supporters of the Podgorica Assembly argued that Montenegro's adherence to the Allies was nominal at best and that members of the royal family had conspired with the enemy after the defeat of the winter of 1915–16. The supporters of Montenegrin independence argued that the Assembly was a fraud perpetrated by Serbian conquerors, who had endeavoured with French connivance to eliminate Montenegro since its defeat.\n\nOn 8 September 1920, D'Annunzio occupied the city of Rijeka (Fiume) and proclaimed the Italian Regency of Carnaro, but the approval of the Treaty of Rapallo on 12 November 1920 turned the territory into an independent state, the Free State of Fiume. Other parts of the Treaty of Rapallo were supposed to solve the dispute between the Kingdom of Italy and the Kingdom of Serbs, Croats and Slovenes (renamed Yugoslavia in 1929). It included Italian annexation of parts of Carniola, several Adriatic islands, and the city of Zadar (Zara).\n\n"}
{"id": "1955213", "url": "https://en.wikipedia.org/wiki?curid=1955213", "title": "Aerography (meteorology)", "text": "Aerography (meteorology)\n\nAerography is the production of weather charts. The information is supplied by radiosonde observations, principally. \"Constant-pressure\" charts are routinely constructed at standard air pressures. Standard air pressures are 850, 700, 500, 400, 300, 250, and 200 millibars (hectopascals) (hPa) (SI). Weather charts are sometimes drawn at lower air pressures that occur above 40,000 feet (12 km). The lines of equal air pressure are called \"isobars\". \"Isotherms\" are the lines of equal air temperature.\n"}
{"id": "28703342", "url": "https://en.wikipedia.org/wiki?curid=28703342", "title": "Ahmad Aali", "text": "Ahmad Aali\n\nAhmad Aali (, (born 1935 in Tabriz) is an Iranian photographer and artist.\n\n\n\n\nInterviews, critics, published articles and essays about photography\n\nBefore 1979 (before Iranian Revolution)\nMore than 18 published materials in:\nKhoushe, Keyhan international, Journal de Teheran, Tehran Journal, Bamshad Magazine, Etela’at newspaper, Tamasha Magazine, Tasvir Magazine, Ayandegan Magazine, Donya-ye Jadid Magazine, Omid-e Iran Magazine.\n\nAfter 1979 (after Iranian Revolution)\nMore than 18 published materials in:\nAks Magazine, Kelk Magazine, Hamshahri newspaper, Mehr Magazine, Salam newspaper, Eghtesad newspaper, Akhabr newspoaper, Rooz-e Haftom weekley magazine, Zaman Monthley magazine, visual art’s magazine, Me’yar magazine, Iran newspaper, Tandis Magazine, Etemad Magazine, Herfeh Honarmand magazine.\n\n\n"}
{"id": "1173584", "url": "https://en.wikipedia.org/wiki?curid=1173584", "title": "Book of Imaginary Beings", "text": "Book of Imaginary Beings\n\nBook of Imaginary Beings was written by Jorge Luis Borges with Margarita Guerrero and published in 1957 under the original Spanish title \"Manual de zoología fantástica\". It was expanded in 1967 and 1969 in Spain to the final \"El libro de los seres imaginarios\". The English edition, created in collaboration with translator Norman Thomas di Giovanni, contains descriptions of 120 mythical beasts from folklore and literature.\n\nIn the preface, Borges states that the book is to be read \"as with all miscellanies... not... straight through... Rather we would like the reader to dip into the pages at random, just as one plays with the shifting patterns of a kaleidoscope\"; and that \"legends of men taking the shapes of animals\" have been omitted.\n\nAlthough a work of fiction, it is situated in a tradition of Paper Museums, bestiaries, and natural history writing.\n\n\n\n"}
{"id": "12220850", "url": "https://en.wikipedia.org/wiki?curid=12220850", "title": "Calyce (mythology)", "text": "Calyce (mythology)\n\nIn Greek mythology, Calyce (Ancient Greek: Καλύκη \"Kalyke\") or Calycia is the name of several characters.\n\n\n"}
{"id": "58226373", "url": "https://en.wikipedia.org/wiki?curid=58226373", "title": "Central European mixed forests", "text": "Central European mixed forests\n\nThe Central European mixed forest ecoregion (WWF ID:PA0412) is a temperate hardwood forest covering much of northeastern Europe, from Germany to Russia. The area is only about one-third forested, with pressure from human agriculture leaving the rest in a patchwork of traditional pasture, meadows, wetlands. The ecoregion is in the Temperate broadleaf and mixed forest biome, and the Palearctic ecozone, with a Humid Continental climate. It covers .\n\nThe ecoregion covers the formerly-glaciated central plains of Central Europe, from eastern Germany and the shores of the Baltic Sea, through large parts of the Czech Republic, Poland, Lithuania, and Belarus, Ukraine and a small part of Russia (in Bryansk Oblast). The terrain is mostly flat lowlands in the center, hilly moraine-dominated in the north, and uplands to the south along the Carpathian mountains. To the north is the Sarmatic mixed forests ecoregion, the forests of which feature more spruce and pine. To the east is the East European forest steppe, in which the forest stands thin out into grasslands. To the south is the Carpathian montane forests ecoregion, featuring mountain pastures and forests of beech, spruce, elm, and dwarf pine. To the north are the Mixed Baltic forests of oaks, hornbeam, and linden trees on flat, acidic soils. To the west is the Western European broadleaf forests ecoregion, which is now mostly cultivated agricultural land.\n\nThe region has a Humid continental, cool summer climate (Koppen classification (Dfb)). This climate is characterized by high variation in temperature, both daily and seasonally; with relatively cool summers. The \"cool summer\" designation indicates that at least no month averages above . The summers become hotter and the winters colder as you move east across the ecoregion, due to the movement towards the center of the continent (\"continentality\"). The mean January temperature is in Germany to in Belarus. Precipitation average between 500 mm and 700 mm, mostly falling during the summer growing season.\n\nOak forests are characteristic throughout the region, with some pine forests in the north. Forest cover ranges from 15% in Ukraine to 33% in the Czech Republic. The most common tree in the ecoregion, covering half of the forested area, is the Scots pine (Pinus sylvestris), which has been planted extensively over the past 200 years. The truly mixed deciduous forests have been replaced mostly by agriculture. The non-forested areas are largely meadows and pastures dedicated to human agricultural uses. There are also extensive wetlands in the lowlands.\nThe wetlands support diverse bird communities, but mammals are heavily pressured by human land use. Because of the uniformity of the terrain and openness to other regions, there are no endemic species in the ecoregion. In some countries, 20-30 of the mammal species are threatened.\n\nThe Central European mixed forest has been affected heavily by human pressure. Most protected areas of nature are small and fragmented. Some of the large or more representative protected areas in the ecoregion include:\n\n"}
{"id": "16543735", "url": "https://en.wikipedia.org/wiki?curid=16543735", "title": "Charles Jacquinot", "text": "Charles Jacquinot\n\nCharles Hector Jacquinot (1796-1879) was a noted mariner, best known for his role in early French Antarctic surveys.\n\nJacquinot served as an ensign on the \"Coquille\". He served with Jules Dumont d'Urville in the Mediterranean and was junior officer on the expedition led by Louis Freycinet from 1817-1820, and on Isidore Duperrey's expedition of 1822-1825.\n\nHis first trip to Antarctica was made in 1826–29 with d'Urville on the \"Astrolabe\", during which he circumnavigated the globe. For that voyage, Jacquinot was awarded the Cross of Honor.\n\nHe was commander of the expedition corvette \"Zelee\" during Durond d'Urville's second expedition from 1837-1840. The ships departed from Toulon in September 1837. The mission of the expedition was to survey the Straits of Magellan, then head to the Weddell Sea. After Dumont d'Urvilles death, he personally edited the five of the 24 volume \"Voyage au Pole Sud et dans Oceane\" official account of the expedition. but the real writer of the 7 \"Histoire du voyage\" last volumes was .\n\nMount Jacquinot was named for him by d'Urville, who was said to have been his best friend.\n\nJacquinot eventually was appointed Vice Admiral. He was commander at Piraeus, Greece, during the Crimean War. He served from 1854 to 1855. He was awarded the Greek Order of the Redeemer.\n\nHe died soon after retiring from the Naval General Staff in 1879. He was said to have been modest, no doubt a result of his request to be buried without military honors.\n\nHe was the brother of Honoré Jacquinot, a surgeon who served as a naturalist on that same voyage and who traveled on that same ship.\n"}
{"id": "8375471", "url": "https://en.wikipedia.org/wiki?curid=8375471", "title": "Cueva del Milodón Natural Monument", "text": "Cueva del Milodón Natural Monument\n\nCueva del Milodón Natural Monument is a Natural Monument located in the Chilean Patagonia, northwest of Puerto Natales and north of Punta Arenas.\n\nThe monument is situated along the flanks of Cerro Benitez. It comprises several caves and a rock formation called \"Silla del Diablo\" (Devil's Chair). The monument includes a cave which is notable for the discovery in 1895 of skin, bones and other parts of a giant ground sloth called \"Mylodon darwini\".\n\nThe largest cave in the monument is the long Milodón Cave. It was discovered in 1895 by Hermann Eberhard, German explorer of Patagonia. He found a large, seemingly fresh piece of skin of an unidentified animal. In 1896 the cave was explored by Otto Nordenskjöld and later it was recognized that the skin belonged to Mylodon – an extinct animal which died 10,200–13,560 years ago.\n\nIn the cave and other caves of the monument have been found remnants of other extinct animals and human remnants.\n\nAt the entrance of the monument is a life size replica of the prehistoric \"Mylodon\", which was a very large herbivore, somewhat resembling a large bear. It became extinct at the end of the Pleistocene Epoch.\n\nInvestigations determined the survival of the Milodon until about 5,000 years ago and confirmed the existence of other animals, such as the \"Dwarf Horse\" Hippidion, the saber-toothed cat Smilodon and the litoptern Macrauchenia\n\nDiverse elements of human habitation are found at Cueva del Milodón including fire-fractured rock, lithic tools and human remains. Human habitation at Cueva del Milodón is dated as early as 6000 BC.\n\n"}
{"id": "78439", "url": "https://en.wikipedia.org/wiki?curid=78439", "title": "Dobson unit", "text": "Dobson unit\n\nThe Dobson unit (DU) is a unit of measurement of the amount of a trace gas in a vertical column through the Earth's atmosphere. It originated, and continues to be primarily used in respect to, atmospheric ozone, whose total column amount, usually termed \"total ozone\", and sometimes \"column abundance\", is dominated by the high concentrations of ozone in the stratospheric ozone layer. \n\nThe Dobson Unit is defined as the thickness (in units of 10 µm) of that layer of pure gas which would be formed by the total column amount at standard conditions for temperature and pressure (STP). This is sometimes referred to as a 'milli-atmo-centimeter.' A typical column amount of 300 DU of atmospheric ozone therefore would form a 3 mm layer of pure gas at the surface of the Earth if its temperature and pressure conformed to STP. \n\nThe Dobson unit is named after Gordon Dobson, a researcher at the University of Oxford who in the 1920s built the first instrument to measure total ozone from the ground, making use of a double prism monochromator to measure the differential absorption of different bands of solar ultraviolet radiation by the ozone layer. This instrument, called the Dobson ozone spectrophotometer, has formed the backbone of the global network for monitoring atmospheric ozone and was the source of the discovery in 1984 of the Antarctic ozone hole.\n\nThe Dobson Unit is not part of the SI International System of Units. To address this shortcoming, a brief study in 1982 examined a number of alternative SI-based units suitable for column amounts of not only ozone but any gas in any planetary atmosphere and proposed the use of the unit of mole per square metre for all cases. Examples range from Earth’s trace gases at levels of micro moles per square meter to Venus’ carbon dioxide at mega moles per square meter:\n\n\nTypical values of total ozone in the Earth’s atmosphere are conveniently represented in millimoles per square metre (mmol m).\n\nA later examination in 1995 of units for use in atmospheric chemistry by the Commission on Atmospheric Chemistry, a part of the International Union of Pure and Applied Chemistry (IUPAC), discouraged the use of special names and symbols for units that are not part of the SI and are not products of powers of SI base units. Although it overlooked the 1982 article it concurred with the view that the Dobson unit should eventually be replaced by an appropriate SI unit and that the unit mmol m-2 was the most convenient and least cumbersome option. It expressed the hope that this unit would eventually supplant the Dobson Unit. However, as of March 2017, there is little evidence that this has occurred; for example the Dobson unit is still used by NASA and by the World Ozone and Ultraviolet Radiation Data Center in their reporting of total ozone amount.\n\nNASA uses a baseline value of 220 DU for ozone. This was chosen as the starting point for observations of the Antarctic ozone hole, since values of less than 220 Dobson units were not found before 1979. Also, from direct measurements over Antarctica, a column ozone level of less than 220 Dobson units is a result of the ozone loss from chlorine and bromine compounds.\n\nIn addition, Dobson units are often used to describe total column densities of sulfur dioxide, which occurs in the atmosphere in small amounts due to the combustion of fossil fuels, from biological processes releasing dimethyl sulfide, or by natural combustion such as forest fires. Large amounts of sulfur dioxide may be released into the atmosphere as well by volcanic eruptions. The Dobson unit is used to describe total column amounts of sulfur dioxide because it appeared in the early days of ozone remote sensing on ultraviolet satellite instruments (such as TOMS).\n\nThe Dobson Unit arises from the ideal gas law. From the real gas law:\n\nformula_1\n\nwhere P and V are pressure and volume, respectively, and n, R and T are the number of moles of gas, the gas constant (8.314 J/mol K), and T is temperature in Kelvin (K).\n\nThe number density of air is the number of molecules or atoms per unit volume:\n\nformula_2\n\nand when plugged into the real gas law, the number density of air is found by using pressure, temperature and the real gas constant.\n\nformula_3\n\nThe number density (molecules/volume) of air at standard temperature and pressure (T = 273K and P = 101325 Pa) is, by using this equation:\n\nformula_4\n\nWith some unit conversions of Joules to Pascals, the equation for molecules/volume is\n\nformula_5\n\nA Dobson Unit is the total amount of a trace gas per unit area. In atmospheric sciences, this is referred to as a column density. How, though, do we go from units of molecules per cubic meter, a volume, to molecules per square centimeter, an area? This must be done by integration. To get a column density, we must integrate the total column over a height. Per the definition of Dobson Units, we see that 1 DU = 0.01 mm of trace gas when compressed down to sea level at standard temperature and pressure. So if we integrate our number density of air from 0 to 0.01 mm, we find the number density which is equal to 1 DU:\n\nformula_6\n\nAnd thus we come up with the value of 1 DU, which is 2.69×10 molecules per meter squared.\n"}
{"id": "12772040", "url": "https://en.wikipedia.org/wiki?curid=12772040", "title": "Doubly ionized oxygen", "text": "Doubly ionized oxygen\n\nIn astronomy and atomic physics, doubly ionized oxygen is the ion O (also known as O III in spectroscopic notation). Its emission forbidden lines in the visible spectrum, primarily at the wavelength 500.7 nm, and secondarily at 495.9 nm, are known in astronomical spectroscopy as [O III]. Before spectra of oxygen ions became known, these lines once led to a spurious identification of the substance as a new chemical element. Concentrated levels of O III are found in diffuse and planetary nebulae. Consequently, narrow band-pass filters that isolate the 501 nm and 496 nm wavelengths of light, that correspond to green-turquoise-cyan spectral colors, are useful in observing these objects, causing them to appear at higher contrast against the filtered and consequently blacker background of space (and possibly light-polluted terrestrial atmosphere) where the frequencies of [O III] are much less pronounced.\n\nThese emission lines were first discovered in the spectra of planetary nebulae in the 1860s. At that time, they were thought to be due to a new element which was named \"nebulium\". In 1927, Ira Sprague Bowen came up with the current explanation of them being due to doubly ionized oxygen.\n\nPermitted lines of O III lie in the Middle Ultraviolet band and are hence inaccessible to terrestrial astronomy.\n\n"}
{"id": "1228141", "url": "https://en.wikipedia.org/wiki?curid=1228141", "title": "Dry season", "text": "Dry season\n\nThe dry season is a yearly period of low rainfall, especially in the tropics. The weather in the tropics is dominated by the tropical rain belt, which moves from the northern to the southern tropics and back over the course of the year. The tropical rain belt lies in the southern hemisphere roughly from October to March; during that time the northern tropics have a dry season with sparser precipitation, and days are typically sunny throughout. From April to September, the rain belt lies in the northern hemisphere, and the southern tropics have their dry season. Under the Köppen climate classification, for tropical climates, a dry season month is defined as a month when average precipitation is below .\n\nThe dry season has low humidity, and some watering holes and rivers dry up. This lack of water (and hence of food) may force many grazing animals to migrate to more fertile spots. Examples of such animals are zebras, elephants, and wildebeest. Because of the lack of water in the plants, bushfires are common.\n\nData shows that in Africa the start of the dry season coincides with a rise in the cases of measles—which researchers believe might be attributed to the higher concentration of people in the dry season, as agricultural operations are all but impossible without irrigation. During this time, some farmers move into cities, creating hubs of higher population density, and allowing the disease to spread more easily.\n\nThe rain belt reaches roughly as far north as the Tropic of Cancer and as far south as the Tropic of Capricorn. Near these latitudes, there is one wet season and one dry season annually. At the equator there are two wet and two dry seasons, as the rain belt passes over twice a year, once moving north and once moving south. Between the tropics and the equator, locations may experience a short wet and a long wet season; and a short dry and a long dry season. Local geography may substantially modify these climate patterns, however.\n\nNew data shows that in the seasonal parts of the South American Amazon forest, foliage growth and coverage varies between the dry and wet seasons—with about 25% more leaves and faster growth in the dry season. Researchers believe that the Amazon itself has an effect in bringing the onset of the wet season: by growing more foliage, it evaporates more water. However, this growth appears only in the undisturbed parts of the Amazon basin, where researchers believe roots can reach deeper and gather more rainwater. It has also been shown that ozone levels are much higher in the dry than in the wet season in the Amazon basin.\n"}
{"id": "2436457", "url": "https://en.wikipedia.org/wiki?curid=2436457", "title": "Edna Walling", "text": "Edna Walling\n\nEdna Margaret Walling (4 December 1896 – 8 August 1973) was one of Australia's most influential landscape designers.\n\nWalling was born in Yorkshire and grew up in the village of Bickleigh in Devon, England. When she was fourteen years old she emigrated to New Zealand and three years later moved with her family to Melbourne, Victoria, Australia.\n\nWalling studied horticulture at Burnley College and after some years as a jobbing gardener she commenced her own landscape design practice in the 1920s, and \"went on to design some significant Arts and Crafts gardens\".\n\nIn the 1920s, Walling began to develop a village at Mooroolbark on the outskirts of Melbourne called Bickleigh Vale. With its unique collection of charming houses and gardens Bickleigh Vale is one of her most acclaimed achievements. It was designed to be 'the nucleus of an English village' and she built the first cottage, named after the village of Sonning on the River Thames in England, as her own home. She sold subdivisions of the land only to people who were prepared to accept designs for cottage and garden prepared by her. \n\nIn 1935 Ellis Stones built a wall for her. Recognizing his ability—which she called 'a rare thing this gift for placing stones' – she suggested that he work for her. She gave him a free hand to create walls, outcrops, pools and paths in her gardens at some of Melbourne's finest homes which assisted in establishing a local garden tradition. Their best collaboration was seen in a free-form swimming pool and outcrop, built in 1939-40 for Edith Hughes-Jones at Olinda, Victoria\n\nIn 1926, Walling began contributing regularly to \"Australian Home Beautiful\". Her design practice grew and she worked across Australia, in Perth, Hobart, Sydney, and Buderim in Queensland. Her Victorian commissions included designing the lily pond for Coombe Cottage, Dame Nellie Melba's residence in Coldstream, in the Yarra Valley Ranges; Durrol for Mrs Stanley Allen at Mount Macedon; Rock Lodge garden for Mrs PF O'Collins in Frankston; Cruden Farm garden for Mrs Keith Murdoch (later Dame Elisabeth), Langwarrin (Gardening Australia website: Cruden Farm). One of her most intact NSW commissions is Markdale, Binda (). Her plans from the 1920s and 1930s show a strong architectural framework with 'low stone walls, wide pergolas and paths – always softened with a mantle of greenery'. She later drew inspiration from the Australian bush, creating a more naturalistic style with boulders, rocky outcrops and indigenous plants. In small suburban gardens, Walling created garden 'rooms' to make the garden appear far larger than it actually was.\n\nHer designs were heavily influenced by her experience of the Devon countryside as a child and designers such as Gertrude Jekyll. The houses of American architect Royal Barry Wills (renowned for his Cape Cod designs) and Lewis Mumford’s books, \"The Culture of Cities\" and \"The Image of the City\", also provided early inspiration.\n\nShe was the author of several books on landscape design:\n\nIn the 1950s, Walling became interested in the conservation of roadside vegetation and was a prolific writer in the press on the subject as well as her 1952 book \"The Australian Roadside\". According to Trisha Dixon, Walling was an important influence on Australian gardening, steering tastes away from an Anglo-centric heritage towards a respect for the Australian climate and landscape.\n\nIn 1967, she moved from Melbourne to Bendles at Buderim in Queensland, where she had hoped to further develop the village concept but it did not progress. Despite her ill-health during her last years at Bendle, Walling continued to write prolifically, rewriting manuscripts, corresponding to newspapers on environmental issues, and trying to republish her books. About a quarter of Walling's designs survive and these are held in the State Library of Victoria and in private collections in Tasmania, South Australia, New South Wales and Victoria.\n\n"}
{"id": "55537933", "url": "https://en.wikipedia.org/wiki?curid=55537933", "title": "Erdemli Pine Groove", "text": "Erdemli Pine Groove\n\nErdemli Pine Groove is a nature park in Turkey. It is also called \"Talat Göktepe Groove\" to commemorate Talat Göktepe a former director of Forestry in Çanakkale who died while fighting against forest fire in 1994. It is located at to the west of Erdemli ilçe (district) of Mersin Province. It lies between the Turkish state highway and the Mediterranean Sea coast.\n\nIt was declared a picnic place in 1962 and a nature park in 2011.\n\nThe length of the grove is about in northeast to southwest direction. Its width is about . The total area is . The coastal part is a sandy beach. There are some sand dunes on the beach. The back part is a groove. Most of the trees are Turkish pine (Pinus brutia), Umbrella pine (Sciadopitys) and myrtle (Myrtus).\n"}
{"id": "41215", "url": "https://en.wikipedia.org/wiki?curid=41215", "title": "Ground (electricity)", "text": "Ground (electricity)\n\nIn electrical engineering, ground or earth is the reference point in an electrical circuit from which voltages are measured, a common return path for electric current, or a direct physical connection to the earth.\n\nElectrical circuits may be connected to ground (earth) for several reasons. In mains powered equipment, exposed metal parts are connected to ground so that if, due to any fault conditions, a \"line\" supply voltage connection occurs to any such conductive parts, the current flow will then be such that any protective equipment installed for either overload or \"leakage\" protection will operate and disconnect the line voltage. This is done to prevent harm resulting to the user from coming in contact with any such dangerous voltage in a situation where the user may, at the same time, also come in contact with an object at ground/earth potential. In electric power distribution systems, a protective earth (PE) conductor is an essential part of the safety provided by the earthing system.\n\nConnection to ground also limits the build-up of static electricity when handling flammable products or electrostatic-sensitive devices. In some telegraph and power transmission circuits, the earth itself can be used as one conductor of the circuit, saving the cost of installing a separate return conductor (see single-wire earth return).\n\nFor measurement purposes, the Earth serves as a (reasonably) constant potential reference against which other potentials can be measured. An electrical ground system should have an appropriate current-carrying capability to serve as an adequate zero-voltage reference level. In electronic circuit theory, a \"ground\" is usually idealized as an infinite source or sink for charge, which can absorb an unlimited amount of current without changing its potential. Where a real ground connection has a significant resistance, the approximation of zero potential is no longer valid. Stray voltages or earth potential rise effects will occur, which may create noise in signals or if large enough will produce an electric shock hazard.\n\nThe use of the term ground (or earth) is so common in electrical and electronics applications that circuits in portable electronic devices such as cell phones and media players as well as circuits in vehicles may be spoken of as having a \"ground\" connection without any actual connection to the Earth, despite \"common\" being a more appropriate term for such a connection. This is usually a large conductor attached to one side of the power supply (such as the \"ground plane\" on a printed circuit board) which serves as the common return path for current from many different components in the circuit.\n\nLong-distance electromagnetic telegraph systems from 1820 onwards used two or more wires to carry the signal and return currents. It was then discovered, probably by the German scientist Carl August Steinheil in 1836–1837, that the ground could be used as the return path to complete the circuit, making the return wire unnecessary. However, there were problems with this system, exemplified by the transcontinental telegraph line constructed in 1861 by the Western Union Company between St. Joseph, Missouri, and Sacramento, California. During dry weather, the ground connection often developed a high resistance, requiring water to be poured on the ground rod to enable the telegraph to work or phones to ring.\n\nLater, when telephony began to replace telegraphy, it was found that the currents in the earth induced by power systems, electrical railways, other telephone and telegraph circuits, and natural sources including lightning caused unacceptable interference to the audio signals, and the two-wire or 'metallic circuit' system was reintroduced around 1883.\n\nAn electrical connection to earth can be used as a reference potential for radio frequency signals for certain kinds of antennas. The part directly in contact with the earth - the \"earth electrode\" - can be as simple as a metal rod or stake driven into the earth, or a connection to buried metal water piping (the pipe must be conductive). Because high frequency signals can flow to earth due to capacitative effects, capacitance to ground is an important factor in effectiveness of signal grounds. Because of this, a complex system of buried rods and wires can be effective. An ideal signal ground maintains a fixed potential (zero) regardless of how much electric current flows into ground or out of ground. Low impedance at the signal frequency of the electrode-to-earth connection determines its quality, and that quality is improved by increasing the surface area of the electrode in contact with the earth, increasing the depth to which it is driven, using several connected ground rods, increasing the moisture content of the soil, improving the conductive mineral content of the soil, and increasing the land area covered by the ground system.\n\nSome types of transmitting antenna systems in the VLF, LF, MF and lower SW range must have a good ground to operate efficiently. For example, a vertical monopole antenna requires a ground plane that often consists of an interconnected network of wires running radially away from the base of the antenna for a distance about equal to the height of the antenna. Sometimes a counterpoise is used as a ground plane, supported above the ground.\n\nElectrical power distribution systems are often connected to ground to limit the voltage that can appear on distribution circuits. A distribution system insulated from ground may attain a high potential due to transient voltages caused by arcing, static electricity, or accidental contact with higher potential circuits. A ground connection of the system dissipates such potentials and limits the rise in voltage of the grounded system.\n\nIn a mains electricity (AC power) wiring installation, the term \"ground conductor\" typically refers to three different conductors or conductor systems as listed below.\n\n\"Equipment earthing conductors\" provide an electrical connection between the physical ground (earth) and the grounding/bonding system, which connects (bonds) the normally non-current-carrying metallic parts of equipment. According to the U.S. National Electrical Code (NEC), the reason for doing this is to limit the voltage imposed by lightning, line surges, and contact with higher voltage lines. \n\n\"Equipment bonding conductors\" provide a low impedance path between normally non-current-carrying metallic parts of equipment and one of the conductors of that electrical system's source. If any exposed metal part should become energized (fault), such as by a frayed or damaged conductor, it creates a short circuit, causing the overprotection device (circuit breaker or fuse) to open, clearing (disconnecting) the fault. It is important to note this action occurs regardless of whether there is a connection to the physical ground (earth); the earth itself has no role in this fault-clearing process since current must return to its source; however, the sources are very frequently connected to the physical ground (earth). (see Kirchhoff's circuit laws). By bonding (interconnecting) all exposed non-current carrying metal objects together and to other metallic objects such as pipes or structural steel, they should remain near the same voltage potential, thus reducing the chance of a shock. This is especially important in bathrooms where one may be in contact with several different metallic systems such as supply and drain pipes and appliance frames. When a system needs to be connected to the physical ground (earth), the equipment bonding conductor also becomes the equipment earthing conductor (see above).\nA (GEC) is used to connect the system grounded (\"neutral\") conductor, or the equipment to a grounding electrode, or a point on the grounding electrode system. This is called \"system grounding\" and most electrical systems are required to be grounded. The U.S. NEC and the UK's BS 7671 list systems that are required to be grounded. According to the NEC, the purpose of connecting an electrical system to the physical ground (earth) is to limit the voltage imposed by lightning events and contact with higher voltage lines, and also for voltage stabilization. In the past, water supply pipes were used as grounding electrodes, but due to the increased use of plastic pipes, which are poor conductors, the use of an actual grounding electrode is required. This type of ground applies to radio antennas and to lightning protection systems.\n\nPermanently installed electrical equipment, unless not required to, has permanently connected grounding conductors. Portable electrical devices with metal cases may have them connected to earth ground by a pin on the attachment plug (see Domestic AC power plugs and sockets). The size of power grounding conductors is usually regulated by local or national wiring regulations.\n\nIn electricity supply systems, an earthing (grounding) system defines the electrical potential of the conductors relative to that of the Earth's conductive surface. The choice of earthing system has implications for the safety and electromagnetic compatibility of the power supply. Regulations for earthing systems vary considerably between different countries.\n\nA functional earth connection serves more than protecting against electrical shock, as such a connection may carry current during the normal operation of a device. Such devices include surge suppression, electromagnetic-compatibility filters, some types of antennas, and various measurement instruments. Generally the protective earth system is also used as a functional earth, though this requires care.\n\nDistribution power systems may be solidly grounded, with one circuit conductor directly connected to an earth grounding electrode system. Alternatively, some amount of electrical impedance may be connected between the distribution system and ground, to limit the current that can flow to earth. The impedance may be a resistor, or an inductor (coil). In a high-impedance grounded system, the fault current is limited to a few amperes (exact values depend on the voltage class of the system); a low-impedance grounded system will permit several hundred amperes to flow on a fault. A large solidly grounded distribution system may have thousands of amperes of ground fault current.\n\nIn a polyphase AC system, an artificial neutral grounding system may be used. Although no phase conductor is directly connected to ground, a specially constructed transformer (a \"zig zag\" transformer) blocks the power frequency current from flowing to earth, but allows any leakage or transient current to flow to ground.\n\nLow-resistance grounding systems use a neutral grounding resistor (NGR) to limit the fault current to 25 A or greater. Low resistance grounding systems will have a time rating (say, 10 seconds) that indicates how long the resistor can carry the fault current before overheating. A ground fault protection relay must trip the breaker to protect the circuit before overheating of the resistor occurs.\n\nHigh-resistance grounding (HRG) systems use an NGR to limit the fault current to 25 A or less. They have a continuous rating, and are designed to operate with a single-ground fault. This means that the system will not immediately trip on the first ground fault. If a second ground fault occurs, a ground fault protection relay must trip the breaker to protect the circuit. On an HRG system, a sensing resistor is used to continuously monitor system continuity. If an open-circuit is detected (e.g., due to a broken weld on the NGR), the monitoring device will sense voltage through the sensing resistor and trip the breaker. Without a sensing resistor, the system could continue to operate without ground protection (since an open circuit condition would mask the ground fault) and transient overvoltages could occur.\n\nWhere the danger of electric shock is high, special ungrounded power systems may be used to minimize possible leakage current to ground. Examples of such installations include patient care areas in hospitals, where medical equipment is directly connected to a patient and must not permit any power-line current to pass into the patient's body. Medical systems include monitoring devices to warn of any increase of leakage current. On wet construction sites or in shipyards, isolation transformers may be provided so that a fault in a power tool or its cable does not expose users to shock hazard.\n\nCircuits used to feed sensitive audio/video production equipment or measurement instruments may be fed from an isolated ungrounded technical power system to limit the injection of noise from the power system.\n\nIn single-wire earth return (SWER) AC electrical distribution systems, costs are saved by using just a single high voltage conductor for the power grid, while routing the AC return current through the earth. This system is mostly used in rural areas where large earth currents will not otherwise cause hazards.\n\nSome high-voltage direct-current (HVDC) power transmission systems use the ground as second conductor. This is especially common in schemes with submarine cables, as sea water is a good conductor. Buried grounding electrodes are used to make the connection to the earth. The site of these electrodes must be chosen carefully to prevent electrochemical corrosion on underground structures.\n\nA particular concern in design of electrical substations is earth potential rise. When very large fault currents are injected into the earth, the area around the point of injection may rise to a high potential with respect to distant points. This is due to the limited finite conductivity of the layers of soil in the earth. The gradient of the voltage (changing voltage within a distance) may be so high that two points on the ground may be at significantly different potentials, creating a hazard to anyone standing on the ground in the area. Pipes, rails, or communication wires entering a substation may see different ground potentials inside and outside the substation, creating a dangerous touch voltage.\n\nSignal grounds serve as return paths for signals and power (at extra low voltages, less than about 50 V) within equipment, and on the signal interconnections between equipment. Many electronic designs feature a single return that acts as a reference for all signals. Power and signal grounds often get connected, usually through the metal case of the equipment. Designers of printed circuit boards must take care in the layout of electronic systems so that high-power or rapidly switching currents in one part of a system do not inject noise into low-level sensitive parts of a system due to some common impedance in the grounding traces of the layout.\n\nVoltage is measured on an interval scale, which means that only differences can be measured. To measure the voltage of a single point, a reference point must be selected to measure against. This common reference point is called \"ground\" and considered to have zero voltage. This signal ground may or may not be connected to a power ground. A system where the system ground is not connected to another circuit or to earth (though there may still be AC coupling) is often referred to as a floating ground or double-insulated.\n\nSome devices require a connection to the mass of earth to function correctly, as distinct from any purely protective role. Such a connection is known as a functional earth- for example some long wavelength antenna structures require a functional earth connection, which generally should not be indiscriminately connected to the supply protective earth, as the introduction of transmitted radio frequencies into the electrical distribution network is both illegal and potentially dangerous. Because of this separation, a purely functional ground should not normally be relied upon to perform a protective function.\nTo avoid accidents, such functional grounds are normally wired in white or cream cable, and not green or green/yellow.\n\nIn television stations, recording studios, and other installations where signal quality is critical, a special signal ground known as a \"technical ground\" (or \"technical earth\", \"special earth\", and \"audio earth\") is often installed, to prevent ground loops. This is basically the same thing as an AC power ground, but no general appliance ground wires are allowed any connection to it, as they may carry electrical interference. For example, only audio equipment is connected to the technical ground in a recording studio. In most cases, the studio's metal equipment racks are all joined together with heavy copper cables (or flattened copper tubing or busbars) and similar connections are made to the technical ground. Great care is taken that no general chassis grounded appliances are placed on the racks, as a single AC ground connection to the technical ground will destroy its effectiveness. For particularly demanding applications, the main technical ground may consist of a heavy copper pipe, if necessary fitted by drilling through several concrete floors, such that all technical grounds may be connected by the shortest possible path to a grounding rod in the basement.\n\nLightning protection systems are designed to mitigate the effects of lightning through connection to extensive grounding systems that provide a large surface area connection to earth. The large area is required to dissipate the high current of a lightning strike without damaging the system conductors by excess heat. Since lightning strikes are pulses of energy with very high frequency components, grounding systems for lightning protection tend to use short straight runs of conductors to reduce the self-inductance and skin effect.\n\nStrictly speaking, the terms \"grounding\" or \"earthing\" are meant to refer to an electrical connection to ground/earth. Bonding is the practice of intentionally electrically connecting metallic items not designed to carry electricity. This brings all the bonded items to the same electrical potential as a protection from electrical shock. The bonded items can then be connected to ground to bring them to earth potential.\n\nIn an electrical substation a ground (earth) mat is a mesh of conductive material installed at places where a person would stand to operate a switch or other apparatus; it is bonded to the local supporting metal structure and to the handle of the switchgear, so that the operator will not be exposed to a high differential voltage due to a fault in the substation.\n\nIn the vicinity of electrostatic sensitive devices, a ground (earth) mat or grounding (earthing) mat is used to ground static electricity generated by people and moving equipment. There are two types used in static control: Static Dissipative Mats, and Conductive Mats.\n\nA static dissipative mat that rests on a conductive surface (commonly the case in military facilities) are typically made of 3 layers (3-ply) with static dissipative vinyl layers surrounding a conductive substrate which is electrically attached to ground (earth). For commercial uses, static dissipative rubber mats are traditionally used that are made of 2 layers (2-ply) with a tough solder resistant top static dissipative layer that makes them last longer than the vinyl mats, and a conductive rubber bottom. Conductive mats are made of carbon and used only on floors for the purpose of drawing static electricity to ground as quickly as possible. Normally conductive mats are made with cushioning for standing and are referred to as \"anti-fatigue\" mats.\n\nFor a static dissipative mat to be reliably grounded it must be attached to a path to ground. Normally, both the mat and the wrist strap are connected to ground by using a common point ground system (CPGS).\n\nIn computer repair shops and electronics manufacturing workers must be grounded before working on devices sensitive to voltages capable of being generated by humans. For that reason static dissipative mats can be and are also used on production assembly floors as \"floor runner\" along the assembly line to draw static generated by people walking up and down.\n\nIsolation is a mechanism that defeats grounding. It is frequently used with low-power consumer devices, and when electronics engineers, hobbyists, or repairmen are working on circuits that would normally be operated using the power line voltage. Isolation can be accomplished by simply placing a \"1:1 wire ratio\" transformer with an equal number of turns between the device and the regular power service, but applies to any type of transformer using two or more coils electrically insulated from each other.\n\nFor an isolated device, touching a single powered conductor does not cause a severe shock, because there is no path back to the other conductor through the ground. However, shocks and electrocution may still occur if both poles of the transformer are contacted by bare skin. Previously it was suggested that repairmen \"work with one hand behind their back\" to avoid touching two parts of the device under test at the same time, thereby preventing a circuit from crossing through the chest and interrupting cardiac rhythms/ causing cardiac arrest.\n\nGenerally every AC power line transformer acts as an isolation transformer, and every step up or down has the potential to form an isolated circuit. However, this isolation would prevent failed devices from blowing fuses when shorted to their ground conductor. The isolation that could be created by each transformer is defeated by always having one leg of the transformers grounded, on both sides of the input and output transformer coils. Power lines also typically ground one specific wire at every pole, to ensure current equalization from pole to pole if a short to ground is occurring.\n\nIn the past, grounded appliances have been designed with internal isolation to a degree that allowed the simple disconnection of ground by cheater plugs without apparent problem (a dangerous practice, since the safety of the resulting floating equipment relies on the insulation in its power transformer). Modern appliances however often include power entry modules which are designed with deliberate capacitive coupling between the AC power lines and chassis, to suppress electromagnetic interference. This results in a significant leakage current from the power lines to ground. If the ground is disconnected by a cheater plug or by accident, the resulting leakage current can cause mild shocks, even without any fault in the equipment. Even small leakage currents are a significant concern in medical settings, as the accidental disconnection of ground can introduce these currents into sensitive parts of the human body. As a result, medical power supplies are designed to have low capacitance.\n\nClass II appliances and power supplies (such as cell phone chargers) do not provide any ground connection, and are designed to isolate the output from input. Safety is ensured by double-insulation, so that two failures of insulation are required to cause a shock.\n\n\n"}
{"id": "5777537", "url": "https://en.wikipedia.org/wiki?curid=5777537", "title": "Hack's law", "text": "Hack's law\n\nHack's law is an empirical relationship between the length of streams and the area of their basins. If \"L\" is the length of the longest stream in a basin, and \"A\" is the area of the basin, then Hack's law may be written as\n\nfor some constant \"C\" where the exponent \"h\" is slightly less than 0.6 in most basins. \"h\" varies slightly from region to region and slightly decreases for larger basins (>8,000 mi², or 20,720 km²). A theoretical value \"h\" = 4/7 ≈ 0.571 for the exponent has been derived (Birnir, 2008). In addition to the catchment-scales, Hack's law was observed on unchanneled small-scale surfaces when the morphology measured at high resolutions (Cheraghi et al., 2018).\n\nThe law is named after American geomorphologist John Tilton Hack. \n\n"}
{"id": "2032430", "url": "https://en.wikipedia.org/wiki?curid=2032430", "title": "High-nutrient, low-chlorophyll regions", "text": "High-nutrient, low-chlorophyll regions\n\nHigh-nutrient, low-chlorophyll (HNLC) regions are regions of the ocean where the abundance of phytoplankton is low and fairly constant despite the availability of macronutrients. Phytoplankton rely on a suite of nutrients for cellular function. Macronutrients (e.g., nitrate, phosphate, silicic acid) are generally available in higher quantities in surface ocean waters, and are the typical components of common garden fertilizers. Micronutrients (e.g., iron, zinc, cobalt) are generally available in lower quantities and include trace metals. Macronutrients are typically available in millimolar concentrations, while micronutrients are generally available in micro- to nanomolar concentrations. In general, nitrogen tends to be a limiting ocean nutrient, but in HNLC regions it is never significantly depleted. Instead, these regions tend to be limited by low concentrations of metabolizable iron. Iron is a critical phytoplankton micronutrient necessary for enzyme catalysis and electron transport.\n\nBetween the 1930s and '80s, it was hypothesized that iron is a limiting ocean micronutrient, but there were not sufficient methods to reliably detect iron in seawater to confirm this hypothesis. In 1989, high concentrations of iron-rich sediments in nearshore coastal waters off the Gulf of Alaska were detected. However, offshore waters had lower iron concentrations and lower productivity despite macronutrient availability for phytoplankton growth. This pattern was observed in other oceanic regions and led to the naming of three major HNLC zones: the North Pacific Ocean, the Equatorial Pacific Ocean, and the Southern Ocean.\n\nThe discovery of HNLC regions has fostered scientific debate about the ethics and efficacy of iron fertilization experiments which attempt to draw down atmospheric carbon dioxide by stimulating surface-level photosynthesis. It has also led to the development of hypotheses such as grazing control which poses that HNLC regions are formed, in part, from the grazing of phytoplankton (e.g. dinoflagellates, ciliates) by smaller organisms (e.g. protists).\n\nPrimary production is the process by which autotrophs use light to convert carbon from aqueous carbon dioxide to sugar for cellular growth. Light catalyzes the photosynthetic process and nutrients are incorporated into organic material. For photosynthesis to occur, macronutrients such as nitrate and phosphate must be available in sufficient ratios and bioavailable forms for biological utilization. The molecular ratio of 106(Carbon):16(Nitrogen):1(Phosphorus) was discovered by Redfield, Ketcham, and Richards (RKR) and is known as the Redfield Ratio. Photosynthesis (forward) and respiration (reverse) is represented by the equation:\n\nPhotosynthesis can be limited by deficiencies of certain macronutrients. However, in the North Pacific, the Equatorial Pacific, and the Southern Ocean macronutrients are found in sufficient ratios, quantities and bioavailable forms to support greater levels of primary production than found. Macronutrient availability in HNLC regions in tandem with low standing stocks of phytoplankton suggests that some other biogeochemical process limits phytoplankton growth.\n\nSince primary production and phytoplankton biomass cannot currently be measured over entire ocean basins, scientists use chlorophyll α as a proxy for primary production. Modern satellite observations monitor and track global chlorophyll α abundances in the ocean via remote sensing. Higher chlorophyll concentrations generally indicate areas of enhanced primary production, and conversely lower chlorophyll levels indicate low primary production. This co-occurrence of low chlorophyll and high macronutrient availability is why these regions are deemed \"high-nutrient, low-chlorophyll.\"\n\nIn addition to the macronutrients needed for organic matter synthesis, phytoplankton need micronutrients such as trace metals for cellular functions. Micronutrient availability can constrain primary production because trace metals are sometimes limiting nutrients. Iron has been determined to be a primary limiting micronutrient in HNLC provinces. Recent studies have indicated that zinc and cobalt may be secondary and/or co-limiting micronutrients.\n\nHNLC regions cover 20% of the world’s oceans and are characterized by varying physical, chemical, and biological patterns. These surface waters have annually varying, yet relatively abundant macronutrient concentrations compared to other oceanic provinces. While HNLC broadly describes the biogeochemical trends of these large ocean regions, all three zones experience seasonal phytoplankton blooms in response to global atmospheric patterns. On average, HNLC regions tend to be growth-limited by iron and variably, zinc. This trace metal limitation leads to communities of smaller sized phytoplankton. Compared to more productive regions of the ocean, HNLC zones have higher ratios of silicic acid to nitrate because larger diatoms, that require silicic acid to make their opal silica shells, are less prevalent. Unlike the Southern Ocean and the North Pacific, the Equatorial Pacific experiences temporal silicate availability which leads to large seasonal diatom blooms.\n\nThe distribution of trace metals and relative abundance of macronutrients are reflected in the plankton community structure. For example, the selection of phytoplankton with a high surface area to volume ratio results in HNLC regions being dominated by nano- and picoplankton. This ratio allows for optimal utilization of available dissolved nutrients. Larger phytoplankton, such as diatoms, cannot energetically sustain themselves in these regions. Common picoplankton within these regions include genera such as prochlorococcus (not generally found in the North Pacific), synechococcus, and various eukaryotes. Grazing protists likely control the abundance and distribution of these small phytoplankton.\n\nThe generally lower net primary production in HNLC zones results in lower biological draw-down of atmospheric carbon dioxide and thus these regions are generally considered a net source of carbon dioxide to the atmosphere. HNLC areas are of interest to geoengineers and some in the scientific community who believe fertilizing large patches of these waters with iron could potentially lower dissolved carbon dioxide and offset increased anthropogenic carbon emissions. Analysis of Antarctic ice core data over the last million years shows correlation between high levels of dust and low temperature, indicating that addition of diffuse iron-rich dust to the sea has been a natural amplifier of climate cooling. \n\nThe discovery and naming of the first HNLC region, the North Pacific, was formalized in a seminal paper published in 1988. The study concluded that surface waters of the eastern North Pacific are generally dominated by picoplankton despite the relative abundance of macronutrients. In other words, larger phytoplankton, such as diatoms which thrive in nutrient-rich waters, were not found. Instead, the surface waters were replete with smaller pico- and nanoplankton. Based on laboratory nutrient experiments, iron was hypothesized to be a key limiting micronutrient.\n\nThe Pacific Ocean is the largest and oldest body of water on Earth. The North Pacific is characterized by the general clockwise rotation of the North Pacific gyre, which is driven by trade winds. Spatial variations in tradewinds result in cooler air temperatures in the western North Pacific and milder air temperatures in the eastern North Pacific (i.e., Subartic Pacific). Iron is supplied to the North Pacific by dust storms that occur in Asia and Alaska as well as iron-rich waters advected from the continental margin, sometimes by eddies such as Haida Eddies.\n\nConcentrations of iron however vary throughout the year. Ocean currents are driven by seasonal atmospheric patterns which transport iron from the Kuril-Kamchatka margin into the western Subarctic Pacific. This introduction of iron provides a subsurface supply of micronutrients, which can be used by primary producers during upwelling of deeper waters to the surface. Seafloor depth may also stimulate phytoplankton blooms in HNLC regions as iron diffuses from the seafloor and alleviates iron limitation in shallow waters. Research conducted in the Gulf of Alaska showed that areas with shallow waters, such as the south shelf of Alaska, have more intense phytoplankton blooms than offshore waters. Volcanic ash from the eruption of the Kasatochi volcano in August 2008 provided an example of natural iron fertilization in the Northeast Pacific Ocean. The region was fertilized by raining volcanic dust containing soluble iron. In the days following, phytoplankton blooms were visible from space.\n\nLimitations in trace metal concentrations in the North Pacific limit diatom blooms throughout the entire year. Even though the North Pacific is an HNLC region, it produces and exports to the ocean interior a relatively high amount of particulate biogenic silica compared to the North Atlantic, which supports significant diatom growth.\n\nThe Equatorial Pacific is an oceanic province characterized by nearly year-round upwelling due to the convergence of trade winds from the northeast and southeast at the Intertropical Convergence Zone. The Equatorial Pacific spans nearly half of Earth’s circumference and plays a major role in global marine new primary production. New production is a term used in biological oceanography to describe the way in which nitrogen is recycled within the ocean. In regions of enhanced new production, nitrate from the aphotic zone makes its way into surface waters, replenishing surface nitrate supply. Despite nitrogen availability in Equatorial Pacific waters, primary production and observed surface ocean biomass is considerably lower compared to other major upwelling regions of the ocean. Thus the Equatorial Pacific is considered one of the three major HNLC regions.\n\nLike other major HNLC provinces, the Equatorial Pacific is considered nutrient-limited due to lack of trace metals such as iron. The Equatorial Pacific receives approximately 7-10 times more iron from Equatorial Undercurrent (EUC) upwelling than from inputs due to settling atmospheric dust. Climate reconstructions of glacial periods using sediment proxy records have revealed that the Equatorial Pacific may have been 2.5 times more productive than the modern equatorial ocean. During these glacial periods the Equatorial Pacific increased its export of marine new production, thus providing a sink of atmospheric carbon dioxide. The science of paleoceanography attempts to understand the interplay of glacial cycles with ocean dynamics. Paleo-oceanographers currently challenge the Aeolian Dust hypothesis which suggests that the transport of atmospheric iron-rich off Central and South America controls the intensity of primary production in the Equatorial Pacific. One study suggests that because EUC upwelling provides most of the bioavailable iron to the equatorial surface waters, the only method to reverse HNLC conditions is by enhancing upwelling. In other words, enhanced regional upwelling, rather than iron-rich atmospheric dust deposition, may explain why this region experiences higher primary productivity during glacial periods.\n\nCompared to the North Pacific and Southern Ocean, Equatorial Pacific waters have relatively low levels of biogenic silica and thus don't support significant standing stocks of diatoms Picoplankton are the most abundant marine primary producers in these regions due mainly to their ability to assimilate low concentrations of trace metals. Various phytoplankton communities within the Equatorial Pacific are grazed at the same rate as their growth rate, which further limits primary production.\n\nThere is no current consensus regarding which of the two main hypotheses (grazing or micronutrients) controls production in these equatorial waters. It is likely that trace metal limitations select for smaller-celled organisms, which thereby increases the grazing pressure of protists. While the Equatorial Pacific maintains HNLC characteristics, productivity can be high at times. Productivity leads to an abundance of seabirds such as storm petrels near the convergence of subtropical water and the equatorial \"cold tongue.\" The Equatorial Pacific contains the world's largest yellowfin tuna fisheries and is home to the spotted dolphin.\n\nThe Southern Ocean is the largest HNLC region in the global ocean. The surface waters of the Southern Ocean have been widely identified as being rich in macronutrients despite low phytoplankton stocks. Iron deposited in the North Atlantic is incorporated into North Atlantic Deep Water and is transported to the Southern Ocean via thermohaline circulation. Eventually mixing with the Antarctic Circumpolar Water, upwelling provides iron and macronutrients to the Southern Ocean surface waters. Therefore, iron inputs and primary production in the Southern Ocean are sensitive to iron-rich Saharan dust deposited over the Atlantic. Because of low atmospheric dust inputs directly onto Southern Ocean surface waters, chlorophyll α concentrations are low. Light availability in the Southern Ocean changes dramatically seasonally, but it does not seem to be a significant constraint on phytoplankton growth.\n\nMacronutrients present in Southern Ocean surface waters come from upwelled deep water. While micronutrients such as zinc and cobalt may possibly co-limit phytoplankton growth in the Southern Ocean, iron appears to be a critical limiting micronutrient. Some regions of the Southern Ocean experience both adequate bioavailable iron and macronutrient concentrations yet phytoplankton growth is limited. Hydrographic studies and explorations of the Southern Drake Passage region have observed this phenomenon around the Crozet Islands, Kerguelen Islands, and South Georgia and the South Sandwich Islands. These areas are adjacent to shelf regions of Antarctica and islands of the Southern Ocean. The micronutrients required for algal growth are believed to be supplied from the shelves themselves. Except in areas close to the Antarctic shelf, micronutrient deficiency severely limits productivity in the Southern Ocean.\n\nIron availability is not the only regulator of phytoplankton productivity and biomass. In the Southern Ocean, prevailing low temperatures are believed to have a negative impact on phytoplankton growth rates. Phytoplankton growth rate is very intense and short lived in open areas surrounded by sea ice and permanent sea-ice zones. Grazing by herbivores such as krill, copepods and salps is believed to suppress phytoplankton standing stock. Unlike the open waters of the Southern Ocean, grazing along continental shelf margins is low, so most phytoplankton that are not consumed sink to the sea floor which provides nutrients to benthic organisms.\n\nGiven the remote location of HNLC areas, scientists have combined modeling and observational data in order to study limits on primary production. Combining these two data sources allows for comparison between the North Pacific, Equatorial Pacific, and Southern Ocean. Two current explanations for global HNLC regions are growth limitations due to iron availability and phytoplankton grazing controls.\n\nIn 1988, John Martin confirmed the hypothesis that iron limits phytoplankton blooms and growth rates in the North Pacific. His work was extrapolated to other HNLC regions through evidence which linked low surface iron concentration with low chlorophyll. In response to iron fertilization experiments (IronEx, SOIREE, SEEDS, etc.) in HNLC areas, large phytoplankton responses such as decreased surface nutrient concentration and increased biological activity were observed.\n\nIron fertilization studies conducted at repeated intervals over the span of a week have produced a larger biological response than a single fertilization event. The biological response size tends to depend on a site’s biological, chemical, and physical characteristics. In the Equatorial and North Pacific, silica is thought to constrain additional production after iron fertilization, while light limits additional production in the Southern Ocean. Native, smaller phytoplankton were initial responders to increased iron, but were quickly outcompeted by larger, coastal phytoplankton such as diatoms. The large bloom response and community shift has led to environmental concerns about fertilizing large sections of HNLC regions. One study suggests that diatoms grow preferentially during fertilization experiments. Some diatoms, such as pseudo-nitzschia, release the neurotoxin domoic acid, poisoning grazing fish. If diatoms grow preferentially during iron fertilization experiments, sustained fertilizations could enhance domoic acid poisoning in the marine food web near fertilized patches.\nIron enters remote HNLC regions through two primary methods: upwelling of nutrient-rich water and atmospheric dust deposition. Iron needs to be replenished frequently and in bioavailable forms because of its insolubility, rapid uptake through biological systems, and binding affinity with ligands. Dust deposition might not result in phytoplankton blooms unless settling dust is in the correct bioavailable form of iron. Additionally, iron must be deposited during productive seasons and coincide with the appropriate RKR-ratios of surface nutrients. Aeolian dust has a larger influence on Northern Hemisphere HNLC regions because more land mass contributes to more dust deposition. Due to the Southern Ocean's isolation from land, upwelling related to eddy diffusivity provides iron to HNLC regions.\n\nFormulated by John Walsh in 1976, the grazing hypothesis states that grazing by heterotrophs suppresses primary productivity in areas of high nutrient concentrations. Predation by microzooplankton primarily accounts for phytoplankton loss in HNLC regions. Grazing by larger zooplankton and advective mixing are also responsible for a small proportion of losses to phytoplankton communities. Constant grazing limits phytoplankton to a low, constant standing stock. Without this grazing pressure, some scientists believe small phytoplankton would produce blooms despite micronutrient depletion because smaller phytoplankton typically have lower iron requirements and can absorb nutrients at a slower rate.\n\nCurrent scientific consensus agrees that HNLC areas lack high productivity because of a combination of iron and physiological limitations, grazing pressure, and physical forcings. The extent to which each factor contributes to low production may differ in each HNLC region. Iron limitation allows for smaller, more iron-frugal phytoplankton to grow at rapid rates, while grazing by microzooplankton maintains stable stocks of these smaller phytoplankton. Once micronutrients become available, grazing may then limit bloom sizes. Additional micronutrient limitations from trace metals like zinc or cobalt may suppress phytoplankton blooms. Turbulent mixing at higher-latitude HNLC regions (North Pacific and Southern Ocean) may mix phytoplankton below the critical depth needed to have community growth.\n\nSince past iron fertilization experiments have resulted in large phytoplankton blooms, some have suggested that large-scale ocean fertilization experiments should be conducted to draw down inorganic anthropogenic carbon dioxide in the form of particulate organic carbon. Fertilization would stimulate biological productivity, leading to a decrease in the amount of inorganic surface carbon dioxide within a fertilized patch. The bloom would then die off and presumably sink to the deep ocean, taking much of the absorbed carbon dixoide to the seafloor and sequestering it from the short-term carbon cycle in the deep ocean or ocean sediments.\n\nTo effectively remove anthropogenic carbon from the atmosphere, iron fertilization would need to result in significant removal of particulate carbon from the surface ocean and transport it to the deep ocean. Various studies estimated that less than 7-10% of carbon taken up during a bloom would be sequestered, and only a 15-25 ppm decrease in atmospheric carbon dioxide would result with sustained global iron fertilization. The amount of carbon dioxide removed may be offset by the fuel cost of acquiring, transporting, and releasing significant amounts of iron into remote HNLC regions.\n\nMany environmental concerns exist for large-scale iron fertilization. While blooms can be studied and traced, scientists still do not know if the extra production gets incorporated into the food chain or falls to the deep ocean after a bloom dies off. Even if carbon is exported to depth, raining organic matter can be respired, potentially creating mid-column anoxic zones or causing acidification of deep ocean water. Pronounced community shifts to diatoms have been observed during fertilization, and it’s still unclear if the change in species composition has any long-term environmental effects.\n\n"}
{"id": "40744502", "url": "https://en.wikipedia.org/wiki?curid=40744502", "title": "High contrast grating", "text": "High contrast grating\n\nIn physics, a high contrast grating is a single layer near-wavelength grating physical structure where the grating material has a large contrast in index of refraction with its surroundings. The term near-wavelength refers to the grating period, which has a value between one optical wavelength in the grating material and that in its surrounding materials. \nThe high contrast gratings have many distinct attributes that are not found in conventional gratings. These features include broadband ultra-high reflectivity, broadband ultra-high transmission, and very high quality factor resonance, for optical beam surface-normal or in oblique incidence to the grating surface. The high reflectivity grating can be ultrathin, only <0.15 optical wavelength. The reflection and transmission phase of the optical beam through the high contrast grating can be engineered to cover a full 2π range while maintaining a high reflection or transmission coefficient.\n\nThe concept of high contrast grating took off with a report on a broadband high reflectivity reflector for surface-normal incident light (the ratio between the wavelength bandwidth with a reflectivity larger than 0.99 and the central wavelength is greater than 30%) in 2004 by Constance J. Chang-Hasnain et al., which was demonstrated experimentally in the same year. The key idea is to have the high-refractive-index material all surrounded by low-refractive-index material. They are subsequently applied as a highly reflective mirror in vertical-cavity surface-emitting lasers, as well as monolithic, continuously wavelength tunable vertical-cavity surface-emitting lasers. The properties of high contrast grating are rapidly explored since then. The following lists some relevant examples:\n\nIn 2008, a single layer of high contrast grating was demonstrated as a high quality factor cavity.\nIn 2009, hollow-core waveguides using high contrast grating were proposed, followed by experimentally demonstration in 2012. This experiment is the first demonstration to show a high contrast grating reflecting optical beam propagating in the direction parallel to the gratings, which is a major distinction from photonic crystal or distributed Bragg reflector.\n\nIn 2010, planar, single-layer lenses and focusing reflectors with high focusing power using a high contrast grating with spatially varying grating dimensions were proposed and demonstrated.\nSome literatures quote the high contrast gratings as photonic crystal slabs or photonic crystal membranes.\n\nFully rigorous electromagnetic solutions exist for gratings, which tends to involve heavy mathematical formulism. A simple analytical formulism to explain the various properties of high contrast grating has been developed. A computational program based on this analytical solution has also been developed to solve the electromagnetic properties of high contrast grating, named High Contrast Grating Solver. The following provides a brief overview of the operation principle of high contrast grating.\n\nThe grating bars can be considered as merely a periodic array of waveguides with wave being guided along the grating thickness direction. Upon plane wave incidence, depending on wavelength and grating dimensions, only a few waveguide-array modes are excited. Due to a large index contrast and near-wavelength dimensions, there exists a wide wavelength range where only two waveguide-array modes have real propagation constants in the z direction and, hence, carry energy. The two waveguide-array modes then depart from the grating input plane and propagate downward to the grating exiting plane, and then reflect back up. After propagating through the grating thickness, each propagating mode accumulates a different phase. At the exiting plane, owing to a strong mismatch with the exiting plane wave, the waveguide modes not only reflect back to themselves but also couple into each other. As the modes propagate and return to the input plane, similar mode coupling occurs. Following the modes through one round trip, the reflectivity solution can be attained. The two modes interfere at the input and exiting plane of the high contrast grating, leading to various distinct properties.\n\nHigh contrast gratings have been employed in many optoelectronic devices. It has been incorporated as the mirrors for vertical-cavity surface-emitting lasers. The light-weight of high contrast grating enables fast microelectromechanical structure actuation for wavelength tuning. The reflection phase of the high contrast grating is engineered to control the emission wavelength of vertical-cavity surface-emitting lasers. By locally changing each grating dimension while keeping its thickness the same, planar, single-layer lenses and focusing reflectors with high focusing power have been obtained. Besides its high reflectivity, the high contrast grating has been designed as a high quality factor resonator. Low-loss hollow-core waveguide are made with high contrast gratings with high reflectivity at oblique incident angle. Applications such as slow light and optical switch can be built on the hollow-core waveguide by using the special phase response and resonance property of high contrast grating. High contrast grating can effectively manipulate the light propagation – directing light from surface-normal to in-plane index-guided waveguide and vice versa.\n"}
{"id": "4287778", "url": "https://en.wikipedia.org/wiki?curid=4287778", "title": "Hydrometeorology", "text": "Hydrometeorology\n\nHydrometeorology is a branch of meteorology and hydrology that studies the transfer of water and energy between the land surface and the lower atmosphere. Hydrologists often utilize meteorologists and products produced by meteorologists As an example, a meteorologist would forecast 2-3 inches of rain is a specific area, and a hydrologist would then forecast what the specific impact of that rain would be on the terrain. UNESCO has several programmes and activities in place that deal with the study of natural hazards of hydrometeorological origin and the mitigation of their effects. Among these hazards are the results of natural processes or atmospheric, hydrological, or oceanographic phenomena such as floods, tropical cyclones, drought and desertification. Many countries have established an operational hydrometeorological capability to assist with forecasting, warning, and informing the public of these developing hazards.\n\nOne of the more significant aspects of hydrometeorology has to do with the prediction and attempt to mitigate the effects of high precipitation events. Beginning with the prediction, there are three primary ways to to model when coming to forecasting. These are now-casting, numerical weather prediction, and statistical techniques. Now-casting is good for predicting events for a few hours out, utilizing observations and live radar data to combine with the numerical weather prediction models. Numerical weather prediction, which is the primary technique used to forecast weather, uses models to take account of the atmosphere, ocean models, and accounts for many other variables when it produces forecasts. These forecasts are generally used for predictions days or weeks out. Finally, statistical techniques utilize regressions and other statistical methods to create long-term projections that go out weeks and months at a time. These models allow scientists to view how a multitude of different variables interact with one another, and they illustrate one picture in how earth's climate interacts with itself.\n\nA major component of hydrometeorology is mitigating the risk associated with flooding and other hydrological threats. First, there has to be knowledge of the possible hydrological threats that are expected within a specific region. After analyzing the possible threats, warning systems are put in place to quickly alert people and communicate to them the identity and magnitude of the threat. Many nations have their own specific regional hydrometeorological centers that communicate threats to the public. Finally, there has to be proper response protocols in place to protect the public during a dangerous event.\n\nCountries with a current operational hydrometeorology service include, among others:\n\n"}
{"id": "2472666", "url": "https://en.wikipedia.org/wiki?curid=2472666", "title": "Interplanetary dust cloud", "text": "Interplanetary dust cloud\n\nThe interplanetary dust cloud, or zodiacal cloud, consists of cosmic dust (small particles floating in outer space) that pervades the space between planets within planetary systems such as the Solar System. This system of particles has been studied for many years in order to understand its nature, origin, and relationship to larger bodies.\n\nIn our Solar System, the interplanetary dust particles have a role in scattering sunlight and in emitting thermal radiation, which is the most prominent feature of the night sky's radiation with wavelengths ranging 5–50 μm. The particle sizes of grains characterizing the infrared emission near Earth's orbit typically range 10–100 μm.\n\nThe total mass of the interplanetary dust cloud is approximately the mass of an asteroid of radius 15 km (with density of about 2.5 g/cm). Straddling the zodiac along the ecliptic, this dust cloud is visible as the zodiacal light in a moonless and naturally dark sky and is best seen toward the Sun's direction during astronomical twilight.\n\nThe sources of interplanetary dust particles (IDPs) include at least: asteroid collisions, cometary activity and collisions in the inner Solar System, Kuiper belt collisions, and interstellar medium grains (Backman, D., 1997). Indeed, one of the longest-standing controversies debated in the interplanetary dust community revolves around the relative contributions to the interplanetary dust cloud from asteroid collisions and cometary activity.\n\nThe main physical processes \"affecting\" (destruction or expulsion mechanisms) interplanetary dust particles are: expulsion by radiation pressure, inward Poynting-Robertson (PR) radiation drag, solar wind pressure (with significant electromagnetic effects), sublimation, mutual collisions, and the dynamical effects of planets (Backman, D., 1997).\n\nThe lifetimes of these dust particles are very short compared to the lifetime of the Solar System. If one finds grains around a star that is older than about 10,000,000 years, then the grains must have been from recently released fragments of larger objects, i.e. they cannot be leftover grains from the protoplanetary disk (Backman, private communication). Therefore, the grains would be \"later-generation\" dust. The zodiacal dust in the Solar System is 99.9% later-generation dust and 0.1% intruding interstellar medium dust. All primordial grains from the Solar System's formation were removed long ago.\n\nParticles which are affected primarily by radiation pressure are known as \"beta meteoroids\". They are generally less than 1.4 × 10 g and are pushed outward from the Sun into interstellar space.\n\nThe interplanetary dust cloud has a complex structure (Reach, W., 1997). Apart from a background density, this includes:\n\nIn 1951, Fred Whipple predicted that micrometeorites smaller than 100 micrometers in diameter might be decelerated on impact with the Earth's upper atmosphere without melting. The modern era of laboratory study of these particles began with the stratospheric collection flights of D. E. Brownlee and collaborators in the 1970s using balloons and then U-2 aircraft.\n\nAlthough some of the particles found were similar to the material in present-day meteorite collections, the nanoporous nature and unequilibrated cosmic-average composition of other particles suggested that they began as fine-grained aggregates of nonvolatile building blocks and cometary ice. The interplanetary nature of these particles was later verified by noble gas and solar flare track observations.\n\nIn that context a program for atmospheric collection and curation of these particles was developed at Johnson Space Center in Texas. This stratospheric micrometeorite collection, along with presolar grains from meteorites, are unique sources of extraterrestrial material (not to mention being small astronomical objects in their own right) available for study in laboratories today.\n\n"}
{"id": "1245472", "url": "https://en.wikipedia.org/wiki?curid=1245472", "title": "J Harlen Bretz", "text": "J Harlen Bretz\n\nJ Harlen Bretz (September 2, 1882 – February 3, 1981) was an American geologist, best known for his research that led to the acceptance of the Missoula Floods and for his work on caves. He was born to Oliver Joseph Bretz and Rhoda Maria Howlett, farmers in Saranac, Michigan, the oldest of five children. He earned a degree in biology from Albion College in 1905, where he also met his wife Fanny Chalis. Thereafter, he became interested in the geology of Eastern Washington state.\n\nBretz was of German heritage. Bretz graduated with his AB from Albion College in 1905. He then started his career as a high school biology teacher in Seattle. During this time he began studying the glacial geology of the Puget Sound area. He continued his studies at the University of Chicago where he earned his Ph.D. in geology in 1913. He became an assistant professor of geology, first at the University of Washington and then the University of Chicago.\n\nHe made important discoveries regarding the origin of the Channeled Scablands.\n\nIn the summer of 1922, and for the next seven years, Bretz conducted field research of the Columbia River Plateau. Since 1910 he had been interested in unusual erosion features in the area after seeing a newly published topographic map of the Potholes Cataract. Bretz coined the term Channeled Scablands in 1923 to describe the area near the Grand Coulee, where massive erosion had cut through basalt deposits. The area was a desert, but Bretz's theories required cataclysmic water flows to form the landscape, for which Bretz coined the term Spokane Floods in a 1925 publication.\n\nBretz published a paper in 1923, arguing that the channeled scablands in Eastern Washington were caused by massive flooding in the distant past. This was seen as arguing for a catastrophic explanation of the geology, against the prevailing view of uniformitarianism, and Bretz's views were initially discredited. However, as the nature of the Ice Age was better understood, Bretz's original research was vindicated, and by the 1950s his conclusions were also vindicated.\n\nBretz encountered resistance to his theories from the geology establishment of the day. The geology establishment was resistant to such a sweeping theory for the origin of a broad landscape for a variety of reasons, including lack of familiarity with the remote areas of the interior Pacific Northwest where the research was based, and the lack of status and reputation of Bretz in the eyes of the largely Ivy League-based geology elites. Furthermore, his theory implied the potential possibilities of a Biblical flood, which the scientific community strongly rejected. The Geological Society of Washington, D.C invited the young Bretz to present his previously published research at a 12 January 1927 meeting where several other geologists presented competing theories. Bretz saw this as an ambush, and referred to the group as six \"challenging elders\". Their intention was to defeat him in a public debate, and thereby end the challenge his theories posed to their conservative interpretation of uniformitarianism.\n\nAnother geologist at the meeting, J.T. Pardee, had worked with Bretz and had evidence of an ancient glacial lake that lent credence to Bretz's theories. Pardee, however, lacked the academic freedom of Bretz (he worked for the US Geological Survey) and did not enter the fray.\n\nBretz defended his theories, kicking off an acrimonious 40 year debate over the origin of the Scablands. As he wrote in 1928, \"Ideas without precedent are generally looked upon with disfavour and men are shocked if their conceptions of an orderly world are challenged.\"\n\nBoth Pardee and Bretz continued their research over the next 30 years, collecting and analyzing evidence that eventually identified Lake Missoula as the source of the Spokane Floods and creator of the Channeled Scablands. Research on open channel hydraulics in the 1970s further vindicated Bretz's and Pardee's theories.\n\nBretz wrote an extremely influential paper on the morphology and origin of limestone caves (1942) together with detailed studies of the caves of Missouri (1956) and Illinois (with Stanley Harris, 1961).\n\nThe National Speleological Society made Bretz an honorary member in 1954.\n\nBretz received the Penrose Medal; the Geological Society of America's highest award, in 1979, at the age of 96. After this award, he told his son: \"All my enemies are dead, so I have no one to gloat over.\"\n\nEach year at Albion College, the J Harlen Bretz Award is given to the most outstanding senior in the geology department.\n\nBretz Drive in Homewood, Illinois was named in his honor.\n\nA plaque was dedicated to Bretz in 1994 outside the Visitor Center at Dry Falls State Park in Coulee City, Washington that reads \"Dedicated to J Harlen Bretz who patiently taught us that catastrophic floods may sometimes play a role in nature's unfolding drama\"\n\nBretz and his wife Fanny had two children, Rudolf Challis Bretz and Rhoda Bretz Riley. The Bretz family settled in Homewood, Illinois where they bought property and constructed a Sears Catalog Home on it. Bretz nicknamed the property \"Boulderstrewn\" because of all the rocks and minerals he collected and was given that were placed around the property. He donated a portion of this collection to Albion College in the 1970s . Boulderstrewn was renowned for being an active place where Bretz hosted many parties with students and faculty from the University of Chicago.\n\n\n"}
{"id": "3734214", "url": "https://en.wikipedia.org/wiki?curid=3734214", "title": "KwaZulu-Cape coastal forest mosaic", "text": "KwaZulu-Cape coastal forest mosaic\n\nThe Kwazulu-Cape coastal forest mosaic is a subtropical moist broadleaf forest ecoregion of South Africa. It covers an area of in South Africa's Eastern Cape and KwaZulu-Natal provinces.\n\nThe Kwazulu-Cape coastal forest mosaic occupies the humid coastal strip between the Indian Ocean and the foothills of the Drakensberg mountains. It is part of a strip of moist coastal forests that extend along Africa's Indian Ocean coast from southern Somalia to South Africa. The northern limit of the ecoregion is at Cape St. Lucia in KwaZulu Natal, where the forests transition to the Maputaland coastal forest mosaic. The southern limit is at Cape St. Francis, east of Port Elizabeth in the Eastern Cape Province, where the KwaZulu-Cape forests transition to the Knysna-Amatole montane forests.\n\nThe ecoregion has a seasonally moist subtropical climate. Rainfall ranges from 1500 mm to 900 mm per year. The northern portion is generally receives more rainfall, typically in the summer months, while the southern portion receives most of its rainfall in the winter months, which is typical of the Mediterranean climate region to the west. Rainfall diminishes away from the coast, and the coastal forest mosaic yields to the drier Maputaland-Pondoland bushland and thickets in the Drakensberg foothills, above 300 to 450 meters elevation.\n\nThe ecoregion comprises a mosaic of different plant communities, including coastal belt forest, sand forest, dune forest, short, dry forests known as Alexandria forest, grasslands, palm woodlands, and thorn scrublands. Forests are typically made up of evergreen trees, interspersed with dry-season semi-deciduous and deciduous trees.\n\n"}
{"id": "31279043", "url": "https://en.wikipedia.org/wiki?curid=31279043", "title": "Lee Carlson", "text": "Lee Carlson\n\nLee Carlson (born February 21, 1958) is an American writer best known for his memoir, \"Passage to Nirvana\", about surviving traumatic brain injury. Prior to publishing \"Passage to Nirvana\" he was a magazine and newspaper journalist specializing in writing about outdoor adventure sports such as skiing and scuba diving. He was senior travel editor for \"Skiing\" magazine, and has worked for media outlets such as \"Outside\" magazine, \"Newsday\", NBC Sports, ESPN and many others.\n\nCarlson grew up in Buffalo, New York, where he attended the Nichols School. According to his memoir, he had an upper middle class upbringing, and was an athletic child, participating in sports that included skiing, sailing, lacrosse, and others, which led to his career as an outdoor travel/sportswriter.\n\nHe attended Skidmore College, where he was a student of Clark Blaise, the writing teacher and short-story writer who was Bernard Malamud's main pupil, and also roommates and good friends with Raymond Carver at the Iowa Writers' Workshop at the University of Iowa. Blaise was the director of the International Writing Program there. Carlson was a painting major and heavily involved in theater at Skidmore, but credits Blaise with his becoming a professional writer. His other major influence and mentor is Peter Matthiessen, with whom Carlson has studied for many years as a Zen Buddhism student.\n\nAfter college, Carlson worked briefly as a private yacht captain, and then moved to New York to pursue his writing career. He held a number of writer/editor positions at small trade magazines, including editor-in-chief of a low-temperature physics magazine. Eventually he secured a job covering two of his main interests, skiing and travel, when he became senior travel editor at \"Skiing\" magazine.\n\nIn 2002, several months after working for NBC at the 2002 Winter Olympics in Salt Lake City, Carlson was hit by a car and suffered a traumatic brain injury, leading to a year of full-time rehab and several years of recovery. After the accident, Carlson, being a lifelong sailor, decided to once again pursue one of his loves and work as a private yacht captain. He then, with his fiancée Meg, purchased and rebuilt a 25-year-old, 60-foot sailboat they renamed \"Nirvana\". The accident, aftermath and boat form the metaphorical journey to recovery detailed in his book.\n\nCarlson lives in Greenport, New York, aboard \"Nirvana\". Carlson holds a 100-ton US Coast Guard captain's license\n\n\n"}
{"id": "2441527", "url": "https://en.wikipedia.org/wiki?curid=2441527", "title": "List of California state forests", "text": "List of California state forests\n\nThis is a list of California state forests, operated by the California Department of Forestry and Fire Protection.\n\n\n\n"}
{"id": "42944416", "url": "https://en.wikipedia.org/wiki?curid=42944416", "title": "List of Iris species", "text": "List of Iris species\n\nBearded rhizomatous irises\n\nSection \"Iris\"\n\nBeardless rhizomatous irises\nIt has been generally divided into 2 sections, 'Limniris', which is further divided down to about 16 series and 'Lophiris' (also known as 'Evansias' or crested iris.\n\nSection \"Lophiris\"\n\nSection \"Unguiculares\"'\n\nUnplaced hybrids\n\nSmooth-bulbed bulbous irises. Formerly genus \"Xiphion\".\n\nSection \"Xiphium\"\n\nBulbous irises. Formerly genus \"Junopsis\".\n\nSection \"Nepalensis\"\n\nSmooth-bulbed bulbous irises known as \"junos\". Formerly genus \"Juno\".\nSection \"Scorpiris\"\n\nSee more species listed in 'Scorpiris' subgenus.\n\nReticulate-bulbed bulbous irises. Formerly genus \"Iridodictyum\".\n\nSection \"Hermodactyloides\"\n\n"}
{"id": "2445538", "url": "https://en.wikipedia.org/wiki?curid=2445538", "title": "List of Louisiana state forests", "text": "List of Louisiana state forests\n\nThe following is a list of Louisiana state forests.\n\n"}
{"id": "6069665", "url": "https://en.wikipedia.org/wiki?curid=6069665", "title": "List of SSSIs in Radnor", "text": "List of SSSIs in Radnor\n\nSSSIs in the UK are notified using the concept of an Area of Search (AOS), an area of between and in size. The Areas of Search were conceived and developed between 1975 and 1979 by the Nature Conservancy Council (NCC), based on regions created by the Local Government Act 1972. Whereas England had its Areas of Search based on 46 counties, those in Wales were based on a combination of the counties and smaller districts. In 1974, Wales was divided into 8 counties, with 37 districts. The NCC created 12 Welsh Areas of Search; they mostly follow county borders, but the larger counties (Dyfed, Powys and Gwynedd) were divided into multiple Areas using district borders. Mid and South Glamorgan were merged into a single AOS, whilst Llanelli district was included in the West Glamorgan AOS.\n\nDue to subsequent local government reorganisation in the UK since 1972, many counties and districts have been divided, merged or renamed. Using the AOS system alone would make it difficult to search for individual SSSI citations via the Countryside Council for Wales (CCW) database without knowing 1972 region divisions. As a result, the CCW groups Welsh SSSIs using the subdivisions of Wales formed in April 1996 by the Local Government (Wales) Act 1994, resulting in 22 principal areas.\n\nRadnor AOS lies within the county of Powys.\n\nFor SSSIs elsewhere in the UK, see List of SSSIs by Area of Search.\n\n"}
{"id": "19317338", "url": "https://en.wikipedia.org/wiki?curid=19317338", "title": "List of Ultras of Central Asia", "text": "List of Ultras of Central Asia\n\nThis is a list of the Ultra prominent peaks (with topographic prominence greater than 1,500 metres) in Central Asia. The list is divided topographically rather than politically. There are 75 in total; 21 in the Pamirs, 1 in the Karakum, 5 in the Alays, 24 in the Tian Shan and 24 in the Altai and Mongolia.\n\n"}
{"id": "4802209", "url": "https://en.wikipedia.org/wiki?curid=4802209", "title": "List of botanical gardens in Canada", "text": "List of botanical gardens in Canada\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "25283053", "url": "https://en.wikipedia.org/wiki?curid=25283053", "title": "List of countries by total primary energy consumption and production", "text": "List of countries by total primary energy consumption and production\n\nThis is a list of countries by Total Primary Energy consumption and production.\n\n"}
{"id": "46553263", "url": "https://en.wikipedia.org/wiki?curid=46553263", "title": "List of earthquakes in Nepal", "text": "List of earthquakes in Nepal\n\nLying in one of the most seismically active regions of the world, Nepal has a long history of earthquakes. The first documented earthquake event in the country dates back to 7 June 1255, during the reign of King Abhaya Malla. The quake, measuring 7.8 on the Richter scale, took the life of the king and wiped out a third of Kathmandu's then population. Nepal has witnessed at least one major earthquake per century ever since. \n\nThe following is a list of earthquakes in Nepal. It includes only \"major\" seismic events with their epicentre in the country, and those that occurred outside the country, that resulted in a significant loss of life and property in the country.\n"}
{"id": "10142344", "url": "https://en.wikipedia.org/wiki?curid=10142344", "title": "List of parks and gardens in Karachi", "text": "List of parks and gardens in Karachi\n\nThe following is a list of parks and gardens in Karachi, Sindh, Pakistan. Many of them are maintained by the City District Government of Karachi.\n\n\n\n\n\n\n\n\n"}
{"id": "9076944", "url": "https://en.wikipedia.org/wiki?curid=9076944", "title": "List of rivers of Cyprus", "text": "List of rivers of Cyprus\n\nMost of the 35 streams are small and impermanent. Melting snow supplies water to a number of these until late April. Others are merely winter torrents which go dry during the summer.\n\n"}
{"id": "32606237", "url": "https://en.wikipedia.org/wiki?curid=32606237", "title": "List of sweet potato cultivars", "text": "List of sweet potato cultivars\n\nThis list of sweet potato cultivars provides some information about varieties and cultivars of sweet potato (\"Ipomoea batatas\"). Sweet potato was first domesticated in the Americas more than 5,000 years ago. As of 2013, there are approximately 7,000 sweet potato cultivars. People grow sweet potato in many parts of the world, including New Zealand, Australia, the Philippines, Japan, Hawaii, China, and North America. However, sweet potato is not widely cultivated in Europe.\n\nPeople breed sweet potatoes mainly either for food (their nutritious storage roots) or for their attractive flowering vines. (The variety 'Vardaman' is grown for both.) The first table below lists sweet potato cultivars grown for their edible roots; the second table lists cultivars bred as ornamental vines. In the first table, the Parentage column briefly explains how the sweet potato cultivar was bred. Sweet potato plants with desirable traits are selectively bred to produce new cultivars.\n\nSweet potato cultivars differ in many ways. One way people compare them is by the size, shape, and color of the roots. The more orange the flesh of a sweet potato root is, the more nutritious carotene it has. (Humans metabolize carotene into vitamin A.) The skin of a sweet potato root is a different color than the flesh. The biological word for the outer skin is \"epidermis\"; the flesh is called the \"pith\" or \"medulla\". The first table below has a general description of the color of the root's flesh and skin.\n\nIn the mid-20th century, sweet potato growers in the Southern United States began marketing orange-fleshed sweet potatoes as \"yams\", in an attempt to differentiate them from pale-fleshed sweet potatoes. Even though these growers called their products yams, true yams are significantly different. All sweet potatoes are variations of one species: \"I. batatas\". Yams are any of various tropical species of the genus \"Dioscorea\". A yam tuber is starchier, dryer, and often larger than the storage root of a sweet potato, and the skin is more coarse. This list does not include yams.\n\nMany of the sweet potato cultivars below were bred at agricultural experiment stations. An agricultural experiment station (AES) is a research center where scientists work to increase the quality and quantity of food production. Agricultural experiment stations are usually operated by a government agency and/or a university.\n\n"}
{"id": "29275483", "url": "https://en.wikipedia.org/wiki?curid=29275483", "title": "List of wadis of Oman", "text": "List of wadis of Oman\n\nThis is a list of wadis in Oman arranged by drainage basin.\n\n\n\n"}
{"id": "6748280", "url": "https://en.wikipedia.org/wiki?curid=6748280", "title": "Material", "text": "Material\n\nA material is a chemical substance or mixture of substances that constitute an object. Materials can be pure or impure, a singular composite or a complex mix, living or non-living matter, whether natural or man-made, either concrete or abstract. Materials can be classified based on different properties such as physical and chemical properties (see List of materials properties), geological, biological, choreographical, or philosophical properties. In the physical sense, materials are studied in the field of materials science.\n\nIn industry, materials are inputs to production or manufacturing processes. They may either be raw material, that is, unprocessed, or processed before being used in more advanced production processes, either by distillation or synthesis (synthetic materials).\n\nTypes of materials include:\n\nMaterials are classified according to many different criteria including their physical and chemical characteristics as well as their intended applications whether it is thermal, optical, electrical, magnetic, or combined. As their methods of usage dictate their physical appearance, they can be designed, tailored, and/or prepared in many forms such as powders, thin or thick films, and plates and could be introduced/studied in a single or multi layers. End products could be pure materials or doped ones with most useful compounds are those with controlled added impurities.The dopants could be added chemically or mixed and implanted physically. In case the impurities were added chemically, the dopants/co-dopants on substitutional/interstitial sites should be optimized and investigated thoroughly as well as any stresses instigated by their presence within the structure; whereas in the case of the physical mixing, the influence of the degree of heterogeneity of the prepared hybrid composites ought to be studied.The different physical and chemical preparation techniques can be used solely or combined including solid state synthesis, hydrothermal, sol-gel, precipitations and coprecipitations, spin coating, physical vapor deposition, and spray pyrolysis. Types of impurities along with their amounts are usually dictated by types of matrices to be added to, and their ability to maximize the desired products’ usefulness. Among the most commonly used characterization techniques are X-ray diffraction (XRD) either single crystal or powder, scanning electron microscopy (SEM), energy dispersive X-ray spectroscopy (EDS), X-ray fluorescence (XRF), differential scanning calorimetry (DSC), UV-Vis absorption Spectroscopy, Fourier transform infra-red (FTIR), and Photoluminescence spectrometry. In addition, it is usually considered of extreme importance to find theoretical models that can confirm and/or predict the experimental findings and assist in discussion, assignment, and the explanation of results and outcomes. Also, vision and room for future modification and development should always be pinpointed. Hence, one can classify the material as a smart one if its presence can serve multi purposes within the final product.\n\n"}
{"id": "2860618", "url": "https://en.wikipedia.org/wiki?curid=2860618", "title": "Near vertical incidence skywave", "text": "Near vertical incidence skywave\n\nNear vertical incidence skywave, or NVIS, is a skywave radio-wave propagation path that provides usable signals in the range between groundwave and conventional skywave distances—usually 30–400 miles (50–650 km). It is used for military and paramilitary communications, broadcasting, especially in the tropics, and by radio amateurs for nearby contacts circumventing line-of-sight barriers. The radio waves travel near-vertically upwards into the ionosphere, where they are refracted back down and can be received within a circular region up to 650 km from the transmitter. If the frequency is too high (that is, above the critical frequency of the ionospheric F layer), refraction fails to occur and if it is too low, absorption in the ionospheric D layer may reduce the signal strength.\n\nThere is no fundamental difference between NVIS and conventional skywave propagation; the practical distinction arises solely from different desirable radiation patterns of the antennas (near vertical for NVIS, near horizontal for conventional long-range skywave propagation).\n\nThe most reliable frequencies for NVIS communications are between 1.8 MHz and 8 MHz. Above 8 MHz, the probability of success begins to decrease, dropping to near zero at 30 MHz. Usable frequencies are dictated by local ionospheric conditions, which have a strong systematic dependence on geographical location. Common bands used in amateur radio at mid-latitudes are 3.5 MHz at night and 7 MHz during daylight, with experimental use of 5 MHz (60 meters) frequencies. During winter nights at the bottom of the sunspot cycle, the 1.8 MHz band may be required. Broadcasting uses the tropical broadcast bands between 2.3 and 5.06 MHz, and the international broadcast bands between 3.9 and 6.2 MHz. Military NVIS communications mostly take place on 2–4 MHz at night and on 5–7 MHz during daylight.\n\nOptimum NVIS frequencies tend to be higher towards the tropics and lower towards the arctic regions. They are also higher during high sunspot activity years. The usable frequencies change from day to night, because sunlight causes the lowest layer of the ionosphere, called the D layer, to increase, causing attenuation of low frequencies during the day while the maximum usable frequency (MUF) which is the critical frequency of the F layer rises with greater sunlight. Real time maps of the critical frequency are available. Use of a frequency about 15% below the critical frequency should provide reliable NVIS service. This is sometimes referred to as the optimum working frequency or FOT. \n\nNVIS is most useful in mountainous areas where line-of-sight propagation is ineffective, or when the communication distance is beyond the 50 mile (80 km) range of groundwave (or the terrain is so rugged and barren that groundwave is not effective), and less than the 300–1500 mile (500–2500 km) range of lower-angle sky-wave propagation. Another interesting aspect of NVIS communication is, that direction finding of the sender is more difficult than for ground-wave communication (i.e. VHF or UHF). For broadcasters, NVIS allows coverage of an entire medium-sized country at much lower cost than with VHF (FM), and daytime coverage, similar to mediumwave (AM broadcast) nighttime coverage at lower cost and often with less interference.\n\nAn NVIS antenna configuration is a horizontally polarized (parallel with the surface of the earth) radiating element that is from 1/20th wavelength (λ) to 1/4 wavelength above the ground. Optimum height is about 1/4 wavelength, and high angle radiation declines only slightly for heights up to about 3/8 wavelength. That proximity to the ground forces the majority of the radiation to go straight up. Overall efficiency of the antenna can be increased by placing a ground wire slightly longer than the antenna parallel to and directly underneath the antenna. One source says that a single ground wire can provide antenna gain in the 3–6 dB range. Another source indicates 2 dB for a single wire and nearly 4 dB for multiple ground wires. Ground wires are more necessary when using lower dipoles over poor soils as without them considerable energy goes into heating the ground.\n\nDepending on the specific requirements, various antennas (i.e. Sloper, T2FD, Dipole) can be used for NVIS communication, with horizontal dipoles or inverted V dipoles at about 0.2 wavelengths above ground giving the best results on transmit and at about 0.16 wavelengths on receive, according to military sources and an extensive study by Dutch researchers. \n\nSignificant increases in communication will obviously be realized when both the transmitting station and the receiving station use NVIS configuration for their antennas. In particular for low profile operations NVIS antennas are a good option.\n\nFor broadcasting, typical antennas consist of a dipole about 1/4 wavelength above ground, or arrays of such dipoles. Up to 16 dipoles can be used, allowing strong signals with relatively low power by concentrating the signal in a smaller area. Limiting the coverage may be dictated by licensing, language or political considerations. Arrays of dipoles can be used to \"slew\" the pattern, so that the transmitter need not be in the center of the coverage footprint. Broadcast NVIS antennas usually use an extensive ground screen to increase gain and stabilize the pattern and feed impedance with changing ground moisture.\n\nA military NVIS antenna is the AS-2259 Antenna, which consists of two V-shaped dipoles: The four dipole wires also serve as guy rope for the antenna mast. An alternative configuration consists of a transmitting loop antenna which is configured for maximum signal transmission upwards.\n\n\n\n"}
{"id": "33462592", "url": "https://en.wikipedia.org/wiki?curid=33462592", "title": "Outline of geophysics", "text": "Outline of geophysics\n\nThe following outline is provided as an overview of and topical guide to geophysics:\n\nGeophysics – the physics of the Earth and its environment in space; also the study of the Earth using quantitative physical methods. The term \"geophysics\" sometimes refers only to the geological applications: Earth's shape; its gravitational and magnetic fields; its internal structure and composition; its dynamics and their surface expression in plate tectonics, the generation of magmas, volcanism and rock formation. However, modern geophysics organizations have a broader definition that includes the hydrological cycle including snow and ice; fluid dynamics of the oceans and the atmosphere; electricity and magnetism in the ionosphere and magnetosphere and solar-terrestrial relations; and analogous problems associated with the Moon and other planets.\n\nGeophysics can be described as all of the following:\n\n\n\nHistory of geophysics\n\nGravity of Earth\n\nGeothermal gradient\n\nAtmospheric electricity\n\n\n\nGeophysical fluid dynamics\n\n\n\n\n\n\n\nRock magnetism\n\n\n\n\nMineral physics\n\nSeismology\n\nAtmospheric sciences\n\nGeology\n\n\n\nList of geophysicists\n\n\n\n"}
{"id": "13827801", "url": "https://en.wikipedia.org/wiki?curid=13827801", "title": "Pernambuco interior forests", "text": "Pernambuco interior forests\n\nThe \"Pernambuco\" interior forests (in reality Paraiba and Pernambuco forest/states and not only \"Pernambuco\") is an ecoregion of the Tropical and subtropical moist broadleaf forests Biome, and the South American Atlantic Forest biome. It lies in eastern Brazil between the coastal Pernambuco coastal forests and the dry Caatinga shrublands of Brazil's interior.\n\nThe Pernambuco interior forests cover an area of , extending across portions of Paraíba, Pernambuco, and Alagoas states. They extend from the Curimataú River in the north to the São Francisco River in the south.\n\nThe Pernambuco interior forests lie inland from the Pernambuco coastal forests, extending from sedimentary plateaus near the coast up the eastern slopes of the Borborema Plateau. In the northern portion of the ecoregion, the interior forests lie close to the coast, just behind the coastal Rio Piranhas mangroves and Atlantic coast restingas.\n\nThe climate is tropical. Annual rainfall ranges from 1,250 to 1,750 mm, with a dry season from October to March.\n\nThe predominant forest type is the four-tiered (emergent, canopy, understory, and forest floor) Atlantic semi-deciduous forest, with emergent trees reaching up to . Many trees shed their leaves during the October to March dry season.\n\nCharacteristic emergent and canopy trees include \"Astronium fraxinifolium\" (family Anacardiaceae), \"Enterolobium contortisiliquum\" (Leguminosae), \"Cordia trichotoma\" (Boraginaceae), and \"Tabebuia chrysotricha\" (Bignoniaceae). The Pernambuco interior forests are home to the largest remaining populations of Brazilwood or Pau-Brasil, \"(Caesalphinia echinata)\"\n\nMany species are endemic to the Pernambuco interior and coastal forests. Bird species include the buff-breasted tody-tyrant \"(Hemitriccus mirandae)\", white-winged cotinga \"(Xipholena atropurpurea)\", seven-colored tanager \"(Tangara fastuosa)\", and yellow-faced siskin \"(Carduelis yarrellii)\".\n\nOnly 5% of the original forest remains; the rest has been cleared for timber and fuel wood, agriculture, or cattle ranching. Most of the remaining forest remnants are small (1-10 hectares) and species-impoverished.\n\nAs of 1997, only three protected areas covered portions of the ecoregion, protecting an area of 90 square kilometers of forest. The Pedra Talhada Biological Reserve, in Quebrangulo, Alagoas, is an important refuge for the endangered and threatened birds of the ecoregion.\n\n\n"}
{"id": "57221478", "url": "https://en.wikipedia.org/wiki?curid=57221478", "title": "Primakov Readings", "text": "Primakov Readings\n\nThe Primakov Readings (Russian: Примаковские чтения) is an international summit aimed at promoting dialogue on trends in global politics and economics among high-ranking experts, diplomats and decision-makers. The summit is named in honor of the academician and statesman Yevgeny Primakov. The Readings are intended both to commemorate Primakov and to continue to develop his ideas through international dialogue.\n\nThe summit is organized by the Institute of World Economy and International Relations (IMEMO) and is held in Moscow, Russia. Approximately 50 leaders from think tanks, universities and the diplomatic community from more than 30 countries participate in the Readings each year.\n\nThe conference focuses on scenarios of the development of international relations, challenges in the sphere of international security and new models of interaction between States, corporations and other entities.\n\nThe purpose of the event is to create in Russia a regularly operating international platform aimed at discussion of problems of world economy, politics and international security with the participation of leading representatives of Russian and foreign research and expert-analytical centers.\n\nPrimakov Readings are designed to:\n\n\nThe first Primakov Readings were held On October 29, 2015 at the (Moscow). The event was created to commemorate one of the most outstanding politician and scientist Yevgeny Maximovich Primakov, who had died in June of that year, and was timed to the day of his birth.\n\nHis friends and colleagues met in order to reminisce the joint work. They appreciated the contribution of Yevgeny Primakov to Russian politics, journalism and science.\n\nNamed \"World Order Crisis: expert community response\", the Readings were held in a three-day format from November 28 to November 30 in Moscow, at the , and consisted of two events: the International Studies Think Tanks Forum (November 28–29) and the Primakov Readings Summit (November 30). The Conference was dedicated to the 60-th anniversary of IMEMO. It was organized in cooperation with Think Tank and Civil Society Program (TTCSP) of the University of Pennsylvania and designed to initiate international discussion among leading representatives of global “think tank” community on major economic, social, political and security challenges and threats for the international system and its sustainability.\n\nProgram\n\nThe event was attended by representatives of government, leading Russian and foreign economists, political scientists and diplomats, outstanding public figures. President of the Russian Federation Vladimir Putin opened the Summit.\n\nDuring Primakov Readings, crisis of the modern world order and the current situation in various regions of the world were discussed. Participants considered the ways of development and practical application of the ideological heritage of academician Y. Primakov.\n\nThe first day of the forum was devoted to regional problems of international relations and challenges to international security – in the Middle East and in the Asia-Pacific region, as well as to trends in the development of the world economy and prospects for Russia. Chairman of the Organizing Committee, Aide to the President of the Russian Federation, Y. Ushakov, and director of the Institute of World Economy and International Relations, academician A. Dynkin, opened the conference.\n\nThe second day of the forum was devoted to trends in the development of modern world order and crisis trends in the sphere of international security. During the event there were thematic sessions, presentations and speeches of experts, as well as of diplomats and politicians, including Deputy Foreign Minister of the Russian Federation S. Ryabkov and chairman of the Foreign Affairs Committee of the Federation Council of Russia, K. Kosachev.\n\nDuring the third day of the conference, Russian President V. Putin gave a speech. Putin shared his memories of Yevgeny Maksimovich, and also expressed his hope for the further application and development of the ideas of this outstanding politician and scientist.\n\nValentina Matvienko, Chairman of the Federation Council of the Federal Assembly of the Russian Federation, and Sergey Lavrov, Minister of Foreign Affairs of the Russian Federation, who recalled the diplomatic activity of Primakov, also shared their memories.\n\nAmong international participants of the Forum were: former Italian Foreign Minister, L. Dini; former Minister of Foreign Affairs of the Arab Republic of Egypt and Secretary-General of the Arab League, Amr Musa; former General Secretary of the North Atlantic Treaty Organization, J. Solana; former Minister of Foreign Affairs of Poland, Adam Rotfeld; former British diplomat, Sir John Holmes; German diplomat, Wolfgang Ischinger; French historian of international relations, Dr. Thomas Gomart; Professor of Russian and European politics at the University of Kent, Richard Sakwa; American political scientist, Robert Legvold; vice chairman of Samsung Group, Lee Jae-yong.\n\nNamed “The World in 2035” in 2017 Primakov Readings caused wide resonance not only in Russia, but also abroad. The forum was attended by 503 Russian and 80 foreign guests (291 representatives of scientific organizations), as well as by 231 representatives of the media. Primakov Readings were covered in more than 70 major Russian and 10 foreign media. In addition, during the event more than 20 interviews with key speakers were taken by various agencies.\n\nProgram\n\nThe first day of the event began with the welcoming words of the organizers of the Primakov Readings. The participants were greeted by the Assistant to the President of the Russian Federation Y. Ushakov, who read out a welcoming address from the President of the Russian Federation, V.V. Putin. Chairman of the Board of Directors of the World Trade Center, President of the Russian Federation Chamber of Commerce and Industry and President of IMEMO A. Dynkin have also delivered their welcoming speeches.\n\nDuring the thematic sessions, the panel on Russian-American relations was held under moderation of the Deputy Foreign Minister of the Russian Federation, S. Ryabkov. Among other experts who participated in discussion were: Richard Burt - former chief negotiator of the Strategic Arms Reduction Treaty, K. Kosachev - chairman of the Foreign Affairs Committee of the Federation Council of Russia, S. Rogov - an academician, director of the Institute for US and Canadian Studies of the Russian Academy of Sciences, an expert on the reduction of nuclear weapons and strategic weapons, Feodor Voytolovsky – Director of IMEMO.\n\nSecond session of the Primakov Readings 2017 was focused on international cooperation in the Indo-Pacific region. The main partner in its conduct was the largest social sciences' think tank in India - the Observer Research Foundation. The session was addressed by outstanding experts in the region: Nandan Unnikrishnan, vice president of the Foundation, one of the leading Indian experts on issues of the post-Soviet space and Russian-Indian relations, Shin Un, former ambassador, president of the National Security Strategy Institute (INSS) and others.\n\nThe third session covered the potential of the emerging new economic landscape in Eurasia. The moderator was Shingo Yamagami, a Japanese eminent statesman who currently is the head of the Institute of International Relations. Among the participants was Guan Guhai – specialist in modern Russian-Chinese relations and leading consultant in several Chinese ministries.\n\nThe last event of the first day was the session on international terrorism, moderated by Robert Legvold, a distinguished Sovietologist, one of the leaders of the Euro-Atlantic Security Initiative. The world's top experts took part in the session: V. Naumkin, adviser to the UN Special Envoy for Syria Staffan de Mistura, and Alex Schmid, editor-in-chief of the largest scientific online journal, Perspectives on Terrorism.\n\nSecond day of the International Forum Primakov Readings was opened by a special session with the Minister of Foreign Affairs of the Russian Federation Sergey Lavrov and the former Secretary of State of the United States of America, Henry Kissinger. They presented their vision for the development of Russian-American relations and global politics in general, and expressed their hope for an early resolution of acute issues, the resumption of dialogue and close cooperation.\n\nDuring the thematic panels, experts discussed relationship between Russia and the EU, the Ukrainian crisis, and a new technological revolution. Primakov Readings 2017 were completed by the speech of the Chairman of the Board of the Center for Strategic Research Alexey Kudrin.\n\nThe main topic of Primakov Readings in 2018 was \"Hedging Risks of Unstable World Order\". The forum participants focused on scenarios of transformation of the international relations system, prospects for the development of the new world order and its stabilization, as well as on the most important challenges and risks of regional and global security.\n\nThe Readings were attended by 65 prominent foreign experts from 22 countries and more than 600 representatives of the Russian scientific and expert community, public authorities, political and business circles, as well as by more than 200 media representatives. The Readings raised wide public resonance both in Russia and abroad: 38 interviews were conducted, with coverage of more than 30 million followers, and in the first week after the event, there were 1,668 reports about \"Primakov Readings\" in the mass media.\n\nAt the opening ceremony prominent personalities such as Aide to the President of the Russian Federation Y.V. Ushakov, Chairman of the Federation Council of the Federal Assembly of the Russian Federation V.I. Matvienko, member of the Council of the State Duma of the Russian Federation, Head of the LDPR faction V.V. Zhirinovsky, Chairman of the Board of Directors of the World Trade Center, President of the Chamber of Commerce and Industry of the Russian Federation S.N. Katyrin and President of IMEMO A.A. Dynkin, made appeals to the participants of the conference. President of the Russian Federation V.V. Putin sent a greeting to the organizers and participants of the International Scientific and Expert Forum, expressing appreciation to the \"Primakov Readings\".\n\nThe first session - \"New Bipolarity” – Myth or Reality?” – touched upon the problems of crisis of supranational and international institutions, as well as the appropriateness of the very concept of \"bipolarity\" in contemporary international relations. The expert from Austria said that Russia and China are moving towards establishing a new world order; however, it is too early to talk about the formation of a \"new bipolarity\". The expert from the United States disagreed, stressing that \"competing bipolarity\" is developing in the world, which will lead to a \"new multipolarity\". The representative from the PRC noted that China is in isolation, which Russia helps to overcome to some extent. In his opinion, the role of the new institutions, in the formation of which China is involved, is important, and that is whyChina’s tactics might be somehow offensive. The representative of China did not rule out the possibility of conflicts between Washington and Beijing. From the Chinese point of view, the new bipolarity is unacceptable and it is preferable to strive for the formation of blocks.\n\nAccording to the Indian expert, the state of world politics can be defined as \"multiple multipolarity\". The Russian participant of the session, in his turn, drew attention to the fact that the forming system is not only a polycentric and hierarchical, but also unstable: on the one hand, there is a growing interdependence, on the other – increasing competitiveness. This trend shows the inconsistency of the dual logic of bipolarity and encourages the need to develop a new language of diplomacy.\n\nThe second session, called \"Infrastructure Rivalry in the Indo-Pacific Region: on Land and at Sea\", raised questions related to the fierce competition for world trade routes, ports, and natural resources in the Indo-Pacific region. In addition the presence of a number of nuclear and threshold states in the region was mentioned during the discussion. Аs well as the complexity of the existence of \"small\" states, forced to maneuver between the political and economic interests of bigger players. The Vietnamese side raised the issue of political trust. The representative from India emphasized the need for economic infusions in the region's infrastructural projects, and especially the problem of competition among those projects. The expert from China stressed that Beijing's infrastructural projects are not imperialistic. He also criticized the idea of the Indian and Pacific oceans’ connection.\n\nThe US side highlighted the main obstacles to cooperation in the region: the lack of security framework between the US and China, coupled with a clash of values on which the US and Chinese foreign policy doctrines are based, the prevalence of bilateral infrastructural projects in the region and the difficulty of harmonizing interests in multilateral projects, as well as the problem of territorial issues in the South China Sea.\n\nAccording to the participants, one of the key and most problematic tasks is to find common grounds and to establish an institutional framework for the trade regime in the region under the conditions of a clash of interests of key players. The representative of Japan paid attention to the North Korean issue and noted that if Trump and Kim Jong-un reached an agreement, Russia and Japan could play a crucial role in the fundamental changes in the region. According to the Russian expert, a new format, combining bilateral and multilateral cooperation with an unlimited number of participants, would be the most promising scenario. From the point of view of Russian interests, according to the speaker, Chinese Belt and Road Initiative is the most perspective and realistic. Depending on what kind of the development of interaction formats prevails, the situation will lead either to an aggravation of competition or to the development of cooperation.\n\nThe third session, \"New ‘Great Game’ in the Middle East\", highlighted the following issues: increasing number of actors in the region (both state and non-state), abandonment of the \"bipolar\" logic concerning the division of spheres of influence in the economy, development of terrorism as a form of business, non-proliferation of nuclear weapons, and politicization of religion.\n\nAccording to the expert from Uzbekistan, there are several insurmountable factors hampering the achievement of stability in the Middle East: a crisis of confidence, a paralysis of the institutions of international security, an attempt by different actors to impose their own rules of the game in the conditions of chaos. Import of alien models and values leads to opposition from the local population and its turn towards Islamic fundamentalism. Political will of the United States and Russia is capable of bringing stability to the region and there are several points of convergence, such as common interests in the fight against extremism, as well as counteraction to radicalization of youth. The representative of India shared this view, stating that it is necessary to work out joint approaches to fighting hotbeds of tension in the region in those areas, where interests of major actors do not conflict. The US side also stressed the presence of numerous actors and the complexity of relationships between them, especially mentioning Russia's ability to find common ground between irreconcilable antagonists. The expert predicted gradual withdrawal of the US from the region.\n\nThe representative from Israel noted that the weakening of state and its subjectivity occurred in the region, which has been related to dynamics of the \"Arab Spring\". In a number of cases, authoritarian regimes were reinforced. The weakening of the state created a vacuum, which brought an opportunity for various regional forces to intervene in conflicts.\n\nRussian participant in the session questioned the existence of the «Great Game» in the Middle East. In his opinion, there is no opposition between two \"superpowers\", but there are three levels of conflict: local, regional and global. A distinguished feature of Russia's foreign policy – an attempt to build mutually beneficial relations with different parties, while Moscow does not try to oust anybody from the region. However, the US has recently become the main producer of hydrocarbons and pursuing new interests.\n\nDuring the discussion of the fourth session - \"Exit Strategy for the Ukraine Crisis\", most participants agreed with the non-alternative nature of the Minsk agreements, as well as with Moscow's declared need to overcome alienation that is artificially fueled in relations between Russia and Ukraine. Representatives of the US and EU at the session noted the need to consider the Ukrainian crisis in context of changes in the post-Soviet space and the determination to fit it into the architecture of European security. Both Russian and Western experts agreed on the need for a peacekeeping operation, primarily with a view to ending violence in the region.\n\nThe second day of Primakov Readings was opened by speech of the Minister of Foreign Affairs of the Russian Federation, S.V. Lavrov, who touched upon the aggravation of international contradictions and the narrowing of space for constructive cooperation between countries, and consequently the risks of uncontrolled escalation.\n\nThe fifth session - \" Is there a Future for Arms Control? \" - expanded traditional understanding of the subject, highlighting also the challenges of cyberspace and the emergence of hypersonic and smart weapons, touching on the philosophical question of whether there could be a regime of arms control in a multipolar world in general.\n\nThe American expert confirmed that Russian issue is being used today in the US domestic politics. Representatives of the Russian side noted, that the nuclear arms race in the 21st century will no longer be a bilateral but multilateral. They also pointed out that a new arms control system cannot be built if the previously created mechanisms are destroyed. The representative from the European Union said that the control system no longer corresponds to the new realities, since it is a product of the bipolar world, and the question is how to organize a multilateral arms control negotiation process in a multipolar uncontrolled system. The expert stressed the need to counteract the US in its desire to extend legal norms of its national legislation to other countries. Indian expert called for a focus on reducing the risks of using nuclear weapons.\n\nThe sixth session - \" Russia and the EU – Interdependence or Confrontation \" – revealed the unfavorable aspects of the deteriorating relations of partner countries, noted by all experts. The representative from France said that the international system will depend, first of all, on relations between the US and China. He noted the possibility of increasing confrontation in cyberspace, in the economy (related to the Chinese Belt and Road Initiative), if Russia continues its course toward strategic loneliness. The Italian expert pointed out the interdependence in relations between Russia and the EU, describing it as \"hybrid cooperation\". The participant noted the special complexity of the Ukrainian issue, as well as of the Serbian and Balkan problems. Representatives of the Russian point of view made in their speech an emphasis on the fall of investment activity between the Russian Federation and the EU. Experts noted an unprecedented crisis of confidence and, in connection with this, Russia's pivot to the East. Nevertheless, they also expressed hope for a more pragmatic approach of European business circles, which will allow separating economic benefits from political processes. The US representative at the session stated that Russia consciously chooses the path of the \"rogue state\": the country does not want to play by international rules and refuses to take responsibility for certain events.\n\nThe seventh session - \" Society changing Tech: Its Expanses and Limits\" - conducted by A.A. Fursenko, the Aide to the President of the Russian Federation, highlighted the role of the state and the market in modern developed societies. Panel discussion touched upon the problems of: resource catastrophe, communication gap, growing interdependence of countries in the field of modern technologies, and state guarantees to cover risks of developing field-specific technologies.\n\nThe specialists discussed possibilities of \"blockchain\" as a possible solution for a number of technological challenges. They agreed on the need to educate specialists aiming at invention and creative thinking, within the framework of solving special tasks for applied purposes. The Minister of Science and Higher Education of the Russian Federation M.M. Kotyukov, stressed that scientific community, business circles and system of education – all participate in the technological development of the country. The Academy of Sciences once took upon itself the role of a major acquirer of R&D. Today the role of the state in the sphere is paramount. The speaker pointed up that in international cooperation, the emphasis should be made both on the development of infrastructure and on the development of those competencies that are necessary for Russia to enter the market of leading projects.\n\nThe American expert noted that under the conditions of \"new normality\" technologies can help to overcome a number of resource constraints. At the same time, technologies are developing at such a speed that governments often do not understand the whole picture, so the task of the academic community is to develop a set of rules and regulations in the field of technology.\n\nAt the Special Session - \"The Future of the Russian Economy\", the Chairman of the Accounts Chamber of the Russian Federation, A.L. Kudrin gave a speech. He touched upon the problems of implementing presidential decrees, forecasted the trends of development of the Russian economy, the demographic situation in the country, the investment market, the development of education and health care system, digitalization and weakening of the regulatory burden on business, as well as public administration reform.\n\nIn the 2017 Global Go To Think Tank Index Report published by the University of Pennsylvania, Primakov Readings were given the 7th place in the list of 10 best conferences in the world.\n\n\nIn 2016, the event was supported by the Russian Science Foundation in partnership with The Think Tanks and Civil Societies Program of the University of Pennsylvania.\n\nIn 2017 IMEMO organized the Primakov Readings with the assistance of the Russian Science Foundation, the Chamber of Commerce and Industry of the Russian Federation, the and the Center for Foreign Policy Cooperation named after E.M. Primakov. International partners of IMEMO were the Observer Research Foundation (India) and The Woodrow Wilson International Center for Scholars (USA).\n\n\n\n"}
{"id": "28498504", "url": "https://en.wikipedia.org/wiki?curid=28498504", "title": "Project Space Track", "text": "Project Space Track\n\nProject Space Track was a research and development project of the US Air Force, to create a system for tracking all artificial earth satellites and space probes, domestic and foreign.\n\nProject Space Track was started at the Air Force Cambridge Research Center at Laurence G. Hanscom Field, now Hanscom Air Force Base, in Bedford, Massachusetts shortly after the launch of Sputnik I. Observations were obtained from some 150 sensors worldwide by 1960 and regular orbital predictions were issued to the sensors and interested parties.\n\nSpace Track was the only organization that used observations from all types of sources: radar, optical, radio, and visual. All unclassified observations were shared with the Smithsonian Astrophysical Observatory. In 1961, the system was declared operational and assigned to the new 1st Aerospace Surveillance and Control Squadron as part of NORAD's Space Detection and Tracking System (SPADATS).\n\nOn 29 November 1957, shortly after the launch of Sputnik I on 4 October, two German expatriates, Dr. G. R. Miczaika (from Prussia) and Dr. Eberhart W. Wahl (from Berlin) formed Project Space Track (originally called Project Harvest Moon). It was established in Building 1535 of the Geophysics Research Directorate (GRD), Air Force Cambridge Research Center, Laurence G. Hanscom Field, Massachusetts. Both scientists had backgrounds in astronomy, although Dr. Wahl’s Ph.D. was in meteorology.\n\nThe mission of Space Track was to track and compute orbits for all artificial earth satellites, including both US and Soviet payloads, booster rockets, and debris. With the Soviet launch of Luna 1 on 2 January 1959, Space Track also started tracking space probes. The first major tracking effort was Sputnik II, containing the dog Laika, launched 3 November 1957.\n\nAn Electronic Support System Program Office, 496L, had been established in February 1959, with the program office at Waltham, Massachusetts under the direction of Col Victor A. Cherbak, Jr. By late 1959, the SPO had received additional responsibilities under the DoD Advanced Research Projects Agency (ARPA) to develop techniques and equipment for military surveillance of satellites . Continuing development of Space Track was an integral part of this effort.\n\nSince December 1958, Space Track had been the interim National Space Surveillance Control Center. In December 1959, Space Track was moved to a new building, the National Space Surveillance Control Center (NSSCC), which was formally dedicated on 9 February 1960. The NSSCC was part of the Air Force Command and Control Development Division (known informally as C²D²), Air Research and Development Command. Dr. Harold O. Curtis of Lincoln Laboratory was the Director of the NSSCC. The name Space Track continued in use.\n\nBy 1960, there were about 70 people in the NSSCC involved in operations.\n\nSpace Track continued tracking satellites and space probes until 1961. In late 1960, USAF Vice Chief of Staff General Curtis E. LeMay decided that the research and development system was ready to become operational.\n\nEleven officers and one Senior Master Sergeant were selected to be the initial cadre of what became the 1st Aerospace Surveillance and Control Squadron. The initial cadre came to Space Track for training that started 7 November 1960. (The cadre was assigned to the new squadron on 6 March 1961.)\n\nOn 1 July 1961 the new squadron became operational under the USAF Air Defense Command at Ent AFB, Colorado Springs, part of NORAD's Space Detection and Tracking System (SPADATS). The first Squadron Commander was Colonel Robert Miller. The Space Track organization at Hanscom Field assumed a backup role for squadron operations.\n\nIn cavalier disregard of the Air Force Regulation on the subject, which specified clearly that unclassified nicknames, such as Space Track, should be two words (while codewords, such as CORONA, which were then themselves classified, should be only one word), ADC immediately decided to rename Space Track as SPACETRACK and the name has stuck since – although the web site of the 614th Air & Space Operations Center, which currently performs the mission, has returned to two words. The 614th is part of the Joint Space Operations Center at Vandenberg AFB, California.\n\nThe Department of Defense had decided that the US Air Force should develop a command and control system for tracking satellites and that the US Army and US Navy should develop sensors for the purpose. US Navy development was at Dahlgren, Virginia and the US Army's program was at the Aberdeen Proving Ground, Maryland.\n\nDrs. Miczaika and Wahl had assembled a list of facilities that could track satellites, either by monitoring telemetry or by using radar. The latter were mostly astronomical radio telescopes equipped with radars used in studying the moon (e.g., Jodrell Bank Observatory in England directed by Sir Bernard Lovell, Millstone Hill of Lincoln Laboratory in Massachusetts directed by Dr. Gordon Pettingill, and a radar at the Stanford Research Institute in California, directed by Walter Jaye). Two USAF radars, one on Shemya Island in the Aleutians and the other at Diyarbakır, Turkey, had been built to observe Soviet missile launches and became valuable for satellite tracking as well. BMEWS prototype radars on Trinidad also participated. Normally, the first radar reports of a new satellite launch from Tyuratam (Baikonur) came from Shemya and the first of a new launch from Kapustin Yar came from Diyarbakır. A USAF radar at the Laredo Test Site in Texas and one at Moorestown, New Jersey also participated later. Observations were received from the Royal Canadian Air Force research radar at Prince Albert, Saskatchewan, Canada. The Goldstone facility of the Jet Propulsion Laboratory was exceptionally helpful with radio observations of Soviet space probes.\n\nIn general, observations were in the form of time, azimuth and elevation (and range, from radars) as measured at the site or, in some cases, such as at Goldstone, in astronomical form (Right Ascension and Declination) Some early observations were very primitive, such as a report that a satellite passed near a star that could be identified.\n\nOn rare occasions, the observations were purely verbal. For example, individuals on ships, planes, and islands in the Caribbean reported sightings of the decay of satellite 1957 β, although one aircraft was able to provide a detailed observation because the navigator happened to be completing a celestial fix at the exact time \n\nSome sites could record the Doppler shift of satellite transmission or, in a few cases, the Doppler shift from their own transmissions reflected from the orbiting object. One doppler site was the Space Track Doppler Field Site at Billerica, Massachusetts. The observations obtained by this technique were the time of closest approach to the station.\n\nThe Navy program was operated as\nNAVSPASUR and is now operated by the US Air Force. The Army program, although achieving accurate tracking results with doppler techniques and furnishing observations to Space Track, did not achieve funding for deployment.\n\nOne of SPASUR’s contributions to satellite tracking was the invention of a map of the earth that showed both poles, so that the position of all satellites, including those in polar orbits, could be shown. This was not possible with Mercator or other projections, which do not show the entire earth. The map was, of course, very distorted at the poles (the North pole was the entire top line of the long map) but the concept proved very useful.\n\nOptical sensors included the twelve Baker-Nunn satellite tracking cameras operated for NASA by the Smithsonian Astrophysical Observatory (SAO), three Baker-Nunn cameras operated by the USAF, and the Boston University camera at Patrick Air Force Base operated by Walter Manning.\n\nSAO cameras were at Woomera, Australia; Jupiter, Florida; Organ Pass, New Mexico; Olifantsfontein, Union of South Africa; Cadiz, Spain; Mitaka, Japan; Nani Tal, India; Arequipa, Peru; Shiraz, Iran; Curaҫao, Netherlands West Indies; Villa Dolores, Argentina; and Haleakala, Maui, Hawaii. USAF cameras were at Oslo, Norway; Edwards AFB, California, and Santiago, Chile. Two additional cameras were later added to the USAF inventory – one of the USAF cameras was transferred to the Royal Canadian Air Force at Cold Lake, Alberta, Canada in 1961.\n\nVolunteer amateur astronomers as part of the SAO Moonwatch Team also contributed observations. Among these many volunteers was Arthur S. Leonard of Davis, California, leader of the Sacramento, California team.\n\nBy 1960, Space Track had about 150 cooperating sensors. Space Track was the only US organization that used all methods of observation to track satellites.\n\nThe observations were recorded on IBM punched cards for computer processing. All unclassified observations were exchanged daily with the Smithsonian Astrophysical Observatory, Cambridge, Massachusetts.\n\nSpace track maintained close contact with the US National Security Agency, the CIA Foreign Missile and Space Analysis Center (FMSAC), and Headquarters USAF Intelligence, Major Harry Holeman.\n\nIt was helpful that the USSR press service, TASS, always announced new Soviet satellite or space probe launches promptly, so Space Track was free to discuss the new objects without worrying about compromising sources. Translations of the Russian announcements were provided by the Foreign Broadcast Information Service (FBIS).\n\nDr. Wahl had been computing all the satellite ephemerides by hand using a Friden Square Root Calculator, the most advanced mechanical calculator then available.\n\nThe method for computing ephemerides (documented in detail in a 1960 report by P.M. Fitzpatrick and G.B. Findley) was originally developed by Dr. Wahl, based on historic astronomical methods.\n\nIn late August 1958, Space Track obtained its first computer, an IBM 610, used in conjunction with the Cambridge Research Center IBM 650. The IBM 610 was a very primitive machine, the programing of which was done with a plug board (similar to the ones used for IBM accounting machines in the early 1950s) and a punched paper tape.\n\nThe new NSSCC building was equipped with an IBM 709 and, a few months later, with an IBM 7090. Major programming of the new computers was done by the Aeronutronic Division of the Ford Motor Company, Newport Beach CA. The Wolf Corporation also supported the NSSCC.\n\nThe ephemeris computations were issued in what was called a bulletin. The bulletin listed each equatorial crossing of the satellite and described the path between crossings. Space track also furnished “look angles,” altitude and azimuth directions so that specific sensors could point in the correct direction to acquire the satellite. Special versions of the look angles were tailored for specific sites, such as the Army and Navy sensor development projects. At the NSSCC, these computations were transmitted by the Duty Controller.\n\nSpace Track also issued public catalogues listing all the satellites, including ones no longer in orbit, called Satellite Situation Reports, which gave basic orbital elements for each piece. At first, this took less than a page of type. The Smithsonian Astrophysical Observatory also issued a similar document but, in 1961, NASA’s Goddard Space Flight Center assumed responsibility for both reports, combining them into one document..\n\nIn October 1960 George Westrum presented a short college-level course in Celestial Mechanics for those NSSCC personnel who wished to participate\n\nBy international agreement under the International Astronomical Union, the satellites and space probes were initially named with Greek letters, following the system for naming stars in constellations. The year of launch was included in the launch names, so Sputnik I was 1957 Alpha. The payload was called Alpha I, when known – in the case of Sputnik I, it wasn’t clear initially which was the payload, so the payload became Alpha II. Other pieces were also numbered, so the carrier rocket was usually Alpha II. The 24 Greek letters were soon used, so the next sequence started Alpha Alpha and so forth. By 1962 Beta Psi had been launched and it was clear that the Greek alphabet system would no longer work. Thereafter, launches were numbered, starting with 1963-1 with the payload normally being 1963-1A, etc..\n\nAs soon as a new satellite or space probe was launched, Space Track alerted the primary sensors and processed observations as they came in, issuing a preliminary tracking bulletin promptly and updating it after about 24 hours, when additional observations from around the world had been obtained. Routine bulletins continued to be issued regularly as needed to keep up with the changing orbits, some of which decayed fairly rapidly in the atmosphere. There was another flurry of activity when the last revolutions occurred, as it was difficult to forecast the exact reentry path.\n\nThe NSSCC had a room dedicated as a filter center for monitoring communications and obtaining observations. The filter center had displays listing the orbiting and decayed satellites and a projector system that could show the motion of one satellite over the earth. The displays were devised by A/3C Peter P. Kamrowski. The center was manned by a Duty Controller and his assistants. The center was designed by Senior Controller 1st Lt Cotter, based on his earlier experience as a volunteer member of the USAF Ground Observer Corps (the Ground Observer Corps filter centers were in turn based on United Kingdom aircraft tracking centers developed during World War II to track Nazi aircraft).\n\nBy 1960, the position of Duty Analyst was established. Once observations had been reduced, the duty analyst reviewed them and decided which orbits needed to be recomputed to bring them up to date. In the case of new launches or decaying satellites, one analyst was dedicated to processing observations for that satellite.\n\nAs with many other activities in the dawning space age, Space Track operations often involved doing things for which no precedent existed.\n\nOn 2 January 1959, the Soviets launched Luna I (aka Mechta (Dream)), their first lunar probe. Tracking data was obtained for Space Track by the Goldstone site of the California Institute of Technology, which verified that the probe had headed for the moon. Dr. Curtis used a plot of this data in a presentation to a committee of the US House of Representatives. His presentation helped influence President Kennedy to establish the Apollo Program. Kenneth E. Kissell later published a Project Space Track analysis of the trajectory.\n\nAt this period, the 6594th Aerospace Test Wing was valiantly trying to achieve a successful launch in the Discoverer satellite program. The satellites, launched from Vandenberg AFB, were all in polar orbits. They were controlled by the 6594th at Palo Alto (later the Air Force Satellite Control Facility at Sunnyvale CA). Lt Cotter was the liaison officer between Space Track and the 6594th. The first 12 launch attempts were failures; the first success was Discoverer I (1959 Beta). Lockheed Corporation, the development contractor, won their bonus payment because the telemetry showed the satellite achieved orbit, but it was never seen again, despite massive Space Track and other efforts to find it.\n\nBy this time Space Track had contacts with many sensors around the world. One of them was at the South Pole, associated with the International Geophysical Year. One of their ninety observations of Discoverer II (1959 Gamma) was sent from Byrd Station saying that the satellite had passed to the left of the zenith at 2.25 degrees, implying an orbital inclination of 89.9 degrees. This report is probably the only direct observation of the inclination of a satellite’s orbit that has ever been made.\n\nBecause the Discoverer satellites carried payloads that were deorbited and recovered from parachutes by aircraft of the 6594th Aerospace Test Wing based in Hawaii, the timing of deorbit was critical. (The deorbit attempt of Discoverer II's payload went seriously wrong: the payload landed on Spitsbergen, instead of coming down over the Pacific ocean. It was recovered by Russian miners, likely very helpful to Russian intelligence and the Russian space program in general ). Later, to improve the accuracy of the deorbit commands, orbital analysts Lt Algimantas Šimoliūnas, Lawrence Cuthbert, or Ed Casey would update the Space Track ephemeris for each Discoverer at the last minute and send the update to the 6594th. The 6594th had a global network of tracking stations (including Alaska, Hawaii, Seychelles, Guam, and the UK), used for command and on-orbit control of the satellites. However, the tracking data was derived from telemetry monitoring and was not as precise as the Space Track data, which was based in major part on radar and optical tracking.\n\nLockheed decided to put a small light on Discoverer XI (1960 Delta). Space Track acted as liaison between the 6594th and the Smithsonian Astrophysical Observatory, to use their Baker-Nunn camera at Cadiz, Spain, to photograph the light. This would give Lockheed valuable information about the accuracy of their orbit computations. The experiment worked very well and was not repeated.\n\nDiscoverer XIX (1960 Tau) had a payload called MIDAS, the developmental version of what later became the Defense Support Program. The Air Force decided that the MIDAS orbit should be classified, which meant that Space Track sensor observations had to be classified also. This led to a surreptitious midnight data transfer in central Concord, Massachusetts between Dr. Gordon Pettingill of Millstone Hill and Lt Cotter, as there was no secure teletypewriter or telephone available.\n\nPerhaps causing inadvertent fireworks in celebration of the activation of the 1st Aerospace Surveillance and Control Squadron, the Ablestar stage for the Navy's Transit 4A satellite, 1961 Omicron, which was launched on 29 June 1961, exploded about 77 minutes after attaining orbit, at 0608Z. The NORAD Ballistic Missile Early Warning System (BMEWS) made early radar observations and Mr. Leonard of the Sacramento, California Moonwatch team alerted Space Track when he saw many fragments where only a few satellites were expected from the launch. In the next few days, this gave Project Space Track its first major effort as a backup for the new squadron. Lawrence W. Cuthbert, 1st Lt Algimantas Šimoliūnas, and Ed Casey achieved a landmark in satellite tracking, plotting observations by hand and identifying orbits for 296 of the fragments. Orbital Analysts at 1st Aero were also heavily involved in the achievement. Observations from the SPASUR fence were very helpful in tracking the fragments (SPASUR had initially refused to send Space Track individual observations, sending instead only orbital parameters, but this policy had fortunately been changed by 1961).\n\nThe technique used to identify multiple objects orbiting in the same orbital plane was refined by Lawrence Cuthbert and published as an automated program by the Wolf Corporation [Later, Larry worked with Bob Morris, Chief Orbital Analyst at Colorado Springs, to develop a program to derive orbital elements for all unknown radar tracks; the methodology worked and it became known as the Cuthbert-Morris Algorithm. The resulting program was called \"Breakup, Lost and Decay\" and, along with subsequent improvements, it has found thousands of the objects in the Space Satellite Catalog. It is still the Air Force Astrodynamic Standard for Uncorrelated Target (UCT) processing.\n\nMost Space Track communication was by teletypewriter or, in some cases, by telephone, mail, or messenger.\n\nThe bulletins and look angles were initially typed by hand by airmen in the communications office and sent by teletypewriter to all the participating sensors. The teletypewriter machines used punched paper tape, before the invention of chadless tape.\n\nEventually, Roy Norris and Lt Cotter inveigled the IBM 610 into cutting paper tapes for the satellite bulletins, so that the airmen in the communication department would not have to type all the data by hand. This was not part of the IBM 610 design and was a surprise to IBM personnel. Later computers would also prepare the bulletin and look angle data tapes automatically.\n\nThere was some limited secure communication: One method valid for sending classified information was a pair of one-time pads. These pads were each made of twin sets of pages, the top one of which had all letters and numbers on a line, perhaps 40 lines to a page. The top sheet was carbonless paper. To use the sheets, one circled each letter or number row-by-row on the top sheet. This marked the second sheet, which had all the letters and numbers scrambled. The scrambled version could then be transmitted by teletypewriter or telephone to the recipient who, using his matching set of one-time pads, could reverse the process and read the secure message.\n\nAnother method Space Track later had was a secure teletypewriter machine that had a pre-punched paper tape attached. The tape served to garble each letter typed, which could then be decrypted by a reverse procedure at the other end of the teletypewriter line. This system was used to communicate with Air Force Intelligence at the Pentagon. More sophisticated cryptographic equipment was available later.\n\nIn addition to data communications, Space Track published a series of technical reports. (e.g. see References,\n\nDr. Wahl presented detailed descriptions of Space Track activity at the first two\nInternational Symposia on Rockets and Astronautics in Tokyo, 1959 and 1960. Dr. Curtis and Lt Cotter made a similar presentation in 1960.\n\nIn 1960, Aeronutronic, a division, of the Ford Motor Company, had a contract with Space Track to develop improved methods of predicting the orbits of decaying satellites, a computer program called Spiral Decay, and for other software for new computers in the new building. (Aeronutronic had been hired to do a system analysis of the control center on 1 October 1959. Detailed reports of this and other Aeronutronic support of Project Space Track are on file at the offices of Lockheed Martin (formerly Loral Corporation) in Colorado Springs, Colorado. An index of the reports is at the National Museum of the Air Force.)\n\nAnother very important group was the employees of Wolf R&D Corporation (Concord, Massachusetts), which did programming and had the contract for operating computers at the NSSCC, including the IBM 7090 mainframe.\n\n\nExcept as noted, all documents referenced are in the archives of the National Museum of the United States Air Force, Wright-Patterson AFB, Ohio. For JPEG copies of the references, see the Talk Page.\n\nCuthbert, Lawrence W.: Ballbuster in Orbit. The Official History of Spacetrack. [Humor] Project Space Track: Bedford MA: June 1965.\n"}
{"id": "11077605", "url": "https://en.wikipedia.org/wiki?curid=11077605", "title": "Submillimeter Wave Astronomy Satellite", "text": "Submillimeter Wave Astronomy Satellite\n\nThe Submillimeter Wave Astronomy Satellite (SWAS) is a NASA submillimeter astronomy satellite, and is the third spacecraft in the Small Explorer program. It was launched on December 6, 1998 (UTC), from Vandenberg Air Force Base aboard a Pegasus XL rocket. The telescope was designed by the Smithsonian Astrophysical Observatory and integrated by Ball Aerospace, while the spacecraft was built by NASA's Goddard Space Flight Center. The mission's principal investigator is Gary J. Melnick.\n\nSWAS was designed to study the composition and structure of interstellar clouds and investigate the processes of stellar and planetary formation. Its sole instrument is a telescope operating in the submillimeter wavelengths of far infrared and microwave radiation. The telescope is composed of three main components: a elliptical off-axis Cassegrain reflector, two Schottky diode receivers, and an acousto-optical spectrometer. The system is sensitive to frequencies between 487–557 GHz (538–616 μm), which allows it to focus on the spectral lines of molecular oxygen (O) at 487.249 GHz; neutral carbon () at 492.161 GHz; isotopic water (HO) at 548.676 GHz; isotopic carbon monoxide (CO) at 550.927 GHz; and water (HO) at 556.936 GHz.\n\nThe Submillimeter Wave Astronomy Satellite mission was approved on April 1, 1989. The project began with the Mission Definition Phase, officially starting on September 29, 1989, and running through January 31, 1992. During this time, the mission underwent a conceptual design review on June 8, 1990, and a demonstration of the Schottky receivers and acousto-optical spectrometer concept was performed on November 8, 1991.\n\nThe mission's Development Phase ran from February 1992, through May 1996. The Submillimeter Wave Telescope underwent a preliminary design review on May 13, 1992, and a critical design review on February 23, 1993. Ball Aerospace was responsible for the construction of and integration of components into the telescope. The University of Cologne delivered the acousto-optical spectrometer to Ball for integration into the telescope on December 2, 1993, while Millitech Corporation delivered the Schottky receivers to Ball on June 20, 1994. Ball delivered the finished telescope to Goddard Space Flight Center on December 20, 1994. Goddard, which was responsible for construction of the spacecraft bus, conducted integration of spacecraft and instrument from January through March 1995. Spacecraft qualification and testing took place between April 1, 1995, and December 15, 1995. After this, SWAS was placed into storage until September 1, 1998, when launch preparation was begun.\n\nThe spacecraft was delivered to Orbital Sciences Corporation at Vandenberg Air Force Base on November 2, 1998, for integration onto their Pegasus XL rocket. Launch occurred on December 6, 1998, at 00:57 UTC, from Orbital Sciences' \"Stargazer\" L-1011 TriStar mothership. Its initial orbit was a near-circular with an inclination of 69.9 degrees.\n\nSWAS was originally scheduled to launch in June 1995, but was delayed due to back-to-back launch failures of the Pegasus XL rocket in June 1994 and June 1995. A launch opportunity in January 1997 was again canceled due to a Pegasus XL launch failure in November 1996.\n\nThe commissioning phase of the mission lasted until December 19, 1998, when the telescope began producing useful science data. The SWAS mission had a planned duration of two years and a cost estimate of , but mission extensions allowed for five and a half years of continuous science operations. During this time, data was taken on more than 200 astronomical objects. The decision was made to end science and spacecraft operations on July 21, 2004, at which time the spacecraft was placed into hibernation.\n\nTo support the \"Deep Impact\" mission at comet 9P/Tempel, SWAS was brought out of hibernation on June 1, 2005. Vehicle check-out was completed on June 5 with no discernible degradation of equipment found. SWAS observations of the comet focused on isotopic water output both before and after the \"Deep Impact\" impactor struck the comet's nucleus on July 4. While water output was found to naturally vary by more than a factor of three during the observation campaign, SWAS data showed that there was no excessive release of water due to the impact event. After three months of observation, SWAS was once again placed into hibernation on September 1, 2005.\n\n, SWAS remains in Earth orbit on stand-by.\n\n\n"}
{"id": "24173924", "url": "https://en.wikipedia.org/wiki?curid=24173924", "title": "The Gardener's Labyrinth", "text": "The Gardener's Labyrinth\n\nThe Gardener's Labyrinth or The Gardeners Labyrinth was an early popular book about gardening. It was written by Thomas Hill, using the pseudonym \"Didymus Mountain\", with Henry Dethick and published in 1577.\n"}
{"id": "51410292", "url": "https://en.wikipedia.org/wiki?curid=51410292", "title": "Tobífera Formation", "text": "Tobífera Formation\n\nTobífera Formation () is a volcano-sedimentary formation of Late Jurassic age. The formation is located in Magallanes Region in southern Patagonia and Tierra del Fuego. The bulk of the formation originates from silicic pyroclastic material during a period of bimodal volcanism in Rocas Verdes Basin, a rift basin. The Tobífera Formation is grouped together with other formations of similar age in Patagonia in the Chon Aike Province an extraordinarily large province of silicic volcanism.\n\nExcept for some western and southern exposures most of the formation is buried and known only from boreholes in Magallanes Basin. The formation is equivalent to El Quemado and Ibañez Formations. Tobífera Formation has an up to thick Basal Clastic Complex, a sub-unit made up of conglomerate and sandstone. Tobífera Formation unconformably overlies metamorphic and igneous basement complexes of Cambrian age.\n\nMuch of the formation is folded and faulted as consequence of the Andean orogeny. At Última Esperanza Province the formation metamorphosed first under greenschist facies and then under prehnite-pumpellyite facies conditions. Some rhyolites of Tobífera Formation were incorporated into Cordillera Darwin Metamorphic Complex. The incorporation of part of Tobífera Formation in the metamorphic complex was accompanied by deformation and metamorphism and occurred in the context of the Andean orogeny in the Cretaceous.\n"}
{"id": "50611173", "url": "https://en.wikipedia.org/wiki?curid=50611173", "title": "Tom Dinwoodie", "text": "Tom Dinwoodie\n\nThomas Linn Dinwoodie (born November 15, 1954) is a cleantech entrepreneur, inventor, and founder of SunPower Corporation Systems (formerly PowerLight Corporation). He holds a long-standing interest in accelerating the transition to clean energy and other climate-sustaining practices. Dinwoodie is also an architect.\n\nFrom 1978 to 1983, Dinwoodie was a research assistant at the MIT Energy Laboratory, where he authored numerous papers on the economics and policy of distributed solar and wind generation, as well as flywheel energy storage.\n\nIn 1981, Dinwoodie was awarded a contract from the U.S. DOE for development of an ultra-low-cost, polymer solar thermal collector. \n\nHe was Founder, President and CEO of TDEnergy, a wind power developer in the U.S. Northeast, from 1982 to 1988. TDEnergy constructed one of the earliest New England wind power facilities in Canaan, NH. \n\nDinwoodie then founded PowerLight Corporation in 1994, and served as its CEO and Chairman of the Board from 1995 to 2007. PowerLight was a global manufacturer, supplier, and systems integrator of solar products and services for the residential, commercial, and utility sectors. In 2004, PowerLight was inducted into the INC 500 Hall of Fame. PowerLight merged with SunPower Corporation in 2007, where Dinwoodie served as CEO and later CTO for the subsidiary, SunPower Corporation, Systems. SunPower produces high-efficiency solar cells and modules. \n\nDinwoodie holds over 30 patents on PV-related products.\n\nDinwoodie holds a BS in Civil and Environmental Engineering from Cornell University, an MS from the Department of Mechanical Engineering from MIT, and an MArch in Architecture from the University of California at Berkeley.\n\nDinwoodie is Executive Producer of \"Time to Choose\", a film by Charles Ferguson that seeks to raise awareness of the causes of, and solutions to, climate change. The film released in June, 2016. \n\nDinwoodie is or has been an advisor to, Board Member, seed investor or founder of a range of clean-tech startups, spanning wind and solar technologies, electric transportation, off-grid micropower, financial services, and clean utilities. These companies include AllPower Labs, Ethical Electric, Etrion Corporation, Fenix Int’l, Keystone Tower Systems, Mosaic, MyDomino, NEW GmbH, NEXTracker, PowerLight, SCOOT, Sistine Solar, Sungevity, SunPower Systems,Solar Grid Storage, and TDEnergy. \n\nDinwoodie serves as Lead Independent Trustee of the Rocky Mountain Institute, a nonprofit think-and-do tank focused on solutions for the transition to a clean energy economy. He also serves on the Sierra Club's Climate Cabinet and Scientific Advisory Panel, the MIT Mechanical Engineering Visiting Committee, and the Advisory Board to The Solutions Project, and is an Advisor to the MIT Energy Club.\n\n"}
{"id": "574742", "url": "https://en.wikipedia.org/wiki?curid=574742", "title": "Twelve Olympians", "text": "Twelve Olympians\n\nIn ancient Greek religion and mythology, the twelve Olympians are the major deities of the Greek pantheon, commonly considered to be Zeus, Hera, Poseidon, Demeter, Athena, Apollo, Artemis, Ares, Aphrodite, Hephaestus, Hermes, and either Hestia or Dionysus. They were called 'Olympians' because, according to tradition, they resided on Mount Olympus.\n\nAlthough Hades was a major ancient Greek god, and was the brother of the first generation of Olympians (Zeus, Poseidon, Hera, Demeter, and Hestia), he resided in the underworld, far from Olympus, and thus was not usually considered to be one of the Olympians.\n\nBesides the twelve Olympians, there were many other cultic groupings of twelve gods.\n\nThe Olympians were a race of deities, primarily consisting of a third and fourth generation of immortal beings, worshipped as the principal gods of the Greek pantheon and so named because of their residency atop Mount Olympus. They gained their supremacy in a ten-year-long war of gods, in which Zeus led his siblings to victory over the previous generation of ruling gods, the Titans. They were a family of gods, the most important consisting of the first generation of Olympians, offspring of the Titans Cronus and Rhea: Zeus, Poseidon, Hera, Demeter and Hestia, along with the principal offspring of Zeus: Athena, Apollo, Artemis, Ares, Aphrodite, Hephaestus, Hermes, and Dionysus. Although Hades was a major deity in the Greek pantheon, and was the brother of Zeus and the other first generation of Olympians, his realm was far away from Olympus in the underworld, and thus he was not usually considered to be one of the Olympians. Olympic gods can be contrasted to chthonic gods including Hades, by mode of sacrifice, the latter receiving sacrifices in a \"bothros\" (βόθρος, \"pit\") or \"megaron\" (μέγαρον, \"sunken chamber\") rather than at an altar.\n\nThe canonical number of Olympian gods was twelve, but besides the (thirteen) principal Olympians listed above, there were many other residents of Olympus, who thus might be called Olympians. Heracles became a resident of Olympus after his apotheosis and married another Olympian resident Hebe. Some others who might be considered Olympians, include: the Muses, the Graces, Iris, Dione, Eileithyia, the Horae, and Ganymede.\n\nBesides the twelve Olympians, there were many other various cultic groupings of twelve gods throughout ancient Greece. The earliest evidence of Greek religious practice involving twelve gods (Greek: δωδεκάθεον, \"dodekatheon\", from δώδεκα \"dōdeka\", \"twelve\" and θεοί \"theoi\", \"gods\") comes no earlier than the late sixth century BC. According to Thucydides, an altar of the twelve gods was established in the agora of Athens by the archon Pisistratus (son of Hippias, and the grandson of the tyrant Pisistratus), in c. 522 BC. The altar became the central point from which distances from Athens were measured and a place of supplication and refuge.\n\nOlympia apparently also had an early tradition of twelve gods. The \"Homeric Hymn to Hermes\" (c. 500 BC) has the god Hermes divide a sacrifice of two cows he has stolen from Apollo, into twelve parts, on the banks of the river Alpheius (presumably at Olympia):\n\nPindar, in an ode written to be sung at Olympia c. 480 BC, has Heracles sacrificing, alongside the Alpheius, to the \"twelve ruling gods\":\n\nAnother of Pindar's Olympian odes mentions \"six double altars\". Herodorus of Heraclea (c. 400 BC) also has Heracles founding a shrine at Olympia, with six pairs of gods, each pair sharing a single altar.\n\nMany other places had cults of the twelve gods, including Delos, Chalcedon, Magnesia on the Maeander, and Leontinoi in Sicily. As with the twelve Olympians, although the number of gods was fixed at twelve, the membership varied. While the majority of the gods included as members of these other cults of twelve gods were Olympians, non-Olympians were also sometimes included. For example, Herodorus of Heraclea identified the six pairs of gods at Olympia as: Zeus and Poseidon, Hera and Athena, Hermes and Apollo, the Graces and Dionysus, Artemis and Alpheus, and Cronus and Rhea. Thus while this list includes the eight Olympians: Zeus, Poseidon, Hera, Athena, Hermes, Apollo, Artemis, and Dionysus, it also contains three clear non-Olympians: the Titan parents of the first generation of Olympians, Cronus and Rhea, and the river god Alpheius, with the status of the Graces (here apparently counted as one god) being unclear.\n\nPlato connected \"twelve gods\" with the twelve months, and implies that he considered Pluto one of the twelve in proposing that the final month be devoted to him and the spirits of the dead.\n\nThe Roman poet Ennius gives the Roman equivalents (the \"Dii Consentes\") as six male-female complements, preserving the place of Vesta (Greek Hestia), who played a crucial role in Roman religion as a state goddess maintained by the Vestals.\n\nThere is no single canonical list of the twelve Olympian gods. The thirteen gods and goddesses most commonly considered to be one of the twelve Olympians are listed below.\n\nMost listings include either one or the other of the following deities as one of the twelve Olympians.\n\n\n"}
{"id": "1259545", "url": "https://en.wikipedia.org/wiki?curid=1259545", "title": "USS Flying Fish (1838)", "text": "USS Flying Fish (1838)\n\nUSS \"Flying Fish\" (1838), a schooner, was formerly the New York City pilot boat \"Independence\". Purchased by the United States Navy at New York City on 3 August 1838 and upon joining her squadron in Hampton Roads 12 August 1838 was placed under command of Passed Midshipman S. R. Knox.\n\nAssigned as a tender in the U.S. Exploring Expedition of 1838–42 commanded by Lieutenant Charles Wilkes, \"Flying Fish\" sailed with her squadron 19 August 1838 to visit Madeira and Rio de Janeiro while bound for Tierra del Fuego, where the squadron arrived early in 1839. From this point, the squadron made its first cruises toward the Antarctic Continent, which it was to discover later the same year after surveys among Pacific islands and a visit to Australia.\n\nAfter the second penetration of the Antarctic, the squadron rendezvoused in New Zealand in April 1840 to survey Pacific islands northward toward the Hawaiians, where the ships were repaired late in the year. \"Flying Fish\" sailed with to resurvey some of the Samoan, Ellice, Kingsmill, and Pescadore Islands before joining the main body of the squadron on the northwest coast of America in July 1841. \"Flying Fish\" made surveys in the Columbia River and around Vancouver, then proceeded to San Francisco, from which the squadron sailed 1 November for the south Pacific. Arriving in the Philippines in mid-January 1842 \"Flying Fish\" and the other ships separated to cruise the Sulu Seas, then make a planned rendezvous at Singapore in February. \n\nFound unfit for further service, \"Flying Fish\" was sold there before the squadron sailed for home 26 February.\n\n"}
{"id": "351039", "url": "https://en.wikipedia.org/wiki?curid=351039", "title": "Viral marketing", "text": "Viral marketing\n\nViral marketing or viral advertising is a business strategy that uses existing social networks to promote a product. Its name refers to how consumers spread information about a product with other people in their social networks, much in the same way that a virus spreads from one person to another. It can be delivered by word of mouth or enhanced by the network effects of the Internet and mobile networks.\n\nThe concept is often misused or misunderstood, as people apply it to any successful enough story without taking into account the word \"viral\".\n\nViral advertising is personal and, while coming from an identified sponsor, it does not mean businesses pay for its distribution. Most of the well-known viral ads circulating online are ads paid by a sponsor company, launched either on their own platform (company webpage or social media profile) or on social media websites such as YouTube. Consumers receive the page link from a social media network or copy the entire ad from a website and pass it along through e-mail or posting it on a blog, webpage or social media profile. Viral marketing may take the form of video clips, interactive Flash games, advergames, ebooks, brandable software, images, text messages, email messages, or web pages. The most commonly utilized transmission vehicles for viral messages include: pass-along based, incentive based, trendy based, and undercover based. However, the creative nature of viral marketing enables an \"endless amount of potential forms and vehicles the messages can utilize for transmission\", including mobile devices.\n\nThe ultimate goal of marketers interested in creating successful viral marketing programs is to create viral messages that appeal to individuals with high social networking potential (SNP) and that have a high probability of being presented and spread by these individuals and their competitors in their communications with others in a short period of time.\n\nThe term \"viral marketing\" has also been used pejoratively to refer to stealth marketing campaigns—marketing strategies that advertise a product to people without them knowing they are being marketed to.\n\nThe emergence of \"viral marketing\", as an approach to advertisement, has been tied to the popularization of the notion that ideas spread like viruses. The field that developed around this notion, memetics, peaked in popularity in the 1990s. As this then began to influence marketing gurus, it took on a life of its own in that new context.\n\nThe term viral strategy was first used in marketing in 1995, in a pre-digital marketing era, by a strategy team at Chiat/Day advertising in LA (now TBWA LA) for the launch of the first PlayStation for Sony Computer Entertainment. Born from a need to combat huge target cynicism the insight was that people reject things pushed at them but seek out things that elude them. Chiat/Day created a 'stealth' campaign to go after influencers/opinion leaders, using street teams for the first time in brand marketing and layered an intricate omni-channel web of info and intrigue. Insiders picked up on it and spread the word. Within 6 months PlayStation was number one in its category—Sony's most successful launch in history.\n\nThere is debate on the origination and the popularization of the specific term \"viral marketing\", though some of the earliest uses of the current term are attributed to the Harvard Business School graduate Tim Draper and faculty member Jeffrey Rayport. The term was later popularized by Rayport in the 1996 \"Fast Company\" article \"The Virus of Marketing\", and Tim Draper and Steve Jurvetson of the venture capital firm Draper Fisher Jurvetson in 1997 to describe Hotmail's practice of appending advertising to outgoing mail from their users. An earlier attestation of the term is found in \"PC User\" magazine in 1989, but with a somewhat differing meaning.\n\nAmong the first to write about viral marketing on the Internet was the media critic Doug Rushkoff. The assumption is that if such an advertisement reaches a \"susceptible\" user, that user becomes \"infected\" (i.e., accepts the idea) and shares the idea with others \"infecting them\", in the viral analogy's terms. As long as each infected user shares the idea with more than one susceptible user on average (i.e., the basic reproductive rate is greater than one—the standard in epidemiology for qualifying something as an epidemic), the number of infected users grows according to an exponential curve. Of course, the marketing campaign may be successful even if the message spreads more slowly, if this user-to-user sharing is sustained by other forms of marketing communications, such as public relations or advertising.\n\nBob Gerstley was among the first to write about algorithms designed to identify people with high \"social networking potential.\" Gerstley employed SNP algorithms in quantitative marketing research. In 2004, the concept of the \"alpha user\" was coined to indicate that it had now become possible to identify the focal members of any viral campaign, the \"hubs\" who were most influential. Alpha users could be targeted for advertising purposes most accurately in mobile phone networks, due to their personal nature.\n\nIn early 2013 the first ever Viral Summit was held in Las Vegas. It attempted to identify similar trends in viral marketing methods for various media.\n\nThis exponential growth is not infinite, because customers, people, are finite. This ceiling is called carrying capacity.\n\nAccording to the book \"Contagious: Why Things Catch On\", there are six key factors that drive virality. They are organized in an acronym called STEPPS which stands for:\n\nAccording to marketing professors Andreas Kaplan and Michael Haenlein, to make viral marketing work, three basic criteria must be met, i.e., giving the right message to the right messengers in the right environment:\n\n\nWhereas Kaplan, Haenlein and others reduce the role of marketers to crafting the initial viral message and seeding it, futurist and sales and marketing analyst Marc Feldman, who conducted IMT Strategies' viral marketing study in 2001, carves a different role for marketers which pushes the 'art' of viral marketing much closer to 'science'.\n\nTo clarify and organize the information related to potential measures of viral campaigns, the key measurement possibilities should be considered in relation to the objectives formulated for the viral campaign. In this sense, some of the key cognitive outcomes of viral marketing activities can include measures such as the number of views, clicks, and hits for specific content, as well as the number of shares in social media, such as likes on Facebook or retweets on Twitter, which demonstrate that consumers processed the information received through the marketing message. Measures such as the number of reviews for a product or the number of members for a campaign webpage quantify the number of individuals who have acknowledged the information provided by marketers. Besides statistics that are related to online traffic, surveys can assess the degree of product or brand knowledge, though this type of measurement is more complicated and requires more resources.\n\nRelated to consumers' attitudes toward a brand or even toward the marketing communication, different online and social media statistics, including the number of likes and shares within a social network, can be used. The number of reviews for a certain brand or product and the quality assessed by users are indicators of attitudes. Classical measures of consumer attitude toward the brand can be gathered through surveys of consumers.\nBehavioral measures are very important because changes in consumers' behavior and buying decisions are what marketers hope to see through viral campaigns. There are numerous indicators that can be used in this context as a function of marketers' objectives. Some of them include the most known online and social media statistics such as number and quality of shares, views, product reviews, and comments. Consumers' brand engagement can be measured through the K-factor, the number of followers, friends, registered users, and time spent on the website. Indicators that are more bottom-line oriented focus on consumers' actions after acknowledging the marketing content, including the number of requests for information, samples, or test-drives. Nevertheless, responses to actual call-to-action messages are important, including the conversion rate.\nConsumers' behavior is expected to lead to contributions to the bottom line of the company, meaning increase in sales, both in quantity and financial amount. However, when quantifying changes in sales, managers need to consider other factors that could potentially affect sales besides the viral marketing activities. Besides positive effects on sales, the use of viral marketing is expected to bring significant reductions in marketing costs and expenses.\n\nViral marketing often involves and utilizes: \n\nViral target marketing is based on three important principles:\n\nBy applying these three important disciplines to an advertising model, a VMS company is able to match a client with their targeted customers at a cost effective advantage.\n\nThe Internet makes it possible for a campaign to go viral very fast; it can, so to speak, make a brand famous overnight. However, the Internet and social media technologies themselves do not make a brand viral; they just enable people to share content to other people faster. Therefore, it is generally agreed that a campaign must typically follow a certain set of guidelines in order to potentially be successful:\n\nThe growth of social networks significantly contributed to the effectiveness of viral marketing. As of 2009, two thirds of the world's Internet population visits a social networking service or blog site at least every week. Facebook alone has over 1 billion active users. In 2009, time spent visiting social media sites began to exceed time spent emailing. A 2010 study found that 52% of people who view news online forward it on through social networks, email, or posts.\n\nThe introduction of social media has caused a change how viral marketing is used and the speed at which information is spread and users interact. This has prompted many companies to use social media as a way to market themselves and their products, with Elsamari Botha and Mignon Reyneke stating that viral messages are \"playing an increasingly important role in influencing and shifting public opinion on corporate reputations, brands, and products as well as political parties and public personalities to name but a few.\"\n\n'The influencers in order to communicate marketing messages to the audiences you seek to reach'. In business, it is indicated that people prefer interaction with humans to a logo. Therefore, it seems that influencers are on behalf of a company to build up a relationship between the brand and their customers. Companies would be left behind if they neglected the trend of influencers in viral marketing, as over 60% of global brands have used influencers in marketing in 2016.\nThe influencer types come along with the level of customers' involvement in companies' marketing. First, unintentional influences, because of brand satisfaction and low involvement, their action is just to deliver a company's message to a potential user. Secondly, users will become salesmen or promoters for a particular company with incentives. For example, ICQ offered their users benefits to create the awareness of their friends. Finally, the mass reached influencers are those who have a huge range of followers on the social network. Recent trend in businesses activity is to offer incentives to individual users for re-posting the advertisement messages to their own profiles. A common type of an incentive puts all the re-posting users into a random draw for a valuable gift \n\nMarketers and agencies commonly consider celebrities as a good influencer with endorsement work. This conception is similar to celebrity marketing. Based on a survey, 69% of company marketing department and 74% of agencies are currently working with celebrities in the UK. The celebrity types come along with their working environment. Traditional celebrities are considered as singles, dancers, actors or models. These types of public characters are continuing to be the most commonly used by company marketers. The survey found that 4 in 10 company having worked with these traditional celebrities in the prior year. However, people these years are spending more time on social media rather than traditional media such as TV. The researchers also claim that customers are not firmly believed celebrities are effectively influential.\n\nSocial media stars among a kind of influencer on viral marketing since consumers are spending more time on the Internet than before. And companies and agencies start to consider collaborating with social media stars as their product endorser.\n\nSocial media stars such as YouTuber Zoella or Instagrammer Aimee Song are followed by millions of people online. These online celebrities are having more connection and influence with their followers because they have more frequent and realistic conversation and interaction on the Internet in terms of comments or likes.\n\nThis trend captured by marketers who are used to explore new potential customers. Agencies are placing social media stars alongside singers and musicians at the top of the heap of celebrity types they had worked with. And there are more than 28% of company marketers having worked with one social media celebrity in the previous year.\n\nThe challenges of strategically maximizing the influence spread in social networks are addressed in management science.\n\nUsing influencers in viral marketing provides companies several benefits. It enables companies to spend little time and budget on their marketing communication and brand awareness promotion. For example, Alberto Zanot, in the 2006 FIFA Football World Cup, shared Zinedine Zidane's headbutt against Italy and engaged more than 1.5 million viewers in less than the very first hour. Secondly, it enhances the credibility of messages. These trust-based relationships grab the audience's attention, create customers' demand, increase sales and loyalty, or simply drive customers' attitude and behavior. In the case of Coke, Millennials changed their mind about the product, from parents' drink to the beverage for teens. It built up Millennials' social needs by 'sharing a Coke' with their friends. This created a deep connection with Gen Y, dramatically increased sales (+11% compared with last year) and market share (+1.6%).\n\nNo doubt that harnessing influencers would be a lucrative business for both companies and influencers. The concept of 'influencer' is no longer just an 'expert' but also anyone who delivers and influence on the credibility of a message (e.g. blogger) In 2014, BritMums, network sharing family's daily life, had 6,000 bloggers and 11,300 views per month on average and became endorsers for some particular brand such as Coca-Cola, Morrison. Another case, Aimee Song who had over 3.6m followers on the Instagram page and became Laura Mercier's social media influencers, gaining $500,000 monthly.\n\nDecision-making process seems to be hard for customers these days. Millers (1956) argued that people suffered from short-term memory. This links to difficulties in customers' decision-making process and Paradox of Choice, as they face various adverts and newspapers daily. \nInfluencers serve as a credible source for customers' decision-making process. Neilsen reported that 80% of consumers appreciated a recommendation of their acquaintances, as they have reasons to trust in their friends delivering the messages without benefits and helping them reduce perceived risks behind choices.\n\nThe main risk coming from the company is for it to target the wrong influencer or segment. Once the content is online, the sender won't be able to control it anymore. It is therefore vital to aim at a particular segment when releasing the message. This is what happened to the company BlendTech which released videos showing the blender could blend anything, and encouraged users to share videos. This mainly caught the attention of teenage boys who thought it funny to blend and destroy anything they could; even though the videos went viral, they did not target potential buyers of the product. This is considered to be one of the major factors that affects the success of the online promotion. It is critical and inevitable for the organisations to target the right audience. Another risk with internet is that a company's video could end up going viral on the other side of the planet where their products are not even for sale.\n\nAccording to a paper by Duncan Watts and colleagues entitled: \"Everyone's an influencer\", the most common risk in viral marketing is that of the influencer not passing on the message, which can lead to the failure of the viral marketing campaign. A second risk is that the influencer modifies the content of the message. A third risk is that influencers pass on the wrong message. This can result from a misunderstanding or as a deliberate move.\n\nBetween 1996–1997, Hotmail was one of the first internet businesses to become extremely successful utilizing viral marketing techniques by inserting the tagline \"Get your free e-mail at Hotmail\" at the bottom of every e-mail sent out by its users. Hotmail was able to sign up 12 million users in 18 months. At the time, this was historically the fastest growth of any user based media company. By the time Hotmail reached \"66 million users\", the company was establishing \"270,000 new accounts each day\".\n\nIn 2000, Slate.com described TiVo's unpublicized gambit of giving free systems to web-savvy enthusiasts to create \"viral\" word of mouth, pointing out that a viral campaign differs from a publicity stunt.\n\nBurger King has used several marketing campaigns. Its The Subservient Chicken campaign, running from 2004 until 2007, was an example of viral or word-of-mouth marketing.\n\nThe Blendtec viral video series \"Will It Blend?\" debuted in 2006. In the show, Tom Dickson, Blendtec founder and CEO, attempts to blend various unusual items in order to show off the power of his blender. \"Will it Blend?\" has been nominated for the 2007 YouTube award for Best Series, winner of .Net Magazine's 2007 Viral Video campaign of the year and winner of the Bronze level Clio Award for Viral Video in 2008. In 2010, Blendtec claimed the top spot on the AdAge list of \"Top 10 Viral Ads of All Time\". The Will It Blend page on YouTube currently shows over 200 million video views.\n\nThe Big Word Project, launched in 2008, aimed to redefine the \"Oxford English Dictionary\" by allowing people to submit their website as the definition of their chosen word. The project, created to fund two Masters students' educations, attracted the attention of bloggers worldwide, and was featured on Daring Fireball and \"Wired Magazine\".\n\nCompanies may also be able to use a viral video that they did not create for marketing purposes. A notable example is the viral video \"The Extreme Diet Coke & Mentos Experiments\" created by Fritz Grobe and Stephen Voltz of EepyBird. After the initial success of the video, Mentos was quick to offer its support. They shipped EepyBird thousands of mints for their experiments. Coke was slower to get involved.\n\nOn March 6, 2012, Dollar Shave Club launched their online video campaign. In the first 48 hours of their video debuting on YouTube they had over 12,000 people signing up for the service. The video cost just $4500 to make and as of November 2015 has had more than 21 million views. The video was considered as one of the best viral marketing campaigns of 2012 and won \"Best Out-of-Nowhere Video Campaign\" at the 2012 AdAge Viral Video Awards.\n\nIn 2014, A.L.S. Ice Bucket Challenge was among the best viral marketing challenges examples in the social network. Millions of people on the social media started filming themselves, pouring a bucket of ice water over their heads and sharing the video with their friends. The challenge was created to give support for fighting amyotrophic lateral sclerosis (ALS), also called Lou Gehrig's disease. People finished the challenge and then nominated the next person they knew on the social media to take the same challenge. By following this trend, Ice Bucket Challenge became a 'fab' on social media with many online celebrities such as Tyler Oakley, Zoe Sugg and huge celebrities and entrepreneurs like Justin Bieber, Mark Zuckerberg and Bill Gates participating. Until September 2014, over 2.4 million ice bucket-related videos had been posted on Facebook, and 28 million people had uploaded, commented on or liked ice bucket-related posts. And about 3.7 million videos had been uploaded on Instagram with the hashtags #ALSicebucketchallenge and #icebucketchallenge. The ALS association didn’t invent the ice bucket challenge, but they sure received a huge amount of donation from this activity. It raised a reported $220 million worldwide for A.L.S. organisations, and this amount is thirteen times as much donation as what it had in the whole preceding year in just eight weeks.\n\nIn mid 2016, an Indian tea company (TE-A-ME) has delivered 6,000 tea bags to Donald Trump and launched a video on YouTube. and Facebook The video campaign received various awards including most creative PR stunt in Southeast Asia after receiving 52000+ video shares, 3.1M video view in first 72-hour and hundreds of publication mentions (including Mashable, \"Quartz\", \"Indian Express\", Buzzfeed) across 80+ countries.\n\n\n86. Viral Marketing: What makes something go viral?\n"}
{"id": "22826277", "url": "https://en.wikipedia.org/wiki?curid=22826277", "title": "World Forum on Energy Regulation", "text": "World Forum on Energy Regulation\n\nThe World Forum on Energy Regulation (WFER) is the leading international conference on energy regulation, held once every three years.\nWFER IV is hosted by the Council of European Energy Regulators CEER and the Greek Regulator. WFER IV will be held in Athens, Greece, from 18 to 21 October 2009. It builds on the themes and key findings of the past three World Fora, held in 2000 (Montreal) 2003 (Rome) and 2006 (Washington).\n\nThe WFER IV will provide a global conference and networking opportunity attended by more than 700 presidents and CEOs from the energy industry, government and international institution officials, high-level policy makers, academics and regulators from over 60 countries.\n\nAmong others, the three-day programme of WFER IV features numerous break-out sessions across four key themes:\n\n\nDistinguished chairpersons and speakers include:\n\n\n"}
{"id": "15620404", "url": "https://en.wikipedia.org/wiki?curid=15620404", "title": "Wyoming Craton", "text": "Wyoming Craton\n\nThe Wyoming Craton is a craton in the west-central United States and western Canada – more specifically, in Montana, Wyoming, southern Alberta, southern Saskatchewan, and parts of northern Utah. Also called the Wyoming Province, it is the initial core of the continental crust of North America.\n\nThe Wyoming Craton was sutured together with the Superior and Hearne-Rae cratons in the mountain-building episode that created the Trans-Hudson Suture Zone to form the core of North America (Laurentia). It was incorporated into southwest Laurentia approximately 1.86 billion years ago.\n\nLocal preservation of 3.6–3.0 Ga gneisses and widespread isotopic evidence for crust of this age incorporated into younger plutons indicates that the Wyoming Craton originated as a 100,000 km middle Archean craton that was modified by late Archean volcanic magmatism and plate movements and Proterozoic extension and rifting.\n\nThe Wyoming, Superior and Hearne-Ray cratons were once sections of separate continents, but today they are all welded together. The collisions of these cratons began before ca. 1.77 Ga, with post-tectonic magmatism at ca. 1.715 Ga (the Harney Peak granite). This tectonic-magmatic interval is 50–60 million years younger than that reported for the Hearne-Superior collision of the Trans-Hudson orogeny in Canada.\n\nYounger metamorphic dates (1.81–1.71 Ga) also typify the eastern and northern Wyoming province peripheries in the western Dakotas and southeastern Montana. The final assembly of the eastern Wyoming Craton as part of the continent Laurentia began during the ca. 1.78–1.74 Ga interval of island-arc accretion along the southern margin of the growing craton.\n\nThe Precambrian basement of Wyoming consists mainly of three major geologic terranes, the Archean Wyoming Craton or Province, the Paleoproterozoic Trans-Hudson orogen, and the Paleoproterozoic Colorado orogeny. The Colorado orogen collided with the Wyoming Craton at 1.78–1.75 Ga. Collision of the Colorado orogen and the Trans-Hudson orogen with the Archean craton produced strong structural overprinting along the southern and eastern margins of the Wyoming craton.\n\nThe Wyoming Craton consists mainly of two gross rock units—granitoid plutons (2.8–2.55 Ga) and gneiss and migmatite—together with subordinate (<10 percent) supracrustal metavolcanic-metasedimentary rocks. The granitoid rocks are mainly potassic granite and were derived principally from reworked older (3.1–2.8 Ga) gneiss. Magnetic contrast between the granitoid rocks and gneiss provides a means to map these gross rock units in covered areas. The overall structural pattern of the Archean units shown by magnetic data is crudely semi-circular and open to the north.\n\nThe present-day lithospheric architecture of the Wyoming Province is the result of cumulative processes of crustal growth, tectonic modification, and lithospheric contrasts that have apparently persisted for billions of years.\n\nThe Wyoming province can be subdivided into three subprovinces, namely, from oldest to youngest, the Montana metasedimentary province, the Beartooth–Bighorn magmatic zone, and the Southern accreted terranes. Archean rocks of the Montana metasedimentary province and the Beartooth-Bighorn magmatic zone are characterized by (1) their antiquity (rock ages to 3.5 Ga, detrital zircon ages up to 4.0 Ga, and Nd model ages exceeding 4.0 Ga); (2) a distinctly enriched Pb/Pb isotopic signature, which suggests that this part of the province was not produced by the amalgamation of already-formed exotic terranes; and (3) a distinctively thick (15–20 km), mafic lower crust. The Montana metasedimentary province and Beartooth–Bighorn magmatic zone were established as cratons by about 3.0–2.8 Ga. Crustal growth occurred through a combination of continental-arc magmatism resulting from oceanic crust subducted beneath continental crust on an adjacent plate, creating an arc-shaped mountain belt, together with terrane accretion in the Southern accreted terranes along the southern margin of the province at 2.68–2.50 Ga. By the end of the Archean, the three subprovinces were joined as part of what is now the Wyoming craton. Subsequent to amalgamation of the Wyoming crust to Laurentia at ca. 1.8–1.9 Ga, Paleoproterozoic crust (1.7–2.4 Ga) was juxtaposed along the southern and western boundaries of the province. Subsequent tectonism and magmatism in the Wyoming region are concentrated in the areas underlain by these Proterozoic mobile belts.\n\nAn analysis by Kevin Chamberlain \"et al.\" (2003), on the basis of differences in late Archean histories, subdivides the Wyoming Province into five subprovinces: three in the Archean core, (1) the Montana metasedimentary province, (2) the Bighorn subprovince, and (3) the Sweetwater subprovince, and two Archean terrains that may have originated elsewhere (that is, allochthonous to the 3.0 Ga craton), (4) the Sierra Madre – Medicine Bow block, and (5) the Black Hills – Hartville block. Based on imaging by the \"Deep Probe\" analysis, a thick lower crustal layer corresponds geographically with the Bighorn subprovince and may be an underplate associated with ca. 2.70 Ga mafic magmatism. The Sweetwater subprovince is characterized by an east–west-tending tectonic grain that was established by three or more roughly contemporaneous late Archean, pulses of basin development, shortening, and arc magmatism. This tectonic grain, including the 2.62 Ga Oregon Trail structure, controlled the locations and orientations of Proterozoic rifting and uplifts related to the Laramide orogeny. If there has been any net crustal growth of the Wyoming Province since 3.0 Ga, it has involved a combination of mafic underplating and arc magmatism.\n\nDuring the Paleoproterozoic, island-arc terrane associated with the Colorado orogeny accreted to the Wyoming Craton along the Cheyenne belt, a 500-km-wide belt of Proterozoic rocks named for Cheyenne, Wyoming. As a result of the collision, older, Archean rocks of the Wyoming province were intensely deformed and metamorphosed for at least 75 km inboard from the suture, which is marked today by the Laramie Mountains. Along the east margin of the craton, collision with the Paleoproterozoic Trans-Hudson orogen intensely deformed Archean cratonic rocks in the Hartville uplift. \n\nMesoproterozoic (~1.4 Ga) anorthosite and syenites of the Laramie Anorthosite Complex and granite (ilmenite-bearing Sherman Granite) intrude into rocks of the Colorado orogen in the Laramie and adjacent Medicine Bow Mountains. Both the anorthosite and granite transect the Cheyenne belt in the Laramide Mountains, and intrude crystalline rocks of the Wyoming province. These intrusions comprise the northernmost segment of a wide belt of 1.4 Ga granitic intrusions that occur throughout the Colorado orogen.\n\nLong after its assembly, the Wyoming Craton owes its spectacular mountainous terranes mainly to a regional episode of compressional deformation during the Laramide orogeny (ca.60 Ma). The basement blocks composed of Precambrian rocks were uplifted locally to high levels in the crust during the deformation, and subsequent erosion has molded the uplifted rocks into the rugged present-day topography. Vertical displacement of the basement surface was as much as 30,000 ft. (9250 m). By contrast, in western Wyoming thrust faulting, associated with the Sevier orogeny of approximately the same age, was thin-skinned, and the lack of disruption of magnetic anomalies in the region indicates that the basement rocks were little disturbed and not significantly uplifted during the thrusting. Even younger high-angle faulting of Pliocene–Pleistocene age has formed the Teton Range. Vertical relief on the east face of the mountains is about 25,000 ft. (7800 m).\n\n"}
