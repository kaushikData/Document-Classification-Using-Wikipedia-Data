{"id": "47745622", "url": "https://en.wikipedia.org/wiki?curid=47745622", "title": "2015 Thailand bolide", "text": "2015 Thailand bolide\n\nOn September 7, 2015, at about 08:40 local time a bolide meteor appeared over Thailand and burned up approximately 100 km (62 mi) above the ground. The meteor briefly flared up producing a green and orange glow before disappearing without a sound of explosion and leaving a white smoke trail. The meteor was recorded by several dashcams during the morning rush hour in Bangkok, and sightings were also reported in Thai towns of Kanchanaburi and Nakhon Ratchasima. The meteor was visible for about four seconds before fading out. As of September 8, 2015 no strewn field has been found. The impact energy was the largest of 2015 at 3.9 kiloton. The last impact this large was on 23 August 2014 over the Southern Ocean.\n\nThe object was initially believed to be FLOCK 1B-11 satellite which was due to burn in the atmosphere about that time or a crashing aircraft. However, because the object travelled in the direction opposite to the east–west axis of artificial satellites, it was identified by the Deputy Director of Thai National Astronomical Research Institute, Saran Poshyachinda, as \"an asteroid\" and as a meteor by the president of the Thai Astronomical Society Prapee Viraporn. The object was also identified as a meteor by the Chachoengsao Observatory astronomer Worawit Tanwutthibundit. Tanwutthibundit, who witnessed the event, estimated the object's speed at nearly 50 miles per second before disintegration. A similar explanation was suggested by the former member of Hubble Space Telescope team, Phil Plait, who said that \"it was almost certainly a good-sized rock burning up in our atmosphere\". According to Plait, the object may have had a steep angle of entry.\n\nSound from the meteor was reported in three districts of Kanchanaburi Province: Thong Pha Phum, Sai Yok and Si Sawat. Governor of Kanchanaburi Province Wan-chai Osukhonthip ordered police and Sai Yok National Park rangers to search Wang Krachae and Bong Ti subdistricts in Sai Yok District for meteor debris.\n\nThe National Astronomical Research Institute of Thailand gave a press release on 14 September, estimating that the meteor was about 3.5 metres in diameter with a mass of 66 tonnes, entering the atmosphere at 21 km/s and having maximum brightness at 29.3 km altitude. The impact energy was equivalent to 3.5 kilotonnes of TNT; its trajectory was 269.8 degrees, with an impact angle of 45.4 degrees. It also estimates that meteorite remnants may have fallen around the area of Sai Yok National Park.\n\nA previous bolide event was recorded in Thailand on March 2, 2015, and was also seen in Bangkok.\n\nOn 2 November 2015, a dramatic green fireball lit up the night sky over Thailand as it streaked past and exploded.\n"}
{"id": "10933069", "url": "https://en.wikipedia.org/wiki?curid=10933069", "title": "A Plague of Frogs", "text": "A Plague of Frogs\n\nA Plague of Frogs is a non-fiction environmental book written by William Souder and published in 2000 by Hyperion Press. The book elaborates on the issue of mutated frogs and the implications for humans. Structurally the book is divided into two parts.\n\nThe book begins with the discovery of a large number of deformed frogs by schoolteacher Cindy Reinitz and her students in August 1995. They were on a field trip to a farm owned by Donald Ney near the town of Henderson, Minnesota. As the students approached the rain-fed pond, they noticed that a large number of the northern leopard frogs (rana pipiens) had deformities, such as missing legs, extra legs, and other disfigurements. Concerned about the possibility that the deformities occurred because of a contaminant in the water, Ms. Reinitz contacted the Minnesota Pollution Control Agency (MPCA). As the MPCA did not have an amphibian specialist at the time, Reinitz was referred to the MPCA's invertebrate researcher, Dr. Judy Helgen. At the time Ms. Helgen was studying frogs as part of an effort to develop a bio-index for measuring the overall health of a pond or wetland. As Helgen was busy at the time, she sent an intern, Joel Chirhart, to investigate the pond. Chirhart is alarmed by the widespread nature of the deformities - present in over 1/3 of frogs collected.\n\nLater that year, Ms. Helgen called Robert McKinnell, a biologist at the University of Minnesota. Mr. McKinnell is considered the authority on frogs in Minnesota, having specialized in herpetology for over 50 years. While initially the two thought that the deformities were an isolated occurrence that would disappear like an earlier outbreak at Granite Falls, the persistence of leg deformities into late summer and the discovery of other outbreaks, elsewhere around Henderson and elsewhere around the state, such as at Litchfield, quickly disabused of that notion.\n\nIn that same month (August 1995), Dennis and Rhonda Bock found many deformed frogs near their lake in Brainerd, Minnesota. At first they were unconcerned, thinking the appearance of these frogs was a freak, but natural occurrence. However, as more reports of frog deformities came from the rest of the state, they grew concerned and called the Minnesota Department of Natural Resources (DNR). While the DNR could not do anything about the frogs, they sent the information on to David Hoppe, a herpetologist at the University of Minnesota Morris campus. Hoppe was unable to come to the Bock's lake until October, because of previous commitments. When he did manage to go to the Bock property, Hoppe was very surprised, as the Bocks lived on a relatively large natural lake, rather than on the small farm ponds where others had reported deformed frogs. The presence of these deformities in both natural and man-made bodies of water cast doubt on the theory of chemical contamination, as the larger, natural lakes should not have been as susceptible to harboring high concentration of pollutants as smaller, man-made agricultural ponds.\n\nThe next year Hoppe, Helgen and McKinnell obtained a $150,000 grant from the state to study the frog deformities. Even more sightings of deformed frogs were reported that year. Short on funds, the MPCA managed to successfully confirm 21 sites with deformed animals. Of the 3000 deformed frogs collected, just under 12 percent had deformities. However, there was a considerable variance in the incidence of deformities between sites and between collection intervals at the same site. As deformed frogs were more systematically collected and dissected, there was considerable evidence of internal abnormality. Digestive systems and reproductive systems were especially hard hit, with some frogs \"starving to death, despite being stuffed to bursting with food\". As the summer of 1996 progressed, Hoppe paid more attention to the Bock lake. Although the frogs at the lake seemed normal at the beginning of the summer, the number and variety of deformities rose dramatically as the season progressed.\n\nAs alarming as the findings at the various sites were, there was very little published material on the subject of frog deformities. Therefore, the researchers could not conclude that there was anything out of the ordinary, simply because they didn't know what the baseline rate of these deformities were. A review of existing literature on the subject showed that although deformities were a known phenomenon, there was no precedent for the variety or the rate of deformity that was being observed.\n\nAs the mystery deepened, internal tensions within the investigative team mounted. David Hoppe became increasing concerned with protecting the integrity of sites from outside interference. He was especially concerned about the Bock's lake, labeled CWB (\"Crow Wing county - Bock\"), as it was one of the few natural bodies of water that had been affected by the deformities. While Helgen was also concerned about the integrity of sites, Hoppe felt that the MPCA, as a public agency was not well equipped to ensure site integrity.\n\nThe story then takes a detour and discusses the findings of Martin Ouellet, a French-Canadian biologist who was studying the effect of agricultural chemicals on frogs in the St. Lawrence River valley. Ouellet found that the frogs in agricultural ponds developed deformities at a significantly higher rate than frogs in natural control environments. He had seen and noted the same abnormalities that were being discovered in Minnesota. He became convinced that the deformities were being caused by agricultural chemicals.\n\nAfter introducing Ouellet, the book moves on to a discussion of possible causes of frog deformities. There are two main theories: parasites, and agricultural chemicals. The parasite theory, advocated by Stan Sessions stated that parasitic cysts from flatworms blocked limb buds, forcing the tadpoles to try to adjust their limb development around the invaders. The pollutant theory, pushed by McKinnell, stated that there were pollutants that were having teratogenic effects on the frogs. The unknown pollutant or pollutants were theorized to be mimics of retinoic acid. Retinoic acid is a hormone that signals limb development in metamorphosing frogs. The pollutant theory stated that these pollutants were disrupting retinoic acid levels in tadpoles, leading to missing or misplaced limbs, in addition to internal developmental abnormalities.\n\nThe rest of the book revolves around the interplay between advocates of these two theories. As both sides move to gather evidence to support their theory (retinoic acid disruption vs. parasites) conflicts inevitably develop. The book covers the emergence and development of those conflicts, especially between Sessions and Ouellet.\n\nThe book equivocates on the actual cause of the deformities. It states that both parasites and deformities can be traced back to environmental changes caused by humans. Parasite ranges are altered by global warming. Modern farming techniques rely on a large variety of chemicals, whose breakdown and interaction in the environment is still virtually unknown. These changes create, as McKinnell puts it, a \"quality of life\" issue for amphibians. The deformities in the frog population are a result of the extraordinary stresses thrust upon them by the modern world.\n\nBill McKibben of \"The Washington Monthly\" has a favorable view of Souder's methodology \"This is a revealing and important book, and you should begin by ignoring the subtitle. Something tells me it was not the author's choice: In fact, he's done a remarkably sober and meticulous job of following a story that's been misreported in almost every newspaper and on every TV station in the country.\"\n\nIn \"The Quarterly Review of Biology\", Paul Stephen Corn appreciates the book's accessibility \"The book is written with little technical jargon and should be accessible to any biologically literate, nonprofessional reader. I also recommend this book to professional audiences, including undergraduates and early graduate students who still harbor illusions that science proceeds the way it is described in textbooks.\"\n\nAnnie Stewart of the Ecologist also appreciates the books easy to read tone \"Souder manages to make frogs as fascinating to the casual reader as they are to the scientists who study them.\"\n\nIlse Heidmann of the Library Journal has a slightly negative review \"While Souder's reporting is exhaustive, it is also repetitious and tiresome at times. Moreover, his frequent suggestions of a looming global catastrophe sound ominously alarmist. But who knows--maybe human extinction is not far behind.\"\n\n\n"}
{"id": "54066699", "url": "https://en.wikipedia.org/wiki?curid=54066699", "title": "Aba (mythology)", "text": "Aba (mythology)\n\nIn Greek mythology, Aba (Ancient Greek: Ἄβα) was a Thracian naiad nymph who mothered by Poseidon, a son Ergiscus (Ἐργίσκος), after whom Çatalca or Ergisce, took its name. She is presumed to be a daughter of the river Hebros.\n"}
{"id": "882160", "url": "https://en.wikipedia.org/wiki?curid=882160", "title": "Accordion effect", "text": "Accordion effect\n\nIn physics, the accordion effect, known also as the slinky effect, concertina effect, elastic band effect, and string instability, occurs when fluctuations in the motion of a travelling body causes disruptions in the flow of elements following it. This can happen in road traffic, foot marching, bicycle and motor racing, and, in general, to processes in a pipeline. These are examples of nonlinear processes. The accordion effect generally decreases the throughput of the system in which it occurs.\n\nThe accordion effect in road traffic refers to the typical decelerations and accelerations of a vehicle when the vehicle in front decelerates and accelerates. These fluctuations in speed propagate backwards and typically get bigger and bigger further down the line, decreasing the throughput of road traffic.\n\n\n"}
{"id": "28940235", "url": "https://en.wikipedia.org/wiki?curid=28940235", "title": "Alaska-St. Elias Range tundra", "text": "Alaska-St. Elias Range tundra\n\nThe Alaska-St. Elias Range tundra is an ecoregion of northwestern North America.\n\nThis ecoregion consists of a long range of high rocky mountains of the Alaska Interior running north from the bottom of the Alaska Peninsula, eastwards taking in the Alaska Range and southwards to include the Wrangell and St. Elias Mountains in eastern Alaska on the Canada–US border as far as Yakutat Bay. Across in Canada, the ecoregion includes the southwestern corner of the Yukon Territory and the northwestern corner of British Columbia. These mountains are largely covered with permanent ice and snow, with permanent snow above about 2150 m, and are separated by valleys filled with ice fields and great glaciers. There are small patches that are not under ice and consist of rock, rubble and alpine tundra. Elevations of the ecoregion range from sea level (on the western coast) to 600 m in the valleys, to peaks over 4,000 m. Indeed, Denali (Mount McKinley), the highest peak in North America at 6,100 m, is located here, while the St Elias Mountains reach as high as 6000 m and are some of the highest peaks in Canada. This ecoregion is largely separated from the coast by the Pacific coastal mountain icefields and tundra so the climate is continental. Rainfall varies from 200 mm per year on the higher slopes to 400 mm per year in the lower areas.\n\nPermafrost is everywhere on the higher slopes but there are patches of alpine tundra plant life at lower elevations, including mountain avens (\"Dryas octopetala\") and \"Erica\"s such as \"Vaccinium vitis-idaea\" and \"Cassiope tetragona\".\n\nAnimals of the area include large brown bears of Denali National Park and the southwestern coast near Iliamna Lake and Kamishak Bay. Other mammals include mountain goat, caribou, moose, Dall sheep, North American beaver and snowshoe hare. The rivers of the area are home to salmon. Birds include willow grouse, Siberian tit, wheatear, Wilson's warbler and boreal chickadee.\n\nThis is a largely unspoilt environment home to large predators, although there is some development associated with tourism, especially at Kantishna near Denali Park, and some mining activity including the abandoned copper mining camp of Kennecott, Alaska in the Wrangell Mountains and coal mining at Nabesna and Healy, Alaska. Protected areas include Lake Clark National Park and Preserve, Denali National Park, the adjacent Denali State Park, Tetlin National Wildlife Refuge and the Wrangell-St. Elias Park and Preserve, all in Alaska, and parts of Tatshenshini–Alsek Provincial Park and Kluane National Park and Reserve in Canada.\n"}
{"id": "38171752", "url": "https://en.wikipedia.org/wiki?curid=38171752", "title": "All Yesterdays", "text": "All Yesterdays\n\nAll Yesterdays is an art book on the palaeoartistic reconstruction of dinosaurs and other extinct animals by John Conway, C.M. Kosemen and Darren Naish. It was published in 2012.\n\nThe book first recounts the history of changing perceptions of dinosaurs as expressed in artwork. It begins with the sluggish and slow dinosaurs seen in the works of Charles R. Knight, and then continues into analyzing reconstructions after the Dinosaur renaissance. It points out that these reconstructions do not take the often bizarre integumentary coverings of living animals into account, and that dinosaurs should be portrayed as natural animals that aren't 'shrink-wrapped' with many of the individual bones visible.\n\nThe remainder of the book consists of pictures with accompanying explanatory texts. Each picture displays a hypothetical adaptation that an extinct animal could have possessed, such as a plesiosaur disguised on the seafloor like a decorator crab or something that dinosaurs aren't usually shown doing, such as a sleeping \"Tyrannosaurus\". The texts describe the adaptations or habits and explain why they are plausible. Some of the entries are deliberately made to break a paleoartistic cliché, such as a \"Tenontosaurus\" walking alone without a predatory \"Deinonychus\" in sight (\"Tenontosaurus\" is almost exclusively depicted in dinosaur art as the prey of \"Deinonychus\").\n\nThe second and last major section of the book is titled \"All Todays\", and depicts animals from the present day as if non-human paleontologists from the future were reconstructing them from fossilized skeletons. Some of the creatures are somewhat recognizable, like a vulture depicted with pterosaur-like wings; others are completely unrecognizable, like a rhinoceros reconstructed with no nose horn and a sail instead of a hump. By showing how completely extant animals might be misunderstood if known only from skeletal remains, \"All Yesterdays\" shows that our own conceptions of extinct animals are likely equally mistaken.\n\n\"All Yesterdays\" has received mostly very enthusiastic reviews from palaeontologists, and is perceived as introducing or popularising a new \"third wave\" approach to palaeoart after the classical period of Knight, Zallinger, Burian and others, and the more modern work of Bakker, Paul, Henderson and others. For example, John Hutchinson of the Royal Veterinary College wrote \"This is a thinking person’s book ... for rumination, to challenge your preconceptions, not to have a flashy coffee table book. It’s not eye candy — it’s more like brain jerky.\" And Mike Taylor wrote \"\"All Yesterdays\" is not only the most beautiful but also the most important palaeoart book of the last four decades\".\n\n"}
{"id": "8237631", "url": "https://en.wikipedia.org/wiki?curid=8237631", "title": "Arctic and Antarctic Research Institute", "text": "Arctic and Antarctic Research Institute\n\nThe Arctic and Antarctic Research Institute, or AARI (, abbreviated as ААНИИ) is the oldest and largest Russian research institute in the field of comprehensive studies of Arctic and Antarctica. It is located in Saint Petersburg. \n\nThe AARI has numerous departments, such as those of oceanography, glaciology, meteorology, hydrology or Arctic river mouths and water resources, geophysics, polar geography, and others. It also has its own computer center, ice research laboratory, experimental workshops, and a museum (the Arctic and Antarctic Museum). \n\nScientists, such as Alexander Karpinsky, Alexander Fersman, Yuly Shokalsky, Nikolai Knipovich, Lev Berg, Otto Schmidt, Rudolf Samoylovich, Vladimir Vize, Nikolai Zubov, Pyotr Shirshov, Nikolai Urvantsev, and Yakov Gakkel have all made their valuable contributions to the work of the AARI.\n\nThroughout its history, the AARI has organized more than a thousand Arctic expeditions, including dozens of high-latitude aerial expeditions, which transported 34(?) manned drifting ice stations \"Severniy Polyus\" (\"Северный полюс\", or North Pole) to Central Arctic. \nThe AARI was founded on 3 March 1920 as the Northern Research and Trade Expedition (Северная научно-промысловая экспедиция) under the Scientific and Technical Department of the All-Union Council of State Economy. In 1925, the expedition was reorganized into the Institute of Northern Studies (Институт по изучению Севера) and five years later - into the All-Union Arctic Institute (Всесоюзный арктический институт). In 1932, the institute was integrated into the Chief Directorate of the Northern Sea Route (Главное управление Северного морского пути). In 1948 the Arctic Geology Research Institute (Научно-исследовательский институт геологии Арктики, or НИИГА) was established on the basis of the geology department of the All-Union Arctic Institute, which would subordinate to the Ministry of Geology of the USSR. \n\nIn 1955, the AARI participated in the organization of Antarctic research. In 1958, it began to organize and lead all of the Soviet Antarctic expeditions, which would later make many geographic discoveries, and in the same year the All-Union Arctic Institute was renamed Arctic and Antarctic Research Institute. In 1963, the AARI was incorporated into the Chief Administration of the Hydrometeorological Service (Главное управление Гидрометеослужбы) under the Council of Ministers of the USSR (now Federal Service for Hydrometeorology and Environmental Monitoring of Russia).\n\nIn 1967, AARI was awarded the Order of Lenin. In 1968, the institute engaged in research of the areas of the Atlantic Ocean contiguous to the Arctic and Antarctica.\n\n\n"}
{"id": "38406200", "url": "https://en.wikipedia.org/wiki?curid=38406200", "title": "Beaver Creek (Pennsylvania)", "text": "Beaver Creek (Pennsylvania)\n\nBeaver Creek is the official GNIS name in these several Pennsylvania waterways:\n\n"}
{"id": "56339021", "url": "https://en.wikipedia.org/wiki?curid=56339021", "title": "Bristol Energy", "text": "Bristol Energy\n\nBristol Energy is a municipally owned energy company founded in 2015 by Bristol City Council. It describes itself as \"a force for social good\" and aims to deliver non-profit energy using renewables at lower prices than large profit making energy corporations.\n\n\n"}
{"id": "52770651", "url": "https://en.wikipedia.org/wiki?curid=52770651", "title": "Carappee Hill Conservation Park", "text": "Carappee Hill Conservation Park\n\nCarappee Hill Conservation Park is a protected area in the Australian state of South Australia located on the Eyre Peninsula in the gazetted locality of Darke Peak about north-east of the town centre in Darke Peak.\n\nThe conservation park was proclaimed on 30 August 1973 under the state’s \"National Parks and Wildlife Act 1972\" in respect to land in section 104 in the cadastral unit of the Hundred of Pascoe which had been dedicated as a water conservation reserve since 1955. As of July 2016, the conservation park covered an area of .\n\nThe conservation park is classified as an IUCN Category III protected area.\n\n\n"}
{"id": "20861", "url": "https://en.wikipedia.org/wiki?curid=20861", "title": "Cavity magnetron", "text": "Cavity magnetron\n\nThe cavity magnetron is a high-powered vacuum tube that generates microwaves using the interaction of a stream of electrons with a magnetic field while moving past a series of open metal cavities (cavity resonators). Electrons pass by the openings to these cavities and cause radio waves to oscillate within, similar to the way a whistle produces a tone when excited by an air stream blown past its opening. The frequency of the microwaves produced, the resonant frequency, is determined by the cavities' physical dimensions. Unlike other vacuum tubes such as a klystron or a traveling-wave tube (TWT), the magnetron cannot function as an amplifier in order to increase the intensity of an applied microwave signal; the magnetron serves solely as an oscillator, generating a microwave signal from direct current electricity supplied to the vacuum tube.\n\nAn early form of magnetron was invented by H. Gerdien in 1910. Another form of magnetron tube, the split-anode magnetron, was invented by Albert Hull of General Electric Research Laboratory in 1920, but it achieved only a frequency of 30 kHz. Similar devices were experimented with by many teams through the 1920s and 1930s. Hans Erich Hollmann filed a patent on a design similar to the modern tube in 1935, but the more stable klystron was preferred for most German radars during World War II. An important advance was the multi-cavity magnetron, first proposed in 1934 by A. L. Samuel of Bell Telephone Laboratories. However, the first truly successful example was developed by Aleksereff and Malearoff in Russia in 1936, which achieved 300 watts at 3 GHz (10 cm wavelength).\n\nThe cavity magnetron was radically improved by John Randall and Harry Boot in 1940 at the University of Birmingham, England. They invented a valve that could produce multi-kilowatt pulses at 10 cm wavelength, an unprecedented discovery. The high power of pulses from their device made centimeter-band radar practical for the Allies of World War II, with shorter wavelength radars allowing detection of smaller objects from smaller antennas. The compact cavity magnetron tube drastically reduced the size of radar sets so that they could be more easily installed in night-fighter aircraft, anti-submarine aircraft and escort ships.\n\nAt the same time, Yoji Ito in Japan was experimenting with magnetrons, and proposed a system of collision avoidance using FM. Only low power was achieved. Ito then traveled to Germany, where he had earlier received his doctorate, and found the Germans were using pulse modulation at VHF with great success. When he returned to Japan, he produced a prototype pulse magnetron with 2 kW in October 1941. This was then widely deployed.\n\nIn the post-war era the magnetron became less widely used in the radar role. This was because the magnetron's output changes from pulse to pulse, both in frequency and phase. This makes the signal unsuitable for pulse-to-pulse comparisons, which is widely used for detecting and removing \"clutter\" from the radar display. The magnetron remains in use in some radars, but has become much more common as a low-cost microwave source for microwave ovens. In this form, approximately one billion magnetrons are in use today.\n\nIn a conventional electron tube (vacuum tube), electrons are emitted from a negatively charged, heated component called the cathode and are attracted to a positively charged component called the anode. The components are normally arranged concentrically, placed within a tubular-shaped container from which all air has been evacuated, so that the electrons can move freely (hence the name \"vacuum\" tubes, called \"valves\" by the British)\n\nIf a third electrode is inserted between the cathode and the anode (called a control grid), the flow of electrons between the cathode and anode can be regulated by varying the voltage on this third electrode. This allows the resulting electron tube (called a \"triode\" because it now has three electrodes) to function as an amplifier because small variations in the electric charge applied to the control grid will result in identical variations in the much larger current of electrons flowing between the cathode and anode.\n\nThe idea of using a grid for control was patented by Lee de Forest, resulting in considerable research into alternate tube designs that would avoid his patents. One concept used a magnetic field instead of an electrical charge to control current flow, leading to the development of the magnetron tube. In this design, the tube was made with two electrodes, typically with the cathode in the form of a metal rod in the center, and the anode as a cylinder around it. The tube was placed between the poles of a horseshoe magnet arranged such that the magnetic field was aligned parallel to the axis of the electrodes.\n\nWith no magnetic field present, the tube operates as a diode, with electrons flowing directly from the cathode to the anode. In the presence of the magnetic field, the electrons will experience a force at right angles to their direction of motion, according to the left-hand rule. In this case, the electrons follow a curved path between the cathode and anode. The curvature of the path can be controlled by varying either the magnetic field, using an electromagnet, or by changing the electrical potential between the electrodes.\n\nAt very high magnetic field settings the electrons are forced back onto the cathode, preventing current flow. At the opposite extreme, with no field, the electrons are free to flow straight from the cathode to the anode. There is a point between the two extremes, the critical value or Hull cut-off magnetic field (and cut-off voltage), where the electrons just reach the anode. At fields around this point, the device operates similar to a triode. However, magnetic control, due to hysteresis and other effects, results in a slower and less faithful response to control current than electrostatic control using a control grid in a conventional triode (not to mention greater weight and complexity), so magnetrons saw limited use in conventional electronic designs.\n\nIt was noticed that when the magnetron was operating at the critical value, it would emit energy in the radio frequency spectrum. This occurs because a few of the electrons, instead of reaching the anode, continue to circle in the space between the cathode and the anode. Due to an effect now known as cyclotron radiation, these electrons radiate radio frequency energy. The effect is not very efficient. Eventually the electrons hit one of the electrodes, so the number in the circulating state at any given time is a small percentage of the overall current. It was also noticed that the frequency of the radiation depends on the size of the tube, and even early examples were built that produced signals in the microwave region.\n\nEarly conventional tube systems were limited to the high frequency bands, and although very high frequency systems became widely available in the late 1930s, the ultra high frequency and microwave regions were well beyond the ability of conventional circuits. The magnetron was one of the few devices able to generate signals in the microwave band and it was the only one that was able to produce high power at centimeter wavelengths.\n\nThe original magnetron was very difficult to keep operating at the critical value, and even then the number of electrons in the circling state at any time was fairly low. This meant that it produced very low-power signals. Nevertheless, as one of the few devices known to create microwaves, interest in the device and potential improvements was widespread.\n\nThe first major improvement was the split-anode magnetron, also known as a negative-resistance magnetron. As the name implies, this design used an anode that was split in two—one at each end of the tube—creating two half-cylinders. When both were charged to the same voltage the system worked like the original model. But by slightly altering the voltage of the two plates, the electron's trajectory could be modified so that they would naturally travel towards the lower voltage side. The plates were connected to an oscillator that reversed the relative voltage of the two plates at a given frequency.\n\nAt any given instant, the electron will naturally be pushed towards the lower-voltage side of the tube. The electron will then oscillate back and forth as the voltage changes. At the same time, a strong magnetic field is applied, stronger than the critical value in the original design. This would normally cause the electron to circle back to the cathode, but due to the oscillating electrical field, the electron instead follows a looping path that continues toward the anodes.\n\nSince all of the electrons in the flow experienced this looping motion, the amount of RF energy being radiated was greatly improved. And as the motion occurred at any field level beyond the critical value, it was no longer necessary to carefully tune the fields and voltages, and the overall stability of the device was greatly improved. Unfortunately, the higher field also meant that electrons often circled back to the cathode, depositing their energy on it and causing it to heat up. As this normally causes more electrons to be released, it could sometimes lead to a runaway effect, damaging the device.\n\nThe great advance in magnetron design was the resonant cavity magnetron or electron-resonance magnetron, which works on entirely different principles. In this design the oscillation is created by the physical shaping of the anode, rather than external circuits or fields.\n\nMechanically, the cavity magnetron consists of a large, solid cylinder of metal with a hole drilled through the center of the circular face. A wire acting as the cathode is run down the center of this hole, and the metal block itself forms the anode. Around this hole, known as the \"interaction space\", are a number of similar holes (\"resonators\") drilled parallel to the interaction space, separated only a very short distance away. A small slot is cut between the interaction space and each of these resonators. The resulting block looks something like the cylinder on a revolver, with a somewhat larger central hole. (Early models were actually cut using Colt pistol jigs.) The parallel sides of the slots act as a capacitor while the anode block itself provides an inductor analog. Thus, each cavity forms its own resonant circuit, the frequency of which is defined by the energy of the electrons and the physical dimensions of the cavity.\n\nThe magnetic field is set to a value well below the critical, so the electrons follow arcing paths towards the anode. When they strike the anode, they cause it to become negatively charged in that region. As this process is random, some areas will become more or less charged than the areas around them. The anode is constructed of a highly conductive material, almost always copper, so these differences in voltage cause currents to appear to even them out. Since the current has to flow around the outside of the cavity, this process takes time. During that time additional electrons will avoid the hot spots and be deposited further along the anode, as the additional current flowing around it arrives too. This causes an oscillating current to form as the current tries to equalize one spot, then another.\n\nThe oscillating currents flowing around the cavities, and their effect on the electron flow within the tube, causes large amounts of microwave radiofrequency energy to be generated in the cavities. The cavities are open on one end, so the entire mechanism forms a single, larger, microwave oscillator. A \"tap\", normally a wire formed into a loop, extracts microwave energy from one of the cavities. In some systems the tap wire is replaced by an open hole, which allows the microwaves to flow into a waveguide.\n\nAs the oscillation takes some time to set up, and is inherently random at the start, subsequent startups will have different output parameters. Phase is almost never preserved, which makes the magnetron difficult to use in phased array systems. Frequency also drifts from pulse to pulse, a more difficult problem for a wider array of radar systems. Neither of these present a problem for continuous-wave radars, nor for microwave ovens.\n\nAll cavity magnetrons consist of a heated cathode placed at a high (continuous or pulsed) negative potential created by a high-voltage, direct-current power supply. The cathode is placed in the center of an evacuated, lobed, circular chamber. A magnetic field parallel to the filament is imposed by a permanent magnet. The magnetic field causes the electrons, attracted to the (relatively) positive outer part of the chamber, to spiral outward in a circular path, a consequence of the Lorentz force. Spaced around the rim of the chamber are cylindrical cavities. Slots are cut along the length of the cavities that open into the central, common cavity space. As electrons sweep past these slots, they induce a high-frequency radio field in each resonant cavity, which in turn causes the electrons to bunch into groups. (This principle of cavity resonator is very similar to blowing a stream of air across the open top of a glass pop bottle.) A portion of the radio frequency energy is extracted by a short antenna that is connected to a waveguide (a metal tube, usually of rectangular cross section). The waveguide directs the extracted RF energy to the load, which may be a cooking chamber in a microwave oven or a high-gain antenna in the case of radar.\n\nThe sizes of the cavities determine the resonant frequency, and thereby the frequency of the emitted microwaves. However, the frequency is not precisely controllable. The operating frequency varies with changes in load impedance, with changes in the supply current, and with the temperature of the tube. This is not a problem in uses such as heating, or in some forms of radar where the receiver can be synchronized with an imprecise magnetron frequency. Where precise frequencies are needed, other devices, such as the klystron are used.\n\nThe magnetron is a self-oscillating device requiring no external elements other than a power supply. A well-defined threshold anode voltage must be applied before oscillation will build up; this voltage is a function of the dimensions of the resonant cavity, and the applied magnetic field. In pulsed applications there is a delay of several cycles before the oscillator achieves full peak power, and the build-up of anode voltage must be coordinated with the build-up of oscillator output.\n\nWhere there are an even number of cavities, two concentric rings can connect alternate cavity walls to prevent inefficient modes of oscillation. This is called pi-strapping because the two straps lock the phase difference between adjacent cavities at pi radians (180°).\n\nThe modern magnetron is a fairly efficient device. In a microwave oven, for instance, a 1.1-kilowatt input will generally create about 700 watts of microwave power, an efficiency of around 65%. (The high-voltage and the properties of the cathode determine the power of a magnetron.) Large S band magnetrons can produce up to 2.5 megawatts peak power with an average power of 3.75 kW. Some large magnetrons are water cooled. The magnetron remains in widespread use in roles which require high power, but where precise control over frequency and phase is unimportant.\n\nIn a radar set, the magnetron's waveguide is connected to an antenna. The magnetron is operated with very short pulses of applied voltage, resulting in a short pulse of high power microwave energy being radiated. As in all primary radar systems, the radiation reflected off a target is analyzed to produce a radar map on a screen.\n\nSeveral characteristics of the magnetron's output make radar use of the device somewhat problematic. The first of these factors is the magnetron's inherent instability in its transmitter frequency. This instability results not only in frequency shifts from one pulse to the next, but also a frequency shift within an individual transmitted pulse. The second factor is that the energy of the transmitted pulse is spread over a relatively wide frequency spectrum, which requires the receiver to have a correspondingly wide bandwidth. This wide bandwidth allows ambient electrical noise to be accepted into the receiver, thus obscuring somewhat the weak radar echoes, thereby reducing overall receiver signal-to-noise ratio and thus performance. The third factor, depending on application, is the radiation hazard caused by the use of high power electromagnetic radiation. In some applications, for example a marine radar mounted on a recreational vessel, a radar with a magnetron output of 2 to 4 kilowatts is often found mounted very near an area occupied by crew or passengers. In practical use these factors have been overcome, or merely accepted, and there are today thousands of magnetron aviation and marine radar units in service. Recent advances in aviation weather avoidance radar and in marine radar have successfully replaced the magnetron with semiconductor microwave oscillators, which have a narrower output frequency range. These allow a narrower receiver bandwidth to be used, and the higher signal to noise ratio in turn allows a lower transmitter power, reducing exposure to EMR.\n\nIn microwave ovens, the waveguide leads to a radio frequency-transparent port into the cooking chamber. As the fixed dimensions of the chamber, and its physical closeness to the magnetron, would normally create standing wave patterns in the chamber, the pattern is randomized by a motorized fan-like \"stirrer\" in the waveguide (more often in commercial ovens), or by a turntable that rotates the food (most common in non-commercial ovens).\n\nIn microwave-excited lighting systems, such as a sulfur lamp, a magnetron provides the microwave field that is passed through a waveguide to the lighting cavity containing the light-emitting substance (e.g., sulfur, metal halides, etc.). Although efficient, these lamps are much more complex than other methods of lighting and therefore not commonly used.\nMore modern variants use HEMTs or GaN-on-SiC power semiconductors to generate the microwaves, which are substantially less complex and can be adjusted to maximize light output using a PID system.\n\nIn 1910 Hans Gerdien of the Siemens corporation invented a magnetron. In 1912, Swiss physicist Heinrich Greinacher was looking for new ways to calculate the electron mass. He settled on a system consisting of a diode with a cylindrical anode surrounding a rod-shaped cathode, placed in the middle of a magnet. The attempt to measure the electron mass failed because he was unable to achieve a good vacuum in the tube. However, as part of this work, Greinacher developed mathematical models of the motion of the electrons in the crossed magnetic and electric fields.\n\nIn the US, Albert Hull put this work to use in an attempt to bypass Western Electric's patents on the triode. Western Electric had gained control of this design by buying Lee De Forest's patents on the control of current flow using electric fields via the \"grid\". Hull intended to use a variable magnetic field, instead of an electrostatic one, to control the flow of the electrons from the cathode to the anode. Working at General Electric's Research Laboratories in Schenectady, New York, Hull built tubes that provided switching through the control of the ratio of the magnetic and electric field strengths. He released several papers and patents on the concept in 1921.\n\nHull's magnetron was not originally intended to generate VHF (very-high-frequency) electromagnetic waves. However, in 1924, Czech physicist August Žáček (1886–1961) and German physicist Erich Habann (1892–1968) independently discovered that the magnetron could generate waves of 100 megahertz to 1 gigahertz. Žáček, a professor at Prague's Charles University, published first; however, he published in a journal with a small circulation and thus attracted little attention. Habann, a student at the University of Jena, investigated the magnetron for his doctoral dissertation of 1924. Throughout the 1920s, Hull and other researchers around the world worked to develop the magnetron. Most of these early magnetrons were glass vacuum tubes with multiple anodes. However, the two-pole magnetron, also known as a split-anode magnetron, had relatively low efficiency.\n\nWhile radar was being developed during World War II, there arose an urgent need for a high-power microwave generator that worked at shorter wavelengths, around 10 cm (3 GHz), rather than the 50 to 150 cm (200 MHz) that was available from tube-based generators of the time. It was known that a multi-cavity resonant magnetron had been developed and patented in 1935 by Hans Hollmann in Berlin. However, the German military considered the frequency drift of Hollman's device to be undesirable, and based their radar systems on the klystron instead. But klystrons could not at that time achieve the high power output that magnetrons eventually reached. This was one reason that German night fighter radars — which never strayed beyond the low-UHF band to start with for front-line aircraft — were not a match for their British counterparts. Likewise, in the UK, Albert Beaumont Wood detailed a system with \"six or eight small holes\" drilled in a metal block, identical to later production designs. However, his idea was rejected by the Navy, who said their valve department was far too busy to consider it.\n\nIn 1940, at the University of Birmingham in the UK, John Randall and Harry Boot produced a working prototype of a cavity magnetron that produced about 400 W. Within a week this had improved to 1 kW, and within the next few months, with the addition of water cooling and many detail changes, this had improved to 10 and then 25 kW. To deal with its drifting frequency, they sampled the output signal and synchronized their receiver to whatever frequency was actually being generated. In 1941, the problem of frequency instability was solved by James Sayers coupling (\"strapping\") alternate cavities within the magnetron which reduced the instability by a factor of 5-6. (For an overview of early magnetron designs, including that of Boot and Randall, see ) According to Andy Manning from the RAF Air Defence Radar Museum, Randall and Boot's discovery was \"a massive, massive breakthrough\" and \"deemed by many, even now, to be the most important invention that came out of the Second World War\", while professor of military history at the University of Victoria in British Columbia, David Zimmerman, states:\n\nBecause France had just fallen to the Nazis and Britain had no money to develop the magnetron on a massive scale, Winston Churchill agreed that Sir Henry Tizard should offer the magnetron to the Americans in exchange for their financial and industrial help. An early 10 kW version, built in England by the General Electric Company Research Laboratories, Wembley, London (not to be confused with the similarly named American company General Electric), was taken on the Tizard Mission in September 1940. As the discussion turned to radar, the US Navy representatives began to detail the problems with their short-wavelength systems, complaining that their klystrons could only produce 10 W. With a flourish, \"Taffey\" Bowen pulled out a magnetron and explained it produced 1000 times that. \n\nBell Telephone Laboratories took the example and quickly began making copies, and before the end of 1940, the Radiation Laboratory had been set up on the campus of the Massachusetts Institute of Technology to develop various types of radar using the magnetron. By early 1941, portable centimetric airborne radars were being tested in American and British aircraft. In late 1941, the Telecommunications Research Establishment in Great Britain used the magnetron to develop a revolutionary airborne, ground-mapping radar codenamed H2S. The H2S radar was in part developed by Alan Blumlein and Bernard Lovell.\n\nThe cavity magnetron was widely used during World War II in microwave radar equipment and is often credited with giving Allied radar a considerable performance advantage over German and Japanese radars, thus directly influencing the outcome of the war. It was later described by American historian James Phinney Baxter III as \"[t]he most valuable cargo ever brought to our shores\".\n\nCentimetric radar, made possible by the cavity magnetron, allowed for the detection of much smaller objects and the use of much smaller antennas. The combination of small-cavity magnetrons, small antennas, and high resolution allowed small, high quality radars to be installed in aircraft. They could be used by maritime patrol aircraft to detect objects as small as a submarine periscope, which allowed aircraft to attack and destroy submerged submarines which had previously been undetectable from the air. Centimetric contour mapping radars like H2S improved the accuracy of Allied bombers used in the strategic bombing campaign, despite the existence of the German FuG 350 \"Naxos\" device to specifically detect it. Centimetric gun-laying radars were likewise far more accurate than the older technology. They made the big-gunned Allied battleships more deadly and, along with the newly developed proximity fuze, made anti-aircraft guns much more dangerous to attacking aircraft. The two coupled together and used by anti-aircraft batteries, placed along the flight path of German V-1 flying bombs on their way to London, are credited with destroying many of the flying bombs before they reached their target.\n\nSince then, many millions of cavity magnetrons have been manufactured; while some have been for radar the vast majority have been for microwave ovens. The use in radar itself has dwindled to some extent, as more accurate signals have generally been needed and developers have moved to klystron and traveling-wave tube systems for these needs.\n\nAt least one hazard in particular is well known and documented. As the lens of the eye has no cooling blood flow, it is particularly prone to overheating when exposed to microwave radiation. This heating can in turn lead to a higher incidence of cataracts in later life. A microwave oven with a warped door or poor microwave sealing can be hazardous.\n\nThere is also a considerable electrical hazard around magnetrons, as they require a high voltage power supply.\n\nSome magnetrons have beryllium oxide (beryllia) ceramic insulators, which are dangerous if crushed and inhaled, or otherwise ingested. Single or chronic exposure can lead to berylliosis, an incurable lung condition. In addition, beryllia is listed as a confirmed human carcinogen by the IARC; therefore, broken ceramic insulators or magnetrons should not be directly handled.\n\nAll magnetrons contain a small amount of thorium mixed with tungsten in their filament. While this is a radioactive metal, the risk of cancer is low as it never gets airborne in normal usage. Only if the filament is taken out of the magnetron, finely crushed, and inhaled can it pose a health hazard.\n\n\n"}
{"id": "38810286", "url": "https://en.wikipedia.org/wiki?curid=38810286", "title": "Corner tube boiler", "text": "Corner tube boiler\n\nA cornertube boiler is a type of natural circulation water-tube boiler which differentiates itself from other water tube boilers by its characteristic water-steam cycle and a pre-separation of heated steam from the steam-water mixture occurs outside the drum and the unheated downcomers.\n\nCornertube boilers were developed for small steam output. The design was based around two factors that, along with excellent water circulation, should be appropriate cooling even at light loads. Its special feature is its Monocoque body i.e. the unheated downcomers form the supporting frame work and not the thermally loaded tubes, hence the name \"corner tube boiler\".Moreover, the piping-arrangement (system) is also responsible; to manage the riser tubes and water distribution in riser tubes and down comers and to collect the steam water mixture and to make a certain amount of pre-separation of steam and water mixture. Put simply, the water circulation takes place simultaneously through the drum and through the unheated down comers outside the drum.\n\nDuring World War II a shortage of fuels like gas and petrol alongside the idea of running diesel engines with Steam occurred and led to the development of a new type of boiler. Dr. Henrich Vorkauf came up with the first design of a new natural circulation boiler which was then installed into a truck in the year 1944.\nUsing this principle Dr. Vorkauf developed a single drum boiler with downcomers in four corners and named this boiler Eckrohrkessel (German name). Corner Tube Boiler is word to word translation of Eckrohrkessel, (Eck=Corner, rohr=tube and kessel=boiler)\n\nThe water flows down from the drum (6) through the down comers (7) and it is distributed in the different riser tubes(4). The steam-water mixture circulates and flows in the upward direction through the riser tubes. In the radiation heated area through the pre-separator (also known as cross-collector) (3),occurs the pre-separation of steam from the steam water mixture. The separated steam flows through the overhead pipe (5) and the steam-water mixture flows through the collector pipe (3) to the drum (6) as well.In the drum occurs the final separation of steam from steam water mixture. The rest amount of water flows through the unheated return tubes (1) and downcomers (7) to the rear wall distributor/header (2). Due to the water returning from the unheated downcomers (1) a lively circulation takes place .\n\n\n\nThe various fuels which can be used or are commonly used in the USA, Europe and Pacific Countries are Bagasse, Biomass, Lignite, Coal, Scaly Bark, Fuel gas, Industrial waste, khuff gas, MFO(marine fuel oil), Organic matter, Oil, Litter, Rice hulls, Rubberwood, Sludge, Wood, Woodchips.\n"}
{"id": "21316814", "url": "https://en.wikipedia.org/wiki?curid=21316814", "title": "Earth System Governance Project", "text": "Earth System Governance Project\n\nThe Earth System Governance Project is a long-term, interdisciplinary social science research programme originally developed under the auspices of the International Human Dimensions Programme on Global Environmental Change. It started in January 2009.\n\nThe Earth System Governance Project currently consists of a network of ca. 300 active and about 2,300 indirectly involved scholars from all continents. The project has evolved into the largest social science research network in the area of governance and global environmental change. The Earth System Governance Project Office is hosted at Lund University, Sweden.\n\nThe Earth System Governance Project aims to contribute to science on the large, complex challenges of governance in an era of rapid and large-scale environmental change. The project seeks to create a better understanding of the role of institutions, organizations and governance mechanisms by which humans regulate their relationship with the natural environment. The Earth System Governance Project aims to integrate governance research at all levels. The project aims to examine problems of the ‘global commons’, but also local problems from air pollution to the preservation of waters, waste treatment or desertification and soil degradation. However, due to natural interdependencies local environmental pollution can be transformed into changes of the global system that affect other localities. Therefore, the Earth System Governance Project looks at institutions and governance processes both local and globally.\n\nThe Earth System Governance Project is a scientific effort, but also aims to assist policy responses to the pressing problems of earth system transformation \n\nThe Earth System Governance Project organizes its research according to a conceptual framework guided by five analytical problems. These are the problems of the overall \"architecture\" of earth system governance, of \"agency\" beyond the state and of the state, of the \"adaptiveness\" of governance mechanisms and processes, of their \"accountability\" and legitimacy and of modes of \"allocation and access\" in earth system governance.\n\nThe concept of Earth System Governance is defined as: \n\nIn 2001, the four then active global change research programmes (DIVERSITAS, International Geosphere-Biosphere Programme, World Climate Research Programme, and International Human Dimensions Programme on Global Environmental Change) agreed to intensify co-operation through setting up an overarching Earth System Science Partnership. The research communities represented in this Partnership contend in the 2001 Amsterdam Declaration on Global Change that the earth system now operates ‘well outside the normal state exhibited over the past 500,000 years’ and that ‘human activity is generating change that extends well beyond natural variability—in some cases, alarmingly so— and at rates that continue to accelerate.’ To cope with this challenge, the four global change research programmes have called ‘urgently’ for strategies for Earth System management’.\n\nIn March 2007, in response to the 2001 Amsterdam Declaration, the Scientific Committee of the International Human Dimensions Programme on Global Environmental Change (IHDP), the overarching social science programme in the field, mandated the drafting of the Science Plan of the Earth System Governance Project by a newly appointed Scientific Planning Committee. The Earth System Governance Project builds on the results of an earlier long-term research programme, the IHDP core project Institutional Dimensions of Global Environmental Change (IDGEC). In 2008, the Earth System Governance Project was officially launched. \n\nIn 2009, the Science and Implementation Plan of the Earth System Governance Project was published. In the science and implementation plan, the conceptual problems, cross-cutting themes, flagship projects, and its policy relevance are outlined in detail. The Science Plan was written by an international, interdisciplinary Scientific Planning Committee chaired by Prof. Frank Biermann, which drew on a consultative process that started in 2004. Several working drafts of this Science Plan have been presented and discussed at a series of international events and conferences, and numerous scholars in the field, as well as practitioners, have offered suggestions, advice, and critique.\n\nSince then, the project has evolved into a broader research alliance that builds on an international network of research centers, lead faculty and research fellows. After the termination of the IHDP in 2014, the activities of the Earth System Governance research alliance are supported by an international steering group of representatives of the main Earth System Governance Research Centres and the global group of lead faculty and research fellows.\n\nFor its activities and implementation, the Earth System Governance Project relies on a global network of experts from different academic and cultural backgrounds. The research network consists of different groups of scientific experts. The Earth System Governance Project operates under the direction of a \"Scientific Steering Group\" chaired by Frank Biermann. The role of the Scientific Steering Committee is to guide the implementation of the Earth System Governance Science Plan. The \"Lead Faculty\" of the Earth System Governance Project is a group of individual scientists who take over (shared) responsibility for the development of research on particular analytical problems. \"Earth System Governance Fellows\" are scientists who link their own research projects with the broader themes and questions raised by the Earth System Governance Science and Implementation Plan.\n\nAn important element in the project organisation is the global alliance of research centres that brings together the VU University Amsterdam; the Australian National University; Chiang Mai University; Colorado State University; Lund University; University of East Anglia; University of Oldenburg; the Stockholm Resilience Centre; the University of Toronto; the Tokyo Institute of Technology and Yale University. In addition, strong networks on earth system governance research exist in China, Latin America, Central and Eastern Europe, and Russia.\n\nSince 2007, the Project has organized major scientific conferences addressing the topics of governance and global environmental change, including:\n\n\nThe network of researchers affiliated with the Earth System Governance Project has brought out many reports and books, and has published in journals such as International Environmental Agreements: Politics, Law and Economics; Ecological Economics; Global Environmental Change; Environmental Science & Policy Global Environmental Politics and Current Opinion in Environmental Sustainability Recurring research topics of the Earth System Governance Project are water governance, climate governance and fragmentation of global environmental governance.\n\nA related MIT Press Book series is designed to address the research challenge of earth system governance. Additionally, the Project publishes regular Working Papers, which are peer-reviewed online publications that broadly address questions raised by the Project’s Science and Implementation Plan.\n\nSeveral special issues of topics related to earth system governance have been published in scientific journals over the last years.\n\nEarth system governance as a research object is quickly emerging, and as a consequence, the number of education programmes on bachelor, master and doctoral level related to earth system governance steadily increases. A number of institutes and universities currently collaborate in a Global Alliance of Earth System Governance Research Centres, including:\n\n\nA substantial number of the workshops and other events of the project are capacity-building activities. The project also organizes, endorses and provides teaching to summer schools and capacity building events and programs. In addition, members of the Scientific Steering Group and staff of the International Project Office give guest lectures around the world.\n\nThe Earth System Governance Project organizes Task Forces, international networks of senior and early career scholars with a series of working groups focused on particular ideas or idea clusters. There are currently two active Task Forces:\n\nThis Task Force aims to explore key concepts with regard to Earth System Governance, such as planetary boundaries, green economy, resilience and the Anthropocene. It aims to critically examine and further refine these novel governance ideas. \n\nThis Task Force seeks aims to advance quantitative earth system governance research by promoting new international research collaborations, fostering interaction and dialogue among existing research projects, and developing architectures to promote the building and sharing of datasets.\n\nIn 2011, the Earth System Governance Project launched an initiative on International Environmental Governance. This initiative aims to provide a forum for discussion of current and ongoing research on international environmental governance and the institutional framework for sustainable development, in the period leading up to the 2012 United Nations Conference on Sustainable Development in Rio de Janeiro, also known as ‘Rio + 20’. In addition, the initiative aims to target decision-makers and to contribute not just to a better understanding but also to actual improvements in international environmental governance towards an institutional framework that enables sustainable development.\n\nThere is widespread support for the Earth System Governance Project in the scientific community, which is reflected in the size of the research network and in various publications by experts. However, criticisms of the Earth System Governance Project have also been made. \n\nIn an internal report of the International Human Dimensions Programme it is stated that the steering group of the Earth System Governance Project is too much dominated by experts from OECD countries. Since then, the Earth System Governance Project has actively sought ways to involve experts from different regions of the world. \n\nThe idea of earth system governance has also been criticized for being too top-down, for placing too much emphasis on global governance structures. According to Mike Hulme, earth system governance represents an attempt to ‘geopolitically engineer’ our way out of the climate crisis. He questions whether the climate is governable and argues that it is way too optimistic and even hubristic to attempt to control the global climate by universal governance regimes. This interpretation of the novel concept, however, has been rejected by other scholars as being too narrow and misleading \n\n\n\n"}
{"id": "14049066", "url": "https://en.wikipedia.org/wiki?curid=14049066", "title": "Energy in Mexico", "text": "Energy in Mexico\n\nEnergy in Mexico describes energy and electricity production, consumption and import in Mexico. Electricity sector in Mexico is the main article of electricity in Mexico.\n\nIn 2008, Mexico produced 234 TWh, from which 86 TWh are coming from thermal plant, 39 TWh from hydro-power, 18 TWh from coal, 9,8 TWh from nuclear power, 7 TWh from geothermal power and 0,255 TWh from wind power. Mexico is among the top oil producers and exporters in the world.\n\nPresident Lázaro Cárdenas expropriated foreign oil companies in the late 1930s. Since then, Pemex, the national company produces oil in Mexico. Main fields are Chicontepec Field, Cantarell Field and Ku-Maloob-Zaap. In 2008, oil production has declined 9,2% to 3,96 millions barils a day while natural gas production increased 14,2% to 6,92 cubic feet.\n\nAccording to IEA Mexico was one of the top oil producers in 2009. Top oil producers in 2009 were (Mt): Russia 494 Mt (13%), Saudi Arabia 452 Mt (12%), US 320 Mt (8%), Iran 206 Mt (5%), China 194 Mt (5%), Canada 152 Mt (4%), Mexico 146 Mt (4%), Venezuela 126 Mt (3%), Kuwait 124 Mt (3%) and United Arab Emirates 120 Mt (3%).\n\nMexico has the third greatest geothermal energy production with an installed capacity of 959.50 MW by December 2007. This represents 3.24% of the total electricity generated in the country. Mexico is also home to the largest geothermal power stations in the world, the Cerro Prieto Geothermal Power Station.\n\n"}
{"id": "5213545", "url": "https://en.wikipedia.org/wiki?curid=5213545", "title": "Energy in the United Kingdom", "text": "Energy in the United Kingdom\n\nEnergy use in the United Kingdom stood at 2,249 TWh (193.4 million tonnes of oil equivalent) in 2014. This equates to energy consumption \"per capita\" of 34.82 MWh (3.00 tonnes of oil equivalent) compared to a 2010 world average of 21.54 MWh (1.85 tonnes of oil equivalent). Demand for electricity in 2014 was 34.42GW on average (301.7TWh over the year) coming from a total electricity generation of 335.0TWh.\n\nSuccessive UK governments have outlined numerous commitments to reduce carbon dioxide emissions. One such announcement was the Low Carbon Transition Plan launched by the Brown ministry in July 2009, which aimed to generate 30% electricity from renewable sources, and 40% from low carbon content fuels by 2020. Notably, the UK is one of the best sites in Europe for wind energy, and wind power production is its fastest growing supply. Wind power contributed 15% of UK electricity generation in 2017.\n\nGovernment commitments to reduce emissions are occurring against a backdrop of economic crisis across Europe. During the European financial crisis, Europe's consumption of electricity shrank by 5%, with primary production also facing a noticeable decline. Britain's trade deficit was reduced by 8% due to substantial cuts in energy imports. Between 2007 and 2015, the UK's peak electrical demand fell from 61.5 GW to 52.7.GW.\n\nUK government energy policy aims to play a key role in limiting greenhouse gas emissions, whilst meeting energy demand. Shifting availabilities of resources and development of technologies also change the country's energy mix through changes in costs. In 2016, the United Kingdom was ranked 12th in the World on the Environmental Performance Index, which measures how well a country carries through environmental policy.\nAs recently as 2004, the UK was a net exporter of energy, however by 2010, more than 25% of UK energy was imported.\n\nScotland cut climate emissions by around 46% between 1990 and 2014. Scotland aims to have a carbon-free electricity sector based on renewable energy sources by 2032. Scotland also aims to repair 250,000 hectares of degraded peatlands, which store a total of 1.7 gigatonnes of CO2. \n\nDuring 2008, the total energy consumed in the United Kingdom was 234.4 million tonnes of oil equivalent (which is around 9.85 EJ = 9.85×10J).\n\nConcerns over peak oil have been raised by high-profile voices in the United Kingdom such as Sir David King and the Industry Task-Force on Peak Oil and Energy Security. The latter's 2010 report states that \"The next five years will see us face another crunch - the oil crunch. This time, we do have the chance to prepare. The challenge is to use that time well.\" (Sir Richard Branson and Ian Marchant).\n\nUnited Kingdom produced 60% of its consumed natural gas in 2010. In five years the United Kingdom moved from almost gas self-sufficient (see North Sea Gas) to 40% gas import in 2010. Gas was almost 40% of total primary energy supply (TPES) and electricity more than 45% in 2010. Underground storage was about 5% of annual demand and more than 10% of net imports. There is an alternative fuel obligation in the United Kingdom. (see Renewable Transport Fuel Obligation)\n\nGasfields include Amethyst gasfield, Armada gasfield, Easington Catchment Area, East Knapton, Everest gasfield and Rhum gasfield.\n\nA gas leak occurred in March 2012 at the Elgin-Franklin fields, where about 200,000 cubic metres of gas was escaping every day. Total missed out on about £83m of potential income.\n\nWith the development of the national grid, the switch to using electricity, United Kingdom electricity consumption increased by around 150% between the post war nationalisation of the industry in 1948 and the mid-1960s. During the 1960s growth slowed as the market became saturated. The United Kingdom is planning to reform its electricity market. It plans to introduce a capacity mechanism and contracts for difference to encourage the building of new generation.\n\nDuring the 1940s some 90% of the generating capacity was fired by coal, with oil providing most of the remainder.\nThe United Kingdom started to develop a nuclear generating capacity in the 1950s, with Calder Hall being connected to the grid on 27 August 1956. Though the production of weapons-grade plutonium was the main reason behind this power station, other civil stations followed, and 26% of the nation's electricity was generated from nuclear power at its peak in 1997.\n\nDespite the flow of North Sea oil from the mid-1970s, oil fuelled generation remained relatively small and continued to decline.\n\nStarting in 1993, and continuing through the 1990s, a combination of factors led to a so-called Dash for Gas, during which the use of coal was scaled back in favour of gas-fuelled generation. This was sparked by the privatisation of the National Coal Board, British Gas and the Central Electricity Generating Board; the introduction of laws facilitating competition within the energy markets; and the availability of cheap gas from the North Sea. In 1990 just 1.09% of all gas consumed in the country was used in electricity generation; by 2004 the figure was 30.25%.\n\nBy 2004, coal use in power stations had fallen to 50.5 million tonnes, representing 82.4% of all coal used in 2004 (a fall of 43.6% compared to 1980 levels), though up slightly from its low in 1999. On several occasions in May 2016, Britain burned no coal for electricity for the first time since 1882. On 21 April 2017, Britain went a full day without using coal power for the first time since the Industrial Revolution, according to the National Grid.\n\nFrom the mid-1990s new renewable energy sources began to contribute to the electricity generated, adding to a small hydroelectricity generating capacity.\n\nIn 2016, total electricity production stood at 357 TWh (down from a peak of 385 TWh in 2005), generated from the following sources:\n\n\n\nThe UK Government energy policy had targeted a total contribution from renewables to achieve 10% by 2010, but it was not until 2012 that this figure was exceeded; renewable energy sources supplied 11.3% (41.3 TWh) of the electricity generated in the United Kingdom in 2012.\nThe Scottish Government has a target of generating 17% to 18% of Scotland's electricity from renewables by 2010, rising to 40% by 2020.\n\nIn the early years of the 2000s, concerns grew over the prospect of an 'energy gap' in United Kingdom generating capacity. This was forecast to arise because it was expected that a number of coal fired power stations would close due to being unable to meet the clean air requirements of the European Large Combustion Plant Directive (directive 2001/80/EC). In addition, the United Kingdom's remaining Magnox nuclear stations were to have closed by 2015. The oldest AGR nuclear power station has had its life extended by ten years, and it was likely many of the others could be life-extended, reducing the potential gap suggested by the current accounting closure dates of between 2014 and 2023 for the AGR power stations.\n\nA report from the industry in 2005 forecast that, without action to fill the gap, there would be a 20% shortfall in electricity generation capacity by 2015. Similar concerns were raised by a report published in 2000 by the Royal Commission on Environmental Pollution \"(Energy - The Changing Climate\"). The 2006 Energy Review attracted considerable press coverage - in particular in relation to the prospect of constructing a new generation of nuclear power stations, in order to prevent the rise in carbon dioxide emissions that would arise if other conventional power stations were to be built.\n\nAmong the public, according to a November 2005 poll conducted by YouGov for Deloitte, 35% of the population expect that by 2020 the majority of electricity generation will come from renewable energy (more than double the government's target, and far larger than the 5.5% generated as of 2008), 23% expect that the majority will come from nuclear power, and only 18% that the majority will come from fossil fuels. 92% thought the Government should do more to explore alternative power generation technologies to reduce carbon emissions.\n\nIn June 2013, the industry regulator Ofgem warned that the UK's energy sector faces \"unprecedented challenges\" and that \"spare electricity power production capacity could fall to 2% by 2015, increasing the risk of blackouts\". Proposed solutions \"could include negotiating with major power users for them to reduce demand during peak times in return for payment\".\n\nThe first move to plug the United Kingdom's projected energy gap was the construction of the conventionally gas-fired Langage Power Station and Marchwood Power Station which became operational in 2010.\n\nIn 2007, proposals for the construction of two new coal-fired power stations were announced, in Tilbury, Essex and in Kingsnorth, Kent. They were to be the first coal-fired stations to be built in the United Kingdom in 20 years. No power station at Tilbury was built and the plans for a new Kingsnorth coal powered station were later shelved.\n\nBeyond these new plants, there were a number of options that might have been used to provide the new generating capacity, while minimising carbon emissions and producing less residues and contamination. Fossil fuel power plants might provide a solution if there was a satisfactory and economical way of reducing their carbon emissions. Carbon capture might provide a way of doing this; however the technology is relatively untried and costs are relatively high. As of 2006 there were no power plants in operation with a full carbon capture and storage system, and as of 2018 the situation is little better.\n\nHowever, due to reducing demand in the late-2000s recession removing any medium term gap, and high gas prices, in 2011 and 2012 over 2 GW of older, less efficient, gas generation plant was mothballed. In 2011 electricity demand dropped 4%, and about 6.5 GW of additional gas-fired capacity was added over 2011 and 2012. Early in 2012 the reserve margin stood at the high level of 32%.\n\nAn important factor in reduced electrical demand in recent years has come from the phasing out of incandescent light bulbs and a switch to compact fluorescent and LED lighting. Research by the University of Oxford has shown that the average annual electrical consumption for lighting in a UK home fell from 720 kWh in 1997 to 508 kWh in 2012. Between 2007 and 2015, the UK's peak electrical demand fell from 61.5 GW to 52.7.GW.\n\nWhile in some ways limited by which powers are devolved, the four countries of the United Kingdom have different energy mixes and ambitions. Scotland currently has a target of 80% of electricity from renewables by 2020, which was increased from an original ambition of 50% by 2020 after it exceeded its interim target of 31 per cent by 2011. It has a quarter of the EU's estimated offshore wind potential, and is at the forefront of testing various marine energy systems.\n\nBritain's fleet of operational reactors consists of 14 advanced gas-cooled reactors on six discrete sites, along with two Magnox units at Wylfa, and one PWR unit at Sizewell B. Overall, the installed nuclear capacity in the United Kingdom is between 10 and 11 GW. In addition, the UK experimented with Fast Breeder reactor technologies at Dounreay in Scotland, however the last fast breeder (with 250MWe of capacity) was shut down in 1994.\n\nWhile nuclear power does not produce significant carbon dioxide in generation (though the construction, mining, waste handling and disposal, and decommissioning do generate some carbon emissions), it raises other environmental and security concerns. Despite this, it has enormous potential for generating electricity, when it is taken into consideration that uranium could last up to a hundred years. However, even with changes to the planning system to speed applications, there are doubts over whether the necessary timescale could be met, and over the financial viability of nuclear power with present oil and gas prices. With no nuclear plants having been constructed since Sizewell B in 1995, there are also likely to be capacity issues within the native nuclear industry. The existing privatised nuclear supplier, British Energy, had been in financial trouble in 2004.\n\nIn October 2010 the coalition British Government gave the go-ahead for the construction of up to eight new nuclear power plants. However, the Scottish Government, with the backing of the Scottish Parliament, has stated that no new nuclear power stations will be constructed in Scotland. In March 2012, E.ON UK and RWE npower announced they would be pulling out of developing new nuclear power plants, placing the future of nuclear power in the United Kingdom in doubt.\n\nThere was an 11% increase in the use of nuclear power in 2011, which helped to bring greenhouse gas emissions down 7% on the previous year.\n\nIn 2007, the United Kingdom Government agreed to an overall European Union target of generating 20% of the European Union's energy supply from renewable sources by 2020. Each European Union member state was given its own allocated target; for the United Kingdom it is 15%. This was formalised in January 2009 with the passage of the EU Renewables Directive. As renewable heat and fuel production in the United Kingdom are at extremely low bases, RenewableUK estimates that this will require 35–40% of the United Kingdom's electricity to be generated from renewable sources by that date, to be met largely by 33–35 GW of installed wind capacity.\n\nThe total of all renewable electricity sources provided for 14.9% of the electricity generated in the United Kingdom in 2013, reaching 53.7 TWh of electricity generated.\n\nIn June 2017 renewables plus nuclear generated more UK power than gas and coal together for the first time. In 2017 new offshore wind power became cheaper than new nuclear power for the first time.\n\nIn December 2007, the United Kingdom Government announced plans for a massive expansion of wind energy production, by conducting a Strategic Environmental Assessment of up to 25 GW worth of wind farm offshore sites in preparation for a new round of development. These proposed sites were in addition to the 8 GW worth of sites already awarded in the 2 earlier rounds of site allocations, Round 1 in 2001 and Round 2 in 2003. Taken together it was estimated that this would result in the construction of over 7,000 offshore wind turbines.\n\nWind power delivers a growing fraction of the energy in the United Kingdom and at the beginning of November 2018, wind power in the United Kingdom consisted of nearly 10,000 wind turbines with a total installed capacity of just over 20 gigawatts: 12,254 megawatts of onshore capacity and 7,897 megawatts of offshore capacity.\n\nAt the end of 2011, there were 230,000 solar power projects in the United Kingdom, with a total installed generating capacity of 750 megawatts (MW). By February 2012 the installed capacity had reached 1,000 MW. Solar power use has increased very rapidly in recent years, albeit from a small base, as a result of reductions in the cost of photovoltaic (PV) panels, and the introduction of a Feed-in tariff (FIT) subsidy in April 2010. In 2012, the government said that 4 million homes across the UK will be powered by the sun within eight years, representing 22,000 MW of installed solar power capacity by 2020.\n\nGas from sewage and landfill (biogas) has already been exploited in some areas. In 2004 it provided 129.3 GW·h (up 690% from 1990 levels), and was the UK's leading renewable energy source, representing 39.4% of all renewable energy produced (including hydro). The UK has committed to a target of 10.3% of renewable energy in transport to comply with the Renewable Energy Directive of the European Union but has not yet implemented legislation to meet this target.\n\nOther biofuels can provide a close-to-carbon-neutral energy source, if locally grown. In South America and Asia, the production of biofuels for export has in some cases resulted in significant ecological damage, including the clearing of rainforest. In 2004 biofuels provided 105.9 GW·h, 38% of it wood. This represented an increase of 500% from 1990.\n\nInvestigations into the exploitation of Geothermal power in the United Kingdom, prompted by the 1973 oil crisis, were abandoned as fuel prices fell. Only one scheme is operational, in Southampton. In 2004 it was announced that a further scheme would be built to heat the UK's first geothermal energy model village near Eastgate, County Durham.\n\nAs of 2012, hydroelectric power stations in the United Kingdom accounted for 1.67 GW of installed electrical generating capacity, being 1.9% of the UK's total generating capacity and 14% of UK's renewable energy generating capacity. Annual electricity production from such schemes is approximately 5,700 GWh, being about 1.5% of the UK's total electricity production.\n\nThere are also pumped-storage power stations in the UK. These power stations are net consumers of electrical energy however they contribute to balancing the grid, which can facilitate renewable generation elsewhere, for example by 'soaking up' surplus renewable output at off-peak times and release the energy when it is required.\n\nCombined heat and power plants, where 'waste' hot water from generating is used for district heating, are also a well tried technology in other parts of Europe. While it heats about 50% of all houses in Denmark, Finland, Poland, Sweden and Slovakia, it currently only plays a small role in the United Kingdom. It has, however, been rising, with total generation standing at 27.9 TWh by 2008. This consisted of 1,439 predominantly gas-fired schemes with a total CHP electrical generating capacity of 5.47 GW, and contributing 7% of the UK's electricity supply. Heat generation utilisation has fallen however from a peak of 65 TWh in 1991 to 49 TWh in 2012.\n\nHistorically, public sector support for energy research and development in the United Kingdom has been provided by a variety of bodies with little co-ordination between them. Problems experienced have included poor continuity of funding, and the availability of funding for certain parts of the research—development—commercialisation process but not others. Levels of public funding have also been low by international standards, and funding by the private sector has also been limited.\n\nResearch in the area of energy is carried out by a number of public and private sector bodies:\n\nThe Engineering and Physical Sciences Research Council funds an energy programme spanning energy and climate change research. It aims to \"develop, embrace and exploit sustainable, low carbon and/or energy efficient technologies and systems\" to enable the United Kingdom \"to meet the Government’s energy and environmental targets by 2020\". Its research includes renewable, conventional, nuclear and fusion electricity supply as well as energy efficiency, fuel poverty and other topics.\n\nSince being established in 2004, the UK Energy Research Centre carries out research into demand reduction, future sources of energy, infrastructure and supply, energy systems, sustainability and materials for advanced energy systems.\n\nThe Energy Technologies Institute, expected to begin operating in 2008, is to 'accelerate the development of secure, reliable and cost-effective low-carbon energy technologies towards commercial deployment'.\n\nIn relation to buildings, the Building Research Establishment carries out some research into energy conservation.\n\nThere is currently international research being conducted into fusion power. The ITER reactor is currently being constructed at Cadarache in France. The United Kingdom contributes towards this project through membership of the European Union. Prior to this, an experimental fusion reactor (the Joint European Torus) had been built at Culham in Oxfordshire.\n\nThe United Kingdom government has instituted several policies intended to promote an increase in efficient energy use. These include the roll out of smart meters, the Green Deal, the CRC Energy Efficiency Scheme, the Energy Savings Opportunity Scheme and Climate Change Agreements.\n\nIn tackling the energy trilemma, saving energy is the cheapest of all measures. The Guardian newspaper reported that in 2012 that by 2050, Germany projects a 25% drop in electricity demand: the UK projects a rise of up to 66%. MP and Secretary of State for Energy and Climate Change Ed Davey pointed out in November 2012 that a modest 10% reduction in 2030 means five fewer power stations and £4bn cut from bills.\n\nThe UK government has implemented measures aimed at cutting the UK's energy use by 11% (around 196TWh) by 2020. If achieved, this improvement would be sufficient to replace 22 UK power stations, while possibly providing a boost to the economy and living standards.\n\nThe Committee on Climate Change publishes an annual progress report in respect to control the climate change in the United Kingdom.\n\n\n"}
{"id": "41481139", "url": "https://en.wikipedia.org/wiki?curid=41481139", "title": "Global surveillance disclosures (1970–2013)", "text": "Global surveillance disclosures (1970–2013)\n\nGlobal surveillance refers to the practice of globalized mass surveillance on entire populations across national borders. Although its existence was first revealed in the 1970s and led legislators to attempt to curb domestic spying by the National Security Agency (NSA), it did not receive sustained public attention until the existence of ECHELON was revealed in the 1980s and confirmed in the 1990s. In 2013 it gained substantial worldwide media attention due to the global surveillance disclosure by Edward Snowden.\n\nIn 1972 NSA analyst Perry Fellwock (under the pseudonym \"Winslow Peck\") introduced the readers of \"Ramparts\" magazine to the NSA and to the UKUSA Agreement. In 1976, a separate article in \"Time Out\" magazine revealed the existence of the GCHQ.\n\nIn 1982 James Bamford's book about the NSA, \"The Puzzle Palace\", was first published. Bamford's second book, \"Body of Secrets: Anatomy of the Ultra-Secret National Security Agency\", was published two decades later.\n\nIn 1988 the ECHELON network was revealed by Margaret Newsham, a Lockheed employee. Newsham told a member of the U.S. Congress that the telephone calls of Strom Thurmond, a Republican U.S. senator, were being collected by the NSA. Congressional investigators determined that \"targeting of U.S. political figures would not occur by accident. but was designed into the system from the start.\"\n\nBy the late 1990s ECHELON was reportedly capable of monitoring up to 90% of all internet traffic. According to the BBC in May 2001, however, \"The US Government still refuses to admit that Echelon even exists.\"\n\nIn the aftermath of the September 11 attacks, William Binney, along with colleagues J. Kirke Wiebe and Edward Loomis and in cooperation with House staffer Diane Roark, asked the U.S. Defense Department to investigate the NSA for allegedly wasting \"millions and millions of dollars\" on Trailblazer, a system intended to analyze data carried on communications networks such as the Internet. Binney was also publicly critical of the NSA for spying on U.S. citizens after the September 11, 2001 attacks. Binney claimed that the NSA had failed to uncover the 9/11 plot despite its massive interception of data.\n\nIn 2001, after the September 11 attacks, MI5 started collecting bulk telephone communications data in the United Kingdom on which telephone numbers called each other and when, authorised the Home Secretary under a little understood general power under the Telecommunications Act 1984 instead of the Regulation of Investigatory Powers Act 2000 would have brought independent oversight and regulation. This was kept secret until announced by the then Home Secretary in 2015.\n\nOn December 16, 2005 \"The New York Times\" published a report under the headline \"Bush Lets U.S. Spy on Callers Without Courts\", which was co-written by Eric Lichtblau and the Pulitzer Prize-winning journalist James Risen. According to \"The Times\", the article's date of publication was delayed for a year (past the next presidential election cycle) because of alleged national security concerns. Russ Tice was later revealed as a major source.\n\nIn 2006 further details of the NSA's domestic surveillance of U.S. citizens was provided by \"USA Today\". The newspaper released a report on May 11, 2006 detailing the NSA's \"massive database\" of phone records collected from \"tens of millions\" of U.S. citizens. According to \"USA Today\", these phone records were provided by several telecom companies such as AT&T, Verizon, and BellSouth. AT&T technician Mark Klein was later revealed as major source, specifically of rooms at network control centers on the internet backbone intercepting and recording all traffic passing through. In 2008 the security analyst Babak Pasdar revealed the existence of the so-called \"Quantico circuit\" that he and his team had set up in 2003. The circuit provided the U.S. federal government with a backdoor into the network of an unnamed wireless provider, which was later independently identified as Verizon.\n\nIn 2007 former Qwest CEO Joseph Nacchio alleged in court and provided supporting documentation that in February 2001 (nearly 7 months prior to the September 11 attacks) that the NSA proposed in a meeting to conduct blanket phone spying. He considered the spying to be illegal and refused to cooperate, and claims that the company was punished by being denied lucrative contracts.\n\nIn 2011 details of the mass surveillance industry were released by WikiLeaks. According to Julian Assange, \"We are in a world now where not only is it theoretically possible to record nearly all telecommunications traffic out of a country, all telephone calls, but where there is an international industry selling the devices now to do it.\"\n\n"}
{"id": "2313489", "url": "https://en.wikipedia.org/wiki?curid=2313489", "title": "HMS Discovery (1774)", "text": "HMS Discovery (1774)\n\nHMS \"Discovery\" was the consort ship of James Cook's third expedition to the Pacific Ocean in 1776–1780. Like Cook's other ships, \"Discovery\" was a Whitby-built collier originally named \"Diligence\" when she was built in 1774. Purchased in 1775, the vessel was measured at 299 tons burthen. Originally a brig, Cook had her changed to a full rigged ship. She was commanded by Charles Clerke, who had previously served on Cook's first two expeditions, and had a complement of 70. When Cook was killed in a skirmish with natives of Hawaii, Clerke transferred to the expedition's flagship HMS \"Resolution\" and John Gore assumed command of \"Discovery\". She returned to Britain under the command of Lieutenant James King, arriving back on 4 October 1780.\n\nAfter returning to the Nore in 1780, \"Discovery\" was fitted out as a transport at Woolwich Dockyard, serving as such between December 1780 and May 1781. She then became a dockyard craft at Woolwich, and was broken up at Chatham Dockyard in October 1797.\n\n\n\n"}
{"id": "9797256", "url": "https://en.wikipedia.org/wiki?curid=9797256", "title": "Hugh Saddler", "text": "Hugh Saddler\n\nHugh Saddler has a degree in science from Adelaide University and a PhD from Cambridge University. He is the author of a book on Australian energy policy, \"Energy in Australia\" and over 50 scientific papers, monographs and articles on energy technology and environmental policy, and is recognised as one of Australia's leading experts in this field.\n\nHugh Saddler is currently a member of the Experts Group on Emissions Trading, appointed by the Australian Greenhouse Office, of the ABS Environmental Statistics Advisory Group, and of the ACT Environment Advisory Committee. In 1998 he was appointed an Adjunct Professor at Murdoch University. He is a Fellow of the Australian Institute of Energy and a member of the International Association for Energy Economics.\n\nHugh Saddler founded the company \"Energy Strategies\" in 1982 and is its Managing Director.\n\n\n\n"}
{"id": "46337367", "url": "https://en.wikipedia.org/wiki?curid=46337367", "title": "Hydraulic fracturing and radionuclides", "text": "Hydraulic fracturing and radionuclides\n\nHydraulic fracturing is the propagation of fractures in a rock layer by pressurized fluid. Induced hydraulic fracturing or hydrofracking, commonly known as fracking, is a technique used to release petroleum, natural gas (including shale gas, tight gas and coal seam gas), or other substances for extraction. Radionuclides are associated with hydraulic fracturing in two main ways. Injection of man-made radioactive tracers, along with the other substances in hydraulic-fracturing fluid, is often used to determine the injection profile and location of fractures created by hydraulic fracturing. In addition, hydraulic fracturing releases naturally occurring heavy metals and radioactive materials from shale deposits, and these substances return to the surface with flowback, also referred to as wastewater.\n\nThere are naturally occurring radioactive material (e.g., radium and radon) in shale deposits. Hydraulic fracturing can dislodge naturally occurring heavy metals and radioactive materials from shale deposits, and these substances return to the surface with flowback, also referred to as wastewater. These naturally occurring radionuclides are of more concern than some man-made radionuclides used in fracture monitoring because of their long half lives. Radium (Ra) is a product of Uranium-238 decay, and is the longest-lived isotope of radium with a half-life of 1601 years; next longest is Radium-228, a product of Thorium-232 breakdown, with a half-life of 5.75 years. Radon (Rn) is a naturally occurring product of the decay of uranium or thorium. Its most stable isotope, Radon-222, has a half-life of 3.8 days. Strontium is also naturally occurring and may be dislodged by the process.\n\nInjection of radioactive tracers, along with the other substances in hydraulic-fracturing fluid, is often used to determine the injection profile and location of fractures created by hydraulic fracturing. Patents describe in detail how several tracers are typically used in the same well. Wells are hydraulically fractured in different stages. Tracers with different half-lives are used for each stage. Their half-lives range from 40.2 hours (Lanthanum-140) to 28.90 years (Strontium-90). Amounts per injection of radionuclide are listed in the US Nuclear Regulatory Commission (NRC) guidelines. The NRC guidelines also list a wide range or radioactive materials in solid, liquid and gaseous forms that are used as field flood or enhanced oil and gas recovery study applications tracers used in single and multiple wells. According to the NRC, some of the most commonly used include Antimony-124, Bromine-82, Iodine-125, Iodine-131, Iridium-192, and Scandium-46. A 2003 publication by the International Atomic Energy Agency (IAEA) provides a detailed description of tracer use, confirms the frequent use of most of the tracers above, and says that Manganese-56, Sodium-24, Technetium-m, Silver-m, Argon-41, and Xenon-133 are also used extensively because they are easily identified and measured. Other potentially suitable tracers are named in various patents. In terms of quantities used, the NRC gives the following examples: Iodine-131, gas form, 100 millicuries total, not to exceed 20 millicuries per injection; Iodine-131, liquid form, 50 millicuries total, not to exceed 10 millicuries per injection; Iridium-192, \"Labeled\" frac sand, 200 millicuries total, not to exceed 15 millicuries per injection; Silver-110m, liquid form, 200 millicuries total, not to exceed 20 millicuries per injection.\n\nOther gamma-emitting tracer isotopes used are Antimony-121, Antimony-122, Antimony-123, Antimony-125, Antimony-126, Antimony-127, Carbon-14, Chromium-51, Cobalt-57, Cobalt-58, Cobalt-60, Gold-198, Iodine-127, Iodine-128, Iodine-129, Iodine-130, Iron-59, Krypton-85, Lanthanum-140, Potassium-39 (activated to Potassium-40), Potassium-41 (activated to Potassium-42), Potassium-43, Rubidium-86, Scandium-45, Scandium-47, Scandium-48, Silver-110, Sodium-22, Strontium-85, Strontium-90, Tritium, Zinc-65, and Zirconium-95.\n\nConcerns have been expressed that both naturally occurring radionuclides and radioactive tracers may return to the surface with flowback and during blow outs. Wastewater from the wells is released into rivers, injected into wells, and evaporated from ponds. \"The Times\" reported that studies by the United States Environmental Protection Agency and a confidential study by the drilling industry concluded that radioactivity in drilling waste cannot be fully diluted in rivers and other waterways. Recycling the wastewater has been proposed as a solution but has its limitations. \"The New York Times\" has reported on radium and gross alpha radiation levels in wastewater (also called flowback) from natural gas wells. It collected data from more than 200 natural gas wells in Pennsylvania. \"The New York Times\" has compiled a map of these wells and their wastewater contamination levels.\n\nPolitical, governmental, and industry pressures have prevented the United States Environmental Protection Agency (EPA) from studying risks associated with radionuclides or other chemicals in hydraulic fracturing fluids in wastewater, source water, and drinking water. The scope of the EPA Hydraulic Fracturing Draft Study Plan was narrowed to exclude them.\n\nAs radon decays, it produces radioactive decay products. If the contaminated dust of these \"radon daughters\" are inhaled, they can lodge in the lungs and increase the risk of developing lung cancer. Iodine in food is absorbed by the body and preferentially concentrated in the thyroid where it is needed for the functioning of that gland. When Iodine-131 is present in high levels in the environment from hydraulic fracturing flowback and blowouts, it can be absorbed through contaminated food and water, and will also accumulate in the thyroid. As it decays, it may cause damage to the thyroid. The primary risk from exposure to high levels of iodine-131 is the chance occurrence of radiogenic thyroid cancer in later life. Other risks include the possibility of non-cancerous growths and thyroiditis. The level of radiation in hydraulic fracturing wastewater has been measured to be as high as 18,035 pCi/L, thousands of times the maximum allowed by the federal standard for drinking water, and there are concerns about radiation exposure during spills and blowouts. Long term exposure to low level radiation is associated with stochastic health effects; the greater the exposure, the more likely the health effects are to occur. A group of doctors from the United States have called for a moratorium on hydraulic fracturing until health effects are more thoroughly studied.\n\nThe US Nuclear Regulatory Commission (NRC) and approved state agencies regulate the use of injected radionuclides in hydraulic fracturing in the United States. Federal and state regulators do not require sewage treatment plants that accept drilling waste to test for radioactivity. In Pennsylvania, where the hydraulic fracturing drilling boom began in 2008, most drinking-water intake plants downstream from those sewage treatment plants have not tested for radioactivity since before 2006. The EPA has asked the Pennsylvania Department of Environmental Protection to require community water systems in certain locations, and centralized wastewater treatment facilities to conduct testing for radionuclides. Safe drinking water standards have not yet been established to account for radioactivity levels of hydraulic fracturing waste water, and although water suppliers are required to inform citizens of radon and other radionuclides levels in their water, this doesn't always happen. Radioactive tracers are not yet listed on FracFocus, a website indicating chemical composition of fracking fluid of individual wells, but federal and state nuclear regulatory agencies keep records of their use.\n"}
{"id": "653500", "url": "https://en.wikipedia.org/wiki?curid=653500", "title": "Ignimbrite", "text": "Ignimbrite\n\nIgnimbrite is a variety of hardened tuff. Ignimbrites are igneous rocks made up of crystal and rock fragments in a glass-shard groundmass, albeit the original texture of the groundmass might be obliterated due to high degrees of welding. The term \"ignimbrite\" is not recommended by the IUGS Subcommission on the Systematics of Igneous Rocks. \n\nIgnimbrite is the deposit of a pyroclastic density current, or pyroclastic flow, which is a hot suspension of particles and gases flowing rapidly from a volcano and driven by being denser than the surrounding atmosphere. New Zealand geologist Patrick Marshall (1869-1950) derived the term \"ignimbrite\" from \"fiery rock dust cloud\" (from the Latin \"igni-\" (fire) and \"imbri-\" (rain)). Ignimbrites form as the result of immense explosions of pyroclastic ash, lapilli and blocks flowing down the sides of volcanoes.\n\nIgnimbrites are made of a very poorly sorted mixture of volcanic ash (or tuff when lithified) and pumice lapilli, commonly with scattered lithic fragments. The ash is composed of glass shards and crystal fragments. Ignimbrites may be loose and unconsolidated, or lithified (solidified) rock called lapilli-tuff. Near the volcanic source, ignimbrites commonly contain thick accumulations of lithic blocks, and distally, many show meter-thick accumulations of rounded cobbles of pumice.\n\nIgnimbrites may be white, grey, pink, beige, brown or black - depending on their composition and density. Many pale ignimbrites are dacitic or rhyolitic. Darker-coloured ignimbrites may be densely welded volcanic glass or, less commonly, mafic in composition.\n\nThere are two main models that have been proposed to explain the deposition of ignimbrites from a pyroclastic density current, the \"en masse\" deposition and the progressive aggradation models.\n\nThe \"en masse\" model was proposed by volcanologist Stephen Sparks in 1976. Sparks attributed the poor sorting in ignimbrites to laminar flows of very high particle concentration. Pyroclastic flows were envisioned as being similar to debris flows, with a body undergoing laminar flow and then stopping \"en masse\". The flow would travel as a plug flow, with an essentially non-deforming mass travelling on a thin shear zone, and the \"en masse\" freezing occurs when the driving stress falls below a certain level. This would produce a massive unit with an inversely graded base.\n\nThere are several problems with the \"en masse\" model. Since ignimbrite is a deposit, its characteristics cannot completely represent the flow, and the deposit may only record the depositional process. Vertical chemical zonation in ignimbrites is interpreted as recording incremental changes in the deposition, and the zonation rarely correlates with flow unit boundaries and may occur within flow units. It has been posited that the chemical changes are recording progressive aggradation at the base of the flow from an eruption whose composition changes with time. For this to be so, the base of the flow cannot be turbulent. The instantaneous deposition of an entire body of material is not possible because displacement of the fluid is not possible instantaneously. Any displacement of the fluid would mobilize the upper part of the flow, and \"en masse\" deposition would not occur. Instantaneously cessation of the flow would cause local compression and extension, which would be evident in the form of tension cracks and small scale thrusting, which is not seen in most ignimbrites.\n\nAn adaptation of the \"en masse\" theory suggests that the ignimbrite records progressive aggradation from a sustained current and that the differences observed between ignimbrites and within an ignimbrite are the result of temporal changes to the nature of the flow that deposited it.\n\nRheomorphic structures are only observed in high grade ignimbrites. There are two types of rheomorphic flow; post-depositional re-mobilization, and late stage viscous flow. While there is currently debate in the field of the relative importance of either mechanism, there is agreement that both mechanisms have an effect. A vertical variation in orientation of the structures is compelling evidence against post-depositional re-mobilization being responsible for the majority of the structures, but more work needs to be carried out to discover if the majority of ignimbrites have these vertical variations in order to say which process is the most common.\n\nA model based on observations at the Wall Mountain Tuff at Florissant Fossil Beds National Monument in Colorado suggests that the rheomorphic structures such as foliation and pyroclasts were formed during laminar viscous flow as the density current comes to a halt. A change from particulate flow to a viscous fluid could cause the rapid \"en masse\" cooling in the last few meters. It is also theorized that transformation occurs at a boundary layer at the base of the flow and that all the materials pass through this layer during deposition.\n\nAnother model proposed is that the density current became stationary before the rheomorphic structures form. Structures such as pervasive foliation are a result of load compaction, and other structures are the result of remobilization by load and deposition on inclined topography. The tuff deposited at Wagontire Mountain in Oregon and Bishop Tuff in California show evidence of late stage viscous flow. These tuffs have a similar chemistry and so must have undergone the same compaction process to have the same foliation.\n\nThe Green Tuff in Pantelleria contains rheomorphic structures which are held to be a result of post-depositional re-mobilization because at that time the Green Tuff was believed to be a fall deposit which has no lateral transport. Similarities between the structures in the Green Tuff and ignimbrites on Gran Canaria suggest post-depositional re-mobilization. This interpretation of the deposition of the Green Tuff has been disputed, suggesting that it is an ignimbrite, and structures such as imbricate fiamme, observed in the Green Tuff, were the result of late stage primary viscous flow. Similar structures observed on Gran Canaria had been interpreted as syn-depositional flow.\n\nSheathfolds and other rheomorphic structures may be the result of a single stage of shear. Shear possibly occurred as the density current passed over the forming deposit. Vertical variations in the orientations of sheathfolds are evidence that rheomorphism and welding can occur syn-depositionally. It is disputed that the shear between the density current and the forming deposit is significant enough to cause all of the rheomorphic structures observed in ignimbrites, although the shear could be responsible for some of the structures such as imbricate fiamme. Load compaction on an inclined slope is likely responsible for the majority of the rheomorphic structures.\n\nIgnimbrite is primarily composed of a matrix of volcanic ash (tephra) which is composed of shards and fragments of volcanic glass, pumice fragments, and crystals. The crystal fragments are commonly blown apart by the explosive eruption. Most are phenocrysts that grew in the magma, but some may be exotic crystals such as xenocrysts, derived from other magmas, igneous rocks, or from country rock.\n\nThe ash matrix typically contains varying amounts of pea- to cobble-sized rock fragments called lithic inclusions. They are mostly bits of older solidified volcanic debris entrained from conduit walls or from the land surface. More rarely, clasts are cognate material from the magma chamber.\n\nIf sufficiently hot when deposited, the particles in an ignimbrite may weld together, and the deposit is transformed into a 'welded ignimbrite', made of \"eutaxitic lapilli-tuff\". When this happens, the pumice lapilli commonly flatten, and these appear on rock surfaces as dark lens shapes, known as fiamme. Intensely welded ignimbrite may have glassy zones near the base and top, called lower and upper 'vitrophyres', but central parts are microcrystalline ('lithoidal').\n\nThe mineralogy of an ignimbrite is controlled primarily by the chemistry of the source magma.\n\nThe typical range of phenocrysts in ignimbrites are biotite, quartz, sanidine or other alkali feldspar, occasionally hornblende, rarely pyroxene and in the case of phonolite tuffs, the feldspathoid minerals such as nepheline and leucite.\n\nCommonly in most felsic ignimbrites the quartz polymorphs cristobalite and tridymite are usually found within the welded tuffs and breccias. In the majority of cases, it appears that these high-temperature polymorphs of quartz occurred post-eruption as part of an autogenic post-eruptive alteration in some metastable form. Thus although tridymite and cristobalite are common minerals in ignimbrites, they may not be primary magmatic minerals.\n\nMost ignimbrites are silicic, with generally over 65% SiO. The chemistry of the ignimbrites, like all felsic rocks, and the resultant mineralogy of phenocryst populations within them, is related mostly to the varying contents of sodium, potassium, calcium, the lesser amounts of iron and magnesium.\n\nSome rare ignimbrites are andesitic, and may even be formed from volatile saturated basalt, where the ignimbrite would have the geochemistry of a normal basalt.\n\nLarge hot ignimbrites can create some form of hydrothermal activity as they tend to blanket the wet soil and bury watercourses and rivers. The water from such substrates will exit the ignimbrite blanket in fumaroles, geysers and the like, a process which may take several years, for example after the Novarupta tuff eruption. In the process of boiling off this water, the ignimbrite layer may become metasomatised (altered). This tends to form chimneys and pockets of kaolin-altered rock.\n\nWelding is a common form of ignimbrite alteration. There are two types of welding, primary and secondary. If the density current is sufficiently hot the particles will agglutinate and weld at the surface of sedimentation to form a viscous fluid; this is primary welding. If during transport and deposition the temperature is low, then the particles will not agglutinate and weld, although welding may occur later if compaction or other factors reduce the minimum welding temperature to below the temperature of the glassy particles; this is secondary welding. This secondary welding is most common and suggests that the temperature of most pyroclastic density currents is below the softening point of the particles.\n\nThe factor that determines whether an ignimbrite has primary welding, secondary welding or no welding is debated:\n\nIgnimbrite originates from explosive eruptions caused by vigorous exsolution of magmatic gases. The escaping gas accelerates the magma up the conduit, resulting in fragmentation to produce pumice and ash, which dispersed in gas will flow downslope or spread where the dispersal is denser than the atmosphere, as pyroclastic density current, sometimes known as a pyroclastic flow.\n\nIgnimbrites form sheets that can cover as much as thousands of square kilometers. Some examples create thick valley-filling deposits, while others form a landscape-mantling veneer that locally thickens in valleys and other paleotopographic depressions.\n\nMany igimbrites are loose unconsolidated deposits, but some exhibit welding, giving the ignimbrite the texture of a solid rock mass, hence the terms commonly used to describe these examples: welded tuff and welded ashflow.\n\nOften, but not always, a caldera will form as a result of a large ignimbrite eruption because the magma chamber underneath will drain and thus can no longer support the weight of the rock above.\n\nIgnimbrite deposits can be voluminous - examples with up to hundreds or even thousands of cubic kilometers are known from individual eruptions in the geological past.\n\nIgnimbrites occur worldwide associated with many volcanic provinces having high-silica content magma and the resulting explosive eruptions.\n\nIgnimbrite occurs very commonly around the lower Hunter Region of the Australian state of New South Wales. The ignimbrite quarried in the Hunter region at locations such as Martins Creek, Brandy Hill, Seaham (Boral) and at abandoned quarry at Raymond Terrace is a volcanic sedimentation rock of Carboniferous age (280-345 million years). It had an extremely violent origin. This material built up to considerable depth and must have taken years to cool down completely. In the process the materials that made up this mixture fused together into a very tough rock of medium density.\n\nIgnimbrite also occurs in the Coromandel region of New Zealand, where the striking orange-brown ignimbrite cliffs form a distinctive feature of the landscape. The nearby Taupo Volcanic Zone is covered in extensive flat sheets of ignimbrite erupted from caldera volcanoes during the Pleistocene and Holocene. The exposed ignimbrite cliffs at Hinuera (Waikato) mark the edges of the ancient Waikato River course which flowed through the valley before the last major Taupo eruption 1,800 years ago (the Hatepe eruption). The west cliffs are quarried to get blocks of Hinuera Stone, the name given to welded ignimbrite used for building cladding. The stone is light grey with traces of green and is slightly porous.\n\nHuge deposits of ignimbrite form large parts of the Sierra Madre Occidental in western Mexico. In the western United States, massive ignimbrite deposits up to several hundred metres thick occur in the Basin and Range Province, largely in Nevada, western Utah, southern Arizona, and north-central and southern New Mexico, and the Snake River Plain. The magmatism in the Basin and Range Province included a massive flare-up of ignimbrite which began about 40 million years ago and largely ended 25 million years ago: the magmatism followed the end of the Laramide orogeny, when deformation and magmatism occurred far east of the plate boundary. Additional eruptions of ignimbrite continued in Nevada until roughly 14 million years ago. Individual eruptions were often enormous, sometimes up to thousands of cubic kilometres in volume, giving them a Volcanic Explosivity Index of 8, comparable to Yellowstone Caldera and Lake Toba eruptions.\n\nSuccessions of ignimbrites make up most of the post-erosional rocks in Gran Canaria Island.\n\nYucca Mountain Repository, a U.S. Department of Energy terminal storage facility for spent nuclear reactor and other radioactive waste, is in a deposit of ignimbrite and tuff.\n\nThe layering of ignimbrites is used when the stone is worked, as it sometimes splits into convenient slabs, useful for flagstones and in garden edge landscaping.\n\nIn the Hunter region of New South Wales ignimbrite serves as an excellent aggregate or 'blue metal' for road surfacing and construction purposes.\n\n\n"}
{"id": "42313034", "url": "https://en.wikipedia.org/wiki?curid=42313034", "title": "Iridium 77", "text": "Iridium 77\n\nIridium 77 is a communications Satellite which is part of a satellite constellation known as Iridium, named after the 77th chemical element of the periodic table, \"iridium\". It was launched in 1998 and as of 2014, operational. It is owned and funded by Iridium, a communications company.\n\nIridium 77 is a part of a space-based communications system called Iridium. Conceived, designed, and built by Motorola, the Iridium system provides wireless, mobile communications through a network of 66 satellites in polar, low-Earth orbits. Inaugurated in November 1998, under the auspices of Iridium LLC, this complex space system allowed callers using hand-held mobile phones and pagers to communicate anywhere in the world—a first in the history of telephony.\n\nIt was launched by Delta II 7920 from Vandenberg Air Force Base on 8 September 1998 at 21:13:00 UTC along with four other satellites, all of which were Iridium satellites.\n\nIridium 77 is 3-axis stabilized, with a hydrazine propulsion system. It has 2 solar panels with 1-axis articulation. The system employs L-Band using FDMA/TDMA to provide voice at 4.8 kbit/s and data at 2.4 kbit/s with 16 dB margin. The satellite has 48 spot beams for Earth coverage and uses Ka-Band for crosslinks and ground commanding.\n\n"}
{"id": "3066768", "url": "https://en.wikipedia.org/wiki?curid=3066768", "title": "Lehmann discontinuity", "text": "Lehmann discontinuity\n\nThe Lehmann discontinuity is an abrupt increase of \"P\"-wave and \"S\"-wave velocities at the depth of 220±30 km, discovered by seismologist Inge Lehmann. It appears beneath continents, but not usually beneath oceans, and does not readily appear in globally averaged studies. Several explanations have been proposed: a lower limit to the pliable asthenosphere, a phase transition, and most plausibly, depth variation in the shear wave anisotropy. Further discussion of the Lehmann discontinuity can be found in the book \"Deformation of Earth Materials\" by Shun-ichirō Karato.\n\n\n"}
{"id": "7418473", "url": "https://en.wikipedia.org/wiki?curid=7418473", "title": "List of Linyphiidae species (A–H)", "text": "List of Linyphiidae species (A–H)\n\nThis page lists all described species of the spider family Linyphiidae as of January 18, 2017, from A to H. Some genera have been updated to the World Spider Catalog version 17.5 .\n\n\"Abacoproeces\" \n\n\"Aberdaria\" \n\n\"Abiskoa\" \n\n\"Acanoides\" \n\n\"Acanthoneta\" \n\n\"Acartauchenius\" \n\n\"Acorigone\" \n\n\"Adelonetria\" \n\n\"Afribactrus\" \n\n\"Afromynoglenes\" \n\n\"Afroneta\" \n\n\"Agnyphantes\" \n\n\"Agyneta\" \n\n\"Agyphantes\" \n\n\"Ainerigone\" \n\n\"Alioranus\" \n\n\"Allomengea\" \n\n\"Allotiso\" \n\n\"Anacornia\" \n\n\"Anguliphantes\" \n\n\"Anibontes\" \n\n\"Annapolis\" \n\n\"Anodoration\" \n\n\"Anthrobia\" \n\n\"Antrohyphantes\" \n\n'Aperturina' - \n\"Aperturina paniculus\" - Thailand, Malaysia\n\n\"Aphileta\" \n\n\"Apobrata\" \n\n\"Aprifrontalia\" \n\n\"Arachosinella\" \n\n\"Araeoncus\" \n\n\"Archaraeoncus\" \n\n\"Arcterigone\" \n\n\"Arcuphantes\" \n\n\"Ascetophantes\" \n\n\"Asemostera\" \n\n\"Asiagone\" \n\n\"Asiceratinops\" \n\n\"Asiophantes\" \n\n\"Asperthorax\" \n\n\"Asthenargellus\" \n\n\"Asthenargoides\" \n\n\"Asthenargus\" \n\n\"Atypena\" \n\n\"Australolinyphia\" \n\n\"Australophantes\" \n\n\"Bactrogyna\" \n\n\"Baryphyma\" \n\n\"Baryphymula\" \n\n\"Bathylinyphia\" \n\n\"Bathyphantes\" \n\n\"Batueta\" \n\n\"Bifurcia\" \n\n\"Birgerius\" \n\n\"Bisetifer\" \n\n\"Bishopiana\" \n\n\"Blestia\" \n\n\"Bolephthyphantes\" \n\n\"Bolyphantes\" \n\n\"Bordea\" \n\n\"Brachycerasphora\" \n\n\"Bursellia\" \n\n\"Caenonetria\" \n\n\"Callitrichia\" \n\n\"Cameroneta\" \n\n\"Canariellanum\" \n\n\"Canariphantes\" \n\n\"Capsulia\" \n\n\"Caracladus\" \n\n\"Carorita\" \n\n\"Cassafroneta\" \n\n\"Catacercus\" \n\n\"Catonetria\" \n\n\"Caucasopisthes\" \n\n\"Cautinella\" \n\n\"Caviphantes\" \n\n\"Centromerita\" \n\n\"Centromerus\" \n\n\"Centrophantes\" \n\n\"Ceraticelus\" \n\n\"Ceratinella\" \n\n\"Ceratinops\" \n\n\"Ceratinopsidis\" \n\n\"Ceratinopsis\" \n\n\"Ceratocyba\" \n\n\"Cheniseo\" \n\n\"Chenisides\" \n\n\"Cherserigone\" \n\n\"Chiangmaia\" \n\n\"Chthiononetes\" \n\n\"Cinetata\" \n\n\"Cirrosus\" \n\n\"Claviphantes\" \n\n\"Cnephalocotes\" \n\n\"Collinsia\" \n\n\"Coloncus\" \n\n\"Comorella\" \n\n\"Concavocephalus\" \n\n\"Conglin\" \n\n\"Connithorax\" \n\n\"Coreorgonal\" \n\n\"Cornicephalus\" \n\n\"Cresmatoneta\" \n\n\"Crispiphantes\" \n\n\"Crosbyarachne\" \n\n\"Crosbylonia\" \n\n\"Cryptolinyphia\" \n\n\"Ctenophysis\" \n\n\"Curtimeticus\" \n\n\"Cyphonetria\" \n\n\"Dactylopisthes\" \n\n\"Dactylopisthoides\" \n\n\"Decipiphantes\" \n\n\"Deelemania\" \n\n\"Dendronetria\" \n\n\"Denisiphantes\" \n\n\"Diastanillus\" \n\n\"Dicornua\" \n\n\"Dicymbium\" \n\n\"Didectoprocnemis\" \n\n\"Diechomma\" \n\n\"Diplocentria\" \n\n\"Diplocephaloides\" \n\n\"Diplocephalus\" \n\n\"Diploplecta\" \n\n\"Diplostyla\" \n\n\"Diplothyron\" \n\n\"Disembolus\" \n\n\"Dismodicus\" \n\n\"Doenitzius\" \n\n\"Dolabritor\" \n\n\"Donacochara\" \n\n\"Drapetisca\" \n\n\"Drepanotylus\" \n\n\"Dresconella\" \n\n\"Dubiaranea\" \n\n\"Dumoga\" \n\n\"Dunedinia\" \n\n\"Eborilaira\" \n\n\"Eldonnia\" \n\n\"Emenista\" \n\n\"Enguterothrix\" \n\n\"Entelecara\" \n\n\"Eordea\" \n\n\"Epibellowia\" \n\n\"Epiceraticelus\" \n\n\"Epigyphantes\" \n\n\"Epigytholus\" \n\n\"Episolder\" \n\n\"Epiwubana\" \n\n\"Eridantes\" \n\n\"Erigone\" \n\n\"Erigonella\" \n\n\"Erigonoploides\" \n\n\"Erigonoplus\" \n\n\"Erigonops\" \n\n\"Erigophantes\" \n\n\"Eskovia\" \n\n\"Eskovina\" \n\n\"Esophyllas\" \n\n\"Estrandia\" \n\n\"Eulaira\" \n\n\"Eurymorion\" \n\n\"Evansia\" \n\n\"Exechopsis\" \n\n\"Exocora\" \n\n\"Fageiella\" \n\n\"Falklandoglenes\" \n\n\"Fissiscapus\" \n\n\"Fistulaphantes\" \n\n\"Flagelliphantes\" \n\n\"Floricomus\" \n\n\"Florinda\" \n\n\"Floronia\" \n\n\"Formiphantes\" \n\n\"Frederickus\" \n\n\"Frontella\" \n\n\"Frontinella\" \n\n\"Frontinellina\" \n\n\"Frontiphantes\" \n\n\"Fusciphantes\" \n\n\"Gibbafroneta\" \n\n\"Gibothorax\" \n\n\"Gigapassus\" \n\n\"Gladiata\" \n\n\"Glebala\" \n\n\"Glomerosus\" \n\n\"Glyphesis\" \n\n\"Gnathonargus\" \n\n\"Gnathonarium\" \n\n\"Gnathonaroides\" \n\n\"Gonatium\" \n\n\"Gonatoraphis\" \n\n\"Goneatara\" \n\n\"Gongylidiellum\" \n\n\"Gongylidioides\" \n\n\"Gongylidium\" \n\n\"Grammonota\" \n\n\"Graphomoa\" \n\n\"Gravipalpus\" \n\n\"Habreuresis\" \n\n\"Halorates\" \n\n\"Haplinis\" \n\n\"Haplomaro\" \n\n\"Helophora\" \n\n\"Helsdingenia\" \n\n\"Herbiphantes\" \n\n\"Heterolinyphia\" \n\n\"Heterotrichoncus\" \n\n\"Hilaira\" \n\n\"Himalaphantes\" \n\n\"Holma\" \n\n\"Holmelgonia\" \n\n\"Holminaria\" \n\n\"Horcotes\" \n\n\"Houshenzinus\" \n\n\"Hubertella\" \n\n\"Hybauchenidium\" \n\n\"Hybocoptus\" \n\n\"Hylyphantes\" \n\n\"Hyperafroneta\" \n\n\"Hypomma\" \n\n\"Hypselistes\" \n\n\"Hypselocara\" \n\n\"Hypsocephalus\" \n\n"}
{"id": "9020581", "url": "https://en.wikipedia.org/wiki?curid=9020581", "title": "List of coconut palm diseases", "text": "List of coconut palm diseases\n\nThis article is a list of diseases of coconut palms (\"Cocos nucifera\").\n\n\n"}
{"id": "34535212", "url": "https://en.wikipedia.org/wiki?curid=34535212", "title": "List of earthquakes in Vanuatu", "text": "List of earthquakes in Vanuatu\n\nEarthquakes in Vanuatu are frequent and are sometimes accompanied by tsunami, though these events are not often destructive. The archipelago, which was formerly known as New Hebrides, lies atop a complex and active plate boundary in the southwestern Pacific Ocean.\n\nThe primary tectonic feature of the island chain is the New Hebrides Subduction Zone, the convergent boundary of the Australian and Pacific Plates. Along the Wadati–Benioff zone, earthquake activity has been observed as shallow, intermediate, and deep-focus events at depths of up to . Volcanic activity is also present along this north-northwest trending and northeast-dipping oceanic trench.\n\nWhile much of the island arc experiences intermediate-depth earthquakes along a Wadati–Benioff zone that dips steeply at 70°, the area adjacent to the d'Entrecasteaux Ridge does not. There is a corresponding gap in seismicity that occurs below where it intrudes into the subduction zone from the west. According to the NUVEL-1 global relative plate motion model, convergence is occurring at roughly per year. The uncertainty, which also affects the Tonga arc, is due to the influence of spreading at the North Fiji basin. Of the 58 M7 or greater events that occurred between 1909 and 2001, few were studied.\n\n\nSources\n\n"}
{"id": "7379631", "url": "https://en.wikipedia.org/wiki?curid=7379631", "title": "List of lakes by depth", "text": "List of lakes by depth\n\nThis page lists the world's deepest lakes.\n\nThis list contains all lakes whose maximum depth is reliably known to exceed \"\n\nGeologically, the Caspian Sea, like the Black and Mediterranean seas, is a remnant of the ancient Tethys Ocean. The deepest area is oceanic rather than continental crust. However, it is generally regarded by geographers as a large endorheic salt lake.\n\nMean depth can be a more useful indicator than maximum depth for many ecological purposes. Unfortunately, accurate mean depth figures are only available for well-studied lakes, as they must be calculated by dividing the lake's volume by its surface area. A reliable volume figure requires a bathymetric survey. Therefore, mean depth figures are not available for many deep lakes in remote locations.\n\nThe Caspian Sea ranks much further down the list on mean depth, as it has a large continental shelf (significantly larger than the oceanic basin that contains its greatest depths).\n\n\n\n\n\"Note: Lake depths often vary depending on sources. The depths used here are the most reliable figures available in recent sources. See the articles on individual lakes for more details and data sources.\"\n\n\n"}
{"id": "38347801", "url": "https://en.wikipedia.org/wiki?curid=38347801", "title": "List of nature centers in Pennsylvania", "text": "List of nature centers in Pennsylvania\n\nThis is a list of nature centers and environmental education centers in the state of Pennsylvania.\n\nTo use the sortable tables: click on the icons at the top of each column to sort that column in alphabetical order; click again for reverse alphabetical order.\n\n"}
{"id": "206589", "url": "https://en.wikipedia.org/wiki?curid=206589", "title": "Lunar geologic timescale", "text": "Lunar geologic timescale\n\nThe lunar geological timescale (or selenological timescale) divides the history of Earth's Moon into five generally recognized periods: the Copernican, Eratosthenian, Imbrian (Late and Early epochs), Nectarian, and Pre-Nectarian. The boundaries of this time scale are related to large impact events that have modified the lunar surface, changes in crater formation through time, and the size-frequency distribution of craters superposed on geological units. The absolute ages for these periods have been constrained by radiometric dating of samples obtained from the lunar surface. However, there is still much debate concerning the ages of certain key events, because correlating lunar regolith samples with geological units on the Moon is difficult, and most lunar radiometric ages have been highly affected by an intense history of bombardment.\n\nThe primary geological processes that have modified the lunar surface are impact cratering and volcanism, and by using standard stratigraphic principles (such as the law of superposition) it is possible to order these geological events in time. At one time, it was thought that the mare basalts might represent a single stratigraphic unit with a unique age, but it is now recognized that mare volcanism was an ongoing process, beginning as early as 4.2 Ga (1 Ga = 1 billion years ago) and continuing to perhaps as late as 1.2 Ga. Impact events are by far the most useful for defining a lunar stratigraphy as they are numerous and form in a geological instant. The continued effects of impact cratering over long periods of time modify the morphology of lunar landforms in a quantitative way, and the state of erosion of a landform can also be used to assign a relative age.\n\nThe lunar geological time scale has been divided into five periods (Pre-Nectarian, Nectarian, Imbrian, Eratosthenian, and Copernican) with one of these (the Imbrian) being subdivided into two epochs. These divisions of geological time are based on the recognition of convenient geomorphological markers, and as such, they should not be taken to imply that any fundamental changes in geological processes have occurred at these boundaries. The Moon is unique in the solar system in that it is the only body (other than the Earth) for which we possess rock samples with a known geological context. By correlating the ages of samples obtained from the Apollo missions to known geological units, it has been possible to assign absolute ages to some of these geological periods. The timeline below represents one such attempt, but it is important to note (as is discussed below) that some of the ages are either uncertain, or disputed. In many lunar highland regions, it is not possible to distinguish between Nectarian and Pre-Nectarian materials, and these deposits are sometimes labeled as just Pre-Imbrian.\n\nThe Pre-Nectarian period is defined from the point at which the lunar crust formed, to the time of the Nectaris impact event. Nectaris is a multi-ring impact basin that formed on the near side of the Moon, and its ejecta blanket serves as a useful stratigraphic marker. 30 impact basins from this period are recognized, the oldest of which is the South Pole–Aitken basin. This geological period has been informally subdivided into the Cryptic and Basin Groups 1-9, but these divisions are not used on any geological maps.\n\nThe Nectarian period encompasses all events that occurred between the formation of the Nectaris and Imbrium impact basins. 12 multi-ring impact basins are recognized in the Nectarian period, including the Serenitatis and Crisium basins. One of the scientific objectives of the Apollo 16 mission was to date material excavated by the Nectaris impact basin. Nevertheless, the age of the Nectaris basin is somewhat contentious, with the most frequently cited numbers being 3.92 Ga, and less frequently 3.85 Ga. Recently, it has been suggested that the Nectaris basin could be, in fact, much older at ~4.1 Ga.\n\nThe Imbrian period has been subdivided into Late and Early epochs. The Early Imbrian is defined as the time between the formation of the Imbrium and Orientale impact basins. The Imbrium basin is believed to have formed at 3.85 Ga, though a minority opinion places this event at 3.77 Ga. The Schrödinger basin is the only other multi-ring basin that is Lower Imbrian in age, and no large multi-ring basins formed after this epoch.\n\nThe Late Imbrian is defined as the time between the formation of the Orientale basin, and the time at which craters of a certain size (D) have been obliterated by erosional processes. The age of the Orientale basin has not been directly determined, though it must be older than 3.72 Ga (based on Upper Imbrian ages of mare basalts) and could be as old as 3.84 Ga based on the size-frequency distributions of craters superposed on Orientale ejecta. About two-thirds of the Moon's mare basalts erupted within the Upper Imbrian Series, with many of these lavas filling the depressions associated with older impact basins.\n\nThe base of the Eratosthenian period is defined by the time at which craters on a geological unit of a certain size D have been almost completely obliterated by erosional processes. The principal erosional agent on the Moon is impact cratering itself, though seismic modification could play a minor role as well. The absolute age of this boundary is not well defined, but is commonly quoted as being near 3.2 Ga. The younger boundary of this period is defined based on the recognition that freshly excavated materials on the lunar surface are generally bright and that they become darker over time as a result of space weathering processes. Operationally, this period was originally defined as the time at which impact craters lost their bright ray systems. This definition, however, has recently been subjected to some criticism as some crater rays are bright for compositional reasons that are unrelated to the amount of space weathering they have incurred. In particular, if the ejecta from a crater formed in the highlands (which is composed of bright anorthositic materials) is deposited on the low albedo mare, it will remain bright even after being space weathered.\n\nThe Copernican period is the youngest geological period of the Moon. Originally, the presence of a bright ray system surrounding an impact crater was used to define Copernican units, but as mentioned above, this is complicated by the presence of compositional ray systems. The base of the Copernican period does not correspond to the formation of the impact crater Copernicus. The age of the base of the Copernican is not well constrained, but a commonly quoted number is 1.1 Ga. The Copernican extends until the present day.\n\nThe divisions of the lunar geologic time scale are based on the recognition of a few convenient geomorphological markers. While these divisions are extremely useful for ordering geological events in a relative manner, it is important to realize that the boundaries do not imply any fundamental change of geological processes. Furthermore, as the oldest geological periods of the Moon are based exclusively on the times of individual impact events (in particular, Nectaris, Imbrium, and Orientale), these punctual events will most likely not correspond to any specific geological event on the other terrestrial planets, such as Mercury, Venus, Earth, or Mars.\n\nNevertheless, at least one notable scientific work has advocated using the lunar geological time scale to subdivide the Hadean eon of Earth's geologic time scale. In particular, it is sometimes found that the Hadean is subdivided into the Cryptic, Basin Groups 1-9, Nectarian, and Early Imbrian. This notation is not entirely consistent with the above lunar geologic time scale in that the Cryptic and Basin Groups 1-9 (both of which are only informal terms that are not used in geologic maps) comprise the Pre-Nectarian period.\n\n\nCited references\n\nGeneral references\n"}
{"id": "29773157", "url": "https://en.wikipedia.org/wiki?curid=29773157", "title": "Ministry of Energy and Coal Mining (Ukraine)", "text": "Ministry of Energy and Coal Mining (Ukraine)\n\nThe Ministry of Energy and Coal Mining of Ukraine () is the main body in the system of central government responsible for realization of electric power-generating state policies; nuclear-industrial, and oil-gas complexes often referred simply as the \"Fuel-Energy Complex\". The ministry is coordinated by the Cabinet of Ministers.\n\n\n\n\n\n\n\nPrevious names:\n\nThe ministry also absorbed a separate Ministry of Coal Mining which existed since 1954 until 1999 and was revived in 2005-2010.\n\nMinistry of Coal Mining of Ukraine existed at least since 1954.\n\n"}
{"id": "990894", "url": "https://en.wikipedia.org/wiki?curid=990894", "title": "Miombo", "text": "Miombo\n\nMiombo is the vernacular word for \"Brachystegia\", a genus of tree comprising a large number of tree species together with Julbernadia species in woodlands. Miombo woodland is classified in the tropical and subtropical grasslands, savannas, and shrublands biome (in the World Wildlife Fund scheme). The biome includes four woodland savanna ecoregions (listed below) characterized by the predominant presence of miombo species, with a range of climates from humid to semi-arid, and tropical to subtropical or even temperate.\n\nCharacteristically the trees shed their leaves for a short period in the dry season to reduce water loss, and produce a flush of new leaves just before the onset of the rainy season with rich gold and red colours masking the underlying chlorophyll, reminiscent of temperate autumn colours. \n\nThe name miombo is used in a number of Bantu languages in the region such as Swahili, Shona and Bemba. In Bemba, the word \"miombo\" is the plural of the word \"muombo\", which is the specific name for the species \"Brachystegia longifolia\".\n\nMiombo woodlands form a broad belt across south-central Africa, running from Angola in the west to Tanzania to the east. These woodlands are dominated by trees of subfamily Caesalpinioideae, particularly miombo (\"Brachystegia\"), \"Julbernardia\" and \"Isoberlinia\", which are rarely found outside miombo woodlands. The four ecoregions are:\n\nMoreover miombo woodlands could be classified as dry or wet based on the per annum amount and distribution of rainfall. Where by dry woodlands occurs in those areas receiving less than 1000 mm annual rainfall. Mostly in Zimbabwe, Central Tanzania and southern areas of Mozambique, Malawi and Zambia. On the contrary wet woodlands whilst more than 1000 mm annual rainfall are located in Northern Zambia, eastern Angola. central Malawi and south western Tanzania. \n\nThese miombo woodlands are also important to the livelihoods of many rural people, who depend on the resources available from the woodland. The wide variety of species provides non-timber products such as fruits, honey, fodder for livestock and fuelwood. \n\nDespite the relatively nutrient-poor soil, long dry season (and low rainfall in some areas) the woodland is home to many species, including several miombo specialist endemic bird species. The predominant tree is miombo (\"Brachystegia\" spp.). It also provides food and cover for mammals such as the African elephant (\"Loxodonta africana\"), African wild dog (\"Lycaon pictus\"), sable antelope (\"Hippotragus niger\") and Lichtenstein's hartebeest (\"Sigmoceros lichtensteinii\").\n\n\n"}
{"id": "2355524", "url": "https://en.wikipedia.org/wiki?curid=2355524", "title": "Narrow bipolar pulse", "text": "Narrow bipolar pulse\n\nNarrow bipolar pulses are high-energy, high-altitude, intra-cloud electrical discharges associated with thunderstorms. NBP are similar to other forms of lightning events such as return strokes and dart leaders, but produce an optical emission of at least an order of magnitude smaller. They typically occur in the 10–20 km altitude range and can emit a power on the order of a few hundred gigawatts. They produce far-field asymmetric bipolar electric field change signatures (called narrow bipolar events).\n\n"}
{"id": "397292", "url": "https://en.wikipedia.org/wiki?curid=397292", "title": "Nicolas Baudin", "text": "Nicolas Baudin\n\nNicolas Thomas Baudin (17 February 1754 – 16 September 1803) was a French explorer, cartographer, naturalist and hydrographer.\n\nBorn a commoner in Saint-Martin-de-Ré on the Île de Ré on 17 February 1754, Baudin joined the merchant navy at the age of 15 and the French East India Company at the age of 20. \n\nBaudin then joined the \"La Marine Royale\" (French Navy) in 1774 and served in the Caribbean as an \"officier bleu\" during the American War of Independence of 1775–1783.\n\nIn 1785 Baudin and his brother Alexandre were respectively masters of the \"St Remy\" and \"Caroline\", taking Acadian settlers from Nantes to La Nouvelle Orléans. In New Orleans local merchants contracted him to take a cargo of wood, salted meat, cod and flour to Isle de France (now Mauritius), which he did in \"Josephine\" (also called \"Pepita\"), departing New Orleans on 14 July 1786 and arriving at Isle de France on 27 March 1787. In the course of the voyage, \"Josephine\" had called at Cap‑Français in Haiti to make a contract to transport slaves there from Madagascar; while in Haiti he also encountered the Austrian botanist Franz Josef Maerter, who apparently informed him that another Austrian botanist, Franz Boos, was at the Cape of Good Hope awaiting a ship to take him to Mauritius. \"Josephine\" called at the Cape and took Boos on board. At Mauritius, Boos chartered Baudin to transport him and the collection of plant specimens he had gathered there and at the Cape back to Europe, which Baudin did, \"Josephine\" arriving at Trieste on 18 June 1788. The Imperial government in Vienna was contemplating organizing another natural-history expedition, to which Boos would be appointed, in which two ships would be sent to the Malabar and Coromandel coasts of India, the Persian Gulf, Bengal, Ceylon, Sumatra, Java, Borneo, Cochin China, Tongking, Japan and China. Baudin had been given reason to hope that he would be given command of the ships of this expedition.\n\nLater in 1788 Baudin sailed on a commercial voyage from Trieste to Canton in \"Jardiniere\". He apparently arrived at Canton from Mauritius under the flag of the United States of America, probably to avoid the possibility of having his ship seized by the Chinese for payment of the debts owed them by the Imperial Asiatic Company of Trieste. From there, he sent \"Jardiniere\" under her second captain on a fur-trading venture to the north-west coast of America, but the ship foundered off Asuncion Island in the Marianas in late 1789.\n\nBaudin made his way to Mauritius, where he purchased a replacement ship, \"Jardiniere II\", but this vessel was wrecked in a cyclone that struck Port Louis on 15 December 1789. Baudin embarked on the Spanish Royal Philippines Company ship, \"Placeres\", which sailed from Port Louis for Cadiz in August 1790. \"Placeres\" called at the Cape of Good Hope where it took on board the large number of plant and animal specimens collected in South Africa for the Imperial palace at Schönbrunn by Georg Scholl, the assistant of Franz Boos. Because of the poor condition of the ship, \"Placeres\" had to put in at the island of Trinidad in the West Indies, where Scholl's collection of specimens was deposited.\n\nBaudin proceeded to Martinique, from where he addressed an offer to the Imperial government in Vienna to conduct to Canton commissioners who would be empowered to negotiate with the Chinese merchants there a settlement of the debts incurred by the Imperial Asiatic Company, which would enable the company to renew its trade with China. On its return voyage from Canton, the proposed expedition would call at the Cape of Good Hope to pick up Scholl and the remainder of his natural-history collection for conveyance to Schönbrunn.\n\nAfter returning to Vienna in September 1791, Baudin continued to press his case for an expedition under the Imperial flag to the Indian Ocean and China, and in January 1792 he was granted a commission of captain in the Imperial navy for this purpose. A ship, called \"Jardiniere\", was acquired and the botanists Franz Bredemeyer and Joseph van der Schot appointed to the expedition. After delays caused by the outbreak of war between France and Austria (April 1792), \"Jardiniere\" departed from the Spanish port of Málaga on 1 October 1792. From the Cape of Good Hope \"Jardiniere\" sailed across the Indian Ocean to the coast of New Holland (Australia), but two consecutive cyclones prevented the expedition from doing any work there and forced Baudin to take the ship to Bombay for repairs.\n\nFrom Bombay the expedition proceeded to the Persian Gulf, the Red Sea and the east coast of Africa, where it gathered botanical and zoological collections. The expedition came to an abrupt end in June 1794 when \"Jardiniere\" went aground in a storm while attempting to enter Table Bay at the Cape of Good Hope. Baudin survived the wreck and made his way to the United States, from where he went to France. He managed to send \"Jardiniere\"s cargo of natural history specimens to the island of Trinidad.\n\nIn Paris, Baudin visited Antoine de Jussieu at the Museum National d'Histoire Naturelle in March 1796 to suggest a botanical voyage to the Caribbean, during which he would recover the collection of specimens he had left in Trinidad. The Museum and the French government accepted the proposal, and Baudin was appointed commander of an expedition in the ship \"Belle Angélique,\" with four assigned botanists: René Maugé, André Pierre Ledru, Anselme Riedlé and Stanislas Levillain. \"Belle Angélique\" cleared Le Havre on 30 September 1796 for the Canary Islands, where the ship was condemned as unseaworthy. The expedition sailed from the Canaries in a replacement vessel, \"Fanny\", and reached Trinidad in April 1797. The British, who had just captured the island from the Spanish in February 1797, refused to allow Baudin to recover the collection of natural-history specimens. Baudin took \"Fanny\" to St. Thomas and St. Croix, and then to Puerto Rico, specimens being collected in all three islands. At St Croix, \"Fanny\" was replaced by a newly purchased ship, renamed \"Belle Angelique\". The expedition returned to France in June 1798 with a large collection of plants, birds and insects, which was incorporated into Napoleon Bonaparte's triumphal procession celebrating his recent Italian victories.\nOn 24 July 1798, at the suggestion of the Ministry of Marine, Baudin presented to the Assembly of Professors and Administrators of the National Museum of Natural History a plan for a hydrographic-survey expedition to the South Seas, which would include a search for fauna and flora that could be brought back for cultivation in France. The expedition would also have the aim of promoting the economic and commercial interests of France in the regions to be visited. The expedition would require two well-equipped ships, which would carry a team of astronomers, naturalists and scientific draughtsmen over whom Baudin as commander would have absolute authority. The first part of the voyage would be devoted to a thorough exploration of the coast of Chile and the collection of animal, bird and plant specimens suitable for acclimatization in France, followed by a survey of the coasts from Peru to Mexico. The expedition would then continue into the Pacific Ocean, including a visit to Tahiti and the Society Islands, and would be completed with a survey of the yet unexplored south-west coast of New Holland (Australia). After considering this extensive proposal, the French government decided to proceed with an expedition confined to a survey of western and southern New Holland (as Australia was called at the time).\n\nIn October 1800 Baudin was selected to lead what has become known as the Baudin expedition to map the coast of Australia (New Holland). He had two ships, and captained by Hamelin, and a suite of nine zoologists and botanists, including Jean Baptiste Leschenault de la Tour. He reached Australia in May 1801, and would explore and map the western coast and a part of the little-known southern coast of the continent. The scientific expedition proved a great success, with more than 2500 new species discovered.\nThe French also met Aboriginal peoples and treated them with great respect.\n\nIn April 1802 Baudin met Matthew Flinders, also engaged in charting the coastline, in Encounter Bay in present-day South Australia. Baudin then stopped at the British colony at Sydney for supplies. In Sydney he bought a new ship — — named after the wood it was made from. From there he sent home \"Naturaliste\", which had on board all of the specimens that had been discovered by Baudin and his crew. He then headed for Tasmania, before continuing north to Timor. Baudin then sailed for home, stopping at Mauritius.\n\nAccording to recent researches by academics from the University of Adelaide, during Baudin's expedition, François Péron, who had become the chief zoologist and intellectual leader of the mission, wrote a report for Napoleon on ways to invade and capture the British colony at Sydney Cove.\n\nBaudin died of tuberculosis at Mauritius on 16 September 1803, at the age of 49, apparently in the home of Madame Alexandrine Kerivel. Baudin's exact resting place is not known, but the historian Auguste Toussaint believed that he was interred in the Kerivel family vault. However, the historian Edward Duyker likes to think that Baudin was buried in Le Cimetière de l’Ouest in the district of Port Louis \"just a few hundred metres from the explorer’s certain love: the sea\".\n\nIn South Australia, the following places bear Baudin's name – Baudin Beach on Kangaroo Island, Baudin Rocks on the south-east coast of the state and Nicolas Baudin Island on the west coast of Eyre Peninsula. A number of monuments have been established around Australia, including eight at various locations around Western Australia.\n\nFour animals are named in honour of Baudin:\n\n\n\n"}
{"id": "22286", "url": "https://en.wikipedia.org/wiki?curid=22286", "title": "Oligocene", "text": "Oligocene\n\nThe Oligocene () is a geologic epoch of the Paleogene Period and extends from about 33.9 million to 23 million years before the present ( to ). As with other older geologic periods, the rock beds that define the epoch are well identified but the exact dates of the start and end of the epoch are slightly uncertain. The name Oligocene was coined in 1854 by the German paleontologist Heinrich Ernst Beyrich; the name comes from the Ancient Greek (\"olígos\", \"few\") and (\"kainós\", \"new\"), and refers to the sparsity of extant forms of molluscs. The Oligocene is preceded by the Eocene Epoch and is followed by the Miocene Epoch. The Oligocene is the third and final epoch of the Paleogene Period.\nThe Oligocene is often considered an important time of transition, a link between the archaic world of the tropical Eocene and the more modern ecosystems of the Miocene. Major changes during the Oligocene included a global expansion of grasslands, and a regression of tropical broad leaf forests to the equatorial belt.\n\nThe start of the Oligocene is marked by a notable extinction event called the Grande Coupure; it featured the replacement of European fauna with Asian fauna, except for the endemic rodent and marsupial families. By contrast, the Oligocene–Miocene boundary is not set at an easily identified worldwide event but rather at regional boundaries between the warmer late Oligocene and the relatively cooler Miocene.\n\nOligocene faunal stages from youngest to oldest are:\nThe Paleogene Period general temperature decline is interrupted by an Oligocene 7-million-year stepwise climate change. A deeper 8.2 °C, 400,000-year temperature depression leads the 2 °C, seven-million-year stepwise climate change 33.5 Ma (million years ago). The stepwise climate change began 32.5 Ma and lasted through to 25.5 Ma, as depicted in the PaleoTemps chart. The Oligocene climate change was a global increase in ice volume and a 55 m (181 feet) decrease in sea level (35.7–33.5 Ma) with a closely related (25.5–32.5 Ma) temperature depression. The 7-million-year depression abruptly terminated within 1–2 million years of the La Garita Caldera eruption at 28–26 Ma. A deep 400,000-year glaciated Oligocene Miocene boundary event is recorded at McMurdo Sound and King George Island.\n\nDuring this epoch, the continents continued to drift toward their present positions. Antarctica became more isolated and finally developed an ice cap.\n\nMountain building in western North America continued, and the Alps started to rise in Europe as the African plate continued to push north into the Eurasian plate, isolating the remnants of the Tethys Sea. A brief marine incursion marks the early Oligocene in Europe. Marine fossils from the Oligocene are rare in North America. There appears to have been a land bridge in the early Oligocene between North America and Europe, since the faunas of the two regions are very similar. Sometime during the Oligocene, South America was finally detached from Antarctica and drifted north towards North America. It also allowed the Antarctic Circumpolar Current to flow, rapidly cooling the Antarctic continent.\n\nAngiosperms continued their expansion throughout the world as tropical and sub-tropical forests were replaced by temperate deciduous forests. Open plains and deserts became more common and grasses expanded from their water-bank habitat in the Eocene moving out into open tracts. However, even at the end of the period, grass was not quite common enough for modern savannas.\n\nIn North America, subtropical species dominated with cashews and lychee trees present, and temperate trees such as roses, beeches, and pines were common. The legumes spread, while sedges, bulrushes, and ferns continued their ascent.\n\nEven more open landscapes allowed animals to grow to larger sizes than they had earlier in the Paleocene epoch 30 million years earlier. Marine faunas became fairly modern, as did terrestrial vertebrate fauna on the northern continents. This was probably more as a result of older forms dying out than as a result of more modern forms evolving. Many groups, such as equids, entelodonts, rhinos, merycoidodonts, and camelids, became more able to run during this time, adapting to the plains that were spreading as the Eocene rainforests receded. The first felid, \"Proailurus\", originated in Asia during the late Oligocene and spread to Europe.\n\nSouth America was isolated from the other continents and evolved a quite distinct fauna during the Oligocene. The South American continent became home to strange animals such as pyrotheres and astrapotheres, as well as litopterns and notoungulates. Sebecosuchians, terror birds, and carnivorous metatheres, like the borhyaenids remained the dominant predators.\n\nBrontotheres died out in the Earliest Oligocene, and creodonts died out outside Africa and the Middle East at the end of the period. Multituberculates, an ancient lineage of primitive mammals that originated back in the Jurassic, also became extinct in the Oligocene, aside from the gondwanatheres. The Oligocene was home to a wide variety of strange mammals. A good example of this would be the White River Fauna of central North America, which were formerly a semiarid prairie home to many different types of endemic mammals, including entelodonts like \"Archaeotherium\", camelids (such as \"Poebrotherium\"), running rhinoceratoids, three-toed equids (such as \"Mesohippus\"), nimravids, protoceratids, and early canids like \"Hesperocyon\". Merycoidodonts, an endemic American group, were very diverse during this time. In Asia during the Oligocene, a group of running rhinoceratoids gave rise to the indricotheres, like \"Paraceratherium\", which were the largest land mammals ever to walk the Earth.\n\nThe marine animals of Oligocene oceans resembled today's fauna, such as the bivalves. Calcareous cirratulids appeared in the Oligocene. The fossil record of marine mammals is a little spotty during this time, and not as well known as the Eocene or Miocene, but some fossils have been found. The baleen whales and toothed whales had just appeared, and their ancestors, the archaeocete cetaceans began to decrease in diversity due to their lack of echolocation, which was very useful as the water became colder and cloudier. Other factors to their decline could include climate changes and competition with today's modern cetaceans and the carcharhinid sharks, which also appeared in this epoch. Early desmostylians, like \"Behemotops\", are known from the Oligocene. Pinnipeds appeared near the end of the epoch from an otter-like ancestor.\n\nThe Oligocene sees the beginnings of modern ocean circulation, with tectonic shifts causing the opening and closing of ocean gateways. Cooling of the oceans had already commenced by the Eocene/Oligocene boundary, and they continued to cool as the Oligocene progressed. The formation of permanent Antarctic ice sheets during the early Oligocene and possible glacial activity in the Arctic may have influenced this oceanic cooling, though the extent of this influence is still a matter of some significant dispute.\n\nThe opening and closing of ocean gateways: the opening of the Drake Passage; the opening of the Tasmanian Gateway and the closing of the Tethys seaway; along with the final formation of the Greenland–Iceland–Faroes sill; played vital parts in reshaping oceanic currents during the Oligocene. As the continents shifted to a more modern configuration, so too did ocean circulation.\n\nThe Drake Passage is located between South America and Antarctica. Once the Tasmanian Gateway between Australia and Antarctica opened, all that kept Antarctica from being completely isolated by the Southern Ocean was its connection to South America. As the South American continent moved north, the Drake Passage opened and enabled the formation of the Antarctic Circumpolar Current (ACC), which would have kept the cold waters of Antarctica circulating around that continent and strengthened the formation of Antarctic Bottom Water (ABW). With the cold water concentrated around Antarctica, sea surface temperatures and, consequently, continental temperatures would have dropped. The onset of Antarctic glaciation occurred during the early Oligocene, and the effect of the Drake Passage opening on this glaciation has been the subject of much research. However, some controversy still exists as to the exact timing of the passage opening, whether it occurred at the start of the Oligocene or nearer the end. Even so, many theories agree that at the Eocene/Oligocene (E/O) boundary, a yet shallow flow existed between South America and Antarctica, permitting the start of an Antarctic Circumpolar Current.\n\nStemming from the issue of when the opening of the Drake Passage took place, is the dispute over how great of an influence the opening of the Drake Passage had on the global climate. While early researchers concluded that the advent of the ACC was highly important, perhaps even the trigger, for Antarctic glaciation and subsequent global cooling, other studies have suggested that the δO signature is too strong for glaciation to be the main trigger for cooling. Through study of Pacific Ocean sediments, other researchers have shown that the transition from warm Eocene ocean temperatures to cool Oligocene ocean temperatures took only 300,000 years, which strongly implies that feedbacks and factors other than the ACC were integral to the rapid cooling.\n\nThe latest hypothesized time for the opening of the Drake Passage is during the early Miocene. Despite the shallow flow between South America and Antarctica, there was not enough of a deep water opening to allow for significant flow to create a true Antarctic Circumpolar Current. If the opening occurred as late as hypothesized, then the Antarctic Circumpolar Current could not have had much of an effect on early Oligocene cooling, as it would not have existed.\n\nThe earliest hypothesized time for the opening of the Drake Passage is around 30 Ma. One of the possible issues with this timing was the continental debris cluttering up the seaway between the two plates in question. This debris, along with what is known as the Shackleton Fracture Zone, has been shown in a recent study to be fairly young, only about 8 million years old. The study concludes that the Drake Passage would be free to allow significant deep water flow by around 31 Ma. This would have facilitated an earlier onset of the Antarctic Circumpolar Current.\n\nCurrently, an opening of the Drake Passage during the early Oligocene is favored.\n\nThe other major oceanic gateway opening during this time was the Tasman, or Tasmanian, depending on the paper, gateway between Australia and Antarctica. The time frame for this opening is less disputed than the Drake Passage and is largely considered to have occurred around 34 Ma. As the gateway widened, the Antarctic Circumpolar Current strengthened.\n\nThe Tethys Seaway was not a gateway, but rather a sea in its own right. Its closing during the Oligocene had significant impact on both ocean circulation and climate. The collisions of the African plate with the European plate and of the Indian subcontinent with the Asian plate, cut off the Tethys Seaway that had provided a low-latitude ocean circulation. The closure of Tethys built some new mountains (the Zagros range) and drew down more carbon dioxide from the atmosphere, contributing to global cooling.\n\nThe gradual separation of the clump of continental crust and the deepening of the tectonic sill in the North Atlantic that would become Greenland, Iceland, and the Faroe Islands helped to increase the deep water flow in that area. More information about the evolution of North Atlantic Deep Water will be given a few sections down.\n\nEvidence for ocean-wide cooling during the Oligocene exists mostly in isotopic proxies. Patterns of extinction and patterns of species migration can also be studied to gain insight into ocean conditions. For a while, it was thought that the glaciation of Antarctica may have significantly contributed to the cooling of the ocean, however, recent evidence tends to deny this.\n\nIsotopic evidence suggests that during the early Oligocene, the main source of deep water was the North Pacific and the Southern Ocean. As the Greenland-Iceland-Faroe sill deepened and thereby connected the Norwegian–Greenland sea with the Atlantic Ocean, the deep water of the North Atlantic began to come into play as well. Computer models suggest that once this occurred, a more modern in appearance thermo-haline circulation started.\n\nEvidence for the early Oligocene onset of chilled North Atlantic deep water lies in the beginnings of sediment drift deposition in the North Atlantic, such as the Feni and Southeast Faroe drifts.\n\nThe chilling of the South Ocean deep water began in earnest once the Tasmanian Gateway and the Drake Passage opened fully. Regardless of the time at which the opening of the Drake Passage occurred, the effect on the cooling of the Southern Ocean would have been the same.\n\nRecorded extraterrestrial impacts:\n\n\nLa Garita Caldera (28 through 26 million years ago, VEI=9.2)\n\n\n\n"}
{"id": "52278312", "url": "https://en.wikipedia.org/wiki?curid=52278312", "title": "Omicron Orionis", "text": "Omicron Orionis\n\nOmicron Orionis refers to 2 distinct star systems in the constellation Orion:\n\n\nAll of them were member of asterism 參旗 (Sān Qí), \"Banner of Three Stars\", \"Net\" mansion.\n\no Orionis (22 Orionis)\n"}
{"id": "3231202", "url": "https://en.wikipedia.org/wiki?curid=3231202", "title": "Pan-STARRS", "text": "Pan-STARRS\n\nThe Panoramic Survey Telescope and Rapid Response System (Pan-STARRS 1; obs. code: F51 and Pan-STARRS 2 obs. code: F52) located at Haleakala Observatory, Hawaii, USA, consists of astronomical cameras, telescopes and a computing facility that is surveying the sky for moving or variable objects on a continual basis, and also producing accurate astrometry and photometry of already-detected objects.\n\nThe Pan-STARRS Project is a collaboration between the University of Hawaii Institute for Astronomy, MIT Lincoln Laboratory, Maui High Performance Computing Center and Science Applications International Corporation. Telescope construction was funded by the U.S. Air Force.\n\nBy detecting differences from previous observations of the same areas of the sky, Pan-STARRS is discovering a large number of new asteroids, comets, variable stars, supernovae and other celestial objects. Its primary mission is now to detect Near-Earth Objects that threaten impact events and it is expected to create a database of all objects visible from Hawaii (three-quarters of the entire sky) down to apparent magnitude 24. Construction of Pan-STARRS was funded in large part by the United States Air Force through their Research Labs. Additional funding to complete Pan-STARRS2 came from the NASA Near Earth Object Observation Program. Most of the funding presently used to operate the Pan-STARRS telescopes comes from the NASA Near Earth Object Observation Program. The Pan-STARRS NEO survey searches all the sky north of declination −47.5.\n\nThe first Pan-STARRS telescope (PS1) is located at the summit of Haleakalā on Maui, Hawaii, and went online on December 6, 2008, under the administration of the University of Hawaii. PS1 began full-time science observations on May 13, 2010, and the PS1 Science Mission ran until March 2014. Operations were funded by the PS1 Science Consortium, PS1SC, a consortium including the Max Planck Society in Germany, National Central University in Taiwan, Edinburgh, Durham and Queen's Belfast Universities in the UK, and Johns Hopkins and Harvard Universities in the United States and the Las Cumbres Observatory Global Telescope Network. Consortium observations for the all sky (as visible from Hawaii) survey were completed in April 2014.\n\nHaving completed PS1, the Pan-STARRS Project is now focusing on building Pan-STARRS 2 (PS2), for which first light was achieved in 2013, with full science operations scheduled for 2014 and then the full array of four telescopes, sometimes called PS4. Completing the array of four telescopes is estimated at a total cost of US$100 million for the entire array.\n\nAs of mid-2014, Pan-STARRS 2 was in the process of being commissioned. In the wake of substantial funding problems, no clear timeline existed for additional telescopes beyond the second. In March 2018, Pan-STARRS 2 was credited by the Minor Planet Center for the potentially hazardous Apollo asteroid , its first minor-planet discovery made at Haleakala on 13 May 2015.\n\nPan-STARRS currently (2018) consists of two 1.8 m Ritchey–Chrétien telescopes located at Haleakala in Hawaii.\n\nThe initial telescope, PS1, saw first light using a low-resolution camera in June 2006. The telescope has a 3° field of view, which is extremely large for telescopes of this size, and is equipped with the largest digital camera ever built, recording almost 1.4 billion pixels per image. The focal plane has 60 separately mounted close packed CCDs arranged in an 8 × 8 array. The corner positions are not populated, because the optics do not illuminate the corners. Each CCD device, called an Orthogonal Transfer Array (OTA), has 4800 × 4800 pixels, separated into 64 cells, each of 600 × 600 pixels. This gigapixel camera or 'GPC' saw first light on August 22, 2007, imaging the Andromeda Galaxy.\n\nAfter initial technical difficulties that were later mostly solved, PS1 began full operation on May 13, 2010. Nick Kaiser, principal investigator of the Pan-STARRS project, summed it up saying “PS1 has been taking science-quality data for six months, but now we are doing it dusk-to-dawn every night.” (quote: June 15, 2010). The PS1 images however remain slightly less sharp than initially planned, which significantly affects some scientific uses of the data.\n\nEach image requires about 2 gigabytes of storage and exposure times will be 30 to 60 seconds (good enough to record objects down to apparent magnitude 22), with an additional minute or so used for computer processing. Since images will be taken on a continuous basis, it is expected that 10 Terabytes of data will be acquired by PS4 every night. Comparing against a database of known unvarying objects compiled from earlier observations will yield objects of interest: anything that has changed brightness and/or position for any reason. As of June 30/10 University of Hawaii in Honolulu received an $8.4 million contract modification under the PanSTARRS multi-year program to develop and deploy a telescope data management system for the project. At this time, all funds have been committed (FA9451-06-2-0338; P00008)\n\nThe very large field of view of the telescopes and the short exposure times enable approximately 6000 square degrees of sky to be imaged every night. The entire sky is 4π steradians, or 4π × (180/π)² ≈ 41,253.0 square degrees, of which about 30,000 square degrees are visible from Hawaii, which means that the entire sky can be imaged in a period of 40 hours (or about 10 hours per night on four days). Given the need to avoid times when the Moon is bright, this means that an area equivalent to the entire sky will be surveyed four times a month, which is entirely unprecedented. By the end of its initial three-year mission in April 2014, PS1 had imaged the sky 12 times in each of 5 filters (g,r,i,z,y).\n\nPan-STARRS is currently mostly funded by a grant from the NASA Near Earth Object Observations program. It therefore spends 90% of its observing time in dedicated searches for Near Earth Objects.\n\nSystematically surveying the entire sky on a continuous basis is an unprecedented project and is expected to produce a dramatically larger number of discoveries of various types of celestial objects. For instance, the current leading asteroid discovery survey, the Mount Lemmon Survey, reaches an apparent magnitude of 21.5 V and concentrates its searches mostly near the ecliptic; Pan-STARRS will go 3 magnitudes fainter and cover the entire sky visible from Hawaii. The ongoing survey will also complement the efforts to map the infrared sky by the NASA WISE orbital telescope, with the results of one survey complementing and extending the other.\n\nAccording to Defense Industry Daily significant limitations were put on the PS1 survey to avoid recording sensitive objects. Streak detection software (known as \"Magic\") was used to censor pixels containing information about satellites in the image. Early versions of this software were immature, leaving a filling factor of 68% of the full field of view (which figure includes gaps between the detectors), but by March 2010 this had improved to 76%, a small reduction from the approximately 80% available. At the end of 2011, the USAF completely eliminated the masking requirement (for all images, past and future). Thus, with the exception of a few non-functioning OTA cells, the entire field of view can be used.\n\nIn addition to the large number of expected discoveries in the asteroid belt, Pan-STARRS is expected to detect at least 100,000 Jupiter trojans (compared to 2900 known as of end-2008); at least 20,000 Kuiper belt objects (compared to 800 known as of mid-2005); thousands of trojan asteroids of Saturn, Uranus, and Neptune (currently eight Neptune trojans are known, none for Saturn, and one for Uranus); and large numbers of centaurs and comets.\n\nApart from dramatically adding to the number of known Solar System objects, Pan-STARRS will remove or mitigate the observational bias inherent in many current surveys. For instance, among currently known objects there is a bias favoring low orbital inclination, and thus an object such as escaped detection until recently despite its bright apparent magnitude of 17, which is not much fainter than Pluto. Also, among currently known comets, there is a bias favoring those with short perihelion distances. Reducing the effects of this observational bias will enable a more complete picture of Solar System dynamics. For instance, it is expected that the number of Jupiter trojans larger than 1 km may in fact roughly match the number of asteroid-belt objects, although the currently known population of the latter is several orders of magnitude larger. Pan-STARRS data will elegantly complement the WISE (infrared) survey. WISE infrared images will permit an estimate of size for asteroids and trojan objects tracked over longer periods of time by Pan-STARRS.\n\nIn 2017, Pan-STARRS detected the first known interstellar object, 1I/2017 U1 ‘Oumuamua, passing through the Solar System. During the formation of a planetary system, it is thought that a very large number of objects are ejected due to gravitational interactions with planets (as many as 10 such objects in the case of the Solar System). Objects ejected from planetary systems of other stars might plausibly be throughout the Milky Way and some may pass through the Solar System.\n\nPan-STARRS may detect collisions involving small asteroids. These are quite rare and none have yet been observed, but with a dramatic increase in the number of asteroids discovered it is expected from statistical considerations that some collision events may be observed.\n\nIt is expected that Pan-STARRS will discover an extremely large number of variable stars, including such stars in other nearby galaxies; in fact, this may lead to the discovery of hitherto unknown dwarf galaxies. In discovering a large number of Cepheid variables and eclipsing binary stars, it will help determine distances to nearby galaxies with greater precision. It is expected to discover a large number of Type Ia supernovae in other galaxies, which are important in studying the effects of dark energy, and also optical afterglows of gamma ray bursts.\n\nBecause very young stars (such as T Tauri stars) are usually variable, Pan-STARRS should discover a large number of these and improve our understanding of them. It is also expected that Pan-STARRS may discover a large number of extrasolar planets by observing their transits across their parent stars, as well as gravitational microlensing events.\n\nPan-STARRS will also measure proper motion and parallax and should thereby discover a large number of brown dwarfs and white dwarfs and other nearby faint objects, and it should be able to conduct a complete census of all stars within 100 parsecs of the Sun. Prior proper motion and parallax surveys often did not detect faint objects such as the recently discovered Teegarden's star, which are too faint for projects such as Hipparcos.\n\nAlso, by identifying stars with large parallax but very small proper motion for follow-up radial velocity measurements, Pan-STARRS may even be able to permit the detection of hypothetical Nemesis-type objects if these actually exist.\n\n\n"}
{"id": "22112142", "url": "https://en.wikipedia.org/wiki?curid=22112142", "title": "Patriots (novel series)", "text": "Patriots (novel series)\n\nThe \"Patriots\" novel series is a five-novel series by survivalist novelist and former U.S. Army officer and blogger, James Wesley Rawles. It is followed by his \"Counter-Caliphate Chronicles\" novel series.\n\nPatriots: A Novel of Survival in the Coming Collapse, the first book in the series, was first distributed as shareware in 1995 and first published in paperback in 1998. It was updated and re-published in paperback 2009, and then in hardback in 2012. In one week of April 2009, shortly after its release, it was ranked #6 in Amazon.com's overall book sales rankings, which was attributed by the Library Journal to the book's appeal to \"a small but vociferous group of people concerned with survivalism\".\n\nSet in the near future midst hyperinflation and a catastrophic global economic collapse, \"Patriots\" tells the story of a group of survivalists that flee riots and chaos in metropolitan Chicago to a survivalist retreat that they have prepared near Bovill, Idaho.\n\nThe first novel is based on a 19-chapter draft that Rawles wrote in 1990, and first distributed as shareware, under the title \"The Gray Nineties\". It was later expanded to 27 chapters and retitled \"Triple Ought\", and then 33 chapters, under the title \"TEOTWAWKI: The End of the World as We Know It\". In 1997, the rights to the novel were purchased by Huntington House Publishers, a small Christian publishing firm in Lafayette, Louisiana. They abridged the book to 31 chapters and re-titled it \"Patriots: Surviving the Coming Collapse\". This was the publisher's best-selling title from November 1998 to January 2005. In early 2005, Huntington House went out of business, and the copyright reverted to the author. In November 2006, responding to pent-up market demand, Rawles self-published a restored 33 chapter edition of the novel, through XLibris, a vanity press. \"Patriots\" was the best-selling title for XLibris from late 2006 to early 2009. In late 2008, the rights to the novel were purchased by Ulysses Press of Berkeley, California. After updating the novel and adding both a glossary and an index, in April 2009 Ulysses Press released the 33 chapter edition under the new title \"Patriots: A Novel of Survival in the Coming Collapse\".\n\nMuch of \"Patriots\" takes place in the Intermountain west, specifically in the Palouse Hills region, in and around Moscow, Idaho.\n\nCritical reception for the various releases of the book has been generally positive, gaining a cult following among the survivalist community and a positive review from the \"Frankfurter Allgemeine Zeitung\". A reviewer for the \"Tennessean\" newspaper called the novel a \"combination military thriller and how-to survivalist guide.\"\n\nOver time the book has gained a larger following, with Rawles speculating that the ongoing financial crisis accounted for the book's popularity among a wider readership traditionally not interested in survivalist themes. Sara Nelson of \"The Daily Beast\" referred to the novel as \"The Most Dangerous Novel in America.\"\n\nThe Kirkus Review described the series as \"long on details about guns, survival techniques and military capabilities and short on the suspense\".\n\nThe first translation of \"Patriots\" was released in May, 2012. This was a Spanish edition, titled \"Patriotas\". It was translated by Ernesto Rubio Garcia and published by La Factoria De Ideas, in Madrid, Spain. .\n\nRawles authored four sequels in the \"Patriots\" series. The first two sequels were published by Simon & Schuster: \"Survivors: A Novel of the Coming Collapse\" in 2011 and \"Founders: A Novel of the Coming Collapse\" in 2012. Both of these sequels were also been produced as audiobooks (by Brilliance Audio), and as e-books. In 2013, E.P. Dutton released the fourth novel in the series, titled \"Expatriates\". This sequel is set primarily in Australia, the Philippines, and Tavares, Florida. This was followed in 2014 by a fourth sequel, titled \"Liberators: A Novel of the Coming Global Collapse\". It was released on October 21, 2014.\n\nSales for the sequels were strong, with \"Survivors\", \"Founders\" and \"Expatriates\" all achieving places on \"The New York Times\" Best Seller list. \nMuch of Survivors: A Novel of the Coming Collapse takes place in the Four Corners region, specifically in and around Bloomfield and Farmington, New Mexico, although the book's climax takes place in and near Prescott, Arizona and several sub-plots take place as far away as Afghanistan. The cover artwork was created by Tony Mauro, Jr.\n\nOn its release day, October 4, 2011, \"Survivors\" rose to #2 in Amazon's overall book sales ranks and #1 in their action-adventure category. On October 23, 2011, it was listed at #3 in the New York Times bestseller list in the fiction hardback category. Rawles and \"Survivors\" were the centerpiece of a Vancouver Sun article by Kim Murphy about the American Redoubt movement that was run by dozens of newspapers, including the \"Los Angeles Times\" A review in \"The New American\" magazine was positive, summarizing: \"In short, Rawles' Survivors is well worth reading; astute readers may find themselves making notes of passages pertaining to survival planning which will be worth returning to once one has finished reading the novel.\" Marvin Olasky of \"World (magazine)\" called \"Survivors\" \"...not as well-written as some articles Rawles has penned\"\n\nRawles uses an unusual contemporaneous approach to writing sequels. Rather than the traditional formula of following the same group of characters farther into the future, he instead uses a novel sequence method that portrays different characters in different geographic regions, but in the same near-future timeframe as in \"Patriots.\" In his Introductory note to \"Survivors\", Rawles stated: \"Unlike most novel sequels, the storyline of \"Survivors\" is contemporaneous with the events described in my previously-published novel \"Patriots\". Thus, there is no need to read it first (or subsequently), but you'll likely find it entertaining.\"\n\nThe first of several translations of \"Survivors\" was released in May 2014. This was a Spanish edition, titled \"Supervivientes\". It was translated by Ernesto Rubio Garcia and published by La Factoria De Ideas, in Madrid, Spain. . A Kindle edition in Spanish was also released in May, 2014. Additional translations into French, German, Russian, Bulgarian, Portuguese, and Korean are planned.\n\nFounders: A Novel of the Coming Collapse is a 2012 \"New York Times\" best-selling novel by author James Wesley Rawles and is a sequel to \"Survivors: A Novel of the Coming Collapse\". The novel was released on September 25, 2012, by Atria Books. The book peaked at #4 in Amazon's overall book sales ranks, on its release day. The book premiered on the New York Times Bestsellers list at #11, but dropped to #27 a week later.\n\n\"Founders: A Novel of the Coming Collapse\" is set primarily in Kentucky, Tennessee, and Montana. It details how U.S. Army Captain Andy Laine infiltrates the Provisional Government's New Army headquartered at Fort Knox in the midst of a War of Resistance. It also details the cross-country trek of Ken and Terry Layton, and introduces a new character: Joshua Watanabe, a U.S. Air Force NCO, stationed at Malmstrom Air Force Base, in Montana. The cover artwork was created by Tony Mauro, Jr.\n\n\"Founders\" was released on September 25, 2012, by Atria Books. The book premiered on the New York Times Bestsellers list at #11, but dropped to #27 a week later.\n\nIn his brief review of \"Founders\", Gregory Cowles of the influential \"The New York Times Book Review\" poked fun at the comma in Rawles's name, but granted: \"Rawles is a well-known survivalist, and he's surely the only writer on this list whose fans frequently ask him how best to stockpile food (it depends on which food) or whether to favor bullets over gold during the total collapse of civilization (\"You can't defend yourself near as well with a Krugerrand\").\"\n\nThe third sequel in the Patriots series is entitled \"Expatriates: A Novel of the Coming Global Collapse\" (). It was written under contract for E.P. Dutton. The book was released on October 1, 2013. It is set \"primarily in central Florida, the Philippines, and northern Australia.\" The cover artwork was created by Tony Mauro, Jr. The audio book was narrated by Eric G. Dove. The publisher's web page summarizes the storyline: \"When the United States suffers a major socioeconomic collapse, a power vacuum sweeps the globe. A newly radicalized Islamic government rises to power in Indonesia, invades the Philippines, East Timor, Papua New Guinea, and finally northern Australia. No longer protected by American military interests, Australia must repel an invasion alone.\"\n\nThe fourth and final sequel in the \"Patriots\" novel series is a 416-page book entitled \"Liberators: A Novel of the Coming Global Collapse\". It was released on October 21, 2014. This sequel was written under contract for E.P. Dutton. It is set primarily in the Bella Coola region of western Canada.\n\nReviews of \"Liberators\" were also positive. Publishers Weekly called \"Liberators\" the \"rousing fifth after-the-apocalypse thriller [installment in the novel series]\" and also mentioned that \"Supporters of the 'prepper' movement…will lap up every detail.\" Jeff Soyer of \"North Country Review of Books\" gave the novel a three star rating overall, and a four star rating for Writing Style. Mark Rubinstein of The Huffington Post called the book \"[A]nother entertaining and thought-provoking novel, describing steps people can take in the event of a global collapse.\" In a radio interview on October 20, 2014, Alan Colmes mentioned that the novel's title indicates that Rawles actually expects an economic collapse in the near future, and Rawles confirmed that because of uncontrolled government spending and indebtedness he does indeed anticipate a collapse. The novel debuted at #48 in Amazon.com's overall rankings, #1 in their Science Fiction-Dystopian novels category, #1 in their Mystery novels category, and at #1 in their Action & Adventure, War & Military novels category. The novel premiered at #20 on the \"Publishers Weekly\" hardcover bestsellers list, reported on November 3, 2014.\n\nRawles followed \"Patriots\" series with the \"Counter-Caliphate Chronicles\" novel series. On December 1, 2015, Rawles released the novel \"Land of Promise\", the first book in the \"Counter-Caliphate Chronicles\" novel series. This science fiction novel is a geopolitical thriller that is a considerable departure from his previous \"Patriots\" thriller novel series. Set in the late 2130s, \"Land of Promise\" fictionally describes the world under the economic and military domination of a Global Islamic Caliphate, brought about by a fictional new branch of Islam, called The Thirdists. The novel also describes the establishment of a Christian nation of refuge called The Ilemi Republic, in East Africa. It is the first release from Liberty Paradigm Publishing, a publishing venture launched by Rawles in partnership with his literary agent Robert Gottlieb of Trident Media Group.\n\n\"It's in our fallen, sinful nature for tyrants to rise up in every nation. And unfortunately, it's also in our nature that the vast majority in every nation is either too stupid or too apathetic to do anything about it until the tyrants have put up their barbed wire and spilled a lot of blood.\" - Protagonist wife Mary Gray, in \"Patriots\"\n\n\n"}
{"id": "99773", "url": "https://en.wikipedia.org/wiki?curid=99773", "title": "Prajapati", "text": "Prajapati\n\nPrajapati (IAST: \"\", \"lord of creation and protector\") is a Vedic deity of Hinduism. The term also connotes many different gods, depending on the Hindu text, ranging from being the creator god to being same as one of the following: Brahma, Vishnu, Shiva, Agni, Indra, Vishvakarma, Bharata, Kapila and many others. According to George Williams, the inconsistent, varying and evolving Prajapati concept in Hindu mythology reflects the diverse Hindu cosmology. In classical and medieval era literature, Prajapati is equated to the metaphysical concept called Brahm as Prajapati-Brahm (Svayambhu Brahm), or alternatively Brahm is described as one who existed before Prajapati.\n\nPrajapati (Sanskrit: ) is a compound of \"praja\" (creation, procreative powers) and \"pati\" (lord, master). The term means \"lord of creatures\", or \"lord of all born beings\". In the later Vedic texts, Prajapati is a distinct Vedic deity, but whose significance diminishes. Later, the term is synonymous with other gods, particularly Brahma or Vishnu or Shiva. Still later, the term evolves to mean any divine, semi-divine or human sages who create something new.\n\nThe origins of Prajapati are unclear. He appears late in the Vedic layer of texts, and the hymns that mention him provide different cosmological theories in different chapters. He is missing from the Samhita layer of Vedic literature, conceived in the Brahmana layer, states Jan Gonda. Prajapati is younger than Savitr, and the word was originally an epithet for the sun. His profile gradually rises in the Vedas, peaking within the Brahmanas. Scholars such as Renou, Keith and Bhattacharji posit Prajapati originated as an abstract or semi-abstract deity in the later Vedic milieu as speculations evolved from the archaic to more learned speculations.\n\nA possible connection between Prajapati (and related figures in Indian tradition) and the Prōtogonos (, literally \"first-born\") of the Greek Orphic tradition has been proposed:\n\nAccording to Robert Graves, the name of /PRA-JĀ[N]-pati/ ('progeny-potentate') is etymologically equivalent to that of the oracular god at Colophon (according to Makrobios), namely /prōtogonos/. The cosmic egg concept linked to Prajapati and Protogonos is common in many parts of the world, states David Leeming, which appears in later Orphic cult in Greece.\n\nPrajapati is described in many ways and inconsistently in Hindu texts, both in the Vedas and in the post-Vedic texts. These range from being the creator god to being same as one of the following: Brahma, Vishnu, Shiva, Agni, Indra, Vishvakarma, Bharata, Kapila and many others.\n\nHis role varies within the Vedic texts such as being one who created heaven and earth, all of water and beings, the chief, the father of gods, the creator of \"devas\" and \"asuras\", the cosmic egg and the Purusha (spirit). His role peaked in the Brahmanas layer of Vedic text, then declined to being a group of helpers in the creation process. In some Brahmana texts, his role remains ambiguous since he co-creates with the powers with goddess Vac (sound).\n\nIn the \"Rigveda\", Prajapati appears as an epithet for Savitr, Soma, Agni and Indra, who are all praised as equal, same and lord of creatures. Elsewhere, in hymn 10.121 of the \"Rigveda\", is described \"Hiranyagarbha\" (golden embryo) that was born from the waters containing everything, which produced Prajapati. It then created \"manah\" (mind), \"kama\" (desire) and \"tapas\" (heat). However, this Prajapati is a metaphor, one of many Hindu cosmology theories, and there is no supreme deity in the \"Rigveda\". One of the striking features about the Hindu Prajapati myths, states Jan Gonda, is the idea that work of creation is a gradual process, completed in stages of trial and improvement.\n\nIn the \"Shatapatha Brahmana\", embedded inside the \"Yajurveda\", Prajapati emanated from Purusha (cosmic spirit) and Prajapati co-creates the world with goddess of Language. It also includes the \"golden cosmic egg\" mythology, wherein Prajapati is stated to be born from a golden egg in primeval sea after the egg was incubated for a year. His sounds became the sky, the earth and the seasons. When he inhaled, he created the devas (gods), fire and light. When he exhaled, he created the asuras (demons) and darkness. Then, together with goddess of Language, he created all beings and time. In Chapter 10 of the \"Shatapatha Brahmana\", as well as chapter 13 of \"Pancavimsa Brahmana\", is presented another theory wherein he (Prajapati) is a mother, becomes self-pregnant with all living creatures self-generated, evil \"Mrtyu\" seizes these beings within his womb, but because these beings are part of the eternal Prajapati, they desire to live long like him.\n\nThe \"Aitareya Brahmana\" offers a different myth, wherein Prajapati having created the gods, metamorphosed into a stag and approached his daughter dawn who was in the form of a doe, to produce other earthly beings. The gods were horrified by the incest, and joined forces to produce angry destructive Rudra to punish Prajapati for \"doing what is not done\". Prajapati was killed by Rudra. The \"Kausitaki Brahmana\" offers a yet another myth, wherein Prajapati created from his own self fire, sun, moon, wind and feminine dawn. The first four saw dawn and released their seeds, which became existence (\"Bhava\"). \n\nIn section 2.266 of Jaiminiya Brahmana, Prajapati is presented as a spiritual teacher. His student Varuna lives with him for 100 years, studying the art and duties of being the \"father-like king of gods\".\n\nPrajapati appears in early Upanishads, among the most influential texts in Hinduism. He is described in the Upanishads in diverse ways. For example, in different Upanishads, he is presented as the personification of creative power after Brahman, the same as the wandering eternal soul, as symbolism for unmanifest obscure first born, as manifest procreative sexual powers, the knower particularly of Atman (soul, self), a spiritual teacher that is within each person. The \"Chandogya Upanishad\", as an illustration, presents him as follows:\n\nIn the \"Mahabharata\", Brahma is declared to be a Prajapati who creates many males and females, and imbues them with desire and anger, the former to drive them into reproducing themselves and the latter to prevent them from being like gods. Other chapters of the epics and Puranas declare Shiva or Vishnu to be Prajapati.\n\nThe \"Bhagavad Gita\" uses the epithet Prajapati to describe Krishna, along with many other epithets.\n\nThe Grhyasutras include Prajapati as among the deities invoked during wedding ceremonies and prayed to for blessings of prosperous progeny, and harmony between husband and wife.\n\nPrajapati is identified with the personifications of Time, Fire, the Sun, etc. He is also identified with various mythical progenitors, especially (Manu Smrti 1.34) the ten lords of created beings first created by Brahmā, the Prajapatis Marichi, Atri, Angiras, Pulastya, Pulaha, Kratu, Vasishtha, Prachetas or Daksha, Bhrigu, Nārada.\n\nIn the Puranas, there are groups of Prajapati called \"Prajapatayah\" who were rishis (sages) or \"grandfathers\" from whom all of humanity resulted, followed by Prajapatis list that widely varies in number and name between different texts. According to George Williams, the inconsistent, varying and evolving Prajapati concept in Hindu mythology reflects the diverse Hindu cosmology.\n\nThe \"Mahabharata\" and the genre of Puranas call various gods and sages as Prajapati. Some illustrations, states Roshen Dalal, include Agni, Bharata, Shashabindu, Shukra, Havirdhaman, Indra, Kapila, Kshupa, Prithu-Vainya, Soma, Svishtakrit, Tvashtr, Vishvakarma and Virana.\n\nIn the medieval era texts of Hinduism, Prajapati refers to legendary agents of creation, working as gods or sages, who appear in every cycle of creation-maintenance-destruction (\"manvantara\"). Their numbers vary between seven, ten, sixteen or twenty-one. \n\nTheir creative role varies. Pulaha, for example, is the mythical mind-born son of Brahma and a great rishi. As one of the Prajapatis, he helps create living wildlife such as lions, tigers, bears, wolves, as well mythical beasts such as kimpurushas and shalabhas.\n\nHindu temples in Bali Indonesia called \"Pura Prajapati\", also called \"Pura Mrajapati\", are common. They are most associated with funeral rituals and the Ngaben (cremation) ceremony for the dead.\n\n\n\n"}
{"id": "203200", "url": "https://en.wikipedia.org/wiki?curid=203200", "title": "Quark star", "text": "Quark star\n\nA quark star is a hypothetical type of compact exotic star, where extremely high temperature and pressure has forced nuclear particles to form a continuous state of matter that consists primarily of free quarks.\n\nIt is well known that massive stars can collapse to form neutron stars, under extreme temperatures and pressures. In simple terms, neutrons usually have space separating them due to degeneracy pressure keeping them apart. Under extreme conditions such as a neutron star, the pressure separating nucleons is overwhelmed by gravity, and the separation between them breaks down, causing them to be packed extremely densely and form an immensely hot and dense state known as neutron matter, where they are only held apart by the strong interaction. Because these neutrons are made of quarks, it is hypothesized that under even more extreme conditions, the degeneracy pressure keeping the quarks apart within the neutrons might break down in much the same way, creating an ultra-dense phase of degenerate matter based on densely packed quarks. This is seen as plausible, but is very hard to prove, as scientists cannot easily create the conditions needed to investigate the properties of quark matter, so it is unknown whether this actually occurs.\n\nIf quark stars can form, then the most likely place to find quark star matter would be inside neutron stars that exceed the internal pressure needed for quark degeneracy - the point at which neutrons (which are formed from quarks bound together) break down into a form of dense quark matter. They could also form if a massive star collapses at the end of its life, provided that it is possible for a star to be large enough to collapse beyond a neutron star but not large enough to form a black hole. However, as scientists are unable so far to explore most properties of quark matter, the exact conditions and nature of quark stars, and their existence, remain hypothetical and unproven. The question whether such stars exist and their exact structure and behavior is actively studied within astrophysics and particle physics.\n\nIf they exist, quark stars would resemble and be easily mistaken for neutron stars: they would form in the death of a massive star in a Type II supernova, they would be extremely dense and small, and possess a very high gravitational field. They would also lack some features of neutron stars, unless they also contained a shell of neutron matter, because free quarks are not expected to have properties matching degenerate neutron matter. For example, they might be radio-silent, or not have typical size, electromagnetic, or temperature measurements, compared to other neutron stars.\n\nThe hypothesis about quark stars was first proposed in 1965 by Soviet physicists D. D. Ivanenko and D. F. Kurdgelaidze. Their existence has not been confirmed. The equation of state of quark matter is uncertain, as is the transition point between neutron-degenerate matter and quark matter. Theoretical uncertainties have precluded making predictions from first principles. Experimentally, the behaviour of quark matter is being actively studied with particle colliders, but this can only produce very hot (above 10 K) quark-gluon plasma blobs the size of atomic nuclei, which decay immediately after formation. The conditions inside compact stars with extremely high densities and temperatures well below 10 K can not be recreated artificially, so there are no known methods to produce, store or study \"cold\" quark matter directly as it would be found inside quark stars. The theory predicts quark matter to possess some peculiar characteristics under these conditions.\n\nIt is theorized that when the neutron-degenerate matter, which makes up neutron stars, is put under sufficient pressure from the star's own gravity or the initial supernova creating it, the individual neutrons break down into their constituent quarks (up quarks and down quarks), forming what is known as quark matter. This conversion might be confined to the neutron star's center or it might transform the entire star, depending on the physical circumstances. Such a star is known as a quark star.\n\nOrdinary quark matter consisting of up and down quarks (also referred to as \"u\" and \"d\" quarks) has a very high Fermi energy compared to ordinary atomic matter and is only stable under extreme temperatures and/or pressures. This suggests that the only stable quark stars will be neutron stars with a quark matter core, while quark stars consisting entirely of ordinary quark matter will be highly unstable and dissolve spontaneously.\n\nIt has been shown that the high Fermi energy making ordinary quark matter unstable at low temperatures and pressures can be lowered substantially by the transformation of a sufficient number of u and d quarks into strange quarks, as strange quarks are, relatively speaking, a very heavy type of quark particle. This kind of quark matter is known specifically as strange quark matter and it is speculated and subject to current scientific investigation whether it might in fact be stable under the conditions of interstellar space (i.e. near zero external pressure and temperature). If this is the case (known as the Bodmer–Witten assumption), quark stars made entirely of quark matter would be stable if they quickly transform into strange quark matter.\n\nQuark stars made of strange quark matter are known as strange stars, and they form a subgroup under the quark star category.\n\nTheoretical investigations have revealed that quark stars might not only be produced from neutron stars and powerful supernovas, they could also be created in the early cosmic phase separations following the Big Bang. If these primordial quark stars transform into strange quark matter before the external temperature and pressure conditions of the early Universe makes them unstable, they might turn out stable, if the Bodmer–Witten assumption holds true. Such primordial strange stars could survive to this day.\n\nQuark stars have some special characteristics that separate them from ordinary neutron stars.\n\nUnder the physical conditions found inside neutron stars, with extremely high densities but temperatures well below 10 K, quark matter is predicted to exhibit some peculiar characteristics. It is expected to behave as a Fermi liquid and enter a so-called color-flavor-locked (CFL) phase of color superconductivity, where \"color\" refers to the six \"charges\" exhibited in the strong interaction, instead of the positive and the negative charges in electromagnetism. At slightly lower densities, corresponding to higher layers closer to the surface of the compact star, the quark matter will behave as a non-CFL quark liquid, a phase that is even more mysterious than CFL and might include color conductivity and/or several additional yet undiscovered phases. None of these extreme conditions can currently be recreated in laboratories so nothing can be inferred about these phases from direct experiments.\n\nIf the conversion of neutron-degenerate matter to (strange) quark matter is total, a quark star can to some extent be imagined as a single gigantic hadron. But this \"hadron\" will be bound by gravity, rather than the strong force that binds ordinary hadrons.\n\nAt least under the assumptions mentioned above, the probability of a given neutron star being a quark star is low, so in the Milky Way there would only be a small population of quark stars. If it is correct however, that overdense neutron stars can turn into quark stars, that makes the possible number of quark stars higher than was originally thought, as observers would be looking for the wrong type of star.\n\nQuark stars and strange stars are entirely hypothetical , but there are several candidates.\n\nObservations released by the Chandra X-ray Observatory on April 10, 2002 detected two possible quark stars, designated RX J1856.5-3754 and 3C58, which had previously been thought to be neutron stars. Based on the known laws of physics, the former appeared much smaller and the latter much colder than it should be, suggesting that they are composed of material denser than neutron-degenerate matter. However, these observations are met with skepticism by researchers who say the results were not conclusive; and since the late 2000s, the possibility that RX J1856 is a quark star has been excluded.\n\nAnother star, XTE J1739-285, has been observed by a team led by Philip Kaaret of the University of Iowa and reported as a possible quark star candidate.\n\nIn 2006, Y. L. Yue et al., from Peking University, suggested that PSR B0943+10 may in fact be a low-mass quark star.\n\nIt was reported in 2008 that observations of supernovae SN 2006gy, SN 2005gj and SN 2005ap also suggest the existence of quark stars. It has been suggested that the collapsed core of supernova SN 1987A may be a quark star.\n\nIn 2015, Z.G. Dai et al. from Nanjing University suggested that Supernova ASASSN-15lh is a newborn strange quark star.\n\nApart from ordinary quark matter and strange quark matter, other types of quark-gluon plasma might theoretically occur or be formed inside neutron stars and quark stars. This includes the following, some of which has been observed and studied in laboratories:\n\n\n\n\n"}
{"id": "53716476", "url": "https://en.wikipedia.org/wiki?curid=53716476", "title": "Seaweed fuel", "text": "Seaweed fuel\n\nSeaweed fuel or seaweed oil is an alternative to liquid fossil fuels that uses seaweed as its source of energy-rich oils. Like fossil fuel, seaweed fuel releases when burnt, but unlike fossil fuel, algae fuel and other biofuels only release recently removed from the atmosphere via photosynthesis as the seaweed or plant grew.\n\nSeaweed does not require fresh water and can as such be grown in the sea.\n\nSeaweed can be converted into various types of fuels, depending on the technique and the part of the cells used. Also, using techniques such as fast hydrothermal liquefaction, oil yield for the seaweed can be increased.\n\nTransesterification of seaweed oil (into biodiesel) is possible with species such as \"Chaetomorpha linum\", \"Ulva lactuca\", \"Enteromorpha compressa\" (\"Ulva\").\n\nFollowing species are being investigated as suitable species for producing ethanol and/or butanol from:\n\n"}
{"id": "34147023", "url": "https://en.wikipedia.org/wiki?curid=34147023", "title": "Slack bus", "text": "Slack bus\n\nIn electrical power systems a slack bus (or swing bus), defined as a Vδ bus, is used to balance the active power |P| and reactive power |Q| in a system while performing load flow studies. The slack bus is used to provide for system losses by emitting or absorbing active and/or reactive power to and from the system.\n\nFor power systems engineers, a load flow study explains the power system conditions at various intervals during operation. It aims to minimize the difference between the calculated and actual quantities. Here, the slack bus can contribute to the minimization by having an unconstrained real and reactive power input.\n\nformula_1\n\nformula_2\n\nformula_3\n\nThe use of a slack bus has an inherent disadvantage when dealing with uncertain input variables: the slack bus must absorb all uncertainties arising from the system and thus must have the widest possible nodal power distributions. Even moderate amounts of uncertainty in a large system may allow the resulting distributions to contain values beyond the slack bus's margins.\n\nA load flow approach able to directly incorporate uncertainties into the solution processes can be very useful. The results from such analyses give solutions over the range of the uncertainties, i.e., solutions that are sets of values or regions instead of single values.\n\nBuses are of 3 types and are classified as:\n\nThe slack bus provides or absorbs active and reactive power to and from the transmission line to provide for losses, since these variables are unknown until the final solution is established. The slack bus is the only bus for which the system reference phase angle is defined. From this, the various angular differences can be calculated in the power flow equations. If a slack bus is not specified, then a generator bus with maximum real power |P| acts as the slack bus. A given scheme can involve more than one slack bus.\n\nThe most common formulation of the load flow problem specifies all input variables (PQ at loads, PV at generators) as deterministic values. Each set of specified values corresponds to one system state, which depends on a set of system conditions. When those conditions are uncertain, numerous scenarios must be analyzed.\n\nA classic load flow analysis consists of calculating voltage magnitude and phase angle at the buses, as well as the active and reactive line flows for the specified terminal (or bus conditions). Four variables are associated with each bus:\n\nBased on these values, a bus may be classified into the above-mentioned three categories as -\n\nReal and reactive powers (i.e. complex power) cannot be fixed. The net complex power flow into the network is not known in advance, and the system power losses are unknown until the study is complete. It is necessary to have one bus (i.e. the slack bus) at which complex power is unspecified so that it supplies the difference in the total system load plus losses and the sum of the complex powers specified at the remaining buses. The complex power allocated to this bus is computed as part of the solution. In order for the variations in real and reactive powers of the slack bus to be a small percentage of its generating capacity during the solution process, the bus connected to the largest generating station is normally selected as the slack bus. The slack bus is crucial to a load flow problem since it will account for transmission line losses. In a load flow problem, conservation of energy results in the total generation equaling to the sum of the loads. However, there still would be a discrepancy in these quantities due to line losses, which are dependent on line current. Yet to determine line current, angles and voltages of the buses connected to the line would be needed. Here, the slack bus will be required to account for line losses and serve as a generator, injecting real power to the system.\n\nThe solution requires mathematical formulation and numerical solution. Since load flow problems generate non-linear equations that computers cannot solve quickly, numerical methods are required. The following methods are commonly used algorithms:\n\n\n\n"}
{"id": "130312", "url": "https://en.wikipedia.org/wiki?curid=130312", "title": "Storm", "text": "Storm\n\nA storm is any disturbed state of an environment or in an astronomical body's atmosphere especially affecting its surface, and strongly implying severe weather. It may be marked by significant disruptions to normal conditions such as strong wind, tornadoes, hail, thunder and lightning (a thunderstorm), heavy precipitation (snowstorm, rainstorm), heavy freezing rain (ice storm), strong winds (tropical cyclone, windstorm), or wind transporting some substance through the atmosphere as in a dust storm, blizzard, sandstorm, etc.\n\nStorms have the potential to harm lives and property via storm surge, heavy rain or snow causing flooding or road impassibility, lightning, wildfires, and vertical wind shear. Systems with significant rainfall and duration help alleviate drought in places they move through. Heavy snowfall can allow special recreational activities to take place which would not be possible otherwise, such as skiing and snowmobiling.\nThe English word comes from Proto-Germanic \"*sturmaz\" meaning \"noise, tumult\".\n\nStorms are created when a center of low pressure develops with the system of high pressure surrounding it. This combination of opposing forces can create winds and result in the formation of storm clouds such as cumulonimbus. Small localized areas of low pressure can form from hot air rising off hot ground, resulting in smaller disturbances such as dust devils and whirlwinds.\n\nThere are many varieties and names for storms:\n\n\nA strict meteorological definition of a terrestrial storm is a wind measuring 10 or higher on the Beaufort scale, meaning a wind speed of 24.5 m/s (89 km/h, 55 mph) or more; however, popular usage is not so restrictive. Storms can last anywhere from 12 to 200 hours, depending on season and geography. In North America, the east and northeast storms are noted for the most frequent repeatability and duration, especially during the cold period. Big terrestrial storms alter the oceanographic conditions that in turn may affect food abundance and distribution: strong currents, strong tides, increased siltation, change in water temperatures, overturn in the water column, etc. \nStorms do not only occur on Earth; other planetary bodies with a sufficient atmosphere (gas giants in particular) also undergo stormy weather. The Great Red Spot on Jupiter provides a well-known example. Though technically an anticyclone, with greater than hurricane wind speeds, it is larger than the Earth and has persisted for at least 340 years, having first been observed by astronomer Galileo Galilei. Neptune also had its own lesser-known Great Dark Spot.\n\nIn September 1994, the Hubble telescope – using Wide Field Planetary Camera 2 – imaged storms on Saturn generated by upwelling of warmer air, similar to a terrestrial thunderhead. The east-west extent of the same-year storm equalled the diameter of Earth. The storm was observed earlier in September 1990 and acquired the name Dragon Storm.\n\nThe dust storms of Mars vary in size, but can often cover the entire planet. They tend to occur when Mars comes closest to the Sun, and have been shown to increase the global temperature.\n\nOne particularly large Martian storm was exhaustively studied up close due to coincidental timing. When the first spacecraft to successfully orbit another planet, Mariner 9, arrived and successfully orbited Mars on 14 November 1971, planetary scientists were surprised to find the atmosphere was thick with a planet-wide robe of dust, the largest storm ever observed on Mars. The surface of the planet was totally obscured. Mariner 9's computer was reprogrammed from Earth to delay imaging of the surface for a couple of months until the dust settled, however, the surface-obscured images contributed much to the collection of Mars atmospheric and planetary surface science.\n\nTwo extrasolar planets are known to have storms: HD 209458 b and HD 80606 b. The former's storm was discovered on June 23, 2010 and measured at 6,200 km/h, while the latter produces winds of 17,700 kilometers (11,000 mi) per hour across the surface. The spin of the planet then creates giant swirling shock-wave storms that carry the heat aloft.\n\nShipwrecks are common with the passage of strong tropical cyclones. Such shipwrecks can change the course of history, as well as influence art and literature. A hurricane led to a victory of the Spanish over the French for control of Fort Caroline, and ultimately the Atlantic coast of North America, in 1565.\n\nStrong winds from any storm type can damage or destroy vehicles, buildings, bridges, and other outside objects, turning loose debris into deadly flying projectiles. In the United States, major hurricanes comprise just 21% of all landfalling tropical cyclones, but account for 83% of all damage. Tropical cyclones often knock out power to tens or hundreds of thousands of people, preventing vital communication and hampering rescue efforts. Tropical cyclones often destroy key bridges, overpasses, and roads, complicating efforts to transport food, clean water, and medicine to the areas that need it. Furthermore, the damage caused by tropical cyclones to buildings and dwellings can result in economic damage to a region, and to a diaspora of the population of the region.\n\nThe storm surge, or the increase in sea level due to the cyclone, is typically the worst effect from landfalling tropical cyclones, historically resulting in 90% of tropical cyclone deaths. The relatively quick surge in sea level can move miles/kilometers inland, flooding homes and cutting off escape routes. The storm surges and winds of hurricanes may be destructive to human-made structures, but they also stir up the waters of coastal estuaries, which are typically important fish breeding locales.\n\nCloud-to-ground lightning frequently occurs within the phenomena of thunderstorms and have numerous hazards towards landscapes and populations. One of the more significant hazards lightning can pose is the wildfires they are capable of igniting. Under a regime of low precipitation (LP) thunderstorms, where little precipitation is present, rainfall cannot prevent fires from starting when vegetation is dry as lightning produces a concentrated amount of extreme heat. Wildfires can devastate vegetation and the biodiversity of an ecosystem. Wildfires that occur close to urban environments can inflict damages upon infrastructures, buildings, crops, and provide risks to explosions, should the flames be exposed to gas pipes. Direct damage caused by lightning strikes occurs on occasion. In areas with a high frequency for cloud-to-ground lightning, like Florida, lightning causes several fatalities per year, most commonly to people working outside.\n\nPrecipitation with low potential of hydrogen levels (pH), otherwise known as acid rain, is also a frequent risk produced by lightning. Distilled water, which contains no carbon dioxide, has a neutral pH of 7. Liquids with a pH less than 7 are acidic, and those with a pH greater than 7 are bases. “Clean” or unpolluted rain has a slightly acidic pH of about 5.2, because carbon dioxide and water in the air react together to form carbonic acid, a weak acid (pH 5.6 in distilled water), but unpolluted rain also contains other chemicals. Nitric oxide present during thunderstorm phenomena, caused by the splitting of nitrogen molecules, can result in the production of acid rain, if nitric oxide forms compounds with the water molecules in precipitation, thus creating acid rain. Acid rain can damage infrastructures containing calcite or other solid chemical compounds containing carbon. In ecosystems, acid rain can dissolve plant tissues of vegetations and increase acidification process in bodies of water and in soil, resulting in deaths of marine and terrestrial organisms.\n\nHail damage to roofs often goes unnoticed until further structural damage is seen, such as leaks or cracks. It is hardest to recognize hail damage on shingled roofs and flat roofs, but all roofs have their own hail damage detection problems. Metal roofs are fairly resistant to hail damage, but may accumulate cosmetic damage in the form of dents and damaged coatings. Hail is also a common nuisance to drivers of automobiles, severely denting the vehicle and cracking or even shattering windshields and windows. Rarely, massive hailstones have been known to cause concussions or fatal head trauma. Hailstorms have been the cause of costly and deadly events throughout history. One of the earliest recorded incidents occurred around the 9th century in Roopkund, Uttarakhand, India. The largest hailstone in terms of diameter and weight ever recorded in the United States fell on July 23, 2010 in Vivian, South Dakota in the United States; it measured in diameter and in circumference, weighing in at . This broke the previous record for diameter set by a hailstone 7 inches diameter and 18.75 inches circumference which fell in Aurora, Nebraska in the United States on June 22, 2003, as well as the record for weight, set by a hailstone of that fell in Coffeyville, Kansas in 1970.\n\nVarious hazards, ranging from hail to lightning can affect outside technology facilities, such as antennas, satellite dishes, and towers. As a result, companies with outside facilities have begun installing such facilities underground, in order to reduce the risk of damage from storms.\n\nSubstantial snowfall can disrupt public infrastructure and services, slowing human activity even in regions that are accustomed to such weather. Air and ground transport may be greatly inhibited or shut down entirely. Populations living in snow-prone areas have developed various ways to travel across the snow, such as skis, snowshoes, and sleds pulled by horses, dogs, or other animals and later, snowmobiles. Basic utilities such as electricity, telephone lines, and gas supply can also fail. In addition, snow can make roads much harder to travel and vehicles attempting to use them can easily become stuck.\n\nThe combined effects can lead to a \"snow day\" on which gatherings such as school, work, or church are officially canceled. In areas that normally have very little or no snow, a snow day may occur when there is only light accumulation or even the threat of snowfall, since those areas are unprepared to handle any amount of snow. In some areas, such as some states in the United States, schools are given a yearly quota of snow days (or \"calamity days\"). Once the quota is exceeded, the snow days must be made up. In other states, all snow days must be made up. For example, schools may extend the remaining school days later into the afternoon, shorten spring break, or delay the start of summer vacation.\n\nAccumulated snow is removed to make travel easier and safer, and to decrease the long-term effect of a heavy snowfall. This process utilizes shovels and snowplows, and is often assisted by sprinkling salt or other chloride-based chemicals, which reduce the melting temperature of snow. In some areas with abundant snowfall, such as Yamagata Prefecture, Japan, people harvest snow and store it surrounded by insulation in ice houses. This allows the snow to be used through the summer for refrigeration and air conditioning, which requires far less electricity than traditional cooling methods.\n\nHail can cause serious damage, notably to automobiles, aircraft, skylights, glass-roofed structures, livestock, and most commonly, farmers' crops. Wheat, corn, soybeans, and tobacco are the most sensitive crops to hail damage. Hail is one of Canada's most expensive hazards. Snowfall can be beneficial to agriculture by serving as a thermal insulator, conserving the heat of the Earth and protecting crops from subfreezing weather. Some agricultural areas depend on an accumulation of snow during winter that will melt gradually in spring, providing water for crop growth. If it melts into water and refreezes upon sensitive crops, such as oranges, the resulting ice will protect the fruit from exposure to lower temperatures. Although tropical cyclones take an enormous toll in lives and personal property, they may be important factors in the precipitation regimes of places they affect and bring much-needed precipitation to otherwise dry regions. Hurricanes in the eastern north Pacific often supply moisture to the Southwestern United States and parts of Mexico. Japan receives over half of its rainfall from typhoons. Hurricane Camille averted drought conditions and ended water deficits along much of its path, though it also killed 259 people and caused $9.14 billion (2005 USD) in damage.\n\nHail is one of the most significant thunderstorm hazards to aircraft. When hail stones exceed in diameter, planes can be seriously damaged within seconds. The hailstones accumulating on the ground can also be hazardous to landing aircraft. Strong wind outflow from thunderstorms causes rapid changes in the three-dimensional wind velocity just above ground level. Initially, this outflow causes a headwind that increases airspeed, which normally causes a pilot to reduce engine power if they are unaware of the wind shear. As the aircraft passes into the region of the downdraft, the localized headwind diminishes, reducing the aircraft's airspeed and increasing its sink rate. Then, when the aircraft passes through the other side of the downdraft, the headwind becomes a tailwind, reducing lift generated by the wings, and leaving the aircraft in a low-power, low-speed descent. This can lead to an accident if the aircraft is too low to effect a recovery before ground contact. As the result of the accidents in the 1970s and 1980s, in 1988 the U.S. Federal Aviation Administration mandated that all commercial aircraft have on-board wind shear detection systems by 1993. Between 1964 and 1985, wind shear directly caused or contributed to 26 major civil transport aircraft accidents in the U.S. that led to 620 deaths and 200 injuries. Since 1995, the number of major civil aircraft accidents caused by wind shear has dropped to approximately one every ten years, due to the mandated on-board detection as well as the addition of Doppler weather radar units on the ground. (NEXRAD)\n\nMany winter sports, such as skiing, snowboarding, snowmobiling, and snowshoeing depend upon snow. Where snow is scarce but the temperature is low enough, snow cannons may be used to produce an adequate amount for such sports. Children and adults can play on a sled or ride in a sleigh. Although a person's footsteps remain a visible lifeline within a snow-covered landscape, snow cover is considered a general danger to hiking since the snow obscures landmarks and makes the landscape itself appear uniform.\n\nAccording to the Bible, a giant storm sent by God flooded the Earth. Noah and his family and the animals entered the Ark, and \"the same day were all the fountains of the great deep broken up, and the windows of heaven were opened, and the rain was upon the earth forty days and forty nights.\" The flood covered even the highest mountains to a depth of more than twenty feet, and all creatures died; only Noah and those with him on the Ark were left alive. In the New Testament, Jesus Christ is recorded to have calmed a storm on the Sea of Galilee.\n\nThe Gilgamesh flood myth is a deluge story in the \"Epic of Gilgamesh\".\n\nIn Greek mythology there were several gods of storms: Briareos, the god of sea storms; Aigaios, a god of the violent sea storms; and Aiolos, keeper of storm-winds, squalls and tempests.\n\nThe \"Sea Venture\" was wrecked near Bermuda in 1609, which led to the colonization of Bermuda and provided the inspiration for Shakespeare's play \"The Tempest\"(1611). Specifically, Sir Thomas Gates, future governor of Virginia, was on his way to England from Jamestown, Virginia. On Saint James Day, while he was between Cuba and the Bahamas, a hurricane raged for nearly two days. Though one of the small vessels in the fleet sank to the bottom of the Florida Straits, seven of the remaining vessels reached Virginia within several days after the storm. The flagship of the fleet, known as \"Sea Adventure\", disappeared and was presumed lost. A small bit of fortune befell the ship and her crew when they made landfall on Bermuda. The vessel was damaged on a surrounding coral reef, but all aboard survived for nearly a year on the island. The British colonists claimed the island and quickly settled Bermuda. In May 1610, they set forth for Jamestown, this time arriving at their destination.\n\nThe children's novel \"The Wonderful Wizard of Oz\", written by L. Frank Baum and illustrated by W. W. Denslow, chronicles the adventures of a young girl named Dorothy Gale in the Land of Oz, after being swept away from her Kansas farm home by a tornado. The story was originally published by the George M. Hill Company in Chicago on May 17, 1900 and has since been reprinted numerous times, most often under the name \"The Wizard of Oz\", and adapted for use in other media. Thanks in part to the 1939 MGM movie, it is one of the best-known stories in American popular culture and has been widely translated. Its initial success, and the success of the popular 1902 Broadway musical which Baum adapted from his original story, led to Baum's writing thirteen more Oz books.\n\nHollywood director King Vidor (February 8, 1894 – November 1, 1982) survived the Galveston Hurricane of 1900 as a boy. Based on that experience, he published a fictionalized account of that cyclone, titled \"Southern Storm\", for the May 1935 issue of \"Esquire\" magazine. Erik Larson excerpts a passage from that article in his 2005 book, \"Isaac's Storm\":\n\nNumerous other accounts of the Galveston Hurricane of 1900 have been made in print and in film. Larson cites many of them in \"Isaac's Storm\", which centrally features that storm, as well as chronicles the creation of the Weather Bureau (which came to known as the National Weather Service) and that agency's fateful rivalry with the weather service in Cuba, and a number of other major storms, such as those which ravaged Indianola, Texas in 1875 and 1886.\n\nThe Great Storm of 1987 is key in an important scene near the end of \"Possession: A Romance\", the bestselling and Man Booker Prize-winning novel by A. S. Byatt. The Great Storm of 1987 occurred on the night of October 15–16, 1987, when an unusually strong weather system caused winds to hit much of southern England and northern France. It was the worst storm to hit England since the Great Storm of 1703 (284 years earlier) and was responsible for the deaths of at least 22 people in England and France combined (18 in England, at least four in France).\n\nHurricane Katrina (2005) has been featured in a number of works of fiction.\n\nThe Romantic seascape painters J. M. W. Turner and Ivan Aivazovsky created some of the most lasting impressions of the sublime and stormy seas that are firmly imprinted on the popular mind. Turner's representations of powerful natural forces reinvented the traditional seascape during the first half of the nineteenth century.\n\nUpon his travels to Holland, he took note of the familiar large rolling waves of the English seashore transforming into the sharper, choppy waves of a Dutch storm. A characteristic example of Turner’s dramatic seascape is \"The Slave Ship\" of 1840. Aivazovsky left several thousand turbulent canvases in which he increasingly eliminated human figures and historical background to focus on such essential elements as light, sea, and sky. His grandiose \"Ninth Wave\" (1850) is an ode to human daring in the face of the elements.\n\nThe 1926 silent film \"The Johnstown Flood\" features the Great Flood of 1889 in Johnstown, Pennsylvania. The flood, caused by the catastrophic failure of the South Fork Dam after days of extremely heavy rainfall, prompted the first major disaster relief effort by the American Red Cross, directed by Clara Barton. The Johnstown Flood was depicted in numerous other media (both fictional and in non-fiction), as well.\n\nWarner Bros.' 2000 dramatic disaster film \"The Perfect Storm\", directed by Wolfgang Petersen, is an adaptation of Sebastian Junger's 1997 non-fiction book of the same title. The book and film feature the crew of the \"Andrea Gail\", which got caught in the Perfect Storm of 1991. The 1991 Perfect Storm, also known as the Halloween Nor'easter of 1991, was a nor'easter that absorbed Hurricane Grace and ultimately evolved into a small hurricane late in its life cycle.\n\nStorms were also portrayed in several works of music. Examples of storm music are like Vivaldi's \"Four Seasons\" violin concerto RV 315 (\"Summer\") (third movement: Presto), Beethoven's \"Pastoral Symphony\" (the fourth movement),and a scene in Act II of Rossini's opera \"The Barber of Seville\".\n\n"}
{"id": "627964", "url": "https://en.wikipedia.org/wiki?curid=627964", "title": "Survival of the fittest", "text": "Survival of the fittest\n\n\"Survival of the fittest\" is a phrase that originated from Darwinian evolutionary theory as a way of describing the mechanism of natural selection. The biological concept of fitness is defined as reproductive success. In Darwinian terms the phrase is best understood as \"Survival of the form that will leave the most copies of itself in successive generations.\"\n\nHerbert Spencer first used the phrase, after reading Charles Darwin's \"On the Origin of Species\", in his \"Principles of Biology\" (1864), in which he drew parallels between his own economic theories and Darwin's biological ones: \"This survival of the fittest, which I have here sought to express in mechanical terms, is that which Mr. Darwin has called 'natural selection', or the preservation of favoured races in the struggle for life.\"\n\nDarwin responded positively to Alfred Russel Wallace's suggestion of using Spencer's new phrase \"survival of the fittest\" as an alternative to \"natural selection\", and adopted the phrase in \"The Variation of Animals and Plants under Domestication\" published in 1868. In \"On the Origin of Species\", he introduced the phrase in the fifth edition published in 1869, intending it to mean \"better designed for an immediate, local environment\".\n\nHerbert Spencer first used the phrase – after reading Charles Darwin's \"On the Origin of Species\" – in his \"Principles of Biology\" of 1864 in which he drew parallels between his economic theories and Darwin's biological, evolutionary ones, writing, \"This survival of the fittest, which I have here sought to express in mechanical terms, is that which Mr. Darwin has called 'natural selection', or the preservation of favored races in the struggle for life.\"\n\nIn July 1866 Alfred Russel Wallace wrote to Darwin about readers thinking that the phrase \"natural selection\" personified nature as \"selecting\", and said this misconception could be avoided \"by adopting Spencer's term\" \"Survival of the fittest\". Darwin promptly replied that Wallace's letter was \"as clear as daylight. I fully agree with all that you say on the advantages of H. Spencer's excellent expression of 'the survival of the fittest'. This however had not occurred to me till reading your letter. It is, however, a great objection to this term that it cannot be used as a substantive governing a verb\". Had he received the letter two months earlier, he would have worked the phrase into the fourth edition of the \"Origin\" which was then being printed, and he would use it in his \"next book on Domestic Animals etc.\".\n\nDarwin wrote on page 6 of \"The Variation of Animals and Plants under Domestication\" published in 1868, \"This preservation, during the battle for life, of varieties which possess any advantage in structure, constitution, or instinct, I have called Natural Selection; and Mr. Herbert Spencer has well expressed the same idea by the Survival of the Fittest. The term \"natural selection\" is in some respects a bad one, as it seems to imply conscious choice; but this will be disregarded after a little familiarity\". He defended his analogy as similar to language used in chemistry, and to astronomers depicting the \"attraction of gravity as ruling the movements of the planets\", or the way in which \"agriculturists speak of man making domestic races by his power of selection\". He had \"often personified the word Nature; for I have found it difficult to avoid this ambiguity; but I mean by nature only the aggregate action and product of many natural laws,—and by laws only the ascertained sequence of events.\"\n\nIn the first four editions of \"On the Origin of Species\", Darwin had used the phrase \"natural selection\". \nIn Chapter 4 of the 5th edition of \"The Origin\" published in 1869, Darwin implies again the synonym: \"Natural Selection, or the Survival of the Fittest\". By \"fittest\" Darwin meant \"better adapted for the immediate, local environment\", not the common modern meaning of \"in the best physical shape\" (think of a puzzle piece, not an athlete). In the introduction he gave full credit to Spencer, writing \"I have called this principle, by which each slight variation, if useful, is preserved, by the term Natural Selection, in order to mark its relation to man's power of selection. But the expression often used by Mr. Herbert Spencer of the Survival of the Fittest is more accurate, and is sometimes equally convenient.\"\n\nIn \"The Man Versus The State\", Spencer used the phrase in a postscript to justify a plausible explanation of how his theories would not be adopted by \"societies of militant type\". He uses the term in the context of societies at war, and the form of his reference suggests that he is applying a general principle.\n\nThough Spencer’s conception of organic evolution is commonly interpreted as a form of Lamarckism, Herbert Spencer is sometimes credited with inaugurating Social Darwinism. The phrase \"survival of the fittest\" has become widely used in popular literature as a catchphrase for any topic related or analogous to evolution and natural selection. It has thus been applied to principles of unrestrained competition, and it has been used extensively by both proponents and opponents of Social Darwinism.\n\nEvolutionary biologists criticise the manner in which the term is used by non-scientists and the connotations that have grown around the term in popular culture. The phrase also does not help in conveying the complex nature of natural selection, so modern biologists prefer and almost exclusively use the term natural selection. The biological concept of fitness refers to reproductive success, as opposed to survival, and is not explicit in the specific ways in which organisms can be more \"fit\" (increase reproductive success) as having phenotypic characteristics that enhance survival and reproduction (which was the meaning that Spencer had in mind).\n\nWhile the phrase \"survival of the fittest\" is often used to mean \"natural selection\", it is avoided by modern biologists, because the phrase can be misleading. For example, survival is only one aspect of selection, and not always the most important. Another problem is that the word \"fit\" is frequently confused with a state of physical fitness. In the evolutionary meaning \"fitness\" is the rate of reproductive output among a class of genetic variants.\n\nThe phrase can also be interpreted to express a theory or hypothesis: that \"fit\" as opposed to \"unfit\" individuals or species, in some sense of \"fit\", will survive some test.\n\nInterpretations of the phrase as expressing a theory are in danger of being tautological, meaning roughly \"those with a propensity to survive have a propensity to survive\"; to have content the theory must use a concept of fitness that is independent of that of survival.\n\nInterpreted as a theory of species survival, the theory that the fittest species survive is undermined by evidence that while direct competition is observed between individuals, populations and species, there is little evidence that competition has been the driving force in the evolution of large groups such as, for example, amphibians, reptiles, and mammals. Instead, these groups have evolved by expanding into empty ecological niches. In the punctuated equilibrium model of environmental and biological change, the factor determining survival is often not superiority over another in competition but ability to survive dramatic changes in environmental conditions, such as after a meteor impact energetic enough to greatly change the environment globally. The main land dwelling animals to survive the K-Pg impact 66 million years ago had the ability to live in underground tunnels, for example.\n\nIn 2010 Sahney et al. argued that there is little evidence that intrinsic, biological factors such as competition have been the driving force in the evolution of large groups. Instead, they cited extrinsic, abiotic factors such as expansion as the driving factor on a large evolutionary scale. The rise of dominant groups such as amphibians, reptiles, mammals and birds occurred by opportunistic expansion into empty ecological niches and the extinction of groups happened due to large shifts in the abiotic environment.\n\nIt has been claimed that \"the survival of the fittest\" theory in biology was interpreted by late 19th century capitalists as \"an ethical precept that sanctioned cut-throat economic competition\" and led to the advent of the theory of \"social Darwinism\" which was used to justify laissez-faire economics, war and racism. However, these ideas predate and commonly contradict Darwin's ideas, and indeed their proponents rarely invoked Darwin in support. The term \"social Darwinism\" referring to capitalist ideologies was introduced as a term of abuse by Richard Hofstadter's \"Social Darwinism in American Thought\" published in 1944.\n\nRussian anarchist Peter Kropotkin viewed the concept of \"survival of the fittest\" as supporting co-operation rather than competition. In his book \"\" he set out his analysis leading to the conclusion that the fittest was not necessarily the best at competing individually, but often the community made up of those best at working together. He concluded that In the animal world we have seen that the vast majority of species live in societies, and that they find in association the best arms for the struggle for life: understood, of course, in its wide Darwinian sense – not as a struggle for the sheer means of existence, but as a struggle against all natural conditions unfavourable to the species. The animal species, in which individual struggle has been reduced to its narrowest limits, and the practice of mutual aid has attained the greatest development, are invariably the most numerous, the most prosperous, and the most open to further progress.\n\nApplying this concept to human society, Kropotkin presented mutual aid as one of the dominant factors of evolution, the other being self-assertion, and concluded that In the practice of mutual aid, which we can retrace to the earliest beginnings of evolution, we thus find the positive and undoubted origin of our ethical conceptions; and we can affirm that in the ethical progress of man, mutual support not mutual struggle – has had the leading part. In its wide extension, even at the present time, we also see the best guarantee of a still loftier evolution of our race.\n\n\"Survival of the fittest\" is sometimes claimed to be a tautology. The reasoning is that if one takes the term \"fit\" to mean \"endowed with phenotypic characteristics which improve chances of survival and reproduction\" (which is roughly how Spencer understood it), then \"survival of the fittest\" can simply be rewritten as \"survival of those who are better equipped for surviving\". Furthermore, the expression \"does\" become a tautology if one uses the most widely accepted definition of \"fitness\" in modern biology, namely reproductive success itself (rather than any set of characters conducive to this reproductive success). This reasoning is sometimes used to claim that Darwin's entire theory of evolution by natural selection is fundamentally tautological, and therefore devoid of any explanatory power.\n\nHowever, the expression \"survival of the fittest\" (taken on its own and out of context) gives a very incomplete account of the mechanism of natural selection. The reason is that it does not mention a key requirement for natural selection, namely the requirement of \"heritability\". It is true that the phrase \"survival of the fittest\", in and by itself, is a tautology if fitness is defined by survival and reproduction. Natural selection is the portion of variation in reproductive success that is caused by \"heritable\" characters (see the article on natural selection).\n\nIf certain heritable characters increase or decrease the chances of survival and reproduction of their bearers, then it follows mechanically (by definition of \"heritable\") that those characters that improve survival and reproduction will increase in frequency over generations. This is precisely what is called \"evolution by natural selection\". On the other hand, if the characters which lead to differential reproductive success are not heritable, then no meaningful evolution will occur, \"survival of the fittest\" or not: if improvement in reproductive success is caused by traits that are not heritable, then there is no reason why these traits should increase in frequency over generations. In other words, natural selection does not simply state that \"survivors survive\" or \"reproducers reproduce\"; rather, it states that \"survivors survive, reproduce and \"therefore\" propagate any \"heritable\" characters which have affected their survival and reproductive success\". This statement is not tautological: it hinges on the testable hypothesis that such fitness-impacting heritable variations actually exist (a hypothesis that has been amply confirmed.)\n\nMomme von Sydow suggested further definitions of 'survival of the fittest' that may yield a testable meaning in biology and also in other areas where Darwinian processes have been influential. However, much care would be needed to disentangle tautological from testable aspects. Moreover, an \"implicit shifting between a testable and an untestable interpretation can be an illicit tactic to immunize natural selection ... while conveying the impression that one is concerned with testable hypotheses\".\n\nSkeptic Society founder and \"Skeptic\" magazine publisher Michael Shermer addresses the tautology problem in his 1997 book, \"Why People Believe Weird Things\", in which he points out that although tautologies are sometimes the beginning of science, they are never the end, and that scientific principles like natural selection are testable and falsifiable by virtue of their predictive power. Shermer points out, as an example, that population genetics accurately demonstrate when natural selection will and will not effect change on a population. Shermer hypothesizes that if hominid fossils were found in the same geological strata as trilobites, it would be evidence against natural selection.\n\n\n\n\n"}
{"id": "1448018", "url": "https://en.wikipedia.org/wiki?curid=1448018", "title": "Teleonomy", "text": "Teleonomy\n\nTeleonomy is the quality of apparent purposefulness and goal-directedness of structures and functions in living organisms brought about by the exercise, augmentation, and, improvement of reasoning. The term derives from two Greek words, τέλος \"telos\" (\"end, purpose\") and νόμος \"nomos\" (\"law\"), and means \"end-directed\" (literally \"purpose-law\"). Teleonomy is sometimes contrasted with teleology, where the latter is understood as a purposeful goal-directedness brought about through human or divine intention. Teleonomy is thought to derive from evolutionary history, adaptation for reproductive success, and/or the operation of a program. Teleonomy is related to programmatic or computational aspects of purpose.\n\nColin Pittendrigh, who coined the term in 1958, applied it to biological phenomena that appear to be end-directed, hoping to limit the much older term teleology to actions planned by an agent who can internally model alternative futures with intention, purpose and foresight:\nIn 1965 Ernst Mayr cited Pittendrigh and criticized him for not making a \"clear distinction between the two teleologies of Aristotle\"; evolution involves Aristotle's material causes and formal causes rather than efficient causes. Mayr adopted Pittendrigh's term, but supplied his own definition:\n\nRichard Dawkins described the properties of \"archeo-purpose\" (by natural selection) and \"neo-purpose\" (by evolved adaptation) in his talk on the \"Purpose of Purpose\". Dawkins attributes the brain's flexibility as an evolutionary feature in adapting or subverting goals to making neo-purpose goals on an overarching evolutionary archeo-purpose. Language allows groups to share neo-purposes, and cultural evolution - occurring much faster than natural evolution - can lead to conflict or collaborations.\n\nIn behavior analysis, Hayne Reese made the adverbial distinction between purposefulness (having an internal determination) and purposiveness (serving or effecting a useful function). Reese implies that non-teleological statements are called teleonomic when they represent an \"if A then C\" phenomenon's antecedent; where, teleology is a consequent representation. The concept of purpose, as only being the teleology final cause, requires supposedly impossible time reversal; because, the future consequent determines the present antecedent. Purpose, as being both in the beginning and the end, simply rejects teleology, and addresses the time reversal problem. In this, Reese sees no value for teleology and teleonomic concepts in behavior analysis; however, the concept of purpose preserved in process can be useful, if not reified. A theoretical time-dimensional tunneling and teleological functioning of temporal paradox would also fit this description without the necessity of a localized intelligence. Whereas the concept of a teleonomic process, such as evolution, can simply refer to a system capable of producing complex products without the benefit of a guiding foresight.\n\nIn 1966 George C. Williams approved of the term in the last chapter of his \"Adaptation and Natural Selection; a critique of some current evolutionary thought\". In 1970, Jacques Monod, in \"Chance and Necessity, an Essay on the Natural Philosophy of Modern Biology\", suggested teleonomy as a key feature that defines life:\n\nIn 1974 Ernst Mayr illustrated the difference in the statements:\n\nSubsequently, philosophers like Ernest Nagel further analysed the concept of goal-directedness in biology and by 1982, philosopher and historian of science David Hull joked about the use of teleology and teleonomy by biologists:\n\nThe concept of teleonomy was largely developed by Mayr and Pittendrigh to separate biological evolution from teleology. Pittendrigh's purpose was to enable biologists who had become overly cautious about goal-oriented language to have a way of discussing the goals and orientations of an organism's behaviors without inadvertently invoking teleology. Mayr was even more explicit, saying that while teleonomy certainly operates on the level of organisms, the process of evolution itself is necessarily non-teleonomic.\n\nEvolution largely hoards hindsight, as variations unwittingly make \"predictions\" about structures and functions which could successfully cope with the future, and which participate in a process of natural selection that culls the unfit, leaving the fit to the next generation. Information accumulates about functions and structures that are successful, exploiting feedback from the environment via the selection of fitter coalitions of structures and functions. Robert Rosen has described these features as an anticipatory system which builds an internal model based on past and possible future states.\n\nIn 1962, Grace A. de Laguna's \"The Role of Teleonomy in Evolution\" attempted to show how different stages of evolution were characterized by different types of teleonomy. de Laguna points out that humans have oriented teleonomy so that the teleonomic goal is not restricted to the reproduction of humans, but also to cultural ideals.\n\nIn recent years, a few biologists believe that the separation of teleonomy from the process of evolution has gone too far. Peter Corning notes that behavior, which is a teleonomic trait, is responsible for the construction of biological niches, which is an agent of selection. Therefore, it would be inaccurate to say that there was no role for teleonomy in the process of evolution, since teleonomy dictates the fitness landscape according to which organisms are selected. Corning calls this phenomenon \"teleonomic selection\".\n\nIn teleology, Kant's positions as expressed in Critique of Judgment, were neglected for many years because in the minds of many scientists they were associated with vitalist views of evolution. Their recent rehabilitation is evident in teleonomy, which bears a number of features, such as the description of organisms, that are reminiscent of the Aristotelian conception of final causes as essentially recursive in nature. Kant's position is that, even though we cannot know whether there are final causes in nature, we are constrained by the peculiar nature of the human understanding to view organisms teleologically. Thus the Kantian view sees teleology as a necessary principle for the study of organisms, but only as a regulative principle, and with no ontological implications.\n\nTalcott Parsons, in the later part of his working with a theory of social evolution and a related theory of world-history, adopted the concept of teleonomy as the fundamental organizing principle for directional processes and his theory of societal development in general. In this way, Parsons tried to find a theoretical compromise between voluntarism as a principle of action and the idea of a certain directionality in history.\n\nTeleonomy is closely related to concepts of emergence, complexity theory, and self-organizing systems. It has extended beneath biology to be applied in the context of chemistry. Some philosophers of biology resist the term and still employ \"teleology\" when analyzing biological function and the language used to describe it, while others endorse it.\n\n\n\n"}
{"id": "7238563", "url": "https://en.wikipedia.org/wiki?curid=7238563", "title": "The Birds of Africa", "text": "The Birds of Africa\n\nThe Birds of Africa is an eight-volume ornithological handbook. \nIts authors/editors are C. Hilary Fry, Stuart Keith and Emil Urban, and each volume contains colour plates painted by Martin Woodcock.\n\nIt covers all breeding species in full, with details of range, status, description, voice, general habits and breeding; non-breeding visitors are treated more briefly, with emphasis on their status and behaviour whilst in Africa.\n\nThe series contained the following volumes:\n"}
{"id": "18562555", "url": "https://en.wikipedia.org/wiki?curid=18562555", "title": "The Oil Drum", "text": "The Oil Drum\n\n\"The Oil Drum\" was rated one of the top five sustainability blogs of 2007 by Nielsen Netratings, and was read by a diverse collection of public figures, including Roscoe Bartlett, Paul Krugman, James Howard Kunstler, Richard Rainwater, and Radiohead. In 2008, the site received the M. King Hubbert Award for Excellence in Energy Education from the U.S. chapter of the Association for the Study of Peak Oil and Gas (ASPO).\n\n\"The Oil Drum\" was started in March 2005 by Kyle Saunders (username \"Prof. Goose\"), a professor of political science at Colorado State University, and Dave Summers (username \"Heading Out\"), a professor of mining engineering at Missouri University of Science and Technology (then known as University of Missouri-Rolla). The site first rose to prominence following its coverage of the impact of Hurricanes Katrina and Rita on oil and gas production. The staff grew by dozens and became well known for rigorous, quantitative analysis of energy production and consumption. A notable example is former editor Stuart Staniford's analysis of the depletion of Saudi Arabia's Ghawar oil field (Depletion Levels in Ghawar).\n\nThe site started out on the Blogger platform, moved to Scoop in August 2005, and to Drupal in December 2006.\n\nIn 2013, The Oil Drum announced that it would stop publishing new content and would turn into an archive resource. Reasons cited for this change include server costs and a dwindling number of contributors of high-quality content.\n\n"}
{"id": "2230859", "url": "https://en.wikipedia.org/wiki?curid=2230859", "title": "Tudigong", "text": "Tudigong\n\nTudigong (土地公 \"Lord of the Soil and the Ground\") or Tudishen (土地神 \"God of the Soil and the Ground\"), also known simply as Tudi (土地 \"Soil-Ground\") is a tutelary deity of a locality and the human communities who inhabit it in Chinese folk religion.\n\nOther names of the god include:\n\nExtended titles of the god include:\n\nCommoners often call Tudigong \"grandfather\" (\"yeye\"), which reflects his close relationship with the common people.\n\nIn the countryside, he is sometimes given a wife, \"Tǔdìpó\" (土地婆 \"Grandmother of the Soil and the Ground\"), placed next to him on the altar. She may be seen as a just and benevolent deity on the same rank as her husband, or as a grudging old woman holding back her husband's benedictions, which explains why one does not always receive fair retribution for good behaviour.\n\nAnother story says that Tudipo is supposed to be a young lady. After Tudigong received a heavenly rank, he gave everything that the people asked for. When one of the gods went down to Earth to do inspections, he saw that Tudigong was distributing blessings unnecessarily. Soon after that, the god went to the Celestial Palace and reported to the Jade Emperor.\n\nAfter the Jade Emperor knew this, he found out that there was a lady that was going to be killed, but she was not guilty. Thus, the Jade Emperor told a god to go down to Earth and bring the lady to heaven. When the lady was brought to the Celestial Palace, the Jade Emperor bestowed her to Tudigong as his wife. She was ordered to look after how many blessings Tudigong distributes and that they not be unnecessarily distributed. This is why many people do not want to pay respect to Tudipo, because they are afraid that she will not let Tudigong give lots of wealth to them.\n\nThe Landlord God () is a deity worshipped in Chinese folk beliefs who is analogous but is not to be confused with Tudigong. The tablet for the Landlord God is typically inscribed with (middle two rows) \"left: The Earth God of Overseas Tang People (overseas Chinese; 唐番地主財神), right: The Dragon of Five Sides and Five Lands (五方五土龍神; fengshui). The side inscriptions mean \"The wealth comes from ten thousand directions and the business comes from thousands of miles.\" It is believed that the Landlord God has powers to help gather wealth, and the position of the tablet must be placed properly according to the laws of fengshui.\n\nThe Village God has developed from land worship. Before Chenghuangshen (\"City God\") became more prominent in China, land worship had a hierarchy of deities conforming strictly to social structure, in which the emperor, kings, dukes, officials and common people were allowed to worship only the land gods within their command; the highest land deity was the Houtu (\"Queen of the Earth\").\n\nRanked lower than City Gods, the Village Gods have been very popular among villagers as the grassroot deities since the 14th century during the Ming dynasty. Some scholars speculate that this change came because of an imperial edict, because it is reported that the Hongwu Emperor of the Ming dynasty was born in a Village God shrine. The image of the Village God is that of a simply clothed, smiling, white-bearded man. His wife, the Grandmother of the Village, looks like a normal old lady.\n\nIn Taiwan, festivals dedicated to Tudigong typically take place on the second day of the second month and the 15th day of the eighth month on the Chinese lunar calendar.\n\n"}
{"id": "3042286", "url": "https://en.wikipedia.org/wiki?curid=3042286", "title": "Wastage (military)", "text": "Wastage (military)\n\nWastage was a British term used during the First World War. It was used to describe the losses experienced during the war that occurred during times when no infantry attack was taking place. Wastage was deaths as a result of artillery fire on soldiers who were manning their own trenches, not actively attacking the enemy's trenches. On the \"quietest days\" of the war, the British were losing 7,000 men killed and wounded per day to wastage. Even during major battles, wastage could often exceed casualties suffered during an infantry attack. Such was the case at 3rd Ypres and other major battles, especially later in the war.\n"}
{"id": "10928315", "url": "https://en.wikipedia.org/wiki?curid=10928315", "title": "Wind power in Asia", "text": "Wind power in Asia\n\nWind power in Asia is an important component in the Asian energy industry and one of the key sources of renewable energy in the region. As of April 2016, the installed capacity of wind power in Asia (excluding the Middle East) totalled 175,831 MW. Asia is the fastest growing region in terms of wind energy, having increased its installed capacity by 33,858 MW in 2005 (a 24% increase over 2014). China, with 145,362 MW of installed capacity, is the world's largest generator of electricity from wind energy. India is the second largest in Asia with an installed capacity of 25,088 MW. Other key countries include Japan (1,394 MW), Taiwan (188 MW), South Korea (173 MW) and the Philippines (33 MW).\n\n"}
{"id": "9132854", "url": "https://en.wikipedia.org/wiki?curid=9132854", "title": "Woollybear Festival", "text": "Woollybear Festival\n\nThe annual Dick Goddard Woollybear Festival is held every Fall in downtown Vermilion, Ohio, on Lake Erie. The one-day, family event, which began in 1973, features a woolly bear costume contest in which children, even pets, are dressed up as various renditions of the woolly bear caterpillar.\n\nThe festival is held every year around October 1 on a Sunday on which the Cleveland Browns have an away game. It is touted as the largest one-day festival in Ohio.\n\nThe festival is the brainchild of legendary Cleveland TV personality Dick Goddard, former retired longtime weatherman at WJW-TV. In much the same way Punxsutawney Phil is celebrated in the century-old tradition of Groundhog Day in which he predicts the end of winter, the Woolly Bear Caterpillar is similarly celebrated for its mythical association to winter forecasting. After the caterpillars' eggs hatch in Fall, folklore suggests the severity of an upcoming winter can be gauged by observing the amount of black versus orange in the caterpillars' bands.\n\nIn later years, the event organizers have promoted the event as the \"Dick Goddard Woollybear Festival\" to honor Goddard's legacy.\n\nAccording to the festival's website:\n\n\"In 1972 the newly elected officers of the Parent Teachers Association at the Firelands-Florence Township Elementary School in the tiny community of Birmingham in Erie County were looking around for a vehicle to raise funds. They heard about Goddard’s idea of a Woollybear Festival. They contacted him and offered to stage the festival with his help.\"\n\n\"The first Woollybear Festival was held in Birmingham and attracted perhaps 2,000 people. The parade was short—just the Firelands High School Band, some boy scouts and the local fire department, along with personalities from TV8—and they decided to go around the parade route twice, just to make it look longer.\"\n\nAttracting 2,000 spectators in the first year, the number grew to an estimated 15,000 by the eighth festival and quickly overwhelmed the town of Birmingham. Of the 13 cities that expressed interest, organizers selected Vermilion as the new home.\n\nThe parade in 2006 involved over 20 marching bands, 2,000 marchers, hundreds of animals, and over 100,000 spectators.\n\nThe \"Woollybear 500\" is a race that starts off with the Chief of police and the Chief of fire selecting individual woollybears and racing against each other. The woollybears are obtained by the Vermilion Chamber and details of the training and skills of said woollybears are not divulged to the participants. The race is monitored by professionals from TV-8. No woollybears are harmed in the participation of these races.\n\n\n"}
{"id": "53556535", "url": "https://en.wikipedia.org/wiki?curid=53556535", "title": "Xrafstar", "text": "Xrafstar\n\nXrafstar or Khrafstra (; ) is a cover term in Zoroastrianism for the animals that are harmful or repulsive. They were considered creations of the Evil Spirit Angra Mainyu and killing them was seen as meritorious. In the Young Avesta and Middle Persian texts, the class of xrafstars includes frogs, reptiles, scorpions and insects like ants or wasps, whereas predators such as the wolf are not referred to as xrafstars, even though they too are considered to be creations of evil.\n\nHerodotus narrates about the practice of killing xrafstars among the Magi: \"The Magi are a very peculiar race, different entirely from the Egyptian priests, and indeed from all other men whatsoever. The Egyptian priests make it a point of religion not to kill any live animals except those which they offer in sacrifice. The Magi, on the contrary, kill animals of all kinds with their own hands, excepting dogs and men. They even seem to take a delight in the employment, and kill, as readily as they do other animals, ants and snakes, and such like flying or creeping things\".\n\n"}
