{"id": "52378360", "url": "https://en.wikipedia.org/wiki?curid=52378360", "title": "Agri Bavnehøj", "text": "Agri Bavnehøj\n\nAgri Bavnehøj (or Agri Bavnehoej) is a Danish hill and vista point with a bronze age burial mound, located 137 meters above sea level. It is close to the village of Agri, in Mols Bjerge National Park on the southern part of the Djursland peninsula, northeast of Denmark’s second largest city, Aarhus. The mound was built 1800 – 1000 years BC. Agri Bavnehøj is the least known of four vista points and burial mounds on southern Djursland, despite being the highest (by a few meters). The others are Stabelhøje, Trehøje and Ellemandsbjerg.\n\nFrom the mound there is a view over Århus Bay, Jutland, Southern Djursland, Ebeltoft Bay, the Kattegat Sea and central parts of Mols Bjerge National Park. On a clear day one can also see across The Kattegat Sea to the island Zealand, where the Danish capital Copenhagen lies. Agri Bavnehøj is less than a kilometer east of another vista point, Stabelhøje, 135 meters above sea level, where the view is perhaps even better. The feeling of height is emphasized by the view being all the way down to sea level. The Agri Bavnehøj hill was formed by ice sheet movements 12.000 years ago in connection with the last ice age.\n\nThe old Danish word, bavn, in Bavnehøj, means stack of wood or fire placed on high ground. The bronze age people and their ancestors used Agri Bavnehøj and other hilltop barrows as part of a warning system, where one lit fires on hilltops to warn neighbor communities of dangers, such as invasions. The neighbors in return lit fires on their own designated hilltops, creating a telegraph chain of fires, spreading the word of unrest. This function was maintained all the way up to the 18-hundreds - actually as late as in the Three Year War (Treårskrigen) 1848 -50, that amongst other things saw the withdrawal of the Danish general Olaf Rye’s army past Århus over southern Djursland to the peninsula Helgenæs that he fortified at the narrow Dragsmur passage 8 kilometers from Agri Bavnehøj. From Helgenæs Rye succeeded in shipping the army to Fredericia in the southern part of Jutland in Denmark, where the army was able to continue fighting, after having maneuvered around the German front by sailing round it. \nThe burial mound on Agri Bavnehøj is made from approximately 650.000 blocks of stacked turf blocks giving the mound a height of 5 – 6 meters. Building a mound this seize corresponds to peeling the top turf away from around 7 hectares of land. The construction of large bronze age mounds such as Agri Bavnehøj is an undertaking that must have involved many people working with primitive pre-iron age tools. The Agri mound was part of a civilisation that built the 60.000 bronze age mounds registered in Denmark today. It has been calculated that in the bronze age period from 1800 – 500 BC, 100 – 150 burial mounds were built every year. This can be seen as a testimony to the existence of organized communities in a 2500 – 3800 year old pre Christian civilization, that must have had a pervasive religious belief containing a uniform concept of burial traditions.\n\nIn the earliest part of the bronze age, tribal leaders and other important people were buried in the mounds encased in a hollowed out oak log. Often the mounds were used multiple times, with both early oak log burials and later urn burials found in the same mound. The Bronze Age in what is now Denmark was in many ways an international affair with import of copper and tin from far away for bronze production, and export of cattle. Archeologists reckon that the burials changed from coffin burials to cremation due to international influence.\nA hallmark of the turf built bronze age mounds is that a layer of metallic oxides tended to wash out from the top turf creating a hard and air tight cap isolating the inner mound from the atmosphere. This air tight lid prevents decay of the burials due to oxidation including protecting offerings accompanying the deceased inside the mounds.\n\nAgri Bavnehøj is situated in the protected not farmed part of The National Park Mols Bjerge 13 kilometers from the partly tourism based coastal town, Ebeltoft. The hills are accessible by car via small country roads leading to a parking lot close to the mound, including an info-poster, benches and toilet facilities. Horseback riders often use the designated horse trailer parking facilities by Agri Bavnehøj as a starting point for rides in the park. From the facilities there is a footpath to the top of the mound. \nThe hilly small roads up to Agri Bavnehøj traverse some of Denmark’s longest hills.\n\n"}
{"id": "52077117", "url": "https://en.wikipedia.org/wiki?curid=52077117", "title": "Alker", "text": "Alker\n\nAlker is an earth-based stabilized building material produced by the addition of gypsum, lime, and water to earth with the appropriate granulometric structure and with a cohesive property. Unbaked and produced on-site either as adobe blocks or by pouring into mouldings (the rammed earth technique), it has significant economical and ecological advantages. Its physical and mechanical properties are superior to traditional earth construction materials, and are comparable to other stabilized earthen materials. The ratios of the mixture are determined in accordance with the purpose of construction. Alker has primarily been used as a wall construction material; for this purpose, the addition of 8-10% gypsum, 2.5-5% lime, and 20% water to earth produces optimum results. These ratios may change according to the nature and content of clay in the soil.\n\nThe initial research for Alker was completed in 1980 at the Faculty of Architecture of Istanbul Technical University. The word Alker is an abbreviation combining the first syllables of the Turkish words for Gypsum (\"Alçı\") and Adobe (\"Kerpiç\"). Alker was inspired by a traditional plaster material consisting of a mixture of earth, gypsum and lime, which has been in use in the earthen architecture of Anatolia since the neolithic era due to its high water resistance. The initial project for Alker was based on the addition only of gypsum to earth with the appropriate qualities. The addition of lime was introduced later, and improved the material’s earthquake resistant properties. Research on the properties and application methods of Alker has continued, mainly at Istanbul Technical University.\nAlker has been utilized in numerous constructions in Turkey, where it was first developed, as well as in other countries. One of the earliest among these, constructed in 1995 in Istanbul Technical University’s Ayazağa Campus, has been in continuous use without necessitating significant repair. In this particular construction process, the material was poured into mouldings and rammed, with a view to exploring possibilities for mass construction with Alker.\n\nAlker is characterized by its quick setting time (approximately 20 minutes), hence preventing clay shrinkage and eliminating the need for curing and drying processes. If needed, a retarding agent may also be added to the mixture. It is a porous material with a lower volumetric weight, and nearly four times more pressure resistance compared to traditional earthen wall materials. Structurally, Alker is comparable to concrete as a conglomerate material. It must be noted however that while properties of concrete improve in direct ratio to the amount of cement it contains, increased amounts of clay (the binding element) in the Alker mixture have negative effects on its physical properties, particularly in terms of pressure and erosion resistance. \n\nAlker exhibits high resistance to water-related erosion, in contrast to traditional unbaked earthen building materials which are characterized by poor resistance to water. In erosion tests pure earthen materials completely dissolve; the erosion rate in Alker is minimal. The material gains a rigidity of 0.375 MPa during the setting process, within the first twenty minutes after pouring. It gains rigidity while containing 20% moisture, which makes it possible to remove mouldings and stack blocks shortly after pouring the material. \n\nIts unit weight is lower than those of comparable building materials. Its shrinkage and expansion rates are low, and are comparable to those of concrete. As such, it can be poured continuously without necessitating a contraction joint. It is characterized by resistance to water and moisture. The ratio of lime in the mixture can be modified in order to completely eliminate water-related erosion. Experiments on capillary water absorption have shown that increased amounts of lime in the mixture results in an increase in quantity and in reduced width of capillary canals, proving the material’s erosion resistance. Compressive and shear strength and modules of elasticity and rigidity present advantages in terms of earthquake resistance. Once the mixture is poured into a mould, the production process is completed, and a significant degree of rigidity is reached. It does not require curing and drying, providing economy of time, labor, and energy. Resistance to pressure is 3,5 - 4 MPa. The lime in the mixture reduces resistance to pressure to a minimal degree, while increasing elasticity and resistance to impact. During pressure tests cube-shaped blocks fracture in pyramidal forms, comparable to concrete blocks, and do not disintegrate in the way unstabilized earthen blocks do. \n\nAlker is not a patented material. It has been developed with the aim of creating a widely used low-cost Ecological Building material available for self-building as well as for larger sustainable architecture projects. A number of projects have been developed that are based on Alker (gypsum- and lime-stabilized earth) technology. Among these is cast earth, which uses the Alker mixture with the addition of a retarding agent in order to lengthen the setting time. If Alker is to be produced on the construction site, addition of a retarding agent is not necessary. \n\nStabilization of earth only with gypsum addition does not produce material with the same physical and mechanical properties as that with lime and gypsum addition, and increased amounts of gypsum result in raised costs.\n\n"}
{"id": "9008914", "url": "https://en.wikipedia.org/wiki?curid=9008914", "title": "Animal fiber", "text": "Animal fiber\n\nAnimal fibers are natural fibers that consist largely of particular proteins. Instances are silk, hair/fur (including wool) and feathers. The animal fibers used most commonly both in the manufacturing world as well as by the hand spinners are wool from domestic sheep and silk. Also very popular are alpaca fiber and mohair from Angora goats. Unusual fibers such as Angora wool from rabbits and Chiengora from dogs also exist, but are rarely used for mass production. \n\nNot all animal fibers have the same properties, and even within a species the fiber is not consistent. Merino is a very soft, fine wool, while Cotswold is coarser, and yet both merino and Cotswold are types of sheep. This comparison can be continued on the microscopic level, comparing the diameter and structure of the fiber. With animal fibers, and natural fibers in general, the individual fibers look different, whereas all synthetic fibers look the same. This provides an easy way to differentiate between natural and synthetic fibers under a microscope.\n\nSilk is a \"natural\" protein fiber, some forms of which can be woven into textiles. The best-known type of silk is obtained from cocoons made by the larvae of the silkworm \"Bombyx mori\" reared in captivity. Rearing of silks is called sericulture. Degummed fibers from \"B. mori\" are 5-10 μm in diameter. The shimmering appearance for which silk is prized comes from the fibers' triangular prism-like cross-sectional structure which allows silk cloth to refract incoming light at different angles.\n\nThe length of the silk fiber depends on how it has been prepared. Since the cocoon is made of one strand, if the cocoon is unwound carefully the fibers can be very long.\n\nSpider silk is the strongest natural fiber known. The strongest dragline silk is five times stronger than steel and three times tougher than Kevlar. It is also highly elastic, the silk of the ogre-faced spider can be stretched six times its original length without damage. As of 2005, there is no synthetic material in production that can match spider silk, but it is actively being sought by the U.S. military for such applications as body armour, parachutes and rope. Genetically engineered goats have been raised to produce spider silk in their milk at a cost of around $1,500 per gram.\n\nWool is the fiber derived from the fur of animals of the Caprinae family, principally sheep, but the hair of certain species of other mammals such as goats, alpacas, and rabbits may also be called wool. \nAlpaca fiber is that of an alpaca. It is warmer than sheep's wool and lighter in weight. It is soft, fine, glossy, and luxurious. The thickness of quality fiber is between 12-29 micrometers. Most alpaca fiber is white, but it also comes in various shades of brown and black.\n\nAngora wool or Angora fiber refers to the down coat produced by the Angora rabbit. There are many types of Angora rabbits - English, French, German and Giant. Angora is prized for its softness, thin fibers of around 12-16 micrometers for quality fiber, and what knitters refer to as a halo (fluffiness). The fiber felts very easily. Angora fiber comes in white, black, and various shades of brown.\n\nBison is the soft undercoat of the American Bison. The coat of the bison contains two different types of fiber. The main coat is made up of coarse fibers (average 59 micrometers) called guard hairs, and the downy undercoat (average 18.5 micrometers). This undercoat is shed annually and consists of fine, soft fibers which are very warm and protect the animal from harsh winter conditions. \n\nCashmere wool is wool obtained from the Cashmere goat. Cashmere is characterized by its luxuriously soft fibers, with high napability and loft. In order for a natural goat fiber to be considered Cashmere, it must be under 18.5 micrometers in diameter and be at least 3.175 centimeters long. It is noted as providing a natural light-weight insulation without bulk. Fibers are highly adaptable and are easily constructed into fine or thick yarns, and light to heavy-weight fabrics.\n\nMohair is a silk-like fabric or yarn made from the hair of the Angora goat. It is both durable and resilient. It is notable for its high luster and sheen, and is often used in fiber blends to add these qualities to a textile. Mohair also takes dye exceptionally well.\n\nWool has two qualities that distinguish it from hair or fur: it has scales which overlap like shingles on a roof and it is crimped; in some fleeces the wool fibers have more than 20 bends per inch. Wool varies in diameter from below 17 micrometers to over 35 micrometers. The finer the wool, the softer it will be, while coarser grades are more durable and less prone to pilling.\n\nQiviut is the fine underwool of the muskox. Qiviut fibers are long (about 5 to 8 cm), fine (between 15 and 20 micrometers in diameter), and relatively smooth. It is approximately eight times warmer than sheep's wool and does not felt or shrink.\n\nHand spinners also use fiber from animals such as llamas, camels, yak, and possums. These fibers are generally used in clothing.\n\nHair from animals such as horses is also an animal fiber. Horsehair is used for brushes, the bows of musical instruments and many other things. The best artists brushes are made from Siberian weasel, many other fibers are used including ox hair and hog bristle. Camel-hair brushes are usually made from squirrel, cheaper ones from pony, but never camels. Chiengora is dog hair.\n\nWool from a wide range of animals can be used for handicrafts and garments. The table below lists a variety of animal fibers and the approximate average diameter of each. \n\n"}
{"id": "21898509", "url": "https://en.wikipedia.org/wiki?curid=21898509", "title": "Anti-degradant", "text": "Anti-degradant\n\nAn anti-degradant, or deterioration inhibitor is an ingredient in rubber compounds to deter the aging of rubber products.\n\nAnti-degradants include antioxidants and antiozonants. Since the aging of rubber is caused largely by oxygen, materials that quickly react with oxygen can be used as anti-degradant depending on the type of rubber, although organic compounds that easily react with oxygen are recommended for use as anti-degradant (chemical antioxidant).\n\nOn the other hand, the surface of rubber is sometimes covered with materials that do not easily react with oxygen to prevent direct contact between rubber and oxygen (physical anti-degradant, e.g., wax products).\n\nChemical antioxidants are classified into amine type anti-degradant and phenolic type anti-degradant depending on the chemical composition or into polymer stabilizers, thermal anti-degradants, and deterioration inhibitors depending on the major aging action; in many cases, however, it is difficult to distinguish their effects.\n\nAnti-degradants are further classified into staining anti-degradants or non-staining anti-degradants depending on whether or not rubber is colored, discolored, or otherwise stained.\n"}
{"id": "2266125", "url": "https://en.wikipedia.org/wiki?curid=2266125", "title": "Brisote", "text": "Brisote\n\nBrisote (also brisole) is a term used to describe the northeast trade wind when it is blowing more strongly than usual over Cuba. The typical strength of this wind is 9 m s; anything blowing at a stronger rate may be described as a brisote. A brisote may be associated with tropical cyclones passing north-east of the island.\n"}
{"id": "54021404", "url": "https://en.wikipedia.org/wiki?curid=54021404", "title": "Charge transport mechanisms", "text": "Charge transport mechanisms\n\nCharge transport mechanisms are theoretical models that aim to quantitatively describe the electric current flow through a given medium.\n\nCrystalline solids and molecular solids are two opposite extreme cases of materials that exhibit substantially different transport mechanisms. While in atomic solids transport is \"intra\"-molecular, also known as band transport, in molecular solids the transport is \"inter\"-molecular, also known as hopping transport. The two different mechanisms result in different charge mobilities.\n\nIn disordered solids, disordered potentials result in weak localization effects (traps), which reduce the mean free path, and hence the mobility, of mobile charges. Carrier recombination also decreases mobility.\n\nStarting with Ohm's law and using the definition of conductivity, it is possible to derive the following common expression for current as a function of carrier mobility μ and applied electric field E:\n\nThe relationship formula_2 holds when the concentration of localized states is significantly higher than the concentration of charge carriers, and assuming that hopping events are independent from each other.\n\nGenerally, the carrier mobility μ depends on temperature T, on the applied electric field E, and the concentration of localized states N. Depending on the model, increased temperature may either increase or decrease carrier mobility, applied electric field can increase mobility by contributing to thermal ionization of trapped charges, and increased concentration of localized states increases the mobility as well. Charge transport in the same material may have to be described by different models, depending on the applied field and temperature.\n\nCarrier mobility strongly depends on the concentration of localized states in a non-linear fashion. In the case of nearest-neighbour hopping, which is the limit of low concentrations, the following expression can be fitted to the experimental results:\nwhere formula_4 is the concentration and formula_5 is the localization length of the localized states. This equation is characteristic of incoherent hopping transport, which takes place at low concentrations, where the limiting factor is the exponential decay of hopping probability with inter-site distance.\n\nSometimes this relation is expressed for conductivity, rather than mobility:\nwhere formula_4 is the concentration of randomly distributed sites, formula_8 is concentration independent, formula_5 is the localization radius, and formula_10 is a numerical coefficient.\n\nAt high concentrations, a deviation from the nearest-neighbour model is observed, and variable-range hopping is used instead to describe transport. Variable range hopping can be used to describe disordered systems such as molecularly-doped polymers, low molecular weight glasses and conjugated polymers. In the limit of very dilute systems, the nearest-neighbour dependence formula_11 is valid, but only with formula_12.\n\nAt low carrier densities, the Mott formula for temperature-dependent conductivity is used to describe hopping transport. In variable hopping it is given by:\nwhere formula_14 is a parameter signifying a characteristic temperature. \nFor low temperatures, assuming a parabolic shape of the density of states near the Fermi level, the conductivity is given by:\n\nAt high carrier densities, an Arrhenius dependence is observed:\nIn fact, the electrical conductivity of disordered materials under DC bias has a similar form for a large temperature range, also known as activated conduction:\n\nHigh electric fields cause an increase in the observed mobility:\nIt was shown that this relationship holds for a large range of field strengths.\n\nThe real and imaginary parts of the AC conductivity for a large range of disordered semiconductors has the following form:\nwhere C is a constant and s is usually smaller than unity.\n\nSimilar to electron conduction, the electrical resistance of thin-film electrolytes depends on the applied electric field, such that when the thickness of the sample is reduced, the conductivity improves due to both the reduced thickness and the field-induced conductivity enhancement. The field dependence of the current density j through an ionic conductor, assuming a random walk model with independent ions under a periodic potential is given by:\nwhere α is the inter-site separation.\n\nCharacterization of transport properties requires fabricating a device and measuring its current-voltage characteristics. Devices for transport studies are typically fabricated by thin film deposition or break junctions. The dominant transport mechanism in a measured device can be determined by differential conductance analysis. In the differential form, the transport mechanism can be distinguished based on the voltage and temperature dependence of the current through the device.\n\nIt is common to express the mobility as a product of two terms, a field-independent term and a field-dependent term:\nwhere formula_23 is the activation energy and β is model-dependent. For Poole–Frenkel hopping, for example,\n\nTunneling and thermionic emission are typically observed when the barrier height is low.\nThermally-assisted tunneling is a \"hybrid\" mechanism that attempts to describe a range of simultaneous behaviours, from tunneling to thermionic emission.\n\n\n"}
{"id": "2107907", "url": "https://en.wikipedia.org/wiki?curid=2107907", "title": "Cooperative Institute for Climate Applications and Research", "text": "Cooperative Institute for Climate Applications and Research\n\nThe Cooperative Institute for Climate Applications and Research (CICAR) formalizes a major collaborative relationship between the National Oceanic and Atmospheric Administration (NOAA) Office of Oceanic and Atmospheric Research (OAR) and Columbia University.\n\nThe CICAR research themes are:\n\n"}
{"id": "175875", "url": "https://en.wikipedia.org/wiki?curid=175875", "title": "Critical mass", "text": "Critical mass\n\nA critical mass is the smallest amount of fissile material needed for a sustained nuclear chain reaction. The critical mass of a fissionable material depends upon its nuclear properties (specifically, the nuclear fission cross-section), its density, its shape, its enrichment, its purity, its temperature, and its surroundings. The concept is important in nuclear weapon design.\n\nWhen a nuclear chain reaction in a mass of fissile material is self-sustaining, the mass is said to be in a \"critical\" state in which there is no increase or decrease in power, temperature, or neutron population.\n\nA numerical measure of a critical mass is dependent on the effective neutron multiplication factor , the average number of neutrons released per fission event that go on to cause another fission event rather than being absorbed or leaving the material. When \"k\" = 1, the mass is critical, and the chain reaction is self-sustaining.\nA \"subcritical\" mass is a mass of fissile material that does not have the ability to sustain a fission chain reaction. A population of neutrons introduced to a subcritical assembly will exponentially decrease. In this case, \"k\" < 1. A steady rate of spontaneous fissions causes a proportionally steady level of neutron activity. The constant of proportionality increases as increases.\n\nA \"supercritical\" mass is one in which, once fission has started, it will proceed at an increasing rate. The material may settle into equilibrium (\"i.e.\" become critical again) at an elevated temperature/power level or destroy itself. In the case of supercriticality, \"k\" > 1.\n\nDue to spontaneous fission a supercritical mass will undergo a chain reaction. For example, a spherical critical mass of pure uranium-235 will have a mass of 52 kg and will experience around 15 spontaneous fission events per second. The probability that one such event will cause a chain reaction depends on how much the mass exceeds the critical mass. If there is uranium-238 present, the rate of spontaneous fission will be much higher. Fission can also be initiated by neutrons produced by cosmic rays.\n\nThe mass where criticality occurs may be changed by modifying certain attributes such as fuel, shape, temperature, density and the installation of a neutron-reflective substance. These attributes have complex interactions and interdependencies. These examples only outline the simplest ideal cases:\n\nIt is possible for a fuel assembly to be critical at near zero power. If the perfect quantity of fuel were added to a slightly subcritical mass to create an \"exactly critical mass\", fission would be self-sustaining for only one neutron generation (fuel consumption then makes the assembly subcritical again).\n\nIf the perfect quantity of fuel were added to a slightly subcritical mass, to create a barely supercritical mass, the temperature of the assembly would increase to an initial maximum (for example: 1 K above the ambient temperature) and then decrease back to the ambient temperature after a period of time, because fuel consumed during fission brings the assembly back to subcriticality once again.\n\nA mass may be exactly critical without being a perfect homogeneous sphere. More closely refining the shape toward a perfect sphere will make the mass supercritical. Conversely changing the shape to a less perfect sphere will decrease its reactivity and make it subcritical.\n\nA mass may be exactly critical at a particular temperature. Fission and absorption cross-sections increase as the relative neutron velocity decreases. As fuel temperature increases, neutrons of a given energy appear faster and thus fission/absorption is less likely. This is not unrelated to Doppler broadening of the U resonances but is common to all fuels/absorbers/configurations. Neglecting the very important resonances, the total neutron cross-section of every material exhibits an inverse relationship with relative neutron velocity. Hot fuel is always less reactive than cold fuel (over/under moderation in LWR is a different topic). Thermal expansion associated with temperature increase also contributes a negative coefficient of reactivity since fuel atoms are moving farther apart. A mass that is exactly critical at room temperature would be sub-critical in an environment anywhere above room temperature due to thermal expansion alone.\n\nThe higher the density, the lower the critical mass. The density of a material at a constant temperature can be changed by varying the pressure or tension or by changing crystal structure (see allotropes of plutonium). An ideal mass will become subcritical if allowed to expand or conversely the same mass will become supercritical if compressed. Changing the temperature may also change the density; however, the effect on critical mass is then complicated by temperature effects (see \"Changing the temperature\") and by whether the material expands or contracts with increased temperature. Assuming the material expands with temperature (enriched uranium-235 at room temperature for example), at an exactly critical state, it will become subcritical if warmed to lower density or become supercritical if cooled to higher density. Such a material is said to have a negative temperature coefficient of reactivity to indicate that its reactivity decreases when its temperature increases. Using such a material as fuel means fission decreases as the fuel temperature increases.\n\nSurrounding a spherical critical mass with a neutron reflector further reduces the mass needed for criticality. A common material for a neutron reflector is beryllium metal. This reduces the number of neutrons which escape the fissile material, resulting in increased reactivity.\n\nIn a bomb, a dense shell of material surrounding the fissile core will contain, via inertia, the expanding fissioning material. This increases the efficiency. A tamper also tends to act as a neutron reflector. Because a bomb relies on fast neutrons (not ones moderated by reflection with light elements, as in a reactor), because the neutrons reflected by a tamper are slowed by their collisions with the tamper nuclei, and because it takes time for the reflected neutrons to return to the fissile core, they take rather longer to be absorbed by a fissile nucleus. But they do contribute to the reaction, and can decrease the critical mass by a factor of four. Also, if the tamper is (e.g. depleted) uranium, it can fission due to the high energy neutrons generated by the primary explosion. This can greatly increase yield, especially if even more neutrons are generated by fusing hydrogen isotopes, in a so-called boosted configuration.\n\nThe critical size is the minimum size of a nuclear reactor core or nuclear weapon that can be made for a specific geometrical arrangement and material composition. The critical size must at least include enough fissionable material to reach critical mass. If the size of the reactor core is less than a certain minimum, too many fission neutrons escape through its surface and the chain reaction is not sustained.\n\nThe shape with minimal critical mass and the smallest physical dimensions is a sphere. Bare-sphere critical masses at normal density of some actinides are listed in the following table.\n\nThe critical mass for lower-grade uranium depends strongly on the grade: with 20% U it is over 400 kg; with 15% U, it is well over 600 kg.\n\nThe critical mass is inversely proportional to the square of the density. If the density is 1% more and the mass 2% less, then the volume is 3% less and the diameter 1% less. The probability for a neutron per cm travelled to hit a nucleus is proportional to the density. It follows that 1% greater density means that the distance travelled before leaving the system is 1% less. This is something that must be taken into consideration when attempting more precise estimates of critical masses of plutonium isotopes than the approximate values given above, because plutonium metal has a large number of different crystal phases which can have widely varying densities.\n\nNote that not all neutrons contribute to the chain reaction. Some escape and others undergo radiative capture.\n\nLet \"q\" denote the probability that a given neutron induces fission in a nucleus. Let us consider only prompt neutrons, and let \"ν\" denote the number of prompt neutrons generated in a nuclear fission. For example, \"ν\" ≈ 2.5 for uranium-235. Then, criticality occurs when \"ν·q\" = 1. The dependence of this upon geometry, mass, and density appears through the factor \"q\".\n\nGiven a total interaction cross section σ (typically measured in barns), the mean free path of a prompt neutron is formula_1 where \"n\" is the nuclear number density. Most interactions are scattering events, so that a given neutron obeys a random walk until it either escapes from the medium or causes a fission reaction. So long as other loss mechanisms are not significant, then, the radius of a spherical critical mass is rather roughly given by the product of the mean free path formula_2 and the square root of one plus the number of scattering events per fission event (call this \"s\"), since the net distance travelled in a random walk is proportional to the square root of the number of steps:\n\nformula_3\n\nNote again, however, that this is only a rough estimate.\n\nIn terms of the total mass \"M\", the nuclear mass \"m\", the density ρ, and a fudge factor \"f\" which takes into account geometrical and other effects, criticality corresponds to\n\nformula_4\n\nwhich clearly recovers the aforementioned result that critical mass depends inversely on the square of the density.\n\nAlternatively, one may restate this more succinctly in terms of the areal density of mass, Σ:\n\nformula_5\n\nwhere the factor \"f\" has been rewritten as \"f\"' to account for the fact that the two values may differ depending upon geometrical effects and how one defines Σ. For example, for a bare solid sphere of Pu criticality is at 320 kg/m, regardless of density, and for U at 550 kg/m. \nIn any case, criticality then depends upon a typical neutron \"seeing\" an amount of nuclei around it such that the areal density of nuclei exceeds a certain threshold.\n\nThis is applied in implosion-type nuclear weapons where a spherical mass of fissile material that is substantially less than a critical mass is made supercritical by very rapidly increasing ρ (and thus Σ as well) (see below). Indeed, sophisticated nuclear weapons programs can make a functional device from less material than more primitive weapons programs require.\n\nAside from the math, there is a simple physical analog that helps explain this result. Consider diesel fumes belched from an exhaust pipe. Initially the fumes appear black, then gradually you are able to see through them without any trouble. This is not because the total scattering cross section of all the soot particles has changed, but because the soot has dispersed. If we consider a transparent cube of length \"L\" on a side, filled with soot, then the optical depth of this medium is inversely proportional to the square of \"L\", and therefore proportional to the areal density of soot particles: we can make it easier to see through the imaginary cube just by making the cube larger.\n\nSeveral uncertainties contribute to the determination of a precise value for critical masses, including (1) detailed knowledge of fission cross sections, (2) calculation of geometric effects. This latter problem provided significant motivation for the development of the \"Monte Carlo method\" in computational physics by Nicholas Metropolis and Stanislaw Ulam. In fact, even for a homogeneous solid sphere, the exact calculation is by no means trivial. Finally, note that the calculation can also be performed by assuming a continuum approximation for the neutron transport. This reduces it to a diffusion problem. However, as the typical linear dimensions are not significantly larger than the mean free path, such an approximation is only marginally applicable.\n\nFinally, note that for some idealized geometries, the critical mass might formally be infinite, and other parameters are used to describe criticality. For example, consider an infinite sheet of fissionable material. For any finite thickness, this corresponds to an infinite mass. However, criticality is only achieved once the thickness of this slab exceeds a critical value.\n\nUntil detonation is desired, a nuclear weapon must be kept subcritical. In the case of a uranium bomb, this can be achieved by keeping the fuel in a number of separate pieces, each below the critical size either because they are too small or unfavorably shaped. To produce detonation, the pieces of uranium are brought together rapidly. In Little Boy, this was achieved by firing a piece of uranium (a 'doughnut') down a gun barrel onto another piece (a 'spike'). This design is referred to as a \"gun-type fission weapon\".\n\nA theoretical 100% pure Pu weapon could also be constructed as a gun-type weapon, like the Manhattan Project's proposed Thin Man design. In reality, this is impractical because even \"weapons grade\" Pu is contaminated with a small amount of Pu, which has a strong propensity toward spontaneous fission. Because of this, a reasonably sized gun-type weapon would suffer nuclear reaction (predetonation) before the masses of plutonium would be in a position for a full-fledged explosion to occur.\nInstead, the plutonium is present as a subcritical sphere (or other shape), which may or may not be hollow. Detonation is produced by exploding a shaped charge surrounding the sphere, increasing the density (and collapsing the cavity, if present) to produce a prompt critical configuration. This is known as an \"implosion type weapon\".\n\nThe event of fission must release, on the average, more than one free neutron of the desired energy level in order to sustain a chain reaction, and each must find other nuclei and cause them to fission. Most of the neutrons released from a fission event come immediately from that event, but a fraction of them come later, when the fission products decay, which may be on the average from microseconds to minutes later. This is fortunate for atomic power generation, for without this delay \"going critical\" would always be an immediately catastrophic event, as it is in a nuclear bomb where upwards of 80 generations of chain reaction occur in less than a microsecond, far too fast for man, or even machine, to react. Physicists recognize two points in the gradual increase of neutron flux which are significant: critical, where the chain reaction becomes self-sustaining thanks to the contributions of both kinds of neutron generation, and prompt critical, where the immediate \"prompt\" neutrons alone will sustain the reaction without need for the decay neutrons. Nuclear power plants operate between these two points of reactivity, while above the prompt critical point is the domain of nuclear weapons and some nuclear power accidents, such as the Chernobyl disaster.\n\nA convenient unit for the measurement of the reactivity is that suggested by Louis Slotin: that of the dollar and cents.\n\n"}
{"id": "854413", "url": "https://en.wikipedia.org/wiki?curid=854413", "title": "DLR-Archenhold Near Earth Objects Precovery Survey", "text": "DLR-Archenhold Near Earth Objects Precovery Survey\n\nDANEOPS, the DLR-Archenhold Near Earth Objects Precovery Survey, has been initiated to systematically search existing photographic plate archives for precovery images of known NEOs. It has so far (July 2004) precovered or recovered some 145 objects.\n\n"}
{"id": "48594435", "url": "https://en.wikipedia.org/wiki?curid=48594435", "title": "Dark diversity", "text": "Dark diversity\n\nDark diversity is the set of species that are absent from a study site but present in the surrounding region and potentially able to inhabit particular ecological conditions.\n\nDark diversity is part of the species pool concept. A species pool is defined as set of all species that are able to inhabit particular site and that are present in the surrounding region or landscape. Dark diversity comprises species that belong to a particular species pool but that are not currently present at a site. Dark diversity is related to \"habitat-specific\" or \"filtered\" species pool which only includes species that can both disperse to and potentially inhabit the study site. For example, if fish diversity in a coral reef site has been sampled, dark diversity includes all fish species from the surrounding region that are currently absent but can potentially disperse to and colonize the study site.\n\nDark diversity name is borrowed from dark matter: matter which cannot be seen and directly measured, but its existence and properties are inferred from its gravitational effects on visible matter. Similarly, dark diversity cannot be seen directly when only the sample is observed, but it is present if broader scale is considered, and its existence and properties can be estimated when proper data is available. With dark matter we can better understand distribution and dynamics of galaxies, with dark diversity we can understand composition and dynamics of ecological communities.\n\nDark diversity is the counterpart of observed diversity (alpha diversity) present in a sample. Dark diversity is habitat-specific in respect that the study site must contain favorable ecological conditions for species belonging to dark diversity. The habitat concept can be narrower (e.g. microhabitat in an old-growth forest) or broader (e.g. terrestrial habitat). Thus, habitat specificity does not mean that all species in dark diversity can inhabit all localities within study sample, but there must be ecologically suitable parts.\n\nHabitat-specificity is making the distinction between dark diversity and beta diversity. If beta diversity is the association between alpha and gamma diversity, dark diversity connects alpha diversity and habitat-specific (filtered) species pool. Habitat-specific species pool only these which can potentially inhabit focal study site.\nObserved diversity can be studied at any scale, and sites with varying heterogeneity. This is also true for dark diversity. Consequently, as local observed diversity can be linked to very different sample sizes, dark diversity can be applied at any study scale (1x1 m sample in a vegetation, bird count transect in a landscape, 50x50 km UTM grid cell).\n\nRegion size determines likelihood of dispersal to study site and selecting appropriate scale depends on research question. For a more general study, a scale comparable to biogeographic region can be used (e.g. a small country, a state, or radius of few hundred km). If we want to know which species potentially can inhabit study site in the near future (for example 10 years), landscape scale is appropriate.\n\nTo separate ecologically suitable species, different methods can be used. Environmental niche modelling can be applied for a large number of species. Expert opinion can be used. Data on species' habitat preferences is available in books, e.g. bird nesting habitats. This can also be quantitative, for example plant species indicator values, according to Ellenberg. A recently developed method estimates dark diversity from species co-occurrence matrices. An online tool is available for the co-occurrence method.\n\nDark diversity allows meaningful comparisons of biodiversity. The community completeness index can be used: log(observed diversity / dark diversity). This express the local diversity at the relative scale, filtering out the effect of regional species pool. For example, if completeness of plant diversity was studied at the European scale, it did not exhibit the latitudinal pattern seen with observed richness and species pool values. Instead, high completeness was characteristic to regions with lower human impact, indicating that anthropogenic factors are among the most important local scale biodiversity determinants in Europe.\n\nDark diversity studies can be combined with functional ecology to understand why species pool is poorly realized in a locality. For example, if functional traits were compared between grassland species in observed diversity and dark diversity, it become evident, that dark diversity species have in general poorer dispersal abilities.\n\nDark diversity can be useful in prioritizing nature conservation, to identify in different regions most complete sites. Dark diversity helps to estimate the relative loss of local diversity, but also restoration progress. Dark diversity of alien species, weeds and pathogens can be useful to prepare for future invasions in time.\n\nRecently, dark diversity concept was used in to explain mechanisms behind plant diversity-productivity relationship.\n\n"}
{"id": "41216691", "url": "https://en.wikipedia.org/wiki?curid=41216691", "title": "David Vanderpool", "text": "David Vanderpool\n\nDavid Vanderpool (born February 18, 1960) is an American medical missionary and the CEO and founder of Live Beyond, which has provided medical, spiritual and logistical support to disaster ridden countries.\n\nVanderpool's work is unusual in that he combines his medical training with an explicit effort to convert his patients to Christianity.\n\nVanderpool was born in Dallas, Texas, graduated from St. Mark's School of Texas, and received his undergraduate degree from Abilene Christian University in 1982. He then attended the School of Medicine at Texas Tech University Health Sciences Center. After medical school, Vanderpool completed two surgical residencies at Baylor University Medical Center where he trained as a vascular surgeon.\n\nVanderpool wife, Laurie, is a speaker for Women's Retreats and Bible Studies, and speaks frequently for Down Syndrome organizations.\n\nVanderpool remained in Texas after residency and practiced as a vascular surgeon before moving to Brentwood, Tennessee in 2001 and opening his private practice Lave MD in 2003.\n\nDr. Vanderpool created Lave MD to act as both a medical facility and a spa. \nAfter the establishment of his international organization in 2005, Dr. Vanderpool used much of Lave MD's proceeds to fund the organization's efforts abroad.\n\nIn 2005, after Hurricane Katrina hit the southeastern part of the United States, Dr. Vanderpool delivered healthcare across the Mississippi Coast out of a trailer. His goal was to provide as much free healthcare as possible while the medical infrastructure could recover. Months later, Dr. Vanderpool established his organization, Medical Mobile Disaster Relief, as a 501(c)(3) non-profit organization with goals to provide disaster relief through medical clinics, clean water projects, and micro-finance projects to areas hit by disasters. \nVanderpool and the Mobile Medical Disaster Relief team began working in Mozambique in 2006. His goal was to provide healthcare to the indigenous people of the country in addition to enhancing the economy by implementing micro-finance projects among widows living in the communities. By 2008, Dr. Vanderpool teamed up with the Belmont School of Nursing to construct a nursing curriculum that could teach the Mozambique women to be self-sufficient in caring for themselves and their children.\n\nVanderpool and his Mobile Medical Disaster Relief partnered with PACODEP (Partners in Community Development) in Ghana in order to provide medical care, educate the locals on water purification and distribute water purifiers. PACODEP works to free enslaved children who are trafficked through Ghana for purposes of fishing work. Dr. Vanderpool has also partnered with local hospitals in Ghana in order to provide free invasive surgeries to these rescued children.\n\nDr. Vanderpool partnered with Mission Lazarus in 2009 to build a sustainable medical clinic in Cedeño, Honduras. Given sufficient medical supplies and equipment, Vanderpool allowed Mission Lazarus to take over the clinic and provision of healthcare for the people of Cedeño.\n\nDr. Vanderpool shifted his focus in the aftermath of the earthquake that devastated the entire country of Haiti in 2010. In 2010, Vanderpool officially changed the name of his organization from Mobile Medical Disaster Relief to Live Beyond with a stated mission to be \"an organization that chooses to Live Beyond...ourselves, our culture, our borders and this life so that others can Live Beyond…disease, hunger, poverty and despair.\"\n\nThe initial aid Vanderpool and his team brought to Haiti was primarily mobile medical care to relieve the thousands devastated and injured by the earthquake. Since then however, Vanderpool has grounded his missionary work in Thomazeau, Haiti where his organization began building a base. Vanderpool continued to provide free medical care, establishing a surgical hospital and clinic. In addition, clean water projects, orphanages, and other widow and orphan advocacy projects were begun.\n\nDr. Vanderpool is a Christian, who combines religion and the spread of his faith with his medical work. In Haiti, he has made it one of his objectives to bring Haiti away from its traditional voodoo culture and provide \"spiritual guidance\" to the Haitians in the role of Christianity, with the belief that Christianity will lead to a better Haiti. Prior to moving to Haiti, each medical outreach trip made by Vanderpool and other Live Beyond participants included prayer and Christian ministry along with healthcare to the voodoo priests, island chiefs, idol worshipers and the sick and dying in Haiti. On the Live Beyond site, Vanderpool's religious impact since being in Haiti has been characterized as leading to the baptism of dozens, the saving of tribes through \"Bibles being read in their own languages\" and \"the Kingdom is being expanded.\" Dr. Vanderpool promotes religious missionary work in tandem with his medical relief and sustainable development efforts. A worship center is being built in Thomazeau, Haiti and monthly mission trips are promoted and scheduled for Americans to travel to Thomazeau and volunteer.\n\n"}
{"id": "7732103", "url": "https://en.wikipedia.org/wiki?curid=7732103", "title": "Denton Hill State Park", "text": "Denton Hill State Park\n\nDenton Hill State Park is a Pennsylvania state park in Ulysses Township, Potter County, Pennsylvania, in the United States. The park is a downhill skiing resort. Denton Hill State Park is on U.S. Route 6 between Coudersport and Galeton. In 2000 the park became part of the Hills Creek State Park complex, an administrative grouping of eight state parks in Potter and Tioga counties.\n\nDenton Hill State Park was the first ski operation run by the Pennsylvania Department of Forests and Waters (a predecessor to the Pennsylvania Department of Conservation and Natural Resources). The park was established in 1951, opened in 1959, and the lodge and ski area were built between 1958 and 1967. The ski area was operated by the state until 1979, when it became a concession run by a private contractor (\"Ski Denton\" as of 2011).\n\nThe skiing ranges from easy, beginner slopes to some of the most dangerous, expert, black diamond slopes on the east coast of the United States. There are four ski lifts at the park, each of which stops at a different elevation on the slopes. Lift-accessed Snow tubing is also available at Denton Hill State Park, or Ski Denton.\n\nThere are five chalets on the grounds of Ski Denton. Each cabin has wall to wall carpeting and two bedrooms that sleep up to six individuals. There is a large living room with a cathedral ceiling in each cabin. The cabins also have a full kitchen, dining room, and bathroom. A hostel style bunkhouse sits atop the main lodge.\n\nSki Denton opens its slopes and trails for lift-serviced mountain bikers in the off season. There are over of maintained downhill and cross-country single track and trails available for riding. A mountain bike trail begins at Denton Hill State Park and is long, passing through Patterson State Park on its way to Cherry Springs State Park. Part of the Susquehannock Trail System, an loop hiking trail, forms the southern border of the park.\n\nThe following state parks are within of Denton Hill State Park:\n"}
{"id": "25643772", "url": "https://en.wikipedia.org/wiki?curid=25643772", "title": "Diurnal cycle", "text": "Diurnal cycle\n\nA diurnal cycle is any pattern that recurs every 24 hours as a result of one full rotation of the Earth around its own axis.\n\nIn climatology, the diurnal cycle is one of the most basic forms of climate patterns. The most familiar such pattern is the diurnal temperature variation. Such a cycle may be approximately sinusoidal, or include components of a truncated sinusoid (due to the sun's rising and setting) and thermal relaxation (Newton cooling) at night.\n\nDiurnal cycles of environmental conditions (light or temperature) can result in similar cycles in dependent biological processes, such as photosynthesis in plants, or clinical depression in humans. Plant responses to environmental cycles may even induce indirect cycles in rhizosphere microbial activities, including nitrogen fixation.\n\nA semi-diurnal cycle refers to a pattern that occurs about every twelve hours or about twice a day. Often these can be related to lunar tides, in which case the interval is closer to 12 hours and 25 minutes.\n\n"}
{"id": "36238152", "url": "https://en.wikipedia.org/wiki?curid=36238152", "title": "Driving factors", "text": "Driving factors\n\nIn energy monitoring and targeting, a driving factor is something recurrent and measurable whose variation explains variation in energy consumption. The term \"independent variable\" is sometimes used as a synonym.\n\nOne of the most common driving factors is the weather, expressed usually as heating or cooling degree days. In energy-intensive processes, production throughputs would usually be used. For electrical circuits feeding outdoor lighting, the number of hours of darkness can be employed. For a borehole pump, the quantity of water delivered would be used; and so on. What these examples all have in common is that on a weekly basis (say) numerical values can be recorded for each factor and one would expect particular streams of energy consumption to correlate with them either singly or in a multi-variate model.\n\nCorrelation is arguably more important than causality. Variation in the driving factor merely has to \"explain\" variation in consumption; it does not necessarily have to \"cause\" it, although that will in most scenarios be the case.\n\nDriving factors differ from \"static\" factors, such as building floor areas, which determine energy consumption but change only rarely (if at all).\n"}
{"id": "15068567", "url": "https://en.wikipedia.org/wiki?curid=15068567", "title": "Einstein–Hopf drag", "text": "Einstein–Hopf drag\n\nIn physics, the Einstein–Hopf drag (named after Albert Einstein and Ludwig Hopf) is a velocity-dependent drag force upon charged particles that are being bathed in thermal radiation.\n\n"}
{"id": "37765572", "url": "https://en.wikipedia.org/wiki?curid=37765572", "title": "Frances Barkley", "text": "Frances Barkley\n\nFrances Barkley was wife of Captain Charles William Barkley, who traveled with him. She is considered to be the first European woman to have ever visited Canada's west coast. Frances was the first woman to sail around the world without deception. Only two women are known to have sailed around the world before Frances: Jeanne Baré, disguised as a man, and Rose de Freycinet, wife of Louis de Freycinet, as a stowaway.\n"}
{"id": "19466135", "url": "https://en.wikipedia.org/wiki?curid=19466135", "title": "General Bathymetric Chart of the Oceans", "text": "General Bathymetric Chart of the Oceans\n\nThe General Bathymetric Chart of the Oceans (GEBCO) is a publicly available bathymetric chart of the world's oceans. The project was conceived with the aim of preparing a global series of charts showing the general shape of the seafloor. Over the years it has become a reference map of the bathymetry of the world’s oceans for scientists and others.\n\nGEBCO operates under the joint auspices of the International Hydrographic Organization (IHO). and the Intergovernmental Oceanographic Commission (IOC) of UNESCO. Its work is done by an international group of experts in seafloor mapping who develop a range of bathymetric data sets and data products.\n\nAlthough originally GEBCO published paper contour charts, today it has moved into the digital age and collects digital depths of the ocean from wherever they are available. GEBCO provides a range of bathymetric data sets and data products, including:\n\nThe grids are available to download from the British Oceanographic Data Centre (BODC) in the form of netCDF files, along with free software for displaying and accessing data in ASCII and netCDF. The grids can be used with the Generic Mapping Tools (GMT) system.\n\nThe GEBCO chart series was initiated in 1903 by an international group of geographers and oceanographers, under the leadership of Prince Albert I of Monaco. At that time there was an explosion of interest in the study of the natural world and this group recognized the importance of a set of maps describing the shape of the ocean floor. The first hundred years of the project were described in the book \"The History of GEBCO 1903-2003\" published by GITC in 2003. Nowadays GEBCO’s role has become increasingly important, due to the increased interest in the oceans for scientific research and for the exploitation and conservation of resources.\n\nSince 1903, five separate editions of paper, bathymetric contour charts covering the whole world have been produced. GEBCO is now maintained in digital form as the GEBCO Digital Atlas.\n\nThe Nippon Foundation of Japan has provided funding for GEBCO to train a new generation of scientists and hydrographers in ocean bathymetry. The 12-month course, leading to a Postgraduate Certificate in Ocean Bathymetry (PCOB), has been held at the University of New Hampshire, USA, since 2004. 60 GEBCO scholars from 31 different countries have completed the course and are supporting GEBCO programs.\n\n"}
{"id": "167120", "url": "https://en.wikipedia.org/wiki?curid=167120", "title": "Gravel", "text": "Gravel\n\nGravel is a loose aggregation of rock fragments. Gravel is classified by particle size range and includes size classes from granule- to boulder-sized fragments. In the Udden-Wentworth scale gravel is categorized into granular gravel () and pebble gravel (). ISO 14688 grades gravels as fine, medium, and coarse with ranges 2 mm to 6.3 mm to 20 mm to 63 mm. One cubic metre of gravel typically weighs about 1,800 kg (or a cubic yard weighs about 3,000 pounds).\n\nGravel is an important commercial product, with a number of applications. Many roadways are surfaced with gravel, especially in rural areas where there is little traffic. Globally, far more roads are surfaced with gravel than with concrete or tarmac; Russia alone has over of gravel roads. Both sand and small gravel are also important for the manufacture of concrete.\n\nLarge gravel deposits are a common geological feature, being formed as a result of the weathering and erosion of rocks. The action of rivers and waves tends to pile up gravel in large accumulations. This can sometimes result in gravel becoming compacted and lithified into the sedimentary rock called conglomerate. Where natural gravel deposits are insufficient for human purposes, gravel is often produced by quarrying and crushing hard-wearing rocks, such as sandstone, limestone, or basalt. Quarries where gravel is extracted are known as gravel pits. Southern England possesses particularly large concentrations of them due to the widespread deposition of gravel in the region during the Ice Ages.\n\nAs of 2006, the United States is the world's leading producer and consumer of gravel.\n\nThe word \"gravel\" comes from the Breton language. In Breton, \"grav\" means coast. Adding the \"-el\" suffix in Breton denotes the component parts of something larger. Thus \"gravel\" means the small stones which make up such a beach on the coast. Many dictionaries ignore the Breton language, citing Old French \"gravele\" or \"gravelle\".\n\nGravel often has the meaning a mixture of different size pieces of stone mixed with sand and possibly some clay. In American English, rocks broken into small pieces by a crusher are known as crushed stone.\n\nTypes of gravel include:\n\n\nIn locales where gravelly soil is predominant, plant life is generally more sparse. This outcome derives from the inferior ability of gravels to retain moisture, as well as the corresponding paucity of mineral nutrients, since finer soils that contain such minerals are present in smaller amounts.\n\n"}
{"id": "527661", "url": "https://en.wikipedia.org/wiki?curid=527661", "title": "Hubble Ultra-Deep Field", "text": "Hubble Ultra-Deep Field\n\nThe Hubble Ultra-Deep Field (HUDF) is an image of a small region of space in the constellation Fornax, containing an estimated 10,000 galaxies. The original release was combined from Hubble Space Telescope data accumulated over a period from September 24, 2003, through to January 16, 2004. Looking back approximately 13 billion years (between 400 and 800 million years after the Big Bang) it has been used to search for galaxies that existed at that time. The HUDF image was taken in a section of the sky with a low density of bright stars in the near-field, allowing much better viewing of dimmer, more distant objects. In August and September 2009, the HUDF field was observed at longer wavelengths (1.0 to 1.6 micrometers) using the infrared channel of the recently attached Wide Field Camera 3 (WFC3) instrument. When combined with existing HUDF data, astronomers were able to identify a new list of potentially very distant galaxies.\n\nLocated southwest of Orion in the southern-hemisphere constellation Fornax, the rectangular image is 2.4 arcminutes to an edge, or 3.4 arcminutes diagonally. This is approximately one tenth of the angular diameter of a full moon viewed from Earth (which is less than 34 arcminutes), smaller than a 1 mm by 1 mm square of paper held at 1 meter away, and equal to roughly one twenty-six-millionth of the total area of the sky. The image is oriented so that the upper left corner points toward north (−46.4°) on the celestial sphere.\n\nOn September 25, 2012, NASA released a further refined version of the Ultra-Deep Field dubbed the eXtreme Deep Field (XDF). The XDF reveals galaxies that span back 13.2 billion years in time, revealing a galaxy theorized to be formed only 450 million years after the big bang event. On June 3, 2014, NASA released the Hubble Ultra-Deep Field image composed of, for the first time, the full range of ultraviolet to near-infrared light.\n\nIn the years since the original Hubble Deep Field, the Hubble Deep Field South and the GOODS sample were analyzed, providing increased statistics at the high redshifts probed by the HDF. When the Advanced Camera for Surveys (ACS) detector was installed on the HST, it was realized that an ultra-deep field could observe galaxy formation out to even higher redshifts than had currently been observed, as well as providing more information about galaxy formation at intermediate redshifts (z~2). A workshop on how to best carry out surveys with the ACS was held at StScI in late 2002. At the workshop Massimo Stiavelli advocated an Ultra Deep Field as a way to study the objects responsible for the reionization of the Universe. Following the workshop, the STScI Director Steven Beckwith decided to devote 400 orbits of Director's Discretionary time to the UDF and appointed Stiavelli as the lead of the Home Team implementing the observations.\n\nUnlike the Deep Fields, the HUDF does not lie in Hubble's Continuous Viewing Zone (CVZ). The earlier observations, using the Wide Field and Planetary Camera 2 (WFPC2) camera, were able to take advantage of the increased observing time on these zones by using wavelengths with higher noise to observe at times when earthshine contaminated the observations; however ACS does not observe at these wavelengths, so the advantage was reduced.\n\nAs with the earlier fields, this one was required to contain very little emission from our galaxy, with little Zodiacal dust. The field was also required to be in a range of declinations such that it could be observed both by southern hemisphere instruments, such as the Atacama Large Millimeter Array, and northern hemisphere ones, such as those located on Hawaii. It was ultimately decided to observe a section of the Chandra Deep Field South, due to existing deep X-ray observations from Chandra X-ray Observatory and two interesting objects already observed in the GOODS sample at the same location: a redshift 5.8 galaxy and a supernova. The coordinates of the field are right ascension , declination (J2000). The field is 200 arcseconds to a side, with a total area of 11 square arcminutes, and lies in the constellation of Fornax.\n\nFour filters were used on the ACS, centered on 435, 606, 775 and 850 nm, with exposure times set to give equal sensitivity in all filters. These wavelength ranges match those used by the GOODS sample, allowing direct comparison between the two. As with the Deep Fields, the HUDF used Directors Discretionary Time. In order to get the best resolution possible, the observations were dithered by pointing the telescope at slightly different positions for each exposure—a process trialled with the Hubble Deep Field—so that the final image has a higher resolution than the pixels on their own would normally allow.\n\nThe observations were done in two sessions, from September 23 to October 28, 2003, and December 4, 2003, to January 15, 2004. The total exposure time is just under 1 million seconds, from 400 orbits, with a typical exposure time of 1200 seconds. In total, 800 ACS exposures were taken over the course of 11.3 days, 2 every orbit, and NICMOS observed for 4.5 days. All the individual ACS exposures were processed and combined by Anton Koekemoer into a single set of scientifically useful images, each with a total exposure time ranging from 134,900 seconds to 347,100 seconds. To observe the whole sky to the same sensitivity, the HST would need to observe continuously for a million years.\n\nThe sensitivity of the ACS limits its capability of detecting galaxies at high redshift to about 6. The deep NICMOS fields obtained in parallel to the ACS images could in principle be used to detect galaxies at redshift 7 or higher but they were lacking visible band images of similar depth. These are necessary to identify high redshift objects as they should not be seen in the visible bands. In order to obtain deep visible exposures on top of the NICMOS parallel fields a follow-up program, HUDF05, was approved and granted 204 orbits to observe the two parallel fields (GO-10632). The orientation of the HST was chosen so that further NICMOS parallel images would fall on top of the main UDF field.\n\nAfter the installation of WFC3 on Hubble in 2009, the HUDF09 programme (GO-11563) devoted 192 orbits to observations of three fields, including HUDF, using the newly available F105W, F125W and F160W infra-red filters (which correspond to the Y, J and H bands):\n\nThe HUDF is the deepest image of the universe ever taken and has been used to search for galaxies that existed between 400 and 800 million years after the Big Bang (redshifts between 7 and 12). Several galaxies in the HUDF are candidates, based on photometric redshifts, to be amongst the most distant astronomical objects. The red dwarf UDF 2457 at distance of 59,000 light-years is the furthest star resolved by the HUDF. The star near the center of the field is USNO-A2.0 0600-01400432 with apparent magnitude of 18.95.\n\nThe field imaged by the ACS contains over 10,000 objects, the majority of which are galaxies, many at redshifts greater than 3, and some that probably have redshifts between 6 and 7. The NICMOS measurements may have discovered galaxies at redshifts up to 12.\n\nThe HUDF has revealed high rates of star formation during the very early stages of galaxy formation, within a billion years after the Big Bang. It has also enabled improved characterization of the distribution of galaxies, their numbers, sizes and luminosities at different epochs, aiding investigation into the evolution of galaxies. Galaxies at high redshifts have been confirmed to be smaller and less symmetrical than ones at lower redshifts, illuminating the rapid evolution of galaxies in the first couple of billion years after the Big Bang.\n\nThe Hubble eXtreme Deep Field (HXDF), released on September 25, 2012, is an image of a portion of space in the center of the Hubble Ultra Deep Field image. Representing a total of two million seconds (approximately 23 days) of exposure time collected over 10 years, the image covers an area of 2.3 arcminutes by 2 arcminutes, or approximately 80% of the area of the HUDF. This represents approximately one thirty-two millionth of the sky.\n\nThe HXDF contains approximately 5,500 galaxies, the oldest of which are seen as they were 13.2 billion years ago. The faintest galaxies are one ten-billionth the brightness of what the human eye can see. The red galaxies in the image are the remnants of galaxies after major collisions during their elderly years. Many of the smaller galaxies in the image are very young galaxies that eventually developed into major galaxies, similar to the Milky Way and other galaxies in our galactic neighborhood.\n\n\n"}
{"id": "30406566", "url": "https://en.wikipedia.org/wiki?curid=30406566", "title": "IEC 60870-5", "text": "IEC 60870-5\n\nIEC 60870 part 5 is one of the IEC 60870 set of standards which define systems used for telecontrol (supervisory control and data acquisition) in electrical engineering and power system automation applications. Part 5 provides a communication profile for sending basic telecontrol messages between two systems, which uses permanent directly connected data circuits between the systems. The IEC Technical Committee 57 (Working Group 03) have developed a protocol standard for telecontrol, teleprotection, and associated telecommunications for electric power systems. The result of this work is IEC 60870-5. Five documents specify the base IEC 60870-5:\n\nThe IEC Technical Committee 57 has also generated companion standards:\n\nIEC 60870-5-101/102/103/104 are companion standards generated for basic telecontrol tasks, transmission of integrated totals, data exchange from protection equipment & network access of IEC101 respectively.\n\nIEC 60870-5-101 [IEC101] is a standard for power system monitoring, control & associated communications for telecontrol, teleprotection, and associated telecommunications for electric power systems. This is completely compatible with IEC 60870-5-1 to IEC 60870-5-5 standards and uses standard asynchronous serial tele-control channel interface between DTE and DCE. The standard is suitable for multiple configurations like point-to-point, star, mutidropped etc.\n\n\nCharacter format of IEC 101 uses 1 start bit, 1 stop bit, 1 parity bit & 8 data bits. FT1.2 (defined in IEC 60870-5-1) is used for frame format of IEC 101 which is suitable for asynchronous communication with hamming distance of 4. This uses 3 types of frame formats -\n\"Frame with variable length ASDU\", \"Frame with fixed length\" & \"single character\". Single character is used for acknowledgments, fixed length frames are used for commands & variable lengths are used for sending data. The details of variable length frame is given below\n\n\nIEC 60870-5-103 [IEC103] is a standard for power system control and associated communications. It defines a companion standard that enables interoperability between protection equipment and devices of a control system in a substation. The device complying with this standard can send the information using two methods for data transfer - either using the explicitly specified application service data units (ASDU) or using generic services for transmission of all the possible information. The standard supports some specific protection functions and provides the vendor a facility to incorporate its own protective functions on private data ranges.\n\nIEC 103 uses FT1.2 (defined in IEC 60870-5-1) for frame format having options of \"Frame with variable length\", \"Frame with fixed length\" & \"single character\" similar to IEC 101. Single character is used for acknowledgments, fixed length frames are used for commands & variable lengths are used for sending data. However the frame format of IEC 103 differs from IEC 101 in information object address which is split into function type (ftype) and information number (inumber) in IEC 103. Also IEC 103 can have only single information object in a frame whereas IEC 101 can have multiple information objects. Many of the field sizes are also restricted in IEC 103. The details of variable length frame is given below\n\n\nIEC 60870-5-104 (IEC 104) protocol is an extension of IEC 101 protocol with the changes in transport, network, link & physical layer services to suit the complete network access. The standard uses an open TCP/IP interface to network to have connectivity to the LAN (Local Area Network) and routers with different facility (ISDN, X.25, Frame relay etc.) can be used to connect to the WAN (Wide Area Network). Application layer of IEC 104 is preserved same as that of IEC 101 with some of the data types and facilities not used. There are two separate link layers defined in the standard, which is suitable for data transfer over Ethernet & serial line (PPP - Point-to-Point Protocol). The control field data of IEC104 contains various types of mechanisms for effective handling of network data synchronization.\n\nThe security of IEC 104, by design has been proven to be problematic, as many of the other SCADA protocols developed around the same time. Though the IEC technical committee (TC) 57 have published a security standard IEC 62351, which implements end-to-end encryption which would prevent such attacks as replay, man-in-the-middle and packet injection. Unfortunately due to the increase in complexity vendors are reluctant to roll this out on their networks.\n\n"}
{"id": "8413554", "url": "https://en.wikipedia.org/wiki?curid=8413554", "title": "Indigenous Australian seasons", "text": "Indigenous Australian seasons\n\nIndigenous Australians have distinct ways of dividing the year up. Naming and understanding of seasons differed between groups, and depending on where in Australia the group lives. Below are a few examples of different groups and their seasons.\n\nThe Yolngu, Indigenous Australians of North-East Arnhem Land, identify six seasons. Non-Indigenous people currently living in the Top End identify two— the Wet and the Dry. (Arguably, the \"build-up\" period between dry and wet is coming to be identified as a distinct third season.) The six Yolngu seasons, and their characteristics, are:\n\nThe Anangu Pitjantjatjara of northern South Australia and the southern part of the Northern Territory, live in Central Australia. Where non-Indigenous people name four seasons here, they name more. Examples of some of their seasons include.\n\nNoongar season doesn't follow a rigid cycle with timing dependent on subtle changes in the weather with wind, rain and temperature. The cycles are part of katitjin-bidi or knowledge trails that lead groups to reliable sources of food and water.\n\nSource: Swan River System, Landscape Description (Report No 27/28 1997), 6. Resource Inventory, 6.2 Cultural Context pp41–42 Lisa Chalmers (Waterways Management Planning, Water and Rivers Commission), for the Swan River Trust. The section references \"Hunters And Gatherers\", \"Landscope Volume 8\", 1, 31–35, (P. Bindon & T. Walley, 1993) and \"Broken Spears: Aboriginals and Europeans in the South West of Australia\", Perth: \"Focus\" (N. Green, 1984). Portal page for the entire report. Retrieved 9 June 2007.\n\n\"See also\": Noongar seasons\n\n\n"}
{"id": "46633984", "url": "https://en.wikipedia.org/wiki?curid=46633984", "title": "International Center on Small Hydro Power", "text": "International Center on Small Hydro Power\n\nThe International Center on Small Hydro Power (ICSHP) is a non-profit institution operating under the auspices of the United Nations Industrial Development Organization (UNIDO) to promote the development of small hydro power. The ICSHP is also the headquarters of the International Network on Small Hydro Power (INSHP).\n\nThe ICSHP was formed in 1994 by the United Nations Development Program (UNDP) and the United Nations Industrial Development Organization (UNIDO) in co-operation with the Chinese Ministry of Water Resources and the Chinese Ministry of Commerce. Following approval from the State Council of the People's Republic of China in 1999 the ICSHP became the first international institution to be established in China. The following year an official agreement between the Chinese government brought the ICSHP directly under the auspices of UNIDO. Its establishment has been described as 'one of the major achievements of China's reform and a symbol of world SHP demand, which reflects the international status of China's SHP development'\n\nThe ICSHP was established 'to promote small hydropower development worldwide'. It has a specific focus on developing countries, south-south co-operation and the promotion of small hydro power for 'the social, economic and environmental development of rural areas'. The organisation provides training, advice, research and information exchange on small hydro power development and management. The mission of the ICSHP is also in keeping with UNIDO's focus on promoting South-South Co-operation, 'by encouraging more horizontal investment flows'.\n\nIn 2013 the ICSHP and UNIDO published the World Small Hydro Power Development Report. According to Li Yong, Director-General of UNIDO, the report was \"a world first compilation of global small hydropower data\". The report assessed small hydro power development in 149 countries across 20 regions from over 60 different authors. It found that while installed small hydro power was estimated at 75 GW, potential small hydro power was approximately 173 GW. Over 50% of the world's potential small hydro power was found to be in Asia however the report noted \"It is possible in the future that more small hydropower potential might be identified both on the African and American continents\". In 2015 it was announced that a second, updated edition of the report would be published in 2016.\n\nThe ICSHP is based in Hangzhou, China with additional bases in Hunan and Gansu provinces and sub-bases in India, Nigeria and Colombia\n\n"}
{"id": "23158223", "url": "https://en.wikipedia.org/wiki?curid=23158223", "title": "Jack Collom", "text": "Jack Collom\n\nJohn Aldridge \"Jack\" Collom (November 8, 1931 – July 2, 2017) was an American poet, essayist, and creative writing pedagogue. Included among the twenty-five books he published during his lifetime were \"Red Car Goes By: Selected Poems 1955–2000\"; \"Poetry Everywhere: Teaching Poetry Writing in School and in the Community\"; and \"Second Nature\", which won the 2013 Colorado Book Award for Poetry. In the fields of education and pedagogy, he was involved in eco-literature, ecopoetics, and creative writing instruction for children.\n\nJack Collom was born John Aldridge Collom in Chicago on November 8, 1931. He grew up in the small town of Western Springs, Illinois, spent much of his time birdwatching, and over the years became an inveterate bird-watcher. Collom moved to Fraser, Colorado, in 1947. He studied Forestry at Colorado A&M College where he earned a B.S. in 1952. Afterwards, he spent four years in the U.S. Air Force, and he started writing poetry in 1955 while stationed in Tripoli, Libya. His unit was next stationed at Neubiberg, a base just south of Munich, in Bavaria. It is there he met his first wife (a native German), in 1956. After his discharge from the military, he moved back to the US after a brief time living in Germany, and worked in factories for twenty years while writing poetry.\n\nHe received his B.A. in English (1972) and M.A. in English literature (1974) from the University of Colorado, where he had studied on the G.I. Bill. In 1974, he began teaching in the \"Poetry-in-the-Schools\" programs in Colorado, Wyoming, and Nebraska. In 1980, he began teaching poetry in the public schools of New York City, by way of the \"Poets In Public Service\" and \"Teachers & Writers\" programs. Collom continued to teach creative writing to children for the next 35 years, in both elementary and secondary schools, where he developed a pedagogy for this type of educational approach.\n\nSubsequently, Teachers & Writers Collaborative published three books of Collom's essays and commentary on this experience (which included the young students' poems), notably \"Poetry Everywhere\" and \"Moving Windows\".\n\nFrom 1966 to 1977, he published the work of many writers in a little magazine called \"The\". He was twice awarded Poetry Fellowships from the National Endowment for the Arts, and received a Foundation for Contemporary Arts Grants to Artists award (2012). From 1986 until his death in 2017, Collom taught at Naropa University's Jack Kerouac School of Disembodied Poetics as an adjunct professor, where he shaped Writing Outreach, a community creative-writing project, into a course. In 1989, he pioneered Eco-Lit, one of the first ecology literature courses ever offered in the United States. Some of his accomplishments as an environmentalist-poet are documented in \"American Environmental Leaders: From Colonial Times to the Present\". His nature writings and essays about the environment were published in various venues, including \"ecopoetics\", \"The Alphabet of Trees: A Guide to Writing Nature Poetry\", and \"ISLE\", the journal of Interdisciplinary Studies in Literature and the Environment.\n\nHe read and taught throughout the United States, in Mexico, Costa Rica, Austria, Belgium, and Germany. In 2008, he was the plenary speaker at the \"Poetic Ecologies\" conference at the Université Libre de Bruxelles. In 2009, he led a three-week Creativity and Aging Program at Woodland Pattern in Milwaukee, Wisconsin.\n\nHe worked with numerous dancers, visual artists and musician/composers, and recorded three CDs: \"Calluses of Poetry\" and \"Colors Born of Shadow\", with Ken Bernstein, and \"Blue Yodel Blue Heron\", with Dan Hankin and Sierra Collom.\n\nIn 2001, his adopted hometown of Boulder, Colorado, declared and celebrated a \"Jack Collom Day\".\n\nCollom was married three times. He had three sons by his first marriage: Nathaniel, Christopher, and Franz. He had a daughter, Sierra, through a second marriage.\n\nJack Collom died in Boulder, Colorado on July 2, 2017. He is survived by his wife, Jennifer Heath, his four grown children, and a grandson.\n\n\n\n"}
{"id": "17379", "url": "https://en.wikipedia.org/wiki?curid=17379", "title": "Kami", "text": "Kami\n\nIn Shinto, kami are not separate from nature, but are of nature, possessing positive and negative, and good and evil characteristics. They are manifestations of , the interconnecting energy of the universe, and are considered exemplary of what humanity should strive towards. Kami are believed to be \"hidden\" from this world, and inhabit a complementary existence that mirrors our own: . To be in harmony with the awe-inspiring aspects of nature is to be conscious of .\n\nThough the word kami is translated multiple ways into English, no one English word expresses its full meaning. The ambiguity of the meaning of kami is necessary, as it conveys the ambiguous nature of kami themselves.\n\n\"Kami\" is the Japanese word for a god, deity, divinity, or spirit. It has been used to describe mind (心霊), God (ゴッド), supreme being (至上者), one of the Shinto deities, an effigy, a principle, and anything that is worshipped.\n\nAlthough \"deity\" is the common interpretation of \"kami\", some Shinto scholars argue that such a translation can cause a misunderstanding of the term. The wide variety of usage of the word \"kami\" can be compared to the Sanskrit \"Deva\" and the Hebrew \"Elohim\", which also refer to God, gods, angels, or spirits.\n\nSome etymological suggestions are:\n\nBecause Japanese does not normally distinguish grammatical number in nouns (the singular and plural forms of nouns in Japanese are the same), it is sometimes unclear whether \"kami\" refers to a single or multiple entities. When a singular concept is needed, or is used as a suffix. The reduplicated term generally used to refer to multiple kami is kamigami.\n\nGender is also not implied in the word \"kami\", and as such it can be used to refer to either male or female. The word , the use of female kami is a fairly new tradition.\n\nWhile Shinto has no founder, no overarching doctrine, and no religious texts, the \"Kojiki\" (the Ancient Chronicles of Japan), written in 712 CE, and the \"Nihonshoki \"(Chronicles of Japan), written in 720 CE, contain the earliest record of Japanese creation myths. The \"Kojiki\" also includes descriptions of various kami.\n\nIn the ancient traditions there were five defining characteristics of kami:\nKami are an ever-changing concept, but their presence in Japanese life has remained constant. The kami’s earliest roles were as earth-based spirits, assisting the early hunter-gatherer groups in their daily lives. They were worshipped as gods of earth (mountains) and sea. As the cultivation of rice became increasingly important and predominant in Japan, the kami’s identity shifted to more sustaining roles that were directly involved in the growth of crops; roles such as rain, earth, and rice. This relationship between early Japanese people and the kami was manifested in rituals and ceremonies meant to entreat the kami to grow and protect the harvest. These rituals also became a symbol of power and strength for the early emperors. See Niiname-sai (i.e., 新嘗祭, which also reads as Shinjō-sai).\n\nThere is a strong tradition of myth-histories in the Shinto faith; one such myth details the appearance of the first emperor, grandson of the Sun Goddess Amaterasu. In this myth, when Amaterasu sent her grandson to earth to rule, she gave him five rice grains, which had been grown in the fields of heaven (Takamagahara). This rice made it possible for him to transform the \"wilderness\".\n\nSocial and political strife have played a key role in the development of new sorts of kami, specifically the goryo-shin (the sacred spirit kami). The goryo are the vengeful spirits of the dead whose lives were cut short, but they were calmed by the devotion of Shinto followers and are now believed to punish those who do not honor the kami.\n\nThe pantheon of kami, like the kami themselves, is forever changing in definition and scope. As the needs of the people have shifted, so too have the domains and roles of the various kami. Some examples of this are related to health, such as the kami of smallpox whose role was expanded to include all contagious diseases, or the kami of boils and growths who has also come to preside over cancers and cancer treatments.\n\nIn the ancient animistic religions, kami were understood as simply the divine forces of nature. Worshippers in ancient Japan revered creations of nature which exhibited a particular beauty and power such as waterfalls, mountains, boulders, animals, trees, grasses, and even rice paddies. They strongly believed the spirits or resident kami deserved respect.\n\nIn 927 CE, the \"Engi-shiki\" (延喜式, literally, Procedures of the Engi Era) was promulgated in fifty volumes. This, the first formal codification of Shinto rites and \"norito\" (liturgies and prayers) to survive, became the basis for all subsequent Shinto liturgical practice and efforts. It listed all of the 2,861 Shinto shrines existing at the time, and the 3,131 official-recognized and enshrined kami. The number of kami has grown and far exceeded this figure through the following generations as there are over 2,446,000 individual kami enshrined in Tokyo's Yasukuni Shrine alone.\n\nKami are the central objects of worship for the Shinto faith. The ancient animistic spirituality of Japan was the beginning of modern Shinto, which became a formal spiritual institution later, in an effort to preserve the traditional beliefs from the encroachment of imported religious ideas. As a result, the nature of what can be called kami is very general and encompasses many different concepts and phenomena.\n\nSome of the objects or phenomena designated as kami are qualities of growth, fertility, and production; natural phenomena like wind and thunder; natural objects like the sun, mountains, rivers, trees, and rocks; some animals; and ancestral spirits. Included within the designation of ancestral spirits are spirits of the ancestors of the Imperial House of Japan, but also ancestors of noble families as well as the spirits of the ancestors of all people, which when they died were believed to be the guardians of their descendants.\n\nThere are other spirits designated as kami as well. For example, the guardian spirits of the land, occupations, and skills; spirits of Japanese heroes, men of outstanding deeds or virtues, and those who have contributed to civilization, culture, and human welfare; those who have died for the state or the community; and the pitiable dead. Not only spirits superior to man can be considered kami; spirits that are considered pitiable or weak have also been considered kami in Shinto.\n\nThe concept of kami has been changed and refined since ancient times, although anything that was considered to be kami by ancient people will still be considered kami in modern Shinto. Even within modern Shinto, there are no clearly defined criteria for what should or should not be worshipped as kami. The difference between modern Shinto and the ancient animistic religions is mainly a refinement of the kami-concept, rather than a difference in definitions.\n\nAlthough the ancient designations are still adhered to, in modern Shinto many priests also consider kami to be anthropomorphic spirits, with nobility and authority. One such example is the mythological figure Amaterasu-ōmikami, the sun goddess of the Shinto pantheon. Although these kami can be considered deities, they are not necessarily considered omnipotent or omniscient, and like the Greek Gods, they had flawed personalities and were quite capable of ignoble acts. In the myths of Amaterasu, for example, she could see the events of the human world, but had to use divination rituals to see the future.\n\nThere are considered to be three main variations of kami: amatsu-kami (the heavenly deities), kunitsu-kami (the gods of the earthly realm), and . (\"八百万\" literally means eight million, but idiomatically it expresses \"uncountably many\" and \"all around\"—like many East Asian cultures, the Japanese often use the number 8, representing the cardinal and ordinal directions, to symbolize ubiquity.) These classifications of kami are not considered strictly divided, due to the fluid and shifting nature of kami, but are instead held as guidelines for grouping them.\n\nThe ancestors of a particular family can also be worshipped as kami. In this sense, these kami are worshipped not because of their godly powers, but because of a distinctive quality or virtue. These kami are celebrated regionally, and several miniature shrines (\"hokora\") have been built in their honor. In many cases, people who once lived are thus revered; an example of this is Tenjin, who was Sugawara no Michizane (845-903 CE) in life.\n\nWithin Shinto it is believed that the nature of life is sacred, because the kami began human life. Yet people cannot perceive this divine nature, which the kami created, on their own; therefore, magokoro, or purification, is necessary in order to see the divine nature. This purification can only be granted by the kami. In order to please the kami and earn magokoro, Shinto followers are taught to uphold the four affirmations of Shinto.\n\nThe first affirmation is to hold fast to tradition and the family. Family is seen as the main mechanism by which traditions are preserved. For instance, in marriage or birth, tradition is potentially observed and passed onto future generations. The second affirmation is to have a love of nature. Nature objects are worshipped as sacred, because the kami inhabit them. Therefore, to be in contact with nature means to be in contact with the gods. The third affirmation is to maintain physical cleanliness. Followers of Shinto take baths, wash their hands, and rinse out their mouths often. The last affirmation is to practice matsuri, which is the worship and honor given to the kami and ancestral spirits.\n\nShinto followers also believe that the kami are the ones who can either grant blessings or curses to a person. Shinto believers desire to appease the evil kami to \"stay on their good side\", and also to please the good kami. In addition to practicing the four affirmations daily, Shinto believers also wear \"omamori\" to aid them in remaining pure and protected. Mamori are charms that keep the evil kami from striking a human with sickness or causing disaster to befall them.\n\nThe kami are both worshipped and respected within the religion of Shinto. The goal of life to Shinto believers is to obtain \"magokoro\", a pure sincere heart, which can only be granted by the kami. As a result, Shinto followers are taught that humankind should venerate both the living and the nonliving, because both possess a divine superior spirit within: the kami.\n\nOne of the first recorded rituals we know of is Niiname-sai, the ceremony in which the Emperor offers newly harvested rice to the kami to secure their blessing for a bountiful harvest. A yearly festival, Niiname-sai is also performed when a new Emperor comes to power, in which case it is called Onamesai. In the ceremony the Emperor offers crops from the new harvest to the kami, including rice, fish, fruits, soup, and stew. The Emperor first feasts with the deities, then the guests. The feast could go on for some time; for example, the Showa Emperor's feast spanned two days.\n\nVisitors to a Shinto shrine follow a purification ritual before presenting themselves to the kami. This ritual begins with hand washing, and swallowing and later spitting a small amount of water in front of the shrine to purify the body, heart, and mind. Once this is complete they turn their focus to gaining the kami’s attention. The traditional method of doing this is to bow twice, clap twice and bow again, alerting the kami to their presence and desire to commune with them. During the last bow, the supplicant offers words of gratitude and praise to the kami; if they are offering a prayer for aid they will also state their name and address. After the prayer and/or worship they repeat the two bows, two claps and a final bow in conclusion.\n\nShinto practitioners also worship at home. This is done at a \"kamidana\" (household shrine), on which an \"ofuda\" (kami name card or charm card) with the name of their protector or ancestral kami is positioned. Their protector kami is determined by their or their ancestors’ relationship to the kami.\n\nAscetic practices, shrine rituals and ceremonies, and Japanese festivals are the most public ways that Shinto devotees celebrate and offer adoration for the kami. Kami are celebrated during their distinct festivals that usually take place at the shrines dedicated to their worship. Many festivals involve believers, who are usually intoxicated, parading, sometimes running, toward the shrine while carrying mikoshi (portable shrines) as the community gathers for the festival ceremony. Yamamoto Guji, the high priest at the Tsubaki Grand Shrine, explains that this practice honors the kami because \"it is in the festival, the matsuri, the greatest celebration of life can be seen in the world of Shinto and it is the people of the community who attend festivals as groups, as a whole village who are seeking to unlock the human potential as children of kami.\" During the New Year Festival, families purify and clean their houses in preparation for the upcoming year. Offerings are also made to the ancestors so that they will bless the family in the future year.\n\nShinto ceremonies are so long and complex that in some shrines it can take ten years for the priests to learn them. The priesthood was traditionally hereditary. Some shrines have drawn their priests from the same families for over a hundred generations. It is not uncommon for the clergy to be female priestesses. The priests may be assisted by \"miko\", young unmarried women acting as shrine maidens. Neither priests nor priestesses live as ascetics; in fact, it is common for them to be married, and they are not traditionally expected to meditate. Rather, they are considered specialists in the arts of maintaining the connection between the kami and the people.\n\nIn addition to these festivals, ceremonies marking rites of passage are also performed within the shrines. Two such ceremonies are the birth of a child and the Shichi-Go-San. When a child is born they are brought to a shrine so that they can be initiated as a new believer and the kami can bless them and their future life. The Shichi-Go-San (the Seven-Five-Three) is a rite of passage for five-year-old boys and three- or seven-year-old girls. It is a time for these young children to personally offer thanks for the kami’s protection and to pray for continued health.\n\nMany other rites of passage are practiced by Shinto believers, and there are also many other festivals. The main reason for these ceremonies is so that Shinto followers can appease the kami in order to reach magokoro. Magokoro can only be received through the kami. Ceremonies and festivals are long and complex because they need to be perfect to satisfy the kami. If the kami are not pleased with these ceremonies, they will not grant a Shinto believer magokoro.\n\n\n\n"}
{"id": "34308412", "url": "https://en.wikipedia.org/wiki?curid=34308412", "title": "Laurentaeglyphea", "text": "Laurentaeglyphea\n\nLaurentaeglyphea neocaledonica is a species of glypheoid lobster, and the only species in the genus Laurentaeglyphea. It is known from a single specimen collected on a guyot in the Coral Sea between Australia and New Caledonia. It is thought to be an active predator with colour vision, unlike its nearest living relative, \"Neoglyphea inopinata\".\n\n\"Laurentaeglyphea\" is known from a single adult female specimen, with a carapace in size. In life, the animal is whitish and marked with red patches, especially on the abdomen and the distal segments of the first pereiopods; the markings are much fainter on the carapace.\n\n\"Laurantaeglyphea\" has large reniform (kidney-shaped) eyes, more developed in the lower half than the upper. The epistome, behind the two pairs of antennae on the ventral side, is large, but considerably shorter than that of \"Neoglyphea\". \"Laurantaeglyphea\" has five pairs of pereiopods, all without true chelae (claws).\n\nThe single known specimen of \"Laurentaeglyphea\" was collected at a depth of on Banc Capel (Chesterfield Plateau; ) in the Coral Sea.\n\nThe ecology of \"Laurentaeglyphea\" is very different from that of its closest living relative, \"Neoglyphea inopinata\". Banc Capel is a guyot – a former atoll with steep sides and a flat top – and is swept by strong currents. There are no sandy or muddy substrates, the surface being occupied by rocks or gravel scree. It is dominated by sponges, including the genus \"Phloedictyon\" and gorgonians. Other decapods found in the same trawls including the slipper lobster \"Ibacus brucei\", the crab \"Randallia\" and swimming crabs.\n\nOn the basis of its large eyes, \"Laurentaeglyphea\" is thought to be an active predator, perhaps one with similar hunting behaviour to that of stomatopods. The presence of patterned pigmentation on an animal that lives at a depth of around suggests that it does not live in a burrow. In the clear waters of the Coral Sea, sufficient light penetrates to these depths for a wide range of colours to be represented among the fauna. The eyes of \"Laurentaeglyphea\" are thought to be adapted to colour vision, even if it is biased towards the shorter wavelengths (blues and greens).\n\nThe collected specimen of \"Laurentaeglyphea\" was observed to be very active and aggressive, using its semichelate first pereiopods to attack.\n\n\"Laurentaeglyphea neocaledonica\" was originally described by Bertrand Richer de Forges in 2006, on the basis of a single specimen (the holotype). He named the species \"Neoglyphea neocaledonica\", where the specific epithet \"\" refers to New Caledonia, the nearest land to the site where the holotype was collected. Later that year, Jacques Forest erected the new genus \"Laurentaeglyphea\" for the new species, separating it from \"Neoglyphea inopinata\", the only other species in the genus \"Neoglyphea\". The genus name \"Laurentaeglyphea\" commemorates Michèle de Saint Laurent, who had discovered and co-described the first Recent specimen of the infraorder Glypheidea.\n\nThe two species of living glypheids are considered \"living fossils\".\n"}
{"id": "12994448", "url": "https://en.wikipedia.org/wiki?curid=12994448", "title": "List of Pyrenean three-thousanders", "text": "List of Pyrenean three-thousanders\n\nThis list contains all of the Pyrenean three-thousanders, namely the 129 mountain summits of or more above sea level in the Pyrenees, a range of mountains in southwest Europe that forms a natural border between France and Spain. The Pyrenees separate the Iberian Peninsula from the rest of continental Europe, and extend for about from the Bay of Biscay (Cap Higuer) to the Mediterranean Sea (Cap de Creus). The highest mountain in the Pyrenees is Aneto in Spain at .\n\nThe summits meeting the 3,000-metre criterion were defined by a UIAA-sponsored joint Franco-Spanish team led by Juan Buyse. The UIAA list, published in 1990, also contains 83 secondary summits in addition to the 129 principal ones listed here, and divides the range into 11 zones. According to the latest surveys, three of the peaks in the original list are actually below 3000m but are still included below. \n\nThe selection criteria used here are quite broad – many of the peaks included are secondary summits of major mountains. Using prominence as a criterion, only one summit is an ultra-prominent peak, Aneto, a further three have a prominence of 1000m (Pico Posets, Pica d'Estats, Vignemale), and five more have a prominence of over 600m. Only 17 in total have a prominence of more than 300m, commonly used as a criterion for determining an independent mountain, and are indicated in bold in the table below. 28 more have a prominence of over 100m and can be considered significant summits.\n\nAll the peaks in this list are in Spain (59 peaks) or France (26 peaks), or delimit the border between the two countries (45). The two highest major mountains and their subsidiary summits (Aneto and Posets - Zone 7 and 9) are entirely in Spain, together with the Besiberri peaks (zone 10) while Pic Long and surrounding mountains (zone 5) are entirely in France. Most of the other mountains lie on or close to the border. The small country of Andorra is located in the eastern portion of the Pyrenees and is surrounded by Spain and France; its highest mountain – Coma Pedrosa at – falls below the 3,000-metre threshold. The mountains are listed by height within each of the 11 zones.\n\nFor the complete list see: Pyrenees#Highest summits\n\nZone 1 : Balaïtous-Enfer-Argualas\n\nZone 2 : Vignemale\n\nZone 3 : Monte Perdido\n\nZone 4 : La Munia\n\nZone 5 : Néouvielle-Pic Long\n\nZone 6 : Batoua-Batchimale\n\nZone 7 : Posets-Eristé\n\nZone 8 : Clarabide-Perdiguero-Boum\n\nZone 9 : Maladeta-Aneto\n\nZone 10 : Besiberris\n\nZone 11 : Estats-Montcalm\n\n\n"}
{"id": "5867937", "url": "https://en.wikipedia.org/wiki?curid=5867937", "title": "List of Sites of Special Scientific Interest in Somerset", "text": "List of Sites of Special Scientific Interest in Somerset\n\nThis is a list of the Sites of Special Scientific Interest (SSSIs) in Somerset, England, United Kingdom. In England the body responsible for designating SSSIs is Natural England, which chooses a site because of its fauna, flora, geological or physiographical features. There are 127 sites designated in this Area of Search, of which 83 have been designated due to their biological interest, 35 due to their geological interest, and 9 for both.\n\nNatural England took over the role of designating and managing SSSIs from English Nature in October 2006 when it was formed from the amalgamation of English Nature, parts of the Countryside Agency and the Rural Development Service. Natural England, like its predecessor, uses the 1974–1996 county system, and as such the same approach is followed here, therefore some sites you may expect to find in this list could be in the Avon list. The data in the table is taken from English Nature in the form of citation sheets for each SSSI.\n\nFor other counties, see List of SSSIs by Area of Search.\n"}
{"id": "15561956", "url": "https://en.wikipedia.org/wiki?curid=15561956", "title": "List of ecoregions in Tunisia", "text": "List of ecoregions in Tunisia\n\nThe following is a list of ecoregions in Tunisia, according to the Worldwide Fund for Nature (WWF).\n\n\n\n\n\n\n\n"}
{"id": "40362626", "url": "https://en.wikipedia.org/wiki?curid=40362626", "title": "List of national parks of Bangladesh", "text": "List of national parks of Bangladesh\n\nThis is a list of national parks in Bangladesh.\n"}
{"id": "8246967", "url": "https://en.wikipedia.org/wiki?curid=8246967", "title": "List of publicised titan arum blooms in cultivation", "text": "List of publicised titan arum blooms in cultivation\n\nThis list of publicized titan arum blooms in cultivation is a partial listing of flowering events of the titan arum (\"Amorphophallus titanum\") in cultivation.\n\n"}
{"id": "25707472", "url": "https://en.wikipedia.org/wiki?curid=25707472", "title": "List of rivers of Burkina Faso", "text": "List of rivers of Burkina Faso\n\nThis is a list of streams and rivers in Burkina Faso. Burkina Faso has three main rivers — the Black Volta, the Red Volta and the White Volta. Areas near rivers are affected by tsetse flies and simulium flies. This list is arranged by drainage basin, with respective tributaries indented under each larger stream's name.\n\n\n"}
{"id": "25686721", "url": "https://en.wikipedia.org/wiki?curid=25686721", "title": "Lluçanès", "text": "Lluçanès\n\nLluçanès () is a natural region transitioning between the Plain of Vic and Berguedà, in the pre-Pyrenees. Although not an officially recognized comarca of Catalonia, it has a strong historical, natural and social personality.\n\nIt is made up of the following municipalities:\n\nLluçanès is a plateau of about 400 km, situated in the north-east of the \"depressió central\" of Catalonia\n\nThe major waterways of the comarca include the Ter and Llobregat rivers, into which the Riera de Merlès, Riera de Lluçanès and Gavarresa feed.\n\nLluçanès has a Mediterranean climate transitioning to continental. It has a median temperature of about 12 degrees Celsius, and receives an annual precipitation of between 600 and 900 liters.\n\nMost villages have their own infant and primary schools.\n\nThe high school responsible for the secondary education is Castell del Quer Institute\n\nIn the \"Report on the revision of Catalonia's territorial organisation model\" of 2000, known as the \"Roca Report\", commissioned by the Catalan government, it was recommended that Lluçanès should become a sub-comarca of Osona. However, following a local campaign for full comarca status, a non-binding referendum was held in the region in July 2015, in which a majority in each of the municipalities of Alpens, Lluçà, Olost, Oristà, Perafita, Prats del Lluçanès, Sant Martí d'Albars, and Sobremunt, all currently in Osona, voted to join a new full comarca, while the other five municipalities voted to remain in their existing comarcas. Several local and national officials expressed disappointment at the result which excludes regions strongly associated with Lluçanès from the proposed new comarca, and have called for discussions on ways to resolve the issue, including a repeat of the vote.\n\n\n"}
{"id": "30055561", "url": "https://en.wikipedia.org/wiki?curid=30055561", "title": "MetPetDB", "text": "MetPetDB\n\nMetPetDB is a relational database and repository for global geochemical data on and images collected from metamorphic rocks from the earth's crust. MetPetDB is designed and built by a global community of metamorphic petrologists in collaboration with computer scientists at Rensselaer Polytechnic Institute as part of the National Cyberinfrastructure Initiative and supported by the National Science Foundation. MetPetDB is unique in that it incorporates image data collected by a variety of techniques, e.g. photomicrographs, backscattered electron images (SEM), and X-ray maps collected by wavelength dispersive spectroscopy or energy dispersive spectroscopy.\n\nMetPetDB was built for the purpose of archiving published data and for storing new data for ready access to researchers and students in the petrologic community. This database facilitates the gathering of information for researchers beginning new projects and permits browsing and searching for data relating to anywhere on the globe. MetPetDB provides a platform for collaborative studies among researchers anywhere on the planet, serves as a portal for students beginning their studies of metamorphic geology, and acts as a repository of vast quantities of data being collected by researchers globally.\n\nThe basic structure of MetPetDB is based on a geologic sample and derivative subsamples. Geochemical data are linked to subsamples and the minerals within them, while image data can relate to samples or subsamples. MetPetDB is designed to store the distinct spatial/textural context of mineral analysis that is a crucial to petrologic interpretation. A web-based user interface allows a user to become members and download their search results. Approved members may become contributors and upload data to catalogue and share with the public. More information about the data model and the design of the database is available on the MetPetDB Support Wiki.\n\nThe database houses a wide range of information available for samples from all over the globe to be grouped into two categories: (a) observations and measurements (e.g. mineral data, images, chemical analyses), for which robust data models already exist, and (b) interpretative results (e.g. P-T conditions, crystallization ages, cooling rates, etc.), which are conclusions based on the observational data. Development of a robust data model for interpretative data is currently underway as of December 2010. The database system is beginning to incorporate a number of tools for data analysis and calculation that adds considerable power to the researcher.\n\nMetPetDB differs from other Geochemistry relational databases (e.g. GEOROC, NAVDAT, PetDB) in that it incorporates unpublished data in addition to data published in peer-reviewed journals. The vast majority of data collected by metamorphic geologists is not presented in publication, and therefore a forum for sharing this data with the public is an objective of MetPetDB. Contributors to MetPetDB also have the ability to store private data and create projects, or collections of private, public, and published data for sharing and organization. A comprehensive list of the publications and their published samples are located at MetPetDB Published Samples\n\n"}
{"id": "31352461", "url": "https://en.wikipedia.org/wiki?curid=31352461", "title": "Mobile source air pollution", "text": "Mobile source air pollution\n\nMobile source air pollution includes any air pollution emitted by motor vehicles, airplanes, locomotives, and other engines and equipment that can be moved from one location to another. Many of these pollutants contribute to environmental degradation and have negative effects on human health. To prevent unnecessary damage to human health and the environment, environmental regulatory agencies such as the U.S. Environmental Protection Agency have established policies to minimize air pollution from mobile sources. Similar agencies exist at the state level. Due to the large number of mobile sources of air pollution, and their ability to move from one location to another, mobile sources are regulated differently from stationary sources, such as power plants. Instead of monitoring individual emitters, such as an individual vehicle, mobile sources are often regulated more broadly through design and fuel standards. Examples of this include corporate average fuel economy standards and laws that ban leaded gasoline in the United States. The increase in the number of motor vehicles driven in the U.S. has made efforts to limit mobile source pollution challenging. As a result, there have been a number of different regulatory instruments implemented to reach the desired emissions goals.\n\nThere are a number of different mobile sources of air pollution, some contributing more to pollution than others. As mentioned previously, mobile sources are regulated differently from stationary sources due to the large number of sources and their ability to move from one location to another. Different mobile sources operate differently and generate different emission types and levels. The E.P.A. differentiates between mobile sources by classifying them as either on-road vehicles or non-road vehicles. On-road vehicles and non-road vehicles are often subject to different regulations.\n\n\n\nThere are a number of different pollutants that are emitted by mobile sources. Some make up a large portion of the total air concentration for that particular pollutant while others do not make up as much of the total air concentration.\n\n\n\nEPA has ten regional offices, each of which is responsible for the execution of programs within several states and territories. EPA's website provides a detailed list of state agencies which administer the environmental regulations at the state level. California is the only state which has its own regulatory agency, the California Air Resources Board (CARB). The other states are allowed to follow CARB or federal regulations.\n\nFederal, state, and local governments utilize a wide range of policy instruments to control pollution from mobile sources. On the federal level, many different agencies are responsible for regulating, or at least creating policies to limit, pollution from mobile sources. This is necessary given the broad range of objects that are considered “mobile sources,” from aircraft and off-road vehicles, to locomotives and on-road vehicles. The Federal Aviation Administration, for example, establishes standards to limit emissions from aircraft, whereas the U.S. Department of Transportation and Environmental Protection Agency administer various aspects of on-road vehicle fuel economy regulations. On the state level, mandatory vehicle emissions-testing programs are often required as part of the annual motor-vehicle registration process.\n\nMany governments throughout the world require manufacturers of particular products to attach information-related labels to their products. Common examples in the United States include food nutrition and ingredient labels for food products, Surgeon General labels on alcohol and tobacco products, and labels for common household pesticides. Like mobile sources of air pollution, there is a broad range of products that may require government labeling regulation, therefore numerous federal agencies oversee various label-related regulation programs. For example, the US Food and Drug Administration oversees food nutrition and ingredient label regulations, whereas the US Environmental Protection Agency sets specific standards for the labeling of pesticides.\n\nThe primary goal of labeling regulations is to provide consumers and other product users with important information about the product. Essentially, labeling policies are designed to correct the market failure of imperfect information. For consumers to make the best decisions when allocating scarce resources, such as income, detailed information about particular products may be required. In this sense, labels also help correct information asymmetries that often exist within many market transactions.\n\nIn the United States, all new cars and light-duty trucks are required to have labels that display specific fuel economy information. The US Environmental Protection Agency calculates the average fuel economy for each vehicle manufacturer, and provides the data to the National Highway Traffic Safety Administration (NHTSA), which administers and enforces the Corporate Average Fuel Economy (CAFE) program. The purpose of the program is (1) to reduce emissions by requiring vehicle manufacturers to meet minimum fuel economy levels, and (2) to provide consumers with fuel economy information before purchasing new vehicles.\n\nEPA and NHTSA are redesigning the labels to provide even more information to consumers. The new labels will, for the first time, provide information about each vehicle's greenhouse gas emissions, as required by the Energy Independence and Security Act of 2007. The agencies are proposing two different label designs and are seeking public comments about which labels will be most helpful to consumers. Consumers can submit comments about the two proposed label styles on EPA's website here and here.\n\nAnother common policy instrument used by governments to influence market behavior is taxation. In the case of mobile source air pollution, the United States government has established many different taxes to limit emissions from various mobile sources. Perhaps one of the most well known is the gas guzzler tax, established by the Energy Tax Act of 1978. The act set minimum fuel economy standards for all new cars sold in the United States.\n\nThe tax is levied against manufacturers of new \"cars\" that fail to meet the minimum fuel economy level of 22.5 miles per gallon. The tax does not apply to minivans, sport utility vehicles, or pick-up trucks, as these made up a small portion of the US fleet when the tax was established in 1978. Manufacturers pay a level of tax based upon the average fuel economy for each particular vehicle produced, ranging from $1,000 for vehicles achieving at least 21.5 but less than 22.5 MPG, to $7,000 for each vehicle achieving less than 12.5 MPG. Vehicles that achieve a minimum average fuel economy of 22.5 MPG are not subject to the gas guzzler tax.\n\nGovernments may also offer tax credits to encourage certain types of behavior within market economies. For example, if a government wants to encourage consumers to purchase more fuel-efficient vehicles, the government could offer tax credits to effectively lower the price of each vehicle. The logic of this approach is consistent with the laws of supply and demand, namely, that as the price of a good decreases, the quantity demanded of that good will increase. This is true given that other important factors, such as current levels of supply and demand, remain constant.\n\nThe US federal government currently utilizes numerous tax credits to reduce emissions from mobile sources. One of the more common tax credits is the \"Qualified Plug-In Electric Drive Motor Vehicle Tax Credit.\" This credit is available \"for the purchase of a new qualified plug-in electric drive motor vehicle that draws propulsion using a traction battery that has at least four kilowatt hours of capacity, uses an external source of energy to recharge the battery, has a gross vehicle weight rating of up to 14,000 pounds, and meets specified emission standards.\" The credit ranges from $2,500 to $7,000, depending upon the vehicle's weight rating. Consumers who purchase the new Chevrolet Volt are eligible for the full $7,500 credit. Another tax credit targeted at consumers is the \"Fuel Cell Motor Vehicle Tax Credit,\" which was originally set at $8,000 for the purchase of qualified light-duty fuel cell vehicles. On December 31, 2009, the tax credit was reduced to $4,000.\n\nTax credits to limit mobile source pollution can also be targeted at producers of particular products. For example, \"Advanced Biofuel Production Payments\" are available to \"eligible producers of advanced biofuels,\" or for fuels derived from \"renewable biomass other than corn kernel starch.\" Such producers \"may receive payments to support expanded production of advanced biofuels,\" dependent upon the \"quantity and duration of production by the eligible producer; the net nonrenewable energy content of the advanced biofuel, if sufficient data is available; the number of producers participating in the program; and the amount of funds available.\" While many critics have argued that biofuels can actually increase greenhouse gas emissions, research from the US Department of Energy indicates that biofuels \"burn cleaner than gasoline, resulting in fewer greenhouse gas emissions, and are fully biodegradable, unlike some fuel additives.\"\n\nOther important policy instruments that can be utilized by governments are voluntary programs. These programs bring together various stakeholders with the goal of achieving some particular policy outcome. The Department of Energy, for example, created the \"Clean Cities\" program to reduce petroleum use in the transportation sector. The Clean Cities program partners with more than 80 volunteer organizations throughout the United States, developing public-private partnerships that promote alternative fuels and advanced vehicles, fuel blends, fuel economy, hybrid vehicles, and idle reduction. The three primary goals of the program are \nThe program was initiated in 1993 and has saved nearly of petroleum since its inception.\n\nAnother example of a voluntary program is the Environmental Protection Agency's \"SmartWay Transport Partnership.\" This voluntary partnership between the EPA and the ground freight industry is designed to reduce greenhouse gases and air pollution through increased fuel efficiency programs. EPA provides partners with \"benefits and services that include fleet management tools, technical support, information, public recognition, and use of the SmartWay Transport Partner logo.\"\n\n\"Clean Construction USA\" is an additional voluntary program administered by EPA that promotes the reduction of diesel exhaust emissions from construction equipment and other construction vehicles. The program encourages proper operations and maintenance, the use of emission-reducing technologies, and the use of cleaner fuels.\n\nSubsidies are another powerful policy tool used by governments to influence economic behavior. Subsidies can take many forms, ranging from tax credits to direct cash payments. To limit mobile source pollution from airports, for example, the Federal Aviation Administration's \"Voluntary Airport Low Emission Program\" provides funding to U.S. commercial service airports located in air quality non attainment and maintenance areas. While the funding can be used to reduce emissions from both mobile and stationary sources at the airport, much of the program's emphasis is on mobile source emission reduction. The program promotes the use of electric ground support equipment, such as electric bag tugs that take luggage from the airplane to the baggage claim. Other airport equipment that can be electronically operated include various types of belt loaders, along with the pushback tractors that assist airplanes when departing from the gate.\n\nAnother important goal of the program is to install underground fuel hydrants at airports. These would eliminate the need for fuel trucks, an important source of mobile emissions. The Voluntary Airport Low Emission Program was established under the Vision 100 Century of Aviation Reauthorization Act of 2003.\n\nNumerous states have emissions-testing programs to limit pollution from on-road vehicles, such as cars and light-duty trucks. Each of these vehicles must meet specific emissions targets before being allowed to obtain or renew vehicle registrations. Many of these programs are administered on the local and county level. For example, the Clean Air Car Check is a vehicle emissions-testing program for all vehicles registered in Lake and Porter counties in Indiana. The two counties were designated as non-attainment areas for ozone levels in 1977 by the Environmental Protection Agency. By 1990, the two counties were reclassified as severe non-attainment areas, a designation which requires states to create State Implementation Plans to attain and maintain certain air pollution standards. Although the counties were again reclassified in 2010, this time as attainment areas, the two counties will maintain their vehicle inspection and maintenance program because it is a \"key piece of Indiana's plan to prevent backsliding so that the area can remain in attainment.\"\n\nAccording to the Corporate Average Fuel Economy standard (CAFE) regulation, which was enacted in 1975, every seller of automobiles in the US had to achieve by 1985 a minimum sales-weighted average fuel efficiency of 27.5 miles per gallon (MPG). This standard had to be achieved for domestically produced and imported cars separately. Failure to meet the prescribed standard incurred a penalty of $5 per car per 1/10 of a gallon that the corporate average fuel economy fell below the standard. The first idea about the environmental impact of the CAFE regulation can be obtained by examining its effects on the average fuel efficiency of domestic and foreign firms; these effects are largest for the domestic production of US manufacturers, whose corporate average fuel efficiency would be lower by 1.2 MPG in the absence of CAFE standards. CAFE standards also lead to approximately fuel consumption savings per year. Contrary to the CAFE standards, gasoline taxes affect not only new but also used cars, so that there is no reason to expect any substitution towards less fuel efficient used cars when taxes are raised. Small tax increases are insufficient to induce fuel cost savings of the same order of magnitude as CAFE.\n\nLead was originally added to fuel as an additive to prevent engine knocking. In the 1970s, virtually all gasoline used in the United States contained lead with an average concentration of almost 2.4 grams per gallon. By the mid 1970s, the EPA began formulating plans to phase lead out of fuel for two main reasons. There was growing concern over lead's potential effects on human health, especially with respected hypertension and cognitive development in children. Additionally, the introduction of the catalytic converter in new automobiles manufactured after 1975 required an adjustment to the fuel standards. Catalytic converters were utilized in new automobiles to help meet the hydrocarbon, carbon monoxide, and nitrogen oxide emission standards mandated by the 1970 Clean Air Act. Unfortunately, the catalytic converters could only function properly with unleaded fuel.\n\nIn order to protect human health and ensure that catalytic converters were operating properly, the EPA required that the average lead content of all gasoline sold be reduced from 1.7 grams per gallon after January 1, 1975 to 0.5 grams per gallon by January 1, 1979. Eventually, the EPA lowered the average lead concentration standard goal to 0.1 gm/gal by January 1, 1986. The EPA defined \"averages\" in a way that allowed refiners who owned more than one refinery to average or \"trade\" among refineries to satisfy their lead limits each quarter. Taking note of the trading that was taking place, the EPA permitted refiners to bank credits for use until the end of 1987. EPA enforcement relied on reporting requirements and random testing of gasoline samples.\n\nThe EPA has officially concluded its effort to phase out lead in fuel. As of 1996, manufacturers are no longer required to place \"unleaded fuel only\" labels on the dashboard and on or around the fuel filler inlet area of each new motor vehicle. Additionally, several record keeping and reporting requirements for gasoline refiners and importers have been lifted. Critics have viewed the lead credit trading program as a successful implementation of a cap and trade system allowing for the gradual reduction of a pollutant. Lead credit trading as a percentage of lead use rose above 40 percent by 1987. An estimated 20 percent of refineries participated in trading early in the program, eventually rising to 60 percent of refineries.\n\nIn 2007, the Mobile Source Air Toxics Rule was created to help limit the hazardous emissions generated as a result of fuel combustion in mobile sources. Benzene is one particular component of gasoline that is known to pose a hazard to human health. In 2007, benzene concentrations in gasoline averaged 1% by volume. The EPA mandated refiners and importers to begin producing gasoline with annual an average benzene content no greater than 0.62% beginning in 2011. The EPA has listed certain technologies that can be utilized in order to achieve the new standards, but refiners can petition the EPA to approve additional technologies.\n\nRefiners and importers could earn credits by reducing benzene levels below 0.62% before 2011. These credits could be auctioned to other companies, essentially creating a marketable allowance approach for reducing benzene content in gasoline. The nationwide banking and trading system does nave some limitations. No individual refiner or importer could produce gasoline with benzene concentrations exceeding 1.3% by volume, even with credits.\n\nThe final rule can be found at the EPA's website. Control of Hazardous Air Pollutants From Mobile Sources:Early Credit Technology Requirement Revision\n\n"}
{"id": "14071017", "url": "https://en.wikipedia.org/wiki?curid=14071017", "title": "Nabak-kimchi", "text": "Nabak-kimchi\n\nNabak-kimchi () is a watery kimchi, similar to \"dongchimi\", in Korean cuisine. It is made of thinly sliced Korean radish and napa cabbage (called \"baechu\", hangul 배추, in Korean) into a rectangular shape as main ingredients and salted them with mixed vegetables and spices such as cucumber, scallion, water dropwort (called \"minari\", 미나리 in Korean), garlic, ginger, red chilies, chili pepper powder, sugar, salt, and water.\n\n\"Nabak gimchi\" looks similar to \"dongchimi\" in form but is commonly consumed during spring and summer, whereas \"dongchimi\" is most commonly eaten in winter. Besides, chili pepper powders is added to make \"nabak kimchi\" and makes the kimchi color a rose pink unlike the white colored \"dongchimi\".\n\nThe term \"nabak\" originated from \"nabaknabak\" (hangul 나박나박) which is an adverb in Korean language and means \"making flattened or slicing thinly\".\n\n\n"}
{"id": "2391854", "url": "https://en.wikipedia.org/wiki?curid=2391854", "title": "Naomi James", "text": "Naomi James\n\nDame Naomi Christine James, DBE (née Power; born 2 March 1949) is the first woman to have sailed single-handed (i.e., solo) around the world via Cape Horn, the second woman to have ever sailed solo around the world. She departed Dartmouth, Devon on 9 September 1977 and finished her voyage around the globe on 8 June 1978 after 272 days, thus improving Sir Francis Chichester's solo round-the-world sailing record by two days.\n\nShe was born in New Zealand on a landlocked sheep farm and did not learn how to swim until the age of 23. She worked as a hairdresser until she boarded a passenger boat for Europe.\n\nIn the summer of 1975 in Saint-Malo, France she met her future husband Rob James, who was skippering yachts for Chay Blyth and who had come into port with a charter boat. She learned about sailing from Rob James, and while waiting for him to return from an ocean race and marry her, she made the decision to sail single-handed around the world, non-stop. She told Rob her dream on their honeymoon, and had only six-weeks sailing experience at the time. Chay Blyth lent her the boat \"Spirit of Cutty Sark\" (later renamed \"Express Crusader\"), other people raised money for supplies, and the \"Daily Express\" raised sponsorship money. She sailed around the world aboard the yacht \"Express Crusader\". During her voyage, she once nearly lost her mast, capsized and had no radio for several weeks.\n\nNaomi James was made a Dame Commander of the Order of the British Empire in 1979 in recognition of her achievements.\n\nShe was the subject of \"This Is Your Life\" in 1979 when she was surprised by Eamonn Andrews.\n\nAfter her voyage, she found a house with her husband in Cork Harbour, Ireland.\n\nNaomi was reunited with the Express Crusader (fitted out and renamed Kriter Lady) for the 1980 Europe 1 STAR. She was the first woman back and broke the women's speed record with a time of 25 days, 19 hours. Rob also competed in that race, finishing twelfth in the trimaran Boatfile.\n\nShe gave up sailing in 1982 after winning the two thousand mile Round Britain Race with her husband Rob James, because she suffered badly from sea sickness during that voyage (possibly augmented by morning sickness due to her pregnancy). In 1983 while sailing in the same boat which won the race, her husband fell overboard and drowned off Salcombe, Devon. Her daughter was born 10 days later.\n\nDame Naomi was inducted into the New Zealand Sports Hall of Fame in 1990. She received her Ph.D in Philosophy from University College Cork in 2006. \n\nKrystyna Chojnowska-Liskiewicz of Poland was the first woman to sail around the world solo, completing her 401-day voyage (via the Panama Canal) on 21 April 1978, less than two months before James, starting and finishing in the Canary Islands.\n\nJames' voyage is notable as she was the first woman to single-handedly sail the clipper route, eastabout and south of the three great capes; and she completed a fast (although not without outside assistance) circumnavigation in just 272 days. According to the rules of the World Sailing Speed Record Council, a circumnavigation of the globe for speed record purposes has to start and finish in the English Channel; James started and finished her voyage in Dartmouth, therefore fulfilling this condition.\n\nIn 1988, Kay Cottee of Australia became the first woman to complete a \"non-stop\" single-handed circumnavigation, on \"Blackmore's First Lady\".\n\nThe first woman to sail around the world was Jeanne Baret, a French woman who, disguised as a man, sailed on the Etoile, one of the two ships on the French expedition led by Louis-Antoine de Bougainville. Baret was a herbalist and assisted in the identification of new species. The expedition left France in April 1768.\n\n"}
{"id": "678477", "url": "https://en.wikipedia.org/wiki?curid=678477", "title": "Ningal", "text": "Ningal\n\nNingal ( , \"Great Lady/Queen\") was a goddess of reeds in the Sumerian mythology, daughter of Enki and Ningikurga and the consort of the moon god Nanna by whom she bore the sun god Utu, his sister Inanna, and in some texts, Ishkur. She is chiefly recognised at Ur, and was probably first worshipped by cow-herders in the marsh lands of southern Mesopotamia.\n"}
{"id": "2051493", "url": "https://en.wikipedia.org/wiki?curid=2051493", "title": "Palus Putredinis", "text": "Palus Putredinis\n\nPalus Putredinus (Latin for \"Marsh of Decay\") is an area of the lunar surface that stretches from the crater Archimedes southeast toward the rugged Montes Apenninus range located on the southeastern edge of Mare Imbrium. This region is a nearly level, lava-flooded plain bounded by the crater Autolycus to the north and the foothills of the Montes Archimedes to the west. The selenographic coordinates are 26.5° N, 0.4° E, and it lies within a diameter of 161 km.\n\nIn the southern part of this area is a rille system designated Rimae Archimedes. To the south is a prominent linear rille named Rima Bradley, and to the east is the Rima Hadley, which served as the landing site for Apollo 15, and the Rimae Fresnel. Just to the northwest of the Palus Putredinus midpoint is the nearly submerged crater Spurr. Luna 2 crashed in this area.\n"}
{"id": "305992", "url": "https://en.wikipedia.org/wiki?curid=305992", "title": "Regenerative brake", "text": "Regenerative brake\n\nRegenerative braking is an energy recovery mechanism which slows a vehicle or object by converting its kinetic energy into a form which can be either used immediately or stored until needed. In a nutshell, the electric motor is using the vehicle's momentum to recover energy that would be otherwise lost to the brake discs as heat. This contrasts with conventional braking systems, where the excess kinetic energy is converted to unwanted and wasted heat by friction in the brakes, or with dynamic brakes, where energy is recovered by using electric motors as generators but is immediately dissipated as heat in resistors. In addition to improving the overall efficiency of the vehicle, regeneration can greatly extend the life of the braking system as its parts do not wear as quickly.\n\nThe most common form of regenerative brake involves an electric motor as an electric generator. In electric railways the electricity generated is fed back into the supply system. In battery electric and hybrid electric vehicles, the energy is stored chemically in a battery, electrically in a bank of capacitors, or mechanically in a rotating flywheel. Hydraulic hybrid vehicles use hydraulic motors to store energy in the form of compressed air. In a fuel cell powered vehicle, the electric energy generated by the motor is used to break waste water down into oxygen, and hydrogen which goes back into the fuel cell for later reuse.\n\nRegenerative braking is not by itself sufficient as the sole means of safely bringing a vehicle to a standstill, or slowing it as required, so it must be used in conjunction with another braking system such as friction-based braking.\n\nRegenerative and friction braking must both be used, creating the need to control them to produce the required total braking. The GM EV-1 was the first commercial car to do this. In 1997 and 1998 engineers Abraham Farag and Loren Majersik were issued two patents for this \"brake-by-wire\" technology.\n\nEarly applications commonly suffered from a serious safety hazard: in many early electric vehicles with regenerative braking, the same controller positions were used to apply power and to apply the regenerative brake, with the functions being swapped by a separate manual switch. This led to a number of serious accidents when drivers accidentally accelerated when intending to brake, such as the runaway train accident in Wädenswil, Switzerland in 1948, which killed twenty-one people.\n\nElectric motors, when used in reverse function as generators, convert mechanical energy into electrical energy. Vehicles propelled by electric motors use them as generators when using regenerative braking, braking by transferring mechanical energy from the wheels to an electrical load.\n\nEarly examples of this system were the front-wheel drive conversions of horse-drawn cabs by Louis Antoine Krieger in Paris in the 1890s. The Krieger electric landaulet had a drive motor in each front wheel with a second set of parallel windings (bifilar coil) for regenerative braking.\nIn England, \"automatic regenerative control\" was introduced to tramway operators by John S. Raworth's Traction Patents 1903–1908, offering them economic and operational benefits\n\nThese included tramway systems at Devonport (1903), Rawtenstall, Birmingham, Crystal Palace-Croydon (1906), and many others. Slowing the speed of the cars or keeping it in control on descending gradients, the motors worked as generators and braked the vehicles. The tram cars also had wheel brakes and track slipper brakes which could stop the tram should the electric braking systems fail. In several cases the tram car motors were shunt wound instead of series wound, and the systems on the Crystal Palace line utilized series-parallel controllers. Following a serious accident at Rawtenstall, an embargo was placed on this form of traction in 1911; the regenerative braking system was reintroduced twenty years later.\nRegenerative braking has been in extensive use on railways for many decades. The Baku-Tbilisi-Batumi railway (Transcaucasus Railway or Georgian railway) started utilizing regenerative braking in the early 1930s. This was especially effective on the steep and dangerous Surami Pass. In Scandinavia the Kiruna to Narvik electrified railway carries iron ore on the steeply-graded route from the mines in Kiruna, in the north of Sweden, down to the port of Narvik in Norway to this day. The rail cars are full of thousands of tons of iron ore on the way down to Narvik, and these trains generate large amounts of electricity by regenerative braking, with a maximum recuperative braking force of 750 kN. From Riksgränsen on the national border to the Port of Narvik, the trains use only a fifth of the power they regenerate. The regenerated energy is sufficient to power the empty trains back up to the national border. Any excess energy from the railway is pumped into the power grid to supply homes and businesses in the region, and the railway is a net generator of electricity.\n\nElectric cars used regenerative braking since the earliest experiments, but this was often a complex affair where the driver had to flip switches between various operational modes in order to use it. The Baker Electric Runabout and the Owen Magnetic were early examples, which used many switches and modes controlled by an expensive \"black box\" or \"drum switch\" as part of their electrical system. These, like the Krieger design, could only practically be used on downhill portions of a trip, and had to be manually engaged.\n\nImprovements in electronics allowed this process to be fully automated, starting with 1967's AMC Amitron experimental electric car. Designed by Gulton Industries the motor controller automatically began battery charging when the brake pedal was applied. Many modern hybrid and electric vehicles use this technique to extend the range of the battery pack, especially those using an AC drive train (most earlier designs used DC power).\n\nAn AC/DC rectifier and a very large capacitor may be used to store the regenerated energy, rather than a battery. The use of a capacitor allows much more rapid peak storage of energy, and at higher voltages. Mazda uses this system in some current (2018) road cars, where it is branded i-ELOOP.\n\nIn 1886 the Sprague Electric Railway & Motor Company, founded by Frank J. Sprague, introduced two important inventions: a constant-speed, non-sparking motor with fixed brushes, and regenerative braking.\n\nDuring braking, the traction motor connections are altered to turn them into electrical generators. The motor fields are connected across the main traction generator (MG) and the motor armatures are connected across the load. The MG now excites the motor fields. The rolling locomotive or multiple unit wheels turn the motor armatures, and the motors act as generators, either sending the generated current through onboard resistors (dynamic braking) or back into the supply (regenerative braking). Compared to electro-pneumatic friction brakes, braking with the traction motors can be regulated faster improving the performance of wheel slide protection.\n\nFor a given direction of travel, current flow through the motor armatures during braking will be opposite to that during motoring. Therefore, the motor exerts torque in a direction that is opposite from the rolling direction.\n\nBraking effort is proportional to the product of the magnetic strength of the field windings, multiplied by that of the armature windings.\n\nSavings of 17%, and less wear on friction braking components, are claimed for Virgin Trains Pendolinos. The Delhi Metro reduced the amount of carbon dioxide () released into the atmosphere by around 90,000 tons by regenerating 112,500 megawatt hours of electricity through the use of regenerative braking systems between 2004 and 2007. It was expected that the Delhi Metro would reduce its emissions by over 100,000 tons of per year once its phase II was complete, through the use of regenerative braking.\n\nElectricity generated by regenerative braking may be fed back into the traction power supply; either offset against other electrical demand on the network at that instant, used for head end power loads, or stored in lineside storage systems for later use.\n\nA form of what can be described as regenerative braking is used on some parts of the London Underground, achieved by having small slopes leading up and down from stations. The train is slowed by the climb, and then leaves down a slope, so kinetic energy is converted to gravitational potential energy in the station. This is normally found on the deep tunnel sections of the network and not generally above ground or on the cut and cover sections of the Metropolitan and District Lines.\n\nWhat are described as dynamic brakes (\"rheostatic brakes\" in the UK) on electric traction systems, unlike regenerative brakes, dissipate electric energy as heat rather than using it, by passing the current through large banks of resistors. Vehicles that use dynamic brakes include forklift trucks, diesel-electric locomotives, and trams. This heat can be used to warm the vehicle interior, or dissipated externally by large radiator-like cowls to house the resistor banks.\n\nGeneral Electric's experimental 1936 steam turbine locos featured true regeneration. These two locomotives ran the steam water over the resistor packs, as opposed to air cooling used in most dynamic brakes. This energy displaced the oil normally burned to keep the water hot, and thereby recovered energy that could be used to accelerate again.\n\nThe main disadvantage of regenerative brakes when compared with dynamic brakes is the need to closely match the generated current with the supply characteristics and increased maintenance cost of the lines. With DC supplies, this requires that the voltage be closely controlled. The AC power supply and frequency converter pioneer Miro Zorič and his first AC power electronics have also enabled this to be possible with AC supplies. The supply frequency must also be matched (this mainly applies to locomotives where an AC supply is rectified for DC motors).\n\nIn areas where there is a constant need for power unrelated to moving the vehicle, such as electric train heat or air conditioning, this load requirement can be utilized as a sink for the recovered energy via modern AC traction systems. This method has become popular with North American passenger railroads where head end power loads are typically in the area of 500 kW year round. Using HEP loads in this way has prompted recent electric locomotive designs such as the ALP-46 and ACS-64 to eliminate the use of dynamic brake resistor grids and also eliminates any need for any external power infrastructure to accommodate power recovery allowing self-powered vehicles to employ regenerative braking as well.\n\nA small number of steep grade railways have used 3-phase power supplies and induction motors. This results in a near constant speed for all trains, as the motors rotate with the supply frequency both when driving and braking.\n\nKinetic energy recovery systems (KERS) were used for the motor sport Formula One's 2009 season, and are under development for road vehicles. KERS was abandoned for the 2010 Formula One season, but re-introduced for the 2011 season. By 2013, all teams were using KERS with Marussia starting use for the 2013 season. One of the main reasons that not all cars used KERS immediately is because it raises the car's center of gravity, and reduces the amount of ballast that is available to balance the car so that it is more predictable when turning. FIA rules also limit the exploitation of the system. The concept of transferring the vehicle's kinetic energy using flywheel energy storage was postulated by physicist Richard Feynman in the 1950s and is exemplified in such systems as the Zytek, Flybrid, Torotrak and Xtrac used in F1. Differential based systems also exist such as the Cambridge Passenger/Commercial Vehicle Kinetic Energy Recovery System (CPC-KERS).\n\nXtrac and Flybrid are both licensees of Torotrak's technologies, which employ a small and sophisticated ancillary gearbox incorporating a continuously variable transmission (CVT). The CPC-KERS is similar as it also forms part of the driveline assembly. However, the whole mechanism including the flywheel sits entirely in the vehicle's hub (looking like a drum brake). In the CPC-KERS, a differential replaces the CVT and transfers torque between the flywheel, drive wheel and road wheel.\n\nThe first of these systems to be revealed was the Flybrid. This system weighs 24 kg and has an energy capacity of 400 kJ after allowing for internal losses. A maximum power boost of 60 kW (81.6 PS, 80.4 HP) for 6.67 seconds is available. The 240 mm diameter flywheel weighs 5.0 kg and revolves at up to 64,500 rpm. Maximum torque is 18 Nm (13.3 ftlbs). The system occupies a volume of 13 litres.\n\nFormula One have stated that they support responsible solutions to the world's environmental challenges, and the FIA allowed the use of KERS in the regulations for the 2009 Formula One season. Teams began testing systems in 2008: energy can either be stored as mechanical energy (as in a flywheel) or as electrical energy (as in a battery or supercapacitor).\n\nTwo minor incidents were reported during testing of KERS systems in . The first occurred when the Red Bull Racing team tested their KERS battery for the first time in July: it malfunctioned and caused a fire scare that led to the team's factory being evacuated. The second was less than a week later when a BMW Sauber mechanic was given an electric shock when he touched Christian Klien's KERS-equipped car during a test at the Jerez circuit.\n\nWith the introduction of KERS in the 2009 season, four teams used it at some point in the season: Ferrari, Renault, BMW, and McLaren. During the season, Renault and BMW stopped using the system. Vodafone McLaren Mercedes became the first team to win a F1 GP using a KERS equipped car when Lewis Hamilton won the Hungarian Grand Prix on 26 July 2009. Their second KERS equipped car finished fifth. At the following race, Lewis Hamilton became the first driver to take pole position with a KERS car, his teammate, Heikki Kovalainen qualifying second. This was also the first instance of an all KERS front row. On 30 August 2009, Kimi Räikkönen won the Belgian Grand Prix with his KERS equipped Ferrari. It was the first time that KERS contributed directly to a race victory, with second placed Giancarlo Fisichella claiming \"Actually, I was quicker than Kimi. He only took me because of KERS at the beginning\".\n\nAlthough KERS was still legal in F1 in the 2010 season, all the teams had agreed not to use it. New rules for the 2011 F1 season which raised the minimum weight limit of the car and driver by 20 kg to 640 kg, along with the FOTA teams agreeing to the use of KERS devices once more, meant that KERS returned for the 2011 season. This is still optional as it was in the 2009 season; in the 2011 season 3 teams elected not to use it. For the 2012 season, only Marussia and HRT raced without KERS, and by 2013, with the withdrawal of HRT, all 11 teams on the grid were running KERS.\n\nFor the 2014 season, the power output of the MGU-K (The replacement of the KERS and part of the ERS system that also includes a turbocharger waste heat recovery system) increased from 60 kW to 120 kW and it is allowed to recover 2 mega- joules per lap. This was to balance the sport's move from 2.4 litre V8 engines to 1.6 litre V6 engines. The fail-safe settings of the brake-by-wire system that now supplements KERS came under examination as a contributing factor in the crash of Jules Bianchi at the 2014 Japanese Grand Prix.\n\nBosch Motorsport Service is developing a KERS for use in motor racing. These electricity storage systems for hybrid and engine functions include a lithium-ion battery with scalable capacity or a flywheel, a four to eight kilogram electric motor (with a maximum power level of ), as well as the KERS controller for power and battery management. Bosch also offers a range of electric hybrid systems for commercial and light-duty applications.\n\nAutomakers including Honda have been testing KERS systems. At the 2008 1,000 km of Silverstone, Peugeot Sport unveiled the Peugeot 908 HY, a hybrid electric variant of the diesel 908, with KERS. Peugeot planned to campaign the car in the 2009 Le Mans Series season, although it was not capable of scoring championship points. Peugeot plans also a compressed air regenerative braking powertrain called Hybrid Air.\n\nVodafone McLaren Mercedes began testing of their KERS in September 2008 at the Jerez test track in preparation for the 2009 F1 season, although at that time it was not yet known if they would be operating an electrical or mechanical system. In November 2008 it was announced that Freescale Semiconductor would collaborate with McLaren Electronic Systems to further develop its KERS for McLaren's Formula One car from 2010 onwards. Both parties believed this collaboration would improve McLaren's KERS system and help the system filter down to road car technology.\n\nToyota has used a supercapacitor for regeneration on Supra HV-R hybrid race car that won the 24 Hours of Tokachi race in July 2007.\n\nBMW has used regenerative braking on their E90 3 Series as well as in current models like F25 5 Series under the EfficientDynamics moniker. Volkswagen have regenerative braking technologies under the BlueMotion brand in such models as the MK7 Golf and MK7 Golf Estate / Wagon models, other VW group brands like SEAT, Skoda and Audi.\n\nKTM racing boss Harald Bartol has revealed that the factory raced with a secret kinetic energy recovery system (KERS) fitted to Tommy Koyama's motorcycle during the 2008 season-ending 125cc Valencian Grand Prix. This was against the rules, so they were banned from doing it afterwards.\n\nRegenerative braking is also possible on a non-electric bicycle. The EPA, working with students from the University of Michigan, developed the hydraulic Regenerative Brake Launch Assist (RBLA).\nIt is naturally available on electric bicycles with direct-drive hub motors.\n\nAutomobile Club de l'Ouest, the organizer behind the annual 24 Hours of Le Mans event and the Le Mans Series is currently \"studying specific rules for LMP1 that will be equipped with a kinetic energy recovery system.\" Peugeot was the first manufacturer to unveil a fully functioning LMP1 car in the form of the 908 HY at the 2008 Autosport 1000 km race at Silverstone.\n\nThe energy of a flywheel can be described by this general energy equation, assuming the flywheel is the system:\n\nWhere:\n\nAn assumption is made that during braking there is no change in the potential energy, enthalpy of the flywheel, pressure or volume of the flywheel, so only kinetic energy will be considered. As the car is braking, no energy is dispersed by the flywheel, and the only energy into the flywheel is the initial kinetic energy of the car. The equation can be simplified to:\n\nWhere:\n\nThe flywheel collects a percentage of the initial kinetic energy of the car, and this percentage can be represented by formula_8. The flywheel stores the energy as rotational kinetic energy. Because the energy is kept as kinetic energy and not transformed into another type of energy this process is efficient. The flywheel can only store so much energy, however, and this is limited by its maximum amount of rotational kinetic energy. This is determined based upon the inertia of the flywheel and its angular velocity. As the car sits idle, little rotational kinetic energy is lost over time so the initial amount of energy in the flywheel can be assumed to equal the final amount of energy distributed by the flywheel. The amount of kinetic energy distributed by the flywheel is therefore:\n\nRegenerative braking has a similar energy equation to the equation for the mechanical flywheel. Regenerative braking is a two-step process involving the motor/generator and the battery. The initial kinetic energy is transformed into electrical energy by the generator and is then converted into chemical energy by the battery. This process is less efficient than the flywheel. The efficiency of the generator can be represented by:\n\nWhere:\n\nThe only work into the generator is the initial kinetic energy of the car and the only work produced by the generator is the electrical energy. Rearranging this equation to solve for the power produced by the generator gives this equation:\n\nWhere:\n\nThe efficiency of the battery can be described as:\n\nWhere:\nThe work out of the battery represents the amount of energy produced by the regenerative brakes. This can be represented by:\n\nIn the case of internal combustion engines, the sketch of the DoE shows that average car efficiency amounts to less than 20%. We can see for ourselves that braking in proportion to the useful mechanic energy amounts to 6/13 i.e. 46% in towns, and 2/20 i.e. 10% on motorways.\n\nIn regards to electric cars, the DoE explains that the efficiency between the electric motor and the wheels amounts to 60% (however for the overall conversion see Embodied energy#Embodied energy in the energy field).\n\nLet us consider the electric motor efficiency formula_21 and the braking proportion in towns formula_22 and on motorways formula_23.\n\nLet us introduce formula_24 which is the recuperated proportion of braking energy. Theoretically, it can reach up to 80%. Thus formula_25 in the best case.\n\nUnder these circumstances, formula_26 being the energy flux arriving at the electric engine, formula_27 the energy flux lost while braking and formula_28 the recuperated energy flux, an equilibrium is reached according to the equations\n\nformula_29 and formula_30\n\nthus formula_31\n\nIt is as though the old energy flux formula_26 was replaced by a new one formula_33\n\nThe expected gain amounts to formula_34\n\nThe higher the recuparation efficiency, the higher the recuperation.\n\nThe higher the efficiency between the electric motor and the wheels, the higher the recuperation.\n\nThe higher the braking proportion, the higher the recuperation.\n"}
{"id": "533365", "url": "https://en.wikipedia.org/wiki?curid=533365", "title": "Rift", "text": "Rift\n\nIn geology, a rift is a linear zone where the lithosphere is being pulled apart and is an example of extensional tectonics.\n\nTypical rift features are a central linear downfaulted depression, called a graben, or more commonly a half-graben with normal faulting and rift-flank uplifts mainly on one side. Where rifts remain above sea level they form a rift valley, which may be filled by water forming a rift lake. The axis of the rift area may contain volcanic rocks, and active volcanism is a part of many, but not all active rift systems.\n\nMajor rifts occur along the central axis of most mid-ocean ridges, where new oceanic crust and lithosphere is created along a divergent boundary between two tectonic plates.\n\n\"Failed rifts\" are the result of continental rifting that failed to continue to the point of break-up. Typically the transition from rifting to spreading develops at a triple junction where three converging rifts meet over a hotspot. Two of these evolve to the point of seafloor spreading, while the third ultimately fails, becoming an aulacogen.\n\nMost rifts consist of a series of separate segments that together form the linear zone characteristic of rifts. The individual rift segments have a dominantly half-graben geometry, controlled by a single basin-bounding fault. Segment lengths vary between rifts, depending on the elastic thickness of the lithosphere. Areas of thick colder lithosphere, such as the Baikal Rift have segment lengths in excess of 80 km, while in areas of warmer thin lithosphere, segment lengths may be less than 30 km. Along the axis of the rift the position, and in some cases the polarity (the dip direction), of the main rift bounding fault changes from segment to segment. Segment boundaries often have a more complex structure and generally cross the rift axis at a high angle. These segment boundary zones accommodate the differences in fault displacement between the segments and are therefore known as accommodation zones.\n\nAccommodation zones take various forms, from a simple relay ramp at the overlap between two major faults of the same polarity, to zones of high structural complexity, particularly where the segments have opposite polarity. Accommodation zones may be located where older crustal structures intersect the rift axis. In the Gulf of Suez rift, the Zaafarana accommodation zone is located where a shear zone in the Arabian-Nubian Shield meets the rift.\n\nRift flanks or shoulders are elevated areas around rifts. Rift shoulders are typically about 70 km wide. Contrary to what was previously thought, elevated passive continental margins (EPCM) such as the Brazilian Highlands, the Scandinavian Mountains and India's Western Ghats, are not rift shoulders.\n\nAt the onset of rifting, the upper part of the lithosphere starts to extend on a series of initially unconnected normal faults, leading to the development of isolated basins. In subaerial rifts, drainage at this stage is generally internal, with no element of through drainage.\n\nAs the rift evolves, some of the individual fault segments grow, eventually becoming linked together to form the larger bounding faults. Subsequent extension becomes concentrated on these faults. The longer faults and wider fault spacing leads to more continuous areas of fault-related subsidence along the rift axis. Significant uplift of the rift shoulders develops at this stage, strongly influencing drainage and sedimentation in the rift basins.\n\nDuring the climax of lithospheric rifting, as the crust is thinned, the Earth's surface subsides and the Moho becomes correspondingly raised. At the same time, the mantle lithosphere becomes thinned, causing a rise of the top of the asthenosphere. This brings high heat flow from the upwelling asthenosphere into the thinning lithosphere, heating the orogenic lithosphere for dehydration melting, typically causing extreme metamorphism at high thermal gradients of greater than 30 °C. The metamorphic products are high to ultrahigh temperature granulites and their associated migmatite and granites in collisional orogens, with possible emplacement of metamorphic core complexes in continental rift zones but oceanic core complexes in spreading ridges. This leads to a kind of orogeneses in extensional settings, which is referred as to rifting orogeny.\n\nOnce rifting ceases, the mantle beneath the rift cools and this is accompanied by a broad area of post-rift subsidence. The amount of subsidence is directly related to the amount of thinning during the rifting phase calculated as the beta factor (initial crustal thickness divided by final crustal thickness), but is also affected by the degree to which the rift basin is filled at each stage, due to the greater density of sediments in contrast to water. The simple 'McKenzie model' of rifting, which considers the rifting stage to be instantaneous, provides a good first order estimate of the amount of crustal thinning from observations of the amount of post-rift subsidence. This has generally been replaced by the 'flexural cantilever model', which takes into account the geometry of the rift faults and the flexural isostasy of the upper part of the crust.\n\nSome rifts show a complex and prolonged history of rifting, with several distinct phases. The North Sea rift shows evidence of several separate rift phases from the Permian through to the Earliest Cretaceous, a period of over 100 million years.\n\nMany rifts are the sites of at least minor magmatic activity, particularly in the early stages of rifting. Alkali basalts and bimodal volcanism are common products of rift-related magmatism.\n\nRecent studies indicate that post-collisional granites in collisional orogens are the product of rifting magmatism at converged plate margins.\n\nThe sedimentary rocks associated with continental rifts host important deposits of both minerals and hydrocarbons.\n\nSedEx mineral deposits are found mainly in continental rift settings. They form within post-rift sequences when hydrothermal fluids associated with magmatic activity are expelled at the seabed.\n\nContinental rifts are the sites of significant oil and gas accumulations, such as the Viking Graben and the Gulf of Suez Rift. Thirty percent of giant oil and gas fields are found within such a setting. In 1999 it was estimated that there were 200 billion barrels of recoverable oil reserves hosted in rifts. Source rocks are often developed within the sediments filling the active rift (syn-rift), forming either in a lacustrine environment or in a restricted marine environment, although not all rifts contain such sequences. Reservoir rocks may be developed in pre-rift, syn-rift and post-rift sequences. Effective regional seals may be present within the post-rift sequence if mudstones or evaporites are deposited. Just over half of estimated oil reserves are found associated with rifts containing marine syn-rift and post-rift sequences, just under a quarter in rifts with a non-marine syn-rift and post-rift, and an eighth in non-marine syn-rift with a marine post-rift.\n\n\n\n"}
{"id": "43267397", "url": "https://en.wikipedia.org/wiki?curid=43267397", "title": "San Pablo Formation", "text": "San Pablo Formation\n\nThe San Pablo Formation is an Late/Upper Miocene epoch geologic formation of the East Bay region in the San Francisco Bay Area, California. \n\nIt is found on the south shore of San Pablo Bay, in western Contra Costa County. \n\nIt is series of marine sandstones with tuffs and ashes. It overlies the Briones Formation, and underlies the Pinole Tuff Formation. \n\nIt preserves fossils dating back to the Neogene period. \n\n"}
{"id": "164321", "url": "https://en.wikipedia.org/wiki?curid=164321", "title": "Secondary circulation", "text": "Secondary circulation\n\nA secondary circulation is a circulation induced in a rotating system. For example, the primary circulation of Earth's atmosphere is zonal. If however a parcel of air, that moves in a purely zonal direction, is accelerated or decelerated zonally, the Coriolis force will add a meridional component to its velocity. This meridional circulation is then the secondary circulation.\n\n"}
{"id": "1589335", "url": "https://en.wikipedia.org/wiki?curid=1589335", "title": "Sial", "text": "Sial\n\nIn geology, the term 'sial' refers to the composition of the upper layer of the Earth's crust, namely rocks rich in silicates and aluminium minerals. It is sometimes equated with the continental crust because it is absent in the wide oceanic basins, but \"sial\" is a geochemical term rather than a plate tectonic term. As these elements are less dense than the majority of the earth's elements, they tend to be concentrated in the upper layer of the crust.\n\nGeologists often refer to the rocks in this layer as felsic, because they contain high levels of feldspar, an aluminium silicate mineral series. However, the sial \"actually has quite a diversity of rock types, including large amounts of basaltic rocks.\"\n\nThe name 'sial' was taken from the first two letters of silica and of alumina. The sial is often contrasted to the 'sima,' the next lower layer in the Earth, which is often exposed in the ocean basins; and the nickel-iron alloy core, sometimes referred to as the \"Nife\". These geochemical divisions of the Earth's interior (with these names) were first proposed by Eduard Suess in the 19th century. This model of the outer layers of the earth has been confirmed by petrographic, gravimetric, and seismic evidence.\n\nSial has a lower density (2700–2800 kg/m) than sima, which is primarily due to increased amounts of aluminium, and decreased amounts of iron and magnesium. The base of the sial is not a strict boundary, the sial grades into the denser rocks of the sima. The Conrad discontinuity has been proposed as the boundary, but little is known about it, and it doesn't seem to match the point of geochemical change. Instead, the boundary has been arbitrarily set at a mean density of 2800 kg/m.\n\nBecause of the large pressures, over geologic time, the sima flows like a very viscous liquid, so, in a real sense, the sial floats on the sima, in isostatic equilibrium. Mountains extend down as well as up, much like icebergs on the ocean; so that on the continental plates the sial runs between 5 km and 70 km deep.\n\n\n"}
{"id": "3895745", "url": "https://en.wikipedia.org/wiki?curid=3895745", "title": "Smart meter", "text": "Smart meter\n\nA smart meter is an electronic device that records consumption of electric energy and communicates the information to the electricity supplier for monitoring and billing. Smart meters typically record energy hourly or more frequently, and report at least daily. Smart meters enable two-way communication between the meter and the central system. Such an advanced metering infrastructure (AMI) differs from automatic meter reading (AMR) in that it enables two-way communication between the meter and the supplier. Communications from the meter to the network may be wireless, or via fixed wired connections such as power line carrier (PLC). Wireless communication options in common use include cellular communications (which can be expensive), Wi-Fi (readily available), wireless ad hoc networks over Wi-Fi, wireless mesh networks, low power long range wireless (LORA), ZigBee (low power, low data rate wireless), and Wi-SUN (Smart Utility Networks).\n\nThe term \"Smart Meter\" often refers to an electricity meter, but it also may mean a device measuring natural gas or water consumption.\n\nSimilar meters, usually referred to as interval or time-of-use meters, have existed for years, but \"Smart Meters\" usually involve real-time or near real-time sensors, power outage notification, and power quality monitoring. These additional features are more than simple automated meter reading (AMR). They are similar in many respects to Advanced Metering Infrastructure (AMI) meters. Interval and time-of-use meters historically have been installed to measure commercial and industrial customers, but may not have automatic reading.\n\nResearch by the UK consumer group, showed that as many as one in three confuse smart meters with energy monitors, also known as in-home display monitors. The roll-out of smart meters is claimed to be one strategy for saving energy. While energy suppliers in the UK could save around £300 million a year from their introduction, benefits to users of electricity depends on their using the information to change their pattern of energy use. For example, smart meters may facilitate taking advantage of lower off-peak time tariffs, and selling electricity back to the grid with net metering.\n\nThe installed base of smart meters in Europe at the end of 2008 was about 39 million units, according to analyst firm Berg Insight. Globally, Pike Research found that smart meter shipments were 17.4 million units for the first quarter of 2011. Visiongain determined that the value of the global smart meter market would reach US$7 billion in 2012.\n\nSmart meters may be part of a \"smart grid\", but do not themselves constitute a smart grid.\n\nIn 1972, Theodore Paraskevakos, while working with Boeing in Huntsville, Alabama, developed a sensor monitoring system that used digital transmission for security, fire, and medical alarm systems as well as meter reading capabilities. This technology was a spin-off from the automatic telephone line identification system, now known as Caller ID.\n\nIn 1974, Paraskevakos was awarded a U.S. patent for this technology. In 1977, he launched Metretek, Inc., which developed and produced the first fully automated, commercially available remote meter reading and load management system. Since this system was developed pre-Internet, Metretek utilized the IBM series 1 mini-computer. For this approach, Paraskevakos and Metretek were awarded multiple patents.\n\nSince the inception of electricity deregulation and market-driven pricing throughout the world, utilities have been looking for a means to match consumption with generation. Non-smart electrical and gas meters only measure total consumption, providing no information of when the energy was consumed. Smart meters provide a way of measuring this site-specific information, allowing utility companies to charge different prices for consumption according to the time of day and the season.\n\nUtility companies say that smart metering offers potential benefits to householders. These include, a) an end to estimated bills, which are a major source of complaints for many customers b) a tool to help consumers better manage their energy purchases—stating that smart meters with a display outside their homes could provide up-to-date information on gas and electricity consumption and in doing so help people to manage their energy use and reduce their energy bills. Electricity pricing usually peaks at certain predictable times of the day and the season. In particular, if generation is constrained, prices can rise if power from other jurisdictions or more costly generation is brought online. Proponents assert that billing customers at a higher rate for peak times encourages consumers to adjust their consumption habits to be more responsive to market prices and assert further, that regulatory and market design agencies hope these \"price signals\" could delay the construction of additional generation or at least the purchase of energy from higher priced sources, thereby controlling the steady and rapid increase of electricity prices. There are some concerns, however, that low income and vulnerable consumers may not benefit from intraday time-of-use tariffs.\n\nAn academic study based on existing trials showed that homeowners' electricity consumption on average is reduced by approximately 3-5%.\n\nThe ability to connect/disconnect service and read meter consumption remotely are major labor savings for the utility and can cause large layoffs of meter readers.\n\nThe American Council for an Energy-Efficient Economy (ACEEE) reviewed more than 36 different residential smart metering and feedback programmes internationally. This is the most extensive study of its kind (as of January 2011). Their conclusion was: “To realise potential feedback-induced savings, advanced meters [smart meters] must be used in conjunction with in-home (or on-line) displays and well-designed programmes that successfully inform, engage, empower and motivate people.\" There are near-universal calls from both the energy industry and consumer groups for a national social marketing campaign to help raise awareness of smart metering and to give customers the information and support they need to become more energy efficient, and what changes they must make to realize the potential of proposed smart meters.\n\nIn 2004, the Essential Services Commission of Victoria, Australia (ESC) released its changes to the Electricity Customer Metering Code and Procedure to implement its decision to mandate interval meters for 2.6 million Victorian electricity customers.\n\nThe ESC's Final Paper titled \"Mandatory Rollout of Interval Meters for Electricity Customers\" foreshadowed the changes to be implemented and contained the rollout timetable requiring interval meters to be installed for all small businesses and residences. The rollout commenced in mid-2009 and was completed at the end of 2013.\n\nThe Commonwealth issued a Joint Communiqué at the Council of Australian Governments meeting in Canberra on 10 February 2006 committing all governments to the progressive rollout of smart metering technology from 2007.\n\nIn 2009 the Victorian Auditor General undertook a review of the program and found that there were \"significant inadequacies\" in advice to government and that project governance \"has not been appropriate\".\n\nMeters installed in Victoria have been deployed with limited smart functionality that is being increased over time. 30-minute interval data is available, remote cut-off and start-up energization is available, and the Home Area Network were available for households in 2012.\n\nIn May 2010 it was reported that the program was expected to cost A$500 million more than originally estimated by proponents, with a total cost of $1.6 billion.\n\nIn November 2010 the Victorian Labor Party was voted out of state government. The incoming coalition stated that the meter program would be reviewed and the Auditor General's recommendations implemented, specifically commenting on program governance, customer data protection, and cost recovery. In January 2011 the Energy Minister, Michael O'Brien, said he was not ruling out a suspension of the program. This review, delivered in December 2012 endorsed the continuation of the roll out, with minor changes.\n\nThe Victorian Government after initially halting the planned implementation of Time-of-Use tariffs for general consumers has now allowed their introduction from mid-2013.\n\nAs shown in the chart below, Victorian metering charges increased by approximately $60 per meter per year after the introduction of AMI cost recovery from customers in 2010 and a projected increase to 125.73 by 2016-2017.\n\nBy mid-July 2013, the first Smart Meter In-Home Displays were being made available to Victorian consumers. At the beginning of 2014 there were three approved Smart Meter In-Home Displays directly available to consumers.\n\nAnnual meter charge increases with smart meter costs in 2010 and projections to 2017 ($)\nThe Ontario Energy Board in Ontario, Canada has worked to define the technology and develop the regulatory framework for its implementation. The Government of Ontario set a target of deploying smart meters to 800,000 homes and small businesses (i.e. small \"general service\" customers under 50 kW demand) by the end of 2007, which was surpassed, and throughout the province by the end of 2010. Notably, the addition of smart meters to the grid in Ontario has been financed, in part, by the Green Energy Act 2009 and the resulting tariff known as the Global Adjustment. This fee has played a major role in financing additional investments in the grid, such as smart meters and new high capacity transmission lines.\n\nBC Hydro in British Columbia, Canada supplied Itron smart meters to most of its customers by the end of 2012.\n\nSmart meter installations have been associated with several fires in Canada, but these were probably caused by pre-existing problems unrelated to the meters. BC Hydro says that \"the risk of a smart meter installation causing an electrical problem is extremely low\", and it assists homeowners if repairs are necessary for a safe installation.\n\nIn November 2011, the Union of British Columbia Municipalities voted in favour of a moratorium to temporarily suspend smart meter installations. The provincial government insists that installations will proceed, based on global standards. As of May 2012, 39 municipalities in British Columbia have passed motions opposing the installation of smart meters. The utility company, BC Hydro, is not legally obliged to abide by these city decisions. In September 2013 BC Hydro announced the \"Meter Choices Program\", which lets customers keep their old meter or have a smart meter with the radio off. Both options have an additional monthly negative option fee in the range of $20–$33 per month, depending on specifics.\n\nMarijuana grow-ops, a major illicit industry in British Columbia, steal significant amounts of electricity. The installation of smart meters is part of BC Hydro's electricity theft reduction program.\n\nThe world's largest smart meter deployment was undertaken by Enel SpA, the dominant utility in Italy with more than 30 million customers. Between 2000 and 2005 Enel deployed smart meters to its entire customer base.\n\nThese meters are fully electronic and smart, with integrated bi-directional communications, advanced power measurement and management capabilities, an integrated, software-controllable disconnect switch, and an all solid-state design. They communicate over low voltage power lines, without the need for WiFi, to data concentrators that communicate via IP to Enel's enterprise servers. Standards-based power line technology from Echelon Corporation is used.\n\nThe system provides a wide range of advanced features, including the ability to remotely turn power on or off to a customer, read usage information from a meter, detect a service outage, change the maximum amount of electricity that a customer may demand at any time, detect \"unauthorized\" use of electricity and remotely shut it off, and remotely change the meter's billing plan from credit to prepay, as well as, from flat-rate to multi-tariff.\n\nThe Energy Conservation Center in Japan promotes energy efficiency including smart metering. Public utilities have started to test metering with integrated communication devices. Private entities have already implemented efficient energy systems with integrated feedback methods such as alerts or triggers.\n\nThe company Oxxio introduced the first smart meter for both electricity and gas in the Netherlands in 2005. In 2007, the Dutch government proposed that all seven million households of the country should have a smart meter by 2013, as part of a national energy reduction plan.\n\nIn August 2008 the roll out of these seven million meters was delayed for several reasons. Main reasons for the delay were that there was limited possibility foreseen to register small scale local energy production (e.g. by solar panels), and that there was uncertainty in parliament on future developments in smart meters.\n\nOn April 7, 2009 the Dutch government had to back down after consumer groups raised privacy concerns. Instead of a mandatory roll-out smart meters are voluntary.\n\nBy the end of 2016 there were 3 million households (39%) with smart meters installed. The Dutch government aims for a minimum of 80%, but preferably 100%, of households with smart meters installed in 2020.\n\nIn November 2005, energy supplier Meridian Energy introduced the usage of smart meters in the Central Hawkes Bay area with more than 1,000 households participating. By late 2006, over 6,300 smart meters had been installed as part of the initial trial. On June 28, 2007, the first roll-out began for households in Christchurch and there were plans to install more than 112,000 smart meters by January 2009. These smart meters are made by a Christchurch-based company, Arc Innovations, which is a wholly owned subsidiary of Meridian Energy.\n\nIn June 2009, the Parliamentary Commissioner for the Environment released a report that was critical of the \"lack of smartness\" in the 150,000 smart meters installed in New Zealand thus far. Dr Jan Wright called for government leadership for this \"infrastructure of national importance.\" Wright emphasised that the meters were capable of being smart, but that the failure to include the HAN chips at the initial installation meant that currently only the power retailers benefited, not consumers, nor the environment.\n\nIn 2015 the New Zealand office of the Privacy Commissioner raised concerns that smart meters recorded personal information including the time that homeowners were home or not, the office said \"the [smart meter] readings undoubtedly contained private information, and power companies needed to be more upfront about their handling of the torrent of personal information gathered by smart meters\".\n\nNorthern Europe became the hotspot for AMM (Advanced Metering Management) in Europe in 2003 when Sweden announced the decision to require monthly readings of all electricity meters by 2009. Soon activities spread to the other Nordic countries. Vattenfall, Fortum, and E.ON decided to deploy AMM in Finland as well as in Sweden, as the leading industry players in both countries at the time.\n\nDevelopments in Denmark took off in 2004 with several ambitious projects being announced by the country’s largest utilities. Norway has taken a more cautious stance, but in June 2007 the Norwegian energy authority, NVE, declared that it would recommend new legislation requiring smart meters to take effect in 2013. As of August 2007, almost all of the DSOs in Sweden had signed contracts for AMM solutions. Norway was lagging behind with just 6 percent. Altogether contracts for nearly 8 million smart meters are still open in the Nordic region.\n\n\"Information gathered from \"SmartRegions\" program\".\n\nSpain is a country with 46 million inhabitants and approximately 26 million electricity customers. Three major energy players act in the country, Endesa, Iberdrola, and \"Gas Natural/Unión Fenosa\", with a market share of almost 95%. ESMA (2010, 26-28) provides a good overview of the situation in Spain. By 2013, Endesa has replaced about 30% of meters (based on the Meters and More technology), of about 3.5 million customers.\n\nRegulations existing in Spain related to implementation of smart meters:\n\nThe Smart Metering obligations were established in December 2007 with the national meter substitution plan for end-users up to 15 kW. The aim is to support remote energy management systems. The plan is managed by the Ministry of Industry, with a deadline for the completion of the plan by 31 December 2018. All DSOs had to submit their substitution plans to the regional governments. A binding target of 30% of all customers was set for 2010. This initial target could not be reached by any of the DSOs, however, due to a late approval of the substitution plan (in May 2009), technological uncertainties in terms of system communication, alleged supply problems of certified meters that were available only in June 2010 and ongoing negotiations with the regulators about the level of cost acceptance.\n\nIn August 2007, the UK Government held a consultation on improving metering and information about power consumption. The consultation attached the necessary draft regulations and proposed that, from 2008, domestic customers should provide comparative historical consumption data and electricity suppliers should provide a real-time display unit within time limits. For business customers, it was proposed that gas and electricity suppliers should install smart meters in those parts of the SME sector where it had been shown to be cost-effective.\n\nIn December 2009, the United Kingdom's Department of Energy and Climate Change announced its intention to have smart meters in all homes in Great Britain by 2020. The model is a competitive, supplier-led roll-out with a central communications body, called the Data Communications Company (DCC), which was established in September 2013. The DCC runs a central database that holds meter readings. As well as the DCC, the government established Smart Energy GB to lead the nationwide publicity for the roll-out programme.\n\nTo decide whether or not to mandate smart meters, the government devised impact assessments to ascertain whether there was a positive business case for smart metering. These looked at potential costs and benefits to suppliers, network operators, customers, and Britain as a whole. DECC’s Impact Assessment, updated in January 2014, concluded that there was a positive business case overall for a smart meter roll-out of £6.2 billion of net benefits.\n\nAfter testing the time needed for the design, build and test phases of industry’s programmes, the Government reviewed the programme timetable in 2013. The consistent message was that more time was needed if the roll-out of smart meters was to get off to the best possible start and customers were to be sure of a quality service. In December 2013, the Government expected that the smart meter roll-outs would be completed by the end of 2020.\n\nConsumer groups such as Consumer Focus and the Consumers Association raised concerns about the extent to which consumer interests are protected during the roll-out, e.g., protection from pressure-selling while energy representatives are in their homes fitting meters. The Government banned sales activities during meter installation visits and put in place a number of consumer protection measures, including ensuring that consumers have choice and control over how their energy consumption data is used, apart from where it is required for billing purposes.\n\nThe 2015 manifesto of the Conservative government had implied a requirement for all consumers to have a smart meter by the end of 2020. The wording of the 2017 election manifesto was changed to promise only to \"offer\" one. After the Conservative Party gained the most seats of any party in 2017, it was explained that nothing had changed, the intention being to impose an obligation on suppliers but not on users, and that the government intended to \"try and ensure there is a meter in every home\" by 2020.\n\nThe United Kingdom roll-out is considered to be the largest programme ever undertaken, involving visits to 30 million homes and 2 million small businesses to replace meters for both gas and electricity. Most households will have smart meters installed by their energy company before 2020. At the end of 2013 there were 295,700 smart meters installed in domestic properties in Great Britain. In March 2016, figures released by the Department for Energy and Climate Change showed that only 2.3 million smart meters had been installed in UK homes. This was well behind schedule to hit the government’s target to have 50 million in place by 2020. As of August 2018, 11 million SMETS1 devices were functioning normally, with 637,000 in \"dumb\" mode, according to the government. 42 million remained to be installed by the 2020 deadline.\n\nUntil 2018, the vast majority of smart meters being installed were \"first generation\" SMETS1 types. If a user of one of these meters switches electricity supplier, the meter’s smart functions are lost, and it must be read manually. \n\nSMETS2 meters allow easy switching and better remote reading, but only 450 had been installed (80 in households); by June 2018 over 1,000 had been installed, increasing to 2,000 in August.\n\nSMETS1 meters are not allowed to be installed after 13 January 2019.\n\nThe Home Area Network is implemented using ZigBee. This is the network your appliances use to communicate with your controller.\n\nCitizens Advice said in August 2018 that 80% of people with a smart meter were happy with them, but it had 1,000 calls in 2017 about problems, including first-generation smart meters losing their functionality, aggressive sales practices, and still having to send smart meter readings.\n\nRoss Anderson of the Foundation for Information Policy Research has criticised the UK's programme on grounds that it is unlikely to lower energy consumption, is rushed and expensive, and does not promote metering competition. Anderson writes, \"the proposed architecture ensures continued dominance of metering by energy industry incumbents whose financial interests are in selling more energy rather than less\", and urged ministers \"to kill the project and instead promote competition in domestic energy metering, as the Germans do – and as the UK already has in industrial metering. Every consumer should have the right to appoint the meter operator of their choice\".\n\nIn a 2011 submission to the Public Accounts Committee, Anderson wrote that Ofgem were \"making all the classic mistakes which have been known for years to lead to public-sector IT project failures\" and that the \"most critical part of the project - how smart meters will talk to domestic appliances so as to facilitate demand response - is essentially ignored.\"\n\nThe high number of SMETS1 meters installed has been criticised by Peter Earl, head of energy at the price comparison website comparethemarket.com. He said, \"The Government expected there would only be a small number of the first-generation of smart meters before Smets II came in, but the reality is there are now at least five million and perhaps as many as 10 million Smets I meters.\"\n\nUK smart meters use the mobile phone network to communicate, so they do not work properly when coverage is weak. A solution has been proposed but was not operational as of March 2017.\n\nIn March 2018 the National Audit Office, which watches over public spending, opened an investigation into the smart meter programme, which had cost £11bn by then, paid for by electricity users through higher bills.\n\nRoss Anderson and Alex Henney have written that \"Ed Miliband cooked the books\" to make the case for smart meters appear economically viable. They say that the first three cost-benefit analyses of residential smart meters found that it would cost more than it would save, but \"ministers kept on trying until they got a positive result... To achieve 'profitability' the previous government stretched the assumptions shamelessly\".\n\nAn economist at Ofgem who wanted to go public with his concerns about the smart meter programme was threatened with prosecution.\n\nOn July 20, 2006, California's energy regulators approved a program to roll out conventional meters retrofit with communications co-processor electronics to 9 million gas and electric household customers in the Northern California territory of Pacific Gas and Electric (PG&E). These meters report electricity consumption on an hourly basis. This enables PG&E to set pricing that varies by season and time-of-the day, rewarding customers who shift energy use to off-peak periods. The peak pricing program began on a voluntary basis and the full rollout is expected to take five years. The smart grid allows PG&E to give customers timing and pricing options for upload to the grid.\n\nThe largest municipal utility in the U.S., the Los Angeles Department of Water and Power (LADWP), has chosen to expand its advanced metering infrastructure (AMI) serving its commercial and industrial (C&I) customers. LADWP has already purchased 9,000. The utility's commercial and industrial customers may tailor their daily energy consumption around the data provided by the smart meters, thus creating potential for reducing their monthly electricity bill and, at a broader level, contributing to global energy conservation.\n\nCalifornia's utilities had nearly completed the rollout of tens of millions of smart meters in 2012. In January of that year they met strong opposition, including a moratorium in Santa Cruz based on health and privacy concerns.\n\nIn spring 2012, Baltimore Gas and Electric (BGE) of Maryland began installing or upgrading approximately two million electric and gas meters in every home and small business in their service area. This process was to take about three years to complete. The meters initially gave information to users on energy use. Some features, such as energy budgeting and tracking and personalized energy efficiency, were to be implemented by 2012, with further functions in 2013.\n\nAustin Energy, the nation's ninth-largest community-owned electric utility, with nearly 400,000 electricity customers in and around Austin, Texas, began deploying a two-way RF mesh network and approximately 260,000 residential smart meters in 2008. More than 165,000 two-way meters have been installed by spring 2009, and integration with AE's meter data management system is underway. A previous project in 2002 exchanged approximately 140,000 mechanical meters for smart meters at residential apartments, condominiums, and other high-meter-density locations.\n\nCenterpoint Energy in Houston, Texas, deployed smart meters to more than 2 million electricity customers in the Houston-Metro and Galveston service locations. Estimated completion of CenterPoint Energy's smart meter deployment was 2012. In October 2009, the U.S. Department of Energy awarded a $200 million grant for use in deployment of Centerpoint Energy's smart meter network.\n\nOncor Electric Delivery, based in Dallas, Texas, is deploying smart meters to more than three million customers in North Texas. Oncor’s full deployment was scheduled to be complete by the end of 2012. The Oncor Advanced Metering System (AMS) supports 15-minute-interval data, remote disconnects, and a Home Area Network (HAN) using ZigBee Smart Energy Protocol 1.0. The AMS supports text messages, pricing signals, and load control to home users through the Smart Meter Texas Portal, which is a joint project by Oncor, CenterPoint, and AEP Texas under the direction of the Texas Public Utility Commission.\n\nSan Antonio, Texas-based CPS Energy has launched a pilot program with 40,000 smart meters deployed as of the summer of 2011. CPS plans to complete installation of smart meters (electricity and gas) for all customers by the end of 2016. Each meter reports data to CPS every 15 minutes over a wireless network, making the data available to the customer through the CPS website.\n\nEl Paso Electric does not have any plans to implement Smart Grid meters as of April 2014.\n\nSeveral retail electric providers including TXU Energy and Direct Energy introduced rate plans that encourage smart meter users to shift load to off-peak hours. Excluding TNMP (PNM Resources), smart meter conversion rates for commercial and residential customers in utilities open to retail electric competition were in excess of 99% as of January 2015. Some utilities offer free electricity at night.\n\nFlorida-based Florida Power & Light began installing smart meters in 2009 in the Miami-Dade area for residential customers. All customers were expected to be completed by 2013. Individual counties are considering \"opt-out\" options, however, and to date, customers are being allowed to register on an \"opt-out\" registry in counties that have not begun.\n\nSix thousand end users in parts of Oklahoma and Arkansas are part of a program with in-home devices and automation systems conducted by the Oklahoma Gas and Electric (OG&E) company. The project costs $357 million, $130 million of which was funded by the federal government. The dynamic pricing program is opt-in and the data are also used to study consumer behavior.\n\nThe city of Duncan, Oklahoma, conducted a smart grid implementation to automatically collect electricity and water usage data from 9,000 electric meters and 12,000 water meters. Duncan’s project was funded by the American Recovery and Reinvestment Act and a 15-year, $14.2 million energy savings performance contract. Honeywell installed the smart meter network as part of a broader program mostly unrelated to smart meters. The program was purchased as part of an unusual $1.7 million savings guarantee.\n\nThe European Union has certain directives on smart meters that each member state must implement in its own legislation. According to Directive 2009/72/EC of the European Parliament and of the Council and Directive 2009/73/EC of the European Parliament and of the Council \"Member States are required to ensure the implementation of smart metering systems that assist the active participation of consumers in the electricity supply and gas supply markets\". There is a European Commission Web site, SmartRegions, promoting smart metering.\n\nA smart metering pilot project, called \"Linky\", is being conducted by Electricité Réseau Distribution France (ERDF) involving 300,000 clients supplied by 7,000 low-voltage transformers. In June 2008 ERDF awarded the AMM pilot project to a consortium managed by Atos Origin, including Actaris, Landis+Gyr, and Iskraemeco. The aim of the trial is to deploy 300,000 meters and 6k concentrators in two distinct geographic areas, the Indre-et-Loire (37) department and the Lyon urban region (69). This project affecting 1% of LV customers is a precursor to national deployment for 35 million clients in France. The experimentation phase started in March 2010. A key determining factor is the interoperability of the equipment of various suppliers. The general deployment phase, involving replacement of 35 million meters, started in December 2015 and continue through 2021.\n\nFrom 2010, various cities as Paris, Lyon, and Grasse decided to install smart individual sub meters (using AMR technology) in each apartment belonging to the city. This project has to purpose to improve the efficiency of the water management. Regarding Paris's area, it represents around 150.000 smart sub meter that been installed. \nThese installations are being conducted by C3E (Conseils en Environnement, Energie et EAU). Indeed, C3E is a unique consulting firm specialized in water system applications related to savings and management efficiency. Established in 2001, C3E is a pioneer in this sector and single reference in this field for nearly 10 years. C3E's customers include the largest communities in France, social landlords, trustees. C3E is also find as water engineering consultant in various international projects.\n\nOn 8 July 2016, the new “Digitisation of the Energy Turnaround Act” cleared the final legislative hurdle in the German Federal Council Bundesrat of Germany. The new law initiates the roll-out of smart meters and connected infrastructures in Germany and defines roles and tasks for market participants.\n\nThe new Act is based on the Third Energy Package introduced by the EU in 2009. The Directives of this Package require EU member states to equip at least 80% of consumers with intelligent metering systems by 2020, subject to a positive national commercial assessment of the roll-out.\n\nThe key objective of the new law is to facilitate the implementation of Smart meter and Smart Meter Gateway devices. In that respect, Germany implements the EU Directives 2009/72/EG and 2009/73/EG into German law. The Act introduces specific and detailed requirements for the design of the smart meter devices and for the transmission of data. The goal is to open up the energy market to digitalisation, while ensuring a high standard of data protection and ICT security.\n\nThe Smart Meters Operation Act (\"Messstellenbetriebsgesetz\") sets out, in 77 sections, rules on the marketing and use of Smart Meters and Smart Meter Gateways.\n\nThe Act introduced new regulated market roles, particularly the role of the Meter Operator, responsible the implementation, operation and maintenance of Smart Meters, with specific legal obligations in that role. The responsibility connected with the roll out of the Meter Operator rests on the energy supply grid operator (\"Versorgungsnetzbetreiber\"). Using a special public procurement procedure, they can transfer this position to a third party service provider. Another role is Smart Meter Gateway Administrator, responsible for the correct allocation and security of the data collected and delivered by the Smart Meter Gateway.\n\nThe Act defines technical requirements, particularly the reliability and security of energy measurement and the transmission of data. Compliance with the new rules is controlled and supervised by both the Federal Office for Information Security (\"Bundesamt für Sicherheit in der Informationstechnik\") and the Federal Network Agency (\"Bundesnetzagentur\").\n\nThe Act determines a roll-out plan regarding the installation of Smart Meters. The roll-out began in 2017 and continues until 2032. The process comprises different roll-out periods for different types of end consumers and plant operators, depending on the amount of energy consumption that they use and respectively input feed in. For some types of consumers and operators, the roll-out must be completed before the end of 2024.\n\nThe Act requires operators to equip consumers with more than 6,000 kWh yearly consumption, and plant operators with an installed capacity of more than 7 kW, with Smart Meters. Below these consumption levels, equipment is optional.\n\nManufacturers must supply the devices according to certification by the Federal Office for Information Security. The Federal Office for Information Security provides an overview of its current technical requirements on its website.\n\nUpon the Green Party becoming part of the coalition government in 2007, Eamon Ryan, the Minister for Communications, Energy and Natural Resources, pledged to introduce smart meters for every home in the Republic of Ireland within five years. In an interview the minister said he envisaged a situation where smart meters would use plug-in hybrid cars as storage for micro-generated renewable energy by intelligently diverting the energy into the car. A leading energy expert has expressed concerns that whatever system of smart metering arises in Ireland must give homeowners the possibility of automatically responding to fluctuating electricity prices by, for instance, buying electricity when at its cheapest, and selling micro-generated electricity from wind turbines or solar photovoltaic panels into the grid when the best price is available.\n\nItaly has already deployed smart electrical meters. Legislation forced gas utilities to deploy smart gas meters from large industrial consumers down to almost every residential customer by the end of 2016.\n\nRemote reading and management of smart gas meters must be fully independent of the existing system of smart electrical meters. The technology intended to be used is mainly radio-based. More information can be found on the webpage published by the authority.\n\nSmart Grid implementation is underway in Iran.\nFAHAM is the National Smart Metering Program. The functional, technical, security, economic, and general requirements of this project were published as a document after a longtime workgroup of various stakeholders including representative of grid operators, meter manufactures, communication providers, business layer software providers, domestic and international consultants. The procedure of producing this document was base on EPRI Methodology (IEC 62559). In these technical documents all of the business and functional use cases, the conceptual architecture, mandatory international standards for electric, water and gas metering systems(for all types of consumers), telecommunication requirements, system interfaces and security mandates are defined.\nThe ministry of energy decided to perform a pilot project called FAHAM-phase1 to test the technical and implementation challenges for implementing smart metering for all of the approximately thirty million electricity consumer.\n\nMalta is in the process of implementing smart meters in all commercial and private households. Enemalta, a governmental company responsible for electricity is responsible for the introduction of the smart meters. This occurred in phases and every meter in Malta was to be 'smart' by 2012. The cost was approximately 40 million euros, paid by the Ministry of Infrastructure, Technology, and Communication. The 'smart meters' being used in Malta are manufactured by IBM. A pilot project is currently underway and more than 5,000 are being installed. The installation is paid for by the government, i.e. the taxpayers.\n\nCommunication is a critical technological requirement for smart meters. Each meter must be able to reliably and securely communicate the information collected to some central location. Considering the varying environments and locations where meters are found, that problem can be daunting. Among the solutions proposed are: the use of cell and pager networks, satellite, licensed radio, combination licensed and unlicensed radio, and power line communication. Not only the medium used for communication purposes, but also the type of network used, is critical. As such, one would find: fixed wireless, wireless mesh network and wireless ad hoc networks, or a combination of the two. There are several other potential network configurations possible, including the use of Wi-Fi and other internet related networks. To date no one solution seems to be optimal for all applications. Rural utilities have very different communication problems from urban utilities or utilities located in difficult locations such as mountainous regions or areas ill-served by wireless and internet companies.\n\nIn addition to communication with the head-end network, smart meters may need to be part of a home area network, which can include an in-premises display and a hub to interface one or more meters with the head end. Technologies for this network vary from country to country, but include power line communication, wireless ad hoc network, and ZigBee.\n\nANSI C12.18 is an ANSI standard that describes a protocol used for two-way communications with a meter, mostly used in North American markets. The C12.18 standard is written specifically for meter communications via an ANSI Type 2 Optical Port, and specifies lower-level protocol details. ANSI C12.19 specifies the data tables that are used. ANSI C12.21 is an extension of C12.18 written for modem instead of optical communications, so it is better suited to automatic meter reading.\n\nIEC 61107 is a communication protocol for smart meters published by the IEC that is widely used for utility meters in the European Union. It is superseded by IEC 62056, but remains in wide use because it is simple and well-accepted. It sends ASCII data using a serial port. The physical media are either modulated light, sent with an LED and received with a photodiode, or a pair of wires, usually modulated by EIA-485. The protocol is half-duplex. IEC 61107 is related to, and sometimes wrongly confused with, the FLAG protocol. Ferranti and Landis+Gyr were early proponents of an interface standard that eventually became a sub-set of IEC1107.\n\nThe Open Smart Grid Protocol (OSGP) is a family of specifications published by the European Telecommunications Standards Institute (ETSI) used in conjunction with the ISO/IEC 14908 control networking standard for smart metering and smart grid applications. Millions of smart meters based on OSGP are deployed worldwide. Numerous major security flaws in the OSGP protocol have been identified.\n\nThere is a growing trend toward the use of TCP/IP technology as a common communication platform for Smart Meter applications, so that utilities can deploy multiple communication systems, while using IP technology as a common management platform. A universal metering interface would allow for development and mass production of smart meters and smart grid devices prior to the communication standards being set, and then for the relevant communication modules to be easily added or switched when they are. This would lower the risk of investing in the wrong standard as well as permit a single product to be used globally even if regional communication standards vary.\n\nSome smart meters may use a test IR LED to transmit non-encrypted usage data that bypasses meter security by transmitting lower level data in real time.\n\nThe other critical technology for smart meter systems is the information technology at the utility that integrates the Smart Meter networks with the utility applications, such as billing and CIS. This includes the Meter Data Management system.\n\nIt also is important for smart grid implementations that power line communication (PLC) technologies used within the home over a Home Area Network (HAN), are standardized and compatible. The HAN allows HVAC systems and other household appliances to communicate with the smart meter, and from there to the utility. Currently there are several broadband or narrowband standards in place, or being developed, that are not yet compatible. To address this issue, the National Institute for Standards and Technology (NIST) established the PAP15 group, which studies and recommends coexistence mechanisms with a focus on the harmonization of PLC standards for the HAN. The objective of the group is to ensure that all PLC technologies selected for the HAN coexist as a minimum. The two main broadband PLC technologies selected are the HomePlug AV / IEEE 1901 and ITU-T G.hn technologies. Technical working groups within these organizations are working to develop appropriate coexistence mechanisms. The HomePlug Powerline Alliance has developed a new standard for smart grid HAN communications called the HomePlug Green PHY specification. It is interoperable and coexistent with the widely deployed HomePlug AV technology and with the new IEEE 1901 global standard and is based on Broadband OFDM technology. ITU-T commissioned in 2010 a new project called G.hnem, to address the home networking aspects of energy management, built upon existing Low Frequency Narrowband OFDM technologies.\n\nGoogle.org's PowerMeter, until its demise in 2011, was able to use a smart meter for tracking electricity usage, as can eMeter's Energy Engage as in, for example, the PowerCentsDC(TM) demand response program. \n\n\"Advanced Metering Infrastructure\" (AMI) refers to systems that measure, collect, and analyze energy usage, and communicate with metering devices such as electricity meters, gas meters, heat meters, and water meters, either on request or on a schedule. These systems include hardware, software, communications, consumer energy displays and controllers, customer associated systems, meter data management software, and supplier business systems.\n\nGovernment agencies and utilities are turning toward advanced metering infrastructure (AMI) systems as part of larger “smart grid” initiatives. AMI extends automatic meter reading (AMR) technology by providing two way meter communications, allowing commands to be sent toward the home for multiple purposes, including time-based pricing information, demand-response actions, or remote service disconnects. Wireless technologies are critical elements of the neighborhood network, aggregating a mesh configuration of up to thousands of meters for back haul to the utility’s IT headquarters.\n\nThe network between the measurement devices and business systems allows collection and distribution of information to customers, suppliers, utility companies, and service providers. This enables these businesses to participate in demand response services. Consumers can use information provided by the system to change their normal consumption patterns to take advantage of lower prices. Pricing can be used to curb growth of peak demand consumption.\nAMI differs from traditional automatic meter reading (AMR) in that it enables two-way communications with the meter. Systems only capable of meter readings do not qualify as AMI systems.\n\nSome groups have expressed concerns regarding the cost, health, fire risk, security and privacy effects of smart meters and the remote controllable \"kill switch\" that is included with most of them. Many of these concerns regard wireless-only smart meters with no home energy monitoring or control or safety features. Metering-only solutions, while popular with utilities because they fit existing business models and have cheap up-front capital costs, often result in such \"backlash\". Often the entire smart grid and smart building concept is discredited in part by confusion about the difference between home control and home area network technology and AMI. The attorneys general of both Illinois and Connecticut have stated that they do not believe smart meters provide any financial benefit to consumers, however, the cost of the installation of the new system is absorbed by those customers.\n\nSmart meters expose the power supply to cyberattacks that could lead to power outages, both by cutting off people's electricity and by overloading the grid. However many cyber security experts state that smart meters of UK and Germany have a relatively high cybersecurity and that any such attack there would thus require extraordinarily high efforts or financial resources.\n\nImplementing security protocols that protect these devices from malicious attacks been problematic, due to their limited computational resources and long operational life.\n\nThe current version of IEC 62056 includes the possibility to encrypt, authenticate, or sign the meter data.\n\nOne proposed smart meter data verification method involves analyzing the network traffic in real time to detect anomalies using an Intrusion Detection System (IDS) By identifying exploits as they are being leveraged by attackers, an IDS mitigates the suppliers' risks of energy theft by consumers and denial-of-service attacks by hackers. Energy utilities must choose between a centralized IDS, embedded IDS, or dedicated IDS depending on the individual needs of the utility. Researchers have found that for a typical advanced metering infrastructure, the centralized IDS architecture is superior in terms of cost efficiency and security gains.\n\nIn the United Kingdom, the Data Communication Company, which transports the commands from the supplier to the smart meter, performs an additional anomality check on commands issued (and signed) by the energy supplier.\n\nAccording to a report by Brian Krebs, in 2009 a Puerto Rico electricity supplier asked the FBI to investigate large-scale thefts of electricity related to its smart meters. The FBI found that former employees of the power company and the company that made the meters were being paid by consumers to reprogram the devices to show incorrect results, as well as teaching people how to do it themselves.\nMost health concerns about the meters arise from the pulsed radiofrequency (RF) radiation emitted by wireless smart meters.\n\nMembers of the California State Assembly asked the California Council on Science and Technology (CCST) to study the issue of potential health impacts from smart meters. The CCST report in April 2011 found no health impacts, based both on lack of scientific evidence of harmful effects from radio frequency (RF) waves and that the RF exposure of people in their homes to smart meters is likely to be minuscule compared to RF exposure to cell phones and microwave ovens.\n\nIssues surrounding smart meters causing fires have also been reported, particularly involving the manufacturer Sensus. In 2012. PECO Energy Company replaced the Sensus meters it had deployed in the Philadelphia region after reports that a number of the units had overheated and caused fires. In July 2014, SaskPower, the province-run utility company of the Canadian province of Saskatchewan, halted its roll-out of Sensus meters after similar, isolated incidents were discovered. Shortly afterward, Portland General Electric announced that it would replace 70,000 smart meters that had been deployed in the state of Oregon after similar reports. The company noted that it had been aware of the issues since at least 2013, and that they were limited to certain models it had installed between 2010 and 2012. On July 30, 2014, after a total of eight recent fire incidents involving the meters, SaskPower was ordered by the Government of Saskatchewan to immediately end its smart meter program, and remove the 105,000 smart meters it had installed.\n\nOne technical reason for privacy concerns is that these meters send detailed information about how much electricity is being used each time. More frequent reports provide more detailed information. Infrequent reports may be of little benefit for the provider, as it doesn't allow as good demand management in the response of changing needs for electricity. On the other hand, very frequent reports would allow the utility company to infer behavioral patterns for the occupants of a house, such as when the members of the household are probably asleep or absent. Current trends are to increase the frequency of reports. A solution that benefits both provider and user privacy would be to adapt the interval dynamically. Another solution involves an energy storage installed at the household used to reshape the energy consumption profile. In British Columbia the electric utility is government-owned and as such must comply with privacy laws that prevent the sale of data collected by smart meters; many parts of the world are serviced by private companies that are able to sell their data. In Australia debt collectors can make use of the data to know when people are at home. Used as evidence in a court case in Austin, Texas, police agencies secretly collected smart meter power usage data from thousands of residences to determine which used more power than \"typical\" to identify marijuana growing operations.\n\nSmart meter power data usage patterns can reveal much more than how much power is being used. Research has demonstrated that smart meters sampling power levels at two-second intervals can reliably identify when different electrical devices are in use.\n\nRoss Anderson has written about privacy concerns. He writes \"It is not necessary for my meter to tell the power company, let alone the government, how much I used in every half-hour period last month\"; that meters can provide \"targeting information for burglars\"; that detailed energy usage history can help energy companies to sell users exploitative countracts; and that there may be \"a temptation for policymakers to use smart metering data to target any needed power cuts.\"\n\nReviews of smart meter programs, moratoriums, delays, and \"opt-out\" programs are some responses to the concerns of customers and government officials. In response to residents who did not want a smart meter, in June 2012 a utility in Hawaii changed their smart meter program to \"opt out\". The utility said that once the smart grid installation project is nearing completion, KIUC may convert the deferral policy to an opt-out policy or program and may charge a fee to those members to cover the costs of servicing the traditional meters. Any fee would require approval from the Hawaii Public Utilities Commission.\n\nAfter receiving numerous complaints about health, hacking, and privacy concerns with the wireless digital devices, the Public Utility Commission of the US state of Maine voted to allow customers to opt out of the meter change at a cost of $12 a month. In Connecticut, another US state to consider smart metering, regulators declined a request by the state's largest utility, Connecticut Light & Power, to install 1.2 million of the devices, arguing that the potential savings in electric bills do not justify the cost. CL&P already offers its customers time-based rates. The state's Attorney General George Jepsen was quoted as saying the proposal would cause customers to spend upwards of $500 million on meters and get few benefits in return, a claim that Connecticut Light & Power disputed.\n\nThere are questions whether electricity is or should be primarily a \"when you need it\" service where the inconvenience/cost-benefit ratio of time shifting of loads is poor. In the Chicago area, Commonwealth Edison ran a test installing smart meters on 8,000 randomly selected households together with variable rates and rebates to encourage cutting back during peak usage. In the \"Crain's Chicago Business\" article \"Smart grid test underwhelms. In pilot, few power down to save money.\", it was reported that fewer than 9% exhibited any amount of peak usage reduction and that the overall amount of reduction was \"statistically insignificant\". This was from a report by the Electric Power Research Institute, a utility industry think tank who conducted the study and prepared the report. Susan Satter, senior assistant Illinois attorney general for public utilities said \"It's devastating to their plan...The report shows zero statistically different result compared to business as usual.\" \n\nBy 2016, the 7 million smart meters in Texas had not persuaded many people to actually check their energy data as the process was too difficult.\n\nSmart meters can allow real-time pricing, and in theory this could help smooth power consumption as consumers adjust their demand in response to price changes. However, modelling by researchers at the University of Bremen suggests that in certain circumstances, \"Power demand fluctuations are not dampened but amplified instead.\"\n\nIn 2013, \"Take Back Your Power\", an independent Canadian documentary directed by Josh del Sol was released describing \"dirty electricity\" and the aforementioned issues with smart meters. The film explores the various contexts of the health, legal, and economic concerns, and features narration from mayor of Peterborough, Ontario, Daryl Bennett, as well as American researcher De-Kun Li, journalist Blake Levitt, and Dr. Sam Milham. It won a Leo Award for best feature-length documentary and the Annual Humanitarian Award from Indie Fest the following year.\n\n"}
{"id": "47663720", "url": "https://en.wikipedia.org/wiki?curid=47663720", "title": "Soda locomotive", "text": "Soda locomotive\n\nSoda locomotives were a variant of fireless locomotives, in which steam was raised in a boiler, expanded through cylinders in the usual way, and then condensed in a tank of caustic soda that surrounded the boiler. Dissolving water in caustic soda liberated heat, which generated more steam from the boiler, until the caustic soda became too dilute to release heat at a useful temperature.\n\nThese closed-loop steam engines had no firebox. The boiler was jacketed by a container loaded with about 5 tons of caustic soda (sodium hydroxide). When water or steam came in contact with the caustic soda, it would generate heat — enough to actually run the boiler and generate more steam. Steam emanating from the boiler would be fed through pistons to propel the locomotive forward, and the exhaust steam from the pistons would be fed into the caustic soda to continue the cycle. These vehicles were virtually silent, because the steam was not released into the atmosphere.\n\nA soda locomotive could run for several hours, but eventually the soda would become diluted and wouldn't produce enough heat to continue generating steam. For reconcentrating, the caustic soda was either transferred out of the boiler of the locomotive and boiled in open vats, or, rather more conveniently, by injecting superheated steam at a high enough temperature to boil off the water in solution. A stationary boiler would be hooked up and feed superheated steam through the soda to boil off the water and effectively recharge the soda.\n\nThese locomotives were always called \"soda locomotives\" (sodium carbonate) although \"caustic soda locomotives\" (sodium hydroxide) would be a more precise description. The misleading terminology was most likely used, to increase their acceptance by the public, which was used to using washing soda but might have been frightened by the nasty alkali burns sitting next to several tons of hot caustic soda.\n\nOther salts such as calcium chloride could also be used.\n\n\n\n"}
{"id": "11096735", "url": "https://en.wikipedia.org/wiki?curid=11096735", "title": "Static light scattering", "text": "Static light scattering\n\nStatic light scattering is a technique in physical chemistry that measures the intensity of the scattered light to obtain the average molecular weight \"M\" of a macromolecule like a polymer or a protein in solution. Measurement of the scattering intensity at many angles allows calculation of the root mean square radius, also called the radius of gyration \"R\". By measuring the scattering intensity for many samples of various concentrations, the second virial coefficient \"A\", can be calculated.\n\nStatic light scattering is also commonly utilized to determine the size of particle suspensions in the sub-μm and supra-μm ranges, via the Lorenz-Mie(see Mie scattering) and Fraunhofer diffraction formalisms, respectively.\n\nFor static light scattering experiments, a high-intensity monochromatic light, usually a laser, is launched in a solution containing the macromolecules. One or many detectors are used to measure the scattering intensity at one or many angles. The angular dependence is required to obtain accurate measurements of both molar mass and size for all macromolecules of radius above 1–2% the incident wavelength. Hence simultaneous measurements at several angles relative to the direction of incident light, known as multi-angle light scattering (MALS) or multi-angle laser light scattering (MALLS), is generally regarded as the standard implementation of static light scattering. Additional details on the history and theory of MALS may be found in multi-angle light scattering.\n\nTo measure the average molecular weight directly without calibration from the light scattering intensity, the laser intensity, the quantum efficiency of the detector, and the full scattering volume and solid angle of the detector needs to be known. Since this is impractical, all commercial instruments are calibrated using a strong, known scatterer like toluene since the Rayleigh ratio of toluene and a few other solvents were measured using an absolute light scattering instrument.\n\nFor a light scattering instrument composed of many detectors placed at various angles, all the detectors need to respond the same way. Usually detectors will have slightly different quantum efficiency, different gains and are looking at different geometrical scattering volumes. In this case a normalization of the detectors is absolutely needed. To normalize the detectors, a measurement of a pure solvent is made first. Then an isotropic scatterer is added to the solvent. Since isotropic scatterers scatter the same intensity at any angle, the detector efficiency and gain can be normalized with this procedure. It is convenient to normalize all the detectors to the 90° angle detector.\n\nformula_1\n\nwhere \"I(90)\" is the scattering intensity measured for the Rayleigh scatterer by the 90° angle detector.\n\nThe most common equation to measure the weight-average molecular weight, \"M\", is the Zimm equation (the right hand side of the Zimm equation is provided incorrectly in some texts, as noted by Hiemenz and Lodge):\n\nformula_2\n\nwhere\n\nformula_3\n\nand\n\nformula_4\n\nwith\n\nformula_5\n\nand the scattering vector for vertically polarized light is\n\nformula_6\n\nwith \"n\" the refractive index of the solvent, λ the wavelength of the light source, \"N\" Avogadro's number (6.022x10), \"c\" the solution concentration, and d\"n\"/d\"c\" the change in refractive index of the solution with change in concentration. The intensity of the analyte measured at an angle is \"I(θ)\". In these equation the subscript A is for analyte (the solution) and T is for the toluene with the Rayleigh Ratio of toluene, \"R\" being 1.35x10 cm for a HeNe laser. As described above, the radius of gyration, \"R\", and the second virial coefficient, \"A\", are also calculated from this equation. The refractive index increment \"dn/dc\" characterizes the change of the refractive index \"n\" with the concentration \"c\", and can be measured with a differential refractometer.\n\nA Zimm plot is built from a double extrapolation to zero angle and zero concentration from many angle and many concentration measurements. In the most simple form, the Zimm equation is reduced to:\n\nformula_7\n\nfor measurements made at low angle and infinite dilution since \"P(0) = 1\".\n\nThere are typically a number of analyses developed to analyze the scattering of particles in solution to derive the above named physical characteristics of particles. A simple static light scattering experiment entails the average intensity of the sample that is corrected for the scattering of the solvent will yield the Rayleigh ratio, \"R\" as a function of the angle or the wave vector \"q\" as follows:\n\nThe scattered intensity can be plotted as a function of the angle to give information on the \"R\" which can simply be calculated using the Guinier approximation as follows:\n\nformula_8\n\nwhere \"ln(ΔR(θ)) = lnP(θ)\" also known as the form factor with \"q = 4πnsin(θ/2)/λ\". Hence a plot of the corrected Rayleigh ratio, \"ΔR(θ) vs sin(θ/2)\" or \"q\" will yield a slope \"R/3\". However, this approximation is only true for \"qR < 1\". Note that for a Guinier plot, the value of \"dn/dc\" and the concentration is not needed.\n\nThe Kratky plot is typically used to analyze the conformation of proteins, but can be used to analyze the random walk model of polymers. A Kratky plot can be made by plotting \"sin(θ/2)ΔR(θ) vs sin(θ/2)\" or \"qΔR(θ) vs q\".\n\nFor polymers and polymer complexes which are of a monodisperse nature (formula_9) as determined by static light scattering, a Zimm plot is a conventional means of deriving the parameters such as \"R\", molecular mass \"M\" and the second virial coefficient \"A\".\n\nOne must note that if the material constant \"K\" is not implemented, a Zimm plot will only yield \"R\". Hence implementing \"K\" will yield the following equation:\n\nformula_10\n\nExperiments are performed at several angles, which satisfy condition formula_11 and at least 4 concentrations. Performing a Zimm analysis on a single concentration is known as a partial Zimm analysis and is only valid for dilute solutions of strong point scatterers. The partial Zimm however, does not yield the second virial coefficient, due to the absence of the variable concentration of the sample. More specifically, the value of the second virial coefficient is either assumed to equal zero or is inputted as a known value in order to perform the partial Zimm analysis.\n\nStatic light scattering assumes that each detected photon has only been scattered exactly once. Therefore, analysis according to the calculations stated above will only be correct if the sample has been diluted sufficiently to ensure that photons are not scattered multiple times by the sample before being detected. Accurate interpretation becomes exceedingly difficult for systems with non-negligible contributions from multiple scattering. In many commercial instruments where analysis of the scattering signal is automatically performed, the error may never be noticed by the user. Particularly for larger particles and those with high refractive index contrast, this limits the application of standard static light scattering to very low particle concentrations. On the other hand, for soluble macromolecules that exhibit a relatively low refractive index contrast versus the solvent, including most polymers and biomolecules in their respective solvents, multiple scattering is rarely a limiting factor even at concentrations that approach the limits of solubility.\n\nHowever, as shown by Schaetzel, it is possible to suppress multiple scattering in static light scattering experiments via a cross-correlation approach. The general idea is to isolate singly scattered light and suppress undesired contributions from multiple scattering in a static light scattering experiment. Different implementations of cross-correlation light scattering have been developed and applied. Currently, the most widely used scheme is the so-called 3D-dynamic light scattering method. The same method can also be used to correct dynamic light scattering data for multiple scattering contributions.\n\nSamples that change their properties after dilution may not be analyzed via static light scattering in terms of the simple model presented here as the Zimm equation. A more sophisticated analysis known as 'composition-gradient static (or multi-angle) light scattering' (CG-SLS or CG-MALS) is an important class of methods to investigate protein–protein interactions, colligative properties and other macromolecular interactions as it yields, in addition to size and molecular weight, information on the affinity and stoichiometry of molecular complexes formed by one or more associating macromolecular/biomolecular species. In particular, static light scattering from a dilution series may be analyzed to quantify self-association, reversible oligomerization and non-specific attraction or repulsion, while static light scattering from mixtures of species may be analyzed to quantify hetero-association.\n\n\n"}
{"id": "19221718", "url": "https://en.wikipedia.org/wiki?curid=19221718", "title": "Subhelic arc", "text": "Subhelic arc\n\nA subhelic arc is a rare halo, formed by internal reflection through ice crystals, that curves upwards from the horizon and touches the tricker arc above the anthelic point. Subhelic arcs are a result of ray entrance and exit through prism end faces with two intermediate internal reflections. \n\nA subhelic arc is formed when sun rays enter one end face of an ice crystal in singly oriented columns and Parry columns, reflect off two of the crystals side faces, and exits the crystal through the opposite end face. The ray leave the crystal in the exact opposite angle, resulting in a net deviation angle of 120°, the angle for the formation of 120° parhelia. \n\nThe subhelic arc touches the top of the tricker arc, an indication the two have closely related ray paths.\n\nThe subhelic arc crosses the parhelic circle in an acute angle, and at a sun elevation of 27° it passes exactly through the 120° parhelion.\n\n\n\n"}
{"id": "18077304", "url": "https://en.wikipedia.org/wiki?curid=18077304", "title": "Tellervo", "text": "Tellervo\n\nTellervo is the Finnish goddess of forests. She was the daughter of Tapio, an East Finnish forest spirit.\n"}
{"id": "30832113", "url": "https://en.wikipedia.org/wiki?curid=30832113", "title": "United States hydrogen policy", "text": "United States hydrogen policy\n\nThe principle of a fuel cell was discovered by Christian Friedrich Schönbein in 1838, and the first fuel cell was constructed by Sir William Robert Grove in 1839. The fuel cells made at this time were most similar to today's phosphoric acid fuel cells. Most hydrogen fuel cells today are of the proton exchange membrane (PEM) type. A PEM converts the chemical energy released during the electrochemical reaction of hydrogen and oxygen into electrical energy. The Energy Policy Act of 1992 was the first national legislation that called for large-scale hydrogen research. A five-year program was conducted that investigated the production of hydrogen from renewable energy sources and the feasibility of existing natural gas pipelines to carry hydrogen. It also called for the research into hydrogen storage systems for electric vehicles and the development of fuel cells suitable to power an electric motor vehicle.\n\nHydrogen is an energy carrier and can be used to store and deliver energy as needed. When used in a fuel cell, the hydrogen atom dissociates into a positively charged hydrogen ion and a negatively charged electron which is diverted to an electric load. A fuel cell can be used to power anything in much the same way that batteries are used. According to the U.S. Department of Energy, \"Eventually hydrogen will join electricity as the major energy carrier, supplying every end-use energy need in the economy, including transportation, central and distributed electric power, portable power, and combined heat and power for buildings and industrial processes.\"\n\nHydrogen fuel cell vehicles are still in the pre-production stage of development. While hydrogen vehicles exist worldwide in many forms, there is no consumer-level vehicle available for purchase off the lot. The Honda FCX Clarity is the only vehicle that exists for consumers and currently can only be leased for three years in the Torrance, Santa Monica, and Irvine areas of California for $600/month. Cars are typically refilled with a couple of kilograms over 5–10 minutes.\n\nOne major vehicular application for hydrogen fuel cells are forklift fleets in manufacturing facilities or distribution centers. Forklifts can not burn gasoline or diesel fuel inside factories, since the fumes present health hazards, so most indoor fleets rely on lead–acid batteries. Hydrogen presents an advantage over batteries in this case, since batteries require a long charging time and need to be replaced periodically. A hydrogen fuel cell will last much longer than a lead-acid battery pack, and hydrogen tanks can be refilled in several minutes. Fuel-cell vehicles cost more, but companies are reporting lowered logistical costs due to the increased work hours that a hydrogen-powered forklift can provide when compared to battery-powered forklifts. By 2013, there were more than 4,000 fuel cell forklifts in the US.\n\nHydrogen fuel cells also have the potential to replace or supplement utility-bought electricity by supplying energy on-site in the form of a stationary power device. Major corporations like Ebay, Google, Wal-Mart, Coca-Cola, FedEx, Adobe and Sierra Nevada have been using some form of fuel cells for several years and are reporting hundreds of thousands of dollars worth of energy savings every year.\n\nThe second Bush administration showed much interest in developing hydrogen fuel cell technologies within the transportation sector. This interest was mainly driven by the desire to decrease the United States' dependence on foreign oil and reduce the environmental impact of the transportation sector. The initiative to promote fuel cell technology was announced by former President Bush in his State of the Union Address in 2003. Former President Bush stated: \"With a new national commitment, our scientists and engineers will overcome obstacles ... so that the first car driven by a child born today could be powered by hydrogen, and pollution-free. Join me in this important innovation to make our air significantly cleaner, and our country much less dependent on foreign sources of energy.\"\n\nAlong with this new national commitment, a Hydrogen Posture Plan was created to begin to map the future of hydrogen technology research, development and demonstration. To accelerate research, development, and demonstration former President Bush announced plans to appropriate $1.2 billion.\n\nOn May 11, 2009 President Barack Obama effectively reduced funds and the Bush Administrations' attempt to advance fuel cell vehicle technology and commercialization at a fast pace. The elimination of this appropriation of funds was said to have saved $100 million per year. Department of Energy Secretary Steven Chu said the technology and improvements necessary for deployment of fuel cell vehicles were most likely to not be commercially ready or economically viable after 10 to 15 years of research and development. A more detailed table of public hydrogen research and development funding is provided by the Department of Energy: Hydrogen Program.\n\nIn March 2006, the FreedomCAR & Fuel Partnership devised a document that stated the Partnership's Plans. The FreedomCAR & Fuel Partnership was formed in 2003 as an expansion of the existing FreedomCAR Partnership. The FreedomCAR Partnership Plan was created by former Secretary of Energy Spencer Abraham and senior executives of DaimlerChrysler Corporation, Ford Motor Company, and General Motors Corporation. The partners consist of the United States Department of Energy (DOE), BP America, Chevron Corporation, ConocoPhillips, Exxon Mobil Corporation, Shell Hydrogen LLC, and the United States Council for Automotive Research (USCAR)—a legal partnership among DaimlerChrysler Corporation, Ford Motor Company, and General Motors Corporation.\n\nThe FreedomCAR & Fuel Partnership was created to stimulate research and development for proposed future transportation sector technologies with the goal of minimizing foreign oil dependence. The FreedomCAR & Fuel Partnership allowed for the Cooperative Automotive Research group to \"examine and advance the pre-competitive, high-risk research needed to develop the component and infrastructure technologies necessary to enable a full range of affordable cars and light trucks, and the fueling infrastructure for them that will reduce the dependence of the nation's personal transportation system on imported oil and minimize harmful vehicle emissions, without sacrificing freedom of mobility and freedom of vehicle choice.\" Along with other high risk technologies, hydrogen fuel cell vehicle technology was identified as potential venue for research and development.\n\nThe purposes of Title VIII are: \"(1) to enable and promote comprehensive development, demonstration, and commercialization of hydrogen and fuel cell technology in partnership with industry; (2) to make critical public investments in building strong links to private industry, institutions of higher education, National Laboratories, and research institutions to expand innovation and industrial growth; (3) to build a mature hydrogen economy that creates fuel diversity in the massive transportation sector of the United States; (4) to sharply decrease the dependency of the United States on imported oil, eliminate most emissions from the transportation sector, and greatly enhance our energy security; and (5) to create, strengthen, and protect a sustainable national energy economy.\"\n\nThe Energy Policy Act of 2005 calls for a wide-reaching research and development program on technologies relating to the production, purification, distribution, storage, and use of hydrogen energy, fuel cells, and related infrastructure with the goal of demonstrating and commercializing the use of hydrogen for transportation, utility, industrial, commercial, and residential applications. The Secretary is directed to collaborate with private industry, academic institutions and federal agencies to address the many issues related to hydrogen. By 2015 automakers must have made a commitment to offer hydrogen fuel cell vehicles in the mass consumer market and by model year 2020, hydrogen fuel cell vehicles must have achieved higher fuel economy, lower emissions, and the equivalent or improved safety capability of comparable light duty vehicles of model year 2005. The infrastructure goals are similar. The goals of the program are to enable a commitment by 2015 that will lead to infrastructure by 2020 which provides safe refueling capability and widespread availability of hydrogen from domestic energy sources. The act also appropriates funds for hydrogen supply and fuel cell technologies up to year 2020, yet the amounts for each year from 2011 through 2020 are \"as necessary\".\n\nSection 806 calls for the establishment of a \"Fuel Cell Technical Task Force\" chaired by the Secretary and with representatives from several Federal Agencies. The Task Force is responsible for planning a safe, economical and environmentally sound hydrogen infrastructure, increasing the use of fuel cells in government, increasing the use of hydrogen in distributed power generation, establishing uniform hydrogen codes, standards and safety protocols, and defining vehicle hydrogen fuel system integrity safety performance.\n\nSection 807 establishes \"The Hydrogen Technical and Fuel Cell Advisory Committee\" which will contain between 12 and 25 members representing domestic industry, academia, professional societies, government agencies, Federal laboratories, previous advisory panels, and financial, environmental, and other appropriate organizations. The committee is required to review and make recommendations to the Secretary on all things related to hydrogen energy and fuel cells and the Secretary is required to transmit a biennial report to Congress describing these recommendations.\n\nSection 808 calls for the funding of a limited number of demonstration projects that involve using hydrogen at existing facilities, depend on reliable power from hydrogen, lead to the replication of hydrogen technologies and draw such technologies into the marketplace, include vehicle, portable and stationary demonstrations, raise awareness of hydrogen technology among the public, and facilitate identification of optimum hydrogen technology compared to alternatives. The funding for the demonstrations described in this section will come in the form of grants which are scheduled to continue through year 2020.\n\nSection 812 establishes a commitment to explore the production of hydrogen using solar and wind technologies. It calls for 5 projects in each that will develop systems that may be used for the production of electricity or hydrogen, or both in conjunction.\n\nLow-Cost Renewable Hydrogen And Infrastructure For Vehicle Propulsion\n\nEstablishes a research, development, and demonstration program for universities and institutions to determine the feasibility of using hydrogen in light-weight vehicles and the integration of the associated infrastructure using off-the-shelf technologies. The universities and institutions must have expertise in testing hydrogen and methane-powered vehicles. By 2007, a vehicle must have been developed that has a curb weight of 2000 lbs or less before modifications that: operates solely on hydrogen, qualifies as a light-duty passenger vehicle and uses hydrogen produced from water using only solar energy.\n\nH-Prize\n\nThis section amends the Energy Policy Act of 2005 in which it adds a new subsection. The Secretary is required to carry out a program to competitively award cash prizes to advance the research, development, demonstration and commercial application of hydrogen energy technologies. The prize competitions are to be widely advertised to attract individuals, universities, and small and large businesses and must include announcements in the Federal Register. The categories that projects must adhere to are as follows: (1) advancements in technologies, components, or systems related to hydrogen production, storage, distribution, utilization; (2) prototypes of hydrogen-powered vehicles or other hydrogen-based products that best meet or exceed objective performance criteria, such as the completion of a race over a certain distance or terrain or generation of energy at certain levels of efficiency and; (3) transformational changes in technologies for the distribution or production of hydrogen that meet or exceed far-reaching objective criteria, which shall include minimal carbon emissions and which may include cost criteria designed to facilitate the eventual market success of a winning technology.\n\nPermitting Hydrogen Facilities\n\n\nThere are several federal incentives and laws regarding hydrogen in the transportation sector. They can be found on the Alternative Fuels and Advanced Vehicles Data Center with a brief description of each and the reference numbers to access the legislation. The table below links you directly to several federal codes, laws, and legislation for various hydrogen technologies.\n\nSeveral states have energy acts that establish provisions for tax exemptions and corporate income tax credits aimed at promoting infrastructure development that supports hydrogen stationary power technologies. In addition, there are production tax credits that provide corporate income tax credits based on the amount of electricity produced from stationary hydrogen power sources. All state policies, including incentives, tax credits, grants, RPS, interconnection standards, initiatives, and partnerships about stationary hydrogen fuel cell installations can be found on the State Fuel Cell and Hydrogen Database created by FuellCells.org. FuelCells.org also provides a fuel cell Policy Activity Wrapup for fuel cells and hydrogen for the year 2010. This wrap-up includes 2010 legislation and policy only. Several of the incentives and policies involving stationary hydrogen power are explained below.\n\nColorado: Renewable Energy Standard increased. This new bill boosts Colorado's renewable portfolio standard percentages to achieve 30% renewable generation by 2020. A fuel cell using hydrogen derived from an eligible energy resource is an eligible electric generation technology.\n\nDistrict of Columbia: Net metering cap is raised. Residential or commercial customers with systems powered by renewable energy sources, combined heat and power, fuel cells and microturbines are eligible to net meter up to a maximum capacity of 1 megawatts, raised from 100 kilowatts.\n\nMaryland: Legislation adds fuel cells as eligible net metering resource. House Bill 821 was passed in May 2010, adding fuel cells among the list of eligible customer-generators for net energy metering. System size is limited to 2 megawatts.\n\nNew York: \nRenewable Portfolio Standards: incentives for fuel cell projects. The New York Public Service Commission (NYPSC) made available $30 million in incentives to encourage customer sited projects for large scale projects involving fuel cells. The incentives are generally granted only for installed capacity not exceeding the customer's electrical load. The total value of incentives is capped at $50,000 for systems smaller than 25 kW and $1 million for larger systems.\n\nNet metering and interconnection standards updated. The NYPSC approved tariff filings of the six investor owned utilities in New York to encourage the installation of residential micro CHP and fuel cell electric generation systems that enable homeowners to sell excess power to the utility. New Yorks' net metering rules permit residential fuel cell and CHP installations of up to 10 kW each.\n\nOklahoma: Renewable energy goal established. Oklahoma established a goal for electric utilities in the state to have 15% of the total installed generation capacity to be derived from renewable sources by 2015. Eligible renewable resources include wind, solar, hydropower, hydrogen, and several others.\n\nWest Virginia: Net metering standards implemented. Net metering is available to all retail electricity customers. System capacity limits vary depending on the customer type and the electric utility type. Systems that generate electricity using alternative resources, including hydrogen, are allowed.\n\nWhile the federal government has several hydrogen policies in place for both stationary and mobile hydrogen applications, the states also have their own individual incentives and laws. A database that lists the incentives, laws, and regulations state by state can be found on the U.S. Department of Energy's Alternative Fuels and Advanced Vehicle Data Center's website. The website provides the various laws as well as the statute numbers of the legislation that made the laws active. The table below lists the state incentives, laws, and regulations for hydrogen fuel in the transportation sector state by state. The links will take you to the actual state legislation that codified the laws.\n\nThe US federal government has created several programs to promote alternative fuels including hydrogen.\n\nThe US Department of Energy created Clean Cities to, \"advance the energy, economic, and environmental security of the United States by supporting local initiatives to adopt practices that reduce the use of petroleum in the transportation sector\". Clean Cities performs these duties through a network of more than 80 offices that develop public/private partnerships to promote alternative fuels, advanced vehicles, fuel blends, hybrid vehicles, and idle reduction. They also provide information about financial opportunities, coordinate technical assistance projects, update and maintain energy databases, publish fact sheets, newsletters, and related technical and informational material.\n\nThe State Energy Program (SEP) provides grants to states to assist in designing, developing, and implementing renewable energy and energy efficient programs. Each states energy office gets SEP funding and manages all SEP funded projects. For more information about the SEP, including project descriptions, visit the SEP website.\n\nClean Ports USA is an incentive-based program designed to help reduce emissions by encouraging port authorities to redesign and replace older diesel engines with new technologies and cleaner fuels. The U.S. Environmental Protection Agency's National Clean Diesel Campaign offers funding to port authorities to help them overcome the obstacles that prevent the adoption of cleaner diesel technologies.\n\nClean Construction USA promotes the reduction of diesel exhaust emissions from construction equipment and vehicles. They do so by promoting proper operations and maintenance, use of emission-reducing technologies, and use of cleaner fuels. It is part of the U.S. Environmental Protection Agency's National Clean Diesel Campaign, which helps fun for clean diesel construction equipment projects.\n\nClean Agriculture USA promotes the reduction of diesel exhaust emissions from agricultural equipment and vehicles by promoting proper operations and maintenance by farmers, ranchers, and agribusinesses. It is also part of the National Clean Diesel Campaign.\n\nThe Air Pollution Control Program helps state and local agencies in planning, developing, establishing, improving, and maintaining adequate programs for prevention and control of air pollution. Plans emphasize alternative fuels, vehicle maintenance, and transportation choices to reduce miles traveled. Eligible applicants may receive funding for up to 60% of project costs. Reference 42 U.S. Code 7405 is the law that created the grant for support of air pollution planning and control programs.\n\nThe Alternative Transportation in the Parks and Public Lands Program provides funding to support public transportation projects in parks and on public lands. The goal is to include conservation of natural, historical, and cultural resources, and reduced congestion and pollution. The FTC administers the program while partnering with the Department of the Interior and the Forest Service. Eligible projects include capital and planning expenses for alternative transportation systems such as clean fuel shuttle vehicles. Reference 49 U.S. Code 5320 is the law that created program.\n\nThe goal of the NFCBP is to assist in the development of commercially viable fuel cell bus technologies and related infrastructure with funding awarded through a competitive grant process. Consideration is given to those that have managed advanced transportation projects, including projects related to hydrogen and fuel cell public transportation operations for a period of at least five years. Reference 49 U.S. Code 5309 is the law that created the program.\n\nThe CMAQ program funds state departments of transportation (DOTS), municipal planning organizations, and transit agencies for projects and programs in air quality maintenance to reduce transportation related emissions. Eligible activities include traffic flow improvements, idle reduction equipment, development of alternative fueling infrastructure, etc. Reference 23 U.S. Code 149 is the law that created the program.\n\nThe Clean Fuels Grant Program assists in achieving or maintaining the National Ambient Air Quality Standards through grant funding. The program accelerates the deployment of advanced bus technologies by supporting the use of low-emission vehicles in transit fleets. It assists in constructing alternative fuel statins, modifying garage facilities to accommodate clean fuel vehicles, and assisting with the use of biodiesel. For more information, visit the Clean Fuels Grant Program fact sheet. Reference 49 U.S. Code 5308 is the law that created the program.\n\nThe Transit Investments for Greenhouse Gas and Energy Reduction (TIGGER) program works with public transit agencies to create strategies for reducing greenhouse gas emissions and energy use from transit operations. Eligible projects include on board vehicle energy management such as energy storage, regenerative braking, fuel cells, and turbines.\n\nPublic understanding of Hydrogen will have a tremendous impact on current as well as future policy initiatives for vehicle as well as portable and stationary applications. Numerous studies have been performed to analyze the public’s current perception and understanding of hydrogen. Most energy producing technologies have an attached combination of positive and negative stigmas and means of understanding by the general public. For example, individuals who are aware of the environmental effects of a possible nuclear meltdown may deem nuclear power as a negative entity. On the contrary, individuals who are aware of the quantities of carbon emissions being reduced by using one less fossil fuel driven power plant may find nuclear power as a positive entity. A combination of scientific understanding with common associated social themes anchored by pre-existing knowledge will have a significant impact on the future hydrogen policy.\n\nMany studies have been carried out on the topic of Hydrogen Public Perception and degree of acceptance. The Institute for Social, Cultural and Public Policy Research at the University of Salford, evaluated a variety of survey based studies and performed a critical analysis of these selected findings. The article emphasizes that public perception is largely formed on an overall uneducated or misinformed hydrogen knowledge base. Dr. Miriam Ricci states within the article that \"Providing factual information on the whole hydrogen chain, not just applications, and the implications it might have on the lives of citizens it may have is a necessary first step.\" This conclusion reached by many other scholars has been implemented in current US Hydrogen Policy through the appropriations of hydrogen demonstration and public outreach funding described in section 808 of the Energy Policy Act of 2005.\n\nAnother concern presented by the Institute for Social, Cultural and Public Policy Research at the University of Salford is that of public distrust of government regulatory committees. In a free-association based survey carried out by Fionnguala Sherry-Brennan of the Manchester Architecture Research Centre (MARC) at the University of Manchester, randomly selected individuals were asked to describe what other words come to mind when discussing hydrogen. The survey concluded that 9.1 percent of the study population associated the word \"hydrogen\" with \"bomb\" and only 0.6 percent of the study population associated the word \"safe.\" There are definite safety concerns regarding a possible hydrogen economy and these concerns have formed current policy and regulations regarding refueling stations and hydrogen production stations as mentioned earlier in this article.\n"}
{"id": "20616827", "url": "https://en.wikipedia.org/wiki?curid=20616827", "title": "Wetland conservation", "text": "Wetland conservation\n\nWetland conservation is aimed at protecting and preserving areas where water exists at or near the Earth's surface, such as swamps, marshes and bogs. Wetlands cover at least six per cent of the Earth and have become a focal issue for conservation due to the ecosystem services they provide. More than three billion people, around half the world’s population, obtain their basic water needs from inland freshwater wetlands. The same number of people rely on rice as their staple food, a crop grown largely in natural and artificial wetlands. In some parts of the world, such as the Kilombero wetland in Tanzania, almost the entire local population relies on wetland cultivation for their livelihoods.\n\nFisheries are also an extremely important source of protein and income in many wetlands. According to the United Nations Food and Agriculture Organization, the total catch from inland waters (rivers and wetlands) was 8.7 million metric tonnes in 2002. In addition to food, wetlands supply fibre, fuel and medicinal plants. They also provide valuable ecosystems for birds and other aquatic creatures, help reduce the damaging impact of floods, control pollution and regulate the climate. From economic importance, to aesthetics, the reasons for conserving wetlands have become numerous over the past few decades.\n\nVarious definitions of wetlands exist. The Convention on Wetlands of International Importance, also known as the Ramsar Convention, defines wetlands as including: lakes and rivers, swamps and marshes, wet grasslands and peatlands, oases, estuaries, deltas and tidal flats, near-shore marine areas, mangroves and coral reefs, and human-made sites such as fish ponds, rice paddies, reservoirs, and salt pans. Meanwhile, the United States Environmental Protection Agency (EPA) or Wetlands Reserve Program, describes wetlands as \"those areas that are inundated or saturated by surface or groundwater at a frequency and duration sufficient to support, and that under normal circumstances do support, a prevalence of vegetation typically adapted for life in saturated soil conditions. Wetlands generally include swamps, marshes, bogs and similar areas.\" Wetlands vary widely in their salinity levels, climatic zones, supported flora, surrounding geography, whether they are coastal or inland and so on.\n\nThe main functions performed by wetlands are: water filtration, water storage, biological productivity, and provide habitat for wildlife. Additional functions and uses of wetlands are described in wetland.\n\nWetlands aid in water filtration by removing excess nutrients, slowing the water allowing particulates to settle out of the water which can then be absorbed into plant roots. Studies have shown that up to 92% of phosphorus and 95% of nitrogen can be removed from passing water through a wetland. Wetlands also let pollutants settle and stick to soil particles, up to 70% of sediments in runoff. Some wetland plants have even been found with accumulations of heavy metals more than 100,000 times that of the surrounding waters' concentration. Without these functions, the waterways would continually increase their nutrient and pollutant load, leading to an isolated deposit of high concentrations further down the line. An example of such a situation is the Mississippi River’s dead zone, an area where nutrient excess has led to large amounts of surface algae, which use up the oxygen and create hypoxic conditions (very low levels of oxygen).\n\nWetlands can even filter out and absorb harmful bacteria from the water. Their complex food chain hosts various microbes and bacteria, which invertebrates feed on. These invertebrates can filter up to 90% of bacteria out of the water this way.\n\nWetlands can store approximately 1-1.5 million gallons of floodwater per acre. When you combine that with the approximate total acres of wetlands in the United States (107.7 million acres), you get an approximate total of 107.7 - 161.6 million million gallons of floodwater US wetlands can store. By storing and slowing water, wetlands allow groundwater to be recharged. \"A 550,000 acre swamp in Florida has been valued at $25 million per year for its role in storing water and recharging the aquifer.\" And combining the ability of wetlands to store and slow down water with their ability to filter out sediments, wetlands serve as strong erosion buffers.\n\nThrough wetlands ability to absorb nutrients, they are able to be highly biologically productive (able to produce biomass quickly). Freshwater wetlands are even comparable to tropical rainforests in plant productivity. Their ability to efficiently create biomass may become important to the development of alternative energy sources.\n\nWhile wetlands only cover around 5% of the Conterminous United States’s land surface, they support 31% of the plant species. They also support, through feeding and nesting, up to ½ of the native North American bird species. Bird populations, while playing a major role in food webs, are also the focus of several, well-funded recreation sports. (Waterfowl hunting and bird watching to name a pair)\n\nWildlife Habitat is important not only for the conservation of species but also for a number of recreational opportunities. As a conservation purpose, wildlife habitat is managed for maintaining and using the resources in sustainable manner. Ninety-five percent of all commercially harvested fish and shellfish in the United States are wetland dependent. Muscatatuck National Wildlife Refuge is an example of recreational destination for hunting, fishing, wildlife observation and photography that has a good wildlife management. Some parts of the area are wetlands managed for providing habitat of migratory birds, such as waterfowl and songbirds. The 14 million United States hunters generate in excess of $50 billion annually in economic activity. This does not include the 60 million people that watch migratory birds as a hobby. The Florida Keys wetland area generates more than $800 million in annual tourism income alone.\nConservation efforts vary in intensity and method by country. The following list is not comprehensive.\n\nOver 90% of the wetlands in New Zealand have been drained since European settlement, predominantly to create farmland. Wetlands now have a degree of protection under the Resource Management Act 1991.\n\nThe fragments of wetland habitats that are still in existence in the Republic of Macedonia are present as marsh or swamp communities. These patches are present at Studenchishte (small fragment near Ohrid Lake), Pelagonia (village Chepigovo), Negortsi Spa, Bansko, Belchishte wetland and Monospitovo marsh. The large areas of swamps that used to be present in most of the valleys in contemporary Republic of Macedonia have undergone a great transformation over the last 50–60 years. The main cause for their reduction is land reclamation, drainage, and conversion into arable land for agricultural needs (Smith and Smith, 2003). Some of the remaining wetlands (Negortsi Spa, Bansko) are of great importance for understanding the genesis of marsh vegetation in the Republic of Macedonia as many mountainous marshes and peat bogs suffered anthropogenic transformation due to the capturing of water from mountain springs and streams for the purposes of generating drinking water (Smith, 2003). Accordingly, the fragmentation and transformation of previous swamps had a major impact on faunal distribution and abundance. Amphibians are the most affected species along with invertebrate and vertebrate groups including the European Otter (Lutra Lutra L.). It is listed as Nearly Threatened according to the IUCN’s red list and mainly found now only in the Belchishte wetland (Smith 2003; Smith and Smith, 2003). The otter has importance for the wetland communities not just in Macedonia but also in other European countries such as the Netherlands and Germany (Reuther, 1995; Reuther et al., 2001). Macedonian wetlands lie within the network of some of the bigger rivers in Macedonia including the Vardar River, a catchment area equal to 80% of Macedonian territory.\n\nThe South African Department of Environmental Affairs in conjunction with the departments of Water Affairs and Forestry, and of Agriculture, supports the conservation and rehabilitation of wetlands through the Working for Wetlands program. The aim of this program is to encourage the protection, rehabilitation and sustainable use of South African wetlands through co-operative governance and partnerships. The program is also a poverty relief effort, providing employment in wetland maintenance.\n\nThe Swedish national wetland inventory (VMI) is one of the world's most extensive systematic inventories of nature types that has ever been done. VMI has surveyed the wetlands of Sweden below the alpine region during a 25-year period. In total 35 000 objects (sites) are included in VMI, corresponding to an area of 4.3 million hectares, or 10% of the land area of Sweden. The aim of the survey has been to increase the general knowledge of wetlands in Sweden, as a basis for environmental monitoring and natural resources planning. By investigating the impact of human activities on wetlands and identifying the most valuable wetlands, their values can be preserved for future generations. The results from the inventory were also meant to function as background data for the authorities' decisions concerning e.g. drainage permits.\n\nThe US wetland conservation efforts are rooted partly in legislative requirements specifying that when a proposal is made to drain or fill a wetland, the proposers in many cases must offset the loss by restoring or constructing wetlands nearby that are of the same or greater size and levels of function. Several states within the US have additional requirements that must be met when wetland alteration is proposed. In addition, several federal programs provide financial incentives for wetland protection to private individuals whose land contains wetlands not completely protected by federal or state laws. \n\nA restoration project by the State of Florida in the Everglades acquired U.S. Sugar Corporation land allowing for water delivery, water treatment, and water storage of sufficient quantity and quality to mimic the Everglades' natural system.\n\nWeishan Wetland Park | ASLA National Awards | Professional Category | Award of Honor - General Design | \n\nLiupanshui Minghu Wetland Park | ASLA National Awards | Professional Category | Award of Honor - General Design | \n\nHunter's Point South Waterfront Park | ASLA National Awards | Professional Category | Award of Honor - General Design | \n\nQunli Stormwater Park in Haerbin | ASLA National Awards | Professional Category | Award of Excellent - General Design | \n\nStone River in Eastern New York State | ASLA National Awards | Professional Category | Award of Honor - General Design | \n\nShanghai Houtan Park | ASLA National Awards | Professional Category | Award of Excellent - General Design | \n\nThe Qinhuangdao Beach Restoration | ASLA National Awards | Professional Category | Award of Honor - General Design | \n\nTianjin Qiaoyuan Park | ASLA National Awards | Professional Category | Award of Honor - General Design | \n\nBeijing Olympic Forest Park | ASLA National Awards | Professional Category | Award of Honor - General Design | \n\nLagoon Park in California | ASLA National Awards | Professional Category | Award of Honor - General Design | \n\n"}
