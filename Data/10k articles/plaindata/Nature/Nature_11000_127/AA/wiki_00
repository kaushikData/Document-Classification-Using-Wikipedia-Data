{"id": "44915134", "url": "https://en.wikipedia.org/wiki?curid=44915134", "title": "2014 XL7", "text": "2014 XL7\n\nOn 1 September 2014 the asteroid passed about from Earth, but at that time the asteroid had an apparent magnitude of 25 and was roughly 25 degrees from the Sun. It was discovered on 11 December 2014 by the Mount Lemmon Survey at an apparent magnitude of 20 using a reflecting telescope. The asteroid has an observation arc of 35 days with an uncertainty parameter of 6. The asteroid was last observed on 15 January 2015, and is still being actively observed to better constrain the orbit. The asteroid will not drop below magnitude 25 until March 2015.\n\nWith an absolute magnitude of 21, the asteroid is about 170–380 meters in diameter.\n\nWith an observation arc of 19 days, it had a cumulative Palermo Scale rating of –2.85 and was briefly the 9th most dangerous asteroid known. It was calculated that on 4 June 2065 there was a 1 in 270000 chance of impact and on 4 June 2076 there was a 1 in 137000 chance of impact. On 15 January 2015 the asteroid was recovered by Cerro Paranal Observatory which extended the observation arc to 35 days, and was removed from the Sentry Risk Table.\n"}
{"id": "56736428", "url": "https://en.wikipedia.org/wiki?curid=56736428", "title": "2017 VR12", "text": "2017 VR12\n\nIt orbits the Sun at a distance of 1.0–1.7 AU once every 1 years and 7 months (585 days; semi-major axis of 1.37 AU). Its orbit has an eccentricity of 0.27 and an inclination of 9° with respect to the ecliptic. is a V-type asteroid with a bright surface.\n\n passed 0.0097 AU (3.76 lunar distances) from Earth on 7 March 2018, the closest approach by this asteroid currently known. It brightened to 12th magnitude, making it one of the brightest Near Earth asteroids of the year. It was observed by radar from Goldstone, Green Bank and Arecibo Observatory. Images revealed that is a slightly elongated and angular body with a size of approximately 160 by 100 meters.\n\nImages obtained at Green Bank and Arecibo observatories in 2018, revealed that is a slightly elongated and angular body with a size of approximately 160 by 100 meters.\n\nOn 5 March 2018, a rotational lightcurve was obtained from photometric observations by astronomers at the Northolt Branch Observatories. Lightcurve analysis gave a rotation period of 1.5 hours with a brightness amplitude between 0.4 and 0.5 magnitude ().\n\n"}
{"id": "2202535", "url": "https://en.wikipedia.org/wiki?curid=2202535", "title": "Alastair Fitter", "text": "Alastair Fitter\n\nAlastair Hugh Fitter CBE FRS (born 20 June 1948) is a British ecologist at the University of York.\n\nFitter was educated at Oxford and at Liverpool, and came to the Department of Biology in York in 1972. In 2004 he was appointed Pro-Vice-Chancellor, with the Research portfolio. He is a member of Council of the Natural Environment Research Council.\n\nFitter's research interests include plant and microbial behaviour in a changing world; functional ecology of roots and mycorrhizal associations under field conditions; root system architecture; carbon cycling in soil, especially in relation to mycorrhizas; phenological responses to climate change. \n\nAlastair Fitter is the son of the naturalist and author Richard Fitter (1913–2005), and together in 2002 they published an article in \"Science\" on the changing phenology of wild flowers due to global warming. They have also collaborated on numerous field guides and other natural history books.\n\nFitter was elected Fellow of the Royal Society (FRS) in 2005. He was appointed Commander of the Order of the British Empire (CBE) in the 2010 New Year Honours for services to environmental science. He received a President’s Medal from the British Ecological Society.\n\n"}
{"id": "285239", "url": "https://en.wikipedia.org/wiki?curid=285239", "title": "Andesite", "text": "Andesite\n\nAndesite ( or ) is an extrusive igneous, volcanic rock, of intermediate composition, with aphanitic to porphyritic texture. In a general sense, it is the intermediate type between basalt and rhyolite, and ranges from 57 to 63% silicon dioxide (SiO) as illustrated in TAS diagrams. The mineral assemblage is typically dominated by plagioclase plus pyroxene or hornblende. Magnetite, zircon, apatite, ilmenite, biotite, and garnet are common accessory minerals. Alkali feldspar may be present in minor amounts. The quartz-feldspar abundances in andesite and other volcanic rocks are illustrated in QAPF diagrams.\n\nClassification of andesites may be refined according to the most abundant phenocryst. Example: \"hornblende-phyric andesite\", if hornblende is the principal accessory mineral.\n\nAndesite can be considered as the extrusive equivalent of plutonic diorite. Characteristic of subduction zones, andesite represents the dominant rock type in island arcs. The average composition of the continental crust is andesitic. Along with basalts they are a major component of the Martian crust. The name \"andesite\" is derived from the Andes mountain range.\n\nMagmatism in island arc regions (i.e., active oceanic margins) comes from the interplay of the subducting plate and the \"mantle wedge\", the wedge-shaped region between the subducting and overriding plates.\n\nDuring subduction, the subducted oceanic crust is submitted to increasing pressure and temperature, leading to metamorphism. Hydrous minerals such as amphibole, zeolites, chlorite etc. (which are present in the oceanic lithosphere) dehydrate as they change to more stable, anhydrous forms, releasing water and soluble elements into the overlying wedge of mantle. Fluxing water into the wedge lowers the solidus of the mantle material and causes partial melting. Due to the lower density of the partially molten material, it rises through the wedge until it reaches the lower boundary of the overriding plate. Melts generated in the mantle wedge are of basaltic composition, but they have a distinctive enrichment of soluble elements (e.g. potassium (K), barium (Ba), and lead (Pb)) which are contributed from sediment that lies at the top of the subducting plate. Although there is evidence to suggest that the subducting oceanic crust may also melt during this process, the relative contribution of the three components (crust, sediment, and wedge) to the generated basalts is still a matter of debate.\n\nBasalt thus formed can contribute to the formation of andesite through fractional crystallization, partial melting of crust, or magma mixing, all of which are discussed next.\n\nAndesite is typically formed at convergent plate margins but may also occur in other tectonic settings. Intermediate volcanic rocks are created via several processes:\n\n\nTo achieve andesitic composition via fractional crystallization, a basaltic magma must crystallize specific minerals that are then removed from the melt. This removal can take place in a variety of ways, but most commonly this occurs by crystal settling. The first minerals to crystallize and be removed from a basaltic parent are olivines and amphiboles. These mafic minerals settle out of the magma, forming mafic cumulates. There is geophysical evidence from several arcs that large layers of mafic cumulates lie at the base of the crust. Once these mafic minerals have been removed, the melt no longer has a basaltic composition. The silica content of the residual melt is enriched relative to the starting composition. The iron and magnesium contents are depleted. As this process continues, the melt becomes more and more evolved eventually becoming andesitic. Without continued addition of mafic material, however, the melt will eventually reach a rhyolitic composition.\n\nPartially molten basalt in the mantle wedge moves upwards until it reaches the base of the overriding crust. Once there, the basaltic melt can either underplate the crust, creating a layer of molten material at its base, or it can move into the overriding plate in the form of dykes. If it underplates the crust, the basalt can (in theory) cause partial melting of the lower crust due to the transfer of heat and volatiles. Models of heat transfer, however, show that arc basalts emplaced at temperatures 1100–1240 °C cannot provide enough heat to melt lower crustal amphibolite. Basalt can, however, melt pelitic upper crustal material. Andesitic magmas generated in island arcs, therefore, are probably the result of partial melting of the crust.\n\nIn continental arcs, such as the Andes, magma often pools in the shallow crust creating magma chambers. Magmas in these reservoirs become evolved in composition (dacitic to rhyolitic) through both the process of fractional crystallization and partial melting of the surrounding country rock. Over time as crystallization continues and the system loses heat, these reservoirs cool. In order to remain active, magma chambers must have continued recharge of hot basaltic melt into the system. When this basaltic material mixes with the evolved rhyolitic magma, the composition is returned to andesite, its intermediate phase.\n\nIn 2009, researchers revealed that andesite was found in two meteorites (numbered GRA 06128 and GRA 06129) that were discovered in the Graves Nunataks icefield during the US Antarctic Search for Meteorites 2006/2007 field season. This possibly points to a new mechanism to generate andesite crust.\n\n\n"}
{"id": "6128820", "url": "https://en.wikipedia.org/wiki?curid=6128820", "title": "Backup battery", "text": "Backup battery\n\nA backup battery provides power to a system when the primary source of power is unavailable. Backup batteries range from small single cells to retain clock time and date in computers, up to large battery room facilities that power uninterruptible power supply systems for large data centers. Small backup batteries may be primary cells; rechargeable backup batteries are kept charged by the prime power supply.\n\nBackup batteries in aircraft keep essential instruments and devices running in the event of an engine power failure. Each aircraft has enough power in the backup batteries to facilitate a safe landing. The batteries keeping navigation, ELUs (emergency lighting units), emergency pressure or oxygen systems running at altitude, and radio equipment operational. Larger aircraft have control surfaces that run on these backups as well. Aircraft batteries are either nickel-cadmium or valve-regulated lead acid type. The battery keeps all necessary items running for between 30 minutes and 3 hours. Large aircraft may have a ram air turbine to provide additional power during engine failures.\n\nBackup batteries are almost always used in burglar alarms. The backup battery prevents the burglar from disabling the alarm by turning off power to the building. Additionally these batteries power the remote cellular phone systems that thwart phone line snipping as well. The backup battery usually has a lifespan of 3-10 years depending on the make and model, and so if the battery runs flat, there is only one main source of power to the whole system which is the mains power. Should this fail as well (for example, a power cut), it usually triggers a third backup battery located in the bellboxes on the outside of the house which simply triggers the bell or siren. This however means that the alarm cannot be stopped in any way apart from physically going outside to the bellbox and disabling the siren. It is also why if there is a power outage in the area, most burglar alarms do start ringing and cannot be realistically stopped until the main power is restored.\n\nModern personal computer motherboards have a backup battery to run the real-time clock circuit and retain configuration memory while the system is turned off. This is often called the CMOS battery or BIOS battery. The original IBM AT through to the PS/2 range, used a relatively large primary lithium battery, compared to later models, to retain the clock and configuration memory. These early machines required the backup battery to be replaced periodically due to the relatively large power consumption. Some manufacturers of clone machines used a rechargeable battery to avoid the problems that could be created by a failing battery. Modern systems use a coin style primary battery. In these later machines, the current draw is almost negligible and the primary batteries usually outlast the system that they support. It is rare to find rechargeable batteries in such systems.\nBackup batteries are used in uninterruptible power supplies (UPS), and provide power to the computers they supply for a variable period after a power failure, usually long enough to at least allow the computer to be shut down gracefully. These batteries are often large valve regulated lead-acid batteries in smaller or portable systems. Data center UPS backup batteries may be wet cell lead-acid or nickel cadmium batteries, with lithium ion cells available in some ratings. \n\nServer-grade disk array controllers often contain onboard disk buffer, and provide an option for a \"backup battery unit\" (BBU) to maintain the contents of this cache after power loss. If this battery is present, disk writes can be considered completed when they reach the cache, thus speeding up I/O throughput by not waiting for the hard drive. This operation mode is called \"write-back caching\".\n\nA local backup battery unit is necessary in some telephony and combined telephony/data applications built with use of digital passive optical networks. In such networks there are active units on telephone exchange side and on the user side, but nodes between them are all passive in the meaning of electrical power usage. So, if a building (such as an apartment house) loses power, the network continues to function. The user side must have standby power since operating power isn't transferred over data optical line.\n\nA valve-regulated lead-acid battery (VRLA) is a battery type that is popular in telecommunications network environments as a reliable backup power source. VRLA batteries are used in the outside plant at locations such as Controlled Environmental Vaults (CEVs), Electronic Equipment Enclosures (EEEs), and huts, and in uncontrolled structures such as cabinets.\nGR-4228, \"VRLA Battery String Certification Levels Based on Requirements for Safety and Performance\", is a new industry-approved set of VRLA requirements that provides a three-level compliance system. The compliance system provides a common framework for evaluating and qualifying various valve-regulated lead-acid battery technologies. The framework intends to alleviate the complexities associated with product introduction and qualification. \nFor a VRLA, the quality system employed by the manufacturer is an important key to the overall reliability of it. The manufacturing processes, test and inspection procedures, and quality program used by a manufacturer should be adequate to ensure that the final product meets the needs of the end user, the application, and industry-accepted standards and processes (i.e., ANSI/IEC, TL9000, and GR-78, \"Generic Requirements for the Physical Design and Manufacture of Telecommunications Products and Equipment\".\n\nCartridge-based video games sometimes contain a battery which is used to preserve the contents of a small RAM chip on which saved games and/or high scores are recorded.\n\nPower failure in a hospital would result in life-threatening conditions for patients. Patients undergoing surgery or on life support are reliant on a consistent power supply. Backup generators or batteries supply power to critical equipment until main power can be restored.\n\nPower failure in a power station that produces electricity would result in a blackout situation that would cause irreparable damage to equipment such as the turbine-generator. The safety of power station employees is a major concern during an unscheduled power outage at a power plant. A bank of large station backup batteries are used to power uninterruptible power supplies as well as directly power emergency oil pumps for up to 8 hours while normal power is being restored to the power station.\n\n"}
{"id": "32187715", "url": "https://en.wikipedia.org/wiki?curid=32187715", "title": "Bombardment of Upolu", "text": "Bombardment of Upolu\n\nThe Bombardment of Upolu, in 1841, was the second engagement with islanders of the Pacific Ocean during the United States Exploring Expedition.\n\nFollowing the murder of an American sailor on the island of Upolu, Samoa, two United States Navy warships were dispatched to investigate. When the principal local chief would not hand over those suspected of the murder, they bombarded one village and went ashore and burned down others.\n\nThe American expedition of discovery first arrived off Upolu in October 1839 while conducting surveys of the region. Because United States-flagged merchant ships had traded a lot with the natives in the previous decades, Commander Charles Wilkes decided on establishing a treaty with the seven chiefs on the island which would govern future relations. Wilkes then drafted what he called the \"commercial regulations\" that, among other things, provided that the Samoans would hand over any natives found guilty of murdering foreigners. An incident had occurred a few years before in which the followers of Chief Oportuno had killed three sailors from an American merchantman, so Wilkes wanted a treaty to handle such a situation. All of the stipulations were agreed to and were officially signed on November 5, 1839, the same day that James C. William was appointed the American consul to the island. With that accomplished, Commander Wilkes left Upolu to continue his voyage around the world.\n\nTrade with the Samoans went well until about a year later, when the natives at Upolu killed another American.\n\nWhen Commander Wilkes learned of the death, he detached two vessels from his squadron to sail back to Samoa. The twenty-two gun sloop-of-war USS \"Peacock\" and the small two gun schooner USS \"Flying Fish\" were under the command of Lieutenant William L. Hudson and Commandant Samuel R. Knox, respectively. The two vessels arrived off Upolu on February 24, 1841. The Americans decided to meet with the principal chief Malietoa to demand that the murderer or murderers be handed over.\n\nMalietoa refused to surrender the suspects, so Lieutenant Hudson decided to land \"70 odd men\", including a force of no more than twenty marines, and bombard the village of Saulafata. After preparations for battle were completed, the landing party boarded boats and waited off the \"Peacock\"s starboard quarter while she and the \"Flying Fish\" shelled the Samoans. It was still the morning of February 24 when the American warships opened fire with grapeshot and round shot. The grapeshot had no effect and fell short of target, but the round shot quickly began scoring hits upon the buildings on shore.\n\nThe native warriors did not resist the attack in any way, and after the first cannon was fired, they retreated from the beach to gather their families and belongings before fleeing into the jungle. After eighteen shots, the ships ceased firing, and the shore party was sent into Saulafata. There the marines and sailors were divided into three units under Lieutenants William M. Walker of the Marine Corps, De Haven and George F. Emmos, as well as a few midshipmen. Two units began burning the forty of fifty huts with torches, while the third unit remained at the boats. There was no fighting. None of the Samoans were even seen after the first cannon was fired. With Saulafata destroyed, the Americans returned to their ships, but when they got there, Lieutenant Hudson ordered them to return ashore and destroy the villages of Fusi and Sallesesi. So again the party was landed after first receiving \"a taste of grog\" as encouragement. There were over 100 huts between the two villages, and the second was destroyed in the same manner as the first, without any resistance from the natives. The Americans then returned to the beach and destroyed all the canoes they could find before reboarding their ships and sailing away to rejoin Commander Wilkes.\n\n"}
{"id": "12049944", "url": "https://en.wikipedia.org/wiki?curid=12049944", "title": "C. Leroy Ellenberger", "text": "C. Leroy Ellenberger\n\nCharles Leroy Ellenberger (born 1942, known as C. Leroy) is perhaps best known as a one-time advocate, but now a critic of, controversial writer Immanuel Velikovsky and his works on catastrophism. He first read \"Worlds in Collision\" in 1969. In 1979, he became a contributing editor (and later Senior Editor & Executive Secretary) to the Velikovsky-inspired Kronos journal, and has contributed material to many other publications. In 1980 he was selected by the editor of \"Astronomy\" magazine to debate James Oberg on Velikovsky. His confidence in the validity of Velikovsky's ideas was shaken in January 1982 when Kronos sponsored his attendance at the semi-annual AAAS meeting in Washington, D.C., in order to distribute information on Velikovsky. In a wide-ranging conversation with Jeremy Cherfas, then a writer for the British weekly science magazine New Scientist over how the press misunderstood Velikovsky, Cherfas had counter-arguments to many points that Ellenberger was not able to rebut. According to Professor of Social Theory Alfred de Grazia at New York University, \"By 1983 Ellenberger was preparing to abandon much of quantavolution and found now that the story of Velikovsky was not without its shady tones, and more important, that Arctic ice cores and bristlecone pine dating technologies were directly contradicting Holocene quantavolutions . . . ; further, that Gentry's studies of the surprising 'instant' polonium halos of creation . . . were probably invalid.\" Henry Bauer described Ellenberger's role in the Velikovsky scene as follows: \". . . was a confidant to Velikovsky, a frequent visitor (often with camera) from April 1978 to his death in November 1979, and a Senior Editor of the Velikovskian journal \"Kronos\", until the evidence forced him to conclude that Velikovsky's scientific claims were baseless. Velikovsky inscribed his copy of \"Ramses II and His Time\" 'To Leroy who is consumed by the sacred flame of search for truth', 20 May 1978, and gave him permission to sell 'Velikovsky's right!' T-shirts. Alfred de Grazia, impetus for \"The Velikovsky Affair\" (1966), appointed him chronicler of the continuing Velikovsky controversy in 1980. Ellenberger's last contact with Velikovsky was a phone call from him two days before he died.\" Also, he \"has tried unceasingly but to little avail to have his former colleagues acknowledge the accumulating evidence, for example, from Greenland ice cores, that Velikovsky's claimed catastrophes did not in fact occur. Ellenberger points out, too, that Velikovsky's writings have become superfluous: astronomically plausible argument and speculation about relatively recent cosmic catastrophism can now be found in the work of Victor Clube and Bill Napier (\"The Cosmic Serpent\", 1982; \"The Cosmic Winter\", 1990), where the testimony of myth and historical records is also taken into account.\"\n\nEllenberger has degrees in chemical engineering and finance & operations research (B.S., Washington Univ.; M.B.A., Univ. of Pennsylvania). He is currently a Medical Article Retrieval Specialist in St. Louis, Missouri.\n\nIn 1984, Ellenberger noted:\n\nEllenberger's most widely read criticisms of Velikovsky were two 1985 correspondences to \"Nature\": \"Falsifying Velikovsky\" vol. 316, p. 386, and \"Velikovsky's evidence?\" vol. 318, p. 204, and two 1987 letters to the editor in New York \"Times\": May 15, p. 14, and August 29, p. 14. While citing these publications, Richard J. Huggett, Senior Lecturer in Geography, University of Manchester, averred that Ellenberger \"has, since his conversion to the anti-Velikovsky camp in 1984, relentlessly and mercilessly tried to show why Velikovsky's ideas were downright silly. . . .\" The second \"Times\" letter was rebutted by Clark Whelton in a letter published September 29. Although the \"Times\" did not print Ellenberger's point-by-point surrebuttal to Whelton's letter, it was distributed (a) privately by mail with the September 1, 1987 \"Dear Friends\" letter and (b) to all attendees at the August 1990 \"Reconsidering Velikovsky\" Conference in Toronto.\nIn 1994, Ellenberger's invitation to a conference on Velikovsky was rescinded, due to other participants' stating that they would not attend if he participated. This incident came about through the efforts of a group of individuals who in 1992 had deleted the section \"Magnetism, Dynamos and Neptune\" from Ellenberger's memoir for \"Aeon\" that explained the ignorance of Velikovsky and many of his supporters concerning the role of electromagnetism in astronomy and the origin of planetary magnetic fields. Previously, he was an invited speaker at Milton Zysman's August 1990 \"Reconsidering Velikovsky\" Conference at University of Toronto, identified on the program as \"Velikovsky's most unrelenting critic\" who was interviewed for \"The Globe and Mail\", and he was the keynote speaker at the August 1992 Canadian Society for Interdisciplinary Studies conference in Haliburton, Ontario. He is also the author of the article \"Top Ten Reasons Why Velikovsky Is Wrong About \"Worlds In Collision\"\" which he says:\n\nOf these attempts to convince Velikovsky's supporters, Henry Bauer noted Ellenberger \"has tried unceasingly but to little avail to have his former colleagues acknowledge the accumulating evidence, for example, from Greenland ice cores, that Velikovsky's claimed catastrophes did not in fact occur.\" His resignation from \"Kronos\" as senior editor in December 1986 was acknowledged by Martin Gardner, who previously noted Ellenberger's \"vitriolic\" letters defending Velikovsky. Regarding Ellenberger's defection, \"Skeptic\" editor Michael Shermer declared: \"One major strike against Velikovsky is that Leroy Ellenberger, a one-time Velikovsky supporter, after stepping outside of the paradigm to examine the evidence in a clearer light, now completely rejects all tenets of the theory.\" Sagan biographer Keay Davidson credits Ellenberger \"In my experience\" as \"the single richest source of information on the Velikovsky controversy.\" Astronomer Dennis Rawlins hails Ellenberger \"the world's top anti-Velikovsky expert\". NASA astronomer David Morrison, who has monitored the Velikovsky scene since 1972, has thanked Ellenberger for helping \"to look at these issues from the other side and to appreciate how poorly the scientific critics communicated with the public.\" Ellenberger's role as a Velikovsky turncoat and critic has been recently affirmed by Ronald H. Fritze in \"Invented Knowledge: False History, Fake Science and Pseudo-religions\". Princeton historian Michael Gordin acknowledged \"a special debt of gratitude\" to Ellenberger for his contributions to \"The Pseudoscience Wars\".\n\nEllenberger came to accept Victor Clube and Bill Napier's model as a scientifically valid and intellectually satisfying replacement for Velikovsky-inspired models of recent, interplanetary catastrophism. Astronomer David Morrison noted \"In fact, the work of Clube and Napier attracts many people who were once impressed by Velikovsky, such as Leroy Ellenberger, at one time a member of the Velikovsky inner circle and now one of the most outspoken critics of his current followers\". Since 1990, Ellenberger has actively promoted Clube and Napier's model, now named \"coherent catastrophism\", in articles for \"Skeptic\", \"C&C Review\", and \"Catastrophism and Ancient History,\" letters to editors, and postcard mailing campaigns to Velikovskians.\n\n"}
{"id": "33781805", "url": "https://en.wikipedia.org/wiki?curid=33781805", "title": "Clipperton Fracture Zone", "text": "Clipperton Fracture Zone\n\nThe Clipperton Fracture Zone is a geological submarine fracture zone of the Pacific Ocean, with a length of some 4500 miles (7240 km). It is one of the five major lineations of the northern Pacific floor, south of the Clarion Fracture Zone, discovered by the Scripps Institution of Oceanography in 1950. The fracture, an unusually mountainous topographical feature, begins east-northeast of the Line Islands and ends in the Middle America Trench off the coast of Central America. It roughly forms a line on the same latitude as Kiribati and Clipperton Island. The fracture can be divided into four distinct parts:\n\n- The first, 127°–113° W, is a broad, low welt of some 900 miles, with a central trough 10 to 30 miles wide;\n\n- the second, 113°-107° W, is a volcano enriched ridge, 60 miles wide and 330 miles long;\n\n- the third, 107°-101° W, is a low welt with a central trough 1,200–2,400 feet deep which transects the Albatross Plateau; and\n\n- the fourth, 101°-96° W, contains the Tehuantepec Ridge which extends 400 miles northeast to the continental margin.\n\nThe Nova-Canton Trough is often seen as an extension of the fracture.\n\nIn 2016, the seafloor in the Clipperton Fracture Zone – an area being targeted for deep-sea mining – was found to contain an abundance and diversity of life, with more than half of the species collected being new to science.\n"}
{"id": "54744260", "url": "https://en.wikipedia.org/wiki?curid=54744260", "title": "Columbus Atlantic Trophy", "text": "Columbus Atlantic Trophy\n\nThe Columbus Atlantic Trophy is an award for the fastest non-stop two way crossing of the Atlantic Ocean within a given time period. It was inaugurated in 1992 by New York Yacht Club in concert with Costa Smeralda Yacht Club of Sardinia,\n\nThe Columbus Atlantic Trophy is a two-foot silver goblet, which was actually given in 1928 to the NYYC by King Alfonso XIII of Spain, as the prize for the 1928 trans-Atlantic yacht race won by NYYC's yacht \"Elena\".\nThe trophy is available to any vessel that can set a trans-Atlantic record one way, then (within a set time-frame) set the corresponding record back. A further stipulation is that the vessel must travel without stopping, and without re-fuelling en route.\n\nAt the time of its inauguration the non-stop east-west record was that set by the American liner \"United States\" in July 1952, on her return to the US, crossing in 84 hours 12 minutes at an average speed of 34.51 knots. The west- east record was that of the SeaCat \"Hoverspeed Great Britain\" in 1990, crossing in 79 hours 55 minutes at an average of 36.96 knots (and beating \"United States\" easterly voyage by 2hr 45 min, adding 1.37knots to the average speed).\n\nIn 1992 \"Destriero\" broke both records, crossing east to west (though by a different route, from Tarifa, Spain to New York) and two weeks later returning at an average speed of 53.09 knots. This also beat the unqualified trans-Atlantic record set by \"Gentry Eagle\" in 1989, which crossed west to east in 62 hour 7 minutes, at an average speed of 47.4 knots. \n\nThis record has not so far been broken.\n\n"}
{"id": "31825960", "url": "https://en.wikipedia.org/wiki?curid=31825960", "title": "Convergence Festival", "text": "Convergence Festival\n\nConvergence Festival is an annual sustainable living festival held in Ireland. Running since 2000, and organised by the Sustainable Ireland Co-Operative, the Convergence Festival features a combination of cultural and educational events celebrating all aspects of sustainability.\n\nThe first Convergence Festival took place in Dublin's Temple Bar in the year 2000. Held every year since (twice in some years) Convergence has featured many important conferences, forums and debates on green building, sustainable economies and resilient communities. The schedule has also included many films, concerts, fashion shows and exhibitions, as well as food and craft fairs.\n\nAccording to the organizers: \"for more than a decade this unique programme of events has celebrated community resilience and social inclusion whilst championing the changes needed to create a sustainable economy, society and built environment.\n\nThe 16th running of the festival, Convergence 16, takes place from 16 to 26 June 2011. The festival has grown to become a nationwide programme events with activities taking place in Dublin, Cork, Sligo, Tipperary and Kilkenny. The focus will be on Ireland's huge potential for developing green jobs and resilient communities. According to the organizers, \"these two exciting areas of opportunity are absolutely key to any plans to reinvigorate this country.\"\n\n\n https://web.archive.org/web/20110702061306/http://cultivate.ie/index.php?option=com_content&view=section&layout=blog&id=88888891&Itemid=88889226\n\n"}
{"id": "12407850", "url": "https://en.wikipedia.org/wiki?curid=12407850", "title": "Critical Power Coalition", "text": "Critical Power Coalition\n\nThe Critical Power Coalition (CPC) is a non-profit consortium whose member companies include Caterpillar Inc.; Cummins Power Generation; MTU Detroit Diesel Power Generation; Digital Power Group; Eaton Corporation; Eaton Corporation/ Powerware Division; EDSA; EnerSys; EYP Mission Critical Facilities; Holder Construction; Liebert/ Emerson Electric; MGE UPS Systems; Power Management Concepts; S&C Electric Company; Siemens Energy & Automation; and Square D/Schneider Electric. \n\nThe CPC's mission is to address operational, technology, and policy issues relating to on-site quality, reliability, and continuity of electric power where and when it is vital. Over the past two decades, data centers, financial institutions, hospitals, airports, telecommunications facilities. emergency response centers, manufacturing plants, have invested in hardware, systems, software, and engineering services to ensure the uninterrupted supply of high-quality, ultra-reliable power to keep critical facilities and equipment operating when grid power is either inadequate, or fails. For example, following the North American Northeast Blackout of 2003, research among data center operators showed that – of 500 companies surveyed – nearly half experienced significant financial losses, with 10% of them experiencing losses exceeding $1 million (U.S.) [1]\n\nThe CPC is dedicated to addressing the continued divergence inherent in two core trends: as conventional grid power inherent limitations and vulnerabilities grow, many private sector and government operations are increasingly dependent on digital equipment that is collaterally dependent on critical-quality electricity.\n\nThe CPC is the first non-profit organization for users and providers of Critical Power, with a mission to: \n\n\n\n"}
{"id": "13566263", "url": "https://en.wikipedia.org/wiki?curid=13566263", "title": "Dukhin number", "text": "Dukhin number\n\nThe Dukhin number () is a dimensionless quantity that characterizes the contribution of the surface conductivity to various electrokinetic and electroacoustic effects, as well as to electrical conductivity and permittivity of fluid heterogeneous systems. \n\nIt was introduced by Lyklema in “Fundamentals of Interface and Colloid Science”. A recent IUPAC Technical Report used this term explicitly and detailed several means of measurement in physical systems.\n\nThe Dukhin number is a ratio of the surface conductivity formula_1 to the fluid bulk electrical conductivity K multiplied by particle size \"a\":\n"}
{"id": "46187865", "url": "https://en.wikipedia.org/wiki?curid=46187865", "title": "Emma Marris", "text": "Emma Marris\n\nEmma Marris (born January 15, 1979) is an American non-fiction writer whose works focus on modern environmentalism. Before becoming an author, she wrote for \"Nature\" for five years. Her book \"Rambunctious Garden: Saving Nature in a Post-Wild World\" focuses on looking at nature as a hybrid of the natural world and human modification.\n"}
{"id": "23516216", "url": "https://en.wikipedia.org/wiki?curid=23516216", "title": "Energistics", "text": "Energistics\n\nEnergistics is a global, non-profit, industry consortium that facilitates an inclusive user community for the development, adoption and maintenance of collaborative, open standards for the energy industry in general and specifically for oil and gas exploration and production.\n\nThe open standards that Energistics encourages the industry to use deliver business value to the upstream oil and natural gas industry through business process efficiencies across the entire exploration and production life cycle. Energistics is a membership organization. The work of the consortium concentrates on helping upstream oil and natural gas companies through the development, support, and promotion of standards that address data definition, handling, storage, and exchange in the context of technology, computing, communications, and business processes.\n\nRegions are a means of building local Energistics communities around the world, with local activities and events. There are eight regions: Africa, Asia Pacific, Eastern Europe, Latin America, Middle East, North America, South Asia, and Western Europe.\n\nThe predecessor of Energistics was formed in October 1990 by five founding sponsor oil companies: BP, Chevron, Elf (since merged into Total), Mobil (since merged into ExxonMobil), and Texaco (since merged into Chevron) under the name Petrotechnical Open Software Corporation (POSC).\n\nThe mission of the new organization was defined as developing, supporting, evolving, and promoting open standards for the scientific, engineering, and operations aspects of the oil and gas exploration and production industry, known as Energy eStandards.\n\nIn the early years, the organization established an open process, acquired resources, and pursued a set of deliverables. The use of the specifications was intended to enable greater quality, consistency, and integration of data and data use. The initial deliverables were known as the Software Integration Platform (SIP) Specifications.\n\nIn 1993, Version 1.0 of the specifications were published as a collection of hard-cover bound books. The published specifications included base computing, data model, data access, data exchange, and user interface.\n\nDuring the next three years, the organization engaged in educational, testing and support activities, including two proof-of-concept implementations of the SIP enabling middleware and a multi-stage, multi-member pilot implementation program called the Industry Implementation Pilot (IIP). The IIP involved both energy company in-house developers and commercial vendor developers building up aspects of an infill drilling scenario.\n\nIn 1996, the board of directors commissioned a study of the benefits of using the SIP specifications, which projected savings of US$1–3 per barrel of oil gained through improvements in data quality, data accessibility, and exploitation of information and knowledge.\n\nAdditions and enhancements to the SIP specifications were published in the following few years, including SIP Version 2.2 in 1997 and software applications interoperability specifications in 1999. During these years, the organization transitioned to a fully member-elected board of directors.\n\nThe SIP Version 2.3 incremental update came out in 2000 and 2001, along with the first XML data schema specifications for basic well data (WellMasterML) and well log display parameters (LogGraphicsML) as well as a series of XML-oriented public seminars. The future course of the organization was shifting from data store and middleware specifications to subject matter data exchange specifications.\n\nThis transition progress in 2002 with the agreement to receive custodianship of the WITSML Standards for drilling data exchange based on XML and Web Services technologies. In the same year, the first of a number of member Special Interest Groups (SIGs) was organized as the user community for subject-specific standards. The subject matter of the first SIG was E&P data stores and their use.\n\nIn 2003, a SIG was formed to support the WITSML Standards. The final release of the SIP Specifications, Version 3.0, came out during that year. Reference standards for both well log data and E&P document and dataset cataloging were published, along with an E&P business process reference model.\n\nDuring 2004, the organization decided to improve the alignment of its name with its mission by redefining the meaning of the name POSC to mean the Petrotechnical Open Standards Consortium.\n\nThe second XML and Web Services family of standards was initiated in August 2005 with the agreement to host the first year of the PRODML, Production XML Markup Language initiative, after which the PRODML SIG was formed. A major new release of the WITSML Standards was released in 2005. Also, an open source data conversion utility for LAS to WITSML well log dataset conversion was developed and released.\n\nBuilding on the most valuable initiatives and an increased emphasis on wide-scale standards adoption, the organization rebranded itself as Energistics in November 2006. This coincided with the release of Version 1.0 of the PRODML Standards and an update to Units of Measure specifications.\n\nIn 2007, a WITSML-based electronic permitting XML schema specifications was published following a multi-year collaboration with US state regulatory agencies in cooperation with API PIDX's REGS EC User Group.\n\nDuring 2008, WITSML Standards, Version 1.4.0 were released. Also, updated application interoperability specifications were submitted by OpenSpirit Corporation, which followed from the previous work in the area published originally in 1999.\n\n2009 saw the formation of the RESQML SIG to address reservoir characterization standards development as a natural successor to the RESCUE Work Group's C++ Class Library. Also, updated PRODML Standards for both data and services specifications were released.\n\nIn 2011 the Standards DevKit was developed by ExxonMobil and is licensed to Energistics for maintenance, support and administration. The DevKit supports the latest versions of WITSML, PRODML and RESQML. Further development will be guided by Energistics and the user community.\n\nIn early 2012, Energistics, along with 11 other standards organizations, formed the Standards Leadership Council (SLC). The intent of the SLC is to formally unite the leaders of organizations that provide open and freely-available standards to the upstream oil and natural gas industry.\nWITSML v1.4.1.1 was published in July 2012 and includes updates and bug fixes to v1.4.1 (published in 2011). A certification program for v1.4.1.1 servers is under development.\n\nIn May 2014, the Energy Industry Profile (EIP) Metadata Standard was published. EIP is an open, non-proprietary metadata exchange standard designed to document structured and unstructured information resources of importance to members of the energy community and to maximize metadata interoperability within the industry.\n\nIn July 2014, the Unit of Measure Standard V1.0 was published. This includes the Unit of Measure Dictionary and the Unit Symbol Grammar Specification. Contributors to this standard include the Energistics community, The Professional Petroleum Data Management (PPDM™) Association and the Society of Exploration Geophysicists (SEG). Also published in 2014 was the Energy Industry Profile (EIP) v1.0 of ISO 19115-1. During the summer and fall of 2014, PRODML v1.3 and RESQML v2.0 were published.\n\nIn 2015, a game changer standard was published - Energistics Transfer Protocol (ETP) v1.0 was released to the industry. ETP enables the efficient transfer of data between applications and is a part of the Common Technical Architecture (CTA) used by WITSML, RESQML and PRODML.\n"}
{"id": "48629578", "url": "https://en.wikipedia.org/wiki?curid=48629578", "title": "Energy in Myanmar", "text": "Energy in Myanmar\n\nMyanmar had a total primary energy supply (TPES) of 16.57 Mtoe in 2013. Electricity consumption was 8.71 TWh. 65% of the primary energy supply consists of biomass energy, used almost exclusively (97%) in the residential sector. Myanmar’s energy consumption per capita is one of the lowest in Southeast Asia. Contributing factors are the low income and the low electrification rate. Energy consumption is however growing rapidly, with an average annual growth rate of 3.3% from 2000 to 2007.\n\nMost of electricity (74.7%) is produced by hydroelectricity. The rest is from fossil fuels, with gas as the main fuel (20.5%) followed by coal and oil.In 2011, Myanmar had an installed electricity generation capacity of about 3,344 MW, with a low electrification rate of 27%. Electrification rate is especially low in rural villages, which are mainly not connected to the power grid. Firewood is used as a primary source of energy in these areas, a contributing factor to the observed decrease in forests in the country.\n\nMyanmar has abundant energy resources, particularly hydropower and natural gas. In 2013, Myanmar exported 8561 ktoe of natural gas and 144 ktoe of crude oil. The country is one of the five major energy exporters in the region and is the second biggest exporter of natural gas in the Asia Pacific region after Indonesia. According to the World Energy Council, gas reserves are estimated at 244 Mtoe. Oil and coal play a smaller role with reserves estimated at 7 and 1 Mtoe, respectively.\n\nHydropower resources are estimated to be about 40 GW at a capacity factor of 0.40, giving a total yearly hydropower generation capacity of about 140 TWh. Installed hydropower capacity as of 2011 was 1.54 GW with a total generation of 3.9 TWh, there is therefore substantial opportunity for further growth of this energy source.\nThe Shweli 1 hydroelectric power plant, with a capacity of 600 MW, started operation in 2008. The Yeywa hydropower plant opened in 2010 with a capacity of 790 MW, the largest in the country. Several other hydropower projects are under construction or planned. Planned major hydropower plants have been designed mainly for export. The Myitsone Dam project, with a capacity of 6,000 MW, is expected to supply 100% of its electricity to China, while the Tasang Dam project with a planned capacity of 7,110 MW is planned to supply 1,500 MW to Thailand.\n\n"}
{"id": "12661414", "url": "https://en.wikipedia.org/wiki?curid=12661414", "title": "Field emitter array", "text": "Field emitter array\n\nA field emitter array (FEA) is a particular form of large-area field electron source. FEAs are prepared on a silicon substrate by lithographic techniques similar to those used in the fabrication of integrated circuits. Their structure consists of a very large number of individual, similar, small field electron emitters, usually organized in a regular two-dimensional pattern. FEAs need to be distinguished from \"film\" or \"mat\" type large-area sources, where a thin film-like layer of material is deposited onto a substrate, using a uniform deposition process, in the hope or expectation that (as a result of statistical irregularities in the process) this film will contain a sufficiently large number of individual emission sites.\n\nThe original field emitter array was the \"Spindt array\", in which the individual field emitters are small sharp molybdenum cones. Each is deposited inside a cylindrical void in an oxide film, with a counterelectrode deposited on the top of the film. The counterelectrode (called the \"gate\") contains a separate circular aperture for each conical emitter. The device is named after Charles A. Spindt, who developed this technology at SRI International, publishing the first article describing a single emitter tip microfabricated on a wafer in 1968.\nSpindt, Shoulders and Heynick filed a U.S. Patent in 1970 for a vacuum device comprising an array of emitter tips.\n\nEach individual cone is referred to as a \"Spindt tip\". Because Spindt tips have sharp apices, they can generate a high local electric field using a relatively low gate voltage (less than 100 V). Using lithographic manufacturing techniques, individual emitters can be packed extremely close together, resulting in a high average (or \"macroscopic\") current density of up to 2×10 A/m . Spindt-type emitters have a higher emission intensity and a more narrow angular distribution than other FEA technologies.\n\nNano-Spindt arrays represent an evolution of the traditional Spindt-type emitter. Each individual tip is several orders of magnitude smaller; as a result, gate voltages can be lower, since the distance from tip to gate is reduced. In addition, the current extracted from each individual tip is lower, which should result in improved reliability.\n\nAn alternative form of FEA is fabricated by creating voids in an oxide film (as for a Spindt array) and then using standard methods to grow one or more carbon nanotubes (CNTs) in each void.\n\nIt is also possible to grow \"free-standing\" CNT arrays.\n\nEssentially very small electron beam generators, FEAs have been applied in many different domains. FEAs have been used to create flat panel displays (where they are known as field emission displays (or \"nano-emissive displays\"). They may also be used in microwave generators, and in RF communications, where they could serve as the cathode in traveling wave tubes (TWTs).\n\nRecently, there has been renewed interest in using field effect arrays as cold cathodes in X-ray tubes. FEAs offer a number of potential advantages over conventional thermionic cathodes, including low power consumption, instantaneous switching, and independence of current and voltage.\n\n"}
{"id": "15027508", "url": "https://en.wikipedia.org/wiki?curid=15027508", "title": "Hail spike", "text": "Hail spike\n\nA hail spike or three body scatter spike (TBSS) is an artifact on a weather radar display indicative of large hail. They are identified by a spike of weak reflectivity echoes that extend out from a thunderstorm, and away from the radar site.\n\nGenerally known as hail spikes, these are the result of energy from the radar hitting hail, or very heavy rain, and being reflected to the ground, where they reflect back to the hail and then to the radar as in the image on the left. This results in the radar picking up the energy from the multiple path at a later time than the energy that came back directly from the hail to the radar. Both are however on the same radial angle from the radar as the antenna did not have the time to turn significantly. \n\nThe multipath echoes are then analyzed on the radar display as echoes extending in a radial direction behind the actual location of the hail/heavy rain core. The loss of energy by due to multiple reflections means weaker return echoes. The hail spike region has thus comparatively quite weaker echoes than the echoes directly from the hail or heavy rain core.\n\nSince hail cores are most intense at higher elevations, hail spikes usually appear at the levels aloft that accompany the most intense hail. Because of this, hail spikes are usually not seen at lower elevations. Another restriction to detection is that the signal of the radar beam has to do multiple reflections, each time weakening it, and are therefore usually noticeable only in extremely large hailstone cases.\n\nIn rare cases more than one hail spike has been documented with a single storm in one volume scan. This indicates that more than one hail core exists in the same storm and far enough apart that each core can be sampled by the radar individually.\nBecause of their observed accuracy in indicating large hail aloft, TBSS's are used operationally by the National Weather Service to identify thunderstorms that could likely produce large, severe hail. This would warrant the issuance of a severe thunderstorm warning or mention of large hail in a tornado warning.\n\n\n"}
{"id": "57525289", "url": "https://en.wikipedia.org/wiki?curid=57525289", "title": "Institut für Meereskunde Kiel", "text": "Institut für Meereskunde Kiel\n\nThe Institut für Meereskunde (IfM, Institute of Marine Sciences) in Kiel, Germany, existed from April 1, 1937 to January 1, 2004. It was an essential element of the long history of marine sciences in Kiel. This history started with the work of published in 1697 and is today continued within the GEOMAR Helmholtz Centre for Ocean Research Kiel.\n\nEarly work in physical/chemical oceanography started with Samuel Reyher, professor at the \"Christian Albrecht [University of Kiel]\" (CAU), when he published his investigation on salinity in the ice-covered Kiel Fjord in 1697. Other professors of Kiel University followed with work on marine topics. (1736-1807) studied tides of the North Sea, (1773-1852) studied hydrographic and chemical conditions in the western Baltic Sea, and (1798-1848) discussed optical effects in the sea.\n\nSystematic marine research, however, began only later during the 19th century with biological studies, most notably by the zoologist (1825-1908), the physiologist Victor Hensen (1835-1924), the marine production biologist Karl Brandt (1854-1931), and the zoologist (1808-1878) who participated in the first of the Danish Galathea expeditions (1845-1847). The in the Atlantic Ocean on the ’’National’’ in 1889 was the beginning of deep-sea expeditions carried out by Kiel scientists.\n\nSystematic physical and chemical marine studies also started during the second half of the 19th century. Two people need to be acknowledged in particular. The merchant and industrialist (1822-1889) from Hamburg managed to convene a group united by marine research interests. The physicist Gustav Karsten (1820-1900) from Kiel belonged to this group. A long-term cooperation developed among these two people. Beginning in 1859, Meyer carried out first hydrographic observations in Kiel Fjord and started systematic measurements for the investigation of seasonal changes in the Baltic Sea. Meyer received his honorary doctor from Kiel University in 1866.\n\nA major step was the establishment of the “Prussian Commission for the Scientific Investigation of German Seas in Kiel“ (also called the \"Kiel Commission\"; in German: \"Preußische Kommission zur wissenschaftlichen Untersuchung der deutschen Meere in Kiel\"). Meyer was a founding member and held the first chair until 1880, with Karsten being his successor until 1896. Möbius and Hensen were also members of the commission. The main objective was the improvement of fisheries. But it became evident soon that this goal could not be reached without an improved knowledge of the hydrography and the development of new approaches and methods. Subsequently the Kiel Commission played an important role in the development of marine sciences in Gemany. A \"Laboratory for international marine science\" (in German: \"Laboratorium für die internationale Meeresforschung\") was established by the Commission in 1902. Furthermore, the geographer Otto Krümmel became professor at Kiel University in 1883, strengthening hydrographic work. He wrote the first textbook (2 volumes) on oceanography in German language. The Institute of Zoology and Zoological Museum of Kiel University became the center of marine biological research, led by (1898-1976) from 1924 to 1934. In addition, Kiel was the home of outstanding scientists and engineers in marine acoustics as part of industry and navy, in particular Alexander Behm (1880-1952), Hugo Lichte (1891-1963) and (1880-1961).\n\nThe University of Kiel decided to concentrate marine sciences within an institution of the university, and the \"Institut für Meereskunde\" (IfM) was founded in 1937. Activities of the former laboratories of the Kiel Commission were included in the new institution. The institute had three departments: Biology, Hydrography and Chemistry, and Geology. In addition to disciplinary work, interdisciplinary approaches took root. Studies in the Baltic Sea were supposed to be in the focus, but not exclusively. A building in Kitzeberg on the eastern shore of Kiel Fjord was provided. Remane was director of the IfM from 1937 to 1944. The marine chemist (1901-1944) became his successor on May 1, 1944. He perished shortly after the appointment on July 24, 1944, together with eight other members of the institute, when bombs destroyed the institute’s building during World War II.\n\nAfter the end of the war in 1945, work at the IfM already started again in 1946.Georg Wüst (1890-1977) became Professor of Oceanography and Meteorology at the CAU and Director of the IfM. He had been a scientific leader at the former Institute and Museum of Marine Sciences at Berlin University (in German: “Institut und “ and had mostly performed research on the circulation of water masses in the global ocean and in particular in the deep Atlantic. In Kiel he succeeded in retrieving several earlier members of the institute, getting the permission to use an old villa on the western shore of Kiel as an institute’s building, obtaining the research cutter “Südfall“ and restarting educational programs. He began research work in the Baltic Sea with a total staff of 15.\n\nAfter Wüst’s retirement Günter Dietrich (1911-1972) became his successor. He had been a student at the University of Berlin and had also studied the circulation of water masses in the Atlantic Ocean with Georg Wüst and Albert Defant at the former Berlin institute. Wüst’s and Dietrich’s experience in deep ocean studies strengthened the orientation of the IfM work towards the open and deep ocean, in particular in physical and chemical oceanography. The IfM became the leading marine research institution in West Germany during those years, carrying on the scientific legacy of the former Berlin institute in research and education. Technically, the successor of the earlier Museum of Marine Sciences in Berlin today is the German Museum of Technology in Berlin.\n\nAn important milestone in the IfM’s development was the commission of the new research vessel , operated jointly by the German Hydrographic Office (see Federal Maritime and Hydrographic Agency of Germany) and the German Research Foundation (see Deutsche Forschungsgemeinschaft. The ship was used by all interested research groups in Germany. The first major expedition 1964/65 to the Red Sea and Indian Ocean was part of the \"International Indian Ocean Expedition\". The German contribution was led by Günter Dietrich and , both from Kiel University, and the majority of scientists and technicians were based in Kiel. This expedition was the restart of deep-sea oceanography in Kiel.\n\nThe financial needs of the IfM were increasing with its tasks and number of personnel, having increased to 124 in the year 1968. In order to obtain additional funding from the federal and other state governments, the IfM joined the group of “blue list” research institutions (see Leibniz Association in 1968. New statutes introduced a collegial system, with the acting director being elected for two-year terms by a panel composed of department heads and employee representatives. Although the funding was no longer related to the university budget, strong ties to the CAU prevailed. Professors were selected jointly by the IfM and the CAU.\n\nDuring the following years the IfM became partner in a considerable number of international research programs. A relevant number of IfM scientists worked at foreign research institutions for extended time periods, primarily in the USA. Later a large number of foreign investigators joined the IfM as visiting scientists. The number of employees was increasing steadily.\n\nIn the meantime marine geology and geophysics groups had developed parallel to the IfM groups at the CAU. Parts of these activities were combined in the new geoscience institute “Geomar” established in 1987. At the end of 2001 Geomar had 153 members of staff. The staff of the IfM had increased to 252 by that time. With the aim of achieving an increasing collaboration of marine disciplines in Kiel, the “IfM” and “Geomar” joined organizationally to form “IFM-GEOMAR” as part of the “Leibniz Association” on January 1, 2004. The new institution had 389 members of staff at its beginning. Due to fiscal considerations, IFM-GEOMAR was transferred to an institution of the Helmholtz Association of German Research Centres on January 1, 2004, thus being primarily funded by the federal government. The name changed to GEOMAR Helmholtz Centre for Ocean Research Kiel.\n\nIn addition to Kiel vessels, the IfM staff frequently used all of the larger research ships available in Germany:, , , , , ).\n\nThe “German Research Foundation” (Deutsche Forschungsgemeinschaft) supported long-term special research programs (SFBs). IfM scientists participated in several of these SFBs, often in a leadership function:\n\nThe participation of the IfM in international research programs was of major importance. Among those programs were:\n\nICES Overflow ’73 \n\nICES International Overflow Expedition 1960 \n\nInternational Indian Ocean Expedition, IIOE \n\nJoint Air Sea Interaction Study, JASIN \n\nBALTIC 75 Experiment\n\nGlobal Atmospheric Research Program, GARP\n\nGARP Atlantic Tropical Experiment, GATE\n\nFirst International Biomass Experiment, FIBEX, 1980/81\n\nJoint Global Ocean Flux Study, JGOFS\n\nWorld Ocean Circulation Experiment, WOCE \n"}
{"id": "55509020", "url": "https://en.wikipedia.org/wiki?curid=55509020", "title": "Kawanda–Birembo High Voltage Power Line", "text": "Kawanda–Birembo High Voltage Power Line\n\nKawanda–Birembo High Voltage Power Line is a high voltage electricity power line, under construction, connecting the high voltage substation at Kawanda, in Uganda to another high voltage substation at Birembo, in Rwanda.\n\nThe 220 kilo Volt power line starts at the UETCL power station at Kawanda, Wakiso District, in Uganda's Central Region, approximately , by road, north of Kampala, the capital and largest city of Uganda. From here, the line travels to the southwestern Ugandan town of Masaka, a straight-line distance of about . From Masaka, the power line continues west to the town of Mbarara, a straight distance of approximately . From Mbarara the power line travels in a general southerly direction to the town of Mirama Hills, a distance of about . From a substation in the Mirama Hills/Kagitumba neighborhood, the power line continues in a southwesterly direction to end at a substation in Birembo, Kinyinya Sector, Gasabo District, Rwanda, in the northern suburbs of Kigali, the capital and largest city of Rwanda, a straight-line distance of about .\n\nThis power transmission line connects the electricity grid of Uganda to that of neighboring Rwanda. It is in line with the Nile Equatorial Lakes Subsidiary Action Program, Interconnection of Electric Grids Project, led by Regional Manager, Grania Rubomboras. The power line is being developed in tandem with Karuma Hydroelectric Power Station, whose capacity output of 600MW is expected to be consumed locally and the balance sold regionally, with Rwanda, Burundi and the Democratic Republic of the Congo as potential customers.\n\nThe project on the Uganda side is divided into three sections:\n(a) the Kawanda–Masaka section, measuring about (b) the Masaka–Mbarara section, measuring about and (c) the Mbarara–Mirama Hills section, measuring about .\n\nThe Kawanda–Masaka section was contructed at a budgeted cost of US$ 153.20 million, of which the World Bank lent US$ 120 million. Completion was expected in January 2019. However, in July 2018, the Daily Monitor reported that the 220kV line had been commissioned.\n\nThe Masaka–Mbarara section was budgeted at €50 million, to be borrowed from the European Union Africa Infrastructure Fund. Work is expected to start in the fourth quarter of 2017 and is expected to conclude in 2019. In March 2018, The Uganda Independent reported that the Ugandan government borrowed €37.1 million from the French Development Agency and another €35 million from the German Development Bank to finance the Masaka-Mbarara section of this transmission line. The Mbarara–Mirama Hills section was completed in 2015.\n\nThe Mirama Hills/Kagitumba–Birembo section measures approximately . The Mbarara–Birembo section measures about . As reported by the EastAfrican in May 2015, this 220kV network already exists. Rwanda is also in the process of building a 220kV substation in Birembo.\n\nAt a later date, the entire Kawanda–Birembo High Voltage Power Line is expected to be upgraded to 400kV. In May 2018, the Ugandan government borrowed €37.1 million (about US$44.2 million), from the French Development Agency (AFD), to upgrade the , between the towns of Masaka and Mbarara to 400kV.\n\n\n"}
{"id": "49310869", "url": "https://en.wikipedia.org/wiki?curid=49310869", "title": "List of Fusarium species", "text": "List of Fusarium species\n\n\"Fusarium\" species include:\n"}
{"id": "3397403", "url": "https://en.wikipedia.org/wiki?curid=3397403", "title": "List of Lepidoptera that feed on grapevines", "text": "List of Lepidoptera that feed on grapevines\n\nGrapevines (\"Vitis\" species) are used as food plants by the caterpillars of several Lepidoptera (butterflies and moths). These include:\n\nMonophagous species which feed exclusively on \"Vitis\":\n\nPolyphagous species which feed on \"Vitis\" among other plants:\n\n"}
{"id": "37019651", "url": "https://en.wikipedia.org/wiki?curid=37019651", "title": "List of equations in wave theory", "text": "List of equations in wave theory\n\nThis article summarizes equations in the theory of waves.\n\nA wave can be longitudinal where the oscillations are parallel (or antiparallel) to the propagation direction, or transverse where the oscillations are perpendicular to the propagation direction. These oscillations are characterized by a periodically time-varying displacement in the parallel or perpendicular direction, and so the instantaneous velocity and acceleration are also periodic and time varying in these directions. (the apparent motion of the wave due to the successive oscillations of particles or fields about their equilibrium positions) propagates at the phase and group velocities parallel or antiparallel to the propagation direction, which is common to longitudinal and transverse waves. Below oscillatory displacement, velocity and acceleration refer to the kinematics in the oscillating directions of the wave - transverse or longitudinal (mathematical description is identical), the group and phase velocities are separate.\n\nRelation between space, time, angle analogues used to describe the phase:\n\nformula_1\n\nIn what follows \"n, m\" are any integers (Z = set of integers); formula_2.\n\nGravitational radiation for two orbiting bodies in the low-speed limit.\n\nA common misconception occurs between phase velocity and group velocity (analogous to centres of mass and gravity). They happen to be equal in non-dispersive media. In dispersive media the phase velocity is not necessarily the same as the group velocity. The phase velocity varies with frequency.\n\nIntuitively the wave envelope is the \"global profile\" of the wave, which \"contains\" changing \"local profiles inside the global profile\". Each propagates at generally different speeds determined by the important function called the \"Dispersion Relation\". The use of the explicit form \"ω\"(\"k\") is standard, since the phase velocity \"ω\"/\"k\" and the group velocity d\"ω\"/d\"k\" usually have convenient representations by this function.\n\n\nComplex amplitude of wave \"n\"\nformula_3\n\nResultant complex amplitude of all \"N\" waves\nformula_4\n\nModulus of amplitude\nformula_5\n\nThe transverse displacements are simply the real parts of the complex amplitudes.\n\n1-dimensional corollaries for two sinusoidal waves\n\nThe following may be deduced by applying the principle of superposition to two sinusoidal waves, using trigonometric identities. The \"angle addition\" and \"sum-to-product\" trigonometric formulae are useful; in more advanced work complex numbers and fourier series and transforms are used.\n\n\n\n"}
{"id": "17500066", "url": "https://en.wikipedia.org/wiki?curid=17500066", "title": "List of low-energy building techniques", "text": "List of low-energy building techniques\n\nLow-energy buildings, which include zero-energy buildings, passive houses and green buildings, may use any of a large number of techniques to lower energy use.\n\nThe following are some of the techniques used to achieve low-energy buildings, which excludes energy generation (microgeneration).\n\n\n\n"}
{"id": "55974555", "url": "https://en.wikipedia.org/wiki?curid=55974555", "title": "List of major snow and ice events in the United States", "text": "List of major snow and ice events in the United States\n\nThe following is a list of major snow and ice events in the United States that have caused noteworthy damage and destruction in their wake. The categories presented below are not used to measure the strength of a storm, but are rather indicators of how severely the snowfall affected the population in the storm's path. Some information such as snowfall amounts or lowest pressure may be unavailable due to a lack of documentation. Winter storms can produce both ice and snow, but are usually more notable in one of these two categories. The \"Maximum accumulation\" sections reflect the more notable category which is represented in inches of snow unless otherwise stated.\n\n\nThe following is a table that shows winter season summaries dating back to 2009. While there is no well-agreed-upon date used to indicate the start of winter in the Northern Hemisphere, there are two definitions of winter which may be used. The first is astronomical winter, which has the season starting on a date known as the winter solstice, often on or around December 21. The season lasts until the spring equinox, which often occurs on or around March 20. The second has to do with meteorological winter which varies with latitude for a start date. Winter is often defined by meteorologists to be the three calendar months with the lowest average temperatures. Since both definitions span the calendar year, it is possible to have a winter storm in two different years.\n\n"}
{"id": "386779", "url": "https://en.wikipedia.org/wiki?curid=386779", "title": "List of mountains in Korea", "text": "List of mountains in Korea\n\nThe following is a list of mountains in Korea:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "42588379", "url": "https://en.wikipedia.org/wiki?curid=42588379", "title": "List of power stations in Nepal", "text": "List of power stations in Nepal\n\nThe following is a list of the power stations in Nepal.\n\n\nUpcoming Hydro-power Projects in Nepal\n\nSource: Bidhyut Magazine/Semi- Annual Report - NEA, Bhadra 2063; NEA Annual Report 2073 B.S.\n\n"}
{"id": "8800899", "url": "https://en.wikipedia.org/wiki?curid=8800899", "title": "List of protoplanetary nebulae", "text": "List of protoplanetary nebulae\n\nThis is a list of protoplanetary nebulae. These objects represent the final stage before a planetary nebula. During this stage, the red giant star begins to slowly expel its outermost layers of material. A protoplanetary nebula usually glows with the light from its parent star. This stage is usually brief, typically lasting no more than a few thousand years.\n"}
{"id": "8591785", "url": "https://en.wikipedia.org/wiki?curid=8591785", "title": "List of stars in Pavo", "text": "List of stars in Pavo\n\nThis is the list of notable stars in the constellation Pavo, sorted by decreasing brightness.\n\n\n"}
{"id": "25582624", "url": "https://en.wikipedia.org/wiki?curid=25582624", "title": "Mackenzie Large Igneous Province", "text": "Mackenzie Large Igneous Province\n\nThe Mackenzie Large Igneous Province (MLIP) is a major Mesoproterozoic large igneous province of the southwestern, western and northwestern Canadian Shield in Canada. It consists of a group of related igneous rocks that were formed during a massive igneous event starting about 1,270 million years ago. The large igneous province extends from the Arctic in Nunavut to near the Great Lakes in Northwestern Ontario where it meets with the smaller Matachewan dike swarm. Included in the Mackenzie Large Igneous Province are the large Muskox layered intrusion, the Coppermine River flood basalt sequence and the massive northwesterly trending Mackenzie dike swarm.\n\nAs a large igneous province, it is an extremely large area of related igneous rocks that were emplaced over an extremely short geological time span. The igneous rocks comprising the Mackenzie Large Igneous Province originated from processes not associated with normal plate tectonics and seafloor spreading. It is one of the several large igneous provinces scattered throughout the Canadian landscape, which can be thousands of kilometres in volume and area. The Mackenzie Large Igneous Province is one of the world's largest Proterozoic magmatic provinces, as well as one of the most well-preserved continental flood basalt terrains on Earth. Igneous rocks of the Mackenzie Large Igneous Province are generally mafic in composition, including basalt and gabbro.\n\nEven though the Mackenzie Large Igneous Province is classified as a large igneous province like other extremely large accumulations of igneous rocks on Earth, it is much larger than large igneous province standards. The standard size classification for large igneous provinces is a minimum areal extent of . However, the Mackenzie dike swarm itself occupies an area of at least , making the Mackenzie Large Igneous Province larger than the Ontong Java Plateau (in the southwestern Pacific Ocean) and the U.S. state of Alaska.\n\nLike most large igneous provinces, the Mackenzie Large Igneous Province has its origins in a mantle plume—an upwelling zone of abnormally hot rock within the Earth's mantle. As the head of the Mackenzie plume encountered the Earth's lithosphere, it spread out and melted catastrophically to form large volumes of basaltic magma. This resulted in the creation of a stationary volcanic zone west of Victoria Island that experienced considerable volcanism known as the Mackenzie hotspot. Evidence for the Mackenzie hotspot include the existence of the giant mafic Mackenzie dike swarm because of its fanning pattern adjacent to the Muskox intrusion.\n\nThe size of the Mackenzie hotspot is considered to have been about in diameter. This calculation is based on the analysis of magmatic fabric in the Mackenzie dike swarm, which shows that magma flow was only vertical close to the middle of the Mackenzie plume and only subhorizontal away from the plume. However, if subhorizontal flow is a result of dike ascent to a level of impartial lightness in the Earth's crust, it would not be related to the size of the Mackenzie plume. Instead, the analysis of dike swarm geometry could possibly maintain evidence for the smallest diameter of the Mackenzie plume. The outer limit separating the zone of fanning dike geometry and subparallel dikes might be suggestive of the smallest diameter for the Mackenzie plume because it is not probable that the stress related to a magmatic zone has consequence over a region that is lesser than the Mackenzie plume, which created the feature. From this analysis, the smallest diameter of the Mackenzie plume would have been about . Uranium-lead dating of certain Mackenzie dikes from an array of distances from an assigned focal point give an age of million years. This indicates that the Mackenzie hotspot essentially emplaced the Mackenzie Large Igneous Province as a whole throughout the associated landscape. The associated Mackenzie plume is consistent with mantle plumes that have deep origins within the Earth's mantle.\n\nThe Mackenzie hotspot is interpreted to have been similar to the early volcanism of the Yellowstone hotspot. Both hotspots produced massive qualities of basaltic lava flows that were identical with the formation of dike swarms during a short period of time at the beginning of mantle plume volcanism. It is estimated that the majority of volcanism that formed the Mackenzie Large Igneous Province took place no more than two million years, and subsequent volcanism is unknown. However, the younger and smaller Franklin Large Igneous Province just to the northeast is considered to have been formed by a similar mantle plume between 727 and 721 million years ago. The short time span of two million years for magma emplacement in the Mackenzie Large Igneous Province is also present for the Yellowstone hotspot.\n\nAt the beginning of the Mackenzie magmatic event, the Mackenzie hotspot collided with lithosphere that was already in an extensional regime that allowed rifting to occur. Passive rifting has been interpreted as the mechanism that produced the opening of the former Poseidon Ocean, the geometry of which would have been partly controlled by dike swarm geometry. Fahrig (1987) proposed that the Mackenzie plume impact resulted in the emplacement of a triple junction that had a large mafic dike swarm on every rift arm. Two of the first arms formed the Poseidon Ocean basin and the third arm failed thus forming an aulacogen. This tectonic setting suggestion can be comparable with the early volcano-tectonic evolution of the Yellowstone hotspot, which developed two arms instead of three, followed by failure of both arms. At the Mackenzie hotspot, rifting is considered to have been passive and to have taken place in the crust above the hotspot that should have been weakened by the Mackenzie plume. Crustal uplift may have also provided stresses contributing to rifting.\n\nA slightly younger but possibly related geologic feature is the long Midcontinent Rift System adjacent to the southern end of the Mackenzie Large Igneous Province. The Lake Superior portion of the Midcontinent Rift System is bounded on the south by pre-existing continental faults that had substantial right-lateral movement before the formation of the Midcontinent Rift System. This period of rifting was a large event for copper mineralization, and the rifting event later deceased when the Grenville orogeny collision occurred.\n\nThe massive extent of the Mackenzie Large Igneous Province contains a number of magmatic features that were formed during the extensive Mackenzie magmatic event. This includes flood basalts, layered intrusions, sills and dikes, which are widespread throughout the large igneous province. With an area of , the Mackenzie event is the largest magmatic event ever to occur on the Canadian landscape. At least two magmatic formations can be considered large igneous provinces in their own advantage, both of which cover an area of more than .\n\nAdjacent to McGregor Lake in western Nunavut lies the massive Muskox intrusion. It remains as one of the largest and most studied layered intrusions on Earth, as well as one of the most valuable from an economic perspective. The intrusion represents the oldest igneous formation of the Mackenzie magmatic event, having formed between 1,905 and 1,155 million years ago. It maintains a triangular trough-shaped magma chamber that extends below the surface. With a width of and a length of over , the Muskox intrusion is overlain by a sequence of Coppermine flood basalts that remains thick.\n\nThe Muskox intrusion can be separated into three sections, including an olivine gabbro feeder dike to the intrusion, another contact margin zone, and an upper layered series. Because of different levels of erosion and outcroppings of the Muskox intrusion at higher structural levels, north of the Coppermine River the overlying margin zone and layered series covers the feeder dike section. The layering series dips gently north as do the overlying Coppermine flood basalts. Weathering of the Muskox dipping sequence has exposed a cross-section through the entire dipping sequence, starting with the Coppermine flood basalts in the north above the roof of the magma chamber, down through the igneous layering of the Muskox intrusion and into the keel region of the intrusion and its intersection with the olivine gabbro feeder dike that forms the southernmost sector. The margin zone characterizes the western and eastern outer limits of the intrusion.\n\nWidespread throughout the Mackenzie Large Igneous Province is the Mackenzie dike swarm. This extensive group of radially oriented dikes is more than wide and long, extending from Northwestern Ontario through northern Manitoba and northern Saskatchewan to Nunavut and the Northwest Territories. The Mackenzie dike swarm was emplaced into older metamorphic and igneous rocks of the Canadian Shield about 1,200 million years ago. Individual dikes of the Mackenzie swarm are respectively to long and thick. This indicates that the Mackenzie dikes are larger than those associated with the Columbia River Basalt Group in the United States, which are respectively to long and to thick. The size differentiation of the Columbia River and Mackenzie dikes suggests that the crude estimates for both dike length and thickness ratio are within the ranges for the Mackenzie hotspot and for the early stage of the Yellowstone hotspot.\nThe Mackenzie dike swarm is the largest dike swarm known on Earth and is one of the several dike swarms found throughout the Canadian Shield. Mafic dikes cut Archean and Proterozoic rocks of the Canadian Shield, including those in the Athabasca Basin in Saskatchewan, the Thelon Basin in Nunavut and the Baker Lake Basin in the Northwest Territories. The mafic dikes display evidence that the unmetamorphosed basin-fill sequence was deposited before the Mackenzie dikes were intruded into the associated basins. When the giant Mackenzie dike swarm intruded into the Canadian Shield, it partly uplifted and intruded the Slave craton in the Northwest Territories and Nunavut. This was the last major event to affect the core of the Slave craton, although later on some younger mafic magmatism registered along its boundaries. This includes the magmatic events that formed the 723 million year old Franklin Large Igneous Province and the 780 million year old Hottah gabbro sheets. Since the Mackenzie dike swarm intruded the Slave craton, the craton has been repeatedly submerged under seas.\n\nIn northern Yukon, the 1,265 to 1,269 million year old Bear River dikes are interpreted to represent the western extension of the Mackenzie dike swarm. They display geologic similarities with the Mackenzie dike swarm and the Coppermine River flood basalts, and are therefore regarded as products of the Mackenzie plume. The dikes intrude through Early Proterozoic sedimentary strata of the Wernecke Supergroup, some of which exist as separate intrusions while others occur in swarms of up to eight dikes. Individual dikes range from to thick and up to long. Medium to fine grained diorite and gabbro comprise the Bear River dikes and are occasionally altered by metamorphism to form greenschist. Apart from two dikes that display differentiation, such as containing weak penetrative foliation of unknown age and origin and being cross-cut by undated hematitic veins, the Bear River dikes are interpreted to have formed during a single magmatic phase.\n\nBetween 1,200 and 740 million years ago, a series of flood basalt eruptions took place. At the northern portion of the Mackenzie Large Igneous Province, vast volumes of basaltic lava paved over a large area of the northwestern Canadian Shield. This extensive volcanism constructed a large lava plateau with an area of , representing a volume of lavas of at least . This extensive area of flood basalt lava flows has been termed the Coppermine River flood basalts hence given the location of the flood basalt sequence. With an area of and a volume of , the Coppermine River flood basalt sequence is larger than the Columbia River Basalt Group in the United States and comparable in size to the Deccan Traps in west-central India. This makes the Coppermine River flood basalts one of the largest flood basalt events ever to appear on the North American continent, as well as on Earth. The maximum thickness of the Coppermine River flood basalts is and consist of 150 lava flows, each to thick.\n\nThe Coppermine River flood basalts were extruded shortly after a period of crustal uplift that later resulted in a short period of collapse in the associated landscape. This sudden uplift was likely caused by rising magma of the Mackenzie plume, which later resulted in the appearance of the Mackenzie hotspot. The early Muskox intrusion is considered to have originally been a sill-shaped magma reservoir for the overlying Coppermine River flood basalts during their formation. With the Coppermine River basalts comprising more than 100 individual lava flows, the potential volumes of silicate magma that moved through the Muskox conduit were in the order of .\n\nFurther to the northeast, the Nauyat Formation flood basalts on northwestern Baffin Island of Nunavut were erupted on a smaller scale about 900 million years ago. These flood basalts reach a thickness of . Just southeast of the Queen Maud Gulf, the Ekalulia Formation flood basalts remain to thick. They appear green in colour and contain the magnesium iron silicate mineral olivine. Minor pillow lavas also exist in the Ekalulia flood basalts.\n\nHeavy platinum group elements and copper mineralization exists in the basal margin of the Muskox intrusion. Research operated by Muskox Minerals Corp. proclaims that this extensive layered intrusion has the possibility to evolve into a massive expected source of copper, nickel and platinum group metals. The expected mineralization potential for the Muskox intrusion is supported as a result of its strong similarities to the Noril'sk-Talnakh intrusions in Siberia where the richest orebodies exist on Earth. Because the Muskox intrusion has strong similarities to the Noril'sk-Talnakh area in Siberia, a number of mineral explorations have taken place. The first mineral exploration of the Muskox intrusion occurred during the 1950s when surface prospecting began by the International Nickel Company of Canada, which is now known as Vale Inco. In the 1980s, many small companies with little financing and fragmented claim blocks attended sampling and a number of drilling operations on outcroppings of the Muskox intrusion that contained platinum group elements.\n\nExposed portions of the feeder dike south of the Coppermine River comprise bodies of large semi-massive and disseminated copper-nickel sulfides rich in platinum group metals. At the margins of the Muskox intrusion, sulfide bearing zones more than long contain palladium, platinum, gold, copper and nickel. This is the location where casual mineral exploration companies searched the Muskox intrusion in the past.\n\n"}
{"id": "23129145", "url": "https://en.wikipedia.org/wiki?curid=23129145", "title": "Manganosite", "text": "Manganosite\n\nManganosite is a rare mineral composed of manganese(II) oxide MnO. It was first described in 1817 for an occurrence in the Harz Mountains, Saxony-Anhalt, Germany. It has also been reported from Langban and Nordmark, Sweden and at Franklin Furnace, New Jersey. It also occurs in Japan, Kyrgyzstan and Burkina Faso.\n\nIt occurs in manganese nodules. It also occurs as alteration of manganese minerals such as rhodocrosite during low oxygen metamorphism and metasomatism.\n"}
{"id": "31885413", "url": "https://en.wikipedia.org/wiki?curid=31885413", "title": "Marine technology", "text": "Marine technology\n\nMarine technology is defined by WEGEMT (a European association of 40 universities in 17 countries) as \"technologies for the safe use, exploitation, protection of, and intervention in, the marine environment.\" In this regard, according to WEGEMT, the technologies involved in marine technology are the following: naval architecture, marine engineering, ship design, ship building and ship operations; oil and gas exploration, exploitation, and production; hydrodynamics, navigation, sea surface and sub-surface support, underwater technology and engineering; marine resources (including both renewable and non-renewable marine resources); transport logistics and economics; inland, coastal, short sea and deep sea shipping; protection of the marine environment; leisure and safety.\n\nAccording to the Cape Fear Community College of Wilmington, North Carolina, the curriculum for a marine technology program provides practical skills and academic background that are essential in succeeding in the area of marine scientific support. Through a marine technology program, students aspiring to become marine technologists will become proficient in the knowledge and skills required of scientific support technicians. \n\nThe educational preparation includes classroom instructions and practical training aboard ships, such as how to use and maintain electronic navigation devices, physical and chemical measuring instruments, sampling devices, and data acquisition and reduction systems aboard ocean-going and smaller vessels, among other advanced equipment.\n\nAs far as marine technician programs are concerned, students learn hands-on to trouble shoot, service and repair four- and two-stroke outboards, stern drive, rigging, fuel & lube systems, electrical including diesel engines.\n\nMarine technology is related to the marine science and technology industry, also known as maritime commerce. The Executive Office of Housing and Economic Development (EOHED) of the government of Massachusetts in the United States defined marine science and technology industry as any business that deals primarily with or relates to the sea. A marine science industry includes businesses and technologies, research facilities, and higher education learning institutions. Companies and businesses involved in marine science and industry produce products such as ropes used for commercial fishing, undersea robotics, and stabilized sensor systems. The marine science industry has five sub-sectors, namely marine instrumentation and equipment, marine services, marine research and education, marine materials and supply, and shipbuilding and design.\n\n"}
{"id": "4437283", "url": "https://en.wikipedia.org/wiki?curid=4437283", "title": "Mass transfer coefficient", "text": "Mass transfer coefficient\n\nIn engineering, the mass transfer coefficient is a diffusion rate constant that relates the mass transfer rate, mass transfer area, and concentration change as driving force:\n\nformula_1\n\nWhere:\n\nThis can be used to quantify the mass transfer between phases, immiscible and partially miscible fluid mixtures (or between a fluid and a porous solid). Quantifying mass transfer allows for design and manufacture of separation process equipment that can meet specified requirements, estimate what will happen in real life situations (chemical spill), etc.\n\nMass transfer coefficients can be estimated from many different theoretical equations, correlations, and analogies that are functions of material properties, intensive properties and flow regime (laminar or turbulent flow). Selection of the most applicable model is dependent on the materials and the system, or environment, being studied.\n\n\nNote, the units will vary based upon which units the driving force is expressed in. The driving force shown here as 'formula_6' is expressed in units of moles per unit of volume, but in some cases the driving force is represented by other measures of concentration with different units. For example, the driving force may be partial pressures when dealing with mass transfer in a gas phase and thus use units of pressure.\n\n"}
{"id": "18881256", "url": "https://en.wikipedia.org/wiki?curid=18881256", "title": "Metacommunity", "text": "Metacommunity\n\nAn ecological metacommunity is a set of interacting communities which are linked by the dispersal of multiple, potentially interacting species. The term is derived from the field of community ecology, which is primarily concerned with patterns of species distribution, abundance and interactions. Metacommunity ecology combines the importance of local factors (environmental conditions, competition, predation) and regional factors (dispersal of individuals, immigration, emigration) to explain patterns of species distributions that happen in different spatial scales. \n\nThere are four theoretical frameworks, or unifying themes, that each detail specific mechanistic processes useful for predicting empirical community patterns. These are the patch dynamics, species sorting, source–sink dynamics (or mass effect) and neutral model frameworks. Patch dynamics models describe species composition among multiple, identical patches, such as islands. In this framework, species are able to persist on patches through tradeoffs in colonization ability and competitive ability, where less competitive species can disperse to unoccupied patches faster then they go extinct in others. Species sorting models describe variation in abundance and composition within the metacommunity due to individual species responses to environmental heterogeneity, such that certain local conditions may favor certain species and not others. Under this perspective, species are able to persist in patches with suitable environmental conditions resulting in a strong correlation between local species composition and the environment. This model represents the classical theories of the niche-centric era of G. Evelyn Hutchinson and Robert MacArthur. Source-sink models describe a framework in which dispersal and environmental heterogeneity interact to determine local and regional abundance and composition. This framework is derived from the metapopulation ecology term describing source–sink dynamics at the population level. High levels of dispersal among habitat patches allows populations to be maintained in environments that are normally outside the species environmental range. Finally, the neutral perspective describes a framework where species are essentially equivalent in their competitive and dispersal abilities, and local and regional composition and abundance is determined primarily by stochastic demographic processes and dispersal limitation. The neutral perspective was recently popularized by Stephen Hubbell following his groundbreaking work on the unified neutral theory of biodiversity.\n"}
{"id": "61768", "url": "https://en.wikipedia.org/wiki?curid=61768", "title": "Mount Sinai", "text": "Mount Sinai\n\nMount Sinai ( or ; or ; , \"Har Sinai\"; ; ), also known as Mount Horeb or Gabal Musa, is a mountain in the Sinai Peninsula of Egypt that is a possible location of the biblical Mount Sinai, which is considered a holy site by the Abrahamic religions. Mount Sinai is mentioned many times in the Book of Exodus and other books of the Bible, and the Quran. According to Jewish, Christian, and Islamic tradition, the biblical Mount Sinai was the place where Moses received the Ten Commandments.\n\nMount Sinai is a moderately high mountain near the city of Saint Catherine in the Sinai region. It is next to Mount Catherine (at , the highest peak in Egypt). It is surrounded on all sides by higher peaks of the mountain range.\n\nMount Sinai's rocks were formed in the late stage of the Arabian-Nubian Shield's (ANS) evolution. Mount Sinai displays a ring complex that consists of alkaline granites intruded into diverse rock types, including volcanics. The granites range in composition from syenogranite to alkali feldspar granite. The volcanic rocks are alkaline to peralkaline and they are represented by subaerial flows and eruptions and subvolcanic porphyry. Generally, the nature of the exposed rocks in Mount Sinai indicates that they originated from differing depths.\n\nThe biblical Mount Sinai is one of the most important sacred places in the Jewish, Christian and Islamic religions.\n\nAccording to the Hebrew Bible, it was the mountain where God gave laws to the Israelites. However, the earliest Christian traditions place this event at the nearby Mount Serbal, at the foot of which a monastery was founded in the 4th century; it was only in the 6th century that the monastery moved to the foot of Mount Catherine, following the guidance of Josephus' earlier claim that Sinai was the highest mountain in the area.\n\nThe earliest references to Jebel Musa as Mount Sinai or Mount Sinai being located in the present-day Sinai peninsula are inconclusive. There is evidence that prior to 100 CE, well before the Christian monastic period, Jewish sages equated Jebel Musa with Mount Sinai. Graham Davies of Cambridge University argues that early Jewish pilgrimages identified Jebel Musa as Mount Sinai and this identification was later adopted by the Christian pilgrims. R. K. Harrison states that \"Jebel Musa . . . seems to have enjoyed special sanctity long before Christian times, culminating in its identification with Mt. Sinai.\" \n\nChristians settled upon this mountain in the third century AD. Georgians from the Caucasus moved to the Sinai Peninsula in the fifth century, and a Georgian colony was formed there in the ninth century. Georgians erected their own churches in the area of the modern Mount Sinai. The construction of one such church was connected with the name of David The Builder, who contributed to the erection of churches in Georgia and abroad as well. There were political, cultural, and religious motives for locating the church on Mount Sinai. Georgian monks living there were deeply connected with their motherland. The church had its own plots in Kartli. Some of the Georgian manuscripts of Sinai remain there, but others are kept in Tbilisi, St. Petersburg, Prague, New York City, Paris, or in private collections.\nSome modern biblical scholars now believe that the Israelites would have crossed the Sinai peninsula in a direct route, rather than detouring to the southern tip (assuming that they did not cross the eastern branch of the Red Sea/Reed Sea), and therefore look for the biblical Mount Sinai elsewhere.\n\nAccording to some scholars, the Song of Deborah suggests that God dwelt at Mount Seir, so many scholars favour a location in Nabatea (modern Arabia). Alternatively, the biblical descriptions of Sinai can be interpreted as describing a volcano, and so a small number of scholars have considered equating Sinai with locations in northwestern Saudi Arabia, such as Jabal al-Lawz, as there are no volcanoes on the Sinai peninsula.\n\nSaint Catherine's Monastery (Greek: ) lies on the Sinai Peninsula, at the mouth of an inaccessible gorge at the foot of modern Mount Sinai in Saint Catherine at an elevation of 1550 meters. The monastery is Greek Orthodox and is a UNESCO World Heritage Site. According to the UNESCO report (60100 ha / Ref: 954) and website hereunder, this monastery has been called the oldest working Christian monastery in the world – although the Monastery of Saint Anthony, situated across the Red Sea in the desert south of Cairo, also lays claim to that title.\n\nThere are two principal routes to the summit. The longer and shallower route, \"Siket El Bashait\", takes about 2.5 hours on foot, though camels can be used. The steeper, more direct route (\"Siket Sayidna Musa\") is up the 3,750 \"steps of penitence\" in the ravine behind the monastery.\n\nThe summit of the mountain has a mosque that is still used by Muslims. It also has a Greek Orthodox chapel, constructed in 1934 on the ruins of a 16th-century church, that is not open to the public. The chapel encloses the rock which is considered to be the source for the biblical Tablets of Stone. At the summit also is \"Moses' cave\", where Moses was said to have waited to receive the Ten Commandments.\n\n\n"}
{"id": "22621", "url": "https://en.wikipedia.org/wiki?curid=22621", "title": "Oceania", "text": "Oceania\n\nOceania (, , ) is a geographic region comprising Australasia, Melanesia, Micronesia and Polynesia. Spanning the eastern and western hemispheres, Oceania covers an area of and has a population of /1e6 round 0 million. Situated in the southeast of the Asia-Pacific region, Oceania is the smallest continental grouping in land area and the second smallest in population after Antarctica.\n\nThe islands at the geographic extremes of Oceania are Bonin Islands, a politically integral part of Japan; Hawaii, a state of the United States; Clipperton Island, a possession of France; the Juan Fernández Islands, belonging to Chile; and the Campbell Islands, belonging to New Zealand. Oceania has a diverse mix of economies from the highly developed and globally competitive financial markets of Australia and New Zealand, which rank high in quality of life and human development index, to the much less developed economies that belong to countries such as of Kiribati and Tuvalu, while also including medium-sized economies of Pacific islands such as Palau, Fiji and Tonga. The largest and most populous country in Oceania is Australia, with Sydney being the largest city of both Oceania and Australia.\n\nThe first settlers of Australia, New Guinea, and the large islands just to the east arrived between 50,000 and 30,000 years ago. Oceania was first explored by Europeans from the 16th century onward. Portuguese navigators, between 1512 and 1526, reached the Tanimbar Islands, some of the Caroline Islands and west Papua New Guinea. On his first voyage in the 18th century, James Cook, who later arrived at the highly developed Hawaiian Islands, went to Tahiti and followed the east coast of Australia for the first time. The Pacific front saw major action during the Second World War, mainly between Allied powers the United States and Australia, and Axis power Japan.\n\nThe arrival of European settlers in subsequent centuries resulted in a significant alteration in the social and political landscape of Oceania. In more contemporary times there has been increasing discussion on national flags and a desire by some Oceanians to display their distinguishable and\nindividualistic identity. The rock art of Australian Aborigines is the longest continuously practiced artistic tradition in the world. Puncak Jaya in Papua is often considered the highest peak in Oceania. Most Oceanian countries have a parliamentary representative democratic multi-party system, with tourism being a large source of income for the Pacific Islands nations.\n\nThe geographer Conrad Malte-Brun coined the French term \"Océanie\" 1812. \"Océanie\" derives from the Latin word , and this from the Greek word (\"ōkeanós\"), \"ocean\". Natives and inhabitants of this region are called Oceanians or Oceanicans. The term \"Oceania\" is used because, unlike the other continental groupings, it is the ocean that links the parts of the region together.\n\nIndigenous Australians are the original inhabitants of the Australian continent and nearby islands who migrated from Africa to Asia around 70,000 years ago and arrived in Australia around 50,000 years ago. They are believed to be among the earliest human migrations out of Africa. Although they likely migrated to Australia through Southeast Asia they are not demonstrably related to any known Asian or Polynesian population. There is evidence of genetic and linguistic interchange between Australians in the far north and the Austronesian peoples of modern-day New Guinea and the islands, but this may be the result of recent trade and intermarriage.\n\nThey reached Tasmania approximately 40,000 years ago by migrating across a land bridge from the mainland that existed during the last ice age. It is believed that the first early human migration to Australia was achieved when this landmass formed part of the Sahul continent, connected to the island of New Guinea via a land bridge. The Torres Strait Islanders are indigenous to the Torres Strait Islands, which are at the northernmost tip of Queensland near Papua New Guinea. The earliest definite human remains found in Australia are that of Mungo Man, which have been dated at about 40,000 years old.\n\nThe original inhabitants of the group of islands now named Melanesia were likely the ancestors of the present-day Papuan-speaking people. Migrating from South-East Asia, they appear to have occupied these islands as far east as the main islands in the Solomon Islands archipelago, including Makira and possibly the smaller islands farther to the east.\n\nParticularly along the north coast of New Guinea and in the islands north and east of New Guinea, the Austronesian people, who had migrated into the area somewhat more than 3,000 years ago, came into contact with these pre-existing populations of Papuan-speaking peoples. In the late 20th century, some scholars theorized a long period of interaction, which resulted in many complex changes in genetics, languages, and culture among the peoples.\n\nThe Polynesian people are considered to be by linguistic, archaeological and human genetic ancestry a subset of the sea-migrating Austronesian people and tracing Polynesian languages places their prehistoric origins in the Malay Archipelago, and ultimately, in Taiwan. Between about 3000 and 1000 BC speakers of Austronesian languages began spreading from Taiwan into Island South-East Asia, as tribes whose natives were thought to have arrived through South China about 8,000 years ago to the edges of western Micronesia and on into Melanesia.\n\nIn the archaeological record there are well-defined traces of this expansion which allow the path it took to be followed and dated with some certainty. It is thought that by roughly 1400 BC, \"Lapita Peoples\", so-named after their pottery tradition, appeared in the Bismarck Archipelago of north-west Melanesia.\n\nEaster Islanders claimed that a chief Hotu Matu'a arrived on the island in one or two large canoes with his wife and extended family. They are believed to have been Polynesian. Published literature suggests the island was settled around AD 300–400, or at about the time of the arrival of the earliest settlers in Hawaii. Around 1200, Tahitian explorers found and began settling the area. This date range is based on glottochronological calculations and on three radiocarbon dates from charcoal that appears to have been produced during forest clearance activities. Moreover, a recent study which included radiocarbon dates from what is thought to be very early material suggests that the island was settled as recently as 1200.\n\nMicronesia began to be settled several millennia ago, although there are competing theories about the origin and arrival of the first settlers. There are numerous difficulties with conducting archaeological excavations in the islands, due to their size, settlement patterns and storm damage. As a result, much evidence is based on linguistic analysis.\n\nThe earliest archaeological traces of civilization have been found on the island of Saipan, dated to 1500 BC or slightly before. The ancestors of the Micronesians settled there over 4,000 years ago. A decentralized chieftain-based system eventually evolved into a more centralized economic and religious culture centered on Yap and Pohnpei. The prehistory of many Micronesian islands such as Yap are not known very well.\n\nThe first people of the Northern Mariana Islands navigated to the islands at some period between 4000 BC to 2000 BC from South-East Asia. They became known as the Chamorros, and spoke an Austronesian language called Chamorro. The ancient Chamorro left a number of megalithic ruins, including Latte stone. The Refaluwasch, or Carolinian, people came to the Marianas in the 1800s from the Caroline Islands. Micronesian colonists gradually settled the Marshall Islands during the 2nd millennium BC, with inter-island navigation made possible using traditional stick charts.\n\nFrom 1527 to 1595 a number of other large Spanish expeditions crossed the Pacific Ocean, leading to the discovery of the Marshall Islands and Palau in the North Pacific, as well as Tuvalu, the Marquesas, the Solomon Islands archipelago, the Cook Islands and the Admiralty Islands in the South Pacific.\n\nIn the quest for Terra Australis, Spanish explorations in the 17th century, such as the expedition led by the Portuguese navigator Pedro Fernandes de Queirós, discovered the Pitcairn and Vanuatu archipelagos, and sailed the Torres Strait between Australia and New Guinea, named after navigator Luís Vaz de Torres. Willem Janszoon, made the first completely documented European landing in Australia (1606), in Cape York Peninsula. Abel Janszoon Tasman circumnavigated and landed on parts of the Australian continental coast and discovered Van Diemen's Land (now Tasmania), New Zealand in 1642, and Fiji islands. He was the first known European explorer to reach these islands.\n\nOn 23 April 1770 British explorer James Cook made his first recorded direct observation of indigenous Australians at Brush Island near Bawley Point. On 29 April, Cook and crew made their first landfall on the mainland of the continent at a place now known as the Kurnell Peninsula. It is here that James Cook made first contact with an aboriginal tribe known as the Gweagal. His expedition became the first recorded Europeans to have encountered its eastern coastline of Australia.\n\nIn 1789 the Mutiny on the Bounty against William Bligh led to several of the mutineers escaping the Royal Navy and settling on Pitcairn Islands, which later became a British colony. Britain also established colonies in Australia in 1788, New Zealand in 1840 and Fiji in 1872, with much of Oceania becoming part of the British Empire. The Gilbert Islands (now known as Kiribati) and the Ellice Islands (now known as Tuvalu) came under Britain's sphere of influence in the late 19th century.\n\nFrench Catholic missionaries arrived on Tahiti in 1834; their expulsion in 1836 caused France to send a gunboat in 1838. In 1842, Tahiti and Tahuata were declared a French protectorate, to allow Catholic missionaries to work undisturbed. The capital of Papeetē was founded in 1843. On 24 September 1853, under orders from Napoleon III, Admiral Febvrier Despointes took formal possession of New Caledonia and Port-de-France (Nouméa) was founded 25 June 1854.\n\nThe Spanish explorer Alonso de Salazar landed in the Marshall Islands in 1529. They were named by Krusenstern, after English explorer John Marshall, who visited them together with Thomas Gilbert in 1788, en route from Botany Bay to Canton (two ships of the First Fleet). In 1905 the British government transferred some administrative responsibility over south-east New Guinea to Australia (which renamed the area \"Territory of Papua\"); and in 1906, transferred all remaining responsibility to Australia. The Marshall Islands were claimed by Spain in 1874. Germany established colonies in New Guinea in 1884, and Samoa in 1900. The United States also expanded into the Pacific, beginning with Baker Island and Howland Island in 1857, and with Hawaii becoming a U.S. territory in 1898. Disagreements between the US, Germany and UK over Samoa led to the Tripartite Convention of 1899.\n\nOne of the first land offensives in Oceania was the Occupation of German Samoa in August 1914 by New Zealand forces. The campaign to take Samoa ended without bloodshed after over 1,000 New Zealanders landed on the German colony. Australian forces attacked German New Guinea in September 1914. A company of Australians and a British warship besieged the Germans and their colonial subjects, ending with a German surrender.\n\nThe attack on Pearl Harbor by the Japanese Imperial General Headquarters, was a surprise military strike conducted by the Imperial Japanese Navy against the United States naval base at Pearl Harbor, Hawaii, on the morning of 7 December 1941. The attack led to the United States' entry into World War II. The Japanese subsequently invaded New Guinea, Solomon Islands and other Pacific islands. The Japanese were turned back at the Battle of the Coral Sea and the Kokoda Track campaign before they were finally defeated in 1945. Some of the most prominent Oceanic battlegrounds were the Battle of Bita Paka, the Solomon Islands campaign, the Air raids on Darwin, the Kokada Track, and the Borneo campaign. The United States fought the Battle of Guam from July 21 to August 10, 1944, to recapture the island from Japanese military occupation.\n\nAustralia and New Zealand became dominions in the 20th century, adopting the Statute of Westminster Act in 1942 and 1947 respectively. In 1946, Polynesians were granted French citizenship and the islands' status was changed to an overseas territory; the islands' name was changed in 1957 to \"Polynésie Française\" (French Polynesia). Hawaii became a U.S. state in 1959. Fiji and Tonga became independent in 1970. On 1 May 1979, in recognition of the evolving political status of the Marshall Islands, the United States recognized the constitution of the Marshall Islands and the establishment of the Government of the Republic of the Marshall Islands. The South Pacific Forum was founded in 1971, which became the Pacific Islands Forum in 2000.\n\nOceania was originally conceived as the lands of the Pacific Ocean, stretching from the Strait of Malacca to the coast of the Americas. It comprised four regions: \"Polynesia\", \"Micronesia\", \"Malaysia\" (now called the Malay Archipelago), and \"Melanesia\". Today, parts of three geological continents are included in the term \"Oceania\": Eurasia, Australia, and Zealandia, as well the non-continental volcanic islands of the Philippines, Wallacea, and the open Pacific.\n\nOceania extends to New Guinea in the west, the Bonin Islands in the northwest, the Hawaiian Islands in the northeast, Rapa Nui and Sala y Gómez Island in the east, and Macquarie Island in the south. Not included are the Pacific islands of Taiwan, the Ryukyu Islands, the Japanese archipelago, and the Maluku Islands, all on the margins of Asia, and the Aleutian Islands of North America. In its periphery, Oceania sprawls 28 degrees north to the Bonin Islands in the northern hemisphere, and 55 degrees south to Macquarie Island in the southern hemisphere.\n\nOceanian islands are of four basic types: continental islands, high islands, coral reefs and uplifted coral platforms. High islands are of volcanic origin, and many contain active volcanoes. Among these are Bougainville, Hawaii, and Solomon Islands.\n\nOceania is one of eight terrestrial ecozones, which constitute the major ecological regions of the planet. Related to these concepts are Near Oceania, that part of western Island Melanesia which has been inhabited for tens of millennia, and Remote Oceania which is more recently settled. Although the majority of the Oceanian islands lie in the South Pacific, a few of them are not restricted to the Pacific Ocean – Kangaroo Island and Ashmore and Cartier Islands, for instance, are situated in the Southern Ocean and Indian Ocean, respectively, and Tasmania's west coast faces the Southern Ocean.\n\nThe coral reefs of the South Pacific are low-lying structures that have built up on basaltic lava flows under the ocean's surface. One of the most dramatic is the Great Barrier Reef off northeastern Australia with chains of reef patches. A second island type formed of coral is the uplifted coral platform, which is usually slightly larger than the low coral islands. Examples include Banaba (formerly Ocean Island) and Makatea in the Tuamotu group of French Polynesia.\n\nMicronesia, which lies north of the equator and west of the International Date Line, includes the Mariana Islands in the northwest, the Caroline Islands in the center, the Marshall Islands to the west and the islands of Kiribati in the southeast.\n\nMelanesia, to the southwest, includes New Guinea, the world's second largest island after Greenland and by far the largest of the Pacific islands. The other main Melanesian groups from north to south are the Bismarck Archipelago, the Solomon Islands archipelago, Santa Cruz, Vanuatu, Fiji and New Caledonia.\n\nPolynesia, stretching from Hawaii in the north to New Zealand in the south, also encompasses Tuvalu, Tokelau, Samoa, Tonga and the Kermadec Islands to the west, the Cook Islands, Society Islands and Austral Islands in the center, and the Marquesas Islands, Tuamotu, Mangareva Islands, and Easter Island to the east.\n\nAustralasia comprises Australia, New Zealand, the island of New Guinea, and neighbouring islands in the Pacific Ocean. Most of Australasia lies on the southern portion of the Indo-Australian Plate, flanked by the Indian Ocean to the west and the Southern Ocean to the south. The bulk of Australasia sits on the Indo-Australian Plate, together with India.\n\nThe Pacific Plate, which makes up most of Oceania, is an oceanic tectonic plate that lies beneath the Pacific Ocean. At , it is the largest tectonic plate. The plate contains an interior hot spot forming the Hawaiian Islands. It is almost entirely oceanic crust. The oldest member disappearing by way of the plate tectonics cycle is early-Cretaceous (145 to 137 million years ago).\n\nAustralia, being part of the Indo-Australian plate, is the lowest, flattest, and oldest landmass on Earth and it has had a relatively stable geological history. Geological forces such as tectonic uplift of mountain ranges or clashes between tectonic plates occurred mainly in Australia's early history, when it was still a part of Gondwana. Australia is situated in the middle of the tectonic plate, and therefore currently has no active volcanism.\n\nThe geology of New Zealand is noted for its volcanic activity, earthquakes and geothermal areas because of its position on the boundary of the Australian Plate and Pacific Plates. Much of the basement rock of New Zealand was once part of the super-continent of Gondwana, along with South America, Africa, Madagascar, India, Antarctica and Australia. The rocks that now form the continent of Zealandia were nestled between Eastern Australia and Western Antarctica.\n\nThe Australia-New Zealand continental fragment of Gondwana split from the rest of Gondwana in the late Cretaceous time (95–90 Ma). By 75 Ma, Zealandia was essentially separate from Australia and Antarctica, although only shallow seas might have separated Zealandia and Australia in the north. The Tasman Sea, and part of Zealandia then locked together with Australia to form the Australian Plate (40 Ma), and a new plate boundary was created between the Australian Plate and Pacific Plate.\n\nMost islands in the Pacific are high islands (volcanic islands), such as, Easter Island, American Samoa and Fiji, among others, having peaks up to 1300 m rising abruptly from the shore. The Northwestern Hawaiian Islands were formed approximately 7 to 30 million years ago, as shield volcanoes over the same volcanic hotspot that formed the Emperor Seamounts to the north and the Main Hawaiian Islands to the south. Hawaii's tallest mountain Mauna Kea is above mean sea level.\n\nThe most diverse country of Oceania when it comes to the environment is Australia, with tropical rainforests in the north-east, mountain ranges in the south-east, south-west and east, and dry desert in the centre. Desert or semi-arid land commonly known as the outback makes up by far the largest portion of land. The coastal uplands and a belt of Brigalow grasslands lie between the coast and the mountains, while inland of the dividing range are large areas of grassland. The northernmost point of the east coast is the tropical-rainforested Cape York Peninsula.\n\nProminent features of the Australian flora are adaptations to aridity and fire which include scleromorphy and serotiny. These adaptations are common in species from the large and well-known families Proteaceae (\"Banksia\"), Myrtaceae (\"Eucalyptus\" – gum trees), and Fabaceae (\"Acacia\" – wattle). The flora of Fiji, Solomon Islands, Vanuatu and New Caledonia is tropical dry forest, with tropical vegetation that includes palm trees, premna protrusa, psydrax odorata, gyrocarpus americanus and derris trifoliata.\n\nNew Zealand's landscape ranges from the fjord-like sounds of the southwest to the tropical beaches of the far north. South Island is dominated by the Southern Alps. There are 18 peaks of more than 3000 metres (9800 ft) in the South Island. All summits over 2,900 m are within the Southern Alps, a chain that forms the backbone of the South Island; the highest peak of which is Aoraki/Mount Cook, at . Earthquakes are common, though usually not severe, averaging 3,000 per year. There is a wide variety of native trees, adapted to all the various micro-climates in New Zealand.\n\nIn Hawaii, one endemic plant, \"Brighamia\", now requires hand-pollination because its natural pollinator is presumed to be extinct. The two species of \"Brighamia\" – \"B. rockii\" and \"B. insignis\" – are represented in the wild by around 120 individual plants. To ensure these plants set seed, biologists rappel down cliffs to brush pollen onto their stigmas.\n\nThe aptly-named Pacific kingfisher is found in the Pacific Islands, as is the Red-vented bulbul, Polynesian starling, Brown goshawk, Pacific Swallow and the Cardinal myzomela, among others. Birds breeding on Pitcairn include the fairy tern, common noddy and red-tailed tropicbird. The Pitcairn reed warbler, endemic to Pitcairn Island, was added to the endangered species list in 2008.\n\nNative to Hawaii is the Hawaiian crow, which has been extinct in the wild since 2002. The brown tree snake is native to northern and eastern coasts of Australia, Papua New Guinea, Guam and Solomon Islands. Native to Australia, New Guinea and proximate islands are birds of paradise, honeyeaters, Australasian treecreeper, Australasian robin, kingfishers, butcherbirds and bowerbirds.\n\nA unique feature of Australia's fauna is the relative scarcity of native placental mammals, and dominance of the marsupials – a group of mammals that raise their young in a pouch, including the macropods, possums and dasyuromorphs. The passerines of Australia, also known as songbirds or perching birds, include wrens, the magpie group, thornbills, corvids, pardalotes, lyrebirds. Predominant bird species in the country include the Australian magpie, Australian raven, the pied currawong, crested pigeons and the laughing kookaburra. The koala, emu, platypus and kangaroo are national animals of Australia, and the Tasmanian devil is also one of the well-known animals in the country. The goanna is a predatory lizard native to the Australian mainland.\n\nThe birds of New Zealand evolved into an avifauna that included a large number of endemic species. As an island archipelago New Zealand accumulated bird diversity and when Captain James Cook arrived in the 1770s he noted that the bird song was deafening. The mix includes species with unusual biology such as the kakapo which is the world's only flightless, nocturnal, lek breeding parrot, but also many species that are similar to neighboring land areas. Some of the more well known and distinctive bird species in New Zealand are the kiwi, kea, takahe, kakapo, mohua, tui and the bellbird. The tuatara is a notable reptile endemic to New Zealand.\n\nThe Pacific Islands are ruled by a tropical rainforest and tropical savanna climate. In the tropical and subtropical Pacific, the El Niño Southern Oscillation (ENSO) affects weather conditions. In the tropical western Pacific, the monsoon and the related wet season during the summer months contrast with dry winds in the winter which blow over the ocean from the Asian landmass. November is the only month in which all the tropical cyclone basins are active.\n\nTo the southwest of the region, in the Australian landmass, the climate is mostly desert or semi-arid, with the southern coastal corners having a temperate climate, such as oceanic and humid subtropical climate in the east coast and Mediterranean climate in the west. The northern parts of the country have a tropical climate. Snow falls frequently on the highlands near the east coast, in the states of Victoria, New South Wales, Tasmania and in the Australian Capital Territory.\n\nMost regions of New Zealand belong to the temperate zone with a maritime climate (Köppen climate classification: Cfb) characterised by four distinct seasons. Conditions vary from extremely wet on the West Coast of the South Island to almost semi-arid in Central Otago and subtropical in Northland. Snow falls in New Zealand's South Island and at higher altitudes in the North Island. It is extremely rare at sea level in the North Island.\n\nHawaii, although being in the tropics, experiences many different climates, depending on latitude and its geography. The island of Hawaii for example hosts 4 (out of 5 in total) climate groups on a surface as small as according to the Köppen climate types: tropical, arid, temperate and polar. The Hawaiian Islands receive most of their precipitation during the winter months (October to April). A few islands in the northwest, such as Guam, are susceptible to typhoons in the wet season.\n\nThe highest recorded temperature in Oceania occurred in Oodnadatta, South Australia (2 January 1960), where the temperature reached . The lowest temperature ever recorded in Oceania was , at Ranfurly in Otago in 1903, with a more recent temperature of recorded in 1995 in nearby Ophir. Pohnpei of the Senyavin Islands in Micronesia is the wettest settlement in Oceania, and one of the wettest places on earth, with annual recorded rainfall exceeding each year in certain mountainous locations. The Big Bog on the island of Maui is the wettest place, receiving an average each year.\n\nThe linked map below shows the exclusive economic zones (EEZs) of the islands of Oceania and neighbouring areas, as a guide to the following table (there are no political boundaries that can be drawn on a map of the Pacific at this scale).\n\nThe demographic table below shows the subregions and countries of geopolitical Oceania. The countries and territories in this table are categorised according to the scheme for geographic subregions used by the United Nations. The information shown follows sources in cross-referenced articles; where sources differ, provisos have been clearly indicated. These territories and regions are subject to various additional categorisations, depending on the source and purpose of each description.\n\nThe predominant religion in Oceania is Christianity (73%). A 2011 survey found that 92% in Melanesia, 93% in Micronesia and 96% in Polynesia described themselves as Christians. Traditional religions are often animist, and prevalent among traditional tribes is the belief in spirits (\"masalai\" in Tok Pisin) representing natural forces. In the 2013 census, 48% of New Zealanders affiliated themselves with Christianity and 42% declared no religion. In the 2016 Census, 52% of the Australian population declared some variety of Christianity and 30% stated \"no religion\".\n\nIn recent Australian and New Zealand censuses, large proportions of the population say they belong to \"no religion\" (which includes atheism, agnosticism, deism, secular humanism, and rationalism). In Tonga, everyday life is heavily influenced by Polynesian traditions and especially by the Christian faith. The Ahmadiyya mosque in Marshall Islands is the only mosque in Micronesia. Another one in Tuvalu belongs to the same sect. The Bahá'í House of Worship in Tiapapata, Samoa, is one of seven designations administered in the Bahá'í Faith.\n\nOther religions in the region include Islam, Buddhism and Hinduism, which are prominent minority religions in Australia and New Zealand. Judaism, Sikhism and Jainism are also present. Sir Isaac Isaacs was the first Australian born Governor General of Australia and was the first Jewish vice-regal representative in the British Empire. Prince Philip Movement is followed around Yaohnanen village on the southern island of Tanna in Vanuatu.\n\nNative languages of Oceania fall into three major geographic groups:\n\nColonial languages include English in Australia, New Zealand, Hawaii, and many other territories; French in New Caledonia and French Polynesia, Japanese in the Bonin Islands, Spanish on Easter Island. There are also Creoles formed from the interaction of Malay or the colonial languages with indigenous languages, such as Tok Pisin, Bislama, Chavacano, various Malay trade and creole languages, Hawaiian Pidgin, Norfuk, and Pitkern. Contact between Austronesian and Papuan resulted in several instances in mixed languages such as Maisin.\n\nImmigrants brought their own languages to the region, such as Mandarin, Italian, Arabic, Polish, Hindi, German, Spanish, Korean, Cantonese and Greek, among others, namely in Australia and New Zealand, or Fiji Hindi in Fiji.\n\nThe most multicultural areas in Oceania, which have a high degree of immigration, are Australia, New Zealand and Hawaii. Since 1945, more than 7 million people have settled in Australia. From the late 1970s, there was a significant increase in immigration from Asian and other non-European countries, making Australia a multicultural country.\n\nSydney is the most multicultural city in Oceania, having more than 250 different languages spoken with about 40 percent of residents speaking a language other than English at home. Furthermore, 36 percent of the population reported having been born overseas, with top countries being Italy, Lebanon, Vietnam and Iraq, among others. Melbourne is also fairly multicultural, having the largest Greek-speaking population outside of Europe, and the second largest Asian population in Australia after Sydney.\n\nEuropean migration to New Zealand provided a major influx following the signing of the Treaty of Waitangi in 1840. Subsequent immigration has been chiefly from the British Isles, but also from continental Europe, the Pacific, The Americas and Asia. Auckland is home to over half (51.6 percent) of New Zealand's overseas born population, including 72 percent of the country's Pacific Island-born population, 64 percent of its Asian-born population, and 56 percent of its Middle Eastern and African born population.\n\nHawaii is a majority-minority state. Chinese workers on Western trading ships settled in Hawaii starting in 1789. In 1820, the first American missionaries arrived to preach Christianity and teach the Hawaiians Western ways. , a large proportion of Hawaii's population have Asian ancestry – especially Filipino, Japanese, Korean and Chinese. Many are descendants of immigrants brought to work on the sugarcane plantations in the mid-to-late 19th century. Almost 13,000 Portuguese immigrants had arrived by 1899; they also worked on the sugarcane plantations. Puerto Rican immigration to Hawaii began in 1899 when Puerto Rico's sugar industry was devastated by two hurricanes, causing a worldwide shortage of sugar and a huge demand for sugar from Hawaii.\n\nBetween 2001 and 2007 Australia's Pacific Solution policy transferred asylum seekers to several Pacific nations, including the Nauru detention centre. Australia, New Zealand and other nations took part in the Regional Assistance Mission to Solomon Islands between 2003 and 2017 after a request for aid.\n\nArchaeology, linguistics, and existing genetic studies indicate that Oceania was settled by two major waves of migration. The first migration took place approximately 40 thousand years ago, and these migrants, Papuans, colonised much of Near Oceania. Approximately 3.5 thousand years ago, a second expansion of Austronesian speakers arrived in Near Oceania, and the descendants of these people spread to the far corners of the Pacific, colonising Remote Oceania.<ref name=\"doi10.1016/j.ajhg.2014.03.014\"></ref>\n\nMitochondrial DNA (mtDNA) studies quantify the magnitude of the Austronesian expansion and demonstrate the homogenising effect of this expansion. With regards to Papuan influence, autochthonous haplogroups support the hypothesis of a long history in Near Oceania, with some lineages suggesting a time depth of 60 thousand years. Santa Cruz, a population located in Remote Oceania, is an anomaly with extreme frequencies of autochthonous haplogroups of Near Oceanian origin.\n\nLarge areas of New Guinea are unexplored by scientists and anthropologists due to extensive forestation and mountainous terrain. Known indigenous tribes in Papua New Guinea have very little contact with local authorities aside from the authorities knowing who they are. Many remain preliterate and, at the national or international level, the names of tribes and information about them is extremely hard to obtain. The Indonesian provinces of Papua and West Papua on the island of New Guinea are home to an estimated 44 uncontacted tribal groups.\n\nNew Zealand and Australia are the only developed nations in the region, although the economy of Australia is by far the largest and most dominant economy in the region and one of the largest in the world. Australia's per-capita GDP is higher than that of the UK, Canada, Germany, and France in terms of purchasing power parity. New Zealand is also one of the most globalised economies and depends greatly on international trade.\n\nThe Australian Securities Exchange in Sydney is the largest stock exchange in Australia and in the South Pacific. New Zealand is the 53rd-largest national economy in the world measured by nominal gross domestic product (GDP) and 68th-largest in the world measured by purchasing power parity (PPP). In 2012, Australia was the 12th largest national economy by nominal GDP and the 19th-largest measured by PPP-adjusted GDP.\n\nMercer Quality of Living Survey ranks Sydney tenth in the world in terms of quality of living, making it one of the most livable cities. It is classified as an Alpha+ World City by GaWC. Melbourne also ranked highly in the world's most liveable city list, and is a leading financial centre in the Asia-Pacific region. Auckland and Wellington, in New Zealand, are frequently ranked among the world's most liveable cities.\n\nThe majority of people living in Australia and to a lesser extent, New Zealand work in mining, electrical and manufacturing sectors also. Australia boasts the largest amount of manufacturing in the region, producing cars, electrical equipment, machinery and clothes.\n\nThe overwhelming majority of people living in the Pacific islands work in the service industry which includes tourism, education and financial services. Oceania's largest export markets include Japan, China, the United States and South Korea. The smallest Pacific nations rely on trade with Australia, New Zealand and the United States for exporting goods and for accessing other products. Australia and New Zealand's trading arrangements are known as Closer Economic Relations. Australia and New Zealand, along with other countries, are members of Asia-Pacific Economic Cooperation (APEC) and the East Asia Summit (EAS), which may become trade blocs in the future particularly EAS.\n\nThe main produce from the Pacific is copra or coconut, but timber, beef, palm oil, cocoa, sugar and ginger are also commonly grown across the tropics of the Pacific. Fishing provides a major industry for many of the smaller nations in the Pacific, although many fishing areas are exploited by other larger countries, namely Japan. Natural Resources, such as lead, zinc, nickel and gold, are mined in Australia and Solomon Islands. Oceania's largest export markets include Japan, China, the United States, India, South Korea and the European Union.\n\nEndowed with forest, mineral, and fish resources, Fiji is one of the most developed of the Pacific island economies, though it remains a developing country with a large subsistence agriculture sector. Agriculture accounts for 18% of gross domestic product, although it employed some 70% of the workforce as of 2001. Sugar exports and the growing tourist industry are the major sources of foreign exchange. Sugar cane processing makes up one-third of industrial activity. Coconuts, ginger, and copra are also significant.\n\nThe history of Hawaii's economy can be traced through a succession of dominant industries; sandalwood, whaling, sugarcane, pineapple, the military, tourism and education. Hawaiian exports include food and clothing. These industries play a small role in the Hawaiian economy, due to the shipping distance to viable markets, such as the West Coast of the contiguous U.S. The state's food exports include coffee, macadamia nuts, pineapple, livestock, sugarcane and honey. , Honolulu was ranked high on world livability rankings, and was also ranked as the 2nd safest city in the U.S.\n\nTourists mostly come from Japan, the United Kingdom and the United States. Fiji currently draws almost half a million tourists each year; more than a quarter from Australia. This contributes $1 billion or more since 1995 to Fiji's economy but the Government of Fiji islands underestimate these figures due to invisible economy inside tourism industry.\n\nVanuatu is widely recognised as one of the premier vacation destinations for scuba divers wishing to explore coral reefs of the South Pacific region. Tourism has been promoted, in part, by Vanuatu being the site of several reality-TV shows. The ninth season of the reality TV series \"Survivor\" was filmed on Vanuatu, entitled \" – Islands of Fire\". Two years later, Australia's \"Celebrity Survivor\" was filmed at the same location used by the US version.\nTourism in Australia is an important component of the Australian economy. In the financial year 2014/15, tourism represented 3% of Australia's GDP contributing A$47.5 billion to the national economy. In 2015, there were 7.4 million visitor arrivals. Popular Australian destinations include the Sydney Harbour (Sydney Opera House, Sydney Harbour Bridge, Royal Botanic Garden, etc.), Gold Coast (theme parks such as Warner Bros. Movie World, Dreamworld and Sea World), Walls of Jerusalem National Park and Mount Field National Park in Tasmania, Royal Exhibition Building in Melbourne, the Great Barrier Reef in Queensland, The Twelve Apostles in Victoria, Uluru (Ayers Rock) and the Australian outback.\n\nTourism in New Zealand contributes NZ$7.3 billion (or 4%) of the country's GDP in 2013, as well as directly supporting 110,800 full-time equivalent jobs (nearly 6% of New Zealand's workforce). International tourist spending accounted for 16% of New Zealand's export earnings (nearly NZ$10 billion). International and domestic tourism contributes, in total, NZ$24 billion to New Zealand's economy every year. Tourism New Zealand, the country's official tourism agency, is actively promoting the country as a destination worldwide. Milford Sound in South Island is acclaimed as New Zealand's most famous tourist destination.\n\nIn 2003 alone, according to state government data, there were over 6.4 million visitors to the Hawaiian Islands with expenditures of over $10.6 billion. Due to the mild year-round weather, tourist travel is popular throughout the year. In 2011, Hawaii saw increasing arrivals and share of foreign tourists from Canada, Australia and China increasing 13%, 24% and 21% respectively from 2010.\n\nAustralia is a federal parliamentary constitutional monarchy with Elizabeth II at its apex as the Queen of Australia, a role that is distinct from her position as monarch of the other Commonwealth realms. The Queen is represented in Australia by the Governor-General at the federal level and by the Governors at the state level, who by convention act on the advice of her ministers. There are two major political groups that usually form government, federally and in the states: the Australian Labor Party and the Coalition which is a formal grouping of the Liberal Party and its minor partner, the National Party. Within Australian political culture, the Coalition is considered centre-right and the Labor Party is considered centre-left. The Australian Defence Force is by far the largest military force in Oceania.\n\nNew Zealand is a constitutional monarchy with a parliamentary democracy, although its constitution is not codified. Elizabeth II is the Queen of New Zealand and the head of state. The Queen is represented by the Governor-General, whom she appoints on the advice of the Prime Minister. The New Zealand Parliament holds legislative power and consists of the Queen and the House of Representatives. A parliamentary general election must be called no later than three years after the previous election. New Zealand is identified as one of the world's most stable and well-governed states, with high government transparency and among the lowest perceived levels of corruption.\n\nIn Samoan politics, the Prime Minister of Samoa is the head of government. The 1960 constitution, which formally came into force with independence from New Zealand in 1962, builds on the British pattern of parliamentary democracy, modified to take account of Samoan customs. The national government (\"malo\") generally controls the legislative assembly. Politics of Tonga takes place in a framework of a constitutional monarchy, whereby the King is the Head of State.\n\nFiji has a multiparty system with the Prime Minister of Fiji as head of government. The executive power is exercised by the government. Legislative power is vested in both the government and the Parliament of Fiji. Fiji's Head of State is the President. He is elected by Parliament of Fiji after nomination by the Prime Minister or the Leader of the Opposition, for a three-year term.\n\nIn the politics of Papua New Guinea the Prime Minister is the head of government. In Kiribati, the President of Kiribati is the head of government, and of a multi-party system.\n\nNew Caledonia remains an integral part of the French Republic. Inhabitants of New Caledonia are French citizens and carry French passports. They take part in the legislative and presidential French elections. New Caledonia sends two representatives to the French National Assembly and two senators to the French Senate.\n\nHawaii is dominated by the Democratic Party. As codified in the Constitution of Hawaii, there are three branches of government: executive, legislative and judicial. The governor is the only state public official elected statewide; all others are appointed by the governor. The lieutenant governor acts as the Secretary of State. The governor and lieutenant governor oversee twenty agencies and departments from offices in the State Capitol.\n\nSince 1788, the primary influence behind Australian culture has been Anglo-Celtic Western culture, with some Indigenous influences. The divergence and evolution that has occurred in the ensuing centuries has resulted in a distinctive Australian culture. Since the mid-20th century, American popular culture has strongly influenced Australia, particularly through television and cinema. Other cultural influences come from neighbouring Asian countries, and through large-scale immigration from non-English-speaking nations. \"The Story of the Kelly Gang\" (1906), the world's first feature length film, spurred a boom in Australian cinema during the silent film era. The Australian Museum in Sydney and the National Gallery of Victoria in Melbourne are the oldest and largest museums in Oceania. The city's New Year's Eve celebrations are the largest in Oceania.\n\nAustralia is also known for its cafe and coffee culture in urban centres. Australia and New Zealand were responsible for the flat white coffee. Most Indigenous Australian tribal groups subsisted on a simple hunter-gatherer diet of native fauna and flora, otherwise called bush tucker. The first settlers introduced British food to the continent, much of which is now considered typical Australian food, such as the Sunday roast. Multicultural immigration transformed Australian cuisine; post-World War II European migrants, particularly from the Mediterranean, helped to build a thriving Australian coffee culture, and the influence of Asian cultures has led to Australian variants of their staple foods, such as the Chinese-inspired dim sim and Chiko Roll.\n\nThe music of Hawaii includes traditional and popular styles, ranging from native Hawaiian folk music to modern rock and hip hop. Hawaii's musical contributions to the music of the United States are out of proportion to the state's small size. Styles such as slack-key guitar are well known worldwide, while Hawaiian-tinged music is a frequent part of Hollywood soundtracks. Hawaii also made a major contribution to country music with the introduction of the steel guitar. The Hawaiian religion is polytheistic and animistic, with a belief in many deities and spirits, including the belief that spirits are found in non-human beings and objects such as animals, the waves, and the sky.\n\nThe cuisine of Hawaii is a fusion of many foods brought by immigrants to the Hawaiian Islands, including the earliest Polynesians and Native Hawaiian cuisine, and American, Chinese, Filipino, Japanese, Korean, Polynesian and Portuguese origins. Native Hawaiian musician and Hawaiian sovereignty activist Israel Kamakawiwoʻole, famous for his medley of \"Somewhere Over the Rainbow/What a Wonderful World\", was named \"The Voice of Hawaii\" by NPR in 2010 in its 50 great voices series.\n\nNew Zealand as a culture is a Western culture, which is influenced by the cultural input of the indigenous Māori and the various waves of multi-ethnic migration which followed the British colonisation of New Zealand. Māori people constitute one of the major cultures of Polynesia. The country has been broadened by globalisation and immigration from the Pacific Islands, East Asia and South Asia. New Zealand marks two national days of remembrance, Waitangi Day and ANZAC Day, and also celebrates holidays during or close to the anniversaries of the founding dates of each province.\n\nThe New Zealand recording industry began to develop from 1940 onwards and many New Zealand musicians have obtained success in Britain and the United States. Some artists release Māori language songs and the Māori tradition-based art of \"kapa haka\" (song and dance) has made a resurgence. The country's diverse scenery and compact size, plus government incentives, have encouraged some producers to film big budget movies in New Zealand, including \"Avatar\", \"The Lord of the Rings\", \"The Hobbit\", \"The Chronicles of Narnia\", \"King Kong\" and \"The Last Samurai\".\n\nThe national cuisine has been described as Pacific Rim, incorporating the native Māori cuisine and diverse culinary traditions introduced by settlers and immigrants from Europe, Polynesia and Asia. New Zealand yields produce from land and sea – most crops and livestock, such as maize, potatoes and pigs, were gradually introduced by the early European settlers. Distinctive ingredients or dishes include lamb, salmon, koura (crayfish), dredge oysters, whitebait, paua (abalone), mussels, scallops, pipi and tuatua (both are types of New Zealand shellfish), kumara (sweet potato), kiwifruit, tamarillo and pavlova (considered a national dish).\n\nThe fa'a Samoa, or traditional Samoan way, remains a strong force in Samoan life and politics. Despite centuries of European influence, Samoa maintains its historical customs, social and political systems, and language. Cultural customs such as the Samoa 'ava ceremony are significant and solemn rituals at important occasions including the bestowal of \"matai\" chiefly titles. Items of great cultural value include the finely woven \"'ie toga\".\n\nThe Samoan word for dance is \"siva\" with unique gentle movements of the body in time to music and which tell a story, although the Samoan male dances can be more snappy. The \"sasa\" is also a traditional dance where rows of dancers perform rapid synchronised movements in time to the rhythm of wooden drums \"(pate)\" or rolled mats. Another dance performed by males is called the \"fa'ataupati\" or the slap dance, creating rhythmic sounds by slapping different parts of the body. As with other Polynesian cultures (Hawaiian, Tahitian and Māori) with significant and unique tattoos, Samoans have two gender specific and culturally significant tattoos.\n\nThe artistic creations of native Oceanians varies greatly throughout the cultures and regions. The subject matter typically carries themes of fertility or the supernatural.\nPetroglyphs, Tattooing, painting, wood carving, stone carving and textile work are other common art forms. Art of Oceania properly encompasses the artistic traditions of the people indigenous to Australia and the Pacific Islands. These early peoples lacked a writing system, and made works on perishable materials, so few records of them exist from this time.\n\nIndigenous Australian rock art is the oldest and richest unbroken tradition of art in the world, dating as far back as 60,000 years and spread across hundreds of thousands of sites. These rock paintings served several functions. Some were used in magic, others to increase animal populations for hunting, while some were simply for amusement. Sculpture in Oceania first appears on New Guinea as a series of stone figures found throughout the island, but mostly in mountainous highlands. Establishing a chronological timeframe for these pieces in most cases is difficult, but one has been dated to 1500 BC.\n\nBy 1500 BC the Lapita culture, descendants of the second wave, would begin to expand and spread into the more remote islands. At around the same time, art began to appear in New Guinea, including the earliest examples of sculpture in Oceania. Starting around 1100 AD, the people of Easter Island would begin construction of nearly 900 moai (large stone statues). At about 1200 AD, the people of Pohnpei, a Micronesian island, would embark on another megalithic construction, building Nan Madol, a city of artificial islands and a system of canals. Hawaiian art includes wood carvings, feather work, petroglyphs, bark cloth (called kapa in Hawaiian and tapa elsewhere in the Pacific) and tattoos. Native Hawaiians had neither metal nor woven cloth.\n\nRugby union is one of the region's most prominent sports, and is the national sport of New Zealand, Samoa, Fiji and Tonga. The most popular sport in Australia is cricket, the most popular sport among Australian women is netball, while Australian rules football is the most popular sport in terms of spectatorship and television ratings. Rugby is the most popular sport among New Zealanders. In Papua New Guinea, the most popular sport is Rugby league.\n\nAustralian rules football is the national sport in Nauru and is the most popular football code in Australia in terms of attendance. It has a large following in Papua New Guinea, where it is the second most popular sport after Rugby League. It attracts significant attention across New Zealand and the Pacific Islands. Fiji's sevens team is one of the most successful in the world, as is New Zealand's.\n\nCurrently Vanuatu is the only country in Oceania to call association football its national sport. However, it is also the most popular sport in Kiribati, Solomon Islands and Tuvalu, and has a significant (and growing) popularity in Australia. In 2006 Australia joined the Asian Football Confederation and qualified for the 2010 and 2014 World Cups as an Asian entrant.\n\nAustralia has hosted two Summer Olympics: Melbourne 1956 and Sydney 2000. Also, Australia has hosted five editions of the Commonwealth Games (Sydney 1938, Perth 1962, Brisbane 1982, Melbourne 2006, Gold Coast 2018). Meanwhile, New Zealand has hosted the Commonwealth Games three times: Auckland 1950, Christchurch 1974 and Auckland 1990. The Pacific Games (formerly known as the South Pacific Games) is a multi-sport event, much like the Olympics on a much smaller scale, with participation exclusively from countries around the Pacific. It is held every four years and began in 1963.\nAustralia and New Zealand competed in the games for the first time in 2015.\n\n\n\n"}
{"id": "456204", "url": "https://en.wikipedia.org/wiki?curid=456204", "title": "Organic geochemistry", "text": "Organic geochemistry\n\nOrganic geochemistry is the study of the impacts and processes that organisms have had on the Earth. The study of organic geochemistry is usually traced to the work of Alfred E. Treibs, \"the father of organic geochemistry.\" Treibs first isolated metalloporphyrins from petroleum. This discovery established the biological origin of petroleum, which was previously poorly understood. Metalloporphyrins in general are highly stable organic compounds, and the detailed structures of the extracted derivatives made clear that they originated from chlorophyll.\n\nThe relationship between the occurrence of organic compounds in sedimentary deposits and petroleum deposits has long been of interest. Studies of ancient sediments and rock provide insights into the origins and sources of oil petroleum geochemistry and the biochemical antecedents of life. \n\nModern organic geochemistry includes studies of recent sediments to understand the carbon cycle, climate change, and ocean processes. \n\n"}
{"id": "20277638", "url": "https://en.wikipedia.org/wiki?curid=20277638", "title": "Pellet Fuels Institute", "text": "Pellet Fuels Institute\n\nPellet Fuels Institute (PFI) is a North American trade organization that represents manufacturers, retailers and distributors of wood pellet fuel supplies and appliances. The PFI was formed in 1985 as the Fiber Fuels Institute. \n\nHeadquartered in Arlington, Virginia, PFI maintains National Residential Pellet Fuel Standards, compiles and publishes data on sales and manufacturing output relating to the U.S. pellet fuel industry, hosts industry conferences, and provides outreach to consumers on the use of pellet fuel as an alternative energy thermal source.\n\n"}
{"id": "2345019", "url": "https://en.wikipedia.org/wiki?curid=2345019", "title": "Quartz latite", "text": "Quartz latite\n\nA quartz latite is a volcanic rock or fine grained intrusive rock equivalent to a latite with a phenocryst modal composition containing 5-20% quartz. Above 20% quartz, the rock would be classified as a rhyolite. It is the fine grained equivalent of a quartz monzonite containing approximately equal amounts of plagioclase and alkali feldspar.\n"}
{"id": "10121426", "url": "https://en.wikipedia.org/wiki?curid=10121426", "title": "Radius of maximum wind", "text": "Radius of maximum wind\n\nThe radius of maximum wind (RMW) is the distance between the center of a cyclone and its band of strongest winds. It is a parameter in atmospheric dynamics and tropical cyclone forecasting. The highest rainfall rates occur near the RMW of tropical cyclones. The extent of a cyclone's storm surge and its maximum potential intensity can be determined using the RMW. As maximum sustained winds increase, the RMW decreases. Recently, RMW has been used in descriptions of tornadoes. When designing buildings to prevent against failure from atmospheric pressure change, RMW can be used in the calculations.\n\nThe RMW is traditionally measured by reconnaissance aircraft in the Atlantic basin. It can also be determined on weather maps as the distance between the cyclone center and the system's greatest pressure gradient. Using weather satellite data, the distance between the coldest cloud top temperature and the warmest temperatature within the eye, in infrared satellite imagery, is one method of determining RMW. The reason why this method has merit is that the strongest winds within tropical cyclones tend to be located under the deepest convection, which is seen on satellite imagery as the coldest cloud tops. Use of velocity data from Doppler weather radar can also be used to determine this quantity, both for tornadoes and tropical cyclones near the coast.\n\nIn the case of tornadoes, knowledge of the RMW is important as atmospheric pressure change (APC) within sealed buildings can cause failure of the structure. Most buildings have openings totaling one square foot per volume to help equalize air pressure between the inside and outside of the structures. The APC is around one-half of its maximum value at the RMW, which normally ranges between and from the center (or eye) of the tornado. The widest tornado as measured by actual radar wind measurements was the Mulhall tornado in northern Oklahoma, part of the 1999 Oklahoma tornado outbreak, which had a radius of maximum wind of over .\n\nAn average value for the RMW of was calculated as the mean (or average) of all hurricanes with a lowest central atmospheric pressure between a pressure of and . As tropical cyclones intensify, maximum sustained winds increase as the RMW decreases. However, values for RMW produced based on central pressure or maximum wind speed could be substantial scattering around the regression\nlines. The heaviest rainfall within intense tropical cyclones has been observed in the vicinity of the RMW.\n\nThe radius of maximum wind helps determine the direct strikes of tropical cyclones. Tropical cyclones are considered to have made a direct strike to a landmass when a tropical cyclone passes close enough to a landmass that areas inside the radius of maximum wind are experienced on land. The radius of maximum wind is used within the maximum potential intensity equation. The Emanuel equation for Maximum Intensity Potential relies upon the winds near the RMW of a tropical cyclone to determine its ultimate potential.\n\nThe highest storm surge is normally coincident with the radius of maximum wind. Because the strongest winds within a tropical cyclone lie at the RMW, this is the region of a tropical cyclone which generates the dominant waves near the storm, and ultimately ocean swell away from the cyclone. Tropical cyclones mix the ocean water within a radius three times that of the RMW, which lowers sea surface temperatures due to upwelling.\n\nMuch is still unknown about the radius of maximum wind in tropical cyclones, including whether or not it can be predictable.\n\n"}
{"id": "47928489", "url": "https://en.wikipedia.org/wiki?curid=47928489", "title": "Renewable thermal energy", "text": "Renewable thermal energy\n\nRenewable thermal energy is the technology of gathering thermal energy from a renewable energy source for immediate use or for storage in a thermal battery for later use. The most popular form of renewable thermal energy is the sun and the solar energy is harvested by solar collectors to heat water, buildings, pools and various processes. Another example of Renewable Thermal is a Geothermal or ground source Heat Pump (GHP) system, where thermal stored in the ground from the summer is extracted from the ground to heat a building in another season. This example system is \"renewable\" because the source of excess heat energy is a reliably recurring process that occurs each summer season.\n\nSolar energy has been in use for centuries for heating dwellings and to produce hot water before low cost natural gas was discovered. It gained attention during and after the oil embargo of 1973 as engineers investigated ways to produce thermal energy from a renewable source instead of fossil fuels. \nThe history of utilizing the ground as a heat source is more recent and has gained prominence in recent years especially in rural areas where natural gas heating may not be available. The outer crust of the Earth is a Thermal Battery that maintains a median temperature which is the same as the average air temperature at that location. This \"average ground temperature\" is a combination in balance of solar gain from the sun, thermal gain from the core of the earth, and heat loss due to conduction, evaporation, and radiation. The graphic at the right shows a map of the \"average ground temperature\" at locations within the United States.\n\nSolar energy is considered to be the most popular form of renewable thermal energy in the world. Solar thermal energy is generally collected by either liquid or air solar collectors. Liquid solar collectors are used to heat water for domestic hot water, process applications and for swimming pools. Millions of solar water systems are being used world wide. \nAir solar collectors are normally used to heat buildings and for processes such as crop drying. Air collectors are typically building integrated on south facing walls to maximize the low winter sun angles when buildings in cold climates require heating.\n\nA ground heat exchanger (GHEX) is an area of the earth that is used as an annual cycle thermal battery. These thermal batteries are un-encapsulated areas of the earth into which pipes have been placed in order to transfer thermal energy. Energy is added to the GHEX by running a higher temperature fluid through the pipes and thus raising the temperature of the local earth. Energy can also be taken from the GHEX by running a lower temperature fluid through those same pipes.\n\nGHEX thermal batteries are implemented in two forms. The picture above depicts what is known as a \"horizontal\" GHEX where trenching is used to place an amount of pipe in a closed loop in the ground. GHEX's are also formed by drilling boreholes into the ground, either vertically or horizontally, and then the pipes are inserted in the form of a closed-loop with a \"u-bend\" fitting on the far end of the loop. These drilled GHEX thermal batteries are also sometimes called \"borehole thermal energy storage systems\".\n\nHeat energy can be added to or removed from a GHEX Thermal Battery at any point in time. However, they are most often used as an \"Annual-Cycle Thermal Battery\" where energy is extracted from a building during the summer season to cool a building and added to the GHEX, and then that same energy is later extracted from the GHEX in the winter season to heat the building. This annual cycle of energy addition and subtraction is highly predictable based on energy modeling of the building served. A Thermal Battery used in this mode is a Renewable Energy source as the energy extracted in the winter will be restored to the GHEX the next summer in a continually repeating cycle. This Annual-Cycle Thermal Battery is a solar powered thermal storage because it is the heat from the sun in the summer that is removed from a building and stored in the ground for use in the next winter season for heating.\n\nThe state of New York took a big step in September 2015 when it created a new office titled Director of Renewable Thermal. The NY Director of Renewable Thermal will oversee a team to help companies develop and implement renewable, low-carbon cooling and heating systems. NY State considers this initiative a critical component of NYSERDA’s strategy to enable net-zero energy buildings, which produce the same amount of energy as they consume. It also will further advance New York’s progress toward creating self-sustaining energy markets for clean, renewable technologies.\n\nRenewable Thermal has been a core resource in many states Renewable Portfolio Standards. The report says: \"State Renewable Portfolio Standard (RPS) programs have historically focused on electricity generation. However, some states have started incorporating renewable thermal power for heat generation into their RPS as a way to support the development and market growth of solar thermal, biomass thermal, geothermal, and other renewable thermal technologies.\" Further: \"Renewable thermal energy has many of the same benefits as other renewable technologies, including improved air quality, economic development and job creation, and the promotion of regional energy security.\"\n\nIn a recent article, Bill Nowak, the Executive Director of the NY-GEO industry trade group, stated: \"According to the recently adopted New York State energy plan, on-site combustion (largely for heating buildings) is responsible for 35 percent of fossil fuel greenhouse gas emissions in New York State. In-state electricity generation is responsible for only 18 percent. We strongly support cleaning up electricity generation in New York, but stress that renewable thermal is the next wave in resisting climate change.\"\n"}
{"id": "43516671", "url": "https://en.wikipedia.org/wiki?curid=43516671", "title": "Robbie Hood", "text": "Robbie Hood\n\nRobbie Hood is an atmospheric scientist who studies hurricanes. She was lead scientist for the Convection and Moisture Experiment at NASA.\n"}
{"id": "14983028", "url": "https://en.wikipedia.org/wiki?curid=14983028", "title": "Secondary flow", "text": "Secondary flow\n\nIn fluid dynamics, a secondary flow is a relatively minor flow superimposed on the primary flow, where the primary flow usually matches very closely the flow pattern predicted using simple analytical techniques that assume the fluid is inviscid. (An inviscid fluid is a theoretical fluid having zero viscosity.)\n\nThe primary flow of a fluid, particularly in the majority of the flow field remote from solid surfaces immersed in the fluid, is usually very similar to what would be predicted using the basic principles of physics, and assuming the fluid is inviscid. However, in real flow situations, there are regions in the flow field where the flow is significantly different in both speed and direction to what is predicted for an inviscid fluid using simple analytical techniques. The flow in these regions is the secondary flow. These regions are usually in the vicinity of the boundary of the fluid adjacent to solid surfaces where viscous forces are at work, such as in the boundary layer.\n\nThe basic principles of physics and the Coriolis effect satisfactorily explain that the direction of the wind in the atmosphere is parallel to the isobars. Measurements of wind speed and direction at heights well above ground level confirm that the speed of the wind matches that predicted by considerations of gradient flow, and the direction of the wind is indeed parallel to the isobars in the region. However, from ground level up to heights where the influence of the earth’s surface can be neglected, the wind speed is less than predicted by the barometric pressure gradient, and the wind direction is partly across the isobars rather than parallel to them. This flow of air across the isobars near ground level is a \"secondary flow\". It does not conform to the primary flow, which is parallel to the isobars.\n\nAt heights well above ground level there is a balance between the Coriolis effect, the local pressure gradient, and the velocity of the wind. This is balanced flow. Closer to the ground the air is not able to accelerate to the speed necessary for balanced flow. Interference by the surface of the ground or water, and by obstructions such as terrain, waves, trees and buildings, cause drag on the atmosphere and prevent the air from accelerating to the speed necessary to achieve balanced flow. As a result, the wind direction near ground level is partly parallel to the isobars in the region, and partly across the isobars in the direction from higher pressure to lower pressure.\n\nAs a result of the slower wind speed at the earth’s surface, in a region of low pressure the barometric pressure is usually significantly higher at the surface than would be expected, given the barometric pressure at mid altitudes, due to Bernoulli's principle. Hence, the secondary flow toward the center of a region of low pressure is also drawn upward by the significantly lower pressure at mid altitudes. This slow, widespread ascent of the air in a region of low pressure can cause widespread cloud and rain if the air is of sufficiently high relative humidity.\n\nIn a region of high pressure (an anticyclone) the secondary flow includes a slow, widespread descent of air from mid altitudes toward ground level, and then outward across the isobars. This descent causes a reduction in relative humidity and explains why regions of high pressure usually experience cloud-free skies for many days.\n\nThe primary flow around a tropical cyclone is parallel to the isobars – and hence circular. The closer to the center of the cyclone, the faster is the wind speed. In accordance with Bernoulli's principle where the wind speed is fastest the barometric pressure is lowest. Consequently, near the center of the cyclone the barometric pressure is very low. There is a strong pressure gradient across the isobars toward the center of the cyclone. This pressure gradient provides the centripetal force necessary for the circular motion of each parcel of air. This strong gradient, coupled with the slower speed of the air near the Earth’s surface, causes a secondary flow at surface level toward the center of the cyclone, rather than a wholly circular flow.\n\nEven though the wind speed near the center of a tropical cyclone is very fast, at any point on the Earth’s surface it is not as fast as it is above that point away from the retarding influence of the Earth's surface. The slower speed of the air at the earth’s surface prevents the barometric pressure from falling as low as would be expected from the barometric pressure at mid altitudes. This is compatible with Bernoulli's principle. The secondary flow at the Earth's surface is toward the center of the cyclone but is then drawn upward by the significantly lower pressure at mid and high altitudes. As the secondary flow is drawn upward the air cools and its pressure falls, causing extremely heavy rainfall over several days.\n\nTornadoes and dust devils display localised vortex flow. Their fluid motion is similar to tropical cyclones but on a much smaller scale so that the Coriolis effect is not significant. The primary flow is circular around the vertical axis of the tornado or dust devil. As with all vortex flow, the speed of the flow is fastest at the core of the vortex. In accordance with Bernoulli's principle where the wind speed is fastest the air pressure is lowest; and where the wind speed is slowest the air pressure is highest. Consequently, near the center of the tornado or dust devil the air pressure is low. There is a pressure gradient toward the center of the vortex. This gradient, coupled with the slower speed of the air near the earth’s surface, causes a \"secondary flow\" toward the center of the tornado or dust devil, rather than in a purely circular pattern.\n\nThe slower speed of the air at the surface prevents the air pressure from falling as low as would normally be expected from the air pressure at greater heights. This is compatible with Bernoulli's principle. The secondary flow is toward the center of the tornado or dust devil, and is then drawn upward by the significantly lower pressure several thousands of feet above the surface in the case of a tornado, or several hundred feet in the case of a dust devil. Tornadoes can be very destructive and the secondary flow can cause debris to be swept into a central location and carried to low altitudes.\n\nDust devils can be seen by the dust stirred up at ground level, swept up by the secondary flow and concentrated in a central location. The accumulation of dust then accompanies the secondary flow upward into the region of intense low pressure that exists outside the influence of the ground.\n\nWhen water in a circular bowl or cup is moving in circular motion the water displays vortex flow – the water at the center of the bowl or cup spins at relatively high speed, and the water at the perimeter spins more slowly. The water is a little deeper at the perimeter and a little more shallow at the center, and the surface of the water is not flat but displays the characteristic depression toward the axis of the spinning fluid. At any elevation within the water the pressure is a little greater near the perimeter of the bowl or cup where the water is a little deeper, than near the center. The water pressure is a little greater where the water speed is a little slower, and the pressure is a little less where the speed is faster, and this is consistent with Bernoulli's principle.\n\nThere is a pressure gradient from the perimeter of the bowl or cup toward the center. This pressure gradient provides the centripetal force necessary for the circular motion of each parcel of water. The pressure gradient also accounts for a \"secondary flow\" of the boundary layer in the water flowing across the floor of the bowl or cup. The slower speed of the water in the boundary layer is unable to balance the pressure gradient. The boundary layer spirals inward toward the axis of circulation of the water. On reaching the center the secondary flow is then upward toward the surface, progressively mixing with the primary flow. Near the surface there may also be a slow secondary flow outward toward the perimeter.\n\nThe secondary flow along the floor of the bowl or cup can be seen by sprinkling heavy particles such as sugar, sand, rice or tea leaves into the water and then setting the water in circular motion by stirring with a hand or spoon. The boundary layer spirals inward and sweeps the heavier solids into a neat pile in the center of the bowl or cup. With water circulating in a bowl or cup, the primary flow is purely circular and might be expected to fling heavy particles outward to the perimeter. Instead, heavy particles can be seen to congregate in the center as a result of the secondary flow along the floor.\n\nWater flowing through a bend in a river must follow curved streamlines to remain within the banks of the river. The water surface is slightly higher near the concave bank than near the convex bank. (The \"concave bank\" has the greater radius. The \"convex bank\" has the smaller radius.) As a result, at any elevation within the river, water pressure is slightly higher near the concave bank than near the convex bank. A pressure gradient results from the concave bank toward the other bank. Centripetal forces are necessary for the curved path of each parcel of water, which is provided by the pressure gradient.\n\nThe primary flow around the bend is vortex flow – fastest speed where the radius of curvature of the stream itself is smallest and slowest speed where the radius is largest. The higher pressure near the concave (outer) bank is accompanied by slower water speed, and the lower pressure near the convex bank is accompanied by faster water speed, and all this is consistent with Bernoulli's principle.\n\nA \"secondary flow\" results in the boundary layer along the floor of the river bed. The boundary layer is not moving fast enough to balance the pressure gradient and so its path is partly downstream and partly across the stream from the concave bank toward the convex bank, driven by the pressure gradient. The secondary flow is then upward toward the surface where it mixes with the primary flow or moves slowly across the surface, back toward the concave bank. This motion is called helicoidal flow.\n\nOn the floor of the river bed the secondary flow sweeps sand, silt and gravel across the river and deposits the solids near the convex bank, in similar fashion to sugar or tea leaves being swept toward the center of a bowl or cup as described above. This process can lead to accentuation or creation of D-shaped islands, meanders through creation of cut banks and opposing point bars which in turn may result in an oxbow lake. The convex (inner) bank of river bends tends to be shallow and made up of sand, silt and fine gravel; the concave (outer) bank tends to be steep and elevated due to heavy erosion.\n\nSecondary flows are important in understanding the performance of turbines and other turbomachinery.\n\nMany types of secondary flows occur in turbomachinery, including inlet prerotation (intakes vorticity), tip clearance flow (tip leakage), flows at off-design performance (e.g. flow separation), and secondary vorticity flows. \nAlthough secondary flows occur in all turbomachinery, it is particularly considered in axial flow compressors because of the thick boundary layers on the annulus walls.\n\nFor such axial-flow compressors, consider a set of guide vanes with an approach velocity c1. The velocity profile will be non-uniform due to friction between the annulus wall and the fluid. The vorticity of this boundary layer is normal to the approach velocity formula_1 and of magnitude\n\nformula_2\n\nWhere z is the distance to the wall. As the vorticity of each blade onto each other will be of opposite directions, a secondary vorticity will be generated. If the deflection angle, e, between the guide vanes is small, the magnitude of the secondary vorticity is represented as\n\nformula_3\n\nThis secondary flow will be the integrated effect of the distribution of secondary vorticity along the blade length.\n\n\n\n"}
{"id": "30960051", "url": "https://en.wikipedia.org/wiki?curid=30960051", "title": "Sengkang Riverside Park", "text": "Sengkang Riverside Park\n\nSengkang Riverside Park is a riverine park located at Anchorvale Street abutting Sungei Punggol, Singapore. The park consist of three open-space land parcels and is also home to a constructed wetland. The Sengkang Sports Complex is located just adjacent to the riverine park, connected by a floating wetland.\n\nThe 21-hectare park was opened to the public in November 2008. The park is situated alongside the Punggol Reservoir. The integration of urban planning and protection of our water resources has earned Sengkang Riverside Park an ABC Waters Certification.\n\nThe constructed wetland is an artificial marshes or swamps, created to process waste water, surface run-off or to treat natural sewage. It also acts as a wildlife habitat and the water collected in the pond can be used for plant watering. The wetland have two sedimentation basins to slow down the high water speeds in storms, so that minimal damage is done to wetland habitat.\n\nOn 7 November 2010, Prime Minister Lee Hsien Loong officially opened the floating wetland that is designed with a \"fruitful\" theme. The wetland is about half the size of a football field, and connects the Sengkang Riverside Park with the Sengkang Sports Complex. The floating wetland helps to collect and filter rainwater naturally through its aquatic plants. The wetland also acts as a habitat for fish, birds and other wildlife. Since the completion of the wetland, more birds and dragonflies were noticed to be attracted in the area.\n\nThe Visitor Centre is a covered meeting point, located at the central part of the park. The sheltered centre provides visitors to the park with a shelter from bad weather. Public toilets and vending machines are also available here.\n\nSengkang Riverside Park is also unique for its Fruit Tree Trail that consist of 16 different fruit trees, with some that cannot be found in the local supermarkets. The trail runs along the parameters of the constructed wetlands. The 16 different fruit trees that can be found in the trail are Mangosteen Tree, Ordeal Tree , Custard Apple, Pomelo, Lime, Weeping Tea Tree, Island Lychee, Mango, Pond Apple Tree, Asam Tree, Java Olive Tree, Elephant Apple, Fish Killer Tree, Starfruit, Pig's Mango and Wine Palm.\n\nVisitors to the park can use the available facilities to cycle and exercise at the park. There are various resting points throughout the park and along the tracks. The Civic Event Lawn at the park provides a venue for events to be hosted here.\n\nThe park can be reach via the Sengkang LRT Line at Farmway LRT Station & Kupang LRT Station. Visitors can also choose to reach there by car, where the park's car park is located in front of the Visitor Centre.\n\n\n"}
{"id": "1251925", "url": "https://en.wikipedia.org/wiki?curid=1251925", "title": "Soil fertility", "text": "Soil fertility\n\nSoil fertility refers to the ability of a soil to sustain agricultural plant growth, i.e. to provide plant habitat and result in sustained and consistent yields of high quality. A fertile soil has the following properties:\n\nThe following properties contribute to soil fertility in most situations:\n\nIn lands used for agriculture and other human activities, maintenance of soil fertility typically requires the use of soil conservation practices. This is because soil erosion and other forms of soil degradation generally result in a decline in quality with respect to one or more of the aspects indicated above.\n\nBioavailable phosphorus is the element in soil that is most often lacking. Nitrogen and potassium are also needed in substantial amounts. For this reason these three elements are always identified on a commercial fertilizer analysis. For example, a 10-10-15 fertilizer has 10 percent nitrogen, 10 percent (PO) available phosphorus and 15 percent (KO) water-soluble potassium. Sulfur is the fourth element that may be identified in a commercial analysis—e.g. 21-0-0-24 which would contain 21% nitrogen and 24% sulfate.\n\nInorganic fertilizers are generally less expensive and have higher concentrations of nutrients than organic fertilizers. Also, since nitrogen, phosphorus and potassium generally must be in the inorganic forms to be taken up by plants, inorganic fertilizers are generally immediately bioavailable to plants without modification. However, some have criticized the use of inorganic fertilizers, claiming that the water-soluble nitrogen doesn't provide for the long-term needs of the plant and creates water pollution. Slow-release fertilizers may reduce leaching loss of nutrients and may make the nutrients that they provide available over a longer period of time.\n\nSoil fertility is a complex process that involves the constant cycling of nutrients between organic and inorganic forms. As plant material and animal wastes are decomposed by micro-organisms, they release inorganic nutrients to the soil solution, a process referred to as mineralization. Those nutrients may then undergo further transformations which may be aided or enabled by soil micro-organisms. Like plants, many micro-organisms require or preferentially use inorganic forms of nitrogen, phosphorus or potassium and will compete with plants for these nutrients, tying up the nutrients in microbial biomass, a process often called immobilization. The balance between immobilization and mineralization processes depends on the balance and availability of major nutrients and organic carbon to soil microorganisms. Natural processes such as lightning strikes may fix atmospheric nitrogen by converting it to (NO). Denitrification may occur under anaerobic conditions (flooding) in the presence of denitrifying bacteria. Nutrient cations, including potassium and many micronutrients, are held in relatively strong bonds with the negatively charged portions of the soil in a process known as cation exchange.\n\nIn 2008 the cost of phosphorus as fertilizer more than doubled, while the price of rock phosphate as base commodity rose eight-fold. Recently the term peak phosphorus has been coined, due to the limited occurrence of rock phosphate in the world.\n\nPhotosynthesis is the process whereby plants use light energy to drive chemical reactions which convert CO into sugars. As such, all plants require access to both light and carbon dioxide to produce energy, grow and reproduce.\n\nWhile typically limited by nitrogen, phosphorus and potassium, low levels of carbon dioxide can also act as a limiting factor on plant growth. Peer-reviewed and published scientific studies have shown that increasing CO is highly effective at promoting plant growth up to levels over 300 ppm. Further increases in CO can, to a very small degree, continue to increase net photosynthetic output.\n\nSoil depletion occurs when the components which contribute to fertility are removed and not replaced, and the conditions which support soil's fertility are not maintained. This leads to poor crop yields. In agriculture, depletion can be due to excessively intense cultivation and inadequate soil management.\n\nSoil fertility can be severely challenged when land use changes rapidly. For example, in Colonial New England, colonists made a number of decisions that depleted the soils, including: allowing herd animals to wander freely, not replenishing soils with manure, and a sequence of events that led to erosion. William Cronon wrote that \"...the long-term effect was to put those soils in jeopardy. The removal of the forest, the increase in destructive floods, the soil compaction and close-cropping wrought by grazing animals, plowing--all served to increase erosion.\"\n\nKarl Marx wrote of the role of capitalism in soil depletion. In \"Capital, Volume I\", he wrote:\n\nOne of the most widespread occurrences of soil depletion is in tropical zones where nutrient content of soils is low. The combined effects of growing population densities, large-scale industrial logging, slash-and-burn agriculture and ranching, and other factors, have in some places depleted soils through rapid and almost total nutrient removal.\n\nThe depletion of soil has affected the state of plant life and crops in agriculture in many countries. In the middle east for example, many countries in that find it difficult to grow produce because of droughts, lack of soil, and lack of irrigation. \"The Middle East\" has three countries that indicate a decline in crop production. the highest rates of productivity decline are found in hilly and dryland areas. Many countries in Africa also undergo a depletion of fertile soil. In regions of dry climate like Sudan and the countries that make up the Sahara Desert, droughts and soil degradation is common. Cash crops such as teas, maize, and beans that require a variety of nutrients in order to grow healthy. Soil fertility has decline in the farming regions of Africa and the use of artificial and natural fertilizers has been used to regain the nutrients of ground soil. \n\nTopsoil depletion occurs when the nutrient-rich organic topsoil, which takes hundreds to thousands of years to build up under natural conditions, is eroded or depleted of its original organic material. Historically, many past civilizations' collapses can be attributed to the depletion of the topsoil. Since the beginning of agricultural production in the Great Plains of North America in the 1880s, about one-half of its topsoil has disappeared.\n\nDepletion may occur through a variety of other effects, including overtillage (which damages soil structure), underuse of nutrient inputs which leads to mining of the soil nutrient bank, and salinization of soil.\n\nThe quality of irrigation water is very important to maintain soil fertility and tilth, and for using more soil depth by the plants. When soil is irrigated with high alkaline water, unwanted sodium salts build up in the soil which would make soil draining capacity very poor. So plant roots can not penetrate deep into the soil for optimum growth in Alkali soils. When soil is irrigated with low pH / acidic water, the useful salts ( Ca, Mg, K, P, S, etc.) are removed by draining water from the acidic soil and in addition unwanted aluminium and manganese salts to the plants are dissolved from the soil impeding plant growth. When soil is irrigated with high salinity water or sufficient water is not draining out from the irrigated soil, the soil would convert into saline soil or lose its fertility. Saline water enhance the turgor pressure or osmotic pressure requirement which impedes the off take of water and nutrients by the plant roots.\n\nTop soil loss takes place in alkali soils due to erosion by rain water surface flows or drainage as they form colloids (fine mud) in contact with water. Plants absorb water-soluble inorganic salts only from the soil for their growth. Soil as such does not lose fertility just by growing crops but it lose its fertility due to accumulation of unwanted and depletion of wanted inorganic salts from the soil by improper irrigation and acid rain water (quantity and quality of water). The fertility of many soils which are not suitable for plant growth can be enhanced many times gradually by providing adequate irrigation water of suitable quality and good drainage from the soil.\n\n"}
{"id": "1522331", "url": "https://en.wikipedia.org/wiki?curid=1522331", "title": "Soil horizon", "text": "Soil horizon\n\nA soil horizon is a layer parallel to the soil surface, whose physical characteristics differ from the layers above and beneath. Each soil type usually has three or four horizons. Horizons are defined in most cases by obvious physical features, mainly colour and texture. These may be described both in absolute terms (particle size distribution for texture, for instance) and in terms relative to the surrounding material, i.e. ‘coarser’ or ‘sandier’ than the horizons above and below. Water dissolves and removes nutrients as it passes through the soil.\n\nIdentification and description of the horizons present at a given site is the first step in soil classification at higher levels, through the use of systems such as the USDA soil taxonomy or the Australian Soil Classification. The World Reference Base for Soil Resources lists 37 diagnostic horizons. \n\nMost soils, especially in temperate climates, conform to a similar general pattern of horizons, often represented as an ‘ideal’ soil in diagrams. Each main horizon is denoted by a capital letter, which may then be followed by several alphanumerical modifiers highlighting particular outstanding features of the horizon. While the general O-A-B-C-R sequence seems fairly universal, some variation exists between the classification systems in different parts of the world. In addition, the exact definition of each main horizon may differ slightly – for instance, the US system uses the thickness of a horizon as a distinguishing feature, while the Australian system does not. It should be emphasised that no one system is more correct – as artificial constructs, their utility lies in their ability to accurately describe local conditions in a consistent manner. Also, many subtropical and tropical areas have soils such as oxisols or aridisols that have very different horizons from an \"ideal\" soil, or no horizons at all.\n\nThe following horizons are listed by their position from top to bottom within the soil profile. Not all of these layers are present in every location – for instance, P horizons only form in areas which have been waterlogged for long periods of time. Soils with a history of human interference, for instance through major earthworks or regular deep ploughing, may lack distinct horizons almost completely. When examining soils in the field, attention must be paid to the local geomorphology and the historical uses to which the land has been put in order to ensure that the appropriate names are applied to the observed horizons. The horizon not listed is the O horizon which is grass and animal/plant life. Soil has three main horizons (A, B, and C), which will be explained below along with other layers.\n\nSoil generally consists of visually and texturally distinct layers, which can be summarised as follows from top to bottom:\n\nThe \"O\" stands for organic matter. It is a surface layer, dominated by the presence of large amounts of organic material in varying stages of decomposition. The O horizon should be considered distinct from the layer of leaf litter covering many heavily vegetated areas, which contains no weathered mineral particles and is not part of the soil itself. O horizons may be divided into O1 and O2 categories, whereby O1 horizons contain decomposed matter whose origin can be spotted on sight (for instance, fragments of rotting leaves), and O2 horizons containing only well-decomposed organic matter, the origin of which is not readily visible.\n\nThese horizons are also heavily organic, but are distinct from O horizons in that they form under waterlogged conditions. The “P” designation comes from their common name, peats. They may be divided into P1 and P2 in the same way as O Horizons. This layer accumulates iron, clay, aluminium and organic compounds, a process referred to as illuviation.\n\nThe A horizon is the top layer of the soil horizons, often referred to as 'topsoil'. This layer has a layer of dark decomposed organic materials, which is called \"humus\".The technical definition of an A horizon may vary, but it is most commonly described in terms relative to deeper layers. \"A\" Horizons may be darker in colour than deeper layers and contain more organic material, or they may be lighter but contain less clay or sesquioxides. The A is a surface horizon, and as such is also known as the zone in which most biological activity occurs. Soil organisms such as earthworms, potworms (enchytraeids), arthropods, nematodes, fungi, and many species of bacteria and archaea are concentrated here, often in close association with plant roots. Thus the A horizon may be referred to as the biomantle. However, since biological activity extends far deeper into the soil, it cannot be used as a chief distinguishing feature of an A horizon.\n\n“E”, being short for eluviated, is most commonly used to label a horizon that has been significantly leached of its mineral and/or organic content, leaving a pale layer largely composed of silicates. These are present only in older, well-developed soils, and generally occur between the A and B horizons. In regions where this designation is not employed, leached layers are classified firstly as an A or B according to other characteristics, and then appended with the designation “e” (see the section below on horizon suffixes). In soils that contain gravels, due to animal bioturbation, a stonelayer commonly forms near or at the base of the E horizon.\n\nThe above layers may be referred to collectively as the \"solum\". The layers below have no collective name but are distinct in that they are noticeably less affected by surface soil-forming processes.\n\nThe B horizon is commonly referred to as \"subsoil\", and consists of mineral layers which may contain concentrations of clay or minerals such as iron or aluminium oxides or organic material moved there by leaching. Accordingly, this layer is also known as the \"illuviated\" horizon or the \"zone of accumulation\". In addition, it is defined as having a distinctly different structure or consistency than the horizon(s) above and the horizon(s) below. They may also have stronger colours (is higher chroma) than the A horizon.\n\nAs with the A horizon, the B horizon may be divided into B1, B2, and B3 types under the Australian system. B1 is a transitional horizon of the opposite nature to an A3 – dominated by the properties of the B horizons below it, but containing some A-horizon characteristics. B2 horizons have a concentration of clay, minerals, or organics and feature the strongest pedological development within the profile. B3 horizons are transitional between the overlying B layers and the material beneath it, whether C or D horizon.\n\nThe A3, B1, and B3 horizons are not tightly defined, and their use is generally at the discretion of the individual worker.\nPlant roots penetrate through this layer, but it has very little humus. It is usually brownish or reddish due to clay and iron oxides that wash down from A horizon.\n\nThe C horizon (parent rock) is below the B Horizon. This layer is little affected by soil forming processes (weathering), and the lack of pedological development is one of the defining attributes. The C Horizon may contain lumps or more likely large shelves of unweathered rock, rather than being made up solely of small fragments as in the solum. \"Ghost\" rock structure may be present within these horizons. The C horizon also contains parent material. The A and B layers usually originate from the C horizon. The C horizon forms as bed rock weathers and rock breaks up into particles.\n\nD horizons are not universally distinguished, but in the Australian system refer to \"any soil material below the solum that is unlike the solum in general character, is not C horizon, and cannot be given reliable designation… [it] may be recognized by the contrast in pedologic organization between it and the overlying horizons\" Also is located at the bottom of the diagram (MacDonald et al., 1990, p. 106).\n\nR horizons denote the layer of partially weathered bedrock at the base of the soil profile. Unlike the above layers, R horizons largely comprise continuous masses (as opposed to boulders) of hard rock that cannot be excavated by hand. Soils formed \"in situ\" will exhibit strong similarities to this bedrock layer. \n\nL (Limnic) horizons or layers indicate mineral or organic material that has been deposited in water by precipitation or through the actions of aquatic organisms. Included are coprogenous earth (sedimentary peat), diatomaceous earth, and marl; and is usually found as a remnant of past bodies of standing water.\n\nIn addition to the main descriptors above, several modifiers exist to add necessary detail to each horizon. Firstly, each major horizon may be divided into sub-horizons by the addition of a numerical subscript, based on minor shifts in colour or texture with increasing depth (e.g., B21, B22, B23 etc.). While this can add necessary depth to a field description, workers should bear in mind that excessive division of a soil profile into narrow sub-horizons should be avoided. Walking as little as ten metres in any direction and digging another hole can often reveal a very different profile in regards to the depth and thickness of each horizon. Over-precise description can be a waste of time, and as a rule of thumb, layers thinner than 5 cm (2 inches) or so are best described as pans or segregations within a horizon rather than as a distinct layer. \n\nSuffixes describing particular physical features of a horizon may also be added. These vary considerably between countries, but a limited selection of common ones are listed here:\n\n\nThe US system employs largely similar suffixes, with a few important differences. For instance, 'e' under the US system denotes a horizon containing \"organic material of intermediate decomposition\" rather than a bleached horizon. A full list of suffixes is available online as part of the USDA Soil Survey Manual.\n\nWhile soil formation is generally described as occurring \"in situ\", as rock breaks down and is mixed with other materials, the process is often far more complicated. For instance, a fully formed profile may have developed in an area only to be buried by wind- or water-deposited sediments which later formed into another soil profile. This sort of occurrence is most common in coastal areas, and descriptions are modified by numerical prefixes. Thus, a profile containing a buried sequence could be structured O, A1, A2, B2, 2A2, 2B21, 2B22, 2C with the buried profile commencing at 2A2.\n\n\n"}
{"id": "14542277", "url": "https://en.wikipedia.org/wiki?curid=14542277", "title": "Spruce-fir forests", "text": "Spruce-fir forests\n\nFir and spruce forests are greatly affected by slight fluctuations in climate. Temperature is the primary determinate for spatial patterns of fir and spruce. The two dominant trees in this type of forest are \"Picea engelmannii\" (Engelmann spruce) and \"Abies lasiocarpa\" (subalpine fir). Although thick-barked trees, such as the \"Pinus resinosa\", frequently survive fire, the thin bark of spruce make them more vulnerable. Trees such as the Douglas fir withstand much of the fire due to the thicker bark they have. The scale of the burn mosaic during a fire, relative to species niche requirements and mobility, can have major impacts on flora and fauna dynamics.\n\nForests of the Southern Appalachian Mountains had covered approximately 140 km² on peaks and ranges. Today this forest type occupies less than 70 km². Much of this forest loss is due to logging, followed by slash fires. The Waterrock Knob fire was composed of different group of plant species than are normally found in burned spruce and fir forests. The tree layer of this stand consists of fewer than expected stems and has low basal area. The shrub layer is very dense, which may reduce successful tree reproduction. The herbaceous layer growth is not typical of young, disturbed spruce and fir stands. These vegetation characteristics suggest that possibly a hot fire in conjunction with a steep rocky slope and shallow soils have been some of the reasons for the reduced development of a typical spruce and fir stand. Burned soil has been an important factor in determining earlier and present vegetation patterns and species composition.\n\nPost fire effects of the western United States forests include an important study of the fire of Yellowstone National Park in 1988. Historically, controlled burns had been utilized to thin forests. By the 1970s, Yellowstone started a natural fire management plan to allow the process of lightning caused fires to continue influencing wild land succession. In 1988, 248 fires were started in Yellowstone National Park. As for the animals that were killed by the fires, the U.S National Park Service tallied 345 elk, 36 deer, 12 moose, 6 black bears, 9 bison and 1 grizzly bear. Fish were also killed due to heated water. Surveys indicated that less than 1% of the soils were heated enough to burn below ground plant seeds and roots. The U.S Congress launched a massive study of the long term ecological effects caused by the Yellowstone fires. The short term effects proved most wildlife populations showed no effects or rebounded quickly. In the years following the fire, precipitation combined with short term ash and nutrient influx led to a stunning display of wild flowers on the burned areas.\n\nBecause plants are immobile, they must develop resistances to disturbances through natural selection. Individual plant species vary their resistance to fire injury in predictable ways. These resistances make it possible for the biodiversity to greatly increase during recolonization after a fire. Studies show that in a spruce and fir forest, eleven years after a fire, there is a greater diversity of herbs than in a community where there was no fire. The shrubs took longer to regenerate but soon there was greater diversity in the shrub population as well. Spruce and fir forests have a greater biodiversity than most other forests because of their multiple layers of canopy and dense understories. This creates a heterogeneous, diverse stand structure which leads to an assortment of fire types which usually leave patches of unburned trees. Because the soil is oxidized by the fire, seed germination is encouraged. Also, many herb species were present in burned areas but not areas that were unaffected by fire. Fir and spruce trees are wind-dispersed, so the number of regenerated trees depends on the distance from the unburned stands to the sample location. \n\nThe time it takes for fir and spruce to regenerate varies greatly, but it takes a number of years if no roots or snags remain. Regeneration time depends on a number of environmental factors, such as species type, wind strength, and aspect. It may be much more difficult for fir and spruce to be established on a slope, not only because of the dispersion technique, but also because erosion is greatly increased by fire, making it harder for the seeds to take root. Also, fir and spruce differ in survival techniques. Fir uses rapid growth, short lifespan, and easy establishment to come back more quickly after a fire, while spruce relies on longer lifespan and larger basal area to survive. Spruce has a much lower establishment rate, but the larger basal area increases its chances of survival and allows it to regenerate by both wind dispersion and growth from the roots remaining after a fire. The result is that fir and spruce repopulate the site contemporaneously, although most of the initial biomass is dominated by fir and later on the dominance shifts to spruce. Recurrent high-intensity crown fire also helps spruce and fir forests by preventing fir from overtaking spruce through competitive exclusion. Because spruce is shade-intolerant, it requires an open canopy to be established. This means that without fire to wipe out fir trees, thus creating holes in the canopy, spruce would be outcompeted by fir.\n\nFire has an interesting relationship with micro fauna in forests. Damage done to trees by fire hurts the vascular cambium, thus leaving trees more susceptible to insectivorous and fungal attacks. Fungal infections are not as common as insect attacks, but can be just as deadly. The fungus \"Amylostereum areolatum\" weakens trees and allows insects such as the \"Sirex noctilio\" (European wood wasp) to take over massive numbers of forests. The most common problems in spruce and fir forests are bark beetles, budworms, and gall-forming insects, to which spruce is extremely susceptible. Several gall-forming insects are present in spruce forests, including the Eastern Spruce gall adelgid and the Cooley's Spruce gall adelgid, which normally would not harm forests, unless the trees are unusually vulnerable to them, as they are after a fire. Budworm larvae feed on the leaves of spruce and fir trees, and can become present in large amounts, which is when they become detrimental to a forest. Bark beetles are the most common insect killer of spruce and fir forests because they can spread quickly, breed rapidly, and can easily devour thousands of acres before actions can be taken against them. During the 1990s, the bark beetles affected almost three million acres (12,000 km²) of spruce forests. The attacks of these insects, in turn, raise the mortality rate of trees in the burn area, which provides even more fuel for the next fire.\n\nA number of larger animals are supported by fir and spruce forests, such as moose, deer, elk, birds, snowshoe hares, and other small mammals. Effects on bird populations after fire in fir and spruce forests varied. Of the 41 avian species observed in 3 or more studies comparing post fire and adjacent unburned forests, 22% are consistently more abundant in burned forests and 34% are more abundant in unburned forests. In general, woodpeckers and aerial foragers are more abundant in burned forests and foliage grazing species are more abundant in unburned forests. Within the spruce and fir community type, trees often lose their lower branches, becoming unavailable to hare and other small mammals for food or cover during the seven to nine months of winter. This makes larger mammals more densely populated in fir and spruce forests. Fire does not displace fauna that are dependent on fir and spruce forests. When fir and spruce begin sprouting, they are utilized for food and the patches of trees remaining provides shelter. Because of the heterogeneity in fir and spruce forests, patches of trees are always left in nature. Fire suppression, on the other hand, alters the natural patch dynamics, thus greatly reducing the number of mammals present. The trees all grow older, close the canopy, the understory is repressed, branches fall off during the winter, and for the majority of the year there is no available food. Also, if fire is suppressed for a number of years and then a crown fire breaks out in the area, it will quickly spread throughout the dense canopy. No patches will be left for shelter and the fir and spruce will take much longer to regenerate because of the distance from the remaining stands to the center of the burned site.\n\n"}
{"id": "621887", "url": "https://en.wikipedia.org/wiki?curid=621887", "title": "Tectonophysics", "text": "Tectonophysics\n\nTectonophysics, a branch of geophysics, is the study of the physical processes that underlie tectonic deformation. The field encompasses the spatial patterns of stress, strain, and differing rheologies in the lithosphere and asthenosphere of the Earth; and the relationships between these patterns and the observed patterns of deformation due to plate tectonics.\n\nTectonophysics is concerned with movements in the Earth's crust and deformations over scales from meters to thousands of kilometers. Examples of such processes include mountain building, the formation of sedimentary basins, postglacial rebound of regions such as Fennoscandia, plate tectonics, volcanoes and earthquakes. This involves the measurement of a hierarchy of strains in rocks and plates as well as deformation rates; the study of laboratory analogues of natural systems; and the construction of models for the history of deformation.\n\nTectonophysics was defined as a field in 1954 when Mikhail Vladimirovich Gzovskii published three papers in the journal \"Izvestiya Akad. Nauk SSSR, Sireya Geofizicheskaya\": \"On the tasks and content of tectonophysics\", \"Tectonic stress fields\", and \"Modeling of tectonic stress fields\". He defined the main goals of tectonophysical research to be study of the mechanisms of folding and faulting as well as large structural units of the Earth's crust. He later created the Laboratory of Tectonophysics at the Institute of Physics of the Earth, Academy of Sciences of the USSR, Moscow.\n\n\n"}
{"id": "216238", "url": "https://en.wikipedia.org/wiki?curid=216238", "title": "Value of life", "text": "Value of life\n\nThe value of life is an economic value used to quantify the benefit of avoiding a fatality. It is also referred to as the cost of life, value of preventing a fatality (VPF) and implied cost of averting a fatality (ICAF). In social and political sciences, it is the marginal cost of death prevention in a certain class of circumstances. In many studies the value also includes the quality of life, the expected life time remaining, as well as the earning potential of a given person especially for an after the fact payment in a wrongful death claim lawsuit.\n\nAs such, it is a statistical term, the cost of reducing the average number of deaths by one. It is an important issue in a wide range of disciplines including economics, health care, adoption, political economy, insurance, worker safety, environmental impact assessment, and globalization.\n\nIn industrial nations, the justice system considers a human life \"priceless\", thus illegalizing any form of slavery; i.e., humans cannot be bought at any price. However, with a limited supply of resources or infrastructural capital (e.g. ambulances), or skill at hand, it is impossible to save every life, so some trade-off must be made. Also, this argument neglects the statistical context of the term. It is not commonly attached to lives of individuals or used to compare the value of one person's life relative to another person's. It is mainly used in circumstances of saving lives as opposed to taking lives or \"producing\" lives.\n\nThere is no standard concept for the value of a specific human life in economics. However, when looking at risk/reward trade-offs that people make with regard to their health, economists often consider the value of a statistical life (VSL). The EPA does not place a dollar value on individual lives. Rather, when conducting a cost-benefit analysis of new environmental policies, the Agency uses estimates of how much people are willing to pay for small reductions in their risks of dying from adverse health conditions that may be caused by environmental pollution.These estimates of willingness to pay for small reductions in mortality risks are often referred to as the value of a statistical life (VSL). The VSL is the value that an individual places on a marginal change in their likelihood of death. Note that the VSL is very different from the value of an actual life. It is the value placed on changes in the likelihood of death, not the price someone would pay to avoid certain death. This is best explained by way of an example. From the EPA's website:Suppose each person in a sample of 100,000 people were asked how much he or she would be willing to pay for a reduction in their individual risk of dying of 1 in 100,000, or 0.001%, over the next year. Since this reduction in risk would mean that we would expect one fewer death among the sample of 100,000 people over the next year on average, this is sometimes described as \"one statistical life saved.” Now suppose that the average response to this hypothetical question was $100. Then the total dollar amount that the group would be willing to pay to save one statistical life in a year would be $100 per person × 100,000 people, or $10 million. This is what is meant by the \"value of a statistical life.” Economists often estimate the VSL by looking at the risks that people are voluntarily willing to take and how much they must be paid for taking them. These types of studies, which look at a person's actual choices, are known as revealed preference studies. A common source of such choices is the labor market, where jobs with greater risk of death are seen to correlate with higher wages.\nMuch of this research uses a wage hedonic approach, which looks at how wages change with changes in job characteristics. Such studies regress wages on job characteristics like risk of death, occupation, industry, risk of injury, location, etc. By controlling for as many job characteristics as possible, researchers hope to tease out the portion of the wage that is compensating for the risk of death on the job. A recent summary of this literature is the 2003 paper by Viscusi and Aldy.\n\nAnother method economists can use to estimate the VSL is by simply asking people (e.g. through questionnaires) how much they would be willing to pay for a reduction in the likelihood of dying, perhaps by purchasing safety improvements. These types of studies are referred to as stated preference studies. A well known problem with this method is the so-called \"hypothetical bias\", whereby people tend to overstate their valuation of goods and services.\n\nVSL estimates from wage hedonics are what the EPA uses when evaluating health benefits from programs. From the EPA's:\n\nData for these studies frequently include wage data from the Current Population Survey and workplace risk data from the Census of Fatal Occupational Injuries. Both sets of data are collected on behalf of the Bureau of Labor Statistics.\n\nAs an example, the EPA's retrospective study of the clean air act, which focused \"primarily on the criteria pollutants sulfur dioxide, nitrogen oxides, carbon monoxide, particulate matter, ozone, and lead\", found that the benefits of the program, mostly from improvements to health, outweighed the costs:\n\nThe value of improvements to health conditions were converted to dollars by estimating the resulting decrease in the incidence of fatalities and valuing this change using estimations of the VSL from the literature. A table of specific values used for the various improvements in health can be found in the report's executive summary.\n\nThe cigarette industry was particularly concerned with value of life calculations since it came under regular attack in the 1980s and 1990s for the \"social cost\" of smoking on the national economy. The economic argument for increasing excise taxes on cigarettes was that these taxes compensated the state for a whole range of externalities that smoking imposed, including the costs of hospital and medical care for smokers and non-smokers alike, disability pensions for smoking-related diseases, welfare payments made to surviving spouses, the cost of street, home and office cleaning, the burden of home and forest fires, etc. \n\nTo counter this argument, the tobacco industry was increasingly forced to fall back on calculations made by a network of employed academics, who were paid to write op-ed articles for their local newspapers expressing the opinion that smokers already 'paid their way'. They relied on an argument by Kip Viscusi which became known as the \"death benefits\"—the idea that, since smokers died earlier than non-smokers, the nation was being saved hospital, pension and nursing-home costs, and that these offset many of the external costs (depending on how these are calculated).\n\nSince resources are finite, trade-offs are inevitable, even regarding potential life-or-death decisions. The assignment of a value to individual life is one possible approach to attempting to make rational decisions about these trade-offs.\n\nWhen deciding on the appropriate level of healthcare spending, a typical method is to equate the marginal cost of the healthcare to the marginal benefits received. In order to obtain a marginal benefit amount, some estimation of the dollar value of life is required.\nHowever the level that can be done is limited by available resources. So even though it may be 'beneficial' to spend more it is not possible, or else requires deficit spending which causes other intended consequences that are hoped to be less.\n\nThe rational strategy is to determine how much can be spent on saving lives and then allocating it to the most beneficial ways until the money is fully allocated.\n\nIn risk management activities such as in the areas of workplace safety, and insurance, it is often useful to put a precise economic value on a given life. There can be no such thing as a perfectly safe or risk free system—one can always make a system safer by spending more money. However, there are diminishing returns involved.\n\nIn transportation modes it is very important to consider the external cost that is paid by the society but is not calculated, for making it more sustainable. The external cost, although consisting of impacts on climate, crops and public health among others, is largely determined by impacts on mortality rate.\n\nEquivalent parameters are used in many countries, with significant variation in the value assigned.\n\nIn Australia, the value of a statistical life has been set at:\n\nIn New Zealand, the value of a statistical life has been set at:\n\nAccording to different estimates life value in Russia varies from $40,000 up to $2 million. On the results of opinion poll life value (as the cost of financial compensation for the death) in the beginning of 2015 was about $71,500.\n\nThe following estimates have been applied to the value of life. The estimates are either for \"one year\" of additional life or for the statistical value of a \"single\" life.\n\nThe income elasticity of the value of statistical life has been estimated at 0.5 to 0.6. Developing markets have smaller statistical value of life. The statistical value of life also decreases with age.\n\nHistorically, children were not valued very much but modern cultural norms attach a much higher value.\n\n\n"}
{"id": "5630755", "url": "https://en.wikipedia.org/wiki?curid=5630755", "title": "Voltage compensation", "text": "Voltage compensation\n\nIn a power system, voltage at various buses tends to increase or decrease during its daily operation. To ensure constant voltage to consumers, various techniques are utilized.\n\nWhen the voltage is below the required level, reactive power produced by inductance needs to be offset by capacitance.\n\nWhen the voltage is above the required level, reactive power produced by capacitance needs to be offset by inductance.\n"}
{"id": "6840428", "url": "https://en.wikipedia.org/wiki?curid=6840428", "title": "Yedigöller National Park", "text": "Yedigöller National Park\n\nThe Yedigöller National Park (, \"seven lakes\") also known as Seven Lakes National Park is located in the northern part of Bolu Province in Turkey. The park is categorized under IUCN II and was established in 1965. The park is best known for the seven lakes formed by landslides and for its profusion of plant life. \n\nThe park is located at an elevation of in Mengen district in the north of Bolu Province and to the south of Zonguldak in the western Black Sea region. Established in 1965, it encompasses an area of . There are seven lakes in the park which were formed due to landslides. The seven lakes are Büyükgöl, Deringöl, Seringöl, Nazlıgöl, Sazlıgöl, Incegöl and Küçükgöl. Several streams flow through the park some of which have handcrafted bridges across them and a small water fall. \n\nThe park is located between Istanbul and Ankara. Access is from the Yenicaga road, from the Ankara – Istanbul highway. During the winter the alternate road is only from the Yenicaga – Mengen – Yazicik road. It is to the north of Bolu. Kapankaya mountain peak is within the park and provides a vantage location to view the scenic beauty of the park. It is accessible all through the year.\n\nThe park also has trekking paths and camps to stay, in the form of tents, caravans, rest houses and bungalows. There are also hot springs in the park, and the ski centers here are well known in Turkey.\n\nThe specific studies related to Saprobic values of decayed organic matter of the park indicate a range between oligosaprobic and beta-mesosaprobic status. The water quality of the streams in the park is classified as Class I and II.\n\nThe vegetation consists of beech trees, oaks, hornbeams, firs, elms, hazel nuts, spruces, alders, lime trees, black pines and Scotch pines.\n\nWildlife within the park includes, but is not limited to, red deer, roe deer, wild boar, brown bears, wolves, red fox, lynx, jungle cats, otters and squirrels. As result of better protection conditions in the park the population of animals has increased. The park has an exclusive protected area for deer. A trout farm has also been established in the park. New insect species \"Ephemeroptera\" identified in the park are \"Ecdyonurus starmachi,\" \"Paraleptophlebia cincta,\" \"Caenis martae,\" and \"Baetis lapponicus.\"\n"}
