{"id": "18160509", "url": "https://en.wikipedia.org/wiki?curid=18160509", "title": "100% renewable energy", "text": "100% renewable energy\n\nThe endeavor to use 100% renewable energy for electricity, heating/cooling and transport is motivated by global warming, pollution and other environmental issues, as well as economic and energy security concerns. Shifting the total global primary energy supply to renewable sources requires a transition of the energy system. In 2013 the Intergovernmental Panel on Climate Change said that there are few fundamental technological limits to integrating a portfolio of renewable energy technologies to meet most of total global energy demand. Renewable energy use has grown much faster than even advocates anticipated.\n\nIn 2014, renewable sources such as wind, geothermal, solar, biomass, and burnt waste provided 19% of the total energy consumed worldwide, with roughly half of that coming from traditional use of biomass. The most important sector is electricity with a renewable share of 22.8%, most of it coming from hydropower with a share of 16.6%, followed by wind with 3.1%. According to the REN21 2017 global status report, these figures had increased to 19.3% for energy in 2015 and 24.5% for electricity in 2016. There are many places around the world with grids that are run almost exclusively on renewable energy. At the national level, at least 30 nations already have renewable energy contributing more than 20% of the energy supply.\n\nProfessors S. Pacala and Robert H. Socolow of Princeton University have developed a series of “climate stabilization wedges” that can allow us to maintain our quality of life while avoiding catastrophic climate change, and \"renewable energy sources,\" in aggregate, constitute the largest number of their \"wedges.\"\n\nMark Z. Jacobson, professor of civil and environmental engineering at Stanford University and director of its Atmosphere and Energy program, says that producing all new energy with wind power, solar power, and hydropower by 2030 is feasible, and that existing energy supply arrangements could be replaced by 2050. Barriers to implementing the renewable energy plan are seen to be \"primarily social and political, not technological or economic\". Jacobson says that energy costs today with a wind, solar, and water system should be similar to today's energy costs from other optimally cost-effective strategies. The main obstacle against this scenario is the lack of political will. Jacobson's conclusions have been disputed by other researchers.\n\nSimilarly, in the United States, the independent National Research Council has noted that “sufficient domestic renewable resources exist to allow renewable electricity to play a significant role in future electricity generation and thus help confront issues related to climate change, energy security, and the escalation of energy costs … Renewable energy is an attractive option because renewable resources available in the United States, taken collectively, can supply significantly greater amounts of electricity than the total current or projected domestic demand.\"\n\nThe main barriers to the widespread implementation of large-scale renewable energy and low-carbon energy strategies are political rather than technological. According to the 2013 \"Post Carbon Pathways\" report, which reviewed many international studies, the key roadblocks are: climate change denial, the fossil fuels lobby, political inaction, unsustainable energy consumption, outdated energy infrastructure, and financial constraints.\n\nUsing 100% renewable energy was first suggested in a Science paper\npublished in 1975 by Danish physicist Bent Sørensen, which was followed by several other proposals. In 1976 energy policy analyst Amory Lovins coined the term \"soft energy path\" to describe an alternative future where energy efficiency and appropriate renewable energy sources steadily replace a centralized energy system based on fossil and nuclear fuels.\n\nIn 1998 the first detailed analysis of scenarios with very high shares of renewables were published. These were followed by the first detailed 100% scenarios. In 2006 a PhD thesis was published by Czisch in which it was shown that in a 100% renewable scenario energy supply could match demand in every hour of the year in Europe and North Africa. In the same year Danish Energy professor Henrik Lund published a first paper in which he addresses the optimal combination of renewables, which was followed by several other papers on the transition to 100% renewable energy in Denmark. Since then Lund has been publishing several papers on 100% renewable energy. After 2009 publications began to rise steeply, covering 100% scenarios for countries in Europe, America, Australia and other parts of the world.\n\nEven in the early 21st century it was extraordinary for scientists and decision-makers to consider the concept of 100 per cent renewable electricity. However, renewable energy progress has been so rapid that things have totally changed since then:\n\nSolar photovoltaic modules have dropped about 75 per cent in price. Current scientific and technological advances in the laboratory suggest that they will soon be so cheap that the principal cost of going solar on residential and commercial buildings will be installation. On-shore wind power is spreading over all continents and is economically competitive with fossil and nuclear power in several regions. Concentrated solar thermal power (CST) with thermal storage has moved from the demonstration stage of maturity to the limited commercial stage and still has the potential for further cost reductions of about 50 per cent.\nRenewable energy use has grown much faster than even advocates had anticipated. Wind turbines generate 39 percent of Danish electricity, and Denmark has many biogas digesters and waste-to-energy plants as well. Together, wind and biomass provide 44% of the electricity consumed by the country's six million inhabitants. In 2010, Portugal's 10 million people produced more than half their electricity from indigenous renewable energy resources. Spain's 40 million inhabitants meet one-third of their electrical needs from renewables.\n\nRenewable energy has a history of strong public support. In America, for example, a 2013 Gallup survey showed that two in three Americans want the U.S. to increase domestic energy production using solar power (76%), wind power (71%), and natural gas (65%). Far fewer want more petroleum production (46%) and more nuclear power (37%). Least favored is coal, with about one in three Americans favouring it.\n\nREN21 says renewable energy already plays a significant role and there are many policy targets which aim to increase this:\n\nAt the national level, at least 30 nations around the world already have renewable energy contributing more than 20% of energy supply. National renewable energy markets are projected to continue to grow strongly in the coming decade and beyond, and some 120 countries have various policy targets for longer-term shares of renewable energy, including a binding 20% by 2020 target for the European Union. Some countries have much higher long-term policy targets of up to 100% renewables. Outside Europe, a diverse group of 20 or more other countries target renewable energy shares in the 2020–2030 time frame that range from 10% to 50%.\nNuclear power involves substantial accident risks (e.g., Fukushima nuclear disaster, Chernobyl disaster) and the unsolved problem of safe long-term high-level radioactive waste management, and carbon capture and storage has rather limited safe storage potentials. These constraints have also led to an interest in 100% renewable energy. A well established body of academic literature has been written over the past decade, evaluating scenarios for 100% renewable energy for various geographical areas. In recent years, more detailed analyses have emerged from government and industry sources. The incentive to use 100% renewable energy is created by global warming and ecological as well as economic concerns, post peak oil.\n\nThe first country to propose 100% renewable energy was Iceland, in 1998. Proposals have been made for Japan in 2003, and for Australia in 2011. Albania, Iceland, and Paraguay obtain essentially all of their electricity from renewable sources (Albania and Paraguay 100% from hydroelectricity, Iceland 72% hydro and 28% geothermal). Norway obtains nearly all of its electricity from renewable sources (97 percent from hydropower). Iceland proposed using hydrogen for transportation and its fishing fleet. Australia proposed biofuel for those elements of transportation not easily converted to electricity. The road map for the United States, commitment by Denmark, and Vision 2050 for Europe set a 2050 timeline for converting to 100% renewable energy, later reduced to 2040 in 2011. Zero Carbon Britain 2030 proposes eliminating carbon emissions in Britain by 2030 by transitioning to renewable energy. In 2015, Hawaii enacted a law that the Renewable Portfolio Standard shall be 100 percent by 2045. This is often confused with renewable energy. If electricity produced on the grid is 65 GWh from fossil fuel and 35 GWh from renewable energy and rooftop off grid solar produces 80 GWh of renewable energy then the total renewable energy is 115 GWh and the total electricity on the grid is 100 GWh. Then the RPS is 115 percent.\n\nCities like Paris and Strasbourg in France, planned to use 100% renewable energy by 2050.\n\nIt is estimated that the world will spend an extra $8 trillion over the next 25 years to prolong the use of non-renewable resources, a cost that would be eliminated by transitioning instead to 100% renewable energy. Research that has been published in Energy Policy suggests that converting the entire world to 100% renewable energy by 2030 is both possible and affordable, but requires political support. It would require building many more wind turbines and solar power systems but wouldn't utilize bioenergy. Other changes involve use of electric cars and the development of enhanced transmission grids and storage. As part of the Paris Agreement, countries periodically update their climate change targets for the future, by 2018 no G20 country had committed to a 100% renewable target.\n\n\"\" is a German documentary film released in 2010. It shows the vision of a global society, which lives in a world where the energy is produced 100% with renewable energies, showing a complete reconstruction of the economy, to reach this goal. In 2011, Hermann Scheer wrote the book \"The Energy Imperative: 100 Percent Renewable Now\", published by Routledge.\n\n\"Reinventing Fire\" is a book by Amory Lovins released in October 2011. By combining reduced energy use with energy efficiency gains, Lovins says that there will be a $5 trillion saving and a faster-growing economy. This can all be done with the profitable commercialization of existing energy-saving technologies, through market forces, led by business. Bill Clinton says the book is a \"wise, detailed and comprehensive blueprint\". The first paragraph of the preface says:\n\nImagine fuel without fear. No climate change. No oil spills, dead coal miners, dirty air, devastated lands, lost wildlife. No energy poverty. No oil-fed wars, tyrannies, or terrorists. Nothing to run out. Nothing to cut off. Nothing to worry about. Just energy abundance, benign and affordable, for all, for ever.\nThe Intergovernmental Panel on Climate Change has said that there are few fundamental technological limits to integrating a portfolio of renewable energy technologies to meet most of total global energy demand. In a 2011 review of 164 recent scenarios of future renewable energy growth, the report noted that the majority expected renewable sources to supply more than 17% of total energy by 2030, and 27% by 2050; the highest forecast projected 43% supplied by renewables by 2030 and 77% by 2050.\n\nIn 2011, the International Energy Agency has said that solar energy technologies, in its many forms, can make considerable contributions to solving some of the most urgent problems the world now faces:\n\nThe development of affordable, inexhaustible and clean solar energy technologies will have huge longer-term benefits. It will increase countries’ energy security through reliance on an indigenous, inexhaustible and mostly import-independent resource, enhance sustainability, reduce pollution, lower the costs of mitigating climate change, and keep fossil fuel prices lower than otherwise. These advantages are global. Hence the additional costs of the incentives for early deployment should be considered learning investments; they must be wisely spent and need to be widely shared.\nIn 2011, the refereed journal \"Energy Policy\" published two articles by Mark Z. Jacobson, a professor of engineering at Stanford University, and research scientist Mark A. Delucchi, about changing our energy supply mix and \"Providing all global energy with wind, water, and solar power\". The articles analyze the feasibility of providing worldwide energy for electric power, transportation, and heating/cooling from wind, water, and sunlight (WWS), which are safe clean options. In Part I, Jacobson and Delucchi discuss WWS energy system characteristics, aspects of energy demand, WWS resource availability, WWS devices needed, and material requirements. They estimate that 3,800,000 5 MW wind turbines, 5350 100 MW geothermal power plants, and 270 new 1300 MW hydroelectric power plants will be required. In terms of solar power, an additional 49,000 300 MW concentrating solar plants, 40,000 300 MW solar photovoltaic power plants, and 1.7 billion 3 kW rooftop photovoltaic systems will also be needed. Such an extensive WWS infrastructure could decrease world power demand by 30%. In Part II, Jacobson and Delucchi address variability of supply, system economics, and energy policy initiatives associated with a WWS system. The authors advocate producing all new energy with WWS by 2030 and replacing existing energy supply arrangements by 2050. Barriers to implementing the renewable energy plan are seen to be \"primarily social and political, not technological or economic\". Energy costs with a WWS system should be similar to today's energy costs.\n\nIn general, Jacobson has said wind, water and solar technologies can provide 100 per cent of the world's energy, eliminating all fossil fuels. He advocates a \"smart mix\" of renewable energy sources to reliably meet electricity demand:\n\nBecause the wind blows during stormy conditions when the sun does not shine and the sun often shines on calm days with little wind, combining wind and solar can go a long way toward meeting demand, especially when geothermal provides a steady base and hydroelectric can be called on to fill in the gaps.\nA 2012 study by the University of Delaware for a 72 GW system considered 28 billion combinations of renewable energy and storage and found the most cost-effective, for the PJM Interconnection, would use 17 GW of solar, 68 GW of offshore wind, and 115 GW of onshore wind, although at times as much as three times the demand would be provided. 0.1% of the time would require generation from other sources.\n\nIn March 2012, Denmark's parliament agreed on a comprehensive new set promotional programs for energy efficiency and renewable energy that will lead to the country getting 100 percent of electricity, heat and fuels from renewables by 2050.\nIRENEC is an annual conference on 100% renewable energy started in 2011 by Eurosolar Turkey. The 2013 conference was in Istanbul.\n\nMore recently, Jacobson and his colleagues have developed detailed proposals for switching to 100% renewable energy produced by wind, water and sunlight, for New York, California and Washington states, by 2050. , a more expansive new plan for the 50 states has been drawn up, which includes an online interactive map showing the renewable resource potential of each of the 50 states. The 50-state plan is part of The Solutions Project, an independent outreach effort led by Jacobson, actor Mark Ruffalo, and film director Josh Fox.\n\n, many detailed assessments show that the energy service needs of a world enjoying radically higher levels of wellbeing, can be economically met entirely through the diverse currently available technological and organisational innovations around wind, solar, biomass, biofuel, hydro, ocean and geothermal energy. Debate over detailed plans remain, but transformations in global energy services based entirely around renewable energy are in principle technically practicable, economically feasible, socially viable, and so realisable. This prospect underpins the ambitious commitment by Germany, one of the world's most successful industrial economies, to undertake a major energy transition, Energiewende.\n\nIn 2015 a study was published in \"Energy and Environmental Science\" that describes a pathway to 100% renewable energy in the United States by 2050 without using biomass. Implementation of this roadmap is regarded as both environmentally and economically feasible and reasonable, as by 2050 it would save about $600 Billion Dollars health costs a year due to reduced air pollution and $3.3 Trillion global warming costs. This would translate in yearly cost savings per head of around $8300 compared to a business as usual pathway. According to that study, barriers that could hamper implementation are neither technical nor economic but social and political, as most people didn't know that benefits from such a transformation far exceeded the costs.\n\nIn June 2017, twenty-one researchers published an article in the Proceedings of the National Academy of Sciences of the United States of America rejecting Jacobson's earlier PNAS article, accusing him of modeling errors and of using invalid modeling tools. They further asserted he made implausible assumptions through his reliance upon increasing national energy storage from 43 minutes to 7 weeks, increasing hydrogen production by 100,000%, and increasing hydropower by the equivalent of 600 Hoover Dams. Article authors David G. Victor called Jacobson's work \"dangerous\" and Ken Caldeira emphasized that increasing hydropower output by 1,300 gigawatts, a 25% increase, is the equivalent flow of 100 Mississippi Rivers. Jacobson published a response in the same issue of the PNAS and also authored a blog post where he asserted the researchers were advocates of the fossil fuel industry. Another study published in 2017 confirmed the earlier results for a 100% renewable power system for North America, without changes in hydropower assumptions, but with more realistic emphasis on a balanced storage portfolio, in particular seasonal storage, and for competitive economics.\n\nIn 2015, Jacobson and Delucchi, together with Mary Cameron and Bethany Frew, examined with computer simulation (LOADMATCH), in more detail how a wind-water-solar (WWS) system can track the energy demand from minute to minute. This turned out to be possible in the United States for 5 years.\nIn 2017, the plan was further developed for 139 countries by a team of 27 researchers and in 2018, Jacobson and Delucchi with Mary Cameron and Brian Mathiesen published the LOADMATCH results for 20 regions in which the 139 countries in the world are divided. According to this research, a WWS system can follow the demand in all regions.\n\nThe program LOADMATCH receives as input estimated series, per half minute during 2050-2055, of\nand specifications of\n\nThe program has been carried out for each region 10-20 times with adapted input for the storage capacities, until a solution was found in which the energy demand was followed, per half minute for 5 years, with low costs.\n\nThe WWS system is assumed to connect in the electric network\n\nThe following places meet 90% or more of their average yearly electricity demand with renewable energy (incomplete list):\nSome other places have high percentages, for example the electricity sector in Denmark, , is 40% wind power, with plans in place to reach 85%. The electricity sector in Canada and the electricity sector in New Zealand have even higher percentages, 65% and 75% respectively, and Austria is approaching 70%. , the electricity sector in Germany sometimes meets almost 100% of the electricity demand with PV and wind power, and renewable electricity is over 25%. Albania has 94.8% of installed capacity as hydroelectric, 5.2% diesel generator; but Albania imports 39% of its electricity. In 2016, Portugal achieved 100% renewable electricity for four days between 7 May and 11 May, partly because efficient energy use had reduced electricity demand. France and Sweden have low carbon intensity, since they predominately use a mixture of nuclear power and hydroelectricity.\n\nAlthough electricity is currently a big fraction of primary energy; it is to be expected that with renewable energy deployment primary energy use will go down sharply as electricity use increases, as it is likely to be combined with some degree of further electrification. For example, electric cars achieve much better miles per gallon equivalent than fossil fuel cars, and another example is renewable heat such as in the case of Denmark which is proposing to move to greater use of heat pumps for heating buildings which provide multiple kilowatts of heat per kilowatt of electricity.\n\nThe most significant barriers to the widespread implementation of large-scale renewable energy and low carbon energy strategies, at the pace required to prevent runaway climate change, are primarily political and not technological. According to the 2013 \"Post Carbon Pathways\" report, which reviewed many international studies, the key roadblocks are:\n\nNASA Climate scientist James Hansen discusses the problem with rapid phase out of fossil fuels and said that while it is conceivable in places such as New Zealand and Norway, \"suggesting that renewables will let us phase rapidly off fossil fuels in the United States, China, India, or the world as a whole is almost the equivalent of believing in the Easter Bunny and Tooth Fairy.\" In 2013, Smil analyzed proposals to depend on wind and solar-generated electricity including the proposals of Jacobson and colleagues, and writing in an issue of \"Spectrum\" prepared by the Institute of Electrical and Electronics Engineers, he identified numerous points of concern, such as cost, intermittent power supply, growing NIMBYism, and a lack of infrastructure as negative factors and said that \"History and a consideration of the technical requirements show that the problem is much greater than these advocates have supposed.\" Smil and Hansen are concerned about the variable output of solar and wind power, but American physicist Amory Lovins has said that the electricity grid can cope, just as it routinely backs up nonworking coal-fired and nuclear plants with working ones.\n\nIn 1999 American academic Dr. Gregory Unruh published a dissertation identifying the systemic barriers to the adoption and diffusion of renewable energy technologies. This theoretical framework was called Carbon Lock-in and pointed to the creation of self-reinforcing feedbacks that arise through the co-evolution of large technological systems, like electricity and transportation networks, with the social and political institutions that support and benefit from system growth. Once established, these techno-institutional complexes become \"locked-in\" and resist efforts to transform them towards more environmentally sustainable systems based on renewable sources.\n\nLester R. Brown founder and president of the Earth Policy Institute, a nonprofit research organization based in Washington, D.C., says a rapid transition to 100% renewable energy is both possible and necessary. Brown compares with the U.S. entry into World War II and the subsequent rapid mobilization and transformation of the US industry and economy. A quick transition to 100% renewable energy and saving of our civilization is proposed by Brown to follow an approach with similar urgency.\n\nThe International Energy Agency says that there has been too much attention on issue of the variability of renewable electricity production. The issue of intermittent supply applies to popular renewable technologies, mainly wind power and solar photovoltaics, and its significance depends on a range of factors which include the market penetration of the renewables concerned, the balance of plant and the wider connectivity of the system, as well as the demand side flexibility. Variability will rarely be a barrier to increased renewable energy deployment when dispatchable generation such as hydroelectricity or solar thermal storage is also available. But at high levels of market penetration it requires careful analysis and management, and additional costs may be required for back-up or system modification. Renewable electricity supply in the 20-50+% penetration range has already been implemented in several European systems, albeit in the context of an integrated European grid system:\n\nIn 2011, the Intergovernmental Panel on Climate Change, the world's leading climate researchers selected by the United Nations, said \"as infrastructure and energy systems develop, in spite of the complexities, there are few, if any, fundamental technological limits to integrating a portfolio of renewable energy technologies to meet a majority share of total energy demand in locations where suitable renewable resources exist or can be supplied\". IPCC scenarios \"generally indicate that growth in renewable energy will be widespread around the world\". The IPCC said that if governments were supportive, and the full complement of renewable energy technologies were deployed, renewable energy supply could account for almost 80% of the world's energy use within forty years. Rajendra Pachauri, chairman of the IPCC, said the necessary investment in renewables would cost only about 1% of global GDP annually. This approach could contain greenhouse gas levels to less than 450 parts per million, the safe level beyond which climate change becomes catastrophic and irreversible.\n\nIn November 2014 the Intergovernmental Panel on Climate Change came out with their fifth report, saying that in the absence of any one technology (such as bioenergy, carbon dioxide capture and storage, nuclear, wind and solar), climate change mitigation costs can increase substantially depending on which technology is absent. For example, it may cost 40% more to reduce carbon emissions without carbon dioxide capture. (Table 3.2)\n\nGoogle spent $30 million on their RE<C project to develop renewable energy and stave off catastrophic climate change. The project was cancelled after concluding that a best-case scenario for rapid advances in renewable energy could only result in emissions 55 percent below the fossil fuel projections for 2050.\n\n\n\n"}
{"id": "287159", "url": "https://en.wikipedia.org/wiki?curid=287159", "title": "88 modern constellations", "text": "88 modern constellations\n\nIn contemporary astronomy, a \"constellation\" is one of 88 regions of the sky generally based on the asterisms (which are also called \"constellations\") of Greek and Roman mythology. The number 88, along with the contemporary scientific concept of \"constellation\" as regions of the sky, bordered by arcs of right ascensions and declinations, that together cover the entire celestial sphere, was established in 1922 by the International Astronomical Union.\n\nThe ancient Sumerians, and later the Greeks (as recorded by Ptolemy), established most of the northern constellations in international use today. The constellations along the ecliptic are called the zodiac. When explorers mapped the stars of the southern skies, European and American astronomers proposed new constellations for that region, as well as ones to fill gaps between the traditional constellations. Not all of these proposals caught on, but in 1922, the International Astronomical Union (IAU) adopted the modern list of 88 constellations. After this, Eugène Joseph Delporte drew up precise boundaries for each constellation, so that every point in the sky belonged to exactly one constellation.\n\nSome constellations are no longer recognized by the International Astronomical Union, but may appear in older star charts and other references. Most notable is Argo Navis, which was one of Ptolemy's original 48 constellations.\n\nThe 88 constellations depict 42 animals, 29 inanimate objects and 17 humans or mythological characters.\n\nEach of the IAU constellations has an official 3 letter abbreviation. They are actually abbreviations of the genitive form of the constellation names, so some letters appearing in the abbreviation may come from the genitive form without appearing in the base name (as in \"Sge\" for \"Sagitta/Sagittae\", to avoid confusion with \"Sagittarius\", abbreviated \"Sgr\").\n\nThe majority of the abbreviations are just the first three letters of the constellation, with the first character capitalised: \"Ori\" for \"Orion\", \"Ara\" for \"Ara/Arae\", \"Com\" for \"Coma Berenices\". In cases where this would not unambiguously identify the constellation, or where the name and its genitive differ in the first three letters, other letters beyond the initial three are used: \"Aps\" for \"Apus/Apodis\", \"CrA\" for \"Corona Australis\", \"CrB\" for \"Corona Borealis\", \"Crv\" for \"Corvus\". (\"Crater\" is abbreviated \"Crt\" to prevent confusion with \"CrA\".)\n\nWhen letters are taken from the second word of a two-word name, the first letter from the second word is capitalised: \"CMa\" for \"Canis Major\", \"CMi\" for \"Canis Minor\".\n\nThe abbreviations are unambiguous, with two exceptions. \"Leo\" for the constellation \"Leo\" could be mistaken for \"Leo Minor\" (abbreviated \"LMi\"), and \"Tri\" for \"Triangulum\" could be mistaken for \"Triangulum Australe\" (abbreviated \"TrA\").\n\nFor help with the literary English pronunciations, see the . There is considerable diversity in how Latinate names are pronounced in English. For traditions closer to the original, see Latin spelling and pronunciation.\n\nVarious other unofficial patterns exist alongside the constellations. These are known as \"asterisms\". Examples include the Big Dipper/Plough and the Northern Cross.\nSome ancient asterisms, for example Coma Berenices, Serpens, and portions of Argo Navis, are now officially constellations.\n\n\n"}
{"id": "36547550", "url": "https://en.wikipedia.org/wiki?curid=36547550", "title": "Air basin", "text": "Air basin\n\nAn air basin is an area within a ring or partial ring of mountains that in the absence of winds holds air and smog within the area.\n\n"}
{"id": "18516067", "url": "https://en.wikipedia.org/wiki?curid=18516067", "title": "Annie Aggens", "text": "Annie Aggens\n\nAnnie Aggens is a polar expedition leader for Polar Explorers. She is one of only a few women who have led treks to both the North and South Poles.\n\nIn May 2008, she led an international team on a 25-day trek across the Greenland Ice Cap. She is also the co-author, with Chris Townsend, of the \"Encyclopedia of Outdoor & Wilderness Skills: The Ultimate A-Z Guide for The Adventurous.\" 2003. Ragged Mountain Press. .\n\nIn 2006, she founded ICECAAP, an international consortium of polar explorers dedicated to preserving the polar environment.\n"}
{"id": "44061701", "url": "https://en.wikipedia.org/wiki?curid=44061701", "title": "Atmospheric Environment", "text": "Atmospheric Environment\n\nAtmospheric Environment is a peer-reviewed scientific journal covering research pertaining to air pollution and other ways humans and natural forces affect the Earth's atmosphere. It was established in 1967. In 1990 it was split into two parts, \"Atmospheric Environment, Part A: General Topics\" and \"Atmospheric Environment, Part B: Urban Atmosphere\", which were merged again in 1994. The editors-in-chief are C.K. Chan (Hong Kong University of Science and Technology), H.B. Singh (NASA Ames Research Center), and A. Wiedensohler (Leibniz Institute for Tropospheric Research). According to the \"Journal Citation Reports\", the journal has a 2013 impact factor of 3.062.\n"}
{"id": "974968", "url": "https://en.wikipedia.org/wiki?curid=974968", "title": "Black River (Ontario)", "text": "Black River (Ontario)\n\nBlack River may refer to several rivers by that name in the province of Ontario, Canada:\n\n\n"}
{"id": "2266108", "url": "https://en.wikipedia.org/wiki?curid=2266108", "title": "Brickfielder", "text": "Brickfielder\n\nThe Brickfielder is a hot and dry wind in the desert of Southern Australia that occurs in the summer season. It blows in the coastal regions of the south from the outback, where the sandy wastes, bare of vegetation in summer, are intensely heated by the sun. This hot wind blows strongly, often for several days at a time, defying all attempts to keep the dust down, and parching all vegetation. It is in one sense a healthy wind, as, being exceedingly dry and hot, it destroys many injurious germs. The northern brickfielder is almost invariably followed by a strong \"southerly buster,\" cloudy and cool from the ocean. The two winds are due to the same cause, viz. a cyclonic system over the Australian Bight. These systems frequently extend inland as a narrow V-shaped depression (the apex northward), bringing the winds from the north on their eastern sides and from the south on their western. Hence as the narrow system passes eastward the wind suddenly changes from north to south, and the thermometer has been known to fall in twenty minutes.\n\nThe brickfielder precedes the passage of a frontal zone of a low pressure system passing by, and causes severe dust storms that often last for days and led to its naming as the winds blow up red brick dust. A more frequently used term for the winds is a \"burster\".\n\n\n"}
{"id": "2630316", "url": "https://en.wikipedia.org/wiki?curid=2630316", "title": "Brunt–Väisälä frequency", "text": "Brunt–Väisälä frequency\n\nIn atmospheric dynamics, oceanography, asteroseismology and geophysics, the Brunt–Väisälä frequency, or buoyancy frequency, is the angular frequency at which a vertically displaced parcel will oscillate within a statically stable environment. It is named after David Brunt and Vilho Väisälä. It can be used as a measure of atmospheric stratification.\n\nConsider a parcel of (water or gas) that has density of formula_1 and the environment with a density that is a function of height: formula_2. If the parcel is displaced by a small vertical increment formula_3, it will be subject to an extra gravitational force against its surroundings of:\n\nformula_5 is the gravitational acceleration, and is defined to be positive. We make a linear approximation to formula_6, and move formula_1 to the RHS:\n\nThe above 2nd order differential equation has straightforward solutions of:\n\nwhere the Brunt–Väisälä frequency formula_10 is:\n\nFor negative formula_12, formula_3 has oscillating solutions (and N gives our angular frequency). If it is positive, then there is run away growth – i.e. the fluid is statically unstable.\n\nIn the atmosphere,\n\nIn the ocean where salinity is important, or in fresh water lakes near freezing, where density is not a linear function of temperature,\n\nThe concept derives from Newton's Second Law when applied to a fluid parcel in the presence of a background stratification (in which the density changes in the vertical). The parcel, perturbed vertically from its starting position, experiences a vertical acceleration. If the acceleration is back towards the initial position, the stratification is said to be stable and the parcel oscillates vertically. In this case, and the angular frequency of oscillation is given . If the acceleration is away from the initial position (), the stratification is unstable. In this case, overturning or convection generally ensues.\n\nThe Brunt–Väisälä frequency relates to internal gravity waves and provides a useful description of atmospheric and oceanic stability.\n\n"}
{"id": "17627250", "url": "https://en.wikipedia.org/wiki?curid=17627250", "title": "C/2007 W1 (Boattini)", "text": "C/2007 W1 (Boattini)\n\nC/2007 W1 (Boattini) is a long-period comet discovered on 20 November 2007, by Andrea Boattini at the Mt. Lemmon Survey. At the peak the comet had an apparent magnitude around 5.\n\nOn 3 April 2008, when C/2007 W1 was 0.66AU from the Earth and 1.7AU from the Sun, the coma (expanding tenuous dust atmosphere) of the comet was estimated to be as large as 10 arcminutes. This made the coma roughly 290,000 km in diameter.\n\nOn 12 June 2008, the comet passed within about of the Earth. The comet came to perihelion (closest approach to the Sun) on 24 June 2008 at a distance of 0.8497 AU.\n\nThe comet has an observation arc of 285 days allowing a good estimate of the orbit. The orbit of a long-period comet is properly obtained when the osculating orbit is computed at an epoch after leaving the planetary region and is calculated with respect to the center of mass of the solar system. Using JPL Horizons, the barycentric orbital elements for epoch 2020-Jan-01 generate a semi-major axis of 1,582 AU, an apoapsis distance of 3,163 AU, and a period of approximately 63,000 years.\n\nBefore entering the planetary region, C/2007 W1 had a hyperbolic trajectory. The comet was probably in the outer Oort cloud with a loosely bound chaotic orbit that was easily perturbed by passing stars.\n\n"}
{"id": "5111965", "url": "https://en.wikipedia.org/wiki?curid=5111965", "title": "C Centauri", "text": "C Centauri\n\nThe Bayer designations c Centauri and C Centauri are distinct. For technical reasons both titles redirect here.\n\nThe designation c Centauri is shared by two star systems in the constellation Centaurus:\n\nThe designation C Centauri is shared by three star systems:\n\nNot to be confused with:\n"}
{"id": "208691", "url": "https://en.wikipedia.org/wiki?curid=208691", "title": "CoRoT", "text": "CoRoT\n\nCoRoT (French: ; English: Convection, Rotation and planetary Transits) was a space observatory mission which operated from 2006 to 2013. The mission's two objectives were to search for extrasolar planets with short orbital periods, particularly those of large terrestrial size, and to perform asteroseismology by measuring solar-like oscillations in stars. The mission was led by the French Space Agency (CNES) in conjunction with the European Space Agency (ESA) and other international partners.\n\nAmong the notable discoveries was COROT-7b, discovered in 2009 which became the first exoplanet shown to have a rock or metal-dominated composition.\n\nCoRoT was launched at 14:28:00 UTC on 27 December 2006, atop a Soyuz 2.1b rocket, reporting first light on 18 January 2007. Subsequently, the probe started to collect science data on 2 February 2007. CoRoT is the first spacecraft dedicated to the detection of transiting extrasolar planets, opening the way for more advanced probes such as Kepler as well as future missions such as TESS and PLATO. It detected its first extrasolar planet, COROT-1b, in May 2007, just 3 months after the start of the observations. Mission flight operations were originally scheduled to end 2.5 years from launch but operations were extended to 2013. On 2 November 2012, CoRoT suffered a computer failure that made it impossible to retrieve any data from its telescope. Repair attempts were unsuccessful, so on 24 June 2013 it was announced that CoRoT has been retired and would be decommissioned; lowered in orbit to allow it to burn up in the atmosphere.\n\nThe CoRoT optical design minimized stray light coming from the Earth and provided a field of view of 2.7° by 3.05°. The CoRoT optical path consisted of a diameter off-axis afocal telescope housed in a two-stage opaque baffle specifically designed to block sunlight reflected by the Earth and a camera consisting of a dioptric objective and a focal box. Inside the focal box was an array of four CCD detectors protected against radiation by aluminum shielding 10mm thick. The asteroseismology CCDs are defocused by 760μm toward the dioptric objective to avoid saturation of the brightest stars. A prism in front of the planet detection CCDs gives a small spectrum designed to disperse more strongly in the blue wavelengths.\n\nThe four CCD detectors are model 4280 CCDs provided by E2V Technologies. These CCDs are frame-transfer, thinned, back-illuminated designs in a 2048 pixel by 2048 pixel array. Each pixel is 13.5 × 13.5μm in size which corresponds to an angular pixel size of 2.32 arcsec. The CCDs are cooled to . These detectors are arranged in a square pattern with two each dedicated to the planetary detection and asteroseismology. The data output stream from the CCDs are connected in two \"chains\". Each chain has one planetary detection CCD and one asteroseismology CCD. The field of view for planetary detection is 3.5°.\nThe satellite, built in the Cannes Mandelieu Space Center, had a launch mass of 630 kg, was 4.10 m long, 1.984 m in diameter and was powered by two solar panels.\n\nThe satellite observed perpendicular to its orbital plane, meaning there were no Earth occultations, allowing up to 150 days of continuous observation. These observation sessions, called \"Long Runs\", allowed detection of smaller and long-period planets. During the remaining 30 days between the two main observation periods, CoRoT observed other patches of sky for a few weeks long \"Short Runs\", in order to analyze a larger number of stars for the asteroseismic program. After the loss of half the field of view due to failure of Data Processing Unit No. 1 in March 2009, the observation strategy changed to 3 months observing runs, in order to optimize the number of observed stars and detection efficiency.\n\nIn order to avoid the Sun entering in its field of view, during the northern summer CoRoT observed in an area around Serpens Cauda, toward the galactic center, and during the winter it observed in Monoceros, in the Galactic anticenter. Both these \"eyes\" of CoRoT have been studied in preliminary observations carried out between 1998 and 2005, allowing the creation of a database, called COROTSKY, with data about the stars located in these two patches of sky. This allowed selecting the best fields for observation: the exoplanet research program requires a large number of dwarf stars to be monitored, and to avoid giant stars, for which planetary transits are too shallow to be detectable. The asteroseismic program required stars brighter than magnitude 9, and to cover as many different types of stars as possible. In addition, in order to optimize the observations, the fields shouldn't be too sparse – fewer targets observed – or too crowded – too many stars overlapping. Several fields have been already observed:\n\nThe spacecraft monitored the brightness of stars over time, searching for the slight dimming that happens in regular intervals when planets transit their host star. In every field, CoRoT recorded the brightness of thousands stars in the V-magnitude range from 11 to 16 for the extrasolar planet study. In fact, stellar targets brighter than 11 saturated the exoplanets CCD detectors, yielding inaccurate data, whilst stars dimmer than 16 don't deliver enough photons to allow planetary detections. CoRoT was sensitive enough to detect rocky planets with a radius two times larger than Earth, orbiting stars brighter than 14; it is also expected to discover new gas giants in the whole magnitude range.\n\nCoRoT also studied asteroseismology. It can detect luminosity variations associated with acoustic pulsations of stars. This phenomenon allows calculation of a star's precise mass, age and chemical composition and will aid in comparisons between the sun and other stars. For this program, in each field of view there was be one main target star for asteroseismology as well as up to nine other targets. The number of observed targets have dropped to half after the loss of Data Processing Unit No. 1.\n\nThe mission began on 27 December 2006 when a Russian Soyuz 2-1b rocket lifted the satellite into a circular polar orbit with an altitude of 827 km . The first scientific observation campaign started on 3 February 2007.\n\nUntil March 2013, the mission's cost will amount to , of which 75% is paid by the French space agency CNES and 25% is contributed by Austria, Belgium, Germany, Spain, Brazil and the European Space Agency ESA.\n\nThe primary contractor for the construction of the CoRoT vehicle was CNES, to which individual components were delivered for vehicle assembly. The COROT equipment bay, which houses the data acquisition and pre-processing electronics, was constructed by the LESIA Laboratory at the Paris Observatory and took 60 to complete. The design and building of the instruments were done by the Laboratoire d'études spatiales et d'instrumentation en astrophysique (LESIA) de l'Observatoire de Paris, the Laboratoire d'Astrophysique de Marseille, the Institut d'Astrophysique Spatiale (IAS) from Orsay, the Centre spatial de Liège (CSL) in Belgium, the IWF in Austria, the DLR (Berlin) in Germany and the ESA Research and Science Support Department. The 30 cm afocal telescope Corotel has been realized by Alcatel Alenia Space in the Centre spatial de Cannes Mandelieu.\n\nBefore the beginning of the mission, the team stated with caution that CoRoT would only be able to detect planets few times larger than Earth or greater, and that it was not specifically designed to detect habitable planets. According to the press release announcing the first results, CoRoT's instruments are performing with higher precision than had been predicted, and may be able to find planets down to the size of Earth with short orbits around small stars.\nThe transit method requires the detection of at least two transits, hence the planets detected will mostly have an orbital period under 75-day. Candidates that show only one transit have been found, but uncertainty remains about their exact orbital period.\n\nCoRoT should be assumed to detect a small percentage of planets within the observed star fields, due to the low percentage of exoplanets that would transit from the angle of observation of the Solar System. The chances of seeing a planet transiting its host star is inversely proportional to the diameter of the planet's orbit, thus close in planets detections will outnumber outer planets ones. The transit method is also biased toward large planets, since their very depth transits are more easily detected than the shallows eclipses induced by terrestrial planets.\n\nOn 8 March 2009 the satellite suffered a loss of communication with Data Processing Unit No. 1, processing data from one of the two photo-detector chains on the spacecraft. Science operations resumed early April with Data Processing Unit No. 1 offline while Data Processing Unit No. 2 operating normally. The loss of photo-detector chain number 1 results in the loss of one CCD dedicated to asteroseismology and one CCD dedicated to planet detection. The field of view of the satellite is thus reduced by 50%, but without any degradation of the quality of the observations. The loss of channel 1 appears to be permanent.\n\nThe rate of discoveries of transiting planets is dictated by the need of ground-based, follow-up observations, needed to verify the planetary nature of the transit candidates. Candidate detections have been obtained for about 2.3% of all CoRoT targets, but finding periodic transit events isn't enough to claim a planet discovery, since several configurations could mimic a transiting planet, such as stellar binaries, or an eclipsing fainter star very close to the target star, whose light, blended in the light curve, can reproduce transit-like events. A first screening is executed on the light curves, searching hints of secondary eclipses or a rather V-shaped transit, indicative of a stellar nature of the transits. For the brighter targets, the prism in front of the exoplanets CCDs provides photometry in 3 different colors, enabling to reject planet candidates that have different transit depths in the three channels, a behaviour typical of binary stars. These tests allow to discard 83% of the candidate detections, whilst the remaining 17% are screened with photometric and radial velocity follow-up from a network of telescopes around the world. Photometric observations, required to rule out a possible contamination by a diluted eclipsing binary in close vicinity of the target, is performed on several 1 m-class instruments, but also employs the 2 m Tautenburg telescope in Germany and the 3,6 m CFHT/Megacam in Hawaii. The radial velocity follow-up allows to discard binaries or even multiple star system and, given enough observations, provide the mass of the exoplanets found. Radial velocity follow-up is performed with high-precision spectrographs, namely SOPHIE, HARPS and HIRES. Once the planetary nature of the candidate is established, high-resolution spectroscopy is performed on the host star, in order to accurately determine the stellar parameters, from which further exoplanet characteristics can be derived. Such work is done with large aperture telescopes, as the UVES spectrograph or HIRES.\n\nInteresting transiting planets could be further followed-up with the infrared Spitzer Space Telescope, to give an independent confirmation at a different wavelength and possibly detect reflected light from the planet or the atmospheric compositions. COROT-7b and COROT-9b have already been observed by Spitzer.\n\nPapers presenting the results of follow-up operations of planetary candidates in the IRa01, LRc01, LRa01, SRc01 fields have been published. Sometimes the faintness of the target star or its characteristics, such as a high rotational velocity or strong stellar activity, do not allow to determine unambiguously the nature or the mass of the planetary candidate.\n\nStars vibrate according to many different pulsation modes in much the same way that musical instruments emit a variety of sounds. Listening to an air on the guitar does not leave any doubt as to the nature of the instrument, and an experienced musician can even deduce the cords' material and tension. Similarly, stellar pulsation modes are characteristic of global stellar properties and of the internal physical conditions. Analyzing these modes is thus a way of probing stellar interiors to infer stellar chemical composition, rotation profiles and internal physical properties such as temperatures and densities. Asteroseismology is the science which studies the vibration modes of a star. Each of these modes can be mathematically represented by a spherical harmonic of degree l and azimuthal order m. Some examples are presented here below with a color scheme in which blue (red) indicates contracting (expanding) material. The pulsation amplitudes are highly exaggerated.\n\nWhen applied to the Sun, this science is called helioseismology and has been ongoing for a few decades by now. The solar surface helium abundance was derived very accurately for the first time, which has definitely shown the importance of microscopic diffusion in the solar structure. Helioseismology analyses have also unveiled the solar internal rotational profile, the precise extent of the convective envelope and the location of the helium ionization zone. Despite enormous technical challenges, it was thus tempting to apply similar analyses to stars. From the ground this was only possible for stars close to the Sun such as α Centauri, Procyon, β Virginis... The goal is to detect extremely small light variations (down to 1 ppm) and to extract the frequencies responsible for these brightness fluctuations. This produces a frequency spectrum typical of the star under scrutiny. Oscillation periods vary from a few minutes to several hours depending of the type of star and its evolutionary state. To reach such performances, long observing times devoid of day/night alternations are required. Space is thus the ideal asteroseismic laboratory. By revealing their microvariability, measuring their oscillations at the ppm level, CoRoT has provided a new vision of stars, never reached before by any ground-based observation.\n\nAt the beginning of the mission, two out of four CCDs were assigned to asteroseismic observations of bright stars (apparent magnitude 6 to 9) in the so-called \"sismo field\" while the other CCDs were reserved for exoplanet hunting in the so-called \"exo field\". Albeit with a lower signal to noise ratio, interesting science on stars was also obtained from the exoplanets channel data, where the probe records several thousands of light curves from every observed field. Stellar activity, rotation periods, star spot evolution, star–planet interactions, multiple star systems are nice extras in addition to the main asteroseismic program. This exo field also turned out to be of incalculable richness in asteroseismic discoveries. During the first six years of its mission, CoRoT has observed about 150 bright stars in the \"sismo field\" and more than 150 000 weak stars in the \"exo field\". The figure shows where most of them are located in the Hertzsprung–Russell diagram together with some others observed from the ground.\n\nDiscoveries were numerous. Let us cite the first detection of solar-like oscillations in stars other than the Sun, the first detection of non-radial oscillations in red giant stars, the detection of solar-like oscillations in massive stars, the discovery of hundreds of frequencies in δ Scuti stars, the spectacular time evolution of the frequency spectrum of a Be (emission lines B) star during an outburst, the first detection of a deviation from a constant period spacing in gravity modes in an SPB (Slowly Pulsating B) star. Interpreting those results opened new horizons in our vision of stars and galaxies. In October 2009 the CoRoT mission was the subject of a special issue of \"Astronomy and Astrophysics\", dedicated to the early results of the probe. Below are some examples of breakthrough contributions to stellar astrophysics, based on CoRoT's data:\n\nAbove the convective core where mixing of chemicals is instantaneous and efficient, some layers can be affected by partial or total mixing during the main sequence phase of evolution. The extent of this \"extra mixed zone\" as well as the mixing efficiency are, however, difficult to assess. This additional mixing has very important consequences since it involves longer time scales for nuclear burning phases and may in particular affect the value of the stellar mass at the transition between those stars which end up their life as white dwarfs and those which face a final supernova explosion. The impact on the chemical evolution of the galaxy is obvious. Physical reasons for this extra-mixing are various, either a mixing induced by internal rotation or a mixing resulting from convective bubbles crossing the convective core boundary to enter the radiative zone where they finally lose their identity (overshooting), or even some other poorly known processes.\n\n\nFollowing exhaustion of hydrogen in the core, the overall stellar structure drastically changes. Hydrogen burning now takes place in a narrow shell surrounding the newly processed helium core. While the helium core quickly contracts and heats up, the layers above the hydrogen-burning shell undergo important expansion and cooling. The star becomes a red giant whose radius and luminosity increase in time. These stars are now located on the so-called red giant branch of the Hertzsprung–Russell diagram; they are commonly named \"RGB stars\". Once their central temperature reaches 100 10 K, helium starts burning in the core. For stellar masses smaller than about 2 M, this new combustion takes place in a highly degenerate matter and proceeds through a helium flash. The readjustment following the flash brings the red giant to the so-called red clump (RC) in the Hertzsprung-Russell diagram.\n\nWhether RGB or RC, these stars all have an extended convective envelope favorable to the excitation of solar-like oscillations. A major success of CoRoT has been the discovery of radial and long-lived non-radial oscillations in thousands of red giants in the exo field. For each of them, the frequency at maximum power ν in the frequency spectrum as well as the large frequency separation between consecutive modes Δν could be measured, defining a sort of individual seismic passport.\n\n\nMassive variable main sequence stars have frequency spectra dominated by acoustic modes excited by the κ mechanism at work in layers where partial ionization of iron group elements produce a peak in opacity. In addition the most advanced of these stars present mixed modes i.e. modes with a g-character in deep layers and p-character in the envelope. Hydrogen burning takes place in a convective core surrounded by a region of varying chemical composition and an envelope mostly radiative except for tiny convective layers related to partial ionization of helium and/or iron group elements. As in lower mass stars the extent of the fully or partially mixed region located just above the convective core \"(extra-mixed zone)\" is one of the main uncertainties affecting theoretical modeling.\n\nAnother unexpected CoRoT discovery was the presence of solar-like oscillations in massive stars. The small convective shell related to the opacity peak resulting from the ionization of iron group elements at about 200 000 K (iron opacity peak) could indeed be responsible for the stochastic excitation of acoustic modes like those observed in our Sun.\n\nDuring a 23-day observing run in March 2008, CoRoT observed 636 members of the young open cluster NGC 2264. The so-called \"Christmas tree cluster\", is located in the constellation Monoceros relatively close to us at a distance of about 1800 light years. Its age is estimated to be between 3 and 8 million years. At such a young age, the cluster is an ideal target to investigate many different scientific questions connected to the formation of stars and early stellar evolution. The CoRoT data of stars in NGC 2264 allow us to study the interaction of recently formed stars with their surrounding matter, the rotation and activity of cluster members as well as their distribution, the interiors of young stars by using asteroseismology, and planetary and stellar eclipses.\n\nThe stellar births and the stars' childhoods remain mostly hidden from us in the optical light because the early stars are deeply embedded in the dense molecular cloud from which they are born. Observations in the infrared or X-ray enable us to look deeper into the cloud, and learn more about these earliest phases in stellar evolution.\nTherefore, in December 2011 and January 2012, CoRoT was part of a large international observing campaign involving four space telescopes and several ground-based observatories. All instruments observed about 4000 stars in the young cluster NGC 2264 simultaneously for about one month at different wavelengths. The Canadian space mission MOST targeted the brightest stars in the cluster in the optical light, while CoRoT observed the fainter members. MOST and CoRoT observed NGC 2264 continuously for 39 days. The NASA satellites Spitzer and Chandra measured at the same time the stars in the infrared (for 30 days) and the X-ray domains (for 300 kiloseconds). Ground-based observations were taken also at the same time, for example, with the ESO Very Large Telescope in Chile, the Canadian-French-Hawaiian Telescope in Hawaii, the McDonald Observatory in Texas, or the Calar Alto Observatory in Spain.\n\nThe CoRoT observations led to the discovery of about a dozen pulsating pre-main sequence (PMS) δ Scuti stars and the confirmation of the existence of γ Doradus pulsations in PMS stars. Also the presence of hybrid δ Scuti/γ Doradus pulsations was confirmed in members of NGC 2264. The CoRoT observations included also the well known pre-main sequence pulsators, V 588 Mon and V 589 Mon, which were the first discovered members of this group of stars. The precision attained in the CoRoT light curves also revealed the important role of granulation in pre-main sequence stars.\n\nThe investigation of T Tauri stars and their interaction with their circumstellar matter using CoRoT data revealed the existence of a new class, the AA Tauri type objects. Previously to the CoRoT observations, T Tauri stars were known to either show sinusoidal light variations that are caused by spots on the stellar surface, or completely irregular variability that is caused by the gas and dust disks surrounding the young stars. AA Tauri type objects show periodically occurring minima that are different in depth and width, hence are semi-regular variables. With the CoRoT observations this class of objects could be established. Exciting insights into the earliest phases of stellar evolution also come from the comparison of the variability present in the optical light to that in the infrared and the X-ray regime.\n\nA large number of binary systems with non-radially pulsating members were observed by CoRoT. Some of them, which were eclipsing binaries with members of γ Doradus type, were discovered during CoRoT runs. The eclipse phenomenon plays a key role since global parameters can immediately follow, bringing invaluable constraints, in addition to the seismic ones, to stellar modeling.\n\n\nTo find extra solar planets, CoRoT uses the method of transits detection. The primary transit is the occultation of a fraction of the light from a star when a celestial object, such as a planet, passes between the star and the observer. Its detection is made possible by the sensitivity of CCD to very small changes in light flux. Corot is capable of detecting changes in brightness of about 1/10,000. Scientists can thus hope finding planets with a size of approximately 2 times that of the Earth with this method, a class of planet called Super-Earth; detection of Corot-7b, whose radius is 1.7 times that of the Earth has shown that these predictions were correct. Corot takes an exposure of 32 seconds duration, each 32 seconds, but the image is not fully transmitted to Earth because the data flow would be too large. The onboard computer performs an important work of data reduction: the field around each target star, previously selected by the exoplanets team, is defined on a certain number of pixels described by a particular mask, the sum all pixels within the mask is then performed and several exposures are added (usually 16, which amounts to an integration time of about 8 minutes) before sending this information to the ground. For some stars, considered particularly of interest, data of each exposure is transmitted every 32 seconds. Such a sampling of 32s or 512s is well suited to the detection of a planetary transit that lasts from a little less than an hour to several hours.\nA feature of this method is that it requires to detect at least three successive transits separated by two equal time intervals before one can consider a target as a serious candidate. A planet of orbital period \"T\" should at least be observed for a time interval between 2\"T\" and 3\"T\" to have a chance to detect three transits. The distance of the planet to the star ( which is characterized by \"a\" the semi-major axis of the elliptical orbit ) is linked to its orbital period by the second law of Kepler / Newton \"a\" = \"T\" M, using respectively as units for \"a\", M and \"T\": the distance from the Earth to the Sun (150 million km), the mass of the Sun, the orbital period of the Earth (1 year); this implies that if the observing time is less a year, for example, the orbits of the detectable planets will be significantly smaller than that of the Earth.\nSo, for Corot, due to the maximum duration of 6 months of observation for each star field, only planets closer to their stars than 0.3 Astronomical Units (less than the distance between the Sun and Mercury) can be detected, therefore generally not in the so-called habitable zone. The Kepler mission (NASA) has continuously observed the same field for many years and thus had the ability to detect Earth sized planets located farther from their stars.\n\nThe moderate number of exoplanets discovered by Corot (32 during the 6 years of operation), is explained by the fact that a confirmation should absolutely be provided by ground-based telescopes, before any announcement is made. Indeed, in the vast majority of cases, the detection of several transits does not mean the detection of a planet, but rather that of a binary star system, either one that corresponds to a grazing occultation of a star by the other, or that the system is close enough to a bright star (the CoRoT target) and the effect of transit is diluted by the light of this star; in both cases the decrease in brightness is low enough to be compatible with that of a planet passing in front of the stellar disk. To eliminate these cases, one performs observations from the ground using two methods: radial velocity spectroscopy and imaging photometry with a CCD camera. In the first case, the mass of the binary stars is immediately detected and in the second case one can expect to identify in the field the binary system near the target star responsible for the alert: the relative decline of brightness will be greater than the one seen by Corot which adds all the light in the mask defining the field of measurement. In consequence, the COROT exoplanet science team has decided to publish confirmed and fully characterized planets only and not simple candidate lists. This strategy, different from the one pursued by the Kepler mission, where the candidates are regularly updated and made available to the public, is quite lengthy. On the other hand, the approach also increases the scientific return of the mission, as the set of published COROT discoveries constitute some of the best exoplanetary studies carried out so far.\n\nCoRoT discovered its first two planets in 2007: the hot Jupiters COROT-1b and COROT-2b. Results on asteroseismology were published in the same year.\n\nIn May 2008, two new exoplanets of Jupiter size, COROT-4b and COROT-5b, as well as an unknown massive celestial object, COROT-3b, were announced by ESA.\n\nIn February 2009, during the First Corot Symposium, the super-earth COROT-7b was announced, which at the time was the smallest exoplanet to have its diameter confirmed, at 1.58 Earth diameters. The discoveries of a second non-transiting planet in the same system, COROT-7c, and of a new Hot Jupiter, COROT-6b, were also announced at the Symposium.\n\nIn March 2010 COROT-9b was announced. It's a long period planet (95.3 days) in an orbit close to that of Mercury.\n\nIn June 2010 the CoRoT team announced six new planets, COROT-8b, COROT-10b, COROT-11b, COROT-12b, COROT-13b, COROT-14b, and a brown dwarf, COROT-15b. All the planets announced are Jupiter sized, except COROT-8b, which appears to be somewhat between Saturn and Neptune. The probe was also able to tentatively detect the reflected light at optical wavelengths of HD46375 b, a non-transiting planet.\n\nIn June 2011, during the Second CoRoT Symposium, the probe added ten new objects to the Exoplanet catalogue: COROT-16b, COROT-17b, COROT-18b, COROT-19b, COROT-20b, COROT-21b, COROT-22b, COROT-23b, COROT-24b, COROT-24c.\n\nAs of November 2011, around 600 additional candidate exoplanets are being screened for confirmation.\n\nAmong the exoplanets CoRoT detected, one can highlight a subset with the most original features :\n\nThe following transiting planets have been announced by the mission.\n\n\"Light green rows indicate that the planet orbits one of the stars in a binary star system.\"\nThe following table illustrates brown dwarf detected by COROT as well as non-transiting planets detected in the follow-up program:\n All Corot planets were detected during long runs \"i.e.\" of at least 70 days. The \"detection team \" found on average between 200 and 300 cases of periodic events for each run, corresponding to 2–3% of the stars monitored. Of these, only 530 in total were selected as candidate planets (223 in the direction of the galactic anti-center and 307 towards the center ). Only 30 of them were finally found to be true planets, \"i.e.\" about 6%, other cases being eclipsing binaries ( 46%) or unresolved cases (48%).\n\nThe CoRoT planets cover the wide range of properties and features found in the disparate family of exoplanets: for instance, the masses of CoRoT planets cover a range of almost four orders of magnitude, as shown on Figure.\n\nTracing the mass of the planet versus the mass of the star (Figure), one finds that the CoRoT data set, with its lower scatter than other experiments, indicates a clear trend that massive planets tend to orbit massive stars, which is consistent with the most commonly accepted models of planetary formation.\n\n\n"}
{"id": "41413797", "url": "https://en.wikipedia.org/wiki?curid=41413797", "title": "Countess of Dufferin Fund", "text": "Countess of Dufferin Fund\n\nThe Countess of Dufferin Fund was established by Hariot Hamilton-Temple-Blackwood, Marchioness of Dufferin and Ava, more commonly known as Lady Dufferin, in 1885 and was dedicated to improving women’s healthcare in India. The Fund was founded after Queen Victoria gave Lady Dufferin the task of improving healthcare for women in India. The Fund provided scholarships for women to be educated in the medical field as doctors, hospital assistants, nurses, and midwives. It also financed the construction of female hospitals, dispensaries, and female only wards in preexisting hospitals. The Fund marks the beginning of Western medicine for women in India and global health as a diplomatic concern.\n\nDuring the 19th century there was a major push in India to improve healthcare for women, especially maternal health. Lying-in hospitals were built as well as training and teaching hospitals. Many hospitals were also constructing wards for women and learning to treat female-specific diseases.\n\nIn 1885, Lady Dufferin set up the Fund after being contacted by Queen Victoria who gave her the task of helping the suffering women of India. Queen Victoria had been recently contacted by Elizabeth Bielby, a missionary in India who focused on women’s health. During Beilby’s mission, she had treated the Maharani of Puna who gave her a message to relay to the Queen of England. The message said that “the women of India suffer when they are sick.” In response, Queen Victoria wrote back to the Maharani saying: Lady Dufferin then started the Fund after being summoned to Windsor Castle by the Queen who gave her the task of improving healthcare and education for the women of India. A visit from Dr. Mary Scharlieb, the first female British doctor to practice in India, also mobilized the Queen to act on women’s poor health and suffering in India. She met with Queen Victoria and expressed a similar message as the Maharni’s: the dire situation of Indian women.\n\nLady Dufferin established the Fund in 1885 and immediately began creating projects and channeling money towards women’s health and teaching in India. The Countess of Dufferin Fund is also known as “The National Association for Supplying Medical Aid to the Women of India” and the “Lady Dufferin Fund.” This Fund marked one of the first diplomatic pushes to improve global health in the world, and the introduction of western medicine for women in India.\n\nThe Fund had three primary goals: providing medical tuition, medical relief, and female nurses and midwives to assist in hospitals and private homes. The Fund supplied scholarships for the medical education (medical tuition) of women in India. The education of traditional Indian midwives, called dias, was a major goal of the Fund because many western doctors observed the dais’s practices and found their traditions to be harmful. For example, the dais’s would massage the abdomen of the mother to speed up labor however that tradition caused uterine prolapse, a widespread issue amongst Indian women at the time. Because the dias’ methods were viewed as violent and extremely harmful, the Fund put forth money to educate them on successful ways to help women before, during, and after childbirth. The Fund also provided medical relief by establishing dispensaries and cottage hospitals for women and children under female superintendence. In addition, it opened female wards in existing hospitals also under female management as well as all women hospitals called zenana hospitals. The Fund also supplied trained female nurses and midwives in hospitals and in private homes.\n\nThe Fund financed treatment and teaching hospitals in Bihar, Calcutta, Madras, Karachi, Delhi, Bombay and in many of the United Provinces (roughly present day Uttar Pradesh and Uttarakhand). Most notably, the Fund sponsored the Lady Aitchison Hospital in Lahore, India. The Lady Aitchison Hospital, also known as the Aitchison Memorial Hospital, was a major center for training nurses and tradition midwives like the dias. Many of the hospitals the Fund financed are still functioning today. For example, the Lady Dufferin Hospital in Karachi is the largest solely female dedicated hospital today in Pakistan.\n\nThe Fund’s major financial basis was donations. Many saw this as a source of instability because donations were based on the popularity of Lady Dufferin and her husband as well as the favors expected by the donors in return. The administration of the Fund consisted of a central committee of members of the Viceroy’s Council and Home Department. It also included many influential Englishmen and Indians such as the Maharja Sir Jotendro Mohun Tagore, Sir Syed Ahmed Khan, and Sir Dinshaw Maneckji Petit.\n\nThere were three major criticisms of the Fund: its teaching, effectiveness, and integrity. Many believed the Fund was inefficient. Some argued that they inadequately taught doctors and employed subpar medical practitioners. By October 1908, only 43 completely qualified women medical professionals were working under the Fund however only 11 held university degrees. The British Medical Journal wrote in 1908 that, “The Government appears to have a perfect delight in swamping the country with unqualified medical practitioners.\" The Fund was also criticized as ineffective for placing male doctors in Zenana hospitals. Zenana women’s traditions forbade them from seeing, so they could be treated by the doctors the Fund provided. In addition, the Fund was also criticized for not giving the women they were educating enough reason to stay with learning medicine. The Fund paid for their education however their salaries were not high enough so it made more financial sense for the women to marry before they could give back to the hospital that educated them. The Fund set up this cycle of educating women who would then work for them, however, it was ineffective in some cases because the women would not stick with the medical profession for financial reasons. Because of the many criticisms of ineffectiveness, some of the Fund’s highest critics like The British Medical Journal called for a reorganization of the Fund. Some also believed vanity, not philanthropy, was the source of motivation for establishing the Fund. They criticized both Lady Dufferin for starting the Fund for her personal image and Queen Victoria for supporting the Fund for Britain’s international image.\n\nThe Fund continued even after Lady Dufferin’s term ended. Lady Lansdowne, who succeeded Lady Dufferin, continued to put work into the Fund which was passed down from vicereine to vicereine until 1947. In 1947, India gained independence from Great Britain and the Fund was taken over by the Central Indian Government by the Countess of Dufferin’s Fund Act, 1957. Once the Fund was taken over by the Central Government, it became obsolete. In 2005 a bill was passed repealing the Countess of Dufferin’s Fund Act, 1957, for unclear reasons.\n\n"}
{"id": "42946536", "url": "https://en.wikipedia.org/wiki?curid=42946536", "title": "Court of the Myrtles", "text": "Court of the Myrtles\n\nThe \"Court of the Myrtles\" (Patio de los Arrayanes) is part of the palace and fortress complex of the Alhambra. It is located east of the \"Gilded Room\" (Cuarto Dorado) and west of the \"Patio of the Lions\" and the \"Baths\". Its current name is due to the myrtle bushes that surround the central pond and the bright green colour of which contrasts with the white marble of the patio. It was also called the Patio of the Pond or the Reservoir (Patio del Estanque o de la Alberca) because of the central pond, which is 34 metres long and 7,10 meters wide. The patio is divided in two sides by the pond, which receives its water from two fountains. The space has chambers and porticoes around it. These porticoes rest on columns with cubic capitals, which have seven semicircular arches decorated with fretwork rhombuses and inscriptions praising God. The central arch is greater than the other six and has solid scallops decorated with stylised vegetal forms and capitals of mocarabes.\n\nThe most important chambers that surround the Patio are the ones in the north side, which are part of the Comares Palace, the official residence of the King.\n\nThe name of the Palace, \"Comares\", has led to various etymological research. For instance, Diego de Guadix wrote a dictionary about Arabic words in which it is said that \"Comares\" originally comes from \"cun\" and \"ari\". The first term means \"stand up\" and the second one \"look\", in other words it would have meant \"Stand up and look around\" or possibly \"Open your eyes and see\", which is a way of referring the beauty of the place.\n\nIn the sixteenth century, a historian from Granada called Luis de Mármol Carvajal claimed that the term \"Comares\" derived from the word \"Comaraxía\", that actually has a meaning related to a craftsmanship labor very appreciated by Muslims: a manufacturing technique of glass for exterior and ceilings.\n\nA third suggested theory is that the name comes from the Arab word \"qumariyya\" or \"qamariyya\". These ones designate the stained glasses that can even be glimpsed from the Hall of the Ambassadors' balcony.\n\nThere's another possibility that says that \"Qumarish\" is the name of a region in the North of Africa where most craftsmen came from, in other words, the place might be called Comares in honour of the people who worked there.\n\n\n"}
{"id": "27684483", "url": "https://en.wikipedia.org/wiki?curid=27684483", "title": "Eco-investing", "text": "Eco-investing\n\nEco-investing or green investing, is a form of socially responsible investing where investments are made in companies that support or provide environmentally friendly products and practices. These companies encourage (and often profit from) new technologies that support the transition from carbon dependence to more sustainable alternatives.\n\nAs industries' environmental impacts become more apparent, green topics have not only taken center stage in pop culture, but the financial world as well. In the 1990s many investors “began to look for those companies that were better than their competitors in terms of managing their environmental impact.” While some investors still focus their funds to avoid only “the most egregious polluters,” the emphasis for many investors has switched to changing “the way money is used,” and using “it in a positive, transformative way to get us from where we are now ultimately to a truly sustainable society.”\n\nThe Global Climate Prosperity Scoreboard – launched by Ethical Markets Media and The Climate Prosperity Alliance to monitor private investments in green companies – estimated that over $1.248 trillion has been invested in solar, wind, geothermal, ocean/hydro and other green sectors since 2007. This number represents investments from North America, China, India, and Brazil, as well at other developing countries.\n\nWhile many eco-investments may be considered socially responsible investments, and vice versa, the two are not mutually inclusive. Socially responsible investing is the practice of investing only in those companies which satisfy certain moral or ethical criteria. This may include companies with an interest in the environment, but also supports various other social and religious issues.\n\nEco-investing narrows in on the interests of sustainable environmental issues. Specifically, eco-investments focus on companies who work on renewable energy and clean technologies.\n\nThere are several sectors that fall under the eco-investing umbrella. Renewable energy refers to both solar, wind, tidal current,wave and conventional hydro technology. This includes companies that build solar panels or wind turbines, or the raw materials and services that contribute to these technologies It also refers to Energy Storage – companies that develop and use technologies to store large amounts of energy, particularly renewable energies. A good example of this is the fuel cells used in hybrid cars. Also under the renewable energy sector are Biofuels. This group includes companies that use or supply biological resources (like algae, corn or waster wood) to create energy or fuel. Other technologies that are included in the renewable energy group are: Geothermal (companies who use or convert heat to electric energy) and Hydroelectricity (companies who harness water energy to make electricity).\n\nThe Buildings and Efficiency sector refers to companies that manufacture green building materials or energy-efficient services in the world of engineering and architecture. Green building materials include energy-efficient glass, insulation, and lighting among others. Recycling companies and energy conservation companies also fall under this sector.\n\nThe Eco Living sector refers to companies that offer sustainable goods and services for healthy living. This includes organic farming, green pesticides, health care and pharmaceuticals.\n\nGreen investment has significantly grown in the UK and there are now 136 funds listed on the Worldwise Investor fund library under the themes: Agriculture, Carbon, Clean Energy, Forestry, Environmental, Multi-thematic and Water. All of these funds account for around £21.8bn in the UK.\n\n\n"}
{"id": "9375891", "url": "https://en.wikipedia.org/wiki?curid=9375891", "title": "Energy in France", "text": "Energy in France\n\nEnergy in France is the energy and electricity production, consumption and import in France.\n\nThe electricity sector in France is dominated by nuclear power, which accounted for 72.3% of total production in 2016, while renewables and fossil fuels accounted for 17.8% and 8.6%, respectively. France has the largest share of nuclear electricity in the world. The country is also among the world's biggest net exporters of electricity. The French nuclear power sector is almost entirely owned by the French government and the degree of the government subsidy is difficult to ascertain because of a lack of transparency.\n\nÉlectricité de France (EDF) is the main electricity generation and distribution company in France. It was founded on April 8, 1946 as a result of the nationalisation of a number of electricity producers, transporters and distributors by the Communist Minister of Industrial Production Marcel Paul. Until November 19, 2004 it was a government corporation, but it is now a limited-liability corporation under private law (\"société anonyme\"). The French government partially floated shares of the company on the Paris Stock Exchange in November 2005, although it retains almost 85% ownership as of the end of 2007.\n\nEDF held a monopoly in the distribution, but not the production, of electricity in France until 1999, when the first European Union directive to harmonize regulation of electricity markets was implemented.\n\nEDF is one of the world's largest producers of electricity. In 2003, it produced 22% of the European Union's electricity, primarily from nuclear power:\n\nA report was published in 2011 by the World Energy Council in association with Oliver Wyman, entitled \"Policies for the future: 2011 Assessment of country energy and climate policies\", which ranks country performance according to an energy sustainability index. The best performers were Switzerland, Sweden, and France.\n\nPiper Jaffray expected strong growth in France in 2009 and 2010, partly because of an expected decline in the price of solar panels and partly because of subsidies introduced in 2006 making themselves felt. France should be a key driver for solar together with Italy during 2009-2010. Piper Jaffray believes that France would add 500 megawatts of capacity in both 2009 and 2010. France has 50 megawatts of solar power capacity now.\n\nHydroelectric dams in France include Eguzon dam, Étang de Soulcem, and Lac de Vouglans.\n\nIn July 2015, the French parliament passed a comprehensive energy and climate law that includes a mandatory renewable energy target requiring 40% of national electricity production to come from renewable sources by 2030. For context, 19.5% of the country's electricity was generated by renewable energy in 2014 (13.8% hydro, 3.5% wind, 1.2% solar, 1.0% others).\n\n"}
{"id": "2179144", "url": "https://en.wikipedia.org/wiki?curid=2179144", "title": "Glass with embedded metal and sulfides", "text": "Glass with embedded metal and sulfides\n\nGlass with embedded metal and sulfides (GEMS) are tiny spheroids in cosmic dust particles with bulk compositions that are approximately chondritic. They form the building blocks of anhydrous interplanetary dust particles (IDPs) in general, and \"cometary\" IDPs, in particular. Their compositions, mineralogy and petrography appear to have been shaped by exposure to ionizing radiation. Since the exposure occurred prior to the accretion of cometary IDPs, and therefore comets themselves, GEMS are likely either solar nebula or presolar interstellar grains. The properties of GEMS (size, shape, mineralogy) bear a strong resemblance to those of interstellar silicate grains as inferred from astronomical observations.\n"}
{"id": "40918726", "url": "https://en.wikipedia.org/wiki?curid=40918726", "title": "Inside of a Dog", "text": "Inside of a Dog\n\nInside of a Dog: What Dogs See, Smell, and Know is a book written by cognitive scientist, Alexandra Horowitz. Horowitz walks the reader through the cognitive process of dogs in relation to how they perceive their day-to-day activities. The author explains the animal's cognitive abilities, and allows the reader insight into what it might be like to be a dog. The book also contains a brief interview with the author.\n\nAlexandra Horowitz lives in New York with her husband and son. She has her B.A. in philosophy from the University of Pennsylvania and a Ph.D. in cognitive science from the University of California, San Diego. Before her scientific career, Horowitz worked as a lexicographer at Merrian-Webster and served on the staff of The New Yorker. Horowitz teaches psychology at Barnard College, Columbia University.\n\n\"Inside of a Dog\" has been extensively reviewed, was Number 1 on the New York Times' bestseller list and remained on the list for 64 weeks.\n\n"}
{"id": "78172", "url": "https://en.wikipedia.org/wiki?curid=78172", "title": "International Geophysical Year", "text": "International Geophysical Year\n\nThe International Geophysical Year (IGY; ) was an international scientific project that lasted from July 1, 1957, to December 31, 1958. It marked the end of a long period during the Cold War when scientific interchange between East and West had been seriously interrupted. Sixty-seven countries participated in IGY projects, although one notable exception was the mainland People's Republic of China, which was protesting against the participation of the Republic of China (Taiwan). East and West agreed to nominate the Belgian Marcel Nicolet as secretary general of the associated international organization.\n\nThe IGY encompassed eleven Earth sciences: aurora and airglow, cosmic rays, geomagnetism, gravity, ionospheric physics, longitude and latitude determinations (precision mapping), meteorology, oceanography, seismology, and solar activity. The timing of the IGY was particularly suited for studying some of these phenomena, since it covered the peak of solar cycle 19.\n\nBoth the Soviet Union and the U.S. launched artificial satellites for this event; the Soviet Union's \"Sputnik 1\", launched on October 4, 1957, was the first successful artificial satellite. Other significant achievements of the IGY included the discovery of the Van Allen radiation belts by Explorer 1 and the defining of mid-ocean submarine ridges, an important confirmation of plate-tectonic theory. Also detected was the rare occurrence of hard solar corpuscular radiation that could be highly dangerous for manned space flight.\n\nThe origin of the International Geophysical Year can be traced to the International Polar Years held in 1882–1883 and 1932–1933 (and which would be held again in 2007–2009). On 5 April 1950, several top scientists (including Lloyd Berkner, Sydney Chapman, S. Fred Singer, and Harry Vestine), met in James Van Allen's living room and suggested that the time was ripe to have a worldwide Geophysical Year instead of a Polar Year, especially considering recent advances in rocketry, radar, and computing. Berkner and Chapman proposed to the International Council of Scientific Unions that an International Geophysical Year (IGY) be planned for 1957–58, coinciding with an approaching period of maximum solar activity. In 1952, the IGY was announced. Joseph Stalin's death in 1953 opened the way for international collaboration with the Soviet Union. \n\nOn 29 July 1955, James C. Hagerty, president Dwight D. Eisenhower's press secretary, announced that the United States intended to launch \"small Earth circling satellites\" between 1 July 1957 and 31 December 1958 as part of the United States contribution to the International Geophysical Year (IGY). Project Vanguard would be managed by the Naval Research Laboratory and to be based on developing sounding rockets, which had the advantage that they were primarily used for non-military scientific experiments.\n\nFour days later, at the Sixth Congress of International Astronautical Federation in Copenhagen, scientist Leonid I. Sedov spoke to international reporters at the Soviet embassy, and announced his country's intention to launch a satellite as well, in the \"near future\".\n\nTo the surprise of many, the USSR launched Sputnik 1 as the first artificial Earth satellite on October 4, 1957. After several failed Vanguard launches, Wernher von Braun and his team convinced President Dwight D. Eisenhower to use one of their US Army missiles for the Explorer program (there then being no inhibition about using military rockets to get into space). On November 8, 1957, the US Secretary of Defense instructed the US Army to use a modified Jupiter-C rocket to launch a satellite. The US achieved this goal only four months later with Explorer 1, on February 1, 1958, but after Sputnik 2 in November 3, 1957, making Explorer 1 the third artificial Earth satellite. Vanguard 1 became the fourth, launched on March 17, 1958. The Soviet victory in the \"Space Race\" would be followed by considerable political consequences,\none of which was the creation of the US space agency NASA on July 29, 1958.\n\nThe British-American survey of the Atlantic, carried out between September 1954 and July 1959, that discovered full length of the mid-Atlantic ridges (plate tectonics), was a major discovery during the IGY.\n\nAlthough the 1932 Polar Year accomplished many of its goals, it fell short on others because of the advance of World War II. In fact, because of the war, much of the data collected and scientific analyses completed during the 1932 Polar Year were lost forever, something that was particularly troubling to the IGY organizing committee. The committee resolved that \"all observational data shall be available to scientists and scientific institutions in all countries.\" They felt that without the free exchange of data across international borders, there would be no point in having an IGY.\n\nIn April 1957, just three months before the IGY began, scientists representing the various disciplines of the IGY established the World Data Center system. The United States hosted World Data Center \"A\" and the Soviet Union hosted World Data Center \"B.\" World Data Center \"C\" was subdivided among countries in Western Europe, Australia, and Japan. Today, NOAA hosts seven of the fifteen World Data Centers in the United States.\n\nEach World Data Center would eventually archive a complete set of IGY data to deter losses prevalent during the International Polar Year of 1932. Each World Data Center was equipped to handle many different data formats, including computer punch cards and tape—the original computer media. In addition, each host country agreed to abide by the organizing committee’s resolution that there should be a free and open exchange of data among nations. \n\nThe IGY triggered an 18-month year of Antarctic science. The International Council of Scientific Unions, a parent body, broadened the proposals from polar studies to geophysical research. More than 70 existing national scientific organizations then formed IGY committees, and participated in the cooperative effort.\n\nHalley Research Station was founded in 1956, for the IGY, by an expedition from the Royal Society. The bay where the expedition set up their base was named Halley Bay, after the astronomer Edmond Halley.\n\nIn Japan, the Antarctic exploration was planned in 1955 by Monbushō and Science and technology Agency. Japan Maritime Safety Agency offered ice breaker Sōya as the South Pole observation ship. The first Antarctic observation corps commanded by Takeshi Nagata left Japan in 1956, arriving at Antarctica on January 29, 1957. Showa Station was the first Japanese observation base on Antarctica and was set up on same day.\n\nFrance contributed with Dumont d'Urville Station and Charcot Station in Adélie Land. As a forerunner expedition, the ship \"Commandant Charcot\" of the French Navy spent nine months of 1949/50 at the coast of Adelie Land. Ionospheric soundings were performed aboard this ship. The first French station, Port Martin, was completed April 9, 1950, but destroyed by fire the night of January 22 to 23, 1952.\n\nBelgium established the King Baudouin Base in 1958. The expedition was led by Gaston de Gerlache, son of Adrien de Gerlache who had led the 1897–1899 Belgian Antarctic Expedition.\n\nThe Amundsen–Scott South Pole Station was erected as the first permanent structure at the South Pole in January 1957. It survived intact for 53 years, but was slowly buried in the ice (as all structures there eventually sink into the icy crust), until it was demolished in December 2010 for safety reasons.\n\nIce Skate 2 was a floating research station constructed and manned by U.S. scientists. It mapped the bottom of the Arctic Ocean. Zeke Langdon was a meteorologist on the project. Ice Skate 2 was planned to be manned in 6 month shifts. But due to soft ice surfaces for landing some crew members were stationed for much longer. At one point they lost all communications with anyone over their radios for one month except the expedition on the South Pole. At one point the ice sheet broke up and their fuel tanks started floating away from the base. They had to put pans under the plane engines as soon as they landed as any oil spots would go straight through the ice in the intense sunshine. Their only casualty was a man who got too close to the propeller with the oil pan.\n\nNorbert Untersteiner was the project leader for Drifting Station Alpha and in 2008 produced and narrated a documentary about the project for the National Snow and Ice Data Center.\n\nThe participating countries for the IGY included the following: \nIn the end, the IGY was a resounding success, and it led to advancements that live on today. For example, the work of the IGY led directly to the Antarctic Treaty, which called for the use of Antarctica for peaceful purposes and cooperative scientific research. Since then, international cooperation has led to protecting the Antarctic environment, preserving historic sites, and conserving the animals and plants. Today, 41 nations have signed the Treaty and international collaborative research continues.\n\nThe ICSU World Data System (WDS) was created by the 29th General Assembly of the International Council for Science (ICSU) and builds on the 50-year legacy of the former ICSU World Data Centres (WDCs) and former Federation of Astronomical and Geophysical data-analysis Services (FAGS).\n\nThis World Data System, hosts the repositories for data collected during the IGY.\nSeven of the 15 World Data Centers in the United States are co-located at NOAA National Data Centers or at NOAA affiliates. \nThese ICSU Data Centers not only preserve historical data, but also promote research and ongoing data collection.\n\nThe fourth International Polar Year on 2007–2008 focused on climate change and its effects on the polar environment. Sixty countries participated in this effort and it will include studies in the Arctic and Antarctic.\n\n\n\n\n\n"}
{"id": "17248404", "url": "https://en.wikipedia.org/wiki?curid=17248404", "title": "Lampad", "text": "Lampad\n\nThe Lampads or Lampades () are the nymphs of the Underworld in Greek mythology.\n\nCompanions of Hecate, the Greek goddess of witchcraft and crossroads, they were a gift from Zeus for Hecate's loyalty in the Titanomachy. They bear torches and accompany Hecate on her night-time travels and hauntings. Some accounts tell of how the light of the Lampads' torches has the power to drive one to madness.\n\nThe Lampads were probably the daughters of various Underworld gods, Daimones, river gods, or Nyx.\n\nThe Lampads' Roman name is \"\" (\"infernal nymphs\").\n\n\n"}
{"id": "53646845", "url": "https://en.wikipedia.org/wiki?curid=53646845", "title": "Liquid slugging", "text": "Liquid slugging\n\nLiquid slugging is the phenomenon of liquid entering the cylinder of a reciprocating compressor, a common cause of failure. Under normal conditions, the intake and output of a compressor cylinder is entirely vapor or gas, when a liquid accumulates at the suction port liquid slugging can occur. As more of the practically incompressible liquid enters, strain is placed upon the system leading to a variety of failures.\n"}
{"id": "6164372", "url": "https://en.wikipedia.org/wiki?curid=6164372", "title": "List of Araneidae species: A", "text": "List of Araneidae species: A\n\nThis page lists all described species of the spider family Araneidae as of Dec. 20, 2016, that start with the letter A.\n\n\"Acacesia\" \n\n\"Acantharachne\" \n\n\"Acanthepeira\" \n\n\"Acroaspis\" \n\n\"Acrosomoides\" \n\n\"Actinacantha\" \n\n\"Actinosoma\" \n\n\"Aculepeira\" \n\n\"Acusilas\" \n\n\"Aethriscus\" \n\n\"Aethrodiscus\" \n\n\"Aetrocantha\" \n\n\"Afracantha\" \n\n\"Agalenatea\" \n\n\"Alenatea\" \n\n\"Allocyclosa\" \n\n\"Alpaida\" \n\n\"Amazonepeira\" \n\n\"Anepsion\" \n\n\"Arachnura\" \n\n\"Araneus\" \n\n\"Araniella\" \n\n\"Aranoethra\" \n\n\"Argiope\" \n\n\"Arkys\" \n\n\"Artonis\" \n\n\"Aspidolasius\" \n\n\"Augusta\" \n\n\"Austracantha\" \n\n"}
{"id": "3300675", "url": "https://en.wikipedia.org/wiki?curid=3300675", "title": "List of Lepidoptera that feed on dandelions", "text": "List of Lepidoptera that feed on dandelions\n\nDandelions (\"Taraxacum\" species) are used as food plants by the caterpillars of a number of Lepidoptera species, including:\n\n\n"}
{"id": "44305784", "url": "https://en.wikipedia.org/wiki?curid=44305784", "title": "List of Pertusaria species", "text": "List of Pertusaria species\n\n\"Pertusaria\" is a large genus of crustose lichens in the family Pertusariaceae. A 2008 estimate placed 500 species in the genus.\nA B C D E F G H I J K L M N O P Q R S T U V U W X Y Z\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "26520220", "url": "https://en.wikipedia.org/wiki?curid=26520220", "title": "List of ecoregions in Turkey", "text": "List of ecoregions in Turkey\n\nThe following is a list of ecoregions in Turkey, according to the Worldwide Fund for Nature (WWF):\n\n\n\n\n\n"}
{"id": "9132644", "url": "https://en.wikipedia.org/wiki?curid=9132644", "title": "List of foliage plant diseases (Acanthaceae)", "text": "List of foliage plant diseases (Acanthaceae)\n\nThis is a list of diseases of foliage plants belonging to the Acanthaceae.\n\n"}
{"id": "42200098", "url": "https://en.wikipedia.org/wiki?curid=42200098", "title": "List of invasive plant species in Hawaii", "text": "List of invasive plant species in Hawaii\n\nNumerous plants have been introduced to Hawaii, and many of them have become invasive species.\n\nThe following are some of invasive plant species established in Hawaii:\n\nThe following are some of the naturalized trees and shrubs of Hawaii:\n\n"}
{"id": "42199879", "url": "https://en.wikipedia.org/wiki?curid=42199879", "title": "List of invasive plant species in West Virginia", "text": "List of invasive plant species in West Virginia\n\nNumerous plants have been introduced to West Virginia in the United States, and many of them have become invasive species.\n\nThe following are some of invasive plant species established in West Virginia:\n\n\n"}
{"id": "35133583", "url": "https://en.wikipedia.org/wiki?curid=35133583", "title": "List of microorganisms used in food and beverage preparation", "text": "List of microorganisms used in food and beverage preparation\n\nThis is an incomplete list of bacteria and microscopic fungi that are used in preparing food.\n\n"}
{"id": "21770682", "url": "https://en.wikipedia.org/wiki?curid=21770682", "title": "Lists of nebulae", "text": "Lists of nebulae\n\nThe following articles contain lists of nebulae:\n\n"}
{"id": "47278528", "url": "https://en.wikipedia.org/wiki?curid=47278528", "title": "Manu'a Project", "text": "Manu'a Project\n\nThe Manu'a Project is an initiative by the Fanua Foundation, and it is an international environmental and clean energy foundation that focuses on helping Pacific Island Countries and territories to reduce their overall dependency on fossil fuels through project based initiatives. The Manu’a project aims at creating an island of the same name which will be the first fossil fuel free island in the whole world.\n\nThe Fanua Foundation, a non-Profit, formed in American Samoa by US military concerns helping Pacific island countries and territories reduce their overall dependency on fossil fuels. Due to their often extremely remote locations and difficult logistical chains, the Pacific Islands suffer have some of the highest fossil fuel costs and subsequently the highest electricity costs in the world. The Fanua Foundation believes this negatively affects public services, such as health, education, governmental and infrastructure services. The Manu’a Project is intended to bring about cost effective access to clean electricity, cooking technologies, and transportation needs in Pacific for future generations.\n\nThe Manu’a project is being funded by the United States Department of Interior, Office of Insular Affairs. It also received a donation worth $40,000 by the United States Environmental Protection Agency. The United Nations Environmental program (UNEP) has also shown interest and support in the Manu’a project. The project is managed and awarded by the American Samoa Power Authority (ASPA). The Fanua foundation is working to fund and manage the project from the 80% renewable energy point to generate the remaining 20% of power from renewable energy, to convert cars and trucks to hydrogen, donate both hydrogen trucks and electric cars, and convert oven and burners to hydrogen fuel.\n\n\n\n"}
{"id": "13312743", "url": "https://en.wikipedia.org/wiki?curid=13312743", "title": "Mariager Fjord", "text": "Mariager Fjord\n\nWith a length of approximately 35 km (22 statute miles), Mariager Fjord is the longest fjord in Denmark. Mariager Fjord cuts into the Jutland peninsula from the \"Kattegat\" sea and ends at the town of Hobro; other important towns along the fjord are Hadsund and Mariager from which the fjord takes its name. Mariager fjord makes up most of the southern limit of the traditional region of Himmerland.\n\nIn Danish language, any type of inlet in Denmark is called a fjord, even lagoons. Geologically, Mariager Fjord isn't a true fjord, but an inlet of the förde type.\n\nThe width of Mariager Fjord varies from 4½ km to 250 metres (2,8-0,16 miles) and its area is about 47 kms (18 square miles). The depth is up to 30 metres (ca. 100 ft).\n\nAs a result of the 2007 Municipal Reform, the new Mariagerfjord municipality was created around Mariager Fjord by fusion of four former municipalities.\n"}
{"id": "28743603", "url": "https://en.wikipedia.org/wiki?curid=28743603", "title": "Mavis Batey", "text": "Mavis Batey\n\nMavis Lilian Batey, MBE (née Lever; 5 May 1921 – 12 November 2013), was an English code-breaker during World War II. Her work at Bletchley Park was one of the keys to the success of D-Day. She later became a garden historian, who campaigned to save historic parks and gardens, and an author. Batey was awarded the Veitch Memorial Medal in 1985, and made a Member of the Order of the British Empire (MBE) in 1987, in both cases for her work on the conservation of gardens.\n\nMavis Lilian Lever was born on 5 May 1921 in Dulwich to her seamstress mother and postal worker father. She was brought up in Norbury and went to Coloma Convent Girls' School in Croydon. She was studying German at University College, London at the outbreak of World War II, concentrating on the German romantics. She decided to interrupt her studies to assist with the war effort. Originally, she applied to be a nurse, but discovered that her linguistic skills were in high demand.\n\nAt first she was employed by the London Section to check the personal columns of \"The Times\" for coded spy messages. Then, in 1940, she was recruited to work as a codebreaker at Bletchley Park. She worked as an assistant to Dilly Knox, and was closely involved in the decryption effort before the Battle of Matapan. According to \"The Daily Telegraph\", she became so familiar with the styles of individual enemy operators that she could determine that two of them had a girlfriend called Rosa. Batey had developed a successful technique that could be used elsewhere.\n\nAlthough Mavis was just 19, she started working on the Italian Naval Enigma machine, and by late March of 1941 she effectively broke into their framework, deciphering a message which said \"Today's the day minus three\". She and her colleagues worked for three days and nights and discovered that the Italians were intending to assault a Royal Navy convoy transporting supplies from Cairo, Egypt to Greece. The messages they deciphered provided a detailed plan of the Italian assault.\n\nIn December 1941 she broke a message between Belgrade and Berlin that enabled Knox's team to work out the wiring of the Abwehr Enigma, an Enigma machine previously thought to be unbreakable. Later, Batey broke another Abwehr machine, the GGG. This enabled the British to be able to read the Abwehr messages and confirm that the Germans believed the Double-Cross intelligence they were being fed by the double agents who were recruited by Britain as spies.\n\nWhile at Bletchley Park she met Keith Batey, a mathematician and fellow codebreaker whom she married in 1942.\n\nMavis Batey wrote a biography of Dilly Knox: ‘Dilly: The Man Who Broke Enigmas’. The book gives a summary of the government codes and cypher school's codebreaking operation in Bletchley Park. It also describes her code breaking of the Italian Enigma which contributed to the British Navy's success at the Battle of Cape Matapan.\n\nBatey spent some time after 1945 in the Diplomatic Service, and then brought up three children, two daughters and a son. She published a number of books on garden history, as well as some relating to Bletchley Park, and served as President of the Garden History Society, of which she became Secretary in 1971.\n\nShe was awarded the Veitch Memorial Medal in 1985, and made a Member of the Order of the British Empire (MBE) in 1987, in both cases for her work on the conservation of gardens.\n\nBatey, aged 92 and a widow since 2010, died on 12 November 2013.\n\n\n"}
{"id": "8311459", "url": "https://en.wikipedia.org/wiki?curid=8311459", "title": "Melia (consort of Poseidon)", "text": "Melia (consort of Poseidon)\n\nIn Greek mythology, Melia or Melie (Ancient Greek: Μελίη) was a Bithynian nymph, who was, by Poseidon, the mother of Amycus and Mygdon, both kings of the Bebryces. The name Melia perhaps derived from a misreading of a line of Apollonius of Rhodes containing \"Βιθυνὶς Μελίη\", which instead of being read as Melia from Bithynia, might instead be read as Bithynis the Melia, i.e. Bithynis the ash tree nymph.\n\n"}
{"id": "4820663", "url": "https://en.wikipedia.org/wiki?curid=4820663", "title": "Normative mineralogy", "text": "Normative mineralogy\n\nNormative mineralogy is a calculation of the composition of a rock sample that estimates the \"idealised mineralogy\" of a rock according to the principles of geochemistry.\n\nNormative mineral calculations can be achieved via either the CIPW Norm or the Barth-Niggli Norm (also known as the Cation Norm).\n\nNormative calculations are used to produce an idealised mineralogy of a crystallized melt. First, a rock is chemically analysed to determine the elemental constituents. Results of the chemical analysis traditionally are expressed as oxides (e.g., weight percent Mg is expressed as weight percent MgO). The normative mineralogy of the rock then is calculated, based upon assumptions about the order of mineral formation and known phase relationships of rocks and minerals, and using simplified mineral formulas. The calculated mineralogy can be used to assess concepts such as silica saturation of melts.\n\nBecause the normative calculation is essentially a computation, it can be achieved relatively painlessly via computer programs.\n\nThe CIPW Norm was developed in the early 1900s by the petrologists Cross, Iddings, Pirsson and the geochemist Washington. The CIPW normative mineralogy calculation is based on the typical minerals that may be precipitated from an anhydrous melt at low pressure, and simplifies the typical igneous geochemistry seen in nature with the following four constraints:\n\n\nThis is an artificial set of constraints, and therefore the results of the CIPW norm do not reflect the true course of igneous differentiation in nature.\n\nThe primary benefit of calculating a CIPW norm is determining what the ideal mineralogy of an aphanitic or porphyritic igneous rock is. Secondly, the degree of silica saturation of the melt that formed the rock can be assessed in the absence of diagnostic feldspathoid species.\n\nThe silica saturation of a rock varies not only with silica content but the proportion of the various alkalis and metal species within the melt. The silica saturation eutectic plane is thus different for various families of rocks and cannot be easily estimated, hence the requirement to calculate whether the rock is silica saturated or not.\n\nThis is achieved by assigning cations of the major elements within the rock to silica anions in modal proportion, to form solid solution minerals in the idealised mineral assemblage starting with phosphorus for apatite, chlorine and sodium for halite, sulfur and FeO into pyrite, FeO and CrO is allocated for chromite, FeO and equal molar amount of TiO for ilmenite, CaO and CO for calcite, to complete the most common non-silicate minerals. \nFrom the remaining chemical constituents, AlO and KO are allocated with silica for orthoclase; sodium, aluminium and potassium for albite, and so on until either there is no silica left (in which case feldspathoids are calculated) or excess, in which case the rock contains normative quartz.\n\nNormative mineralogy is an \"estimate\" of the mineralogy of the rock. It usually differs from the visually observable mineralogy, at least inasmuch as the types of mineral species, especially amongst the ferromagnesian minerals and feldspars, where it is possible to have many solid solution series of minerals, or minerals with similar Fe and Mg ratios substituting, especially with water (e.g.; amphibole and biotite replacing pyroxene).\n\nHowever, in aphanites, or rocks with phenocrysts clearly out of equilibrium with the groundmass, a normative mineral calculation is often the best to understand the evolution of the rock and its relationship to other igneous rocks in the region.\n\nThe CIPW Norm or Cation Norm is a useful tool for assessing silica saturation or oversaturation; estimations of minerals in a mathematical model are based on many assumptions and the results must be balanced with the observable mineralogy. The following areas create the most errors in calculations;\n\n\nFor this reason it is not advised to utilise a CIPW norm on kimberlites, lamproites, lamprophyres and some silica-undersaturated igneous rocks. In the case of carbonatite, it is improper to use a CIPW norm upon a melt rich in carbonate.\n\nIt is possible to apply the CIPW norm to metamorphosed igneous rocks. The validity of the method holds as true for metamorphosed igneous rocks as any igneous rock, and in this case it is useful in deriving an assumed mineralogy from a rock that may have no remnant protolith mineralogy remaining.\n\n\n\n"}
{"id": "14471451", "url": "https://en.wikipedia.org/wiki?curid=14471451", "title": "Outline of World War II", "text": "Outline of World War II\n\nThe following outline is provided as an overview of and topical guide to World War II:\n\nWorld War II, or the Second World War – global military conflict from 1939 to 1945, which was fought between the Allied powers of the United States, United Kingdom, and Soviet Union against the Axis powers of Germany, Italy, and Japan, with their respective allies. Over 60 million people, the majority of them civilians, were killed, making it the deadliest conflict in human history.\n\n\nCauses of World War II\n\nThe Holocaust\n\nParticipants in World War II\n\n\n\n\n\n\n\n\n\n\n18 airbrone division\n\nTechnology during World War II\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "24032", "url": "https://en.wikipedia.org/wiki?curid=24032", "title": "Positron emission tomography", "text": "Positron emission tomography\n\nPositron-emission tomography (PET) is a nuclear medicine functional imaging technique that is used to observe metabolic processes in the body as an aid to the diagnosis of disease. The system detects pairs of gamma rays emitted indirectly by a positron-emitting radionuclide, most commonly fluorine-18, which is introduced into the body on a biologically active molecule called a radioactive tracer. Three-dimensional images of tracer concentration within the body are then constructed by computer analysis. In modern PET-CT scanners, three-dimensional imaging is often accomplished with the aid of a CT X-ray scan performed on the patient during the same session, in the same machine.\n\nIf the biologically active tracer molecule chosen for PET is fludeoxyglucose (FDG), an analogue of glucose, the concentrations of tracer imaged will indicate tissue metabolic activity as it corresponds to the regional glucose uptake. Use of this tracer to explore the possibility of cancer metastasis (i.e., spreading to other sites) is the most common type of PET scan in standard medical care (representing 90% of current scans). Metabolic trapping of the radioactive glucose molecule allows the PET scan to be utilized. The same tracer may also be used for PET investigation and diagnosis of types of dementia. Less often, other radioactive tracers, usually but not always labeled with fluorine-18, are used to image the tissue concentration of other types of molecules of interest.\n\nOne of the disadvantages of PET scanners is their operating cost.\n\nPET is both a medical and research tool. It is used heavily in clinical oncology (medical imaging of tumours and the search for metastases), and for clinical diagnosis of certain diffuse brain diseases such as those causing various types of dementias. PET is also an important research tool to map normal human brain and heart function, and support drug development.\n\nPET is also used in pre-clinical studies using animals, where it allows repeated investigations into the same subjects. This is particularly valuable in cancer research, as it results in an increase in the statistical quality of the data (subjects can act as their own control) and substantially reduces the numbers of animals required for a given study.\n\nAlternative methods of scanning include x-ray computed tomography (CT), magnetic resonance imaging (MRI) and functional magnetic resonance imaging (fMRI), ultrasound and single-photon emission computed tomography (SPECT).\n\nWhile some imaging scans such as CT and MRI isolate organic anatomic changes in the body, PET and SPECT are capable of detecting areas of molecular biology detail (even prior to anatomic change). PET scanning does this using radiolabelled molecular probes that have different rates of uptake depending on the type and function of tissue involved. Changing of regional blood flow in various anatomic structures (as a measure of the injected positron emitter) can be visualized and relatively quantified with a PET scan.\n\nPET imaging is best performed using a dedicated PET scanner. It is also possible to acquire PET images using a conventional dual-head gamma camera fitted with a coincidence detector. Although the quality of gamma-camera PET is considerably lower and acquisition is slower, this method allows institutions with low demand for PET to provide on-site imaging, instead of referring patients to another centre or relying on a visit by a mobile scanner.\n\nPET scanning with the tracer fluorine-18 (F-18) fluorodeoxyglucose (FDG), called FDG-PET, is widely used in clinical oncology. This tracer is a glucose analog that is taken up by glucose-using cells and phosphorylated by hexokinase (whose mitochondrial form is greatly elevated in rapidly growing malignant tumors). A typical dose of FDG used in an oncological scan has an effective radiation dose of 14 mSv. Because the oxygen atom that is replaced by F-18 to generate FDG is required for the next step in glucose metabolism in all cells, no further reactions occur in FDG. Furthermore, most tissues (with the notable exception of liver and kidneys) cannot remove the phosphate added by hexokinase. This means that FDG is trapped in any cell that takes it up until it decays, since phosphorylated sugars, due to their ionic charge, cannot exit from the cell. This results in intense radiolabeling of tissues with high glucose uptake, such as the normal brain, liver, kidneys, and most cancers. As a result, FDG-PET can be used for diagnosis, staging, and monitoring treatment of cancers, particularly in Hodgkin's lymphoma, non-Hodgkin lymphoma, and lung cancer. \n\nA few other isotopes and radiotracers are slowly being introduced into oncology for specific purposes. For example, C-labelled metomidate (11C-metomidate), has been used to detect tumors of adrenocortical origin. Also, FDOPA PET/CT (or F-18-DOPA PET/CT), in centers which offer it, has proven to be a more sensitive alternative to finding, and also localizing, pheochromocytoma than the MIBG scan.\n\n\nCardiology, atherosclerosis and vascular disease study: In clinical cardiology, FDG-PET can identify so-called \"hibernating myocardium\", but its cost-effectiveness in this role versus SPECT is unclear. FDG-PET imaging of atherosclerosis to detect patients at risk of stroke is also feasible and can help test the efficacy of novel anti-atherosclerosis therapies.\n\nImaging infections with molecular imaging technologies can improve diagnosis and treatment follow-up. PET has been widely used to image bacterial infections clinically by using fluorodeoxyglucose (FDG) to identify the infection-associated inflammatory response.\n\nThree different PET contrast agents have been developed to image bacterial infections in vivo: [F]maltose, [F]maltohexaose and [F]2-fluorodeoxysorbitol (FDS). FDS has also the added benefit of being able to target only Enterobacteriaceae.\n\nPharmacokinetics: In pre-clinical trials, it is possible to radiolabel a new drug and inject it into animals. Such scans are referred to as biodistribution studies. The uptake of the drug, the tissues in which it concentrates, and its eventual elimination, can be monitored far more quickly and cost effectively than the older technique of killing and dissecting the animals to discover the same information. Much more commonly, drug occupancy at a purported site of action can be inferred indirectly by competition studies between unlabeled drug and radiolabeled compounds known apriori to bind with specificity to the site. A single radioligand can be used this way to test many potential drug candidates for the same target. A related technique involves scanning with radioligands that compete with an endogenous (naturally occurring) substance at a given receptor to demonstrate that a drug causes the release of the natural substance. \n\nPET technology for small animal imaging: A miniature PE tomograph has been constructed that is small enough for a fully conscious and mobile rat to wear on its head while walking around. This RatCAP (Rat Conscious Animal PET) allows animals to be scanned without the confounding effects of anesthesia. PET scanners designed specifically for imaging rodents, often referred to as microPET, as well as scanners for small primates, are marketed for academic and pharmaceutical research. The scanners are apparently based on microminiature scintillators and amplified avalanche photodiodes (APDs) through a new system recently invented uses single chip silicon photomultipliers.\n\nIn 2018 the UC Davis School of Veterinary Medicine became the first veterinary center to employ a small clinical PET-scanner as a pet-PET scan, for clinical (rather than research) animal diagnosis. Because of cost as well as the marginal utility of detecting cancer metastases in companion animals (the primary use of this modality), veterinary PET scanning is expected to be rarely available in the immediate future.\n\nMusculoskeletal imaging: PET has been shown to be a feasible technique for studying skeletal muscles during exercises like walking. One of the main advantages of using PET is that it can also provide muscle activation data about deeper lying muscles such as the vastus intermedialis and the gluteus minimus, as compared to other muscle studying techniques like electromyography, which can be used only on superficial muscles (i.e., directly under the skin). A clear disadvantage is that PET provides no timing information about muscle activation because it has to be measured after the exercise is completed. This is due to the time it takes for FDG to accumulate in the activated muscles.\n\nPET scanning is non-invasive, but it does involve exposure to ionizing radiation.\n\n18F-FDG, which is now the standard radiotracer used for PET neuroimaging and cancer patient management, has an effective radiation dose of 14 mSv.\n\nThe amount of radiation in 18F-FDG is similar to the effective dose of spending one year in the American city of Denver, Colorado (12.4 mSv/year). For comparison, radiation dosage for other medical procedures range from 0.02 mSv for a chest x-ray and 6.5–8 mSv for a CT scan of the chest. Average civil aircrews are exposed to 3 mSv/year, and the whole body occupational dose limit for nuclear energy workers in the USA is 50mSv/year. For scale, see Orders of magnitude (radiation).\n\nFor PET-CT scanning, the radiation exposure may be substantial—around 23–26 mSv (for a 70 kg person—dose is likely to be higher for higher body weights).\n\nRadionuclides used in PET scanning are typically isotopes with short half-lives such as carbon-11 (~20 min), nitrogen-13 (~10 min), oxygen-15 (~2 min), fluorine-18 (~110 min), gallium-68 (~67 min), zirconium-89 (~78.41 hours), or rubidium-82(~1.27 min). These radionuclides are incorporated either into compounds normally used by the body such as glucose (or glucose analogues), water, or ammonia, or into molecules that bind to receptors or other sites of drug action. Such labelled compounds are known as radiotracers. PET technology can be used to trace the biologic pathway of any compound in living humans (and many other species as well), provided it can be radiolabeled with a PET isotope. Thus, the specific processes that can be probed with PET are virtually limitless, and radiotracers for new target molecules and processes are continuing to be synthesized; as of this writing there are already dozens in clinical use and hundreds applied in research. At present, by far the most commonly used radiotracer in clinical PET scanning is fluorodeoxyglucose (also called FDG or fludeoxyglucose), an analogue of glucose that is labeled with fluorine-18. This radiotracer is used in essentially all scans for oncology and most scans in neurology, and thus makes up the large majority of all of the radiotracer (> 95%) used in PET and PET-CT scanning.\n\nDue to the short half-lives of most positron-emitting radioisotopes, the radiotracers have traditionally been produced using a cyclotron in close proximity to the PET imaging facility. The half-life of fluorine-18 is long enough that radiotracers labeled with fluorine-18 can be manufactured commercially at offsite locations and shipped to imaging centers. Recently rubidium-82 generators have become commercially available. These contain strontium-82, which decays by electron capture to produce positron-emitting rubidium-82.\n\nTo conduct the scan, a short-lived radioactive tracer isotope is injected into the living subject (usually into blood circulation). Each tracer atom has been chemically incorporated into a biologically active molecule. There is a waiting period while the active molecule becomes concentrated in tissues of interest; then the subject is placed in the imaging scanner. The molecule most commonly used for this purpose is F-18 labeled fluorodeoxyglucose (FDG), a sugar, for which the waiting period is typically an hour. During the scan, a record of tissue concentration is made as the tracer decays.\n\nAs the radioisotope undergoes positron emission decay (also known as positive beta decay), it emits a positron, an antiparticle of the electron with opposite charge. The emitted positron travels in tissue for a short distance (typically less than 1 mm, but dependent on the isotope), during which time it loses kinetic energy, until it decelerates to a point where it can interact with an electron. The encounter annihilates both electron and positron, producing a pair of annihilation (gamma) photons moving in approximately opposite directions. These are detected when they reach a scintillator in the scanning device, creating a burst of light which is detected by photomultiplier tubes or silicon avalanche photodiodes (Si APD). The technique depends on simultaneous or coincident detection of the pair of photons moving in approximately opposite directions (they would be exactly opposite in their center of mass frame, but the scanner has no way to know this, and so has a built-in slight direction-error tolerance). Photons that do not arrive in temporal \"pairs\" (i.e. within a timing-window of a few nanoseconds) are ignored.\n\nThe most significant fraction of electron–positron annihilations results in two 511 keV gamma photons being emitted at almost 180 degrees to each other; hence, it is possible to localize their source along a straight line of coincidence (also called the line of response, or LOR). In practice, the LOR has a non-zero width as the emitted photons are not exactly 180 degrees apart. If the resolving time of the detectors is less than 500 picoseconds rather than about 10 nanoseconds, it is possible to localize the event to a segment of a chord, whose length is determined by the detector timing resolution. As the timing resolution improves, the signal-to-noise ratio (SNR) of the image will improve, requiring fewer events to achieve the same image quality. This technology is not yet common, but it is available on some new systems.\n\nThe raw data collected by a PET scanner are a list of 'coincidence events' representing near-simultaneous detection (typically, within a window of 6 to 12 nanoseconds of each other) of annihilation photons by a pair of detectors. Each coincidence event represents a line in space connecting the two detectors along which the positron emission occurred (i.e., the line of response (LOR)).\n\nAnalytical techniques, much like the reconstruction of computed tomography (CT) and single-photon emission computed tomography (SPECT) data, are commonly used, although the data set collected in PET is much poorer than CT, so reconstruction techniques are more difficult. Coincidence events can be grouped into projection images, called sinograms. The sinograms are sorted by the angle of each view and tilt (for 3D images). The sinogram images are analogous to the projections captured by computed tomography (CT) scanners, and can be reconstructed in a similar way. The statistics of data thereby obtained are much worse than those obtained through transmission tomography. A normal PET data set has millions of counts for the whole acquisition, while the CT can reach a few billion counts. This contributes to PET images appearing \"noisier\" than CT. Two major sources of noise in PET are scatter (a detected pair of photons, at least one of which was deflected from its original path by interaction with matter in the field of view, leading to the pair being assigned to an incorrect LOR) and random events (photons originating from two different annihilation events but incorrectly recorded as a coincidence pair because their arrival at their respective detectors occurred within a coincidence timing window).\n\nIn practice, considerable pre-processing of the data is required—correction for random coincidences, estimation and subtraction of scattered photons, detector dead-time correction (after the detection of a photon, the detector must \"cool down\" again) and detector-sensitivity correction (for both inherent detector sensitivity and changes in sensitivity due to angle of incidence).\n\nFiltered back projection (FBP) has been frequently used to reconstruct images from the projections. This algorithm has the advantage of being simple while having a low requirement for computing resources. Disadvantages are that shot noise in the raw data is prominent in the reconstructed images, and areas of high tracer uptake tend to form streaks across the image. Also, FBP treats the data deterministically—it does not account for the inherent randomness associated with PET data, thus requiring all the pre-reconstruction corrections described above.\n\nStatistical, likelihood-based approaches:\nStatistical, likelihood-based \niterative expectation-maximization algorithms such as the Shepp-Vardi algorithm\nare now the preferred method of reconstruction. These algorithms compute an estimate of the likely distribution of annihilation events that led to the measured data, based on statistical principles. The advantage is a better noise profile and resistance to the streak artifacts common with FBP, but the disadvantage is higher computer resource requirements. A further advantage of statistical image reconstruction techniques is that the physical effects that would need to be pre-corrected for when using an analytical reconstruction algorithm, such as scattered photons, random coincidences, attenuation and detector dead-time, can be incorporated into the likelihood model being used in the reconstruction, allowing for additional noise reduction. Iterative reconstruction has also been shown to result in improvements in the resolution of the reconstructed images, since more sophisticated models of the scanner Physics can be incorporated into the likelihood model than those used by analytical reconstruction methods, allowing for improved quantification of the radioactivity distribution.\n\nResearch has shown that Bayesian methods that involve a Poisson likelihood function and an appropriate prior probability (e.g., a smoothing prior leading to total variation regularization or a Laplacian distribution leading to formula_1-based regularization in a wavelet or other domain), such as via Ulf Grenander's Sieve estimator \nor via Bayes penalty methods\nor via I.J. Good's roughness method\n\n, may yield superior performance to expectation-maximization-based methods which involve a Poisson likelihood function but do not involve such a prior.\n\nAttenuation correction: Quantitative PET Imaging requires attenuation correction. In these systems attenuation correction is based on a transmission scan using Ge rotating rod source.\n\ntransmission scans directly measure attenuation values at 511keV. Attenuation occurs when photons emitted by the radiotracer inside the body are absorbed by intervening tissue between the detector and the emission of the photon. As different LORs must traverse different thicknesses of tissue, the photons are attenuated differentially. The result is that structures deep in the body are reconstructed as having falsely low tracer uptake. Contemporary scanners can estimate attenuation using integrated x-ray CT equipment, in place of earlier equipment that offered a crude form of CT using a gamma ray (positron emitting) source and the PET detectors.\n\nWhile attenuation-corrected images are generally more faithful representations, the correction process is itself susceptible to significant artifacts. As a result, both corrected and uncorrected images are always reconstructed and read together.\n\n2D/3D reconstruction: Early PET scanners had only a single ring of detectors, hence the acquisition of data and subsequent reconstruction was restricted to a single transverse plane. More modern scanners now include multiple rings, essentially forming a cylinder of detectors.\n\nThere are two approaches to reconstructing data from such a scanner: 1) treat each ring as a separate entity, so that only coincidences within a ring are detected, the image from each ring can then be reconstructed individually (2D reconstruction), or 2) allow coincidences to be detected between rings as well as within rings, then reconstruct the entire volume together (3D).\n\n3D techniques have better sensitivity (because more coincidences are detected and used) and therefore less noise, but are more sensitive to the effects of scatter and random coincidences, as well as requiring correspondingly greater computer resources. The advent of sub-nanosecond timing resolution detectors affords better random coincidence rejection, thus favoring 3D image reconstruction.\n\nTime-of-flight (TOF) PET: For modern systems with a higher time resolution (roughly 3 nanoseconds) a technique called \"Time-of-flight\" is used to improve the overall performance. Time-of-flight PET makes use of very fast gamma-ray detectors and data processing system which can more precisely decide the difference in time between the detection of the two photons. Although it is technically impossible to localize the point of origin of the annihilation event exactly (currently within 10 cm) thus image reconstruction is still needed, TOF technique gives a remarkable improvement in image quality, especially signal-to-noise ratio.\n\nPET scans are increasingly read alongside CT or magnetic resonance imaging (MRI) scans, with the combination (called \"co-registration\") giving both anatomic and metabolic information (i.e., what the structure is, and what it is doing biochemically). Because PET imaging is most useful in combination with anatomical imaging, such as CT, modern PET scanners are now available with integrated high-end multi-detector-row CT scanners (so-called \"PET-CT\"). Because the two scans can be performed in immediate sequence during the same session, with the patient not changing position between the two types of scans, the two sets of images are more precisely registered, so that areas of abnormality on the PET imaging can be more perfectly correlated with anatomy on the CT images. This is very useful in showing detailed views of moving organs or structures with higher anatomical variation, which is more common outside the brain.\n\nAt the Jülich Institute of Neurosciences and Biophysics, the world's largest PET-MRI device began operation in April 2009: a 9.4-tesla magnetic resonance tomograph (MRT) combined with a positron emission tomograph (PET). Presently, only the head and brain can be imaged at these high magnetic field strengths.\n\nFor brain imaging, registration of CT, MRI and PET scans may be accomplished without the need for an integrated PET-CT or PET-MRI scanner by using a device known as the N-localizer.\n\nThe minimization of radiation dose to the subject is an attractive feature of the use of short-lived radionuclides. Besides its established role as a diagnostic technique, PET has an expanding role as a method to assess the response to therapy, in particular, cancer therapy, where the risk to the patient from lack of knowledge about disease progress is much greater than the risk from the test radiation.\n\nLimitations to the widespread use of PET arise from the high costs of cyclotrons needed to produce the short-lived radionuclides for PET scanning and the need for specially adapted on-site chemical synthesis apparatus to produce the radiopharmaceuticals after radioisotope preparation. Organic radiotracer molecules that will contain a positron-emitting radioisotope cannot be synthesized first and then the radioisotope prepared within them, because bombardment with a cyclotron to prepare the radioisotope destroys any organic carrier for it. Instead, the isotope must be prepared first, then afterward, the chemistry to prepare any organic radiotracer (such as FDG) accomplished very quickly, in the short time before the isotope decays. Few hospitals and universities are capable of maintaining such systems, and most clinical PET is supported by third-party suppliers of radiotracers that can supply many sites simultaneously. This limitation restricts clinical PET primarily to the use of tracers labelled with fluorine-18, which has a half-life of 110 minutes and can be transported a reasonable distance before use, or to rubidium-82 (used as rubidium-82 chloride) with a half-life of 1.27 minutes, which is created in a portable generator and is used for myocardial perfusion studies. Nevertheless, in recent years a few on-site cyclotrons with integrated shielding and \"hot labs\" (automated chemistry labs that are able to work with radioisotopes) have begun to accompany PET units to remote hospitals. The presence of the small on-site cyclotron promises to expand in the future as the cyclotrons shrink in response to the high cost of isotope transportation to remote PET machines. In recent years the shortage of PET scans has been alleviated in the US, as rollout of radiopharmacies to supply radioisotopes has grown 30%/year.\n\nBecause the half-life of fluorine-18 is about two hours, the prepared dose of a radiopharmaceutical bearing this radionuclide will undergo multiple half-lives of decay during the working day. This necessitates frequent recalibration of the remaining dose (determination of activity per unit volume) and careful planning with respect to patient scheduling.\n\nThe concept of emission and transmission tomography was introduced by David E. Kuhl, Luke Chapman and Roy Edwards in the late 1950s. Their work later led to the design and construction of several tomographic instruments at the University of Pennsylvania. In 1975 tomographic imaging techniques were further developed by Michel Ter-Pogossian, Michael E. Phelps, Edward J. Hoffman and others at Washington University School of Medicine.\n\nWork by Gordon Brownell, Charles Burnham and their associates at the Massachusetts General Hospital beginning in the 1950s contributed significantly to the development of PET technology and included the first demonstration of annihilation radiation for medical imaging. Their innovations, including the use of light pipes and volumetric analysis, have been important in the deployment of PET imaging. In 1961, James Robertson and his associates at Brookhaven National Laboratory built the first single-plane PET scan, nicknamed the \"head-shrinker.\"\n\nOne of the factors most responsible for the acceptance of positron imaging was the development of radiopharmaceuticals. In particular, the development of labeled 2-fluorodeoxy-D-glucose (2FDG) by the Brookhaven group under the direction of Al Wolf and Joanna Fowler was a major factor in expanding the scope of PET imaging. The compound was first administered to two normal human volunteers by Abass Alavi in August 1976 at the University of Pennsylvania. Brain images obtained with an ordinary (non-PET) nuclear scanner demonstrated the concentration of FDG in that organ. Later, the substance was used in dedicated positron tomographic scanners, to yield the modern procedure.\n\nThe logical extension of positron instrumentation was a design using two 2-dimensional arrays. PC-I was the first instrument using this concept and was designed in 1968, completed in 1969 and reported in 1972. The first applications of PC-I in tomographic mode as distinguished from the computed tomographic mode were reported in 1970. It soon became clear to many of those involved in PET development that a circular or cylindrical array of detectors was the logical next step in PET instrumentation. Although many investigators took this approach, James Robertson and Zang-Hee Cho were the first to propose a ring system that has become the prototype of the current shape of PET.\n\nThe PET-CT scanner, attributed to Dr. David Townsend and Dr. Ronald Nutt, was named by TIME Magazine as the medical invention of the year in 2000.\n\nAs of August 2008, Cancer Care Ontario reports that the current average incremental cost to perform a PET scan in the province is Can$1,000–1,200 per scan. This includes\nthe cost of the radiopharmaceutical and a stipend for the physician reading the scan.\n\nIn England, the NHS reference cost (2015-2016) for an adult outpatient PET scan is £798, and £242 for direct access services.\n\nIn Australia, as of July 2018, the Medicare Benefits Schedule Fee for whole body FDG PET ranges from AUD953-999, depending on the indication for the scan.\n\nThe overall performance of PET systems can be evaluated by quality control tools such as the Jaszczak phantom.\n\n\n"}
{"id": "46493925", "url": "https://en.wikipedia.org/wiki?curid=46493925", "title": "Rade Malobabić", "text": "Rade Malobabić\n\nRade Malobabić (d. June 26, 1917) was a Serbian government operative. He was best known for helping the Black Hand in the assassination of Archduke Franz Ferdinand, which initiated World War I.\n\nMalobabić was a Serbian military intelligence operative who was stationed in Austria leading up to World War I. He also worked with the Serbian terrorist group, the Black Hand, which was the group that was responsible for assassinating Archduke Franz Ferdinand.\n\nMalobabić was hired to organize the assassination of the Archduke, and perform tasks such as smuggling weapons and providing information to other operatives that were a part of the killing mission. He played a major role in the first assassination attempt on June 28, 1914, which had failed through another operative’s error. He did not play a direct role in the second, successful attempt on June 28, but he was subsequently linked to it and got arrested. In 1915 Malobabić broke after being tortured and accused Apis of plotting the whole plan.\n\nAfter Malobabić was freed from prison, Apis took him in and gave him a job as his aide. Soon after Apis was tried for the assassination of the Archduke, so was Malobabić. On June 26, 1917 Malobabić and Apis were executed by firing squad., almost to the day 3 years after the Archduke was assassinated.\n"}
{"id": "98132", "url": "https://en.wikipedia.org/wiki?curid=98132", "title": "Radio wave", "text": "Radio wave\n\nRadio waves are a type of electromagnetic radiation with wavelengths in the electromagnetic spectrum longer than infrared light. Radio waves have frequencies as high as 300 gigahertz (GHz) to as low as 30 hertz (Hz). At 300 GHz, the corresponding wavelength is 1 mm, and at 30 Hz is 10,000 km. Like all other electromagnetic waves, radio waves travel at the speed of light. They are generated by electric charges undergoing acceleration, such as time varying electric currents. Naturally occurring radio waves are emitted by lightning and astronomical objects. \n\nRadio waves are generated artificially by transmitters and received by radio receivers, using antennas. Radio waves are very widely used in modern technology for fixed and mobile radio communication, broadcasting, radar and other navigation systems, communications satellites, wireless computer networks and many other applications. Different frequencies of radio waves have different propagation characteristics in the Earth's atmosphere; long waves can diffract around obstacles like mountains and follow the contour of the earth (ground waves), shorter waves can reflect off the ionosphere and return to earth beyond the horizon (skywaves), while much shorter wavelengths bend or diffract very little and travel on a line of sight, so their propagation distances are limited to the visual horizon.\n\nTo prevent interference between different users, the artificial generation and use of radio waves is strictly regulated by law, coordinated by an international body called the International Telecommunications Union (ITU), which defines radio waves as \"electromagnetic waves of frequencies arbitrarily lower than 3 000 GHz, propagated in space without artificial guide\". The radio spectrum is divided into a number of radio bands on the basis of frequency, allocated to different uses.\n\nRadio waves were first predicted by mathematical work done in 1867 by Scottish mathematical physicist James Clerk Maxwell. Maxwell noticed wavelike properties of light and similarities in electrical and magnetic observations. His mathematical theory, now called Maxwell's equations, described light waves and radio waves as waves of electromagnetism that travel in space, radiated by a charged particle as it undergoes acceleration. In 1887, Heinrich Hertz demonstrated the reality of Maxwell's electromagnetic waves by experimentally generating radio waves in his laboratory, showing that they exhibited the same wave properties as light: standing waves, refraction, diffraction, and polarization. Radio waves, originally called \"Hertzian waves\", were first used for communication in the mid 1890s by Guglielmo Marconi, who developed the first practical radio transmitters and receivers. The modern term \"\"radio wave\" replaced the original name \"Hertzian wave\"\" around 1912.\n\nRadio waves in vacuum travel at the speed of light. When passing through a material medium, they are slowed according to that object's permeability and permittivity. Air is thin enough that in the Earth's atmosphere radio waves travel very close to the speed of light.\n\nThe wavelength is the distance from one peak of the wave's electric field (wave's peak/crest) to the next, and is inversely proportional to the frequency of the wave. The distance a radio wave travels in one second, in a vacuum, is which is the wavelength of a 1 hertz radio signal. A 1 megahertz radio signal has a wavelength of .\n\nThe study of radio propagation, how radio waves move in free space and over the surface of the Earth, is vitally important in the design of practical radio systems. Radio waves passing through different environments experience reflection, refraction, polarization, diffraction, and absorption. Different frequencies experience different combinations of these phenomena in the Earth's atmosphere, making certain radio bands more useful for specific purposes than others. Practical radio systems mainly use three different techniques of radio propagation to communicate:\n\nIn radio communication systems, information is carried across space using radio waves. At the sending end, the information to be sent, in the form of a time-varying electrical signal, is applied to a radio transmitter. The information signal can be an audio signal representing sound from a microphone, a video signal representing moving images from a video camera, or a digital signal representing data from a computer. In the transmitter, an electronic oscillator generates an alternating current oscillating at a radio frequency, called the \"carrier\" because it serves to \"carry\" the information through the air. The information signal is used to modulate the carrier, altering some aspect of it, \"piggybacking\" the information on the carrier. The modulated carrier is amplified and applied to an antenna. The oscillating current pushes the electrons in the antenna back and forth, creating oscillating electric and magnetic fields, which radiate the energy away from the antenna as radio waves. The radio waves carry the information to the receiver location.\n\nAt the receiver, the oscillating electric and magnetic fields of the incoming radio wave push the electrons in the receiving antenna back and forth, creating a tiny oscillating voltage which is a weaker replica of the current in the transmitting antenna. This voltage is applied to the radio receiver, which extracts the information signal. The receiver first uses a bandpass filter to separate the desired radio station's radio signal from all the other radio signals picked up by the antenna, then amplifies the signal so it is stronger, then finally extracts the information-bearing modulation signal in a demodulator. The recovered signal is sent to a loudspeaker or earphone to produce sound, or a television display screen to produce a visible image, or other devices. A digital data signal is applied to a computer or microprocessor, which interacts with a human user.\n\nThe radio waves from many transmitters pass through the air simultaneously without interfering with each other. They can be separated in the receiver because each transmitter's radio waves oscillate at a different rate, in other words each transmitter has a different frequency, measured in kilohertz (kHz), megahertz (MHz) or gigahertz (GHz). The bandpass filter in the receiver consists of a tuned circuit which acts like a resonator, similarly to a tuning fork. It has a natural resonant frequency at which it oscillates. The resonant frequency is set equal to the frequency of the desired radio station. The oscillating radio signal from the desired station causes the tuned circuit to oscillate in sympathy, and it passes the signal on to the rest of the receiver. Radio signals at other frequencies are blocked by the tuned circuit and not passed on.\n\nRadio waves are \"nonionizing radiation\", which means they do not have enough energy to separate electrons from atoms or molecules, ionizing them, or break chemical bonds, causing chemical reactions or DNA damage. The main effect of absorption of radio waves by materials is to heat them, similarly to the infrared waves radiated by sources of heat such as a space heater or wood fire. The oscillating electric field of the wave causes polar molecules to vibrate back and forth, increasing the temperature; this is how a microwave oven cooks food. However, unlike infrared waves, which are mainly absorbed at the surface of objects and cause surface heating, radio waves are able to penetrate the surface and deposit their energy inside materials and biological tissues. The depth to which radio waves penetrate decreases with their frequency, and also depends on the material's resistivity and permittivity; it is given by a parameter called the \"skin depth\" of the material, which is the depth within which 63% of the energy is deposited. For example, the 2.45 GHz radio waves (microwaves) in a microwave oven penetrate most foods approximately 2.5 to 3.8 cm (1 to 1.5 inches). Radio waves have been applied to the body for 100 years in the medical therapy of diathermy for deep heating of body tissue, to promote increased blood flow and healing. More recently they have been used to create higher temperatures in hyperthermia treatment, to kill cancer cells. Looking into a source of radio waves at close range, such as the waveguide of a working radio transmitter, can cause damage to the lens of the eye by heating. A strong enough beam of radio waves can penetrate the eye and heat the lens enough to cause cataracts.\n\nSince the heating effect is in principle no different from other sources of heat, most research into possible health hazards of exposure to radio waves has focused on \"nonthermal\" effects; whether radio waves have any effect on tissues besides that caused by heating. Electromagnetic radiation has been classified by the International Agency for Research on Cancer (IARC) as \"Possibly carcinogenic to humans\". The conceivable evidence of cancer risk via Personal exposure to RF-EMF with mobile telephone use was identified. \n\nRadio waves can be shielded against by a conductive metal sheet or screen, an enclosure of sheet or screen is called a Faraday cage. A metal screen shields against radio waves as well as a solid sheet as long as the holes in the screen are smaller than about 1/20 of wavelength of the waves.\n\nSince radio frequency radiation has both an electric and a magnetic component, it is often convenient to express intensity of radiation field in terms of units specific to each component. The unit \"volts per meter\" (V/m) is used for the electric component, and the unit \"amperes per meter\" (A/m) is used for the magnetic component. One can speak of an electromagnetic field, and these units are used to provide information about the levels of electric and magnetic field strength at a measurement location.\n\nAnother commonly used unit for characterizing an RF electromagnetic field is \"power density\". Power density is most accurately used when the point of measurement is far enough away from the RF emitter to be located in what is referred to as the far field zone of the radiation pattern. In closer proximity to the transmitter, i.e., in the \"near field\" zone, the physical relationships between the electric and magnetic components of the field can be complex, and it is best to use the field strength units discussed above. Power density is measured in terms of power per unit area, for example, milliwatts per square centimeter (mW/cm). When speaking of frequencies in the microwave range and higher, power density is usually used to express intensity since exposures that might occur would likely be in the far field zone.\n\n\n"}
{"id": "2696472", "url": "https://en.wikipedia.org/wiki?curid=2696472", "title": "Saint Marcellus' flood", "text": "Saint Marcellus' flood\n\nSaint Marcellus' flood or Grote Mandrenke (Low Saxon: ; \"Great Drowning of Men\") was a massive southwesterly Atlantic gale (also known as a European windstorm) which swept across the British Isles, the Netherlands, northern Germany, and Denmark (including Schleswig/Southern Jutland) around 16 January 1362, causing at minimum 25,000 deaths. The storm tide is also called the \"Second St. Marcellus flood\" because it peaked 17 January, the feast day of St. Marcellus. A previous \"First St. Marcellus flood\" drowned 36,000 people along the coasts of West Friesland and Groningen on 16 January 1219.\n\nAn immense storm tide of the North Sea swept far inland from England and the Netherlands to Denmark and the German coast, breaking up islands, making parts of the mainland into islands, and wiping out entire towns and districts, such as Rungholt, said to have been located on the island of Strand in North Frisia, Ravenser Odd in East Yorkshire and the harbour of Dunwich.\n\nThis storm tide, along with others of like size in the 13th century and 14th century, played a part in the formation of the Zuiderzee, and was characteristic of the unsettled and changeable weather in northern Europe at the beginning of the Little Ice Age.\n\n"}
{"id": "33484392", "url": "https://en.wikipedia.org/wiki?curid=33484392", "title": "Skotina", "text": "Skotina\n\nSkotina () is a rural settlement of the former municipality of East Olympos, itself part of the municipality of Dio-Olympos, in the Pieria regional unit, Central Macedonia, Greece. \n\nThe name is taken due to handmade costumes (woolen fabrics, handmade costumes), which were made in the region that made the cloaks and the sails for sailing ships of the era.\n\nThe village is built on the southeastern edge of the mount Olympus, 32 km from the city of Katerini, and has a view towards the Thermaic Gulf. Ancient places nearby are Herakleion (4 km) and Leivithra (2 km).\n\nThe community of Skotina had a population 949 inhabitants as of 2011. The community consists of the settlements of Skotina, Paralia Skotinas and the nowadays uninhabited Palia Skotina.\n\n\n"}
{"id": "57135949", "url": "https://en.wikipedia.org/wiki?curid=57135949", "title": "Slavic water spirits", "text": "Slavic water spirits\n\nIn Slavic paganism there are a variety of female tutelary spirits associated with water. They have been compared to the Greek \"Nymphs\", and they may be either white (beneficent) or black (maleficent). They may be called Boginka (plural \"Boginky\") literally \"little goddess\", Navia (and Navka or Mavka, pl. \"Navy\", \"Navky\"), Rusalka (pl. \"Rusalky\"), and Vila (plural \"Vily\").\n\nThe Proto-Slavic root *\"navь-\", which forms one of the names for these beings, means \"dead\", as these minor goddesses are conceived as the spirits of dead children or young women. They are represented as half-naked beautiful girls with long hair, but in the South Slavic tradition also as birds who soar in the depths of the skies. They live in waters, woods and steppes, and they giggle, sing, play music and clap their hands. They are so beautiful that they bewitch young men and might bring them to death by drawing them into deep water.\n\n\"Navia\", spelled in various ways in the Slavic languages, refers to the souls of the dead. \"Navka\" and \"Mavka\" (pl. \"Navky\" and \"Mavky\") are variations with the diminutive suffix -\"ka\". They are also known as \"Lalka\" (pl. \"Lalky\"). The Proto-Slavic root *\"navь-\", means \"dead\", \"deceased\" or \"corpse\". The word \"Nav\" is also the name of the underworld, Vyraj, which is presided by the chthonic god Veles.\n\nThe world of the dead is believed to be separated from the world of the living either by a sea or a river located deep underground. In the folk beliefs of Ruthenia, Veles lives in a swamp located at the centre of Nav, sitting on a golden throne at the base of the world tree, and wielding a sword. Symbolically, the Nav is also described as a huge green plain–pasture, onto which Veles guides the souls. The entrance to Nav is guarded by a \"zmey\", a dragon.\n\nAccording to Stanisław Urbańczyk, amongst other scholars, \"Navia\" was a general name for demons arising from the souls of tragic and premature deaths, the killers and the killed, warlocks, and the drowned. They were said to be hostile and unfavourable towards the living, being jealous of life. In Bulgarian folklore there exists the character of twelve Navias who suck the blood out of women giving birth, whereas in the \"Primary Chronicle\" the Navias are presented as a demonic personification of the 1092 plague in Polotsk. According to folk beliefs, Navias may take the form of birds.\n\nAccording to Vladimir Propp, \"Rusalka\" (pl. \"Rusalky\") was an appellation used by the early Slavs for tutelary deities of water who favour fertility, and they were not considered evil entities before the nineteenth century. They came out of the water in spring to transfer life-giving moisture to the fields, thus nurturing the crops.\n\nIn nineteenth-century descriptions, however, the Rusalka became an unquiet, dangerous, unclean spirit (\"Nav\"). According to Dmitry Zelenin, young women, who either committed suicide by drowning due to an unhappy marriage (they might have been jilted by their lovers or abused and harassed by their much older husbands) or who were violently drowned against their will (especially after becoming pregnant with unwanted children), must live out their designated time on earth as Rusalkas. Original Slavic lore suggests that not all Rusalkas were linked with death from water.\n\nThey appear in the form of beautiful girls, with long hair, generally naked but covered with their long tresses, with wreaths of sedge on their heads. They live in groups in crystal palaces at the bottom of rivers, emerging only in springtime; others live in fields and forests. In springtime, they dance and sing along the riverbanks promoting the growth of rye. After the first thunder, they return to their rivers or rise to the skies.\n\nThey were worshipped together with ancestors during the \"Rosalia\" (or \"Rusalye\") festival in spring, originally a Roman festival for offering roses (and other flowers) to gods and ancestors; from the festival derives the term \"Rusalka\" itself. Another time associated with the Rusalkas is the green week (or \"Rusalnaya nedelja\", \"week of the Rusalkas\") in early June; a common feature of this celebration was the ritual banishment or burial of the Rusalka at the end of the week, which remained popular in Russia, Belarus, and Ukraine until the 1930s.\n\nThey were worshipped together with ancestors during the \"Rosalia\" (or \"Rusallia\") festival in spring, originally a Roman festival for offering roses (and other flowers) to gods and ancestors whose practice spread among the South Slavs and from them to other Slavs; from the festival derives the term \"Rusalka\" itself.\n\n\"Vila\" (pl. \"Vily\") are another type of minor goddesses, already identified as Nymphs by the Greek historian Procopius; their name comes from the same root as the name of Veles. They are described as beautiful, eternally young, dressed in white, with eyes flashing like thunders, and provided with wings. They live in the clouds, in mountain woods or in the waters. They are well-disposed towards men, and they are able to turn themselves into horses, wolves, snakes, falcons and swans. The cult of the Vilas was still practised among South Slavs in the early twentieth century, with offerings of fruits and flowers in caves, cakes near wells, and ribbons hanged to the branches of trees.\n\n\n\n"}
{"id": "24663798", "url": "https://en.wikipedia.org/wiki?curid=24663798", "title": "Snow patch", "text": "Snow patch\n\nSnow patch is a geomorphological pattern of snow and firn accumulation which lies on the surface longer time than other seasonal snow cover. There are two types to distinguish; seasonal snow patches and perennial snow patches. Seasonal patches usually melt during the late summer but later than rest of the snow. Perennial snow patches are stable for more than two years and also have bigger influence to surroundings. (Demek, 1987). \n\nSnow patches often start in sheltered places where both thermal and orographical conditions are favourable for the conservation of snow such as small existing depressions, gullies or other concave patterns. Main process that creates these accumulations is called nivation. It is a complex of processes that includes freeze–thaw action (weathering by the alternate freezing and melting of ice), mass movement (the downhill movement of substances under gravity), and erosion by meltwater which is main agent of the surroundings influence. (Berrisford, 1991). \n\nThere is high soil moisture around the snow patch that supports growing of specific vegetation. Snow patch vegetation is very distinctive. It is usually dominated by species that tolerate a shortened growing season and is predominantly herbaceous. With increasing duration of snow persistence, non – vascular plants predominated over vascular plants for example Salicetum herbaceae, Salix herbacea etc. (Wahren, Williams, Papst, 2001)\n\n"}
{"id": "5554551", "url": "https://en.wikipedia.org/wiki?curid=5554551", "title": "Soil plant atmosphere continuum", "text": "Soil plant atmosphere continuum\n\nThe soil-plant-atmosphere continuum (SPAC) is the pathway for water moving from soil through plants to the atmosphere. Continuum in the description highlights the continuous nature of water connection through the pathway. The low water potential of the atmosphere, and relatively higher (i.e. less negative) water potential inside leaves, leads to a diffusion gradient across the stomatal pores of leaves, drawing water out of the leaves as vapour. As water vapour transpires out of the leaf, further water molecules evaporate off the surface of mesophyll cells to replace the lost molecules since water in the air inside leaves is maintained at saturation vapour pressure. Water lost at the surface of cells is replaced by water from the xylem, which due to the cohesion-tension properties of water in the xylem of plants pulls additional water molecules through the xylem from the roots toward the leaf. \n\nThe transport of water along this pathway occurs in components, variously defined among scientific disciplines:\n\nSPAC integrates these components and is defined as a:\n...concept recognising that the field with all its components (soil, plant, animals and the ambient atmosphere taken together) constitutes a physically integrated, dynamic system in which the various flow processes involving energy and matter occur simultaneously and independently like links in the chain. \n\nThis characterises the state of water in different components of the SPAC as expressions of the energy level or water potential of each. Modelling of water transport between components relies on SPAC, as do studies of water potential gradients between segments.\n\n"}
{"id": "3613601", "url": "https://en.wikipedia.org/wiki?curid=3613601", "title": "Thai highlands", "text": "Thai highlands\n\nThe Thai highlands or Hills of northern Thailand is a mountainous natural region in the north of Thailand. Its mountain ranges are part of the system of hills extending through Laos, Burma, and China and linking to the Himalayas, of which they may be considered foothills.\n\nThe highlands in the north of Thailand are characterized by a pattern of generally steep hill ranges, intermontane basins and alluvial gorges. Elevations are generally moderate, little above for the highest summits. There is a wide range of elevations though, with floors ranging between above sea level. \nTowards the Lao border, the divide to the Mekong basin becomes higher with peaks occasionally rising above and streams flowing in narrow steep valleys.\n\nThe climate is typical of tropical mountains with clearly delineated wet and dry seasons. Winter temperatures can be cool with frosts occurring most years at higher elevations, but no snow even on the highest peaks.\n\nThe region of the Thai Highlands encompasses the nine administrative provinces of northern Thailand, based on the six-region system, as well as parts of Tak and Sukhothai Provinces.\n\nSome areas of the highlands are sparsely populated.\n\nExcept for the Daen Lao Range (ทิวเขาแดนลาว) at the far northern edge, all ranges in the north of Thailand are roughly aligned from north to south. They are linked to a wide system of ranges in neighboring Burma and Laos that do not have a specific name for the whole, \"Thai highlands\" being the term generally restricted to the Thai area. Broadly defined, and based on their geological composition, there are two mountainous subsystems in Northern Thailand:\n\nA great part of the highland area is drained by rivers Ping, Wang, Yom and Nan, all tributaries of the Chao Phraya River flowing in a roughly southern direction. The ranges separating the main rivers are generally steep, high and continuous. Towards the east, as well as in the Wang and Yom drainage basins, they are lower. The Pai River in the northwest flows westwards into the Salween and the northeastern part is drained by rivers of the Mekong basin, like the Kok and Ing.\n\nGeologically in the southern subranges of the Shan Hills layers of alluvium are superimposed on hard rock.\nThe ranges closer to Laos consist of Permo-Carboniferous limestone, which makes for a more jagged and steep relief, despite the more moderate height. Most of the Thai highlands are part of the Shan-Thai Terrane, a tectonic plate.\n\nThe natural environment of the hills used to be dense montane rain forest. Swidden agricultural practices and logging have much reduced the old-growth forest areas which have been replaced by secondary forest.\n\nFor centuries the Thai highlands have been inhabited with hill tribes mostly from Chinese or Tibeto-Burman descent, such as the Akha, Yao, Lahu, Khmu, Hmong and Lisu. These human groups immigrated into this relatively empty region fleeing persecution or harsh central rule in their respective environments, as well as seeking new land for their shifting agricultural productions system. For the past decades these groups have been undergoing a process of integration into the Thai mainstream.\n\nOwing to the unrest in Burma, some refugee camps have been established for cross-border refugees in the Thai highlands. Certain Kayah and Karen communities, like the \"long-necked Karen\", are regularly visited by organized tourist groups.\n\nAt higher elevations, above , one of the main crops was opium until the 1990s, when the combined effects of development became evident—from the construction of roads into the remote area, increasingly efficient policing, and opium replacement programs.\n\nYearly wildfires are started by local farmers during the dry season in different areas of northern Thailand. Often speculators also hire people to set forests on fire in order to claim land title deeds for the areas that, post-fire, become \"degraded forest\". The smoke produced by these fires is the main cause of the intense seasonal air pollution in the Thai highlands, also known as the \"northern haze\". Fires also contribute to the floods in the country by denuding forest undergrowth and the dry forest soil leads to lower water intake for the trees to extract when the rains arrive.\n\nPresently large tracts of the mountains are covered with a mixed vegetation resulting from the capacity of the efficient shifting agricultural system being exceeded. As a result, large areas end up becoming dominated by \"Imperata cylindrica\" grass, which is used throughout Thailand as roofing material. Cattle can graze on the grass to an extent, as agricultural science research in the 1970s showed. The longer term environmental care of the region is associated with forestry and in the lower reaches, perennial fruit like peaches and other trees. Some projects for the restoration of forest cover have been undertaken in ecologically degraded areas.\n\n\n"}
{"id": "533410", "url": "https://en.wikipedia.org/wiki?curid=533410", "title": "UoSAT-2", "text": "UoSAT-2\n\nUoSAT-2, which is also known as UO-11 and OSCAR-11, is a British satellite orbiting in Low Earth Orbit. The satellite functions as an amateur radio transmitter (known as an OSCAR) and was built at the University of Surrey. It launched into orbit in March 1984 and remains orbital and active, though unstable with irregular periods of transmission. All of the Analog telemetry channels have failed, making telemetry from OSCAR 11 useless. The satellite was still heard transmitting telemetry in 2015, thirty years after launch.\n\nIt was operated by Surrey Satellite Technology Ltd (SSTL).\n\nThe satellite was the second in the UoSAT series of satellites built by University of Surrey; preceded by UoSAT-1 and followed by UoSAT-3.\n\nThe satellite carries a Digitalker speech synthesiser, magnetometers, a CCD camera, a Geiger-Müller tube, and a microphone to detect the vibrations of micrometeoroid impacts. Like UoSAT-1 it transmits telemetry data on the VHF beacon at 1200 baud, using asynchronous AFSK, though now all analogue telemetry channels have failed; on an FM receiver the audio signal resembles the cassette data format of the contemporary BBC Micro computer. Actually it is a BASICODE signal, but no citation. Slight modulation had also been observed on the S band beacon.\n\nUoSAT-2's solar arrays were bought at a premium compared to those of UoSAT-1, the design having been space tested by its predecessor.\n\nThe British affiliate of AMSAT distributed a library of software for the BBC Micro to track UoSAT-2 and other satellites and analyse telemetry broadcasts. A commercial fixed-frequency receiver, \"Astrid\", was also produced by British firm MM Microwave for the education market, with accompanying BBC Micro software to display raw telemetry frames. For versatility the \"Astrid\" set included a demodulator to load signals through the serial port of any computer.\n\nAccording to a February 2008 status report the satellite had no viable battery backup, operating only from its solar panels, and a watchdog timer on board was suspending activity for up to three weeks following any power anomaly. At the time of the report it was experiencing continuous sunlight for the last time: since the middle of March 2008 there have been eclipses in its orbit which will continue \"permanently\", limiting transmissions to \"a short time, possibly less [than] a single orbit, every 21 days.\" The eclipses are now expected to continue until 2019.\n\nAfter a 21-month gap in observations, UoSAT-2 resumed sending telemetry sometime before 10 December 2009, and is apparently continuing the watchdog-controlled transmission regime, though now on a ten-days-on, ten-days-off schedule. Its condition has not otherwise improved apart from some recovery of battery power, allowing broadcasts to continue into each eclipse.\n\nCurrent observation reports for UoSAT-2 can be viewed and logged at the Oscar Satellite Status Page.\n\nThe satellite was instrumental in providing a communications link, known as \"Nordski Comm\", from the Ski-Trek support teams to the expedition party. The position of the skiers' emergency beacon was calculated daily by Cospas-Sarsat ground stations and relayed to them, and thousands of amateur radio listeners, as a spoken message from the Digitalker on board UoSAT-2. The message could also serve as an emergency channel to the skiers in the event that all other radio links failed.\n\n"}
{"id": "10682125", "url": "https://en.wikipedia.org/wiki?curid=10682125", "title": "Upper and lower tangent arcs", "text": "Upper and lower tangent arcs\n\nTangent arcs are a type of halos, an atmospheric optical phenomenon, which appear above and below the Sun or Moon, tangent to the 22° halo. To produce these arcs, rod-shaped hexagonal ice crystals need to have their long axis aligned horizontally.\n\nThe shape of an upper tangent arc varies with the elevation of the sun; while the sun is low (less than 29–32°) it appears as an arc over the sun forming a sharp angle. As the sun rises, the curved wings of the arc lower towards the 22° halo while gradually becoming longer. As the sun rises over 29–32°, the upper tangent arc unites with the lower tangent arc to form the circumscribed halo.\n\nThe lower tangent arc is rarely observable, appearing under and tangent to a 22° halo centred on the sun. Just like upper tangent arcs, the shape of a lower arc is dependent on the altitude of the sun. As the sun slips over the horizon the lower tangent arc forms a sharp, wing-shaped angle below the sun. As the sun rises above over the horizon, the arc first folds upon itself and then takes the shape of a wide arc. As the sun reaches 29-32° over the horizon, it finally begins to widen and merge with the upper tangent arc to form the circumscribed halo.\n\nSince by definition, the sun elevation must exceed 22° above the horizon, most observations are from elevated observation points such as mountains and planes.\n\nBoth the upper and lower tangent arc form when hexagonal rod-shaped ice crystals in cirrus clouds have their long axis oriented horizontally. Each crystal can have its long axis oriented in a different horizontal direction, and can rotate around the long axis. Such a crystal configuration also produces other halos, including 22° halos and sun dogs; a predominant horizontal orientation is required to produce a crisp upper tangent arc. Like all colored halos, tangent arcs grade from red towards the Sun (i.e., downwards) to blue away from it, because red light is refracted less strongly than blue light.\n\n\n"}
{"id": "19281502", "url": "https://en.wikipedia.org/wiki?curid=19281502", "title": "Van Krevelen diagram", "text": "Van Krevelen diagram\n\nVan Krevelen diagrams are graphical plots developed by\nDirk Willem van Krevelen (chemist and professor of fuel technology at the TU Delft) and used to assess the origin and maturity of kerogen and petroleum. The diagram cross-plots the hydrogen:carbon atomic ratio as a function of the oxygen:carbon atomic ratio.\n\nDifferent types of kerogen have differing potentials to produce oil during maturation. These various types of kerogen can be distinguished on a van Krevelen diagram.\n\n\n"}
{"id": "227630", "url": "https://en.wikipedia.org/wiki?curid=227630", "title": "World economy", "text": "World economy\n\nThe world economy or global economy is the economy of the world, considered as the international exchange of goods and services that is expressed in monetary units of account. In some contexts, the two terms are distinguished: the \"international\" or \"global economy\" being measured separately and distinguished from national economies while the \"world economy\" is simply an aggregate of the separate countries' measurements. Beyond the minimum standard concerning value in production, use and exchange the definitions, representations, models and valuations of the world economy vary widely. It is inseparable from the geography and ecology of Earth.\n\nIt is common to limit questions of the world economy exclusively to human economic activity and the world economy is typically judged in monetary terms, even in cases in which there is no efficient market to help valuate certain goods or services, or in cases in which a lack of independent research or government cooperation makes establishing figures difficult. Typical examples are illegal drugs and other black market goods, which by any standard are a part of the world economy, but for which there is by definition no legal market of any kind.\n\nHowever, even in cases in which there is a clear and efficient market to establish a monetary value, economists do not typically use the current or official exchange rate to translate the monetary units of this market into a single unit for the world economy since exchange rates typically do not closely reflect worldwide value, for example in cases where the volume or price of transactions is closely regulated by the government.\nRather, market valuations in a local currency are typically translated to a single monetary unit using the idea of purchasing power. This is the method used below, which is used for estimating worldwide economic activity in terms of real United States dollars or euros. However, the world economy can be evaluated and expressed in many more ways. It is unclear, for example, how many of the world's 7.62 billion people have most of their economic activity reflected in these valuations.\n\nAccording to Maddison, until the middle of 19th century, global output was dominated by China and India. Waves of Industrial Revolution in Western Europe and Northern America shifted the shares to the Western Hemisphere. As of 2017, the following 15 countries or regions have reached an economy of at least US$2 trillion by GDP in nominal or PPP terms: Brazil, China, India, Germany, France, Indonesia, Italy, Japan, South Korea, Mexico, Russia, Turkey, the United Kingdom, the United States and the European Union.\n\nThe following two tables list the country groups with individual countries designated by the IMF. Members of the G-20 major economies are in bold.\n\nThe following two tables list the 25 largest economies by GDP (nominal), twenty largest economies by GDP (PPP). Members of the G-20 major economies are in bold.\n\nThe following is a list of the twenty largest economies by nominal GDP at peak value as of the specific year according to International Monetary Fund.\n\nThe following is a list of twenty largest economies by GDP based on purchasing power parity at peak value as of the specific year according to the International Monetary Fund and the \"CIA World Factbook\".\n\n\n\n\n\n\n\nTelephones – main lines in use: 843,923,500 (2007)4,263,367,600 (2008)\n\nTransportation infrastructure worldwide includes:\n\n\nTo promote exports, many government agencies publish on the web economic studies by sector and country. Among these agencies include the USCS (US DoC) and FAS (USDA) in the United States, EDC and AAFC in Canada, Ubifrance in France, UKTI in the UK, HKTDC and JETRO in Asia, Austrade and NZTE in Oceania. Through Partnership Agreements, the Federation of International Trade Associations publishes studies from several of these agencies (USCS, FAS, AAFC, UKTI and HKTDC) as well as other non-governmental organizations on its website GlobalTrade.net.\n\n\nRegional economies:\n\nEvents:\n\nLists:\n\n"}
