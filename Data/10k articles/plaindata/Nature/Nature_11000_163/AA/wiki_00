{"id": "8593762", "url": "https://en.wikipedia.org/wiki?curid=8593762", "title": "300B", "text": "300B\n\nIn electronics the 300B is a directly-heated power triode vacuum tube with a four-pin base, introduced in 1938 by Western Electric to amplify telephone signals. It measures 6.4 inches high and 2.4 inches wide, and the anode can dissipate 40 watts. In the 1980s it started to be used increasingly by audiophiles in home audio equipment. The 300B has good linearity, low noise and good reliability; it is often used in single-ended triode (SET) audio amplifiers of about eight watts output; a push-pull pair can output 20 watts.\n\nWestern Electric, now a small, privately owned company in Rossville, Georgia resumed production of the original 300B in 2018 using the original, 1938 manufacturing standards on a modernized assembly line housed at the Rossville Works.\n\n\n\n"}
{"id": "1321415", "url": "https://en.wikipedia.org/wiki?curid=1321415", "title": "99942 Apophis", "text": "99942 Apophis\n\n99942 Apophis (, previously known by its provisional designation ) is a 370 meter diameter near-Earth asteroid that caused a brief period of concern in December 2004 because initial observations indicated a probability of up to 2.7% that it would hit Earth on April 13, 2029. Additional observations provided improved predictions that eliminated the possibility of an impact on Earth or the Moon in 2029. However, until 2006, a possibility remained that during the 2029 close encounter with Earth, Apophis would pass through a gravitational keyhole, a small region no more than about 0.5 mile wide, or 0.8 km that would set up a future impact exactly seven years later on April 13, 2036. This possibility kept it at Level 1 on the Torino impact hazard scale until August 2006, when the probability that Apophis would pass through the keyhole was determined to be very small. By 2008, the keyhole had been determined to be less than 1 km wide. During the short time when it had been of greatest concern, Apophis set the record for highest rating on the Torino scale, reaching level 4 on 27 December 2004.\n\nAs of 2014, the diameter of Apophis is estimated to be approximately . Preliminary observations by Goldstone radar in January 2013 effectively ruled out the possibility of an Earth impact by Apophis in 2036. By May 6, 2013 (April 15, 2013 observation arc), the probability of an impact on April 13, 2036 had been eliminated. Using observations through February 26, 2014, the odds of an impact on April 12, 2068, as calculated by the JPL Sentry risk table are 1 in 150,000. , there were seven asteroids with a more notable cumulative Palermo Technical Impact Hazard Scale than Apophis. On average, one asteroid the size of Apophis (370 metres) can be expected to impact Earth about every 80,000 years.\n\nApophis was discovered on June 19, 2004, by Roy A. Tucker, David J. Tholen, and Fabrizio Bernardi at the Kitt Peak National Observatory. On December 21, 2004, Apophis passed from Earth. Precovery observations from March 15, 2004, were identified on December 27, and an improved orbit solution was computed. Radar astrometry in January 2005 further refined its orbit solution.. The discovery was notable in that it was at a very low solar elongation (56°) and at very long range (1.1 AU). See diagram below:\n\nWhen first discovered, the object received the provisional designation , and news and scientific articles about it referred to it by that name. When its orbit was sufficiently well calculated, it received the permanent number 99942 (on June 24, 2005). Receiving a permanent number made it eligible for naming by its discoverers, and it received the name \"Apophis\" on July 19, 2005. Apophis is the Greek name of an enemy of the Ancient Egyptian sun-god Ra: Apep, the Uncreator, an evil serpent that dwells in the eternal darkness of the Duat and tries to swallow Ra during his nightly passage. Apep is held at bay by Set, the Ancient Egyptian god of storms and the desert.\nDavid J. Tholen and Tucker—two of the co-discoverers of the asteroid—are reportedly fans of the TV series \"Stargate SG-1\". One of the show's persistent villains is an alien named Apophis. He is one of the principal threats to the existence of civilization on Earth through the first few seasons, thus likely why the asteroid was named after him. In the fictional world of the show, the alien's backstory was that he had lived on Earth during ancient times and had posed as a god, thereby giving rise to the myth of the Egyptian god of the same name.\n\nBased upon the observed brightness, Apophis's diameter was initially estimated at ; a more refined estimate based on spectroscopic observations at NASA's Infrared Telescope Facility in Hawaii by Binzel, Rivkin, Bus, and Tokunaga (2005) is . NASA's impact risk page lists the diameter at and lists a mass of 4 kg based on an assumed density of 2.6 g/cm. The mass estimate is more approximate than the diameter estimate, but should be accurate to within a factor of three. Apophis's surface composition probably matches that of LL chondrites.\n\nBased on Goldstone and Arecibo radar images taken in 2012-2013, Brozović et al. have estimated that Apophis is an elongated object 450 × 170 metres in size, and that it is bilobed (possibly a contact binary) with a relatively bright surface albedo of . Its rotation axis has an obliquity of -59° against the ecliptic, which means that Apophis is a retrograde rotator.\n\nDuring the 2029 approach, Apophis's brightness will peak at magnitude 3.1, easily visible to the naked eye if one knows where to look, with a maximum angular speed of 42° per hour. The maximum apparent angular diameter will be ~2 arcseconds, so that it will be barely resolved by ground-based telescopes not equipped with adaptive optics. Due to the small distance, it is likely that tidal forces will alter Apophis's rotation axis. A partial resurfacing of the asteroid is possible, which might change its spectral class from a weathered Sq- to an unweathered Q-type.\n\nAfter the Minor Planet Center confirmed the June discovery of Apophis, an April 13, 2029 close approach was flagged by NASA's automatic Sentry system and NEODyS, a similar automatic program run by the University of Pisa and the University of Valladolid. On that date, it will become as bright as magnitude 3.1 (visible to the naked eye from rural as well as darker suburban areas, visible with binoculars from most locations). The close approach will be visible from Europe, Africa, and western Asia. During the close approach in 2029 Earth will perturb Apophis from an Aten class orbit with a semi-major axis of 0.92 AU to an Apollo class orbit with a semi-major axis of 1.1 AU.\nAfter Sentry and NEODyS announced the possible impact, additional observations decreased the uncertainty in Apophis's trajectory. As they did, the probability of an impact event in 2029 temporarily climbed, peaking at 2.7% (1 in 37) on 27 December 2004. This probability, combined with its size, caused Apophis to be assessed at level 4 on the Torino Scale and 1.10 on the Palermo Technical Impact Hazard Scale, scales scientists use to represent how dangerous a given asteroid is to Earth. These are the highest values for which any object has been rated on either scale. The chance that there would be an impact in 2029 was eliminated by late December 27, 2004 as a result of a precovery image that extended the observation arc back to March 2004. The danger of a 2036 passage was lowered to level 0 on the Torino Scale in August 2006. With a cumulative Palermo Scale rating of −3.2, the risk of impact from Apophis is less than one thousandth the background hazard level.\n\nOn April 13, 2029, Apophis will pass Earth within the orbits of geosynchronous communication satellites, but will come no closer than above Earth's surface. The 2029 pass will be much closer than had first been predicted in 2004 when the observation arc was very short. The pass in late March 2036 will be no closer than about —and will most likely miss Earth by about .\n\nIn July 2005, former Apollo astronaut Rusty Schweickart, as chairman of the B612 Foundation, formally asked NASA to investigate the possibility that the asteroid's post-2029 orbit could be in orbital resonance with Earth, which would increase the probability of future impacts. Schweickart also asked NASA to investigate whether a transponder should be placed on the asteroid to enable more accurate tracking of how its orbit is affected by the Yarkovsky effect. On January 31, 2011, astronomers took the first new images of Apophis in more than 3 years.\n\nThe close approach in 2029 will substantially alter the object's orbit, prompting Jon Giorgini of JPL to say: \"If we get radar ranging in 2013 [the next good opportunity], we should be able to predict the location of out to at least 2070.\" Apophis passed within of Earth in 2013, allowing astronomers to refine the trajectory for future close passes. Just after the closest approach on 9 January 2013, the asteroid peaked at an apparent magnitude  of about 15.7. Goldstone observed Apophis during that approach from January 3 through January 17. The Arecibo Observatory observed Apophis once it entered Arecibo's declination window after February 13, 2013.\n\nA NASA assessment as of 21 February 2013 that does not use the 2013 radar measurements gave an impact probability of 2.3 in a million for 2068. As of 6 May 2013, using observations through April 15, 2013, the odds of an impact on 12 April 2068 as calculated by the JPL Sentry risk table had increased to 3.9 in a million (1 in 256,000).\n\nAs of April 2018, Apophis has not been observed since 2015, mostly because its orbit has put it very near the Sun from the perspective of Earth. It has not been further than 60 degrees from the Sun at any point since April 2014, and will remain so until December 2019. With the most recent 2015 observations, the 12 April 2068 impact is now 6.7 in a million (1 in 150,000), and the asteroid has a cumulative 9 in a million (1 in 110,000) chance of impacting Earth before 2106.\n\nThe Sentry Risk Table estimates that Apophis would make atmospheric entry with 750 megatons of kinetic energy. The impacts that created Meteor Crater or the Tunguska event are estimated to be in the 3–10 megaton range. The biggest hydrogen bomb ever exploded, the Tsar Bomba, was around 57 megatons while the 1883 eruption of Krakatoa was the equivalent of roughly 200 megatons. In comparison, the Chicxulub impact has been estimated to have released about as much energy as 100,000,000 megatons (100 teratons).\n\nThe exact effects of any impact would vary based on the asteroid's composition, and the location and angle of impact. Any impact would be extremely detrimental to an area of thousands of square kilometres, but would be unlikely to have long-lasting global effects, such as the initiation of an impact winter. Assuming Apophis is a stony asteroid, if it were to impact into sedimentary rock, Apophis would create a impact crater.\n\nIn 2008, the B612 Foundation made estimates of Apophis's path if a 2036 Earth impact were to occur, as part of an effort to develop viable deflection strategies. The result was a narrow corridor a few kilometres wide, called the \"path of risk\", extending across southern Russia, across the north Pacific (relatively close to the coastlines of California and Mexico), then right between Nicaragua and Costa Rica, crossing northern Colombia and Venezuela, ending in the Atlantic, just before reaching Africa. Using the computer simulation tool NEOSim, it was estimated that the hypothetical impact of Apophis in countries such as Colombia and Venezuela, which were in the path of risk, could have more than 10 million casualties. However, the exact location of the impact would be known weeks or even months in advance, allowing any nearby inhabited areas to be completely evacuated and significantly decreasing the potential loss of life and property. A deep-water impact in the Atlantic or Pacific oceans would produce an incoherent short-range tsunami with a potential destructive radius (inundation height of >2 m) of roughly for most of North America, Brazil and Africa, for Japan and for some areas in Hawaii.\n\nIn 2007, The Planetary Society, a California-based space advocacy group, organized a $50,000 competition to design an unmanned space probe that would 'shadow' Apophis for almost a year, taking measurements that would \"determine whether it will impact Earth, thus helping governments decide whether to mount a deflection mission to alter its orbit\". The society received 37 entries from 20 countries on 6 continents.\n\nThe commercial competition was won by a design called 'Foresight' created by SpaceWorks Enterprises, Inc. SpaceWorks proposed a simple orbiter with only two instruments and a radio beacon at a cost of ~140 million USD, launched aboard a Minotaur IV between 2012 and 2014, to arrive at Apophis five to ten months later. It would then rendezvous with, observe, and track the asteroid. Foresight would orbit the asteroid to gather data with a multi-spectral imager for one month. It would then leave orbit and fly in formation with Apophis around the Sun at a range of two kilometres (1.2 miles). The spacecraft would use laser ranging to the asteroid and radio tracking from Earth for ten months to accurately determine the asteroid's orbit and how it might change.\n\nPharos, the winning student entry, would be an orbiter with four science instruments (a multi-spectral imager, near-infrared spectrometer, laser rangefinder, and magnetometer) that would rendezvous with and track Apophis. Earth-based tracking of the spacecraft would then allow precise tracking of the asteroid. The Pharos spacecraft would also carry four instrumented probes that it would launch individually over the course of two weeks. Accelerometers and temperature sensors on the probes would measure the seismic effects of successive probe impacts, a creative way to explore the interior structure and dynamics of the asteroid.\n\nSecond place, for $10,000, went to a European team led by Deimos Space S.L. of Madrid, Spain, in cooperation with EADS Astrium, Friedrichshafen, Germany; University of Stuttgart, Germany; and Università di Pisa, Italy. Juan L. Cano was principal investigator.\n\nAnother European team took home $5,000 for third place. Their team lead was EADS Astrium Ltd, United Kingdom, in conjunction with EADS Astrium SAS, France; IASF-Roma, INAF, Rome, Italy; Open University, UK; Rheinisches Institut für Umweltforschung, Germany; Royal Observatory of Belgium; and Telespazio, Italy. The principal investigator was Paolo D'Arrigo.\n\nTwo teams tied for second place in the Student Category: Monash University, Clayton Campus, Australia, with Dilani Kahawala as principal investigator; and University of Michigan, with Jeremy Hollander as principal investigator. Each second place team won $2,000. A team from Hong Kong Polytechnic University and Hong Kong University of Science and Technology, under the leadership of Peter Weiss, received an honorable mention and $1,000 for the most innovative student proposal.\n\nChina plans an exploration fly-by mission to Apophis after 2020 when the asteroid comes to within a distance of 30,000 kilometers of Earth. The distance, a hair's breadth in astronomical terms, is within the orbit of the moon, and even closer than some man-made satellites. It will be the closest asteroid of its size in recorded history. This fly by mission to Apophis is part of an asteroid exploration mission planned after China's Mars mission in 2020 currently in development, according to Ji Jianghui, a researcher at the Purple Mountain Observatory of the Chinese Academy of Sciences and a member of the expert committee for scientific goal argumentation of deep space exploration in China. The whole mission will include exploration and close study of three asteroids by sending a probe to fly side by side with Apophis for a period to conduct close observation, and land on the asteroid 1996 FG3 to conduct in situ sampling analysis on the surface. The probe is also expected to conduct a fly-by of a third asteroid to be determined at a later time.\nThe whole mission would last around six years, said Ji.\n\nApophis is one of two asteroids that were considered by the European Space Agency as the target of its Don Quijote mission concept to study the effects of impacting an asteroid.\n\nStudies by NASA, ESA, and various research groups in addition to the Planetary Society contest teams, have described a number of proposals for deflecting Apophis or similar objects, including gravitational tractor, kinetic impact, and nuclear bomb methods.\n\nOn December 30, 2009, Anatoly Perminov, the director of the Russian Federal Space Agency, said in an interview that Roscosmos will also study designs for a possible deflection mission to Apophis.\n\nOn August 16, 2011, researchers at China's Tsinghua University proposed launching a mission to knock Apophis onto a safer course using an impactor spacecraft in a retrograde orbit, steered and powered by a solar sail. Instead of moving the asteroid on its potential resonant return to Earth, Shengping Gong and his team believe the secret is shifting the asteroid away from entering the gravitational keyhole in the first place.\n\nOn February 15, 2016, Sabit Saitgarayev, of the Makeyev Rocket Design Bureau, announced intentions to use Russian ICBMs to target relatively small near-Earth objects. Although the report stated that likely targets would be between the 20 to 50 metres in size, it was also stated that 99942 Apophis would be an object subject to tests by the program.\n\n\n\nRisk assessment\n\nNASA\n"}
{"id": "1479362", "url": "https://en.wikipedia.org/wiki?curid=1479362", "title": "Aquarius Reef Base", "text": "Aquarius Reef Base\n\nThe Aquarius Reef Base is an underwater habitat located 5.4 miles (9 kilometers) off Key Largo in the Florida Keys National Marine Sanctuary. It is deployed on the ocean floor 62 feet (19 meters) below the surface and next to a deep coral reef named Conch Reef.\n\nAquarius is one of three undersea laboratories in the world dedicated to science and education. Two additional undersea facilities, also located in Key Largo, FL are owned and operated by Marine Resources Development Foundation. Aquarius was owned by the National Oceanic and Atmospheric Administration (NOAA) and operated by the University of North Carolina–Wilmington until 2013 when Florida International University assumed operational control.\n\nFlorida International University (FIU) took ownership of Aquarius in October 2014. As part of the FIU Marine Education and Research Initiative, the Medina Aquarius Program is dedicated to the study and preservation of marine ecosystems worldwide and is enhancing the scope and impact of FIU on research, educational outreach, technology development, and professional training. At the heart of the program is the Aquarius Reef Base.\n\nAquarius, designed by Perry Submarine Builders of Florida and constructed by Victoria Machine Works, was built in Victoria, Texas, in 1986. Its original name was \"the George F. Bond\", who was the father of Sealab in particular and saturation diving in general. Underwater operations were first planned for Catalina Island, California, but were moved to the U.S. Virgin Islands. Following Hurricane Hugo in 1989, Aquarius was taken to Wilmington, NC for repairs and refurbishment and was redeployed in the Florida Keys in 1993. Aquarius is located under of water at the base of a coral reef within the Florida Keys National Marine Sanctuary, an ideal site for studying the health of sensitive coral reefs.\n\nThe laboratory is most often used by marine biologists for whom Aquarius acts as home base as they study the coral reef, the fish and aquatic plants that live nearby and the composition of the surrounding seawater. Aquarius houses sophisticated lab equipment and computers, enabling scientists to perform research and process samples without leaving their underwater facilities.\n\nThe habitat accommodates four scientists and two technicians for missions averaging ten days. Scientists on the Aquarius are often called \"Aquanauts\" (as they live underwater at depth pressure for a period equal to or greater than 24 continuous hours without returning to the surface). A technique known as saturation diving allows the aquanauts to live and work underwater for days or weeks at a time. After twenty four hours underwater at any depth, the human body becomes saturated with dissolved gas. With saturation diving, divers can accurately predict exactly how much time they need to decompress before returning to the surface. This information limits the risk of decompression sickness. By living in the Aquarius habitat and working at the same depth on the ocean floor, Aquarius aquanauts are able to remain underwater for the duration of their mission. \nIn addition, because Aquarius allows saturation diving, dives from the habitat can last for up to nine hours at a time; by comparison, surface dives usually last between one and two hours. These long dive times allow for observation that would not otherwise be possible. Way stations on the reef outside Aquarius allow aquanauts to refill their scuba tanks during dives.\n\nAquarius consists of three compartments. Access to the water is made via the 'wet porch', a chamber equipped with a moon pool, which keeps the air pressure inside the wet porch the same as the water pressure at that depth ('ambient pressure'), about 2.6 atmospheres, through hydrostatic equilibrium. The main compartment is strong enough, like a submarine, to maintain normal atmospheric pressure, and can also be pressurized to ambient pressure, and is usually held at a pressure in between. The smallest compartment, the Entry Lock, is between the other two and functions as an airlock in which personnel wait while pressure is adjusted to match either the wet porch or the main compartment.\nThis design enables personnel to return to the surface without the need for a decompression chamber when they get there. Personnel stay inside the main compartment for 17 hours before ascending as the pressure is slowly reduced, so that they do not suffer decompression sickness after the ascent.\n\nSeveral missions on the Aquarius have been canceled due to hurricane activity. During Hurricane Gordon in 1994, a crew of scientists and divers had to evacuate Aquarius and climb up a rescue line to the surface in 15-foot seas after one of the habitat's generators caught fire. In 1998, Hurricane Georges nearly destroyed Aquarius, breaking a joint in one of its legs and moving two 8000-pound weights on the wet porch nearly off the structure. Both Hurricane Georges and Hurricane Mitch, later in 1998, also destroyed way stations outside Aquarius used to refill aquanauts' scuba tanks. In 2005, Hurricane Rita broke two of the habitat's seabed anchors and moved one end of Aquarius by twelve feet. In 2017 Hurricane Irma ripped the habitat's 94,000 pound life support buoy from its moorings and blew it 14 miles away to the Lignum Vitae Channel, as well as damaging the underwater living quarters and 'wet porch' area. , no scientists or staff members had been injured at Aquarius due to storms \n\nSince 2001, NASA has used Aquarius for its NEEMO (NASA Extreme Environment Mission Operations) analog missions, sending groups of astronauts to simulate human spaceflight space exploration missions. Much like space, the undersea world is a hostile, alien place for humans to live. NEEMO crew members experience some of the same challenges there that they would on a distant asteroid, planet or moon. During NEEMO missions in Aquarius, the aquanauts are able to simulate living on a spacecraft and test spacewalk techniques for future space missions. The underwater condition has the additional benefit of allowing NASA to \"weight\" the aquanauts to simulate different gravity environments.\n\nA diver named Dewey Smith died during a dive from Aquarius in May 2009. A subsequent investigation determined that Smith's death was caused by a combination of factors, including the failure of the electronic functions of his Inspiration closed circuit rebreather due to hydrodynamic forces from a hydraulic impact hammer being used nearby.\n\nDue to budget cuts, NOAA ceased funding Aquarius after September 2012, with no further missions scheduled after a July 2012 mission that included pioneering female diver Sylvia Earle in its aquanaut crew. The University of North Carolina Wilmington was also unable to provide funding to continue operations. The Aquarius Foundation was set up in an attempt to keep Aquarius functioning. In a two-week series the daily cartoon strip \"Sherman's Lagoon\" featured the potential closing of the Aquarius facility in the week starting September 10, 2012, and continued with a cameo appearance of Sylvia Earle in the week starting September 17, 2012, to discuss the importance of Aquarius. In January 2013, a proposal to keep Aquarius running under Florida International University administration was accepted.\n\nFrom June 1 to July 2, 2014, Fabien Cousteau and his crew spent 31 days living and working in Aquarius in tribute to Jacques Cousteau's 30-day underwater expedition in 1963. Cousteau estimated the team collected the equivalent of two years' worth of surface diving data during the mission, enough for ten scientific papers.\n\n"}
{"id": "14312506", "url": "https://en.wikipedia.org/wiki?curid=14312506", "title": "Barotse Floodplain", "text": "Barotse Floodplain\n\nThe Barotse Floodplain also known as the Bulozi Plain, Lyondo or the Zambezi Floodplain is one of Africa's great wetlands, on the Zambezi River in the Western Province of Zambia. It is a designated Ramsar site, regarded as being of high conservation value.\n\nThe name recognises the floodplain as spawning the culture and way of life of the Lozi people, \"Rotse\" being a variant of \"Lozi\", and \"Ba\" meaning \"people\". They became a powerful kingdom in Central/Southern Africa under their King or Litunga, Lewanika, whose realm extended up to 300 km from the plain and was called Barotseland.\n\nThe region is a flat plateau at an elevation of about 1000 m tilting very slightly to the south. The Zambezi and its headwaters rise on the higher ground to the north, which enjoys good rainfall (1400 mm annually) in a rainy season from October to May. A flood moves down the river reaching a flat region, formed from Kalahari sands, about five hundred kilometres across. To the south around the Ngonye Falls harder rock is found at the surface and has resisted the river's tendency to cut a channel down into it, and so acts a bit like a dam. Behind it the floodplain has formed. Below the falls the river falls nearly twice as fast as it does on the plain, and flows more swiftly in a narrower valley less prone to flooding.\n\nThe floodplain stretches from the Zambezi's confluence with the Kabompo and Lungwebungu Rivers in the north, to a point about 230 km south, above the Ngonye falls and south of Senanga. Along most of its length its width is over 30 km, reaching 50 km at the widest, just north of Mongu, principal town of the plain, situated at its edge. The main body of the plain covers about 5500 km²,\nbut the maximum flooded area is 10 750 km² when the floodplains of several tributaries are taken into account, such as the Luena Flats. The Barotse Floodplain is the second largest wetland in Zambia after the Lake Bangweulu system, which differs in having a large permanent lake and swamps, and a much smaller area which dries out annually.\n\nThe satellite photo was taken in April 2004 at the peak of the flood, Note that the northern part of the plain, near Lukulu, is less flooded, the land there varies a bit more in height and the water tends to keep to the many river channels.\n\nThe peak of the flood occurs on the floodplain about 3 months after the peak of the rainy season in January–February. The flood usually peaks in April, and recedes in May to July, when grasses quickly grow on the exposed plain. At the river's lowest water in November the floodplain still contains about 537 km² of lagoons, swamps and channels. The flood leaves behind a fertile grey to black soil overlaying the Kalahari sands, enriched by silt deposited by the flood as well as humus from vegetation killed by the initial flood, and from decaying aquatic plants left to dry out in the mud. It provides a good soil, but in the late dry season it bakes hard in the heat of the sun.\n\nAs the floods recede, water is left behind in lagoons, swamps, and oxbow lakes.\n\nThe floodplain is in the Zambezian flooded grasslands ecoregion, and is bordered by slightly higher sandy ground on which grow dry grasslands (Western Zambezian grasslands) with woodland savanna (Zambezian Baikiaea woodlands) to the east and south, and patches of evergreen forest (Cryptosepalum dry forests) in the north and east.\n\nThe flood provides aquatic habitats for fish such as tigerfish and bream, crocodiles, hippopotamus, waterbirds, fish-eating birds, and lechwe, the wading antelope. After the flood, the plain is a habitat for grazing animals such as wildebeest, zebra, tsessebe and small antelope such oribi and steenbok, and their predators.\nThese herbivores have been displaced in most areas by the cattle grazed by the Lozi, but they have provided a large game reserve on the dry grassland to the west, the Liuwa Plain National Park, once the Litunga's hunting grounds, established as a game reserve by Lewanika in the 19th century. In addition the entire western of the Zambezi within the country is a Game Management Area.\n\nAbout 250,000 people live on the plain with a similar number of cattle, migrating to grasslands at the edge of the floodplain when the flood arrives. The floodplain is one of the most productive areas for raising cattle in the country.\n\nThe Lozi also catch fish, eating about five times as much as the national average. At the height of the flood they use fish traps and spears for fishing, and they use gill nets in the lagoons left behind by the falling flood. Fish spawn just before the flood, the first floodwaters are naturally hypoxic (low in oxygen) which kills most fish, while eggs survive.\n\nThe Lozi cultivate crops on the floodplain such as maize, rice, sweet potato, and sugar cane.\n\nNovember to January are lean months.\nStored produce from the previous growing season is almost used up and in any case would need to be transported during the migration, while the new season's crops and grasses are not yet productive, and at the same time fishing stops for the spawning season. Hunting and trapping animals, which might have filled the gap, is no longer available to most people, and trapping waterbirds is one of the few alternatives to buying flour.\n\nThe floodplain determines and dominates the way of life, economy, society and culture of the Lozi, who are skilled boat-builders, paddlers and swimmers.\nThe annual migration with the flood is celebrated in the Kuomboka ceremony held at Mongu, capital of Barotseland and its successor, the Western Province.\n\nIn the occasional very wet year such as 2005, lives and property are lost in floods on the Barotse Plain. More often, however, it is a very good example of the principle that natural annual flooding by rivers is valuable and productive for wildlife and human populations, while damming rivers to control floods, as has happened with the Kafue Flats, is potentially damaging to the environment.\n\nDevelopment on the plain has been restricted until now to— \n\nTight control over access to the floodplain by the Litunga and the homogeneity of the indigenous people have slowed commercial development by outsiders. \nA new project, the Mongu-Kalabo road, will have more far-reaching consequences. Around 2002 construction started of a 46-kilometre causeway across the centre of the floodplain to take a paved highway from Mongu to Kalabo, via the ferry across the main river channel at Sandaula, which would then be replaced by a 500-metre bridge. Originally intended to be completed in 2006, it has been delayed by the difficulty of building on the floodplain. There is no rock in the region, and the causeway has been built from sand and gravel scooped out from shallow depressions next to it. Higher than usual floods washed away large sections. The contractor was a Kuwait-based company which acknowledged that conditions were difficult. The \"Times of Zambia\" reported that the company underestimated the floodplain environment, and abandoned the contract. Subsequent modifications have been made to raise the road height and to increase the number and size of culverts, and this in turn led to funding problems. The road has been completed in 2016. A Chinese contractor finished the 286.9 million US dollars project. The road stretches 34 Kilometres in the Baroste plains with 26 bridges across it. No information is available on the impact the structure will have on flood flow patterns, river channels, silt deposition, or on fish and wildlife movements.\n\n\n"}
{"id": "35944333", "url": "https://en.wikipedia.org/wiki?curid=35944333", "title": "Bayhead", "text": "Bayhead\n\nBayhead or baygall is a swamp habitat where bay laurels predominate. The bayhead swamp is also known as a Forested Seep. Many bayhead swamps can be found in margins of creeks with little or no creek banks. Also found in deep depressional areas in flatwoods. The swamps can be described as \"distinct wetland communities in the Natural Communities of Louisiana\". Most bayhead swamps are semi-permanently saturated, or flooded. Forested seeps are typically mixed with pine-hardwood forests, mostly on hill sides. Some herbs can be found in this area such as various Sphagnum.\n\nThe characteristics of a bayhead swamp are mostly broad, shallow drains, which can be found near margins of creeks with little or no creek banks. Most are also near closed to nearly closed canopies.\n\nThe bayhead swamps has around 20 different animal species which are considered to be endangered.\n"}
{"id": "11661976", "url": "https://en.wikipedia.org/wiki?curid=11661976", "title": "Bermuda Atlantic Time-series Study", "text": "Bermuda Atlantic Time-series Study\n\nThe Bermuda Atlantic Time-series Study (BATS) is a long-term oceanographic study by the Bermuda Institute of Ocean Sciences (BIOS). Based on regular (monthly or better) research cruises, it samples an area of the western Atlantic Ocean nominally at the coordinates . The cruise programme routinely samples physical properties such as ocean temperature and salinity, but focuses on variables of biological or biogeochemical interest including: nutrients (nitrate, nitrite, phosphate and silicic acid), dissolved inorganic carbon, oxygen, HPLC of pigments, primary production and sediment trap flux. The BATS cruises began in 1988 but are supplemented by biweekly Hydrostation \"S\" cruises to a neighbouring location () that began in 1954. The data collected by these cruises are available online.\n\nBetween 1998 and 2013, research conducted at BATS has generated over 450 peer-reviewed articles. Among the findings are measurements showing the gradual acidification of the surface ocean, where surface water pH, carbonate ion concentration, and the saturation state for calcium carbonate minerals, such as aragonite, have all decreased since 1998. Additionally, studies at BATS have shown changes in the Revelle factor, suggesting that the capacity of North Atlantic Ocean surface waters to absorb carbon dioxide has diminished, even as seawater pCO has kept pace with increasing atmospheric pCO.\n\n\n"}
{"id": "33327002", "url": "https://en.wikipedia.org/wiki?curid=33327002", "title": "Cabbeling", "text": "Cabbeling\n\nCabbeling is when two separate water parcels mix to form a third which sinks below both parents. The combined water parcel is denser than the original two water parcels.\n\nThe two parent water parcels may have the same density, but they have different properties; for instance, different salinities and temperatures. Seawater almost always gets more dense if it gets either slightly colder or slightly saltier. But medium-warm, medium-salty water can be denser than both fresher, colder water and saltier, warmer water; in other words, the equation of state for seawater is monotonic, but non-linear. See diagram.\n\nCabbeling may also occur in fresh water, since pure water is densest at about 4 °C (39 °F). A mixture of 1 °C water and 6 °C water, for instance, might have a temperature of 4 °C, making it denser than either parent. Ice is also less dense than water, so although ice floats in warm water, meltwater sinks in warm water.\n\nThe densification of the new mixed water parcel is a result of a slight contraction upon mixing; a decrease in volume of the combined water parcel. A new water parcel that has the same mass, but is lower in volume, will be denser. Denser water sinks or downwells in the otherwise neutral surface of the water body, where the two initial water parcels originated.\n\nThe importance of this process in oceanography was first pointed out by Witte, in a 1902 publication ().\n\nThe German origin of the term has caused some etymological confusion and disagreements as to the correct spelling of the term; for details, see the Wiktionary entry on cabelling. Oceanographers generally follow Stommel and refer to the process as \"cabbeling\".\n\nCabbeling may occur in high incidence in high latitude waters. Polar region waters are a place where cold and fresh water melting from sea ice meets warmer, saltier water. Ocean currents are responsible for bringing this warmer, saltier water to higher latitudes, especially on the eastern shores of Northern Hemisphere continents, and on the western shores of Southern Hemisphere continents. The phenomenon of cabbeling has been particularly noted in the Weddell Sea and the Greenland Sea.\n"}
{"id": "53513712", "url": "https://en.wikipedia.org/wiki?curid=53513712", "title": "Cerro Santa Ana Natural Monument", "text": "Cerro Santa Ana Natural Monument\n\nThe Cerro Santa Ana Natural Monument () Is a protected area with a natural monument status located in the center of the Paraguaná peninsula north of the Falcón state in the jurisdiction of the municipalities Falcón and Carirubana, between the populations of Santa Ana and Buena Vista, in the South American country of Venezuela. It has an area of 1900 hectares and a maximum height of 830 masl. It was declared Natural Monument on June 14, 1972 according to Decree No. 1.005, under the protection of INPARQUES.\n\nUnlike the rest of the peninsula Santa Ana Hill has as characteristics the contrast of its greenery with the xerophytic vegetation of the area of Paraguaná.\n\nIt has three peaks: Santa Ana (highest), Buena Vista and Moruy. In the ascent there are progressive changes in climate and vegetation. In the lower parts a dry environment of xerophytic vegetation predominates with thorny forest, while in the higher parts a woody vegetation develops, with leafy trees of 10 to 15 meters in height, with abundant epiphytes, mosses and lichens favored by the high humidity.\n\nIts fauna is predominantly the birds that live in the humid jungle.\n\n"}
{"id": "9573286", "url": "https://en.wikipedia.org/wiki?curid=9573286", "title": "Cers (wind)", "text": "Cers (wind)\n\nThe Cers, also called the Narbonnais by those who live southeast of Narbonne, is a very dry wind that is colder during the winter and warmer during the summer. Originating from moist Atlantic air-masses flowing across the Toulouse area, the Cers is intensified through the Lauragais gap. Cers winds are frequent across the Aude region in south-western France. Exceptionally red sunsets and lenticularis clouds usually herald the onset of the Cers. It is closely related to the Mistral, but the term Cers refers specifically to the very strong wind in the bas-Languedoc. \nThe opposite to this wind type is the Marin a south east wind which occurs across the Iberian Peninsula.\n\n\n"}
{"id": "17872457", "url": "https://en.wikipedia.org/wiki?curid=17872457", "title": "Data collector", "text": "Data collector\n\nIn the UK electricity system, a data collector (DC) is responsible for determining the amount of electricity supplied so that the customer can be correctly billed.\n\nFor half-hourly metered supplies, the HHDC retrieves the energy consumption data from the meter and makes any necessary estimates. The HH data is a history of the customer's electricity consumption for each half-hour period.\n\nFor non-half-hourly metered supplies, the NHHDC determines the consumption by calculating the advance (the difference between the last two meter reads, this is annualised to produce an annual advance (AA), this being the data the supplier will pay on).\n\nThe NHHDC is also responsible determining the estimated annual consumption (EAC), which is a forecast for a year ahead. The EAC is initially provided by the supplier, and thereafter forecast based on actual meter readings.\n\nData from the DC (EAC/ AA's) is provided to the non half hourly data aggregator, which aggregates the individual values provided into a single figure split in a number of ways e.g. geographically or by supplier. This aggregated data is then provided to the Central Systems, maintained by Elexon, to determine the value of energy which has been used by suppliers so that they are able to settle with the distribution network which generated the energy. The process is known as balancing and settlement and is regulated according to the Balancing and Settlement Code (BSC).\n\nA data collection charge is a fee paid to the data collector for determining the energy consumption of the supply. The charge may be shown separately on an electricity bill or it may be incorporated in the electricity supplier's overall charges.\n\n"}
{"id": "1116216", "url": "https://en.wikipedia.org/wiki?curid=1116216", "title": "Delta ray", "text": "Delta ray\n\nA delta ray is a secondary electron with enough energy to escape a significant distance away from the primary radiation beam and produce further ionization\", and is sometimes used to describe any recoil particle caused by secondary ionization. The term was coined by J. J. Thomson.\n\nA delta ray is characterized by very fast electrons produced in quantity by alpha particles or other fast energetic charged particles knocking orbiting electrons out of atoms. Collectively, these electrons are defined as delta radiation when they have sufficient energy to ionize further atoms through subsequent interactions on their own. Delta rays appear as branches in the main track of a cloud chamber (See Figs. 1,2). These branches will appear nearer the start of the track of a heavy charged particle, where more energy is imparted to the ionized electrons.\n\nOtherwise called a knock-on electron, the term \"delta ray\" is also used in high energy physics to describe single electrons in particle accelerators that are exhibiting characteristic deceleration. In a bubble chamber, electrons will lose their energy more quickly than other particles through Bremsstrahlung and will create a spiral track due to their small mass and the magnetic field. The Bremsstrahlung rate is proportional to the square of the acceleration of the electron.\n\n\n"}
{"id": "22316336", "url": "https://en.wikipedia.org/wiki?curid=22316336", "title": "Elizabeth A. Widjaja", "text": "Elizabeth A. Widjaja\n\nDr. Elizabeth Anita Widjaja (born 1951) is a Senior Principal Researcher of bamboo taxonomy at the Herbarium Bogoriense, Botany Division, Biological Research Centre at the Indonesian Institute of Sciences in Bogor, Indonesia.\nShe is especially interested in Indonesian bamboo and Malesian bamboo generally, and promotes the cultivation of bamboo for the prevention of erosion.\n\nDr. Widjaja recently said, of bamboo as a source of biofuel, that:\n\n\"Bambusa lako\" (Timor black bamboo) was described and separated from the Indonesian black bamboo species \"Gigantochloa atroviolacea\" by Professor Widjaja in 1997, as its appearance (morphology) differed.\n\n\n"}
{"id": "1625195", "url": "https://en.wikipedia.org/wiki?curid=1625195", "title": "Ficus Ruminalis", "text": "Ficus Ruminalis\n\nThe Ficus Ruminalis was a wild fig tree that had religious and mythological significance in ancient Rome. It stood near the small cave known as the Lupercal at the foot of the Palatine Hill and was the spot where according to tradition the floating makeshift cradle of Romulus and Remus landed on the banks of the Tiber. There they were nurtured by the she-wolf and discovered by Faustulus. The tree was sacred to Rumina, one of the birth and childhood deities, who protected breastfeeding in humans and animals. St. Augustine mentions a Jupiter Ruminus.\n\nThe wild fig tree was thought to be the male, wild counterpart of the cultivated fig, which was female. In some Roman sources, the wild fig is \"caprificus\", literally \"goat fig\". The fruit of the fig tree is pendulous, and the tree exudes a milky sap if cut. \"Rumina\" and \"Ruminalis\" (\"of Rumina\") were connected by some Romans to \"rumis\" or \"ruma\", \"teat, breast,\" but modern linguists think it is more likely related to the names \"Roma\" and \"Romulus,\" which may be based on \"rumon\", perhaps a word for \"river\" or an archaic name for the Tiber.\n\nThe tree is associated with the legend of Romulus and Remus, and stood where their cradle came to rest on the banks of the Tiber, after their abandonment. The tree offered the twins shade and shelter in their suckling by a she-wolf, just outside the nearby Lupercal cave, until their discovery and fostering by the shepherd Faustulus and his wife Acca Larentia. Remus was eventually killed by Romulus, who went on to found Rome on the Palatine Hill, above the cave.\n\nA statue of the she-wolf was supposed to have stood next to the \"Ficus Ruminalis\". In 296 BC, the curule aediles Gnaeus and Quintus Ogulnius placed images of Romulus and Remus as babies suckling under her teats. It may be this sculpture group that is represented on coins.\n\nThe Augustan historian Livy says that the tree still stood in his day, but his younger contemporary Ovid observes only \"vestigia\", \"traces,\" perhaps the stump. A textually problematic passage in Pliny seems to suggest that the tree was miraculously transplanted by the augur Attus Navius to the Comitium. This fig tree, however, was the \"Ficus Navia\", so called for the augur. Tacitus refers to the \"Ficus Navia\" as the \"Arbor Ruminalis\", an identification that suggests it had replaced the original \"Ficus Ruminalis\", either symbolically after the older tree's demise, or literally, having been cultivated as an offshoot. The \"Ficus Navia\" grew from a spot that had been struck by lightning and was thus regarded as sacred. Pliny's obscure reference may be to the statue of Attus Navius in front of the Curia Hostilia: he stood with his \"lituus\" raised in an attitude that connected the \"Ficus Navia\" and the accompanying representation of the she-wolf to the \"Ficus Ruminalis\", \"as if\" the tree had crossed from one space to the other. When the \"Ficus Navia\" drooped, it was taken as a bad omen for Rome. When it died, it was replaced. In 58 AD, it withered, but then revived and put forth new shoots.\n\nIn the archaeology of the Comitium, several irregular stone-lined shafts in rows, dating from Republican phases of pavement, may have been apertures to preserve venerable trees during rebuilding programs. Pliny mentions other sacred trees in the Roman Forum, with two additional figs. One fig was removed with a great deal of ritual fuss because its roots had undermined a statue of Silvanus. A relief on the \"Plutei\" of Trajan depicts Marsyas the satyr, whose statue stood in the Comitium, next to a fig tree that is placed on a plinth, as if it too were a sculpture. It is unclear whether this representation means that sacred trees might be replaced with artificial or pictorial ones. The apertures were paved over in the time of Augustus, an event that may explain Ovid's \"vestigia\".\n\n\n"}
{"id": "18993869", "url": "https://en.wikipedia.org/wiki?curid=18993869", "title": "Gas", "text": "Gas\n\nGas is one of the four fundamental states of matter (the others being solid, liquid, and plasma). A pure gas may be made up of individual atoms (e.g. a noble gas like neon), elemental molecules made from one type of atom (e.g. oxygen), or compound molecules made from a variety of atoms (e.g. carbon dioxide). A gas mixture would contain a variety of pure gases much like the air. What distinguishes a gas from liquids and solids is the vast separation of the individual gas particles. This separation usually makes a colorless gas invisible to the human observer. The interaction of gas particles in the presence of electric and gravitational fields are considered negligible as indicated by the constant velocity vectors in the image.\n\nThe gaseous state of matter is found between the liquid and plasma states, the latter of which provides the upper temperature boundary for gases. Bounding the lower end of the temperature scale lie degenerative quantum gases which are gaining increasing attention. High-density atomic gases super cooled to incredibly low temperatures are classified by their statistical behavior as either a Bose gas or a Fermi gas. For a comprehensive listing of these exotic states of matter see list of states of matter.\n\nThe only chemical elements that are stable diatomic homonuclear molecules at STP are hydrogen (H), nitrogen (N), oxygen (O), and two halogens: fluorine (F) and chlorine (Cl). When grouped together with the monatomic noble gases – helium (He), neon (Ne), argon (Ar), krypton (Kr), xenon (Xe), and radon (Rn) – these gases are called \"elemental gases\".\n\nThe word \"gas\" was first used by the early 17th-century Flemish chemist Jan Baptist van Helmont. He identified carbon dioxide, the first known gas other than air. Van Helmont's word appears to have been simply a phonetic transcription of the Ancient Greek word χάος \"Chaos\" – the \"g\" in Dutch being pronounced like \"ch\" in \"loch\" (voiceless velar fricative, IPA |x]) – in which case Van Helmont was simply following the established alchemical usage first attested in the works of Paracelsus. According to Paracelsus's terminology, \"chaos\" meant something like \"ultra-rarefied water\".\n\nAn alternative story is that Van Helmont's word is corrupted from \"gahst\" (or \"geist\"), signifying a ghost or spirit. This was because certain gases suggested a supernatural origin, such as from their ability to cause death, extinguish flames, and to occur in \"mines, bottom of wells, churchyards and other lonely places\".\n\nBecause most gases are difficult to observe directly, they are described through the use of four physical properties or macroscopic characteristics: pressure, volume, number of particles (chemists group them by moles) and temperature. These four characteristics were repeatedly observed by scientists such as Robert Boyle, Jacques Charles, John Dalton, Joseph Gay-Lussac and Amedeo Avogadro for a variety of gases in various settings. Their detailed studies ultimately led to a mathematical relationship among these properties expressed by the ideal gas law (see simplified models section below).\n\nGas particles are widely separated from one another, and consequently, have weaker intermolecular bonds than liquids or solids. These intermolecular forces result from electrostatic interactions between gas particles. Like-charged areas of different gas particles repel, while oppositely charged regions of different gas particles attract one another; gases that contain permanently charged ions are known as plasmas. Gaseous compounds with polar covalent bonds contain permanent charge imbalances and so experience relatively strong intermolecular forces, although the molecule while the compound's net charge remains neutral. Transient, randomly induced charges exist across non-polar covalent bonds of molecules and electrostatic interactions caused by them are referred to as Van der Waals forces. The interaction of these intermolecular forces varies within a substance which determines many of the physical properties unique to each gas. A comparison of \"boiling points\" for compounds formed by ionic and covalent bonds leads us to this conclusion. The drifting smoke particles in the image provides some insight into low-pressure gas behavior.\n\nCompared to the other states of matter, gases have low density and viscosity. Pressure and temperature influence the particles within a certain volume. This variation in particle separation and speed is referred to as \"compressibility\". This particle separation and size influences optical properties of gases as can be found in the following list of refractive indices. Finally, gas particles spread apart or diffuse in order to homogeneously distribute themselves throughout any container.\n\nWhen observing a gas, it is typical to specify a frame of reference or length scale. A \"larger\" length scale corresponds to a macroscopic or global point of view of the gas. This region (referred to as a volume) must be sufficient in size to contain a large sampling of gas particles. The resulting statistical analysis of this sample size produces the \"average\" behavior (i.e. velocity, temperature or pressure) of all the gas particles within the region. In contrast, a \"smaller\" length scale corresponds to a microscopic or particle point of view.\n\nMacroscopically, the gas characteristics measured are either in terms of the gas particles themselves (velocity, pressure, or temperature) or their surroundings (volume). For example, Robert Boyle studied pneumatic chemistry for a small portion of his career. One of his experiments related the macroscopic properties of pressure and volume of a gas. His experiment used a J-tube manometer which looks like a test tube in the shape of the letter J. Boyle trapped an inert gas in the closed end of the test tube with a column of mercury, thereby making the number of particles and the temperature constant. He observed that when the pressure was increased in the gas, by adding more mercury to the column, the trapped gas' volume decreased (this is known as an inverse relationship). Furthermore, when Boyle multiplied the pressure and volume of each observation, the product was constant. This relationship held for every gas that Boyle observed leading to the law, (PV=k), named to honor his work in this field.\n\nThere are many mathematical tools available for analyzing gas properties. As gases are subjected to extreme conditions, these tools become more complex, from the Euler equations for inviscid flow to the Navier–Stokes equations that fully account for viscous effects. These equations are adapted to the conditions of the gas system in question. Boyle's lab equipment allowed the use of algebra to obtain his analytical results. His results were possible because he was studying gases in relatively low pressure situations where they behaved in an \"ideal\" manner. These ideal relationships apply to safety calculations for a variety of flight conditions on the materials in use. The high technology equipment in use today was designed to help us safely explore the more exotic operating environments where the gases no longer behave in an \"ideal\" manner. This advanced math, including statistics and multivariable calculus, makes possible the solution to such complex dynamic situations as space vehicle reentry. An example is the analysis of the space shuttle reentry pictured to ensure the material properties under this loading condition are appropriate. In this flight regime, the gas is no longer behaving ideally.\n\nThe symbol used to represent \"pressure\" in equations is \"p\" or \"P\" with SI units of pascals.\n\nWhen describing a container of gas, the term pressure (or absolute pressure) refers to the average force per unit area that the gas exerts on the surface of the container. Within this volume, it is sometimes easier to visualize the gas particles moving in straight lines until they collide with the container (see diagram at top of the article). The force imparted by a gas particle into the container during this collision is the change in momentum of the particle. During a collision only the normal component of velocity changes. A particle traveling parallel to the wall does not change its momentum. Therefore, the average force on a surface must be the average change in linear momentum from all of these gas particle collisions.\n\nPressure is the sum of all the normal components of force exerted by the particles impacting the walls of the container divided by the surface area of the wall.\n\n The symbol used to represent \"temperature\" in equations is \"T\" with SI units of kelvins.\n\nThe speed of a gas particle is proportional to its absolute temperature. The volume of the balloon in the video shrinks when the trapped gas particles slow down with the addition of extremely cold nitrogen. The temperature of any physical system is related to the motions of the particles (molecules and atoms) which make up the [gas] system. In statistical mechanics, temperature is the measure of the average kinetic energy stored in a particle. The methods of storing this energy are dictated by the degrees of freedom of the particle itself (energy modes). Kinetic energy added (endothermic process) to gas particles by way of collisions produces linear, rotational, and vibrational motion. In contrast, a molecule in a solid can only increase its vibrational modes with the addition of heat as the lattice crystal structure prevents both linear and rotational motions. These heated gas molecules have a greater speed range which constantly varies due to constant collisions with other particles. The speed range can be described by the Maxwell–Boltzmann distribution. Use of this distribution implies ideal gases near thermodynamic equilibrium for the system of particles being considered.\n\nThe symbol used to represent \"specific volume\" in equations is \"v\" with SI units of cubic meters per kilogram.\nThe symbol used to represent volume in equations is \"V\" with SI units of cubic meters.\n\nWhen performing a thermodynamic analysis, it is typical to speak of intensive and extensive properties. Properties which depend on the amount of gas (either by mass or volume) are called \"extensive\" properties, while properties that do not depend on the amount of gas are called \"intensive\" properties. Specific volume is an example of an \"intensive\" property because it is the ratio of volume occupied by a \"unit of mass\" of a gas that is identical throughout a system at equilibrium. 1000 atoms a gas occupy the same space as any other 1000 atoms for any given temperature and pressure. This concept is easier to visualize for solids such as iron which are incompressible compared to gases. Since a gas fills any container in which it is placed, volume is an \"extensive property\".\n\nThe symbol used to represent density in equations is ρ (rho) with SI units of kilograms per cubic meter. This term is the reciprocal of specific volume.\n\nSince gas molecules can move freely within a container, their mass is normally characterized by density. Density is the amount of mass per unit volume of a substance, or the inverse of specific volume. For gases, the density can vary over a wide range because the particles are free to move closer together when constrained by pressure or volume. This variation of density is referred to as compressibility. Like pressure and temperature, density is a state variable of a gas and the change in density during any process is governed by the laws of thermodynamics. For a static gas, the density is the same throughout the entire container. Density is therefore a scalar quantity. It can be shown by kinetic theory that the density is \"inversely\" proportional to the size of the container in which a fixed mass of gas is confined. In this case of a fixed mass, the density decreases as the volume increases.\n\nIf one could observe a gas under a powerful microscope, one would see a collection of particles (molecules, atoms, ions, electrons, etc.) without any definite shape or volume that are in more or less random motion. These neutral gas particles only change direction when they collide with another particle or with the sides of the container. In an ideal gas, these collisions are perfectly elastic. This particle or microscopic view of a gas is described by the Kinetic-molecular theory. The assumptions behind this theory can be found in the postulates section of Kinetic Theory.\n\nKinetic theory provides insight into the macroscopic properties of gases by considering their molecular composition and motion. Starting with the definitions of momentum and kinetic energy, one can use the conservation of momentum and geometric relationships of a cube to relate macroscopic system properties of temperature and pressure to the microscopic property of kinetic energy per molecule. The theory provides averaged values for these two properties.\n\nThe theory also explains how the gas system responds to change. For example, as a gas is heated from absolute zero, when it is (in theory) perfectly still, its internal energy (temperature) is increased. As a gas is heated, the particles speed up and its temperature rises. This results in greater numbers of collisions with the container per unit time due to the higher particle speeds associated with elevated temperatures. The pressure increases in proportion to the number of collisions per unit time.\n\nBrownian motion is the mathematical model used to describe the random movement of particles suspended in a fluid. The gas particle animation, using pink and green particles, illustrates how this behavior results in the spreading out of gases (entropy). These events are also described by particle theory.\n\nSince it is at the limit of (or beyond) current technology to observe individual gas particles (atoms or molecules), only theoretical calculations give suggestions about how they move, but their motion is different from Brownian motion because Brownian motion involves a smooth drag due to the frictional force of many gas molecules, punctuated by violent collisions of an individual (or several) gas molecule(s) with the particle. The particle (generally consisting of millions or billions of atoms) thus moves in a jagged course, yet not so jagged as would be expected if an individual gas molecule were examined.\n\nAs discussed earlier, momentary attractions (or repulsions) between particles have an effect on gas dynamics. In physical chemistry, the name given to these intermolecular forces is \"van der Waals force\". These forces play a key role in determining physical properties of a gas such as viscosity and flow rate (see physical characteristics section). Ignoring these forces in certain conditions allows a real gas to be treated like an ideal gas. This assumption allows the use of ideal gas laws which greatly simplifies calculations.\n\nProper use of these gas relationships requires the kinetic-molecular theory (KMT). When gas particles experience intermolecular forces they gradually influence one another as the spacing between them is reduced (the hydrogen bond model illustrates one example). In the absence of any charge, at some point when the spacing between gas particles is greatly reduced they can no longer avoid collisions between themselves at normal gas temperatures. Another case for increased collisions among gas particles would include a fixed volume of gas, which upon heating would contain very fast particles. This means that these ideal equations provide reasonable results except for extremely high pressure (compressible) or high temperature (ionized) conditions. All of these excepted conditions allow energy transfer to take place within the gas system. The absence of these internal transfers is what is referred to as ideal conditions in which the energy exchange occurs only at the boundaries of the system. Real gases experience some of these collisions and intermolecular forces. When these collisions are statistically negligible (incompressible), results from these ideal equations are still meaningful. If the gas particles are compressed into close proximity they behave more like a liquid (see fluid dynamics).\n\nAn \"equation of state\" (for gases) is a mathematical model used to roughly describe or predict the state properties of a gas. At present, there is no single equation of state that accurately predicts the properties of all gases under all conditions. Therefore, a number of much more accurate equations of state have been developed for gases in specific temperature and pressure ranges. The \"gas models\" that are most widely discussed are \"perfect gas\", \"ideal gas\" and \"real gas\". Each of these models has its own set of assumptions to facilitate the analysis of a given thermodynamic system. Each successive model expands the temperature range of coverage to which it applies.\n\nThe equation of state for an ideal or perfect gas is the ideal gas law and reads\nwhere \"P\" is the pressure, \"V\" is the volume, \"n\" is amount of gas (in mol units), \"R\" is the universal gas constant, 8.314 J/(mol K), and \"T\" is the temperature. Written this way, it is sometimes called the \"chemist's version\", since it emphasizes the number of molecules \"n\". It can also be written as\nwhere formula_3 is the specific gas constant for a particular gas, in units J/(kg K), and ρ = m/V is density. This notation is the \"gas dynamicist's\" version, which is more practical in modeling of gas flows involving acceleration without chemical reactions.\n\nThe ideal gas law does not make an assumption about the specific heat of a gas. In the most general case, the specific heat is a function of both temperature and pressure. If the pressure-dependence is neglected (and possibly the temperature-dependence as well) in a particular application, sometimes the gas is said to be a perfect gas, although the exact assumptions may vary depending on the author and/or field of science.\n\nFor an ideal gas, the ideal gas law applies without restrictions on the specific heat. An ideal gas is a simplified \"real gas\" with the assumption that the compressibility factor \"Z\" is set to 1 meaning that this pneumatic ratio remains constant. A compressibility factor of one also requires the four state variables to follow the ideal gas law.\n\nThis approximation is more suitable for applications in engineering although simpler models can be used to produce a \"ball-park\" range as to where the real solution should lie. An example where the \"ideal gas approximation\" would be suitable would be inside a combustion chamber of a jet engine. It may also be useful to keep the elementary reactions and chemical dissociations for calculating emissions.\n\nEach one of the assumptions listed below adds to the complexity of the problem's solution. As the density of a gas increases with rising pressure, the intermolecular forces play a more substantial role in gas behavior which results in the ideal gas law no longer providing \"reasonable\" results. At the upper end of the engine temperature ranges (e.g. combustor sections – 1300 K), the complex fuel particles absorb internal energy by means of rotations and vibrations that cause their specific heats to vary from those of diatomic molecules and noble gases. At more than double that temperature, electronic excitation and dissociation of the gas particles begins to occur causing the pressure to adjust to a greater number of particles (transition from gas to plasma). Finally, all of the thermodynamic processes were presumed to describe uniform gases whose velocities varied according to a fixed distribution. Using a non-equilibrium situation implies the flow field must be characterized in some manner to enable a solution. One of the first attempts to expand the boundaries of the ideal gas law was to include coverage for different thermodynamic processes by adjusting the equation to read \"pV = constant\" and then varying the \"n\" through different values such as the specific heat ratio, \"γ\".\n\nReal gas effects include those adjustments made to account for a greater range of gas behavior:\n\nFor most applications, such a detailed analysis is excessive. Examples where real gas effects would have a significant impact would be on the Space Shuttle re-entry where extremely high temperatures and pressures were present or the gases produced during geological events as in the image of the 1990 eruption of Mount Redoubt.\n\nBoyle's Law was perhaps the first expression of an equation of state. In 1662 Robert Boyle performed a series of experiments employing a J-shaped glass tube, which was sealed on one end. Mercury was added to the tube, trapping a fixed quantity of air in the short, sealed end of the tube. Then the volume of gas was carefully measured as additional mercury was added to the tube. The pressure of the gas could be determined by the difference between the mercury level in the short end of the tube and that in the long, open end. The image of Boyle's Equipment shows some of the exotic tools used by Boyle during his study of gases.\n\nThrough these experiments, Boyle noted that the pressure exerted by a gas held at a constant temperature varies inversely with the volume of the gas. For example, if the volume is halved, the pressure is doubled; and if the volume is doubled, the pressure is halved. Given the inverse relationship between pressure and volume, the product of pressure (\"P\") and volume (\"V\") is a constant (\"k\") for a given mass of confined gas as long as the temperature is constant. Stated as a formula, thus is:\n\nBecause the before and after volumes and pressures of the fixed amount of gas, where the before and after temperatures are the same both equal the constant \"k\", they can be related by the equation:\n\nformula_5\n\nIn 1787, the French physicist and balloon pioneer, Jacques Charles, found that oxygen, nitrogen, hydrogen, carbon dioxide, and air expand to the same extent over the same 80 kelvin interval. He noted that, for an ideal gas at constant pressure, the volume is directly proportional to its temperature:\n\nIn 1802, Joseph Louis Gay-Lussac published results of similar, though more extensive experiments. Gay-Lussac credited Charles' earlier work by naming the law in his honor. Gay-Lussac himself is credited with the law describing pressure, which he found in 1809. It states that the pressure exerted on a container's sides by an ideal gas is proportional to its temperature.\n\nIn 1811, Amedeo Avogadro verified that equal volumes of pure gases contain the same number of particles. His theory was not generally accepted until 1858 when another Italian chemist Stanislao Cannizzaro was able to explain non-ideal exceptions. For his work with gases a century prior, the number that bears his name Avogadro's constant represents the number of atoms found in 12 grams of elemental carbon-12 (6.022×10 mol). This specific number of gas particles, at standard temperature and pressure (ideal gas law) occupies 22.40 liters, which is referred to as the molar volume.\n\nAvogadro's law states that the volume occupied by an ideal gas is proportional to the number of moles (or molecules) present in the container. This gives rise to the molar volume of a gas, which at STP is 22.4 dm (or litres). The relation is given by\n\nwhere n is equal to the number of moles of gas (the number of molecules divided by Avogadro's Number).\n\nIn 1801, John Dalton published the Law of Partial Pressures from his work with ideal gas law relationship: The pressure of a mixture of non reactive gases is equal to the sum of the pressures of all of the constituent gases alone. Mathematically, this can be represented for \"n\" species as:\n\nThe image of Dalton's journal depicts symbology he used as shorthand to record the path he followed. Among his key journal observations upon mixing unreactive \"elastic fluids\" (gases) were the following:\n\nThermodynamicists use this factor (\"Z\") to alter the ideal gas equation to account for compressibility effects of real gases. This factor represents the ratio of actual to ideal specific volumes. It is sometimes referred to as a \"fudge-factor\" or correction to expand the useful range of the ideal gas law for design purposes. \"Usually\" this \"Z\" value is very close to unity. The compressibility factor image illustrates how Z varies over a range of very cold temperatures.\n\nIn fluid mechanics, the Reynolds number is the ratio of inertial forces (\"vρ\") to viscous forces (\"μ/L\"). It is one of the most important dimensionless numbers in fluid dynamics and is used, usually along with other dimensionless numbers, to provide a criterion for determining dynamic similitude. As such, the Reynolds number provides the link between modeling results (design) and the full-scale actual conditions. It can also be used to characterize the flow.\n\nViscosity, a physical property, is a measure of how well adjacent molecules stick to one another. A solid can withstand a shearing force due to the strength of these sticky intermolecular forces. A fluid will continuously deform when subjected to a similar load. While a gas has a lower value of viscosity than a liquid, it is still an observable property. If gases had no viscosity, then they would not stick to the surface of a wing and form a boundary layer. A study of the delta wing in the Schlieren image reveals that the gas particles stick to one another (see Boundary layer section).\n\nIn fluid dynamics, turbulence or turbulent flow is a flow regime characterized by chaotic, stochastic property changes. This includes low momentum diffusion, high momentum convection, and rapid variation of pressure and velocity in space and time. The satellite view of weather around Robinson Crusoe Islands illustrates one example.\n\nParticles will, in effect, \"stick\" to the surface of an object moving through it. This layer of particles is called the boundary layer. At the surface of the object, it is essentially static due to the friction of the surface. The object, with its boundary layer is effectively the new shape of the object that the rest of the molecules \"see\" as the object approaches. This boundary layer \"can\" separate from the surface, essentially creating a new surface and completely changing the flow path. The classical example of this is a stalling airfoil. The delta wing image clearly shows the boundary layer thickening as the gas flows from right to left along the leading edge.\n\nAs the total number of degrees of freedom approaches infinity, the system will be found in the macrostate that corresponds to the highest multiplicity. In order to illustrate this principle, observe the skin temperature of a frozen metal bar. Using a thermal image of the skin temperature, note the temperature distribution on the surface. This initial observation of temperature represents a \"microstate\". At some future time, a second observation of the skin temperature produces a second microstate. By continuing this observation process, it is possible to produce a series of microstates that illustrate the thermal history of the bar's surface. Characterization of this historical series of microstates is possible by choosing the macrostate that successfully classifies them all into a single grouping.\n\nWhen energy transfer ceases from a system, this condition is referred to as thermodynamic equilibrium. Usually this condition implies the system and surroundings are at the same temperature so that heat no longer transfers between them. It also implies that external forces are balanced (volume does not change), and all chemical reactions within the system are complete. The timeline varies for these events depending on the system in question. A container of ice allowed to melt at room temperature takes hours, while in semiconductors the heat transfer that occurs in the device transition from an on to off state could be on the order of a few nanoseconds.\n\n\n"}
{"id": "1539049", "url": "https://en.wikipedia.org/wiki?curid=1539049", "title": "Glass microsphere", "text": "Glass microsphere\n\nGlass microspheres are microscopic spheres of glass manufactured for a wide variety of uses in research, medicine, consumer goods and various industries. Glass microspheres are usually between 1 and 1000 micrometers in diameter, although the sizes can range from 100 nanometers to 5 millimeters in diameter. Hollow glass microspheres, sometimes termed microballoons or glass bubbles, have diameters ranging from 10 to 300 micrometers.\n\nHollow spheres are used as a lightweight filler in composite materials such as syntactic foam and lightweight concrete. Microballoons give syntactic foam its light weight, low thermal conductivity, and a resistance to compressive stress that far exceeds that of other foams. These properties are exploited in the hulls of submersibles and deep-sea oil drilling equipment, where other types of foam would implode. Hollow spheres of other materials create syntactic foams with different properties: ceramic balloons e.g. can make a light syntactic aluminium foam.\n\nHollow spheres also have uses ranging from storage and slow release of pharmaceuticals and radioactive tracers to research in controlled storage and release of hydrogen. Microspheres are also used in composites to fill polymer resins for specific characteristics such as weight, sandability and sealing surfaces. When making surfboards for example, shapers seal the EPS foam blanks with epoxy and microballoons to create an impermeable and easily sanded surface upon which fiberglass laminates are applied.\n\nGlass microspheres can be made by heating tiny droplets of dissolved water glass in a process known as ultrasonic spray pyrolysis (USP), and properties can be improved somewhat by using a chemical treatment to remove some of the sodium. Sodium depletion has also allowed hollow glass microspheres to be used in chemically sensitive resin systems, such as long pot life epoxies or non-blown polyurethane composites\n\nAdditional functionalities, such as silane coatings, are commonly added to the surface of hollow glass microspheres to increase the matrix/microspheres interfacial strength (the common failure point when stressed in a tensile manner).\n\nMicrospheres made of high quality optical glass, can be produced for research on the field of optical resonators or cavities.\n\nGlass microspheres are also produced as waste product in coal-fired power stations. In this case the product would be generally termed \"cenosphere\" and carry an aluminosilicate chemistry (as opposed to the sodium silica chemistry of engineered spheres). Small amounts of silica in the coal are melted and as they rise up the chimneystack, expand and form small hollow spheres. These spheres are collected together with the ash, which is pumped in a water mixture to the resident ash dam. Some of the particles do not become hollow and sink in the ash dams, while the hollow ones float on the surface of the dams. They become a nuisance, especially when they dry, as they become airborne and blow over into surrounding areas.\n\nMicrospheres have been used to produce focal regions, known as photonic nanojets and whose sizes are below the micrometric scale. Previous research has demonstrated experimentally and with simulations the use of microspheres in order to increase the signal intensity obtained in different experiments. A confirmation of the photonic jet in the microwave scale, observing the backscattering enhancement that occurred when metallic particles were introduced in the focus area. A measurable enhancement of the backscattered light in the visible range was obtained when a gold nanoparticle was placed inside the photonic nanojet region produced by a dielectric microsphere with a 4.4 μm diameter. A use of nanojets produced by transparent microspheres in order to excite optical active materials, under upconversion processes with different numbers of excitation photons, has been analyzed as well.\n\nMonodisperse glass microspheres have high sphericity and a very tight particle size distribution, often with CV<10% and specification of >95% of particles in size range. Monodisperse glass particles are often used as spacers in adhesives and coatings, such as bond line spacers in epoxies. Just a small amount of spacer grade monodisperse microspheres can create a controlled gap, as well as define and maintain specified bond line thickness. Spacer grade particles can also be used as calibration standards and tracer particles for qualifying medical devices. High quality spherical glass microspheres are often used in gas plasma displays, automotive mirrors, electronic displays, flip chip technology, filters, microscopy, and electronic equipment.\n\nOther applications include syntactic foams and particulate composites and reflective paints.\n\nDispensing of microspheres can be a difficult task. When utilizing microspheres as a filler for standard mixing and dispensing machines, a breakage rate of up to 80% can occur, depending upon factors such as pump choice, material viscosity, material agitation, and temperature. Customized dispensers for microsphere-filled materials may reduce the microsphere breakage rate to a minimal amount. A progressive cavity pump is the pump of choice for dispensing materials with microspheres, which can reduce microsphere breakage as much as 80%.[sources needed]\n\n"}
{"id": "43616057", "url": "https://en.wikipedia.org/wiki?curid=43616057", "title": "Global citizenship education", "text": "Global citizenship education\n\nGlobal citizenship education (GCE) is a form of civic learning that involves students' active participation in projects that address global issues of a social, political, economic, or environmental nature. The two main elements of GCE are 'global consciousness'; the moral or ethical aspect of global issues, and 'global competencies', or skills meant to enable learners to participate in changing and developing the world. The promotion of GCE was a response by governments and NGOs to the emergence of supranational institution, regional economic blocs, and the development of information and communications technologies. These have allresulted in the emergence of a more globally oriented and collaborative approach to education. GCE addresses themes such as peace and human rights, intercultural understanding, citizenship education, respect for diversity and tolerance, and inclusiveness.Schools in action, global citizens for sustainable development: a guide for students\n\nGlobal citizenship consists of voluntary practices oriented to human rights, social justice, and environmentalism at the local, regional, and global level. Unlike national citizenship, global citizenship does not denote any legal status or allegiance to an actual form of government. The emergence of regional economic blocs, supra-national political institutions such as the European Union, and the advancement of ICTs, has caused governments to try to prepare national populations to be competitive in the global jobs market. This has led to the introduction of global citizenship education programs at primary, secondary, and tertiary level, but also at independent NGOs, grass roots organizations, and other large scale educational organizations, such as the International Baccalaureate Organization and UNESCO.\n\nThe most important features of global citizenship education are voluntary action that can extend from local to international collectives; the practice of cultural empathy; and a focus on active participation in social and political life at the local and global level. In the late 1990s, OXFAM UK designed a curriculum for global citizenship education\nwhich stressed \"the 'active' role of global citizens\".In this approach, individuals and groups both inside and outside the educational sector might take action that addresses human rights, trade, poverty, health, and environmental issues, for example. This is sometimes called the 'global consciousness' aspect of GCE. However, organizations such as UNESCO have also begun to emphasize 'global competencies', including science and technology into their GCE curricula, to \"strengthen linkages between education and economic development\".\n\nIn the present era of globalization, the recognition of global interdependence on the part of the general public has led to a higher degree of interest in global citizenship in education. No Though modern schooling may have been oriented to education suitable for the nation-state throughout the 19th and 20th centuries, in the 21st century, citizenship is understood in global terms, so that schooling might improve individual nations' global competitiveness. Many universities worldwide have responded to the need for a globally oriented education by sending their students to study abroad in increasing numbers, and some have announced that this will soon become a mandatory degree requirement.\n\nMany governments also now promote GCE for the cohesion of society. The large numbers of people migrating across national borders means that the diversity of ethnic, religious, and linguistic groups, \"has raised [...] complex and difficult questions about citizenship, human rights, democracy, and education\". In addition, global issues related to sustainability, such as the world's future energy arrangements, have also been incorporated into the domain of global citizenship education.\n\nWhile GCE can take different forms, it has some common elements, which include fostering in learners the following competences:\n\nMost educators agree that \"global citizenship is a learned and nurtured behavior\", and the most widely used classroom strategy for developing global skills is project-based learning. This pedagogical technique can be utilized in the case of almost any school subject, \"[and] is the primary pedagogical strategy in the discourse of global competencies. Educators see it as an important method for developing the tools- technical and emotional- for success in the global society\". With the aim of nurturing students' potential to be both learners and citizens, the project-based approach has been used successfully in community-based learning, for example.\n\nAnother important pedagogical feature of GCE is learning through communicative practices outside the classroom that \"harness […] the educational force of the wider culture\". If students are encouraged \"to see themselves as political agents\", educators assume they are more likely to acquire the knowledge, skills and abilities that enable them to become agents of change. Another important element of the student-centered participatory nature of GCED, is that students, through their engagement with others via Social Network Services, create their own forms of global citizenship through dialogue, learning, and action. This is an important element, for example, in the activities of grassroots organizations like 'GIN' (Global Issues Network), which involves students and teachers in projects that address global issues such as human rights, trade rules, and deforestation. Such student-driven, student-led projects combine both the 'global consciousness' and 'global competence' aspects of GCED.\n\n\nOrganizations implementing GCE programs, such as UNESCO, now emphasize the importance of expanding both students' 'global consciousness' and 'global competence'. 'Global consciousness' represents the ethical or moral dimension of global citizenship, whereas 'global competence' \"features a blend of the technical-rational and the dispositional or attitudinal\".\n\nHowever, some view global consciousness and global competence as being closely related. The OECD, for instance, focuses on global competencies called 'psychosocial resources', of which there are three main types: \"using tools interactively (technology and language skills), interacting in heterogeneous groups (cooperation, empathy), and acting autonomously (realizing one's identity, conducting life plans, defending and asserting rights\".\n\nGCE identifies three learner attributes, which refer to the traits and qualities that global citizenship education aims to develop in learners and correspond to the key learning outcomes mentioned earlier. These are: informed and critically literate; socially connected and respectful of diversity; ethically responsible and engaged. The three learner attributes draw on a review of the literature and of citizenship education conceptual frameworks, a review of approaches and curricula, as well as technical consultations and recent work by UNESCO on global citizenship education.\n\nLearners develop their understanding of the world, global themes, governance structures and systems, including politics, history and economics; understand the rights and responsibilities of individuals and groups (for example, women’s and children’s rights, indigenous rights, corporate social responsibility); and, recognise the interconnectedness of local, national and global issues, structures and processes. Learners develop the skills of critical inquiry (for example, where to find information and how to analyse and use evidence), media literacy and an understanding of how information is mediated and communicated. They develop their ability to inquire into global themes and issues (for example, globalisation, interdependence, migration, peace and conflict, sustainable development) by planning investigations, analysing data and communicating their findings. A key issue is the way in which language is used and, more specifically, how critical literacy is affected by the dominance of the English language and how this influences non-English speakers’ access to information.\n\nLearners learn about their identities and how they are situated within multiple relationships (for example, family, friends, school, local community, country), as a basis for understanding the global dimension of citizenship. They develop an understanding of difference and diversity (for example, culture, language, gender, sexuality, religion), of how beliefs and values influence people’s views about those who are different, and of the reasons for, and impact of, inequality and discrimination. Learners also consider common factors that transcend difference, and develop the knowledge, skills, values and attitudes required for respecting difference and living with others.\n\nLearners explore their own beliefs and values and those of others. They understand how beliefs and values inform social and political decision-making at local, national, regional and global levels, and the challenges for governance of contrasting and conflicting beliefs and values. Learners also develop their understanding of social justice issues in local, national, regional and global contexts and how these are interconnected. Ethical issues (for example, relating to climate change, consumerism, economic globalisation, fair trade, migration, poverty and wealth, sustainable development, terrorism, war) are also addressed. Learners are expected to reflect on ethical conflicts related to social and political responsibilities and the wider impact of their choices and decisions. Learners also develop the knowledge, skills, values and attitudes to care for others and the environment and to engage in civic action. These include compassion, empathy, collaboration, dialogue, social entrepreneurship and active participation. They learn about opportunities for engagement as citizens at local, national and global levels, and examples of individual and collective action taken by others to address global issues and social injustice.\n\nHigh Resolves is a secondary school educational initiative (implemented by the FYA, the only national, independent non-profit organisation for young people in Australia) consisting of a Global Citizenship Programme for Year 8 students and a Global Leadership Programme for Year 9 and 10 students. It aims to enable students to consider their personal role in developing their society as a global community through workshops, simulations, leadership skills training and hands-on action projects.\n\nIn England, the Department for Education and Skills produced \"Developing the global dimension in the school curriculum\", a publication for head teachers, teachers, senior managers and those with responsibility for curriculum development. It aims to show how the global dimension can be integrated in the curriculum and across the school. It provides examples of how to integrate the global dimension from age 3 to age 16, outlining eight key concepts – global citizenship, conflict resolution, diversity, human rights, interdependence, sustainable development, values and perceptions, and social justice. For example, it gives guidance for the promotion of personal, social and emotional development of the youngest learners through discussion of photographs of children from around the world, activities, stories, and discussion of different places children have visited.\n\nActivate is a network of young leaders in South Africa which aims to bring about change though creative solutions to problems in society. Youth from all backgrounds and provinces in the country participate in a two-year programme. In the first year, there are three residential training programmes, working on a particular task. In the second year, participants form action groups on specific tasks, taking their work into the public domain. In one example, an Activator describes how he works in his local community to discourage young people away from joining gangs and engaging in substance abuse. He draws on his own negative experiences with gangs and drugs, having served seven years in jail. On being interviewed, he states: “My vision for South Africa is to see young people standing up and becoming role models... Be yourself, be real and pursue your dreams”.\n\nCouncil for Global Citizenship Education — part of the Global Citizenship Foundation, a non-profit organisation based in India — is an initiative for schools to adopt a participatory whole-school approach to global citizenship education. The initiative fosters continuous professional development (CPD) of educators; teacher led contextualization, design, and development of GCED curriculum; engagement of children through the '100 Acts of Global Citizenship' School Challenge; and community through a Global Citizenship Festival at '100 Acts of Global Citizenship' participating schools. It has also been implemented in 10 States of India.\n\nPeace First, a non-profit organisation based in the United States, has a programme in which youth volunteers work with children to design and implement community projects in a participatory way. The rationale is that children are natural creative thinkers and problem solvers. The programme focuses on developing social and emotional skills of self- awareness, empathy, inclusivity and relationships. It has also been implemented in rural areas of Colombia through a partnership between local governments and Colombian NGOs. Peace First has additionally developed a curriculum that can be used in schools. It addresses themes such as friendship, fairness, cooperation, con ict resolution and consequences of actions through experiential activities and cooperative games. For example, 1st graders learn about communicating their feelings, 3rd graders develop skills and awareness around communication and cooperation, 4th graders practice courage and taking a stand and 5th graders explore how to resolve and de-escalate conflicts.\n\nTokyo Global Engineering Corporation is an education-services organization that provides capstone education programs free of charge to engineering students and other stakeholders. These programs are intended to complement—but not to replace—coursework required by academic degree programs of study. The programs are educational opportunities, and students are not paid money for their participation. All correspondence among members is completed via e-mail, and all meetings are held via Skype, with English as the language of instruction and publication. Students and other stakeholders are never asked to travel or leave their geographic locations, and are encouraged to publish organizational documents in their personal, primary languages, when English is a secondary language.\n\nGCE and ESD pursue the same vision: It is all about empowering learners of all ages to become proactive contributors to a more just, peaceful, tolerant, inclusive and sustainable world. Both GCED and ESD:\nBoth GCE and ESD help learners understand the interconnected world in which they live and the complexities of the global challenges faced. GCE and ESD help learners to develop their knowledge, skills, attitudes and values so that they can address these challenges responsibly and effectively now and in the future.\n\nSome fundamentalist critics believe GCE might undermine religious education and promote secular values. Others are concerned that the pedagogical approach of most global citizenship education curricula are too often produced in particular Northern, Western contexts. Some critics claim that GCE curricula promote values that are too individualistic. Dill, for example, claims that \"the majority of the world experiences social and communal life not in terms of isolated individuals, but as collective identities and traditions. For many of these groups, the dominant forms of global citizenship education and its moral order will be experienced as coercive and unjust', so 'global' citizenship curriculum should be seen as a local practice, \"which diverse cultures will conceptualize and construct differently\".\n\n\n"}
{"id": "23602070", "url": "https://en.wikipedia.org/wiki?curid=23602070", "title": "History of mineralogy", "text": "History of mineralogy\n\nEarly writing on mineralogy, especially on gemstones, comes from ancient Babylonia, the ancient Greco-Roman world, ancient and medieval China, and Sanskrit texts from ancient India. Books on the subject included the Naturalis Historia of Pliny the Elder which not only described many different minerals but also explained many of their properties. The German Renaissance specialist Georgius Agricola wrote works such as \"De re metallica\" (\"On Metals\", 1556) and \"De Natura Fossilium\" (\"On the Nature of Rocks\", 1546) which began the scientific approach to the subject. Systematic scientific studies of minerals and rocks developed in post-Renaissance Europe. The modern study of mineralogy was founded on the principles of crystallography and microscopic study of rock sections with the invention of the microscope in the 17th century.\n\nThe ancient Greek writers Aristotle (384–322 BC) and Theophrastus (370–285 BC) were the first in the Western tradition to write of minerals and their properties, as well as metaphysical explanations for them. The Greek philosopher Aristotle wrote his \"Meteorologica\", and in it theorized that all the known substances were composed of water, air, earth, and fire, with the properties of dryness, dampness, heat, and cold. The Greek philosopher and botanist Theophrastus wrote his \"De Mineralibus\", which accepted Aristotle's view, and divided minerals into two categories: those affected by heat and those affected by dampness.\n\nThe metaphysical emanation and exhalation (\"anathumiaseis\") theory of Aristotle included early speculation on earth sciences including mineralogy. According to his theory, while metals were supposed to be congealed by means of moist exhalation, dry gaseous exhalation (\"pneumatodestera\") was the efficient material cause of minerals found in the Earth's soil. He postulated these ideas by using the examples of moisture on the surface of the earth (a moist vapor 'potentially like water'), while the other was from the earth itself, pertaining to the attributes of hot, dry, smoky, and highly combustible ('potentially like fire'). Aristotle's metaphysical theory from times of antiquity had wide-ranging influence on similar theory found in later medieval Europe, as the historian Berthelot notes:\n\"The theory of exhalations was the point of departure for later ideas on the generation of metals in the earth, which we meet with Proclus, and which reigned throughout the middle ages.\"\n\nAncient Greek terminology of minerals has also stuck through the ages with widespread usage in modern times. For example, the Greek word asbestos (meaning 'inextinguishable', or 'unquenchable'), for the unusual mineral known today containing fibrous structure. The ancient historians Strabo (63 BC–19 AD) and Pliny the Elder (23–79 AD) both wrote of asbestos, its qualities, and its origins, with the Hellenistic belief that it was of a type of vegetable. Pliny the Elder listed it as a mineral common in India, while the historian Yu Huan (239–265 AD) of China listed this 'fireproof cloth' as a product of ancient Rome or Arabia (Chinese: Daqin). Although documentation of these minerals in ancient times does not fit the manner of modern scientific classification, there was nonetheless extensive written work on early mineralogy.\n\nFor example, Pliny devoted five entire volumes of his work Naturalis Historia (77 AD) to the classification of \"earths, metals, stones, and gems\". He not only describes many minerals not known to Theophrastus, but discusses their applications and properties. He is the first to correctly recognise the origin of amber for example, as the fossilised remnant of tree resin from the observation of insects trapped in some samples. He laid the basis of crystallography by discussing crystal habit, especially the octahedral shape of diamond. His discussion of mining methods is unrivalled in the ancient world, and includes, for example, an eye-witness account of gold mining in northern Spain, an account which is fully confirmed by modern research.\n\nHowever, before the more definitive foundational works on mineralogy in the 16th century, the ancients recognized no more than roughly 350 minerals to list and describe.\n\nWith philosophers such as Proclus, the theory of Neoplatonism also spread to the Islamic world during the Middle Ages, providing a basis for metaphysical ideas on mineralogy in the medieval Middle East as well. The medieval Islamic scientists expanded upon this as well, including the Persian scientist Ibn Sina (ابوعلى سينا/پورسينا) (980-1037 AD), also known as \"Avicenna\", who rejected alchemy and the earlier notion of Greek metaphysics that metallic and other elements could be transformed into one another. However, what was largely accurate of the ancient Greek and medieval metaphysical ideas on mineralogy was the slow chemical change in composition of the Earth's crust. There was also the Islamic alchemist and scientist Jābir ibn Hayyān (721-815 AD), who was the first to bring the experimental method into alchemy. Aided by Greek mathematics and Islamic mathematics, he discovered the syntheses for hydrochloric acid, nitric acid, distillation and crystallization (the latter two being essential for the understanding of modern mineralogy).\n\nIn the early 16th century AD, the writings of the German scientist Georg Bauer, pen-name Georgius Agricola (1494-1555 AD), in his \"Bermannus, sive de re metallica dialogus\" (1530) is considered to be the official establishment of mineralogy in the modern sense of its study. He wrote the treatise while working as a town physician and making observations in Joachimsthal, which was then a center for mining and metallurgic smelting industries. In 1544, he published his written work \"De ortu et causis subterraneorum\", which is considered to be the foundational work of modern physical geology. In it (much like Ibn Sina) he heavily criticized the theories laid out by the ancient Greeks such as Aristotle. His work on mineralogy and metallurgy continued with the publication of \"De veteribus et novis metallis\" in 1546, and culminated in his best known works, the \"De re metallica\" of 1556. It was an impressive work outlining applications of mining, refining, and smelting metals, alongside discussions on geology of ore bodies, surveying, mine construction, and ventilation. He praises Pliny the Elder for his pioneering work Naturalis Historia and makes extensive references to his discussion of minerals and mining methods. For the next two centuries this written work remained the authoritative text on mining in Europe.\n\nAgricola had many various theories on mineralogy based on empirical observation, including understanding of the concept of ore channels that were formed by the circulation of ground waters ('succi') in fissures subsequent to the deposition of the surrounding rocks. As will be noted below, the medieval Chinese previously had conceptions of this as well.\n\nFor his works, Agricola is posthumously known as the \"Father of Mineralogy\".\n\nAfter the foundational work written by Agricola, it is widely agreed by the scientific community that the \"Gemmarum et Lapidum Historia\" of Anselmus de Boodt (1550–1632) of Bruges is the first definitive work of modern mineralogy. The German mining chemist J.F. Henckel wrote his \"Flora Saturnisans\" of 1760, which was the first treatise in Europe to deal with geobotanical minerals, although the Chinese had mentioned this in earlier treatises of 1421 and 1664. In addition, the Chinese writer Du Wan made clear references to weathering and erosion processes in his \"Yun Lin Shi Pu\" of 1133, long before Agricola's work of 1546.\n\nIn ancient China, the oldest literary listing of minerals dates back to at least the 4th century BC, with the \"Ji Ni Zi\" book listing twenty four of them. Chinese ideas of metaphysical mineralogy span back to at least the ancient Han Dynasty (202 BC–220 AD). From the 2nd century BC text of the \"Huai Nan Zi\", the Chinese used ideological Taoist terms to describe meteorology, precipitation, different types of minerals, metallurgy, and alchemy. Although the understanding of these concepts in Han times was Taoist in nature, the theories proposed were similar to the Aristotelian theory of mineralogical exhalations (noted above). By 122 BC, the Chinese had thus formulated the theory for metamorphosis of minerals, although it is noted by historians such as Dubs that the tradition of alchemical-mineralogical Chinese doctrine stems back to the School of Naturalists headed by the philosopher Zou Yan (305 BC–240 BC). Within the broad categories of rocks and stones (shi) and metals and alloys (jin), by Han times the Chinese had hundreds (if not thousands) of listed types of stones and minerals, along with theories for how they were formed.\n\nIn the 5th century AD, Prince Qian Ping Wang of the Liu Song Dynasty wrote in the encyclopedia \"Tai-ping Yu Lan\" (circa 444 AD, from the lost book \"Dian Shu\", or \"Management of all Techniques\"):\n\"The most precious things in the world are stored in the innermost regions of all. For example, there is orpiment. After a thousand years it changes into realgar. After another thousand years the realgar becomes transformed into yellow gold.\"\nIn ancient and medieval China, mineralogy became firmly tied to empirical observations in pharmaceutics and medicine. For example, the famous horologist and mechanical engineer Su Song (1020–1101 AD) of the Song Dynasty (960–1279 AD) wrote of mineralogy and pharmacology in his \"Ben Cao Tu Jing\" of 1070. In it he created a systematic approach to listing various different minerals and their use in medicinal concoctions, such as all the variously known forms of mica that could be used to cure various ills through digestion. Su Song also wrote of the subconchoidal fracture of native cinnabar, signs of ore beds, and provided description on crystal form. Similar to the ore channels formed by circulation of ground water mentioned above with the German scientist Agricola, Su Song made similar statements concerning copper carbonate, as did the earlier \"Ri Hua Ben Cao\" of 970 AD with copper sulfate.\n\nThe Yuan Dynasty scientist Zhang Si-xiao (died 1332 AD) provided a groundbreaking treatise on the conception of ore beds from the circulation of ground waters and rock fissures, two centuries before Georgius Agricola would come to similar conclusions. In his \"Suo-Nan Wen Ji\", he applies this theory in describing the deposition of minerals by evaporation of (or precipitation from) ground waters in ore channels.\n\nIn addition to alchemical theory posed above, later Chinese writers such as the Ming Dynasty physician Li Shizhen (1518–1593 AD) wrote of mineralogy in similar terms of Aristotle's metaphysical theory, as the latter wrote in his pharmaceutical treatise \"Běncǎo Gāngmù\" (本草綱目, \"Compendium of Materia Medica\", 1596). Another figure from the Ming era, the famous geographer Xu Xiake (1587–1641) wrote of mineral beds and mica schists in his treatise. However, while European literature on mineralogy became wide and varied, the writers of the Ming and Qing dynasties wrote little of the subject (even compared to Chinese of the earlier Song era). The only other works from these two eras worth mentioning were the \"Shi Pin\" (Hierarchy of Stones) of Yu Jun in 1617, the \"Guai Shi Lu\" (Strange Rocks) of Song Luo in 1665, and the \"Guan Shi Lu\" (On Looking at Stones) in 1668. However, one figure from the Song era that is worth mentioning above all is Shen Kuo.\n\nThe medieval Chinese Song Dynasty statesman and scientist Shen Kuo (1031-1095 AD) wrote of his land formation theory involving concepts of mineralogy. In his \"Meng Xi Bi Tan\" (梦溪笔谈; \"Dream Pool Essays\", 1088), Shen formulated a hypothesis for the process of land formation (geomorphology); based on his observation of marine fossil shells in a geological stratum in the Taihang Mountains hundreds of miles from the Pacific Ocean. He inferred that the land was formed by erosion of the mountains and by deposition of silt, and described soil erosion, sedimentation and uplift. In an earlier work of his (circa 1080), he wrote of a curious fossil of a sea-orientated creature found far inland. It is also of interest to note that the contemporary author of the \"Xi Chi Cong Yu\" attributed the idea of particular places under the sea where serpents and crabs were petrified to one Wang Jinchen. With Shen Kuo's writing of the discovery of fossils, he formulated a hypothesis for the shifting of geographical climates throughout time. This was due to hundreds of petrified bamboos found underground in the dry climate of northern China, once an enormous landslide upon the bank of a river revealed them. Shen theorized that in pre-historic times, the climate of Yanzhou must have been very rainy and humid like southern China, where bamboos are suitable to grow.\n\nIn a similar way, the historian Joseph Needham likened Shen's account with the Scottish scientist Roderick Murchison (1792–1871), who was inspired to become a geologist after observing a providential landslide. In addition, Shen's description of sedimentary deposition predated that of James Hutton, who wrote his groundbreaking work in 1802 (considered the foundation of modern geology). The influential philosopher Zhu Xi (1130–1200) wrote of this curious natural phenomena of fossils as well, and was known to have read the works of Shen Kuo. In comparison, the first mentioning of fossils found in the West was made nearly two centuries later with Louis IX of France in 1253 AD, who discovered fossils of marine animals (as recorded in Joinville's records of 1309 AD).\n\nPerhaps the most influential mineralogy text in the 19th and 20th centuries was the \"Manual of Mineralogy\" by James Dwight Dana, Yale professor, first published in 1848. The fourth edition was entitled \"Manual of Mineralogy and Lithology\" (ed. 4, 1887). It became a standard college text, and has been continuously revised and updated by a succession of editors including W. E. Ford (13th-14th eds., 1912–1929), Cornelius S. Hurlbut (15th-21st eds., 1941–1999), and beginning with the 22nd by Cornelis Klein. The 23rd edition is now in print under the title \"Manual of Mineral Science (Manual of Mineralogy)\" (2007), revised by Cornelis Klein and Barbara Dutrow.\n\nEqually influential was Dana's \"System of Mineralogy\", first published in 1837, which has consistently been updated and revised. The 6th edition (1892) being edited by his son Edward Salisbury Dana. A 7th edition was published in 1944, and the 8th edition was published in 1997 under the title \"Dana's New Mineralogy: The System of Mineralogy of James Dwight Dana and Edward Salisbury Dana\", edited by R. V. Gaines \"et al.\"\n\n\n\n"}
{"id": "24013482", "url": "https://en.wikipedia.org/wiki?curid=24013482", "title": "Impact Field Studies Group", "text": "Impact Field Studies Group\n\nThe Impact Field Studies Group (IFSG) is a scientific organization emphasizing geologic field research of suspected and confirmed sites of impact craters and impact structures. The group is composed of researchers, professionals and students involved in study of impact sites. IFSG's web site is hosted by the Department of Earth and Planetary Sciences at University of Tennessee, Knoxville.\n\nIFSG member David Rajmon maintains for the organization a list which was originally called the Suspected Earth Impact Sites (SEIS) list from 2004 to 2009. It was then renamed to simply the Impact Database. The list classifies impact sites as confirmed, most probable, probable, possible, improbable, rejected and proposed (unevaluated). The list retains rejected entries because they tend to be submitted repeatedly.\n\nThe IFSG Impact Database accepts submissions of proposed new impact sites. However, they require submitters do some significant homework as listed below first.\n\nIFSG arranges field trips to impact-related sites. Past events have included\n\n"}
{"id": "48429332", "url": "https://en.wikipedia.org/wiki?curid=48429332", "title": "Isoazimuth", "text": "Isoazimuth\n\nThe isoazimuth is the locus of the points on the Earth's surface whose initial orthodromic course with respect to a fixed point is constant.\n\nThat is, if the initial orthodromic course Z from the starting point \"S\" to the fixed point \"X\" is 80 degrees, the associated isoazimuth is formed by all points whose initial orthodromic course with respect to point \"X\" is 80° (with respect to true north). The isoazimuth is written using the notation \"isoz(X, Z)\".\n\nThe isoazimuth is of use when navigating with respect to an object of known location, such as a radio beacon. A straight line called the \"azimuth line of position\" is drawn on a map, and on most common map projections this is a close enough approximation to the isoazimuth. On the Littrow projection, the correspondence is exact. This line is then crossed with an astronomical observation called a Sumner line, and the result gives an estimate of the navigator's position.\n\nLet X be a fixed point on the Earth of coordinates latitude: B2, and longitude: L2. In a terrestrial spherical model, the equation of isoazimuth line of initial course C passing through point S(B, L) is:\nformula_1\n\nIn this case the X point is the illuminating pole of the observed star, and the angle Z is its azimuth. The equation of the \"isoazimuthal\" curve for a star with coordinates (Dec, Gha), -\"Declination\" and \"Greenwich Hour Angle\"-, observed under an azimuth Z is given by:\nwhere lha is the local hour angle, and all points with latitude B and longitude L, they define the curve.\n\n\n"}
{"id": "343528", "url": "https://en.wikipedia.org/wiki?curid=343528", "title": "Kola Superdeep Borehole", "text": "Kola Superdeep Borehole\n\nThe Kola Superdeep Borehole () is the result of a scientific drilling project of the Soviet Union in the Pechengsky District, on the Kola Peninsula. The project attempted to drill as deep as possible into the Earth's crust. Drilling began on 24 May 1970 using the \"Uralmash-4E\", and later the \"Uralmash-15000\" series drilling rig. Boreholes were drilled by branching from a central hole. The deepest, SG-3, reached in 1989 and is the deepest artificial point on Earth, . The borehole is in diameter.\n\nIn terms of true vertical depth, it is the deepest borehole in the world. For two decades it was also the world's longest borehole in terms of measured depth along the well bore, until it was surpassed in 2008 by the Al Shaheen oil well in Qatar, and in 2011 by the Sakhalin-I Odoptu OP-11 Well (offshore from the Russian island of Sakhalin).\n\nThe main target depth was set at . On 6 June 1979, the world depth record held by the Bertha Rogers hole in Washita County, Oklahoma, United States, at was broken. In 1983, the drill passed , and drilling was stopped for about a year for numerous scientific and celebratory visits to the site. This idle period may have contributed to a breakdown on 27 September 1984: after drilling to , a section of the drill string twisted off and was left in the hole. Drilling was later restarted from .\n\nThe hole reached in 1989. In that year, the hole depth was expected to reach by the end of 1990 and by 1993. Because of higher-than-expected temperatures at this depth and location, instead of the expected , drilling deeper was deemed unfeasible and the drilling was stopped in 1992.\n\nThe Kola borehole penetrated about a third of the way through the Baltic Shield continental crust, estimated to be around deep, reaching Archaean rocks at the bottom. The project has been a site of extensive geophysical studies. The stated areas of study were the deep structure of the Baltic Shield, seismic discontinuities and the thermal regime in the Earth's crust, the physical and chemical composition of the deep crust and the transition from upper to lower crust, lithospheric geophysics, and to create and develop technologies for deep geophysical study.\n\nTo scientists, one of the more fascinating findings to emerge from this well is that no transition from granite to basalt was found at the depth of about , where the velocity of seismic waves has a discontinuity. Instead the change in the seismic wave velocity is caused by a metamorphic transition in the granite rock. In addition, the rock at that depth had been thoroughly fractured and was saturated with water, which was surprising. This water, unlike surface water, must have come from deep-crust minerals and had been unable to reach the surface because of a layer of impermeable rock.\n\nMicroscopic plankton fossils were found 6 kilometers (4 mi) below the surface.\n\nAnother unexpected discovery was a large quantity of hydrogen gas. The mud that flowed out of the hole was described as \"boiling\" with hydrogen.\n\nThe project ended in 1995 due to the dissolution of the Soviet Union. The site has since been abandoned, and is now an environmental hazard. The ruins of the site, however, are frequently visited by curious sightseers.\n\n\nThe Kola Superdeep Borehole was the longest and deepest borehole in the world for nearly 20 years. In May 2008, a new record for borehole length was established by the extended-reach drilling (ERD) well BD-04A, which was drilled by Transocean for Maersk Oil in the Al Shaheen Oil Field in Qatar. It was drilled to , with a record horizontal reach of in only 36 days.\n\nOn 28 January 2011, Exxon Neftegas Ltd., operator of the Sakhalin-I project, drilled the world's longest extended-reach well offshore on the Russian island of Sakhalin. It has surpassed the length of both the Al Shaheen well and the Kola borehole. The Odoptu OP-11 well reached a measured total length of and a horizontal displacement of . Exxon Neftegas completed the well in 60 days.\n\nOn 27 August 2012, Exxon Neftegas Ltd beat its own record by completing the Z-44 Chayvo well. This ERD well reached a measured total length of .\n\nIn terms of depth below the surface, the Kola Superdeep Borehole SG-3 retains the world record at in 1989 and is still the deepest artificial point on Earth.\n\n\n\n"}
{"id": "2020211", "url": "https://en.wikipedia.org/wiki?curid=2020211", "title": "Lacus Spei", "text": "Lacus Spei\n\nLacus Spei (Latin for \"Lake of Hope\") is a small lunar mare that is located in the northeastern part of the Moon's near side. To the north is the crater Mercurius and to the west-southwest lies Schumacher.\n\nThe selenographic coordinates of this feature are 43.0° N, 65.0° E, and it lies within a diameter of 77 km. The main part of the lake occupies a region averaging about 50 km in diameter, with an extension leading to the northeast. The surface has the same low albedo as the larger mare features on the Moon, becoming lighter in hue near the edges. The only feature on this bay is the circular, cup-shaped satellite crater Zeno P. The crater Zeno lies to the east-northeast, closer to the lunar limb.\n"}
{"id": "181908", "url": "https://en.wikipedia.org/wiki?curid=181908", "title": "List of Poaceae genera", "text": "List of Poaceae genera\n\nThe true grasses (Poaceae) are one of the largest plant families, with around 12,000 species and roughly 800 genera. They contain, among others, the cereal crop species and other plants of economic importance, such as the bamboos, and several important weeds.\n\nGrasses probably originated in the understory of tropical rainforests in the Late Cretaceous, but have since come to occupy a wide range of different habitats. Notably, they are the dominant species in grasslands, open habitats that cover around one fifth of the earth's terrestrial surface. The C photosynthetic pathway has evolved at least 22 times independently in the grasses; C species are more competitive than C plants in open habitats with high light intensity and warm temperatures.\n\nThe deeper relationships in the family have been resolved by recent molecular phylogenetic work. This has been translated into a modern classification which divides the grasses into twelve subfamilies and a number of tribes, with large tribes further divided into subtribes.\n\nAnomochlooideae, Pharoideae and Puelioideae are early diverging lineages containing only a few species. Most of the diversity falls into the two big BOP and PACMAD clades, which each contain roughly half of the family's species. C lineages have only evolved in the PACMAD clade, whereas many lineages in the BOP clade have evolved adaptations to cold climate.\n\nWhile the higher-level classification of the grasses is now relatively well understood, taxonomic efforts continue at the species and genera level, and with continuing phylogenetic research, a number of names is likely to change. The list of genera below is therefore likely to evolve with further study.\n"}
{"id": "5873120", "url": "https://en.wikipedia.org/wiki?curid=5873120", "title": "List of Sites of Special Scientific Interest in Buckinghamshire", "text": "List of Sites of Special Scientific Interest in Buckinghamshire\n\nBuckinghamshire is a county in south-east England. Its county town is Aylesbury, and it is surrounded by Northamptonshire to the north, Bedfordshire, Hertfordshire and Greater London to the east, Surrey and Berkshire to the south, and Oxfordshire to the west. Under Buckinghamshire County Council there are four districts, Aylesbury Vale, Chiltern, South Bucks and Wycombe, while Milton Keynes has a separate unitary borough council. Buckinghamshire has an area of 1874 km², and a population of 739,600.\n\nIn England, Sites of Special Scientific Interest (SSSIs) are designated by Natural England, which is responsible for protecting England's natural environment. Designation as an SSSI gives legal protection to the most important wildlife and geological sites. As of April 2016, there are 65 SSSIs in this Area of Search, 55 of which have been designated for biological interest and 10 for geological interest. Thirty are in the Chilterns Area of Outstanding Natural Beauty, three are in National Nature Reserves, four are in Special Areas of Conservation, and seventeen are managed by the Berkshire, Buckinghamshire and Oxfordshire Wildlife Trust.\n\n\n"}
{"id": "12247811", "url": "https://en.wikipedia.org/wiki?curid=12247811", "title": "List of diseases of the common bean", "text": "List of diseases of the common bean\n\nThis article is a list of diseases of common bean (\"Phaseolus vulgaris\").\n\n"}
{"id": "18003216", "url": "https://en.wikipedia.org/wiki?curid=18003216", "title": "List of environmental journals", "text": "List of environmental journals\n\nThis is a list of scholarly, peer-reviewed academic journals focused on the biophysical environment and/or humans' relations with it. Inclusion of journals focused on the built environment is appropriate. Included in this list are journals from a wide variety of interdisciplinary fields including from the environmental sciences, environmental social sciences, environmental humanities, etc.\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "46938174", "url": "https://en.wikipedia.org/wiki?curid=46938174", "title": "List of mountain peaks of Montana", "text": "List of mountain peaks of Montana\n\nThis article comprises three sortable tables of major mountain peaks of the U.S. State of Montana.\n\nThe summit of a mountain or hill may be measured in three principal ways:\n\nOf the highest major summits of Montana, three peaks exceed elevation and 44 peaks exceed elevation.\n\nOf the most prominent summits of Montana, four peaks are ultra-prominent summits with more than of topographic prominence and 39 peaks exceed of topographic prominence.\n\nOf the most isolated major summits of Montana, eight peaks exceed of topographic isolation .\n\n\n"}
{"id": "1765011", "url": "https://en.wikipedia.org/wiki?curid=1765011", "title": "List of national parks of Hungary", "text": "List of national parks of Hungary\n\nHungary has ten national parks which cover approximately 10 percent of the country's territory. The parks are managed by the National Parks of Hungary government agency (Hungarian: \"Nemzeti park igazgatóság\").\n\n\n"}
{"id": "8591434", "url": "https://en.wikipedia.org/wiki?curid=8591434", "title": "List of stars in Circinus", "text": "List of stars in Circinus\n\nThis is the list of notable stars in the constellation Circinus, sorted by decreasing brightness.\n\n\n"}
{"id": "498677", "url": "https://en.wikipedia.org/wiki?curid=498677", "title": "Lists of fictional species", "text": "Lists of fictional species\n\nThere are a number of lists of fictional species:\n\n\n\n\n\n\n\n"}
{"id": "53954980", "url": "https://en.wikipedia.org/wiki?curid=53954980", "title": "Mehmet Akif Ersoy Nature Park", "text": "Mehmet Akif Ersoy Nature Park\n\nMehmet Akif Ersoy Nature Park () is a nature park located in Sarıyer district of Istanbul Province, Turkey.\n\nSituated at Bahçeköy neighborhood of Sarıyer and inside the Belgrad Forest, it covers an area of . The area was declared a nature park by the Ministry of Environment and Forest in 2011, and is one of the nine nature parks inside the Belgrad Forest. The protected area is named in honor of Mehmet Akif Ersoy (1873–1936), poet, writer, academic, member of parliament, and the author of the Turkish National Anthem.\n\nThe nature park offers outdoor recreational activities such as picnicing, hiking and cycling for visitors on daily basis. There is an outdoor restaurant, an outdoor coffeehouse and playgrounds for children. Admission is charged for visitors.\n\nThe nature park has rich flora and fauna.\n\nThe flora of the area is generally composed of middle aged and old forest trees. In the forest section of the nature park, vegetation is found like oriental plane (\"Platanus orientalis\"), black pine (\"Pinus nigra\"), silver linden (\"Tilia argentea\"), hornbeam (\"Carpinus betulus\"), Hungarian oak (\"Quercus frainetto\"), sessile oak (\"Quercus petraea\"), common ash (\"Fraxinus excelsior\"), oriental beech (\"Fagus orientalis\"), alder (\"Alnus glutinosa\"), blackberry (\"Rubus\") and tree heath (\"Erica arborea\"). Plants seen around the coffeehouse and the activity area are oriental thuja (\"Platycladus orientalis\"), blackthorn (\"Prunus spinosa\"), evergreen spindle (\"Euonymus japonicus\"), Chinese photinia (\"Photinia serratifolia\"), bay laurel (\"Laurus nobilis\"), Australian laurel (\"Pittosporum tobira\"), goat willow (\"Salix caprea\"), butcher's-broom (\"Ruscus aculeatus\"), wild privet, (\"Ligustrum vulgare\"), common lavender (\"Lavandula angustifolia\"), purple dead-nettle (\"Lamium purpureum\") and dandelion (\"Taraxacum officinale\").\n\nMainly observed animals in the nature park are the mammal squirrel, \nthe reptile lizard and the bird species woodpecker, European goldfinch, magpie, parrow.\n\n"}
{"id": "1932328", "url": "https://en.wikipedia.org/wiki?curid=1932328", "title": "Mesoporous material", "text": "Mesoporous material\n\nA mesoporous material is a material containing pores with diameters between 2 and 50 nm, according to IUPAC nomenclature. For comparison, IUPAC defines microporous material as a material having pores smaller than 2 nm in diameter and macroporous material as a material having pores larger than 50 nm in diameter.\n\nTypical mesoporous materials include some kinds of silica and alumina that have similarly-sized mesopores. Mesoporous oxides of niobium, tantalum, titanium, zirconium, cerium and tin have also been reported. However, the flagship of mesoporous materials is mesoporous carbon, which has direct applications in energy storage devices. Mesoporous carbon has porosity within the mesopore range and this significantly increases the specific surface area. An other very common mesoporous material is Activated Carbon which is typically composed of a carbon framework with both mesoporosity and microporosity depending on the conditions under which it was synthesized.\n\nAccording to IUPAC, a mesoporous material can be disordered or ordered in a mesostructure. In crystalline inorganic materials, mesoporous structure noticeably limits the number of lattice units, and this significantly changes the solid-state chemistry. For example, the battery performance of mesoporous electroactive materials is significantly different from that of their bulk structure.\n\nA procedure for producing mesoporous materials (silica) was patented around 1970, and methods based on the Stöber process from 1968 were still in use in 2015. It went almost unnoticed and was reproduced in 1997. Mesoporous silica nanoparticles (MSNs) were independently synthesized in 1990 by researchers in Japan. They were later produced also at Mobil Corporation laboratories and named Mobil Crystalline Materials, or MCM-41. The initial synthetic methods did not permit to control the quality of the secondary level of porosity generated. It was only by employing quaternary ammonium cations and silanization agents during the synthesis that the materials exhibited a true level of hierarchical porosity and enhanced textural properties.\n\nSince then, research in this field has steadily grown. Notable examples of prospective industrial applications are catalysis, sorption, gas sensing, ion exchange, optics, and photovoltaics.\n\nIt should be taken into account that this mesoporosity refers to the classification of nanoscale porosity, and mesopores may be defined differently in other contexts; for example, mesopores are defined as cavities with sizes in the range 30 μm–75 μm in the context of porous aggregations such as soil.\n\n"}
{"id": "5717049", "url": "https://en.wikipedia.org/wiki?curid=5717049", "title": "Milarepa's Cave", "text": "Milarepa's Cave\n\nMilarepa's Cave or Namkading Cave is a cave where the great Tibetan Buddhist philosopher, and Vajrayana Mahasiddha, Milarepa (c. 1052—c. 1135 CE), spent many years of his life in the eleventh century, 11 km north of the town of Nyalam, below the roadside and above the Matsang river in Nyalam County, Tibet.\n\nThere is also a cave associated with Milarepa in Nepal on the Annapurna Circuit at approximately 4000m just outside Manang. It is credited to have been the residence of the famous Saint Milarepa during his stay in modern-day northern Nepal. This site also includes a holy spring, gompa, and bow from the local archer who met and tried to kill Milarepa. In the classical songs of Milarepa, he sings of a deer, a dog and a hunter, the chain of causation and compassion. In local tradition this is the site of this famous tale. The cave is located beyond the Gompa, the locals pray from the edge of a glacial moraine in directly line of sight of the cave as its approach directly is on steep scree slope.\n\nA path leads down from the roadside through the village and down a hillside where there is a small monastery (gompa) named Nyanang Pelgye Ling Monastery, or Phelgyeling which is built around the cave. The Assembly Hall of the monastery has the statue of Buddha Shakyamuni in center which is 7feet in height.\n\nThe monastery used to be a Kagyupa but, later the monastery was converted to Gelugpa by H.H.5th Dalai Lama, Later Phelgyeling Monastery was affiliated with the Gelugpa Sera Monastery in Lhasa.\n\nMilarepa's cave, which overlooks the entrance to the hidden valley of Lapchi Gang, is entered from the gompa's vestibule. Pilgrim's offerings of decorated stones along the path and sweet-smelling herbs and wild flowers growing all around make this a place of great peace and beauty. The cave itself is kept as a shrine by two monks, guarding a statue of Milarepa enclosed in a glass case. In the cave is an impression in the rock attributed to Milarepa's sitting meditation posture and a hand print said to have been created when Milarepa helped Rechungpa (1083/4-1161 CE), his student, use a boulder to prop up the ceiling. There are images of Milarepa, Tsongkhapa, and Shri Devi, a protectress whose mule is said to have left a footprint in the stone when she visited Milarepa in a vision.\n\nRestoration work within the cave and the monastery was undertaken by artists and craftsmen from Nepal and was financed in the 1970s by the Chinese government, which has been in military occupation of Tibet since 1950.\n\nThe cave and the Pelgye Ling temple have been the subject of Richard Gere's artistic photo work,\n\"Milarepa's Cave, Nyelam Pelgye Ling Temple, Tibet\" (1993).\n\n\n"}
{"id": "1517745", "url": "https://en.wikipedia.org/wiki?curid=1517745", "title": "Monocrystalline whisker", "text": "Monocrystalline whisker\n\nA monocrystalline whisker is a filament of material that is structured as a single, defect-free crystal. Some typical whisker materials are graphite, alumina, iron, silicon carbide and silicon. Single-crystal whiskers of these (and some other) materials are known for having very high tensile strength (on the order of 10–20 GPa). Whiskers are used in some composites, but large-scale fabrication of defect-free whiskers is very difficult.\n\nPrior to the discovery of carbon nanotubes, single-crystal whiskers had the highest tensile strength of any materials known, and were featured regularly in science fiction as materials for fabrication of space elevators, arcologies, and other large structures. Despite showing great promise for a range of applications, their usage has been hindered by concerns over their effects on health when inhaled.\n\n\n"}
{"id": "2755147", "url": "https://en.wikipedia.org/wiki?curid=2755147", "title": "Myzocytosis", "text": "Myzocytosis\n\nMyzocytosis (from Greek: myzein, (') meaning \"to suck\" and kytos (') meaning \"container\", hence referring to \"cell\") is a method of feeding found in some heterotrophic organisms. It is also called \"cellular vampirism\" as the predatory cell pierces the cell wall and/or cell membrane of the prey cell with a feeding tube, the conoid, sucks out the cellular content and digests it.\n\nMyzocytosis is found in Myzozoa and also in some species of Ciliophora (both comprise the alveolates). A classic example of myzocytosis is the feeding method of the infamous predatory ciliate, \"Didinium\", where it is often depicted devouring a hapless \"Paramecium\". The suctorian ciliates were originally thought to have fed exclusively through myzocytosis, sucking out the cytoplasm of prey via superficially drinking straw-like pseudopodia. It is now understood that suctorians do not feed through myzocytosis, but actually, instead, manipulate and envenomate captured prey with their tentacle-like pseudopodia.\n\n"}
{"id": "1685252", "url": "https://en.wikipedia.org/wiki?curid=1685252", "title": "Northwest Atlantic Fisheries Organization", "text": "Northwest Atlantic Fisheries Organization\n\nThe Northwest Atlantic Fisheries Organization (NAFO) is an intergovernmental organization with a mandate to provide scientific advice and management of fisheries in the northwestern part of the Atlantic Ocean. NAFO is headquartered in Dartmouth, Nova Scotia, Canada.\n\nNAFO's overall objective is to contribute through consultation and cooperation to the optimum utilization, rational management and conservation of the fishery resources of the Convention Area.\n\nThe NAFO Convention on Future Multilateral Cooperation in the Northwest Atlantic Fisheries applies to most fishery resources of the Northwest Atlantic except salmon, tunas/marlins, whales, and sedentary species (e.g. shellfish).\n\nIn 2007 NAFO adopted an Amended Convention. The Convention must be ratified by 3/4 of the Members to come into effect. The original objective was modernized to include an ecosystem approach to fisheries management. It now expands beyond a sustainable use of the commercial northwest Atlantic fishery resources by committing to also protect the associated marine ecosystems from adverse fisheries effects.\n\nIn 1950, the fishing nations who operated fleets on the continental shelf of Canada and the United States began to recognize that fishing resources were finite and sought to establish an international multinational organization to provide for cooperation in preserving fish stocks. This organization, the International Commission for the Northwest Atlantic Fisheries, or ICNAF, was organized that year and mandated to use modern scientific methods in providing advice to member nations.\n\nThe ICNAF was supported by the \"International Convention for the Northwest Atlantic Fisheries\", however between 1973-1982 the United Nations and its member states negotiated the \"Third Convention of the Law of the Sea\" - one component of which, the concept of nations being allowed to declare an Exclusive Economic Zone (EEZ), was ratified and implemented in 1977. This extension of national fisheries jurisdiction over large areas of the continental shelf in this region by Canada, the United States, Greenland and St. Pierre and Miquelon required that the ICNAF be replaced with a new convention.\n\nIn 1979 ICNAF was replaced by the Northwest Atlantic Fisheries Organization (NAFO) which was established under the \"Convention on Future Multilateral Cooperation in the Northwest Atlantic Fisheries\". NAFO continues ICNAF's legacy under a mandate of providing scientific advice to member states with the premise of ensuring the conservation and management of fish stocks in the region. The NAFO mandate includes most fishery resources in the Northwest Atlantic, except salmon, tunas/marlins, whales, and sedentary species (e.g. shellfish).\n\nIn 2014, NAFO regulates eleven fish species (19 stocks) and a fishing ban (moratorium) is in place for 8 fish stocks belonging to five species (cod, American plaice, witch flounder, capelin and shrimp; for many of these stocks the fishing moratorium started more than a decade ago. Recently however, two stocks (redfish and cod) were reopened to fishing after a decade long moratoria. NAFO co-manages the pelagic redfish in Subarea 2 and Div. 1F-3K (off Greenland) with its sister organization, NEAFC.\n\nNAFO is implementing the Precautionary Approach that takes into account scientific uncertainties and thus allows for improved protection of the resources. Recently NAFO Adopted the MSE (Management Strategy Evaluation) for the Greenland halibut stock. This approach considers a survey based harvest control rule (HCR) in setting the TAC. NAFO was the first regional fisheries management organization to regulate the fishery of a species (thorny skate) belonging to skates or sharks (elasmobranchs).\n\nManagement measures of NAFO (see NAFO Conservation and Enforcement Measures are updated every year) include Total Allowable Catches (TACs) and quotas for regulated stocks, as well as restrictions for by-catch, minimum fish size, area, and time. In addition, NAFO imposes a number of control measures on the international fishery in the NAFO Regulatory Area, for example authorization to fish, vessel and gear requirements, controlled chartering arrangements, and product labelling requirements.\n\nNAFO also requires that fishing vessels record and communicate their catches and fishing efforts. Reliability of these records is enhanced by the Observer Program and the NAFO Vessel Monitoring System (VMS). The Observer Program requires each fishing vessel in the NAFO area to carry an independent observer on board. Inder the VMS each vessel fishing in the NAFO area is equipped with a satellite monitoring device that automatically and continuously (every two hours) reports the position.\n\nNAFO has also adopted Port State Control Measures that apply to landings or transshipments in ports of Contracting Parties by fishing vessels flying the flag of another Contracting Party. The provisions apply to landing or transshipment of fish caught in the Regulatory Area, or fish products originating from such fish, that have not been previously landed or offloaded at a port.\n\nNAFO’s joint (international) inspection and surveillance scheme includes frequent and indiscriminate at-sea inspections by authorized inspectors from NAFO member states. In addition to at-sea inspections, NAFO requires obligatory port inspections. It is the duty of flag states to perform follow-up investigations and to prosecute. NAFO publishes an annual Compliance Report.\n\nNAFO has identified 18 areas, within its Convention area, as being vulnerable to bottom contact gears and subsequently closed the areas to bottom fishing. These closures will be reviewed in 2014. NAFO has also delineated existing bottom fishing areas (footprint), in response to the United Nations General Assembly (UNGA Res. 61/105, paragraph 83) request for Regional Fisheries Management Organizations to regulate bottom fisheries that cause a significant adverse impact on vulnerable marine ecosystems.\n\nNAFO has been criticized for not being able to efficiently stop overfishing when many fish stocks in the Northwest Atlantic (both coastal and high-seas stocks) collapsed. Anger among fishermen in Eastern Canada, particularly Newfoundland and Labrador that was directed against international fishing vessels led to Canada exercising extra-territorial jurisdiction on a Spanish-flagged fishing trawler named \"Estai\" in 1995 during the so-called Turbot War.\n\nYear joined in brackets.\n\n\n"}
{"id": "21544", "url": "https://en.wikipedia.org/wiki?curid=21544", "title": "Nuclear fusion", "text": "Nuclear fusion\n\nIn nuclear physics, nuclear fusion is a reaction in which two or more atomic nuclei are combined to form one or more different atomic nuclei and subatomic particles (neutrons or protons). The difference in mass between the reactants and products is manifested as either the release or absorption of energy. This difference in mass arises due to the difference in atomic \"binding energy\" between the atomic nuclei before and after the reaction. Fusion is the process that powers active or \"main sequence\" stars, or other high magnitude stars.\n\nA fusion process that produces a nucleus lighter than iron-56 or nickel-62 will generally yield a net energy release. These elements have the smallest mass per nucleon and the largest binding energy per nucleon, respectively. Fusion of light elements toward these releases energy (an exothermic process), while a fusion producing nuclei heavier than these elements will result in energy retained by the resulting nucleons, and the resulting reaction is endothermic. The opposite is true for the reverse process, nuclear fission. This means that the lighter elements, such as hydrogen and helium, are in general more fusible; while the heavier elements, such as uranium, thorium and plutonium, are more fissionable. The extreme astrophysical event of a supernova can produce enough energy to fuse nuclei into elements heavier than iron.\n\nIn 1920, Arthur Eddington suggested hydrogen-helium fusion could be the primary source of stellar energy. Quantum tunneling was discovered by Friedrich Hund in 1929, and shortly afterwards Robert Atkinson and Fritz Houtermans used the measured masses of light elements to show that large amounts of energy could be released by fusing small nuclei. Building on the early experiments in nuclear transmutation by Ernest Rutherford, laboratory fusion of hydrogen isotopes was accomplished by Mark Oliphant in 1932. In the remainder of that decade, the theory of the main cycle of nuclear fusion in stars were worked out by Hans Bethe. Research into fusion for military purposes began in the early 1940s as part of the Manhattan Project. Fusion was accomplished in 1951 with the Greenhouse Item nuclear test. Nuclear fusion on a large scale in an explosion was first carried out on November 1, 1952, in the Ivy Mike hydrogen bomb test.\n\nResearch into developing controlled thermonuclear fusion for civil purposes began in earnest in the 1940s, and it continues to this day.\n\nThe release of energy with the fusion of light elements is due to the interplay of two opposing forces: the nuclear force, which combines together protons and neutrons, and the Coulomb force, which causes protons to repel each other. Protons are positively charged and repel each other by the Coulomb force, but they can nonetheless stick together, demonstrating the existence of another, short-range, force referred to as nuclear attraction. Light nuclei (or nuclei smaller than iron and nickel) are sufficiently small and proton-poor allowing the nuclear force to overcome repulsion. This is because the nucleus is sufficiently small that all nucleons feel the short-range attractive force at least as strongly as they feel the infinite-range Coulomb repulsion. Building up nuclei from lighter nuclei by fusion releases the extra energy from the net attraction of particles. For larger nuclei, however, no energy is released, since the nuclear force is short-range and cannot continue to act across longer atomic length scales. Thus, energy is not released with the fusion of such nuclei; instead, energy is required as input for such processes.\n\nFusion powers stars and produces virtually all elements in a process called nucleosynthesis. The Sun is a main-sequence star, and, as such, generates its energy by nuclear fusion of hydrogen nuclei into helium. In its core, the Sun fuses 620 million metric tons of hydrogen and makes 606 million metric tons of helium each second. The fusion of lighter elements in stars releases energy and the mass that always accompanies it. For example, in the fusion of two hydrogen nuclei to form helium, 0.7% of the mass is carried away in the form of kinetic energy of an alpha particle or other forms of energy, such as electromagnetic radiation.\n\nIt takes considerable energy to force nuclei to fuse, even those of the lightest element, hydrogen. When accelerated to high enough speeds, nuclei can overcome this electrostatic repulsion and brought close enough such that the attractive nuclear force is greater than the repulsive Coulomb force. The strong force grows rapidly once the nuclei are close enough, and the fusing nucleons can essentially \"fall\" into each other and the result is fusion and net energy produced. The fusion of lighter nuclei, which creates a heavier nucleus and often a free neutron or proton, generally releases more energy than it takes to force the nuclei together; this is an exothermic process that can produce self-sustaining reactions.\n\nEnergy released in most nuclear reactions is much larger than in chemical reactions, because the binding energy that holds a nucleus together is greater than the energy that holds electrons to a nucleus. For example, the ionization energy gained by adding an electron to a hydrogen nucleus is —less than one-millionth of the released in the deuterium–tritium (D–T) reaction shown in the adjacent diagram. The complete conversion of one gram of matter would release 9×10 joules of energy. Fusion reactions have an energy density many times greater than nuclear fission; the reactions produce far greater energy per unit of mass even though \"individual\" fission reactions are generally much more energetic than \"individual\" fusion ones, which are themselves millions of times more energetic than chemical reactions. Only direct conversion of mass into energy, such as that caused by the annihilatory collision of matter and antimatter, is more energetic per unit of mass than nuclear fusion.\n\nResearch into using fusion for the production of electricity has been pursued for over 60 years. Successful accomplishment of controlled fusion has been stymied by scientific and technological difficulties; nonetheless, important progress has been made. At present, controlled fusion reactions have been unable to produce break-even (self-sustaining) controlled fusion. The two most advanced approaches for it are magnetic confinement (toroid designs) and inertial confinement (laser designs).\n\nWorkable designs for a toroidal reactor that theoretically will deliver ten times more fusion energy than the amount needed to heat plasma to the required temperatures are in development (see ITER). The ITER facility is expected to finish its construction phase in 2019. It will start commissioning the reactor that same year and initiate plasma experiments in 2020, but is not expected to begin full deuterium-tritium fusion until 2027.\n\nThe US National Ignition Facility, which uses laser-driven inertial confinement fusion, was designed with a goal of break-even fusion; the first large-scale laser target experiments were performed in June 2009 and ignition experiments began in early 2011.\n\nAn important fusion process is the stellar nucleosynthesis that powers stars and the Sun. In the 20th century, it was recognized that the energy released from nuclear fusion reactions accounted for the longevity of stellar heat and light. The fusion of nuclei in a star, starting from its initial hydrogen and helium abundance, provides that energy and synthesizes new nuclei as a byproduct of the fusion process. Different reaction chains are involved, depending on the mass of the star (and therefore the pressure and temperature in its core).\n\nAround 1920, Arthur Eddington anticipated the discovery and mechanism of nuclear fusion processes in stars, in his paper \"The Internal Constitution of the Stars\". At that time, the source of stellar energy was a complete mystery; Eddington correctly speculated that the source was fusion of hydrogen into helium, liberating enormous energy according to Einstein's equation \"E = mc\". This was a particularly remarkable development since at that time fusion and thermonuclear energy, and even that stars are largely composed of hydrogen (see metallicity), had not yet been discovered. Eddington's paper, based on knowledge at the time, reasoned that:\n\nAll of these speculations were proven correct in the following decades.\n\nThe primary source of solar energy, and similar size stars, is the fusion of hydrogen to form helium (the proton-proton chain reaction), which occurs at a solar-core temperature of 14 million kelvin. The net result is the fusion of four protons into one alpha particle, with the release of two positrons and two neutrinos (which changes two of the protons into neutrons), and energy. In heavier stars, the CNO cycle and other processes are more important. As a star uses up a substantial fraction of its hydrogen, it begins to synthesize heavier elements. The heaviest elements are synthesized by fusion that occurs as a more massive star undergoes a violent supernova at the end of its life, a process known as supernova nucleosynthesis.\n\nA substantial energy barrier of electrostatic forces must be overcome before fusion can occur. At large distances, two naked nuclei repel one another because of the repulsive electrostatic force between their positively charged protons. If two nuclei can be brought close enough together, however, the electrostatic repulsion can be overcome by the quantum effect in which nuclei can tunnel through coulomb forces.\n\nWhen a nucleon such as a proton or neutron is added to a nucleus, the nuclear force attracts it to all the other nucleons of the nucleus (if the atom is small enough), but primarily to its immediate neighbours due to the short range of the force. The nucleons in the interior of a nucleus have more neighboring nucleons than those on the surface. Since smaller nuclei have a larger surface area-to-volume ratio, the binding energy per nucleon due to the nuclear force generally increases with the size of the nucleus but approaches a limiting value corresponding to that of a nucleus with a diameter of about four nucleons. It is important to keep in mind that nucleons are quantum objects. So, for example, since two neutrons in a nucleus are identical to each other, the goal of distinguishing one from the other, such as which one is in the interior and which is on the surface, is in fact meaningless, and the inclusion of quantum mechanics is therefore necessary for proper calculations.\n\nThe electrostatic force, on the other hand, is an inverse-square force, so a proton added to a nucleus will feel an electrostatic repulsion from \"all\" the other protons in the nucleus. The electrostatic energy per nucleon due to the electrostatic force thus increases without limit as nuclei atomic number grows.\n\nThe net result of the opposing electrostatic and strong nuclear forces is that the binding energy per nucleon generally increases with increasing size, up to the elements iron and nickel, and then decreases for heavier nuclei. Eventually, the binding energy becomes negative and very heavy nuclei (all with more than 208 nucleons, corresponding to a diameter of about 6 nucleons) are not stable. The four most tightly bound nuclei, in decreasing order of binding energy per nucleon, are , , , and . Even though the nickel isotope, , is more stable, the iron isotope is an order of magnitude more common. This is due to the fact that there is no easy way for stars to create through the alpha process.\n\nAn exception to this general trend is the helium-4 nucleus, whose binding energy is higher than that of lithium, the next heaviest element. This is because protons and neutrons are fermions, which according to the Pauli exclusion principle cannot exist in the same nucleus in exactly the same state. Each proton or neutron's energy state in a nucleus can accommodate both a spin up particle and a spin down particle. Helium-4 has an anomalously large binding energy because its nucleus consists of two protons and two neutrons, so all four of its nucleons can be in the ground state. Any additional nucleons would have to go into higher energy states. Indeed, the helium-4 nucleus is so tightly bound that it is commonly treated as a single quantum mechanical particle in nuclear physics, namely, the alpha particle.\n\nThe situation is similar if two nuclei are brought together. As they approach each other, all the protons in one nucleus repel all the protons in the other. Not until the two nuclei actually come close enough for long enough so the strong nuclear force can take over (by way of tunneling) is the repulsive electrostatic force overcome. Consequently, even when the final energy state is lower, there is a large energy barrier that must first be overcome. It is called the Coulomb barrier.\n\nThe Coulomb barrier is smallest for isotopes of hydrogen, as their nuclei contain only a single positive charge. A diproton is not stable, so neutrons must also be involved, ideally in such a way that a helium nucleus, with its extremely tight binding, is one of the products.\n\nUsing deuterium-tritium fuel, the resulting energy barrier is about 0.1 MeV. In comparison, the energy needed to remove an electron from hydrogen is 13.6 eV, about 7500 times less energy. The (intermediate) result of the fusion is an unstable He nucleus, which immediately ejects a neutron with 14.1 MeV. The recoil energy of the remaining He nucleus is 3.5 MeV, so the total energy liberated is 17.6 MeV. This is many times more than what was needed to overcome the energy barrier.\n\nThe reaction cross section σ is a measure of the probability of a fusion reaction as a function of the relative velocity of the two reactant nuclei. If the reactants have a distribution of velocities, e.g. a thermal distribution, then it is useful to perform an average over the distributions of the product of cross section and velocity. This average is called the 'reactivity', denoted <σv>. The reaction rate (fusions per volume per time) is <σv> times the product of the reactant number densities:\n\nIf a species of nuclei is reacting with a nucleus like itself, such as the DD reaction, then the product formula_2 must be replaced by formula_3.\n\nformula_4 increases from virtually zero at room temperatures up to meaningful magnitudes at temperatures of 10–100 keV. At these temperatures, well above typical ionization energies (13.6 eV in the hydrogen case), the fusion reactants exist in a plasma state.\n\nThe significance of formula_4 as a function of temperature in a device with a particular energy confinement time is found by considering the Lawson criterion. This is an extremely challenging barrier to overcome on Earth, which explains why fusion research has taken many years to reach the current advanced technical state.\n\nIf matter is sufficiently heated (hence being plasma), fusion reactions may occur due to collisions with extreme thermal kinetic energies of the particles. Thermonuclear weapons produce what amounts to an uncontrolled release of fusion energy. Controlled thermonuclear fusion energy has yet to be achieved.\n\nInertial confinement fusion (ICF) is a method aimed at releasing fusion energy by heating and compressing a fuel target, typically a pellet containing deuterium and tritium.\n\nInertial electrostatic confinement is a set of devices that use an electric field to heat ions to fusion conditions. The most well known is the fusor. Starting in 1999, a number of amateurs have been able to do amateur fusion using these homemade devices. Other IEC devices include: the Polywell, MIX POPS and Marble concepts.\n\nIf the energy to initiate the reaction comes from accelerating one of the nuclei, the process is called \"beam-target\" fusion; if both nuclei are accelerated, it is \"beam-beam\" fusion.\n\nAccelerator-based light-ion fusion is a technique using particle accelerators to achieve particle kinetic energies sufficient to induce light-ion fusion reactions. Accelerating light ions is relatively easy, and can be done in an efficient manner—requiring only a vacuum tube, a pair of electrodes, and a high-voltage transformer; fusion can be observed with as little as 10 kV between the electrodes. The key problem with accelerator-based fusion (and with cold targets in general) is that fusion cross sections are many orders of magnitude lower than Coulomb interaction cross sections. Therefore, the vast majority of ions expend their energy emitting bremsstrahlung radiation and the ionization of atoms of the target. Devices referred to as sealed-tube neutron generators are particularly relevant to this discussion. These small devices are miniature particle accelerators filled with deuterium and tritium gas in an arrangement that allows ions of those nuclei to be accelerated against hydride targets, also containing deuterium and tritium, where fusion takes place, releasing a flux of neutrons. Hundreds of neutron generators are produced annually for use in the petroleum industry where they are used in measurement equipment for locating and mapping oil reserves.\n\nMuon-catalyzed fusion is a fusion process that occurs at ordinary temperatures. It was studied in detail by Steven Jones in the early 1980s. Net energy production from this reaction has been unsuccessful because of the high energy required to create muons, their short 2.2 µs half-life, and the high chance that a muon will bind to the new alpha particle and thus stop catalyzing fusion.\n\nSome other confinement principles have been investigated.\n\nAntimatter-initialized fusion uses small amounts of antimatter to trigger a tiny fusion explosion. This has been studied primarily in the context of making nuclear pulse propulsion, and pure fusion bombs feasible. This is not near becoming a practical power source, due to the cost of manufacturing antimatter alone.\n\nPyroelectric fusion was reported in April 2005 by a team at UCLA. The scientists used a pyroelectric crystal heated from −34 to 7 °C (−29 to 45 °F), combined with a tungsten needle to produce an electric field of about 25 gigavolts per meter to ionize and accelerate deuterium nuclei into an erbium deuteride target. At the estimated energy levels, the D-D fusion reaction may occur, producing helium-3 and a 2.45 MeV neutron. Although it makes a useful neutron generator, the apparatus is not intended for power generation since it requires far more energy than it produces.\n\nHybrid nuclear fusion-fission (hybrid nuclear power) is a proposed means of generating power by use of a combination of nuclear fusion and fission processes. The concept dates to the 1950s, and was briefly advocated by Hans Bethe during the 1970s, but largely remained unexplored until a revival of interest in 2009, due to the delays in the realization of pure fusion.\nProject PACER, carried out at Los Alamos National Laboratory (LANL) in the mid-1970s, explored the possibility of a fusion power system that would involve exploding small hydrogen bombs (fusion bombs) inside an underground cavity. As an energy source, the system is the only fusion power system that could be demonstrated to work using existing technology. However it would also require a large, continuous supply of nuclear bombs, making the economics of such a system rather questionable.\n\nAt the temperatures and densities in stellar cores the rates of fusion reactions are notoriously slow. For example, at solar core temperature (\"T\" ≈ 15 MK) and density (160 g/cm), the energy release rate is only 276 μW/cm—about a quarter of the volumetric rate at which a resting human body generates heat. Thus, reproduction of stellar core conditions in a lab for nuclear fusion power production is completely impractical. Because nuclear reaction rates depend on density as well as temperature and most fusion schemes operate at relatively low densities, those methods are strongly dependent on higher temperatures. The fusion rate as a function of temperature (exp(−\"E\"/\"kT\")), leads to the need to achieve temperatures in terrestrial reactors 10–100 times higher temperatures than in stellar interiors: \"T\" ≈ 0.1–1.0×10 K.\n\nIn artificial fusion, the primary fuel is not constrained to be protons and higher temperatures can be used, so reactions with larger cross-sections are chosen. Another concern is the production of neutrons, which activate the reactor structure radiologically, but also have the advantages of allowing volumetric extraction of the fusion energy and tritium breeding. Reactions that release no neutrons are referred to as \"aneutronic\".\n\nTo be a useful energy source, a fusion reaction must satisfy several criteria. It must:\n\n\nFew reactions meet these criteria. The following are those with the largest cross sections:\n\nFor reactions with two products, the energy is divided between them in inverse proportion to their masses, as shown. In most reactions with three products, the distribution of energy varies. For reactions that can result in more than one set of products, the branching ratios are given.\n\nSome reaction candidates can be eliminated at once. The D-Li reaction has no advantage compared to p- because it is roughly as difficult to burn but produces substantially more neutrons through - side reactions. There is also a p- reaction, but the cross section is far too low, except possibly when \"T\" > 1 MeV, but at such high temperatures an endothermic, direct neutron-producing reaction also becomes very significant. Finally there is also a p- reaction, which is not only difficult to burn, but can be easily induced to split into two alpha particles and a neutron.\n\nIn addition to the fusion reactions, the following reactions with neutrons are important in order to \"breed\" tritium in \"dry\" fusion bombs and some proposed fusion reactors:\n\nThe latter of the two equations was unknown when the U.S. conducted the Castle Bravo fusion bomb test in 1954. Being just the second fusion bomb ever tested (and the first to use lithium), the designers of the Castle Bravo \"Shrimp\" had understood the usefulness of Lithium-6 in tritium production, but had failed to recognize that Lithium-7 fission would greatly increase the yield of the bomb. While Li-7 has a small neutron cross-section for low neutron energies, it has a higher cross section above 5 MeV. The 15 Mt yield was 150% greater than the predicted 6 Mt and caused unexpected exposure to fallout.\n\nTo evaluate the usefulness of these reactions, in addition to the reactants, the products, and the energy released, one needs to know something about the cross section. Any given fusion device has a maximum plasma pressure it can sustain, and an economical device would always operate near this maximum. Given this pressure, the largest fusion output is obtained when the temperature is chosen so that <σv>/T is a maximum. This is also the temperature at which the value of the triple product \"nT\"τ required for ignition is a minimum, since that required value is inversely proportional to <σv>/T (see Lawson criterion). (A plasma is \"ignited\" if the fusion reactions produce enough power to maintain the temperature without external heating.) This optimum temperature and the value of <σv>/T at that temperature is given for a few of these reactions in the following table.\n\nNote that many of the reactions form chains. For instance, a reactor fueled with and creates some , which is then possible to use in the - reaction if the energies are \"right\". An elegant idea is to combine the reactions (8) and (9). The from reaction (8) can react with in reaction (9) before completely thermalizing. This produces an energetic proton, which in turn undergoes reaction (8) before thermalizing. Detailed analysis shows that this idea would not work well, but it is a good example of a case where the usual assumption of a Maxwellian plasma is not appropriate.\n\nAny of the reactions above can in principle be the basis of fusion power production. In addition to the temperature and cross section discussed above, we must consider the total energy of the fusion products \"E\", the energy of the charged fusion products \"E\", and the atomic number \"Z\" of the non-hydrogenic reactant.\n\nSpecification of the - reaction entails some difficulties, though. To begin with, one must average over the two branches (2i) and (2ii). More difficult is to decide how to treat the and products. burns so well in a deuterium plasma that it is almost impossible to extract from the plasma. The - reaction is optimized at a much higher temperature, so the burnup at the optimum - temperature may be low. Therefore, it seems reasonable to assume the but not the gets burned up and adds its energy to the net reaction, which means the total reaction would be the sum of (2i), (2ii), and (1):\n\nFor calculating the power of a reactor (in which the reaction rate is determined by the D-D step), we count the - fusion energy \"per D-D reaction\" as \"E\" = (4.03 MeV + 17.6 MeV)×50% + (3.27 MeV)×50% = 12.5 MeV and the energy in charged particles as \"E\" = (4.03 MeV + 3.5 MeV)×50% + (0.82 MeV)×50% = 4.2 MeV. (Note: if the tritium ion reacts with a deuteron while it still has a large kinetic energy, then the kinetic energy of the helium-4 produced may be quite different from 3.5 MeV, so this calculation of energy in charged particles is only an approximation of the average.) The amount of energy per deuteron consumed is 2/5 of this, or 5.0 MeV (a specific energy of about 225 million MJ per kilogram of deuterium).\n\nAnother unique aspect of the - reaction is that there is only one reactant, which must be taken into account when calculating the reaction rate.\n\nWith this choice, we tabulate parameters for four of the most important reactions\n\nThe last column is the neutronicity of the reaction, the fraction of the fusion energy released as neutrons. This is an important indicator of the magnitude of the problems associated with neutrons like radiation damage, biological shielding, remote handling, and safety. For the first two reactions it is calculated as (\"E\"-\"E\")/\"E\". For the last two reactions, where this calculation would give zero, the values quoted are rough estimates based on side reactions that produce neutrons in a plasma in thermal equilibrium.\n\nOf course, the reactants should also be mixed in the optimal proportions. This is the case when each reactant ion plus its associated electrons accounts for half the pressure. Assuming that the total pressure is fixed, this means that particle density of the non-hydrogenic ion is smaller than that of the hydrogenic ion by a factor 2/(\"Z\"+1). Therefore, the rate for these reactions is reduced by the same factor, on top of any differences in the values of <σv>/T. On the other hand, because the - reaction has only one reactant, its rate is twice as high as when the fuel is divided between two different hydrogenic species, thus creating a more efficient reaction.\n\nThus there is a \"penalty\" of (2/(Z+1)) for non-hydrogenic fuels arising from the fact that they require more electrons, which take up pressure without participating in the fusion reaction. (It is usually a good assumption that the electron temperature will be nearly equal to the ion temperature. Some authors, however discuss the possibility that the electrons could be maintained substantially colder than the ions. In such a case, known as a \"hot ion mode\", the \"penalty\" would not apply.) There is at the same time a \"bonus\" of a factor 2 for - because each ion can react with any of the other ions, not just a fraction of them.\n\nWe can now compare these reactions in the following table.\n\nThe maximum value of <σv>/T is taken from a previous table. The \"penalty/bonus\" factor is that related to a non-hydrogenic reactant or a single-species reaction. The values in the column \"inverse reactivity\" are found by dividing 1.24 by the product of the second and third columns. It indicates the factor by which the other reactions occur more slowly than the - reaction under comparable conditions. The column \"Lawson criterion\" weights these results with \"E\" and gives an indication of how much more difficult it is to achieve ignition with these reactions, relative to the difficulty for the - reaction. The next-to-last column is labeled \"power density\" and weights the practical reactivity by \"E\". The final column indicates how much lower the fusion power density of the other reactions is compared to the - reaction and can be considered a measure of the economic potential.\n\nThe ions undergoing fusion in many systems will essentially never occur alone but will be mixed with electrons that in aggregate neutralize the ions' bulk electrical charge and form a plasma. The electrons will generally have a temperature comparable to or greater than that of the ions, so they will collide with the ions and emit x-ray radiation of 10–30 keV energy, a process known as Bremsstrahlung.\n\nThe huge size of the Sun and stars means that the x-rays produced in this process will not escape and will deposit their energy back into the plasma. They are said to be opaque to x-rays. But any terrestrial fusion reactor will be optically thin for x-rays of this energy range. X-rays are difficult to reflect but they are effectively absorbed (and converted into heat) in less than mm thickness of stainless steel (which is part of a reactor's shield). This means the bremsstrahlung process is carrying energy out of the plasma, cooling it.\nThe ratio of fusion power produced to x-ray radiation lost to walls is an important figure of merit. This ratio is generally maximized at a much higher temperature than that which maximizes the power density (see the previous subsection). The following table shows estimates of the optimum temperature and the power ratio at that temperature for several reactions:\n\nThe actual ratios of fusion to Bremsstrahlung power will likely be significantly lower for several reasons. For one, the calculation assumes that the energy of the fusion products is transmitted completely to the fuel ions, which then lose energy to the electrons by collisions, which in turn lose energy by Bremsstrahlung. However, because the fusion products move much faster than the fuel ions, they will give up a significant fraction of their energy directly to the electrons. Secondly, the ions in the plasma are assumed to be purely fuel ions. In practice, there will be a significant proportion of impurity ions, which will then lower the ratio. In particular, the fusion products themselves \"must\" remain in the plasma until they have given up their energy, and \"will\" remain some time after that in any proposed confinement scheme. Finally, all channels of energy loss other than Bremsstrahlung have been neglected. The last two factors are related. On theoretical and experimental grounds, particle and energy confinement seem to be closely related. In a confinement scheme that does a good job of retaining energy, fusion products will build up. If the fusion products are efficiently ejected, then energy confinement will be poor, too.\n\nThe temperatures maximizing the fusion power compared to the Bremsstrahlung are in every case higher than the temperature that maximizes the power density and minimizes the required value of the fusion triple product. This will not change the optimum operating point for - very much because the Bremsstrahlung fraction is low, but it will push the other fuels into regimes where the power density relative to - is even lower and the required confinement even more difficult to achieve. For - and -, Bremsstrahlung losses will be a serious, possibly prohibitive problem. For -, p- and p- the Bremsstrahlung losses appear to make a fusion reactor using these fuels with a quasineutral, isotropic plasma impossible. Some ways out of this dilemma are considered—and rejected—in \"fundamental limitations on plasma fusion systems not in thermodynamic equilibrium\". This limitation does not apply to non-neutral and anisotropic plasmas; however, these have their own challenges to contend with.\n\nIn a classical picture, nuclei can be understood as hard spheres that repel each other through the Coulomb force but fuse once the two spheres come close enough for contact. Estimating the radius of an atomic nuclei as about one femtometer, the energy needed for fusion of two hydrogen is:\n\nformula_6\n\nThis would imply that for the core of the sun, which has a Boltzmann distribution with a temperature of around 1.4 keV, the probability hydrogen would reach the threshold is formula_7, that is, fusion would never occur. However, fusion in the sun does occur due to quantum mechanics.\n\nThe probability that fusion occurs is greatly increased compared to the classical picture, thanks to the smearing of the effective radius as the DeBroglie wavelength as well as quantum tunnelling through the potential barrier. To determine the rate of fusion reactions, the value of most interest is the cross section, which describes the probability that particle will fuse by giving a characteristic area of interaction. An estimation of the fusion cross sectional area is often broken into three pieces:\n\nWhere formula_9 is the geometric cross section, is the barrier transparency and is the reaction characteristics of the reaction. \n\nformula_9 is of the order of the square of the de-Broglie wavelength formula_11 where formula_12 is the reduced mass of the system and formula_13 is the center of mass energy of the system. \n\nMore detailed forms of the cross section can be derived through nuclear physics based models and R matrix theory.\n\nThe Naval Research Lab's plasma physics formulary gives the total cross section in barns as a function of the energy (in keV) of the incident particle towards a target ion at rest fit by the formula:\n\nformula_19 with the following coefficient values:\n\nBosch-Hale also reports a R-matrix calculated cross sections fitting observation data with Padé approximants. With energy in units of keV and cross sections in units of millibarn, the astrophysical factor has the form:\n\nformula_20, with the coefficient values: \n\n\n"}
{"id": "55254898", "url": "https://en.wikipedia.org/wiki?curid=55254898", "title": "O'Reilly General Hospital", "text": "O'Reilly General Hospital\n\nO'Reilly General Hospital was an army hospital created by the U.S. Government in February 1941. It was built in Springfield, Missouri to provide long-term medical care for returning soldiers of World War II. It became known as \"The hospital with a soul.\"\n\nHospital got its name from former Surgeon Gen. Robert Maitland O’Reilly. The location was chosen in February 1941. The commander of the post was Colonel George B. Foster Jr. who declared on May 15, 1941 that it should be a \"hospital with a soul.\" It was dedicated on November 8, 1941 with room for 1,000 beds. \n\nA 160-acre area of land was selected for the site. Already built on the land was a \"castle\" built by the Knights of Pythias. The castle was bought from the Knights of Pythias for $40,625 (half its assessed value) using an order of immediate possession. The castle was renovated and converted into the Enlisted Men's Service Club. The army built a ballroom, bowling alley and gymnasium inside of the building. Part of the basement was converted into a prison for Italian and German prisoners who required medical care. The hospital began to conduct training for field medics. Many of the patients treated had severe burns. The hospital began using new innovations in plastic surgery to help the burn victims. It became a primary provider of reconstructive surgery and physical therapy.\n\nDuring the winter of 1944, area residents worked to makes sure all the patents in the hospital had presents. \n\nOn September 30, 1946, at the end of the war, the hospital closed and ceased being operated as by the army. The United States Veterans Administration reopened it in February 1947. The closed it down again in August 1952. It was then declared surplus property by the government. The site remained unused until the General Council of the Assemblies of God bought most of the property in December 1954. Part of the grounds are now the armory for the Army reserve and National Guard.\n\nThe army initially built 91 buildings for the hospital. It was eventually expanded to 258 buildings and 3,426 beds. They offered an occupation classes for staff and patients to learn new skills and take high-school exams.\n\nIt was built between the roads of Division and Glenstone Avenue. The location is now occupied by the Army Reserve, Army National Guard and Evangel University.\n\nThe army Army Surgeon General recognized the army hospital as the \"best in the Nation.\" Over 100,000 patients were treated over its five-year period of operation. The hospital became a model for other army hospitals. The average cost per patient was five dollars a day. Average recovery time dropped to 24 days from and average of 35 days.\n"}
{"id": "22330549", "url": "https://en.wikipedia.org/wiki?curid=22330549", "title": "OSO 7", "text": "OSO 7\n\nOSO 7 or Orbiting Solar Observatory 7 (NSSDC ID: 1971-083A), before launch known as OSO H is the seventh in the series of American Orbiting Solar Observatory satellites launched by NASA between 1962 and 1975. OSO 7 was launched from Cape Canaveral on 29 September 1971 by a Delta N rocket into a 33.1° inclination, low-Earth (initially 321 by 572 km) orbit, and re-entered the Earth's atmosphere on 9 July 1974. \nIt was built by the Ball Brothers Research Corporation (BBRC), now known as Ball Aerospace, in Boulder Colorado.\n\nWhile the basic design of all the OSO satellites was similar, the OSO 7 was larger [total spacecraft mass was 635 kg (1397 lb)] than the OSO 1 through OSO 6, with a larger squared-off solar array in the non-rotating \"Sail\", and a deeper rotating section, the \"Wheel\".\n\nThe \"Sail\" portion of the spacecraft, which was stabilized to face the Sun in all the OSO series satellites, carried two instruments on OSO 7, which continuously viewed the Sun during orbit day.\nThese were:\n\n\nThe rotating \"Wheel\" component of the spacecraft, which provided overall gyroscopic stability to the satellite, carried four instruments, which looked radially outwards and scanned across the Sun every 2 seconds.\nTwo of these were solar-observing instruments, and the other two were cosmic X-ray instruments:\n\nAmong the notable scientific results from OSO 7 were:\n\n\nThe OSO 7 was nearly lost at launch, due to a loss of hydraulic pressure in the second-stage guidance control system ~7 seconds prior to SECO. The nominal plan was for the spacecraft to be separated from the second stage with the spin axis normal to the Sun direction, so that the sail could be oriented to the Sun, allowing the batteries to be fully charged on orbit. As it was, the orbit was slightly eccentric instead of circular, and the orientation of the spacecraft immediately after launch was unknown, so that the sail could not acquire Sun lock. The spacecraft was launched with its batteries fully charged, giving approximately 12 hours for the controllers, directed by NASA's John Thole, to recover before the spacecraft lost power and command ability. Several hours passed as engineers attempted to interpret the signal strength from the tumbling spacecraft in terms of its transmitting antenna pattern. Finally, an hour or two before the end, Thole decided to abandon caution and \"start slewing\", and by luck and skill, control was regained.\n\nBecause the resulting orbital apogee was ~572 km instead of the planned ~350 km for the nominal circular orbit, several times each day OSO 7 passed fairly deeply into the Van Allen radiation belts, so that bombardment by high energy protons made it somewhat radioactive. The activity then decayed slowly during other times of the day. The complexly varying instrument internal radioactivity complicated the analysis of data from the sensitive X-ray and gamma-ray instruments on board.\n\nThe flight spare for OSO H was later acquired by the U.S. Air Force, modified and re-instrumented, and then launched in 1979 as P78-1 (also known as Solwind), the satellite which was shot down by the USAF in a successful anti-satellite missile test in 1985. OSO 7 and P78-1 were not identical in appearance, but more similar to each other than either were to the earlier OSO 1 through OSO 6 spacecraft, or to the final OSO 8.\n\n\n\"The content of this article was adapted and expanded from NASA's HEASARC: Observatories OSO 7 and NASA's National Space Science Data Center: OSO 7 (Public Domain)\"\n"}
{"id": "39832745", "url": "https://en.wikipedia.org/wiki?curid=39832745", "title": "Offshore Energy and Jobs Act", "text": "Offshore Energy and Jobs Act\n\nThe Offshore Energy and Jobs Act () is a bill that was introduced into the United States House of Representatives during the 113th United States Congress. The Offshore Energy and Jobs Act would revise existing law governing the leasing, operation, and development of oil and natural gas resources available in the Outer Continental Shelf (OCS). The bill is primarily supported by Republicans and is opposed by President Barack Obama.\n\n\"This summary is based largely on the summary provided by the Congressional Research Service, a public domain source.\"\n\nThe Offshore Energy and Jobs Act would amend the Outer Continental Shelf Lands Act (OCSLA) to direct the United States Secretary of the Interior to implement a leasing program that includes at least 50% of the available unleased acreage within each Outer Continental Shelf (OCS) planning area considered to have the largest undiscovered, technically recoverable oil and natural gas resources, with an emphasis on offering the most geologically prospective parts of the planning area.\n\nThe act instructs the Secretary, in developing a five-year oil and gas leasing program, to determine a specified domestic strategic production goal for the development of oil and natural gas as a result of that program.\n\nIt would also require the Secretary to: (1) develop and submit a new five-year oil and gas leasing program, (2) conduct offshore oil and gas Lease Sale 220 within one year after enactment of this Act, (3) make replacement lease blocks available in the Virginia lease sale planning area that are acceptable for oil and gas exploration and production if the Secretary of Defense proposes deferral from a lease offering due to defense-related activities irreconcilable with mineral exploration and development.\n\nThe bill instructs the Secretary to conduct a lease sale within two years after enactment of the Act for areas off the coast of South Carolina that have the most geologically promising hydrocarbon resources and constituting at least 25% of the leasable area within the South Carolina offshore administrative boundaries.\n\nIt also directs the Secretary to: (1) offer for sale by December 31, 2014, leases of tracts in the Santa Maria and Santa Barbara/Ventura Basins of the Southern California OCS Planning Area; and (2) prepare a multisale environmental impact statement pursuant to the National Environmental Policy Act of 1969 for all lease sales required under this Act.\n\nThe Offshore Energy and Jobs Act, if passed, would also:\n\nH.R. 2231 would revise existing laws and policies regarding the development of oil and gas resources on the Outer Continental Shelf (OCS). It would direct the Department of the Interior (DOI) to adopt a new leasing plan for the 2015–2020 period, require auctions of leases in certain areas in the Atlantic and Pacific OCS, and reduce the department’s discretion regarding which regions would be included in future lease sales. Under this bill, some of the offsetting receipts from leases issued in newly available areas would be spent, without further appropriation, to make payments to states. Finally, H.R. 2231 would direct DOI to collect fees from certain firms that operate in the OCS and to implement various administrative reforms.\n\nCBO estimates that enacting H.R. 2231 would reduce net direct spending by $1.5 billion over the 2014–2023 period. Pay-as-you-go procedures apply because enacting the legislation would reduce direct spending. In addition, CBO estimates that implementing the bill would cost $40 million over the 2013–2018 period, assuming appropriation of the necessary amounts. Enacting this bill would not affect revenues.\n\nThe Offshore Energy and Jobs Act was introduced into the House on June 4, 2013 by Rep. Doc Hastings (R-WA). The bill was referred to the United States House Committee on Natural Resources and the United States House Natural Resources Subcommittee on Energy and Mineral Resources on June 4, 2013. The subcommittee held hearings on June 6 and 11, before discharging the bill back to the full committee on June 12, 2013. On June 12, the House Committee on Natural Resources marked-up the bill and ordered it reported in amended form by a vote of 23-18. On June 24, 2013, the bill was reported alongside House Report 113-125. On June 28, 2013, the House voted 235-186 in Roll Call Vote 304 to pass the Offshore Energy and Jobs Act. Sixteen Democrats voted in favor of the bill.\n\nIn a statement released on June 25, 2013, President of the United States Barack Obama issued a statement threatening to veto the Offshore Energy and Jobs Act if changes weren't made. In his statement, the President criticized the bill for setting \"unworkable deadlines,\" for reducing the net return to taxpayers of money from the leasing of public land, and for the details of the reorganization of the former Minerals Management Service. Three days later, the House passed the bill over the President's objections.\n\nThe House Republicans have three main talking points about their reasons for supporting the Offshore Energy and Jobs Act. Those three talking points are:\n\nDemocrats responded to these arguments by insisting that there were enough open areas for energy companies to lease from and denied that the President's plan had increased gas prices.\n\n\n"}
{"id": "786562", "url": "https://en.wikipedia.org/wiki?curid=786562", "title": "Polar easterlies", "text": "Polar easterlies\n\nThe polar easterlies (also Polar Hadley cells) are the dry, cold prevailing winds that blow from the high-pressure areas of the polar highs at the North and South Poles towards low-pressure areas within the Westerlies at high latitudes. Cold air subsides at the poles creating the high pressure, forcing an equatorward outflow of air; that outflow is then deflected westward by the Coriolis effect. Unlike the westerlies in the middle latitudes, the polar easterlies are often weak and irregular. These prevailing winds blow from the polar easterlies are one of the five primary wind zones, known as wind belts, that make up our atmosphere's circulatory system. This particular belt of wind begins at approximately 60 degrees north and south latitude and reaches to the poles. When air moves near the poles, cold temperatures shrink the air. This promotes air from warmer latitudes to flow into the area, causing a polar high-pressure zone. Air from this high-pressure zone then rushes toward the low-pressure zone surrounding the sub-polar region. The flow of air is altered by the Earth's rotation and deflected west, hence the name, between high- and low-pressure zones. Temperature and air pressure differentials and the earth's rotation combine to form global currents.\n\nAnother example of the phenomenon would be the trade winds, which reach from where the doldrums (areas that receive the largest levels of thermal radiation from the sun) leave off at about 5 degrees north and south latitude to as far as 30 degrees north and south. They are caused by air moving into the area from the subtropics to fill the void left by the doldrums' rising air. The trade winds blow steadily westward out of the northeast in the Northern Hemisphere and out of the southeast in the Southern Hemisphere. Sailors in earlier times, traveling by wind current and sail only, counted upon the reliability of the trade winds to help them along on their voyages. Similarly, they sought to avoid the doldrums, where the wind was so scant they risked coming to a complete stop.\n\n"}
{"id": "46517855", "url": "https://en.wikipedia.org/wiki?curid=46517855", "title": "Relaxor ferroelectric", "text": "Relaxor ferroelectric\n\nRelaxor ferroelectrics are ferroelectric materials that exhibit high electrostriction. , although they have been studied for over fifty years, the mechanism for this effect is still not completely understood, and is the subject of continuing research.\n\nExamples of relexor ferroelectrics include:\n"}
{"id": "4430013", "url": "https://en.wikipedia.org/wiki?curid=4430013", "title": "SAIDI", "text": "SAIDI\n\nThe System Average Interruption Duration Index (SAIDI) is commonly used as a reliability indicator by electric power utilities. SAIDI is the average outage duration for each customer served, and is calculated as:\n\nformula_1\n\nwhere formula_2 is the number of customers and formula_3 is the annual outage time for location formula_4, and formula_5 is the total number of customers served. In other words,\n\nformula_6\n\nSAIDI is measured in units of time, often minutes or hours. It is usually measured over the course of a year, and according to IEEE Standard 1366-1998 the median value for North American utilities is approximately 1.50 hours.\n\n"}
{"id": "10688242", "url": "https://en.wikipedia.org/wiki?curid=10688242", "title": "Santa Lucia Cloud Forest", "text": "Santa Lucia Cloud Forest\n\nSanta Lucia Cloud Forest () is a cloud forest reserve, located about 80 km northwest of Quito, in the province of Pichincha, in Ecuador. This is at the far south of the southern phase of the Choco-Andean Rainforest Corridor. Rainforest Concern works with the Santa Lucia co-operative, a community-based conservation organisation dedicated to conservation and to sustainable development so that they can make a modest living whilst conserving their remaining cloud forest.\n\nThe community owns over of montane cloud forest, of which about 80% is still in its prime, virgin state and the area has now been declared part of a Bosque Protector (Protected Forest). The community-based organisation formed by local campesino families manage their own resources and they have three basic aims:\n\n\n"}
{"id": "15839473", "url": "https://en.wikipedia.org/wiki?curid=15839473", "title": "Santé engagé", "text": "Santé engagé\n\nSanté engagé (in Mauritian creole: sante angaze) is a genre of Mauritian music which consists of singing protest songs. It is a way to protest against political/social oppression and repression through music. The genre mixes traditional Mauritian sega with Indian and Western influences.\n\nThe 1960s and 1970s (“\"les années de braise\"”) have been a very bustling period in the history of Mauritius. It has been a period of high interracial tension leading to more than 300 deaths due to interracial affrays. It has also been an indecisive period with regard to the cultural heirloom of the island with a deep reconsidering of the multicultural image.\n\nMauritius attained independence from Great Britain in 1968 without real exhilaration. A profound division prevailed between, on one side, the members of the independent movement and, on the other side, the anti-independentists. The island is faced with a vertiginously high level of unemployment and people still had in mind the recent interracial tensions peaking in 1964 and 1968.\n\nUnder the pretext of security and law and order, the newly formed government of Mauritius took highly repressive measures against opponents who intended to put forward ideas adverse to those of the governing regime.\n\nEven though independence was declared in 1968, British colonial presence and dominance remained clearly visible throughout the 1970s.\n\nFaced with such restrictions, youngsters (mostly university students) started to set up groups to debate on all ideological topics including politics and culture. Several students, among others Dev Virahsawmy, Jooneed Jeeroburkhan, Tirat Ramkisoon, Krishen Matis, Ah-Ken Wong, Kreeti Goburdhun, Vella Vengaroo created the Club of Students (\"Club des Etudiants\"). With the participation of Paul Bérenger, the Club of Students became the Club of Militant Students (\"Clubs des Etudiants militants\"). The Club later gave way to the creation of the Mauritian Militant Movement (in 1969) lauding national unity and social justice, inspired from the Libertarian Marxism movement.\n\nAlong with this political movement, youngsters started militating for cultural freedom. The government redoubled its repressive actions against them, using the police force and \"tappeurs\" for assaults against the youngsters. Many members of the MMM are jailed as political prisoners.\n\nThe young generation however never abdicated. Freedom of culture was a fundamental element of their resistance, specially the fight for the recognition of the Mauritian creole as a full-fledged language.\n\nIn the early 1970s they started to create cultural groups (in Mauritian creole: Grup Kiltirel) to debate about culture. Along with this new form of resistance, young artists started to write songs reflecting their thoughts. The Groups travelled throughout the island to meet people and make their voice heard. They also sang those songs with committed lyrics which, in a certain way, clearly expressed the reality of the moment. The theme of these songs where various: politics, injustice, oppression, etc. The very first santé engazé is believed to be \"Montagne Bertlo\" written and composed by Jooneed Jeeroburkhan in 1969 in Mauritius, after completing his higher studies in Canada (on a Commonwealth Scholarship).\n\nResistance and denunciation through music quickly became popular. It helped to transmit ideas more easily. It is this style of music that is called \"santé engazé\" (committed song).\n\nIt is commonly agreed that \"sante angaze\" really erupted with the Grup Kiltirel Soley Ruz (or Soley Ruz) formed in 1973. This cultural group, gathered many talented friends-artists, among others Bam Cuttayen, Micheline Virahsawmy, Rosemay Nelson, Nitish Joganah, Ram Joganah and Lélou Ménwar, Bruno Jacques, Paulo . They travelled throughout the island to sing their committed songs. Soley Ruz quickly became popular around the island.\n\nIn 1975, another grup kiltirel is formed (Grup Kiltirel Morisien) with, among others, Odile Chevreau, Siven Chinien, Habib Moosaheb, Rama Poonoosamy as members. Subsequently other groups were formed: \"Fangurin, Kler de linn, Fler kanne, Flamboyant, Sitronel, Ebène, Grup kiltirel IDP\".\n\nAfter the dissolution of Soley Ruz in 1979, many of its members started solo careers. Ram and Nitish Joganah created in the same year a new group, the Grup Latanier which would become the new symbol of \"sante angaze\". Santé engazé stayed extremely popular among the population during the 1980s, specially within the working-class.\n\n\"Sante angaze\" is a musical style which blends Sega music, Oriental music and Occidental music. It is a fusion of musical styles which were popular at the time of its creation, i.e., in the 1970s. In fact it perfectly reflected the frame of mind of those who were at the origins of this style, that is a mixture of the cultural diversity of the island, towards cultural unity.\n\nTraditional instruments of the sega music (ravanne, maravane, triangle) were thus mixed with oriental instruments (tabla, harmonium, sitar). Other instruments such as guitar, violin and drums completed that mixture.\n\nEven though the sonority of such committed songs are close to the sega music, the outcome is quite different insofar as the sega music has a more festive rhythm and lesser oriented on demonstrating and passing on a particular message.\n\nThe rhythm of \"sante angaze\" is generally slower than the sega and influenced by many other musical styles.\n\nDue to the context during which it has been created and its relationship with militancy, \"sante angaze\" (at least in its early years) is closely related to politics. The songs denounced political injustice and unfairness becoming intrinsically a form of militancy. The artists were themselves militants.\nSanté engazé has played a great part during general elections acting as the voice of the militant movement. The most notable example in this respect is the songs written and sung by Siven Chinien. His album \"soldat lalit\" was played (and is still played) throughout the general elections campaign, with the title song – \"Soldat Lalit\" – becoming the anthem of the MMM.\n\nSoley Ruz was created in 1973 at the initiative of various friends-artists, among others Bam Cuttayen, Micheline Virahsawmy, Rosemay Nelson, Nitish Joganah, Ram Joganah and Lélou Ménwar,Bruno Jacques,paulo, marino . They released three albums: \"Nuvo Kiltir\" in 1974, \"Buké Banané\" in 1976 and \"Later 7 Couler\" in 1978. The grup was dissolved in 1979 and most of its members started solo careers. Some continued in the field of santé engazé while others changed to adopt other musical styles.\n\nBam Cuttayen was a songwriter and singer who has deeply marked the socio-cultural history of Mauritius. He was born in 1951 in Quatre Bornes (Mauritius) and died in 2002. He dedicated his life to his fight against poverty and injustice. His songs reflect his battle. Bam Cuttayen released his first album \"Fler raket\" in 1980. Three other albums followed: Pei larm kuler in 1981, \"Zenfan later\" in 1986 and \"Brin soley\" in 1993. In 2003, a posthumous album (\"Parol envolé\") of his songs has been edited.\n\nSiven Chinien was a songwriter, singer and a militant activist. Even though he has composed songs on extremely various themes, his work is mostly remembered for his commitment to the Mauritian Militant Movement and his numerous militant songs in favour of the working-class.\n\nGrup Latanier was created by Ram and Nitish Joganah after they parted from Soley Ruz. The group has since remained very active and released numerous albums.\n\nZul Ramiah is a songwriter and singer. He began his musical career within the \"Grup kiltirel IDP\" before contributing to the albums of Siven Chinien (Soldat lalit) and Bam Cuttayen. Zul Ramiah also participated to Grup Latanier's first album \"krapo kriyé\". He then went on to start a solo career.\n\nGrup Zenfan Dodo is a group from Mahebourg. Led by singers Ruben Gopal and Kishan Pem, they have released several albums since the mid 90s.\n\n\"Sante angaze\" has radically transformed the musical landscape of Mauritius.\n\nEven though many people today consider \"sante angaze\" as a revolved musical style, the latter still exists and much appreciated. \"Santé engazé\" has simply evolved. The militantly oriented protests of its origins have gradually evolved and broaden to other subjects, not specially related to politics. The core of its philosophy however remains the same: raise people's consciousness through music. \"Sante angaze\" is today less related to the musical style than the lyrics of songs. In other words, what we call a \"sante angaze\" today is a song with a committed lyrics, passing a committed message, whatever the musical style. That's why today it is better to talk of \"lamizik angaze\" (committed music) rather than \"sante angaze\".\n\nThe evolution and influence of \"sante angaze\" can clearly be noticed within the new generation of artists in Mauritius. The music of Mauritius has diversified through years but the impact of santé engazé is visible insofar as the new generation of artists, whatever their musical style (sega music, seggae, reggae, ragga, dancehall, etc.), try to use music to transmit a specific message. Their inspirations remain wide-ranging: poverty, injustice, social wrongdoings, war, cultural unity, etc. These artists are the sons and heirs of santé engazé. However, the vast majority of them deny to be politically oriented preferring to remain neutral and denouncing through music.\n\n"}
{"id": "11999012", "url": "https://en.wikipedia.org/wiki?curid=11999012", "title": "Scribing (cartography)", "text": "Scribing (cartography)\n\nScribing was used to produce lines for cartographic map compilations before the use of computer based geographic information systems. Lines produced by manual scribing are sharp, clear and even. \n\nAn impression of the corrected compilation sheet is photographed onto scribe sheet material or drawn using pencil. While working over a light table, lines on the scribe sheet are traced with a metal or sapphire-tipped scribe tool to remove thin lines of translucent coating to produce a handmade negative image. This compares with drafting where an ink image is made on tracing paper by depositing ink using a pen to produce a positive image. Scribing produces a result superior to drafting, but is more time consuming.\n\nThe scribe sheet is made of a stable plastic base material and coated with a material which is designed for easy removal using a scribing tool to produce a cleanly cut line. Various colours are used, and orange is said to produce the least eye-strain for the cartographer.\n\nOne scribe sheet is produced for each map colour. Corrections can be made by \"duffing\" (re-coating) the scribe sheet with special duffing liquid. The detail can then be re-scribed. Printing plates are produced from the finished scribe sheets, one for each colour of the map. \n\nA tripod or trolley arrangement is used to hold the scribe stylus. A stylus of required thickness is set in the trolley and the surface material is removed by applying light pressure as the trolley is moved over the image. Care must be taken to ensure the base material is not gouged or distorted.\n\nEither a round point or chisel point stylus may be used. Chisel points must be set at right angles to the direction of movement. As well as single line gravers, double and triple lines can be produced with double and triple graver stylus. Small circles can be produced using motorised versions of scribing tools, and symbols, figures etc., can be produced using plastic or metal templates.\n\n‘Peelcoat’ is used to produce a negative of an area of detail such as a lake or forest. The border of the area is cut or scribed on the peelcoat and the coat of the sheet within the area is peeled off to produce a negative image.\n\nA stipple pattern can be used to produce an area symbol over the peeled surface. A stipple sheet with a simple repeating symbol (such as that for swamp or sand) is combined with the area by photographing the stipple onto the peelcoat.\n"}
{"id": "48685269", "url": "https://en.wikipedia.org/wiki?curid=48685269", "title": "South Atlantic–Gulf water resource region", "text": "South Atlantic–Gulf water resource region\n\nThe South Atlantic–Gulf water resource region is one of 21 major geographic areas, or regions, in the first level of classification used by the United States Geological Survey to divide and sub-divide the United States into successively smaller hydrologic units. These geographic areas contain either the drainage area of a major river, or the combined drainage areas of a series of rivers.\n\nThe South Atlantic–Gulf region, which is listed with a 2-digit hydrologic unit code (HUC) of 03, has an approximate size of , and consists of 18 subregions, which are listed with the 4-digit HUCs 0301 through 0318.\n\nThis region includes the drainage that ultimately discharges into: (a) the Atlantic Ocean within and between the states of Virginia and Florida; (b) the Gulf of Mexico within and between the states of Florida and Louisiana; and (c) the associated waters. The geographic area of the South Atlantic–Gulf region includes all of Florida and South Carolina, and parts of Alabama, Georgia, Louisiana, Mississippi, North Carolina, Tennessee, and Virginia.\n\n"}
{"id": "4876259", "url": "https://en.wikipedia.org/wiki?curid=4876259", "title": "The Secret Thoughts of Cats", "text": "The Secret Thoughts of Cats\n\nThe Secret Thoughts of Cats was written and illustrated by Steven Appleby, and first published in 1996. \n\nAlso known as the Infinite Subtlety of Cat Expressions, it focuses on the fact that cat expressions are always the same, no matter what the situation (apart from when asleep or dead). It has a picture of a cat with the same expression on every page, and a description of what it’s thinking or doing underneath. It finishes with ‘Afterthoughts’ of cats, a selection of observations made on cat behaviour. The first page is a picture of cat hairs, the last, cat hairs on cushions.<br>\nIt was written in memory of: Terry, Dibble, Sally, and for Jim.\n"}
{"id": "9241790", "url": "https://en.wikipedia.org/wiki?curid=9241790", "title": "Trench foot", "text": "Trench foot\n\nTrench foot is a medical condition caused by prolonged exposure of the feet to damp, unsanitary, and cold conditions. It is one of many immersion foot syndromes. The use of the word \"trench\" in the name of this condition is a reference to trench warfare, mainly associated with World War I.\n\nAffected feet may become numb, by erythema (turning red) or cyanosis (turning blue) as a result of poor blood supply, and may begin emanating a decaying odor if the early stages of necrosis (tissue death) set in. As the condition worsens, feet may also begin to swell. Advanced trench foot often involves blisters and open sores, which lead to fungal infections; this is sometimes called tropical ulcer (jungle rot). If left untreated, trench foot usually results in gangrene, which may require amputation. If trench foot is treated properly, complete recovery is normal, though it is marked by severe short-term pain when feeling returns.\n\nUnlike frostbite, trench foot does not require freezing temperatures; it can occur in temperatures up to 16° Celsius (about 60° Fahrenheit) and within as little as 13 hours. Exposure to these environmental conditions causes deterioration and destruction of the capillaries and leads to morbidity of the surrounding flesh. Excessive sweating (hyperhidrosis) has long been regarded as a contributory cause; unsanitary, cold, and wet conditions can also cause trench foot.\n\nTrench foot can be prevented by keeping the feet clean, warm, and dry. It was also discovered in World War I that a key preventive measure was regular foot inspections; soldiers would be paired and each made responsible for the feet of the other, and they would generally apply whale oil to prevent trench foot. If left to their own devices, soldiers might neglect to take off their own boots and socks to dry their feet each day, but if it were the responsibility of another, this became less likely. Later on in the war, instances of trench foot began to decrease, probably as a result of the introduction of the aforementioned measures; of wooden duckboards to cover the muddy, wet, cold ground of the trenches; and of the increased practice of troop rotation, which kept soldiers from prolonged time at the front.\n\nThe mainstay of treatment, like the treatment of gangrene, is surgical debridement, and often includes amputation. Self-treatment consists of changing socks two or three times a day and usage of plenty of talcum powder. Wherever possible, shoes and socks should be taken off, feet bathed for five minutes, patted dry, talcum powder applied and feet elevated to let air get to them.\n\nTrench foot was first documented by Napoleon's army in 1812. It became prevalent during the retreat from Russia and was first described by French army surgeon Dominique Jean Larrey. It was also a problem for soldiers engaged in trench warfare during the winters of World War I (hence the name).\n\nTrench foot made a reappearance in the British Army during the Falklands War in 1982. Some people were even reported to have developed trench foot at the 1998 and 2007 Glastonbury Festivals, the 2009 and 2013 Leeds Festivals, as well as the 2012 Download Festival, as a result of the sustained cold, wet, and muddy conditions at the events.\n\n"}
{"id": "21279257", "url": "https://en.wikipedia.org/wiki?curid=21279257", "title": "Öko-Institut", "text": "Öko-Institut\n\nThe Öko-Institut (Institute for Applied Ecology) (sometimes spelt Oeko-Institut) is a non-profit, private-sector environmental research institute with its head office in Freiburg im Breisgau, Germany.\n\nOriginally emerging in 1977 from the anti-nuclear movement, today the institute employs around 150 staff members in offices in Freiburg, Darmstadt and Berlin. It produces scientific studies and advises policymakers, environmental NGOs, institutions and companies. Some 100 projects, both national and international, are completed each year.\n\nThe institute’s thematic priorities are:\n\n\nThe institute is constituted as a registered association with around 3,000 members, of which almost 40 are local authorities. It finances its work primarily by acquiring third-party funding for its projects. This is complemented by membership fees and donations. Annual revenue is €8.5 million. The Öko-Institut has published an e-paper named eco@work since summer 2006.\n\nThe institute is a founding member of the EnergieVision association, which awards the ok-power label for green electricity products. It also launched the EcoTopTen consumer information campaign — the EcoTopTen web portal provides product recommendations for sustainable consumption.\n\n"}
{"id": "55619793", "url": "https://en.wikipedia.org/wiki?curid=55619793", "title": "ʻOumuamua", "text": "ʻOumuamua\n\nʻOumuamua () is the first interstellar object detected passing through the Solar System. Formally designated 1I/2017 U1, it was discovered by Robert Weryk using the Pan-STARRS telescope at Haleakala Observatory, Hawaii, on 19 October 2017, 40 days after it passed its closest point to the Sun. When first seen, it was about from Earth (about 85 times as far away as the Moon), and already heading away from the Sun.\n\nʻOumuamua is a small object, estimated to be about in size. It has a dark red color, similar to objects in the outer Solar System. ʻOumuamua showed no signs of a comet tail despite its close approach to the Sun, but has since undergone non-gravitational acceleration, potentially consistent with a push from solar radiation pressure. It has significant elongation and rotation rate, so it is thought to be metal-rich with a relatively high density. ʻOumuamua is tumbling, rather than smoothly rotating, and is moving so fast relative to the Sun that there is no chance it originated in the Solar System. It also means that ʻOumuamua cannot be captured into a solar orbit, so it will eventually leave the Solar System and resume traveling through interstellar space. ʻOumuamua's system of origin and the amount of time it has spent traveling amongst the stars are unknown.\n\nAs the first known object of its type, ʻOumuamua presented a unique case for the International Astronomical Union, which assigns designations for astronomical objects. Originally classified as comet C/2017 U1, it was later reclassified as asteroid A/2017 U1, due to the absence of a coma. Once it was unambiguously identified as coming from outside the Solar System, a new designation was created: I, for Interstellar object. ʻOumuamua, as the first object so identified, was designated 1I, with rules on the eligibility of objects for I-numbers, and the names to be assigned to these interstellar objects, yet to be codified. The object may be referred to as 1I; 1I/2017 U1; 1I/ʻOumuamua; or 1I/2017 U1 (ʻOumuamua).\n\nThe name comes (), and reflects the way this object is like a scout or messenger sent from the distant past to reach out to humanity. It roughly translates to \"first distant messenger\". The first character is a Hawaiian ʻokina, not an apostrophe, and is represented by a single quotation mark and pronounced as a glottal stop; the name was chosen by the Pan-STARRS team in consultation with Kaʻiu Kimura and Larry Kimura of the University of Hawaii at Hilo.\n\nBefore the official name was decided upon, the name \"Rama\" was suggested, the name given to an alien spacecraft discovered under similar circumstances in the 1973 science fiction novel \"Rendezvous with Rama\" by Arthur C. Clarke.\n\nObservations and conclusions concerning the trajectory of ʻOumuamua were primarily obtained with data from the Pan-STARRS1 Telescope, part of the Spaceguard Survey, and the Canada-France-Hawaii Telescope (CFHT), and its composition and shape from the Very Large Telescope and the Gemini South telescope in Chile, as well as the Keck II telescope in Hawaii. These were collected by Karen J. Meech, Robert Weryk and their colleagues and published in \"Nature\" on 20 November, 2017. Post announcement, the space-based telescopes Hubble and Spitzer joined in the observations.\nʻOumuamua is small and dark. It was not seen in STEREO HI-1A observations near its perihelion on 9 September 2017, limiting its brightness to ~13.5 mag. By the end of October, ʻOumuamua had already faded to apparent magnitude ~23, and by mid-December 2017, it was expected to be too faint and fast moving to be studied by even the largest ground-based telescopes.\n\nʻOumuamua was compared to the fictional alien spacecraft \"Rama\" because of its interstellar origin. Adding to the coincidence, both the real and the fictional objects are unusually elongated and limited in size. However, ʻOumuamua has a reddish hue and unsteady brightness, which are typical of asteroids.\n\nThe SETI Institute's radio telescope, the Allen Telescope Array, examined ʻOumuamua, but detected no unusual radio emissions. More detailed observations, using the Breakthrough Listen hardware and the Green Bank Telescope, were performed; the data were searched for narrowband signals and none were found. Given the close proximity to this interstellar object, limits were placed to putative transmitters with the extremely low power of 0.08 watts.\n\nʻOumuamua appears to have come from roughly the direction of Vega in the constellation Lyra. The incoming direction of motion of ʻOumuamua is 6° from the solar apex (the direction of the Sun's movement relative to local stars), which is the most likely direction for approaches from objects outside the Solar System. On 26 October, two precovery observations from the Catalina Sky Survey were found dated 14 and 17 October. A two-week observation arc had verified a strongly hyperbolic trajectory. It has a hyperbolic excess velocity (velocity at infinity, formula_1) of , its speed relative to the Sun when in interstellar space.\n\nBy mid-November, astronomers were certain that it was an interstellar object. Based on observations spanning 34 days, ʻOumuamua's orbital eccentricity is 1.20, the highest ever observed. An eccentricity exceeding 1.0 means an object exceeds the Sun's escape velocity, is not bound to the Solar System and may escape to interstellar space. While an eccentricity slightly above 1.0 can be obtained by encounters with planets, as happened with the previous record holder, C/1980 E1, ʻOumuamua's eccentricity is so high that it could not have been obtained through an encounter with any of the planets in the Solar System. Even undiscovered planets in the Solar System, if any should exist, could not account for ʻOumuamua's trajectory nor boost its speed to the observed value. For these reasons, ʻOumuamua can only be of interstellar origin.\n\nʻOumuamua entered the Solar System from north of the plane of the ecliptic. The pull of the Sun's gravity caused it to speed up until it reached its maximum speed of as it passed south of the ecliptic on 6 September and made a sharp turn upward at its closest approach to the Sun (perihelion) on 9 September at a distance of from the Sun, i.e., about 17% closer than Mercury's closest approach to the Sun. The object is now heading away from the Sun towards Pegasus at an angle of 66° from the direction of its approach.\n\nOn the outward leg of its journey through the Solar System, ʻOumuamua passed within the orbit of Earth on 14 October at a distance of approximately from Earth, and went back north of the ecliptic on 16 October and passed beyond the orbit of Mars on 1 November. It passed beyond Jupiter's orbit in May 2018, and will pass beyond Saturn's orbit in January 2019 and Neptune's orbit in 2022. As it leaves the Solar System it will be approximately right ascension 23h51m and declination +24°45', in Pegasus. It will continue to slow down until it reaches a speed of 26.33 km/s relative to the Sun, the same speed it had before its approach to the Solar System. It will take the object roughly 20,000 years to leave the Solar System completely.\n\nOn 27 June 2018, astronomers reported a non-gravitational acceleration to ʻOumuamua's trajectory, potentially consistent with a push from solar radiation pressure. Initial speculation as to the cause of this acceleration pointed to comet off-gassing, whereby portions of the object are ejected as the sun heats the surface. However, multiple objections have been raised to this possibility. Researchers point out that no such tail of gasses was ever observed following the object. Additionally, the anomalous acceleration was not observed when ʻOumuamua was passing at its closest to the sun as would be expected. A follow up analysis of these claims identifies that, were ʻOumuamua a comet, the off-gassing should have caused such an increase in rotational torque as to tear the object apart.\n\nAccounting for Vega's proper motion, it would have taken ʻOumuamua 600,000 years to reach the Solar System from Vega. But as a nearby star, Vega was not in the same part of the sky at that time. Astronomers calculate that one hundred years ago the asteroid was from the Sun and traveling at 26.33 km/s with respect to the Sun. This interstellar speed is very close to the mean motion of material in the Milky Way in the neighborhood of the Sun, also known as the local standard of rest (LSR), and especially close to the mean motion of a relatively close group of red dwarf stars. This velocity profile also indicates an extrasolar origin, but appears to rule out the closest dozen stars. In fact, the strong correlation between ʻOumuamua's velocity and the local standard of rest might mean that it has circulated the Milky Way several times and thus may have originated from an entirely different part of the galaxy.\n\nIt is unknown how long the object has been traveling among the stars. The Solar System is likely the first star system that ʻOumuamua has closely encountered since being ejected from its birth star system, potentially several billion years ago. It has been speculated that the object may have been ejected from a stellar system in one of the local kinematic associations of young stars (specifically, Carina or Columba) within a range of about 100 parsecs, some 45 million years ago. The Carina and Columba associations are now very far in the sky from the Lyra constellation, the direction from which ʻOumuamua came when it entered the Solar System. Others have speculated that it was ejected from a white dwarf system and that its volatiles were lost when its parent star became a red giant. About 1.3 million years ago the object may have passed within a distance of to the nearby star TYC 4742-1027-1, but its velocity is too high to have originated from that star system, and it probably just passed through the system's Oort cloud at a speed of . A more recent study (August 2018) using Gaia Data Release 2 has updated the possible past close encounters and has identified four stars that ʻOumuamua passed relatively close to and at moderately low velocities in the past few million years. \nThis study also identifies future close encounters of ʻOumuamua on its outgoing trajectory from the Sun.\n\nAccording to one hypothesis, ʻOumuamua could be a fragment from a tidally disrupted planet. If true, this would make ʻOumuamua a rare object, of a type much less abundant than most extrasolar \"dusty-snowball\" comets or asteroids.\n\nInitially, ʻOumuamua was announced as comet C/2017 U1 (PANSTARRS) based on a strongly hyperbolic trajectory. In an attempt to confirm any cometary activity, very deep stacked images were taken at the Very Large Telescope later the same day, but the object showed no presence of a coma. Accordingly, the object was renamed A/2017 U1, becoming the first comet ever to be re-designated as an asteroid. Once it was identified as an interstellar object, it was designated 1I/2017 U1, the first member of a new class of objects. The lack of a coma limits the amount of surface ice to a few square meters, and any volatiles (if they exist) must lie below a crust at least thick. It also indicates that the object must have formed within the frost line of its parent stellar system or have been in the inner region of that stellar system long enough for all near-surface ice to sublimate, as may be the case with damocloids. It is difficult to say which scenario is more likely due to the chaotic nature of small body dynamics, although if it formed in a similar manner to Solar System objects, its spectrum indicates that the latter scenario is true. Any meteoric activity from ʻOumuamua would have been expected to occur on 18 October 2017 coming from the constellation Sextans, but no activity was detected by the Canadian Meteor Orbit Radar.\n\nOn 27 June 2018, astronomers reported that ʻOumuamua was thought to be a mildly active comet, and not an asteroid, as previously thought. This was determined by measuring a non-gravitational boost to ʻOumuamua's acceleration, consistent with comet outgassing. However, studies submitted in October 2018 suggest that the object is neither an asteroid nor a comet.\n\nSpectra recorded by the William Herschel Telescope on 25 October showed that the object was featureless, and colored red like Kuiper belt objects. Spectra from the Hale Telescope showed a less-red color resembling comet nuclei or Trojans. Its spectrum is similar to that of D-type asteroids.\nʻOumuamua is rotating around a non-principal axis, a type of movement known as tumbling. This accounts for the various rotation periods reported, such as 8.10 hours, (±0.42 hours) (±0.02 hours) with a lightcurve amplitude of , whereas Meech et al. reported a rotation period of 7.3 hours and a lightcurve amplitude of 2.5 magnitudes. Most likely, ʻOumuamua was set tumbling by a collision in its system of origin, and remains tumbling since the time scale for dissipation of this motion is very long, at least a billion years.\n\nThe large variations on the light curves indicate that ʻOumuamua may be either a highly elongated object, comparable to or greater than the most elongated Solar System objects, or an extremely flat object, a pancake or oblate sphereoid. However, the size and shape have not been directly observed as ʻOumuamua appears as nothing more than a point source of light even in the most powerful telescopes. Neither the albedo or triaxial ellipsoid shape are precisely known. If cigar-shaped, the longest-to-shortest axis ratio could be 5:1 or greater. Assuming an albedo of 10% (typical for D-type asteroids) and a 6:1 ratio, ʻOumuamua has dimensions of approximately with an average diameter of about . According to astronomer David Jewitt, the object is physically unremarkable except for its highly elongated shape. Bannister et al. have suggested that it could also be a contact binary, although this may not be compatible with its rapid rotation. One speculation regarding its shape is that it is a result of a violent event (such as a collision or stellar explosion) that caused its ejection from its system of origin. JPL News reported that ʻOumuamua \"is up to one-quarter mile, , long and highly-elongated-perhaps 10 times as long as it is wide\".\n\nLight curve observations suggest the asteroid may be composed of dense metal-rich rock that has been reddened by millions of years of exposure to cosmic rays. It is thought that its surface contains tholins, which are irradiated organic compounds that are more common in objects in the outer Solar System and can help determine the age of the surface. This possibility is inferred from spectroscopic characterization and its dark and reddened color, and from the expected effects of interstellar radiation. Despite the lack of any cometary coma when it approached the Sun, it may still contain internal ice, hidden by \"an insulating mantle produced by long-term cosmic ray exposure\".\nIn December 2017, astronomer Avi Loeb of Harvard University, an adviser to the Breakthrough Listen Project, cited ʻOumuamua's unusually elongated shape as one of the reasons why the Green Bank Telescope in West Virginia would listen for radio emissions from it to see if there were any unexpected signs that it might be of artificial origin, although earlier limited observations by other radio telescopes such as the SETI Institute's Allen Telescope Array had produced no such results. On 13 December 2017, the Green Bank Telescope observed the asteroid for six hours across four bands of radio frequency. No radio signals from ʻOumuamua were detected in this very limited scanning range, but observations are ongoing.\n\nIn September 2018, astronomers described several possible home star systems from which ʻOumuamua may have originated.\n\nOn October 26, 2018, Avi Loeb and his postdoc Shmuel Bialy submitted a paper exploring the possibility of ʻOumuamua being an artificial thin solar sail accelerated by solar radiation pressure in an effort to help explain the object's non-gravitational acceleration. Other scientists have stated that the available evidence is insufficient to consider such a premise.\nIn response, Avi Loeb wrote an article detailing six anomalous properties of `Oumuamua that make it unusual, unlike any comets or asteroids seen before. A subsequent report on observations by the Spitzer Space Telescope set a tight limit on cometary outgassing of any carbon-based molecules and indicated that `Oumuamua is at least ten times more shiny than a typical comet. A detailed podcast produced by Rob Reid provides the full details about the differences between `Oumuamua and known comets.\n\nOn November 27, 2018, Avi Loeb and his undergraduate student at Harvard College, Amir Siraj, proposed a search for `Oumuamua-like objects which are trapped in the Solar System as a result of losing orbital energy through a close encounter with Jupiter. They identified 4 candidates for trapped interstellar objects that could be visited by dedicated missions, and pointed out that future sky surveys, such as with LSST, should find many more.\n\nʻOumuamua was at first thought to be traveling too fast for any existing spacecraft to reach. The Initiative for Interstellar Studies (i4is) launched Project Lyra to assess the feasibility of a mission to ʻOumuamua. Several options for sending a spacecraft to ʻOumuamua within a time-frame of 5 to 10 years were suggested. One option is using first a Jupiter flyby followed by a close solar flyby at in order to take advantage of the Oberth effect. Different mission durations and their velocity requirements were explored with respect to the launch date, assuming direct impulsive transfer to the intercept trajectory. Using a powered Jupiter flyby, a solar Oberth maneuver and Parker Solar Probe heat shield technology, a Falcon Heavy-class launcher would be able to launch a spacecraft of dozens of kilograms towards 1I/ʻOumuamua, if launched in 2021. More advanced options of using solar, laser electric, and laser sail propulsion, based on Breakthrough Starshot technology, have also been considered. The challenge is to get to the asteroid in a reasonable amount of time (and so at a reasonable distance from Earth), and yet be able to gain useful scientific information. To do this, decelerating the spacecraft at ʻOumuamua would be \"highly desirable, due to the minimal science return from a hyper-velocity encounter\". If the investigative craft goes too fast, it would not be able to get into orbit or land on the asteroid and would fly past it. The authors conclude that, although challenging, an encounter mission would be feasible using near-term technology. Seligman and Laughlin adopt a complementary approach to the Lyra study but also conclude that such missions, though challenging to mount, are both feasible and scientifically attractive.\n\nAstronomers estimate that several interstellar objects of extrasolar origin (like ‘Oumuamua) pass inside the orbit of Earth each year, and that 10,000 are passing inside the orbit of Neptune on any given day. If the estimate is correct, this provides future opportunities for studies of interstellar objects. However, with current space technology, close visits and orbital missions are challenging due to their high speeds, though not impossible.\n\nIn November 2018, Harvard astronomers reported that there should be hundreds of 'Oumuamua-size interstellar objects in the Solar System, based on calculated orbital characteristics, and presented several known examples, namely, 2011 SP25, 2017 RR2, 2017 SV13, and 2018 TL6. These are all orbiting the sun, but with unusual orbits, and are assumed to have been trapped at some occasion.\n\n\n"}
