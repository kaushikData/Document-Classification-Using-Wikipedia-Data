{"id": "53002047", "url": "https://en.wikipedia.org/wiki?curid=53002047", "title": "2011 in arthropod paleontology", "text": "2011 in arthropod paleontology\n\nThis list of fossil arthropods described in 2012 is a list of new taxa of trilobites, fossil insects, crustaceans, arachnids and other fossil arthropods of every kind that have been described during the year 2012. The list only includes taxa at the level of genus or species.\n"}
{"id": "13820259", "url": "https://en.wikipedia.org/wiki?curid=13820259", "title": "A-Q", "text": "A-Q\n\nGilbert Bani (born August 1, 1986) popularly known by his stage name A-Q, is a Nigerian rapper, recording artist, dancer, songwriter, and performer.\n\nA-Q was nominated for The Headies for best rap song artist 2016, for \"Agu Ji Ndi Men\".\n\nBorn into a family of six as the last child, A-Q real name Gilbert Bani was raised in Surulere where he has lived for most of his life.\n\nA-Q attended Kings College Lagos, and had his tertiary education at the University of Lagos. At a young age he started collecting hip-hop tracks, learning the lyrics and miming them. It didn’t take long before he started writing his own lyrics.\n\nA-Q signed his first record deal in 2001 with Big Leaf Records where he was for two years before he started working on his own in 2003. His first single \"(W)rap Nigeria\", produced by kraftmatics, debuted in 2004 followed by a music video. He subsequently released a full compilation album \"Listen and Understand\" in 2005 followed by another single, \"Things That We Do\", which got him an international online distribution deal. In 2006 A-Q put out a mixtape titled \"Maga Must Pay vol 1.\"\n\nA-Q took a break from music to finish his education in University of Lagos but still managed to release a single, \"Make Money\", and accompanying video in 2008 produced by Laylow and featuring Morachi and Xtrim off an online mixtape titled \"Love and Money.\"\n\nA-Q graduated in 2010 and returned to the entertainment industry, this time co-floating a record label, Hustle Inc, on whose platform he started Black Friday Twitter Freestyles, a weekly free download release. This got him online acknowledgement including from producer Don Jazzy who styled him “one of the sickest rappers alive”. He recorded and released \"The Past Present and Future\" album in December 2010 which had the singles \"Names\" and \"Champagne and Rum\". He released more music including \"Distractions\" featuring Vector in 2012, \"Machine Gun Flow\", \"555 (5beats 5verses and 5blessings)\", and \"Why\" featuring Pamela.\n\nHe released an EP in December 2012 titled \"Make Your Best Rapper Look Stupid.\" He is currently working on his sophomore album titled \"G.I.L.B.E.R.T (Grace and Glory).\"\n\n\n\n"}
{"id": "391341", "url": "https://en.wikipedia.org/wiki?curid=391341", "title": "Año Nuevo State Park", "text": "Año Nuevo State Park\n\nAño Nuevo State Park is a state park of California, USA, encompassing Año Nuevo Island and Año Nuevo Point, which are known for their pinniped rookeries. Located in San Mateo County, the low, rocky, windswept point juts out into the Pacific Ocean about south of San Francisco and the Golden Gate. Año Nuevo State Natural Reserve, formerly a separate unit of the California state park system, was merged into Año Nuevo State Park in October 2008. The coastal geographic center, or coastal-midpoint of California is located at the Northern end of this park at N 37°09′58″, W 122°21'40\", as the absolute geographic center of California falls at N 37°09′58″, W 119°26′58″W. \n\nThe reserve contains a diversity of plant communities, including old growth forest, freshwater marsh, red alder riparian forest and knobcone pine forest. Its four perennial streams support steelhead and coho salmon, and its wetlands are habitat to the rare San Francisco garter snake and California red-legged frog. Cultural resources include the remnants of a prehistoric Native American village site and a number of structures from the 19th century Cascade Ranch. In conjunction with adjacent and nearby public lands, the unit permits the protection of important regional ecological corridors.\n\nThe point remains undeveloped, much as Sebastián Vizcaíno saw it from his passing ship in 1603.\n\nThe Quiroste Ohlone people were the first known to inhabit the Año Nuevo area. After Mission Santa Cruz was founded in 1791, the Quiroste population plunged due to the various diseases that the Spanish had brought with them.\n\nThe Spanish maritime explorer Sebastian Vizcaino sailed by the point on January 3, 1603. His diarist and chaplain of the expedition, Father Antonio de la Ascensión, named it Punta de Año Nuevo (New Year's Point) for the day on which they sighted it in 1603. They had recently stopped in Monterey and had passed Año Nuevo while heading north.\n\nThe first European land exploration of Alta California was the Spanish Portolà expedition of 1760-70. Guided by Vizcaino's landmarks as it traveled north along the coast in 1769, the explorers recognized Point Año Nuevo. On the return journey to San Diego, the party camped at Año Nuevo Creek on November 19. Franciscan missionary Juan Crespi noted in his diary, \"We...halted on a steep rock, in sight of the point which we judged to be Año Nuevo, on the bank of an arroyo which empties into the sea.\"\n\nAfter the mid-19th century, developments occurred, including the development of Año Nuevo Island and agriculture in the surrounding region. Once home to a large population of elephant seals, hunters greatly reduced the seal population by overhunting the species. Since the mid-20th century, the seal population has had a rebound and multiplied greatly. Elephant seals can be seen today on the wildlife reserve.\n\nNorthern elephant seals, California sea lions, sea otters, and other marine mammals come ashore to rest, mate, and give birth in the sand dunes or on the beaches and offshore islands. Hundreds of thousands of people come to witness it each year.\n\nAño Nuevo State Park is the site of one of the largest mainland breeding colonies for the northern elephant seal (another is in Piedras Blancas State Marine Reserve and Marine Conservation Area— south near the town of Cambria and the San Simeon approach to Hearst Castle). The seals attracted so much interest that early reservations are needed during the winter breeding season. The males battle for mates on the beaches and the females give birth to their pups on the dunes.\n\nFrom mid-December through late March, daily access to the reserve is available on guided walks only. Most of the adult seals are gone by mid-March, leaving behind the weaned pups who remain through April. The elephant seals return to Año Nuevo's beaches during the spring (females) and summer (males) months to molt and can be observed during this time through a permit system. Reservations for a guided walk during the breeding season are available by phone.\n\nA Visitor Center features natural history exhibits and a bookstore offering educational items such as books, postcards and posters. Restrooms, drinking water and picnic tables are available near the Visitor Center only. Food and beverages are not sold at the reserve, although drinking water is available.\n\nThe parking lot is just west of California Route 1 at .\n\nThe former Cascade Ranch unit of the reserve includes steep unmarked hikes in the coastal hills inland of the highway.\n\nAño Nuevo State Marine Conservation Area and Greyhound Rock State Marine Conservation Area are two adjoining marine protected areas off the coast of Año Nuevo State Park. Like underwater parks, these marine protected areas help conserve ocean wildlife and marine ecosystems.\n\nPigeon Point Light Station State Historic Park, Butano State Park and numerous other parks and beaches are within of Año Nuevo.\n\n\n"}
{"id": "34850627", "url": "https://en.wikipedia.org/wiki?curid=34850627", "title": "Beaver Creek (Manitoba)", "text": "Beaver Creek (Manitoba)\n\nBeaver Creek may refer to one of nine rivers in Manitoba, Canada:\n\nThere is also an unincorporated place on Lake Winnipeg and Manitoba Provincial Road 234 called Beaver Creek; it is near the mouth of the Beaver Creek with CGNDB Unique Identifier GABZL. Beaver Creek Provincial Park is also nearby.\n\n"}
{"id": "184495", "url": "https://en.wikipedia.org/wiki?curid=184495", "title": "Biefeld–Brown effect", "text": "Biefeld–Brown effect\n\nThe Biefeld–Brown effect is an electrical phenomenon that produces an ionic wind that transfers its momentum to surrounding neutral particles. It describes a force observed on an asymmetric capacitor when high voltage is applied to the capacitor's electrodes. Once suitably charged up to high DC potentials, a thrust from the negative terminal to the positive terminal is generated. The effect was named by inventor Thomas Townsend Brown who claimed that he did a series of experiments with professor of astronomy Paul Alfred Biefeld, a former teacher of Brown whom Brown claimed was his mentor and co-experimenter at Denison University in Ohio.\n\nThe use of an asymmetric capacitor, with the negative electrode being larger than the positive electrode, allowed for more thrust to be produced in the direction from the low-flux to the high-flux region compared to a conventional capacitor. These asymmetric capacitors became known as Asymmetrical Capacitor Thrusters (ACT). The Biefeld–Brown effect can be observed in ionocrafts and lifters, which utilize the effect to produce thrust in the air without requiring any combustion or moving parts.\n\nIn his 1960 patent titled \"Electrokinetic Apparatus,\" Brown refers to electrokinesis to describe the Biefeld–Brown effect, linking the phenomenon to the field of electrohydrodynamics (EHD). Brown also believed the Biefeld–Brown effect could produce an anti-gravity force, referred to as \"electrogravitics\" based on it being an electricity/gravity phenomenon. However, there is little evidence that supports Brown's claim on the effect's anti-gravity properties.\n\nThe \"Biefeld–Brown effect\" was the name given to a phenomenon observed by Thomas Townsend Brown while he was experimenting with X-ray tubes during the 1920s while he was still in high school. When he applied a high voltage electrical charge to a Coolidge tube that he placed on a scale, Brown noticed a difference in the tubes mass depending on orientation, implying some kind of net force. This discovery caused him to assume that he had somehow influenced gravity electronically and led him to design a propulsion system based on this phenomenon. On 15 April 1927, he applied for a patent, entitled \"Method of Producing Force or Motion,\" that described his invention as an electrical-based method that could control gravity to produce linear force or motion. In 1929, Brown published an article for the popular American magazine \"Science and Invention\", which detailed his work. The article also mentioned the \"gravitator,\" an invention by Brown which produced motion without the use of electromagnetism, gears, propellers, or wheels, but instead using the principles of what he called \"electro-gravitation.\" He also claimed that the asymmetric capacitors were capable of generating mysterious fields that interacted with the Earth's gravitational pull and envisioned a future where gravitators would propel ocean liners and even space cars. At some point this effect also gained the moniker \"Biefeld–Brown effect\", probably coined by Brown to claim Denison University professor of physics and astronomy Paul Alfred Biefeld as his mentor and co-experimenter. Brown attended Denison for a year before he dropped out and records of him even having an association with Biefeld are sketchy at best.\n\nBrown filed another patent in 1960 that detailed the physics of the Biefeld–Brown effect, making the following claims:\n\n\nIn 1965, Brown filed a patent that claimed that a net force on the asymmetric capacitor can exist even in a vacuum. However, there is little experimental evidence that serves to validate his claims.\n\nThe effect is generally believed to rely on corona discharge, which allows air molecules to become ionized near sharp points and edges. Usually, two electrodes are used with a high voltage between them, ranging from a few kilovolts and up to megavolt levels, where one electrode is small or sharp, and the other larger and smoother. The most effective distance between electrodes occurs at an electric potential gradient of about 10 kV/cm, which is just below the nominal breakdown voltage of air between two sharp points, at a current density level usually referred to as the saturated corona current condition. This creates a high field gradient around the smaller, positively charged electrode. Around this electrode, ionization occurs, that is, electrons are stripped from the atoms in the surrounding medium; they are literally pulled right off by the electrode's charge.\n\nThis leaves a cloud of positively charged ions in the medium, which are attracted to the negative smooth electrode by Coulomb's Law, where they are neutralized again. This produces an equally scaled opposing force in the lower electrode. This effect can be used for propulsion (see EHD thruster), fluid pumps and recently also in EHD cooling systems. The velocity achievable by such setups is limited by the momentum achievable by the ionized air, which is reduced by ion impact with neutral air. A theoretical derivation of this force has been proposed (see the external links below).\n\nHowever, this effect works using either polarity for the electrodes: the small or thin electrode can be either positive or negative, and the larger electrode must have the opposite polarity. On many experimental sites it is reported that the thrust effect of a lifter is actually a bit stronger when the small electrode is the positive one. This is possibly an effect of the differences between the ionization energy and electron affinity energy of the constituent parts of air; thus the ease of which ions are created at the 'sharp' electrode.\n\nAs air pressure is removed from the system, several effects combine to reduce the force and momentum available to the system. The number of air molecules around the ionizing electrode is reduced, decreasing the quantity of ionized particles. At the same time, the number of impacts between ionized and neutral particles is reduced. Whether this increases or decreases the maximum momentum of the ionized air is not typically measured, although the force acting upon the electrodes reduces, until the glow discharge region is entered. The reduction in force is also a product of the reducing breakdown voltage of air, as a lower potential must be applied between the electrodes, thereby reducing the force dictated by Coulomb's Law.\n\nDuring the glow discharge region, the air becomes a conductor. Though the applied voltage and current will propagate at nearly the speed of light, the movement of the conductors themselves is almost negligible. This leads to a Coulomb force and change of momentum so small as to be zero.\n\nBelow the glow discharge region, the breakdown voltage increases again, whilst the number of potential ions decreases, and the chance of impact lowers. Experiments have been conducted and found to both prove and disprove a force at very low pressure. It is likely that the reason for this is that at very low pressures, only experiments which used very large voltages produced positive results, as a product of a greater chance of ionization of the extremely limited number of available air molecules, and a greater force from each ion from Coulomb's Law; experiments which used lower voltages have a lower chance of ionization and a lower force per ion. Common to positive results is that the force observed is small in comparison to experiments conducted at standard pressure.\n\nBrown believed that his large, high voltage, high capacity capacitors produced an electric field strong enough to marginally interacted with the Earth's gravitational pull, a phenomenon he labeled electrogravitics. Several researchers claim that conventional physics cannot adequately explain the phenomenon. The effect has become something of a cause célèbre in the UFO community, where it is seen as an example of something much more exotic than electrokinetics. Charles Berlitz devoted an entire chapter of his book \"The Philadelphia Experiment\" to a retelling of Brown's early work with the effect, implying he had discovered a new electrogravity effect and that it was being used by UFOs. Today, the Internet is filled with sites devoted to this interpretation of the effect.\n\nThere have been follow-ups on the claims that this force can be produced in a full vacuum, meaning it is an unknown anti-gravity force, and not just the more well known ion wind. As part of a study in 1990, U.S. Air Force researcher R. L. Talley conducted a test on a Biefeld–Brown-style capacitor to replicate the effect in a vacuum. Despite attempts that increased the driving DC voltage to about 19 kV in vacuum chambers up to 10 torr, Talley observed no thrust in terms of static DC potential applied to the electrodes. In 2003, NASA scientist Jonathan Campbell tested a lifter in a vacuum at 10 torr with a voltage of up to 50 kV, only to observe no movement from the lifter. Campbell pointed out to a Wired magazine reporter that creating a true vacuum similar to space for the test requires tens of thousands of dollars in equipment.\n\nAround the same time in 2003, researchers from the Army Research Laboratory (ARL) tested the Biefeld–Brown effect by building four different-sized asymmetric capacitors based on simple designs found on the Internet and then applying a high voltage of around 30 kV to them. According to their report, the researchers claimed that the effects of ion wind was at least three orders of magnitude too small to account for the observed force on the asymmetric capacitor in the air. Instead, they proposed that the Biefeld–Brown effect may be better explained using ion drift instead of ion wind due to how the former involves collisions instead of ballistic trajectories. Around ten years later, researchers from the Technical University of Liberec conducted experiments on the Biefeld–Brown effect that supported ARL's claim that assigned ion drift as the most likely source of the generated force.\n\nIn 2004, Martin Tajmar published a paper that also failed to replicate Brown's work and suggested that Brown may have instead observed the effects of a corona wind triggered by insufficient outgassing of the electrode assembly in the vacuum chamber and therefore misinterpreted the corona wind effects as a possible connection between gravitation and electromagnetism.\n\nT. T. Brown was granted a number of patents on his discovery:\n\nHistorically, numerous patents have been granted for various applications of the effect, from electrostatic dust precipitation, to air ionizers, and also for flight. A particularly notable patent — — was granted to G.E. Hagen in 1964, for apparatus more or less identical to the later so-called 'lifter' devices. Other ionic US patents of interest: , , , , , , , , , , , .\n\n\n"}
{"id": "3495631", "url": "https://en.wikipedia.org/wiki?curid=3495631", "title": "China Geological Survey", "text": "China Geological Survey\n\nThe China Geological Survey (CGS) () is a government-owned, not-for-profit, Chinese organization researching China's mineral resources. It is the largest Geoscience agency in China since being reinstated in 1999.\n\nThe China Geological Survey originated in the early days of the Republic of China (when it had control over mainland China). Many prominent geologists and paleontologists worked with the Survey in the early days, such as Davidson Black or Teilhard de Chardin. It was disbanded after the People's Republic of China gained control over mainland China and reinstated in 1999.\n\n\n"}
{"id": "29179018", "url": "https://en.wikipedia.org/wiki?curid=29179018", "title": "Converted-wave analysis", "text": "Converted-wave analysis\n\nDuring seismic exploration, P-waves (also known as primary or compressive waves) penetrate down into the earth. When a P-wave hits an interface (e.g., solid-liquid), it can reflect upwards as an S-wave (also known as a secondary, shear or transverse wave). Other P-wave to S-wave (P-S) conversions can occur, but the down-up conversion is the primary focus. Unlike P-waves, converted shear waves are largely unaffected by fluids. By analyzing the original and converted waves, seismologists obtain additional subsurface information, especially due to (1) differential velocity (V/V), (2) asymmetry in the waves' angles of incidence and reflection and (3) amplitude variations.\nAs opposed to analysis of P-wave to P-wave (P-P) reflection, c-wave (P-S) analysis is more complex. C-wave analysis requires at least three times as many measurement channels per station. Variations in reflection depths can cause significant analytic problems. Gathering, mapping, and binning c-wave data is also more difficult than P-P data. However, c-wave analysis can provide additional information needed to create a three-dimensional depth image of rock type, structure, and saturant. For example, changes in V with respect to V suggest changing lithology and pore geometry.\n"}
{"id": "23251268", "url": "https://en.wikipedia.org/wiki?curid=23251268", "title": "Cross-laminates", "text": "Cross-laminates\n\nCross-laminates are synthetic, flexible films of which the properties have been enhanced via mechanical manipulation of the film. Nowadays, the most well-known type of cross-laminated films are the Valéron films.\n"}
{"id": "11522143", "url": "https://en.wikipedia.org/wiki?curid=11522143", "title": "Deep-cycle battery", "text": "Deep-cycle battery\n\nA deep-cycle battery is a lead-acid battery designed to be regularly deeply discharged using most of its capacity. In contrast, starter batteries (e.g. most automotive batteries) are designed to deliver short, high-current bursts for cranking the engine, thus frequently discharging only a small part of their capacity. While a deep-cycle battery can be used as a starting battery, the lower \"cranking current\" implies that an oversized battery may be required.\n\nA deep-cycle battery is designed to discharge between 45% and 75% of its capacity, depending on the manufacturer and the construction of the battery. Although these batteries can be cycled down to 20% charge, the best lifespan vs cost method is to keep the average cycle at about 45% discharge. There is an inverse correlation between the depth of discharge (DOD) of the battery, and the number of charge and discharge cycles it can perform.\n\nDeep-cycle lead-acid batteries generally fall into two distinct categories; flooded (FLA) and valve-regulated lead-acid (VRLA), with the VRLA type further subdivided into two types, Absorbed Glass Mat (AGM) and Gel. The reinforcement of absorbed glass mat separators helps to reduce damage caused by spilling and jolting vibrations. Further, flooded deep-cycle batteries can be divided into subcategories of Tubular-plated Opzs or flat plated. The difference generally affects the cycle life and performance of the cell. The structural difference between deep-cycle batteries and cranking batteries is in the lead battery plates. Deep cycle battery plates have thicker active plates, with higher-density active paste material and thicker separators. Alloys used for the plates in a deep cycle battery may contain more antimony than that of starting batteries. The thicker battery plates resist corrosion through extended charge and discharge cycles.\n\n\nThe term \"flooded\" is used because this type of battery contains a quantity of electrolyte fluid so that the plates are completely submerged. The electrolyte level should be above the tops of plates which serves as a reservoir to make sure that water loss during charging does not lower the level below the plate tops and cause damage. Flooded batteries will decompose some water from the electrolyte during charging, so regular maintenance of flooded batteries requires inspection of electrolyte level and addition of water. Major modes of failure of deep-cycle batteries are loss of the active material due to shedding of the plates, and corrosion of the internal grid that supports active material. The capacity of a deep cycle battery is usually limited by electrolyte capacity and not by the plate mass, to improve life expectancy.\n\nThe vast majority of deep cycle batteries on the market today are lead acid batteries. Lead acid batteries are recycled 98% by volume, 99.5% by weight. The plastic cases, lead plates, sulfuric acid, solder, and other metals are 100% recovered for reuse. The only part of a battery that is not recyclable is the paper separators that wrap the plates. Due to the acid bath the paper sits in, the fiber length is reduced so far that it cannot be rewoven.\n\nIndustry wide, there is a greater than 98% rate of recovery on all lead acid batteries sold in the United States, resulting in a virtually closed manufacturing cycle.\n\n\n"}
{"id": "1760016", "url": "https://en.wikipedia.org/wiki?curid=1760016", "title": "Destratification", "text": "Destratification\n\nDestratification is a process in which the air or water is mixed in order to eliminate stratified layers of temperature, plant, or animal life. The first example of destratification was in 1919, in a small reservoir.\n\nA pond's condition deteriorates when the bottom environment cannot support animal life. The bottom is the area that runs out of oxygen first, it is where the most oxygen is used, and it is the farthest from the surface where it is replenished. Without oxygen a lake or pond's self-purification capability is not only reduced, it is reversed. The small animals, snails, worms, bacteria, etc., which help keep a pond clean cannot live, and the pond's nutrients are then recycled from the sediment. This forms a layer of muck at the bottom which serves as a fertilizer for weed and excessive algae growth. It can also cause large fish kills.\n\n\n"}
{"id": "1202358", "url": "https://en.wikipedia.org/wiki?curid=1202358", "title": "Diesel exhaust", "text": "Diesel exhaust\n\nDiesel exhaust is the gaseous exhaust produced by a diesel type of internal combustion engine, plus any contained particulates. Its composition may vary with the fuel type or rate of consumption, or speed of engine operation (e.g., idling or at speed or under load), and whether the engine is in an on-road vehicle, farm vehicle, locomotive, marine vessel, or stationary generator or other application.\n\nDiesel exhaust is a Group 1 carcinogen, which causes lung cancer and has a positive association with bladder cancer. It contains several substances that are also listed individually as human carcinogens by the IARC.\n\nMethods exist to reduce nitrogen oxides (NO) and particulate matter (PM) in the exhaust.\n\nThe primary products of petroleum fuel combustion in air are carbon dioxide, water, and nitrogen. The other components exist primarily from incomplete combustion and pyrosynthesis.\nWhile the distribution of the individual components of raw (untreated) diesel exhaust varies depending on factors like load, engine type, etc., the adjacent table shows a typical composition.\n\nThe physical and chemical conditions that exist inside any such diesel engines under any conditions differ considerably from spark-ignition engines, because, by design, diesel engine power is directly controlled by the fuel supply, not by control of the air/fuel mixture, as in conventional gasoline engines. As a result of these differences, diesel engines generally produce a different array of pollutants than spark-driven engines, differences that are sometimes qualitative (what pollutants are there, and what are not), but more often quantitative (how much of particular pollutants or pollutant classes are present in each). For instance, diesel engines produce one-twenty-eighth the carbon monoxide that gasoline engines do, as they burn their fuel in excess air even at full load.\n\nHowever, the lean-burning nature of diesel engines and the high temperatures and pressures of the combustion process result in significant production of gaseous nitrogen oxides (NO), an air pollutant that constitutes a unique challenge with regard to their reduction. While total nitrogen oxides from petrol cars have decreased by around 96% through adoption of exhaust catalytic converters as of 2012, diesel cars still produce nitrogen oxides at a similar level to those bought 15 years earlier under real-world tests; hence, diesel cars emit around 20 times more nitrogen oxides than petrol cars. Modern on-road diesel engines typically use selective catalytic reduction (SCR) systems to meet emissions laws, as other methods such as exhaust gas recirculation (EGR) cannot adequately reduce NO to meet the newer standards applicable in many jurisdictions. Auxiliary diesel systems designed to remediate the nitrogen oxide pollutants are described in a separate section below.\n\nMoreover, the fine particles (fine particulate matter) in diesel exhaust (e.g., soot, sometimes visible as opaque dark-colored smoke) has traditionally been of greater concern, as it presents different health concerns and is rarely produced in significant quantities by spark-ignition engines. These especially harmful particulate contaminants are at their peak when such engines are run without sufficient oxygen to fully combust the fuel; when a diesel engine runs at idle, enough oxygen is usually present to burn the fuel completely. (The oxygen requirement in non-idling engines is usually mitigated using turbocharging.). From the particle emission standpoint, exhaust from diesel vehicles has been reported to be significantly more harmful than those from petrol vehicles.\n\nDiesel exhausts, long known for their characteristic smells, changed significantly with the reduction of sulfur content of diesel fuel, and again when catalytic converters were introduced in exhaust systems. Even so, diesel exhausts continue to contain an array of inorganic and organic pollutants, in various classes, and in varying concentrations (see below), depending on fuel composition and engine running conditions.\n\nThe following are classes of chemical compounds that have been found in diesel exhaust.\n\nThe following are classes of specific chemicals that have been found in diesel exhaust.\nIncludes all regioisomers of this aromatic compound. See \"ortho-, meta-, and para-\"isomer descriptions at each compound's article.\n\nVehicle exhaust contains much water vapor. There has been research into ways that troops in deserts can recover drinkable water from their vehicles' exhaust gases.\n\nTo rapidly reduce particulate matter from heavy-duty diesel engines in California, the California Air Resources Board created the Carl Moyer Program to provide funding for upgrading engines ahead of emissions regulations. In 2008 the California Air Resources Board also implemented the 2008 California Statewide Truck and Bus Rule which requires all heavy-duty diesel trucks and buses, with a few exceptions, that operate in California to either retrofit or replace engines in order to reduce diesel particulate matter. The US Mine Safety and Health Administration (MSHA) issued a health standard in January 2001 designed to reduce diesel exhaust exposure in underground metal and nonmetal mines; on September 7, 2005, MSHA published a notice in the Federal Register proposing to postpone the effective date from January 2006 until January 2011.\n\nEmissions from diesel vehicles have been reported to be significantly more harmful than those from petrol vehicles. Diesel combustion exhaust is a source of atmospheric soot and fine particles, which is a component of the air pollution implicated in human cancer, heart and lung damage, and mental functioning. Moreover, diesel exhaust contains contaminants listed as carcinogenic for humans by the IARC (part of the World Health Organization of the United Nations), as present in their List of IARC Group 1 carcinogens. Diesel exhaust pollution is thought to account for around one quarter of the pollution in the air in previous decades, and a high share of sickness caused by automotive pollution.\n\nExposure to diesel exhaust and diesel particulate matter (DPM) is an occupational hazard to truckers, railroad workers and occupants of residential homes in vicinity of a rail yard, and miners using diesel-powered equipment in underground mines. Adverse health effects have also been observed in the general population at ambient atmospheric particle concentrations well below the concentrations in occupational settings.\n\nIn March 2012, U.S. government scientists showed that underground miners exposed to high levels of diesel fumes have a threefold increased risk for contracting lung cancer compared with those exposed to low levels. The $11.5 million Diesel Exhaust in Miners Study (DEMS) followed 12,315 miners, controlling for key carcinogens such as cigarette smoke, radon, and asbestos. This allowed scientists to isolate the effects of diesel fumes.\n\nFor over 10 years, concerns have been raised in the USA regarding children's exposure to DPM as they ride diesel-powered school buses to and from school. In 2013, the Environmental Protection Agency (EPA) established the Clean School Bus USA initiative in an effort to unite private and public organizations in curbing student exposures.\n\nDiesel particulate matter (DPM), sometimes also called diesel exhaust particles (DEP), is the particulate component of diesel exhaust, which includes diesel soot and aerosols such as ash particulates, metallic abrasion particles, sulfates, and silicates. When released into the atmosphere, DPM can take the form of individual particles or chain aggregates, with most in the invisible sub-micrometre range of 100 nanometers, also known as ultrafine particles (UFP) or PM0.1.\n\nThe main particulate fraction of diesel exhaust consists of fine particles. Because of their small size, inhaled particles may easily penetrate deep into the lungs. The polycyclic aromatic hydrocarbons (PAHs) in the exhaust stimulate nerves in the lungs, causing reflex coughing, wheezing and shortness of breath. The rough surfaces of these particles makes it easy for them to bind with other toxins in the environment, thus increasing the hazards of particle inhalation.\n\nA study of particulate matter (PM) emissions from transit buses running on ULSD and a mixture of biodiesel and conventional diesel (B20) was reported by Omidvarborna and coworkers, where they conclude PM emissions appeared lower in cases of mixed diesel/biodiesel use, where they were dependent on the engine model, cold and hot idle modes, and fuel type, and that heavy metals in PM emitted during hot idling were greater than those from cold idling; reasons for PM reduction in biodiesel emissions were suggested to result from the oxygenated structure of biodiesel fuel, as well as arising from changes in technology (including the use of a catalytic converter in this test system). Other studies concluded that while in certain specific cases (i.e. low loads, more saturated feedstocks, ...), NOx emissions can be lower than with diesel fuel, in most cases NOx emissions are higher, and the NOx emissions even go up as more biofuel is mixed in. Pure biodiesel (B100) even ends up having 10-30% more NOx emissions compared to regular diesel fuel.\n\nExposures have been linked with acute short-term symptoms such as headache, dizziness, light-headedness, nausea, coughing, difficult or labored breathing, tightness of chest, and irritation of the eyes and nose and throat. Long-term exposures can lead to chronic, more serious health problems such as cardiovascular disease, cardiopulmonary disease, and lung cancer. \nElemental carbon attributable to traffic was significantly associated with wheezing at age 1 and persistent wheezing at age 3 in the Cincinnati Childhood Allergy and Air Pollution Study birth cohort study.\n\nThe NERC-HPA funded Traffic Pollution and Health in London project at King's College London is currently seeking to refine understanding of the health effects of traffic pollution. Ambient traffic-related air pollution was associated with decreased cognitive function in older men.\n\nMortality from diesel soot exposure in 2001 was at least 14,400 out of the German population of 82 million, according to the official report 2352 of the \"Umweltbundesamt Berlin\" (Federal Environmental Agency of Germany).\n\nThe study of nanoparticles and nanotoxicology is in its infancy, and health effects from nanoparticles produced by all types of diesel engines are still being uncovered. It is clear, that diesel health detriments of fine particle emissions are severe and pervasive. Although one study found no significant evidence that short-term exposure to diesel exhaust results in adverse extrapulmonary effects, effects that are correlated with an increase in cardiovascular disease, a 2011 study in \"The Lancet\" concluded that traffic exposure is the single most serious preventable trigger of heart attack in the general public, as the cause of 7.4% of all attacks. It is impossible to tell how much of this effect is due to the stress of being in traffic and how much is due to exposure to exhaust.\n\nSince the study of the detrimental health effects of nanoparticles (nanotoxicology) is still in its infancy, and the nature and extent of negative health impacts from diesel exhaust continues to be discovered. There is little controversy, however, that the public health impact of diesels is higher than that of petrol-fuelled vehicles despite the wide uncertainties.\n\nThe types and quantities of nanoparticles can vary according to operating temperatures and pressures, presence of an open flame, fundamental fuel type and fuel mixture, and even atmospheric mixtures. As such, the resulting types of nanoparticles from different engine technologies and even different fuels are not necessarily comparable. One study has shown that 95% of the volatile component of diesel nanoparticles is unburned lubricating oil. Long-term effects still need to be further clarified, as well as the effects on susceptible groups of people with cardiopulmonary diseases.\n\nDiesel engines can produce black soot (or more specifically diesel particulate matter) from their exhaust. The black smoke consists of carbon compounds that have not burned because of local low temperatures where the fuel is not fully atomized. These local low temperatures occur at the cylinder walls, and at the surface of large droplets of fuel. At these areas where it is relatively cold, the mixture is rich (contrary to the overall mixture which is lean). The rich mixture has less air to burn and some of the fuel turns into a carbon deposit. Modern car engines use a diesel particulate filter (DPF) to capture carbon particles and then intermittently burn them using extra fuel injected directly into the filter. This prevents carbon buildup at the expense of wasting a small quantity of fuel.\n\nThe full load limit of a diesel engine in normal service is defined by the \"black smoke limit\", beyond which point the fuel cannot be completely burned. As the \"black smoke limit\" is still considerably lean of stoichiometric, it is possible to obtain more power by exceeding it, but the resultant inefficient combustion means that the extra power comes at the price of reduced combustion efficiency, high fuel consumption and dense clouds of smoke. This is only done in high performance applications where these disadvantages are of little concern.\n\nWhen starting from cold, the engine's combustion efficiency is reduced because the cold engine block draws heat out of the cylinder in the compression stroke. The result is that fuel is not burned fully, resulting in blue and white smoke and lower power outputs until the engine has warmed. This is especially the case with indirect injection engines, which are less thermally efficient. With electronic injection, the timing and length of the injection sequence can be altered to compensate for this. Older engines with mechanical injection can have mechanical and hydraulic governor control to alter the timing, and multi-phase electrically controlled glow plugs, that stay on for a period after start-up to ensure clean combustion; the plugs are automatically switched to a lower power to prevent their burning out.\n\nExperiments in 2013 showed that diesel exhaust impaired bees' ability to detect the scent of oilseed rape flowers.\n\nWith emissions standards tightening, diesel engines are having to become more efficient and have fewer pollutants in their exhaust. For instance, light duty truck must now have NOx emissions less than 0.07 g/mile, and in the U.S., by 2010, NOx emissions must be less than 0.03 g/mile. Moreover, in recent years the United States, Europe, and Japan have extended emissions control regulations from covering on-road vehicles to include farm vehicles and locomotives, marine vessels, and stationary generator applications. Changing to a different fuel (i.e. dimethyl ether, and other bioethers as diethyl ether ) tends to be a very effective means to reduce pollutants such as NOx and CO. When running on dimethyl ether (DME) for instance, particulate matter emissions are near-nonexistent, and the use of diesel particulate filters could even be omitted. Also, given that DME can be made from animal, food, and agricultural waste, it can even be carbon-neutral (unlike regular diesel). Mixing in bioether (or other fuels such as hydrogen) into conventional diesel also tends to have a beneficial effect on the pollutants that are emitted. \nIn addition to changing the fuel, US engineers have also come up with two other principles and distinct systems to all on-market products that meet the U.S. 2010 emissions criteria, selective non-catalytic reduction (SNCR), and exhaust gas recirculation (EGR). Both are in the exhaust system of diesel engines, and are further designed to promote efficiency.\n\nSelective catalytic reduction (SCR) injects a reductant such as ammonia or urea — the latter aqueous, where it is known as diesel exhaust fluid, DEF) — into the exhaust of a diesel engine to convert nitrogen oxides (NO) into gaseous nitrogen and water. SNCR systems have been prototyped that reduce 90% of the NO in the exhaust system, with commercialized systems being somewhat lower. SCR systems do not necessarily need particulate matter (PM) filters; when SNCR and PM filters are combined, some engines have been shown to be 3-5% more fuel efficient. A disadvantage of the SCR system, in addition to added upfront development cost (which can be offset by compliance and improved performance), is the need to refill the reductant, the periodicity of which varies with the miles driven, load factors, and the hours used. The SNCR system is not as efficient at higher revolutions per minute (rpm). SCR is being optimized to have higher efficiency with broader temperatures, to be more durable, and to meet other commercial needs.\n\nExhaust gas recirculation (EGR), on diesel engines, can be used to achieve a richer fuel to air mixture and a lower peak combustion temperature. Both effects reduce NO emissions, but can negatively impact efficiency and the production of soot particles. The richer mix is achieved by displacing some of the intake air, but is still lean compared to petrol engines, which approach the stoichiometric ideal. The lower peak temperature is achieved by a heat exchanger that removes heat before re-entering the engine, and works due to the exhaust gases' higher specific heat capacity than air. With the greater soot production, EGR is often combined with a particulate matter (PM) filter in the exhaust. In turbocharged engines, EGR needs a controlled pressure differential across the exhaust manifold and intake manifold, which can be met by such engineering as use of a variable geometry turbocharger, which has inlet guide vanes on the turbine to build exhaust backpressure in the exhaust manifold directing exhaust gas to the intake manifold. It also requires additional external piping and valving, and so requires additional maintenance.\n\nJohn Deere, the farm equipment manufacturer is implementing such a combined SCR-EGR design, in a 9-liter \"inline 6\" diesel engine that involves both system types, a PM filter and additional oxidation catalyst technologies. The combined system incorporates two turbochargers, the first on the exhaust manifold, with variable geometry and containing the EGR system; and a second a fixed geometry turbocharger. Recirculated exhaust gas and the compressed air from the turbochargers have separate coolers, and air merges before entering the intake manifold, and all subsystems are controlled by a central engine control unit that optimizes minimization of pollutants released in the exhaust gas.\n\nA new technology being tested in 2016 has been created by Air Ink which collects carbon particles using a \"Kaalink\" cylindrical device that is retrofitted into a vehicle's exhaust system, after processing to remove heavy metals and carcinogens, the company plans to use the carbon to make ink.\n\n\n\n"}
{"id": "4734359", "url": "https://en.wikipedia.org/wiki?curid=4734359", "title": "Energy Institute", "text": "Energy Institute\n\nThe Energy Institute (EI) is a UK chartered professional membership body.\n\nThe Energy Institute is licensed by the Engineering Council to offer Chartered, Incorporated and Engineering Technician status to engineers and also by the Science Council and Society for the Environment to offer registration as Chartered Scientist and Chartered Environmentalist.\n\nIt was formed in 2003 from a merger between the Institute of Petroleum and the Institute of Energy. The EI offers three pre professional grades of membership: Affiliate, Graduate and Student, along with a selection of professional grades. The professional grades are: Fellow (FEI), Fellow Chartered Engineer (CEng FEI), Fellow Chartered Scientist (CSci FEI), Member (MEI), Member Chartered Engineer (CEng MEI), Member Chartered Scientist (CSci MEI) Member or Fellow and Chartered Environmentalist (CEnv MEI or FEI), Technician Member (TMEI) and Technician Member Engineering Technician (EngTech TMEI).\n\nThe EI publishes two membership magazines: Petroleum Review and Energy World.\n\n"}
{"id": "43699304", "url": "https://en.wikipedia.org/wiki?curid=43699304", "title": "Energy rate density", "text": "Energy rate density\n\nEnergy rate density is the amount of free energy per unit time per unit mass (in CGS metric units erg/s/g; in MKS units joule/s/kg). It is terminologically (but not always numerically) equivalent to power density when measured in SI units of W/kg. Regardless of the units used, energy rate density describes the flow of energy through any system of given mass, and has been proposed as a measure of system complexity. The more complex the system, the more energy flows per second through each gram. \n\nEnergy rate density is actually a general term that is equivalent to more specialized terms used by many different disciplinary scientists. For example, in astronomy it is called the luminosity-to-mass ratio (the inverse of the mass-luminosity ratio), in physics the power density, in geology the specific radiant flux (where “specific” denotes per unit mass), in biology the specific metabolic rate, and in engineering the power-to-weight ratio. Interdisciplinary researchers prefer to use the general term, energy rate density, not only to stress the intuitive notion of energy flow (in contrast to more colloquial connotations of the word \"power\"), but also to unify its potential application among all the natural sciences, as in the cosmology of cosmic evolution. When the energy rate density for systems including our galaxy, sun, earth, plants, animals, society are plotted according to when, in historical time, they first emerged, a clear increase in energy rate density over time is observed. \n\nThis term has in recent years gained many diverse applications in various disciplines, including history, cosmology, economics, philosophy, and behavioral biology. \n\n\n"}
{"id": "7815174", "url": "https://en.wikipedia.org/wiki?curid=7815174", "title": "Entropy and life", "text": "Entropy and life\n\nResearch concerning the relationship between the thermodynamic quantity entropy and the evolution of life began around the turn of the 20th century. In 1910, American historian Henry Adams printed and distributed to university libraries and history professors the small volume \"A Letter to American Teachers of History\" proposing a theory of history based on the second law of thermodynamics and on the principle of entropy. The 1944 book \"What is Life?\" by Nobel-laureate physicist Erwin Schrödinger stimulated research in the field. In his book, Schrödinger originally stated that life feeds on negative entropy, or negentropy as it is sometimes called, but in a later edition corrected himself in response to complaints and stated the true source is free energy. More recent work has restricted the discussion to Gibbs free energy because biological processes on Earth normally occur at a constant temperature and pressure, such as in the atmosphere or at the bottom of an ocean, but not across both over short periods of time for individual organisms.\n\nIn 1863, Rudolf Clausius published his noted memoir \"On the Concentration of Rays of Heat and Light, and on the Limits of Its Action\" wherein he outlined a preliminary relationship, as based on his own work and that of William Thomson (Lord Kelvin), between his newly developed concept of entropy and life. Building on this, one of the first to speculate on a possible thermodynamic perspective of evolution was the Austrian physicist Ludwig Boltzmann. In 1875, building on the works of Clausius and Kelvin, Boltzmann reasoned:\n\nIn 1876, American civil engineer Richard Sears McCulloh, in his \"Treatise on the Mechanical Theory of Heat and its Application to the Steam-Engine\", which was an early thermodynamics textbook, states, after speaking about the laws of the physical world, that \"there are none that are established on a firmer basis than the two general propositions of Joule and Carnot; which constitute the fundamental laws of our subject.\" McCulloch then goes on to show that these two laws may be combined in a single expression as follows:\n\nwhere\n\nMcCulloch then declares that the applications of these two laws, i.e. what are currently known as the first law of thermodynamics and the second law of thermodynamics, are innumerable. He then states:\n\nMcCulloch then gives a few of what he calls the “more interesting examples” of the application of these laws in extent and utility. The first example he gives is physiology, wherein he states that “the body of an animal, not less than a steamer, or a locomotive, is truly a heat engine, and the consumption of food in the one is precisely analogous to the burning of fuel in the other; in both, the chemical process is the same: that called combustion.” He then incorporates a discussion of Lavoisier’s theory of respiration with cycles of digestion and excretion, perspiration, but then contradicts Lavoisier with recent findings, such as internal heat generated by friction, according to the new theory of heat, which, according to McCulloch, states that the “heat of the body generally and uniformly is diffused instead of being concentrated in the chest”. McCulloch then gives an example of the second law, where he states that friction, especially in the smaller blooded-vessels, must develop heat. Without doubt, animal heat is thus in part produced. He then asks: “but whence the expenditure of energy causing that friction, and which must be itself accounted for?\"\n\nTo answer this question he turns to the mechanical theory of heat and goes on to loosely outline how the heart is what he calls a “force-pump”, which receives blood and sends it to every part of the body, as discovered by William Harvey, that “acts like the piston of an engine and is dependent upon and consequently due to the cycle of nutrition and excretion which sustains physical or organic life.” It is likely, here, that McCulloch was modeling parts of this argument on that of the famous Carnot cycle. In conclusion, he summarizes his first and second law argument as such:\n\nIn the 1944 book \"What is Life?\", Nobel-laureate physicist Erwin Schrödinger theorizes that life, contrary to the general tendency dictated by the Second law of thermodynamics, decreases or maintains its entropy by feeding on negative entropy. In his note to Chapter 6 of \"What is Life?\", however, Schrödinger remarks on his usage of the term negative entropy:\n\nThis is what is argued to differentiate life from other forms of matter organization. In this direction, although life's dynamics may be argued to go against the tendency of second law, which states that the entropy of an isolated system tends to increase, it does not in any way conflict or invalidate this law, because the principle that entropy can only increase or remain constant applies only to a closed system which is adiabatically isolated, meaning no heat can enter or leave. Whenever a system can exchange either heat or matter with its environment, an entropy decrease of that system is entirely compatible with the second law. The problem of organization in living systems increasing despite the second law is known as the Schrödinger paradox.\n\nIn 1964, James Lovelock was among a group of scientists who were requested by NASA to make a theoretical life detection system to look for life on Mars during the upcoming space mission. When thinking about this problem, Lovelock wondered “how can we be sure that Martian life, if any, will reveal itself to tests based on Earth’s lifestyle?” To Lovelock, the basic question was “What is life, and how should it be recognized?” When speaking about this issue with some of his colleagues at the Jet Propulsion Laboratory, he was asked what he would do to look for life on Mars. To this, Lovelock replied \"I’d look for an entropy reduction, since this must be a general characteristic of life.\"\n\nIn recent years, the thermodynamic interpretation of evolution in relation to entropy has begun to utilize the concept of the Gibbs free energy, rather than entropy. This is because biological processes on earth take place at roughly constant temperature and pressure, a situation in which the Gibbs free energy is an especially useful way to express the second law of thermodynamics. The Gibbs free energy is given by:\n\nThe minimization of the Gibbs free energy is a form of the principle of minimum energy, which follows from the entropy maximization principle for closed systems. Moreover, the Gibbs free energy equation, in modified form, can be utilized for open systems when chemical potential terms are included in the energy balance equation. In a popular 1982 textbook, \"Principles of Biochemistry\", noted American biochemist Albert Lehninger argues that the order produced within cells as they grow and divide is more than compensated for by the disorder they create in their surroundings in the course of growth and division. In short, according to Lehninger, \"living organisms preserve their internal order by taking from their surroundings free energy, in the form of nutrients or sunlight, and returning to their surroundings an equal amount of energy as heat and entropy.\"\n\nSimilarly, according to the chemist John Avery, from his 2003 book \"Information Theory and Evolution\", we find a presentation in which the phenomenon of life, including its origin and evolution, as well as human cultural evolution, has its basis in the background of thermodynamics, statistical mechanics, and information theory. The (apparent) paradox between the second law of thermodynamics and the high degree of order and complexity produced by living systems, according to Avery, has its resolution \"in the information content of the Gibbs free energy that enters the biosphere from outside sources.\" The process of natural selection responsible for such local increase in order may be mathematically derived directly from the expression of the second law equation for connected non-equilibrium open systems.\n\nThe second law of thermodynamics applied on the origin of life is a far more complicated issue than the further development of life, since there is no \"standard model\" of how the first biological lifeforms emerged; only a number of competing hypotheses. The problem is discussed within the area of abiogenesis, implying gradual pre-Darwinian \"chemical evolution\". In 1924, Alexander Oparin suggested that sufficient energy was provided in a \"primordial soup\". The Belgian scientist Ilya Prigogine was awarded with a Nobel prize in 1977 for an analysis in this area. A related topic is the probability that life would emerge, which has been discussed in several studies, for example by Russell Doolittle.\n\nIn 2013 Azua-Bustos and Vega argued that disregarding the type of lifeform that could be envisioned both on Earth and elsewhere in the Universe, all should share in common the attribute of being entities that decrease their internal entropy at the expense of free energy obtained from its surroundings. As entropy allows the quantification of the degree of disorder in a system, any envisioned lifeform must have a higher degree of order than its supporting environment. These authors showed that by using fractal mathematics analysis alone, they could readily quantify the degree of structural complexity difference (and thus entropy) of living processes as distinct entities separate from their similar abiotic surroundings. This approach may allow the future detection of unknown forms of life both in the Solar System and on recently discovered exoplanets based on nothing more than entropy differentials of complementary datasets (morphology, coloration, temperature, pH, isotopic composition, etc.). \nDetecting ‘life as we don't know it’ by fractal analysis\n\nThe notion of entropy as disorder has been transferred from thermodynamics to psychology by Polish psychiatrist Antoni Kępiński, who admitted being inspired by Erwin Schrödinger. In his theoretical framework devised to explain mental disorders (the information metabolism theory), the difference between living organisms and other systems was explained as the ability to maintain order. Contrary to inanimate matter, organisms maintain the particular order of their bodily structures and inner worlds which they impose onto their surroundings and forward to new generations. The life of an organism or the species ceases as soon as it loses that ability. Maintaining of that order requires continual exchange of information between the organism and its surroundings. In higher organisms, information is acquired mainly through receptors and metabolised in the nervous system. The result is action - some form of motion, for example locomotion, speech, internal motion of organs, secretion of hormones etc. The reaction of organism becomes an informational signal to other organisms. Information metabolism, which allows to maintain the order, is possible only if a hierarchy of value exists, as the signals coming to the organism must be structured. In humans that hierarchy has three levels i.e. biological, emotional and sociocultural. Kępiński explained how various mental disorders are caused by distortions of that hierarchy and the return to mental health is possible through its restoration. \n\nThe idea was continued by Struzik who proposed that Kępiński's information metabolism theory may be seen as an extension of Brilluoin's negentropy principle of information. In 2011 the notion of \"psychological entropy\" has been reintroduced to psychologists by Hirsh et al. Simiraly to Kępiński, these authors noted that uncertainty management is a critical ability for any organism. Uncertainty, arising due to the conflict between competing perceptual and behavioral affordances, is experienced subjectively as anxiety. Hirsh and his collaborators proposed that both the perceptual and behavioral domains may be conceptualized as probability distributions and that the amount of uncertainty associated with a given perceptual or behavioral experience can be quantified in terms of Claude Shannon’s entropy formula.\n\nFor nearly a century and a half, beginning with Clausius' 1863 memoir \"On the Concentration of Rays of Heat and Light, and on the Limits of its Action\", much writing and research has been devoted to the relationship between thermodynamic entropy and the evolution of life. The argument that life feeds on negative entropy or negentropy was asserted by physicist Erwin Schrödinger in a 1944 book \"What is Life?\". He posed, \"How does the living organism avoid decay?\" The obvious answer is: \"By eating, drinking, breathing and (in the case of plants) assimilating.\" Recent writings have used the concept of Gibbs free energy to elaborate on this issue.\nWhile energy from nutrients is necessary to sustain an organism's order, there is also the Schrödinger prescience: \"An organism's astonishing gift of concentrating a stream of order on itself and thus escaping the decay into atomic chaos – of drinking orderliness from a suitable environment – seems to be connected with the presence of the aperiodic solids...\" We now know that the 'aperiodic' crystal is DNA and that the irregular arrangement is a form of information. \"The DNA in the cell nucleus contains the master copy of the software, in duplicate. This software seems to control by \"specifying an algorithm, or set of instructions, for creating and maintaining the entire organism containing the cell.\" DNA and other macromolecules determine an organism's life cycle: birth, growth, maturity, decline, and death. Nutrition is necessary but not sufficient to account for growth in size as genetics is the governing factor. At some point, organisms normally decline and die even while remaining in environments that contain sufficient nutrients to sustain life. The controlling factor must be internal and not nutrients or sunlight acting as causal exogenous variables. Organisms inherit the ability to create unique and complex biological structures; it is unlikely for those capabilities to be reinvented or be taught each generation. Therefore, DNA must be operative as the prime cause in this characteristic as well. Applying Boltzmann's perspective of the second law, the change of state from a more probable, less ordered and high entropy arrangement to one of less probability, more order, and lower entropy seen in biological ordering calls for a function like that known of DNA. DNA's apparent information processing function provides a resolution of the paradox posed by life and the entropy requirement of the second law.\n\nIn 1982, American biochemist Albert Lehninger argued that the \"order\" produced within cells as they grow and divide is more than compensated for by the \"disorder\" they create in their surroundings in the course of growth and division. \"Living organisms preserve their internal order by taking from their surroundings free energy, in the form of nutrients or sunlight, and returning to their surroundings an equal amount of energy as heat and entropy.\"\n\nEvolution-related concepts:\n\nIn a study titled \"Natural selection for least action\" published in the \"Proceedings of the Royal Society A.\", Ville Kaila and Arto Annila of the University of Helsinki describe how the second law of thermodynamics can be written as an equation of motion to describe evolution, showing how natural selection and the principle of least action can be connected by expressing natural selection in terms of chemical thermodynamics. In this view, evolution explores possible paths to level differences in energy densities and so increase entropy most rapidly. Thus, an organism serves as an energy transfer mechanism, and beneficial mutations allow successive organisms to transfer more energy within their environment.\n\nEntropy is defined for equilibrium systems, so objections to the extension of the second law and entropy to biological systems, especially as it pertains to its use to support or discredit the theory of evolution, have been stated. Live systems and indeed much of the systems and processes in the universe operate far from equilibrium, whereas the second law succinctly states that isolated systems evolve toward thermodynamic equilibrium — the state of maximum entropy.\n\nHowever, entropy is well defined much more broadly based on the probabilities of a system's states, whether or not the system is a dynamical one (for which equilibrium could be relevant). Even in those physical systems where equilibrium could be relevant, (1) live systems cannot persist in isolation and (2) the second principle of thermodynamics does not require that free energy be transformed into entropy along the shortest path: live organisms absorb energy from sunlight or from energy-rich chemical compounds and finally return part of such energy to the environment as entropy (heat and low free-energy compounds such as water and CO).\n\n\n\n"}
{"id": "441852", "url": "https://en.wikipedia.org/wiki?curid=441852", "title": "Ethical consumerism", "text": "Ethical consumerism\n\nEthical consumerism (alternatively called ethical consumption, ethical purchasing, moral purchasing, ethical sourcing, ethical shopping or green consumerism) is a type of consumer activism that is based on the concept of dollar voting. It is practiced through 'positive buying' in that ethical products are favoured, or 'moral boycott', that is negative purchasing and company-based purchasing.\n\nThe term \"ethical consumer\", now used generically, was first popularised by the UK magazine \"Ethical Consumer\", first published in 1989. \"Ethical Consumer\" magazine's key innovation was to produce 'ratings tables', inspired by the criteria-based approach of the then emerging ethical investment movement. \"Ethical Consumer\"'s ratings tables awarded companies negative marks (and from 2005 overall scores) across a range of ethical and environmental categories such as 'animal rights', 'human rights' and 'pollution and toxics', empowering consumers to make ethically informed consumption choices and providing campaigners with reliable information on corporate behaviour. Such criteria-based ethical and environmental ratings have subsequently become commonplace both in providing consumer information and in business-to-business corporate social responsibility and sustainability ratings such as those provided by Innovest, Calvert Foundation, Domini, IRRC, TIAA–CREF and KLD Analytics. Today, Bloomberg and Reuters provide \"environmental, social and governance\" ratings direct to the financial data screens of hundreds of thousands of stock market traders. The not-for-profit Ethical Consumer Research Association continues to publish \"Ethical Consumer\" and its associated website, which provides free access to ethical ratings tables.\n\nIn \"Unequal Freedoms: The Global Market As An Ethical System\" (1998), John McMurtry argues that no purchasing decision exists that does not itself imply some moral choice, and that there is no purchasing that is not ultimately moral in nature. This mirrors older arguments, especially by the Anabaptists, e.g. Mennonites, Amish, that one must accept all personal moral and spiritual liability of all harms done at any distance in space or time to anyone by one's own choices. It is often suggested that Judeo-Christian scriptures further direct followers towards practising good stewardship of the Earth, under an obligation to a God who is believed to have created the planet for us to share with other creatures. A similar argument presented from a secular humanist point of view is that it is simply better for human beings to acknowledge that the planet supports life only because of a delicate balance of many different factors.\n\nSome trust criteria, e.g. creditworthiness or implied warranty, are considered to be part of any purchasing or sourcing decision. However, these terms refer to broader systems of guidance that would, ideally, cause any purchasing decision to disqualify offered products or services based on non-price criteria that affect the moral rather than the functional liabilities of the entire production process. Paul Hawken, a proponent of Natural Capitalism, refers to \"comprehensive outcomes\" of production services as opposed to the \"culminative outcomes\" of using the product of such services. Often, moral criteria are part of a much broader shift away from commodity markets towards a deeper service economy where all activities, from growing to harvesting to processing to delivery, are considered part of the value chain and for which consumers are \"responsible\".\n\nAndrew Wilson, Director of the UK's Ashridge Centre for Business and Society, argues that \"Shopping is more important than voting\", and that the disposition of money is the most basic role we play in any system of economics. Some theorists believe that it is the clearest way that we express our actual moral choices, i.e., if we say we care about something but continue to buy from parties that have a high probability of risk of harm or destruction of that thing, we don't really care about it, we are practicing a form of simple hypocrisy.\n\nIn an effort by churches to advocate moral and ethical consumerism, many have become involved in the Fair Trade movement:\n\nA number of standards, labels and marks have been introduced for ethical consumers, such as the following:\n\n\nAlong with disclosure of ingredients, some mandatory labelling of origins of clothing or food is required in all developed nations. This practice has been extended in some developing nations, e.g., where every item carries the name, phone number and fax number of the factory where it was made so a buyer can inspect its conditions. And, more importantly, to prove that the item was not made by \"prison labor\", use of which to produce export goods is banned in most developed nations. Such labels have also been used for boycotts, as when the merchandise mark Made in Germany was introduced in 1887.\n\nThese labels serve as tokens of some reliable validation process, some instructional capital, much as does a brand name or a nation's flag. They also signal some social capital, or trust, in some community of auditors that must follow those instructions to validate those labels.\nSome companies in the United States, though currently not required to reduce their carbon footprint, are doing so voluntarily by changing their energy use practices, as well as by directly funding (through carbon offsets), businesses that are already sustainable—or are developing or improving green technologies for the future.\n\nIn 2009, Atlanta's Virginia-Highland neighborhood became the first Carbon-Neutral Zone in the United States. Seventeen merchants in Virginia-Highland allowed their carbon footprint to be audited. Now, they are partnered with the Valley Wood Carbon Sequestration Project—thousands of acres of forest in rural Georgia—through the Chicago Climate Exchange (CCX). The businesses involved in the partnership display the Verus Carbon Neutral seal in each store front and posted a sign prominently declaring the area's Carbon Neutral status. (CCX ceased trading carbon credits at the end of 2010 due to inactivity in the U.S. carbon markets, although carbon exchanges were intended to still be facilitated.)\n\nOver time, some theorists suggest, the amount of social capital or trust invested in nation-states (or \"flags\") will continue to decrease, and that placed in corporations (or \"brands\") will increase. This can only be offset by retrenched national sovereignty to reinforce shared national standards in tax, trade, and tariff laws, and by placing the trust in civil society in such \"moral labels\". These arguments have been a major focus of the anti-globalization movement, which includes many broader arguments against the amoral nature of markets as such. However, the economic school of Public Choice Theory pioneered by James M. Buchanan has offered counter-arguments based on economic demonstration to this theory of 'amoral markets' versus 'moral governments'.\n\nEthical Consumer Research Association, the alternative consumer organisation, collects and categorises information of more than 30,000 companies according to their performance in five main areas, composing the Ethiscore:\n\nGfK NOP, the market research group, has made a five-country study of consumer beliefs about the ethics of large companies. The report was described in a \"Financial Times\" article published on February 20, 2007 entitled \"Ethical consumption makes mark on branding\", and was followed up by an online debate/discussion hosted by FT.com. The countries surveyed were Germany, the United States, Britain, France and Spain. More than half of respondents in Germany and the US believed there is a serious deterioration in standards of corporate practice. Almost half of those surveyed in Britain, France and Spain held similar beliefs.\n\nAbout a third of respondents told researchers they would pay higher prices for ethical brands though perception of various companies' ethical or unethical status varied considerably from country to country.\n\nThe most ethically perceived brands were The Co-op (in the UK), Coca-Cola (in the US), Danone (in France), Adidas (in Germany) and Nestlé (in Spain). Coca-Cola, Danone, Adidas and Nestlé did not appear anywhere in the UK's list of 15 most ethical companies. Nike appeared in the lists of the other four countries but not in the UK's list.\n\nIn the UK, The Co-operative Bank has produced an Ethical Consumerism Report (formerly the Ethical Purchasing Index) since 2001. The report measures the market size and growth of a basket of 'ethical' products and services, and valued UK ethical consumerism at GBP36.0 billion (~USD54.4 billion) in 2008, and GBP47.2 billion (USD72.5 billion) in 2012.\n\nA number of organisations provide research-based evaluations of the behavior of companies around the world, assessing them along ethical dimensions such as human rights, the environment, animal welfare and politics. Green America is a not-for-profit membership organization founded in 1982 that provides the \"Green American Seal of Approval\" and produces a \"Responsible Shopper\" guide to \"alert consumers and investors to problems with companies that they may shop with or invest in.\" The Ethical Consumer Research Association is a not-for-profit workers' co-operative founded in the UK in 1988 to \"provide information on the companies behind the brand names and to promote the ethical use of consumer power\" which provides an online searchable database under the name Corporate Critic or Ethiscore. The Ethiscore is a weightable numerical rating designed as a quick guide to the ethical status of companies, or brands in a particular area, and is linked to a more detailed ethical assessment. \"alonovo\" is an online shopping portal that provides similar weightable ethical ratings termed the \"Corporate Social Behavior Index\".\n\nThe consumer rationalizes unnecessary and even unwanted consumption by saying that \"it's for a good cause\". As a result, the consumer buys pink ribbons during National Breast Cancer Awareness Month, green products to support the environment, candy and popcorn from school children, greeting cards and gift wrap from charities, and many other, often unwanted objects. The consumer avoids considering whether the price offered is fair, whether a small cash donation would be more effective with far less work, or even whether selling the item is consistent with the ostensible mission, such as when sports teams sell candy.\n\nSome of these efforts are based on concept brands: the consumer is buying an association with women's health or environmental concerns as much as she or he is buying a tangible product.\n\nIn response to an increasing demand for ethical consumerism surrounding gift giving occasions, charities have promoted an alternative gift market, in which charitable contributions are made on behalf of the gift \"recipient\". The \"recipient\" receives a card explaining the selected gift, while the actual gift item (frequently agricultural supplies or domestic animals) is sent to a family in a poor community.\n\nCritics argue that the ability to effect structural change is limited in ethical consumerism. Some cite the preponderance of niche markets as the actual effect of ethical consumerism, while others argue that information is limited regarding the outcomes of a given purchase, preventing consumers from making informed ethical choices. Critics have also argued that the uneven distribution of wealth prevents consumerism, ethical or otherwise, from fulfilling its democratic potential.\n\nOne study suggests that \"Buying Green\" serves as a license for unethical behavior – in their 2009 paper, \"Do Green Products Make Us Better People?\", Nina Mazar, Chen-Bo Zhong state the following:\n\nIn line with the halo associated with green consumerism, people act more altruistically after mere exposure to green than conventional products. However, people act less altruistically and are more likely to cheat and steal after purchasing green products as opposed to conventional products. Together, the studies show that consumption is more tightly connected to our social and ethical behaviors in directions and domains other than previously thought.\n\nIn a 2010 \"The Guardian\" article, British environmental writer and activist George Monbiot argued that green consumers who do not articulate their values are part of \"a catastrophic mistake\" on the grounds that such consumerism \"strengthens extrinsic values\" (those that \"concern status and self-advancement\"), thereby \"making future campaigns less likely to succeed\".\n\n\n"}
{"id": "57977652", "url": "https://en.wikipedia.org/wiki?curid=57977652", "title": "Evolution in fiction", "text": "Evolution in fiction\n\nEvolution has been an important theme in fiction, including speculative evolution in science fiction, since the late 19th century, though it began before Charles Darwin's time, and reflects progressionist and Lamarckist views as well as Darwin's. Darwinian evolution is pervasive in literature, whether taken optimistically in terms of how humanity may evolve towards perfection, or pessimistically in terms of the dire consequences of the interaction of human nature and the struggle for survival. Other themes include the replacement of humanity, either by other species or by intelligent machines.\n\nCharles Darwin's evolution by natural selection, as set out in his 1859 \"On the Origin of Species\", is the dominant theory in modern biology, but it is accompanied as a philosophy and in fiction by two earlier evolutionary theories, progressionism (orthogenesis) and Lamarckism. Progressionism is the view that evolution is progress towards some goal of perfection, and that it is in some way directed towards that goal. Lamarckism, a philosophy that long predates Jean-Baptiste de Lamarck, is the view that evolution is guided by the inheritance of characteristics acquired by use or disuse during an animal's lifetime.\n\nIdeas of progress and evolution were popular, long before Darwinism, in the 18th century, leading to Nicolas-Edme Rétif's allegorical 1781 story \"\" (The Southern Hemisphere Discovery by a Flying Man). \n\nThe evolutionary biologist Kayla M. Hardwick quotes from the 2013 film \"Man of Steel\", where the villain Faora states: \"The fact that you possess a sense of morality, and we do not, gives us an evolutionary advantage. And if history has taught us anything, it is that evolution always wins.\" She points out that the idea that evolution wins is progressionist, while (she argues) the idea that evolution gives evil an advantage over the moral and good, driving the creation of formidable monsters, is a popular science fiction misconception. Hardwick gives as examples of the evolution of \"bad-guy traits\" the Morlocks in H. G. Wells's 1895 \"The Time Machine\", the bugs' caste system in Robert Heinlein's 1959 \"Starship Troopers\", and the effective colonisation by Don Siegel's 1956 \"Invasion of the Body Snatchers\" aliens.\n\nIn French 19th century literature, evolutionary fantasy was Lamarckian, as seen in Camille Flammarion's 1887 \"Lumen\" and his 1894 \"\", J.-H. Rosny's 1887 \"Les Xipéhuz\" and his 1910 \"La mort de la terre\", and Jules Verne's 1901 \"La grande forêt, le village aérien\". The Lamarckist philosopher Henri Bergson's creative evolution driven by the supposed élan vital likely inspired J. D. Beresford's English evolutionary fantasy, his 1911 \"The Hampdenshire Wonder\".\n\nDarwin's version of evolution has been widely explored in fiction, both in fantasies and in imaginative explorations of its grimmer \"survival of the fittest\" effects, with much attention focused on possible human evolution. H. G. Wells's \"The Time Machine\" already mentioned, his 1896 \"The Island of Dr Moreau\", and his 1898 \"The War of the Worlds\" all pessimistically explore the possible dire consequences of the darker sides of human nature in the struggle for survival. More broadly, Joseph Conrad's 1899 \"Heart of Darkness\" and R. L. Stevenson's 1886 \"Dr Jekyll and Mr Hyde\" portray Darwinian thinking in mainstream English literature.\n\nThe evolutionary biologist J. B. S. Haldane wrote an optimistic tale, \"The Last Judgement\", in the 1927 collection \"Possible Worlds\". This influenced Olaf Stapledon's 1930 \"Last and First Men\", which portrays the many species that evolved from humans in a billion-year timeframe. A different take on Darwinism is the idea, popular from the 1950s onwards, that humans will evolve more or less godlike mental capacity, as in Arthur C. Clarke's 1950 \"Childhood's End\" and Brian Aldiss's 1959 \"Galaxies Like Grains of Sand\". Another science fiction theme is the replacement of humanity on Earth by other species or intelligent machines. For instance, Olof Johannesson's 1966 \"The Great Computer\" gives humans the role of enabling intelligent machines to evolve, while Kurt Vonnegut's 1985 \"Galapagos\" is one of several novels to depict a replacement species.\n\n"}
{"id": "48692630", "url": "https://en.wikipedia.org/wiki?curid=48692630", "title": "Fiber modification", "text": "Fiber modification\n\nFibre modification is a research field in which researchers aim at developing and applying technologies to impart new properties to natural fibres such as those in paper, in order to increase their functionality. Research areas in this field include many different technologies, amongst which the chemical modifications of fibres are widely used. One important sector of application of the chemical modifications is the treatment of wood for giving it enhanced properties such as higher mechanical properties, water impermeability, less hygroscopicity, bacterial and fungal resistance. Transferring and adapting the technical knowledge on fibre modification available for the wood sector to the recycled paper sector is an innovative use of these chemical treatments which has been the subject of studies that have been carried out within an EU co-funded project called Fibre+.\n\nThe project Consortium included members representing Paper and Packaging European Associations (CEPI, FEFCO), and research institutes specialized in the wood and paper for packaging sector and paper and packaging companies. The focus of the project was on chemical modification of the paper made recycled fibres, investigating the possibility to transfer wood fibre modifications technique from the wood sector to paper and packaging sector. The aim was to enhance the properties of papers and of the packaging, as the recycling process causes the deterioration of fibres.\n\nThe chemical modification of recycled fibres aimed at the creation of a new generation of packaging papers characterized for being more recyclable, less hygroscopic, stiff and durable. The high recycling rate of papers in Europe (which is at the level of 72%) and the consequent importance that recycling has for the circular economy, were at the basis of this study. Paper products form part of an integrated carbon cycle based on the photosynthetic conversion of water, carbon dioxide, nutrients and solar energy into renewable wood-based biomass. Once consumed, paper may be recovered and used again either as a source of secondary fibres, to produce recycled paper or as bio-fuel. Fibre packages or corrugated containers made from corrugated board were the ones that were dealt with in the project, as they are considered as being the most prominent structural application of paper.\n\nFibre modification with chemicals or enzymes had been investigated in the production of fibreboards. Fibre modification applying steam (steam-exploded wood) has been proved an efficient pre-treatment method in producing thermoplastic composites.\nCurrent theories for interfibre bonding during papermaking process are based on general recognition of the hydrogen bonding model. Consequently, all effort for boosting fibre strength is connected to mechanical beating of fibres in order to generate more flexible and fragmented fibres for increased bonding areas. As a consequence, significant drawbacks are obtained in terms of water retention ratio resulting in poor dewatering behaviour and high energy consumption. New mechanisms for interpretation and control of interfibre bonding are still upcoming. One way to overcome these drawbacks could be the molecular coating of cellulose fibres using polymers targeted on entropy controlled mixing of polymers and cellulose gel resulting in higher bonding forces. Theoretical results as well as experimental data on how application of polymeric layers (e.g. carboxymethyl cellulose) and enzymes on cellulose fibres can lead to sheets of high bonding strength without any mechanical beating have been already presented. However, these attempts were still far from any industrial implementation and their application would have been costly and would not solve the problem of raw material availability.\n\nBased on this state-of-the-art, the objective of the project was to modify and thus improve the characteristics of different types of recovered fibres used for the production of a variety of packaging grade papers used as linerboard and corrugating medium for corrugated board manufacturing in Europe. Information on the actual furnish characteristics and composition of packaging materials is expected to help European packaging industry to evaluate its sources of supply and to adopt suitable methods and processes to improve the available resources in an optimal manner. In the case of packaging, scientific technical knowledge of practical industrial relations between fibre characteristics, paper properties and corrugated board properties also is needed.\n\nFrom wood fibre modification to paper technology chemical wood modification aims at altering the structure of the cell wall matrix. Wood\nproperties are improved considerably by converting hydrophilic OH-groups into larger more hydrophobic groups. Also the physical fixation of modifying chemicals in the cell wall matrix can considerably change the wood properties. In addition to a hydrophobing effect, the treatments reduce the volume of cell wall nano-pores and thus decrease the incorporation of water molecules into the cell wall matrix. On a macroscopic scale, wood modification can change important properties of the wood including biological durability (resistance against fungi), dimensional stability, hardness and UV-stability.\n\nSince paper is produced from wood fibres, it was possible to transfer some of the developments achieved in wood technology to paper technology. Several technologies (e.g. chemical modification, nano-scale celluloses, polyelectrolytes, functional polymers based on cellulose, hemicelluloses and starch) were researched and used by different research groups around Europe (e.g. PTS and University of Goettingen, Germany; Kungliga Tekniska högskolan (KTH), Sweden.) There is already well established knowledge on chemical fibre modification of recycled fibres. Adamopoulos and Mai (2011) modified recycled fibres with N-methylol compounds and glutaraldehyde with significant improvement on fibre characteristics and paper sheet performance. Laboratory sheets manufactured with a variety of chemically modified recycled fibres were found to be superior in stiffness and hygroscopic properties than these manufactured from unmodified ones. The intention of the Fibre+ project is to build on the existing knowledge on fibre modification for adapting, implementing and disseminating this innovative technology in European paper SMEs.\n\nResults of the Fibre+ project on recycled fibres for packaging paper and information on potential developments of the Fibre+ concept can be found on the Fibre+ website including scientific articles that have been published as a consequence of the RTD work that has been carried out during the project.\n\n"}
{"id": "12476254", "url": "https://en.wikipedia.org/wiki?curid=12476254", "title": "Filler (materials)", "text": "Filler (materials)\n\nFillers are particles added to material (plastics, composite material, concrete) to lower the consumption of more expensive binder material or to better some properties of the mixtured material. Worldwide, more than 53 million tons of fillers (with a total sum of approximately EUR16 billion) are used every year in different application areas, such as paper, plastics, rubber, paints, coatings, adhesives and sealants. As such, fillers, produced by more than 700 companies, rank among the world's major raw materials and are contained in a variety of goods for daily consumer needs.\n\nIn the past, fillers were used predominantly to cheapen end products, in which case they were called extenders. Among the 21 most important fillers, calcium carbonate holds the largest market volume and is mainly used in the plastics sector. While the plastic industry mostly consumes ground calcium carbonate (GCC), the paper industry primarily uses precipitated calcium carbonate (PCC) that is derived from natural minerals. Wood flour and saw dust are used as filler in thermosetting plastic. \n\nIn some cases, fillers also enhance properties of the products, e.g. in composites. In such cases, a beneficial chemical interaction develops between the host material and the filler. As a result, a number of optimized types of fillers, nano-fillers or surface treated goods have been developed.\n\n"}
{"id": "685239", "url": "https://en.wikipedia.org/wiki?curid=685239", "title": "Gavin Maxwell", "text": "Gavin Maxwell\n\nGavin Maxwell FRSL, FIAL, FZS (Sc.), FRGS (15 July 19147 September 1969) was a Scottish naturalist and author, best known for his nonfiction writing and his work with otters. He wrote the book \"Ring of Bright Water\" (1960) about how he brought an otter back from Iraq and raised it in Scotland. The otter was of a previously unknown sub-species which was subsequently named after Maxwell. \"Ring of Bright Water\" sold more than a million copies and was made into a film starring Bill Travers and Virginia McKenna in 1969. The title \"Ring of Bright Water\" was taken from the poem \"The Marriage of Psyche\" by Kathleen Raine, who said in her autobiography that Maxwell had been the love of her life.\n\nGavin Maxwell was the youngest son of Lieutenant-Colonel Aymer Maxwell and Lady Mary Percy, fifth daughter of the seventh Duke of Northumberland. His paternal grandfather, Sir Herbert Maxwell, was an archaeologist, politician and natural historian.\n\nMaxwell was raised in the small village of Elrig, near Port William, in Wigtownshire, south-western Scotland. Maxwell's relatives still live in the area and the family's ancient estate and grounds are in nearby Monreith.\n\nMaxwell's education took place at a succession of preparatory and public schools, including the sporty Heddon Court School at East Barnet, St Cyprian's School, where he found encouragement for his interest in natural history, and Stowe School. In \"The Rocks Remain\", he relates how family pressure led him to take a degree in Estate Management at Hertford College, Oxford, where he spent his time pursuing sporting and leisure activities instead of studying. He cheated his way through the intermediate exams but passed the final examinations honestly, having crammed the entire three-year course in six weeks.\n\nDuring World War II, Maxwell served as an instructor with the Special Operations Executive. After the war, he purchased the Isle of Soay off Skye in the Inner Hebrides, Scotland. According to his book \"Harpoon at a Venture\" (1952), bad planning and a lack of finance meant his attempt to establish a basking shark fishery there between 1945-48 proved unsuccessful and the island was sold on to his business partner, Tex Geddes. He became a close friend of Elias Canetti.\n\nIn 1956, Maxwell toured the reed marshes of Southern Iraq with explorer Wilfred Thesiger. Maxwell's account of their trip appears in \"A Reed Shaken By The Wind\", later published under the title \"People of the Reeds\". It was hailed by \"The New York Times\" as \"near perfect\".\n\nMaxwell next moved to Sandaig (which he called \"Camusfeàrna\" in his books), a small community opposite Isleornsay on a remote part of the Scottish mainland. This is where his \"otter books\" are set. After \"Ring of Bright Water\" (1960), he wrote \"The Rocks Remain\" (1963), in which the otters Edal, Teko, Mossy and Monday show great differences in personality. \"The Rocks Remain\" is a sequel to \"Ring of Bright Water\", and demonstrates the difficulty Maxwell was having, possibly as a result of his mental state, in remaining focused on one project and the impact that had on his otters, Sandaig and his own life.\n\nIn 1966, he travelled to Morocco, tracing the dramatic lives of the last rulers of Morrakesh under the French. His account of the trip was published as \"Lords of the Atlas: The Rise and Fall of the House of Glaoua 1893-1956\". During the Moroccan Years of Lead, the regime there considered his book subversive and banned its importation.\n\nIn \"The House of Elrig\" (1965), Maxwell describes his family history and his passion for the calf-country, Galloway, where he was born. It was during this period that he met ornithologist Peter Scott and the young Terry Nutkins, who later became a children's television presenter. Privately homosexual, Maxwell married Lavinia Renton (daughter of The Right Honourable Sir Alan Lascelles and granddaughter of Viscount Chelmsford, Wilfred Thesiger's uncle) on 1 February 1962. The marriage lasted little more than a year and they divorced in 1964. Maxwell also lived in Paultons Square in London.\n\nIn 1968, Maxwell's Sandaig home was destroyed by fire and he moved to the lighthouse keepers' cottages on Eilean Bàn (White Island), an island between the Isle of Skye and the Scottish mainland by Kyleakin. He invited John Lister-Kaye to join him on Eilean Bàn and help him build a zoo on the island and work on a book about British wild mammals. Lister-Kaye accepted the invitation, but both projects were abandoned when Maxwell died from cancer the following year.\n\nEilean Bàn now supports a pier of the Skye Bridge, built during the 1990s. Despite modern traffic a hundred feet or so above it, the island is a commemorative otter sanctuary and houses a museum dedicated to Maxwell. Another memorial is a bronze otter erected at Monreith near to St Medan's Golf Club.\n\nAccording to Douglas Botting, Maxwell suffered from bipolar disorder throughout his life. Maxwell's literary agent was Peter Janson-Smith, who was also agent for James Bond author Ian Fleming.\n\nMaxwell's book \"Ring of Bright Water\" describes how, in 1956, he brought a smooth-coated otter back from Iraq and raised it in \"Camusfearna\" (Sandaig) on the west coast of Scotland. He took the otter, called Mijbil, to the London Zoological Society, where it was decided that this was a previously unknown sub-species of smooth-coated otter. It was therefore named \"Lutrogale perspicillata maxwelli\" (or, colloquially, \"Maxwell's otter\") after him. While it was thought to have become extinct in the alluvial salt marshes of Iraq as a result of the large-scale drainage of the area that started in the 1960s, newer surveys suggest large populations remain throughout its range.\n\nIn his book \"The Marsh Arabs\", Wilfred Thesiger wrote:\n\nThe otter became woven into the fabric of Maxwell's life. Kathleen Raine's relationship with Maxwell deteriorated after 1956 when she indirectly caused the death of Mijbil. Raine held herself responsible not only for losing Mijbil but for a curse she had uttered shortly beforehand, frustrated by Maxwell's homosexuality: \"Let Gavin suffer in this place as I am suffering now.\" Raine blamed herself thereafter for all Maxwell's misfortunes, beginning with Mijbil's death and ending with the cancer that took his life in 1969.\n\nMaxwell's ashes were scattered on the site of his house \"Camusfeàrna\".\n\n\n\n"}
{"id": "55321500", "url": "https://en.wikipedia.org/wiki?curid=55321500", "title": "Geochemistry of carbon", "text": "Geochemistry of carbon\n\nThe geochemistry of carbon is the study of the transformations involving the element carbon within the systems of the Earth. To a large extent this study is organic geochemistry, but it also includes the very important carbon dioxide. Carbon is transformed by life, and moves between the major phases of the Earth, including the water bodies, atmosphere, and the rocky parts. Carbon is important in the formation of organic mineral deposits, such as coal, petroleum or natural gas. Most carbon is cycled through the atmosphere into living organisms and then respirated back into the atmosphere. However an important part of the carbon cycle involves the trapping of living matter into sediments. The carbon then becomes part of a sedimentary rock when lithification happens.\nHuman technology or natural processes such as weathering, or underground life or water can return the carbon from sedimentary rocks to the atmosphere. From that point it can be transformed in the rock cycle into metamorphic rocks, or melted into igneous rocks. Carbon can return to the surface of the Earth by volcanoes or via uplift in tectonic processes. Carbon is returned to the atmosphere via volcanic gases.\nCarbon undergoes transformation in the mantle under pressure to diamond and other minerals, and also exists in the Earth's outer core in solution with iron, and may also be present in the inner core.\n\nCarbon can form a huge number of different stable compounds. It is an essential component of living matter.\nLiving organisms can live in a limited range of conditions on the Earth that are limited by temperature and the existence of liquid water. The potential habitability of other planets or moons can also be assessed by the existence of liquid water.\n\nCarbon makes up only 0.08% of the combination of the lithosphere, hydrosphere, and atmosphere. Yet it is the twelfth most common element there. In the rock of the lithosphere, carbon commonly occurs as carbonate minerals containing calcium or magnesium. It is also found as fossil fuels in coal and petroleum and gas. Native forms of carbon are mcuh rarer, requiring pressure to form. Pure carbon exists as graphite or diamond.\n\nThe deeper parts of Earth such as the mantle are very hard to discover. Few samples are known, in the form of uplifed rocks, or xenoliths. Even fewer remain in the same state they were in where the pressure and temperature is much higher. Some diamonds retain inclusions held at pressures they were formed at, but the temperature is much lower at the surface. Iron meteorites may represent samples of the core of an asteroid, but it would have formed under different conditions to the Earth's core. Therefore experimental studies are conducted in which minerals or substances are compressed and heated to determine what happens in similar conditions to the planetary interior.\n\nThe two common isotopes of carbon are stable. On Earth, carbon 12, C is by far the most common at 98.894%. Carbon 13 is much rarer averaging 1.106%. This percentage can vary slightly and its value is important in isotope geochemistry whereby the origin of the carbon is suggested.\n\nCarbon can be produced in stars at least as massive as the Sun by fusion of three helium-4 nuclei: He + He + He --> C. This is the triple alpha process.\nIn stars as massive as the Sun, carbon 12 is also converted to carbon 13 and then onto nitrogen 14 by fusion with protons. C + H --> C + e. C + H --> N. In more massive stars, two carbon nuclei can fuse to magnesium, or a carbon and an oxygen to sulfur.\n\nIn molecular clouds, simple carbon molecules are formed, including carbon monoxide and dicarbon. Reactions with the trihydrogen cation of the simple carbon molecules yield carbon containing ions that readily react to form larger organic molecules. Carbon compounds that exist as ions, or isolated gas molecules in the interstellar medium, can condense onto dust grains. Carbonaceous dust grains consist mostly of carbon. Grains can stick together to form larger aggregates.\n\nMeteorites and interplanetary dust shows the composition of solid material at the start of the Solar System, as they have not been modified since its formation. Carbonaceous chondrites are meteorites with around 5% carbon compounds. Their composition resembles the Sun's minus the very volatile elements like hydrogen and noble gases.\nThe Earth is believed to have formed by the gravitational collapse of material like meteorites.\n\nImportant effects on Earth in the first Hadian Era include strong solar winds during the T-Tauri stage of the Sun. The Moon forming impact caused major changes to the surface. Juvenile volatiles outgased from the early molten surface of the Earth. These included carbon dioxide and carbon monoxide. The emissions probably did not include methane, but the Earth was probably free of molecular oxygen. The Late Heavy Bombardment was between 4.0 and 3.8 billion years ago (Ga). To start with, the Earth did not have a crust as it does today. Plate tectonics in its present form commenced about 2.5 Ga.\n\nEarly sedimentary rocks formed under water date to 3.8 Ga. Pillow lavas dating from 3.5 Ga prove the existence of oceans. Evidence of early life is given by fossils of stromatolites, and later by chemical tracers.\n\nOrganic matter continues to be added to the Earth from space via interplanetary dust, which also includes some interstellar particles. The amounts added to the Earth were around 60,000 tonnes per year about 4 Ga.\n\nBiological sequestration of carbon causes enrichment of carbon-12, so that substances that originate from living organisms have a higher carbon-12 content. Due to the kinetic isotope effect, chemical reactions can happen faster with lighter isotopes, so that photosynthesis fixes lighter carbon-12 faster than carbon-13. Also lighter isotopes diffuse across a biological membrane faster. Enrichment in carbon 13 is measured by delta C(o/oo) = [(C/C)sample/(C/C)standard - 1] * 1000.\nThe common standard for carbon is Cretaceous Peedee formation belemnite.\n\nComplex molecules, in particular those containing carbon can be in the form of stereoisomers. With abiotic processes they would be expected to be equally likely, but in carbonaceous chondrites this is not the case. The reasons for this are unknown.\n\nThe outer layer of the Earth, the crust along with its outer layers contain about 10 kg of carbon. This is enough for each square meter of the surface to have 200 tons of carbon.\n\nCarbon added to sedimentary rocks can take the form of carbonates, or organic carbon compounds. In order of source quantity the organic carbon comes from phytoplankton, plants, bacteria and zooplankton. However terrestrial sediments may be mostly from higher plants, and some oxygen deficient sediments from water may be mostly bacteria. Fungi and other animals make insignificant contributions. On the oceans the main contributor of organic matter to sediments is plankton, either dead fragments or faecal pellets termed marine snow. Bacteria degrade this matter in the water column, and the amount surviving to the ocean floor is inversely proportional to the depth. This is accompanied by biominerals consisting of silicates and carbonates. The particulate organic matter in sediments is about 20% of known molecules 80% of material that cannot be analysed. Detritivores consume some of the fallen organic materials. Aerobic bacteria and fungi also consume organic matter in the oxic surface parts of the sediment. Coarse-grained sediments are oxygenated to about half a meter, but fine grained clays may only have a couple of millimetres exposed to oxygen. The organic matter in the oxygenated zone will become completely mineralized if it stays there long enough.\n\nDeeper in sediments where oxygen is exhausted, anaerobic biological processes continue at a slower rate. These include anaerobic mineralization making ammonium, phosphate and sulfide ions; fermentation making short chain alcohols, acids or methyl amines; acetogenesis making acetic acid; methanogenesis making methane, and sulfate, nitrite and nitrate reduction. Carbon dioxide and hydrogen are also outputs. Under freshwater, sulfate is usually very low, so methanogensis is more important. Yet other bacteria can convert methane, back into living matter, by oxidising with other substrates. Bacteria can reside at great depths in sediments. However sedimentary organic matter accumulates the indigestible components.\n\nDeep bacteria may be lithotrophes, using hydrogen, and carbon dioxide as a carbon source.\n\nIn the oceans and other waters there is much dissolved organic materials. These are several thousand years old on average, and are called gelbstoff (yellow substance) particularly in fresh waters. Much of this is tannins. The nitrogen containing materials here appear to be amides, perhaps from peptidoglycans from bacteria. Microorganisms have trouble consuming the high molecular weight dissolved substances, but quickly consume small molecules.\n\nFrom terrestrial sources black carbon produced by charring is an important component. Fungi are important decomposers in soil.\n\nProteins are normally hydrolysed slowly even without enzymes or bacteria, with a half life of 460 years, but can be preserved if they are desiccated, pickled or frozen. Being enclosed in bone also helps preservation. Over time the amino acids tend to racemize, and those with more functional groups are lost earlier. Protein still will degrade on the timescale of a million years. DNA degrades rapidly, lasting only about four years in water. Cellulose and chitin have a half life in water at 25° of about 4.7 million years. Enzymes can accelerate this by a factor of 10. About 10 tons of chiting are produced each year, but it is almost all degraded.\n\nLignin is only efficiently degraded by fungii, white rot, or brown rot. These require oxygen.\n\nLipids are hydrolysed to fatty acids over long time periods. Plant cuticle waxes are very difficult to degrade, and may survive over geological time periods.\n\nMore organic matter is preserved in sediments if there is high primary production, or the sediment is fine-grained. The lack of oxygen helps preservation greatly, and that also is caused by a large supply of organic matter. Soil does not usually preserve organic matter, it would need to be acidified or water logged, as in the bog. Rapid burial ensures the material gets to an oxygen free depth, but also dilutes the organic matter. A low energy environment ensures the sediment is not stirred up and oxygenated. Salt marshes and mangroves meet some of these requirements, but unless the sea level is rising will not have a chance to accumulate much. Coral reefs are very productive, but are well oxygenated, and recycle everything before it is buried.\n\nIn dead \"Sphagnum\", sphagnan a polysaccharide with D-lyxo-5-hexosulouronic acid is a major remaining substance. It make the bog very acidic, so that bacteria cannot grow. Not only that, the plant ensures there is no available nitrogen. Holocellulose also absorbs any digestive enzymes around. Together this leads to major accumulation of peat under sphagnum bogs.\n\nEarth's mantle is a significant reservoir of carbon. The mantle contains more carbon than the crust, oceans, biosphere, and atmosphere put together. The figure is estimated to be very roughly 10 kg. Carbon concentration in the mantle is very variable, varying by more than a factor of 100 between different parts.\n\nThe form carbon takes depends on its oxidation state, which depends on the oxygen fugacity of the environment. Carbon dioxide and carbonate are found where the oxygen fugacity is high. Lower oxygen fugacity results in diamond formation, first in eclogite, then peridotite, and lastly in fluid water mixtures. At even lower oxygen fugacity, methane is stable in contact with water, and even lower, metallic iron and nickel form along with carbides. Iron carbides include FeC and FeC.\n\nMinerals that contain carbon include calcite and its higher density polymorphs. Other significant carbon minerals include magnesium and iron carbonates. Dolomite is stable above 100 km depth. Below 100 km, dolomite reacts with orthopyroxine (found in peridotite) to yield magnesite (an iron magnesium carbonate). Below 200 km deep, carbon dioxide is reduced by ferrous iron (Fe), forming diamond, and ferric iron (Fe). Even deeper pressure induced disproportionation of iron minerals produces more ferric iron, and metallic iron. The metallic iron combines with carbon to form the mineral cohenite with formula FeC. Cohenite also contains some nickel substituting for iron. This form or carbon is called \"carbide\". Diamond forms in the mantle below 150 km deep, but because it is so durable, it can survive in eruptions to the surface in kimberlites, lamproites, or ultramafic lamprophyres.\n\nXenoliths can come from the mantle, and different compositions come from different depths. Above 90 km (3.2 GPa) spinel peridotite occurs, below this garnet peridotite is found.\n\nInclusions trapped in diamond can reveal the material and conditions much deeper in the mantle. Large gem diamonds are usually formed in the transition zone part of the mantle, (410 to 660 km deep) and crystallise from a molten iron-nickel-carbon solution, that also contains sulfur and trace amounts of hydrogen, chromium, phosphorus and oxygen. Carbon atoms constitute about 12% of the melt (about 3% by mass). Inclusions of the crystallised metallic melt are sometimes included in diamonds. Diamond can be caused to precipitate from the liquid metal, by increasing pressure, or by adding sulfur.\n\nFluid inclusions in crystals from the mantle have contents that most often are liquid carbon dioxide, but which also include carbon oxysulfide, methane and carbon monoxide\n\nMaterial is added by subduction from the crust. This includes the major carbon containing sediments such as limestone, or coal. Each year 2×10 kg of CO is transferred from the crust to the mantle by subduction. (1700 tons of carbon per second).\n\nUpwelling mantle material can add to the crust at mid oceanic ridges. Fluids can extract carbon from the mantle and erupt in volcanoes. At 330 km deep a liquid consisting of carbon dioxide and water can form. It is highly corrosive, and dissolves incompatible elements from the solid mantle. These elements include uranium, thorium, potassium, helium and argon. The fluids can then go on to cause metasomatism or extend to the surface in carbonatite eruptions. The total mid oceanic ridge, and hot spot volcanic emissions of carbon dioxide match the loss due to subduction: 2×10 kg of CO per year.\n\nIn slowly convecting mantle rocks, diamond that slowly rises above 150 km will slowly turn into graphite or be oxidised to carbon dioxide or carbonate minerals.\n\nEarth's core is believed to be mostly and alloy of iron and nickel. The density indicates that it also contains a significant amount of lighter elements. Elements such as hydrogen would be stable in the Earth's core, however the conditions at the formation of the core would not be suitable for its inclusion. Carbon is a very likely constituent of the core. Preferential partitioning of the carbon isotopeC into the metallic core, during its formation, may explain why there seems to be more C on the surface and mantle of the Earth compared to other solar system bodies (−5‰ compared to -20‰). The difference can also help to predict the value of the carbon proportion of the core.\n\nThe outer core has a density around 11 cm, and a mass of 1.3×10kg. It contains roughly 10 kg of carbon.\nCarbon dissolved in liquid iron affect the solution of other elements. Dissolved carbon changes lead from a siderophile to a lithophile. It has the opposite effect on tungsten and molybdenum, causing more tungsten or molybdenum to dissolve in the metallic phase. The measured amounts of these elements in the rocks compared to the Solar System can be explained by a 0.6% carbon composition of the core.\n\nThe inner core is about 1221 km in radius. It has a density of 13 g cm, and a total mass of 9×10 kg and a surface area of 18,000,000 square kilometers. Experiments with mixtures under pressure and temperature attempt to reproduce the known properties of the inner and outer core. Carbides are among the first to precipitate from a molten metal mix, and so the inner core may be mostly iron carbides, FeC or FeC. At atmospheric pressure (100 kpa)the iron-FeC eutectic point is at 4.1% carbon. This percentage decreases as pressure increases to around 50 GPa. Above that pressure the percentage of carbon at the eutectic increases. The pressure on the inner core ranges from 330 GPa to 360 GPa at the centre of the Earth. The temperature at the inner core surface is about 6000 K. The material of the inner core must be stable at the pressure and temperature found there, and more dense than that of the outer core liquid. Extrapolations show that either FeC or FeC match the requirements. FeC is 8.4% carbon, and FeC is 6.7% carbon. The inner core is growing by about 1 mm per year, or adding about 18 cubic kilometres per year. This is about 18×10kg of carbon added to the inner core every year. It contains about 8×10 kg of carbon.\n\nIn order to determine the fate of natural carbon containing substances deep in the Earth, experiments have been conducted to see what happens when high pressure, and or temperatures are applied. Such substances include carbon dioxide, carbon monoxide, graphite, methane, and other hydrocarbons such as benzene, carbon dioxide water mixtures and carbonate minerals such as calcite, magnesium carbonate, or ferrous carbonate. Under super high pressures carbon may take on a higher coordination number than the 4 found in sp compounds like diamond, or the 3 found in carbonates. Perhaps carbon can substitute into silicates, or form a silicon oxycarbide. Carbides may be possible.\n\nAt 15 Gpa graphite changes to a hard transparent form, that is not diamond. Diamond is very resistant to pressure, but at about 1 TPa (1000 Gpa) transforms to a BC-8 form.\n\nCarbides are predicted to be more likely lower in the mantle as experiments have shown a much lower oxygen fugacity for high pressure iron silicates. Cohenite remains stable to over 187 Gpa, but is predicted to have a denser orthorhombic \"Cmcm\" form in the inner core.\n\nUnder 0.3 GPa pressure, carbon dioxide is stable at room temperature in the same form as dry ice. Over 0.5 GPa carbon dioxide forms a number of different solid forms containing molecules. At pressures over 40 Gpa and high temperatures, carbon dioxide forms a covalent solid that contains CO tetrahedra, and has the same structure as β-cristobalite. This is called phase V or CO-V. When CO-V is subjected to high temperatures, or higher pressures, experiments show it breaks down to form diamond and oxygen. In the mantle the geotherm would mean that carbon dioxide would be a liquid till a pressure of 33 GPa, then it would adopt the solid CO-V form till 43 Gpa, and deeper than that would make diamond and fluid oxygen.\n\nHigh pressure carbon monoxide forms the high energy polycarbonyl covalent solid, however it is not expected to be present.\n\nUnder 1.59 GPa pressure at 25 °C, methane converts to a cubic solid. The molecules are rotationally disordered. But over 5.25 GPa the molecules become locked into position and cannot spin. Other hydrocarbons under high pressure have hardly been studied.\n\nCalcite changes to calcite-II and calcite-III at pressures of 1.5, and 2.2 GPa. Siderite undergoes a chemical change at 10 GPa at 1800K to form FeO. Dolomite decomposes 7GPa and below 1000 °C to yield aragonite and magnesite. However there are forms of iron containing dolomite stable at higher pressures and temperatures. Over 130 GPa aragonite undergoes a transformation to a SP tetrahedrally connected carbon, in a covalent network in a \"C\"222 structure. Magnesite can survive 80 GPa, but with more than 100 GPa (as at a depth of 1800 km it changes to forms with three-member rings of CO tetrahedra (CO). If iron is present in this mineral, at these pressures it will convert to magnetite and diamond. Melted carbonates with SP carbon are predicted to be very viscous.\n\nSome minerals that contain both silicate and carbonate exist, spurrite and tilleyite. But high pressure forms have not been studied. There have been attempts to make silicon carbonate. Six coordinated silicates mixed with carbonate should not exist on Earth, may exist on more massive planets.\n"}
{"id": "51628478", "url": "https://en.wikipedia.org/wiki?curid=51628478", "title": "German Climate Action Plan 2050", "text": "German Climate Action Plan 2050\n\nThe German Climate Action Plan 2050 () is a climate protection policy document approved by the German government on 14November 2016. The plan outlines measures by which Germany can meet its various national greenhouse gas emissions reduction goals through to 2050 (see table) and service its international commitments under the 2016 Paris Climate Agreement. The Federal Ministry for the Environment, Nature Conservation, Building and Nuclear Safety (BMUB), under minister Barbara Hendricks, led the development of the plan. The plan was progressively watered down since a draft was first leaked in 2016. Projections from the environment ministry in September 2016 indicate that Germany will likely miss its 2020 climate target.\n\nThe Climate Action Plan2050 should not be confused with an earlier document, the Climate Action Programme2020 (\"\"), approved in December 2014 and which only covers the period until 2020. In early 2017 it was agreed that the 2020 Programme would be scrapped.\n\nGermany announced the following official greenhouse gas emissions targets on 28September 2010.\n\nRegarding European Union policy, in October 2009 the Council of the European Union agreed that the appropriate abatement objective for Europe and other developed economies was 80–95% below 1990 levels by 2050 (consistent with Germany). \nIn October 2014 the European Council endorsed a binding European Union target of an at least 40% reduction in domestic greenhouse gas emissions by 2030 relative to 1990 (less stringent than Germany).\n\nThe Climate Action Programme2020 (\"\") is an attempt by Germany to help meet its official 2020 greenhouse gas emissions reductions target after a 'climate gap' had been identified. Projections from 2013 show Germany may miss its 40% reductions target by about without additional measures. The first draft of the programme was released in 2014. How best to control the contribution from coal-fired generation remained controversial, but economy and energy minister Sigmar Gabriel (SPD) dismissed plans aired in his ministry in October 2014 to retire 10GW of coal capacity.\n\nThe Climate Action Programme2020 was approved on 3December 2014. The official document is available in English.\n\nThe most significant part of the new package is a pledge to cut electricity sector emissions substantially by 2020. To do so, the government proposes to cap emissions from the sector at 22million tonnes between 2016 and 2020 or 4.4million tonnes each year. Once the cap is in place, energy companies will be allocated allowances based on their current emissions. If they cut their emissions by a greater amount, they may be able to sell their surplus to other companies. The package also contains measures to improve energy efficiency and the transport sector. Environmental groups criticized the package for not going far enough in reducing the reliance on coal-fired generation.\n\nIn December 2015, the government remained confident that its 2020 emissions target will be met through the measures contained in the programme and elsewhere. However a government report, released on 30September 2016, shows that Germany will meet its 2020 greenhouse gas commitments only under \"a best case scenario\". The report, prepared for the European Union, is available in German. The analysis projects Germany's greenhouse gas emissions forward for the next 20years.\n\nA PwC report from November 2016 evaluates the economic and ecological effects of the Climate Action Programme2020 and finds that the economic benefits outweigh the costs of the proposed measures. Based on the report, environment minister Barbara Hendricks said in a press release that \"the programme will create additional jobs and a GDP increase of 1% by 2020\".\n\nIn early 2017 the government agreed to scrap the 2020 plan entirely.\n\nThe history of the Climate Action Plan2050 is quite involved. The Federal Ministry for the Environment, Nature Conservation, Building and Nuclear Safety (BMUB) is the lead agency for the plan, under the direction of environment minister Barbara Hendricks (SPD). A novel public consultation process was utilized to collect ideas from state and city governments, advocacy groups, and citizens and these ideas were then used to help create the first version. Internal drafts of the plan were leaked three times to the media in 2016, the first in , the second in , and the third in .\n\nThe notion of a climate action plan arose from the coalition agreement between the CDU, CSU, and SPD parties in 2013. The agreement stated:\n\nA public consultation with stakeholders began on 2015 with a Kick-off Conference in Berlin. A discussion paper for this exercise was dated 9June 2015 and is available in English. The consultation process involved a series of meetings with states (\"\"), municipalities, associations, and citizens, with delegates selected to represent these various groupings in subsequent forums. The process was organized by the Wuppertal Institute and the dialog agency IFOK. The resulting proposals for action were collated and presented at a meeting at the environment ministry (BMUB) on 2016. The final report is available in German. This input was used to help create the first draft of the Climate Action Plan2050. The German government also explained its public participation process at the 2015 United Nations Climate Change Conference () on 9December 2015. The design of the consultation process was novel for Germany. An explanatory video from the BMUB is available.\n\nA draft of the plan was leaked to media for the first time in 2016. It was the result of consolidating a long catalog of measures from the consultation process. The leaked plan included proposals for:\n\n\nThe plan also suggested a \"pluralistic\" (meaning diverse groups are represented) commission be established, tasked with developing a coal phase-out plan by .\n\nFollowing the leaked document, state governments became increasingly concerned that they were being railroaded into climate change goals that could damage their regional economies.\n\nA draft of the plan was leaked to media for the second time in 2016. The draft was dated 21June 2016 and is available for download in German. Unlike the first leak, this draft was compiled after consultation with the economics and energy ministry (BMWi).\n\nThe new draft shows that individual sectors may escape specific emissions targets and that an end date for coal-fired generation has been omitted. Earlier versions had contained sector-specific targets for energy, transport, industry, buildings, and agriculture. The energy sector which was previously slated to make a \"considerable\" contribution is now required to make an \"adequate\" contribution. Rather than saying that coal-fired generation must \"end well before 2050\" the new draft emphasizes that \"the importance of power production from coal will decrease\" and that there will be a \"step-by-step reduction\". The previous draft stated that the transport sector would need to deliver \"disproportionately high\" emissions reductions (on account of the poor performance of the sector to date), that has now diminished to an \"ambitious\" contribution. Other proposals remain, including the development of ecological tax reform. The plan also includes targets for modernizing the heating and cooling systems in buildings, including no new fossil-fueled heating systems in houses after 2030.\n\nThe proposal for a Commission on Climate Protection, Growth, Structural Change, and the Completion of the \"Energiewende\" (\"\") remains, but now without the specific task of developing a roadmap for the phase-out of coal.\n\nThe weakening of the plan is also a result of industry associations repeatedly criticizing sector targets and itemized measures, which they fear would harm Germany's economic performance and international competitiveness.\n\nAn official draft was released on 6September 2016 and is available in German. This draft retains a provision for the establishment of a Commission on Climate Protection, Growth, Structural Change, and the Completion of the \"Energiewende\".\n\nThe content however has been watered down considerably. Concrete emissions reductions targets were removed altogether. The previously leaked June 2016 draft stated that by 2030 \"a large majority of newly registered cars\" would need to be powered by either electricity or biofuels. But the new draft merely states that \"the government aims to significantly lower car emissions by 2030\" and that electric cars can contribute to that goal.\n\nEnvironmental groups have become increasingly critical of the plan as each iteration led to a watering down of its climate protection provisions. The NGOs were particularly concerned about the lack of detail concerning a coal phase-out. On 24 September 2016, Greenpeace Germany issued a report critical of the watering down of the results of the consultation process.\n\nIn , the plan went to the Chancellery (') to coordinate the final version to be agreed by the German cabinet (').\n\nMedia reports in 2016 suggest a deepening rift between the economics and energy ministry (BMWi) and the environment ministry (BMUB) over the plan and the exit process for coal-fired generation (\"\"). Economy and energy minister Sigmar Gabriel opposes the setting of a coal exit date before job alternatives for lignite workers have been determined.\n\nA revised draft plan, circulated by the environment ministry on 4November 2016, was obtained by \"Süddeutsche Zeitung\". The plan has become more stringent and now includes passages that had earlier been removed during the negotiations between ministries. The new plan also precludes new coal-fired generation and the expansion of existing open-pit mines and calls on the government to lobby for an EU-wide floor price for auctioned EU ETS emissions allowances.\n\nOn 7November 2016, over 40 German companies, including energy suppliers EnBW and MVV Energie and network operator 50Hertz, together with Commerzbank, Deutsche Telekom, IKEA, and Hochtief, are lobbying for a more ambitious program, one which ensures that Germany's Paris Agreement commitments are met. The companies want sector-specific emissions targets for 2030 and state that \"only in this way can new business models and concrete plans for decarbonisation be developed\". They want the goal to be the \"rapid switch to 100% renewable energy\". British economist Nicholas Stern also backs a more ambitious plan. The original statement from the companies is available.\n\nOn 8November 2016, economy and energy minister Gabriel vetoed the plan amid concerns by trade union IG BCE and supported by the industry group. The draft does not timetable a phase-out for brown coal, notwithstanding Gabriel said he expected brown coal to remain in use past 2040. On 11November 2016, Reuters news agency reports that chancellor Angela Merkel and ministers Gabriel and Hendricks have agreed on a new draft.\n\nOn 14November 2016, the cabinet officially adopted and released the new plan. It was overseen by a CDU/CSU/SPD grand coalition government, led by Angela Merkel. The timing allowed environment minister Barbara Hendricks to present the German plan at the COP22 climate talks held in Marrakesh, Morocco. Canada, Mexico, and the US also presented climate action plans.\n\nThe official Climate Action Plan2050 is available in German and runs to 91pages. An official summary of the principles and goals underpinning the plan is available in English.\n\nThe plan is to be supplemented by a program of policy measures, developed by the German parliament (\"\"), with the first such program to be in place in 2018. Annual reporting will track progress and should facilitate specific policy adjustments where needed.\n\nThe plan begins with a preamble that states that the document is an evolving work in progress and \"cannot and does not want to be a detailed masterplan\". It adds that there will be \"no rigid provisions\" and that the plan is technology neutral and open to innovation. The preamble stresses that the government will simultaneously maintain German competitiveness:\n\nThe preamble also reiterates Germany's 2010 climate targets (see table) and its 2016 Paris Agreement commitment.\n\nSector targets remained controversial during the development of the plan. Notwithstanding, the official document now specifies sector targets for the first time under a national climate protection policy. The initial sector targets will be subject to a comprehensive impact assessment and consultation and may well be adjusted as a result in 2018.\n\nFor comparison, the Paris Agreement nationally determined contribution (NDC) for Germany is −55% relative to 1990 (which accords with the final cell in the table).\n\nThe plan establishes a commission for growth, structural change, and regional development. But unlike earlier versions, the commission will not be tasked with setting a date for an exit from coal. Instead, the commission will \"support the structural changes\" resulting from transformation and will \"develop a mix of instruments that will bring together economic development, structural change, social acceptability and climate protection\". The commission will be based at the economics and energy ministry, but will consult with other ministries, federal states, municipalities, and unions, as well as with representatives of \"affected\" companies and regions. The commission is scheduled to start work at the beginning of 2018 and to report at the end of 2018.\n\nThe plan stresses to role of the European Union Emission Trading System (EUETS) as the primary climate protection instrument covering the energy sector and parts of industry in central Europe. The government wants to strengthen price signaling and will campaign to make the EUETS \"effective\" at a European level. Earlier provisions to set a floor price for emissions allowances were removed.\n\nThe energy sector GHG target for 2030 is tonnes eq or a reduction of 61–62% relative to 1990. The energy supply must be \"almost completely decarbonised\" by 2050, with renewables as its main source. For the electricity sector, \"in the long-term, electricity generation must be based almost entirely on renewable energies\" and \"the share of wind and solar power in total electricity production will rise significantly\".\n\nIf \"possible and economically sensible\", renewable energy will be used directly in all sectors, and electricity from renewable sources will be used efficiently for heating, transport, and industry. The utilization of biomass will be limited and sourced mostly from waste. The plan states that transitioning to a power supply based on renewables while ensuring supply security is \"technically feasible\". During the transition, \"less carbon-intensive natural gas power plants and the existing most modern coal power plants play an important role as interim technologies\".\n\nThe plan states that \"the climate targets can only be reached if coal-fired power generation is reduced step-by-step\". Moreover, the German government \"in its development cooperation does not lend support to new coal power plants\". Regions which depend on coal, like the \"Lausitz\", need special consideration: \"we must succeed in establishing concrete perspectives for the future of the affected regions, before concrete decisions on the step-by-step withdrawal from the lignite industry can be taken\".\n\nNotwithstanding, a coal phase-out for Germany is implied in the plan, environment minister Barbara Hendricks said in an interview on 21November 2016. \"If you read the Climate Action Plan carefully, you will find that the exit from coal-fired power generation is the immanent consequence of the energy sector target... By 2030... half of the coal-fired power production must have ended, compared to 2014\", Hendricks said.\n\nThe plan also establishes a regional fund to foster new businesses in lignite mining regions. The government will need to ensure that EU competition law does not inhibit the operation of the fund.\n\nThe building sector GHG target for 2030 is tonnes eq or a reduction of 66–67% relative to 1990. Under the plan, Germany's building stock will be largely carbon-neutral by 2050 and their limited energy needs will be met through renewables. Due to the slow turnover of buildings, the groundwork for this goal must be laid by 2030. The government will invest heavily in programs to implement high energy standards for buildings. Heating, cooling, and electricity supply will be progressively switched to renewables. The government will terminate support programs that rely on fossil fuels by 2020 and switch instead to support programs using renewable systems. The aim is to \"make renewables-based systems much more attractive than fossil-fuel-based ones\".\n\nThe transport sector GHG target for 2030 is tonnes eq or a reduction of 40–42% relative to 1990. Unlike earlier drafts, the plan does not now set a deadline for all new cars to be emissions free. Rather the plan states that stricter emissions limits for new cars will be set by the European Union and that \"the government will advocate [an] ambitious development of the targets\" so that its 2030 goal can be met. These cuts will derive from improved efficiencies and increasingly GHG-neutral energy. Such cuts will require \"a significant contribution by the electrification of new cars and should have priority\". The plan emphasizes the contribution that biofuels can make, stating that \"in the target scenario, the energy supply of street and rail traffic, as well as parts of air and maritime traffic and inland shipping is switched to biofuels – if ecologically compatible – and otherwise largely to renewable electricity as well as other GHG-neutral fuels\". The need to support public transport, rail transport, and cycling is stressed in the plan. The plan states a next step is to determine the framework needed to ensure that new powertrain technologies and energy forms will be adopted at scale. This then \"involves the question of when, at the latest, they should be introduced into the market, and what penetration rate they should achieve by what date\".\n\nThe industry GHG target for 2030 is tonnes eq or a reduction of 49–51% relative to 1990. Good progress has been made and, , the sector must reduce its emissions by about another 20%. Notwithstanding, the plan emphasizes the need to maintain international competitiveness:\n\nThe plan continues that, despite the costs and challenges, climate protection could become an \"innovation motor\" for a modern high-tech economy. The plan acknowledges that some industrial emissions cannot be avoided – for instance, those from steel production or chemical plants. Such emissions should be reduced as far as possible by developing new processes and replacing old ones – or through the use of carbon capture and utilization (CCU) or carbon capture and storage (CCS). The government will launch a research and development program to advance these and other low-carbon processes.\n\nThe agricultural sector GHG target for 2030 is tonnes eq or a reduction of 31–34% relative to 1990. The plan acknowledges that agriculture cannot reach zero emissions, due to the biological processes inherent in plant cultivation and livestock farming. Instead the plan will focus on reducing emissions as much as possible and on the more efficient use of resources. About one third of agricultural emissions are due to nitrous oxide from fertilizer use. The government wants to reduce these emissions through better management and through research and development. Another one third of emissions derive from ruminant animals. The government will develop a comprehensive strategy to reduce emissions from livestock farming by 2021. The plan states that 20% of agricultural land by 2030 \"should be used for organic farming\", compared with 6.3% in 2014. The government also advocates the use of financial instruments under a reformed EU Common Agricultural Policy to reduce GHG emissions from the sector.\"\n\nThe land use and forestry sector offers opportunities for carbon sequestration. The government will prioritize improving the performance of forests as carbon sinks. Sustainable forest management will also be promoted and permanent grasslands and marshes are to be preserved. The expansion of settlements and transport infrastructure is to be reduced to per day by 2020 and to zero by 2050.\n\nUnder the plan, the government will consider how to incrementally revise taxation through to 2050, on the understanding that \"environmental taxes and levies can create incentives for ecological economic activity\" and that \"environmentally related taxes and levies can cost-efficiently trigger climate-friendly economic behavior\". The government also intends to reduce environmentally harmful subsidies. In addition, the government will support efforts to reconcile global finance flows with climate protection goals, for instance through its role on the G20 Financial Stability Board.\n\nThe plan is predicated on a gradual transformation, achieved through a learning process involving the scientific community and accompanied by public dialogue. It is therefore intended that the plan can and should respond to changing technological, political, and social conditions. The plan will be reviewed every five years to align it with the evolution of Germany's nationally determined contribution (NDC) under the Paris Agreement. The first update is therefore slated for or . As early as 2018, the government intends to strengthen the plan by quantifying the various emission reduction efforts and their associated ecological, social, and economic impacts. The sector targets may be modified as a result. Annual reporting should help the government to evaluate and adjust its specific climate protection measures in the short-term.\n\nThilo Schaefer, climate and energy expert at the Cologne Institute for Economic Research (IW) comments that \"it looks like the government forgot that the energy sector and industry already participate in the European emissions trading system... in the end, it will simply become more expensive for the affected sectors, but they won't save a single extra ton of ... because the emissions saved by German sectors can be emitted by other states\".\n\nSimone Peter, co-chair of the Green Party, said \"this is not a good plan anymore, it has become an empty shell, because the ministerial colleagues of [environment minister] Hendricks have removed anything that could be of relevance – be it the coal exit, the end of the combustion engine, or a transition in agriculture\".\n\nKlaus Töpfer, founding director of the (IASS), said \"this plan is certainly not yet capable of securing the German contribution to the Paris Climate Agreement. This will require more work\".\n\nRegine Günther, director of policy and climate at WWF Germany, said \"today's climate action plan is only a fraction of what is needed. The only plus point: all sectors get precise reduction targets, which WWF welcomes. However, the list of negatives is much longer: there are no appropriate measures to reach those targets. There is also a blank on the issue of coal. The plan completely dropped the urgently needed ban on further extension of open cast mining. The commission on coal will not start until 2018, after the federal elections. A minimum price on carbon is also missing completely. With this plan, the industry and energy lobbies have proven how well placed they are in the economy ministry. With such a plan there can be no ambitious climate protection.\"\n\nOttmar Edenhofer, a German climate economist and director of the MCC, criticized the plan. He said economy and energy minister \"Sigmar Gabriel yielded to lobbyists and sadly put short-term interests before long-term interests\".\n\n\n"}
{"id": "4121526", "url": "https://en.wikipedia.org/wiki?curid=4121526", "title": "Hugh Auchincloss Brown", "text": "Hugh Auchincloss Brown\n\nHugh Auchincloss Brown (23 December 1879 – 19 November 1975) was an electrical engineer best known for advancing a theory of catastrophic pole shift. Brown claimed that massive accumulation of ice at the poles caused recurring tipping of the axis in cycles of approximately 4000–7500 years. Brown argued that because the earth wobbles on the axis and the crust slides on the mantle, a shift was demonstrably imminent, and suggested the use of nuclear explosions to break up the ice to forestall catastrophe.\n\n\n"}
{"id": "3986111", "url": "https://en.wikipedia.org/wiki?curid=3986111", "title": "Intumescent", "text": "Intumescent\n\nAn intumescent is a substance that swells as a result of heat exposure, thus increasing in volume and decreasing in density. Intumescents are typically used in passive fire protection and require listing, approval and compliance in their installed configurations in order to comply with the national building codes and laws.\n\nThe details for individual building parts are specified in technical standards which are compiled and published by national or international standardization bodies like the British Standards Institute (BSI), the German Institute for Standardization (DIN), the American Society for Testing and Materials (ASTM) or the International Standardization Organization (ISO).\n\nIntumescent coatings for steel constructions must be approved in standardized fire tests. Some important fire test standards are listed below:\n\n\nThese intumescents produce a light char, which is a poor conductor of heat, thus retarding heat transfer. Typically the light char consist of microporous carbonaceous foam formed by a chemical reaction of three main components: ammonium polyphosphate, pentaerythritol and melamine. The reaction takes place in a matrix formed by the molten binder which is typically based on vinyl acetate copolymers or styrene acrylates.\n\nAblative coatings contain a significant amount of hydrates. When the hydrates are heated, they decompose, and water vapour is released, which has a cooling effect. Once the water is spent, the insulation characteristics of the char that remains can slow down heat transfer from the exposed side to the unexposed side of an assembly.\n\nSoft char producers are typically used in thin film intumescents for fireproofing structural steel as well as in firestop pillows. Typically, the expansion pressure that is created for these products is very low, because the soft carbonaceous char has little substance, which is beneficial if the aim is to produce a layer of insulation.\n\nHarder chars are produced with sodium silicates and graphite. These products are suitable for use in plastic pipe firestops as well as exterior steel fireproofing. In those applications, it is necessary to produce a more substantial char capable of exerting quantifiable expansion pressure. In the case of firestops, a melting, burning plastic pipe must be squeezed together and shut so that there will be no opening for fire to propagate to an otherwise fire-resistance rated wall or floor assembly. In the case of exterior fireproofing, a hydrocarbon fire must be held off with quite potentially more kinetic energy than a house fire. Intumescents that produce hard chars are unsuitable for interior spray fireproofing.\n\nIntumescents are used to achieve passive fire protection for such applications as firestopping, fireproofing, gasketing and window casings. Such applications are relevant for buildings, offshore constructions, ships and aircraft.\n\nSome intumescents are susceptible to environmental influences, such as humidity, which can reduce or negate their ability to function. In Germany, the \"Deutsches Institut für Bautechnik\", DIBt, quantifies the ability of intumescents to stand the test of time against various environmental exposures. DIBt-approved firestops and fireproofing materials are available in Canada and the U.S.\n\n"}
{"id": "53679009", "url": "https://en.wikipedia.org/wiki?curid=53679009", "title": "Leaf fiber", "text": "Leaf fiber\n\nLeaf fibers or hard fibers are a type of plant fiber mainly used for cordage (producing rope). They are the toughest of the plant fibers which is most likely due to their increased lignin content when compared to the other groups of plant fibers. They are typically characterized as being very tough and rigid lending them towards being used in rope production over clothing or paper like other plant fibers.\n\nLeaf fibers can be found in the vascular bundles of plant leaves and therefore consist of both phloem and xylem tissues and any other vascular sheathing tissues (for example sclerenchyma cells). More specifically, leaf fibers are typically found in monocotyledonous leaves.\n\nThe fibers are harvested from plants in long, thin bundles mainly through the process of decortication which is where the non-fibrous tissues are scraped away from the plant fibers by hand or in a machine. For the majority of cases, the leaves must be hand-picked from the plant at maturity before undergoing decortication which causes the harvesting of hard fibers to be a very energy and time intensive task.\n\nSisal and abaca are the primary leaf fibers that are harvested and sold. These are both mainly used to make rope or matting but, as technology continues to advance these, and other, hard fibers are being able to be broken down and pulped to be used in paper products.\n\nNot much research is being looked into the possibilities and abilities of leaf fibers as they are very hard to harvest and process so synthetic fibers are more commonly used in their place.\n\n"}
{"id": "4543284", "url": "https://en.wikipedia.org/wiki?curid=4543284", "title": "List of Lepidoptera that feed on rhododendrons", "text": "List of Lepidoptera that feed on rhododendrons\n\nRhododendron species are used as food plants by the larvae of some species of the order Lepidoptera, including:\n\n\n"}
{"id": "445513", "url": "https://en.wikipedia.org/wiki?curid=445513", "title": "List of Solar System objects", "text": "List of Solar System objects\n\nThe following is a list of Solar System objects by orbit, ordered by increasing distance from the Sun. Most named objects in this list have a diameter of 500 km or more.\n\nThe Solar System also contains:\n\ninterstellar medium\n\n"}
{"id": "31260502", "url": "https://en.wikipedia.org/wiki?curid=31260502", "title": "List of longest mountain chains on Earth", "text": "List of longest mountain chains on Earth\n\nThe world's longest above-water mountain range is the Andes, about long. The range stretches from north to south through seven countries in South America, along the west coast of the continent: Venezuela, Colombia, Ecuador, Peru, Bolivia, Chile, and Argentina. Aconcagua is the highest peak, at about . \n\nThis list does not include submarine mountain ranges. If submarine mountains are included, the longest is the global mid-ocean ridge system which extends for about .\n\nMountain chains are typically formed by the process of plate tectonics. Tectonic plates slide very slowly over the Earth's mantle, a lower place of rock that is heated from the Earth's interior. Several huge sections of the earth’s crust are impelled by heat currents in the mantle, producing tremendous forces that can buckle the material at the edges of the plates to form mountains. Usually one plate is forced underneath the other, and the lower plate is slowly absorbed by the mantle. Where the two plates pass one another, heated rock from the mantle can burst through the crust to form volcanoes. The movement of the plates against one another can also cause earthquakes.\n\n\n"}
{"id": "745931", "url": "https://en.wikipedia.org/wiki?curid=745931", "title": "List of rivers of Slovakia", "text": "List of rivers of Slovakia\n\nThis is a list of the major rivers that flow through Slovakia and their lengths.\n\n\"ordered against the direction of the river flow; H stands for Hungary\"\n\n"}
{"id": "8075386", "url": "https://en.wikipedia.org/wiki?curid=8075386", "title": "List of the non-vascular plants of Canada", "text": "List of the non-vascular plants of Canada\n\n\"Parent page: Flora of Canada\"\n\nThis is a listing of the non-vascular plants of Canada, and includes the mosses, liverworts and hornworts.\n\nIDD - incomplete distribution data\n"}
{"id": "29276596", "url": "https://en.wikipedia.org/wiki?curid=29276596", "title": "List of wadis of Kuwait", "text": "List of wadis of Kuwait\n\nKuwait does not have any permanent rivers, but does have some wadis, the most notable of which is Wadi al Batin, which forms the border between Kuwait and Iraq. Also listed are the river-like marine channels around Bubiyan Island.\n\n\n"}
{"id": "267366", "url": "https://en.wikipedia.org/wiki?curid=267366", "title": "Mauna Kea", "text": "Mauna Kea\n\nMauna Kea ( or , ) is a dormant volcano on the island of Hawaii. Its peak is above sea level, making it the highest point in the state of Hawaii. Most of the mountain is under water, and when measured from its oceanic base, Mauna Kea becomes the tallest mountain in the world measuring over . Mauna Kea is about a million years old, and has thus passed the most active shield stage of life hundreds of thousands of years ago. In its current post-shield state, its lava is more viscous, resulting in a steeper profile. Late volcanism has also given it a much rougher appearance than its neighboring volcanoes; contributing factors include the construction of cinder cones, the decentralization of its rift zones, the glaciation on its peak, and the weathering effects of the prevailing trade winds. Mauna Kea last erupted 6,000 to 4,000 years ago and is now considered dormant.\n\nIn Hawaiian mythology, the peaks of the island of Hawaii are sacred. An ancient law allowed only high-ranking aliʻi to visit its peak. Ancient Hawaiians living on the slopes of Mauna Kea relied on its extensive forests for food, and quarried the dense volcano-glacial basalts on its flanks for tool production. When Europeans arrived in the late 18th century, settlers introduced cattle, sheep and game animals, many of which became feral and began to damage the mountain's ecological balance. Mauna Kea can be ecologically divided into three sections: an alpine climate at its summit, a \"Sophora chrysophylla\"–\"Myoporum sandwicense\" (or māmane–naio) forest on its flanks, and an \"Acacia koa\"–\"Metrosideros polymorpha\" (or koa–ōhia) forest, now mostly cleared by the former sugar industry, at its base. In recent years, concern over the vulnerability of the native species has led to court cases that have forced the Hawai'i Department of Land and Natural Resources to eradicate all feral species on the mountain.\n\nWith its high elevation, dry environment, and stable airflow, Mauna Kea's summit is one of the best sites in the world for astronomical observation. Since the creation of an access road in 1964, thirteen telescopes funded by eleven countries have been constructed at the summit. The Mauna Kea Observatories are used for scientific research across the electromagnetic spectrum and comprise the largest such facility in the world. Their construction on a landscape considered sacred by Native Hawaiians continues to be a topic of debate.\n\nMauna Kea is one of five volcanoes that form the island of Hawaii, the largest and youngest island of the Hawaiian–Emperor seamount chain. Of these five hotspot volcanoes, Mauna Kea is the fourth oldest and fourth most active. It began as a preshield volcano driven by the Hawaii hotspot around one million years ago, and became exceptionally active during its shield stage until 500,000 years ago. Mauna Kea entered its quieter post-shield stage 250,000 to 200,000 years ago, and is currently dormant. Mauna Kea does not have a visible summit caldera, but contains a number of small cinder and pumice cones near its summit. A former summit caldera may have been filled and buried by later summit eruption deposits.\n\nMauna Kea is over in volume, so massive that it and its neighbor, Mauna Loa, depress the ocean crust beneath it by .\nThe volcano continues to slip and flatten under its own weight at a rate of less than per year. Much of its mass lies east of its present summit. Mauna Kea stands above sea level, just higher than its neighbor Mauna Loa, and is the highest point in the state of Hawaii. Measured from its base on the ocean floor, it rises over , significantly greater than the elevation of Mount Everest above sea level.\n\nLike all Hawaiian volcanoes, Mauna Kea has been created as the Pacific tectonic plate has moved over the Hawaiian hotspot in the Earth's underlying mantle. The Hawaii island volcanoes are the most recent evidence of this process that, over 70 million years, has created the -long Hawaiian Ridge–Emperor seamount chain. The prevailing, though not completely settled, view is that the hotspot has been largely stationary within the planet's mantle for much, if not all of the Cenozoic Era. However, while Hawaiian volcanism is well understood and extensively studied, there remains no definite explanation of the mechanism that causes the hotspot effect.\n\nLava flows from Mauna Kea overlapped in complex layers with those of its neighbors during its growth. Most prominently, Mauna Kea is built upon older flows from Kohala to the northwest, and intersects the base of Mauna Loa to the south. The original eruptive fissures (rift zones) in the flanks of Mauna Kea were buried by its post-shield volcanism. Hilo Ridge, a prominent underwater rift zone structure east of Mauna Kea, was once believed to be a part of the volcano; however, it is now understood to be a rift zone of Kohala that has been affected by younger Mauna Kea flows.\n\nThe shield-stage lavas that built the enormous main mass of the mountain are tholeiitic basalts, like those of Mauna Loa, created through the mixing of primary magma and subducted oceanic crust. They are covered by the oldest exposed rock strata on Mauna Kea, the post-shield alkali basalts of the \"Hāmākua Volcanics\", which erupted between 250,000 and 70–65,000 years ago. The most recent volcanic flows are hawaiites and mugearites: they are the post-shield \"Laupāhoehoe Volcanics\", erupted between 65,000 and 4,000 years ago. These changes in lava composition accompanied the slow reduction of the supply of magma to the summit, which led to weaker eruptions that then gave way to isolated episodes associated with volcanic dormancy. The Laupāhoehoe lavas are more viscous and contain more volatiles than the earlier tholeiitic basalts; their thicker flows significantly steepened Mauna Kea's flanks. In addition, explosive eruptions have built cinder cones near the summit. These cones are the most recent eruptive centers of Mauna Kea. Its present summit is dominated by lava domes and cinder cones up to in diameter and hundreds of meters tall.\n\nMauna Kea is the only Hawaiian volcano with distinct evidence of glaciation. Similar deposits probably existed on Mauna Loa, but have been covered by later lava flows. Despite Hawaii's tropical location, during several past ice ages a drop of only a degree in temperature allowed snow to remain at the mountain's summit through summer, triggering the formation of an ice cap. There are three episodes of glaciation that have been recorded from the last 180,000 years: the \"Pōhakuloa\" (180–130 ka), \"Wāihu\" (80–60 ka) and \"Mākanaka\" (40–13 ka) series. These have extensively sculpted the summit, depositing moraines and a circular ring of till and gravel along the mountain's upper flanks. Subglacial eruptions built cinder cones during the Mākanaka glaciation, most of which were heavily gouged by glacial action. The most recent cones were built between 9000 and 4500 years ago, atop the glacial deposits, although one study indicates that the last eruption may have been around 3600 years ago.\n\nAt their maximum extent, the glaciers extended from the summit down to between of elevation. A small body of permafrost, less than across, was found at the summit of Mauna Kea prior to 1974, and may still be present. Small gullies etch the summit, formed by rain- and snow-fed streams that flow only during winter melt and rain showers.\nOn the windward side of the mountain, stream erosion driven by trade winds has accelerated erosion in a manner similar to that on older Kohala.\n\nMauna Kea is home to Lake Waiau, the highest lake in the Pacific Basin. At an altitude of , it lies within the Puu Waiau cinder cone and is the only alpine lake in Hawaii. The lake is very small and shallow, with a surface area of and a depth of . Radiocarbon dating of samples at the base of the lake indicates that it was clear of ice 12,600 years ago. Hawaiian lava types are typically permeable, preventing the formation of lakes due to infiltration. Here, either sulfur-bearing steam altered the volcanic ash to low-permeability clays, or explosive interactions between rising magma and groundwater or surface water (phreatic eruptions) formed exceptionally fine ash that also would reduce the permeability of the lake bed.\n\nUntil 1993, artesian water was not known to be present in the Island of Hawaii. Drilling by the University of Hawaii at that time encountered an artesian groundwater aquifer at 300 meters below sea level and 100 meters of hole depth within a compacted layer of soil and lava where the flows of both Mauna Loa and Mauna Kea meet (Humuula saddle region). Isotopic composition shows the water present to have been derived from rain coming off Mauna Kea at an elevation higher than 2000 meters above mean sea level. Its presence is attributed to a freshwater head within Mauna Kea's basal lens. Scientists believe there may be more water in Mauna Kea's fresh water lens than current models may indicate. In 2012 two more bore holes were drilled on Mauna Kea and water discovered at much higher elevations than previously believed but shallower than expected. Donald Thomas, director of the University of Hawaii's Center for the Study of Active Volcanoes believes one reason to continue study of the aquifers is due to use and 'occupancy' of the higher elevation areas, stating: \"Nearly all of these activities depend on the availability of potable water that, in most cases, must be trucked to the Saddle from Waimea or Hilo — an inefficient and expensive process that consumes a substantial quantity of our scarce liquid fuels.\"\n\nThe last eruption of Mauna Kea was about 4,600 years ago (about 2600 BC); because of this inactivity, Mauna Kea is assigned a United States Geological Survey hazard listing of 7 for its summit and 8 for its lower flanks, out of the lowest possible hazard rating of 9 (which is given to the extinct volcano Kohala). Twenty percent of the volcano's summit has seen lava flows in the past 10,000 years, and its flanks have seen virtually no lava flows during that time.\n\nDespite its dormancy, Mauna Kea is expected to erupt again, although there would be sufficient warning to evacuate. The telescopes on Mauna Kea's summit would be the first to detect the minute amounts of deformation resulting from the volcano's swelling, acting like expensive tiltmeters. Based on prior eruptions, such an event could occur anywhere on the volcano's upper flanks and would likely produce extended lava flows, mostly of a'a, of in length. Long periods of activity could build a cinder cone at the source. Although not likely in the next few centuries, such an eruption would probably result in little loss of life but significant damage to infrastructure.\n\nThe first Ancient Hawaiians to arrive on Hawaii island lived along the shores, where food and water were plentiful. Settlement expanded inland to the Mauna Loa – Mauna Kea region in the 12th and early 13th centuries. Archaeological evidence suggests that these regions were used for hunting, collecting stone material, and possibly for spiritual reasons or for astronomical or navigational observations. The mountain's plentiful forest provided plants and animals for food and raw materials for shelter. Flightless birds that had previously known no predators became a staple food source.\n\nEarly settlement of the Hawaiian islands led to major changes to local ecosystems and many extinctions, particularly amongst bird species. Ancient Hawaiians brought foreign plants and animals, and their arrival was associated with increased rates of erosion.\nThe prevailing lowland forest ecosystem was transformed from forest to grassland; some of this change was caused by the use of fire, but the prevailing cause of forest ecosystem collapse and avian extinction on Hawaii appears to have been the introduction of the Polynesian (or Pacific) rat.\n\nThe five volcanoes of Hawaii are revered as sacred mountains; and Mauna Kea's summit, the highest, is the most sacred. For this reason, a \"kapu\" (ancient Hawaiian law) restricted visitor rights to high-ranking aliʻi. Hawaiians associated elements of their natural environment with particular deities. In Hawaiian mythology, the summit of Mauna Kea was seen as the \"region of the gods\", a place where benevolent spirits reside. Poliahu, deity of snow, also resides there. In Hawaiian, Mauna Kea is a shortened form of \"Mauna a Wakea\" which denotes the mountain's connection to the sky father Wakea; however, the English translation of Mauna Kea is \"white mountain\" in reference to its seasonally snow-capped summit.\n\nAround AD 1100, natives established adze quarries high up on Mauna Kea to extract the uniquely dense basalt (generated by the quick cooling of lava flows meeting glacial ice during subglacial eruptions) to make tools. Volcanic glass and gabbro were collected for blades and fishing gear, and māmane wood was preferred for the handles. At peak quarry activity after AD 1400, there were separate facilities for rough and fine cutting; shelters with food, water, and wood to sustain the workers; and workshops creating the finished product.\n\nLake Waiau provided drinking water for the workers. Native chiefs would also dip the umbilical cords of newborn babies in its water, to give them the strength of the mountain. Use of the quarry declined between this period and contact with Americans and Europeans. As part of the ritual associated with quarrying, the workers erected shrines to their gods; these and other quarry artifacts remain at the sites, most of which lie within what is now the Mauna Kea Ice Age Reserve.\n\nThis early era was followed by peace and cultural expansion between the 12th and late 18th century. Land was divided into regions designed for both the immediate needs of the populace and the long-term welfare of the environment. These \"ahupuaa\" generally took the form of long strips of land oriented from the mountain summits to the coast. Mauna Kea's summit was encompassed in the ahupuaa of Kaohe, with part of its eastern slope reaching into the nearby Humuula. Principal sources of nutrition for Hawaiians living on the slopes of the volcano came from the māmane–naio forest of its upper slopes, which provided them with vegetation and bird life. Bird species hunted included the uau (\"Pterodroma sandwichensis\"), nēnē (\"Branta sandvicensis\"), and palila (\"Loxioides bailleui\"). The lower koa–ōhia forest gave the natives wood for canoes and ornate bird feathers for decoration.\n\nThere are three accounts of foreigners visiting Hawaii before the arrival of James Cook, in 1778. However, the earliest Western depictions of the isle, including Mauna Kea, were created by explorers in the late 18th and early 19th centuries. Contact with Europe and America had major consequences for island residents. Native Hawaiians were devastated by introduced diseases; port cities including Hilo, Kealakekua, and Kailua grew with the establishment of trade; and the adze quarries on Mauna Kea were abandoned after the introduction of metal tools.\n\nIn 1793, cattle were brought by George Vancouver as a tribute to King Kamehameha I. By the early 19th century, they had escaped confinement and roamed the island freely, greatly damaging its ecosystem. In 1809 John Palmer Parker arrived and befriended Kamehameha I, who put him in charge of cattle management on the island. With an additional land grant in 1845, Parker established Parker Ranch on the northern slope of Mauna Kea, a large cattle ranch that is still in operation today. Settlers to the island burned and cut down much of the native forest for the construction of sugarcane plantations and houses.\n\nThe Saddle Road, named for its crossing of the saddle-shaped plateau between Mauna Kea and Mauna Loa, was completed in 1943, and eased travel to Mauna Kea considerably.\n\nThe Pohakuloa Training Area on the plateau is the largest military training ground in Hawaii. The base extends from the volcano's lower flanks to elevation, on state land leased to the US Army since 1956. There are 15 threatened and endangered plants, three endangered birds, and one endangered bat species in the area.\n\nMauna Kea has been the site of extensive archaeological research since the 1980s. Approximately 27 percent of the Science Reserve had been surveyed by 2000, identifying 76 shrines, 4 adze manufacturing workshops, 3 other markers, 1 positively identified burial site, and 4 possible burial sites. By 2009, the total number of identified sites had risen to 223, and archaeological research on the volcano's upper flanks is ongoing. It has been suggested that the shrines, which are arranged around the volcano's summit along what may be an ancient snow line, are markers for the transition to the sacred part of Mauna Kea. Despite many references to burial around Mauna Kea in Hawaiian oral history, few sites have been confirmed. The lack of shrines or other artifacts on the many cinder cones dotting the volcano may be because they were reserved for burial.\n\nIn pre-contact times, natives traveling up Mauna Kea were probably guided more by landscape than by existing trails, as no evidence of the latter has been found. It is possible that natural ridges and water sources were followed instead. Individuals likely took trips up Mauna Kea's slopes to visit family-maintained shrines near its summit, and traditions related to ascending the mountain exist to this day. However, very few natives actually reached the summit, because of the strict \"kapu\" placed on it.\n\nIn the early 19th century, the earliest notable \"recorded\" ascents of Mauna Kea included the following:\n\nIn the late 19th and early 20th centuries trails were formed, often by the movement of game herds, that could be traveled on horseback. However, vehicular access to the summit was practically impossible until the construction of a road in 1964, and it continues to be restricted. Today, multiple trails to the summit exist, in various states of use.\n\nHawaii's geographical isolation strongly influences its ecology. Remote islands like Hawaii have a large number of species that are found nowhere else (see Endemism in the Hawaiian Islands). The remoteness resulted in evolutionary lines distinct from those elsewhere and isolated these endemic species from external biotic influence, and also makes them especially vulnerable to extinction and the effects of invasive species. In addition the ecosystems of Hawaii are under threat from human development including the clearing of land for agriculture; an estimated third of the island's endemic species have already been wiped out. Because of its elevation, Mauna Kea has the greatest diversity of biotic ecosystems anywhere in the Hawaiian archipelago. Ecosystems on the mountain form concentric rings along its slopes due to changes in temperature and precipitation with elevation. These ecosystems can be roughly divided into three sections by elevation: alpine–subalpine, montane, and basal forest.\n\nContact with Americans and Europeans in the early 19th century brought more settlers to the island, and had a lasting negative ecological effect. On lower slopes, vast tracts of koa–ōhia forest were converted to farmland. Higher up, feral animals that escaped from ranches found refuge in, and damaged extensively, Mauna Kea's native māmane–naio forest. Non-native plants are the other serious threat; there are over 4,600 introduced species on the island, whereas the number of native species is estimated at just 1,000.\n\nThe summit of Mauna Kea lies above the tree line, and consists of mostly lava rock and alpine tundra. An area of heavy snowfall, it is inhospitable to vegetation, and is known as the Hawaiian tropical high shrublands. Growth is restricted here by extremely cold temperatures, a short growing season, low rainfall, and snow during winter months. A lack of soil also retards root growth, makes it difficult to absorb nutrients from the ground, and gives the area a very low water retention capacity.\n\nPlant species found at this elevation include \"Styphelia tameiameiae\", \"Taraxacum officinale\", \"Tetramolopium humile\", \"Agrostis sandwicensis\", \"Anthoxanthum odoratum\", \"Trisetum glomeratum\", \"Poa annua\", \"Sonchus oleraceus\", and \"Coprosma ernodiodes\". One notable species is Mauna Kea silversword (\"Argyroxiphium sandwicense\" var. \"sandwicense\"), a highly endangered endemic plant species that thrives in Mauna Kea's high elevation cinder deserts. At one stage reduced to a population of just 50 plants, Mauna Kea silversword was thought to be restricted to the alpine zone, but in fact has been driven there by pressure from livestock, and can grow at lower elevations as well.\n\nThe Mauna Kea Ice Age Reserve on the southern summit flank of Mauna Kea was established in 1981. The reserve is a region of sparsely vegetated cinder deposits and lava rock, including areas of aeolian desert and Lake Waiau. This ecosystem is a likely haven for the threatened uau (\"Pterodroma sandwichensis\") and also the center of a study on wēkiu bugs (\"Nysius wekiuicola\").\n\nWēkiu bugs feed on dead insect carcasses that drift up Mauna Kea on the wind and settle on snow banks. This is a highly unusual food source for a species in the genus \"Nysius\", which consists of predominantly seed-eating insects. They can survive at extreme elevations of up to because of natural antifreeze in their blood. They also stay under heated surfaces most of the time. Their conservation status is unclear, but the species is no longer a candidate for the Endangered Species List; studies on the welfare of the species began in 1980. The closely related \"Nysius aa\" lives on Mauna Loa. Wolf spiders (Lycosidae) and forest tent caterpillar moths have also been observed in the same Mauna Kea ecosystem; the former survive by hiding under heat-absorbing rocks, and the latter through cold-resistant chemicals in their bodies.\nThe highest forested zone on the volcano, at an elevation of , is dominated by māmane (\"Sophora chrysophylla\") and naio (\"Myoporum sandwicense\"), both endemic tree species, and is thus known as māmane–naio forest. Māmane seeds and naio fruit are the chief foods of the birds in this zone, especially the palila (\"Loxioides bailleui\"). The palila was formerly found on the slopes of Mauna Kea, Mauna Loa, and Hualālai, but is now confined to the slopes of Mauna Kea—only 10% of its former range—and has been declared critically endangered.\n\nThe largest threat to the ecosystem is grazing by feral sheep (\"Ovis aries\"), cattle (\"Bos primigenius\"), and goats (\"Capra hircus\") introduced to the island in the late 18th century. Feral animal competition with commercial grazing was severe enough that a program to eradicate them existed as far back as the late 1920s, and continued through to 1949. One of the results of this grazing was the increased prevalence of herbaceous and woody plants, both endemic and introduced, that were resistant to browsing. The feral animals were almost eradicated, and numbered a few hundred in the 1950s. However, an influx of local hunters led to the feral species being valued as game animals, and in 1959 the Hawaii Department of Land and Natural Resources, the governing body in charge of conservation and land use management, changed its policy to a sustained-control program designed to facilitate the sport.\n\nMouflon (\"Ovis aries orientalis\") was introduced from 1962–1964, and a plan to release axis deer (\"Axis axis\") in 1964 was prevented only by protests from the ranching industry, who said that they would damage crops and spread disease. The hunting industry fought back, and the back-and-forth between the ranchers and hunters eventually gave way to a rise in public environmental concern. With the development of astronomical facilities on Mauna Kea commencing, conservationists demanded protection of Mauna Kea's ecosystem. A plan was proposed to fence 25% of the forests for protection, and manage the remaining 75% for game hunting. Despite opposition from conservationists the plan was put into action. While the land was partitioned no money was allocated for the building of the fence. In the midst of this wrangling the Endangered Species Act was passed; the National Audubon Society and Sierra Club Legal Defense Fund filed a lawsuit against the Hawaii Department of Land and Natural Resources, claiming that they were violating federal law, in the landmark case \"Palila v. Hawaii Department of Land and Natural Resources\" (1978).\n\nThe court ruled in favor of conservationists and upheld the precedence of federal laws before state control of wildlife. Having violated the Endangered Species Act, Hawaii state was required to remove all feral animals from the mountainside. This decision was followed by a second court order in 1981. A public hunting program removed many of the feral animals, at least temporarily. An active control program is in place, though it is not conducted with sufficient rigor to allow significant recovery of the māmane-naio ecosystem. There are many other species and ecosystems on the island, and on Mauna Kea, that remain threatened by human development and invasive species.\n\nThe Mauna Kea Forest Reserve protects of māmane-naio forest under the jurisdiction of the Hawaii Department of Land and Natural Resources. Ungulate hunting is allowed year-round. A small part of the māmane–naio forest is encompassed by the Mauna Kea State Recreation Area.\n\nA band of ranch land on Mauna Kea's lower slopes was formerly \"Acacia koa\" – \"Metrosideros polymorpha\" (koa-ōhia) forest. Its destruction was driven by an influx of European and American settlers in the early 19th century, as extensive logging during the 1830s provided lumber for new homes. Vast swathes of the forest were burned and cleared for sugarcane plantations. Most of the houses on the island were built of koa, and those parts of the forest that survived became a source for firewood to power boilers on the sugarcane plantations and to heat homes. The once vast forest had almost disappeared by 1880, and by 1900, logging interests had shifted to Kona and the island of Maui. With the collapse of the sugar industry in the 1990s, much of this land lies fallow but portions are used for cattle grazing, small-scale farming and the cultivation of eucalyptus for wood pulp.\n\nThe Hakalau Forest National Wildlife Refuge is a major koa forest reserve on Mauna Kea's windward slope. It was established in 1985, covering of ecosystem remnant. Eight endangered bird species, twelve endangered plants, and the endangered Hawaiian hoary bat (\"Lasiurus cinereus semotus\") have been observed in the area, in addition to many other rare biota. The reserve has been the site of an extensive replanting campaign since 1989. Parts of the reserve show the effect of agriculture on the native ecosystem, as much of the land in the upper part of the reserve is abandoned farmland.\n\nBird species native to the acacia koa–ōhia forest include the Hawaiian crow (\"Corvus hawaiiensis\"), the akepa (\"Loxops coccineus\"), Hawaii creeper (\"Oreomystis mana\"), ʻakiapōlāʻau (\"Hemignathus munroi\"), and Hawaiian hawk (\"Buteo solitarius\"), all of which are endangered, threatened, or near threatened; the Hawaiian crow in particular is extinct in the wild, but there are plans to reintroduce the species into the Hakalau reserve.\n\nMauna Kea's summit is one of the best sites in the world for astronomical observation due to favorable observing conditions. The arid conditions are important for submillimeter and infrared astronomy for this region of the electromagnetic spectrum. The summit is above the inversion layer, keeping most cloud cover below the summit and ensuring the air on the summit is dry, and free of atmospheric pollution. The summit atmosphere is exceptionally stable, lacking turbulence for some of the world's best astronomical seeing. The very dark skies resulting from Mauna Kea's distance from city lights are preserved by legislation that minimizes light pollution from the surrounding area; the darkness level allows the observation of faint astronomical objects. These factors historically made Mauna Kea an excellent spot for stargazing.\n\nIn the early 1960s, the Hawaii Island Chamber of Commerce encouraged astronomical development of Mauna Kea, as economic stimulus; this coincided with University of Arizona astronomer Gerard Kuiper's search for sites to use newly improved detectors of infrared light. Site testing by Kuiper's assistant Alika Herring in 1964 confirmed the summit's outstanding suitability. An intense three-way competition for NASA funds to construct a large telescope began between Kuiper, Harvard University, and the University of Hawaii (UH), which only had experience in solar astronomy. This culminated in funds being awarded to the \"upstart\" UH proposal. UH rebuilt its small astronomy department into a new Institute for Astronomy, and in 1968 the Hawaii Department of Land and Natural Resources gave it a 65-year lease for all land within a radius of its telescope, essentially that above . On its completion in 1970, the UH was the seventh largest optical/infrared telescope in the world.\nBy 1970, two telescopes had been constructed by the US Air Force and Lowell Observatory. In 1973, Canada and France agreed to build the 3.6 m CFHT on Mauna Kea. However, local organisations started to raise concerns about the environmental impact of the observatory. This led the Department of Land and Natural Resources to prepare an initial management plan, drafted in 1977 and supplemented in 1980. In January 1982, the UH Board of Regents approved a plan to support the continued development of scientific facilities at the site. In 1998, were transferred from the observatory lease to supplement the Mauna Kea Ice Age Reserve. The 1982 plan was replaced in 2000 by an extension designed to serve until 2020: it instituted an Office of Mauna Kea Management, designated for astronomy, and shifted the remaining to \"natural and cultural preservation\". This plan was further revised to address concern expressed in the Hawaiian community that a lack of respect was being shown toward the cultural values of the mountain.\n\nToday the Mauna Kea Science Reserve has 13 observation facilities, each funded by as many as 11 countries. There are nine telescopes working in the visible and infrared spectrum, three in the submillimeter spectrum, and one in the radio spectrum, with mirrors or dishes ranging from . In comparison, the Hubble Space Telescope has a mirror, similar in size to the UH88, now the second smallest telescope on the mountain.\n\nA \"Save Mauna Kea\" movement, believes development of the mountain to be sacrilegious. Native Hawaiian non-profit groups such as Kahea, concerned with cultural heritage and the environment also oppose development for cultural and religious reasons. The multi-telescope \"outrigger\", proposed in 2006 was eventually canceled. A planned new telescope, the Thirty Meter Telescope (TMT), has attracted controversy and protests. The TMT was approved in April 2013. In October 2014, the groundbreaking ceremony for the telescope was interrupted by protesters causing the project to temporarily halt. In late March 2015, demonstrators blocked access of the road to the summit again. On April 2, 2015, 300 protestors were gathered near the visitor's center when 12 people were arrested with 11 more arrested at the summit. Among the concerns of the protest groups are the land appraisals and Native Hawaiians consultation. Construction was halted on April 7, 2015 after protests expanded over the state. After several halts, the project has been voluntarily postponed. Governor Ige announced substantial changes to the management of Mauna Kea in the future but stated the project can move forward. The Supreme Court of Hawaii approved the resumption of construction on 31 October 2018.\n\nThe summit of Mauna Kea has an alpine climate. Due to the influence of its tropical latitude, temperature swings are very low in spite of its high elevation. Frosts are common year round, but in spite of the elevation no month is nearly below freezing in terms of average high, although March almost falls below in average means.\n\nFrom December to February, the ground above is often covered by snow, ranging from a few inches deep to several feet deep. Snow may fall from to and higher in any month, but occurs most often between October and April to early May.\n\nMauna Kea's coastline is dominated by the Hamakua Coast, an area of rugged terrain created by frequent slumps and landslides on the volcano's flank. The area includes several recreation parks including Kalopa State Recreation Area, Wailuku River State Park and Akaka Falls State Park.\n\nThere are over 3,000 registered hunters on Hawaii island, and hunting, for both recreation and sustenance, is a common activity on Mauna Kea. A public hunting program is used to control the numbers of introduced animals including pigs, sheep, goats, turkey, pheasants, and quail. The Mauna Kea State Recreation Area functions as a base camp for the sport. Birdwatching is also common at lower levels on the mountain. A popular site is \"Kīpuka Pu'u Huluhulu\", a kīpuka on Mauna Kea's flank that formed when lava flows isolated the forest on a hill.\nMauna Kea's great elevation and the steepness of its flanks provide a better view and a shorter hike than the adjacent Mauna Loa. The high elevation with its risk of altitude sickness, weather concerns, steep road grade, and overall inaccessibility make the volcano dangerous and summit trips difficult. Until the construction of roads in the mid-20th century, only the hardy visited Mauna Kea's upper slopes; hunters tracked game animals, and hikers traveled up the mountain. These travelers used stone cabins constructed by the Civilian Conservation Corps in the 1930s as base camps, and it is from these facilities that the modern mid-level Onizuka Center for International Astronomy telescope support complex is derived. The first Mauna Kea summit road was built in 1964, making the peak itself accessible to larger numbers of people.\n\nToday, multiple hiking trails exist, including the Mauna Kea Trail, and by 2007 over 100,000 tourists and 32,000 vehicles were going each year to the Visitor Information Station (VIS) adjacent to the Onizuka Center for International Astronomy. The Mauna Kea Access Road is paved up to the Center at . One study reported that around a third of visitors and two thirds of professional astronomers working on the mountain have experienced symptoms of acute altitude sickness; visitors traveling up the volcano's flanks are advised to stop for at least half an hour and preferably longer at the visitor center to acclimate to the higher elevation. It is strongly recommended to use a four-wheel drive vehicle to drive all the way to the top. Brakes often overheat on the way down and there is no fuel available on Mauna Kea. A free Star Gazing Program is held at the VIS every night from 6-10 pm. Between 5,000 and 6,000 people visit the summit of Mauna Kea each year, and to help ensure safety, and protect the integrity of the mountain, a ranger program was implemented in 2001.\n\n\n"}
{"id": "1432856", "url": "https://en.wikipedia.org/wiki?curid=1432856", "title": "Mesoarchean", "text": "Mesoarchean\n\nThe Mesoarchean (, also spelled Mesoarchaean) is a geologic era within the Archean Eon, spanning . The era is defined chronometrically and is not referenced to a specific level in a rock section on Earth. Fossils from Australia show that stromatolites have lived on Earth since the Mesoarchean. The Pongola glaciation occurred around . The first supercontinent Vaalbara broke up during this era about .\n\nThe earliest reefs date from this era, and were probably formed by stromatolites. The surface temperature during the Mesoarchean was likely not much higher than modern-day temperatures. Atmospheric carbon dioxide concentration was only a few times higher than its pre-industrial value, and the Sun's luminosity was only 70% of its current value, cancelling out the influence of a greater degree of greenhouse effect that may be operating.\n\n"}
{"id": "18806829", "url": "https://en.wikipedia.org/wiki?curid=18806829", "title": "New Sunshine Project", "text": "New Sunshine Project\n\nThe New Sunshine Project (NSS) is a Japanese project to develop the technologies for new energy and global environment integrating the Sunshine, the Moonlight (Energy-saving technology R & D) and the Global Environment Technology Projects since 1993. It is the successor of the 1974 MITI Sunshine Project.\n\nThe New Sunshine Project has multiple programs, the first one being \"World Energy Net-work (WE-NET) research program\" that was running from 1993-2002 Another program was the \"Development for safe utilization and infrastructure of hydrogen\"running from 2003-2007 This program was a project to develop the technologies for safe utilization and infrastructure of hydrogen and was the successor of the World Energy Network (WE-NET) research program.\n\n\"Fundamental research project on advanced hydrogen science\" was a program running from 2006-2012. It was a project to develop the technologies that enables the transportation and storage of large volumes of hydrogen in a compact form. It is the successor of the development for safe utilization and infrastructure of hydrogen research program (2003–2007).\n\n\n"}
{"id": "4462117", "url": "https://en.wikipedia.org/wiki?curid=4462117", "title": "Partnership for a New Generation of Vehicles", "text": "Partnership for a New Generation of Vehicles\n\nThe Partnership for a New Generation of Vehicles was a cooperative research program between the U.S. government and the three major domestic auto corporations, aimed at bringing extremely fuel-efficient (up to vehicles to market by 2003. \n\nThe partnership, formed in 1993, involved eight federal agencies, the national laboratories, universities, and the United States Council for Automotive Research (USCAR), which comprises DaimlerChrysler, Ford Motor Company and General Motors Corporation.\n\n\"Supercar\" was the unofficial description for the R&D program.\n\nOn track to achieving its objectives, the program was cancelled by the George W. Bush Administration in 2001 at the request of the automakers, with some of its aspects shifted to the much more distant FreedomCAR program.\n\nThe main purpose of this program was to develop technologies to reduce the impact of cars and light trucks on the environment as well as decrease U.S. dependency on imported petroleum. It was to make working vehicles that can achieve up to triple the contemporary vehicle fuel efficiency as well as further minimizing emissions, but without sacrificing affordability, performance, or safety. The common term for these vehicles was \"supercar\" because of the technological advances. The goal of achieving the target with a family-sized sedan included using new fuel sources, powerplants, aerodynamics, and lightweight materials. \n\nThe program was established in 1993 to support the domestic U.S. automakers (GM, Ford, and Chrysler) develop prototypes of a safe, clean, affordable car the size of the Ford Taurus, but delivering three times the fuel efficiency.\n\nThe PNGV program \"overcame many challenges and has forged a useful and productive partnership of industry and government participants\", \"resulting in three concept cars that demonstrate the feasibility of a variety of new automotive technologies\" with Diesel-electric transmission.\n\nThe three domestic automakers, GM, Ford, and Chrysler, developed fully operational concept cars. They were full sized, five-passenger family cars that achieved at least .\n\nGeneral Motors developed the 80 mpg Precept, Ford designed the 72 mpg Prodigy, and Chrysler built the 72 mpg ESX-3. They featured aerodynamic, lightweight aluminum or thermoplastic construction and were hybrid powered using 3- or 4-cylinder diesel engines and NiMH/lithium batteries.\n\nResearchers for the PNGV identified a number of ways to reach 80 mpg including reducing vehicle weight, increasing engine efficiency, combining gasoline engines and electric motors in hybrid vehicles, implementing regenerative braking, and switching to high efficiency fuel cell powerplants. Specific new technology breakthroughs achieved under the program included:\n\n\nRalph Nader called PNGV \"an effort to coordinate the transfer of property rights for federally funded research and development to the automotive industry\". \n\nThe program was also criticized by some groups for a focus on diesel solutions, a fuel that is seen by some as having inherently high air pollutant emissions.\n\nElizabeth Kolbert, a staff writer at \"The New Yorker\", described that renewable energy is the main problem, and that \"If someone, somewhere, comes up with a source of power that is safe, inexpensive, and for all intents and purposes inexhaustible, then we, the Chinese, the Indians, and everyone else on the planet can keep on truckin'. Barring that, the car of the future may turn out to be no car at all.\"\n\n"}
{"id": "3919647", "url": "https://en.wikipedia.org/wiki?curid=3919647", "title": "Polyamorphism", "text": "Polyamorphism\n\nPolyamorphism is the ability of a substance to exist in several different amorphous modifications. It is analogous to the polymorphism of crystalline materials. Many amorphous substances can exist with different amorphous characteristics (e.g. polymers). However, polyamorphism requires \"two distinct\" amorphous states with a clear, discontinuous (first-order) phase transition between them. When such a transition occurs between two stable liquid states, a polyamorphic transition may also be referred to as a liquid–liquid phase transition.\n\nEven though amorphous materials exhibit no long-range periodic atomic ordering, there is still significant and varied local structure at inter-atomic length scales (see structure of liquids and glasses). Different local structures can produce amorphous phases of the same chemical composition with different physical properties such as density. In several cases sharp transitions have been observed between two different density amorphous states of the same material. Amorphous ice is one important example (see also examples below). Several of these transitions (including water) are expected to end in a second critical point.\n\nPolyamorphism may apply to all amorphous states, i.e. glasses, other amorphous solids, supercooled liquids, ordinary liquids or fluids. A liquid–liquid transition however, is one that occurs only in the liquid state (red line in the phase diagram, top right). In this article liquid–liquid transitions are defined as transitions between two liquids of the same chemical substance. Elsewhere the term liquid–liquid transition may also refer to the more common transitions between liquid mixtures of different chemical composition.\n\nThe stable liquid state unlike most glasses and amorphous solids, is a thermodynamically stable equilibrium state. Thus new liquid–liquid or fluid-fluid transitions in the stable liquid (or fluid) states are more easily analysed than transitions in amorphous solids where arguments are complicated by the non-equilibrium, non-ergodic nature of the amorphous state.\n\nLiquid–liquid transitions were originally considered by Rapoport in 1967 in order to explain high pressure melting curve maxima of some liquid metals. Rapoport's theory requires the existence of a melting curve maximum in polyamorphic systems.\n\nOne physical explanation for polyamorphism is the existence of a double well inter-atomic pair potential (see lower right diagram). It is well known that the ordinary liquid–gas critical point appears when the inter-atomic pair potential contains a minimum. At lower energies (temperatures) particles trapped in this minimum condense into the liquid state. At higher temperatures however, these particles can escape the well and the sharp definition between liquid and gas is lost. Molecular modelling has shown that addition of a second well produces an additional transition between two different liquids (or fluids) with a second critical point.\n\nPolyamorphism has been experimentally observed or theoretically suggested in silicon, liquid phosphorus, triphenyl phosphate, and in some other molecular network-forming substances.\n\nThe most famous case of polyamorphism is amorphous ice. Pressurizing conventional hexagonal ice crystals to about 1.6 GPa at liquid nitrogen temperature (77 K) converts them to the high-density amorphous ice. Upon releasing the pressure, this phase is stable and has density of 1.17 g/cm at 77 K and 1 bar. Consequent warming to 127 K at ambient pressure transforms this phase to a low-density amorphous ice (0.94 g/cm at 1 bar). Yet, if the high-density amorphous ice is warmed up to 165 K not at low pressures but keeping the 1.6 GPa compression, and then cooled back to 77 K, then another amorphous ice is produced, which has even higher density of 1.25 g/cm at 1 bar. All those amorphous forms have very different vibrational lattice spectra and intermolecular distances. \nA similar abrupt liquid-amorphous phase transition is predicted in liquid silicon when cooled under high pressures. This observation is based on first principles molecular dynamics computer simulations, and might be expected intuitively since tetrahedral amorphous carbon, silicon, and germanium are known to be structurally analogous to water.\n\nYttria-alumina melts are another system reported to exhibit polyamorphism. Observation of a liquid–liquid phase transition in the supercooled liquid has been reported. Though this is disputed in the literature. Polyamorphism has also been reported in Yttria-Alumina glasses. Yttria-Alumina melts quenched from about 1900 °C at a rate ~400 °C/s, can form glasses containing a second co-existing phase. This happens for certain Y/Al ratios (about 20–40 mol% YO). The two phases have the same average composition but different density, molecular structure and hardness. However whether the second phase is glassy or crystalline is also debated.\nContinuous changes in density were observed upon cooling silicon dioxide or germanium dioxide. Although continuous density changes do not constitute a first order transition, they may be indicative of an underlying abrupt transition.\n\nPolyamorphism has also been observed in organic compounds, such as liquid triphenyl phosphite at temperatures about 200 K.\n\nPolyamorphism is also a potentially important area in pharmaceutical science. The amorphous form of a drug typically has much better aqueous solubility (cf. the analogous crystalline form) but the actual local structure in an amorphous pharmaceutical can be different, depending on the method used to form the amorphous phase\n"}
{"id": "29473146", "url": "https://en.wikipedia.org/wiki?curid=29473146", "title": "Pycnanthus angolensis", "text": "Pycnanthus angolensis\n\nPycnanthus angolensis is a species of tree in the nutmeg family, Myristicaceae. It is native to Tropical Africa. Its English language common names include African nutmeg, false nutmeg, boxboard, and cardboard. In Africa it is widely known as ilomba.\n\nThis evergreen tree grows up to 40 meters tall and usually up to a meter wide, sometimes up to 1.5 meters or more. The trunk is straight and cylindrical with fissures and flaking bark. The sap is honey-colored and turns red in time. The branches are in whorls. The leathery leaves are up to 31 centimeters long by 9 wide. The blades have pointed tips, heart-shaped bases, and thick midribs. They are hairless on top and coated with rusty, feltlike hairs on the undersides. The leaves usually bear signs of insect damage, a feature so common it is considered characteristic of the species. The flowers are arranged in dense, rusty panicles up to 15 centimeters long. The individual flowers are difficult to see in the tight panicle until the stamens develop, being only about a millimeter long. The flowers are hairy and fragrant. The fruit is a rounded drupe reaching over 3 centimeters long and wide, borne in clusters. It is hairy brown when new, turning yellow-orange, and has cartilaginous flesh that dries woody. It contains a black seed with a red aril which resembles that of nutmeg. The fruit ripens over a long period continuing into the next flowering season, which begins around October.\n\nThe tree grows in moist rainforests up to about 1200 meters in elevation among other evergreens and semideciduous trees. It occurs in secondary forest, sometimes taking hold in new canopy gaps or clearings. It thrives in sunny locations. It grows in riparian forests such as gallery forests, and in some regions it can be found in swamps. It occurs in areas receiving between about 1300 and 1800 millimeters of rain per year, but the optimal seems to be about 2000 millimeters.\n\nMany animals feed on the fruits, such as hornbills.\n\nThis species has a wide variety of human uses.\n\nIt is harvested for its wood, which is light, soft, and whitish gray or pink-tinged in color. Its popularity rose after World War II when plywood was in demand, and during the mid-20th century it was one of the more valuable timbers in Central Africa. It is not very durable and tends to warp, but it is easy to cut and work and can be used for many purposes. It is suitable for furniture, and in house construction as panelling, siding, roof shingles, and framing. It is used for fuel and paper pulp. The wood can be vulnerable to termites, powderpost beetles, and other pests.\n\nThe yellowish or reddish fat from the aromatic seed is called \"kombo butter\" or \"Angola tallow\". A seed can be up to 70% fat. It is used as a fuel for lighting and is made into soap. Seed remnants are used in compost. When ignited, the oily seed burns slowly and can be used as a candle.\n\nIn Uganda the tree is grown in banana, coffee, and cocoa plantations to shade the crops.\n\nMost parts of the tree have been used in traditional African medicine. The sap has been used to control bleeding. It is made into an eyewash to treat cataracts and filariasis of the eye. The bark has been used as a poison antidote and a treatment for leprosy, anemia, infertility, gonorrhea, and malaria. Leaf extracts are consumed or used in an enema to treat edema. Root extracts are used to treat parasitic infections, such as schistosomiasis. The seed oil is used to treat thrush.\n\nLike the fat of nutmeg, kombo butter is mostly myristic acid, with a high amount of myristoleic acid, as well. It contains the unique compound kombic acid, which was named for the tree under its nomenclatural synonym, \"P. kombo\".\n\nThis tree is cultivated for its products. Seeds are sown in the field, and seedlings are grown in plant nurseries until they have large taproots. They cannot tolerate drought. After a year the tree is about half a meter tall, and within 4 years it can reach 4 meters. A 20-year-old tree can be 25 meters tall. Stands of cultivated trees are pruned and thinned periodically.\n\nPest insects such as \"Monochamus scabiosus\" and \"Mallodon downesi\" and fungi such as \"Ophiostoma\" sp. have been observed, but they are not severe problems. The freshly harvested wood may become discolored due to bacterial growth, the light-colored wood developing dark brown staining.\n\nTrees can be cut when they reach a diameter of 50 centimeters, at around 30 years old.\n\nAgricultural cooperatives in Ghana cultivate the trees for the kombo butter.\n\nThere are common names for the plant in many languages. It is called mkungu mwitu in Swahili, akwa-mili and oje in Igbo, lunaba and munaba in Luganda, akomu in Yoruba, calabo in Spanish, and arbre à suif and faux muscadier in French. Local names include pó casson in São Tomé and Príncipe, gboyei in Sierra Leone and Liberia, otie in Ghana, eteng in Cameroon, lolako in Zaire, and adria, effoi, hétéré, qualélé, and walélé in Ivory Coast.\n"}
{"id": "1258361", "url": "https://en.wikipedia.org/wiki?curid=1258361", "title": "Relative thermal resistance", "text": "Relative thermal resistance\n\nRelative thermal resistance is a measure of the energy required to mix water of two different temperatures (and thus different densities). Comparisons will be made against the differences in the density of water at 3.98 °C and 5 °C. (Wetzel 1983) Since this is a relative measure, there are no units involved.\n\nIt is calculated at intervals of 1 cm using the following equation:\nformula_1\nwhere formula_2 is the density at the bottom of the stratum being considered\nand formula_3 is the density at the top of the stratum being considered\n\n"}
{"id": "17376728", "url": "https://en.wikipedia.org/wiki?curid=17376728", "title": "Renewable Energy Sources and Climate Change Mitigation", "text": "Renewable Energy Sources and Climate Change Mitigation\n\nThe United Nations Intergovernmental Panel on Climate Change (IPCC) published a special report on Renewable Energy Sources and Climate Change Mitigation (SRREN) on May 9, 2011. The report developed under the leadership of Ottmar Edenhofer evaluates the global potential for using renewable energy to mitigate climate change. This IPCC special report provides broader coverage of renewable energy than was included in the IPCC’s 2007 climate change assessment report, as well as stronger renewable energy policy coverage.\n\nRenewable energy can contribute to \"social and economic development, energy access, secure energy supply, climate change mitigation, and the reduction of negative environmental and health impacts\". Under favourable circumstances, cost savings in comparison to non-renewable energy use exist.\n\nPreviously the IPCC examined both renewable energy and energy efficiency in its fourth assessment report, published in 2007, but members decided that renewable energy commercialization merits additional in-depth coverage because of its importance in reducing carbon emissions.\n\nThe outline of the IPCC WG III’s Special Report on Renewable Energy Sources and Climate Change Mitigation (SRREN) was approved at the IPCC Plenary in Budapest in April, 2008. The final report was approved at the 11th session of the IPCC Working Group III, May 2011, in Abu Dhabi. The SRREN addresses the information needs of policy makers, private sector and civil society in a comprehensive way and will provide valuable information for further IPCC publications, including the upcoming IPCC 5th Assessment Report. The SRREN was released for publication on May 9, 2011.\n\nThe Special Report \"aims to provide a better understanding and broader information on the mitigation potential of renewable energy sources: technological feasibility, economic potential and market status, economic and environmental costs&benefits, impacts on energy security, co-benefits in achieving sustainable development, opportunities and synergies, options and constraints for integration into the energy supply systems and in the societies\".\n\nRenewable energy can contribute to \"social and economic development, energy access, secure energy supply, climate change mitigation, and the reduction of negative environmental and health impacts\".\n\nIn the report, the IPCC said \"as infrastructure and energy systems develop, in spite of the complexities, there are few, if any, fundamental technological limits to integrating a portfolio of renewable energy technologies to meet a majority share of total energy demand in locations where suitable renewable resources exist or can be supplied\". Under favourable circumstances, cost savings in comparison to non-renewable energy use exist. IPCC scenarios \"generally indicate that growth in renewable energy will be widespread around the world\". The IPCC said that if governments were supportive, and the full range of renewable technologies were deployed, renewable energy could account for almost 80% of the world's energy supply within four decades. Rajendra Pachauri, chairman of the IPCC, said the necessary investment in renewables would cost only about 1% of global GDP annually. This approach could keep greenhouse gas concentrations to less than 450 parts per million, the safe level beyond which climate change becomes catastrophic and irreversible.\n\n\n"}
{"id": "24648652", "url": "https://en.wikipedia.org/wiki?curid=24648652", "title": "Russia in the European energy sector", "text": "Russia in the European energy sector\n\nThe Russian Federation supplies a significant volume of fossil fuels and is the largest exporter of oil and natural gas to the European Union. In 2007, the European Union imported from Russia 185 million tonnes of crude oil, which accounted for 32.6% of total oil import, and 100.7 million tonnes of oil equivalent of natural gas, which accounted 38.7% of total gas import.\n\nIn 2017, energy products accounted around 60% of the total EU's import from Russia.\n\nThe Urengoy–Pomary–Uzhgorod pipeline was constructed in 1982–1984 with Western financing to provide Soviet gas to the Western European market.\n\nThe Russian state-owned company Gazprom exports natural gas to Europe. It also controls a large number of subsidiaries, including various infrastructure assets. According to the study published by the Research Centre for East European Studies, the liberalization of the EU gas market has driven Gazprom's expansion in Europe by increasing its share in the European downstream market. It has established sale subsidiaries in many of its export markets, and has also invested in access to industrial and power generation sectors in Western and Central Europe. In addition, Gazprom has established joint ventures to build natural gas pipelines and storage depots in a number of European countries. Transneft, a Russian state-owned company responsible for the national oil pipelines, is another Russian company supplying energy to Europe.\n\nIn September 2012 the European Commission has opened formal proceedings to investigate whether Gazprom is hindering competition in Central and Eastern European gas markets, in breach of EU competition law. In particular, the Commission is looking into Gazprom's usage of ‘no resale’ clauses in supply contracts, alleged prevention of diversification of gas supplies, and imposition of unfair pricing by linking oil and gas prices in long-term contracts. The Russian Federation responded by issuing a blocking legislation, which introduced a default rule prohibiting Russian strategic firms, including Gazprom, to comply with any foreign measures or requests. Compliance is made subject to a prior permission granted by the Russian government.\n\nIn the early 1980s there were American efforts, led by the Reagan administration, to convince European countries through which a proposed Soviet gas pipeline was to be built to deny firms responsible for construction the ability to purchase supplies and parts for the pipeline and associated facilities. The pipeline was built despite these protests and the rise of large Russian gas firms such as Gazprom as well as increased Russian fossil fuel production has facilitated a large expansion in the quantity of gas supplied to the European market since the 1990s.\n\nIn 2017, 39% of the European Union's natural gas total imports originated from Russia. As of 2009, Russian natural gas was delivered to Europe through 12 pipelines, of which three were direct pipelines (to Finland, Estonia and Latvia), four through Belarus (to Lithuania and Poland) and five through Ukraine (to Slovakia, Romania, Hungary and Poland). In 2011, an additional pipeline, Nord Stream (directly to Germany through the Baltic Sea), opened.\n\nThe largest importers of Russian gas in the European Union are Germany and Italy, accounting together for almost half of the EU gas imports from Russia. Other larger Russian gas importers (over 5 billion cubic meter per year) in the European Union are France, Hungary, Czech Republic, Poland, Austria and Slovakia. The largest non-EU importers of Russian natural gas are Ukraine, Turkey and Belarus.\n\nThe shares of Russian natural gas in the domestic gas consumption in EU countries were in 2013:\n\nAccording to Eurostat, 30% of EU's petroleum oil import came from Russia in 2017. For Estonia, Poland, Slovakia and Finland, more than 75 % of their imports in petroleum oils originated from Russia.\n\nAt the same time, the variety of national policies and stances of larger exporters \"versus\" larger dependents of Russian gas, together with the segmentation of the European gas market, has become a prominent issue in European politics toward Russia, with significant geopolitical implications for economic and political ties between the EU and Russia. These ties have occasionally led to calls for greater European energy diversity, although such efforts are complicated by the fact that many European customers have long term legal contracts for gas deliveries despite the disputes, most of which stretch beyond 2025–2030.\n\nA number of disputes in which Russia was using pipeline shutdowns in what was described as \"tool for intimidation and blackmail\" caused European Union to significantly increase efforts to diversify its energy sources.\n\nDuring an anti-trust investigation initiated in 2011 against Gazprom a number of internal company documents were seized that documented a number of \"abusive practices\" in an attempt to \"segment the internal [EU] market along national borders\" and impose \"unfair pricing\".\n\n"}
{"id": "26598683", "url": "https://en.wikipedia.org/wiki?curid=26598683", "title": "Scuttling of SMS Cormoran", "text": "Scuttling of SMS Cormoran\n\nThe Scuttling of SMS \"Cormoran\" off Guam on April 7, 1917 was the result of the United States entry into World War I and the internment of the German\nmerchant raider SMS \"Cormoran\". The incident was the only hostile encounter between United States and German military forces during the Pacific Ocean campaign of the war.\n\nSMS \"Cormoran\" was originally a passenger and cargo ship, named \"SS Ryaezan\" and built by the Germans in 1909 for the Russian merchant fleet. When the war broke out, she was captured off Korea by SMS \"Emden\" and transformed into an auxiliary cruiser. \"Cormoran\" was armed with eight 10.5 cm (4.1 in) rapid fire guns from the original SMS \"Cormoran\" and commanded by Captain Adalbert Zuckschwerdt. The number of crew she had on board is unknown. Setting out from Tsingtao on August 10, 1914 for a commerce raiding cruise in the South Pacific, SMS \"Cormoran\" failed to sink any enemy shipping as she spent all of her time avoiding allied warships.\n\nCaptain Zuckschwerdt pulled into Apra Harbor, Guam on December 14 with the intention of receiving coal from the Americans on the island. The United States was a neutral power at this time so the Germans were refused the proper amount of coal needed to continue their voyage, there was little coal on the island for the Americans and Guamanians themselves. So the German sailors were interned and for about two years they lived among the Americans and Guamanians in friendship until the American entry into World War I.\n\nWhen war was declared on April 7, 1917, the United States Marines and sailors on Guam were notified and set out from their base. They embarked the old screw schooner USS \"Supply\" with the goal of capturing the auxiliary cruiser, or destroying it. Not wanting to anger the Germans they had lived with for two years and not wanting to expose the Guamanians to needless harm; the Americans resorted to first requesting that the Germans surrender peacefully. In case anything went wrong, the artillery battery of three 7 inch guns on the western face of Mount Tenjo was also trained on the vessel. Before Captain Zuckschwerdt could refuse surrender a group of United States Navy sailors on the deck of USS \"Supply\" noticed that the Germans were preparing to scuttle their vessel instead of surrendering or attempting an escape. \n\nThe sailors notified the Marines so one of the Americans fired a shot across the German ship's bow with his rifle. The shot was the first to be fired by the United States at the Germans after war had been declared. A similar incident occurred in 1915 at Fort San Felipe del Morro in the Caribbean, America's very first shot was fired, at a different German auxiliary cruiser which was also interned in a neutral American port. Despite the warning shot, which alarmed the Germans, the scuttling continued but at a faster pace. The Germans finished setting their explosives and they began to evacuate. The \"Cormoran\" exploded and sank to the bottom of the harbor where she remains today. USS \"Supply\" quickly became a hospital ship when she came to the aid of the German lifeboats.\n\nAccounts of the event differ but what is known is that nine Germans were killed while scuttling the \"Cormoran\", either by the explosion which crippled the ship or by the American marines who did not want the \"Cormoran\" sunk. The rest of the German crew were captured by the Americans and the dead were buried on Guam with full military honors. The prisoners were sent to various American forts until finally being released after World War I in 1919.\n\nSMS \"Cormoran\" rests 110 feet (34 m) below the water of Apra harbor on her port side. A Japanese cargo ship, the \"Tokai Maru\", which was sunk during World War II leans up against her screw. Together the ships are one of the few places where a diver can visit a sunken vessel of World War I next to a sunken vessel of World War II. The shipwreck was placed on the National Register of Historic Places in 1975 due to its association with the First World War.\n\n\n"}
{"id": "18108162", "url": "https://en.wikipedia.org/wiki?curid=18108162", "title": "Slide (geography)", "text": "Slide (geography)\n\nThe term Slide as used in physical geography refers to the fixed or settled residue of a landslide that has stabilized, sometimes in the form of an alluvial fan. It has become a mostly permanent fixture to the landscape. \n"}
{"id": "32838550", "url": "https://en.wikipedia.org/wiki?curid=32838550", "title": "Social video marketing", "text": "Social video marketing\n\nSocial video marketing is a component of an integrated marketing communications plan designed to increase audience engagement through social activity around a given video. In a successful social video marketing campaign, the content, distribution strategy and consumer self-expression tools combine to allow an individual to “add their voice” or co-create value to a piece of content - then further propagating it out to their social circles. Social video typically benefits from a halo effect cast by the \"influencers” of a given social grouping. Social video marketing draws on consumer-culture theory, economic theory, and social theory around the psychology of sharing. Social video marketing differs from social marketing, which has the intent of influencing behavior for a social good.\n\nMedia publishers and content rights holders create social videos from TV, live video feeds and pre-recorded content in order to generate engagement on social platforms and drive media distribution. They use real-time video editing software to instantly create and share social videos in native formats such as vertical video for Snapchat and square video for Instagram.\n\nSocial video marketing is also distinct from viral marketing which is more closely aligned with the self-replicating nature of both “memorable and sufficiently” interesting content. In contrast to viral video where success is typically measured solely on the pass-along rate or the number of impressions, social video hinges upon leveraging a deeper more contextual relationship between sharer and recipient.\n\nSocial videos tends to be passed along because of a shared interest or a sense of trust between sender and recipient(s). Social videos attract conversation in either a one-to-one or a one-many relationship, with the comments and interactions becoming cumulative, rather than moving in a one-way trajectory, as in the case of a viral video.\n\n\nConditions which have made the market conducive to the rise of social video marketing:\n\n\nReelSEO mentions “social video marketing” frequently and has been using the term since at least 2010. “Social Video Marketing” as a term has also been championed by Internet entrepreneur Shawn Hopwood.\n\nIn a 2011 study published in Psychological Science, a journal of the Association for Psychological Science, Jonah Berger found that subjects the sharing of stories or information may be driven in part by arousal. When people are physiologically aroused, whether due to emotional stimuli or otherwise, the autonomic nervous is activated, which then boosts social transmission. Simply put, evoking certain emotions can help increase the chance a message is shared.\n\n“In a prior paper, we found that emotion plays a big role in which New York Times articles make the most emailed list. But interestingly, we found that while articles evoking more positive emotions were generally more viral, some negative emotions like anxiety and anger actually increased transmission while others like sadness decreased it. In trying to understand why, it seemed like arousal might be a key factor,” says Berger, the Joseph G. Campbell Jr. Assistant Professor of Marketing at the University of Pennsylvania.\n\nIn the study, Berger suggests that feeling fearful, angry, or amused drives people to share news and information. These types of emotions are characterized by high arousal and action, as opposed to emotions like sadness or contentment, which are characterized by low arousal or inaction. “If something makes you angry as opposed to sad, for example, you’re more likely to share it with your family and friends because you’re fired up,” continues Berger.\n\nMore recent video marketing statistics show that 55% of an online audience watch at least one video per day and that 86% of the total video viewers are between the ages of 18-26. This clearly indicates that millennials are watching a lot of video content. In addition to that, the younger age group is more inclined to conduct transactions online which is an added advantage for most of the businesses as they are able to market in front of the decision makers. \n\n“Why Do We Share Stories, News, and Information With Others?” - Psychological Science [3]\n\"Video Strategy\": Why and Where to Post Videos Online [4]\n"}
{"id": "10502400", "url": "https://en.wikipedia.org/wiki?curid=10502400", "title": "Standard day", "text": "Standard day\n\nThe term standard day is used throughout meteorology, aviation, and other sciences and disciplines as a way of defining certain properties of the atmosphere in a manner which allows those who use our atmosphere to effectively calculate and communicate its properties at any given time. For example, a temperature deviation of +8 °C means that the air at any given altitude is 8 °C (14 °F) warmer than what standard day conditions and the measurement altitude would predict, and would indicate a higher density altitude. These variations are extremely important to both meteorologists and aviators, as they strongly determine the different properties of the atmosphere.\n\nFor example, on a cool day, an airliner might have no problem safely departing a medium length runway, but on a warmer day, the density altitude might be higher, require a higher speed ground speed and true airspeed prior to liftoff, which would require more acceleration, a longer runway, and a reduced climb rate after liftoff. The pilot may choose to reduce the gross weight of the aircraft by carrying less fuel or reducing the amount/weight of the cargo, or even reducing the number of passengers (usually the last option). Not carrying sufficient fuel to complete the flight to the destination, the pilot would plan an intermediate fuel stop, which would likely delay the final destination arrival time. In meteorology, departure from standard day conditions is what gives rise to all weather phenomena, including thunderstorms, fronts, clouds, even the heating and cooling of our planet.\n\nFor Pilots: At sea level, Altimeter:29.92 in/Hg at \nThe \"standard day\" model of the atmosphere is defined at sea level, with certain present conditions such as temperature and pressure. But other factors, such as humidity, further alter the nature of the atmosphere, and are also defined under standard day conditions:\n\n\nThe first three properties are usually referred as \"standard day\" conditions, which the viscosity aspect is largely ignored throughout the aviation community. However, viscosity, which is affected by humidity levels, plays a key role in aerodynamic drag, which is why it is a key component of standard day conditions. Because it is a key component of drag, it affects the amount of fuel burned per unit of distance travelled.\n"}
{"id": "4368305", "url": "https://en.wikipedia.org/wiki?curid=4368305", "title": "Timeline of heat engine technology", "text": "Timeline of heat engine technology\n\nThis Timeline of heat engine technology describes how heat engines have been known since antiquity but have been made into increasingly useful devices since the 17th century as a better understanding of the processes involved was gained. They continue to be developed today.\n\nIn engineering and thermodynamics, a heat engine performs the conversion of heat energy to mechanical work by exploiting the temperature gradient between a hot \"source\" and a cold \"sink\". Heat is transferred to the sink from the source, and in this process some of the heat is converted into work.\n\nA heat pump is a heat engine run in reverse. Work is used to create a heat differential. The timeline includes devices classed as both engines and pumps, as well as identifying significant leaps in human understanding.\n\n\n\n\n\n\n\n\n"}
{"id": "4766969", "url": "https://en.wikipedia.org/wiki?curid=4766969", "title": "Vaijayanti", "text": "Vaijayanti\n\nThe Vaijayanti or Vyjayanti is a theological flower, offered to Krishna and Vishnu in worship as a garland, \"Vaijayanti-mala\". Literally meaning, \"the garland of victory\",: An Vaijayanti-mala also finds mention in \"Vishnu sahasranama\", a stotra dedicated to Vishnu in the \"Mahabharata\", as \"vanamali\" (forest flowers). The garland of victory, mentioned in the Mahabharata was made never-wilting lotuses (1.57.15-16), however currently many flowers are believed to be Vaijayanti, including canna lily is sometimes mistaken with original Vaijayanti. Original Vaijayanti plant looks like long grass and hence is difficult to identify, till flowering or seeds start growing in it.\n\nThe necklace of Lord Vishnu is crucial to the revelation of Vaikuntha. Considering Vaijayanthi as a living entity, and assuming relation through feminine genotype, she is the representation of absolute supercontrollership on context of civilization. Her mood transcends the need to magistrate, for she fascinates the applicant civilian from his or her relatively distracting wizardry or witchcraft into a more peaceful cooperativity by reflecting glory through strength absolute.\n\nAccording to her tradition, which is Vedic (of India), she prominently displays five precious gemstones: emerald, sapphire, ruby, pearl and diamond. These correspond with the five classic elements commonly named earth, water, fire, air and ether respectively. Her name, Vaijayanthi, means “triumphant victory.”\n"}
{"id": "2360885", "url": "https://en.wikipedia.org/wiki?curid=2360885", "title": "Valentin Turchin", "text": "Valentin Turchin\n\nValentin Fyodorovich Turchin (, 14 February 1931 in Podolsk – 7 April 2010 in Oakland, New Jersey) was a Soviet and American cybernetician and computer scientist. He developed the Refal programming language, the theory of metasystem transitions and the notion of supercompilation. He was as a pioneer in Artificial Intelligence and a proponent of the global brain hypothesis.\n\nTurchin was born in 1931 in Podolsk, Soviet Union. In 1952, he graduated from Moscow University in Theoretical Physics, and got his Ph.D. in 1957. After working on neutron and solid-state physics at the Institute for Physics of Energy in Obninsk, in 1964 he accepted a position at the Keldysh Institute of Applied Mathematics in Moscow. There he worked in statistical regularization methods and authored REFAL, one of the first AI languages and the AI language of choice in the Soviet Union.\n\nIn the 1960s, Turchin became politically active. In Fall 1968, he wrote the pamphlet \"The Inertia of Fear\", which was\nquite widely circulated in samizdat, the writing began to be circulated under the title \"The Inertia of Fear: Socialism and Totalitarianism\" in Moscow from 1976. Following its publication in the underground press, he lost his research laboratory. In 1970 he authored \"The Phenomenon of Science\", a grand cybernetic meta-theory of universal evolution, which broadened and deepened the earlier book. By 1973, Turchin had founded the Moscow chapter of Amnesty International with Andrey Tverdokhlebov and was working closely with the well-known physicist and Soviet dissident Andrei Sakharov. In 1974 he lost his position at the Institute, and was persecuted by the KGB. Facing almost certain imprisonment, he and his family were forced to emigrate from the Soviet Union in 1977.\n\nHe went to New York, where he joined the faculty of the City University of New York in 1979. In 1990, together with Cliff Joslyn and Francis Heylighen, he founded the Principia Cybernetica Project, a worldwide organization devoted to the collaborative development of an evolutionary-cybernetic philosophy. In 1998, he co-founded the software start-up SuperCompilers, LLC. He retired from his post of Professor of Computer Science at City College in 1999. A resident of Oakland, New Jersey, he died there on 7 April 2010.\n\nHis son, Peter Turchin, is a specialist in population dynamics and mathematical modeling of historical dynamics.\n\nThe philosophical core of Turchin's scientific work is the concept of the metasystem transition, which denotes the evolutionary process through which higher levels of control emerge in system structure and function.\n\nTurchin uses this concept to provide a global theory of evolution and a coherent social systems theory, to develop a complete cybernetics philosophical and ethical system, and to build a constructivist foundation for mathematics.\n\nUsing the REFAL language he has implemented a Supercompiler, a unified method for program transformation and optimization based on a metasystem transition.\n\n\nMost cited publications according to Google Scholar\n\n"}
{"id": "238112", "url": "https://en.wikipedia.org/wiki?curid=238112", "title": "Volcanology", "text": "Volcanology\n\nVolcanology (also spelled vulcanology) is the study of volcanoes, lava, magma, and related geological, geophysical and geochemical phenomena. The term \"volcanology\" is derived from the Latin word \"vulcan\". Vulcan was the ancient Roman god of fire.\n\nA volcanologist is a geologist who studies the eruptive activity and formation of volcanoes, and their current and historic eruptions. Volcanologists frequently visit volcanoes, especially active ones, to observe volcanic eruptions, collect eruptive products including tephra (such as ash or pumice), rock and lava samples. One major focus of enquiry is the prediction of eruptions; there is currently no accurate way to do this, but predicting eruptions, like predicting earthquakes, could save many lives.\n\nIn 1841, the first volcanological observatory, the Vesuvius Observatory, was founded in the Kingdom of the Two Sicilies.\n\nSeismic observations are made using seismographs deployed near volcanic areas, watching out for increased seismicity during volcanic events, in particular looking for long period harmonic tremors, which signal magma movement through volcanic conduits.\n\nSurface deformation monitoring includes the use of geodetic techniques such as leveling, tilt, strain, angle and distance measurements through tiltmeters, total stations and EDMs. This also includes GNSS observations and InSAR. Surface deformation indicates magma upwelling: increased magma supply produces bulges in the volcanic center's surface.\n\nGas emissions may be monitored with equipment including portable ultra-violet spectrometers (COSPEC, now superseded by the miniDOAS), which analyzes the presence of volcanic gases such as sulfur dioxide; or by infra-red spectroscopy (FTIR). Increased gas emissions, and more particularly changes in gas compositions, may signal an impending volcanic eruption.\n\nTemperature changes are monitored using thermometers and observing changes in thermal properties of volcanic lakes and vents, which may indicate upcoming activity.\n\nSatellites are widely used to monitor volcanoes, as they allow a large area to be monitored easily. They can measure the spread of an ash plume, such as the one from Eyjafjallajokull's 2010 eruption, as well as SO emissions. InSAR and thermal imaging can monitor large, scarcely populated areas where it would be too expensive to maintain instruments on the ground.\n\nOther geophysical techniques (electrical, gravity and magnetic observations) include monitoring fluctuations and sudden change in resistivity, gravity anomalies or magnetic anomaly patterns that may indicate volcano-induced faulting and magma upwelling.\n\nStratigraphic analyses includes analyzing tephra and lava deposits and dating these to give volcano eruption patterns, with estimated cycles of intense activity and size of eruptions.\n\nVolcanology has an extensive history. The earliest known recording of a volcanic eruption may be on a wall painting dated to about 7,000 BCE found at the Neolithic site at Çatal Höyük in Anatolia, Turkey. This painting has been interpreted as a depiction of an erupting volcano, with a cluster of houses below shows a twin peaked volcano in eruption, with a town at its base (though archaeologists now question this interpretation). The volcano may be either Hasan Dağ, or its smaller neighbour, Melendiz Dağ.\n\nThe classical world of Greece and the early Roman Empire explained volcanoes as sites of various gods. Greeks considered that Hephaestus, the god of fire, sat below the volcano Etna, forging the weapons of Zeus. The Greek word used to describe volcanoes was \"etna\", or \"hiera\", after Heracles, the son of Zeus. The Roman poet Virgil, in interpreting the Greek mythos, held that the giant Enceladus was buried beneath Etna by the goddess Athena as punishment for rebellion against the gods; the mountain's rumblings were his tormented cries, the flames his breath and the tremors his railing against the bars of his prison. Enceladus' brother Mimas was buried beneath Vesuvius by Hephaestus, and the blood of other defeated giants welled up in the Phlegrean Fields surrounding Vesuvius.\n\nThe Greek philosopher Empedocles (c. 490-430 BCE) saw the world divided into four elemental forces, of Earth, Air, Fire and Water. Volcanoes, Empedocles maintained, were the manifestation of Elemental Fire. Plato contended that channels of hot and cold waters flow in inexhaustible quantities through subterranean rivers. In the depths of the earth snakes a vast river of fire, the \"Pyriphlegethon\", which feeds all the world's volcanoes. Aristotle considered underground fire as the result of \"the...friction of the wind when it plunges into narrow passages.\"\n\nWind played a key role in volcano explanations until the 16th century. Lucretius, a Roman philosopher, claimed Etna was completely hollow and the fires of the underground driven by a fierce wind circulating near sea level. Ovid believed that the flame was fed from \"fatty foods\" and eruptions stopped when the food ran out. Vitruvius contended that sulfur, alum and bitumen fed the deep fires. Observations by Pliny the Elder noted the presence of earthquakes preceded an eruption; he died in the eruption of Vesuvius in 79 CE while investigating it at Stabiae. His nephew, Pliny the Younger gave detailed descriptions of the eruption in which his uncle died, attributing his death to the effects of toxic gases. Such eruptions have been named Plinian in honour of the two authors.\n\n\"Nuées ardentes\" were described from the Azores in 1580. Georgius Agricola argued the rays of the sun, as later proposed by Descartes had nothing to do with volcanoes. Agricola believed vapor under pressure caused eruptions of 'mointain oil' and basalt.\n\nJesuit Athanasius Kircher (1602–1680) witnessed eruptions of Mount Etna and Stromboli, then visited the crater of Vesuvius and published his view of an Earth with a central fire connected to numerous others caused by the burning of sulfur, bitumen and coal.\n\nJohannes Kepler considered volcanoes as conduits for the tears and excrement of the Earth, voiding bitumen, tar and sulfur. Descartes, pronouncing that God had created the Earth in an instant, declared he had done so in three layers; the fiery depths, a layer of water, and the air. Volcanoes, he said, were formed where the rays of the sun pierced the earth.\n\nScience wrestled with the ideas of the combustion of pyrite with water, that rock was solidified bitumen, and with notions of rock being formed from water (Neptunism). Of the volcanoes then known, all were near the water, hence the action of the sea upon the land was used to explain volcanism.\n\nTribal legends of volcanoes abound from the Pacific Ring of Fire and the Americas, usually invoking the forces of the supernatural or the divine to explain the violent outbursts of volcanoes. Taranaki and Tongariro, according to Māori mythology, were lovers who fell in love with Pihanga, and a spiteful jealous fight ensued. Māori will not to this day live between Tongariro and Taranaki for fear of the dispute flaring up again.\n\nIn the Hawaiian religion, Pele ( Pel-a; ) is the goddess of volcanoes and a popular figure in Hawaiian mythology. Pele was used for various scientific terms as for Pele's hair, Pele's tears, and Limu o Pele (Pele's seaweed). A volcano on the Jovian moon Io is also named Pele.\n\nSaint Agatha is patron saint of Catania, close to mount Etna, and an important highly venerated (till today) example of virgin martyrs of Christian antiquity. 253 CE, one year after her violent death, the stilling of an eruption of Mt. Etna was attributed to her intercession. Catania was however nearly completely destroyed by the eruption of Mt. Etna in 1169, and over 15,000 of its inhabitants died. Nevertheless, she was invoked again 1669 and, for an outbreak danginering Nicolosi in 1886. The way she is invoked and dealt with in Italian Folk religion, a sort of quid pro quo way approach to saints, has been related (in the tradition of James Frazer) to earlier pagan believes.\n\nIn 1660 the eruption of Vesuvius rained twinned pyroxene crystals and ash upon the nearby villages. The crystals resembled the crucifix and this was interpreted as the work of Saint Januarius. In Naples, the relics of St Januarius are paraded through town at every major eruption of Vesuvius. The register of these processions and the 1779 and 1794 diary of Father Antonio Piaggio allowed British diplomat and amateur naturalist Sir William Hamilton to provide a detailed chronology and description of Vesuvius' eruptions.\n\n\n\n"}
{"id": "288213", "url": "https://en.wikipedia.org/wiki?curid=288213", "title": "Ys", "text": "Ys\n\nYs (pronounced ), also spelled Is or Kêr-Is in Breton (\"kêr\" is the Breton word for \"city\", see caer), and Ville d'Ys in French, is a mythical city that was built on the coast of Brittany and later swallowed by the ocean. Most versions of the legend place the city in the Baie de Douarnenez.\n\nYs was built on land reclaimed from the sea by Gradlon (Gralon in Breton), King of Cornouaille (Kerne in Breton), upon the request of his daughter Dahut (also called Ahes), who loved the sea. To protect Ys from inundation, a dike was built with a gate that was opened for ships during low tide. The one key that opened the gate was held by the king.\n\nYs is described as a city rich in commerce and the arts, with Gradlon's palace being made of marble, cedar and gold.\n\nOther versions of the legend tell that Ys was founded more than 2,000 years before Gradlon's reign in a then-dry location off the current coast of the Bay of Douarnenez, but the Breton coast had slowly given way to the sea so that Ys was under it at each high tide when Gradlon's reign began.\n\nMost versions of the legend present Gradlon as a pious man with a wayward daughter, princess Dahut, who \"had made a crown of her vices and taken for her pages the seven capital sins.\"\nPrincess Dahut had a lover for whom she threw a secret banquet and, under the influence of wine, she stole the key to the gate from her father and opened the gate, and the water submerged the entire city. Another version of the legend says that she stole the silver key to admit her lover, mistakenly opening the sluices in the dark.\n\nSt Gwénnolé, who, according to one version, had foretold the city's ruin due to its luxury, woke the king and commanded him to flee. He mounted his horse and took his daughter with him. As the water was about to overtake him, a voice called out: \"Throw the demon thou carriest into the sea, if thou dost not desire to perish.\" Dahut fell from the horse's back, and Gradlon was saved.\n\nThough this is the most common version, there is an ancient ballad that blames Gradlon himself for leading his people to extravagances of every kind and says that Dahut received the key from him.\n\nAnother version of the legend directly identifies Dahut's lover with the devil. Ys was the most beautiful and impressive city in Europe, but quickly became a city of sin under the influence of Dahut. She organized orgies and had the habit of killing her lovers when morning broke. Saint Winwaloe decried the corruption of Ys and warned of God's wrath and punishment, but was ignored by Dahut and the populace.\n\nOne day, a knight dressed in red came to Ys. Dahut asked him to come with her, and one night, he agreed. A storm broke out in the middle of the night and the waves could be heard smashing against the gate and the bronze walls. Dahut said to the knight: \"Let the storm rage. The gates of the city are strong, and it is King Gradlon, my father, who owns the only key, attached to his neck.\" The knight replied: \"Your father the king sleeps. You can now easily take his key.\" Dahut stole the key from her father and gave it to the knight, who was none other than the devil. The devil then opened the gate.\n\nBecause the gate was open during storm and at high tide, a wave as high as a mountain collapsed on Ys. King Gradlon and his daughter climbed on Morvarc'h, his magical horse. Saint Winwaloe approached them and told Gradlon: \"Push back the demon sitting behind you!\" Gradlon initially refused, but he finally gave in and pushed his daughter into the sea. The sea swallowed Dahut, who became a mermaid or morgen.\n\nGradlon took refuge in Quimper, which became his new capital. An equestrian statue of Gradlon still stands between the spires of the Cathedral of Saint Corentin in Quimper. Folklore asserts that the bells of the churches of Ys can still be heard in the calm sea. A legend says that when Paris is swallowed, the city of Ys will rise up from under the waves: \"\" (\"Par-Is\" meaning \"similar to Ys\" in Breton).\n\nThis history is also sometimes viewed as the victory of Christianity over druidism, as Gradlon was converted by Saint Winwaloe. Dahut and most inhabitants of Ys were worshippers of Celtic gods. However, another Breton folktale asserts that Gradlon met, spoke with and consoled the last Druid in Brittany, and oversaw his pagan burial, before building a chapel in his sacred grove.\n\nWhile legends and literature about Gradlon are much older, such as the \"Lai de Graelent\" by Marie de France probably written in the late 12th century, the story of Ys appears to have developed between the end of the fifteenth century and the seventeenth century. An early mention of Ys appears in Pierre Le Baud's \"Cronicques et ystoires des Bretons\" in which Gradlon is the king of the city, but Dahut is not mentioned. Bernard d'Argentre's \"La histoire de Bretagne\" and mystery plays on the life of St. Winwaloe, in the sixteenth century, also provide early references to the city. The version of the story of Ys which appears in Albert Le Grand's \"Vie des Saincts de la Bretagne Armorique\" published in the seventeenth century already contains all the basic elements of the later story and has been said to be the first.\n\nThe legend of Ys was confined to the folk of Brittany until 1839, when T. Hersart de la Villemarqué published a collection of popular songs collected from oral tradition, the \"Barzaz Breizh\". The collection achieved a wide distribution and brought Breton folk culture into European awareness. In the second edition, the poem \"Livaden Geris\" (\"The Submersion of Ker-Is\") appeared. The same basic story elements are present, but in this version the holy man is instead St. Corentin.\n\nIt appears that elements of the text of this version were adapted from the medieval Welsh poem \"Seithennin\" about the legend of Cantre'r Gwaelod, a very similar Welsh legend about a land that disappeared beneath the ocean as a result of human error. In this version, Dahut steals the key at the incitement of a lover. Also, here the element of Dahut as a mermaid or morgen has appeared as the last verses of the song refer to a fisherman seeing a mermaid combing her hair and singing a sad song.\n\nEmile Souvestre's \"Le Foyer breton\" also played a great part in making the legend widely known, and many 19th century English tellings of the story are closely derived from the \"Foyer Breton\"'s tale \"Keris\". In Souvestre's telling, the character of the Devil disguised as a man with a red beard has appeared.\n\nSeveral famous artistic adaptations of the Ys legend appeared in the late 19th and early 20th century.\nE. V. Luminais' painting \"Flight of King Gradlon\", depicting Gradlon's escape from Ys, scored a success at the Salon of 1884.\n\n\"Le roi d'Ys\", an opera by the French composer Édouard Lalo which premiered in 1888, transforms the story significantly, replacing the figure of Dahut with Margared, whose motive for opening the gates (with the aid of her own betrothed Karnac) is her jealousy at her sister Rozenn's marriage to Mylio (characters who are also inventions of Lalo).\n\nAlso inspired by the story of Ys is Claude Debussy's \"La cathédrale engloutie\", found in his first book of \"Preludes\" (published 1910). This is a prelude intended to evoke the atmosphere of the legend by its sound.\n\nThe story is also an element in Alexander Blok's 1912 verse drama \"The Rose and the Cross\".\n\nThe city of Ys and the character Dahut feature prominently in the 1934 novel \"Creep, Shadow!\" by A. Merritt.\n\nPoul Anderson and his wife Karen Anderson retold the story in the tetralogy \"The King of Ys\" in the 1980s. They pictured Gradlon as a Roman soldier named Gratillonius.\n\nThe name Dahut and certain thematic elements of the story of Ys can be found in the survival/exploration noir game \"Sunless Sea\".\n\n\"The King in Yellow\" by Robert W. Chambers contains the story \"The Demoiselle d'Ys\".\n\nAlan Stivell's renowned album \"Renaissance of the Celtic Harp\" opens with a track entitled \"Ys\".\n\n\n\n"}
{"id": "33206644", "url": "https://en.wikipedia.org/wiki?curid=33206644", "title": "Zoigê Marsh", "text": "Zoigê Marsh\n\nThe Zoigê Marsh (), also known as the Ruoergai Marsh or the Songpan Grasslands, is located in the eastern part of the Tibetan Plateau and forms the largest high-altitude marsh area in the world. \n\nThe marsh areas are mainly located in northern Sichuan Province, but extend into southern Gansu and southeastern Qinghai. The marshes are formed in a region of poor drainage that is located between the watersheds of the Yellow and Yangtze Rivers. The marshes are located at an altitude of about 3,600 meters above sea level and cover an area of about 2,600 square kilometers. To east, they are bordered by the Min Mountains and to the west by the Amne Machin mountain range. \n\nThe marshes can be divided into four geomorphological regions: the Zoigê plateau plain, the Hong Yuan plateau mound, the A Ba plateau mountain, and the Songpan-Lixan Alp. The soil of the marshes contains a layer of peat that is about 2 to 3 meters thick in most places, but can reach up to 7 meters in thickness. \n\nThe Long March passed through the Zoigê Marsh in August 1935. The marshes were partially drained for grazing of cattle, sheep, and horses in the 1970s. This involved digging 200 km of ditches to drain 1,400 km of marshland. The project continued until the 1990s and exacerbated desertification problems. Herders reported that gold miners have also used cloud-seeding cannons to prevent rainfall, which has caused die-offs in vegetation.\n\nIn 1994, a nature preserve was established in the marshes. From 2010, community efforts to replant local grasses have been underway, with light grazing by yak allowed, so that the animals can tread the seeds down and stimulate regrowth.\n"}
