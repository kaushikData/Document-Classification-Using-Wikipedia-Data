{"id": "54781348", "url": "https://en.wikipedia.org/wiki?curid=54781348", "title": "Archaeological Park of Dion", "text": "Archaeological Park of Dion\n\nThe Archaeological Park of Dion is the most important archaeological site at Mount Olympus in Greece, located in Dion (Greek: Δίον). In the area comprised by the Archaeological Park of Dion, sanctuaries were found from the Hellenistic and Roman periods. The park displays the importance of ancient Dion in the history of Pieria.\nDion is located at the north-east foot of Mount Olympus. It is five kilometers from the sea, 15 kilometers from Katerini and 17 kilometers from the ancient Leivithra. In Hellenistic times the distance to the sea was only 1.5 kilometers. Dion is connected with the Thermaean Gulf by the once navigable river Baphyras.\n\nThe park has an area of 150 hectares, of which nearly 50 hectares belong to the urban area and 50 hectares to the sanctuaries. The other area has not yet been explored archaeologically. In the former urban area residential buildings, a market square, public buildings, churches, bathhouses, shops, workshops and toilets have been found. The sanctuaries, the theaters and the cemetery are located outside the city\n\nHalf an kilometer west of the archaeological park, in the modern village of Dion, is the archaeological museum, built in 1983. Here the finds of ancient Dion and other archaeological places are exhibited. On the first floor there are also exhibits from Pydna and other archaeological sites of Pieria. In a small cinema, visitors will be audiovisual informed about Dion.\n\nThe building was erected directly behind the museum on the western side, specifically for the exhibition of the Dionysos mosaic. On the upper floor, a gallery is arranged around the mosaic so that the visitor can view it from all perspectives. In showcases recently found exhibits are displayed.\n\n424 v. Thucydides mentions Dion as the first city reached by the Spartan general Brasidas, coming from Thessaly (Tempi) in Macedonia. Pausanias mentioned Dion as one of the places at Orpheus had lived.\n\nIn the Hellenistic period, Dion became the religious center of Macedonia. Zeus was venerated here, and Olympic games were held in honor of Zeus and the Muses. The village of Dion gained a certain importance within Greece through the sanctuary and over time developed into a city. Alexander the Great sacrificed to Zeus in Dion before he began his campaign against the Persians. Later, he had 25 bronze statues of the cavaliers fallen in the Battle of the Cranicos, erected in the Zeus Olympios Shrine. In the year 219 BC, the city was destroyed by the Aitolians. Philipp V had the city rebuilt immediately. The Romans took the city 169 BC. Gradually, Roman settlers came to Dion and brought their officialdom, their units of measurement and weight units with them. In the course of the changing owners, more sanctuaries were built. After the middle of the 3rd century AD, the decline started by the raids of neighboring tribes, earthquakes and floods. In the fourth century AD, Dion (Dium) experienced a last flourishing when it became the official seat of a bishop. The place is last mentioned as an administrative district of the Byzantine emperor Constantinos Porphyrogennetos in the 10th century.\n\nAt the end of the 18th century, the French consul Felix de Beaujour visited ancient Dion without knowing the ancient site that was abandoned and covered with the remains of buildings and columns.\n\nIn December 1806 the ancient Dion was rediscovered by the English explorer William M. Leake. He identified the ruins of ancient Dion near the village of Melathria, a small village inhabited by farmers and cattle breeders (Melathria was later renamed Dion). He recognized under the vegetation the ancient Hellenistic theater, the stadium and parts of the city wall. The French archaeologist Léon Heuzey confirmed the discovery in 1855. He mapped parts of the city wall, found the foundations of some towers and noted the inscriptions of some tombstones.\n\nFrom 1912, the year of the liberation of Macedonia by the Ottomans, the ancient Dion was given more attention. The archaeologist G. P. Oikonomos collected and published all the inscriptions he found in the vicinity of Dion.\n\nThe Rector of the University of Thessaloniki and Professor of Archeology, Georgios Sotiriadis, began with the first excavations. They began in June 1928 with the aim of finding the sanctuary of Zeus Olympios. He found and examined several of the tumuli within the city walls. Also, a basilica from early Christian times was discovered. The assumption that a temple was under the basilica proved to be deceptive after which one had dug five meters deep. The most important find of this first period of excavation was a Macedonian vault from the 4th century BC, Which had already been plundered by grave-robbers in ancient times. The excavations were discontinued in 1931.\n\nCharalambos Makaronas found a second Macedonian grave in 1955. A third grave was discovered a year later.\n\nFrom 1961 Georgios Bakalakis resumed the works. In 1962, a large part of the city walls and the defensive towers were discovered during the mapping of the previously known excavation site. During the excavation phase under G. Bakalakis, the Roman theater was located, which extends southeast of the Hellenistic theater. The excavations at the early Christian basilica were completed by Stylianos Pelekanidis.\n\nFrom the summer of 1973, the work was continued under the direction of Professor Dimitrios Pandermalis (University of Thessaloniki). His first goal was to explore the remains of two buildings south of the city area. The excavations revealed the Demeter sanctuary. In the same year, finds of statues of Asklepios, Hygeia and Telesphoros showed that also the Asklepios cult was practiced in Dion. On the main street the armor and the shields were released. The orchestras of the Hellenistic theater were then freed from the overlying layer of the earth. The theater dates from the 5th century BC; the Bacchae of Euripides were premiered here.\n\nIn the summer of 1976 excavations were carried out in the southeastern sector within the city wall. One came upon the great thermae. These were obviously destroyed by an earthquake. The mosaic of a bull in the frigidarium was separated into two parts, the lower part of which is 50 cm lower than the upper one. On the northern side of the baths were statues of the children of Asklepios.\n\nUnder difficult conditions the excavation work on the Isis Shrine took place. Spring water and mud caused the trenches to collapse frequently. A dam was built to continue the work. Again, there were signs of destruction by an earthquake with subsequent flooding. The excavations were completed in 1984.\n\nThe stadium was excavated in 1995 under the leadership of Giorgos Karadedos. Beside the playing field, several rows of seats were found.\n\nIn June 1987 the Dionysosmosaic was cleared and then protected by a roof construction against weather influences.\n\nThe sanctuary of Zeus Olympios was finally discovered in 2000 when investigations were made on the site near the Roman theater.\n\nAfter a flooding of the Isis Shrine of Archaeological Park in 2002, was decided to move the river Vaphyras a few yards westward, so that future floods can no longer harm the sanctuary. In the course of this, artifacts were found which ultimately led to the excavation of the sanctuary of Zeus Hypsistos, the Almighty God. As the work approached its end, the cult statue of Zeus Hypsistos was found in the mud.\n\nBeginning in 2007, under the leadership of Semeli Pingiatoglou, excavations were carried out with the aim of finding the oldest structures of ancient Dion.\n\nThe Dionysus mosaic was removed from its original place of discovery in 2015 and transferred to a specially built building (Archaeotheke).\n\nThe works are still continuing under the direction of the University of Thessaloniki.\n\nAlthough no special sanctuary was erected to him, the river Vaphyras was considered a divinity. About 100 m east of the park entrance is its source. The holy grove of the Muses probably grew here. The head of a statue representing the personalized river was found in the spring area. According to Hesiod, the Vaphyras originates from the cosmic river Okeanos, dominated by the primeval goddess Tethys.\n\nAn Artemis shrine discovered near the river is considered to be dedicated to the goddess Artemis Vaphyria. She watched the transition of young girls to the stage of marriageable women.\n\nThe river Vaphyras is interwoven in Greek mythology with Orpheus and the Muses. In the second century AD, Pausanias writes that the upper part of the Vaphyras bore the name of Helikon. Two-thirds of its length ran underground, before returning to Dion. According to Pausanias, however, this was not always the case. The inhabitants of Dion claimed that the Helikon flowed past Dion. But when the women who killed Orpheus wanted to wash the blood from their hands in the Helikon, the river dried up, for he did not wish to have any part in this act. Only in Dion did he appear again on the surface.\n\nIt consists of several temples and is dated from the archaic period up to the Roman period. The sanctuary of Asklepios is located in the immediate vicinity. The connection between the two sanctuaries is also evident at other archaeological sites in Greece.\n\nIn the open, walled-in area, the goddess was offered liquid sacrifices. The oldest remains of the sanctuary are also found here. End of the 4th century BC two archaic temples were replaced by two Doric temples. Small, one-room temples (Oikos) were consecrated to the gods of the earth like Baubo and Kurotrophos. They were hoped for rich harvest yields. Another temple was erected in honor of Aphrodite. The faithful hoped for increased fertility. In front of the temples there were altars on which carnal sacrifices were offered to the respective gods. Vegetable sacrifices, such as grain or fruit, were spread out on so-called cult tables. Archaeological finds and ancient records have given the water an important role in the Demeter cult. It was among the duties of the priestesses to ensure that pure water was always available. In addition to cleaning rituals, water was seen as the necessary element to allow plant growth. Two circular wells are among the oldest structures of the temple.\n\nIn addition to the usual finds such as statues, pottery, jewelery, oil lamps etc., one found a ring stone from Mycenaean time. It dates from the 14th to 15th century BC, and shows a schematically represented lion in front of a tree. The artifact is a reference to the earlier settlement of Dion. 1990 the foundations of an altar were discovered east of the temple; At this point 1973 the head of a statue of the goddess was recovered.\n\nIn the Late antiquity there were kilns on the grounds of the Demeter Shrine.\n\nIn the fourth century BC, The Asclepion was built. A place was chosen with plenty of water, because it played a special role in the practice of the Asclepion cult. It served the healing of the sick and was needed for the cult practices. So far, the foundations of a building that consisted of two rooms were cleared. The discovery of a toilet near the sanctuary suggests that people (pilgrims) were there for some time to cure their ailments.\n\nA sacred road led to the sanctuary of Zeus Hypsistos (Greek Ζευς ὕψιστος \"Zeus the highest god\"). It was lined with small columns with marble eagles sitting there. These are devotions for the \"Most Holy\". The road led to a large square, on which stood a temple. It consisted of several rooms. In the northernmost room, the temple of Zeus, a statue of Zeus Hypsistos and the figure of a marble eagle were found. The floor was decorated with mosaic, which retained the image of two ravens. Also the floor of the overall building was mosaic. There remained a white bull and double axes. On the western side is a water basin. In front of the temple stands an altar, to the base of which a metal ring was attached, which served to tie the sacrificial animals.\nBoth gods, Zeus Hypsistos and Zeus Olympios, were simultaneously venerated. While Zeus Olympios ruled the people from the top of Mount Olympus, Zeus Hypsistos dominated the sky, that is, all the supernatural.\n\nFollowing the conquest of Dion by the Romans, the Nonae Capratinae was held on 7 July of the year. Female slaves enjoyed certain freedoms at this festival; one of them received the rights of Agoranomos that day. The Agoranomos (composed of the Greek words agora, market, and nomos, law) supervised the trading on the marketplace, set prices and had other tasks. From the inscription of an eagle statuette found in the sanctuary of Zeus Hypsistus in September 2003, it appears that Arura, the servant (probably a slave) of Plutiades, was elected Agoranomos. This statuette is the first proof that the Nonae Capratinae was also celebrated in the Roman provinces, outside of Italy. The connection between the worship of Zeus Hypsistos and the Nonae Capratinae was probably in honor of Jupiter Capitolinus, the Jupiter Optimus Maximus.\n\nIn the Hellenistic period a powerful temple was built on a sacred grove consecrated to Zeus. In this sanctuary were gilded statues of the Macedonian kings. The bronze statues of his cavaliers who had fallen in the Battle of the Cranikos were also errected in the Zeus Olympios shrine. The central square within the building was occupied by a 22 m long altar. The sacrificial animals were tied to metal rings. In the sacrificial ceremonies, the most important part of the Zeus cult, 100 cattle were sacrificed.\n\nWhen the Aitolean league attacked Dion, the sanctuary was destroyed. But it was rebuilt immediately by the materials of the surrounding buildings by the Macedonian King Philip V.\n\nAt the time of the reign of the Macedonian kings, the sanctuary of Zeus Olympios was the most important sanctuary of the city and the religious center of Macedonia. It has not yet been clarified whether the shrine was given its importance by the Olympic Games initiated by King Archelaus, or whether, inspired by Homer's Iliad, it had already played a central role for the region. Deucalion claimed that in Dion, after the sanctuary of Zeus Lykaios, the second oldest altar devoted to Zeus was erected. From the late 8th century BC, Zeus was worshiped at various places in Greece. It was common to all these Zeus sanctuaries that they took place on the summit of a mountain, or near a summit. Inscriptions, pottery and remains of charcoal from the Hellenistic and Roman period on the summit of Agios Antonios (2817 m) near Dion, testify that the cult of Zeus was practiced not only in Dion, as also on Mount Olympus. The Macedonian kings used the temple to archive their royal decrees. Some of them are exhibited in the Archaeological Museum of Dion.\n\nThe most recent of the sanctuaries in Dion is the sanctuary of Isis. It was erected in the second century AD on the site of a former fertility sanctuary. The plant has a considerable size and is traversed by a channel, which is to symbolize the river Nile. The main entrance is in the east, i.e. the side facing the sea. A minor entrance is found on the north side of the sanctuary. The temple and altar of Isis Lochia (Isis as the guardian of the child's bed) are framed in the western part of the complex by two smaller temples of Isis Tyche and the Aphrodite Hypolympiada. In these little temples, springs are still active. In the cult of Isis, water was given sacred meaning. Two rooms, which are located in the north of the temple complex, served as a sanctuary for hypnotherapy, while figures of the patrons of the sanctuary were placed in the other room.\n\nIn the course of the first excavations, a vaulted Macedonian grave with a Doric façade from the 4th century BC was found. The marble doors were broken, the grave was robbed by grave robbers. A funerary couch of marble with the representation of a battle of cavalry and parts of a frieze, displaying lions, remained.\n\nA second grave was excavated in 1955. It contained a stone couch, the floor was designed with colored pebbles.\n\nOne year later, a third grave was released. Next to a stone couch, there were three pedestals.\n\nThe fourth grave was discovered in 1979. A funerary couch with ivory inserts hid behind the marble gates.\n\nThe last grave was found in 1988. Among the finds was a silver quarter-drachmae with depicting Alexander the Great and a golden Charon's penny (Charons Obolus), in which the name \"Epigenis\" was engraved.\n\nAs burial objects gold jewelery, golden and silver coins, glass bottles that may have contained perfumes, glass jars, and a copper mirror had been found. Some tombstones, as well as the burial objects, are displayed in the Archaeological Museum.\n\nThe classical theater, in which the premiere of Bakchen of Euripides took place, was replaced in the 3rd century BC by the Hellenistic theater. A semi-circular mound of earth was laid on which brick-built seats were placed. A digging around the round orchestra ensures the drainage of the rainwater. Underground rooms and hallways allowed the appearance and disappearance of actors and objects. The stage was slightly higher than the orchestra. There were devices which allowed to show special effects. The theater is used today, after a further modernization, for the annual \"Olympus Festival\".\n\nThe Roman theater was built in the 2nd century AD. It had 24 rows of seats arranged in a semicircle. Below the row of seats were 14 brick vaults. The orchestra had a diameter of about 21 meters. The building was built of bricks, field stones and mortar. The stage and the koilon (the auditorium, the seats) were separated; The stage was decorated with marble elements. Among the exhibits excavated there was a statue of Hermes.\n\nSince Dion was one of the few ancient Greek cities that lay in a single plane without any elevation, the city wall was particularly important in defense against attackers. In the east, the Vaphyras' marsh formed some protection, but there was neither a natural elevation nor an acropolis.\n\nThe city wall was built from the limestone of Mount Olympus between 306 and 304 BC, under the rule of the Macedonian king Kassander. It was 2625 meters long, three meters thick and seven to ten meters high. The western side is 642 m long, the southern and northern sections each 682 m long. The eastern part of the fortification has not yet been completely excavated. At a distance of 33 meters (100 Doric feet, 32.8 cm) stood towers with a floor area of seven by seven meters. In the southern and northern wall sections were two city gates, in the western part one city gate was found.\n\nAt the river Vaphyras, to the east of the town, there was probably a port facility. After the attack of the Aitolians (219 BC), in which the city wall was partly destroyed, the protective barrier was repaired immediately. He was given little attention during Roman domination. Sometimes even the masonry fell apart. When the attacks on Dion were piling up in the 3rd century AD, the city wall was repaired. Old sculptures and remains of other buildings were used as building materials. Floods by the Helikon and Vaphyras rivers during the early Christian era considerably reduced the urban area of Dion. The city wall had then only a length of 1600 meters. New walls were built on the north and east sides of the town. Column remains, sculptures and altars were used as building materials. In the 5th century AD the city wall was destroyed, probably by an earthquake. It was not rebuilt afterwards; This lack of protection may have been a reason why the population gradually left the village.\n\nIn the course of the excavations, private houses were brought to light in various parts of the complex. Almost all have mosaic flooring. The names of the former owners occasionally come from preserved parts of a mosaic, or from stamping the lead pipes, which served the water supply. In addition to mosaics, statues, columns, remains of furniture, busts and others were found.\n\nThe most important private building in the city is the Villa of Dionysus. In 1982, archaeologists started exploring the area east of the main road. They found a long-stretched building in the south-western part of which were shops and a bathing establishment. The bathroom could not only be entered from the street but had a separate entrance from the neighboring house. Further excavations brought to light statues of Dionysus, a Nike, and parts of other statues and statuettes. In June 1987 a large mosaic was found in the spacious atrium, which later became named Dionysusmosaic. Obviously, the atrium served as the dining room of the property. Among other finds in this room were a sculpture of four seated philosophers, the statuette of a satyr and a statuette of Heracles. In 1989, four more rooms of the villa were exposed. While two of them had less interesting finds, the third room found four clay storage vessels. In the last room there was a damaged mosaic that represents in its midst the head of a Medusa. There was also a statue of Heracles with a club, bow, arrows, lion's skin and a statue of a deer. Two years earlier, the work in the atrium already found the head of the deer and the hand of Heracles holding the bow. Works in the year 1990 brought parts of a statue to light which is a copy of the \"Eros with a Bow\" of the sculptor Lysippus.\n\nAll the thermal baths have the same structure as they have a pool with cold water basins and other basins with differently tempered water. Also the type of heating, by a hypocaust-system lying under the floor, is the same for all mentioned thermal baths.\n\nThe great thermal baths were built in the 2nd century AD. A hall covered with a mosaic floor leads to the bathing cabins and the water basins. There were rooms where Asclepios was worshiped. Since the thermal baths also served as a place for social gatherings, an Odeon was built for social events such as readings, plays, or musical performances in the complex. The shops and toilets were still part of the thermal baths.\n\nThe so-called \"thermal baths of the main street\" are located east of the main street, opposite the wall with the stone armors and shields. The equipment was comparable to the large thermal baths, the area was only a lot smaller.\n\nThe thermal baths on the market can be found at the north-east end of the Roman market. Mosaic flooring and a reception hall decorated with paintings are the special features of these baths.\n\nBuilt in the 2nd century AD, the Odeon is part of the large thermal baths. The outer mass is 28.46 m by 19.46 m, it offered 400 seats arranged in the form of an amphitheatre around the semicircular orchestra. The excavations of the Odeon began in September 1977 and lasted for two years. The elements of an ancient theater with orchestra, koilon, four interior staircases, stage and two L-shaped staircases were found. The carefully executed stonework work combines Roman architecture with local craftsmanship. The 1.55 m thick outer wall was a major static element; She caught the side pressure of the Koilon and supported the roof. The walls of the building were built of limestone or burnt bricks. The excavations revealed how the building was destroyed. The great cracks in the walls, as well as the lowering of the ground and some walls, indicate a strong earthquake with subsequent fire. In 1990, excavations were once again carried out to measure the entire ground plan of the Odeon for the planned restoration; Shards from the classic period were found.\n\nThe natural stresses where the remains of the Odeon were exposed to, heat, frost and moisture, destroyed parts of the building material (mortar, wood) over the centuries. The binding effect of the mortar subsided and the remaining foundation walls fell apart. The top part of the construction suffered the most, large blocks of the building dissolved and fell. The aim of the restoration was the preservation and reinforcement of the remains of the Odeon. The old mortar was sealed and cracks in the masonry were closed with fresh mortar. The fallen parts of the masonry were moved to their original place and fixed. The final work consisted of building a straight bearing surface from surrounding stones and covering them with specially made bricks. The material of the bricks corresponds to that of the ancient bricks. In the University of Thessaloniki the ancient bricks were examined and their composition determined. The composition of the mortar was determined according to laboratory tests and established in collaboration with the Management Committee for the Conservation of Ancient and Modern Monuments of the Ministry of Culture and Sport. The original construction was covered with lead before restoration. Thus, the old building fabric is severely separated from the new building materials. The floor of the Odeon was covered with pebbles and the architectural elements found on the site, such as columns, were erected in their original place.\n\nThe financing for the protection, conservation and restoration of the Odeon was taken from the EU program \"Macedonia-Thrace 2007-2013\". The works were based on the studies of Professor G. Karadedos, University of Thessaloniki.\n\nThe ancient Dion got its water from the Helikon River, 5 kilometres away. From there, water pipes were moved to Dion.\n\nPartly they were underground, some were built as an aqueduct. Parts of the aqueduct were found in a ravine northwest of the city. Within the city, the water gathered in a large cistern, from which the distribution was carried out in underground pipes to other cisterns or wells. Clay and lead were used as material for the water pipes. The central cistern was built in the 2nd century AD of stone and clay bricks. Two wells, which took care of households and baths, were located in the northeast and southeast of the city area. A third well was, after the destruction of the main cistern by an earthquake, replaced by a smaller cistern.\n\nDespite the organized water supply separate wells existed. So far, seven of them have been located and excavated. They are divided into three different types:\n\nIt is an open space surrounded by shops and halls. On the side facing the mountains, in the middle of the building surrounding the market, there was a temple (Sebasteion) presumably consecrated to the Roman emperors. The floor is slightly above the level of the market square and is decorated with mosaic. Inside, there were remains of murals and fragments of male statues. To the east of the square, opposite the temple, stood a Roman basilica. It was decorated with the frieze of armor and shields, which is now to the west of the main road. Under the supervision of the local authority, banking operations were conducted and commercial contracts were concluded in the basilica.\n\nNear the Villa of Dionysus, on the main road, is the Praetorium located. The building was used as a hostel for officials and emissaries as well as to accommodate ordinary travelers (Taberna). A locally found Latin inscription designates the building as Praetorium with two Tabernae. The entrance was on the south side; in the eastern part of the complex were five bedrooms and a luxurious dining room, the Triclinium. The Tabernae consisted of two larger rooms in the western wing. Here the archaeologists found earthen storage vessels and some lamps. Between Praetorium and the Tabernae was probably a stable. Public toilets were accessible to both, the guests and the city's population. A source served the hygiene, underground channels derived the dirty water.\n\nAt the intersection of the main street and the west gate leading street is the polygonal building. It covers about 1400 m² and probably served as a market hall. The complex is square and was built around a dodecagonal square. The square was surrounded by an arcade connecting the rooms of the building. The access was on the south side. A floor mosaic shows two wrestling athletes and two slaves with rucksacks.\n\nAt its last heyday, when the church appointed Dion to be the seat of a bishop, the basilica of the bishop was built in two phases of construction in the 4th and 5th centuries. It was a three-nave church with narthex. The remains of the walls are painted, the floor was mosaic. A smaller building, situated to the west of the church, served as a baptistery. An earthquake at the end of the fourth century destroyed the building. On its foundations a church was erected, in which the baptistery was integrated. The baptismal font had the shape of a Maltese cross.\n\nIn the middle of the cemetery, a three-nave church was built at the beginning of the 5th century. The center ship had a mosaic floor; Narthex and side ships were covered with clay tiles. Under the floor of the church, graves were discovered. Later on, a grain store were added to the building and a vault where the church treasury was kept.\n\nIn 2015, members of the Aristotle University, Thessaloniki, published a study on the condition of the stone building materials of the Asklepion and the Demeter Sanctuary. The aim of the study was to investigate the decay of the stone monuments and remains of buildings in the archaeological park of Dion. Mainly it was to be researched which environmental influences participate in which form in the decay process of the antique building materials in order to possibly preserve them. The building materials consist mainly of limestone, sandstone, conglomerate and marble.\n\nThe environmental conditions:\n\n\nMostly the surfaces are covered with salt and a black crust containing calcium, magnesium, soda, potash and other substances.\n\nThe studies were performed by using different microscopes and a spectrometer. From December 2010 to November 2011, monthly samples of precipitation were taken. These samples and water samples of the Vaphyras and other waters were analyzed. The temperature fluctuations of the rocks were measured with infrared thermometers.\n\nThe researchers found various organic and inorganic substances that influence the weathering of the monuments. However, the main factor influencing the decomposition of the material is the penetration of water. In combination with heat and cold, it reduces the cohesion of the surface structure and thus leads to the instability of the ancient building material. \n\nThe archaeological walk takes place every year as part of the Olympus Festival. Teachers of the Aristotle University of Thessaloniki guiding through the archaeological park and inform about various topics that touch ancient Dion. The arc spans from Greek mythology, the kingdom of Macedonia and the individual attractions of the park to daily life in the days of Alexander the Great. Embedded in the program is the performance of a short play or the recitation of ancient texts.\n\n\n"}
{"id": "31794032", "url": "https://en.wikipedia.org/wiki?curid=31794032", "title": "Atlantic Equatorial mode", "text": "Atlantic Equatorial mode\n\nThe Atlantic Equatorial Mode or Atlantic Niño is a quasiperiodic interannual climate pattern of the equatorial Atlantic Ocean. It is the dominant mode of year-to-year variability that results in alternating warming and cooling episodes of sea surface temperatures accompanied by changes in atmospheric circulation. The term Atlantic Niño comes from its close similarity with the El Niño-Southern Oscillation (ENSO) that dominates the tropical Pacific basin. The Atlantic Niño is not the same as the Atlantic Meridional (Interhemispheric) Mode that consists of a north-south dipole and operates more on decadal timescales. The equatorial warming and cooling events associated with the Atlantic Niño are known to be strongly related to atmospheric climate anomalies, especially in African countries bordering the Gulf of Guinea. Therefore, understanding of the Atlantic Niño (or lack thereof) has important implications for climate prediction in those regions. Although the Atlantic Niño is an intrinsic mode to the equatorial Atlantic, there may be a tenuous causal relationship between ENSO and the Atlantic Niño in some circumstances.\n\nGlobal tropical variability is dominated by ENSO in the equatorial Pacific. This phenomenon results from air-sea interaction, producing a coupled atmosphere-ocean system that oscillates with periods on the order of three to five years. However, the physical basis for this oscillation is not limited strictly to the Pacific basin, and indeed, a very similar mode of variability exists in the equatorial Atlantic, albeit on a smaller scale.\n\nThe Atlantic Niño is characterized by a sea surface temperature anomaly centered on the equator between 0° and 30°W. Unlike its Pacific counterpart, the Atlantic Niño does not have sea surface temperature anomalies that switch sign from east to west, but rather a single basin-wide anomaly. Additionally, the amplitude of the Atlantic Niño tends to be about half that of El Niño. Not surprisingly, this sea surface temperature anomaly is closely related to a change in the climatological trade winds. A warm anomaly is associated with relaxed trade winds across a large swath of the equatorial Atlantic basin, while a cool anomaly is associated with enhanced easterly wind stress in the same region. These trade wind fluctuations can be understood as the weakening and strengthening of the Atlantic Walker circulation. This is strikingly similar to the wind stress anomalies seen in the Pacific during El Niño (or La Niña) events, although centered farther west in the Atlantic basin. A major difference between El Niño and the Atlantic Niño is that the sea surface temperature anomalies are strictly constrained to the equator in the Atlantic case, while greater meridional extent is observed in the Pacific.\n\nWhile the spatial characteristics of the mature Atlantic Niño are quite similar to its Pacific counterpart, its temporal variability is somewhat different. The Atlantic Niño varies on interannual timescales like El Niño, but also shows more variance on seasonal and annual timescales. That is to say, the Atlantic Niño explains a smaller portion of the total variance in the equatorial Atlantic than does El Niño in the equatorial Pacific. This is due to the fact that there are seasonal climate events that are superimposed on the interannual variability. The Atlantic Niño typically reaches mature phase in boreal summer (though there are exceptions), while El Niño matures in boreal winter. The development of the Atlantic Niño tends to be marked by emerging stationary patterns centered mid-basin. This is in stark contrast to El Niño, which can often develop as warm sea surface temperature anomalies that migrate west from the coast of South America or migrate east from the central Pacific.\n\nWarming or cooling of the equatorial oceans has understandable consequences for atmospheric climate. The equatorial oceans comprise a major portion of the overall heat budget and, therefore, alter convective regimes near the equator. In the case of the Pacific El Niño, enhanced convection over the central Pacific and reduced convection over the Maritime Continent fundamentally change climate not just in the tropics, but globally. Since the Atlantic Niño is physically similar to ENSO, we might expect climate impacts from it as well. However, given its reduced size both spatially (the Atlantic basin is much smaller than the Pacific basin) and in magnitude, the climate impacts of the Atlantic Niño are best seen in the tropical and subtropical regions nearest to the equatorial Atlantic.\n\nThe impact of the Atlantic Niño on African climate can be best understood by assessing how above normal equatorial sea surface temperatures impact the seasonal migration of the Intertropical Convergence Zone (ITCZ). Warm equatorial sea surface temperatures lower surface air pressure which induces more equatorward flow than normal. This, in turn, prevents the ITCZ from migrating as far north as it would under normal conditions during the summer, reducing rainfall in the semi-arid Sahel to the north, and increasing rainfall in regions along the Gulf of Guinea. Increased rainfall relative to normal is typically associated with negative temperature anomalies over these tropical land areas. Some evidence suggests that a warming trend in Indian Ocean equatorial sea surface temperatures contributes to long-term drying of the Sahel, which is exacerbated by periodic warming of the equatorial Atlantic related to the Atlantic Niño. In fact, the ability to predict the Atlantic Niño is a major research question given its impact on seasonal climate.\n\nGlobal tropical variability is largely dominated by the Pacific El Niño, leaving as a valid question whether the Atlantic Niño might be a remote impact of El Niño. There is no apparent contemporaneous relationship between the two, but such a statement is not necessarily useful considering that El Niño peaks in winter while the Atlantic Niño peaks in summer. Lagged analyzes reveal that the most prominent El Niño impact on the tropical Atlantic the following spring and summer is a warm sea surface temperature anomaly centered north of the Atlantic Niño region. This again appears to suggest that there is not causal relationship. However, more rigorous analysis suggests that the competition between cooling that results from increased wind stress and warming that results from increased air temperature, both of which are remote impacts of El Niño on the Atlantic, accounts for a tenuous relationship. When one of these processes dominates over the other, an Atlantic Niño (warm or cool) event could ensue. This is of major interest considering the challenge in seasonal prediction of the Atlantic Niño.\n\n\n\n"}
{"id": "49336757", "url": "https://en.wikipedia.org/wiki?curid=49336757", "title": "Battery storage power station", "text": "Battery storage power station\n\nA battery storage power plant is a form of storage power plant, which uses batteries on an electrochemical basis for energy storage. Unlike common storage power plants, such as the pumped storage power plants with capacities up to 1000 MW, the benefits of battery storage power plants move in the range of a few kW up to the MW range; as of 2017, the largest installed system has a storage capacity of 300 MWh. Battery storage power plants, like all storage power plants, primarily serve to cover peak load and in networks with insufficient control power and the grid stabilization. Small banks of rechargeable batteries with a few kWh of storage capacity, are mostly in the private sector operated in conjunction with wind turbines or similarly sized photovoltaic systems to daytime bring revenue surpluses in yield poorer or unproductive hours in the evening or at night, and to strengthen their own consumption. Sometimes battery storage power stations are built with flywheel storage power systems in order to conserve battery power. Flywheels can handle rapid fluctuations better.\n\nStructurally battery storage power plants and uninterruptible power supplies (UPS) are comparable, although the former are larger. The batteries are housed for security in their own warehouses or in containers. As with a UPS, the problem is that electrochemical energy is stored or emitted in the form of direct current DC, while electric power networks are usually operated with Alternating current AC voltage. For this reason, additional inverters are needed to connect the battery storage power plants to the high voltage network. This kind of power electronics include GTO thyristors, commonly used in the high-voltage direct current transmission (HVDC).\nVarious accumulator systems may be used depending on the power-to-energy ratio, the expected life time and, of course, the costs. In the 1980s, lead-acid batteries were used for the first battery-storage power plants. During the next few decades, nickel-cadmium and sodium-sulfur battery were increasingly used. Since 2010, more and more utility-scale battery storage plants rely on lithium-ion batteries thanks to the fast decrease in the cost of this technology, driven by the electric automotive industry. This is the case of the battery Park Schwerin, the battery storage in Dresden or the storage of BYD in Hong Kong. Lithium-ion batteries are mainly used, some redox flow system have emerged and lead-acid batteries are still used in small budget applications. There are numerous suppliers of large battery storage.\n\nSince they do not require any mechanical movement, battery storage power plants allow extremely short control times and start times, in the range of few 10s of ms at full load. Thanks to that reactivity, they can shave power peaks in the range of minutes, but they can also dampen the fast oscillations (second) that appear when electric power networks are operated close to their maximum capacity. These instabilities consist in voltage fluctuations with periods of up to several 10 seconds and can soar in worst cases to high amplitudes, which can lead to regional blackouts. A battery storage power plants properly dimensioned can efficiently counteract these oscillations. Therefore, applications are found primarily in those regions where electrical power systems are operated at full capacity, causing a risk in the grid stability. Large storage plants (Na-S)can also be used in combination with (Na-S)intermittent renewable energy source in standalone hybrid micro-grids. \n\nSome systems, operating at high temperature (Na-S) or using corrosive components are subject to failure even if they are not used (calendar ageing). Other technologies suffer from deterioration caused by charge-discharge cycles (cycle ageing), especially at high charging rates. These two types of ageing cause a loss of performance (capacity or voltage decrease), overheating and may eventually lead to critical failure (electrolyte leaks, fire, explosion). In order to prevent the loss of performance due to ageing, some batteries can undergo maintenance operation. For example, non-sealed Lead-acid batteries produce hydrogen and oxygen from the aqueous electrolyte when overcharged. The water has to be refilled regularly to avoid damage to the battery and the inflammable gases have to be vented out to avoid explosion risks. However, this maintenance has a cost and recent batteries, such as Li-Ion, are designed to have a long lifespan without maintenance. Therefore, most of the current systems are composed of securely sealed battery packs which are electronically monitored and replaced once their performance falls below a given threshold.\n\nSome of the largest battery storage power plants are exemplified below.\n\nOperated from 1988 to 1997 by the Southern California Edison in the Californian city Chino, the battery storage power station served primarily for grid stabilization and could be used by frequent power outages in the region as a static var compensator and black start of non-black bootable power plants. The plant had a peak power of 14 MW, which was, however, far too little for effective stabilization in the net of Southern California Edison, and a storage capacity of 40 MWh. The system consisted of 8,256 lead-acid batteries in eight strands, which were divided into two halls.\n\nOne of the largest and located with Stand 2010 operating system is operated by the Golden Valley Electric in Fairbanks. The power grid in Alaska is operated due to the large distances as stand-alone grid with no direct connection to neighboring North American interconnections within the North American Electric Reliability Corporation. The battery storage power plant with a maximum capacity of 27 MW is used to stabilize the grid, covering high peak and reactive power compensation. The plant was put into operation in 2003 and consists of 13,760 nickel-cadmium batteries in four strands. The NiCd cells are manufactured by Saft Groupe S.A., the inverters by ABB Group.\n\nThe Chinese company BYD operates a battery banks with 40 MWh capacity and 20 MW maximum power in Hong Kong. The large storage is used to cushion load peaks in energy demand. Likewise, the storage can contribute to the frequency stabilization in the net. The battery is made up of a total of almost 60,000 individual lithium iron phosphate cells, each with 230 amp hour capacity. The project was started in October 2013, and went online in June 2014. The actual installation of the storage lasted three months. The use of price differences between loading and unloading by day and night electricity, an avoided grid expansion for peak loads and revenue for grid services such as Frequency stabilization enable economic operation without subsidies. Currently 3 locations for a 1,000 MW peak power to 200 MWh capacity storage power plant to be examined.\n\nIn Schwerin, Germany the electricity supplier WEMAG operates a lithium-ion battery storage to compensate for short-term power fluctuations. Supplier of battery storage power station is the Berlin company Younicos. The South Korean company Samsung SDI supplied the lithium-ion cells. The storage has a capacity of 5 MWh and an output of 5 MW was in September 2014 in operation. The lithium-ion battery storage consists of 25,600 lithium manganese cells and is about five medium-voltage transformers with both the regional distribution connected as well with the nearby 380 kV high-voltage grid.\n\nThe existing photovoltaic power plant Alt Daber near Wittstock in Brandenburg, Germany received a battery storage of 2 MWh. A special feature is that this is a turnkey solution supplied and installed in containers, for immediate use on site without major construction work. The storage uses lead-acid batteries.\n\nSince July 2014, the energy storage company Nord GmbH & Co. KG has been operating the largest hybrid batteries in Europe in Braderup (Schleswig-Holstein, Germany). The System consists of a lithium-ion battery storage (2 MW power 2 MWh storage) and a vanadium flow battery storage (330 kW power, 1 MWh storage capacity). The lithium-ion modules used are from Sony, the flow battery is made by Vanadis Power GmbH.\n\nThe storage system is connected to the local community wind park (18 MW installed capacity). Depending on wind strength and charging status of each battery a system developed by Bosch distributes the energy generated by the wind turbines to the right battery. Bosch is also responsible for project implementation and system integration. The hybrid battery is connected to the power grid by a ten-kilometer long underground cable. In case of a network congestion the batteries buffer the energy of the wind farm and feed it back into the grid at a convenient time later. With this method, a shutdown of wind turbines can be avoided during times of network congestion so that the energy of the wind is not wasted.\n\nStadtwerke Dresden, Germany (Drewag) have taken a battery storage with a peak power of 2 MW online on March 17, 2015. The costs amounted to 2.7 million euros. lithium polymer batteries are being used. The batteries including the control system are deployed in two 13 m long containers and can store a total of 2.7 MWh. The system is designed to compensate for peak power generation of a nearby solar plant.\n\nIn Feldheim in Brandenburg, Germany, a battery storage with a capacity of 10 MW and a storage capacity of 6.5 MWh was put into operation in September, 2015. The project cost 12.8 million euros. The storage provides energy for the power grid to compensate for fluctuations caused by wind and solar power plants. The store is operated by the company Energiequelle.\n\nEvonik is planning to build six battery storage power plants with a capacity of 15 MW to be put into operation in 2016 and 2017. They are to be situated in North Rhine-Westphalia, Germany at the power plant sites Herne, Lünen and Duisburg-Walsum and in Bexbach, Fenne and Weiher in the Saarland.\n\nThe largest grid storage batteries in the United States include the 31.5 MW battery at Grand Ridge Power plant in Illinois and the 31.5 MW battery at Beech Ridge, West Virginia. Both using lithium ion batteries.\n\nUnder construction in 2015 is the 400 MWh (100 MW for 4 hours) Southern California Edison project. Developed by AES Energy it is a lithium-ion battery system. Southern California Edison found the prices for battery storage comparable with other electricity generators.\n\nIn 2017, Tesla built a 52 MWh project on Kauai, Hawaii to entirely time shift a 13 MW solar farm's output to the evening. The aim is to reduce dependence on fossil fuels on the island.\n\nAt present (2/2016) is under construction a 250 MWh battery storage in Indonesia. There are about 500 villages in Indonesia which should be supplied, so far they depend on the power supply of petroleum. In past the prices fluctuated greatly and there was often power outages. Now the power will be generated through wind and solar power.\n\nOne battery is in Notrees, Texas (36 MW for 40 minutes using lead-acid batteries).\n\nA 13 MWh battery made of worn batteries from electric cars is being constructed in Germany, with an expected second life of 10 years, after which they will be recycled.\n\nIn Ontario, Canada, a battery storage with 53 MWh capacity and 13 MW of power is established by the end of 2016. The Swiss battery manufacturer Leclanché supplies the batteries now. Deltro Energy Inc. will plan and build the plant. The order was placed by the network operator IESO. The energy storage are used to provide fast grid services, mainly for voltage and reactive power control. In Ontario and the surrounding area there are many wind and solar power plants, whereby the power supply varies widely.\n\nIn southern England a battery storage with a capacity of 0.6 MWh and 0.3 MW power was installed for demonstration purposes, made up of 1400 lithium cells installed in a container, with a special feature being the control of the storage.\nWhereas usually a battery storage uses only revenue model, the provision of control energy, which is a very small market, this storage uses three revenue models. The storage has been installed next to a solar system. This way the solar system can be designed larger than the grid power actually permits in the first revenue model. The storage accepts a peak input of the solar system, thus avoiding the cost of a further grid expansion. The second model allows taking up peak input from the power grid and feeding it back to stabilize the grid when necessary. The third model is storing energy and feeding it into the grid at peak prices. The store received an award for top innovation.\n\nIn July 2018, a 50MW battery storage facility with a capacity of 50MWh was installed in Stocking Pelham.\n\nSince January 2016 in South Korea three battery storage power plants are in operation. There are two new systems, a 24 MW system with 9 MWh and a 16 MW system with 6 MWh. These two uses batteries based on lithium-nickel-manganese-cobalt oxide and supplement a few months older system with 16MW and 5MWh whose batteries are based on lithium titanate oxide. Together the systems have a capacity of 56 MW and serve the South Korean utility company Korea Electric Power Corporation (KEPCO) for frequency regulation. The storage comes from the company Kokam. After completion in 2017, the system should have a power of 500 MW. The three already installed storage reduce annual fuel costs by an estimated 13 million US dollars, as well as cutting greenhouse gas emissions. Thus the saved fuel costs will exceed the cost of battery storage significantly.\n\nAn existing system in an Aboriginal community in Australia consisting of a combination photovoltaic system and diesel generator will be extended by a lithium-ion battery to a hybrid system. The battery has a capacity of about 2 MWh and a power of 0.8 MW. The batteries store the excess solar power and take over the previously network-forming functions such as network management and network stabilization of diesel generators. Thus, the diesel generators can be switched off during the day, which leads to cost reduction. Moreover, the share of renewable energy rises in the hybrid system significantly. The system is part of a plan to transform the energy systems of indigenous communities in Australia.\n\nOn the Azores island of Graciosa a 3.2 MWh storage was installed. Along with a 1 MW photovoltaic plant and a 4.5 MW wind farm, the island is almost completely independent of the previously used diesel generators. The old power plant only serves as a backup system in the event that power from solar and wind power plant can not be generated over a longer time because of bad weather. The sharp decline of expensive diesel imports means that electricity is cheaper than before. The so generated profit will each divided in half between the investor of the new plant and the end users. More Azores islands are to follow.\n\nTesla installed a grid storage facility for the Southern California Edison with a capacity of 80 MWh at a power of 20 MW between September 2016 and December 2016. This means that the storage unit (1/2017) is currently one of the largest accumulator batteries on the market. Tesla installed 400 Powerpack-2 modules at the Mira Loma transformer station in California. The capacity serves to store energy at a low network load and then to feed this energy back into the grid at peak load. Prior to this, gas-fired power stations were used.\n\nMitsubishi installed a storage facility in Buzen, Fukuoka Prefecture in Japan with 300 MWh capacity and 50 MW power. The storage is used to stabilize the network to compensate for fluctuations caused by renewable energies. The accumulator is in the power range of pumped storage power plants. The batteries are installed in 252 containers. The plant occupies an area of 14,000 square meters.\n\nTesla, Inc. built the Hornsdale Power Reserve adjacent to the Hornsdale wind farm. it is promoted as the largest lithium-ion battery in the world. Its 100 MW output capacity is contractually divided into two sections: 70 MW running for 10 minutes, and 30 MW with a 3 hour capacity. Samsung 21700-size cells are used.\n\nIt is operated by Tesla and provides a total of of storage capable of discharge at into the power grid. The system helps to prevent load-shedding blackouts and provide stability to the grid (grid services) while other generators can be started in the event of sudden drops in wind or other network issues. It was built in less than 100 days counting from 29 September 2017, when a grid connection agreement was signed with ElectraNet, and some units were operational. The battery construction was completed and testing began on 25 November 2017. It was connected to the grid on 1 December 2017. During two days in January 2018 where South Australia was hit by price spikes, the battery made its owners an estimated 1M AUD as they sold power from the battery to the grid for a price of around 14k AUD/MWh.\n\nIn the US, the market for storage power plants in 2015 increased by 243 percent compared to 2014. In 2016, the UK grid operator National Grid posted independent from technology 200 MW of control power to increase system stability. In this case, only battery storage power plants won the auction. \n"}
{"id": "17116093", "url": "https://en.wikipedia.org/wiki?curid=17116093", "title": "Bistonis", "text": "Bistonis\n\nBistonis is the name of a nymph in Greek mythology who gave birth to a son of Ares, Tereus. Although she is mentioned in several surviving classical texts, she is the main subject of few or none. In at least one poem, written by Moschus in the 3rd century BCE, Lake Bistonis, in Thrace, is referred to as being \"her\" lake, and that lake is described as having a population of nymphs:\n\nHer name is similar to the name of a city in Thrace, Bistonia, said in ancient Greek mythology to have been built on the shores of that lake by Biston, who was the son of Ares and Callirrhoe.\n\nThe Family Magazine; or, Monthly Abstract of General Knowledge (1833-1841); Apr 1835; 2, APS Online \npg. A103\n"}
{"id": "41855006", "url": "https://en.wikipedia.org/wiki?curid=41855006", "title": "Borophene", "text": "Borophene\n\nBorophene is a crystalline atomic monolayer of boron, i.e., it is a two-dimensional allotrope of boron and also known as \"boron sheet\". \nFirst predicted by theory in the mid-1990s, \ndifferent borophene structures were experimentally confirmed in 2015.\n\nExperimentally various atomically-thin, crystalline and metallic borophenes were synthesized on clean metal surfaces under ultrahigh-vacuum conditions. Their atomic structure is still debated, but most likely it consists of mixed triangular and hexagonal motives, such as shown in Figure 1. The atomic structure is a consequence of an interplay between two-center and multi-center in-plane bonding, which is typical for electron deficient elements like boron.\n\nComputational studies by I. Boustani and A. Quandt showed that small boron clusters do not adopt icosahedral geometries likes boranes, instead they turn out to be quasi-planar (see Figure 2).\nThis lead to the discovery of a so called Aufbau principle \nwhich predicts the existence of borophene (boron sheets),\nboron fullerenes (borospherene) \nand boron nanotubes.\n\nAdditional studies showed that extended, triangular borophene (Figure 1(c)) is metallic and adopts a non-planar, buckled geometry.\nFurther computational studies, initiated by the prediction of a stable B boron fullerene, \nsuggested that extended borophene sheets with honeycomb structure and with partially filled hexagonal holes are stable. \nThese borophene structures were also predicted to be metallic. The so called γ sheet (a.k.a. β borophene or υ sheet) is shown in Figure 1(a).\n\nThe planarity of boron clusters was first experimentally confirmed by the research team of L.-S. Wang.\nLater they showed that the structure of (see Figure 2) is the smallest boron cluster to have sixfold symmetry and a perfect hexagonal vacancy, and it can be viewed as a potential basis for extended two-dimensional boron sheets. \n\nAfter the synthesis of silicene multiple groups predicted that borophene could potentially be realized with the support of a metal surface.\nFinally, in 2015 two research teams succeeded in synthesizing different borophene phases on silver (111) surfaces under ultrahigh-vacuum conditions. Among the three borophene phases synthesized (see Figure 1), the \"χ\" borophene was previously predicted by Zeng team in 2012 \nAtomic-scale characterization, supported by theoretical calculations, revealed structures reminiscent of fused boron clusters consisting mixed triangular and hexagonal motives, such as predicted by theory before and shown in Figure 1. Scanning tunneling spectroscopy indicates that the borophenes are indeed metallic. This is in contrast to bulk boron allotropes, which are semiconducting and marked by an atomic structure based on B icosahedra.\n\n"}
{"id": "3237549", "url": "https://en.wikipedia.org/wiki?curid=3237549", "title": "Coherent backscattering", "text": "Coherent backscattering\n\nIn physics, coherent backscattering is observed when coherent radiation (such as a laser beam) propagates through a medium which has a large number of scattering centers (such as milk or a thick cloud) of size comparable to the wavelength of the radiation.\n\nThe waves are scattered many times while traveling through the medium. Even for incoherent radiation, the scattering typically reaches a local maximum in the direction of backscattering. For coherent radiation, however, the peak is two times higher.\n\nCoherent backscattering is very difficult to detect and measure for two reasons. The first is fairly obvious, that it is difficult to measure the direct backscatter without blocking the beam, but there are methods for overcoming this problem. The second is that the peak is usually extremely sharp around the backward direction, so that a very high level of angular resolution is needed for the detector to see the peak without averaging its intensity out over the surrounding angles where the intensity can undergo large dips. At angles other than the backscatter direction, the light intensity is subject to numerous essentially random fluctuations called speckles.\n\nThis is one of the most robust interference phenomena that survives multiple scattering, and it is regarded as an aspect of a quantum mechanical phenomenon known as weak localization (Akkermans et al. 1986). In weak localization, interference of the direct and reverse paths leads to a net reduction of light transport in the forward direction. This phenomenon is typical of any coherent wave which is multiple scattered. It is typically discussed for light waves, for which it is similar to the weak localization phenomenon for electrons in disordered (semi)conductors and often seen as the precursor to Anderson (or strong) localization of light. Weak localization of light can be detected since it is manifested as an enhancement of light intensity in the backscattering direction. This substantial enhancement is called the cone of coherent backscattering.\n\nCoherent backscattering has its origin in the interference between direct and reverse paths in the backscattering direction. When a multiply scattering medium is illuminated by a laser beam, the scattered intensity results from the interference between the amplitudes associated with the various scattering paths; for a disordered medium, the interference terms are washed out when averaged over many sample configurations, except in a narrow angular range around exact backscattering where the average intensity is enhanced. This phenomenon, is the result of many sinusoidal two-waves interference patterns which add up. The cone is the Fourier transform of the spatial distribution of the intensity of the scattered light on the sample surface, when the latter is illuminated by a point-like source. The enhanced backscattering relies on the constructive interference between reverse paths. One can make an analogy with a Young's interference experiment, where two diffracting slits would be positioned in place of the \"input\" and \"output\" scatterers.\n\n"}
{"id": "34298804", "url": "https://en.wikipedia.org/wiki?curid=34298804", "title": "Composition of Mars", "text": "Composition of Mars\n\nThe composition of Mars covers the branch of the geology of Mars that describes the make-up of the planet Mars.\n\nAlso like Earth, Mars is a differentiated planet, meaning that it has a central core made up of metallic iron and nickel surrounded by a less dense, silicate mantle and crust. The planet's distinctive red color is due to the oxidation of iron on its surface.\n\nMuch of what we know about the elemental composition of Mars comes from orbiting spacecraft and landers. (See Exploration of Mars for list.) Most of these spacecraft carry spectrometers and other instruments to measure the surface composition of Mars by either remote sensing from orbit or \"in situ\" analyses on the surface. We also have many actual samples of Mars in the form of meteorites that have made their way to Earth. Martian meteorites (often called SNC's, for Shergottites, Nakhlites, and Chassignites—the groups of meteorites first shown to have a martian origin) provide data on the chemical composition of Mars' crust and interior that would not otherwise be available except through a sample return mission.\nBased on these data sources, scientists think that the most abundant chemical elements in the Martian crust, besides silicon and oxygen, are iron, magnesium, aluminum, calcium, and potassium. These elements are major components of the minerals comprising igneous rocks. The elements titanium, chromium, manganese, sulfur, phosphorus, sodium, and chlorine are less abundant but are still important components of many accessory minerals in rocks and of secondary minerals (weathering products) in the dust and soils (the regolith). Hydrogen is present as water (HO) ice and in hydrated minerals. Carbon occurs as carbon dioxide (CO) in the atmosphere and sometimes as dry ice at the poles. An unknown amount of carbon is also stored in carbonates. Molecular nitrogen (N) makes up 2.7 percent of the atmosphere. As far as we know, organic compounds are absent except for a trace of methane detected in the atmosphere.\n\nOn 16 December 2014, NASA reported the \"Curiosity\" rover detected a \"tenfold spike\", likely localized, in the amount of methane in the Martian atmosphere. Sample measurements taken \"a dozen times over 20 months\" showed increases in late 2013 and early 2014, averaging \"7 parts of methane per billion in the atmosphere.\" Before and after that, readings averaged around one-tenth that level.\n\nThe elemental composition of Mars is different from Earth′s in several significant ways. First, Martian meteorite analysis suggests that the planet's mantle is about twice as rich in iron as the Earth's mantle. Second, its core is richer in sulphur. Third, the Martian mantle is richer in potassium and phosphorus than Earth's and fourth, the Martian crust contains a higher percentage of volatile elements such as sulphur and chlorine than the Earth's crust does. Many of these conclusions are supported by \"in situ\" analyses of rocks and soils on the Martian surface.\n\nMars is fundamentally an igneous planet. Rocks on the surface and in the crust consist predominantly of minerals that crystallize from magma. Most of our current knowledge about the mineral composition of Mars comes from spectroscopic data from orbiting spacecraft, \"in situ\" analyses of rocks and soils from six landing sites, and study of the Martian meteorites. Spectrometers currently in orbit include THEMIS (Mars Odyssey), OMEGA (Mars Express), and CRISM (Mars Reconnaissance Orbiter). The two Mars exploration rovers each carry an Alpha Particle X-ray Spectrometer (APXS), a thermal emission spectrometer (Mini-TES), and Mössbauer spectrometer to identify minerals on the surface.\n\nOn October 17, 2012, the Curiosity rover on the planet Mars at \"Rocknest\" performed the first X-ray diffraction analysis of Martian soil. The results from the rover's CheMin analyzer revealed the presence of several minerals, including feldspar, pyroxenes and olivine, and suggested that the Martian soil in the sample was similar to the \"weathered basaltic soils\" of Hawaiian volcanoes.\n\nThe dark areas of Mars are characterised by the mafic rock-forming minerals olivine, pyroxene, and plagioclase feldspar. These minerals are the primary constituents of basalt, a dark volcanic rock that also makes up the Earth's oceanic crust and the lunar maria.\nThe mineral olivine occurs all over the planet, but some of the largest concentrations are in Nili Fossae, an area containing Noachian-aged rocks. Another large olivine-rich outcrop is in Ganges Chasma, an eastern side chasm of Valles Marineris (pictured). Olivine weathers rapidly into clay minerals in the presence of liquid water. Therefore, areas with large outcroppings of olivine-bearing rock indicate that liquid water has not been abundant since the rocks formed.\n\nPyroxene minerals are also widespread across the surface. Both low-calcium (ortho-) and high-calcium (clino-) pyroxenes are present, with the high-calcium varieties associated with younger volcanic shields and the low-calcium forms (enstatite) more common in the old highland terrain. Because enstatite melts at a higher temperature than its high-calcium cousin, some researchers have argued that its presence in the highlands indicates that older magmas on Mars had higher temperatures than younger ones.\nBetween 1997 and 2006, the Thermal Emission Spectrometer (TES) on the Mars Global Surveyor (MGS) spacecraft mapped the global mineral composition of the planet. TES identified two global-scale volcanic units on Mars. Surface Type 1 (ST1) characterises the Noachian-aged highlands and consists of unaltered plagioclase- and clinopyroxene-rich basalts. Surface Type 2 (ST2) is common in the younger plains north of the dichotomy boundary and is more silica rich than ST1. The lavas of ST2 have been interpreted as andesites or basaltic andesites, indicating the lavas in the northern plains originated from more chemically evolved, volatile-rich magmas. (See Igneous differentiation and Fractional crystallization.) However, other researchers have suggested that ST2 represents weathered basalts with thin coatings of silica glass or other secondary minerals that formed through interaction with water- or ice-bearing materials.\nTrue intermediate and felsic rocks are present on Mars, but exposures are uncommon. Both TES and the Thermal Emission Imaging System (THEMIS) on the Mars Odyssey spacecraft have identified high-silica rocks in Syrtis Major and near the southwestern rim of the crater Antoniadi. The rocks have spectra resembling quartz-rich dacites and granitoids, suggesting that at least some parts of the Martian crust may have a diversity of igneous rocks similar to Earth's. Some geophysical evidence suggests that the bulk of the Martian crust may actually consist of basaltic andesite or andesite. The andesitic crust is hidden by overlying basaltic lavas that dominate the surface composition but are volumetrically minor.\n\nRocks studied by Spirit Rover in Gusev crater can be classified in different ways. The amounts and types of minerals make the rocks primitive basalts—also called picritic basalts. The rocks are similar to ancient terrestrial rocks called basaltic komatiites. Rocks of the plains also resemble the basaltic shergottites, meteorites which came from Mars. One classification system compares the amount of alkali elements to the amount of silica on a graph; in this system, Gusev plains rocks lie near the junction of basalt, picrobasalt, and tephite. The Irvine-Barager classification calls them basalts.\nOn March 18, 2013, NASA reported evidence from instruments on the Curiosity rover of mineral hydration, likely hydrated calcium sulfate, in several rock samples including the broken fragments of \"Tintina\" rock and \"Sutton Inlier\" rock as well as in veins and nodules in other rocks like \"Knorr\" rock and \"Wernicke\" rock. Analysis using the rover's DAN instrument provided evidence of subsurface water, amounting to as much as 4% water content, down to a depth of , in the rover's traverse from the \"Bradbury Landing\" site to the \"Yellowknife Bay\" area in the \"Glenelg\" terrain.\n\nIn the journal Science from September 2013, researchers described a different type of rock called \"\"Jake M\" or \"Jake Matijevic (rock)\",” It was the first rock analyzed by the Alpha Particle X-ray Spectrometer instrument on the Curiosity rover, and it was different from other known martian igneous rocks as it is alkaline (>15% normative nepheline) and relatively fractionated. \"Jake M\" is similar to terrestrial mugearites, a rock type typically found at ocean islands and continental rifts. \"Jake M\" discovery may mean that alkaline magmas may be more common on Mars than on Earth and that Curiosity could encounter even more fractionated alkaline rocks (for example, phonolites and trachytes).\nOn December 9, 2013, NASA researchers described, in a series of six articles in the journal Science, many new discoveries from the Curiosity rover. Possible organics were found that could not be explained by contamination. Although the organic carbon was probably from Mars, it can all be explained by dust and meteorites that have landed on the planet. Because much of the carbon was released at a relatively low temperature in Curiosity’s Sample Analysis at Mars (SAM) instrument package, it probably did not come from carbonates in the sample. The carbon could be from organisms, but this has not been proven. This organic-bearing material was obtained by drilling 5 centimeters deep in a site called \"Yellowknife Bay\" into a rock called “\"Sheepbed mudstone\"”. The samples were named \"John Klein\" and \"Cumberland\". Microbes could be living on Mars by obtaining energy from chemical imbalances between minerals in a process called chemolithotrophy which means “eating rock.” However, in this process only a very tiny amount of carbon is involved — much less than was found at \"Yellowknife Bay\".\nUsing SAM’s mass spectrometer, scientists measured isotopes of helium, neon, and argon that cosmic rays produce as they go through rock. The fewer of these isotopes they find, the more recently the rock has been exposed near the surface. The 4-billion-year-old lakebed rock drilled by Curiosity was uncovered between 30 million and 110 million years ago by winds which sandblasted away 2 meters of overlying rock. Next, they hope to find a site tens of millions of years younger by drilling close to an overhanging outcrop.\n\nThe absorbed dose and dose equivalent from galactic cosmic rays and solar energetic particles on the Martian surface for ~300 days of observations during the current solar maximum was measured. These measurements are necessary for human missions to the surface of Mars, to provide microbial survival times of any possible extant or past life, and to determine how long potential organic biosignatures can be preserved. This study estimates that a few meters drill is necessary to access possible biomolecules. The actual absorbed dose measured by the Radiation Assessment Detector (RAD) is 76 mGy/yr at the surface. Based on these measurements, for a round trip Mars surface mission with 180 days (each way) cruise, and 500 days on the Martian surface for this current solar cycle, an astronaut would be exposed to a total mission dose equivalent of ~1.01 sievert. Exposure to 1 sievert is associated with a 5 percent increase in risk for developing fatal cancer. NASA's current lifetime limit for increased risk for its astronauts operating in low-Earth orbit is 3 percent. Maximum shielding from galactic cosmic rays can be obtained with about 3 meters of Martian soil.\n\nThe samples examined were probably once mud that for millions to tens of millions of years could have hosted living organisms. This wet environment had neutral pH, low salinity, and variable redox states of both iron and sulfur species. These types of iron and sulfur could have been used by living organisms. C, H, O, S, N, and P were measured directly as key biogenic elements, and by inference, P is assumed to have been there as well. The two samples, \"John Klein\" and \"Cumberland\", contain basaltic minerals, Ca-sulfates, Fe oxide/hydroxides, Fe-sulfides, amorphous material, and trioctahedral smectites (a type of clay). Basaltic minerals in the mudstone are similar to those in nearby aeoliandeposits. However, the mudstone has far less Fe-forsterite plus magnetite, so Fe-forsterite (type of olivine) was probably altered to form smectite (a type of clay) and magnetite. A Late Noachian/EarlyHesperian or younger age indicates that clay mineral formation on Mars extended beyond Noachian time; therefore, in this location neutral pH lasted longer than previously thought.\n\n \nMuch of the Martian surface is deeply covered by dust as fine as talcum powder. The global predominance of dust obscures the underlying bedrock, making spectroscopic identification of primary minerals impossible from orbit over many areas of the planet. The red/orange appearance of the dust is caused by iron(III) oxide (nanophase FeO) and the iron(III) oxide-hydroxide mineral goethite.\n\nThe Mars Exploration Rovers identified magnetite as the mineral responsible for making the dust magnetic. It probably also contains some titanium.\n\nThe global dust cover and the presence of other wind-blown sediments has made soil compositions remarkably uniform across the Martian surface. Analysis of soil samples from the Viking landers in 1976, Pathfinder, and the Mars Exploration rovers show nearly identical mineral compositions from widely separated locations around the planet. The soils consist of finely broken up basaltic rock fragments and are highly enriched in sulphur and chlorine, probably derived from volcanic gas emissions.\n\nMinerals produced through hydrothermal alteration and weathering of primary basaltic minerals are also present on Mars. Secondary minerals include hematite, phyllosilicates (clay minerals), goethite, jarosite, iron sulfate minerals, opaline silica, and gypsum. Many of these secondary minerals require liquid water to form (aqueous minerals).\n\nOpaline silica and iron sulphate minerals form in acidic (low pH) solutions. Sulphates have been found in a variety of locations, including near Juventae Chasma, Ius Chasma, Melas Chasma, Candor Chasma, and Ganges Chasma. These sites all contain fluvial landforms indicating that abundant water was once present. Spirit rover discovered sulfates and goethite in the Columbia Hills.\n\nSome of the mineral classes detected may have formed in environments suitable (i.e., enough water and the proper pH) for life. The mineral smectite (a phyllosilicate) forms in near-neutral waters. Phyllosilicates and carbonates are good for preserving organic matter, so they may contain evidence of past life. Sulfate deposits preserve chemical and morphological fossils, and fossils of microorganisms form in iron oxides like hematite. The presence of opaline silica points toward a hydrothermal environment that could support life. Silica is also excellent for preserving evidence of microbes.\n\nLayered sedimentary deposits are widespread on Mars. These deposits probably consist of both sedimentary rock and poorly indurated or unconsolidated sediments. Thick sedimentary deposits occur in the interior of several canyons in Valles Marineris, within large craters in Arabia and Meridiani Planum (see Henry Crater for example), and probably comprise much of the deposits in the northern lowlands (e.g., Vastitas Borealis Formation). The Mars Exploration Rover Opportunity landed in an area containing cross-bedded (mainly eolian) sandstones (Burns formation). Fluvial-deltaic deposits are present in Eberswalde Crater and elsewhere, and photogeologic evidence suggests that many craters and low lying intercrater areas in the southern highlands contain Noachian-aged lake sediments.\n\nWhile the possibility of carbonates on Mars has been of great interest to exobiologists and geochemists alike, there was little evidence for significant quantities of carbonate deposits on the surface. In the summer of 2008, the TEGA and WCL experiments on the 2007 \"Phoenix\" Mars lander found between 3–5wt% (percent by weight) calcite (CaCO) and an alkaline soil. In 2010, analyses by the Mars Exploration Rover \"Spirit\" identified outcrops rich in magnesium-iron carbonate (16–34 wt%) in the Columbia Hills of Gusev crater. The magnesium-iron carbonate most likely precipitated from carbonate-bearing solutions under hydrothermal conditions at near-neutral pH in association with volcanic activity during the Noachian Period.\n\nCarbonates (calcium or iron carbonates) were discovered in a crater on the rim of Huygens Crater, located in the Iapygia quadrangle. The impact on the rim exposed material that had been dug up from the impact that created Huygens. These minerals represent evidence that Mars once had a thicker carbon dioxide atmosphere with abundant moisture, since these kind of carbonates only form when there is a lot of water. They were found with the Compact Reconnaissance Imaging Spectrometer for Mars (CRISM) instrument on the Mars Reconnaissance Orbiter. Earlier, the instrument had detected clay minerals. The carbonates were found near the clay minerals. Both of these minerals form in wet environments. It is supposed that billions of years ago Mars was much warmer and wetter. At that time, carbonates would have formed from water and the carbon dioxide-rich atmosphere. Later the deposits of carbonate would have been buried. The double impact has now exposed the minerals. Earth has vast carbonate deposits in the form of limestone.\n\nThe rocks on the plains of Gusev are a type of basalt. They contain the minerals olivine, pyroxene, plagioclase, and magnetite, and they look like volcanic basalt as they are fine-grained with irregular holes (geologists would say they have vesicles and vugs).\nMuch of the soil on the plains came from the breakdown of the local rocks. Fairly high levels of nickel were found in some soils; probably from meteorites.\nAnalysis shows that the rocks have been slightly altered by tiny amounts of water. Outside coatings and cracks inside the rocks suggest water deposited minerals, maybe bromine compounds. All the rocks contain a fine coating of dust and one or more harder rinds of material. One type can be brushed off, while another needed to be ground off by the Rock Abrasion Tool (RAT).\n\nThere are a variety of rocks in the Columbia Hills (Mars), some of which have been altered by water, but not by very much water.\n\nThe dust in Gusev Crater is the same as dust all around the planet. All the dust was found to be magnetic. Moreover, Spirit found the magnetism was caused by the mineral magnetite, especially magnetite that contained the element titanium. One magnet was able to completely divert all dust hence all Martian dust is thought to be magnetic. The spectra of the dust was similar to spectra of bright, low thermal inertia regions like Tharsis and Arabia that have been detected by orbiting satellites. A thin layer of dust, maybe less than one millimeter thick covers all surfaces. Something in it contains a small amount of chemically bound water.\n\nObservations of rocks on the plains show they contain the minerals pyroxene, olivine, plagioclase, and magnetite. These rocks can be classified in different ways. The amounts and types of minerals make the rocks primitive basalts—also called picritic basalts. The rocks are similar to ancient terrestrial rocks called basaltic komatiites. Rocks of the plains also resemble the basaltic shergottites, meteorites which came from Mars. One classification system compares the amount of alkali elements to the amount of silica on a graph; in this system, Gusev plains rocks lie near the junction of basalt, picrobasalt, and tephite. The Irvine-Barager classification calls them basalts.\nPlain's rocks have been very slightly altered, probably by thin films of water because they are softer and contain veins of light colored material that may be bromine compounds, as well as coatings or rinds. It is thought that small amounts of water may have gotten into cracks inducing mineralization processes).\nCoatings on the rocks may have occurred when rocks were buried and interacted with thin films of water and dust.\nOne sign that they were altered was that it was easier to grind these rocks compared to the same types of rocks found on Earth.\n\nThe first rock that Spirit studied was Adirondack. It turned out to be typical of the other rocks on the plains.\n\nScientists found a variety of rock types in the Columbia Hills, and they placed them into six different categories. The six are: Adirondack, Clovis, Wishstone, Peace, Watchtower, Backstay, and Independence. They are named after a prominent rock in each group. Their chemical compositions, as measured by APXS, are significantly different from each other. Most importantly, all of the rocks in Columbia Hills show various degrees of alteration due to aqueous fluids.\nThey are enriched in the elements phosphorus, sulfur, chlorine, and bromine—all of which can be carried around in water solutions. The Columbia Hills' rocks contain basaltic glass, along with varying amounts of olivine and sulfates.\nThe olivine abundance varies inversely with the amount of sulfates. This is exactly what is expected because water destroys olivine but helps to produce sulfates.\n\nThe Clovis group is especially interesting because the Mossbauer spectrometer (MB) detected goethite in it. Goethite forms only in the presence of water, so its discovery is the first direct evidence of past water in the Columbia Hills's rocks. In addition, the MB spectra of rocks and outcrops displayed a strong decline in olivine presence,\nalthough the rocks probably once contained much olivine. Olivine is a marker for the lack of water because it easily decomposes in the presence of water. Sulfate was found, and it needs water to form.\nWishstone contained a great deal of plagioclase, some olivine, and anhydrate (a sulfate). Peace rocks showed sulfur and strong evidence for bound water, so hydrated sulfates are suspected. Watchtower class rocks lack olivine consequently they may have been altered by water. The Independence class showed some signs of clay (perhaps montmorillonite a member of the smectite group). Clays require fairly long term exposure to water to form.\nOne type of soil, called Paso Robles, from the Columbia Hills, may be an evaporate deposit because it contains large amounts of sulfur, phosphorus, calcium, and iron.\nAlso, MB found that much of the iron in Paso Robles soil was of the oxidized, Fe form, which would happen if water had been present.\n\nTowards the middle of the six-year mission (a mission that was supposed to last only 90 days), large amounts of pure silica were found in the soil. The silica could have come from the interaction of soil with acid vapors produced by volcanic activity in the presence of water or from water in a hot spring environment.\n\nAfter Spirit stopped working scientists studied old data from the Miniature Thermal Emission Spectrometer, or Mini-TES and confirmed the presence of large amounts of carbonate-rich rocks, which means that regions of the planet may have once harbored water. The carbonates were discovered in an outcrop of rocks called \"Comanche.\"\n\nIn summary, Spirit found evidence of slight weathering on the plains of Gusev, but no evidence that a lake was there. However, in the Columbia Hills there was clear evidence for a moderate amount of aqueous weathering. The evidence included sulfates and the minerals goethite and carbonates which only form in the presence of water. It is believed that Gusev crater may have held a lake long ago, but it has since been covered by igneous materials. All the dust contains a magnetic component which was identified as magnetite with some titanium. Furthermore, the thin coating of dust that covers everything on Mars is the same in all parts of Mars.\n\nOpportunity Rover found that the soil at Meridiani Planum was very similar to the soil at Gusev crater and Ares Vallis; however in many places at Meridiani the soil was covered with round, hard, gray spherules that were named \"blueberries.\" These blueberries were found to be composed almost entirely of the mineral hematite. It was decided that the spectra signal spotted from orbit by Mars Odyssey was produced by these spherules. After further study it was decided that the blueberries were concretions formed in the ground by water. Over time, these concretions weathered from what was overlying rock, and then became concentrated on the surface as a lag deposit. The concentration of spherules in bedrock could have produced the observed blueberry covering from the weathering of as little as one meter of rock. Most of the soil consisted of olivine basalt sands that did not come from the local rocks. The sand may have been transported from somewhere else.\n\nA Mossbauer spectrum was made of the dust that gathered on Opportunity's capture magnet. The results suggested that the magnetic component of the dust was titanomagnetite, rather than just plain magnetite, as was once thought. A small amount of olivine was also detected which was interpreted as indicating a long arid period on the planet. On the other hand, a small amount of hematite that was present meant that there may have been liquid water for a short time in the early history of the planet.\nBecause the Rock Abrasion Tool (RAT) found it easy to grind into the bedrocks, it is thought that the rocks are much softer than the rocks at Gusev crater.\n\nFew rocks were visible on the surface where Opportunity landed, but bedrock that was exposed in craters was examined by the suite of instruments on the Rover. Bedrock rocks were found to be sedimentary rocks with a high concentration of sulfur in the form of calcium and magnesium sulfates. Some of the sulfates that may be present in bedrocks are kieserite, sulfate anhydrate, bassanite, hexahydrite, epsomite, and gypsum. Salts, such as halite, bischofite, antarcticite, bloedite, vanthoffite, or gluberite may also be present.\nThe rocks contained the sulfates had a light tone compared to isolated rocks and rocks examined by landers/rovers at other locations on Mars. The spectra of these light toned rocks, containing hydrated sulfates, were similar to spectra taken by the Thermal Emission Spectrometer on board the Mars Global Surveyor. The same spectrum is found over a large area, so it is believed that water once appeared over a wide region, not just in the area explored by Opportunity Rover.\n\nThe Alpha Particle X-ray Spectrometer (APXS) found rather high levels of phosphorus in the rocks. Similar high levels were found by other rovers at Ares Vallis and Gusev Crater, so it has been hypothesized that the mantle of Mars may be phosphorus-rich. The minerals in the rocks could have originated by acid weathering of basalt. Because the solubility of phosphorus is related to the solubility of uranium, thorium, and rare earth elements, they are all also expected to be enriched in rocks.\n\nWhen Opportunity rover traveled to the rim of Endeavour crater, it soon found a white vein that was later identified as being pure gypsum. It was formed when water carrying gypsum in solution deposited the mineral in a crack in the rock. A picture of this vein, called \"Homestake\" formation, is shown below.\n\n \nExamination in 2004 of Meridiani rocks, showed the first strong \"in situ\" evidence for past water by detecting the mineral jarosite, which only forms in water. This discovery proved that water once existed in Meridiani Planum. In addition, some rocks showed small laminations (layers) with shapes that are only made by gently flowing water. The first such laminations were found in a rock called \"The Dells.\" Geologists would say that the cross-stratification showed festoon geometry from transport in subaqueous ripples. A picture of cross-stratification, also called cross-bedding, is shown on the left.\n\nBox-shaped holes in some rocks were caused by sulfates forming large crystals, and then when the crystals later dissolved, holes, called vugs, were left behind. The concentration of the element bromine in rocks was highly variable probably because it is very soluble. Water may have concentrated it in places before it evaporated. Another mechanism for concentrating highly soluble bromine compounds is frost deposition at night that would form very thin films of water that would concentrate bromine in certain spots.\n\nOne rock, \"Bounce Rock,\" found sitting on the sandy plains was found to be ejecta from an impact crater. Its chemistry was different from the bedrocks. Containing mostly pyroxene and plagioclase and no olivine, it closely resembled a part, Lithology B, of the shergottite meteorite EETA 79001, a meteorite known to have come from Mars. Bounce rock received its name by being near an airbag bounce mark.\n\nOpportunity Rover found meteorites just sitting on the plains. The first one analyzed with Opportunity's instruments was called \"Heatshield Rock,\" as it was found near where Opportunity's heatshield landed. Examination with the Miniature Thermal Emission Spectrometer (Mini-TES), Mossbauer spectrometer, and APXS lead researchers to, classify it as an IAB meteorite. The APXS determined it was composed of 93% iron and 7% nickel. The cobble named \"Fig Tree Barberton\" is thought to be a stony or stony-iron meteorite (mesosiderite silicate), while \"Allan Hills,\" and \"Zhong Shan\" may be iron meteorites.\n\nObservations at the site have led scientists to believe that the area was flooded with water a number of times and was subjected to evaporation and desiccation. In the process sulfates were deposited. After sulfates cemented the sediments, hematite concretions grew by precipitation from groundwater. Some sulfates formed into large crystals which later dissolved to leave vugs. Several lines of evidence point toward an arid climate in the past billion years or so, but a climate supporting water, at least for a time, in the distant past.\n\nThe Curiosity rover encountered rocks of special interest on the surface of Aeolis Palus near Aeolis Mons (\"Mount Sharp\") in Gale Crater. In the autumn of 2012, rocks studied, on the way from Bradbury Landing to Glenelg Intrique, included \"Coronation\" rock (August 19, 2012), \"Jake Matijevic\" rock (September 19, 2012), \"Bathurst Inlet\" rock (September 30, 2012).\n\nOn September 27, 2012, NASA scientists announced that the \"Curiosity rover\" found evidence for an ancient streambed suggesting a \"vigorous flow\" of water on Mars. \nOn December 3, 2012, NASA reported that \"Curiosity\" performed its first extensive soil analysis, revealing the presence of water molecules, sulfur and chlorine in the Martian soil. On December 9, 2013, NASA reported that, based on evidence from \"Curiosity\" rover studying Aeolis Palus, Gale Crater contained an ancient freshwater lake which could have been a hospitable environment for microbial life.\n\nIn March 2013, NASA reported \"Curiosity\" found evidence that geochemical conditions in Gale Crater were once suitable for microbial life after analyzing the first drilled sample of Martian rock, \"John Klein\" rock at \"Yellowknife Bay\" in Gale Crater. The rover detected water, carbon dioxide, oxygen, sulfur dioxide and hydrogen sulfide. Chloromethane and dichloromethane were also detected. Related tests found results consistent with the presence of smectite clay minerals.\n\nOn 16 December 2014, NASA reported the \"Curiosity\" rover detected a \"tenfold spike\", likely localized, in the amount of methane in the Martian atmosphere. Sample measurements taken \"a dozen times over 20 months\" showed increases in late 2013 and early 2014, averaging \"7 parts of methane per billion in the atmosphere.\" Before and after that, readings averaged around one-tenth that level.\n\nIn addition, high levels of organic chemicals, particularly chlorobenzene, were detected in powder drilled from one of the rocks, named \"Cumberland\", analyzed by the Curiosity rover.\n\n"}
{"id": "11530049", "url": "https://en.wikipedia.org/wiki?curid=11530049", "title": "Dew point depression", "text": "Dew point depression\n\nThe dew point depression (T-Td) is the difference between the temperature and dew point temperature at a certain height in the atmosphere.\n\nFor a constant temperature, the smaller the difference, the more moisture there is, and the higher the relative humidity. In the lower troposphere, more moisture (small dew point depression) results in lower cloud bases and lifted condensation levels (LCL). LCL height is an important factor modulating severe thunderstorms. One example concerns tornadogenesis, with tornadoes most likely if the dew point depression is 20 °F (11 °C) or less, and the likelihood of large, intense tornadoes increasing as dew point depression decreases. LCL height also factors in downburst and microburst activity. Conversely, instability is increased when there is a mid-level dry layer (large dew point depression) known as a \"dry punch\", which is favorable for convection if the lower layer is buoyant.\n\nAs it measures moisture content in the atmosphere, the dew point depression is also an important indicator in agricultural and forest meteorology, particularly in predicting wildfires.\n\n"}
{"id": "2026812", "url": "https://en.wikipedia.org/wiki?curid=2026812", "title": "EISCAT", "text": "EISCAT\n\nEISCAT (European Incoherent Scatter Scientific Association) operates three incoherent scatter radar systems, at 224 MHz, 931 MHz in Northern Scandinavia and one at 500 MHz on Svalbard, used to study the interaction between the Sun and the Earth as revealed by disturbances in the ionosphere and magnetosphere. At the Ramfjordmoen facility (near Tromsø, Norway), it also operates an ionospheric heater facility, similar to HAARP. Additional receiver stations are located in Sodankylä, Finland, and Kiruna, Sweden. The EISCAT Svalbard radar (ESR) is located in Longyearbyen, Norway. The EISCAT Headquarters are also located in Kiruna.\n\nEISCAT is funded and operated by research institutes and research councils of Norway, Sweden, Finland, Japan, China and the United Kingdom (the EISCAT Associates). Institutes in other countries also contribute to operations, including Russia, Ukraine, Germany and South Korea.\n\nThe system was also tested for space debris tracking and the radars were proven to be capable of statistical observations of Low-Earth orbit (LEO) debris (altitudes of 500 to 1500 km) down to 2 cm in size. Since these measurements are insufficient to determine complete orbits, the radar has only limited space surveillance value. Because the space debris tracking change is only a dedicated back-end computer system, the primary EISCAT observations are not compromised. As a result of that, the EISCAT radars allow continuous monitoring of the LEO debris in a beam park mode, functioning as a space surveillance system part of the European Space Agency's Space Situational Awareness Programme (SSA).\n\nIn 1973, the EISCAT proposal — which was originally planned for France, Germany and the three Nordic countries — seemed moribund. Then, Welsh physicist Granville Beynon became involved and by 1975, the agreement was signed, with the UK as a member. The proposal for UK membership had originally been turned down by the appropriate SRC committee. Beynon, however, persuaded the Board to reverse the decision of the committee and as a result of his efforts, hundreds of European scientists have had the opportunity to use the world's most advanced ionospheric radar.\n\nIn 2008, Doritos embarked upon an \"out-of-this-world\" advertising campaign, literally beaming a 30-second advertisement for Doritos brand tortilla chips into a solar system 42 light years away. This project is in collaboration with EISCAT Space Centre in Svalbard. The \"You Make It, We'll Play It\" contest chose the winning advertisement that was transmitted on June 12, 2008. The ad was beamed towards a distant star, within the Ursa Major constellation which is orbited by planets which may harbor life.\n\nEISCAT operates several facilities north of the Scandinavian arctic circle.\n\nThe Tromsø VHF transmitter, together with the Kiruna and Sodankylä VHF receivers, form a multistatic radar system.\n\nAdditionally, the Kilpisjärvi Atmospheric Imaging Receiver Array (KAIRA), near Kilpisjärvi, Finland can also serve as a VHF receiver in conjunction with the Tromsø transmitter.\n\nEISCAT is planning on building a next generation radar capable of providing 3D monitoring of the atmosphere and ionosphere. The new system is called EISCAT_3D.\n\nIn 2008, the European Strategy Forum on Research Infrastructures selected EISCAT_3D for its \"Roadmap 2008 for Large-Scale European Research Infrastructures for the next 20–30 years.\"\n\nEISCAT_3D will be a multistatic radar composed of five phased-array antenna fields. each field will have around 10,000 crossed dipole antenna elements. All five sites will act as receivers, with a single core site transmitting at 233 MHz (VHF band). The sites will be spread over Finland, Norway and Sweden. Each site will have a central array, surrounded by a set of smaller arrays, providing a high spatial resolution via aperture synthesis.\n\nDuring the summer of 2017, EISCAT will build a 91-element subarray at the site at Ramfjordmoen for hardware testing purposes and the full system is expected to be operational around 2021.\nThe KAIRA system is also a pathfinder for the development of EISCAT_3D.\n\n"}
{"id": "52434172", "url": "https://en.wikipedia.org/wiki?curid=52434172", "title": "Electricity sector in Armenia", "text": "Electricity sector in Armenia\n\nThe electricity sector of Armenia includes several companies engaged in electricity generation and distribution. Generation is carried out by multiple companies both state-owned and private. Distribution is controlled by Electric Networks of Armenia (ENA), High Voltage Electrical Networks (HVEN CJSC), and Electro Power System Operator. There are over 36,000 km of distribution lines across Armenia.\n\nAs of 2016, the majority of the electricity sector is privatized and foreign-owned (by both Russian and American companies), which is the result of a law passed in 1998 that allowed for the privatization of electricity generation and distribution in the country. Administration, government legislation, and policy of the sector is conducted by the Ministry of Energy Infrastructures and Natural Resources of the Republic of Armenia. Regulation of the sector is performed by the Public Services Regulatory Commission of the Republic of Armenia.\n\nArmenia does not have any fossil-fuel reserves, so it relies on gas imports from Russia and Iran, and nuclear fuel imports from Russia, which, together, result in approximately 66% of electricity production. Armenia is a net-producer of electricity and has exported in excess of 1.3 billion kWh per year since 2014 to Iran, Georgia, and Artsakh.\n\nLarge investments have been made in the electricity sector in Armenia in 2000's. These include the construction of the $247M combined-cycle Yerevan Thermal Power Plant completed in 2010, a $52M loan from the World Bank in 2015 to improve the reliability of electricity distribution across Armenia, and a $42M investment in 2016 by Electric Networks of Armenia to repair distribution networks. \n\nIn June 2016, the Armenian Parliament updated the law “On Energy Saving and Renewable Energy” which encourages the use of solar power in the country and allows users of solar installations of 150 kW or less to sell their excess energy back to the electrical grid.\n\nThe voltage in Armenia is 220 V AC at a frequency of 50 Hz. Armenia uses the European 2-pin C-socket and F-socket plugs.\n\nAccording to International Energy Agency in 2015 electricity generation in Armenia increased since 2009 to nearly 8000 GWh, but still remains below 1990 levels. Also, in 2015 Armenia consumed more than twice as much natural gas than in 2009.\n\nArmenia lacks fossil energy sources, and heavily relies on the production of electricity from a nuclear power plant and hydro power plants, and uses imported fossil fuels to operate thermal power plants. Solar energy and wind energy productions are just a small portion of the overall electricity production. \n\nOut of 3213.2 MW of installed capacity in Armenia, the largest portion of electricity generation comes from Metsamor Nuclear Power Plant at 38%, 33% from hydro power plants, 22% from thermal power plants, and the remaining 7% from other renewable sources. Similar figures are derived from reports published by Electric Networks of Armenia - during the period of 01.01.2012 - 30.06.2017 breakdown of aggregated electricity supply was: ANPP - 35.8%, all HPPs - 35.6%, all TPPs - 28.5%. \n\nThe base loaded capacity is provided by the Metsamor Nuclear Power Plant, while the daily load regulation is provided by both the Sevan-Hrazdan Cascade and the Vorotan cascade hydropower plants. The aforementioned power plants are the primary domestic production energy sources while thermal power plants depend on imported gas.\nNuclear power provides 38% of the electricity in Armenia through one operating nuclear reactor, Unit 2 of Metsamor Nuclear Power Plant, which is a WWER-440 reactor with extra seismic reinforcement. It was created in 1976 and is the only nuclear power plant in the South Caucasus. However, after the Spitak earthquake in 1988, the nuclear power plant's operation was forced to stop, becoming one of the causes of the Armenian energy crisis of 1990's. The second unit of the NPP was restarted in October 1995, putting an end to the 'dark and cold years'.\n\nNuclear fuel must be flown in from Russia and then taken along a dirt road from Yerevan because Armenia's border with Turkey is closed. While the Republic of Armenia is the sole owner of the plant, the Russian company United Energy Systems (UES) manages the Metsamor NPP.\n\nA modernization of NPP is scheduled for 2018, which will enhance the safety and increase installed capacity by 40-50 MW. \n\nArmenia also explores the possibilities of small modular reactor that would give flexibility and the opportunity to build a new nuclear power plant in a short time.\n\nEarlier it was reported that Armenia is looking for a new reactor with 1060 MW capacity in 2026.\n\nFrom the 1960s, the USSR carried out an electricity generation plan with the intention of constructing thermal power plants in the southern regions of Armenia where fossil fuel energy resources were restricted. Construction of thermal power plants started in the energy-intensive regions of Armenia. The first power plant was constructed in Yerevan in 1960, which was followed by Vanadzor Thermal Power Plant in 1961, and Hrazdan Thermal Power Plant in 1963.\n\nThe main source of energy for these power plants was natural gas which was delivered through pipelines running from Turkmenistan through Azerbaijan. Until the late 1980s, Armenia was heavily dependent on thermal and nuclear energy production. During the years of Nagorno-Karabakh War, the energy production of the thermal power plants was also stopped because of a border blockade, and Armenia had a severe energy crisis until the mid-1990s. However, Armenia managed to overcome this crisis. Although most of the technology of some of the thermal power plants is outdated as of December 2016, a lot of upgrades and maintenance have been undertaken on the power plants. The Vanadzor Thermal Power Plant is not operational as of December 2016.\n\nThere are two operational power plants as of December 2016: The Hrazdan Thermal Power Plant with an installed capacity of 1100 MW, and the Yerevan Thermal Power Plant with an installed capacity of 250 MW. The Hrazdan Thermal Power Plant, which is owned by the Russian Federation, produced 12.3% of the electricity produced in Armenia in 2014.\n\nRecent upgrades made to the Yerevan Thermal Power Plant have increased its efficiency to almost 70% by reducing the consumption of fuel, sulfuric acid, and caustic soda, and by reducing emission levels. For its power production, it uses natural gas supplied from Iran and exchanges it with the electricity produced by the plant, while using the surplus energy for domestic consumption.\n\nArmenia does not possess large fossil energy resources like coal, gas, or oil. However, according to a report by the Danish Management Group, Armenia has a large potential for renewable energy. \n\nHydro power plants provide 70 percent of Armenia’s renewable energy, and 33% of the country’s overall electricity production. Major HPP capacities are installed within Sevan-Hrazdan Cascade and Vorotan Cascade. The hydropower potential of Armenia is reported to be 21.8 billion kWh.\n\nAs of the 1st of January, 2018 electricity was generated by 184 small HPPs, with total installed capacity of 353 MW. In 2017 the generation of the electricity from small HPPs was around 862 million kW*h, which is about 11% of the total generated electricity in Armenia (7762 million kW*h). As of the 1st of January, 2018 and according to the provided licenses, 36 additional SHPPs are under construction, with about total projected 69 MW capacity and 250 million kW*h electricity annual supply. \n\nSolar energy potential in Armenia is 1000 MW according to researchers. The reason for this is that average solar radiation in Armenia is almost 1700 kWh/m annually. Currently, the solar technology is used only by some companies, and has no wide use among the citizens of Armenia. One of the well-known utilization examples is the American University of Armenia (AUA) which uses it not only for electricity generation, but also for water heating. The Government of Armenia is promoting utilization of solar energy.\n\nIn March 2018 an international consortium consisting of the Dutch and Spanish companies won the tender for the construction of a 55 MW solar power plant Masrik-1. The solar power station is planned to be built in the community of Mets Masrik of the Gegharkunik region entirely at the expense of foreign investments. The expected volume of investments in this generation facility will be about $ 50 million. Construction of the plant is expected to be completed by 2020.\n\nWind is a rare means of acquiring energy in Armenia, but it can be profitable. According to the research conducted by NREL in 2003, the wind energy potential in Armenia is close to 450 MW. Armenia's first wind power plant started operating in December 2005. It is a wind farm with a total capacity of 2.6 MW built in Puskin pass. This wind farm includes four 660 KW wind turbines. The most promising areas for wind power plants are Zod pass, Bazum Mountain, Jajur pass, the territory of Geghama Mountains, Sevan Pass, Aparan, the highlands between Sisian and Goris, and the region of Meghri.\n\nThe bioenergy sphere is gradually developing in Armenia. There are three rudimentary branches of bio energy: biofuel, biomass and biogas. Many scientists see the future of renewable energy of Armenia in bio energy.\n\nThe first is biofuel. As is accepted worldwide, the substantial sources of bio-ethanol are corn and sugarcane. Through these ingredients, bio ethanol is generated. Even in the case of blending it 50-50 with oil, the price will be cheaper than in ordinary cases. Thus, prices for transportation will decrease as well. The weather in Armenia is not appropriate for growing sugarcane, so the Jerusalem artichoke is considered to replace it. Moreover, its high concentration of carbohydrates makes it a better source for bio-ethanol production. Another type of cheap biofuel is created by compressing straw, sawdust, and the pods of sunflowers in a crusher into granules, which are then burned. It is feasible to receive 2 cubic meters (m) of gas from the burning of 1 kilogram (kg) of those granules. Scientists believe this will give Armenia the opportunity to provide heat for houses and to produce electricity, which would not be dependent on gas pipes or oil.\nThe second one is biomass. Scientists share the opinion that Armenia has the most energy-diverse market in the Caucasus. The reason for this is that, in addition to gas and electricity used for heating, people from many towns and villages use biomass, such as wood and manure. Thus biomass pellets have large prospective as they burn cleaner, hotter, and are more conventional.\nBiogas yielded from manure can be a good source for generating both heat and electricity. An example of this in Armenia is Lusakert Biogas Plant in Nor Geghi, Kotayk Marz. It was built in 2008, and is still working properly with a nominal capacity of 0.85 MW. After being built, the power plant won a National Energy Globe Award.\n\nAccording to Armstat total final consumption of electric energy in 2016 amounted to 458.2 ktoe and was broken down as presented on the graph to the right.\n\nIn 2014, Armenia consumed 5352 GWh of the total 7956 GWh of electricity production (7750 GWh domestic production and 206 GWh imports). This is approximately 67.3% of the total. The biggest consumer was the residential sector (1924 GWh, ~24.2%).\n\nWorld Bank data referring to International Energy Agency demonstrates that in per capita terms electricity consumption in Armenia remains below world average and in 2014 only matched 1992 figure.\n\nTransmission and distribution losses remain high in Armenia, even compared to Russia.\n\nElectricity supplier prices are determined by the Settlement Center of Ministry of Energy Infrastructures and Natural Resources of Armenia.\n\nSolar installations of 150 kW or less are allowed to sell their excess energy back to the electrical grid. \n\nIn February 2018 Armenian parliament adopted a set of amendments and additions to the Law on Energy and a number of related laws, designed to liberalize the national energy market, specify the functions of responsible government agencies and those of the regulator and protect the interests of consumers.\n\nIn the reports published by Electricity Networks of Armenia can be seen, that Yerevan Thermal Power Plant, which is modernized with a funding from Japan and European technologies, is much more energy-efficient than old Thermal Power Plant in Hrazdan and sells electricity to the grid at twice as lower price (15.5 AMD vs. 25 / 31 AMD) is not utilized to its full capacity. Rather, more electricity is acquired from less efficient TPPs in Hrazdan, owned by Gazprom and Tashir Group, and selling electricity at higher prices, which leads to overall higher prices and increased consumer prices. Here shall be noted that Electricity Networks of Armenia are also owned by Tashir Group.\n\nElectricity tariffs are dependent on the time of day (night/day), and the voltage supplied to the customer. Tariffs are determined by the Public Services Regulatory Commission of the Republic of Armenia while wholesale prices are determined by the Settlement Centre CJSC and submitted to Electric Networks of Armenia.\n\nThere were protests (Electric Yerevan) from June to September 2015 over a price increase for electricity, which was eventually increased by 6.93 Armenian dram per kilowatt-hour (AMD/kWh) (~US$0.015/kWh) to 39.78 AMD/kWh (~US$0.0830). From August 1, 2016, prices were decreased by 2.58 AMD/kWh (~US$0.0054) from 48.78 AMD/kWh (~US$0.1018) to 46.2 AMD/kWh (~US$0.0964).\n\nDepending on the amount of electricity consumed, the Government of Armenia subsidizes electricity bills of consumers who utilize less than 500 kWh of electricity per month. \n\nCustomers are billed monthly in kWh. Bills can be paid at physical locations such as Haypost (the Armenian post office), banks, and payment terminals, and electronically via mobile apps and SMS, and via the Internet.\n\nElectric Networks of Armenia (ENA) received a loan from the World Bank in 2016 to pay debts owed to electricity producing companies in Armenia, primarily the Metsamor Nuclear Power Plant and the Yerevan Thermal Power Plant.\n\nThere are numerous investment opportunities in the sector as Armenia has significant potential for electricity production from renewable energy sources such as hydropower, wind, solar, geothermal, and biogas. \n\nMetsamor nuclear power plant provides more that 40 percent of power in Armenia; however, it is aging and will need to be replaced soon. It has received lots of financing for modernizing its systems and safety features. Russia has extended a loan of $270 million and a $30 million grant for extending the lifetime of Metsamor NPP in 2015, which will be coming to an end in 2016. The funds are to be provided for 15 years with a 5-year grace period and an interest rate of annually 3%.\n\nPlans for building a new nuclear power plant have been discussed. In July 2014, the energy minister of Russian Federation announced that Russia is willing to provide 4.5 billion USD out of 5 billion USD needed for construction of a new nuclear power plant. In 2014, the construction of a new power plant was approved by the Armenian government, which was to be started in 2018.\n\nHydro power is the most widely used renewable energy source in Armenia since Soviet times. Armenian government has proposed construction of four large and 30 small hydroelectric plants with a combined 300 MW capacity. \n\nThese include:\n\n\nThe cost of the project is at least 500 million USD and is currently proposed for investment.\n\nArmenia also has a large solar energy potential. Compared with other countries, the average annual energy flow is higher; therefore, there is large interest in this energy sector. \n\nIn May 2018 deputy minister of energy infrastructure and natural resources mentioned that the electricity market liberalization process began and a local production of solar panels kicked off. He said that it is planned to make the solar energy share reach at least 10% in the energy sector by 2022. 314 solar power stations with up to 500 kW capacity are connected to the electricity network in Armenia, while 85 other solar power stations are in the stage of connection with a total capacity of 5.2 MW. 4 systemic solar stations are connected to the network, 7 are in the construction phase with completion planned within this year with the total capacity is 10 MW. Nearly 600 families are already using solar energy in non-gasified communities under state funded projects alone. \n\nIn July 2015, a 58 million USD investment project was launched, which was designed to help the renewable energy sector. This project included plans for solar power stations of 40-50 MW capacity.\n\nThe National Renewable Energy Laboratory of United States has determined the wind potential of Armenia to be about 450 MW. According to the same source, the main prospective places for building wind farms are Zod Pass, in Bazum Mountain, in Jajur Pass, in Gegham Mountains, in Sevan Pass, and in the highlands between Sisian and Goris. Monitoring in Qarahach pass was conducted by the Armenian-Italian private company “Ar Energy.” The company has a license to construct the “Qarahach 1” wind farm with an overall capacity of 20 MW, which will be expanded to 140 MW in the future. Additionally, there is a project for construction of a wind power plant at Simyonovka Pass in Sevan with a cumulative capacity of 34 MW. The project is proposed for investment.\n\nAs of 2018, the Ministry of Energy and Natural Resources of Armenia is considering the development of a geothermal plant on the Jermaghbyur and Karkar sites in a package solution with a single investor.\n\nAn $8.55M grant was awarded by the World Bank in 2015 for further exploration of geothermal resources in Armenia. \n\nReconnaissance drilling for Armenia’s first geothermal power plant in Jermaghbyur (Jermaghbyur Geothermal Power Plant) was conducted in 2016. The drilling works of the first wells with the depth of 1500m and the second well of 1682m have been completed. The total cost of the geothermal power plant construction project at Karkar site is expected to make about $100 million. Karkar geothermal power plant with a capacity of 30 MW will generate around 250 million kWh of electricity in a year.\n\nA high pressure (20-25 atmosphere pressure) hot water (up to 250 °C) considered to be available in depth of 2500-3000 meters in Jermaghbyur is a potential source of geothermal energy with a capacity of 25 MW. \n\nIn 2012, 1.82 million USD was invested by International Bank of Reconstruction and Development in an energy saving program. The program planned to upgrade the insulation of public buildings and heating systems, which included replacing traditional lamps with LEDs, and installing solar water heating panels. On 30 June 2016 the project’s grant component had been completed.\n\n\n"}
{"id": "26589068", "url": "https://en.wikipedia.org/wiki?curid=26589068", "title": "EnergyMap.dk", "text": "EnergyMap.dk\n\nEnergyMap.dk is the national Danish internet portal for energy and climate related solutions. \n\nEnergyMap provides information about Danish companies, organisations, institutions and public authorities, technologies and programmes that may help combat climate change and improve energy efficiency. Users may \"study the solutions, cases, projects and events presented on the portal. Many of the featured sites are open for visitors and many\" also offer R&D information or investment opportunities.\n\nEnergyMap is a part of Climate Consortium Denmark, a public-private partnership established in June 2008, by The Danish Parliament and these Danish business organizations. The purpose of Climate Consortium Denmark is to showcase products and technologies within Cleantech and Clean technology held by Danish business and industry, as well as the knowledge in climate- and energy- technologies held by Danish universities and research institutions.\n\nThe purpose of EnergyMap is to showcase, create awareness and attract attention to climate-friendly products and technologies of Danish companies and knowledge institutions.\n\nThe idea of EnergyMap was born in the high-tech network for renewable energy, VE-Net. VE-Net is supported by The Danish Agency for Science, Technology and Innovation and is run by the Danish Technological Institute in cooperation with The Danish Energy Industries Federation. The latter is responsible for the development and operation of the internet platform. Ownership of EnergyMap has been passed to the Climate Consortium.\n\n\n"}
{"id": "48982381", "url": "https://en.wikipedia.org/wiki?curid=48982381", "title": "Energy Identification Code", "text": "Energy Identification Code\n\nThe Energy Identification Code or EIC is a 16-character code used in Europe to uniquely identify entities and objects related to the electricity and gas sector.\n\nThe EIC code is used for:\n\nThe EIC codes are used -among others- in the ENTSO-E Transparency Platform and in the ARIS platform, both supporting EU regulations on transparency and integrity.\n\nThe scheme is supported by a central issuing office (“CIO”, function exercised by ENTSO-E for both the electricity and the gas sectors) and ENTSO-E-authorised local issuing offices (LIOs) in Europe.\n"}
{"id": "54325825", "url": "https://en.wikipedia.org/wiki?curid=54325825", "title": "Esso Research Centre", "text": "Esso Research Centre\n\nThe Esso Research Centre was a research centre in Oxfordshire.\n\nThe site was Esso's main European technical centre for fuels and lubricants. The site was extended in 1957.\n\nIt was situated on the western side of the A4130, around a half-mile south-west of the junction with the A34. The site had the staff of Esso Research, with around 500 scientists and engineers.\n\nIt conducted research into chemistry.\n"}
{"id": "28064311", "url": "https://en.wikipedia.org/wiki?curid=28064311", "title": "Exotic Zoology", "text": "Exotic Zoology\n\nExotic Zoology is a cryptozoological book by Willy Ley, a science writer and space advocate. The illustrator of the book is Olga Ley.\n\nLey had written a number of books containing scientific oddities; \"Exotic Zoology\" collects the cryptozoological matter from those books. Throughout the book he shows examples of organisms that were rumored to exist, or were thought to be impossible, that were shown to be real; and others that were accepted as fact, that were discovered to have never existed: \"He speculates about dragons and sea serpents, wingless birds and Abominable Snowmen.\" The book, in its description of (fictional) peoples and creatures, has been compared to John Mandeville's \"Travels\". Some of the claims have been criticized or ridiculed, for instance the statement that giant squids had left scars on whales of two feet in diameter.\n\n"}
{"id": "71478", "url": "https://en.wikipedia.org/wiki?curid=71478", "title": "Exotic matter", "text": "Exotic matter\n\nIn physics, exotic matter is matter that somehow deviates from normal matter and has \"exotic\" properties. A broader definition of exotic matter is any kind of non-baryonic matter—that is not made of baryons, the subatomic particles (such as protons and neutrons) of which ordinary matter is composed. Exotic mass has been considered a colloquial term for matters such as dark matter, negative mass, or complex mass.\n\nThere are several types of exotic matter:\n\n\nNegative mass would possess some strange properties, such as accelerating in the direction opposite of applied force. Despite being inconsistent with the expected behavior of \"normal\" matter, negative mass is mathematically consistent and introduces no violation of conservation of momentum or energy. It is used in certain speculative theories, such as on the construction of artificial wormholes and the Alcubierre drive. The closest known real representative of such exotic matter is the region of pseudo-negative-pressure density produced by the Casimir effect.\n\nAccording to mass–energy equivalence, mass formula_1 is in proportion to energy formula_2 and the coefficient of proportionality is formula_3. Actually, formula_4 is still equivalent to formula_2 although the coefficient is another constant such as formula_6. In this case, it is unnecessary to introduce a negative energy because the mass can be negative although the energy is positive. That is to say,\n\nUnder the circumstances，\n\nWhen formula_14,\n\nConsequently,\n\nwhere formula_18 is invariant mass and invariant energy equals formula_19. The squared mass is still positive and the particle can be stable.\n\nSince formula_20,\n\nThe negative momentum is applied to explain negative refraction, inverse Doppler effect and reverse Cherenkov effect observed in a negative index metamaterial. The radiation pressure in the metamaterial is also negative because the force is defined as formula_22. Negative pressure exists in dark energy too. Using these above equations, the energy-momentum relation should be\n\nSubstituting the Planck-Einstein relation formula_24 and de Broglie's formula_25, we obtain the following dispersion relation \n\nof the wave consists of a stream of particles whose energy-momentum relation is formula_23(wave–particle duality) can be excited in a negative index metamaterial.The velocity of such a particle is equal to\n\nand range is from zero to infinity\n\nMoreover, the kinetic energy is also negative\n\nIn fact, the negative kinetic energy exists in some models to describe dark energy (phantom energy) whose pressure is negative. In this way, the negative mass of exotic matter is now associated with negative momentum, negative pressure, negative kinetic energy and FTL (faster-than-light).\n\nA hypothetical particle with complex rest mass would always travel faster than the speed of light. Such particles are called tachyons. There is no confirmed existence of tachyons.\n\nIf the rest mass formula_4 is complex this implies that the denominator is complex because the total energy is observable and thus must be real. Therefore, the quantity under the square root must be negative, which can only happen if \"v\" is greater than \"c\". As noted by Gregory Benford \"et al.,\" special relativity implies that tachyons, if they existed, could be used to communicate backwards in time (see tachyonic antitelephone). Because time travel is considered to be non-physical, tachyons are believed by physicists either not to exist, or else to be incapable of interacting with normal matter.\n\nIn quantum field theory, complex mass would induce tachyon condensation.\n\nAt high pressure, materials such as sodium chloride (NaCl) in the presence of an excess of either chlorine or sodium were transformed into compounds \"forbidden\" by classical chemistry, such as and . Quantum mechanical calculations predict the possibility of other compounds, such as , and . The materials are thermodynamically stable at high pressures. Such compounds may exist in natural environments that exist at high pressure, such as the deep ocean or inside planetary cores. The materials have potentially useful properties. For instance, is a two-dimensional metal, made of layers of pure sodium and salt that can conduct electricity. The salt layers act as insulators while the sodium layers act as conductors.\n\n\n"}
{"id": "31015180", "url": "https://en.wikipedia.org/wiki?curid=31015180", "title": "F-1 (satellite)", "text": "F-1 (satellite)\n\nF-1 is a 1U CubeSat built by FSpace laboratory at FPT University in Vietnam, in partnership with Angstrom Space Technology Center (ASTC), Uppsala University and NanoRacks LLC. Its mission is to train young engineers and students about aerospace engineering and evaluate an advanced 3-axis magnetometer (SDTM) designed in Sweden by ASTC.\n\nF-1 was launched on 21 July 2012 and delivered to International Space Station (ISS) aboard Kounotori 3 along with the Raiko, We Wish, Niwaka and TechEdSat cubesats. Then, on October 4, 2012, it was deployed into orbit from ISS using JEM Small Satellite Orbital Deployer (J-SSOD) which was attached to the Kibo module's robotic arm.\nAs of November 2, 2012, F-1 failed to confirm communication after the orbital deployment.\n\n\n\n1. Backup UHF channel (only operational in daylight):\nPulse-Width-Modulation Morse code telemetry beacon\n\n2. Main VHF channel (operational during night time but may be turned on in daylight later)\n\nF-1’s KISS packet format\n\nNote:\n\n"}
{"id": "446976", "url": "https://en.wikipedia.org/wiki?curid=446976", "title": "GALEX", "text": "GALEX\n\nThe Galaxy Evolution Explorer (GALEX) is an orbiting ultraviolet space telescope launched on April 28, 2003, and operated until early 2012.\n\nAn airlaunched Pegasus rocket placed the craft into a nearly circular orbit at an altitude of and an inclination to the Earth's equator of 29 degrees.\n\nThe first observation was dedicated to the crew of the Space Shuttle Columbia, being images in the constellation Hercules taken on May 21, 2003. This region was selected because it had been directly overhead the shuttle at the time of its last contact with the NASA Mission Control Center.\n\nAfter its primary mission of 29 months, observation operations were extended to almost 9 years with NASA placing it into standby mode on 7 Feb 2012.\n\nNASA cut off financial support for operations of GALEX in early February 2011 as it was ranked lower than other projects which were seeking a limited supply of funding. The mission's life-cycle cost to NASA was $150.6 million. The California Institute of Technology negotiated to transfer control of GALEX and its associated ground control equipment to the California Institute of Technology in keeping with the Stevenson-Wydler Technology Innovation Act. Under this Act, excess research equipment owned by the US government can be transferred to educational institutions and non-profit organizations. In May 2012, GALEX operations were transferred to Caltech.\n\nOn June 28, 2013 NASA decommissioned GALEX. It is expected that the spacecraft will remain in orbit for at least 65 years before it will re-enter the atmosphere.\n\nDuring its initial 29-month mission, which was extended, it made observations in ultraviolet wavelengths to measure the history of star formation in the universe 80 percent of the way back to the Big Bang. Since scientists believe the Universe to be about 13.8 billion years old, the mission will study galaxies and stars across about 10 billion years of cosmic history.\n\nThe spacecraft's mission is to observe hundreds of thousands of galaxies, with the goal of determining the distance of each galaxy from Earth and the rate of star formation in each galaxy. Near- and far-UV emissions as measured by GALEX can indicate the presence of young stars, but may also originate from old stellar populations (e.g. sdB stars).\n\nPartnering with the NASA Jet Propulsion Laboratory (JPL) on the mission are the California Institute of Technology, Orbital Sciences Corporation, University of California, Berkeley, Yonsei University, Johns Hopkins University, Columbia University, and Laboratoire d'Astrophysique de Marseille, France.\n\nThe observatory participated in GOALS with Spitzer, Chandra, and Hubble. GOALS stands for \"Great Observatories All-sky LIRG Survey\", and Luminous Infrared Galaxies were studied at the multiple wavelengths allowed by the telescopes.\n\nThe telescope has a 50 cm diameter aperture primary, in a Richey-Chretien f/6 configuration. It can see light wavelengths from 135 nanometers to 280 nm, with a field of view of 1.2 degrees wide (larger than a full moon). It has gallium-arsenide solar cells which supply nearly 300 watts to the spacecraft.\n\n\n"}
{"id": "37965839", "url": "https://en.wikipedia.org/wiki?curid=37965839", "title": "Henry Neville Southern", "text": "Henry Neville Southern\n\nHenry Neville \"Mick\" Southern (28 September 1908 – 25 August 1986) was an English ornithologist.\n\nBorn in Boston, Lincolnshire, Southern was educated at Wyggeston Grammar School, Leicester where his interest in studying birds started. He went up to Queen’s College, Oxford in 1927. He studied first classics, supported by an open foundation scholarship, and then a second undergraduate degree in zoology, with a four-year gap spent working for the publishers Ward Lock.\n\nAfter graduating for the second time he joined the Bureau of Animal Population at Oxford as a research scientist investigating a new technique for studying rabbits, funded by a Browne Research Scholarship. During World War II, Southern transferred to work on the control of pests, in particular the house mouse, as part of work the Animal Population Bureau took on for the Agricultural Research Council. In 1946 the Department of Zoological Field Studies in Oxford was formed from the Animal Population Bureau and the Edward Grey Institute of Field Ornithology and Mick Southern was made a Senior Research Officer. In this post he conducted a long-term (15 year) population study of the predator-prey relationships between wood-mice and bank voles, and one of their predators, the Tawny Owl. He edited \"The Handbook of British Mammals\" (1964), the journal \"Bird Study\" (1954–60) and the \"Journal of Animal Ecology\" (1968–75). He was awarded a D.Sc. from Oxford in 1972.\n\n\n"}
{"id": "27666821", "url": "https://en.wikipedia.org/wiki?curid=27666821", "title": "ISO 361", "text": "ISO 361\n\nISO 361 \"Basic ionizing radiation symbol\" is an international standard that specifies the shape, proportions, application and restrictions on the use of the symbol.\n\nIt may be used to signify the actual or potential presence of ionizing radiation. It is not used for sound waves and other types of electromagnetic waves. The standard does not specify the radiation levels at which it is to be used. \n\n"}
{"id": "23590185", "url": "https://en.wikipedia.org/wiki?curid=23590185", "title": "Inertia negation", "text": "Inertia negation\n\nInertia negation is a hypothetical process causing physical objects with mass to act as if they were of lower mass or were massless. The effect is the opposite of adding ballast. No such process is known to exist in the real world: if current understanding of physics is correct, such a process would be impossible. There is currently no known material or technology that is able to eliminate or negate the effects of inertia that all objects with mass possess.\n\nAccording to Newton's first law, \"A body will continue in its state of rest or of uniform motion in a straight line, unless compelled to change that state by a net force.\" Inertia is the resistance against changes in the motion of an object. Objects within objects each possess their own inertia, and will collide with each other when the containing object is moved.\n\nA device that would be capable of inertia negation is described as being capable of reducing the inertia of both the larger containing object, and of all contained objects within, so as to make changes in motion easier, and to reduce or prevent damage due to internal collisions. The inertia is not absorbed or redirected but simply ceases to have a physical effect.\n\nAntimatter, while being the opposite of matter, has the same kind of inertia, with the forces oriented in the same direction, as normal matter. Thus, storing antimatter on board a vehicle made of matter would not achieve any kind of inertia negation.\n\nInertia negation is a commonplace technology in numerous science fiction series. It is used as an explanation as to why the crew of starships can withstand complex maneuvres or acceleration to FTL speeds.\n\nNotable appearances include the Star Trek franchise, where inertial dampers protect the crew from the dangers of sudden accelerations. Another example is in the movie Alien.\n\nIn the fictional Mass Effect universe dark energy fields are used ubiquitously to modify mass of objects, e.g. of weapon projectiles to allow use of compact mass accelerators in order to achieve higher muzzle velocity, or even negate the mass of entire spaceships in order to enable FTL travel.\n\nInertia negation is used to counter the effects of sudden acceleration that would impart structural stresses on star ships when suddenly accelerating to or decelerating with the impulse drive, and which would cause passengers to be thrown against walls and crushed by the inertial effects of the vehicle suddenly accelerating or slowing.\n\nSuch a device does not need to negate or alter inertia – a similar effect can be achieved by creating a gravitational field opposing the acceleration of the vessel. Such technology, while still nonexistent at the present time and considered unlikely to be achieved in the foreseeable future, is by far more realistic than manipulating inertial mass.\n\n\nMcCampbell, J. M. \"UFOlogy\", \"UFOlogy\", Celestial Arts, Berkeley, 1976. Retrieved on September 2016.\n"}
{"id": "29856860", "url": "https://en.wikipedia.org/wiki?curid=29856860", "title": "Karst spring", "text": "Karst spring\n\nA karst spring is a spring that is part of a karst system. That includes the underground drainage of a much larger area, which means that karst springs often have a very large discharge. Because of their often conical or bowl shape, such water sources are also known in German-speaking lands as a \"Topf\" (\"pot\") which is reflected in names such as Aachtopf (the source of the Radolfzeller Aach) or Blautopf (the source of the Blau river in Blaubeuren).\n\nKarst springs are usually the end of a cave system at the place where a river cave reaches the Earth's surface. Thus, it is often possible to enter the caves at a karst spring and explore them. Large karst springs are located in many parts of the world. The world's largest karst springs are believed to be in Papua New Guinea, with others located in Mediterranean countries including Bosnia, Turkey, Slovenia and Italy.\n\nAn estavelle or inversac is a ground orifice which, depending on weather conditions and season, can serve either as a sink or as a source of fresh water. It is a type of sinkhole.\n\nA Vauclusian spring is a spring that originates from a shaft or a cave system, with the water surging upwards under relatively high pressure. It is named after the Fontaine de Vaucluse in southern France.\n\nSubmarine karst springs, also known as , occur worldwide, and are most numerous in shallow waters of the Mediterranean Sea.\n\nThe main feature of karst springs results from the fact that water is rapidly transported by underground caverns. This means that there is minimal filtering of the water and little separation of different sediments. Groundwater emerges at the spring within a few days. Storms, snowmelt, and general seasonal changes in rainfall have a very noticeable and rapid effect on karst springs.\n\nMany karst springs dry up during the driest part of the year and are thus known as intermittent springs. Still others are dry most of the year round and only flow after heavy rain. Sources that only flow during wet years are often known in German as \"Hungerbrunnen\" (\"hunger springs\"), the reason being that folklore sees a connection between the flow rate of a spring and poor yield in a wet year. It is however more of a culturally related superstition. Scientific studies on various \"Hungerbrunnen\" have not confirmed such a relationship. An example is the \"Hungerbrunnen\" in the parish of Heuchlingen near Gerstetten.\n\nThe properties of karst springs make them unsuitable for the supply of drinking water. Their uneven flow rate does not support steady rates of consumption, especially in summer when there is lower discharge but higher demand. In addition, poor filtering and high hardness mean that the water quality is poor. For these reasons, karst springs are nowadays rarely used for drinking water.\n\nThe French Realist painter Gustave Courbet (1819-1877) painted a number of karst springs among many landscapes he depicted in the Jura region of eastern France.\n\n\n"}
{"id": "6115176", "url": "https://en.wikipedia.org/wiki?curid=6115176", "title": "Lacus Excellentiae", "text": "Lacus Excellentiae\n\nLacus Excellentiae (Latin for \"Lake of Excellence\") is a relatively small, irregular lunar mare in the southern latitudes of the Moon, amidst the rugged terrain to the south of the larger Mare Humorum. The most prominent feature within the diameter of this basin is the small crater Clausius.\n\nThe selenographic coordinates of this feature are and it lies within a diameter of 184 km. The name of this lunar lake is a relatively recent addition to lunar nomenclature, being officially approved in 1976 by the IAU General Assembly.\n\nThe \"Lacus Excellentiae\" was the impact site of the SMART-1 lunar orbiter. This probe impacted the lunar surface on September 3, 2006 and was observed by astronomers to determine the properties of the ejected materials.\n\n"}
{"id": "10082813", "url": "https://en.wikipedia.org/wiki?curid=10082813", "title": "Libertad 1", "text": "Libertad 1\n\nLibertad 1 () is a single CubeSat built by the Space Program of the Sergio Arboleda University in Colombia. It was launched aboard a Dnepr rocket on April 17, 2007 from the Baikonur Cosmodrome, Kazakhstan and became the first Colombian satellite to orbit the Earth. It used a telemetric payload to keep it in communication with the University. It was expected to have a 50-day lifespan, however news reports two years after it was launched stated the satellite was still working and sending information, passing over Colombia twice a day.\n\n\n\n"}
{"id": "13688338", "url": "https://en.wikipedia.org/wiki?curid=13688338", "title": "List of Florida hurricanes (2000–present)", "text": "List of Florida hurricanes (2000–present)\n\nThe list of Florida hurricanes from 2000 to the present has been marked by several devastating North Atlantic hurricanes; as of 2017, 79 tropical or subtropical cyclones have affected the U.S. state of Florida. Collectively, cyclones in Florida during the time period resulted in more than $123 billion in damage (2017 USD). Additionally, tropical cyclones in Florida were responsible for 145 direct fatalities and at least 92 indirect casualties during the period. Eight cyclones affected the state in both 2004 and 2005, which were the years with the most tropical cyclones affecting the state. Every year included at least one tropical cyclone affecting the state. During the 2004 season, more than one out of every five houses in the state received damage. After Wilma in 2005, it would be 11 years until another hurricane would strike the state, and 12 years until another major hurricane would strike the state.\n\nThe strongest hurricane to hit the state during the time period was Hurricane Michael, which was the strongest hurricane to strike the contiguous United States since Hurricane Katrina. Additionally, hurricanes Charley, Ivan, Jeanne, Dennis, Wilma, Irma, and Michael made landfall on the state as major hurricanes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe following is a list of hurricanes with known deaths in the state.\n\n"}
{"id": "3069716", "url": "https://en.wikipedia.org/wiki?curid=3069716", "title": "List of Lepidoptera that feed on ash trees", "text": "List of Lepidoptera that feed on ash trees\n\nAshes (\"Fraxinus\" species) are used as food plants by the larvae (caterpillars) of a number of Lepidoptera species, including the following. (Unless otherwise stated, records are from the Nearctic region.)\n\n"}
{"id": "1469938", "url": "https://en.wikipedia.org/wiki?curid=1469938", "title": "List of Rosa species", "text": "List of Rosa species\n\nThere is significant disagreement over the number of true rose species. Some species are so similar that they could easily be considered variations of a single species, while other species show enough variation that they could easily be considered to be different species. Lists of rose species usually show more than 360.\n\nThe genus \"Rosa\" is subdivided into four subgenera:\n\n\n\n"}
{"id": "21159208", "url": "https://en.wikipedia.org/wiki?curid=21159208", "title": "List of authors of Climate Change 2007: The Physical Science Basis", "text": "List of authors of Climate Change 2007: The Physical Science Basis\n\nThis is a list of the 620 authors contributing to \"\", which was the 996 page contribution of to the IPCC Fourth Assessment Report. Their report describes the causes and climate consequences of global warming. \n\nThis list is limited to people acknowledged as authors or editors on the report. The additional several hundred reviewers acknowledged in Annex III of the report are not included on this list. Author affiliations and nationalities are summarized from the list in Annex II of the report.\n\nEach author may have contributed to one or more sections and had one or more roles during the writing process. In the table below abbreviations are used to denote the roles and sections:\n\nNote:Not all scientists listed are climatologists \n\n"}
{"id": "37581344", "url": "https://en.wikipedia.org/wiki?curid=37581344", "title": "List of ecoregions in Eswatini", "text": "List of ecoregions in Eswatini\n\nThe following is a list of ecoregions in Swaziland, as identified by the Worldwide Fund for Nature (WWF).\n\n\"by major habitat type\"\n\n\n\n\"by bioregion\"\n\n\n\n"}
{"id": "25741786", "url": "https://en.wikipedia.org/wiki?curid=25741786", "title": "List of gardens in Italy", "text": "List of gardens in Italy\n\nThis article encompasses most of the gardens found in Italy.\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "32455359", "url": "https://en.wikipedia.org/wiki?curid=32455359", "title": "List of lunar features", "text": "List of lunar features\n\nSeveral features cover the surface of the Moon. These are listed below.\n\nAmong many lunar features are several large valleys that have been given names.\nMost of these valleys are named after a nearby crater; see the list of craters on the Moon for more information.\n\nThis is a list of \"maria\" (singular \"mare\") on the Moon. Lunar Maria are the large, dark, regions of the moon. They do not actually contain any water, however they are believed to have been formed from molten rock from the moon's mantle coming out onto the surface of the moon. It also includes the one \"oceanus\" and the features known by the names \"lacus\", \"palus\" and \"sinus\". The modern system of lunar nomenclature was introduced in 1651 by Riccioli. Riccioli's map of the moon was drawn by Francesco Maria Grimaldi, who has a crater named after him.\n\nThere was also a region on the Lunar farside that was briefly misidentified as a mare and named \"Mare Desiderii\" (Sea of Dreams). It is no longer recognized. Other former maria include:\n\n\nA related set of features are the Lunar \"lacus\" (singular \"lacus\", Latin for \"lake\"), which are smaller basaltic plains of similar origin:\n\nA related set of features are the \"sinus\" (singular \"sinus\", Latin for \"bay\") and \"paludes\" (singular \"palus\", Latin for \"marsh\"):\n\nSome sources also list a \"Palus Nebularum\" (\"Marsh of Mists\") at 38.0° N, 1.0° E. However the designation for this feature has not been officially recognized by the IAU.\n\nThese are isolated mountains or massifs.\n\nNote that the heights listed below are not consistent across sources. In the 1960s, the US Army Mapping Service used elevation relative to 1,737,988 meters from the center of the Moon. In the 1970s, the US Defense Mapping Agency used 1,730,000 meters. The Clementine topographic data published in the 1990s uses 1,737,400 meters.\n\nAlso note that this table is not comprehensive, and does not list the highest places on the Moon. Clementine data show a range of about 18,100 meters from lowest to highest point on the Moon. The highest point, located on the far side of the Moon, is approximately 6500 meters higher than Mons Huygens (usually listed as the tallest mountain).\n\nThe Moon's surface is covered in many interesting geological features. In addition to mountains, valleys, and craters, the following surface features have received names in the Lunar nomenclature. Many of these features are named after a nearby crater or mountain.\n\nThe listed diameter for these features is the longest dimension that contains the entire geological formation. The latitude and longitude are in selenographic coordinates.\n\nThese features are notable for their high albedo compared to the surrounding terrain.\n\n\"Note:\" On the far side of the Moon there are unnamed albedo features on Mare Ingenii and Mare Marginis. These are located antipodal to the Mare Imbrium and Mare Orientale impact basins.\n\nA \"catena\" is a chain of craters.\nThese features are wrinkle-ridge systems commonly found on lunar maria.\nThese features are wrinkle ridges commonly found on lunar mare\nThese features form a cape or headland on a mare.\nThese features are lunar rilles.\nThese features are lunar rille systems.\nThese are escarpments in the surface.\n\nThe continental areas between the seas were given comparable names by Giovanni Battista Riccioli, but were opposite the names used for the seas. Thus there were the lands of sterility (Terra Sterilitatis), heat (Terra Caloris), and liveliness (Terra Vitae). However these names for the highland regions are no longer used on recent maps, and \"Terrae\" are not officially recognized as standard lunar nomenclature by the International Astronomical Union.\n\nThe large majority of these features are impact craters. The crater nomenclature is governed by the International Astronomical Union, and this listing only includes features that are officially recognized by that scientific society.\n\nThe lunar craters are listed in the following subsections. Where a formation has associated satellite craters, these are detailed on the main crater description pages.\n\n\nThese were used for references in the Water Features section.\n\nThe following sources were used as references on the individual crater pages.\n\n"}
{"id": "6513985", "url": "https://en.wikipedia.org/wiki?curid=6513985", "title": "Mass in general relativity", "text": "Mass in general relativity\n\nThe concept of mass in general relativity (GR) is more complex than the concept of mass in special relativity. In fact, general relativity does not offer a single definition of the term mass, but offers several different definitions that are applicable under different circumstances. Under some circumstances, the mass of a system in general relativity may not even be defined.\n\nIn special relativity, the invariant mass or \"rest mass\" (\"hereafter simply \"mass\"\") of an isolated system can be defined in terms of the energy and momentum of the system by the relativistic energy–momentum equation:\n\nwhere \"E\" is the total energy of the system, \"p\" is the total momentum of the system and \"c\" is the speed of light. Concisely, in fundamental units where formula_2, the mass of a system in special relativity is the norm of its energy–momentum four vector; otherwise, it is formula_3.\n\nGeneralizing this definition to general relativity, however, is problematic; in fact, it turns out to be impossible to find a general definition for a system's total mass (or energy). The main reason for this is that \"gravitational field energy\" is not a part of the energy–momentum tensor; instead, what might be identified as the contribution of the gravitational field to a total energy is part of the Einstein tensor on the other side of Einstein's equation (and, as such, a consequence of these equations' non-linearity). While in certain situations it is possible to rewrite the equations so that part of the \"gravitational energy\" now stands alongside the other source terms in the form of the stress–energy–momentum pseudotensor, this separation is not true for all observers, and there is no general definition for obtaining it.\n\nHow, then, does one define a concept as a system's total mass – which is easily defined in classical mechanics? As it turns out, at least for spacetimes which are asymptotically flat (roughly speaking, which represent some isolated gravitating system in otherwise empty and gravity-free infinite space), the ADM 3+1 split leads to a solution: as in the usual Hamiltonian formalism, the time direction used in that split has an associated energy, which can be integrated up to yield a global quantity known as the ADM mass (or, equivalently, ADM energy). Alternatively, there is a possibility to define mass for a spacetime that is stationary, in other words, one that has a time-like Killing vector field (which, as a generating field for time, is canonically conjugate to energy); the result is the so-called Komar mass Although defined in a totally different way, it can be shown to be equivalent to the ADM mass for stationary spacetimes. The Komar integral definition can also be generalized to non-stationary fields for which there is at least an asymptotic time translation symmetry; imposing a certain gauge condition, one can define the Bondi energy at null infinity. In a way, the ADM energy measures all of the energy contained in spacetime, while the Bondi energy excludes those parts carried off by gravitational waves to infinity. Great effort has been expended on proving positivity theorems for the masses just defined, not least because positivity, or at least the existence of a lower limit, has a bearing on the more fundamental question of boundedness from below: if there were no lower limit to the energy, then no isolated system would be absolutely stable; there would always be the possibility of a decay to a state of even lower total energy. Several kinds of proofs that both the ADM mass and the Bondi mass are indeed positive exist; in particular, this means that Minkowski space (for which both are zero) is indeed stable. While the focus here has been on energy, analogue definitions for global momentum exist; given a field of angular Killing vectors and following the Komar technique, one can also define global angular momentum.\n\nThe disadvantage of all the definitions mentioned so far is that they are defined only at (null or spatial) infinity; since the 1970s, physicists and mathematicians have worked on the more ambitious endeavor of defining suitable \"quasi-local\" quantities, such as the mass of an isolated system defined using only quantities defined within a finite region of space containing that system. However, while there is a variety of proposed definitions such as the Hawking energy, the Geroch energy or Penrose's quasi-local energy–momentum based on twistor methods, the field is still in flux. Eventually, the hope is to use a suitable defined quasi-local mass to give a more precise formulation of the hoop conjecture, prove the so-called Penrose inequality for black holes (relating the black hole's mass to the horizon area) and find a quasi-local version of the laws of black hole mechanics.\n\nA non-technical definition of a stationary spacetime is a spacetime where none of the metric coefficients formula_4 are functions of time. The Schwarzschild metric of a black hole and the Kerr metric of a rotating black hole are common examples of stationary spacetimes.\n\nBy definition, a stationary spacetime exhibits time translation symmetry. This is technically called a time-like Killing vector. Because the system has a time translation symmetry, Noether's theorem guarantees that it has a conserved energy. Because a stationary system also has a well defined rest frame in which its momentum can be considered to be zero, defining the energy of the system also defines its mass. In general relativity, this mass is called the Komar mass of the system. Komar mass can only be defined for stationary systems.\n\nKomar mass can also be defined by a flux integral. This is similar to the way that Gauss's law defines the charge enclosed by a surface as the normal electric force multiplied by the area. The flux integral used to define Komar mass is slightly different from that used to define the electric field, however - the normal force is not the actual force, but the \"force at infinity\". See the main article for more detail.\n\nOf the two definitions, the description of Komar mass in terms of a time translation symmetry provides the deepest insight.\n\nIf a system containing gravitational sources is surrounded by an infinite vacuum region, the geometry of the space-time will tend to approach the flat Minkowski geometry of special relativity at infinity. Such space-times are known as \"asymptotically flat\" space-times.\n\nFor systems in which space-time is asymptotically flat, the ADM and Bondi energy, momentum, and mass can be defined. In terms of Noether's theorem, the ADM energy, momentum, and mass are defined by the asymptotic symmetries at spatial infinity, and the Bondi energy, momentum, and mass are defined by the asymptotic symmetries at null infinity. Note that mass is computed as the length of the energy–momentum four vector, which can be thought of as the energy and momentum of the system \"at infinity\".\n\nIn the Newtonian limit, for quasi-static systems in nearly flat space-times, one can approximate the total energy of the system by adding together the non-gravitational components of the energy of the system and then subtracting the \"Newtonian\" gravitational binding energy.\n\nTranslating the above statement into the language of general relativity, we say that a system in nearly flat space-time has a total non-gravitational energy E and momentum P given by:\n\nWhen the components of the momentum vector of the system are zero, i.e. P = 0, the approximate mass of the system is just (E+E)/c, E being a negative number representing the Newtonian gravitational self-binding energy.\n\nHence when one assumes that the system is quasi-static, one assumes that there is no significant energy present in the form of \"gravitational waves\". When one assumes that the system is in \"nearly-flat\" space-time, one assumes that the metric coefficients are essentially Minkowskian within acceptable experimental error.\n\nIn 1918, David Hilbert wrote about the difficulty in assigning an energy to a \"field\" and \"the failure of the energy theorem\" in a correspondence with Klein. In this letter, Hilbert conjectured that this failure is a characteristic feature of the general theory, and that instead of \"proper energy theorems\" one had 'improper energy theorems'.\n\nThis conjecture was soon proved to be correct by one of Hilbert's close associates, Emmy Noether. Noether's theorem applies to any system which can be described by an action principle. Noether's theorem associates conserved energies with time-translation symmetries. When the time-translation symmetry is a finite parameter continuous group, such as the Poincaré group, Noether's theorem defines a scalar conserved energy for the system in question. However, when the symmetry is an infinite parameter continuous group, the existence of a conserved energy is not guaranteed. In a similar manner, Noether's theorem associates conserved momenta with space-translations, when the symmetry group of the translations is finite-dimensional. Because General Relativity is a diffeomorphism invariant theory, it has an infinite continuous group of symmetries rather than a finite-parameter group of symmetries, and hence has the wrong group structure to guarantee a conserved energy. Noether's theorem has been extremely influential in inspiring and unifying various ideas of mass, system energy, and system momentum in General Relativity.\n\nAs an example of the application of Noether's theorem is the example of stationary space-times and their associated Komar mass.(Komar 1959). While general space-times lack a finite-parameter time-translation symmetry, stationary space-times have such a symmetry, known as a Killing vector. Noether's theorem proves that such stationary space-times must have an associated conserved energy. This conserved energy defines a conserved mass, the Komar mass.\n\nADM mass was introduced (Arnowitt et al., 1960) from an initial-value formulation of general relativity. It was later reformulated in terms of the group of asymptotic symmetries at spatial infinity, the SPI group, by various authors. (Held, 1980). This reformulation did much to clarify the theory, including explaining why ADM momentum and ADM energy transforms as a 4-vector (Held, 1980). Note that the SPI group is actually infinite-dimensional. The existence of conserved quantities is because the SPI group of \"super-translations\" has a preferred 4-parameter subgroup of \"pure\" translations, which, by Noether's theorem, generates a conserved 4-parameter energy–momentum. The norm of this 4-parameter energy–momentum is the ADM mass.\n\nThe Bondi mass was introduced (Bondi, 1962) in a paper that studied the loss of mass of physical systems via gravitational radiation. The Bondi mass is also associated with a group of asymptotic symmetries, the BMS group at null infinity. Like the SPI group at spatial infinity, the BMS group at null infinity is infinite-dimensional, and it also has a preferred 4-parameter subgroup of \"pure\" translations.\n\nAnother approach to the problem of energy in General Relativity is the use of pseudotensors such as the Landau-Lifshitz pseudotensor.(Landau and Lifshitz, 1962). Pseudotensors are not gauge invariant - because of this, they only give consistent gauge-independent answers for the total energy when additional constraints (such as asymptotic flatness) are met. The gauge dependence of pseudotensors also prevents any gauge-independent definition of the local energy density, as every different gauge choice results in a different local energy density.\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "48603932", "url": "https://en.wikipedia.org/wiki?curid=48603932", "title": "Matt Guthmiller", "text": "Matt Guthmiller\n\nMatthew Lee Guthmiller (born November 29, 1994) is an American aviator, YouTuber, entrepreneur, professional speaker, and student at the Massachusetts Institute of Technology. At age 19, Matt became the youngest person to circumnavigate the globe by air, which he set in a bid to encourage others to pursue ambitious dreams and promote computer science education worldwide. Since then, the record has been succeeded by Australian pilot Lachlan Smart. Guthmiller also founded an early iPhone unlocking company, AnySIMiPhones, in 2007 at age 12. He is currently a senior at the Massachusetts Institute of Technology studying electrical engineering and computer science. His latest project is rumored to deal with quantitative finance.\n\nBorn and raised in Aberdeen, SD, Guthmiller took an interest in aviation from a young age. Growing up he gradually drifted more toward math and science, in particular computer science, which led him to pursue several business ventures.\n\nHe started his first company, AnySIMiPhones, at age 12, selling software to unlock the newly released iPhone, which at the time was only available in the U.S. and only on AT&T, so that it could be used with any (GSM) cell carrier worldwide. After being in business for only a few weeks, Guthmiller sold his company to a similar company, FreeIt4Less, and joined their team. Together they unlocked around 20,000 iPhones in dozens of countries.\n\nAs the unlocking market dried up after Apple announced the iPhone 3G and its availability in multiple markets in 2008, Guthmiller moved on to pursue his interest in finance. Guthmiller pursued several projects aimed at analyzing financial market data with supercomputers in order to uncover ways to predict the likelihood of future price movements. A tweet he posted earlier this year seems to indicate his latest company aims to bring that kind of work to everyday investors.\n\nAfter getting bored one weekend in the summer of 2011, Guthmiller realized he could obtain his pilot certificate in a few months when he turned 17. Having wanted to fly his entire life, he convinced his parents to allow him to do a \"$20, 20-minute intro flight\" at a local flight school. He was instantly hooked. Soloing just a couple weeks later, he earned his private pilot certificate on his 17th birthday before going on to pursue an instrument rating, glider rating, his commercial pilot certificate, and a seaplane rating.\n\nAfter reading a May 3, 2013, article about 20-year-old Californian Jack Wiegand, who was about to become the youngest person to fly solo around the world, Guthmiller decided he wanted to attempt the same record. After a year of planning, Guthmiller left Gillespie Field in El Cajon, California on May 31, 2014, and 44 days, 12 hours later landed back at Gillespie on July 14, 2014 to become the youngest person to ever circumnavigate the globe by aircraft at 19 years, 7 months, and 15 days old. Spending 180 hours in a small, single-engine 1981 Beechcraft Bonanza, he made 23 stops in 15 countries on 5 continents. The longest flight was 16.5 hours from Pago Pago, American Samoa to Hilo, Hawaii. In order to carry enough fuel for such long legs, Guthmiller had the rear four seats removed and extra fuel tanks installed and took off as much as 25% over the aircraft's maximum certificated takeoff weight (authorized by an FAA ferry permit). Guthmiller and the plane have since been featured at EAA Airventure Oshkosh and the National Air and Space Museum Udvar-Hazy Center, and he shares his story with audiences around the country in various speaking engagements.\n\n"}
{"id": "2172345", "url": "https://en.wikipedia.org/wiki?curid=2172345", "title": "Molybdite", "text": "Molybdite\n\nMolybdite is the naturally occurring mineral form of molybdenum trioxide MoO. It occurs as yellow to greenish needles and crystallizes in the orthorhombic crystal system.\n\nMolybdite was first described in 1854 for and occurrence in quartz veins in the Knöttel area of Krupka, Krušné Hory Mountains in the Ústí Region of Bohemia, Czech Republic. It occurs in vein cavities and as coatings in molybdenite ore veins and quartz topaz greisens. Associated minerals include molybdenite, betpakdalite and quartz. The similar mineral ferrimolybdite is often misidentified as molybdite.\n"}
{"id": "16460664", "url": "https://en.wikipedia.org/wiki?curid=16460664", "title": "New Energy and Industrial Technology Development Organization", "text": "New Energy and Industrial Technology Development Organization\n\nThe , or NEDO, is Japan's largest public management organization promoting research and development as well as deployment of industrial, energy and environmental technologies. In 2003, NEDO was reorganized as an Independent Administrative Institution. NEDO has approximately 1,000 personnel and domestic offices in Hokkaido, Kansai, and Kyushu and international offices in Washington D.C., Silicon Valley (California), Paris, Beijing, Bangkok, Jakarta and New Delhi. Its budget for fiscal year 2009 was 234,700,000,000 yen (approximately $2.6 billion), the vast majority of which was provided by Japan's Ministry of Economy, Trade and Industry (METI). Its head office is located just outside Tokyo in Kawasaki City, Kanagawa Prefecture.\n\nNEDO has a number of projects in the United States, notably a smart grid and alternative energy project with two research laboratories in New Mexico, Sandia National Laboratories and Los Alamos National Laboratory (LANL). In March 2010, NEDO and the laboratories signed agreements in which NEDO will fund a smart grid research facility at LANL and smart house demonstration projects in both Los Alamos and Albuquerque, New Mexico. NEDO is investing $30 million (U.S.) on the projects for a four-year period.\n\n\n\n\n"}
{"id": "22153", "url": "https://en.wikipedia.org/wiki?curid=22153", "title": "Nuclear power", "text": "Nuclear power\n\nNuclear power is the use of nuclear reactions that release nuclear energy to generate heat, which most frequently is then used in steam turbines to produce electricity in a nuclear power plant. \nNuclear power can be obtained from nuclear fission, nuclear decay and nuclear fusion. \nPresently, the vast majority of electricity from nuclear power is produced by nuclear fission of uranium and plutonium.\nNuclear decay processes are used in niche applications such as radioisotope thermoelectric generators.\nThe possibility of generating electricity from nuclear fusion is still at a research phase with no commercial applications.\nThis article mostly deals with nuclear fission power for electricity generation.\n\nNuclear power is one of the leading low carbon power generation methods of producing electricity.\nIn terms of total life-cycle greenhouse gas emissions per unit of energy generated, nuclear power has emission values comparable or lower than renewable energy.\nFrom the beginning of its commercialization in the 1970s, nuclear power prevented about 1.84 million air pollution-related deaths and the emission of about 64 billion tonnes of carbon dioxide equivalent that would have otherwise resulted from the burning of fossil fuels in thermal power stations.\n\nCivilian nuclear power supplied 2,488 terawatt hours (TWh) of electricity in 2017, equivalent to about 10% of global electricity generation.\nAs of April 2018, there are 449 civilian fission reactors in the world, with a combined electrical capacity of 394 gigawatt (GW). \nAdditionally, there are 58 reactors under construction and 154 reactors planned, with a combined capacity of 63 GW and 157 GW, respectively. Over 300 more reactors are proposed.\nMost of reactors under construction are of generation III reactor design, with the majority in Asia.\n\nThere is a social debate about nuclear power.\nProponents, such as the World Nuclear Association and Environmentalists for Nuclear Energy, contend that nuclear power is a safe, sustainable energy source that reduces carbon emissions.\nOpponents, such as Greenpeace and NIRS, contend that nuclear power poses many threats to people and the environment.\n\nSome accidents have occurred in nuclear power plants. \nThese include the Chernobyl disaster in the Soviet Union in 1986, the Fukushima Daiichi nuclear disaster in Japan in 2011, and the more contained Three Mile Island accident in the United States in 1979.\nThere have also been some nuclear submarine accidents.\nIn terms of lives lost per unit of energy generated, nuclear reactors have caused the lowest number of fatalities per unit of energy generated when compared to the other major energy producing methods.\nCoal, petroleum, natural gas and hydroelectricity each have caused a greater number of fatalities per unit of energy, due to air pollution and accidents.\n\nIn 1932 physicist Ernest Rutherford discovered that when lithium atoms were \"split\" by protons from a proton accelerator, immense amounts of energy were released in accordance with the principle of mass–energy equivalence. However, he and other nuclear physics pioneers Niels Bohr and Albert Einstein believed harnessing the power of the atom for practical purposes anytime in the near future was unlikely, with Rutherford labeling such expectations \"moonshine.\"\n\nThe same year, his doctoral student James Chadwick discovered the neutron, which was immediately recognized as a potential tool for nuclear experimentation because of its lack of an electric charge. Experimentation with bombardment of materials with neutrons led Frédéric and Irène Joliot-Curie to discover induced radioactivity in 1934, which allowed the creation of radium-like elements at much less the price of natural radium. Further work by Enrico Fermi in the 1930s focused on using slow neutrons to increase the effectiveness of induced radioactivity. Experiments bombarding uranium with neutrons led Fermi to believe he had created a new, transuranic element, which was dubbed hesperium.\n\nIn 1938, German chemists Otto Hahn and Fritz Strassmann, along with Austrian physicist Lise Meitner and Meitner's nephew, Otto Robert Frisch, conducted experiments with the products of neutron-bombarded uranium, as a means of further investigating Fermi's claims. \nThey determined that the relatively tiny neutron split the nucleus of the massive uranium atoms into two roughly equal pieces, contradicting Fermi. \nThis was an extremely surprising result: all other forms of nuclear decay involved only small changes to the mass of the nucleus, whereas this process—dubbed \"fission\" as a reference to biology—involved a complete rupture of the nucleus. \nNumerous scientists, including Leó Szilárd, who was one of the first, recognized that if fission reactions released additional neutrons, a self-sustaining nuclear chain reaction could result. Once this was experimentally confirmed and announced by Frédéric Joliot-Curie in 1939, scientists in many countries (including the United States, the United Kingdom, France, Germany, and the Soviet Union) petitioned their governments for support of nuclear fission research, just on the cusp of World War II, for the development of a nuclear weapon.\n\nIn the United States, where Fermi and Szilárd had both emigrated, the discovery of the nuclear chain reaction led to the creation of the first man-made reactor, known as Chicago Pile-1, which achieved criticality on December 2, 1942. \nThis work became part of the Manhattan Project, a massive secret U.S. government military project to make enriched uranium and by building large production reactors to produce (\"breed\") plutonium for use in the first nuclear weapons. \nThe United States would test an atom bomb in July 1945 with the Trinity test, and eventually two such weapons were used in the atomic bombings of Hiroshima and Nagasaki.\n\nIn August 1945, the first widely distributed account of nuclear energy, in the form of the pocketbook \"The Atomic Age\", discussed the peaceful future uses of nuclear energy and depicted a future where fossil fuels would go unused. \nNobel laureate Glenn Seaborg, who later chaired the Atomic Energy Commission, is quoted as saying \"there will be nuclear powered earth-to-moon shuttles, nuclear powered artificial hearts, plutonium heated swimming pools for SCUBA divers, and much more\".\n\nThe United Kingdom, Canada, and the USSR proceeded to research and develop nuclear industries over the course of the late 1940s and early 1950s. \nElectricity was generated for the first time by a nuclear reactor on December 20, 1951, at the EBR-I experimental station near Arco, Idaho, which initially produced about 100 kW. \nWork was also strongly researched in the United States on nuclear marine propulsion, with a test reactor being developed by 1953 (eventually, the USS Nautilus, the first nuclear-powered submarine, would launch in 1955). \nIn 1953, American President Dwight Eisenhower gave his \"Atoms for Peace\" speech at the United Nations, emphasizing the need to develop \"peaceful\" uses of nuclear power quickly. This was followed by the 1954 Amendments to the Atomic Energy Act which allowed rapid declassification of U.S. reactor technology and encouraged development by the private sector.\n\nOn June 27, 1954, the USSR's Obninsk Nuclear Power Plant became the world's first nuclear power plant to generate electricity for a power grid, and produced around 5 megawatts of electric power.\n\nLater in 1954, Lewis Strauss, then chairman of the United States Atomic Energy Commission (U.S. AEC, forerunner of the U.S. Nuclear Regulatory Commission and the United States Department of Energy) spoke of electricity in the future being \"too cheap to meter\". Strauss was very likely referring to hydrogen fusion —which was secretly being developed as part of Project Sherwood at the time—but Strauss's statement was interpreted as a promise of very cheap energy from nuclear fission. The U.S. AEC itself had issued far more realistic testimony regarding nuclear fission to the U.S. Congress only months before, projecting that \"costs can be brought down... [to]... about the same as the cost of electricity from conventional sources...\"\n\nIn 1955 the United Nations' \"First Geneva Conference\", then the world's largest gathering of scientists and engineers, met to explore the technology. In 1957 EURATOM was launched alongside the European Economic Community (the latter is now the European Union). The same year also saw the launch of the International Atomic Energy Agency (IAEA).\n\nThe world's first commercial nuclear power station, Calder Hall at Windscale, England, was opened in 1956 with an initial capacity of 50 MW (later 200 MW). The first commercial nuclear generator to become operational in the United States was the Shippingport Reactor (Pennsylvania, December 1957).\n\nOne of the first organizations to develop nuclear power was the U.S. Navy, for the purpose of propelling submarines and aircraft carriers. The first nuclear-powered submarine, , was put to sea in December 1954. As of 2016, the U.S. Navy submarine fleet is made up entirely of nuclear-powered vessels, with 75 submarines in service. Two U.S. nuclear submarines, and , have been lost at sea. As of 2016 the Russian Navy was estimated to have 61 nuclear submarines in service; eight Soviet and Russian nuclear submarines have been lost at sea. This includes the reactor accident in 1961 which resulted in 8 deaths and more than 30 other people were over-exposed to radiation. \nThe reactor accident in 1968 resulted in 9 fatalities and 83 other injuries. \nMoreover, sank twice, but was raised after each incident. Several serious nuclear and radiation accidents have involved nuclear submarine mishaps.\n\nThe U.S. Army also had a nuclear power program, beginning in 1954. The SM-1 Nuclear Power Plant, at Fort Belvoir, Virginia, was the first power reactor in the United States to supply electrical energy to a commercial grid (VEPCO), in April 1957, before Shippingport. The SL-1 was a U.S. Army experimental nuclear power reactor at the National Reactor Testing Station in eastern Idaho. It underwent a steam explosion and meltdown in January 1961, which killed its three operators. In the Soviet Union at the Mayak Production Association facility there were a number of accidents, including a 1957 explosion that contaminated a huge territory in the eastern Urals and caused numerous deaths and injuries. The Soviet government kept this accident secret for about 30 years. The event was eventually rated at 6 on the seven-level INES scale (third in severity only to the disasters at Chernobyl and Fukushima).\n\nInstalled nuclear capacity initially rose relatively quickly, rising from less than 1 gigawatt (GW) in 1960 to 100 GW in the late 1970s, and 300 GW in the late 1980s. Since the late 1980s worldwide capacity has risen much more slowly, reaching 366 GW in 2005. Between around 1970 and 1990, more than 50 GW of capacity was under construction (peaking at over 150 GW in the late 1970s and early 1980s) — in 2005, around 25 GW of new capacity was planned. More than two-thirds of all nuclear plants ordered after January 1970 were eventually cancelled. A total of 63 nuclear units were canceled in the United States between 1975 and 1980.\n\nDuring the 1970s and 1980s rising economic costs (related to extended construction times largely due to regulatory changes and pressure-group litigation) and falling fossil fuel prices made nuclear power plants then under construction less attractive. In the 1980s (U.S.) and 1990s (Europe), flat load growth and electricity liberalization also made the addition of large new baseload capacity unattractive.\n\nThe 1973 oil crisis had a significant effect on countries, such as France and Japan, which had relied more heavily on oil for electric generation (39% and 73% respectively) to invest in nuclear power.\n\nSome local opposition to nuclear power emerged in the early 1960s, and in the late 1960s some members of the scientific community began to express their concerns. These concerns related to nuclear accidents, nuclear proliferation, high cost of nuclear power plants, nuclear terrorism and radioactive waste disposal. In the early 1970s, there were large protests about a proposed nuclear power plant in Wyhl, Germany. The project was cancelled in 1975 and anti-nuclear success at Wyhl inspired opposition to nuclear power in other parts of Europe and North America. By the mid-1970s anti-nuclear activism had moved beyond local protests and politics to gain a wider appeal and influence, and nuclear power became an issue of major public protest. Although it lacked a single co-ordinating organization, and did not have uniform goals, the movement's efforts gained a great deal of attention. In some countries, the nuclear power conflict \"reached an intensity unprecedented in the history of technology controversies\". \nIn France, between 1975 and 1977, some 175,000 people protested against nuclear power in ten demonstrations. In West Germany, between February 1975 and April 1979, some 280,000 people were involved in seven demonstrations at nuclear sites. Several site occupations were also attempted. In the aftermath of the Three Mile Island accident in 1979, some 120,000 people attended a demonstration against nuclear power in Bonn. In May 1979, an estimated 70,000 people, including then governor of California Jerry Brown, attended a march and rally against nuclear power in Washington, D.C. Anti-nuclear power groups emerged in every country that has had a nuclear power programme.\n\nHealth and safety concerns, the 1979 accident at Three Mile Island, and the 1986 Chernobyl disaster played a part in stopping new plant construction in many countries, although the public policy organization, the Brookings Institution states that new nuclear units, at the time of publishing in 2006, had not been built in the United States because of soft demand for electricity, and cost overruns on nuclear plants due to regulatory issues and construction delays. By the end of the 1970s it became clear that nuclear power would not grow nearly as dramatically as once believed. Eventually, more than 120 reactor orders in the United States were ultimately cancelled and the construction of new reactors ground to a halt. A cover story in the February 11, 1985, issue of \"Forbes\" magazine commented on the overall failure of the U.S. nuclear power program, saying it \"ranks as the largest managerial disaster in business history\".\n\nUnlike the Three Mile Island accident, the much more serious Chernobyl accident did not increase regulations affecting Western reactors since the Chernobyl reactors were of the problematic RBMK design only used in the Soviet Union, for example lacking \"robust\" containment buildings. Many of these RBMK reactors are still in use today. However, changes were made in both the reactors themselves (use of a safer enrichment of uranium) and in the control system (prevention of disabling safety systems), amongst other things, to reduce the possibility of a duplicate accident.\n\nAn international organization to promote safety awareness and professional development on operators in nuclear facilities was created: World Association of Nuclear Operators (WANO).\n\nOpposition in Ireland and Poland prevented nuclear programs there, while Austria (1978), Sweden (1980) and Italy (1987) (influenced by Chernobyl) voted in referendums to oppose or phase out nuclear power. In July 2009, the Italian Parliament passed a law that cancelled the results of an earlier referendum and allowed the immediate start of the Italian nuclear program. \nAfter the Fukushima Daiichi nuclear disaster a one-year moratorium was placed on nuclear power development, followed by a referendum in which over 94% of voters (turnout 57%) rejected plans for new nuclear power.\n\nSince about 2001 the term \"nuclear renaissance\" has been used to refer to a possible nuclear power industry revival, driven by rising fossil fuel prices and new concerns about meeting greenhouse gas emission limits. \nSince commercial nuclear energy began in the mid-1950s, 2008 was the first year that no new nuclear power plant was connected to the grid, although two were connected in 2009.\n\nFollowing the Tōhoku earthquake on 11 March 2011, one of the largest earthquakes ever recorded, and a subsequent tsunami off the coast of Japan, the Fukushima Daiichi Nuclear Power Plant suffered multiple core meltdowns due to failure of the emergency cooling system for lack of electricity supply. This resulted in the most serious nuclear accident since the Chernobyl disaster.\n\nThe Fukushima Daiichi nuclear accident prompted a re-examination of nuclear safety and nuclear energy policy in many countries and raised questions among some commentators over the future of the renaissance.\nGermany approved plans to close all its reactors by 2022, and Italy re-affirmed its ban on nuclear power in a referendum. \nChina, Switzerland, Israel, Malaysia, Thailand, United Kingdom, and the Philippines reviewed their nuclear power programs.\n\nIn 2011 the International Energy Agency halved its prior estimate of new generating capacity to be built by 2035. \nNuclear power generation had the biggest ever fall year-on-year in 2012, with nuclear power plants globally producing 2,346 TWh of electricity, a drop of 7% from 2011. \nThis was caused primarily by the majority of Japanese reactors remaining offline that year and the permanent closure of eight reactors in Germany.\n\nThe Fukushima Daiichi nuclear accident sparked controversy about the importance of the accident and its effect on nuclear's future. \nThe crisis prompted countries with nuclear power to review the safety of their reactor fleet and reconsider the speed and scale of planned nuclear expansions. \nIn 2011, \"The Economist\" opined that nuclear power \"looks dangerous, unpopular, expensive and risky\", and that \"it is replaceable with relative ease and could be forgone with no huge structural shifts in the way the world works\". \nEarth Institute Director Jeffrey Sachs disagreed, claiming combating climate change would require an expansion of nuclear power.\nInvestment banks were also critical of nuclear soon after the accident.\n\nIn September 2011, German engineering giant Siemens announced it will withdraw entirely from the nuclear industry as a response to the Fukushima accident.\n\nIn February 2012, the United States Nuclear Regulatory Commission approved the construction of two additional reactors at the Vogtle Electric Generating Plant, the first reactors to be approved in over 30 years since the Three Mile Island accident.\nIn October 2016, Watts Bar 2 became the first new United States reactor to enter commercial operation since 1996.\n\nIn 2013 Japan signed a deal worth $22 billion, in which Mitsubishi Heavy Industries would build four modern \"Atmea\" reactors for Turkey. \nIn August 2015, following 4 years of near zero fission-electricity generation, Japan began restarting its nuclear reactors, after safety upgrades were completed, beginning with Sendai Nuclear Power Plant.\n\nBy 2015, the IAEA's outlook for nuclear energy had become more promising. \n\"Nuclear power is a critical element in limiting greenhouse gas emissions,\" the agency noted, and \"the prospects for nuclear energy remain positive in the medium to long term despite a negative impact in some countries in the aftermath of the [Fukushima-Daiichi] accident...it is still the second-largest source worldwide of low-carbon electricity. \nAnd the 72 reactors under construction at the start of last year were the most in 25 years.\"\nAccording to the World Nuclear Association, as of 2015 the global trend was for new nuclear power stations coming online to be balanced by the number of old plants being retired.\n\nAs of 2015, 441 reactors had a worldwide net electric capacity of 382,9 GW, with 67 new nuclear reactors under construction.\nOver half of the 67 total being built were in Asia, with 28 in China, where there is an urgent need to control pollution from coal plants.\nEight new grid connections were completed by China in 2015.\n\nAs of 2018, there are over 150 nuclear reactors planned including 50 under construction. However, while investment on upgrades of existing plant and life-time extensions continues, investment in new nuclear is declining, reaching a 5-year-low in 2017. \n\nIn 2015, the International Energy Agency reported that the Fukushima accident had a strongly negative effect on nuclear power, yet nuclear power prospects are positive in the medium to long term mainly thanks to new construction in Asia.\nIn 2016, the U.S. Energy Information Administration projected for its “base case” that world nuclear power generation would increase from 2,344 terawatt hours (TWh) in 2012 to 4,501 TWh in 2040. \nMost of the predicted increase was expected to be in Asia.\n\nThe future of nuclear power varies greatly between countries, depending on government policies. \nSome countries, many of them in Europe, such as Germany, Belgium, and Lithuania, have adopted policies of nuclear power phase-out. \nAt the same time, some Asian countries, such as China and India, have committed to rapid expansion of nuclear power. \nMany other countries, such as the United Kingdom and the United States, have policies in between. \nJapan generated about 30% of its electricity from nuclear power before the Fukushima accident.\nIn 2015 the Japanese government committed to the aim of restarting its fleet of 40 reactors by 2030 after safety upgrades, and to finish the construction of the Generation III Ōma Nuclear Power Plant. \nThis would mean that approximately 20% of electricity would come from nuclear power by 2030.\nAs of 2018, some reactors have restarted commercial operation following inspections and upgrades with new regulations. \nWhile South Korea has a large nuclear power industry, the new government in 2017, partly influenced by a large anti-nuclear movement, committed to halting nuclear development and to gradually phase out nuclear power as reactors that are now operating or under construction close after 40 years of operations.\n\nThe nuclear power industry in western nations have a history of construction delays, cost overruns, plant cancellations, and nuclear safety issues, despite significant government subsidies and support.\nThese problems are related to very strict safety requirements, uncertain regulatory environment, slow rate of construction, and large stretches of time with no nuclear construction and consequent loss of know-how.\nCommentators therefore argue that nuclear power is impractical in western countries because of high costs, popular opposition, and regulatory uncertainty.\nRecent financial problems of western nuclear companies, most prominently the bankruptcy of Westinghouse in March 2017 because of US$9 billion of losses from nuclear construction projects in the United States, suggest a shift to the East, as the dominant exporter and designer of nuclear fuel and reactors.\n\nThe greatest new build activity is occurring in Asian countries like South Korea, India and China. \nIn March 2016, China had 30 reactors in operation, 24 under construction and plans to build more.\n\nIn 2016 the BN-800 sodium cooled fast reactor in Russia, began commercial electricity generation, while plans for a BN-1200 were initially conceived the future of the fast reactor program in Russia awaits the results from MBIR, an under construction multi-loop Generation IV research facility for testing the chemically more inert lead, lead-bismuth and gas coolants, it will similarly run on recycled MOX (mixed uranium and plutonium oxide) fuel. An on-site pyrochemical processing, closed fuel-cycle facility, is planned, to reduce the necessity for a growth in uranium mining and exploration. In 2017 the manufacture program for the reactor commenced with the facility open to collaboration under the \"International Project on Innovative Nuclear Reactors and Fuel Cycle\", it has a construction schedule, that includes an operational start in 2020. As planned, it will be the world's most-powerful research reactor.\n\nIn the United States, licenses of almost half of the operating nuclear reactors have been extended to 60 years. \nThe U.S. NRC and the U.S. Department of Energy have initiated research into Light water reactor sustainability which is hoped will lead to allowing extensions of reactor licenses beyond 60 years, provided that safety can be maintained, to increase energy security and preserve low-carbon generation sources. \nResearch into nuclear reactors that can last 100 years, known as Centurion Reactors, is being conducted.\n\nAccording to the World Nuclear Association, globally during the 1980s one new nuclear reactor started up every 17 days on average, and in the year 2015 it was estimated that this rate could in theory eventually increase to one every 5 days, although no plans exist for that.\n\nJust as many conventional thermal power stations generate electricity by harnessing the thermal energy released from burning fossil fuels, nuclear power plants convert the energy released from the nucleus of an atom via nuclear fission that takes place in a nuclear reactor. When a neutron hits the nucleus of a uranium-235 or plutonium atom, it can split the nucleus into two smaller nuclei. The reaction is called nuclear fission. The fission reaction releases energy and neutrons. The released neutrons can hit other uranium or plutonium nuclei, causing new fission reactions, which release more energy and more neutrons. This is called a chain reaction. The reaction rate is controlled by control rods that absorb excess neutrons. The controllability of nuclear reactors depends on the fact that a small fraction of neutrons resulting from fission are delayed. The time delay between the fission and the release of the neutrons slows down changes in reaction rates and gives time for moving the control rods to adjust the reaction rate.\n\nA fission nuclear power plant is generally composed of a nuclear reactor, in which the nuclear reactions generating heat take place; a cooling system, which removes the heat from inside the reactor; a steam turbine, which transforms the heat in mechanical energy; an electric generator, which transform the mechanical energy into electrical energy.\n\nNuclear fission power stations, excluding the contribution from naval nuclear fission reactors, provided 11% of the world's electricity in 2012, somewhat less than that generated by hydro-electric stations at 16%. \nSince electricity accounts for about 25% of humanity's energy usage with the majority of the rest coming from fossil fuel reliant sectors such as transport, manufacture and home heating, nuclear fission's contribution to the global final energy consumption was about 2.5%. \nThis is a little more than the combined global electricity production from wind, solar, biomass and geothermal power, which together provided 2% of global final energy consumption in 2014.\n\nIn 2013, the IAEA reported that there were 437 operational civil fission-electric reactors in 31 countries, although not every reactor was producing electricity. \nIn addition, there were approximately 140 naval vessels using nuclear propulsion in operation, powered by about 180 reactors.\n\nNuclear power's share of global electricity production has fallen from 16.5% in 1997 to about 10% in 2017, in large part because the economics of nuclear power have become more difficult.\n\nRegional differences in the use of nuclear power are large.\nThe United States produces the most nuclear energy in the world, with nuclear power providing 19% of the electricity it consumes, while France produces the highest percentage of its electrical energy from nuclear reactors—80% as of 2006. \nIn the European Union as a whole nuclear power provides 30% of the electricity. \nNuclear power is the single largest low-carbon electricity source in the United States, and accounts for two-thirds of the European Union's low-carbon electricity. \nNuclear energy policy differs among European Union countries, and some, such as Austria, Estonia, Ireland and Italy, have no active nuclear power stations. \nIn comparison, France has a large number of nuclear reactors in use and nuclear power supplied over 70% of its electricity in 2017.\n\nMany military and some civilian (such as some icebreakers) ships use nuclear marine propulsion. \nA few space vehicles have been launched using nuclear reactors: 33 reactors belong to the Soviet RORSAT series and one was the American SNAP-10A.\n\nInternational research is continuing into additional uses of process heat such as hydrogen production (in support of a hydrogen economy), for desalinating sea water, and for use in district heating systems.\n\nThe nuclear industry consists of a number of companies, organizations, governmental and international bodies.\nThe main fields of the industry include nuclear reactor building and operation; uranium mining and nuclear fuel production; nuclear waste storage and processing; research and development.\nOther components of the nuclear industry include nuclear regulators and nuclear industry national and international associations.\n\nThe economics of new nuclear power plants is a controversial subject, since there are diverging views on this topic, and multibillion-dollar investments depend on the choice of an energy source. \nNuclear power plants typically have high capital costs for building the plant, but low fuel costs. \nComparison with other power generation methods is strongly dependent on assumptions about construction timescales and capital financing for nuclear plants as well as the future costs of fossil fuels and renewables as well as for energy storage solutions for intermittent power sources. \nCost estimates also need to take into account plant decommissioning and nuclear waste storage costs. \nOn the other hand, measures to mitigate global warming, such as a carbon tax or carbon emissions trading, may favor the economics of nuclear power.\n\nAnalysis of the economics of nuclear power must also take into account who bears the risks of future uncertainties. \nTo date all operating nuclear power plants were developed by state-owned or regulated electric utility monopolies where many of the risks associated with construction costs, operating performance, fuel price, accident liability and other factors were borne by consumers rather than suppliers. \nIn addition, because the potential liability from a nuclear accident is so great, the full cost of liability insurance is generally limited/capped by the government, which the U.S. Nuclear Regulatory Commission concluded constituted a significant subsidy. \nMany countries have now liberalized the electricity market where these risks, and the risk of cheaper competitors emerging before capital costs are recovered, are borne by plant suppliers and operators rather than consumers, which leads to a significantly different evaluation of the economics of new nuclear power plants.\n\nAlthough nuclear power plants can vary their output, the electricity is generally less favorably priced when doing so. \nNuclear power plants are therefore typically run as much as possible to keep the cost of the generated electrical energy as low as possible, supplying mostly base-load electricity.\n\nInternationally the price of nuclear plants rose 15% annually in 1970–1990. \nYet, nuclear power has total costs in 2012 of about $96 per megawatt hour (MWh), most of which involves capital construction costs, compared with solar power at $130 per MWh, and natural gas at the low end at $64 per MWh.\nFollowing the 2011 Fukushima Daiichi nuclear disaster, costs are expected to increase for then-operating and new nuclear power plants, due to increased requirements for on-site spent fuel management and elevated design basis threats.\n\nA nuclear reactor is only part of the fuel life-cycle for nuclear power. \nThe process starts with mining (see \"Uranium mining\"). \nUranium mines are underground, open-pit, or in-situ leach mines. \nIn any case, the uranium ore is extracted, usually converted into a stable and compact form such as yellowcake, and then transported to a processing facility. \nHere, the yellowcake is converted to uranium hexafluoride, which is then generally enriched using various techniques. \nSome reactor designs can also use natural uranium without enrichment.\nThe enriched uranium, containing more than the natural 0.7% uranium-235, is generally used to make rods of the proper composition and geometry for the particular reactor that the fuel is destined for. \nIn modern light-water reactors the fuel rods will spend about 3 operational cycles (typically 6 years total now) inside the reactor, generally until about 3% of their uranium has been fissioned, then they will be moved to a spent fuel pool where the short lived isotopes generated by fission can decay away. \nAfter about 5 years in a spent fuel pool the spent fuel is radioactively and thermally cool enough to handle, and it can be moved to dry storage casks or reprocessed.\n\nUranium is a fairly common element in the Earth's crust: it is approximately as common as tin or germanium, and is about 40 times more common than silver. \nUranium is present in trace concentrations in most rocks, dirt, and ocean water, but can be economically extracted only where it is present in high concentrations. \nStill, as of 2011 the world's measured resources of uranium, economically recoverable at the arbitrary price ceiling of 130 USD/kg, were enough to last for between 70 and 100 years.\n\nAccording to the OECD in 2006, there was an expected 85 years worth of uranium in already identified resources at the then-current utilization rates. In 2007, the OECD estimated 670 years of economically recoverable uranium in total conventional resources and phosphate ores assuming the then-current use rate. The OECD's red book of 2011 said that known uranium resources had grown by 12.5% since 2008 due to increased exploration, with this increase translating into greater than a century of uranium available if the rate of use were to continue at the 2011 level.\n\nLight water reactors make relatively inefficient use of nuclear fuel, mostly fissioning only the very rare uranium-235 isotope. Nuclear reprocessing can make this waste reusable. Newer Generation III reactors also achieve a more efficient use of the available resources than the generation II reactors which make up the vast majority of reactors worldwide.\nWith a pure fast reactor fuel cycle with a burn up of all the Uranium and actinides (which presently make up the most hazardous substances in nuclear waste), there is an estimated 160,000 years worth of Uranium in total conventional resources and phosphate ore at the price of 60–100 US$/kg.\n\nUnconventional uranium resources also exist.\nUranium is naturally present in seawater at a concentration of about 3 micrograms per liter, with 4.5 billion tons of uranium considered present in seawater at any time. \nIn 2012 it was estimated that this fuel source could be extracted at 10 times the current price of uranium. \n\nIn 2014, with the advances made in the efficiency of seawater uranium extraction, it was suggested that it would be economically competitive to produce fuel for light water reactors from seawater if the process was implemented at large scale. \nUranium extracted on an industrial scale from seawater would constantly be replenished by both river erosion of rocks and the natural process of uranium dissolved from the surface area of the ocean floor, both of which maintain the solubility equilibria of seawater concentration at a stable level. \nSome commentators have argued that this strengthens the case for Nuclear power to be considered a renewable energy.\n\nAs opposed to light water reactors which use uranium-235 (0.7% of all natural uranium), fast breeder reactors use uranium-238 (99.3% of all natural uranium). In 2006 it was estimated that with seawater extraction, there was likely some five billion years' worth of uranium-238 for use in these power plants.\n\nBreeder technology has been used in several reactors, but the high cost of reprocessing fuel safely, at 2006 technological levels, requires uranium prices of more than US$200/kg before becoming justified economically. \nBreeder reactors are however being pursued as they have the potential to burn up all of the actinides in the present inventory of nuclear waste while also producing power and creating additional quantities of fuel for more reactors via the breeding process.\n\nAs of 2017, there are only two breeder reactors producing commercial power: the BN-600 reactor and the BN-800 reactor, both in Russia.\nThe BN-600, with a capacity of 600 MW, was built in 1980 in Beloyarsk and is planned to produce power until 2025.\nThe BN-800 is an updated version of the BN-600, and started operation in 2014 with a net electrical capacity of 789 MW.\nThe technical design of a yet larger breeder, the BN-1200 reactor was originally scheduled to be finalized in 2013, with construction slated for 2015 but has since been delayed. \n\nThe Phénix breeder reactor in France was powered down in 2009 after 36 years of operation.\nJapan's Monju breeder reactor restarted (having been shut down in 1995) in 2010 for 3 months, but shut down again after equipment fell into the reactor during reactor checkups.\nThe reactor was decommissioned in 2017.\n\nBoth China and India are building breeder reactors. \nThe Indian 500 MWe Prototype Fast Breeder Reactor is in the commissioning phase, with plans to build five more by 2020. \nThe China Experimental Fast Reactor began operating in 2011.\n\nAnother alternative to fast breeders is thermal breeder reactors that use uranium-233 bred from thorium as fission fuel in the thorium fuel cycle.\nThorium is about 3.5 times more common than uranium in the Earth's crust, and has different geographic characteristics.\nThis would extend the total practical fissionable resource base by 450%. \nIndia's three-stage nuclear power programme features the use of a thorium fuel cycle in the third stage, as it has abundant thorium reserves but little uranium.\n\nThe most important waste stream from nuclear power plants is spent nuclear fuel. \nIt is primarily composed of unconverted uranium as well as significant quantities of transuranic actinides (plutonium and curium, mostly). \nIn addition, about 3% of it is fission products from nuclear reactions. \nThe actinides (uranium, plutonium, and curium) are responsible for the bulk of the long-term radioactivity, whereas the fission products are responsible for the bulk of the short-term radioactivity.\n\nHigh-level radioactive waste management concerns management and disposal of highly radioactive materials created during production of nuclear power. The technical issues in accomplishing this are daunting, due to the extremely long periods radioactive wastes remain deadly to living organisms. Of particular concern are two long-lived fission products, Technetium-99 (half-life 220,000 years) and Iodine-129 (half-life 15.7 million years), which dominate spent nuclear fuel radioactivity after a few thousand years. The most troublesome transuranic elements in spent fuel are Neptunium-237 (half-life two million years) and Plutonium-239 (half-life 24,000 years). Consequently, high-level radioactive waste requires sophisticated treatment and management to successfully isolate it from the biosphere. This usually necessitates treatment, followed by a long-term management strategy involving permanent storage, disposal or transformation of the waste into a non-toxic form.\n\nGovernments around the world are considering a range of waste management and disposal options, usually involving deep-geologic placement, although there has been limited progress toward implementing long-term waste management solutions. This is partly because the timeframes in question when dealing with radioactive waste range from 10,000 to millions of years, according to studies based on the effect of estimated radiation doses.\n\nSome proposed nuclear reactor designs however such as the American Integral Fast Reactor and the Molten salt reactor can use the nuclear waste from light water reactors as a fuel, transmutating it to isotopes that would be safe after hundreds, instead of tens of thousands of years. This offers a potentially more attractive alternative to deep geological disposal.\n\nAnother possibility is the use of thorium in a reactor especially designed for thorium (rather than mixing in thorium with uranium and plutonium (i.e. in existing reactors). Used thorium fuel remains only a few hundreds of years radioactive, instead of tens of thousands of years.\n\nSince the fraction of a radioisotope's atoms decaying per unit of time is inversely proportional to its half-life, the relative radioactivity of a quantity of buried human radioactive waste would diminish over time compared to natural radioisotopes (such as the decay chains of 120 trillion tons of thorium and 40 trillion tons of uranium which are at relatively trace concentrations of parts per million each over the crust's 3 * 10 ton mass). For instance, over a timeframe of thousands of years, after the most active short half-life radioisotopes decayed, burying U.S. nuclear waste would increase the radioactivity in the top 2000 feet of rock and soil in the United States (10 million km) by ≈ 1 part in 10 million over the cumulative amount of natural radioisotopes in such a volume, although the vicinity of the site would have a far higher concentration of artificial radioisotopes underground than such an average.\n\nThe nuclear industry also produces a large volume of low-level radioactive waste in the form of contaminated items like clothing, hand tools, water purifier resins, and (upon decommissioning) the materials of which the reactor itself is built. Low-level waste can be stored on-site until radiation levels are low enough to be disposed as ordinary waste, or it can be sent to a low-level waste disposal site.\n\nIn countries with nuclear power, radioactive wastes account for less than 1% of total industrial toxic wastes, much of which remains hazardous for long periods. Overall, nuclear power produces far less waste material by volume than fossil-fuel based power plants. Coal-burning plants are particularly noted for producing large amounts of toxic and mildly radioactive ash due to concentrating naturally occurring metals and mildly radioactive material from the coal. A 2008 report from Oak Ridge National Laboratory concluded that coal power actually results in more radioactivity being released into the environment than nuclear power operation, and that the population effective dose equivalent, or dose to the public from radiation from coal plants is 100 times as much as from the operation of nuclear plants. \nAlthough coal ash is much less radioactive than spent nuclear fuel on a weight per weight basis, coal ash is produced in much higher quantities per unit of energy generated, and this is released directly into the environment as fly ash, whereas nuclear plants use shielding to protect the environment from radioactive materials, for example, in dry cask storage vessels.\n\nDisposal of nuclear waste is often said to be the among the most problematic aspects of the industry. \nPresently, waste is mainly stored at individual reactor sites and there are over 430 locations around the world where radioactive material continues to accumulate. \nSome experts suggest that centralized underground repositories which are well-managed, guarded, and monitored, would be a vast improvement. \nThere is an \"international consensus on the advisability of storing nuclear waste in deep geological repositories\", with the lack of movement of nuclear waste in the 2 billion year old natural nuclear fission reactors in Oklo, Gabon being cited as \"a source of essential information today.\"\n\nThere are no commercial scale purpose built underground high-level waste repositories in operation. However, in Finland the Onkalo spent nuclear fuel repository is under construction. Construction license for the world's first spent fuel repository was granted in 2015. The Waste Isolation Pilot Plant (WIPP) in New Mexico has been taking nuclear waste since 1999 from production reactors, but as the name suggests is a research and development facility. \nIn 2014 a radiation leak caused by violations in the use of chemically reactive packaging brought renewed attention to the need for quality control management, along with some initial calls for more R&D into the alternative methods of disposal for radioactive waste and spent fuel. \nIn 2017, the facility was formally reopened after three years of investigation and cleanup, with the resumption of new storage taking place later that year.\n\nToday's moderated/\"thermal reactors\" primarily run on the once-thru fuel cycle though they can reuse once-thru reactor-grade plutonium to a limited degree in the form of mixed-oxide or MOX fuel, which is a routine commercial practice in most countries outside the US as it increases the sustainability of nuclear fission and lowers the volume of high level nuclear waste. \nReprocessing can potentially recover up to 95% of the remaining uranium and plutonium in spent nuclear fuel, putting it into new mixed oxide fuel. This produces a reduction in long term radioactivity within the remaining waste, since this is largely short-lived fission products, and reduces its volume by over 90%. Reprocessing of civilian fuel from power reactors is currently done in Europe, Russia, Japan, and India. The full potential of reprocessing had not been achieved as of 2013 because it requires breeder reactors, which were not commercially available at that time.\n\nNuclear reprocessing reduces the volume of high-level waste, but by itself does not reduce radioactivity or heat generation and therefore does not eliminate the need for a geological waste repository. Reprocessing has been politically controversial because of the potential to contribute to nuclear proliferation, the potential vulnerability to nuclear terrorism, the political challenges of repository siting (a problem that applies equally to direct disposal of spent fuel), and because of its high cost compared to the once-through fuel cycle. Several different methods for reprocessing been tried, but many have had safety and practicality problems which have led to their discontinuation.\n\nIn the United States, the Obama administration stepped back from President Bush's plans for commercial-scale reprocessing and reverted to a program focused on reprocessing-related scientific research. Reprocessing is not allowed in the U.S. In the United States, spent nuclear fuel is currently all treated as waste. A major recommendation of the Blue Ribbon Commission on America's Nuclear Future was that \"the United States should undertake an integrated nuclear waste management program that leads to the timely development of one or more permanent deep geological facilities for the safe disposal of spent fuel and high-level nuclear waste\".\n\nUranium enrichment produces large amounts of depleted uranium (DU), which consists of U-238 with most of the easily fissile U-235 isotope removed. \nU-238 is a tough metal with several commercial uses including aircraft production, radiation shielding, and armor, as it has a higher density than lead. \nDepleted uranium is also controversially used in munitions; DU penetrators (bullets or APFSDS tips) \"self sharpen\", due to uranium's tendency to fracture along shear bands.\n\nNuclear reactors have three unique characteristics that affect their safety, as compared to other power plants. First, a very large amount of radioactive materials is present in a nuclear reactor. Their release to the environment could be hazardous. Second, a reactor has no \"natural\" power level. Rapid power increase is possible if the chain reaction cannot be controlled. Third, the fission products in the reactor continue to generate a significant amount of decay heat even after the chain reaction has stopped. If the heat cannot be removed from the reactor, the fuel rods may overheat and release radioactive materials. These three characteristics have to be taken into account when designing nuclear reactors. Multiple barriers separate the radioactive materials from the environment. Reactors are designed so that natural feedback mechanisms prevent an uncontrolled increase of the reactor power. Emergency cooling systems can remove the decay heat from the reactor in case the normal cooling systems fail.\n\nSome serious nuclear and radiation accidents have occurred. \nThe severity of nuclear accidents is generally classified using the International Nuclear Event Scale (INES) introduced by the International Atomic Energy Agency (IAEA).\nThe scale ranks anomalous events or accidents on a scale from 0 (a deviation from normal operation that pose no safety risk) to 7 (a major accident with widespread effects). \nThere have been 3 accidents of level 5 or higher in the civilian nuclear power industry, two of which, the Chernobyl accident and the Fukushima accident, are ranked at level 7.\nThe Chernobyl accident in 1986 caused approximately 50 deaths from direct and indirect effects, and many more serious injuries.\nThe exact death toll due to indirect effects of the widespread radiation contamination is difficult to measure, and depends on the assumptions used. \nSome studies put the total death toll of the Chernobyl accident from indirect effects much higher, in the order of thousands of people.\nThe Fukushima Daiichi nuclear accident was caused by the 2011 Tohoku earthquake and tsunami. \nThe accident has not caused any radiation related deaths, but resulted in radioactive contamination of surrounding areas.\nThe difficult Fukushima disaster cleanup will take 40 or more years, and is expected to cost tens of billions of dollars.\nThe Three Mile Island accident in 1979 was a smaller scale accident, rated at INES level 5. \nThere were no direct or indirect deaths caused by the accident.\n\nOther less serious accidents are more common. Benjamin K. Sovacool has reported that worldwide there have been 99 accidents (defined as either resulting in loss of human life or more than $50,000 of property damage) at nuclear power plants. \nFifty-seven accidents have occurred since the Chernobyl disaster, and 57% (56 out of 99) of all nuclear-related accidents have occurred in the United States.\n\nMilitary nuclear-powered submarine mishaps include the K-19 reactor accident (1961), the K-27 reactor accident (1968), and the K-431 reactor accident (1985). \nInternational research is continuing into safety improvements such as passively safe plants, and the possible future use of nuclear fusion.\n\nAccording to Benjamin K. Sovacool, fission energy accidents ranked first among energy sources in terms of their total economic cost, accounting for 41 percent of all property damage attributed to energy accidents. \nAnother analysis presented in the international journal \"Human and Ecological Risk Assessment\" found that coal, oil, Liquid petroleum gas and hydroelectric accidents (primarily due to the Banqiao dam burst) have resulted in greater economic impacts than nuclear power accidents.\n\nNuclear power works under an insurance framework that limits or structures accident liabilities in accordance with the Paris convention on nuclear third-party liability, the Brussels supplementary convention, the Vienna convention on civil liability for nuclear damage and the Price-Anderson Act in the United States.\nIt is often argued that this potential shortfall in liability represents an external cost not included in the cost of nuclear electricity; but the cost is small, amounting to about 0.1% of the levelized cost of electricity, according to a CBO study.\nThese beyond-regular-insurance costs for worst-case scenarios are not unique to nuclear power, as hydroelectric power plants are similarly not fully insured against a catastrophic event such as the Banqiao Dam disaster, where 11 million people lost their homes and from 30,000 to 200,000 people died, or large dam failures in general. As private insurers base dam insurance premiums on limited scenarios, major disaster insurance in this sector is likewise provided by the state.\n\nIn terms of lives lost per unit of energy generated, nuclear power has caused fewer accidental deaths per unit of energy generated than all other major sources of energy generation. \nEnergy produced by coal, petroleum, natural gas and hydropower has caused more deaths per unit of energy generated due to air pollution and energy accidents. \nThis is found when comparing the immediate deaths from other energy sources to both the immediate nuclear related deaths from accidents and also including the latent, or predicted, indirect cancer deaths from nuclear energy accidents. \nWhen the combined immediate and indirect fatalities from nuclear power and all fossil fuels are compared, including fatalities resulting from the mining of the necessary natural resources to power generation and to air pollution, the use of nuclear power has been calculated to have prevented about 1.8 million deaths between 1971 and 2009, by reducing the proportion of energy that would otherwise have been generated by fossil fuels, and is projected to continue to do so.\nFollowing the 2011 Fukushima nuclear disaster, it has been estimated that if Japan had never adopted nuclear power, accidents and pollution from coal or gas plants would have caused more lost years of life.\n\nForced evacuation from a nuclear accident may lead to social isolation, anxiety, depression, psychosomatic medical problems, reckless behavior, even suicide. \nSuch was the outcome of the 1986 Chernobyl nuclear disaster in Ukraine. \nA comprehensive 2005 study concluded that \"the mental health impact of Chernobyl is the largest public health problem unleashed by the accident to date\". \nFrank N. von Hippel, an American scientist, commented on the 2011 Fukushima nuclear disaster, saying that a disproportionate radiophobia, or \"fear of ionizing radiation could have long-term psychological effects on a large portion of the population in the contaminated areas\". \nA 2015 report in \"Lancet\" explained that serious impacts of nuclear accidents were often not directly attributable to radiation exposure, but rather social and psychological effects. \nEvacuation and long-term displacement of affected populations created problems for many people, especially the elderly and hospital patients.\nIn January 2015, the number of Fukushima evacuees was around 119,000, compared with a peak of around 164,000 in June 2012.\n\nTerrorists could target nuclear power plants in an attempt to release radioactive contamination into the community. The United States 9/11 Commission has said that nuclear power plants were potential targets originally considered for the September 11, 2001 attacks. An attack on a reactor's spent fuel pool could also be serious, as these pools are less protected than the reactor core. The release of radioactivity could lead to thousands of near-term deaths and greater numbers of long-term fatalities.\n\nIf nuclear power use is to expand significantly, nuclear facilities will have to be made extremely safe from attacks that could release massive quantities of radioactivity. \nNew reactor designs have features of passive safety, such as the flooding of the reactor core without active intervention by reactor operators. \nHowever, these safety measures have generally been developed and studied with respect to accidents, not to the deliberate reactor attack by a terrorist group. \nThe U.S. Nuclear Regulatory Commission (NRC) does now also require new reactor license applications to consider security during the design stage. \nIn the United States, the NRC carries out \"Force on Force\" (FOF) exercises at all Nuclear Power Plant (NPP) sites at least once every three years. \nIn the United States, plants are surrounded by a double row of tall fences which are electronically monitored. \nThe plant grounds are patrolled by a sizeable force of armed guards.\n\nInsider sabotage is also a threat because insiders can observe and work around security measures. \nSuccessful insider crimes depended on the perpetrators' observation and knowledge of security vulnerabilities. \nA fire caused 5–10 million dollars worth of damage to New York's Indian Point Energy Center in 1971. \nThe arsonist turned out to be a plant maintenance worker. \nSabotage by workers has been reported at many other reactors in the United States: at Zion Nuclear Power Station (1974), Quad Cities Nuclear Generating Station, Peach Bottom Nuclear Generating Station, Fort St. Vrain Generating Station, Trojan Nuclear Power Plant (1974), Browns Ferry Nuclear Power Plant (1980), and Beaver Valley Nuclear Generating Station (1981). Many reactors overseas have also reported sabotage by workers.\n\nMany technologies and materials associated with the creation of a nuclear power program have a dual-use capability, in that they can be used to make nuclear weapons if a country chooses to do so. When this happens a nuclear power program can become a route leading to a nuclear weapon or a public annex to a \"secret\" weapons program. The concern over Iran's nuclear activities is a case in point.\n\nAs of April 2012 there were thirty one countries that have civil nuclear power plants, of which nine have nuclear weapons, with the vast majority of these nuclear weapons states having first produced weapons, before commercial fission electricity stations. \nMoreover, the re-purposing of civilian nuclear industries for military purposes would be a breach of the Non-proliferation treaty, of which 190 countries adhere to.\n\nA fundamental goal for global security is to minimize the nuclear proliferation risks associated with the expansion of nuclear power. \nThe Global Nuclear Energy Partnership is an international effort to create a distribution network in which developing countries in need of energy would receive nuclear fuel at a discounted rate, in exchange for that nation agreeing to forgo their own indigenous develop of a uranium enrichment program. \nThe France-based Eurodif/\"European Gaseous Diffusion Uranium Enrichment Consortium\" is a program that successfully implemented this concept, with Spain and other countries without enrichment facilities buying a share of the fuel produced at the French controlled enrichment facility, but without a transfer of technology. \nIran was an early participant from 1974, and remains a shareholder of Eurodif via Sofidif.\n\nAccording to Benjamin K. Sovacool, a \"number of high-ranking officials, even within the United Nations, have argued that they can do little to stop states using nuclear reactors to produce nuclear weapons\". \nA 2009 United Nations report said that:\nthe revival of interest in nuclear power could result in the worldwide dissemination of uranium enrichment and spent fuel reprocessing technologies, which present obvious risks of proliferation as these technologies can produce fissile materials that are directly usable in nuclear weapons.\n\nOn the other hand, power reactors can also reduce nuclear weapons arsenals when military grade nuclear materials are reprocessed to be used as fuel in nuclear power plants. \nThe Megatons to Megawatts Program, the brainchild of Thomas Neff of MIT, is the single most successful non-proliferation program to date. \nUp to 2005, the Megatons to Megawatts Program had processed $8 billion of high enriched, weapons grade uranium into low enriched uranium suitable as nuclear fuel for commercial fission reactors by diluting it with natural uranium.\nThis corresponds to the elimination of 10,000 nuclear weapons.\nFor approximately two decades, this material generated nearly 10 percent of all the electricity consumed in the United States (about half of all U.S. nuclear electricity generated) with a total of around 7 trillion kilowatt-hours of electricity produced. Enough energy to energize the entire United States electric grid for about two years. In total it is estimated to have cost $17 billion, a \"bargain for US ratepayers\", with Russia profiting $12 billion from the deal. Much needed profit for the Russian nuclear oversight industry, which after the collapse of the Soviet economy, had difficulties paying for the maintenance and security of the Russian Federations highly enriched uranium and warheads.\n\nThe Megatons to Megawatts Program was hailed as a major success by anti-nuclear weapon advocates as it has largely been the driving force behind the sharp reduction in the quantity of nuclear weapons worldwide since the cold war ended. \nHowever without an increase in nuclear reactors and greater demand for fissile fuel, the cost of dismantling and down blending has dissuaded Russia from continuing their disarmament.\nAs of 2013 Russia appears to not be interested in extending the program.\n\nNuclear power is one of the leading low carbon power generation methods of producing electricity, and in terms of total life-cycle greenhouse gas emissions per unit of energy generated, has emission values comparable to or lower than renewable energy.\nA 2014 analysis of the carbon footprint literature by the Intergovernmental Panel on Climate Change (IPCC) reported that the embodied total life-cycle emission intensity of fission electricity has a median value of 12 g eq/kWh which is the lowest out of all commercial baseload energy sources.\nThis is contrasted with coal and fossil gas at 820 and 490 g eq/kWh. \nFrom the beginning of fission-electric power station commercialization in the 1970s, nuclear power prevented the emission of about 64 billion tonnes of carbon dioxide equivalent that would have otherwise resulted from the burning of fossil fuels in thermal power stations.\n\nAccording to the United Nations (UNSCEAR), regular nuclear power plant operation including the nuclear fuel cycle causes radioisotope releases into the environment amounting to 0.0002 millisieverts (mSv) per year of public exposure as a global average. \nThis is small compared to variation in natural background radiation, which averages 2.4 mSv/a globally but frequently varies between 1 mSv/a and 13 mSv/a depending on a person's location as determined by UNSCEAR. \nAs of a 2008 report, the remaining legacy of the worst nuclear power plant accident (Chernobyl) is 0.002 mSv/a in global average exposure (a figure which was 0.04 mSv per person averaged over the entire populace of the Northern Hemisphere in the year of the accident in 1986, although far higher among the most affected local populations and recovery workers).\n\nClimate change causing weather extremes such as heat waves, reduced precipitation levels and droughts can have a significant impact on all thermal power station infrastructure, including large biomass-electric and fission-electric stations alike, if cooling in these power stations, namely in the steam condenser is provided by certain freshwater sources. While many thermal stations use indirect seawater cooling or cooling towers that in comparison use little to no freshwater, those that were designed to heat exchange with rivers and lakes, can run into economic problems.\n\nThis presently infrequent generic problem may become increasingly significant over time. This can force nuclear reactors to be shut down, as happened in France during the 2003 and 2006 heat waves. Nuclear power supply was severely diminished by low river flow rates and droughts, which meant rivers had reached the maximum temperatures for cooling reactors. During the heat waves, 17 reactors had to limit output or shut down. 77% of French electricity is produced by nuclear power and in 2009 a similar situation created a 8GW shortage and forced the French government to import electricity. Other cases have been reported from Germany, where extreme temperatures have reduced nuclear power production only 9 times due to high temperatures between 1979 and 2007. \n\nIf global warming continues, this disruption is likely to increase or alternatively, station operators could instead retro-fit other means of cooling, like cooling towers, despite these frequently being large structures and therefore sometimes unpopular with the public.\n\nThere is an ongoing debate on the relative benefits of nuclear power compared to renewable energy sources for the generation of low-carbon electricity.\nProponents of renewable energy argue that wind power and solar power are already cheaper and safer than nuclear power.\nNuclear power proponents argue that renewable energy sources such as wind and solar do not offer the scalability necessary for a large scale decarbonization of the electric grid, mainly due to their intermittency.\nAlthough the majority of installed renewable energy across the world is currently in the form of hydro power, solar and wind power are growing at a much higher pace, especially in developed countries.\n\nSeveral studies report that it is in principle possible to cover most of energy generation with renewable sources.\nThe Intergovernmental Panel on Climate Change (IPCC) has said that if governments were supportive, and the full complement of renewable energy technologies were deployed, renewable energy supply could account for almost 80% of the world's energy use within forty years with a necessary investment in renewables of about 1% of global GDP annually.\nThis approach could contain greenhouse gas levels to less than 450 parts per million, the safe level beyond which climate change becomes catastrophic and irreversible.\n\nHowever, other studies suggest that solar and wind energy are not cost-effective compared to nuclear power.\nThe Brookings Institution published \"The Net Benefits of Low and No-Carbon Electricity Technologies\" in 2014 which states, after performing an energy and emissions cost analysis, that \"The net benefits of new nuclear, hydro, and natural gas combined cycle plants far outweigh the net benefits of new wind or solar plants\", with the most cost effective low carbon power technology being determined to be nuclear power.\n\nNuclear power is also proposed as a tested and practical way to implement a low-carbon energy infrastructure, as opposed to renewable sources.\nAnalysis in 2015 by professor and chair of Environmental Sustainability Barry W. Brook and his colleagues on the topic of replacing fossil fuels entirely, from the electric grid of the world, has determined that at the historically modest and proven-rate at which nuclear energy was added to and replaced fossil fuels in France and Sweden during each nation's building programs in the 1980s, nuclear energy could displace or remove fossil fuels from the electric grid completely within 10 years, \"allow[ing] the world to meet the most stringent greenhouse-gas mitigation targets.\". \nIn a similar analysis, Brook had earlier determined that 50% of all global energy, that is not solely electricity, but transportation synthetic fuels etc. could be generated within approximately 30 years, if the global nuclear fission build rate was identical to each of these nation's already proven installation rates in units of installed nameplate capacity, GW per year, per unit of global GDP (GW/year/$).\nThis is in contrast to the conceptual studies for a \"100% renewable energy\" world, which would require an orders of magnitude more costly global investment per year, which has no historical precedent, along with far greater land that would have to be devoted to the wind, wave and solar projects, and the inherent assumption that humanity will use less, and not more, energy in the future.\nAs Brook notes, the \"principal limitations on nuclear fission are not technical, economic or fuel-related, but are instead linked to complex issues of societal acceptance, fiscal and political inertia, and inadequate critical evaluation of the real-world constraints facing [the other] low-carbon alternatives.\"\n\nSeveral studies conclude that wind and solar power have costs that are comparable or lower than nuclear power, when considering price per kWh.\nThe cost of constructing established nuclear power reactor designs has followed an increasing trend due to regulations and court cases whereas the levelized cost of electricity (LCOE) is declining for wind and solar power. \nIn 2010 a report from Solar researchers at Duke University found that solar power may be already cheaper than new nuclear power plants. \nHowever they state that if subsidies were removed for solar power, the crossover point would be delayed by years.\nData from the U.S. Energy Information Administration (EIA) in 2011 estimated that in 2016, solar will have a levelized cost of electricity almost twice as expensive as nuclear (21¢/kWh for solar, 11.39¢/kWh for nuclear), and wind somewhat less expensive than nuclear (9.7¢/kWh).\nHowever, the EIA has also cautioned that levelized costs of intermittent sources such as wind and solar are not directly comparable to costs of \"dispatchable\" sources (those that can be adjusted to meet demand), as intermittent sources need costly large-scale back-up power supplies for when the weather changes.\n\nA 2010 study by the Global Subsidies Initiative compared global relative energy subsidies, or government financial aid for the deployment of different energy sources.\nResults show that fossil fuels receive about 1 U.S. cents per kWh of energy they produce, nuclear energy receives 1.7 cents / kWh, renewable energy (excluding hydroelectricity) receives 5.0 cents / kWh and biofuels receive 5.1 cents / kWh in subsidies.\n\nNuclear power is comparable to, and in some cases lower, than many renewable energy sources in terms of lives lost per unit of electricity delivered.\nHowever, as opposed to renewable energy, conventional designs for nuclear reactors produce intensely radioactive spent fuel that needs to be stored or reprocessed.\nA nuclear plant also needs to be disassembled and removed and much of the disassembled nuclear plant needs to be stored as low level nuclear waste for a few decades.\n\nThe financial costs of every nuclear power plant continues for some time after the facility has finished generating its last useful electricity. Once no longer economically viable, nuclear reactors and uranium enrichment facilities are generally decommissioned, returning the facility and its parts to a safe enough level to be entrusted for other uses, such as greenfield status. \nAfter a cooling-off period that may last decades, reactor core materials are dismantled and cut into small pieces to be packed in containers for interim storage or transmutation experiments. The consensus on how to approach the task is one that is relatively inexpensive, but it has the potential to be hazardous to the natural environment as it presents opportunities for human error, accidents or sabotage.\n\nIn the United States a Nuclear Waste Policy Act and Nuclear Decommissioning Trust Fund is legally required, with utilities banking 0.1 to 0.2 cents/kWh during operations to fund future decommissioning. They must report regularly to the Nuclear Regulatory Commission (NRC) on the status of their decommissioning funds. About 70% of the total estimated cost of decommissioning all U.S. nuclear power reactors has already been collected (on the basis of the average cost of $320 million per reactor-steam turbine unit).\n\nIn the United States in 2011, there are 13 reactors that had permanently shut down and are in some phase of decommissioning. With Connecticut Yankee Nuclear Power Plant and Yankee Rowe Nuclear Power Station having completed the process in 2006–2007, after ceasing commercial electricity production circa 1992. \nThe majority of the 15 years, was used to allow the station to naturally cool-down on its own, which makes the manual disassembly process both safer and cheaper.\nDecommissioning at nuclear sites which have experienced a serious accident are the most expensive and time-consuming.\n\nThe nuclear power debate concerns the controversy which has surrounded the deployment and use of nuclear fission reactors to generate electricity from nuclear fuel for civilian purposes. The debate about nuclear power peaked during the 1970s and 1980s, when it \"reached an intensity unprecedented in the history of technology controversies\", in some countries.\n\nProponents of nuclear energy contend that nuclear power is a sustainable energy source that reduces carbon emissions and increases energy security by decreasing dependence on imported energy sources. \nProponents claim that nuclear power produces virtually no conventional air pollution, such as greenhouse gases and smog, in contrast to the main alternative of fossil-fuel power stations. Nuclear power can produce base-load power unlike many renewables which are intermittent energy sources lacking large-scale and cheap ways of storing energy. M. King Hubbert saw oil as a resource that would run out, and proposed nuclear energy as a replacement energy source. \nProponents claim that the risks of storing waste are small and can be further reduced by using the latest technology in newer reactors, and the operational safety record in the Western world is excellent when compared to the other major kinds of power plants.\n\nOpponents believe that nuclear power poses many threats to people and the environment. \nThese threats include the problems of processing, transport and storage of radioactive nuclear waste, the risk of nuclear weapons proliferation and terrorism, as well as health risks and environmental damage from uranium mining. They also contend that reactors themselves are enormously complex machines where many things can and do go wrong; and there have been serious nuclear accidents. Critics do not believe that the risks of using nuclear fission as a power source can be fully offset through the development of new technology. In years past, they also argued that when all the energy-intensive stages of the nuclear fuel chain are considered, from uranium mining to nuclear decommissioning, nuclear power is neither a low-carbon nor an economical electricity source.\n\nArguments of economics and safety are used by both sides of the debate.\n\nBoth fission and fusion appear promising for space propulsion applications, generating higher mission velocities with less reaction mass. This is due to the much higher energy density of nuclear reactions: some 7 orders of magnitude (10,000,000 times) more energetic than the chemical reactions which power the current generation of rockets.\n\nRadioactive decay has been used on a relatively small scale (few kW), mostly to power space missions and experiments by using radioisotope thermoelectric generators such as those developed at Idaho National Laboratory.\n\nCurrent fission reactors in operation around the world are second or third generation systems, with most of the first-generation systems having been already retired. \nResearch into advanced generation IV reactor types was officially started by the Generation IV International Forum (GIF) based on eight technology goals, including to improve nuclear safety, improve proliferation resistance, minimize waste, improve natural resource utilization, the ability to consume existing nuclear waste in the production of electricity, and decrease the cost to build and run such plants. \nMost of these reactors differ significantly from current operating light water reactors, and are generally not expected to be available for commercial construction before 2030.\n\nOne disadvantage of any new reactor technology is that safety risks may be greater initially as reactor operators have little experience with the new design. \nNuclear engineer David Lochbaum has explained that almost all serious nuclear accidents have occurred with what was at the time the most recent technology. \nHe argues that \"the problem with new reactors and accidents is twofold: scenarios arise that are impossible to plan for in simulations; and humans make mistakes\". \nAs one director of a U.S. research laboratory put it, \"fabrication, construction, operation, and maintenance of new reactors will face a steep learning curve: advanced technologies will have a heightened risk of accidents and mistakes. The technology may be proven, but people are not\".\n\nHybrid nuclear power is a proposed means of generating power by use of a combination of nuclear fusion and fission processes. The concept dates to the 1950s, and was briefly advocated by Hans Bethe during the 1970s, but largely remained unexplored until a revival of interest in 2009, due to delays in the realization of pure fusion. When a sustained nuclear fusion power plant is built, it has the potential to be capable of extracting all the fission energy that remains in spent fission fuel, reducing the volume of nuclear waste by orders of magnitude, and more importantly, eliminating all actinides present in the spent fuel, substances which cause security concerns.\n\nNuclear fusion reactions have the potential to be safer and generate less radioactive waste than fission. \nThese reactions appear potentially viable, though technically quite difficult and have yet to be created on a scale that could be used in a functional power plant. \nFusion power has been under theoretical and experimental investigation since the 1950s.\n\nSeveral experimental nuclear fusion reactors and facilities exist. \nThe largest and most ambitious international nuclear fusion project currently in progress is ITER, a large tokamak under construction in France.\nITER is planned to pave the way for commercial fusion power by demonstrating self-sustained nuclear fusion reactions with positive energy gain. \nConstruction of the ITER facility began in 2007, but the project has run into many delays and budget overruns. \nThe facility is now not expected to begin operations until the year 2027 – 11 years after initially anticipated. A follow on commercial nuclear fusion power station, DEMO, has been proposed. There are also suggestions for a power plant based upon a different fusion approach, that of an inertial fusion power plant.\n\nFusion powered electricity generation was initially believed to be readily achievable, as fission-electric power had been. However, the extreme requirements for continuous reactions and plasma containment led to projections being extended by several decades. In 2010, more than 60 years after the first attempts, commercial power production was still believed to be unlikely before 2050.\n\n\n\n"}
{"id": "34059625", "url": "https://en.wikipedia.org/wiki?curid=34059625", "title": "PetroSkills", "text": "PetroSkills\n\nPetroSkills is a competency based training provider for the Oil and Gas Industry that offers training in disciplines that range from upstream, downstream, to occupational safety and health. PetroSkills was founded in 1963 and is based out of Tulsa, Oklahoma.\n\nIn 2001 PetroSkills began the PetroSkills Alliance by partnering with BP and Shell to involve large oil and gas companies directly into the design of course material, instructor review, and competency analysis.\n\nIn 2008 PetroSkills expanded their number of courses through the acquisition of PetrEX International, Inc.\n\nIn 2010 PetroSkills was acquired by BV Investment Partners. Previous owners, Ford Brett and Dennis Wing retained a minority ownership position during the acquisition. BV partner Justin Harrison was quoted as saying, \"We share management's view that long-term demand drivers for oil and gas resources, the increasing technological complexity of finding and extracting oil and gas, and a demographic shift in the industry's workforce composition will drive a growing and sustainable need for quality technical training and competency-based solutions.\"\nAlso in 2010 PetroSkills partnered with the Southern Alberta Institute of Technology and began providing training to operations and maintenance technicians. This partnership was a \"response to the bigger crew change that will be occurring across operator ranks over the next decade. The demographic changes impacting the engineering ranks within E&P represent a small percentage of the numbers that will be impacted in the operations and maintenance ranks.” With the introduction of PetroSkills O&M, a specific focus was placed on both operations and maintenance (O&M) technicians employed in upstream, midstream and downstream roles, PetroSkills O&M offers companies training through both instructor-led training and e-learning content.\n\nIn 2011 PetroSkills opened its own 325,000 square foot Conference Center on January 14. The center is located in Katy, Texas.\n\nAlso in 2011 PetroSkills launched PetroCore, a browser-based application offering technical professionals in the oil and gas industry anytime, online access to PetroSkills course materials and other technical knowledge resources.\n\nIn 2012 PetroSkills opened a training facility in Bahrain.\n\nAt the end of 2012, PetroSkills announced that it acquired John M. Campbell & Co., a training provider that's focus was heavily on the facilities side of the Oil and Gas industry.\n"}
{"id": "387440", "url": "https://en.wikipedia.org/wiki?curid=387440", "title": "Physical oceanography", "text": "Physical oceanography\n\nPhysical oceanography is the study of physical conditions and physical processes within the ocean, especially the motions and physical properties of ocean waters.\n\nPhysical oceanography is one of several sub-domains into which oceanography is divided. Others include biological, chemical and geological oceanography.\n\nPhysical oceanography may be subdivided into \"descriptive\" and \"dynamical\" physical oceanography.\n\nDescriptive physical oceanography seeks to research the ocean through observations and complex numerical models, which describe the fluid motions as precisely as possible.\n\nDynamical physical oceanography focuses primarily upon the processes that govern the motion of fluids with emphasis upon theoretical research and numerical models. These are part of the large field of Geophysical Fluid Dynamics (GFD) that is shared together with meteorology. GFD is a sub field of Fluid dynamics describing flows occurring on spatial and temporal scales that are greatly influenced by the Coriolis force.\n\nRoughly 97% of the planet's water is in its oceans, and the oceans are the source of the vast majority of water vapor that condenses in the atmosphere and falls as rain or snow on the continents. The tremendous heat capacity of the oceans moderates the planet's climate, and its absorption of various gases affects the composition of the atmosphere. The ocean's influence extends even to the composition of volcanic rocks through seafloor metamorphism, as well as to that of volcanic gases and magmas created at subduction zones.\n\nThe oceans are far deeper than the continents are tall; examination of the Earth's hypsographic curve shows that the average elevation of Earth's landmasses is only , while the ocean's average depth is . Though this apparent discrepancy is great, for both land and sea, the respective extremes such as mountains and trenches are rare.\n\nBecause the vast majority of the world ocean's volume is deep water, the mean temperature of seawater is low; roughly 75% of the ocean's volume has a temperature from 0° – 5 °C (Pinet 1996). The same percentage falls in a salinity range between 34–35 ppt (3.4–3.5%) (Pinet 1996). There is still quite a bit of variation, however. Surface temperatures can range from below freezing near the poles to 35 °C in restricted tropical seas, while salinity can vary from 10 to 41 ppt (1.0–4.1%).\n\nThe vertical structure of the temperature can be divided into three basic layers, a surface mixed layer, where gradients are low, a thermocline where gradients are high, and a poorly stratified abyss.\n\nIn terms of temperature, the ocean's layers are highly latitude-dependent; the thermocline is pronounced in the tropics, but nonexistent in polar waters (Marshak 2001). The halocline usually lies near the surface, where evaporation raises salinity in the tropics, or meltwater dilutes it in polar regions. These variations of salinity and temperature with depth change the density of the seawater, creating the pycnocline.\nEnergy for the ocean circulation (and for the atmospheric circulation) comes from solar radiation and gravitational energy from the sun and moon. The amount of sunlight absorbed at the surface varies strongly with latitude, being greater at the equator than at the poles, and this engenders fluid motion in both the atmosphere and ocean that acts to redistribute heat from the equator towards the poles, thereby reducing the temperature gradients that would exist in the absence of fluid motion. Perhaps three quarters of this heat is carried in the atmosphere; the rest is carried in the ocean.\n\nThe atmosphere is heated from below, which leads to convection, the largest expression of which is the Hadley circulation. By contrast the ocean is heated from above, which tends to suppress convection. Instead ocean deep water is formed in polar regions where cold salty waters sink in fairly restricted areas. This is the beginning of the thermohaline circulation.\n\nOceanic currents are largely driven by the surface wind stress; hence the large-scale atmospheric circulation is important to understanding the ocean circulation. The Hadley circulation leads to Easterly winds in the tropics and Westerlies in mid-latitudes. This leads to slow equatorward flow throughout most of a subtropical ocean basin (the Sverdrup balance). The return flow occurs in an intense, narrow, poleward western boundary current. Like the atmosphere, the ocean is far wider than it is deep, and hence horizontal motion is in general much faster than vertical motion. In the southern hemisphere there is a continuous belt of ocean, and hence the mid-latitude westerlies force the strong Antarctic Circumpolar Current. In the northern hemisphere the land masses prevent this and the ocean circulation is broken into smaller gyres in the Atlantic and Pacific basins.\n\nThe Coriolis effect results in a deflection of fluid flows (to the right in the Northern Hemisphere and left in the Southern Hemisphere). This has profound effects on the flow of the oceans. In particular it means the flow goes \"around\" high and low pressure systems, permitting them to persist for long periods of time. As a result, tiny variations in pressure can produce measurable currents. A slope of one part in one million in sea surface height, for example, will result in a current of 10 cm/s at mid-latitudes. The fact that the Coriolis effect is largest at the poles and weak at the equator results in sharp, relatively steady western boundary currents which are absent on eastern boundaries. Also see secondary circulation effects.\n\nEkman transport results in the net transport of surface water 90 degrees to the right of the wind in the Northern Hemisphere, and 90 degrees to the left of the wind in the Southern Hemisphere. As the wind blows across the surface of the ocean, it \"grabs\" onto a thin layer of the surface water. In turn, that thin sheet of water transfers motion energy to the thin layer of water under it, and so on. However, because of the Coriolis Effect, the direction of travel of the layers of water slowly move farther and farther to the right as they get deeper in the Northern Hemisphere, and to the left in the Southern Hemisphere. In most cases, the very bottom layer of water affected by the wind is at a depth of 100 m – 150 m and is traveling about 180 degrees, completely opposite of the direction that the wind is blowing. Overall, the net transport of water would be 90 degrees from the original direction of the wind.\n\nLangmuir circulation results in the occurrence of thin, visible stripes, called windrows on the surface of the ocean parallel to the direction that the wind is blowing. If the wind is blowing with more than 3 m s, it can create parallel windrows alternating upwelling and downwelling about 5–300 m apart. These windrows are created by adjacent ovular water cells (extending to about deep) alternating rotating clockwise and counterclockwise. In the convergence zones debris, foam and seaweed accumulates, while at the divergence zones plankton are caught and carried to the surface. If there are many plankton in the divergence zone fish are often attracted to feed on them.\n\nAt the ocean-atmosphere interface, the ocean and atmosphere exchange fluxes of heat, moisture and momentum.\n\nThe important heat terms at the surface are the sensible heat flux, the latent heat flux, the incoming solar radiation and the balance of long-wave (infrared) radiation. In general, the tropical oceans will tend to show a net gain of heat, and the polar oceans a net loss, the result of a net transfer of energy polewards in the oceans.\n\nThe oceans' large heat capacity moderates the climate of areas adjacent to the oceans, leading to a maritime climate at such locations. This can be a result of heat storage in summer and release in winter; or of transport of heat from warmer locations: a particularly notable example of this is Western Europe, which is heated at least in part by the north atlantic drift.\n\nSurface winds tend to be of order meters per second; ocean currents of order centimeters per second. Hence from the point of view of the atmosphere, the ocean can be considered effectively stationary; from the point of view of the ocean, the atmosphere imposes a significant wind stress on its surface, and this forces large-scale currents in the ocean.\n\nThrough the wind stress, the wind generates ocean surface waves; the longer waves have a phase velocity tending towards the wind speed. Momentum of the surface winds is transferred into the energy flux by the ocean surface waves. The increased roughness of the ocean surface, by the presence of the waves, changes the wind near the surface.\n\nThe ocean can gain moisture from rainfall, or lose it through evaporation. Evaporative loss leaves the ocean saltier; the Mediterranean and Persian Gulf for example have strong evaporative loss; the resulting plume of dense salty water may be traced through the Straits of Gibraltar into the Atlantic Ocean. At one time, it was believed that evaporation/precipitation was a major driver of ocean currents; it is now known to be only a very minor factor.\n\n\nA Kelvin wave is any progressive wave that is channeled between two boundaries or opposing forces (usually between the Coriolis force and a coastline or the equator). There are two types, coastal and equatorial. Kelvin waves are gravity driven and non-dispersive. This means that Kelvin waves can retain their shape and direction over long periods of time. They are usually created by a sudden shift in the wind, such as the change of the trade winds at the beginning of the El Niño-Southern Oscillation.\n\nCoastal Kelvin waves follow shorelines and will always propagate in a counterclockwise direction in the Northern hemisphere (with the shoreline to the right of the direction of travel) and clockwise in the Southern hemisphere.\n\nEquatorial Kelvin waves propagate to the east in the Northern and Southern hemispheres, using the equator as a guide.\n\nKelvin waves are known to have very high speeds, typically around 2–3 meters per second. They have wavelengths of thousands of kilometers and amplitudes in the tens of meters.\n\n\nRossby waves, or planetary waves are huge, slow waves generated in the troposphere by temperature differences between the ocean and the continents. Their major restoring force is the change in Coriolis force with latitude. Their wave amplitudes are usually in the tens of meters and very large wavelengths. They are usually found at low or mid latitudes.\n\nThere are two types of Rossby waves, barotropic and baroclinic. Barotropic Rossby waves have the highest speeds and do not vary vertically. Baroclinic Rossby waves are much slower.\n\nThe special identifying feature of Rossby waves is that the phase velocity of each individual wave always has a westward component, but the group velocity can be in any direction. Usually the shorter Rossby waves have an eastward group velocity and the longer ones have a westward group velocity.\n\nThe interaction of ocean circulation, which serves as a type of heat pump, and biological effects such as the concentration of carbon dioxide can result in global climate changes on a time scale of decades. Known climate oscillations resulting from these interactions, include the Pacific decadal oscillation, North Atlantic oscillation, and Arctic oscillation. The oceanic process of thermohaline circulation is a significant component of heat redistribution across the globe, and changes in this circulation can have major impacts upon the climate.\n\n and \n\nThis is a coupled ocean/atmosphere wave that circles the Southern Ocean about every eight years. Since it is a wave-2 phenomenon (there are two peaks and two troughs in a latitude circle) at each fixed point in space a signal with a period of four years is seen. The wave moves eastward in the direction of the Antarctic Circumpolar Current.\n\nAmong the most important ocean currents are the:\n\nThe ocean body surrounding the Antarctic is currently the only continuous body of water where there is a wide latitude band of open water. It interconnects the Atlantic, Pacific and Indian oceans, and provide an uninterrupted stretch for the prevailing westerly winds to significantly increase wave amplitudes. It is generally accepted that these prevailing winds are primarily responsible for the circumpolar current transport. This current is now thought to vary with time, possibly in an oscillatory manner.\n\nIn the Norwegian Sea evaporative cooling is predominant, and the sinking water mass, the North Atlantic Deep Water (NADW), fills the basin and spills southwards through crevasses in the submarine sills that connect Greenland, Iceland and Britain. It then flows along the western boundary of the Atlantic with some part of the flow moving eastward along the equator and then poleward into the ocean basins. The NADW is entrained into the Circumpolar Current, and can be traced into the Indian and Pacific basins. Flow from the Arctic Ocean Basin into the Pacific, however, is blocked by the narrow shallows of the Bering Strait.\n\nAlso see marine geology about that explores the geology of the ocean floor including plate tectonics that create deep ocean trenches.\n\nAn idealised subtropical ocean basin forced by winds circling around a high pressure (anticyclonic) systems such as the Azores-Bermuda high develops a gyre circulation with slow steady flows towards the equator in the interior. As discussed by Henry Stommel, these flows are balanced in the region of the western boundary, where a thin fast polewards flow called a western boundary current develops. Flow in the real ocean is more complex, but the Gulf stream, Agulhas and Kuroshio are examples of such currents. They are narrow (approximately 100 km across) and fast (approximately 1.5 m/s).\n\nEquatorwards western boundary currents occur in tropical and polar locations, e.g. the East Greenland and Labrador currents, in the Atlantic and the Oyashio. They are forced by winds circulation around low pressure (cyclonic).\n\nThe Gulf Stream, together with its northern extension, North Atlantic Current, is a powerful, warm, and swift Atlantic Ocean current that originates in the Gulf of Mexico, exits through the Strait of Florida, and follows the eastern coastlines of the United States and Newfoundland to the northeast before crossing the Atlantic Ocean.\n\nThe Kuroshio Current is an ocean current found in the western Pacific Ocean off the east coast of Taiwan and flowing northeastward past Japan, where it merges with the easterly drift of the North Pacific Current. It is analogous to the Gulf Stream in the Atlantic Ocean, transporting warm, tropical water northward towards the polar region.\n\nOcean heat flux is a turbulent and complex system which utilizes atmospheric measurement techniques such as eddy covariance to measure the rate of heat transfer expressed in the unit of joules or watts per second. Heat flux is the difference in temperature between two points through which the heat passes. Most of the Earth's heat storage is within its seas with smaller fractions of the heat transfer in processes such as evaporation, radiation, diffusion, or absorption into the sea floor. The majority of the ocean heat flux is through advection or the movement of the ocean's currents. For example, the majority of the warm water movement in the south Atlantic is thought to have originated in the Indian Ocean. Another example of advection is the nonequatorial Pacific heating which results from subsurface processes related to atmospheric anticlines. Recent warming observations of Antarctic Bottom Water in the Southern Ocean is of concern to ocean scientists because bottom water changes will effect currents, nutrients, and biota elsewhere. The international awareness of global warming has focused scientific research on this topic since the 1988 creation of the Intergovernmental Panel on Climate Change. Improved ocean observation, instrumentation, theory, and funding has increased scientific reporting on regional and global issues related to heat.\n\nTide gauges and satellite altimetry suggest an increase in sea level of 1.5–3 mm/yr over the past 100 years.\n\nThe IPCC predicts that by 2081-2100, global warming will lead to a sea level rise of 260 to 820 mm.\n\nThe rise and fall of the oceans due to tidal effects is a key influence upon the coastal areas. Ocean tides on the planet Earth are created by the gravitational effects of the Sun and Moon. The tides produced by these two bodies are roughly comparable in magnitude, but the orbital motion of the Moon results in tidal patterns that vary over the course of a month.\n\nThe ebb and flow of the tides produce a cyclical current along the coast, and the strength of this current can be quite dramatic along narrow estuaries. Incoming tides can also produce a tidal bore along a river or narrow bay as the water flow against the current results in a wave on the surface.\n\n\"Tide and Current\" (Wyban 1992) clearly illustrates the impact of these natural cycles on the lifestyle and livelihood of Native Hawaiians tending coastal fishponds. \"Aia ke ola ka hana\" meaning . . . \"Life is in labor\".\n\n\"Tidal resonance\" occurs in the Bay of Fundy since the time it takes for a large wave to travel from the mouth of the bay to the opposite end, then reflect and travel back to the mouth of the bay coincides with the tidal rhythm producing the world's highest tides.\n\nAs the surface tide oscillates over topography, such as submerged seamounts or ridges, it generates internal waves at the tidal frequency, which are known as internal tides.\n\nA series of surface waves can be generated due to large-scale displacement of the ocean water. These can be caused by sub-marine landslides, seafloor deformations due to earthquakes, or the impact of a large meteorite.\n\nThe waves can travel with a velocity of up to several hundred km/hour across the ocean surface, but in mid-ocean they are barely detectable with wavelengths spanning hundreds of kilometers.\n\nTsunamis, originally called tidal waves, were renamed because they are not related to the tides. They are regarded as shallow-water waves, or waves in water with a depth less than 1/20 their wavelength. Tsunamis have very large periods, high speeds, and great wave heights.\n\nThe primary impact of these waves is along the coastal shoreline, as large amounts of ocean water are cyclically propelled inland and then drawn out to sea. This can result in significant modifications to the coastline regions where the waves strike with sufficient energy.\n\nThe tsunami that occurred in Lituya Bay, Alaska on July 9, 1958 was high and is the biggest tsunami ever measured, almost taller than the Sears Tower in Chicago and about taller than the former World Trade Center in New York.\n\nThe wind generates ocean surface waves, which have a large impact on offshore structures, ships, coastal erosion and sedimentation, as well as harbours. After their generation by the wind, ocean surface waves can travel (as swell) over long distances.\n\n\n"}
{"id": "38202329", "url": "https://en.wikipedia.org/wiki?curid=38202329", "title": "Pi Pegasi", "text": "Pi Pegasi\n\nThe Bayer designation Pi Pegasi (π Peg / π Pegasi) is shared by two stars, in the constellation Pegasus:\n"}
{"id": "49530543", "url": "https://en.wikipedia.org/wiki?curid=49530543", "title": "Rain (short story)", "text": "Rain (short story)\n\n\"Rain\" is a short story by the British writer W. Somerset Maugham. It was originally published as \"Miss Thompson\" in the April 1921 issue of the American literary magazine \"The Smart Set\".\n\nThe story is set on a Pacific island: a missionary's determination to reform a prostitute leads to tragedy.\n\nIn December 1916 during a tour of the Pacific, Maugham and his secretary/companion Gerald Haxton, on the steamer \"Sonoma\", visited Pago Pago, the capital of American Samoa. Delayed by a quarantine inspection, Maugham, Haxton and others took lodgings there. Other passengers on the \"Sonoma\" included a \"Miss Thompson\", and a medical missionary and his wife, who were models for the characters in \"Rain\".\n\nNear Pago Pago is Rainmaker Mountain, which gives Pago Pago Harbor an unusually high rainfall.\n\nOn the way to Apia in the Pacific, a ship stops at Pago Pago. The passengers include Dr Macphail and his wife, and Davidson, a missionary, and his wife. (The story is seen from Macphail's point of view.) Because of an epidemic of measles (a serious disease for local people) on the island, the ship cannot leave until it is sure none of the crew are infected. The Macphails and the Davidsons find lodgings with Horn, a trader on the front. For most of their stay there is heavy rain which they find oppresive. Macphail hears from Davidson and his wife about the severity of the missionary in his work.\n\nAlso staying there from the ship is Miss Thompson. From her room is heard the sound of a gramophone and men's voices. They remember she came on board at Honolulu and is presumably from Iwelei, the red-light district there. Davidson is determined to stop her activities, and tries to get Horn to stop her having visitors. Macphail feels that Davidson is\nmysteriously at work. He had an impression that he was weaving a net around the woman, carefully, systematically, and suddenly, when everything was ready would pull the strings tight.\n\nDavidson sees the governor of the island and gets him to put Miss Thompson on the next ship, which goes to San Francisco. The governor, aware that the missionaries have influence, does not change his decision when Macphail visits him. Miss Thompson is distraught, as she will be sent to the penitentiary if she returns to San Francisco. Davidson is often with Miss Thompson, whom he now familiarly calls Sadie. Her personality changes and she becomes repentant, it appears. Davidson says to the Macphails, \"It's a true rebirth. Her soul, which was black as night, is now pure and white... All day I pray with her...\"\n\nA few days later Davidson's body is found on the beach; he has cut his throat with a razor. Macphail does not understand what happened until, returning to his lodgings, he finds Sadie Thompson has changed suddenly back to \"the flaunting quean they had known at first\". She breaks into \"a loud, jeering laugh\" at Mrs Davidson and the Macphails, and says to Macphail, \"You men! You filthy, dirty pigs! You're all the same, all of you.\"\n\nThe story has been the basis of a number of adaptations.\n\n\n"}
{"id": "6894628", "url": "https://en.wikipedia.org/wiki?curid=6894628", "title": "Red Triangle (Pacific Ocean)", "text": "Red Triangle (Pacific Ocean)\n\nThe Red Triangle is the colloquial name of a roughly triangle-shaped region off the coast of northern California, extending from Bodega Bay, north of San Francisco, out slightly beyond the Farallon Islands, and down to the Big Sur region, south of Monterey. The area has a very large population of marine mammals, such as elephant seals, harbor seals, sea otters and sea lions, which are favored prey of great white sharks. Around thirty-eight percent of recorded great white shark attacks on humans in the United States have occurred within the Red Triangle—eleven percent of the worldwide total. The area encompasses the beaches of the heavily populated San Francisco Bay Area, and many people enjoy surfing, windsurfing, swimming and diving in these waters.\n\nThe Red Triangle is defined by its vertices: the northern vertex is Bodega Bay (Bodega Head), the western vertex is the Farallones, and the southern vertex is Big Sur or Monterey Bay. The movement of sharks and other large marine animals in this region were studied starting in 1999 under the Tagging of Pacific Predators (TOPP) program, an international collaboration. TOPP found that white sharks in the Red Triangle are genetically distinct from others in the Pacific Ocean, such as those found off South Africa and Australia.\n\nThe population of northeastern Pacific white sharks tend to congregate along the north/central California coast each fall, then leave the area in December for the deep ocean approximately halfway between California and Hawaii in a region that TOPP researchers have called the White Shark Café. Female whites tend to visit the Gulf of the Farallones every two years, which researchers believe to be based on the shark reproductive cycle.\n\n"}
{"id": "3001820", "url": "https://en.wikipedia.org/wiki?curid=3001820", "title": "Rootless cone", "text": "Rootless cone\n\nA rootless cone, also formerly called a pseudocrater, is a volcanic landform which resembles a true volcanic crater, but differs in that it is not an actual vent from which lava has erupted. They are characterised by the absence of any magma conduit which connects below the surface of a planet.\n\nRootless cones are formed by steam explosions as flowing hot lava crosses over a wet surface, such as a swamp, a lake, or a pond. The explosive gases break through the lava surface in a manner similar to a phreatic eruption, and the tephra builds up crater-like forms which can appear very similar to real volcanic craters.\n\nWell known examples are found in Iceland like the craters in the lake Mývatn (\"Skútustaðagígar\"), the Rauðhólar in the region of the capital city Reykjavík or the \"Landbrotshólar located in South-Iceland in the\" Katla UNESCO Global Geopark in the vicinity of Kirkjubæjarklaustur. Rootless cones have also been discovered in the Athabasca Valles region of Mars, where lava flows superheated groundwater in the underlying rocks.\n\nVolcanologists witnessed the formation of a rootless cone for the first time in history during a steam explosion in connection with the first eruption of Eyjafjallajökull in March 2010.\n\n\n"}
{"id": "16669729", "url": "https://en.wikipedia.org/wiki?curid=16669729", "title": "Sam Campbell (writer)", "text": "Sam Campbell (writer)\n\nSamuel Arthur Campbell (1895 - 1962) was born August 1, 1895 in Watseka, Iroquois County, Illinois. He was the youngest of two children born to Arthur J. and Katherine \"Kittie\" (née Lyman) Campbell.\n\nSam Campbell was many things including a writer, lecturer, photographer, and diligent student of nature. He studied wild animals from his home, which he called \"the Sanctuary of Wegimind\", and during his various travels.\n\nCampbell died April 13, 1962 in Barrington, Illinois.\n\n\n"}
{"id": "58881140", "url": "https://en.wikipedia.org/wiki?curid=58881140", "title": "Sector collapse", "text": "Sector collapse\n\nA sector collapse is a collapse of a portion of a volcano and often creates lateral blasts. It can be caused by the intrusion of new magma, phreatic eruption or an earthquake. Many volcanoes are prone to sector collapses.\n\nSector collapses are generally one of the most hazardous volcanic events.\n"}
{"id": "2199744", "url": "https://en.wikipedia.org/wiki?curid=2199744", "title": "Statherian", "text": "Statherian\n\nThe Statherian Period (; , meaning \"stable, firm\") is the final geologic period in the Paleoproterozoic Era and lasted from Mya to Mya (million years ago). Instead of being based on stratigraphy, these dates are defined chronometrically.\n\nThe period was characterized on most continents by either new platforms or final cratonization of fold belts.\n\nBy the beginning of the Statherian, the supercontinent Columbia had assembled.\n\n"}
{"id": "33736456", "url": "https://en.wikipedia.org/wiki?curid=33736456", "title": "Tectonic subsidence", "text": "Tectonic subsidence\n\nTectonic subsidence is the sinking of the Earth's crust on a large scale, relative to crustal-scale features or the geoid. The movement of crustal plates and accommodation spaces created by faulting create subsidence on a large scale in a variety of environments, including passive margins, aulacogens, fore-arc basins, foreland basins, intercontinental basins and pull-apart basins. Three mechanisms are common in the tectonic environments in which subsidence occurs: extension, cooling and loading.\n\nWhere the lithosphere undergoes horizontal extension at a normal fault or rifting center, the crust will stretch until faulting occurs, either by a system of normal faults (which creates horsts and grabens) or by a system of listric faults. These fault systems allow the region to stretch, while also decreasing its thickness. A thinner crust subsides relative to thicker, undeformed crust.\n\nLithospheric stretching/thinning during rifting results in regional necking of the lithosphere (the elevation of the upper surface decreases while the lower boundary rises). The underlying asthenosphere passively rises to replace the thinned mantle lithosphere. Subsequently, after the rifting/stretching period ends, this shallow asthenosphere gradually cools back into mantle lithosphere over a period of many tens of millions of years. Because mantle lithosphere is denser than asthenospheric mantle, this cooling causes subsidence. This gradual subsidence due to cooling is known as \"thermal subsidence\".\n\nThe adding of weight by sedimentation from erosion or orogenic processes, or loading, causes crustal depression and subsidence. Sediments accumulate at the lowest elevation possible, in accommodation spaces. The rate and magnitude of sedimentation controls the rate at which subsidence occurs. By contrast, in orogenic processes, mountain building creates a large load on the Earth's crust, causing flexural depressions in adjacent lithospheric crust.\n\nThese settings are not tectonically active, but still experience large-scale subsidence because of tectonic features of the crust.\n\nIntracontinental basins are large areal depressions that are tectonically inactive and not near any plate boundaries. Multiple hypotheses have been introduced to explain this slow, long-lived subsidence: long-term cooling since the breakup of Pangea, interaction of deformation around the edge of the basin and deep earth dynamics.\n\nTectonic subsidence can occur in these environments as the crust pulls apart.\n\nSuccessful rifting creates a spreading center like a mid-ocean ridge, which moves progressively further from coastlines as oceanic lithosphere is produced. Due to this initial phase of rifting, the crust in a passive margin is thinner than adjacent crust and subsides to create an accommodation space. Accumulation of non-marine sediment forms alluvial fans in the accommodation space. As rifting proceeds, listric fault systems form and further subsidence occurs, resulting in the creation of an ocean basin. After the cessation of rifting, cooling causes the crust to further subside, and loading with sediment will cause further tectonic subsidence.\n\nAulacogens occur at failed rifts, where continental crust does not completely split. Similar to the lithospheric heating that occurs during the formation of passive margins, subsidence occurs due to heated lithosphere sagging as spreading occurs. Once tensional forces cease, subsidence continues due to cooling.\n\nTectonic subsidence can occur in these settings as the plates collide against or under each other.\n\nPull-apart basins have short-lived subsidence that forms from transtensional strike-slip faults. Moderate strike-slip faults create extensional releasing bends and opposing walls pull apart from each other. Normal faults occur, inducing small scale subsidence in the area, which ceases once the fault stops propagating. Cooling occurs after the fault fails to propagate further following the crustal thinning via normal faulting.\n\nForearc basins form in subduction zones as sedimentary material is scraped off the subducting oceanic plate, forming an accretionary prism between the subducting oceanic lithosphere and the overriding continental plate. Between this wedge and the associated volcanic arc is a zone of depression in the sea floor. Extensional faulting due to relative motion between the accretionary prism and the volcanic arc may occur. Abnormal cooling effects due to the cold, water-laden downgoing plate as well as crustal thinning due to underplating may also be at work.\n\nForeland basins are flexural depressions created by large fold thrust sheets that form toward the undeformed continental crust. They form as an isostatic response to an orogenic load. Basin growth is controlled by load migration and corresponding sedimentation rates. The broader a basin is, the greater the subsidence is in magnitude. Subsidence is increased in the adjacent basin as the load migrates further into the foreland, causing subsidence. Sediment eroded fromm the fold thrust is deposited in the basin, with thickening layers toward the thrust belt and thinning layers away from the thrust belt; this feature is called differential subsidence.\n"}
{"id": "18509998", "url": "https://en.wikipedia.org/wiki?curid=18509998", "title": "The Natural History of Iceland", "text": "The Natural History of Iceland\n\nThe Natural History of Iceland () is a natural history of Iceland by Danish lawyer Niels Horrebow. It was first published in Danish in 1752 with an English translation in 1758.\n\nThe book was intended to correct errors in past natural histories of Iceland, particularly the work of Hamburg mayor Johann Anderson, who had written about the island without ever actually visiting it. Anderson had relied entirely on accounts from German and Dutch sea captains, but Horrebow lived in Iceland for two years, studying the animals, plants, weather, and geological features. He also made note of the cultural practices of the Icelandic people. Horrebow's resulting work was published in Danish in 1752, then translated into German (1753), Dutch (1754), English (1758) and French (1764).\n\n\"The Natural History of Iceland\" is often noted for its seventy-second chapter, \"Concerning Snakes\", which, in its English translation, consists solely of one sentence:\n\nSeveral works of English literature make light of this brief passage. For example, James Boswell's \"Life of Johnson\" (1791) relates how Samuel Johnson bragged to a friend that he could recite the chapter in its entirety.\nAnd William Morris' utopian novel \"News From Nowhere\" (1890) contains a short chapter called \"Concerning Politics\", in which a resident of \"Nowhere\" tells the narrator, \"We are very well off as to politics,—because we have none. If you ever make a book out of this conversation, put this in a chapter by itself, after the model of old Horrebow's Snakes in Iceland.\"\n\nThe original Danish version of the chapter on snakes was actually a full paragraph, rather than just one sentence. It was a direct response to a paragraph in Johann Anderson's book, which claimed that snakes could not survive the cold of Iceland. Horrebow's full chapter was translated into English in an 1870 issue of \"Notes and Queries\":\n\nAlthough the contributor to \"Notes and Queries\" remarked that \"Horrebow's chapter is ... not so ridiculous as generally supposed\", the earlier English translation of the chapter is still much better-known. The phrase \"snakes in Iceland\" is listed in the \"Oxford English Dictionary\", where it is traced to the 1758 translation and defined as \"something posited only to be dismissed as non-existent\".\n\n"}
{"id": "90242", "url": "https://en.wikipedia.org/wiki?curid=90242", "title": "Tlazōlteōtl", "text": "Tlazōlteōtl\n\nIn Aztec mythology, Tlazolteotl (or , , ) is a deity of vice, purification, steam baths, \nlust, midwives, filth, and a patroness of adulterers. She is known by three names, ('she who eats or filthy excrescence [sin]') and ('the death caused by lust'), and or (, Deity of Cotton), the latter of which refers to a quadripartite association of four sister deities.\n\nTlazolteotl played an important role in the confession of wrongdoing, through her priests.\n\n=Aztec religion=\nUnder the name of she was thought to be quadrupartite, composed of four sisters of different ages known by the names (the first born), (the younger sister, also ), (the middle sister, also ) and (the youngest sister). When conceived of as four individual deities, they were called or ; individually, they were deities of luxury.\n\nAccording to Aztec belief, it was Tlazolteotl who inspired vicious desires, and who likewise forgave and cleaned away sin. She was also thought to cause disease, especially STDs. It was said that Tlazolteotl and her companions would afflict people with disease if they indulged themselves in forbidden love. The uncleanliness was considered both on a physical and moral level, and could be cured by steam bath, a rite of purification, or calling upon the Tlazōltēteoh, the deities of love and desires.\n\nFor the Aztecs there were two main deities thought to preside over purification: Tezcatlipoca, because he was thought to be invisible and omnipresent, therefore seeing everything; and Tlazolteotl, the deity of lechery and unlawful love. It is said that when a man confessed before Tlazolteotl everything was revealed. Purification with Tlazolteotl would be done through a priest. One could only receive the \"mercy\" once in their life, which is why the practice was most common among the elderly.\n\nThe priest (\"tlapouhqui\") would be consulted by the penitent and would consult the 260-day ritual calendar (tonalpohualli) to determine the best day and time for the purification to take place. On the day of, he would listen to the sins confessed and then render judgment and penance, ranging from fasts to presentation of offerings and ritual song and dance, depending on the nature and the severity of the sin. \n\n was called \"Deity of Dirt\" () and \"Eater of Ordure\" (, 'she who eats dirt [sin]'), with her dual nature of deity of dirt and also of purification. Sins were symbolized by dirt. Her dirt-eating symbolized the ingestion of the sin, and in doing so purified it. She was depicted with ochre-colored symbols of divine excrement around her mouth and nose. In the Aztec language the word for sacred, , comes from , the buttocks, and religious rituals include offerings of \"liquid gold\" (urine) and \"divine excrement\", which Klein jocularly translated to English as \"holy shit\". Through this process, she helped create harmony in communities.\n\n was one of the primary Aztec deities celebrated in the festival of (meaning \"sweeping\") that was held September 2–21 to recognize the harvest season. The ceremonies conducted during this timeframe included ritual cleaning, sweeping, and repairing, as well as the casting of corn seed, dances, and military ceremonies.\n\n\n=Notes=\n\n"}
{"id": "469990", "url": "https://en.wikipedia.org/wiki?curid=469990", "title": "Whirlpool", "text": "Whirlpool\n\nA whirlpool is a body of rotating water produced by the meeting of opposing currents. The vast majority of whirlpools are not very powerful and very small whirlpools can be easily seen when a bath or a sink is draining. More powerful ones in seas or oceans may be termed maelstroms. \"Vortex\" is the proper term for any whirlpool that has a downdraft.\n\nIn oceans, in narrow straits with fast flowing water, whirlpools are normally caused by tides; there are few stories of large ships ever being sucked into such a maelstrom, although smaller craft are in danger. Smaller whirlpools also appear at the base of many waterfalls and can also be observed downstream from manmade structures such as weirs and dams. In the case of powerful waterfalls, like Niagara Falls, these whirlpools can be quite strong.\n\nThe Maelstrom of Saltstraumen is the Earth's strongest maelstrom, and is located close to the Arctic Circle, round the bay on the Highway 17, south-east of the city of Bodø, Norway. The strait at its narrowest is in width and water \"funnels\" through the channel four times a day. It is estimated that of water passes the narrow strait during this event. The water is creamy in colour and most turbulent during high tide, which is witnessed by thousands of tourists. It reaches speeds of , with mean speed of about . As navigation is dangerous in this strait only a small slot of time is available for large ships to pass through. Its impressive strength is caused by the world's strongest tide occurring in the same location during the new and full moon. A narrow channel of length connects the outer Saltfjord with its extension, the large Skjerstadfjord, causing a colossal tide which in turn produces the Saltstraumen maelstrom.\n\nMoskstraumen is an unusual system of whirlpools in the open seas in the Lofoten Islands off the Norwegian coast. It is the second strongest whirlpool in the world with flow currents reaching speeds as high as . It finds mention in several books and movies.\n\nThe Moskstraumen is formed by the combination of powerful semi-diurnal tides and the unusual shape of the seabed, with a shallow ridge between the Moskenesøya and Værøy islands which amplifies and whirls the tidal currents.\n\nThe fictional depictions of the Maelstrom by Edgar Allan Poe, Jules Verne, and Cixin Liu describe it as a gigantic circular vortex that reaches the bottom of the ocean, when in fact it is a set of currents and crosscurrents with a rate of . Poe described this phenomenon in his short story \"A Descent into the Maelstrom,\" which in 1841 was the first to use the word \"maelstrom\" in the English language; in this story related to the Lofoten Maelstrom, two fishermen are swallowed by the maelstrom while one survives miraculously.\n\nThe Corryvreckan is a narrow strait between the islands of Jura and Scarba, in Argyll and Bute, on the northern side of the Gulf of Corryvreckan, Scotland. It is the third-largest whirlpool in the world. Flood tides and inflow from the Firth of Lorne to the west can drive the waters of Corryvreckan to waves of over , and the roar of the resulting maelstrom, which reaches speeds of , can be heard away. Though it was initially classified as non-navigable by the British navy it was later categorized as \"extremely dangerous\".\n\nA documentary team from Scottish independent producers Northlight Productions once threw a mannequin into the Corryvreckan (\"the Hag\") with a life jacket and depth gauge. The mannequin was swallowed and spat up far down current with a depth gauge reading of with evidence of being dragged along the bottom for a great distance.\n\nOld Sow whirlpool is located between Deer Island, New Brunswick, Canada, and Moose Island, Eastport, Maine, USA. It is given the epithet \"pig-like\" as it makes a screeching noise when the vortex is at its full fury and reaches speeds of up to . The smaller whirlpools around this Old Sow are known as \"Piglets.\n\nThe Naruto whirlpools are located in the Naruto Strait near Awaji Island in Japan, which have speeds of .\n\nSkookumchuck Narrows is a tidal rapids that develops whirlpools, on the Sunshine Coast, Canada with current speeds exceeding .\n\nFrench Pass () is a narrow and treacherous stretch of water that separates D'Urville Island from the north end of the South Island of New Zealand. In 2000 a whirlpool there caught student divers, resulting in fatalities.\n\nThere was a short-lived whirlpool that sucked in a portion of the 1300 acre (~530 hectares) Lake Peigneur in Louisiana, United States after a drilling mishap in November 1980. This was not a naturally occurring whirlpool, but a man-made disaster caused by underwater drillers breaking through the roof of a salt mine. The lake then drained into the mine until the mine filled and the water levels equalized but the ten-foot deep lake was now 1,300 feet deep. This mishap resulted in destruction of five houses, loss of nineteen barges and eight tug boats, oil rigs, a mobile home, and most of a botanical garden. The adjacent settlement of Jefferson Island was reduced in area by 10%. A crater 0.5-mile (~1km) across was left behind. Nine of the barges which had sunk floated back.\n\nA more recent example of a man-made whirlpool that received significant media coverage was in early June 2015, when an intake vortex formed in Lake Texoma, on the Oklahoma–Texas border, near the floodgates of the dam that forms the lake. At the time of the whirlpool's formation, the lake was being drained after reaching its highest level ever. The Army Corps of Engineers, which operates the dam and lake, expected that the whirlpool would last until the lake reached normal seasonal levels by late July.\n\nPowerful whirlpools have killed unlucky seafarers, but their power tends to be exaggerated by laymen. There are virtually no stories of large ships ever being sucked into a whirlpool. Tales like those by Paul the Deacon, Edgar Allan Poe, and Jules Verne are entirely fictional.\n\nHowever, temporary whirlpools caused by major engineering disasters are capable of submerging large ships. A prominent example is the drilling disaster that occurred on November 20, 1980, in Lake Peigneur. A drilling platform, eleven barges, several trees, and multiple acres of the surrounding terrain were submerged by the resulting whirlpool. Days after the disaster, once the water pressure equalized, nine of the eleven sunken barges popped out of the whirlpool and refloated on the lake's surface.\n\nApart from Poe and Verne other literary source is of the 1500s, of Olaus Magnus, a Swedish Bishop, who had stated that the maelstrom which was more powerful than \"The Odyssey\" destroyed ships which sank to the bottom of the sea, and even whales were sucked in. Pytheas, the Greek historian, also mentioned that maelstroms swallowed ships and threw them up again.\n\nCharybdis in Greek mythology was later rationalized as a whirlpool, which sucked entire ships into its fold in the narrow coast of Sicily, a disaster faced by navigators.\n\nIn the 8th century, Paul the Deacon, who had lived among the Belgii, described tidal bores and the maelstrom for a Mediterranean audience unused to such violent tidal surges:\n\nThree of the most notable literary references to the Lofoten Maelstrom date from the nineteenth century. The first is the Edgar Allan Poe short story \"A Descent into the Maelström\" (1841). The second is \"20,000 Leagues Under the Sea\" (1870), the famous novel by Jules Verne. At the end of this novel, Captain Nemo seems to commit suicide, sending his \"Nautilus\" submarine into the Maelstrom (although in Verne's sequel Nemo and the Nautilus were seen to have survived). The \"Norway maelstrom\" is also mentioned in Herman Melville's \"Moby-Dick\".\n\nIn the 'Life of St Columba', the author, Adomnan of Iona', attributes to the saint miraculous knowledge of a particular bishop who ran into a whirlpool off the coast of Ireland. In Adomnan's narrative, he quotes Columba saying\n\nOne of the earliest uses in English of the Scandinavian word (\"malström\" or \"malstrøm\") was by Edgar Allan Poe in his short story \"A Descent into the Maelström\" (1841). In turn, the Nordic word is derived from the Dutch \"maelstrom\", modern spelling \"maalstroom\", from \"malen\" (\"to grind\") and \"stroom\" (\"stream\"), to form the meaning \"grinding current\" or literally \"mill-stream\", in the sense of milling (grinding) grain.\n\n\n\n"}
{"id": "6890781", "url": "https://en.wikipedia.org/wiki?curid=6890781", "title": "William J. Long", "text": "William J. Long\n\nWilliam Joseph Long (North Attleboro, Mass., 3 April 1867 – 1952) was an American writer, naturalist and minister. He lived and worked in Stamford, Connecticut as a minister of the First Congregationalist Church.\n\nAs a naturalist, he would leave Stamford every March, often with his son, Brian, and two daughters, Lois and Cesca, to travel to \"the wilderness\" of Maine. There they would stay until the first snows of October, although sometimes he would stay all winter. In the 1920s, he began spending his summers in Nova Scotia, claiming \"the wilderness is getting too crowded\".\n\nHe wrote of these wilderness experiences in the books \"Ways of Wood Folk\", \"Wilderness Ways\", \"Wood-folk Comedies\", \"Northern Trails\", \"Wood Folk at School\", and many others. His earlier books were illustrated by Charles Copeland; two later ones were illustrated by Charles Livingston Bull. Long believed that the best way to experience the wild was to plant yourself and sit for hours on end to let the wild \"come to you; and they will!\"\n\nBecause of the increased public interest in the natural world as a reaction to the Industrial Revolution, Rev. Long's books were finding a large audience and even being issued in schools under the title of The Wood Folk Series. However, his findings and observations clashed with the prevailing scientific wisdom of animal behavior, which believed animals behaved purely on instinct, and could not learn from experience: a bird builds a nest purely by instinct and is not taught the skills required. Rev. Long provided many examples, supposedly from his experience, to cast doubt on that prevailing wisdom, suggesting that in fact animals did learn, and each could become individuals within their species. Some of the more famous observations were that kingfishers would catch fish in a river and then drop them into small pools so their offspring could practice catching the same fish but in an easier environment. He also chronicled a woodcock that made a \"splint\" for its broken leg. He also wrote of foxes that rode on the backs of sheep to escape hunters and porcupines curling into balls and rolling down hills.\n\nAll this led to a belief that Rev. Long (and others) were anthropomorphizing animal behavior, blurring the lines between the animal world and humans. This came to a head when President Teddy Roosevelt's naturalist adviser, John Burroughs, accused Rev. Long (and others) of gross exaggeration, if not outright lies, regarding his books and the reflections of nature therein. Roosevelt wrote \"If he stated that he had seen a weasel kill a deer and then carry it to the top of a pine tree, I would not care how many affidavits he produced, because the feat would be mechanically impossible.\" Long thus found himself at the center of the nature fakers controversy of the early 1900s. Ultimately Burroughs claimed Rev. Long was trying to sell books to gullible readers with such lies and President Theodore Roosevelt himself had Rev. Long's books taken from all school libraries.\n\nThe local Stamford paper chronicled the feud with \"our dashing Rev. Long\". Rev. Long would counter that you cannot \"understand nature when you have a gun on your hip, ride on top of a wagon or horseback, and have a crowd of twenty with you,\" taking aim at Teddy Roosevelt's much publicized and photographed forays into nature. He went so far to state Roosevelt \"never met an animal he didn't kill.\" After a couple months back and forth in the Stamford and national papers, Rev. Long said that \"while obviously we cannot settle this through the media, I invite President Roosevelt anytime to Stamford to settle this like men.\"\n\nRoosevelt never accepted his invitation.\n\n\n\n\n\n"}
{"id": "244878", "url": "https://en.wikipedia.org/wiki?curid=244878", "title": "Winter storm", "text": "Winter storm\n\nA winter storm is an event in which varieties of precipitation are formed that only occur at low temperatures, such as snow or sleet, or a rainstorm where ground temperatures are low enough to allow ice to form (i.e. freezing rain). In temperate continental climates, these storms are not necessarily restricted to the winter season, but may occur in the late autumn and early spring as well. Very rarely, they may form in summer, though it would have to be an abnormally cold summer, such as the summer of 1816 in the Northeastern United States.\n\nSnowstorms are storms where large amounts of snow fall. of snow is enough to create serious disruptions to traffic and school transport (because of the difficulty to drive and maneuver the school buses on slick roads). This is particularly true in places where snowfall is not typical but heavy accumulating snowfalls can occur. In places where snowfall is typical, such small snowfalls are rarely disruptive, because of effective snow and ice removal by municipalities, increased use of four-wheel drive and snow tires, and drivers being more used to winter conditions. Snowfalls in excess of are usually universally disruptive.\n\nA massive snowstorm with strong winds and other conditions meeting certain criteria is known as a blizzard. A large number of heavy snowstorms, some of which were blizzards, occurred in the United States during 1888 and 1947 as well as the early and mid-1990s. The snowfall of 1947 exceeded with drifts and snow piles from plowing that reached and for months, temperatures did not rise high enough to melt the snow. The 1993 \"Superstorm\" manifested as a blizzard in most of the affected areas.\n\nLarge snowstorms could be quite dangerous: a snowstorm will make some unplowed roads impassable, and it is possible for automobiles to get stuck in the snow. Snowstorms exceeding especially in southern or generally warm climates will cave the roofs of some homes and cause the loss of electricity. Standing dead trees can also be brought down by the weight of the snow, especially if it is wet or very dense. Even a few inches of dry snow can form drifts many feet high under windy conditions.\n\nAccumulating snow can make driving motor vehicles very hazardous. Accumulation of snow on roadways reduces friction between tires and the pavement, which in turn lowers the maneuverability of a vehicle considerably. As a result, average driving speeds on public roads and highways are reduced by up to 40% while heavy snow is falling. Visibilities are reduced by falling snow, and this is further exacerbated by strong winds which are commonly associated with winter storms producing heavy snowfall. In extreme cases, this may lead to prolonged whiteout conditions in which visibilities are reduced to only a few feet due to falling or blowing snow. These hazards can manifest even after snowfall has ended when strong winds are present, as these winds will pick up and transport fallen snow back onto roadways and reduce visibilities in the process. This can even result in blizzard conditions if winds are strong enough. Heavy snowfall can immobilize a vehicle entirely, which may be deadly depending on how long it takes rescue crews to arrive. The clogging of a vehicle's tailpipe by snow may lead to carbon monoxide buildup inside the cabin.\n\nDepending on the temperature profile in the atmosphere, snow can be either \"wet\" or \"dry\". Dry snow, being lighter, is transported by wind more easily and accumulates more efficiently. Wet snow is heavier due to the increased water content. Significant accumulations of heavy wet snow can cause roof damage. It also requires considerably more energy to move and this can create health problems while shoveling when combined with the harsh weather conditions. Numerous deaths as a result of heart attacks can be attributed to snow removal. Accretion of wet snow to elevated surfaces occurs when snow is \"sticky\" enough which can cause extensive tree and power line damage in a manner similar to ice accretion during ice storms. Power can be lost for days during a major winter storm, and this usually means the loss of heating inside buildings. Other than the obvious risk of hypothermia due to cold exposure, another deadly element associated with snowstorms is carbon monoxide poisoning which can happen anytime combustion products from generators or heating appliances are not properly vented. Finally, partially or fully melted snow on roadways can refreeze when temperatures fall, creating black ice.\n\nA sudden rise in air temperature is can result in the rapid melting of snow. This can create flooding issues if the snowpack has sufficient water content, and this can be significantly exacerbated by heavy rainfall and by ice jams which may have formed on area rivers during prolonged subfreezing temperatures. The re-freezing of previously melted snow creates potholes in roadways. This process is accelerated when water is melted and re-frozen multiple times. Fog is also known to occur during rapid melting of snow.\n\nAvalanches and cornices occur when significant amounts of snow falls on mountains.\n\nHeavy showers of freezing rain are one of the most dangerous types of winter storm. They typically occur when a layer of warm air hovers over a region, but the ambient temperature a few meters above the ground is near or below , and the ground temperature is sub-freezing.\n\nWhile a snowstorm is somewhat manageable by the standards of the northern United States and Canada, a comparable ice storm can paralyze a region: driving becomes extremely hazardous, telephone and power lines are damaged, and crops may be ruined.\n\nNotable ice storms include an El Niño-related North American ice storm of 1998 that affected much of eastern Canada, including Montreal and Ottawa, as well as upstate New York and part of New England. Three million people lost power, some for as long as six weeks. One-third of the trees in Montreal's Mount Royal park were damaged, as well as a large proportion of the sugar-producing maple trees. The amount of economic damage caused by the storm has been estimated at $3 billion Canadian.\n\nThe Christmas Day Ice Storm of 2000 caused devastating electrical issues in parts of Arkansas, Oklahoma, and Texas. The city of Texarkana, Arkansas experienced the worst damage, at one point losing the ability to use telephones, power and running water. In some areas in Arkansas, Oklahoma, Texas and eventually Louisiana, over an inch of ice accumulated from the freezing rain.\n\nThe Ice Storm of December 2002 in North Carolina resulted in massive power loss throughout much of the state, and property damage due to falling trees. Except in the mountainous western part of the state, heavy snow and icy conditions are rare in North Carolina.\n\nThe Ice Storm of December 2005 was another severe winter storm producing extensive ice damage across a large portion of the Southern United States on December 14 to 16. It led to power outages and at least 7 deaths.\n\nIn January 2005, Kansas had been declared a major disaster zone by President George W. Bush after an ice storm caused nearly $39 million in damages to thirty-two counties. Federal funds were provided to the counties during January 4–6, 2005 to aid the recovery process.\n\nThe January 2009 Central Plains and Midwest ice storm was a crippling and historic ice storm. Most places struck by the storm, saw or more of ice accumulation, and a few inches of snow on top of it. This brought down power lines, causing some people to go without power for a few days, to a few weeks. In some cases, some didn't see power for a month or more. At the height of the storm, more than 2 million people were without power.\n\nIce crystals fall through a cloud of super-cooled droplets—minute cloud droplets that have fallen below freezing temperature but have not frozen. The ice crystal plows into the super-cooled droplets and they immediately freeze to it. This process forms \"graupel\", or snow pellets, as the droplet continues to accumulate on the crystal. The pellets bounce when they hit the ground.\n\nOut ahead of the passage of a warm front, falling snow may partially melt and refreeze into a frozen rain drop before it reaches the ground. These ice pellets are called \"sleet\" in most of the US. Because it is easily seen and does not accumulate ice, it is not as dangerous as freezing rain.\n\n\"Rime\" is a milky white accumulation of super-cooled cloud or fog droplets that freeze when they strike an object that has a temperature of , the freezing point of water. The process is called \"riming\" when super-cooled cloud droplets attach to ice crystals in the formation of graupel. Rime ice can pose a hazard to an airliner when it forms on a wing as an aircraft flies through a cloud of super-cooled droplets.\n\n"}
