{"id": "68554", "url": "https://en.wikipedia.org/wiki?curid=68554", "title": "Ablution in Christianity", "text": "Ablution in Christianity\n\nAblution, in religion, is a prescribed washing of part or all of the body of possessions, such as clothing or ceremonial objects, with the intent of purification or dedication. In Christianity, both baptism and footwashing are forms of ablution. In liturgical churches, ablution can refer to purifying fingers or vessels related to the Eucharist. In the New Testament, washing also occurs in reference to rites of Judaism part of the action of a healing by Jesus, the preparation of a body for burial, the washing of nets by fishermen, a person's personal washing of the face to appear in public, the cleansing of an injured person's wounds, Pontius Pilate's washing of his hands as a symbolic claim of innocence and foot washing, now partly a symbolic rite within the Church. According to the Gospel of Matthew, Pontius Pilate declared himself innocent of the blood of Jesus by washing his hands. This act of Pilate may not, however, have been borrowed from the custom of the Jews. The same practice was common among the Greeks and Romans.\n\nAccording to Christian tradition, the Pharisees carried the practice of ablution to great excess. The Gospel of Mark refers to their ceremonial ablutions: \"For the Pharisees…wash their hands 'oft'\" or, more accurately, \"with the fist\" (R.V., \"diligently\"); or, as Theophylact of Bulgaria explains it, \"up to the elbow,\" referring to the actual word used in the Greek New Testament, πυγμή \"pygmē\", which refers to the arm from the elbow to the tips of the fingers. In the Book of Acts, Paul and other men performed ablution before entering the Temple in Jerusalem: \"Then Paul took the men, and the next day purifying himself with them entered into the temple, to signify the accomplishment of the days of purification, until that an offering should be offered for every one of them.\" \n\nIn the Old Testament, ablution was considered a prerequisite to approaching God, whether by means of sacrifice, prayer, or entering a holy place. The Bible has many rituals of purification relating to menstruation, childbirth, sexual relations, nocturnal emission, unusual bodily fluids, skin disease, death, and animal sacrifices. The Ethiopian Orthodox Tewahedo Church prescribes several kinds of hand washing for example after leaving the latrine, lavatory or bathhouse, or before prayer, or after eating a meal. The women in the Ethiopian Orthodox Tewahedo Church are prohibited from entering the church temple during menses; and the men do not enter a church the day after they have had intercourse with their wives. \nChristianity has always placed a strong emphasis on hygiene, Despite the denunciation of the mixed bathing style of Roman pools by early Christian clergy, as well as the pagan custom of women naked bathing in front of men, this did not stop the Church from urging its followers to go to public baths for bathing, which contributed to hygiene and good health according to the Church Father, Clement of Alexandria. The Church also built public bathing facilities that were separate for both sexes near monasteries and pilgrimage sites; also, the popes situated baths within church basilicas and monasteries since the early Middle Ages. Pope Gregory the Great urged his followers on value of bathing as a bodily need. Contrary to popular belief bathing and sanitation were not lost in Europe with the collapse of the Roman Empire. Soapmaking first became an established trade during the so-called \"Dark Ages\". The Romans used scented oils (mostly from Egypt), among other alternatives. By the mid-19th century, the English urbanised middle classes had formed an ideology of cleanliness that ranked alongside typical Victorian concepts, such as Christianity, respectability and social progress. The Salvation Army has adopted movement of the deployment of the personal hygiene, and by providing personal hygiene products.\n\nThe Bible has many rituals of purification relating to menstruation, childbirth, sexual relations, nocturnal emission, unusual bodily fluids, skin disease, death, and animal sacrifices. \n\nThe Bible includes various regulations about bathing:\nA subsequent seven clean days are then required, culminating in a ritual and temple offering before the \"zav\" is clean of his malady:\n\nAnd also references to hand-washing:\n\nThe Mikveh in the Bible is a bath used for the purpose of ritual immersion. The word is employed in its broader sense but generally means a collection of water. Several biblical regulations specify that full immersion in water is required to regain ritual purity after ritually impure incidents have occurred. A person was required to be ritually pure in order to enter the Temple. In this context, \"purity\" and \"impurity\" are imperfect translations of the Hebrew \"tahara\" and \"tumah\", respectively, in that the negative connotation of the word impurity is not intended; rather being \"impure\" is indicative of being in a state in which certain things are prohibited until one has become \"pure\" again by immersion in a mikveh.\n\nAfter the destruction of the Temple, the mikveh's main uses remained as follows:\n\nTraditionally, Christianity adhered to the biblical regulation requiring the purification of women after childbirth; this practice, was adapted into a special ritual known as the churching of women, for which there exists liturgy in the Church of England's Book of Common Prayer, but its use is now rare in Western Christianity. The churching of women is still performed in a number of Eastern Christian churches (Eastern Orthodox, Oriental Orthodox and Eastern Catholic churches).\n\nThe Ethiopian Orthodox Tewahedo Church prescribes several kinds of hand washing for example after leaving the latrine, lavatory or bathhouse, or before prayer, or after eating a meal. The women in the Ethiopian Orthodox Tewahedo Church are prohibited from entering the church temple during menses; and the men do not enter a church the day after they have had intercourse with their wives.\n\nRoman Catholics, Eastern Orthodox and High church Anglicans are also traditionally required to regularly attend confession, as a form of ritual purification from sin, especially as preparation before receiving the Eucharist. For Catholics, this is required at least once a year and required for those who are guilty of unconfessed mortal sins.\n\nIn Reformed Christianity, ritual purity is achieved though the Confession of Sins, and Assurance of Forgiveness, and Sanctification. Through the power of the Holy Spirit, believers offer their whole being and labor as a 'living sacrifice'; and cleanliness becomes a way of life (See Romans 12:1, and John 13:5-10 (the Washing of the Feet).\n\nIn the Roman Rite, the celebrant washes his hands before vesting for Mass, but with another prayer (\"Da, Domine, virtutem\"). This is said privately in the vestry. He will then wash his hands again after the offertory—this is the ceremony that is known as the \"lavabo\" proper. This washing appears in both the Tridentine Mass, the 1962 edition of which is now an authorized extraordinary form of the Roman Rite), and in the Mass of Paul VI. The reason for this \"second\" washing of hands probably developed from the long ceremony of receiving the loaves and vessels of wine from the people at the offertory that was used in Rome. In the Gallican Rite the offerings were prepared before Mass began, as in the Eastern Liturgy of Preparation, so there was no long version of the offertory nor place for a lavabo before the Eucharistic Prayer. In the Middle Ages, the Roman Rite actually had two washing of hands, one before and one after the offertory. This first one has since disappeared, and the one which remains is the second.\n\nIn the Tridentine Mass and in the similar Anglo-Catholic Mass, the term \"ablutions\" refers to when the priest rinses his hands first in wine and then in water following the Communion. It is to be distinguished from the lavabo, when the celebrant washes his hands with water only, reciting the words of (KJV—in the Septuagint it is Psalm 25) at the offertory.\n\nIn the Mass of Paul VI and the Anglican Eucharist the priest does not normally use wine to wash his hands at the ablution, although this is permitted, but only water.\n\nIn the Eastern Orthodox and Greek-Catholic Churches, the term \"ablution\" refers to consuming the remainder of the Gifts (the Body and Blood of Christ) at the end of the Divine Liturgy. Holy Communion is always received in both Species (the Body and the Blood of Christ) not only by the clergy but also by the faithful. This is accomplished by placing the particles of the consecrated Lamb (bread) into the chalice, and distributing Communion to the faithful with a spoon. The portion which remains in the chalice afterwards must be consumed.\n\nThe ablutions will normally be performed by the deacon, but if no deacon is serving the priest will do them. After the Litany of Thanksgiving that follows Communion, the deacon will come into the sanctuary and kneel, placing his forehead on the Holy Table (Altar) and the priest will bless him to consume the Gifts, which is done at the Prothesis (Table of Oblation). First, using the liturgical spoon he will consume all of the Body and Blood of Christ which remain in the chalice. Then he will pour hot water on the diskos (paten), which is then poured into the chalice and consumed (this is to consume any particles that may remain on the diskos). Next the liturgical spear, spoon and chalice will be rinsed first with wine and then with hot water, which are then consumed. All of the sacred vessels are then wiped dry with a towel, wrapped in their cloth coverings and put away.\n\nBecause the ablutions necessarily require consuming the Holy Mysteries (the Body and Blood of Christ), a priest or deacon may only perform them after having fully prepared himself through fasting and the lengthy Preparation for Holy Communion.\n\nWhen a priest must take Holy Communion to the sick or homebound, if he has not prepared himself to receive the Holy Mysteries, he may ablute the chalice by pouring water into it and asking the one to whom he brought the Sacrament (or a Baptized child who because of their youth is not obliged to prepare for Communion) to consume the ablution.\n\nIf the Reserved Mysteries should become moldy, they must still be consumed in the same manner as the ablutions after Liturgy (normally, a fair amount of wine would be poured over them before consuming them, in order to soften and disinfect them). They should not be burned or buried. To prevent this, when the Mysteries are to be reserved for the sick, they should be thoroughly dried before being placed in the Tabernacle.\n\nIn Orthodox Christianity, there is also an ablution performed on the eighth day after Baptism. Immediately after being Baptized, every person, including an infant, is confirmed using the Mystery (Sacrament) of Chrismation. In the early church, the places where the person was anointed with Chrism were carefully bandaged, and were kept covered for eight days. During this period, the newly illumined (newly baptized person) would also wear his baptismal robe every day. At the end of the eight days, the priest would remove the bandages and baptismal garment and perform ablutions over him. While the bandaging no longer takes place, the ritual ablutions are still performed.\n\nThe newly illumined (newly baptized person) is brought back to the church by his Godparents for the ablutions. The priest stands him in the center of the church, in front of the Holy Doors, facing east. He loosens the belt of the baptismal robe and prays for him, that God may preserve the newly illumined in purity and illumine him by grace. He then dips a sponge in water and sprinkles him in the sign of the cross saying: \"Thou art justified. Thou art illumined. Thou art sanctified. Thou art washed: in the name of the Father, and of the Son, and of the Holy Spirit. Amen.\" Then, as he says the next prayer, he washes each of the places where he had been anointed with Chrism. Next he performs the Tonsure, symbolic of the life of self-sacrifice a Christian must lead.\n\nMany Christian churches practice a ceremony of the Washing of Feet, following the example of Jesus in the Gospel. Some interpret this as an ordinance which the church is obliged to keep as a commandment, see also Biblical law in Christianity. Others interpret it as an example that all should follow. Most denominations that practice the rite will perform it on Maundy Thursday. Often in these services, the bishop will wash the feet of the clergy, and in monasteries the Abbot will wash the feet of the brethren.\n\nSt. Benedict of Nursia lays out in his Rule that the feet of visitors to the monastery should be washed, and also that those who are assigned to serve in the kitchen that week should wash the feet of all the brethren. At one time, most of the European monarchs also performed the Washing of Feet in their royal courts on Maundy Thursday, a practice continued by the Austro-Hungarian Emperor and the King of Spain up to the beginning of the 20th century (see Royal Maundy).\n\nFoot washing is also observed by numerous Protestant and proto-Protestant groups, including Seventh-day Adventist, Pentecostal, and Pietistic groups, some Anabaptists, and several types of Southern Baptists. \nFoot washing rites are also practiced by many Anglican, Lutheran and Methodist churches, whereby foot washing is most often experienced in connection with Maundy Thursday services and, sometimes, at ordination services where the Bishop may wash the feet of those who are to be ordained. Though history shows that foot washing has at times been practiced in connection with baptism, and at times as a separate occasion, by far its most common practice has been in connection with the Lord's supper service. The Moravian Church practiced Foot Washing until 1818. There has been some revival of the practice as other liturgical churches have also rediscovered the practice.\n\nWhen an Orthodox Christian dies, his body is washed and dressed before burial. Although this custom is not considered to impose any sort of ritual purity, it is an important aspect of charitable care for the departed. Ideally, this should not be deferred to an undertaker, but should be performed by family members or friends of the deceased.\n\nWhen an Orthodox priest or bishop dies, these ablutions and vesting are performed by the clergy, saying the same prayers for each vestment that are said when the departed bishop or priest vested for the Divine Liturgy. After the body of a Bishop is washed and vested, he is seated in a chair and the Dikirion and Trikirion are placed in his hands for the final time.\n\nWhen an Orthodox monk dies, his body is washed and clothed in his monastic habit by brethren of his monastery. Two significant differences are that when his mantle is placed on him, its hem is torn to form bands, with which his body is bound (like Lazarus in the tomb), and his klobuk is placed on his head backwards, so that the monastic veil covers his face (to show that he had already died to the world, even before his physical death). When an Orthodox nun dies, the sisterhood of her convent performs the same ministrations for her as are done for monks.\n\nIn the Roman Catholic Church, the Absoute (or \"absolution of the dead\") is a symbolic ablution of the deceased's body following the Requiem Mass. While specific prayers are said, the coffin is incensed and sprinkled with holy water. The absolution of the dead is only performed in context of the Tridentine Mass. Following the Second Vatican Council, the absolution of the dead was removed from the funeral liturgy of the Mass of Paul VI.\n\nWashing and anointing (also called the initiatory) is a temple ordinance practiced by The Church of Jesus Christ of Latter-day Saints (LDS Church) and Mormon fundamentalists as part of the faith's endowment ceremony. It is a purification ritual for adults, usually performed at least a year after baptism. The ordinance is performed by the authority of the Melchizedek priesthood by an officiator of the same sex as the participant. \n\nIn the ritual, a person is sprinkled with water to symbolically wash away the \"blood and sins of this generation.\" After the washing, the person is then anointed to become a \"king and priest\" or a \"queen and priestess\" in the afterlife.\n\nOnce washed and anointed, the participant is dressed in the temple garment, a religious undergarment which the participant is instructed to wear throughout his or her life. (Since 2005, participants in the LDS Church version of the ritual already come clothed in this garment prior to the washing and anointing.) Finally, the participant is given a \"new name\" which he or she is instructed never to reveal except under certain conditions in the temple.\n\nMormons link the ritual to biblical washings and anointings. The temple garment symbolizes the skins of clothing given to Adam and Eve in the Garden of Eden, and the \"new name\" is linked to Revelation 2:17, which states that God will give those who overcome \"a white stone with a new name written on it, known only to him who receives it.\"\n\n"}
{"id": "870686", "url": "https://en.wikipedia.org/wiki?curid=870686", "title": "Alec Rose", "text": "Alec Rose\n\nSir Alec Rose (13 July 1908 – 11 January 1991) was a nursery owner and fruit merchant in England who after serving in the Royal Navy during World War II developed a passion for amateur single-handed sailing. He took part in the second single-handed Atlantic race in 1964 and circumnavigated the globe single-handedly in 1967-68, for which he was knighted. His boat \"Lively Lady\" is still seaworthy and is used for sail training by a charity.\n\nAlec Rose was born in Canterbury. During World War II he served in the Royal Navy as a diesel mechanic on a convoy escort, HMS \"Leith\".\n\nAfter the war, Rose learned to sail in a former ship's lifeboat before buying the 36-foot cutter \"Lively Lady\" second-hand. \"Lively Lady\" was built of paduak by S. J. P. Cambridge, the previous owner, in Calcutta, with the help of two Indian cabinetmakers. Cambridge had studied boat design during the war, and \"Lively Lady\" was basic, but sturdy and stable. \"Lively Lady\" is currently (2015) undergoing restoration in Boathouse 4 in the Historic Dockyard, Portsmouth which is adjacent to the Naval Base.\n\nRose converted \"Lively Lady\" to a ketch by adding a mizzenmast and in 1964 participated in the second single-handed transatlantic race, finishing in fourth place. Not having any means of communication on board, he did not know of his success until after he crossed the finish line. The race started at Plymouth, where Rose was photographed onboard by Eileen Ramsay, the chronicler of sailing in post-war Britain.\n\nWhen Rose heard that Francis Chichester intended to sail single-handedly around the world, he was keen to compete. He attempted to start his journey at approximately the same time as Chichester (sailing \"Gypsy Moth IV\") in 1966, but mechanical failures and a collision off Ushant meant he had to postpone the event until the following year.\n\nThe voyage began on 16 July 1967. While he was away Rose's wife Dorothy ran their fruit and vegetable stall, displaying a map charting his progress. On 17 December, after 155 days and 14,500 miles, he arrived in Melbourne where he met his son who lived there. Among the people who came to watch Rose's arrival was Prime Minister Harold Holt, who disappeared later the same day after going for a swim. Rose stopped once more, an unplanned call into Bluff Harbour, New Zealand to repair a damaged mast.\n\nThe voyage was closely followed by the British and international press and Rose's landfall at 12.33pm in Southsea, Portsmouth on 4 July 1968, 354 days later, was met by cheering crowds of hundreds of thousands. It was 10 days before his 60th birthday. On 10 July 1968, he was made a Knight Bachelor. And he was made a Freeman of the City of Portsmouth in the same year, was guest of honour at the Anglo-American Sporting Club gala evening at the London Hilton, and fêted with \"Lively Lady\" outside the Daily Mirror Building at Holborn Circus. He opened the Bamboo House Chinese restaurant in Southsea in 1968. He was granted the Freedom of the City of London in 1969.\n\nRose's voyages are detailed in his book \"My Lively Lady\". He wrote a children's version, \"Around the world with Lively Lady\" (1968) and another book \"My favourite tales of the sea\" (1969).\n\nIn 1973 Rose was given the honour of firing the starting gun for the first Whitbread Round the World Race. On 17 May 1975, he opened 5th Littlehampton Sea Scouts' HQ \"Gordon Hall\" in Lineside Way, Littlehampton, West Sussex.\n\nRose provided the foreword for the 1980 reprint of Brouscon's \"Tidal Almanac\" of 1546.\n\n\nRose was born in Canterbury and was educated at Simon Langton Grammar School for Boys. In his book \"My Lively Lady\" Rose described himself as a shy youth and a loner, fascinated by nature and the sea. He preferred to be self-employed rather than take a regular job, which allowed him to spend the time (over several years) preparing his yacht for the trans-Atlantic race. Rose and his wife Dorothy ran a greengrocer shop at 38 Osborne Road, Southsea.\n\nAlec Rose died aged 82 on 11 January 1991 at Queen Alexandra Hospital, Portsmouth. At the time of his death he was Admiral of the Ocean Cruising Club, and in an obituary Tim Heywood, a founder member and past Commodore who had known Rose since 1966, described him as \"the epitome of the breed of great seamen: quiet, reserved and humble\". Rose was survived by his wife Dorothy, two sons and two daughters. He bequeathed \"Lively Lady\" to Portsmouth.\n\nAlec Rose Lane in Portsmouth city centre is named after him, as is a Wetherspoon public house in Port Solent, Portsmouth and the 3rd Worthing Scout Groups ‘Rose’ Cub Pack. An elderly people's residence in Gosport bears his name. There is a plaque commemorating his global circumnavigation near his landing point at Southsea. Rose gives his name to the RNSA Sir Alec Rose Trophy for Outstanding Single Handed achievement.\n\n\"Lively Lady\" was displayed at the 2005 London Boat Show. A pub in Bracklesham, near Chichester, West Sussex, is named \"The Lively Lady\" after Rose's yacht.\n\nFrom 2006 to 2008 Alan Priddy, founder of the \"Around and Around\" charity, circumnavigated the globe aboard Rose's yacht \"Lively Lady\". The 60-year-old boat was crewed in stages by a group of 38 disadvantaged young adults. Priddy attributed his passion for sailing to Rose. \"Lively Lady\" was in 2009 leased to \"Around and Around\" for 25 years so the charity could maintain and use her for training. In 2011 the charity announced that, after a refit, \"Lively Lady\" would undertake another circumnavigation to celebrate the 50th anniversary of Rose's achievement.\n\n"}
{"id": "30744522", "url": "https://en.wikipedia.org/wiki?curid=30744522", "title": "Applications of evolution", "text": "Applications of evolution\n\nEvolutionary biology, in particular the understanding of how organisms evolve through natural selection, is an area of science with many practical applications. Creationists often claim that the theory of evolution lacks any practical applications; however, this claim has been refuted by scientists.\n\nThe evolutionary approach is key to much current research in biology that does not set out to study evolution per se, especially in organismal biology and ecology. For example, evolutionary thinking is key to life history theory. Annotation of genes and their function relies heavily on comparative, that is evolutionary, approaches. The field of evolutionary developmental biology investigates how developmental processes work by using the comparative method to determine how they evolved.\n\nA major technological application of evolution is artificial selection, which is the intentional selection of certain traits in a population of organisms. Humans have used artificial selection for thousands of years in the domestication of plants and animals. More recently, such selection has become a vital part of genetic engineering, with selectable markers such as antibiotic resistance genes being used to manipulate DNA in molecular biology. It is also possible to use repeated rounds of mutation and selection to evolve proteins with particular properties, such as modified enzymes or new antibodies, in a process called directed evolution.\n\nAntibiotic resistance can be a result of point mutations in the pathogen genome at a rate of about 1 in 10 per chromosomal replication. The antibiotic action against the pathogen can be seen as an environmental pressure; those bacteria which have a mutation allowing them to survive will live on to reproduce. They will then pass this trait to their offspring, which will result in a fully resistant colony.\n\nUnderstanding the changes that have occurred during organism's evolution can reveal the genes needed to construct parts of the body, genes which may be involved in human genetic disorders. For example, the Mexican tetra is an albino cavefish that lost its eyesight during evolution. Breeding together different populations of this blind fish produced some offspring with functional eyes, since different mutations had occurred in the isolated populations that had evolved in different caves. This helped identify genes required for vision and pigmentation, such as crystallins and the melanocortin 1 receptor. Similarly, comparing the genome of the Antarctic icefish, which lacks red blood cells, to close relatives such as the Antarctic rockcod revealed genes needed to make these blood cells.\n\nAs evolution can produce highly optimised processes and networks, it has many applications in computer science. Here, simulations of evolution using evolutionary algorithms and artificial life started with the work of Nils Aall Barricelli in the 1960s, and was extended by Alex Fraser, who published a series of papers on simulation of artificial selection. Artificial evolution became a widely recognised optimisation method as a result of the work of Ingo Rechenberg in the 1960s and early 1970s, who used evolution strategies to solve complex engineering problems. Genetic algorithms in particular became popular through the writing of John Holland. As academic interest grew, dramatic increases in the power of computers allowed practical applications, including the automatic evolution of computer programs. Evolutionary algorithms are now used to solve multi-dimensional problems more efficiently than software produced by human designers, and also to optimise the design of systems.\n"}
{"id": "33406182", "url": "https://en.wikipedia.org/wiki?curid=33406182", "title": "Baffin coastal tundra", "text": "Baffin coastal tundra\n\nThe Baffin coastal tundra is a small ecoregion of the far north of North America, on the central north coast of Baffin Island in the Canadian territory of Nunavut. This is permafrost tundra with an average annual temperature below freezing.\n\nThis ecoregion is a small stretch of coastal plain on the north coast of Baffin Island. The coast is rocky with many fjords carved by glaciers into the Baffin Mountains. The cold Arctic climate consists of a short summers (mean temperature 1°C) and a long, cold winter (mean temperature -22.5°C).\n\nThe plant cover is sparse in the drier areas while the wetter areas have a fair cover of mosses, sedges, shrubs such as purple saxifrage, Arctic willow, and Arctic poppy and rushes.\n\nThis coast is a breeding area for the snow bunting and is home to polar bear, Arctic hare, Arctic fox, lemming, and caribou.\n\nThis ecoregion is almost intact although there are no protected areas.\n"}
{"id": "10194232", "url": "https://en.wikipedia.org/wiki?curid=10194232", "title": "Canwarn", "text": "Canwarn\n\nCANWARN, acronym for CANadian Weather Amateur Radio Network, is an organized severe weather spotting and reporting program organized and run by the Meteorological Services Division of Environment Canada. What CANWARN members do is called ground truthing, they confirm and add information to the remote sensing observations of satellites and radar as well as provide information not observable by these technologies.\n\nThe program was first theorized by members of the Windsor Amateur Radio Club in Windsor, Ontario in 1986. Randy Mawson VE3TRW, Paul Robertson VE3HFQ, Jerry Beneteau VE3EXT and Bill Leal VE3ES established the original parameters and processes at that time with the first training session held in Windsor during the winter of 1986/1987 at the Windsor Airport, home at the time of the Windsor Weather Office of Environment Canada. Paul VE3HFQ and Bill VE3ES were literally putting the final touches on the station (VE3YQG) located at the Windsor Weather Office in early April 1987 when the very first CANWARN net was called to order. A report of a tornado in south east Michigan on a path towards Essex County was relayed to Environment Canada's severe weather desk in Toronto, Ontario.\n\nLater that year, after the Edmonton Tornado and at the request of the Hage Report CANWARN was expanded beyond the initial program run out of the Windsor (Ontario) Weather Office. Organized storm spotting in Canada had existed prior but operated independently of Environment Canada and never fully achieved the success that the CANWARN program did. Initially, CANWARN was predominantly based in southern Ontario and central Alberta but eventually grew to encompass the entire country by the early 1990s.\n\nThe United States began a national storm spotting program in the 1950s. Prior to that, it too had only local spotting programs. In the 1970s, it increased spotting efforts and launched its Skywarn program, which partly inspired CANWARN. In the 2000s, Europe also began organized spotting efforts under the auspices of Skywarn Europe, which consists of autonomous branches in about a dozen countries.\n\n\n"}
{"id": "217873", "url": "https://en.wikipedia.org/wiki?curid=217873", "title": "Chandra X-ray Observatory", "text": "Chandra X-ray Observatory\n\nThe Chandra X-ray Observatory (CXO), previously known as the Advanced X-ray Astrophysics Facility (AXAF), is a Flagship-class space observatory launched on STS-93 by NASA on July 23, 1999. Chandra is sensitive to X-ray sources 100 times fainter than any previous X-ray telescope, enabled by the high angular resolution of its mirrors. Since the Earth's atmosphere absorbs the vast majority of X-rays, they are not detectable from Earth-based telescopes; therefore space-based telescopes are required to make these observations. Chandra is an Earth satellite in a 64-hour orbit, and its mission is ongoing .\n\nChandra is one of the Great Observatories, along with the Hubble Space Telescope, Compton Gamma Ray Observatory (1991–2000), and the Spitzer Space Telescope. The telescope is named after the Nobel Prize-winning Indian-American astrophysicist Subrahmanyan Chandrasekhar. Its mission is similar to that of ESA's XMM-Newton spacecraft, also launched in 1999.\n\nIn 1976 the Chandra X-ray Observatory (called AXAF at the time) was proposed to NASA by Riccardo Giacconi and Harvey Tananbaum. Preliminary work began the following year at Marshall Space Flight Center (MSFC) and the Smithsonian Astrophysical Observatory (SAO). In the meantime, in 1978, NASA launched the first imaging X-ray telescope, Einstein (HEAO-2), into orbit. Work continued on the AXAF project throughout the 1980s and 1990s. In 1992, to reduce costs, the spacecraft was redesigned. Four of the twelve planned mirrors were eliminated, as were two of the six scientific instruments. AXAF's planned orbit was changed to an elliptical one, reaching one third of the way to the Moon's at its farthest point. This eliminated the possibility of improvement or repair by the space shuttle but put the observatory above the Earth's radiation belts for most of its orbit. AXAF was assembled and tested by TRW (now Northrop Grumman Aerospace Systems) in Redondo Beach, California.\n\nAXAF was renamed Chandra as part of a contest held by NASA in 1998, which drew more than 6,000 submissions worldwide. The contest winners, Jatila van der Veen and Tyrel Johnson (then a high school teacher and high school student, respectively), suggested the name in honor of Nobel Prize–winning Indian-American astrophysicist Subrahmanyan Chandrasekhar. He is known for his work in determining the maximum mass of white dwarf stars, leading to greater understanding of high energy astronomical phenomena such as neutron stars and black holes. Fittingly, the name Chandra means \"moon\" in Sanskrit.\n\nOriginally scheduled to be launched in December 1998, the spacecraft was delayed several months, eventually being launched in July 23, 1999, at 04:31 UTC by during STS-93. Chandra was deployed from \"Columbia\" at 11:47 UTC. The Inertial Upper Stage's first stage motor ignited at 12:48 UTC, and after burning for 125 seconds and separating, the second stage ignited at 12:51 UTC and burned for 117 seconds. At , it was the heaviest payload ever launched by the shuttle, a consequence of the two-stage Inertial Upper Stage booster rocket system needed to transport the spacecraft to its high orbit.\n\nChandra has been returning data since the month after it launched. It is operated by the SAO at the Chandra X-ray Center in Cambridge, Massachusetts, with assistance from MIT and Northrop Grumman Space Technology. The ACIS CCDs suffered particle damage during early radiation belt passages. To prevent further damage, the instrument is now removed from the telescope's focal plane during passages.\n\nAlthough Chandra was initially given an expected lifetime of 5 years, on September 4, 2001, NASA extended its lifetime to 10 years \"based on the observatory's outstanding results.\" Physically Chandra could last much longer. A 2004 study performed at the Chandra X-ray Center indicated that the observatory could last at least 15 years.\n\nIn July 2008, the International X-ray Observatory, a joint project between ESA, NASA and JAXA, was proposed as the next major X-ray observatory but was later cancelled. ESA later resurrected the project as the Advanced Telescope for High Energy Astrophysics (ATHENA) with a proposed launch in 2028.\n\nOn October 10, 2018, Chandra entered safe mode operations, due to a gyroscope failure. NASA reported that all science instruments were safe. Within days, the 3-second error in data from one gyro was understood, and plans made to return Chandra to full service.\n\nThe data gathered by Chandra has greatly advanced the field of X-ray astronomy. Here are some examples of discoveries supported by observations from Chandra:\n\n\nUnlike optical telescopes which possess simple aluminized parabolic surfaces (mirrors), X-ray telescopes generally use a Wolter telescope consisting of nested cylindrical paraboloid and hyperboloid surfaces coated with iridium or gold. X-ray photons would be absorbed by normal mirror surfaces, so mirrors with a low grazing angle are necessary to reflect them. Chandra uses four pairs of nested mirrors, together with their support structure, called the High Resolution Mirror Assembly (HRMA); the mirror substrate is 2 cm-thick glass, with the reflecting surface a 33 nm iridium coating, and the diameters are 65 cm, 87 cm, 99 cm and 123 cm. The thick substrate and particularly careful polishing allowed a very precise optical surface, which is responsible for Chandra's unmatched resolution: between 80% and 95% of the incoming X-ray energy is focused into a one-arcsecond circle. However, the thickness of the substrate limits the proportion of the aperture which is filled, leading to the low collecting area compared to XMM-Newton.\n\nChandra's highly elliptical orbit allows it to observe continuously for up to 55 hours of its 65-hour orbital period. At its furthest orbital point from Earth, Chandra is one of the most distant Earth-orbiting satellites. This orbit takes it beyond the geostationary satellites and beyond the outer Van Allen belt.\n\nWith an angular resolution of 0.5 arcsecond (2.4 µrad), Chandra possesses a resolution over 1000 times better than that of the first orbiting X-ray telescope.\n\nCXO uses mechanical gyroscopes, which are sensors that help determine what direction the telescope is pointed. Other navigation and orientation systems on board CXO include an aspect camera, Earth and Sun sensors, and reaction wheels. It also has two sets of thrusters, one for movement and another for offloading momentum.\n\nThe Science Instrument Module (SIM) holds the two focal plane instruments, the Advanced CCD Imaging Spectrometer (ACIS) and the High Resolution Camera (HRC), moving whichever is called for into position during an observation.\n\nACIS consists of 10 CCD chips and provides images as well as spectral information of the object observed. It operates in the photon energy range of 0.2–10 keV. HRC has two micro-channel plate components and images over the range of 0.1–10 keV. It also has a time resolution of 16 microseconds. Both of these instruments can be used on their own or in conjunction with one of the observatory's two transmission gratings.\n\nThe transmission gratings, which swing into the optical path behind the mirrors, provide Chandra with high resolution spectroscopy. The High Energy Transmission Grating Spectrometer (HETGS) works over 0.4–10 keV and has a spectral resolution of 60–1000. The Low Energy Transmission Grating Spectrometer (LETGS) has a range of 0.09–3 keV and a resolution of 40–2000.\n\nSummary:\n\n\n"}
{"id": "5247888", "url": "https://en.wikipedia.org/wiki?curid=5247888", "title": "Disaster Monitoring Constellation", "text": "Disaster Monitoring Constellation\n\nThe Disaster Monitoring Constellation for International Imaging (DMCii) consists of a number of remote sensing satellites constructed by Surrey Satellite Technology Ltd (SSTL) and operated for the Algerian, Nigerian, Turkish, British and Chinese governments by DMC International Imaging. The DMC provides emergency Earth imaging for disaster relief under the International Charter for Space and Major Disasters, which the DMC formally joined in November 2005. Other DMC Earth imagery is used for a variety of civil applications by a variety of governments. Spare available imaging capacity is sold under contract.\n\nThe DMC provides far larger areas of imagery than, but at comparable resolution to, established government imaging satellites such as Landsat. DMC imagery was deliberately designed to be comparable to Landsat imagery, in order to leverage the expertise and software of the large established remote sensing community used to working with Landsat images. Imagery can be provided far more rapidly from the DMC than from Landsat, thanks to having multiple similar satellites in orbit ready to cross over a point of interest, and the larger images produced. This brings the responsiveness that is needed for emergencies and for disaster support, with images provided across the Internet from the responsive satellite and a member country's ground station within a day or less of a request being made.\n\nThe DMC has monitored the effects and aftermath of the Indian Ocean Tsunami (December 2004), Hurricane Katrina (August 2005), and many other floods, fires and disasters.\n\nThe sun-synchronous orbits of these satellites are coordinated so that the satellites follow each other around an orbital plane, ascending north over the Equator at 10:15 am local time (and 10:30 am local time for Beijing-1).\n\nSome of these satellites also include other imaging payloads and experimental payloads: onboard hardware-based image compression (on BilSAT), a GPS reflectometry experiment and onboard Internet router (on the UK-DMC satellite). The DMC satellites are notable for communicating with their ground stations using the Internet Protocol for payload data transfer and command and control, so extending the Internet into space, and allowing experiments with the Interplanetary Internet to be carried out. Many of the technologies used in the design of the DMC satellites, including Internet Protocol use, were tested in space beforehand on SSTL's earlier UoSAT-12 satellite.\n\n\n"}
{"id": "7309329", "url": "https://en.wikipedia.org/wiki?curid=7309329", "title": "Dong Fang Hong 2", "text": "Dong Fang Hong 2\n\nDong Fang Hong 2 (also romanised as Dongfanghong 2) was the primary television satellite used by China during the later part of the 20th century. It was developed at the Chinese Academy of Space Technology (CAST) and had a design life 4.5 years. The first operational satellite in this group was launched into a geosynchronous orbit on April 8, 1984. Broadcasts from these satellites were able to cover all of China and some neighboring regions.\n\nThree satellites orbited in geosynchronous orbits at 87.5, 110.5, and 98 degrees east. A fourth satellite failed to achieve a stable orbit due to a problem with the third stage.\n\nOperating under the name Dong Fang Hong 2, the Shiyong Tongbu Tongxing Weixing (STTW, , literally Operational Geostationary Communications Satellite) was the designation of a family of indigenous Chinese communications satellites developed under the Project 331 initiative of the late 1970s and early 1980s. Being able to provide both civilian and military long distance telephony, in addition to satellite television, the initiative was promoted by the senior Chinese leadership, most notably Deng Xiaoping and Zhou Enlai, as a means to improve the communications infrastructure of the country, especially in mountainous and hard to reach areas. Building on work that had been ongoing since the 1960s Mao gave his assent to the project in April 1975. The launch vehicle for the satellites was the Long March 3 with which it was developed in parallel and conjunction as part of Project 331. In order to achieve a geostationary orbit, in addition to the normal three stages of the Long March 3, the STTW required a small solid propellant kicker.\n\nAs part of the development program two proof of technology demonstrators were launched (Shiyan Tongbu Tongxing Weixing, \"Test Geostationary Communications Satellite\"). The first of these launches in January 1984 was a failure when the second burn of the hydrogen fuelled cryogenic third stage engine, intended to boost the satellite from low earth orbit into a geostationary transfer orbit, failed. However, despite being stranded in low earth orbit, available telemetry indicated that the satellite itself was sound. The launch of the second test satellite was brought forward to April 1984, and this time both launch vehicle and satellite functioned as intended. Providing 200 telephone lines, and fifteen radio and television channels, the Shiyan Weixing demonstrated the utility of a communications satellite to link the developed coastal regions of Eastern China with the inhospitable and inaccessible regions of Western China.\n\nFunctioning until 1988, when it was parked in a graveyard orbit, Shiyan Weixing paved the way for the operational Shiyong Weixings, which remained China's primary communications satellite until replaced by the Dong Fang Hong 3.\n\nThe satellite was cylindrical with a height of , a diameter of and weighed 441 kilograms (972 lb).\n\nThe \"Dongfanghong II A\" satellite used a parabolic communications antenna mounted to the top of the satellite for broadcast purposes. The antenna could be rotated to keep it aligned with the Earth. Twenty-thousand solar cells were mounted to the satellite to provide power. This was China's first successful use of photovoltaic technology.\n\n"}
{"id": "23479627", "url": "https://en.wikipedia.org/wiki?curid=23479627", "title": "Drought Research Initiative", "text": "Drought Research Initiative\n\nThe Drought Research Initiative(DRI) was established to better understand the physical characteristics of and processes influencing Canadian Prairie droughts, and to contribute to their better prediction, through a focus on the recent severe drought that began in 1999 and largely ended in 2005. It is an interdisciplinary effort that involves 15 funded investigators from 6 Canadian universities and more than 20 collaborators from other universities and federal laboratories as well as partners from three provincial governments (Alberta, Manitoba and Saskatchewan). DRI is achieving its objective by focusing on five complementary research themes, including quantification, understanding, prediction, comparisons with other droughts, and implications for society. Details beyond the scope of this entry can be found in Stewart et al. (2008) or on the DRI website\n\nDrought is an anomaly within the atmospheric, surface and sub-surface cycling of water and energy, usually initiated through large to regional scale atmospheric processes, and enhanced and maintained through regional to local atmospheric, surface hydrology, land surface and groundwater feedbacks that operate throughout the annual cycle. Droughts are a distinctive feature of the Canadian Prairies where the large scale atmospheric circulations are influenced by blocking from intense orography to the west and long distances from all warm ocean-derived atmospheric water sources. \nDRI addresses a multi-year drought that began in 1999 with cessation of its atmospheric component in 2004/2005 and many of its hydrological components in 2005. It was the worst drought for at least a hundred years in parts of the Canadian Prairies. According to Phillips (2002), for the western and central Canadian Prairies during 2001 and 2002, “… it was the worst of times. Even in the dust bowl of the 1930s, no single year between Medicine Hat, Kindersley and Saskatoon was drier than in 2001”. The drought affected agriculture, recreation, tourism, health, hydro-electricity, and forestry in the Prairies. Gross Domestic Product fell some $5.8 billion and employment losses exceeded 41,000 jobs for 2001 and 2002. This drought also contributed to a negative or zero net farm income for several provinces for the first time in 25 years (Statistics Canada, 2003) with agricultural production over Canada dropping an estimated $3.6 billion in 2001/2002. Previously reliable water supplies such as streams, wetlands, dugouts, reservoirs, and groundwater were placed under stress and often failed. More details are available in Wheaton (2005).\nDespite the enormous economic, environmental, and societal impacts of droughts, DRI is undertaking the first integrated drought research program of its kind in Canada with the hope being that the occurrence and nature of drought can be better anticipated on short and long term scales. From a longer-term perspective, this 5-year Network represents an essential step in better predicting droughts over Canada, their detailed structure and their impacts with increasing confidence and better assessing whether there will be a ‘drying of the continental interior’ in the future\n\nDRI is achieving its objective by focusing on five complementary research themes, including quantification, understanding, and better prediction of a particular drought with funding primarily from the Canadian Foundation for Climate and Atmospheric Sciences (CFCAS). Comparisons with other droughts, and implications for society have been funded with additional direct and in-kind support coming from Environment Canada, Agriculture and Agri-Food Canada, Natural Resources Canada, Prairie Farm Rehabilitation Administration (PFRA), Saskatchewan Watershed Authority, Saskatchewan Research Council, Manitoba Water Stewardship and Manitoba Hydro.\n\nTo characterize the drought of 1999-2005 DRI has been quantifying its atmospheric, hydrologic, and land-surface physical features at a variety of spatial and temporal scales. This analysis will be realized by addressing three focused questions:\n\nThe analysis has involved the development of a four-dimensional assessment of the atmosphere during the drought over various temporal scales using a knowledge of temperature, humidity, geopotential height, wind, clouds, precipitation amount, and current weather. At the surface, the spatial and temporal characteristics of vegetative state (in terms of water stress) for major vegetation types (crops and boreal zones), soil moisture, stream network, river flows, lake levels, wetlands, depression storage, groundwater and sub-surface flows be assessed to determine when and where drought is occurring. Although this is a difficult task due to data gaps, and the complexity of the shallow undulations in the prairies topography, an analysis of in-situ data, satellite data and model output now provides a comprehensive overview of the evolution of the drought.\n\nDRI is also focusing on the understanding of the processes and feedbacks governing the formation, evolution, cessation and structure of the drought. This theme addresses the following questions:\nThere are many ways in which drought, basically a sustained precipitation deficit, can be initiated and prolonged. Some processes that reduce precipitation include, large scale circulation anomalies, lack of moisture advected into a region, reduction of local moisture supplies, production of virga as opposed to precipitation, and possibly even the role of aerosols in the dusty environment acting to reduce precipitation. Over the five years of this drought, these processes and numerous other factors were operating at various times and locations to preclude substantial precipitation from being produced over large regions of the Prairies. \nWhile prairie drought processes have characteristic spatial and temporal scales, land surface hydrological/biophysical processes generally operate at small spatial scales (and with larger inherent spatial variability) while atmospheric process have a hierarchy of scales ranging from planetary to local turbulence. DRI is addressing the cascade of energy and movement of moisture through atmospheric processes and the horizontal scale link between land surface and groundwater processes during drought events.\n\nDrought prediction research in DRI is directed at assessing and reducing uncertainties in the prediction of drought and its structure. In particular, prediction research has focused on the evaluation and improvement of prediction models. The modelling tools used are global and regional climate models (GCMs and RCMs), and hydrological models. The hydrological models are driven by output from the atmospheric models, data from ongoing research sites and reanalysis products. DRI modeling studies address the following questions:\nUse is being made of archived historical model outputs and new simulations to address many of these issues. DRI focuses on many facets of a particular extreme and consequently many more variables beyond the traditional temperature and precipitation need to be examined. For example, experiments carried out within DRI are helping to assess the role of soil moisture and snow cover on seasonal prediction.\n\nTheme 4 provides comparisons of the recent drought and previous droughts over this region and those in other regions, in the context of climate variability and change. For many users there is a need to understand a current or recent drought event in the context of historical droughts. These comparisons are guided by the following research questions:\nComparisons between the 1999-2004/05 drought have focused on the internal structure of drought, as well as its presence over a particular region. Many droughts, including the 1999-2005 event have a complex internal structure for their precipitation patterns which often changed dramatically during the drought. More details on these comparisons are available in Bonsal (2008).\n\nTheme 5 applies our progress in drought characterization and prediction to critical issues of importance to society during times of drought. These efforts are linked to the following questions:\nIn order to advance this theme, DRI has carried out an extensive consultative process with the users; has launched a set of simulation exercises to test how users may adapt new DRI products to make better decisions through an exercise known as DEWS (Drought Early Warning System test), and formed a Partners Advisory Committee composed of experts from user agencies and groups to give advice on their needs for DRI research and information. More details are available in Lawford et al. (2008).\n\nDRI is led by two Principal Investigators, Drs. Ronald Stewart and John Pomeroy. A Board of Directors, chaired by Jim Bruce oversees the program strategy and a Science Committee takes the day-to-day decisions about the implementation of the project. In addition, the operational details are implemented by a Secretariat consisting of a Network Manager, a Financial Manager and two Data Managers. A Partners Advisory Committee, chaired by Harvey Hill, also meets and reports on the user needs to the Board of Directors. The program have also established strong linkages and collaborations with research initiatives and departments and on provincial, federal and international levels. The detailed program structure and linkages can be found in Stewart et al., (2008).\n\nDRI has brought together an unprecedented amount of information on Canadian drought. On its web site, one can find many links to various datasets that describe elements of drought in the atmosphere, at the surface or below the surface. Compiling a Prairie-wide database of provincial groundwater and water levels is part of this effort. The data are being brought together in an integrated information system. Synthesis articles, special issue of Atmosphere-Ocean and a public information document about the 1999-2005 drought are in preparation. In addition, new techniques of archiving and making available datasets are being explored as one other legacy of DRI. Undoubtedly, the greatest legacy of DRI will be the young people who have been trained at the PhD, Masters and early scientist level who carried out much of this research and are now pursuing lifetime careers in disciplines related to DRI.\n\n"}
{"id": "50659600", "url": "https://en.wikipedia.org/wiki?curid=50659600", "title": "Dry-seal Wiggins gasholder", "text": "Dry-seal Wiggins gasholder\n\nA dry-seal Wiggins gasholder is a device designed to hold gas.\n\nA dry-seal gasholder can be designed to have a gross (geometric) volume ranging from , whilst having a working pressure range between . The dry-seal gasholder is finished with an anti-corrosive treatment to counteract local climatic conditions and also any chemical attack from the stored medium. This anti-corrosive treatment is fully compatible with the sealing membrane and also the environment.\n\nThe dry seal gasholder has four major elements: the foundation; the main tank; the piston and the sealing membrane. Each of these elements can be divided into various sub-elements and associated accessories.\n\nA concrete and hardcore base designed to withstand the weight of the steel gasholder structure constructed upon it and to withstand dynamic climatic conditions acting upon the gasholder etc.\n\nThe main tank is designed to accommodate the design requirements laid down by the customer and climatic conditions There are three main sub-elements to the tank:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe gasholder piston moves up and down the inside of the shell as gas enters and exits the gasholder.\nThe weight of the piston (less the weight of the level weights) produces the pressure at which the gasholder will operate. The piston is designed to apply an equally distributed weight to ensure that the piston remains level at all times. The piston made up of the following sub-elements:\n\n\n\n\n\n\n\n\n\n\n\n\nThe seal of the gasholder is designed to operate in the conditions specified by the client and to suit the stored medium. The seal rolls from the shell to the abutment surface of the piston and vice versa providing the piston with a frictionless self-centering facility. During depressurisation the seal (Usually rubber) also provides a gas tight facility that protects the holder from vacuum damage by blocking the gas outlet nozzle. During commissioning of the gasholder the sealing membrane is set into an operating condition. This setting must be carried out every time the gasholder is depressurised, otherwise known as \"popping\" the seal.\n\n"}
{"id": "184073", "url": "https://en.wikipedia.org/wiki?curid=184073", "title": "Ebony", "text": "Ebony\n\nEbony is a dense black hardwood, most commonly yielded by several different species in the genus \"Diospyros\", which also contains the persimmons. Ebony is dense enough to sink in water. It is finely-textured and has a very smooth finish when polished, making it valuable as an ornamental wood. The word \"ebony\" comes from the Ancient Egyptian \"hbny\", through the Ancient Greek (\"ébenos\"), into Latin and Middle English.\n\nSpecies of ebony include \"Diospyros ebenum\" (Ceylon ebony), native to southern India and Sri Lanka; \"Diospyros crassiflora\" (Gabon ebony), native to western Africa; and \"Diospyros celebica\" (Makassar ebony), native to Indonesia and prized for its luxuriant, multi-colored wood grain. Mauritius ebony, \"Diospyros tessellaria\", was largely exploited by the Dutch in the 17th century. Some species in the genus \"Diospyros\" yield an ebony with similar physical properties, but striped rather than evenly black (\"Diospyros ebenum\").\n\nEbony has a long history of use, with carved pieces having been found in Ancient Egyptian tombs.\n\nBy the end of the 16th century, fine cabinets for the luxury trade were made of ebony in Antwerp. The wood's dense hardness lent itself to refined moldings framing finely detailed pictorial panels with carving in very low relief (bas-relief), usually of allegorical subjects, or with scenes taken from classical or Christian history. Within a short time, such cabinets were also being made in Paris, where their makers became known as \"ébénistes\", which remains the French term for a cabinetmaker.\n\nModern uses are largely restricted to small items, such as crucifixes, and musical instrument parts, including black piano and harpsichord keys, violin, viola, mandolin, guitar, double bass, and cello fingerboards, tailpieces, pegs, chinrests, and bow frogs. Many plectra, or guitar picks, are made from this black wood.\n\nTraditionally, the black pieces in chess sets were made from ebony, with rare boxwood or ivory being used for the white pieces. Modern East Midlands-style lace-making bobbins, also being small, are often made of ebony and look particularly decorative when bound with brass or silver wire. Due to its strength, many handgun grips and rifle fore-end tips are made of ebony, as are the butts of pool cues.\n\nAs a result of unsustainable harvesting, many species yielding ebony are now considered threatened. Africa in particular has had most of its indigenous ebony cut down illegally.\n\nIn Sri Lanka, ebony is a protected species and harvesting and sale of ebony is illegal and punishable by imprisonment.\n\nIn 2012, the Gibson Guitar company was raided by the US Fish and Wildlife Service for violations of the Lacey Act of 1900, which prohibits the importation of threatened woods and other materials.\n\nPete Lowry, an ebony and rosewood expert at the Missouri Botanical Garden, calls the Madagascar wood trade the \"equivalent of Africa's blood diamonds\".\n\nEbony, when ground into fine dust (for example, during sawing), results in a flammable and toxic powder which can float about in air for several days. Due to the high toxicity of ebony in powdered form, its use in construction work requires government certification in several South Asian countries.\n\n\n"}
{"id": "17806583", "url": "https://en.wikipedia.org/wiki?curid=17806583", "title": "Elsparefonden", "text": "Elsparefonden\n\nThe Danish Electricity Saving Trust (Elsparefonden) is an independent trust under the auspices of the Danish Ministry of Climate and Energy. The Trust works to promote energy savings and a more efficient use of electricity.\n\nThe Trust is financed by a special electricity savings charge of DKK 0.006/kWh payable by households and the public sector. Total annual proceeds amount to approximately DKK 96 million.\n\n"}
{"id": "1144487", "url": "https://en.wikipedia.org/wiki?curid=1144487", "title": "Encephalization Quotient (EQ)", "text": "Encephalization Quotient (EQ)\n\nEncephalization quotient (EQ), encephalization level or just encephalization is a relative brain size measure that is defined as the ratio between observed to predicted brain mass for an animal of a given size, based on nonlinear regression on a range of reference species.\n\nIt is a more refined measurement than the raw brain-to-body mass ratio, as it takes into account allometric effects. The relationship, expressed as a formula, has been developed for mammals, and may not yield relevant results when applied outside this group. \n\nBrain size usually increases with body size in animals (is positively correlated), i.e. large animals usually have larger brains than smaller animals. The relationship is not linear, however. Generally, small mammals have relatively larger brains than big ones. Mice have a direct brain/body size ratio similar to humans (1/40), while elephants have a comparatively small brain/body size (1/560), despite being quite intelligent animals.\n\nSeveral reasons for this trend are possible, one of which is that neural cells have a relative constant size. Some brain functions, like the brain pathway responsible for a basic task like drawing breath, are basically similar in a mouse and an elephant. Thus, the same amount of brain matter can govern breathing in a large or a small body. While not all control functions are independent of body size, some are, and hence large animals need comparatively less brain than small animals. This phenomenon has been called the cephalization factor: C\" = \"E\"÷\"S, where E and S are brain and body weights respectively, and C is the cephalization factor. To determine the formula for this factor, the brain/body weights of various mammals were plotted against each other, and the curve of \"E\" = \"C\"\"S\" chosen as the best fit to that data.\n\nThe cephalization factor and the subsequent encephalization quotient was developed by H.J. Jerison in the late 1960s. The formula for the curve varies, but an empirical fitting of the formula to a sample of mammals gives formula_1. As this formula is based on data from mammals, it should be applied to other animals with caution. For some of the other vertebrate classes the power of 3/4 rather than 2/3 is sometimes used, and for many groups of invertebrates the formula may give no meaningful results at all.\n\nSnell's equation of simple allometry is:\n\nHere \"E\" is the weight of the brain, \"C\" is the cephalization factor and \"S\" is body weight and \"r\" is the exponential constant. The exponential constant for primates is 0.28 and either 0.56 or 0.66 for mammals in general.\n\nThe \"Encephalization Quotient\" (EQ) is the coefficient \"C\" in Snell's allometry equation, usually normalized with respect to a reference species. In the following table, the coefficients have been normalized with respect to the value for the cat, which is therefore attributed an EQ of 1 .\n\nAnother way to calculate encephalization quotient is by dividing the actual weight of an animal's brain with its predicted weight according to Jerison's formula.\n\nThis measurement of approximate intelligence is more accurate for mammals than for other classes and phyla of Animalia.\n\nIntelligence in animals is hard to establish, but the larger the brain is relative to the body, the more brain weight might be available for more complex cognitive tasks. The EQ formula, as opposed to the method of simply measuring raw brain weight or brain weight to body weight, makes for a ranking of animals that coincide better with observed complexity of behaviour.\n\nMean EQ for mammals is around 1, with carnivorans, cetaceans and primates above 1, and insectivores and herbivores below. This reflects two major trends. One is that brain matter is extremely costly in terms of energy needed to sustain it. Animals which live on relatively nutrient poor diets (plants, insects) have relatively little energy to spare for a large brain, while animals living from energy-rich food (meat, fish, fruit) can grow larger brains. The other factor is the brain power needed to catch food. Carnivores generally need to find and kill their prey, which presumably requires more cognitive power than browsing or grazing. The brain size of a wolf is about 30% larger than a similarly sized domestic dog, again reflecting different needs in their respective way of life.\n\nIt is worth noting, however, that of the animals demonstrating the highest EQ's (see associated table), many are primarily herbivorous, including apes, macaques, and proboscideans. The dietary factor, therefore, may be less significant than certain others, like gregariousness.\n\nAnother factor affecting relative brain size is sociality and flock size. For example, dogs (a social species) have a higher EQ than cats (a mostly solitary species). Animals with very large flock size and/or complex social systems consistently score high EQ, with dolphins and orcas having the highest EQ of all cetaceans, and humans with their extremely large societies and complex social life topping the list by a good margin.\n\nBirds generally have lower EQ than mammals, but parrots and particularly the corvids show remarkable complex behaviour and high learning ability. Their brains are at the high end of the bird spectrum, but low compared to mammals. Bird cell size is on the other hand generally smaller than that of mammals, which may mean more brain cells and hence synapses per volume, allowing for more complex behaviour from a smaller brain. Both bird intelligence and brain anatomy are however very different from those of mammals, making direct comparison difficult.\n\nManta rays have the highest EQ among fish, and either octopuses or jumping spiders have the highest among invertebrates. Despite the jumping spider having a huge brain for its size, it is minuscule in absolute terms, and humans have a much higher EQ despite having a lower raw brain-to-body weight ratio. Mean EQs for reptiles are about one tenth of those of mammals. EQ in birds (and estimated EQ in dinosaurs) generally also falls below that of mammals, possibly due to lower thermoregulation and/or motor control demands. Estimation of brain size in the oldest known bird, \"Archaeopteryx\", shows it had an EQ well above the reptilian range, and just below that of living birds.\n\nBiologist Stephen Jay Gould has noted that if one looks at vertebrates with very low encephalization quotients, their brains are slightly less massive than their spinal cords. Theoretically, intelligence might correlate with the absolute amount of brain an animal has after subtracting the weight of the spinal cord from the brain. This formula is useless for invertebrates because they do not have spinal cords or, in some cases, central nervous systems.\n\nBehavioural complexity in living animals can to some degree be observed directly, making the predictive power of the encephalization quotient less relevant. It is however central in paleoneurology, where the endocast of the brain cavity and estimated body weight of an animal is all one has to work from. The behaviour of extinct mammals and dinosaurs is typically investigated using EQ formulas.\n\nEncephalization quotient is also used in estimating evolution of intelligent (human) behaviour in human ancestors. Fossils of archaic humans have EQ e.g. 4.15.\n\nRecent research indicates that whole brain size is a better measure of cognitive abilities than EQ for non-human primates at least. The relationship between brain-to-body mass ratio and complexity is not alone in influencing intelligence. Other factors, such as the recent evolution of the cerebral cortex and different degrees of brain folding, which increases the surface area (and volume) of the cortex, are positively correlated to intelligence in humans.\n\nQuantifying an animal's encephalization has been argued to be directly proportional, although not equal, to that animal's level of intelligence. Aristotle wrote in 335 BCE: \"Of all the animals, man has the brain largest in proportion to his size.\" Also, in 1871, Charles Darwin wrote in his book \"The Descent of Man\": \"No one, I presume, doubts that the large proportion which the size of man's brain bears to his body, compared to the same proportion in the gorilla or orang, is closely connected with his mental powers.\"\n\n\n\n"}
{"id": "760027", "url": "https://en.wikipedia.org/wiki?curid=760027", "title": "Frost line", "text": "Frost line\n\nThe frost line—also known as frost depth or freezing depth—is most commonly the depth to which the groundwater in soil is expected to freeze. The frost depth depends on the climatic conditions of an area, the heat transfer properties of the soil and adjacent materials, and on nearby heat sources. For example, snow cover and asphalt insulate the ground and homes can heat the ground (see also \"heat island\"). The line varies by latitude, it is deeper closer to the poles. Per Federal Highway Administration Publication Number FHWA-HRT-08-057, the maximum frost depth observed in the contiguous United States ranges from zero to about eight feet (2.4m). Below that depth, the temperature varies, but is always above .\n\nAlternatively, in Arctic and Antarctic locations the freezing depth is so deep that it becomes year-round permafrost, and the term \"thaw depth\" is used instead. Finally, in tropical regions, frost line may refer to the vertical geographic elevation below which frost does not occur.\n\n\"Frost front\" refers to the varying position of the frost line during seasonal periods of freezing and thawing.\n\nBuilding codes sometimes take frost depth into account because of frost heaving which can damage buildings by moving their foundations. Foundations are normally built below the frost depth for this reason. Water and sewage pipes are normally buried below the frost line to prevent them from freezing. Alternatively, pipes may be insulated or actively heated using heat-tape or similar products to allow for shallower depths. Due to additional cost, this method is typically only used where deeper trenching is not an option due to utility conflicts, shallow bedrock, or other conditions that make deeper excavation infeasible. \n\nThere are many ways to predict frost depth including factors which relate air temperature, soil temperature and soil properties. \n\n"}
{"id": "2224906", "url": "https://en.wikipedia.org/wiki?curid=2224906", "title": "GMA Pinoy TV", "text": "GMA Pinoy TV\n\nGMA Pinoy TV is a Filipino television channel incorporated in February 2004 by GMA Network and its subsidiaries, GMA International and GMA Worldwide Inc. It was launched in 2005, primarily in Japan and the USA, as a 24-hour international Filipino television station. The channel is targeted towards overseas Filipinos and their families in the Philippines. The network airs shows produced by GMA Network. Drawing on programming from its flagship station in the Philippines, the network's programs can be seen worldwide.\n\nThe programming of GMA Pinoy TV consists mostly of from the Philippines from GMA Network as well as previously aired shows, documentaries, films, and sports events from the Philippines. The delay of GMA Pinoy TV's dramas all air on one-episode delay basis. Most weekend shows are up to date, with the exception of some shows that air on a one-episode delay basis.\n\nGMA Pinoy TV was first launched in Japan on March 2005. It was later launched in several parts of the United States in the same year, such as San Francisco, Los Angeles, and the states of the East Coast. The official GMA Pinoy TV launch in America happened in San Francisco on 23 July 2005 and was aired on \"SOP\" the following Sunday, 31 July 2005.\n\nIn 2006, the so-called Filipino colors—red, blue and yellow—were used for the graphics and since December 2006, the on-screen graphics of GMA Pinoy TV is being changed every Christmas season.\n\nIn 2008, GMA Pinoy TV was also known for the program Dyesebel. Due to the success of the program, GMA Pinoy TV launched a billboard in Daly City, which was there for the entirety of the show.\n\nThe logo also was seen in the upper right and left corners at varied times. It was settled in 2007 that the logo will be seen in the upper right corner. In 2007, GMA Pinoy TV became a huge success and was known better known because of \"Marimar\". Due to the success of the program, GMA Pinoy TV once changed graphics, which has been used from 2007 to October 2010.\n\nAn associated second channel, GMA Life TV, was successfully launched in March 2008 and soon grew to 109,000 subscribers. GMA Pinoy TV had 225,000 subscribers as of September 2009. The percent of subscribers has gone up 34% according to GMA New Media, Inc.\n\nIn 2010, an amount of PhP253 million for the 1st Quarter of 2010's revenues was posted in the Annual Stockholders Meeting of GMA Network for GMA Pinoy TV and GMA Life TV. In May 2010, GMA and GMA Network were acclaimed for their performance of covering the elections. The said election coverage was aired live on GMA Pinoy TV on 9 May until the Election ended. Based on the September 2010 report posted by the Corporate Communications Department of GMA Network, GMA Pinoy TV had a total of 256,000 subscribers while its sister channel, GMA Life TV had 122,000 subscribers. On 10 October 2010 (otherwise known was 10-10-10), GMA Pinoy TV changed graphics with a theme that recognizes and is towards Pinoys all over the world. It also has a globe symbolizing how far GMA Pinoy TV has gone since the start. On Christmas 2010, GMA Pinoy TV used the slogan \"Isang Maligayang at Masaganang Paskong Pinoy, Kapuso.\" This was the first time that GMA Pinoy TV used another Christmas slogan besides the GMA Network Christmas 2010 slogan, \"Isang Pagkilala sa Puso ng Pilipino Ngayong Pasko.\" As for the 3rd Quarter of 2010, GMA Pinoy TV had around 261,000 subscribers posted. As of December 2010, GMA Pinoy TV and GMA Life TV has expanded into the following: Chicago, Illinois (via Comcast Chicago); New York (via Time Warner New York); Washington, D.C., Maryland, Alabama, Virginia (via Comcast); Vancouver, British Columbia (via Telus Canada); and Italy, London, and the rest of UK and rest of Europe (via Infinity IPTV). In a report published showing GMA Network's revenues for 2010 with a total of PhP 14.299 billion, GMA Life TV has 125,000 subscribers (an estimated 800,000 viewers count) while GMA Pinoy TV has an accounted 274,000 subscribers (around 1.8 million viewers).\n\nIn 2011, the delays of the shows on GMA Pinoy TV receded from months of delay to two days delay for primetime shows and three days delay for afternoon dramas. On the first quarter of 2011, GMA Pinoy TV and its home network in the Philippines revealed a new logo with minor changes. The \"GMA\" letters part is seen to be more embossed and the logo has a \"glow\" effect. On 28 February 2011 for the first time in GMA International's history, two programs that started airing on GMA Pinoy TV that were still running with new episodes were switched over to GMA Life TV, where it will then air its new episodes to occupy free time slots. This then led to replays of other Kapuso shows. The shows included \"Pinoy MD\" and \"Love ni Mister, Love ni Misis\". All this will be straightened out once GMA News TV starts its international operations sometime 2nd to 3rd quarter of 2011.\n\nOn 10 March 2011 GMA Pinoy TV and GMA Life TV expanded even more in the United States in Florida. The Florida counties that are part of the extension include Brevard, Flagler, Lake, Marion, Orange, Osceola, Seminole, Sumter and Volusia. On 5 April 2011, though already launched in the Philippines on 27 March 2011, GMA Pinoy TV officially launched the 2011 summer slogan of GMA \"Halo-Halo Ang Summer Saya\". This took some time to launch on GMA Pinoy TV because significant edits were made in which some of the programs on the original version were not slated to air on GMA Pinoy TV like \"Man vs. Beast\" but beside that, the international version of the station ID still included planned shows of Summer 2011. Also in April 2011, GMA Pinoy TV is now being broadcast by COX Communications in Arizona, areas like Tucson and Arizona, on Channel 482. In a report dated 16 May 2011 regarding the consolidated gross revenues of GMA Network, Inc. in the 1st quarter of 2011, it is noted that GMA Pinoy TV's subscriptions grew by 10 percent (approximately 300,000 subscribers) and GMA Life TV's subscriptions grew by 2 percent (approximately 128,000). This consisted of PhP 230 million worth of advertising and subscription revenues. In another recent report with regards to GMA Network's consolidated revenues in the 2nd quarter of 2011, GMA Pinoy TV's subscribers grew to 279,000 from 274,000 (actual count) and an estimated number of 1.85 million viewers and GMA Life TV's subscribers grew as well, an estimated 1 million viewers. Also, GMA Network recently launched GMA News TV in the past month and will be made available to all areas where GMA Pinoy TV and GMA Life TV reaches by 3rd Quarter of 2011. GMA Pinoy TV and GMA Life TV were further made to subscribers in key Filipino-American areas in the US, namely New Jersey, Philadelphia, Pennsylvania, Virginia, Maryland and Washington D.C. They were also launched in many parts of Detroit, Hawaii and upstate New York and New England. The channels were also launched by additional carriers in Australia and Papua New Guinea.\n\nOn 5 October 2011, GMA Pinoy TV accepted two awards from the National Association for Multi-Ethnicity in Communications (NAMIC) in New York for 2011 Excellence in Multicultural Marketing Awards. On 4 November 2011, GMA Pinoy TV launched GMA's 2011 Christmas Station ID in several parts with each cast of most Kapuso shows and it was usually during the show and one right after the show ended. On 24 November 2011, Thanksgiving Day in the US, GMA Pinoy TV launched the Christmas-themed graphics for 2011, which also included the upper-right logo being converted to a Christmas-themed one. This was also the same date that GMA Network revamped its websites, with new features and a new look.\nOn December 2011, GMA Pinoy TV’s subscriber count closed at 291,309, a 7% increase over the 2010 closing subscriber number of 273,317. Because of this, revenues grew by 8% year-on-year and are expected to grow at that rate 2012 onwards.\n\nOn 2012, \"My Beloved\" became is the first primetime TV Drama series of GMA Network to premiere just a day after its original premiere on GMA Pinoy TV. On 15 March 2012, GMA Pinoy TV updated its on-screen graphics, replacing the 2010 version of approximately 30 seconds to one that is approximately 15 seconds.\n\nIn a press statement released by GMA Network, Inc., it was stated that GMA Pinoy TV, as of 1st Quarter of 2012, has approximately 303,000 subscribers, which numbers nearly 2 million viewers.\n\nOn 18 May 2012, GMA Pinoy TV released on-air alternate versions of its \"Up Next\" and \"You're Watching\" bumpers; they were much shorter and instead of clips of the shows, they featured the stars of the respective shows. On 5 June 2012, GMA Pinoy TV began experiencing audio issues with its broadcast, audio that is not so typical of High-Definition, High-Resolution cameras tha GMA uses. On 21 June 2012, the issue was resolved and the original audio type was restored. For the first time in GMA history, \"Idol sa Kusina\", a program originally aired on QTV then GMA News TV and simulcast on GMA Life TV moved to GMA Pinoy TV on 14 July 2012. In September 2012, for the first time in GMA Pinoy TV's history, \"24 Oras\", the network's flagship newscast program will had a four-hour-delayed telecast, meaning in the US, the show aired 7:30am (US-PST), 10:30am (US-EST), and 4:30am (Hawaii). This is the first time the network has placed \"24 Oras\" right after \"Unang Hirit\", which also changed timeslots to give way for 24 Oras.\n\nAs of 30 October 2012, four drama series were premiered that were one-day delayed: \"My Beloved\", \"One True Love\", \"Coffee Prince\", and \"Temptation of Wife\". This is part of GMA Pinoy TV's promise to its viewers to lessen airing delays and more original programming being shown. On 30 November 2012, GMA Pinoy TV updated its upper-right corner logo once more to a Christmas-themed one and one that celebrates GMA Pinoy TV's 7th anniversary and the version of the letters \"GMA\" are the more current one used by GMA. The on-air Christmas graphics which has been annually seen on GMA Pinoy TV was subsequently launched on 1 December 2012 and will last until the first week of January 2013. On 31 December 2012, GMA Pinoy TV aired \"Win na Win sa 2013: The GMA New Year Countdown\" live, making it the third time that GMA aired its New Year countdown live worldwide (previously in 2008 and 2011).\n\nOn 2013, \"Indio\" and \"Mundo Mo'y Akin\" became the fifth and sixth TV Drama series of GMA Network to premiere just a day after its original premiere on GMA Pinoy TV. This makes all the current and future primetime dramas of GMA to air on a one-day episode delay basis, compared to previously where it started first as long as three-weeks delay to as short as two-days delay. For the first time on GMA Pinoy TV, copyrighted material created and produced by another company, National Geographic Channel, was aired on GMA Pinoy TV on 7 April 2013. The copyrighted material aired under license is the National Geographic Channel Documentary entitled \"Inside Malacanang Palace\", which aired on March 2012, is a special documentary that takes the audience to an exclusive guided tour of the Malacanang Palace of the Philippines.\n\nOn the first quarter of 2013, GMA Pinoy TV had a total of 333,000 subscribers while its sister channel, GMA Life TV had 125,000 subscribers. This is based on a report that the Corporate Communications Department of GMA Network has posted. Overall, in 2012 and Q1 of 2013, subscription to GMA International channels increased by 7 percent. On June 2013, SingTel relaunched this channel together with GMA Life TV and in the new Fiesta Pack as well after 2 months of hiatus of this channel.\n\n\"Anna Karenina\" and \"My Husband's Lover\" became the seventh and eighth primetime TV drama series of GMA Network to premiere just a day after its original premiere. On 27 August 2013, \"\" started airing and was the first GMA Afternoon Prime drama series to be on only a one-episode delay basis, as for previous years, GMA Afternoon Prime dramas have been noted to be two to even four episodes delayed. On 10 September 2013, \"Akin Pa Rin ang Bukas\" became the ninth primetime TV drama series of GMA Network to premiere just a day after its original premiere on GMA Pinoy TV.\n\nOn 17 September 2013, GMA Pinoy TV had experienced technical problems and thus a three-hour delay was caused in its programming schedule, which received negative feedback from the viewers as they were not warned beforehand. The management had appealed to the viewers by saying that such was not intentional and was accidental because of technical/conversion issues. The following day, 18 September 2013, the programming staff of GMA Pinoy TV was prompted to change the programming schedule, effectively removing the airings of Eat Bulaga! and 24 Oras just a few hours after its original broadcast. Viewers are still hoping for the programming of GMA Pinoy TV to revert to its original state, which most viewers preferred.\n\nOn 29 September 2013, a fire hit the basement of GMA Network's Building. The fire was put out in less than 50 minutes as the fire correspondents responded immediately. The fire affected the power supply of GMA Network and thus further affected the normal operations of GMA Network, including the operations of GMA Pinoy TV as well as GMA Life TV and GMA News TV International. During the fire, temporary programming was aired consisting of replays of shows from the weekend. The fire had caused disappointment and inconvenience on the part of the viewers/subscribers. However, the management of GMA International assured its subscribers that indeed the appropriate measures are being taken and that normal operations are hoped to be restored at the soonest possible time. On 1 October 2013, programming had slowly been restored. Subscribers reported \"Unang Hirit\", \"24 Oras\", \"The Ryzza Mae Show\" and \"Eat Bulaga!\" among others to be updated and though slightly off their appropriate timeslot, still was managed to be aired over on GMA Pinoy TV. Also, \"My Husband's Lover\" aired back-to-back episodes and thus only on 1 October did it become the first TV series of GMA Drama to air the same episode that aired in the Philippines on the same day that it aired on GMA Pinoy TV. \"Akin Pa Rin ang Bukas\" and \"Kahit Nasaan Ka Man\" followed and also had back-to-back episodes aired; these \"back-to-back\" episode appeared to the viewers as hints that the primetime dramas will begin to air on same-day episode basis. However, this idea was dismissed on 2 October 2013 when these shows had re-aired the \"catch up\" episodes and thus, the primetime dramas were confirmed to be on a one-episode delay basis again. The network's afternoon dramas, \"Prinsesa ng Buhay Ko\", \"Magkano Ba ang Pag-ibig?\" and \"\", are still airing on a one-episode delay basis and were not affected by the \"catching up\" that happened on 1 October. The only afternoon drama left airing on a two-episode delay basis is \"Mga Basang Sisiw\".\n\nOn 5 October 2013, GMA Pinoy TV updated its on-air title screen, in celebration of its \"8 Years of Bringing Global Pinoys Closer to Home.\" On 30 November 2013, GMA Pinoy TV updated its upper-right logo to a Christmas-themed one, alongside GMA's Christmas Theme for 2013, \"Sundan Natin ang Bituin Pabalik sa Kanyang Piling\". The following day, 1 December 2013, the on-air title screen and graphics were updated.\n\nOn 17 December 2013, \"The Ryzza Mae Show\" and \"Eat Bulaga!\" were restored to their late night timeslots in the North American feed. It was noted that subscribers continually pleaded GMA Pinoy TV to restore these shows to their late night timeslots as this is convenient for many working subscribers who are only able to watch at that time. After three months and a day of requests, the management finally granted such and the shows were restored to their respective and consecutive late night timeslots. In addition to their restored timeslots, the two shows air as well right after their restored timeslots and so, overall, \"The Ryzza Mae Show\" and \"Eat Bulaga\" air four times on GMA Pinoy TV daily. The New Year Countdown aired on a three-hour delay basis on GMA Pinoy TV on 31 December 2013. From 31 December 2013 to 13 January 2014, GMA Pinoy TV had a special airing of \"Titser\". This was in the timeslot previously occupied by \"Genesis\" and \"Akin Pa Rin Ang Bukas\", as in the Philippines the timeslots of the two shows were replaced by Hollywood blockbusters.\n\nThe GMA Pinoy TV on-screen graphics and logo returned to its regular version on 5 January 2014, marking the end of the Christmas season. GMA Pinoy TV began inserting English subtitles in certain shows like \"Wish Ko Lang\", \"Pepito Manaloto: Ang Tunay na Kuwento\" and \"Magpakailanman\" on Saturday, 11 January 2014. This is part of GMA Pinoy TV's efforts to expand its audience reach so that not only those who can speak and understand Tagalog can watch GMA Pinoy TV but also those who cannot.\n\nOn 22 April 2014, GMA Pinoy TV began to air \"Full House\", the first Tagalog-dubbed Koreanovela aired on this channel. The shows with English subtitles are noted in the next section \"currently broadcast\". \"Basta Every Day, Happy!\" was supposed to premiere on 12 May 2014 (in some territories like the US on the evening of 11 May 2014) and air throughout its entire duration on GMA Pinoy TV but the programming department of GMA International, GMA's international subsidiary, decided to move the show over to GMA Life TV as it adheres more to GMA Life TV's overall theme as a lifestyle channel as well as the show is a replacement for \"\", so the programming department had seen that the move was done rightfully so.\n\nOn 26 May 2014, GMA Pinoy TV began to air \"Niño\" with no episode delay whatsoever, thus premiering on the same day. This is a historical event for GMA Pinoy TV, as GMA drama series have never aired without episode delays. The nearest to not having a delay was the one-episode\ndelay policy that was established on 14 February 2012 with the premiere of \"My Beloved\". What is going to happen is in the North American feed as well as on the Australian feed, the premiere of \"Niño\" and subsequent new episodes on same-day airing will take place on the evening telecast and replays in the afternoon telecasts while in other feeds, particularly, the Middle East and North Africa, Asia Pacific and European feeds, the premiere and subsequent new episodes will take place in the afternoon telecasts and replays on evening telecasts, thus because these telecasts will take place in the afternoon, such is still not considered delayed, as a new episode of the show still has not premiered. This similar framework followed with \"Ang Dalawang Mrs. Real\", which premiered on 2 June airing on same-day basis as well along with \"Strawberry Lane\" on 15 September and \"Hiram na Alaala\" on 22 September. \"Ilustrado\", the first BayaniSerye on Philippine Primetime, had a world premiere on all feeds of GMA Pinoy TV on 20 October 2014; this was simultaneous with the premiere in the Philippines and this is the first time such has happened in GMA Pinoy TV history.\n\nOn December 2014, GMA Pinoy TV updated its on-air title screen and graphics and upper-right logo to a Christmas-themed one, alongside GMA's Christmas theme for 2014, \"Share the Love\". On December 31, 2014, \"Countdown to 2015: The GMA New Year Countdown Special\" aired live on GMA Pinoy TV.\n\nThe GMA Pinoy TV on-screen graphics and logo returned to its regular version on 4 January 2015, marking the end of the Christmas season. In the 1st quarter of 2015, in response to viewers' requests and to remain competitive in the international market, GMA Pinoy TV began to increase the number of shows with English subtitles, now including daily dramas like \"Let the Love Begin\" and \"The Rich Man's Daughter\".\n\nOn May 10, 2015, Willie Revillame's \"Wowowin\" premiered on GMA Network and on GMA Pinoy TV, serving as his return to the Kapuso Network. It is noted that Revillame's popular \"Wowowee\" aired for five years on GMA and GMA Pinoy TV's rival stations.\n\nOn July 27, 2015, the sixth and final State of the Nation Address of President Benigno S. Aquino II aired live in the Asia Pacific and aired on a delay basis in North America, North Africa, the Middle East and Europe.\n\nIn mid-2015 GMA's programming changed from news to other programs to four-five hours programming of Pinoy Telenovela block starting with its AfternoonPrime to GMA telebabad Programming with some programs one day delayed however, has caught up throughout with its future schedules. Eat Bulaga airs its initial programming, and Wowowin, Daily news as well and other programming.\n\nOn May to June 2011, GMA Pinoy TV and GMA Life TV sent many of its artists to various places worldwide, it's the first time that Kapuso artists were sent almost consecutively, to celebrate Philippine Independence Day, Filipino Culture Fiestas and to promote GMA Pinoy TV and GMA Life TV and its shows. They consisted of Iza Calzado & Dennis Trillo who went to Hawaii on 7 May 2011, then on 28 May 2011, Jolina Magdangal and JayR visited National City in San Diego, CA. On the 113th Philippine Independence Day proper, Dennis Trillo & Iza Calzado were sent this time to Hong Kong to represent GMA Pinoy TV and perform there and in Singapore, Dingdong Dantes and Rachel Ann Go were present. And on 25 June 2011, Carla Abellana, JayR and Frencheska Farr along with Arnel Ignacio are heading off to Toronto, Canada to be a part of the Pinoy Fiesta Trade Show there. A day after, 26 June 2011, Aljur Abrenica, Jennylyn Mercado, Iza Calzado and Mark Bautista will be sent to be part of Philippine-American Friendship Day in Jersey City, New Jersey.\nOn 2 October 2011, Heart Evangelista, Daniel Matsunaga, and former \"Eat Bulaga\" host, Kim Idol graced an event in Australia celebrating the launch of GMA News TV International as well as the strengthening presence of GMA Pinoy TV and GMA Life TV.\n\nOn 16 September 2012, in celebration of GMA Pinoy TV's 7th anniversary, \"Manny Many Prizes' held a special event at Universal Studios Hollywood. The live show featured the show's cast plus Dingdong Dantes, Solenn Heussaff, Julie Anne San Jose, Elmo Magalona and Rochelle Pangilinan.\n\nTo celebrate GMA Pinoy TV's 10th anniversary, GMA Pinoy TV rolled out a series of concerts, entitled \"Kapusong Pinoy\" followed by the name of the place as well as appearances at Filipino cultural events throughout the world, mainly in June and July to celebrate the Philippines' 117th Independence Day. GMA Pinoy TV used the hashtag #Sampuso to celebrate and highlight its 10th year of bringing Filipinos closer to home. The first two concerts were originally supposed to include GMA's Primetime Queen Marian Rivera-Dantes but because of her pregnancy, she chose to withdraw from the concerts for her and her child's safety and AiAi delas Alas served as her replacement in the first two concerts. The first Sampuso concert was on May 29, 2015 (originally slated for April 24, 2015), at the Orpheum Theater in Vancouver, Canada, featuring AiAi delas Alas, Christian Bautista, Jonalyn Viray, Alden Richards, and Betong Sumaya as Antonietta of Bubble Gang. The second Sampuso concert was on May 31, 2015 (originally slated for April 26, 2015), at the City National Grove of Anaheim, California, featuring AiAi delas Alas, Christian Bautista, Jonalyn Viray, Alden Richards, and Betong Sumaya as Antonietta of Bubble Gang.\n\nGMA announced a third concert in celebration of GMA Pinoy TV's 10th anniversary. The concert took place in New York at the Town Hall on September 5, 2015, featuring Primetime King Dingdong Dantes, AiAi delas Alas, Julie Anne San Jose, Christian Bautista, and Betong Sumaya as Antonietta of Bubble Gang. There are more concerts in celebration of GMA Pinoy TV's 10th Anniversary slated.\n\n"}
{"id": "113728", "url": "https://en.wikipedia.org/wiki?curid=113728", "title": "Geothermal energy", "text": "Geothermal energy\n\nGeothermal energy is thermal energy generated and stored in the Earth. Thermal energy is the energy that determines the temperature of matter. The geothermal energy of the Earth's crust originates from the original formation of the planet and from radioactive decay of materials (in currently uncertain but possibly roughly equal proportions). The geothermal gradient, which is the difference in temperature between the core of the planet and its surface, drives a continuous conduction of thermal energy in the form of heat from the core to the surface. The adjective \"geothermal\" originates from the Greek roots \"γη (ge)\", meaning earth, and \"θερμος (thermos)\", meaning hot.\n\nEarth's internal heat is thermal energy generated from radioactive decay and continual heat loss from Earth's formation. Temperatures at the core–mantle boundary may reach over 4000 °C (7,200 °F). The high temperature and pressure in Earth's interior cause some rock to melt and solid mantle to behave plastically, resulting in portions of the mantle convecting upward since it is lighter than the surrounding rock. Rock and water is heated in the crust, sometimes up to 370 °C (700 °F).\n\nWith water from hot springs, geothermal energy has been used for bathing since Paleolithic times and for space heating since ancient Roman times, but it is now better known for electricity generation. Worldwide, 11,700 megawatts (MW) of geothermal power was available in 2013. An additional 28 gigawatts of direct geothermal heating capacity is installed for district heating, space heating, spas, industrial processes, desalination and agricultural applications as of 2010.\n\nGeothermal power is cost-effective, reliable, sustainable, and environmentally friendly, but has historically been limited to areas near tectonic plate boundaries. Recent technological advances have dramatically expanded the range and size of viable resources, especially for applications such as home heating, opening a potential for widespread exploitation. Geothermal wells release greenhouse gases trapped deep within the earth, but these emissions are much lower per energy unit than those of fossil fuels.\n\nThe Earth's geothermal resources are theoretically more than adequate to supply humanity's energy needs, but only a very small fraction may be profitably exploited. Drilling and exploration for deep resources is very expensive. Forecasts for the future of geothermal power depend on assumptions about technology, energy prices, subsidies, plate boundary movement and interest rates. Pilot programs like EWEB's customer opt in Green Power Program show that customers would be willing to pay a little more for a renewable energy source like geothermal. But as a result of government assisted research and industry experience, the cost of generating geothermal power has decreased by 25% over the past two decades. In 2001, geothermal energy costs between two and ten US cents per kWh. \n\nHot springs have been used for bathing at least since Paleolithic times. The oldest known spa is a stone pool on China's Lisan mountain built in the Qin Dynasty in the 3rd century BC, at the same site where the Huaqing Chi palace was later built. In the first century AD, Romans conquered \"Aquae Sulis\", now Bath, Somerset, England, and used the hot springs there to feed public baths and underfloor heating. The admission fees for these baths probably represent the first commercial use of geothermal power. The world's oldest geothermal district heating system in Chaudes-Aigues, France, has been operating since the 14th century. The earliest industrial exploitation began in 1827 with the use of geyser steam to extract boric acid from volcanic mud in Larderello, Italy.\n\nIn 1892, America's first district heating system in Boise, Idaho was powered directly by geothermal energy, and was copied in Klamath Falls, Oregon in 1900. The first known building in the world to utilize geothermal energy as its primary heat source was the Hot Lake Hotel in Union County, Oregon, whose construction was completed in 1907. A deep geothermal well was used to heat greenhouses in Boise in 1926, and geysers were used to heat greenhouses in Iceland and Tuscany at about the same time. Charlie Lieb developed the first downhole heat exchanger in 1930 to heat his house. Steam and hot water from geysers began heating homes in Iceland starting in 1943.\n\nIn the 20th century, demand for electricity led to the consideration of geothermal power as a generating source. Prince Piero Ginori Conti tested the first geothermal power generator on 4 July 1904, at the same Larderello dry steam field where geothermal acid extraction began. It successfully lit four light bulbs. Later, in 1911, the world's first commercial geothermal power plant was built there. It was the world's only industrial producer of geothermal electricity until New Zealand built a plant in 1958. In 2012, it produced some 594 megawatts.\n\nLord Kelvin invented the heat pump in 1852, and Heinrich Zoelly had patented the idea of using it to draw heat from the ground in 1912. But it was not until the late 1940s that the geothermal heat pump was successfully implemented. The earliest one was probably Robert C. Webber's home-made 2.2 kW direct-exchange system, but sources disagree as to the exact timeline of his invention. J. Donald Kroeker designed the first commercial geothermal heat pump to heat the Commonwealth Building (Portland, Oregon) and demonstrated it in 1946. Professor Carl Nielsen of Ohio State University built the first residential open loop version in his home in 1948. The technology became popular in Sweden as a result of the 1973 oil crisis, and has been growing slowly in worldwide acceptance since then. The 1979 development of polybutylene pipe greatly augmented the heat pump’s economic viability.\n\nIn 1960, Pacific Gas and Electric began operation of the first successful geothermal electric power plant in the United States at The Geysers in California. The original turbine lasted for more than 30 years and produced 11 MW net power.\n\nThe binary cycle power plant was first demonstrated in 1967 in the USSR and later introduced to the US in 1981. This technology allows the generation of electricity from much lower temperature resources than previously. In 2006, a binary cycle plant in Chena Hot Springs, Alaska, came on-line, producing electricity from a record low fluid temperature of .\n\nThe International Geothermal Association (IGA) has reported that 10,715 megawatts (MW) of geothermal power in 24 countries is online, which was expected to generate 67,246 GWh of electricity in 2010. This represents a 20% increase in online capacity since 2005. IGA projects growth to 18,500 MW by 2015, due to the projects presently under consideration, often in areas previously assumed to have little exploitable resources.\n\nIn 2010, the United States led the world in geothermal electricity production with 3,086 MW of installed capacity from 77 power plants. The largest group of geothermal power plants in the world is located at The Geysers, a geothermal field in California. The Philippines is the second highest producer, with 1,904 MW of capacity online. Geothermal power makes up approximately 27% of Philippine electricity generation.\n\nIn 2016, Indonesia set in third with 1,647 MW online behind USA at 3,450 MW and the Philippines at 1,870 MW, but Indonesia will become second due to an additional online 130 MW at the end of 2016 and 255 MW in 2017. Indonesia's 28,994 MW are the largest geothermal reserves in the world, and it is predicted to overtake the USA in the next decade.\n\nGeothermal electric plants were traditionally built exclusively on the edges of tectonic plates where high temperature geothermal resources are available near the surface. The development of binary cycle power plants and improvements in drilling and extraction technology enable enhanced geothermal systems over a much greater geographical range. Demonstration projects are operational in Landau-Pfalz, Germany, and Soultz-sous-Forêts, France, while an earlier effort in Basel, Switzerland was shut down after it triggered earthquakes. Other demonstration projects are under construction in Australia, the United Kingdom, and the United States of America.\n\nThe thermal efficiency of geothermal electric plants is low, around 10–23%, because geothermal fluids do not reach the high temperatures of steam from boilers. The laws of thermodynamics limits the efficiency of heat engines in extracting useful energy. Exhaust heat is wasted, unless it can be used directly and locally, for example in greenhouses, timber mills, and district heating. System efficiency does not materially affect operational costs as it would for plants that use fuel, but it does affect return on the capital used to build the plant. In order to produce more energy than the pumps consume, electricity generation requires relatively hot fields and specialized heat cycles. Because geothermal power does not rely on variable sources of energy, unlike, for example, wind or solar, its capacity factor can be quite large – up to 96% has been demonstrated. The global average was 73% in 2005.\n\nGeothermal energy comes in either \"vapor-dominated\" or \"liquid-dominated\" forms. Larderello and The Geysers are vapor-dominated. Vapor-dominated sites offer temperatures from 240 to 300 °C that produce superheated steam.\n\nLiquid-dominated reservoirs (LDRs) were more common with temperatures greater than and are found near young volcanoes surrounding the Pacific Ocean and in rift zones and hot spots. \"Flash plants\" are the common way to generate electricity from these sources. Pumps are generally not required, powered instead when the water turns to steam. Most wells generate 2-10 MWe. Steam is separated from liquid via cyclone separators, while the liquid is returned to the reservoir for reheating/reuse. As of 2013, the largest liquid system is Cerro Prieto in Mexico, which generates 750 MWe from temperatures reaching . The Salton Sea field in Southern California offers the potential of generating 2000 MWe.\n\nLower temperature LDRs (120–200 °C) require pumping. They are common in extensional terrains, where heating takes place via deep circulation along faults, such as in the Western US and Turkey. Water passes through a heat exchanger in a Rankine cycle binary plant. The water vaporizes an organic working fluid that drives a turbine. These binary plants originated in the Soviet Union in the late 1960s and predominate in new US plants. Binary plants have no emissions.\n\nLower temperature sources produce the energy equivalent of 100M BBL per year. Sources with temperatures of 30–150 °C are used without conversion to electricity as district heating, greenhouses, fisheries, mineral recovery, industrial process heating and bathing in 75 countries. Heat pumps extract energy from shallow sources at 10–20 °C in 43 countries for use in space heating and cooling. Home heating is the fastest-growing means of exploiting geothermal energy, with global annual growth rate of 30% in 2005 and 20% in 2012.\n\nApproximately 270 petajoules (PJ) of geothermal heating was used in 2004. More than half went for space heating, and another third for heated pools. The remainder supported industrial and agricultural applications. Global installed capacity was 28 GW, but capacity factors tend to be low (30% on average) since heat is mostly needed in winter. Some 88 PJ for space heating was extracted by an estimated 1.3 million geothermal heat pumps with a total capacity of 15 GW.\n\nHeat for these purposes may also be extracted from co-generation at a geothermal electrical plant.\n\nHeating is cost-effective at many more sites than electricity generation. At natural hot springs or geysers, water can be piped directly into radiators. In hot, dry ground, earth tubes or downhole heat exchangers can collect the heat. However, even in areas where the ground is colder than room temperature, heat can often be extracted with a geothermal heat pump more cost-effectively and cleanly than by conventional furnaces. These devices draw on much shallower and colder resources than traditional geothermal techniques. They frequently combine functions, including air conditioning, seasonal thermal energy storage, solar energy collection, and electric heating. Heat pumps can be used for space heating essentially anywhere.\n\nIceland is the world leader in direct applications. Some 92.5% of its homes are heated with geothermal energy, saving Iceland over $100 million annually in avoided oil imports. Reykjavík, Iceland has the world's biggest district heating system, often used to heat pathways and roads to hinder the accumulation of ice. Once known as the most polluted city in the world, it is now one of the cleanest.\n\nEnhanced geothermal systems (EGS) actively inject water into wells to be heated and pumped back out. The water is injected under high pressure to expand existing rock fissures to enable the water to freely flow in and out. The technique was adapted from oil and gas extraction techniques. However, the geologic formations are deeper and no toxic chemicals are used, reducing the possibility of environmental damage. Drillers can employ directional drilling to expand the size of the reservoir.\n\nSmall-scale EGS have been installed in the Rhine Graben at Soultz-sous-Forêts in France and at Landau and Insheim in Germany.\n\nGeothermal power requires no fuel (except for pumps), and is therefore immune to fuel cost fluctuations. However, capital costs are significant. Drilling accounts for over half the costs, and exploration of deep resources entails significant risks. A typical well doublet (extraction and injection wells) in Nevada can support 4.5 megawatts (MW) and costs about $10 million to drill, with a 20% failure rate.\n\nIn total, electrical plant construction and well drilling cost about €2–5 million per MW of electrical capacity, while the break–even price is 0.04–0.10 € per kW·h. Enhanced geothermal systems tend to be on the high side of these ranges, with capital costs above $4 million per MW and break–even above $0.054 per kW·h in 2007. Direct heating applications can use much shallower wells with lower temperatures, so smaller systems with lower costs and risks are feasible. Residential geothermal heat pumps with a capacity of 10 kilowatt (kW) are routinely installed for around $1–3,000 per kilowatt. District heating systems may benefit from economies of scale if demand is geographically dense, as in cities and greenhouses, but otherwise piping installation dominates capital costs. The capital cost of one such district heating system in Bavaria was estimated at somewhat over 1 million € per MW. Direct systems of any size are much simpler than electric generators and have lower maintenance costs per kW·h, but they must consume electricity to run pumps and compressors. Some governments subsidize geothermal projects.\n\nGeothermal power is highly scalable: from a rural village to an entire city.\n\nThe most developed geothermal field in the United States is The Geysers in Northern California.\n\nGeothermal projects have several stages of development. Each phase has associated risks. At the early stages of reconnaissance and geophysical surveys, many projects are cancelled, making that phase unsuitable for traditional lending. Projects moving forward from the identification, exploration and exploratory drilling often trade equity for financing.\n\nThe Earth's internal thermal energy flows to the surface by conduction at a rate of 44.2 terawatts (TW), and is replenished by radioactive decay of minerals at a rate of 30 TW. These power rates are more than double humanity’s current energy consumption from all primary sources, but most of this energy flow is not recoverable. In addition to the internal heat flows, the top layer of the surface to a depth of is heated by solar energy during the summer, and releases that energy and cools during the winter.\n\nOutside of the seasonal variations, the geothermal gradient of temperatures through the crust is 25–30 °C (77–86 °F) per kilometer of depth in most of the world. The conductive heat flux averages 0.1 MW/km. These values are much higher near tectonic plate boundaries where the crust is thinner. They may be further augmented by fluid circulation, either through magma conduits, hot springs, hydrothermal circulation or a combination of these.\n\nA geothermal heat pump can extract enough heat from shallow ground anywhere in the world to provide home heating, but industrial applications need the higher temperatures of deep resources. The thermal efficiency and profitability of electricity generation is particularly sensitive to temperature. The most demanding applications receive the greatest benefit from a high natural heat flux, ideally from using a hot spring. The next best option is to drill a well into a hot aquifer. If no adequate aquifer is available, an artificial one may be built by injecting water to hydraulically fracture the bedrock. This last approach is called hot dry rock geothermal energy in Europe, or enhanced geothermal systems in North America. Much greater potential may be available from this approach than from conventional tapping of natural aquifers.\n\nEstimates of the potential for electricity generation from geothermal energy vary sixfold, from depending on the scale of investments. Upper estimates of geothermal resources assume enhanced geothermal wells as deep as , whereas existing geothermal wells are rarely more than deep. Wells of this depth are now common in the petroleum industry. The deepest research well in the world, the Kola superdeep borehole, is deep.\n\nMyanmar Engineering Society has identified at least 39 locations (in Myanmar) capable of geothermal power production and some of these hydrothermal reservoirs lie quite close to Yangon which is a significant underutilized resource.\n\nAccording to the Geothermal Energy Association (GEA) installed geothermal capacity in the United States grew by 5%, or 147.05 MW, ce the last annual survey in March 2012. This increase came from seven geothermal projects that began production in 2012. GEA also revised its 2011 estimate of installed capacity upward by 128 MW, bringing current installed U.S. geothermal capacity to 3,386 MW.\n\nGeothermal power is considered to be renewable because any projected heat extraction is small compared to the Earth's heat content. The Earth has an internal heat content of 10 joules (3·10 TW·hr), approximately 100 billion times current (2010) worldwide annual energy consumption. About 20% of this is residual heat from planetary accretion, and the remainder is attributed to higher radioactive decay rates that existed in the past. Natural heat flows are not in equilibrium, and the planet is slowly cooling down on geologic timescales. Human extraction taps a minute fraction of the natural outflow, often without accelerating it.\n\nGeothermal power is also considered to be sustainable thanks to its power to sustain the Earth’s intricate ecosystems. By using geothermal sources of energy present generations of humans will not endanger the capability of future generations to use their own resources to the same amount that those energy sources are presently used. Further, due to its low emissions geothermal energy is considered to have excellent potential for mitigation of global warming.\n\nEven though geothermal power is globally sustainable, extraction must still be monitored to avoid local depletion. Over the course of decades, individual wells draw down local temperatures and water levels until a new equilibrium is reached with natural flows. The three oldest sites, at Larderello, Wairakei, and the Geysers have experienced reduced output because of local depletion. Heat and water, in uncertain proportions, were extracted faster than they were replenished. If production is reduced and water is reinjected, these wells could theoretically recover their full potential. Such mitigation strategies have already been implemented at some sites. The long-term sustainability of geothermal energy has been demonstrated at the Lardarello field in Italy since 1913, at the Wairakei field in New Zealand since 1958, and at The Geysers field in California since 1960.\n\nFalling electricity production may be boosted through drilling additional supply boreholes, as at Poihipi and Ohaaki. The Wairakei power station has been running much longer, with its first unit commissioned in November 1958, and it attained its peak generation of 173MW in 1965, but already the supply of high-pressure steam was faltering, in 1982 being derated to intermediate pressure and the station managing 157MW. Around the start of the 21st century it was managing about 150MW, then in 2005 two 8MW isopentane systems were added, boosting the station's output by about 14MW. Detailed data are unavailable, being lost due to re-organisations. One such re-organisation in 1996 causes the absence of early data for Poihipi (started 1996), and the gap in 1996/7 for Wairakei and Ohaaki; half-hourly data for Ohaaki's first few months of operation are also missing, as well as for most of Wairakei's history.\n\nFluids drawn from the deep earth carry a mixture of gases, notably carbon dioxide (), hydrogen sulfide (), methane () and ammonia (). These pollutants contribute to global warming, acid rain, and noxious smells if released. Existing geothermal electric plants emit an average of of per megawatt-hour (MW·h) of electricity, a small fraction of the emission intensity of conventional fossil fuel plants. Plants that experience high levels of acids and volatile chemicals are usually equipped with emission-control systems to reduce the exhaust.\n\nIn addition to dissolved gases, hot water from geothermal sources may hold in solution trace amounts of toxic elements such as mercury, arsenic, boron, and antimony. These chemicals precipitate as the water cools, and can cause environmental damage if released. The modern practice of injecting cooled geothermal fluids back into the Earth to stimulate production has the side benefit of reducing this environmental risk.\n\nDirect geothermal heating systems contain pumps and compressors, which may consume energy from a polluting source. This parasitic load is normally a fraction of the heat output, so it is always less polluting than electric heating. However, if the electricity is produced by burning fossil fuels, then the net emissions of geothermal heating may be comparable to directly burning the fuel for heat. For example, a geothermal heat pump powered by electricity from a combined cycle natural gas plant would produce about as much pollution as a natural gas condensing furnace of the same size. Therefore, the environmental value of direct geothermal heating applications is highly dependent on the emissions intensity of the neighboring electric grid.\n\nPlant construction can adversely affect land stability. Subsidence has occurred in the Wairakei field in New Zealand. In Staufen im Breisgau, Germany, tectonic uplift occurred instead, due to a previously isolated anhydrite layer coming in contact with water and turning into gypsum, doubling its volume.\nEnhanced geothermal systems can trigger earthquakes as part of hydraulic fracturing. The project in Basel, Switzerland was suspended because more than 10,000 seismic events measuring up to 3.4 on the Richter Scale occurred over the first 6 days of water injection.\n\nGeothermal has minimal land and freshwater requirements. Geothermal plants use per gigawatt of electrical production (not capacity) versus and for coal facilities and wind farms respectively. They use of freshwater per MW·h versus over per MW·h for nuclear, coal, or oil.\n\nSome of the legal issues raised by geothermal energy resources include questions of ownership and allocation of the resource, the grant of exploration permits, exploitation rights, royalties, and the extent to which geothermal energy issues have been recognized in existing planning and environmental laws. Other questions concern overlap between geothermal and mineral or petroleum tenements. Broader issues concern the extent to which the legal framework for encouragement of renewable energy assists in encouraging geothermal industry innovation and development.\n\n\n\n"}
{"id": "27437133", "url": "https://en.wikipedia.org/wiki?curid=27437133", "title": "Haltenpipe", "text": "Haltenpipe\n\nHaltenpipe is a gas transport system which consists of a long pipeline from the Heidrun field to Tjeldbergodden. It started operation in December 1996, and is operated by Gassco. Transport capacity is 2.2. billion cubic metres per year.\n\nThe Haltenpipe PAD () was approved by the Norwegian Parliament in February 1992. The construction of the pipeline started in 1994, and was finished in November 1996. The Haltenpipe Joint Venture was established in June 1995. The operatorship of Haltenpipe has been transferred from Statoil to Gassco. The licence will expire at end of 2020. The current licencees are Petoro, Statoil, ConocoPhilips and Eni.\n\nNatural gas from the Heidrun field is delivered in part through the Åsgard Transport system to Kårstø, and in part through the Haltenpipe gasline to Tjeldbergodden. A methanol plant built by Statoil and ConocoPhilips is located near the gas terminal at Tjeldbergodden. The annual delivery of Heidrun gas to the methanol plant is approximately 700 million cubic metres. The methanol plant initiated production in June 1997. Gas is also delivered to a nearby gas fractionation and liquefaction plant, which has an annual capacity of 35 million cubic metres. The total recoverable gas reserves at the Heidrun field has been estimated to be 41.6 billion cubic metres, while the estimated remaining amount as of 31 December 2008 was 30.1 billion cubic metres.\n\n"}
{"id": "56398219", "url": "https://en.wikipedia.org/wiki?curid=56398219", "title": "History of speciation", "text": "History of speciation\n\nThe scientific study of speciation — how species evolve to become new species — began around the time of Charles Darwin in the middle of the 19th century. Many naturalists at the time recognized the relationship between biogeography (the way species are distributed) and the evolution of species. The 20th century saw the growth of the field of speciation, with major contributors such as Ernst Mayr researching and documenting species' geographic patterns and relationships. The field grew in prominence with the modern evolutionary synthesis in the early part of that century. Since then, research on speciation has expanded immensely.\n\nThe language of speciation has grown more complex. Debate over classification schemes on the mechanisms of speciation and reproductive isolation continue. The 21st century has seen a resurgence in the study of speciation, with new techniques such as molecular phylogenetics and systematics. Speciation has largely been divided into discrete modes that correspond to rates of gene flow between two incipient populations. Today however, research has driven the development of alternative schemes and the discovery of new processes of speciation.\n\nCharles Darwin introduced the idea that species could evolve and split into separate lineages, referring to it as \"specification\" in his 1859 book \"On the Origin of Species\". It was not until 1906 that the modern term \"speciation\" was coined by the biologist Orator F. Cook. Darwin, in his 1859 publication, focused primarily on the changes that can occur within a species, and less on how species may divide into two. It is almost universally accepted that Darwin's book did not directly address its title. Darwin instead saw speciation as occurring by species entering new ecological niches.\n\nControversy exists as to whether Charles Darwin recognized a true geographical-based model of speciation in his publication \"On the Origin of Species\". In chapter 11, \"Geographical Distribution\", Darwin discusses geographic barriers to migration, stating for example that \"barriers of any kind, or obstacles to free migration, are related in a close and important manner to the differences between the productions of various regions [of the world]\". F. J. Sulloway contends that Darwin's position on speciation was \"misleading\" at the least and may have later misinformed Wagner and David Starr Jordan into believing that Darwin viewed sympatric speciation as the most important mode of speciation. Nevertheless, Darwin never fully accepted Wagner's concept of geographical speciation.\n\nThe evolutionary biologist James Mallet maintains that the mantra repeated concerning Darwin's \"Origin of Species\" book having never actually discussed speciation is specious. The claim began with Thomas Henry Huxley and George Romanes (contemporaries of Darwin's), who declared that Darwin failed to explain the origins of inviability and sterility in hybrids. Similar claims were promulgated by the mutationist school of thought during the late 20th century, and even after the modern evolutionary synthesis by Richard Goldschmidt. Another strong proponent of this view about Darwin came from Mayr. Mayr maintained that Darwin was unable to address the problem of speciation, as he did not define species using the biological species concept. However, Mayr's view has not been entirely accepted, as Darwin's transmutation notebooks contained writings concerning the role of isolation in the splitting of species. Furthermore, Many of Darwin's ideas on speciation largely match the modern theories of both adaptive radiation and ecological speciation.\n\nRecognition of geographic factors involved in species populations was present even before Darwin, with many naturalists aware of the role of isolation in species relationships. In 1833, C. L. Gloger published \"The Variation of Birds Under the Influence of Climate\" in which he described geographic variations, but did not recognize that geographic isolation was an indicator of past speciation events. Another naturalist in 1856, Wollaston, studied island beetles in comparison to mainland species. He saw isolation as key to their differentiation. However, he did not recognize that the pattern was due to speciation. One naturalist, Leopold von Buch (1825) did recognize the geographic patterns and explicitly stated that geographic isolation may lead to species separating into new species. Mayr suggests that Von Buch was likely the first naturalist to truly suggest geographic speciation. Other naturalists, such as Henry Walter Bates (1863), recognized and accepted the patterns as evidence of speciation, but in Bate's case, did not propose a coherent model.\n\nIn 1868, Moritz Wagner was the first to propose the concept of geographic speciation in which he used the term \"Separationstheorie\". Edward Bagnall Poulton, the evolutionary biologist and a strong proponent of the importance of natural selection, highlighted the role of geographic isolation in promoting speciation, in the process coining the term \"sympatric speciation\" in 1904.\n\nWagner and other naturalists who studied the geographic distributions of animals, such as Karl Jordan and David Starr Jordan, noticed that closely related species were often geographically isolated from one another (allopatrically distributed) which lead to the advocation of the importance of geographic isolation in the origin of species. Karl Jordan is thought to have recognized the unification of mutation and isolation in the origin of new species — in stark contrast to the prevailing views at the time. David Starr Jordan reiterated Wagner's proposal in 1905, providing a wealth of evidence from nature to support the theory, and asserting that geographic isolation is obvious but had been unfortunately ignored by most geneticists and experimental evolutionary biologists at the time. Joel Asaph Allen suggested the observed pattern of geographic separation of closely related species be called \"Jordan's Law\" (or Wagner's Law). Despite the contentions, most taxonomists did accept the geographic model of speciation.\n\nMany of the early terms used to describe speciation were outlined by Ernst Mayr. He was the first to encapsulate the then contemporary literature in his 1942 publication \"Systematics and the Origin of Species, from the Viewpoint of a Zoologist\" and in his subsequent 1963 publication \"Animal Species and Evolution\". Like Jordan's works, they relied on direct observations of nature, documenting the occurrence of geographic speciation. He described the three modes: geographic, semi-geographic, and non-geographic; which today, are referred to as allopatric, parapatric, and sympatric respectively. Mayr's 1942 publication, influenced heavily by the ideas of Karl Jordan and Poulton, was regarded as the authoritative review of speciation for over 20 years—and is still valuable today.\n\nA major focus of Mayr's works was on the importance of geography in facilitating speciation; with islands often acting as a central theme to many of the speciation concepts put forth. One of which was the concept of peripatric speciation, a variant of allopatric speciation (he has since distinguished the two modes by referring to them as peripatric and dichopatric). This concept arose by an interpretation of Wagner's \"Separationstheorie\" as a form of founder effect speciation that focused on small geographically isolated species. This model was later expanded and modified to incorporate sexual selection by Kenneth Y. Kaneshiro in 1976 and 1980.\n\nMany geneticists at the time did little to bridge the gap between the genetics of natural selection and the origin of reproductive barriers between species. Ronald Fisher proposed a model of speciation in his 1930 publication \"The Genetical Theory of Natural Selection\", where he described disruptive selection acting on sympatric or parapatric populations — with reproductive isolation completed by reinforcement. Other geneticists such as J. B. S. Haldane did not even recognize that species were real, while Sewall Wright ignored the topic, despite accepting allopatric speciation.\n\nThe primary contributors to the incorporation of speciation into modern evolutionary synthesis were Ernst Mayr and Theodosius Dobzhansky. Dobzhansky, a geneticist, published \"Genetics and the Origin of Species\" in 1937, in which he formulated the genetic framework for how speciation could occur. He recognized that speciation was an unsolved problem in biology at the time, rejecting Darwin’s position that new species arose by occupation of new niches — contending that reproductive isolation was instead based on barriers to gene flow. Subsequently, Mayr conducted extensive work on the geography of species, emphasizing the importance of geographic separation and isolation, in which he filled Dobzhansky‘s gaps concerning the origin of biodiversity (in his 1942 book). Both of their works gave rise, not without controversy, to the modern understanding of speciation; stimulating a wealth of research on the topic. Furthermore, this extended to plants as well as animals with G. Ledyard Stebbins’s book, \"Variation and Evolution in Plants\" and the much later, 1981 book, \"Plant Speciation\" by Verne Grant.\n\nIn 1947, \"a consensus had been achieved among geneticists, paleontologists and systematists and that evolutionary biology as an independent biological discipline had been established\" during a Princeton University conference. This 20th century synthesis incorporated speciation. Since then, the ideas have been consistently and repeatedly confirmed.\n\nAfter the synthesis, speciation research continued largely within natural history and biogeography — with much less emphasis on genetics. The study of speciation has seen its largest increase since the 1980s with an influx of publications and a host of new terms, methods, concepts, and theories. This \"third phase\" of work — as Jerry A. Coyne and H. Allen Orr put it — has led to a growing complexity of the language used to describe the many processes of speciation. The research and literature on speciation have become, \"enormous, scattered, and increasingly technical\".\n\nFrom the 1980s, new research tools increased the robustness of research, assisted by new methods, theoretical frameworks, models, and approaches. Coyne and Orr discuss the modern, post-1980s developments centered around five major themes:\n\nEcologists became aware that the ecological factors behind speciation were under-represented. This saw the growth in research concerning ecology's role in facilitating speciation — rightly designated ecological speciation. This focus on ecology generated a host of new terms relating to the barriers to reproduction (\"e.g.\" allochronic speciation, in which gene flow is reduced or removed by timing of breeding periods; or habitat isolation, in which species occupy different habitats within the same area). Sympatric speciation, regarded by Mayr as unlikely, has become widely accepted. Research on the influence of natural selection on speciation, including the process of reinforcement, has grown.\n\nResearchers have long debated the roles of sexual selection, natural selection, and genetic drift in speciation. Darwin extensively discussed sexual selection, with his work greatly expanded on by Ronald Fisher; however, it was not until 1983 that the biologist Mary Jane West-Eberhard recognized the importance of sexual selection in speciation. Natural selection plays a role in that any selection towards reproductive isolation can result in speciation — whether indirectly or directly. Genetic drift has been widely researched from the 1950s onwards, especially with peak-shift models of speciation by genetic drift. Mayr championed founder effects, in which isolated individuals, like those found on islands near a mainland, experience a strong population bottleneck, as they contain only a small sample of the genetic variation in the main population. Later, other biologists such as Hampton L. Carson, Alan Templeton, Sergey Gavrilets, and Alan Hastings developed related models of speciation by genetic drift, noting that islands were inhabited mostly by endemic species. Selection's role in speciation is widely supported, whereas founder effect speciation is not, having been subject to a number of criticisms.\n\nThroughout the history of research concerning speciation, classification and delineation of modes and processes have been debated. Julian Huxley divided speciation into three separate modes: geographical speciation, genetic speciation, and ecological speciation. Sewall Wright proposed ten different, varying modes. Ernst Mayr championed the importance of physical, geographic separation of species populations, maintaining it to be of major importance to speciation. He originally proposed the three primary modes known today: geographic, semi-geographic, non-geographic; corresponding to allopatric, parapatric, and sympatric respectively.\n\nThe phrase \"modes of speciation\" is imprecisely defined, most often indicating speciation occurring as a result of a species geographic distribution. More succinctly, the modern classification of speciation is often described as occurring on a gene flow continuum (i.e., allopatry at formula_1 and sympatry at formula_2) This gene flow concept views speciation as based on the exchange of genes between populations instead of seeing a purely geographic setting as necessarily relevant. Despite this, concepts of biogeographic modes can be translated into models of gene flow (such as that in the image at left); however, this translation has led to some confusion of language in the scientific literature.\nAs research has expanded over the decades, the geographic scheme has been challenged. The traditional classification is considered by some researchers to be obsolete, while others argue for its merits. Proponents of non-geographic schemes often justify non-geographic classifications, not by rejection of the importance of reproductive isolation (or even the processes themselves), but instead by the fact that it simplifies the complexity of speciation. One major critique of the geographic framework is that it arbitrarily separates a biological continuum into discontinuous groups. Another criticism rests with the fact that, when speciation is viewed as a continuum of gene flow, parapatric speciation becomes unreasonably represented by the entire continuum—with allopatric and sympatric existing in the extremes. Coyne and Orr argue that the geographic classification scheme is valuable in that biogeography controls the strength of the evolutionary forces at play, as gene flow and geography are clearly linked. James Mallet and colleagues contend that the sympatric vs. allopatric dichotomy is valuable to determine the degree in which natural selection acts on speciation. Kirkpatrick and Ravigné categorize speciation in terms of its genetic basis or by the forces driving reproductive isolation. Here, the geographic modes of speciation are classified as types of assortive mating. Fitzpatrick and colleagues believe that the biogeographic scheme \"is a distraction that could be positively misleading if the real goal is to understand the influence of natural selection on divergence.\" They maintain that, to fully understand speciation, \"the spatial, ecological, and genetic factors\" involved in divergence must be explored. Sara Via recognizes the importance of geography in speciation but suggests that classification under this scheme be abandoned.\n\nSympatric speciation, from its beginnings with Darwin (who did not coin the term), has been a contentious issue. Mayr, along with many other evolutionary biologists, interpreted Darwins's view of speciation and the origin of biodiversity as arising by species entering new ecological niches—a form of sympatric speciation. Before Mayr, sympatric speciation was regarded as the primary mode of speciation. In 1963, Mayr provided a strong criticism, citing various flaws in the theory. After that, sympatric speciation fell out of favor with biologists and has only recently seen a resurgence in interest. Some biologists, such as James Mallet, believe that Darwin's view on speciation was misunderstood and misconstrued by Mayr. Today, sympatric speciation is supported by evidence from laboratory experiments and observations from nature.\n\nFor most of the history of speciation, hybridization (polyploidy) has been a contentious issue, as botanists and zoologists have traditionally viewed hybridization's role in speciation differently. Carl Linnaeus was the earliest to suggest hybridization in 1760, Øjvind Winge was the first to confirm allopolyploidy in 1917, and a later experiment conducted by Clausen and Goodspeed in 1925 confirmed the findings. Today it is widely recognized as a common mechanism of speciation.\n\nHistorically, zoologists considered hybridization to be a rare phenomenon, while botanists found it to be commonplace in plant species. The botanists G. Ledyard Stebbins and Verne Grant were two of the well known botanists who championed the idea of hybrid speciation during the 1950s to the 1980s. Hybrid speciation, also called polyploid speciation (or polyploidy) is speciation that results by an increase in the number of sets of chromosomes. It is effectively a form of sympatric speciation that happens instantly. Grant coined the term recombinational speciation in 1981; a special form of hybrid speciation where a new species results from hybridization and is itself, reproductively isolated from both its parents. Recently, biologists have increasingly recognized that hybrid speciation can occur in animals as well.\n\nThe concept of speciation by reinforcement has a complex history, with its popularity among scholars changing significantly over time. The theory of reinforcement experienced three phases of historical development:\n\nIt was originally proposed by Alfred Russel Wallace in 1889, termed the Wallace effect—a term rarely used by scientists today. Wallace's hypothesis differed from the modern conception in that it focused on post-zygotic isolation, strengthened by group selection. Dobzhansky was the first to provide a thorough, modern description of the process in 1937, though the actual term itself was not coined until 1955 by W. Frank Blair.\n\nIn 1930, Ronald Fisher laid out the first genetic description of the process of reinforcement in \"The Genetical Theory of Natural Selection\", and in 1965 and 1970 the first computer simulations were run to test for its plausibility. Later, population genetic and quantitative genetic studies were conducted showing that completely unfit hybrids lead to an increase in pre-zygotic isolation. After Dobzhansky's idea rose to the forefront of speciation research, it garnered significant support—with Dobzhansky suggesting that it illustrated the final step in speciation (e.g. after an allopatric population comes into secondary contact). In the 1980s, many evolutionary biologists began to doubt the plausibility of the idea, based not on empirical evidence, but largely on the growth of theory that deemed it an unlikely mechanism of reproductive isolation. A number of theoretical objections arose at the time. Since the early 1990s, reinforcement has seen a revival in popularity, with perceptions by evolutionary biologists accepting its plausibility—due primarily from a sudden increase in data, empirical evidence from laboratory studies and nature, complex computer simulations, and theoretical work.\n\nThe scientific language concerning reinforcement has also differed over time, with different researchers applying various definitions to the term. First used to describe the observed mating call differences in \"Gastrophryne\" frogs within a secondary contact hybrid zone, reinforcement has also been used to describe geographically separated populations that experience secondary contact. Roger Butlin demarcated incomplete post-zygotic isolation from complete isolation, referring to incomplete isolation as reinforcement and completely isolated populations as experiencing reproductive character displacement. Daniel J. Howard considered reproductive character displacement to represent either assortive mating or the divergence of traits for mate recognition (specifically between sympatric populations). Under this definition, it includes pre-zygotic divergence and complete post-zygotic isolation. Maria R. Servedio and Mohamed Noor consider any detected increase in pre-zygotic isolation as reinforcement, as long as it is a response to selection against mating between two different species. Coyne and Orr contend that, \"true reinforcement is restricted to cases in which isolation is enhanced between taxa that can still exchange genes\".\n\n"}
{"id": "35897925", "url": "https://en.wikipedia.org/wiki?curid=35897925", "title": "James Cook Collection: Australian Museum", "text": "James Cook Collection: Australian Museum\n\nThe Australian Museum's Cook Collection was acquired in 1894 when it was transferred from the Government of New South Wales. At that time it consisted of 115 artifacts collected on Captain James Cook's three voyages of discovery Throughout the Pacific Ocean, during the period 1768 - 1780, along with documents and memorabilia related to these voyages. Many of the ethnographic artifacts were collected at a time of first contact between Pacific Peoples and Europeans. In 1935 most of the documents and memorabilia were transferred to the Mitchell Library in the State Library of New South Wales. The provenance of the collection shows that the objects remained in the hands of Captain James Cook's widow, Mrs Elizabeth Cook and her descendants until 1886. In this year Mr John Mackrell, the great nephew of Isaac Smith (Royal Navy officer), Elizabeth Cook's cousin, organized the display of this collection at the request of the NSW Government at the Colonial and Indian Exhibition in London. In 1887 the London-based Agent-General for the New South Wales Government, Saul Samuel, bought John Mackrell's items and also acquired those items belonging to the other relatives Reverend Canon Frederick Bennett, Mrs Thomas Langton, H.M.C.Alexander and Mr William Adams. The collection remained with the Colonial Secretary of NSW until 1894, when it was transferred to the Australian Museum.\n\nCaptain James Cook, FRS, RN (7 November 1728 – 14 February 1779) was a British explorer, navigator and cartographer who ultimately rose to the rank of captain in the Royal Navy. Cook made detailed maps of Newfoundland prior to making three voyages to the Pacific Ocean, during which he achieved the first European contact with the eastern coastline of Australia and the Hawaiian Islands, as well as the first recorded circumnavigation of New Zealand.\n\nCook joined the British merchant navy as a teenager and joined the Royal Navy in 1755. He saw action in the Seven Years' War, and subsequently surveyed and mapped much of the entrance to the Saint Lawrence River during the siege of Quebec. This helped bring Cook to the attention of the Admiralty and Royal Society. This notice came at a crucial moment in both Cook's career and the direction of British overseas exploration, and led to his commission in 1768 as commander of HM Bark \"Endeavour\" for the first of three Pacific voyages.\n\nIn three voyages Cook sailed thousands of miles across largely uncharted areas of the globe. He mapped lands from New Zealand to Hawaii in the Pacific Ocean in greater detail and on a scale not previously achieved. As he progressed on his voyages of discovery he surveyed and named features, and recorded islands and coastlines on European maps for the first time. He displayed a combination of seamanship, superior surveying and cartographic skills, physical courage and an ability to lead men in adverse conditions.\n\nCook was killed in Hawaii in a fight with Hawaiians during his third exploratory voyage in the Pacific in 1779. He left a legacy of scientific and geographical knowledge which was to influence his successors well into the 20th century and numerous memorials worldwide have been dedicated to him. However, his role in opening areas of the Pacific to colonisation and its subsequent effects on indigenous peoples have been the subject of both political and scholarly debate.\n\nCaptain Cook first visited the Hawaiian Islands on 18 January 1778 on his third voyage, and his ships spent three months there, mostly anchored off Kauai and Hawaii. Cook considered them his most important discovery for England, and named the group the Sandwich Islands after his patron Lord Sandwich, First Lord of the Admiralty.\n\nThe objects acquired reflect the fact that the Hawaiians initially treated Cook as a god and presented him with high status goods such as feathered capes, hats, ornaments and images. As this was a point of first contact between Europeans and Hawaiians, a thriving trade developed, and a large number of artefacts, mostly those that were carried or worn, were brought back to England. Hawaiian collections from Cook's voyages show in particular the extent and variation of Hawaiian featherwork at the time.\n\nCook's ships returned to Hawaii nearly a year later after exploring the northwest coast of America, Alaska and Northeast Asia. They spent a few weeks without incident and departed only to return due to a problem with the ship. On 14 February 1779, Cook was killed on Hawaii due to an unfortunate misunderstanding.\n\nThe artifacts in the Australian Museum's Cook collection attributed to Hawaii include a feathered cloak, 4 feathered circlet ornaments, a foundation mat for a feather cloak, a woven cane helmet, a nose whistle, a shark's tooth ring, 3 miniature bone carved turtles, a shark's tooth knife and more than 20 cut pieces of barkcloth.\n\nCook's ships left Hawaii on 2 February 1778 and set course for North America, which they sighted on 7 March. On 29 March they discovered Nootka Sound. The area had been claimed by the Spanish under Juan José Pérez Hernández, 1774 and the English by Sir Francis Drake, 1579. Cook named the area King George Sound. The Resolution and Discovery spent about one month in the area while the ships were repaired and made ready for the expedition to find the northwest passage.\n\nA great deal of trading was done and the largest number of objects collected from the NorthWest Coast is from Nootka Sound. Objects from other groups may have been collected in Nootka. Wooden Combs were characteristic and several were collected (see H000110).\n\nThe next stop was Prince William Sound and Cook Inlet. The ethnographic specimens collected are very mixed and include Tlingit people and Athabaskan. Cook noted that none of the people lived in the bay where they had anchored. Containers of various kinds were made of mountain sheep horn. They appear to be Athabaskan in style (see H000056).\n\nIn Unalaska or the Aleutian Islands Samwell recorded that the crew traded cloth from Hawaii and Tahiti with the locals for arrows and other articles. Only a few objects can be traced to the voyages of Cook. Small ivory birds, possibly gaming pieces, were probably collected in Unalaska (see H000151). A large number of weapons and fishing and hunting implements, as well as a few paddles and canoe models, were collected in the northern parts of America and Asia and are hard to provenance (see H000106 and H00122 and H000139). \nCook visited New Caledonia for two weeks during his second voyage in 1774. Eight days were spent in the Balad area, where Cook participated in a ritual exchange of gifts. Not many artefacts were collected, and most of these were weapons, which the local people were happy to sell to the visitors.New Caledonian hardwood clubs were finely carved and highly polished. (H000147, H000148) Hodges depicts a spearthrower being worn as an ornament to a hat. The Australian Museum's collection includes 3 spearthrowers (one ornamented: H000119). 3 knitted bags for slingstones were collected and the Australian Museum holds one of these (H000115). The Australian Museum's collection also includes 2 combs (H000108,H000109).\n\nThe only known European contact with New Zealand prior to Cook's voyages was a short and hostile interaction when Abel Tasman visited Golden Bay on the north west tip of the South Island in 1642. On 8 October 1769 HMS Endeavour, after 2 months at sea, anchored in a bay on the eastern side of the North Island. This was a case of first contact, and Cook and his crew did not succeed in peaceful interactions with the local Māori group, so the ship left the area with Cook naming it Poverty Bay. However the Endeavour went on to circumnavigate both the North and South Islands and there was contact with many different Māori groups and exchanges of goods were made.\n\nNew Zealand was visited by French ships in 1769 and 1772, and Cook's ships returned on both the 2nd (1773) and third (1777) voyages. Kaeppler suggests that the availability of iron tools and the influence of European ideas and desires for certain goods may have stimulated changes in artefact production over the period of interactions.\n\nAs this AM collection came via Cook's widow and is therefore probably personal to Cook himself, it cannot be identified to a specific voyage, area or date. Kaeppler documents the types of artefacts collected from New Zealand during Cook's voyages. Over 40 cloaks were collected (H000103), a large number of body ornaments (H000222, H000156,H000063), feather boxes and musical instruments. Many weapons of nearly every type were collected, but this Cook collection has only one of these, a stone patu (H000085). There are 3 stone adze blades (H000082,83,84) and two fish hooks (H000137, H000138). Kaeppler notes that food preparation equipment was seldom collected, perhaps because exchanges happened away from the villages, and between men.\n\nThe island of Tahiti, the largest of the 14 Society Islands, was where Cook had his first contact with Pacific Islanders. Samuel Wallis had visited Tahiti on HMS Dolphin in June–July 1767. A brisk trade developed, and large quantities of iron nails and spikes were exchanged for barkcloth, fish hooks, pearls,shells, adzes and other tools. The Frenchman Louis-Antoine de Bougainville also visited in 1768, and the artefacts and stories brought back to Europe from these 2 voyages helped to create the vision of Tahiti as a south pacific paradise.\n\nThe Royal Society chose Tahiti as a suitable place to observe the astronomical event of the Transit of Venus, and for this purpose Cook was sent on his first voyage. On 13 April 1769, after eight months at sea, the Endeavour arrived at Tahiti's Matavai Bay, the same inlet that the Dolphin had visited 2 years before (4 men from the Dolphin had joined Cook's crew). The islanders welcomed Cook and Banks with green banana branches, calling them TAIO or friend, and giving them gifts of perfumed cloth. Soon afterwards a procession of canoes entered the bay and 'Queen' Purea presented Cook with a pig and fresh food,which was reciprocated by Cook with beads and trinkets.\n\nThe nearby promontory which Cook named Point Venus was used to observe the Transit of the planet Venus across the face of the Sun on 3 June 1769. Although this phenomenon was observed at 26 different points around the world, astronomers later realised that the telescopes of the day weren't precise enough to accomplish the task of accurately determining the distance between the Earth and the Sun.\n\nCook and Banks later circumnavigated the island of Tahiti and then visited the Bora Bora group, guided by a Tahitian priest Tupaia (navigator), who with his servant travelled on the Endeavour to Batavia, where he unfortunately became ill and died.\n\nThe Endeavour spent 3 months altogether in the Society Islands, and Cook's ships were to return 3 more times during later voyages (Aug-Sep1773, April-May1774,and Aug-Sep1777). During these visits there was a constant exchange of goods and gifts. Iron nails and axes were valued, and provisions, fish hooks, adzes and barkcloth were exchanged for these. A large number of objects were therefore collected, and many detailed drawings and maps of the islands were produced.\n\nThe items collected cannot be identified to a particular island or voyage, and before Cook arrived there had been a flourishing exchange of artefacts and ideas between the islands. Banks enjoyed the opportunities to witness and sometimes participate in Tahitian life. He took part in a HEIVA public dance ceremony, accompanied by drums and nose flute (seeH000143). He was also allowed to take part in a mourning ceremony and was fascinated by the costume worn by the chief mourner. According to Forster none of these were collected until the 2nd voyage, when parts of at least 10 were taken to England. Tha Australian Museum's collection has an example of the shell breast ornament component (see H000149). Semicircular breast ornaments, or gorgets, were commonly worn, and many were collected by the crew and valued as a special type of artificial curiosity (see H000105 and H000145). Barkcloth was presented in large pieces on several occasions to the visitors. Much of the cloth collected was cut into pieces and placed in books of barkcloth samples.\n\nThe Tongan islands were visited by Le Maire in 1616, and Abel Tasman in 1643, but there are no identified Tongan artefacts collected from these voyages. The northern islands were sighted by the Englishman Samuel Wallis in 1767, and Cook's ships visited the islands for a few days in 1773 and then again briefly in June1774. He named them the Friendly Islands and returned on his third voyage in April–July 1777. It was known that Wallis had traded iron nails for a club in 1767, and the iron tools traded during Cook's second voyage stimulated different carving techniques which are evident in wooden artefacts collected on the third voyage.\n\nMany specimens were collected from Tonga and many are specific to a particular voyage. Some items collected were of Fijian and Samoan origin, and this fact documents the pre-existing trade the Tongans had with their neighbours. Many body ornaments were collected, such as H000151- a small ivory human figure, probably strung on cord and worn as a neck ornament, and 3 shell necklaces H000116, H000117, H000152. Several kinds of pandanus mats were collected (H000098), as were many baskets of different styles and materials (H000102, a KATO MOSI KAKA beaded basket). Fly whisks, food preparation equipment, bowls, neckrests, noseflutes, panpipes, and various tools were collected, but the Australian Museum collection does not include examples of these. Some fishing equipment is present - a fish net H000144, and 11 fish hooks H000121- H000134. Combs collected were made from the midribs of coconut leaflets, intertwined with fine sennit cord to make decorative patterns.\n\nQuiros had named the islands of Vanuatu Australia Del Espiritu Santo, and Bougainville had visited in 1768. Cook made three landings on his second voyage on the islands of Malakula, Erromango, and Tanna, and renamed the group the New Hebrides. There was open warfare, except for Tanna, so it is likely that all the non-weapons in Cook collections came from Tanna. Panpipes (H000112) in the AM collection have been attributed to Tanna. The arrows H000158-H000170 have also been attributed to Vanuatu. There are three clubs in the Banks collection that may possibly have been collected on Cook's second voyage - H000292-H000295, H000366 - can be compared to those in the Forster collection in Göttingen.\n\n"}
{"id": "16569", "url": "https://en.wikipedia.org/wiki?curid=16569", "title": "James P. Hogan (writer)", "text": "James P. Hogan (writer)\n\nJames Patrick Hogan (27 June 1941 – 12 July 2010) was a British science fiction author.\n\nHogan was born in London, England. He was raised in the Portobello Road area on the west side of London. After leaving school at the age of sixteen, he worked various odd jobs until, after receiving a scholarship, he began a five-year program at the Royal Aircraft Establishment at Farnborough covering the practical and theoretical sides of electrical, electronic, and mechanical engineering. He first married at the age of twenty. He married three more times and fathered six children.\n\nHogan worked as a design engineer for several companies and eventually moved into sales in the 1960s, traveling around Europe as a sales engineer for Honeywell. In the 1970s he joined the Digital Equipment Corporation's Laboratory Data Processing Group and in 1977 moved to Boston, Massachusetts to run its sales training program. He published his first novel, \"Inherit the Stars\", in the same year to win an office bet.\n\nHe quit DEC in 1979 and began writing full-time, moving to Orlando, Florida, for a year where he met his third wife Jackie. They then moved to Sonora, California. Hogan died of a heart attack at his home in Ireland on Monday, 12 July 2010, aged 69.\n\nMost of Hogan's fiction is hard science fiction.\n\nHogan's fiction also reflects anti-authoritarian social views and as such forms part of anarchist science fiction. Many of his novels have strong anarchist or libertarian themes, often promoting the idea that new technological advances render certain social conventions obsolete. For example, the effectively limitless availability of energy that would result from the development of controlled nuclear fusion would make it unnecessary to limit access to energy resources. In essence, energy would become free. This melding of scientific and social speculation is clearly present in the novel \"Voyage from Yesteryear\" (strongly influenced by Eric Frank Russell's story \"And Then There Were None\") about a high-tech anarchist society in the Alpha Centauri system, a starship sent from Earth by a dictatorial government, and the events following their first contact. The story features concepts of civil disobedience, post scarcity and gift economy. \n\nIn his later years, Hogan's contrarian and anti-authoritarian views favored those widely considered \"fringe\". He was a proponent of Immanuel Velikovsky's version of catastrophism, and of the Peter Duesberg hypothesis that AIDS is caused by pharmaceutical use rather than HIV (see AIDS denialism). He criticized gradualism in evolution, though he did not propose theistic creationism as an alternative. Hogan was skeptical of the theories on climate change and ozone depletion.\n\nHogan also espoused the idea that the Holocaust did not happen in the manner described by mainstream historians, writing that he found the work of Arthur Butz and Mark Weber to be \"more scholarly, scientific, and convincing than what the history written by the victors says.\" Such theories were seen by many to contradict his views on scientific rationality; he repeatedly stated that these theories held his attention due to the high quality of their presentation – a quality he believed established sources should attempt to emulate, rather than resorting to attacking their originators. \n\nIn March 2010, in an essay defending Holocaust denier Ernst Zündel, Hogan stated that the mainstream history of the Holocaust includes \"claims that are wildly fantastic, mutually contradictory, and defy common sense and often physical possibility.\"\n\n\n\n\nCompilations of novels in the \"Giants series\".\n\n\n"}
{"id": "15944", "url": "https://en.wikipedia.org/wiki?curid=15944", "title": "Jet engine", "text": "Jet engine\n\nA jet engine is a type of reaction engine discharging a fast-moving jet that generates thrust by jet propulsion. This broad definition includes airbreathing jet engines (turbojets, turbofans, ramjets, and pulse jets). In general, jet engines are combustion engines.\n\nCommon parlance applies the term \"jet engine\" more narrowly, referring to various airbreathing jet engine, a type of reaction engine. These typically feature a rotating air compressor powered by a turbine, with the leftover power providing thrust via a propelling nozzle — this process is known as the Brayton thermodynamic cycle. Jet aircraft use such engines for long-distance travel. Early jet aircraft used turbojet engines which were relatively inefficient for subsonic flight. Most modern subsonic jet aircraft use more complex high-bypass turbofan engines. They give higher speed and greater fuel efficiency than piston and propeller aeroengines over long distances. A few air-breathing engines made for high speed applications (ramjets and scramjets) use the ram effect of the vehicle's speed instead of a mechanical compressor.\n\nThe thrust of a typical jetliner engine went from (de Havilland Ghost turbojet ) in the 1950s to (General Electric GE90 turbofan) in the 1990s, and their reliability went from 40 in-flight shutdowns per 100,000 engine flight hours to less than 1 per 100,000 in the late 1990s. This, combined with greatly decreased fuel consumption, permitted routine transatlantic flight by twin-engined airliners by the turn of the century, where before a similar journey would have required multiple fuel stops.\n\nJet engines date back to the invention of the aeolipile before the first century AD. This device directed steam power through two nozzles to cause a sphere to spin rapidly on its axis. It was seen as a curiosity.\n\nJet propulsion only gained practical applications with the invention of the gunpowder-powered rocket by the Chinese in the 13th century as a type of firework, and gradually progressed to propel formidable weaponry. Jet propulsion technology then stalled for hundreds of years.\n\nThe earliest attempts at airbreathing jet engines were hybrid designs in which an external power source first compressed air, which was then mixed with fuel and burned for jet thrust. The Caproni Campini N.1, and the Japanese Tsu-11 engine intended to power Ohka kamikaze planes towards the end of World War II were unsuccessful. \nEven before the start of World War II, engineers were beginning to realize that engines driving propellers were approaching limits due to issues related to propeller efficiency, which declined as blade tips approached the speed of sound. If aircraft performance were to increase beyond such a barrier, a different propulsion mechanism was necessary. This was the motivation behind the development of the gas turbine engine, the commonest form of jet engine.\n\nThe key to a practical jet engine was the gas turbine, extracting power from the engine itself to drive the compressor. The gas turbine was not a new idea: the patent for a stationary turbine was granted to John Barber in England in 1791. The first gas turbine to successfully run self-sustaining was built in 1903 by Norwegian engineer Ægidius Elling. Such engines did not reach manufacture due to issues of safety, reliability, weight and, especially, sustained operation.\n\nThe first patent for using a gas turbine to power an aircraft was filed in 1921 by Frenchman Maxime Guillaume. His engine was an axial-flow turbojet, but was never constructed, as it would have required considerable advances over the state of the art in compressors. Alan Arnold Griffith published \"An Aerodynamic Theory of Turbine Design\" in 1926 leading to experimental work at the RAE.\nIn 1928, RAF College Cranwell cadet Frank Whittle formally submitted his ideas for a turbojet to his superiors. In October 1929 he developed his ideas further. On 16 January 1930 in England, Whittle submitted his first patent (granted in 1932). The patent showed a two-stage axial compressor feeding a single-sided centrifugal compressor. Practical axial compressors were made possible by ideas from A.A.Griffith in a seminal paper in 1926 (\"An Aerodynamic Theory of Turbine Design\"). Whittle would later concentrate on the simpler centrifugal compressor only. Whittle was unable to interest the government in his invention, and development continued at a slow pace.\nIn 1935 Hans von Ohain started work on a similar design in Germany, both compressor and turbine being radial, on opposite sides of same disc, initially unaware of Whittle's work. Von Ohain's first device was strictly experimental and could run only under external power, but he was able to demonstrate the basic concept. Ohain was then introduced to Ernst Heinkel, one of the larger aircraft industrialists of the day, who immediately saw the promise of the design. Heinkel had recently purchased the Hirth engine company, and Ohain and his master machinist Max Hahn were set up there as a new division of the Hirth company. They had their first HeS 1 centrifugal engine running by September 1937. Unlike Whittle's design, Ohain used hydrogen as fuel, supplied under external pressure. Their subsequent designs culminated in the gasoline-fuelled HeS 3 of , which was fitted to Heinkel's simple and compact He 178 airframe and flown by Erich Warsitz in the early morning of August 27, 1939, from Rostock-Marienehe aerodrome, an impressively short time for development. The He 178 was the world's first jet plane. Heinkel applied for a US patent covering the Aircraft Power Plant by Hans Joachim Pabst von Ohain in May 31, 1939; patent number US2256198, with M Hahn referenced as inventor.\nAustrian Anselm Franz of Junkers' engine division (\"Junkers Motoren\" or \"Jumo\") introduced the axial-flow compressor in their jet engine. Jumo was assigned the next engine number in the RLM 109-0xx numbering sequence for gas turbine aircraft powerplants, \"004\", and the result was the Jumo 004 engine. After many lesser technical difficulties were solved, mass production of this engine started in 1944 as a powerplant for the world's first jet-fighter aircraft, the Messerschmitt Me 262 (and later the world's first jet-bomber aircraft, the Arado Ar 234). A variety of reasons conspired to delay the engine's availability, causing the fighter to arrive too late to improve Germany's position in World War II, however this was the first jet engine to be used in service.\nMeanwhile, in Britain the Gloster E28/39 had its maiden flight on 15 May 1941 and the Gloster Meteor finally entered service with the RAF in July 1944. These were powered by turbojet engines from Power Jets Ltd., set up by Frank Whittle. The first two operational turbojet aircraft, the Messerschmitt Me 262 and then the Gloster Meteor entered service within three months of each other in 1944.\n\nFollowing the end of the war the German jet aircraft and jet engines were extensively studied by the victorious allies and contributed to work on early Soviet and US jet fighters. The legacy of the axial-flow engine is seen in the fact that practically all jet engines on fixed-wing aircraft have had some inspiration from this design.\n\nBy the 1950s the jet engine was almost universal in combat aircraft, with the exception of cargo, liaison and other specialty types. By this point some of the British designs were already cleared for civilian use, and had appeared on early models like the de Havilland Comet and Avro Canada Jetliner. By the 1960s all large civilian aircraft were also jet powered, leaving the piston engine in low-cost niche roles such as cargo flights.\n\nThe efficiency of turbojet engines was still rather worse than piston engines, but by the 1970s, with the advent of high-bypass turbofan jet engines (an innovation not foreseen by the early commentators such as Edgar Buckingham, at high speeds and high altitudes that seemed absurd to them), fuel efficiency was about the same as the best piston and propeller engines.\n\nJet engines power jet aircraft, cruise missiles and unmanned aerial vehicles. In the form of rocket engines they power fireworks, model rocketry, spaceflight, and military missiles.\n\nJet engines have propelled high speed cars, particularly drag racers, with the all-time record held by a rocket car. A turbofan powered car, ThrustSSC, currently holds the land speed record.\n\nJet engine designs are frequently modified for non-aircraft applications, as industrial gas turbines or marine powerplants. These are used in electrical power generation, for powering water, natural gas, or oil pumps, and providing propulsion for ships and locomotives. Industrial gas turbines can create up to 50,000 shaft horsepower. Many of these engines are derived from older military turbojets such as the Pratt & Whitney J57 and J75 models. There is also a derivative of the P&W JT8D low-bypass turbofan that creates up to 35,000 HP.\n\nJet engines are also sometimes developed into, or share certain components such as engine cores, with turboshaft and turboprop engines, which are forms of gas turbine engines that are typically used to power helicopters and some propeller-driven aircraft.\n\nThere are a large number of different types of jet engines, all of which achieve forward thrust from the principle of \"jet propulsion\".\n\nCommonly aircraft are propelled by airbreathing jet engines. Most airbreathing jet engines that are in use are turbofan jet engines, which give good efficiency at speeds just below the speed of sound.\n\nGas turbines are rotary engines that extract energy from a flow of combustion gas. They have an upstream compressor coupled to a downstream turbine with a combustion chamber in-between. In aircraft engines, those three core components are often called the \"gas generator.\" There are many different variations of gas turbines, but they all use a gas generator system of some type.\n\nA turbojet engine is a gas turbine engine that works by compressing air with an inlet and a compressor (axial, centrifugal, or both), mixing fuel with the compressed air, burning the mixture in the combustor, and then passing the hot, high pressure air through a turbine and a nozzle. The compressor is powered by the turbine, which extracts energy from the expanding gas passing through it. The engine converts internal energy in the fuel to kinetic energy in the exhaust, producing thrust. All the air ingested by the inlet is passed through the compressor, combustor, and turbine, unlike the turbofan engine described below.\n\nTurbofans differ from turbojets in that they have an additional fan at the front of the engine, which accelerates air in a duct bypassing the core gas turbine engine. Turbofans are the dominant engine type for medium and long-range airliners.\n\nTurbofans are usually more efficient than turbojets at subsonic speeds, but at high speeds their large frontal area generates more drag. Therefore, in supersonic flight, and in military and other aircraft where other considerations have a higher priority than fuel efficiency, fans tend to be smaller or absent.\n\nBecause of these distinctions, turbofan engine designs are often categorized as low-bypass or high-bypass, depending upon the amount of air which bypasses the core of the engine. Low-bypass turbofans have a bypass ratio of around 2:1 or less.\n\nRam compression jet engines are airbreathing engines similar to gas turbine engines and they both follow the Brayton cycle. Gas turbine and ram powered engines differ, however, in how they compress the incoming airflow. Whereas gas turbine engines use axial or centrifugal compressors to compress incoming air, ram engines rely only on air compressed through the inlet or diffuser. A ram engine thus requires a substantial initial forward airspeed before it can function. Ram powered engines are considered the most simple type of air breathing jet engine because they can contain no moving parts.\n\nRamjets are ram powered jet engines. They are mechanically simple, and operate less efficiently than turbojets except at very high speeds.\n\nScramjets differ mainly in the fact that the air does not slow to subsonic speeds. Rather, they use supersonic combustion. They are efficient at even higher speed. Very few have been built or flown. \n\nThe rocket engine uses the same basic physical principles of thrust as a form of reaction engine, but is distinct from the jet engine. in that it does not require atmospheric air to provide oxygen; the rocket carries all components of the reaction mass. However some definitions treat it as a form of jet propulsion.\n\nBecause rockets do not breathe air, this allows them to operate at arbitrary altitudes and in space.\n\nThis type of engine is used for launching satellites, space exploration and manned access, and permitted landing on the moon in 1969.\n\nRocket engines are used for high altitude flights, or anywhere where very high accelerations are needed since rocket engines themselves have a very high thrust-to-weight ratio.\n\nHowever, the high exhaust speed and the heavier, oxidizer-rich propellant results in far more propellant use than turbofans. Even so, at extremely high speeds they become energy-efficient.\n\nAn approximate equation for the net thrust of a rocket engine is:\n\nWhere formula_2 is the net thrust, formula_3 is the specific impulse, formula_4 is a standard gravity, formula_5 is the propellant flow in kg/s, formula_6 is the cross-sectional area at the exit of the exhaust nozzle, and formula_7 is the atmospheric pressure.\n\nCombined-cycle engines simultaneously use two or more different principles of jet propulsion.\n\nA water jet, or pump-jet, is a marine propulsion system that utilizes a jet of water. The mechanical arrangement may be a ducted propeller with nozzle, or a centrifugal compressor and nozzle. The pump-jet must be driven by a separate engine such as a Diesel or gas turbine.\n\nAll jet engines are reaction engines that generate thrust by emitting a jet of fluid rearwards at relatively high speed. The forces on the inside of the engine needed to create this jet give a strong thrust on the engine which pushes the craft forwards.\n\nJet engines make their jet from propellant from tankage that is attached to the engine (as in a 'rocket') as well as in duct engines (those commonly used on aircraft) by ingesting an external fluid (very typically air) and expelling it at higher speed.\n\nThe propelling nozzle is the key component of all jet engines as it creates the exhaust jet. Propelling nozzles turn internal and pressure energy into high velocity kinetic energy. The total pressure and temperature don't change through the nozzle but their static values drop as the gas speeds up.\n\nThe velocity of the air entering the nozzle is low, about Mach 0.4, a prerequisite for minimizing pressure losses in the duct leading to the nozzle. The temperature entering the nozzle may be as low as sea level ambient for a fan nozzle in the cold air at cruise altitudes. It may be as high as the 1000K exhaust gas temperature for a supersonic afterburning engine or 2200K with afterburner lit. The pressure entering the nozzle may vary from 1.5 times the pressure outside the nozzle, for a single stage fan, to 30 times for the fastest manned aircraft at mach 3+.\n\nConvergent nozzles are only able to accelerate the gas up to local sonic (Mach 1) conditions. To reach high flight speeds, even greater exhaust velocities are required, and so a convergent-divergent nozzle is often used on high-speed aircraft.\n\nThe nozzle thrust is highest if the static pressure of the gas reaches the ambient value as it leaves the nozzle. This only happens if the nozzle exit area is the correct value for the nozzle pressure ratio (npr). Since the npr changes with engine thrust setting and flight speed this is seldom the case. Also at supersonic speeds the divergent area is less than required to give complete internal expansion to ambient pressure as a trade-off with external body drag. Whitford gives the F-16 as an example. Other underexpanded examples were the XB-70 and SR-71.\n\nThe nozzle size, together with the area of the turbine nozzles, determines the operating pressure of the compressor.\n\nThis overview highlights where energy losses occur in complete jet aircraft powerplants or engine installations. \n\nA jet engine at rest, as on a test stand, sucks in fuel and generates thrust. How well it does this is judged by how much fuel it uses and what force is required to restrain it. This is a measure of its efficiency. If something deteriorates inside the engine (known as performance deterioration) it will be less efficient and this will show when the fuel produces less thrust. If a change is made to an internal part which allows the air/combustion gases to flow more smoothly the engine will be more efficient and use less fuel. A standard definition is used to assess how different things change engine efficiency and also to allow comparisons to be made between different engines. This definition is called specific fuel consumption, or how much fuel is needed to produce one unit of thrust. For example, it will be known for a particular engine design that if some bumps in a bypass duct are smoothed out the air will flow more smoothly giving a pressure loss reduction of x% and y% less fuel will be needed to get the take-off thrust, for example. This understanding comes under the engineering discipline Jet engine performance. How efficiency is affected by forward speed and by supplying energy to aircraft systems is mentioned later.\n\nThe efficiency of the engine is controlled primarily by the operating conditions inside the engine which are the pressure produced by the compressor and the temperature of the combustion gases at the first set of rotating turbine blades. The pressure is the highest air pressure in the engine. The turbine rotor temperature is not the highest in the engine but is the highest at which energy transfer takes place ( higher temperatures occur in the combustor). The above pressure and temperature are shown on a Thermodynamic cycle diagram.\n\nThe efficiency is further modified by how smoothly the air and the combustion gases flow through the engine, how well the flow is aligned (known as incidence angle) with the moving and stationary passages in the compressors and turbines. Non-optimum angles, as well as non-optimum passage and blade shapes can cause thickening and separation of Boundary layers and formation of Shock waves. It is important to slow the flow (lower speed means less pressure losses or Pressure drop) when it travels through ducts connecting the different parts. How well the individual components contribute to turning fuel into thrust is quantified by measures like efficiencies for the compressors, turbines and combustor and pressure losses for the ducts. These are shown as lines on a Thermodynamic cycle diagram.\n\nThe engine efficiency, or thermal efficiency, known as formula_8. is dependent on the Thermodynamic cycle parameters, maximum pressure and temperature, and on component efficiencies, formula_9, formula_10 and formula_11 and duct pressure losses.\n\nThe engine needs compressed air for itself just to run successfully. This air comes from its own compressor and is called secondary air. It does not contribute to making thrust so makes the engine less efficient. It is used to preserve the mechanical integrity of the engine, to stop parts overheating and to prevent oil escaping from bearings for example. Only some of this air taken from the compressors returns to the turbine flow to contribute to thrust production. Any reduction in the amount needed improves the engine efficiency. Again, it will be known for a particular engine design that a reduced requirement for cooling flow of x% will reduce the specific fuel consumption by y%. In other words, less fuel will be required to give take-off thrust, for example. The engine is more efficient.\n\nAll of the above considerations are basic to the engine running on its own and, at the same time, doing nothing useful, i.e. it is not moving an aircraft or supplying energy for the aircraft's electrical, hydraulic and air systems. In the aircraft the engine gives away some of its thrust-producing potential, or fuel, to power these systems. These requirements, which cause installation losses, reduce its efficiency. It is using some fuel that does not contribute to the engine's thrust.\n\nFinally, when the aircraft is flying the propelling jet itself contains wasted kinetic energy after it has left the engine. This is quantified by the term propulsive, or Froude, efficiency formula_12 and may be reduced by redesigning the engine to give it bypass flow and a lower speed for the propelling jet, for example as a turboprop or turbofan engine. At the same time forward speed increases the formula_8 by increasing the Overall pressure ratio.\n\nThe overall efficiency of the engine at flight speed is defined as formula_14.\n\nThe formula_15 at flight speed depends on how well the intake compresses the air before it is handed over to the engine compressors. The intake compression ratio, which can be as high as 32:1 at Mach 3, adds to that of the engine compressor to give the Overall pressure ratio and formula_8 for the Thermodynamic cycle. How well it does this is defined by its pressure recovery or measure of the losses in the intake. Mach 3 manned flight has provided an interesting illustration of how these losses can increase dramatically in an instant. The North American XB-70 Valkyrie and Lockheed SR-71 Blackbird at Mach 3 each had pressure recoveries of about 0.8, due to relatively low losses during the compression process, i.e. through systems of multiple shocks. During an 'unstart' the efficient shock system would be replaced by a very inefficient single shock beyond the inlet and an intake pressure recovery of about 0.3 and a correspondingly low pressure ratio.\n\nThe propelling nozzle at speeds above about Mach 2 usually has extra internal thrust losses because the exit area is not big enough as a trade-off with external afterbody drag.\n\nAlthough a bypass engine improves propulsive efficiency it incurs losses of its own inside the engine itself. Machinery has to be added to transfer energy from the gas generator to a bypass airflow. The low loss from the propelling nozzle of a turbojet is added to with extra losses due to inefficiencies in the added turbine and fan. These may be included in a transmission, or transfer, efficiency formula_17. However, these losses are more than made up by the improvement in propulsive efficiency. There are also extra pressure losses in the bypass duct and an extra propelling nozzle.\n\nWith the advent of turbofans with their loss-making machinery what goes on inside the engine has been separated by Bennett, for example, between gas generator and transfer machinery giving formula_18.\nThe energy efficiency (formula_15) of jet engines installed in vehicles has two main components:\n\nEven though overall energy efficiency formula_15 is:\n\nfor all jet engines the \"propulsive efficiency\" is highest as the exhaust jet velocity gets closer to the vehicle speed as this gives the smallest residual kinetic energy. For an airbreathing engine an exhaust velocity equal to the vehicle velocity, or a formula_12 equal to one, gives zero thrust with no net momentum change. The formula for air-breathing engines moving at speed formula_25 with an exhaust velocity formula_26, and neglecting fuel flow, is:\n\nAnd for a rocket:\n\nIn addition to propulsive efficiency, another factor is \"cycle efficiency\"; a jet engine is a form of heat engine. Heat engine efficiency is determined by the ratio of temperatures reached in the engine to that exhausted at the nozzle. This has improved constantly over time as new materials have been introduced to allow higher maximum cycle temperatures. For example, composite materials, combining metals with ceramics, have been developed for HP turbine blades, which run at the maximum cycle temperature. The efficiency is also limited by the overall pressure ratio that can be achieved. Cycle efficiency is highest in rocket engines (~60+%), as they can achieve extremely high combustion temperatures. Cycle efficiency in turbojet and similar is nearer to 30%, due to much lower peak cycle temperatures.\n\nThe combustion efficiency of most aircraft gas turbine engines at sea level takeoff conditions\nis almost 100%. It decreases nonlinearly to 98% at altitude cruise conditions. Air-fuel ratio ranges from 50:1 to 130:1. For any type of combustion chamber there is a \"rich\" and \"weak limit\" to the air-fuel ratio, beyond which the flame is extinguished. The range of air-fuel ratio between the rich and weak limits is reduced with an increase of air velocity. If the\nincreasing air mass flow reduces the fuel ratio below certain value, flame extinction occurs.\n\nA closely related (but different) concept to energy efficiency is the rate of consumption of propellant mass. Propellant consumption in jet engines is measured by Specific Fuel Consumption, Specific impulse or Effective exhaust velocity. They all measure the same thing. Specific impulse and effective exhaust velocity are strictly proportional, whereas specific fuel consumption is inversely proportional to the others.\n\nFor airbreathing engines such as turbojets, energy efficiency and propellant (fuel) efficiency are much the same thing, since the propellant is a fuel and the source of energy. In rocketry, the propellant is also the exhaust, and this means that a high energy propellant gives better propellant efficiency but can in some cases actually give \"lower\" energy efficiency.\n\nIt can be seen in the table (just below) that the subsonic turbofans such as General Electric's CF6 turbofan use a lot less fuel to generate thrust for a second than did the Concorde's Rolls-Royce/Snecma Olympus 593 turbojet. However, since energy is force times distance and the distance per second was greater for Concorde, the actual power generated by the engine for the same amount of fuel was higher for Concorde at Mach 2 than the CF6. Thus, the Concorde's engines were more efficient in terms of energy per mile.\n\nThe thrust-to-weight ratio of jet engines with similar configurations varies with scale, but is mostly a function of engine construction technology. For a given engine, the lighter the engine, the better the thrust-to-weight is, the less fuel is used to compensate for drag due to the lift needed to carry the engine weight, or to accelerate the mass of the engine.\n\nAs can be seen in the following table, rocket engines generally achieve much higher thrust-to-weight ratios than duct engines such as turbojet and turbofan engines. This is primarily because rockets almost universally use dense liquid or solid reaction mass which gives a much smaller volume and hence the pressurization system that supplies the nozzle is much smaller and lighter for the same performance. Duct engines have to deal with air which is two to three orders of magnitude less dense and this gives pressures over much larger areas, which in turn results in more engineering materials being needed to hold the engine together and for the air compressor.\n\nPropeller engines handle larger air mass flows, and give them smaller acceleration, than jet engines. Since the increase in air speed is small, at high flight speeds the thrust available to propeller-driven aeroplanes is small. However, at low speeds, these engines benefit from relatively high propulsive efficiency.\n\nOn the other hand, turbojets accelerate a much smaller mass flow of intake air and burned fuel, but they then reject it at very high speed. When a de Laval nozzle is used to accelerate a hot engine exhaust, the outlet velocity may be locally supersonic. Turbojets are particularly suitable for aircraft travelling at very high speeds.\n\nTurbofans have a mixed exhaust consisting of the bypass air and the hot combustion product gas from the core engine. The amount of air that bypasses the core engine compared to the amount flowing into the engine determines what is called a turbofan's bypass ratio (BPR).\n\nWhile a turbojet engine uses all of the engine's output to produce thrust in the form of a hot high-velocity exhaust gas jet, a turbofan's cool low-velocity bypass air yields between 30% and 70% of the total thrust produced by a turbofan system.\n\nThe net thrust (F) generated by a turbofan can also be expanded as:\n\nwhere:\n\nRocket engines have extremely high exhaust velocity and thus are best suited for high speeds (hypersonic) and great altitudes. At any given throttle, the thrust and efficiency of a rocket motor improves slightly with increasing altitude (because the back-pressure falls thus increasing net thrust at the nozzle exit plane), whereas with a turbojet (or turbofan) the falling density of the air entering the intake (and the hot gases leaving the nozzle) causes the net thrust to decrease with increasing altitude. Rocket engines are more efficient than even scramjets above roughly Mach 15.\n\nWith the exception of scramjets, jet engines, deprived of their inlet systems can only accept air at around half the speed of sound. The inlet system's job for transonic and supersonic aircraft is to slow the air and perform some of the compression.\n\nThe limit on maximum altitude for engines is set by flammability- at very high altitudes the air becomes too thin to burn, or after compression, too hot. For turbojet engines altitudes of about 40 km appear to be possible, whereas for ramjet engines 55 km may be achievable. Scramjets may theoretically manage 75 km. Rocket engines of course have no upper limit.\n\nAt more modest altitudes, flying faster compresses the air at the front of the engine, and this greatly heats the air. The upper limit is usually thought to be about Mach 5-8, as above about Mach 5.5, the atmospheric nitrogen tends to react due to the high temperatures at the inlet and this consumes significant energy. The exception to this is scramjets which may be able to achieve about Mach 15 or more, as they avoid slowing the air, and rockets again have no particular speed limit.\n\nThe noise emitted by a jet engine has many sources. These include, in the case of gas turbine engines, the fan, compressor, combustor, turbine and propelling jet/s.\n\nThe propelling jet produces jet noise which is caused by the violent mixing action of the high speed jet with the surrounding air. In the subsonic case the noise is produced by eddies and in the supersonic case by Mach waves. The sound power radiated from a jet varies with the jet velocity raised to the eighth power for velocities up to 2,000 ft/sec and varies with the velocity cubed above 2,000 ft/sec. Thus, the lower speed exhaust jets emitted from engines such as high bypass turbofans are the quietest, whereas the fastest jets, such as rockets, turbojets, and ramjets, are the loudest. For commercial jet aircraft the jet noise has reduced from the turbojet through bypass engines to turbofans as a result of a progressive reduction in propelling jet velocities. For example, the JT8D, a bypass engine, has a jet velocity of 1450 ft/sec whereas the JT9D, a turbofan, has jet velocities of 885 ft/sec (cold) and 1190 ft/sec (hot).\n\nThe advent of the turbofan replaced the very distinctive jet noise with another sound known as \"buzz saw\" noise. The origin is the shockwaves originating at the supersonic fan blades at takeoff thrust.\n\n\n\n"}
{"id": "41341563", "url": "https://en.wikipedia.org/wiki?curid=41341563", "title": "List of Astragalus species", "text": "List of Astragalus species\n\nThis is a list of \"Astragalus\" species, including infraspecific taxa. They are listed according to subgenera (for Old World species) or informal groups called phalanxes (for North American species). Subgenera and phalanxes are further subdivided into sections. Phylogenetic analyses have determined that many of these subgenera, phalanxes, and sections are not monophyletic.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAstragalus armatus ssp armatus\nAstragalus armatus ssp numidicus\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "4464424", "url": "https://en.wikipedia.org/wiki?curid=4464424", "title": "List of Pacific hurricanes", "text": "List of Pacific hurricanes\n\nThis is a list of notable Pacific hurricanes, subdivided by reason for notability. Notability means that it has met some criterion or achieved some statistic, or is part of a top ten for some superlative. It includes lists and rankings of Pacific hurricanes by different characteristics and impacts.\n\nCharacteristics include extremes of location, such as the northernmost or most equator-ward formation or position of a tropical cyclone. Other characteristics include its central pressure, windspeed, category on the Saffir–Simpson scale, cyclogenesis outside of a normal hurricane season's timeframe, or storms that remain unnamed despite forming after tropical cyclone naming began in 1960. Another characteristic is how long a system lasted from formation to dissipation. These include the cost of damage, the number of casualties, as well as meteorological statistics such as rainfall point maximum, wind speed, and minimum pressure.\n\nAdolph and Israel were removed from the list of names during and after the 2001 season due to political sensitivities. Knut was removed from the list in 1988 for unknown reasons. Adele, Iva, and Fefa were also removed in 1970, 1988, and 1991 respectively for unknown reasons. Hazel was replaced in 1965. The name Isis was also pre-emptively removed from the lists of names for 2016 after being deemed inappropriate because of the eponymous militant group in 2015.\n\nThe following tropical cyclones have caused at least 100 deaths.\n\nThe following tropical cyclones have caused at least $1 billion in damage.\nIn the Central Pacific Hurricane Center's (CPHC) area of responsibility (AOR), the season with the most tropical cyclones is the 2015 season with 16 cyclones forming in or entering the region. A season without cyclones has happened a few times since 1966, most recently in 1979.\n\nBefore 1971 and especially 1966, data in this basin is extremely unreliable. The geostationary satellite era began in 1966, and that year is often considered the first year of reliable tropical records. Intensity estimates are most reliable starting in the 1971 season. A few years later, the Dvorak technique came into use. Those two factors make intensity estimates more reliable starting in that year. For these reasons, seasons prior to 1971 are not included.\n\nNaming of tropical cyclones in the eastern north Pacific began in the 1960 season. That year, four lists of names were created. The plan was to proceed in a manner similar to that of the western Pacific; that is, the name of the first storm in one season would be the next unused one from the same list, and when the bottom of one list was reached the next list was started. This scheme was abandoned in 1965 and next year, the lists started being recycled on a four-year rotation, starting with the A name each year. That same general scheme remains in use today, although the names and lists are different. On average, the eastern north Pacific sees about sixteen named storms per year.\n\nBefore 1971 and especially 1966, data in this basin is extremely unreliable. The geostationary satellite era began in 1966, and that year is often considered the first year of reliable tropical records. Intensity estimates are more reliable starting in the 1971 season. A few years later, the Dvorak technique came into use. Those two make intensity estimates more reliable starting in that year. For these reasons, seasons before 1966 are not included in the lowest column.\n\n† Shared by more than five seasons.\nSource:\n\nThe Pacific hurricane season runs from May 15 to November 30. Only systems that develop or enter during the off-season are included. The earliest off-season storm is Pali in 2016 whilst the latest off-season storm was Nine-C during 2015.\n\n†Entered the basin on this date\n\nTropical cyclones have received official names in the east-central Pacific region since 1960. Since this time, 5 systems that have formed in this area have not received a name, plus another possible unnamed subtropical or tropical system in 2006.\n\nSince 1959, only 18 Pacific hurricanes are known to have reached Category 5 and none made landfall while at this intensity.\n\nSince 1900, 126 Pacific hurricanes have attained Category 4 intensity, of which four made landfall at that strength.\n\nSince 1970, 81 Pacific hurricanes have attained Category 3 intensity, of which three made landfall at that strength.\n\nThis lists all Pacific hurricanes that existed as tropical cyclones while in the Pacific Ocean east of the dateline for more than two weeks continuously. Hurricanes John and Dora spent some time in the west Pacific before dissipating. John spent eleven days west of the dateline; if that time was included John would have existed for a total of 30 days and 18 hours, a world record, while including Dora's time in the west Pacific would mean that it existed for 18 days. One Atlantic hurricane, Hurricane Joan, crossed into this basin and was renamed Miriam, giving it a total lifespan of 22 days, but not all of that was in the Pacific. 1993's Greg formed from the remnants of 1993's Tropical Storm Bret. Its time as an Atlantic system is excluded.\n\nAll of these systems except Trudy, Olaf, and Connie existed in both the east and central Pacific, and all except Olaf were hurricanes. Hurricane Trudy of 1990 is thus the longest lived eastern Pacific hurricane to stay in the eastern Pacific. Tropical Storm Olaf of 1997 is hence the longest-lived eastern Pacific tropical cyclone \"not\" to reach hurricane intensity.\n\nNo known tropical cyclone forming in the central north Pacific lasted for longer than 14 days without crossing into another basin. The tropical cyclone forming in the central Pacific that spent the most time there was 2014's Hurricane Ana at 12.75 days from formation to extratropical transition.\n\nBefore the weather satellite era began, the lifespans of many Pacific hurricanes may be underestimated.\n\nThis includes only systems which stayed a tropical cyclone during the passage or that maintained a circulation during the crossover.\n\nIt used to be that when a Pacific named storm crossed North America and made it to the Atlantic (or vice versa), it would receive the next name on the respective basin's list. However, in 2000 this policy was changed so that a tropical cyclone will keep its name if it remains a tropical cyclone during the entire passage. Only if it dissipates and then re-forms does it get renamed.\n\nThis includes only systems which stayed a tropical cyclone during the passage or that maintained a circulation during the crossover.\n\nIn addition to those, there are apparently two additional ones. One existed before 1856 and made it to the Gulf of Mexico. Another Pacific tropical cyclone crossed over central Mexico and also made it to the Gulf sometime after September 9, 1924.\n\nNeither eastern Pacific tropical cyclones passing 140°W, nor central Pacific tropical cyclones crossing the dateline, are notable events. However, very few eastern Pacific proper cyclones that enter the central Pacific make it to the dateline.\n\n† System ceased to be a tropical cyclone and regenerated at least once during its life span.\n\n‡ Hurricane Li formed in the eastern Pacific, but was not named until it crossed into the central Pacific.\n\nIn addition, Hurricane Jimena of 2003 is recognized per NHC, CPHC and JTWC as a storm that existed in all three areas of responsibility, but isn't recognized by the JMA as an official western Pacific tropical cyclone.\n\nTropical cyclones crossing from the western Pacific to the central Pacific are fairly rare, and this has happened only seven times before. Of those seven times, five of them were storms which crossed the dateline twice; from the western to the central pacific and back (or vice versa). No tropical cyclone from the western Pacific has ever traveled east of 140°W.\n\n† System crossed the dateline twice.\n\n<nowiki>*</nowiki> Hurricane/Typhoon John formed in the eastern Pacific.\n\nIn addition, Typhoon June of 1958 and Tropical Storm Wene of 2000 are recognized per CPHC as basin-crossing storms, but aren't recognized as such by the JMA. Also, Tropical Storm Moke of 1984 may have achieved this feat as well, however it isn't recognized as an official western Pacific tropical cyclone by either the JMA or the JTWC.\n\nTropical cyclones crossing from the eastern Pacific to the central Pacific are routine; ones going the other way are not. That event has happened thrice.\n\nIt was previously believed that an Unnamed Hurricane of 1975 crossed 140°W and is still included in the NHC as such, but according to a reanalysis made by the CPHC the storm became extratropical before doing so. In addition, an unofficial cyclone formed on October 30, 2006 in the central Pacific subtropics. It eventually developed an eye-like structure. Its track data indicates that it crossed from the central to the east Pacific because it formed at longitude 149°W and dissipated at 135°W. NASA, which is not a meteorological organization, called this system a subtropical cyclone, and the Naval Research Laboratory Monterey had enough interest in it to call it 91C. The system has also been called extratropical. This cyclone is unofficial because it is not included in the seasonal reports of either Regional Specialized Meteorological Center.\n\nThe apparent increase in recent seasons is spurious; it is due to better estimation and measurement, not an increase in intense storms. That is, until 1988, Pacific hurricanes generally did not have their central pressures measured or estimated from satellite imagery.\n<nowiki>*</nowiki> Estimated from satellite imagery\n\n~ Pressure while East of the International Dateline\n\nIntensity is measured solely by central pressure unless the pressure is not known, in which case intensity is measured by maximum sustained winds.\n\n\n\nAll of these values are point maxima.\n\n\n"}
{"id": "10834877", "url": "https://en.wikipedia.org/wiki?curid=10834877", "title": "List of Superfund sites in Colorado", "text": "List of Superfund sites in Colorado\n\nThis is a list of Superfund sites in Colorado designated under the Comprehensive Environmental Response, Compensation, and Liability Act (CERCLA) environmental law. The CERCLA federal law of 1980 authorized the United States Environmental Protection Agency (EPA) to create a list of polluted locations requiring a long-term response to clean up hazardous material contaminations. These locations are known as Superfund sites, and are placed on the National Priorities List (NPL). \n\nThe NPL guides the EPA in \"determining which sites warrant further investigation\" for environmental remediation. As of May 1, 2010, there were eighteen Superfund sites on the National Priorities List in Colorado. Two more sites have been proposed for entry on the list and three others have been cleaned up and removed from it.\n\n\n"}
{"id": "15565608", "url": "https://en.wikipedia.org/wiki?curid=15565608", "title": "List of ecoregions in Yemen", "text": "List of ecoregions in Yemen\n\nThe following is a list of ecoregions in Yemen, as identified by the Worldwide Fund for Nature (WWF).\n\nYemen lies on the boundary between two of the world's terrestrial ecozones. The Afrotropic ecozone covers the mountainous southern and eastern fringe of the Arabian Peninsula as well as Sub-Saharan Africa and Madagascar. The Palearctic ecozone covers the rest of the Arabian Peninsula as well as temperate Eurasia and Northern Africa.\n\n\n\n"}
{"id": "51208786", "url": "https://en.wikipedia.org/wiki?curid=51208786", "title": "List of flora and fauna named after the Muisca", "text": "List of flora and fauna named after the Muisca\n\nThe Muisca were a people living in the central highlands of Colombia; the Altiplano Cundiboyacense and neighbouring valleys. The variation of climates and ecozones within their territories made the Muisca excellent farmers. Over time, various species of flora and fauna have been discovered in Colombia. This list contains the living genera and species and fossils named after the Muisca, their religion or their settlements.\n\nThree other Muisca etymologies are recognised; Thomagata Patera, named after mythological \"cacique\" Thomagata, and Bochica Patera are volcanoes on Io and BD Bacatá is the highest skyscraper of Colombia.\n\n"}
{"id": "21244432", "url": "https://en.wikipedia.org/wiki?curid=21244432", "title": "List of largest stars", "text": "List of largest stars\n\nBelow is an ordered list of the largest stars currently known by radius. The unit of measurement used is the radius of the Sun (approximately ).\n\nThe exact order of this list is very incomplete, as great uncertainties currently remain, especially when deriving various important parameters used in calculations, such as stellar luminosity and effective temperature. Often stellar radii can only be expressed as an average or within a large range of values. Values for stellar radii vary significantly in sources and throughout the literature, mostly as the boundary of the very tenuous atmosphere (opacity) greatly differs depending on the wavelength of light in which the star is observed.\n\nRadii of several stars can be directly obtained by stellar interferometry. Other methods can use lunar occultations or from eclipsing binaries, which can be used to test other indirect methods of finding true stellar size. Only a few useful supergiant stars can be occulted by the Moon, including Antares and Aldebaran. Examples of eclipsing binaries include Epsilon Aurigae, VV Cephei, and HR 5171.\n\nComplex issues exist in determining the true radii of the largest stars, which in many cases do display significant errors. The following lists are generally based on various considerations or assumptions that include:\n\nIn this list are some examples of more distant extragalactic stars, which may have slightly different properties and natures than the currently largest known stars in the Milky Way:\n\n\n! style=\"background:#efefef; white-space:nobr;\" | Star name\n! style=\"background:#ffdead;\" | Solar radii(Sun = 1)\n! Method\n! Notes\n"}
{"id": "18646359", "url": "https://en.wikipedia.org/wiki?curid=18646359", "title": "List of microbursts", "text": "List of microbursts\n\nThis is a list of notable microbursts.\n\n\n"}
{"id": "48389560", "url": "https://en.wikipedia.org/wiki?curid=48389560", "title": "List of mountain peaks of Washington", "text": "List of mountain peaks of Washington\n\nThis article comprises three sortable tables of major mountain peaks of the U.S. State of Washington.\n\nThe summit of a mountain or hill may be measured in three principal ways:\n\nOf the highest major summits of Washington, Mount Rainier exceeds elevation, Mount Adams exceeds , four peaks exceed elevation, and 25 peaks exceed elevation.\n\nOf the most prominent summits of Washington, Mount Rainier exceeds of topographic prominence, five peaks exceed , seven peaks are ultra-prominent summits with more than of topographic prominence, and 40 peaks exceed of topographic prominence.\n\nOf the most isolated major summits of Washington, Mount Rainier exceeds of topographic isolation, Mount Baker exceeds , and Mount Olympus exceeds of topographic isolation.\n\n\n\n"}
{"id": "4014808", "url": "https://en.wikipedia.org/wiki?curid=4014808", "title": "List of national parks of Denmark", "text": "List of national parks of Denmark\n\nThere are six national parks in the Kingdom of Denmark; five in Denmark proper and one in Greenland.\n\nThe first national park in Denmark proper was Thy National Park (), created in 2008. It is located in Thisted Municipality, Nordjylland. The park is a narrow stretch of land along the North Jutlandic Island's west coast from Hanstholm southward to Agger Tange, excluding Hanstholm, Klitmøller, Nørre Vorupør, Stenbjerg and Agger. Thy National Park is named after Thy, an area that includes not only today's national park but also adjacent land further east. The park has dunes, heath, forests and grassland and also covers several small lakes and a small part of the Limfjord, which is the fjord that separates the North Jutlandic Island from the Cimbrian Peninsula.\n\nIn August 2009, the second national park Mols Bjerge National Park was inaugurated, followed by Wadden Sea National Park in October 2010, and Land of the Scylding National Park in 2015. Two additional areas in mainland Denmark were proposed in 2008 and 2009 to become national parks, but one of these plans was abandoned in 2012. The other, Kongernes Nordsjælland National Park, was inaugurated in 2018.\n\nGreenland has had its own national park since 1974. The Northeast Greenland National Park stretches across three fifths of the northern parts of Greenland's east coast and, since its expansion in 1988, two thirds of the eastern parts of Greenland's north coast. Bounded by the coasts, the park covers the island's entire north-eastern quarter, all of which is almost uninhabited. \n\nLike Greenland, the Faroe Islands forms another autonomous region of the Kingdom of Denmark, but contains no national parks.\n\nNotes\nReferences\n"}
{"id": "34009203", "url": "https://en.wikipedia.org/wiki?curid=34009203", "title": "List of rivers of Sierra Leone", "text": "List of rivers of Sierra Leone\n\nThis is a list of rivers in Sierra Leone. This list is arranged by drainage basin, with respective tributaries indented under each larger stream's name.\n\n\n"}
{"id": "20018018", "url": "https://en.wikipedia.org/wiki?curid=20018018", "title": "List of rivers of Zambia", "text": "List of rivers of Zambia\n\n\n\n\n\n"}
{"id": "8591867", "url": "https://en.wikipedia.org/wiki?curid=8591867", "title": "List of stars in Taurus", "text": "List of stars in Taurus\n\nThis is the list of notable stars in the constellation Taurus, sorted by decreasing brightness.\n\n\n"}
{"id": "767524", "url": "https://en.wikipedia.org/wiki?curid=767524", "title": "List of variable stars", "text": "List of variable stars\n\nThere are over 41,638 known variable stars (2008), with more being discovered regularly, so a complete list of every single variable is impossible at this place (cf. GCVS). The following is a list of variable stars that are well-known, bright, significant, or otherwise interesting.\n\n\n"}
{"id": "26395924", "url": "https://en.wikipedia.org/wiki?curid=26395924", "title": "Lists of forests of the Western Cape", "text": "Lists of forests of the Western Cape\n\nThis is a list of forests of the Western Cape province of South Africa.\n\nThese are generally examples of Southern Afrotemperate Forest, the predominant indigenous forest-type in the Western Cape.\n\n\n"}
{"id": "26908142", "url": "https://en.wikipedia.org/wiki?curid=26908142", "title": "Looming and similar refraction phenomena", "text": "Looming and similar refraction phenomena\n\nWhile mirages are the best known atmospheric refraction phenomena, looming and similar refraction phenomena do not produce mirages. Mirages show an extra image or images of the miraged object, while looming, towering, stooping, and sinking do not. No inverted image is present in those phenomena either. Depending on atmospheric conditions, the objects can appear to be elevated or lowered, stretched or stooped. These phenomena can occur together, changing the appearance of different parts of the objects in different ways. Sometimes these phenomena can occur together with a true mirage.\n\nLooming is the most noticeable and most often observed of these refraction phenomena. It is an abnormally large refraction of the object that increases the apparent elevation of the distant objects and sometimes allows an observer to see objects that are located below the horizon under normal conditions. One of the most famous looming observations was made by William Latham in 1798, who wrote:\nThomas Jefferson noted the phenomenon of looming in his book Notes on the State of Virginia:\n\nHe was unable to explain this phenomenon and did not think refraction could account from the perceived changes of shape of the object in question.\nOther famous observations that were called \"mirages\" may actually be referring to looming. One of those was described in \"Scientific American\" on August 25, 1894 as \"a remarkable mirage seen by the citizens of Buffalo, New York\". Such looming—sometimes with apparent magnification of opposite shores—have been reported over the Great Lakes. Canadian shorelines have been observed from Rochester, New York across Lake Ontario, and from Cleveland across Lake Erie. The land forms over 50 miles distant, normally beyond the horizon, were sometimes perceived as 6 miles away.\n\nLooming is most commonly seen in the polar regions. Looming was sometimes responsible for the errors made by polar explorers; for example, Charles Wilkes charted the coast of Antarctica, where later only water was found. \n\nThe larger the size of the sphere (the planet where an observer is located) the less curved the horizon is. William Jackson Humphreys' calculations showed that an observer could see all the way around a planet nearly six times larger in radius than the Earth, with the same atmosphere as the Earth, because of looming.\n\nSinking is the opposite of looming. In sinking, stationary objects that are normally seen above the horizon appear to be lowered, or may even disappear below the horizon. In looming, the curvature of the rays is increasing, while sinking produces the opposite effect. In general, looming is more noticeable than sinking because objects that appear to grow stand out more than those that appear to shrink.\n\nTowering and stooping are more complex forms of atmospheric refraction than looming and sinking. While looming and sinking change the apparent elevation of an object, towering and stooping change the apparent shape of the object itself. With towering, objects appear stretched; with stooping, objects seem to be shortened. The apparent stretching and shortening of the objects are not symmetrical and depends on the thermal profile of the atmosphere. The curvature of the rays changes more rapidly in some places because the thermal profile is curved.\n\nThese three images were taken from the same place on different days under different atmospheric conditions. The top frame shows looming. The island shape is not distorted, but is elevated. The middle frame shows looming with towering. The lowest frame is a 5-image superior mirage of the islands. The uppermost image is stooped severely. As the image shows, the different refraction phenomena are not independent from each other and may occur together as a combination, depending on atmospheric conditions.\n\n\n"}
{"id": "35543722", "url": "https://en.wikipedia.org/wiki?curid=35543722", "title": "Macroscopic quantum phenomena", "text": "Macroscopic quantum phenomena\n\nMacroscopic quantum phenomena refer to processes showing quantum behavior at the macroscopic scale, rather than at the atomic scale where quantum effects are prevalent. The best-known examples of macroscopic quantum phenomena are superfluidity and superconductivity; other examples include the quantum Hall effect and concerted proton tunneling in ice. Since 2000 there has been extensive experimental work on quantum gases, particularly Bose–Einstein Condensates.\n\nBetween 1996 and 2003 four Nobel Prizes were given for work related to macroscopic quantum phenomena. Macroscopic quantum phenomena can be observed in atomic systems such as superfluid helium and in superconductors, but also in dilute quantum gases, dressed photons such as polaritons and in laser light. Although these media are very different, their behavior is very similar as they all show macroscopic quantum behavior and to such extent they all can be referred to as quantum fluids.\n\nQuantum phenomena are generally classified as macroscopic when the quantum states are occupied by a large number of particles (typically Avogadro's number) or the quantum states involved are macroscopic in size (up to km size in superconducting wires).\n\nThe concept of macroscopically-occupied quantum states is introduced by Fritz London. In this section it will be explained what it means if the ground state is occupied by a very large number of particles. We start with the wave function of the ground state written as\nwith \"Ψ\"₀ the amplitude and formula_1 the phase. The wave function is normalized so that\nThe physical interpretation of the quantity\ndepends on the number of particles. Fig.1 represents a container with a certain number of particles with a small control volume Δ\"V\" inside. We check from time to time how many particles are in the control box. We distinguish three cases:\n\n1. There is only one particle. In this case the control volume is empty most of the time. However, there is a certain chance to find the particle in it given by Eq.(3). The chance is proportional to Δ\"V\". The factor \"ΨΨ\" is called the chance density.\n\n2. If the number of particles is a bit larger there are usually some particles inside the box. We can define an average, but the actual number of particles in the box has relatively large fluctuations around this average.\n\n3. In the case of a very large number of particles there will always be a lot of particles in the small box. The number will fluctuate but the fluctuations around the average are relatively small. The average number is proportional to Δ\"V\" and \"ΨΨ\" is now interpreted as the particle density.\n\nIn quantum mechanics the particle probability flow density \"J\" (unit: particles per second per m²) can be derived from the Schrödinger equation to be\nwith \"q\" the charge of the particle and formula_2 the vector potential. With Eq.(1)\nIf the wave function is macroscopically occupied the particle probability flow density becomes a particle flow density. We introduce the fluid velocity \"v\" via the mass flow density\nThe density (mass per m³) is\nso Eq.(5) results in\nThis important relation connects the velocity, a classical concept, of the condensate with the phase of the wave function, a quantum-mechanical concept.\n\nAt temperatures below the lambda point, helium shows the unique property of superfluidity. The fraction of the liquid that forms the superfluid component is a macroscopic quantum fluid. The helium atom is a neutral particle, so \"q\"=0. Furthermore, when considering helium-4, the relevant particle mass is \"m=m₄\", so Eq.(8) reduces to\nFor an arbitrary loop in the liquid, this gives\nDue to the single-valued nature of the wave function\nwith \"n\" integer, we have\nThe quantity\n\nis the quantum of circulation. For a circular motion with radius \"r\"\n\nIn case of a single quantum (\"n\"=1)\n\nWhen superfluid helium is put in rotation, Eq. (13) will not be satisfied for all loops inside the liquid unless the rotation is organized around vortex lines (as depicted in Fig.2). These lines have a vacuum core with a diameter of about 1 Å (which is smaller than the average particle distance). The superfluid helium rotates around the core with very high speeds. Just outside the core (\"r\" = 1 Å), the velocity is as large as 160 m/s. The cores of the vortex lines and the container rotate as a solid body around the rotation axes with the same angular velocity. The number of vortex lines increases with the angular velocity (as shown in the upper half of the figure). Note that the two right figures both contain six vortex lines, but the lines are organized in different stable patterns.\n\nIn the original paper Ginzburg and Landau observed the existence of two types of superconductors depending \non the energy of the interface between the normal and superconducting states.The Meissner state breaks down when the applied magnetic field is too large. Superconductors can be divided into two classes according to how this breakdown occurs. In Type I superconductors, superconductivity is abruptly destroyed when the strength of the applied field rises above a critical value \"H\". Depending on the geometry of the sample, one may obtain an intermediate state consisting of a baroque pattern of regions of normal material carrying a magnetic field mixed with regions of superconducting material containing no field. In Type II superconductors, raising the applied field past a critical value \"H\" leads to a mixed state (also known as the vortex state) in which an increasing amount of magnetic flux penetrates the material, but there remains no resistance to the flow of electric current as long as the current is not too large. At a second critical field strength \"H\", superconductivity is destroyed. The mixed state is actually caused by vortices in the electronic superfluid, sometimes called fluxons because the flux carried by these vortices is quantized. Most pure elemental superconductors, except niobium and carbon nanotubes, are Type I, while almost all impure and compound superconductors are Type II.\n\nThe most important finding from Ginzburg–Landau theory was made by Alexei Abrikosov in 1957. \nHe used Ginzburg–Landau theory to explain experiments on superconducting alloys and thin films. He found that in a type-II superconductor in a high magnetic field, the field penetrates in a triangular lattice of quantized tubes of flux vortices.\n\nFor superconductors the bosons involved are the so-called Cooper pairs which are quasiparticles formed by two electrons. Hence \"m\" = 2\"m\" and \"q\" = -2\"e\" where \"m\" and \"e\" are the mass of an electron and the elementary charge. It follows from Eq.(8) that\nIntegrating Eq.(15) over a closed loop gives\nAs in the case of helium we define the vortex strength\nand use the general relation\nwhere \"Φ\" is the magnetic flux enclosed by the loop. The so-called fluxoid is defined by\nIn general the values of \"κ\" and \"Φ\" depend on the choice of the loop. Due to the single-valued nature of the wave function and Eq.(16) the fluxoid is quantized\nThe unit of quantization is called the flux quantum\nThe flux quantum plays a very important role in superconductivity. The earth magnetic field is very small (about 50 μT), but it generates one flux quantum in an area of 6 by 6 μm. So, the flux quantum is very small. Yet it was measured to an accuracy of 9 digits as shown in Eq.(21). Nowadays the value given by Eq.(21) is exact by definition.\nIn Fig. 3 two situations are depicted of superconducting rings in an external magnetic field. One case is a thick-walled ring and in the other case the ring is also thick-walled, but is interrupted by a weak link. In the latter case we will meet the famous Josephson relations. In both cases we consider a loop inside the material. In general a superconducting circulation current will flow in the material. The total magnetic flux in the loop is the sum of the applied flux \"Φ\" and the self-induced flux \"Φ\" induced by the circulation current\n\nThe first case is a thick ring in an external magnetic field (Fig. 3a). The currents in a superconductor only flow in a thin layer at the surface. The thickness of this layer is determined by the so-called London penetration depth. It is of μm size or less. We consider a loop far away from the surface so that \"v\"=0 everywhere so \"κ\"=0. In that case the fluxoid is equal to the magnetic flux (\"Φ\"=\"Φ\"). If \"v\"=0 Eq.(15) reduces to\nTaking the rotation gives\nUsing the well-known relations formula_3 and formula_4 shows that the magnetic field in the bulk of the superconductor is zero as well. So, for thick rings, the total magnetic flux in the loop is quantized according to\n\nWeak links play a very important role in modern superconductivity. In most cases weak links are oxide barriers between two superconducting thin films, but it can also be a crystal boundary (in the case of high-Tc superconductors). A schematic representation is given in Fig. 4. Now consider the ring which is thick everywhere except for a small section where the ring is closed via a weak link (Fig. 3b). The velocity is zero except near the weak link. In these regions the velocity contribution to the total phase change in the loop is given by (with Eq.(15))\nThe line integral is over the contact from one side to the other in such a way that the end points of the line are well inside the bulk of the superconductor where \"v\"=0. So the value of the line integral is well-defined (e.g. independent of the choice of the end points). With Eqs.(19), (22), and (26)\nWithout proof we state that the supercurrent through the weak link is given by the so-called DC Josephson relation\nThe voltage over the contact is given by the AC Josephson relation\nThe names of these relations (DC and AC relations) are misleading since they both hold in DC and AC situations. In the steady state (constant formula_5) Eq.(29) shows that \"V\"=0 while a nonzero current flows through the junction. In the case of a constant applied voltage (voltage bias) Eq.(29) can be integrated easily and gives\nSubstitution in Eq.(28) gives\nThis is an AC current. The frequency\nis called the Josephson frequency. One μV gives a frequency of about 500 MHz. By using Eq.(32) the flux quantum is determined with the high precision as given in Eq.(21).\n\nThe energy difference of a Cooper pair, moving from one side of the contact to the other, is Δ\"E\" = 2\"eV\". With this expression Eq.(32) can be written as Δ\"E\" = \"hν\" which is the relation for the energy of a photon with frequency \"ν\".\n\nFigure 5 shows a so-called DC SQUID. It consists of two superconductors connected by two weak links. The fluxoid quantization of a loop through the two bulk superconductors and the two weak links demands\n\nIf the self-inductance of the loop can be neglected the magnetic flux in the loop \"Φ\" is equal to the applied flux\n\nwith \"B\" the magnetic field, applied perpendicular to the surface, and \"A\" the surface area of the loop. The total supercurrent is given by\n\nSubstitution of Eq(33) in (35) gives\n\nUsing a well known geometrical formula we get\n\nSince the sin-function can vary only between −1 and +1 a steady solution is only possible if the applied current is below a critical current given by\n\nNote that the critical current is periodic in the applied flux with period Φ₀. The dependence of the critical current on the applied flux is depicted in Fig. 6. It has a strong resemblance with the interference pattern generated by a laser beam behind a double slit. In practice the critical current is not zero at half integer values of the flux quantum of the applied flux. This is due to the fact that the self-inductance of the loop cannot be neglected.\n\nType-II superconductivity is characterized by two critical fields called \"B\" and \"B\". At a magnetic field \"B\" the applied magnetic field starts to penetrate the sample, but the sample is still superconducting. Only at a field of \"B\" the sample is completely normal. For fields in between \"B\" and \"B\" magnetic flux penetrates the superconductor in well-organized patterns, the so-called Abrikosov vortex lattice similar to the pattern shown in Fig. 2. A cross section of the superconducting plate is given in Fig. 7. Far away from the plate the field is homogeneous, but in the material superconducting currents flow which squeeze the field in bundles of exactly one flux quantum. The typical field in the core is as big as 1 tesla. The currents around the vortex core flow in a layer of about 50 nm with current densities on the order of 15 A/m². That corresponds with 15 million ampère in a wire of one mm².\n\nThe classical types of quantum systems, superconductors and superfluid helium, were discovered in the beginning of the 20th century. Near the end of the 20th century, scientists discovered how to create very dilute atomic or molecular gases, cooled first by laser cooling and then by evaporative cooling. They are trapped using magnetic fields or optical dipole potentials in ultrahigh vacuum chambers. Isotopes which have been used include rubidium (Rb-87 and Rb-85), strontium (Sr-87, Sr-86, and Sr-84) potassium (K-39 and K-40), sodium (Na-23), lithium (Li-7 and Li-6), and hydrogen (H-1). The temperatures to which they can be cooled are as low as a few nanokelvin. The developments have been very fast in the past few years. A team of NIST and the University of Colorado has succeeded in creating and observing vortex quantization in these systems. The concentration of vortices increases with the angular velocity of the rotation, similar to the case of superfluid helium and superconductivity.\n\n"}
{"id": "57401863", "url": "https://en.wikipedia.org/wiki?curid=57401863", "title": "Metasilicic acid", "text": "Metasilicic acid\n\nMetasilicic acid is the name given to the hypothetical chemical compound with formula . It has never been be isolated. It is possible that it may exist in aqueous solution at very low concentration, but it has never been definitively characterized. It may be present in dilute solutions of sodium silicate at low pH.\n\nMetasilicates occur widely in nature as inosilicates.\n"}
{"id": "53211901", "url": "https://en.wikipedia.org/wiki?curid=53211901", "title": "Moreya", "text": "Moreya\n\nMoreya or Moriya (洩矢神 \"Moreya (Moriya)-no-Kami\" or \"Moreya (Moriya)-shin\") is a Japanese deity who appears in various myths and legends of the Suwa region in Nagano Prefecture (historical Shinano Province). The most famous of such stories is his battle and subsequent defeat under the hands of Suwa (Dai)myōjin (諏訪大明神) a.k.a. Takeminakata, the god of the Grand Shrine of Suwa, who in one version of the myths about him was supposed to have established his rule in the region by subjugating local deities who resisted him.\n\nMoreya is also held to be the ancestor of the Moriya clan (守矢氏), who originally served in one of Suwa Shrine's two sub-shrines, the Upper Shrine (上社 \"Kamisha\") as the \"Jinchōkan\" (神長官), a priest subservient to the Upper Shrine's head priest, the \"Ōhōri\" (大祝), a young boy of the Suwa clan believed to be Suwa Myōjin's living incarnation. The \"Jinchōkan\", a hereditary position passed down from father to son, occupied an important position in the Upper Shrine due to his prerogative to call upon the Mishaguji, local agricultural and fertility deities worshiped in Suwa since time immemorial and who occupied a central place in many of the Upper Shrine's religious ceremonies.\n\nDue to the similarity of their names, Moreya has been for a long time conflated with the anti-Buddhist \"Ōmuraji\" Mononobe no Moriya, who was defeated by the pro-Buddhist Soga clan in 587.\n\nTwo documents dated to the mid-1230s (Katei era) name a \"Moriya-no-Ōkami\"/\"Daijin\" (守矢之大神 or 守矢太神), which are probable early references to the god and/or a shrine to him.\n\nWhat is currently the most common rendering of Moreya's name in kanji (洩矢) appear in sources such as the \"Suwa Daimyōjin Ekotoba\" (1356), where the god is pejoratively referred to as an 'evil outlaw' (洩矢の悪賊 \"Moreya no akuzoku\"), and the written genealogy of the Moriya clan, the \"Jinchō Moriya-shi Keifu\" (early Meiji period). It is also the rendering adopted by Moriya Shrine (洩矢神社) in Okaya City in Nagano. \n\nIn addition, a number of texts (see below) refer to the god as \"Moriya Daijin\" (守屋大臣 'Chancellor Moriya'), an epithet also used for the historical Mononobe no Moriya.\n\nFinally, the rendering 守宅 is also attested, although this is more commonly applied to Moreya's son, Moriya/Morita (see below).\n\nThe most famous story involving Moreya is that of his conflict against the god of Suwa Grand Shrine, known either as Takeminakata (the name given to him in the \"Kojiki\" and the official histories) or as Suwa (Dai)myōjin.\n\nThe basic gist of this myth in most sources involves a certain god (in most sources identified with Suwa Myōjin/Takeminakata) defeating Moreya, who is often described as armed with a certain kind of iron weapon or implement, using only a branch or vine of wisteria. Some versions of the story end with the triumphant god planting or discarding the branch, which then turns into a tree or a whole forest of wisterias.\n\nThe earliest written version of this myth is found in a petition (解状 \"gejō\") submitted by Suwa Nobushige (諏訪信重), then high priest (\"Ōhōri\") of the Upper Shrine of Suwa, to the Kamakura shogunate in 1249. The Upper Shrine was at the time involved in a dispute with the high priest of the Lower Shrine as to which of them was the main shrine of the area, and thus, is to set a precedent in religious rites. Against the Lower Shrine's charges, Nobushige argued for his own shrine's superiority by describing its origin myths and its ceremonies.\n\nIn the petition, Nobushige relates a story from \"the ancient customs\" (舊貫) that the Upper Shrine's precincts was once the land of 'Moriya Daijin' (守屋大臣, lit. 'Chancellor Moriya'), who got into a dispute with a god (明神 \"Myōjin\") who came down from heaven and was seeking to gain possession of his land. After arguments and armed conflicts fail to settle the matter, the two finally compete in a tug of war using hooks (\"kagi\") made out of wisteria (藤鎰) and iron (鐵鎰). After his victory, the god imposed punishment on Moriya and built his dwelling (what would become the Upper Shrine) in Moriya's land. The wisteria, which was planted before the god's dwelling, turned into a grove known as the 'Forest of Fujisuwa' (藤諏訪之森 \"Fujisuwa no mori\"), which in turn gave its name to the region.\n\nIn a later passage, Nobushige states that the god brought a mirror (真澄の鏡 \"Masumi no kagami\"), a bell (八栄の鈴 \"Yasaka no suzu\"), a bridle (轡) and a saddle (唐鞍) with him during his descent from heaven, which were preserved in his day as the Upper Shrine's sacred treasures.\n\nWhat is perhaps the most well-known version of the story occurs in Suwa (Kosaka) Enchū's \"Suwa Daimyōjin Ekotoba\" (1356) as an origin myth of Fujishima Shrine (藤島社 \"Fujishima-sha\" or 藤島大明神 \"Fujishima Daimyōjin\") in Suwa City, one of the Upper Shrine's auxiliary shrines where its yearly rice-planting ceremony is traditionally held. In this version, Moreya is described as an \"evil outlaw\" (洩矢の悪賊 \"Moreya no akuzoku\") using a ring of iron (鐵輪) as his weapon.\n\nA record of the Upper Shrine's regulations or taboos (物忌み \"monoimi\") originally enforced in 1238, the \"Suwa Kamisha butsuki\" (or: \"monoimi no\") \"rei no koto\" (諏訪上社物忌令之事), lists 'Fujishima Daimyōjin' as the first of thirteen middle-rank auxiliary shrines of the \"Kamisha\" (中十三所 \"Chū-jūsansho\") and identifies its god as a manifestation of the wrathful Wisdom King Acala (aka Fudō-Myōō). Nowadays, the shrine is instead considered to be dedicated to the goddess Konohana-Sakuya-Hime.\n\nThe myth is retold in a substantially similar fashion in a Shinto-Buddhist liturgical eulogy (講式 \"kōshiki\") to the Suwa deity believed to have been composed by Enchū around the same time as the \"Ekotoba\" known as the \"Suwa Daimyōjin Kōshiki\" (諏方大明神講式).\n\nTwo genealogies of the Suwa clan, the \"Maeda-bon Jinshi Keizu\" ( 神氏系図) and the \"Jinke Keizu\" (神家系図), contain a somewhat euhemerized variant of the myth: the battle is placed during the reign of the Emperor Yōmei (reigned 585-587), Moreya - again referred to as 'Moriya (Daijin)' (守屋(大臣)) - is apparently conflated with the anti-Buddhist \"ōmuraji\" Mononobe no Moriya (died 587), and Suwa Myōjin fights the battle through his avatar, the boy priest Arikazu (有員). \n\nA Muromachi period work authored by Rinzai monk Ten'in Ryūtaku (天隠龍沢, 1422-1500) known as the \"Ten'in-goroku\" (天陰語録) recounts this variant of the myth: here, Arikazu was said to have defeated Moreya/Moriya \"at the time of the 32nd human sovereign Emperor Yōmei, (when) Buddhism spread to the east.\" (人皇卅二代用明天皇、佛法東流之時)\n\nThe \"Jinchō Moriya-shi Keifu\" (神長守矢氏系譜), a genealogy of the Moriya clan compiled at the beginning of the Meiji period by Moriya Saneyoshi (守矢実久), meanwhile identifies Moreya's opponent with Takeminakata as he appears in both the \"Kojiki\" and the \"Sendai Kuji Hongi\": a god driven away from the land of Izumo.\n\nAnother myth involving Moreya concerns the defeat of the god Yatsukao-no-Mikoto (矢塚男命) also known as Ganigawara (蟹河原長者 \"Ganigawara-chōja\") of Tenpaku-shime Shrine (天白七五三社) in Chino City.\n\nA folk version of the story states that after Moreya's defeat in the hands of the Suwa deity (identified here with Takeminakata of Izumo), Ganigawara, a powerful and wealthy god/chieftain in the region, held Moreya in contempt for surrendering to the foreign god and even had messengers publicly revile him as a coward. When Moreya paid no heed to their insults, Ganigawara's servants began to resort to violence, shooting arrows at the palace Moreya and other deities were erecting for Takeminakata. Takeminakata, considering it an affront, then launched an all-out attack against Ganigawara, who was caught unprepared and thus, was quickly defeated.\n\nGanigawara, mortally wounded by an arrow in the assault, repents in his deathbed and, via Moreya, entrusts his youngest daughter to Takeminakata. Takeminakata gives her hand in marriage to Taokihooi-no-Mikoto (手置帆負命) a.k.a. Hikosashiri/Hikosachi-no-Kami (彦狭知神), who was injured by Ganigawara's messengers as he was keeping watch over Takeminakata's abode. Taokihooi then settled down with his new wife in Ganigawara's land, which he also inherited.\n\nBehind the Upper Shrine of Suwa, straddling the border between the cities of Suwa and Ina, stands the 1,650 metre-high Mount Moriya ( \"Moriya-san\"), which is often claimed to be the shrine's \"go-shintai\".\n\nThe god of this mountain, usually called Moriya Daijin (守屋大臣), identified with either Moreya (in Suwa Nobushige's petition, the \"Maeda-bon Jinshi Keizu\", and the \"Jinke Keizu\", the mountain is portrayed as the battleground between Suwa Myōjin/the priest Arikazu and 'Moriya Daijin') or Mononobe no Moriya (a miniature shrine or \"hokora\" located on the mountain's peak is currently considered as the rear shrine or \"oku no miya\" (奥宮) of a shrine to the Mononobe chancellor on the slopes of the mountain), is traditionally held as a weather deity who causes rain to fall when angered. Local rainmaking rituals once involved provoking the god into sending rain by vandalizing the \"hokora\" or throwing it down the mountain side.\n\nPopular belief holds that rain falls whenever clouds gather around the mountain top. A local folk song advises people to prepare to mow the fields whenever the following signs of incoming rain are observed:\n\nThe \"Jinchō Moriya-shi Keifu\" claims Moreya to have had two children, a daughter named Tamaru-hime (多満留姫) and a son named Moriya (守宅神 or 守矢神) or Morita-no-Kami (守田神). Tamaru-hime married Izuhayao (出速雄神), a son of Takeminakata, while Morita became the father of the god Chikatō (千鹿頭神), who married Urako-hime (宇良古比売命), the goddess of Mount Urako (宇良古山) in modern-day Matsumoto City.\n\nCuriously, according to the genealogy Chikatō was succeeded by Kodamahiko (児玉彦命), the son of Katakurabe (片倉辺命), another one of Takeminakata's children, under the orders of the god himself. Kodamahiko was said to have married Mitsutama-hime (美都多麻比売神), the daughter of Moritatsu (守達神) - identified in the genealogy as yet another son of Takeminakata - and begat the god Yakushi (八櫛神).\n\nThe story of Moreya's battle against Suwa Myōjin/Takeminakata/Arikazu has been interpreted either as a mythicization of a conflict between indigenous Jōmon hunter-gatherers of the Suwa region and agrarian Yayoi peoples who began to settle in the area, or between the local clans of Suwa and the expanding Yamato state somewhere during the 6th to 7th centuries CE (Asuka period). Accordingly, Moreya is believed by some to be a deified indigenous priest-chieftain or clan leader who once held political and religious authority over the Suwa region.\n\nThe author (1990) meanwhile theorizes Moreya to be a personification of the guardian nature spirit or Mishaguji worshipped by the Moriya clan. He further proposes that Moreya/Moriya and Moriya/Morita - Moreya's son in the \"Jinchō Moriya-shi Keifu\" - were originally a single figure later split into two, with the elder Moriya (Moreya) being connected with hunting and the younger Moriya (Morita) being mostly associated with sedentary agriculture.\n\nThe similarity between the names of both Moreya/Moriya and the 6th century \"Ōmuraji\" Mononobe no Moriya, who opposed the introduction of Buddhism to Japan, had led to a long-standing conflation of the two figures (cf. Moreya being called \"Moriya Daijin\" in medieval texts).\n\nA certain legend claims that a son of Mononobe no Moriya, named either Takemaro (武麿) or Otogimi (弟君), survived the defeat of the Mononobe in the Battle of Mount Shigi in 587 and fled to Suwa, where he married into the \"Jinchō\" Moriya clan; Takemaro is thus reckoned as a clan ancestor in the \"Jinchō Moriya-shi Keifu\". A small mound in the Moriya clan's historical estate in Chino dated to the 7th century is claimed in family lore to be Takemaro's tomb.\n\nŌwa (1990) believes this story to be modelled after the biography of Mononobe no Masara (物部麻佐良), who according to the \"Sendai Kuji Hongi\" married Imoko (妹古), the daughter of an unidentified 'Suwa-no-Atai' (須羽直) during the reign of Emperor Buretsu in the late 5th century, yet sees it as at least being inspired by a real-life connection between the Mononobe clan and Shinano Province.\n\nMoriya Shrine in Okaya City where Moreya is worshipped currently denies any connection between the god and the Mononobe chancellor.\n\nMoreya is held to be the ancestor of the Moriya clan (守矢氏), which traditionally served as priests of the Suwa \"Kamisha\".\n\nThe chief priest of the \"Kamisha\" was the \"ōhōri\" or \"ōhafuri\" (大祝 'great priest'), who was considered to be a living deity and the embodiment of Suwa Myōjin, the god of the shrine. Assisting the \"ōhōri\" (who by tradition was a member of the Suwa clan, Suwa Myōjin's supposed descendants) were five priests, at the head of which was the \"kan-no-osa\" (神長, also \"jinchō\") or \"jinchōkan\" (神長官), an office occupied by members of the Moriya clan. The \"jinchōkan\" oversaw the \"Kamisha\"'s rites and ceremonies in general and summoned Mishaguji - a god or gods thought to inhabit rocks or trees worshipped in the region since ancient times and regarded by the Moriya as their patron deity - to possess individuals or inanimate objects during ceremonies, being the only one considered to be able to do so.\n\nIt has been observed that the religious climate of Suwa is a syncretism of ancient indigenous beliefs and practices (e.g. the worship of Mishaguji) reorganized under a Yamato framework (e.g. Suwa Myōjin/Takeminakata, the cult of the \"ōhōri\"), but with the local element still predominant. Despite (or perhaps because of) his officially being a living god and the \"Kamisha\"'s chief priest, the Suwa \"ōhōri\" - who assumed the office during childhood - had little, if any, real power or influence in the shrine's affairs, which firmly rested in the hands of the Moriya \"jinchōkan\", with his unique ability to hear Mishaguji and call upon the god(s) to descend upon someone or something and his knowledge of special rituals, which were closely guarded secrets traditionally passed down via word of mouth only to a single individual, the heir to the office of \"jinchōkan\".\n\nThe establishment of State Shinto in the Meiji period abolished the tradition of hereditary succession among Shinto priests and private ownership of shrines. Accordingly, the shrine at Suwa passed under the control of the state, with government appointees replacing the clans who had historically served as its priests, the Suwa and the Moriya among them. As the ancient priestly offices of the shrine became defunct, most of the unwritten tradition once guarded by the Moriya \"jinchōkan\" died with the last occupant of the position. A museum dedicated to preserving information about the Moriya and documents owned by the family, the (神長官守矢史料館 \"Jinchōkan Moriya Shiryōkan\"), currently stands on the clan estate in Chino City, Nagano.\n\nA single shrine to Moreya, Moriya Shrine (洩矢神社 \"Moriya-jinja\"), stands in Okaya City, Nagano, near the Tenryū River. On the opposite bank stands Fujishima Shrine (藤島神社 \"Fujishima-jinja\"), where legend says Suwa Myōjin stood or encamped during the battle. The other Fujishima Shrine (藤島社 \"Fujishima-sha\"), where the wisteria Suwa Myōjin used during the battle was supposedly planted and sprouted, is in Nakasu, Suwa City.\n\n\n\n"}
{"id": "44764612", "url": "https://en.wikipedia.org/wiki?curid=44764612", "title": "Nama Karoo", "text": "Nama Karoo\n\nNama Karoo is a xeric shrubland ecoregion located on the central plateau of South Africa and Namibia. It occupies most of the interior of the western half of South Africa and extends into the southern interior of Namibia.\n\nThe climate tends to be volatile and very harsh. Droughts are frequent with rain primarily falling in the summer. Rainfall can also be varied with it fluctuating between per year. Rainfall is known to be highly seasonal, peaking between December and March. It tends to decrease from the east to west and from north to south. The variability in the inter-annual rainfall also tends to increase with increasing aridity. Temperature variations as large as between day and night are common. Mean maximum temperatures in the mid-summer (January) exceeds , while mean minimum mid-winter (July) temperatures are below freezing.\n\n\n"}
{"id": "383813", "url": "https://en.wikipedia.org/wiki?curid=383813", "title": "Nuclear and radiation accidents and incidents", "text": "Nuclear and radiation accidents and incidents\n\nA nuclear and radiation accident is defined by the International Atomic Energy Agency (IAEA) as \"an event that has led to significant consequences to people, the environment or the facility.\" Examples include lethal effects to individuals, radioactive isotope to the environment, or reactor core melt.\" The prime example of a \"major nuclear accident\" is one in which a reactor core is damaged and significant amounts of radioactive isotopes are released, such as in the Chernobyl disaster in 1986.\n\nThe impact of nuclear accidents has been a topic of debate since the first nuclear reactors were constructed in 1954, and has been a key factor in public concern about nuclear facilities. Technical measures to reduce the risk of accidents or to minimize the amount of radioactivity released to the environment have been adopted, however human error remains, and \"there have been many accidents with varying impacts as well near misses and incidents\". As of 2014, there have been more than 100 serious nuclear accidents and incidents from the use of nuclear power. Fifty-seven accidents have occurred since the Chernobyl disaster, and about 60% of all nuclear-related accidents have occurred in the USA. Serious nuclear power plant accidents include the Fukushima Daiichi nuclear disaster (2011), Chernobyl disaster (1986), Three Mile Island accident (1979), and the SL-1 accident (1961). Nuclear power accidents can involve loss of life and large monetary costs for remediation work.\n\nNuclear-powered submarine accidents include the K-19 (1961), K-11 (1965), K-27 (1968), K-140 (1968), K-429 (1970), K-222 (1980), and K-431 (1985). Serious radiation incidents/accidents include the Kyshtym disaster, Windscale fire, radiotherapy accident in Costa Rica, radiotherapy accident in Zaragoza, radiation accident in Morocco, Goiania accident, radiation accident in Mexico City, radiotherapy unit accident in Thailand, and the Mayapuri radiological accident in India.\n\nThe IAEA maintains a website reporting recent accidents.\n\nOne of the worst nuclear accidents to date was the Chernobyl disaster which occurred in 1986 in Ukraine. The accident killed 31 people directly and damaged approximately $7 billion of property. A study published in 2005 estimates that there will eventually be up to 4,000 additional cancer deaths related to the accident among those exposed to significant radiation levels. Radioactive fallout from the accident was concentrated in areas of Belarus, Ukraine and Russia. Other studies have estimated as many as over a million eventual cancer deaths from Chernobyl. Estimates of eventual deaths from cancer are highly contested. Industry, UN and DOE agencies claim low numbers of legally provable cancer deaths will be traceable to the disaster. The UN, DOE and industry agencies all use the limits of the epidemiological resolvable deaths as the cutoff below which they cannot be legally proven to come from the disaster. Independent studies statistically calculate fatal cancers from dose and population, even though the number of additional cancers will be below the epidemiological threshold of measurement of around 1%. These are two very different concepts and lead to the huge variations in estimates. Both are reasonable projections with different meanings. Approximately 350,000 people were forcibly resettled away from these areas soon after the accident.\n\nSocial scientist and energy policy expert, Benjamin K. Sovacool has reported that worldwide there have been 99 accidents at nuclear power plants from 1952 to 2009 (defined as incidents that either resulted in the loss of human life or more than US$50,000 of property damage, the amount the US federal government uses to define major energy accidents that must be reported), totaling US$20.5 billion in property damages. Fifty-seven accidents have occurred since the Chernobyl disaster, and almost two-thirds (56 out of 99) of all nuclear-related accidents have occurred in the US. There have been comparatively few fatalities associated with nuclear power plant accidents.\n\nThe vulnerability of nuclear plants to deliberate attack is of concern in the area of nuclear safety and security. Nuclear power plants, civilian research reactors, certain naval fuel facilities, uranium enrichment plants, fuel fabrication plants, and even potentially uranium mines are vulnerable to attacks which could lead to widespread radioactive contamination. The attack threat is of several general types: commando-like ground-based attacks on equipment which if disabled could lead to a reactor core meltdown or widespread dispersal of radioactivity; and external attacks such as an aircraft crash into a reactor complex, or cyber attacks.\n\nThe United States 9/11 Commission found that nuclear power plants were potential targets originally considered for the September 11, 2001 attacks. If terrorist groups could sufficiently damage safety systems to cause a core meltdown at a nuclear power plant, and/or sufficiently damage spent fuel pools, such an attack could lead to widespread radioactive contamination. The Federation of American Scientists have said that if nuclear power use is to expand significantly, nuclear facilities will have to be made extremely safe from attacks that could release massive quantities of radioactivity into the community. New reactor designs have features of passive nuclear safety, which may help. In the United States, the NRC carries out \"Force on Force\" (FOF) exercises at all Nuclear Power Plant (NPP) sites at least once every three years.\n\nNuclear reactors become preferred targets during military conflict and, over the past three decades, have been repeatedly attacked during military air strikes, occupations, invasions and campaigns. Various acts of civil disobedience since 1980 by the peace group Plowshares have shown how nuclear weapons facilities can be penetrated, and the group's actions represent extraordinary breaches of security at nuclear weapons plants in the United States. The National Nuclear Security Administration has acknowledged the seriousness of the 2012 Plowshares action. Non-proliferation policy experts have questioned \"the use of private contractors to provide security at facilities that manufacture and store the government's most dangerous military material\". Nuclear weapons materials on the black market are a global concern,<ref name=\"A Nuclear 9/11\">Brian Michael Jenkins. A Nuclear 9/11? \"CNN.com\", September 11, 2008.</ref> and there is concern about the possible detonation of a small, crude nuclear weapon or dirty bomb by a militant group in a major city, causing significant loss of life and property.\n\nThe number and sophistication of cyber attacks is on the rise. \"Stuxnet\" is a computer worm discovered in June 2010 that is believed to have been created by the United States and Israel to attack Iran's nuclear facilities. It switched off safety devices, causing centrifuges to spin out of control. The computers of South Korea's nuclear plant operator (KHNP) were hacked in December 2014. The cyber attacks involved thousands of phishing emails containing malicious codes, and information was stolen.\n\nSerious radiation and other accidents and incidents include:\n\n\nBetween 16 July 1945 and 23 September 1992, the United States maintained a program of vigorous nuclear testing, with the exception of a moratorium between November 1958 and September 1961. By official count, a total of 1,054 nuclear tests and two nuclear attacks were conducted, with over 100 of them taking place at sites in the Pacific Ocean, over 900 of them at the Nevada Test Site, and ten on miscellaneous sites in the United States (Alaska, Colorado, Mississippi, and New Mexico). Until November 1962, the vast majority of the U.S. tests were atmospheric (that is, above-ground); after the acceptance of the Partial Test Ban Treaty all testing was regulated underground, in order to prevent the dispersion of nuclear fallout.\n\nThe U.S. program of atmospheric nuclear testing exposed a number of the population to the hazards of fallout. Estimating exact numbers, and the exact consequences, of people exposed has been medically very difficult, with the exception of the high exposures of Marshall Islanders and Japanese fishers in the case of the Castle Bravo incident in 1954. A number of groups of U.S. citizens — especially farmers and inhabitants of cities downwind of the Nevada Test Site and U.S. military workers at various tests — have sued for compensation and recognition of their exposure, many successfully. The passage of the Radiation Exposure Compensation Act of 1990 allowed for a systematic filing of compensation claims in relation to testing as well as those employed at nuclear weapons facilities. As of June 2009 over $1.4 billion total has been given in compensation, with over $660 million going to \"downwinders\".\n\nThe International Atomic Energy Agency says there is \"a persistent problem with the illicit trafficking in nuclear and other radioactive materials, thefts, losses and other unauthorized activities\". The IAEA Illicit Nuclear Trafficking Database notes 1,266 incidents reported by 99 countries over the last 12 years, including 18 incidents involving HEU or plutonium trafficking:\n\n\nA nuclear meltdown is a severe nuclear reactor accident that results in reactor core damage from overheating. It has been defined as the accidental melting of the core of a nuclear reactor, and refers to the core's either complete or partial collapse. A core melt accident occurs when the heat generated by a nuclear reactor exceeds the heat removed by the cooling systems to the point where at least one nuclear fuel element exceeds its melting point. This differs from a fuel element failure, which is not caused by high temperatures. A meltdown may be caused by a loss of coolant, loss of coolant pressure, or low coolant flow rate or be the result of a criticality excursion in which the reactor is operated at a power level that exceeds its design limits. Alternately, in a reactor plant such as the RBMK-1000, an external fire may endanger the core, leading to a meltdown.\n\nLarge-scale nuclear meltdowns at civilian nuclear power plants include:\n\n\nOther core meltdowns have occurred at:\n\nEight Soviet Navy nuclear submarines have had nuclear core meltdowns or radiation incidents: K-19 (1961), K-11(1965), K-27 (1968), K-140 (1968), K-429 (1970), K-222 (1980), K-314 (1985), and K-431 (1985).\n\nA criticality accident (also sometimes referred to as an \"excursion\" or \"power excursion\") occurs when a nuclear chain reaction is accidentally allowed to occur in fissile material, such as enriched uranium or plutonium. The Chernobyl accident is not universally regarded an example of a criticality accident, because it occurred in an operating reactor at a power plant. The reactor was supposed to be in a controlled critical state, but control of the chain reaction was lost. The accident destroyed the reactor and left a large geographic area uninhabitable. In a smaller scale accident at Sarov a technician working with highly enriched uranium was irradiated while preparing an experiment involving a sphere of fissile material. The Sarov accident is interesting because the system remained critical for many days before it could be stopped, though safely located in a shielded experimental hall. This is an example of a limited scope accident where only a few people can be harmed, while no release of radioactivity into the environment occurred. A criticality accident with limited off site release of both radiation (gamma and neutron) and a very small release of radioactivity occurred at Tokaimura in 1999 during the production of enriched uranium fuel. Two workers died, a third was permanently injured, and 350 citizens were exposed to radiation. In 2016, a criticality accident was reported at the Afrikantov OKBM Critical Test Facility in Russia.\n\nDecay heat accidents are where the heat generated by the radioactive decay causes harm. In a large nuclear reactor, a loss of coolant accident can damage the core: for example, at Three Mile Island a recently shutdown (SCRAMed) PWR reactor was left for a length of time without cooling water. As a result, the nuclear fuel was damaged, and the core partially melted. The removal of the decay heat is a significant reactor safety concern, especially shortly after shutdown. Failure to remove decay heat may cause the reactor core temperature to rise to dangerous levels and has caused nuclear accidents. The heat removal is usually achieved through several redundant and diverse systems, and the heat is often dissipated to an 'ultimate heat sink' which has a large capacity and requires no active power, though this method is typically used after decay heat has reduced to a very small value. The main cause of release of radioactivity in the Three Mile Island accident was a pilot-operated relief valve on the primary loop which stuck in the open position. This caused the overflow tank into which it drained to rupture and release large amounts of radioactive cooling water into the containment building.\n\nIn 2011, an earthquake and tsunami caused a loss of electric power at the Fukushima Daiichi nuclear power plant in Japan. The decay heat could not be removed, and the reactor cores of units 1, 2 and 3 overheated, the nuclear fuel melted, and the containments were breached. Radioactive materials were released from the plant to the atmosphere and to the ocean. \n\nTransport accidents can cause a release of radioactivity resulting in contamination or shielding to be damaged resulting in direct irradiation. In Cochabamba a defective gamma radiography set was transported in a passenger bus as cargo. The gamma source was outside the shielding, and it irradiated some bus passengers.\n\nIn the United Kingdom, it was revealed in a court case that in March 2002 a radiotherapy source was transported from Leeds to Sellafield with defective shielding. The shielding had a gap on the underside. It is thought that no human has been seriously harmed by the escaping radiation.\n\nEquipment failure is one possible type of accident. In Białystok, Poland, in 2001 the electronics associated with a particle accelerator used for the treatment of cancer suffered a malfunction. This then led to the overexposure of at least one patient. While the initial failure was the simple failure of a semiconductor diode, it set in motion a series of events which led to a radiation injury.\n\nA related cause of accidents is failure of control software, as in the cases involving the Therac-25 medical radiotherapy equipment: the elimination of a hardware safety interlock in a new design model exposed a previously undetected bug in the control software, which could have led to patients receiving massive overdoses under a specific set of conditions.\n\nMany of the major nuclear accidents have been directly attributable to operator or human error. This was obviously the case in the analysis of both the Chernobyl and TMI-2 accidents. At Chernobyl, a test procedure was being conducted prior to the accident. The leaders of the test permitted operators to disable and ignore key protection circuits and warnings that would have normally shut the reactor down. At TMI-2, operators permitted thousands of gallons of water to escape from the reactor plant before observing that the coolant pumps were behaving abnormally. The coolant pumps were thus turned off to protect the pumps, which in turn led to the destruction of the reactor itself as cooling was completely lost within the core.\n\nA detailed investigation into SL-1 determined that one operator (perhaps inadvertently) manually pulled the central control rod out about 26 inches rather than the maintenance procedure's intention of about 4 inches.\n\nAn assessment conducted by the Commissariat à l’Énergie Atomique (CEA) in France concluded that no amount of technical innovation can eliminate the risk of human-induced errors associated with the operation of nuclear power plants. Two types of mistakes were deemed most serious: errors committed during field operations, such as maintenance and testing, that can cause an accident; and human errors made during small accidents that cascade to complete failure.\n\nIn 1946 Canadian Manhattan Project physicist Louis Slotin performed a risky experiment known as \"tickling the dragon's tail\" which involved two hemispheres of neutron-reflective beryllium being brought together around a plutonium core to bring it to criticality. Against operating procedures, the hemispheres were separated only by a screwdriver. The screwdriver slipped and set off a chain reaction criticality accident filling the room with harmful radiation and a flash of blue light (caused by excited, ionized air particles returning to their unexcited states). Slotin reflexively separated the hemispheres in reaction to the heat flash and blue light, preventing further irradiation of several co-workers present in the room. However, Slotin absorbed a lethal dose of the radiation and died nine days later. The infamous plutonium mass used in the experiment was referred to as the demon core.\n\nLost source accidents, also referred to as orphan sources, are incidents in which a radioactive source is lost, stolen or abandoned. The source then might cause harm to humans. One case occurred at Yanango where a radiography source was lost, also at Samut Prakarn a phosphorus teletherapy source was lost and at Gilan in Iran a radiography source harmed a welder. The best known example of this type of event is the Goiânia accident in Brazil.\n\nThe International Atomic Energy Agency has provided guides for scrap metal collectors on what a sealed source might look like. The scrap metal industry is the one where lost sources are most likely to be found.\n\nComparing the historical safety record of civilian nuclear energy with other forms of electrical generation, Ball, Roberts, and Simpson, the IAEA, and the Paul Scherrer Institute found in separate studies that during the period from 1970 to 1992, there were just 39 on-the-job deaths of nuclear power plant workers worldwide, while during the same time period, there were 6,400 on-the-job deaths of coal power plant workers, 1,200 on-the-job deaths of natural gas power plant workers and members of the general public caused by natural gas power plants, and 4,000 deaths of members of the general public caused by hydroelectric power plants. In particular, coal power plants are estimated to kill 24,000 Americans per year due to lung disease as well as causing 40,000 heart attacks per year in the United States. According to \"Scientific American\", the average coal power plant emits 100 times more radiation per year than a comparatively sized nuclear power plant in the form of toxic coal waste known as fly ash.\n\nIn terms of energy accidents, hydroelectric plants were responsible for the most fatalities, but nuclear power plant accidents rank first in terms of their economic cost, accounting for 41 percent of all property damage. Oil and hydroelectric follow at around 25 percent each, followed by natural gas at 9 percent and coal at 2 percent. Excluding Chernobyl and the Shimantan Dam, the three other most expensive accidents involved the Exxon Valdez oil spill (Alaska), the Prestige oil spill (Spain), and the Three Mile Island nuclear accident (Pennsylvania).\n\nNuclear safety covers the actions taken to prevent nuclear and radiation accidents or to limit their consequences. This covers nuclear power plants as well as all other nuclear facilities, the transportation of nuclear materials, and the use and storage of nuclear materials for medical, power, industry, and military uses.\n\nThe nuclear power industry has improved the safety and performance of reactors, and has proposed new safer (but generally untested) reactor designs but there is no guarantee that the reactors will be designed, built and operated correctly. Mistakes do occur and the designers of reactors at Fukushima in Japan did not anticipate that a tsunami generated by an earthquake would disable the backup systems that were supposed to stabilize the reactor after the earthquake. According to UBS AG, the Fukushima I nuclear accidents have cast doubt on whether even an advanced economy like Japan can master nuclear safety. Catastrophic scenarios involving terrorist attacks are also conceivable.\n\nIn his book \"Normal Accidents\", Charles Perrow says that unexpected failures are built into society's complex and tightly-coupled nuclear reactor systems. Nuclear power plants cannot be operated without some major accidents. Such accidents are unavoidable and cannot be designed around. An interdisciplinary team from MIT have estimated that given the expected growth of nuclear power from 2005 – 2055, at least four serious nuclear accidents would be expected in that period. To date, there have been five serious accidents (core damage) in the world since 1970 (one at Three Mile Island in 1979; one at Chernobyl in 1986; and three at Fukushima-Daiichi in 2011), corresponding to the beginning of the operation of generation II reactors. This leads to on average one serious accident happening every eight years worldwide.\n\n\n\n"}
{"id": "6864370", "url": "https://en.wikipedia.org/wiki?curid=6864370", "title": "Outgoing longwave radiation", "text": "Outgoing longwave radiation\n\nOutgoing Longwave Radiation (OLR) is the energy radiating from the Earth as infrared radiation at low energy to Space.\n\nOLR is electromagnetic radiation emitted from Earth and its atmosphere out to space in the form of thermal radiation. The flux of energy transported by outgoing longwave radiation is measured in W/m².\n\nOver 99% of outgoing longwave radiation has wavelengths between 4 µm and 100 µm, in the thermal infrared part of the electromagnetic spectrum. Contributions with wavelengths larger than 40 µm are small, therefore often only wavelengths up to 50 µm are considered . In the wavelength range between 4 µm and 10 µm the spectrum of outgoing longwave radiation overlaps that of solar radiation, and for various applications different cut-off wavelengths between the two may be chosen.\n\nRadiative cooling by outgoing longwave radiation is the primary way the Earth System loses energy. The balance between this loss and the energy gained by radiative heating from incoming solar shortwave radiation determines global heating or cooling of the Earth system (Energy budget of Earth’s climate). Local differences between radiative heating and cooling provide the energy that drives atmospheric dynamics.\n\nOLR is a critical component of the Earth's energy budget, and represents the total radiation going to space emitted by the atmosphere. Earth's radiation balance is quite closely achieved since the OLR very nearly equals the Shortwave Absorbed Radiation received at high energy from the sun. Thus, the Earth's average temperature is very nearly stable. The OLR is affected by clouds and dust in the atmosphere, which tend to reduce it to below clear sky values.\n\nGreenhouse gases, such as methane (CH), nitrous oxide (NO), water vapor (HO) and carbon dioxide (CO), absorb certain wavelengths of OLR adding heat to the atmosphere. Some of this thermal radiation is directed back towards the Earth, increasing the average temperature of the Earth's surface. Therefore, an increase in the concentration of a greenhouse gas may contribute to global warming by increasing the amount of radiation that is absorbed and emitted by these atmospheric constituents. If the absorptivity of the gas is high and the gas is present in a high enough concentration, the absorption bandwidth becomes saturated. In this case, there is enough gas present to completely absorb the radiated energy in the absorption bandwidth before the upper atmosphere is reached, and adding a higher concentration of this gas will have no additional effect on the energy budget of the atmosphere. \n\nThe OLR is dependent on the temperature of the radiating body. It is affected by the Earth's skin temperature, skin surface emissivity, atmospheric temperature, water vapor profile, and cloud cover.\n\nLongwave radiation typically refers to radiation in the spectral region from 3 to 100 microns. In the Earth’s climate system, longwave radiation involves processes of absorption, scattering, and emissions from atmospheric gases, aerosols, clouds and the surface. Measuring outgoing longwave radiation at the top of atmosphere and downwelling longwave radiation at the surface are important for understanding how much radiative energy is kept in our climate system, how much reaches and warms the surface, and how the energy in the atmosphere is distributed to affect developments of clouds.\n\nOutgoing longwave radiation (OLR) has been monitored globally since 1975 by a number of successful and valuable satellite missions. These missions include broadband measurements from the Earth Radiation Balance (ERB) instrument on the Nimbus-6 and Nimbus-7 satellites; Earth Radiation Budget Experiment (ERBE) scanner and the ERBE non scanner on NOAA-9, NOAA-10 and NASA Earth Radiation Budget Satellite (ERBS); The Clouds and the Earth's Radiant Energy System (CERES) instrument aboard NASA's Aqua and Terra satellites; and Geostationary Earth Radiation Budget instrument (GERB) instrument on the Meteosat Second Generation (MSG) satellite.\n\nDownwelling longwave radiation at the surface is mainly measured by Pyrgeometer. A most notable ground-based network for monitoring surface longwave radiation is Baseline Surface Radiation Network (BSRN), which provides crucial well-calibrated measurements for studying global dimming and brightening.\n\nMany applications call for calculation of longwave radiation quantities: the balance of global incoming shortwave to outgoing longwave radiative flux determines the Energy budget of Earth’s climate; local radiative cooling by outgoing longwave radiation (and heating by shortwave radiation) drive the temperature and dynamics of different parts of the atmosphere; from the radiance from a particular direction measured by an instrument, atmospheric properties (like temperature or humidity can be retrieved, etc.\nCalculations of these quantities solve the radiative transfer equations that describe radiation in the atmosphere. Usually the solution is done numerically by an Atmospheric radiative transfer code adapted to the specific problem.\n\n\n"}
{"id": "3272196", "url": "https://en.wikipedia.org/wiki?curid=3272196", "title": "Palmason Model", "text": "Palmason Model\n\nThe Palmason Model is a depth, distance, temperature and heat flow gradient model of crustal accretion mechanism through the Iceland lithosphere which denotes the spreading material trajectories from a rift axis. The material erupting at the rift axis will tend to sink down, due to thermal subsidience and spreading, to a depth of many kilometers, while lava flows spreading to a distance of many kilometers away from the rift axis on the surface will sink down to shallower depth. Surface erosion can expose such preserved materials.\n\nThe model was developed in the 1970s and 80s by Guðmundur Pálmason (1928–2004), once one of Iceland's best chess players. As usual outside Iceland, the name of the model misunderstands Icelandic naming conventions, because Pálmason is a patronymic not a surname, and in Iceland he would have properly been referred to by his given first name, Guðmundur.\n"}
{"id": "2718705", "url": "https://en.wikipedia.org/wiki?curid=2718705", "title": "Pi Ursae Minoris", "text": "Pi Ursae Minoris\n\nThe Bayer designation Pi Ursae Minoris (π UMi, π Ursae Minoris) is shared by two star systems, π Ursae Minoris and π Ursae Minoris, in the constellation Ursa Minor. They are separated by 0.64° on the sky.\n\n"}
{"id": "658283", "url": "https://en.wikipedia.org/wiki?curid=658283", "title": "Scoria", "text": "Scoria\n\nScoria is a highly vesicular, dark colored volcanic rock that may or may not contain crystals (phenocrysts). It is typically dark in color (generally dark brown, black or purplish red), and basaltic or andesitic in composition. Scoria is relatively low in density as a result of its numerous macroscopic ellipsoidal vesicles, but in contrast to pumice, all scoria has a specific gravity greater than 1, and sinks in water. The holes or vesicles form when gases that were dissolved in the magma come out of solution as it erupts, creating bubbles in the molten rock, some of which are frozen in place as the rock cools and solidifies. Scoria may form as part of a lava flow, typically near its surface, or as fragmental ejecta (lapilli, blocks and bombs), for instance in Strombolian eruptions that form steep-sided scoria cones. Most scoria is composed of glassy fragments, and may contain phenocrysts. The word \"scoria\" comes from the Greek σκωρία, \"skōria\", rust. A colloquial term for scoria is cinder.\n\nScoria differs from pumice, another vesicular volcanic rock, in having larger vesicles and thicker vesicle walls, and hence is denser. The difference is probably the result of lower magma viscosity, allowing rapid volatile diffusion, bubble growth, coalescence, and bursting.\n\nAs rising magma encounters lower pressures, dissolved gases are able to exsolve and form vesicles. Some of the vesicles are trapped when the magma chills and solidifies. Vesicles are usually small, spheroidal and do not impinge upon one another; instead they open into one another with little distortion.\n\nVolcanic cones of scoria can be left behind after eruptions, usually forming mountains with a crater at the summit. An example is Maungarei in New Zealand, which like Te Tatua-a-Riukiuta in the south of the same city has been extensively quarried. Quincan, a unique form of Scoria, is quarried at Mount Quincan in Far North Queensland, Australia.\n\nScoria has several useful characteristics that influence how it is used. It is somewhat porous, has high surface area and strength for its weight, and often has striking colours. Scoria is often used in landscaping and drainage works. It is also commonly used in gas barbecue grills.\n\nScoria can be used for high-temperature insulation.\n\nScoria is used on oil well sites to limit mud issues with heavy truck traffic.\n\nThe quarry of Puna Pau on Rapa Nui/Easter Island was the source of a red-coloured scoria which the Rapanui people used to carve the pukao (or topknots) for their distinctive moai statues, and to carve some moai from.\n\nIt is also used as a traction aid on ice- and snow-covered roads.\n\n"}
{"id": "36898018", "url": "https://en.wikipedia.org/wiki?curid=36898018", "title": "Soil value", "text": "Soil value\n\nSoil value () or BWZ is a comparative assessment of soil quality used in Germany. It is determined from soil sampling data and ranges from 0 (very low) to 100 (very high).\n\nAccording to the Geological Service of the state of North Rhine-Westphalia, the soil value of arable land is based on a field assessment framework which combines an assessment of soil type, soil condition (soil development) and the parent material of which the soil is composed. The result, the soil value, expresses the relative net income that, under normal and proper management, is determined only by the profitability of the soil. Additions or deductions in this value to take account of variations in yield due to terrain and climate (e.g. average annual temperature) give the field value (\"Ackerzahl\").\nThe following range of values and colour scheme is used: \n\nThe values exhibited by different types of soil are:\n\nAs a yardstick for soil quality, a \"Reich Standard Farm\" was set up during the Third Reich in the parish of Eickendorf in the Magdeburger Börde, an exceptionally rich arable region. During the Reich Soil Assessment (\"Reichsbodenschätzung\") following the Soil Assessment Act in 1934 a soil value of 100 was established in the leading Reich farm of \"Haberhauffe/Jäger\". It was the basis of comparison for the tax rating of farms in Germany. Since this farm was no longer available for comparisons within West Germany after the Second World War, a farm in Machtsum near Harsum in the Hildesheim Börde was designated as the Federal Standard Farm. During later measurements, an even higher value of BWZ - 102.8 - was measured in Mölme, about 20 km east of Hildesheim within the municipality of Söhlde. It is the highest value ever recorded in Germany.\n\nAnother measure, the field value (\"Ackerzahl\") is derived from the soil value, taking account of other factors such as climate and location e.g. along the edge of woodland.\n\n\n"}
{"id": "13881943", "url": "https://en.wikipedia.org/wiki?curid=13881943", "title": "Spaceship Earth (sculpture)", "text": "Spaceship Earth (sculpture)\n\nSpaceship Earth is a 350,000-pound Brazilian blue quartzite sculpture created by Finnish American artist Eino. The sculpture was commissioned by Brian Maxwell of Powerbar the Maxwell Family Foundation for the late environmentalist David Brower and its name was often used by Brower referring to mankind traveling through life in a common vehicle. Today the sculpture is located at Kennesaw State University adjacent to the Social Science building, the first LEED-certified building at the University System of Georgia.\n\nThe 175-ton chunk of rock was formed from 88 individual pieces of quartzite and bonded with specially made polyepoxide. After these pieces were in place, Eino attached a life-size bronze model of the late Brower. Additionally, 2,400 bronze pieces were added to the exterior to outline land masses on the earth.\n\n\"Spaceship Earth\" was completed in August 2006 and unveiled in October. The sculpture broke apart and started spilling over just two months later. First reports cited poor adhesive and unsound construction contributed to its demise while Eino argued that it had to have been an act of vandalism. The sculpture was rededicated in November 2006.\n\nA few months before the November 2000 death of the first Executive Director Sierra Club and lifelong environmentalist David Brower, the founders of PowerBar, Brian and Jennifer Maxwell, commissioned the 175-ton tribute in his honor. It was during a morning run with photographer Galen Rowell and PowerBar's Brian Maxwell that the tribute was conceived. Shortly after Maxwell met with his friend, environmentalist and Finnish-American artist Eino, to create such a sculpture.\n\nEino was sent stone samples from a rock quarry in Brazil at the start of the project and \"[he] thought, oh my, this is three times harder than marble. But immediately I understood this was the right stone. It is more permanent than anything else. No pollution can hurt it. No graffiti will harm it. It was the right stone to be here for 1,000 years.\" Over 175 tons of Brazilian blue quartzite was shipped and stored in a parking lot while Eino and Maxwell sought to secure a home for the sculpture. Locations in Berkeley to Washington, D.C., were proposed, but all were turned down by local art commissions or city councils. For nearly a year, Eino argued to have the sculpture placed in the streets of San Francisco, but the San Francisco Arts Commission’s Visual Arts Committee turned it down by saying it \"not only had little or nothing to do with the city\" but \"was too big and did not represent Brower's ideals.\"\n\nLater, in 2004, Jennifer Maxwell, then the widow of Brian Maxwell, contacted the City of Berkeley to find a home for the tribute. A few members of the Berkeley Arts Commission spoke out against the sculpture's design and how it seemed to be forced on the city. In October 2004, the commission voted 7-2 to accept the sculpture with conditions. For final approval by the Civic Art Commission sculpture could not be adorned with the bronze statue of Brower, and the Maxwell family had to pay the entire installation cost. By 2005, with a push from the Mayor, the Civic Art Commission found nearly 30 different locations the sculpture could live. In August 2005, two Berkeley area commissions voted to consider another location. While negotiations were dragging on with bureaucrats in Berkeley, officials at Kennesaw State University contacted Eino and accepted the memorial.\n\nToday the sculpture resides at Kennesaw State University, adjacent to the Social Science building. Nearly six years after its conception, Spaceship Earth was finished and unveiled on October 20, 2006. In late December 2006, only three months after its installation on campus, \"Spaceship Earth\" collapsed. The sculpture was intended to be a permanent reminder to future generations to take care of their delicate planet. After the collapse, Eino attributed the disaster to vandalism, but later reports associated the collapse with poor construction. Reconstruction was to begin in February 2007 but was delayed until July and was completed by November 2007.\n\n"}
{"id": "545187", "url": "https://en.wikipedia.org/wiki?curid=545187", "title": "Volcano (1997 film)", "text": "Volcano (1997 film)\n\nVolcano is a 1997 American disaster film directed by Mick Jackson and produced by Andrew Z. Davis, Neal H. Moritz and Lauren Shuler Donner. The storyline was conceived from a screenplay written by Jerome Armstrong and Billy Ray. The film features Tommy Lee Jones, Anne Heche, and Don Cheadle. Jones is cast as the head of the Los Angeles County Office of Emergency Management (LAC OEM) which has complete authority in the event of an emergency or natural disaster. His character attempts to divert the path of a dangerous lava flow through the streets of Los Angeles following the formation of a volcano at the La Brea Tar Pits.\n\nA joint collective effort to commit to the film's production was made by the film studios of 20th Century Fox, Moritz Original and Shuler Donner/Donner Productions. It was commercially distributed by 20th Century Fox. \"Volcano\" explores civil viewpoints, such as awareness, evacuation and crisis prevention. Although the film used extensive special effects, it failed to receive any award nominations from mainstream motion picture organizations for its production merits.\n\n\"Volcano\" premiered in theaters nationwide in the United States on April 25, 1997 grossing $49,323,468 in domestic ticket receipts, on a $90 million budget. It earned an additional $73.5 million in business through international release to top out at a combined $122,823,468 in gross revenue. Despite its release and recognition, \"Dante's Peak\" (which was released 2 months before) gained more commercial success than \"Volcano.\" It was also met with mixed critical reviews before its initial screening in cinemas. The Region 1 code widescreen edition of the film featuring special features was released on DVD in the United States on March 9, 1999.\n\nIn downtown Los Angeles, an earthquake strikes. Michael 'Mike' Roark, the head of the city's Office of Emergency Management or OEM, insists on coming to work to help out with the crisis, although he has been on vacation with his daughter Kelly. His associate, Emmit Reese, notes that the quake caused no major damage, but seven utility workers are later burned to death in a storm drain at MacArthur Park. One escapes and survives, but is severely burned on one side of his face. As a precaution, Roark tries to halt the subway lines which run parallel to where the deaths took place, but Los Angeles MTA Chairman Stan Olber mulishly opposes, feeling that there is no threat to the trains. Against regulations, Roark and his coworker Gator Harris venture down the storm sewer in the park to investigate. They are nearly burned alive and barely escape with their lives when hot gases suddenly spew out of a crack in the concrete lining and flood the tunnel. Geologist Dr. Amy Barnes believes that a volcano may be forming beneath the city with magma flowing underground. Unfortunately, she has insufficient evidence to make Roark take action.\n\nThe next morning, at around 5:15 A.M., Barnes, and her assistant Rachel, venture in the storm sewer to investigate the scene of the incident. They discover the crack in the ground that released the gases earlier. While taking samples, a more powerful earthquake strikes, and Rachel is killed when she falls into the crack that is later engulfed by a rush of the hot gases. Minutes later, in the La Brea Tar Pits, volcanic smoke and ash billows out, followed by high velocity lava bombs that burst out of the tar pits, which ignites several buildings. Steam explodes from the sewer system, while a subway train derails underground. \n\nRoark helps injured firefighters out of the area. Moments later, a newly formed volcano erupts from the tar pits, and lava begins to flow freely down Wilshire Boulevard, incinerating everything in its path, including Roark's GMC Suburban, and an LAFD fire truck downed by a lava bomb, killing two firefighters who are trapped inside. Roark and his daughter become separated as she is injured when a nearby lava bomb sputters and burns her leg, and she is taken to Cedars-Sinai Medical Center by Dr. Jaye Calder.\n\nA few minutes later, while in the Red line metro tunnel, the passengers in the derailed subway train are exposed to severe heat and toxic gases, which causes them all to eventually lose consciousness. The conductor however tries but fails to open the doors along the length of the train, until reaching the rear where he sees the incoming lava flow in the tunnel hundreds of meters away. Meanwhile, Olber leads his team through the tunnel to the derailed train, searching for survivors. They manage to save everyone, but Olber notices that the train driver is still missing and goes back; he finds the driver alive but unconscious, just as the lava reaches the train and begins to flow underneath it. Knowing that the train is melting, Olber sacrifices his life to save the driver by jumping into the lava flow, throwing the driver to safety. Roark, Barnes, and police lieutenant Ed Fox devise a plan to stack concrete barriers at the intersection of Wilshire Boulevard and Fairfax Avenue, creating a cul-de-sac to pool the lava as helicopters dump water on it to form a crust, making the operation a success. Barnes later theorizes that the magma is still flowing underground through the Red Line subway extension, and calculates that the main eruption will occur at the end of the line at the Beverly Center near Cedars-Sinai. To prove this, Barnes and Roark lower a video camera into the tunnel to watch it, only for the camera to be incinerated by a fast-moving flow of lava. They calculate the speed and realize that they have 30 minutes until the lava hits the end of the Red Line.\n\nThrough Roark's direction, explosives are used to create channels in the street to divert the flow of lava into Ballona Creek, which will later flow into the Pacific Ocean, but Barnes realizes that the street is sloping in the opposite direction and instead the lava would flow directly towards the injured patients, much to the displeasure of Roark and Fox. Roark devises another plan to demolish a 22 story condominium building to block the lava's path from flowing towards the medical area and the rest of the Los Angeles West Side. Gator refuses to abandon an LAPD SWAT cop, who has gotten trapped under a core column while slotting explosive charges. At that point the lava reaches the dead end of the Subway tunnel extension, and explodes out of the ground in a massive geyser. Gator and the officer sacrifices their lives to detonate the final explosive charge. Roark then spots Kelly nearby, trying to retrieve a small boy who wandered off, putting them in the direct path of the collapsing building. Roark barely manages to save both of them from being crushed as the building collapses. The plan is successful, and the lava flows directly to the ocean. Roark escapes from the wreckage with Kelly and the young child still alive. The death toll is nearly a hundred people, thousands injured, and damages in billions. It starts to rain, with surviving civilians having a sigh a relief. Reese shows up with the family dog Max, along with a call from the Chief of the Los Angeles Police Department on how to rebuild the city. Roark tells Reese that he is on vacation after all, and to tell the chief that too, as he goes home with Kelly while Reese takes over to handle the situation.\n\nThe film ends with an epilogue displaying a graphic stating that the volcano, named \"Mount Wilshire\", is still in an active state.\n\n\nFilming was shot primarily on location in Los Angeles, California. Various filming sites included MacArthur Park, Cedars-Sinai Medical Center and the La Brea Tar Pits. Extensive special effects surrounding certain aspects of the film such as the lava flow, were created by ten separate digital effects companies including VIFX, Digital Magic Company, Light Matters Inc., Pixel Envy and Anatomorphex. An 80% full-size replica of Wilshire Boulevard, which was one of the largest sets ever constructed in the U.S., was assembled in Torrance, California. The computer-generated imagery was coordinated and supervised by Dale Ettema and Mat Beck. Between visuals, miniatures, and animation, over 300 technicians were involved in the production aspects of the special effects.\n\nThe score for the film was originally composed and orchestrated by musical conductor Alan Silvestri. Recording artists James Newton Howard and Dillinger among others, contributed songs to the music listing. The audio soundtrack in Compact Disc format featuring 8 tracks, was officially released by the American recording label Varèse Sarabande on April 22, 1997. The sound effects in the film were supervised by Christopher Boyes. The mixing of the sound elements were orchestrated by Jim Tanenbaum and Dennis Sands.\n\nAmong mainstream critics in the US, the film received generally mixed reviews. Rotten Tomatoes reports that 48% of 42 sampled critics gave the film a positive review, with an average score of 5.1 out of 10. The website's critical consensus reads, \"\"Volcano\"s prodigious pyrotechnics and Tommy Lee Jones crotchety sneers at lava aren't quite enough to save this routine disaster film.\" At Metacritic, which assigns a weighted average using critical reviews, the film received a score of 55 out of 100 based on 22 reviews, indicating \"mixed or average reviews\". Audiences polled by CinemaScore gave the film an average grade of \"B+\" on an A+ to F scale. In 1997, the film was nominated for a Golden Raspberry Award in the category of \"Worst Reckless Disregard for Human Life and Public Property\", but lost to \"Con Air\".\nJanet Maslin, writing in \"The New York Times\", said, \"\"Volcano\" begins so excitably and hurtles so quickly into fiery pandemonium,\" but noted that \"in the disaster realm, it's not easy to have it all. A film this technically clever can't get away with patronizing and familiar genre cliches.\" Roger Ebert in the \"Chicago Sun-Times\" called it a \"surprisingly cheesy disaster epic\" while musing, \"The lava keeps flowing for much of the movie, never looking convincing. I loved it when the firemen aimed their hoses way offscreen into the middle of the lava flow, instead of maybe aiming them at the leading edge of the lava—which they couldn't do, because the lava was a visual effect, and not really there.\" In the \"San Francisco Chronicle\", Mick LaSalle wrote that \"Things go bad after \"Volcano\" plays its last card — the lava — and from there it has nothing to show but more of the same. A host of characters is introduced in the opening scenes, but \"Volcano\" doesn't know what to do with them. It can't make us care.\" Owen Gleiberman of \"Entertainment Weekly\" said, \"\"Volcano\" is cheese, all right, but it's tangy cheese. I'm not sure I've ever seen a disaster movie in which special effects this realistic and accomplished were put to the service of a premise this outlandish.\" He declared: \"\"Volcano\" is jittery in a clinical, self-important way.\" Walter Addiego of the \"San Francisco Examiner\", felt \"\"Volcano\" offers a bit of humor, a minimum of plot distraction and the joys of watching molten rock ooze down Wilshire Boulevard.\" Left equally impressed was James Berardinelli of \"ReelViews\". Commenting on the character significance of Mike Roark, played by Jones, he said it was \"a wonderfully heroic figure — a man of action who never has time to rest. The fate of the city rests on his shoulders, and he knows it. Jones' fierce, unflagging portrayal helps us accept Roark not only as the man to save L.A., but as a loving father who is more concerned about his daughter's safety than that of every other citizen.\" In his summation, he wrote, \"\"Volcano\" has opened the \"summer\" movie season at an astoundingly early late-April date. But there's no mistaking this as anything but a blockbuster trying to get a running jump on competition like \"The Fifth Element\" and \"\". This isn't the kind of film where it's worth waiting for the video tape — it's too big and brash, and demands the speakers and atmosphere of a state-of-the-art theater.\" Kenneth Turan of the \"Los Angeles Times\" added to the positive sentiment by saying the film \"glows with heat. Lava heat. The coast may be toast, but it's the lava, covering everything like a malevolent tide of melted butter, that makes this a disaster picture that's tastier than usual.\"\nWriting for \"Time Out\", author TCh said, \"The most striking aspect of this fun, old-fashioned disaster movie is the novelty of seeing the most familiar of backdrops used as a creative resource in its own right.\" He commended how \"Jones and Heche work hard to dig up an emotional rapport from next to nothing\" while also praising how the \"slow but inexorable progress of the lava makes for more suspense than the usual slam bang firework display.\" Not entirely impressed was Margaret McGurk writing for \"The Cincinnati Enquirer\". She called the film \"depreciating entertainment value of the natural-disaster trend\" while also mentioning how the \"High-caliber special effects are still fun, but all this lock-step storytelling is wearing thin.\" But in a hint of commendation, McGurk thought \"on its own escapist terms, \"Volcano\" dishes up a textbook serving of low-I.Q., high-energy entertainment.\" Describing a comical position on seismic activity, Marc Savlov of \"The Austin Chronicle\" said \"Volcano\" was a \"laughably ridiculous take on what we all secretly dream of: Los Angeles, washed away in a huge, molten tide of cheese — uh, lava, I mean.\" Savlov added, \"Screenwriters Jerome Armstrong and Billy Ray have crammed the script with topical references to the L.A. riots, Rodney King, racial inequality, sexism, the ineffectuality of the 911 system, and reams of very, very bad dialogue. So bad, in fact, that the screening audience I viewed \"Volcano\" with seemed to enjoy it immensely, hooting and hollering and laughing as though it were an old episode of \"Mystery Science Theater 3000\".\"\nRita Kempley of \"The Washington Post\", openly wondered why \"there's no volcano in \"Volcano\"?...The hokey disaster drama features towering plumes of smoke, a splendid display of fireworks and brimstone, and rivers of molten magma, but I'll be darned if there's a burning mountain.\" She concluded her review by declaring that \"While disaster yarns aren't known for subtlety, there are limits, and \"Volcano\" giddily goes beyond them. Director Mick Jackson, who also made Steve Martin's wry \"\"L.A. Story\",\" must have had his hands full with the logistics of this bombastic extravaganza. He sets a blistering pace, but the movie never generates any real thrills.\" Todd McCarthy of \"Variety\" was more positive, writing \"Volcano\" delivered \"enough spectacular action to get it off to a hot B.O. start\" and that \"first-time screenwriters Jerome Armstrong and Billy Ray waste no time with exposition or scene-setting, starting the fireworks with a nerve-jangling morning earthquake that puts city workers on alert for possible damage.\"\n\n\"Volcano\" premiered in cinemas on April 25, 1997. At its widest distribution in the U.S., the film was screened at 2,777 theaters. The film grossed $14,581,740 in box office business averaging $5,256 in revenue per theater in its opening weekend. During that first weekend in release, the film opened in first place beating out the films \"Romy & Michelle's High School Reunion\" and \"Anaconda\". The film's revenue dropped by 37% in its second week of release, earning $9,099,743. In the month of June during its final weekend showing in theaters, the film came out in 12th place grossing $602,076. The film went on to top out domestically at $49,323,468 in total ticket sales through a 7-week theatrical run. Internationally, the film took in an additional $73,500,000 in box office business for a combined total of $122,823,468. For 1997 as a whole, the film would cumulatively rank at a box office performance position of 39.\n\nFollowing its cinematic release in theaters, the film was released in VHS video format on May 26, 1998. The Region 1 Code widescreen edition of the film was released on DVD in the United States on March 9, 1999. Special features for the DVD include interactive menus, scene selection and the original theatrical trailer. It is not enhanced for widescreen televisions. The film was released on Blu-ray Disc on October 1, 2013 by Starz/Anchor Bay.\n\n"}
{"id": "17505649", "url": "https://en.wikipedia.org/wiki?curid=17505649", "title": "Vulcanized fibre", "text": "Vulcanized fibre\n\nVulcanized fibre is a laminated plastic composed of only cellulose. The material is a tough, resilient, hornlike material that is lighter than aluminium, tougher than leather, and stiffer than most thermoplastics. The newer wood-laminating grade of vulcanized fibre is used to strengthen wood laminations used in skis, skateboards, support beams and as a sub-laminate under thin wood veneers.\nA product very similar to vulcanized fibre is leatheroid. Leatheroid, however, is made using a different chemical process.\nThe British patent for vulcanized fibre was obtained in 1859 by the Englishman Thomas Taylor. He gained the patent after the introduction of celluloid in 1856 and before the invention of viscose rayon (regenerated cellulose) in 1894. In 1871 Thomas Taylor obtained the United States Patent for vulcanized fibre. The first organized industrial company to make vulcanized fibre, was the Vulcanized Fibre Company, incorporated first as a New York Corporation formed June 19, 1873 listed with William Courtenay President and Charles F. Cobby Secretary. The first N.Y. corporation was also found in the 1873 N.Y. City Directory which also listed William Courtenay President and Charles F. Cobby Secretary in 1873. From 1873 until 1878 the Vulcanized Fiber Co. had a New York office address of 17 Dey St., while the factory was located in Wilmington Delaware. This can be seen in the many advertisements that were placed in different publications at this time in history. A special charter was granted by the state of Delaware in 1873 until the Delaware corporation was finally incorporated on February 8, 1875 which now listed William Courtenay President and Clement B. Smyth Secretary.\n\nIn 1884 Courtenay & Trull Co. N.Y. was merged into the Vulcanized Fibre Co. which gave the company control over a new invention called by the trade name \"Gelatinized Fibre\".\n\nOn December 4, 1901, during a merger and consolidation the Vulcanized Fibre Co. changed its name to the \"American Vulcanized Fiber Co.\" which was formed for the purpose of consolidating: Kartavert Mfg. Company, Wilmington, Delaware; American Hard Fibre Company, Newark, Delaware; Vulcanized Fibre Company, Wilmington, Delaware. and the Laminar Fibre Company of North Cambridge, Mass.\n\nIn 1922 the name was changed again when it was directly purchased by the National Fibre & Insulation Company of Yorklyn Delaware (who was also the owner of the Keystone Fibre Co.). The president of the National Fibre Company at this time was J. Warren Marshall, who took the same office after consolidating into the new company \"National Vulcanized Fibre Company.\n\nIn 1965 the name was changed again to the NVF Company in order to avoid confusion over the years with a new and changing product line.\n\nThe water power of the Piedmont streams in Northern Delaware led to a proliferation of companies in the vulcanized fibre business. Over the years, these companies reorganized and merged. In 1922 National Vulcanized Fiber Company emerged as the main competitor to Spaulding Fibre, which had begun developing vulcanized products in Rochester, New Hampshire and Tonawanda, New York, nearly a quarter century after the industry began in Delaware. \n\nSome of the companies involved in vulcanized fibre development in the Wilmington region were the Nunsuch Fiber Company, American Hard Fiber Company, American Vulcanized Fibre Company, Continental Fibre Co., Diamond State Fibre Co., and Franklin Fibre Company. In the 1965 Post’s Pulp and Paper Directory, National Vulcanized Fibre Co. was listed as having two mills' producing rag paper for vulcanized fibre. They were at Newark, producing 15 tons a day; and Yorklyn, producing 18 tons a day. This compares with Spaulding Fibre’s Tonawanda plant, then producing 40 tons a day (Post’s directory). The competitors also produced bakelite, but marketed them under different names: Spaulding’s was Spauldite and National’s brand was Phenolite and Iten Industries' Resiten or Itenite.\n\nThe process started with paper made from cotton rags. Before the processing of wood pulp and chemical wood pulps in the mid-19th century, the dominant fibre source for paper making was cotton and linen rags. The cotton rag sheet produced for conversion to vulcanized fibre is made like a sheet suitable for saturating. A paper is made for saturating by omitting any sizing additive, either beater added or surface applied. Today most paper sheets made for writing, printing, and coating have internal (beater added) sizing provided by rosin, alkyl succinic anhydride (ASA), or alkyl ketene dimer (AKD) and surface sizing provided by starch. A sheet made for saturating would have none of those chemical ingredients. The unsized saturating cotton fibre paper prepared for vulcanized fibre would be passed through a vat containing a zinc chloride solution.\n\nZinc chloride is highly soluble in water. The solution used in saturating the paper was 70 Baume in density (1.93 specific gravity) and about . This is roughly a 70% percent zinc chloride solution. Zinc chloride is a mild lewis acid with a solution pH of about 4. Zinc chloride can dissolve cellulose, starch, and silk. The zinc chloride used in making vulcanized fibre swelled and the cellulose. The fibre swelling explains why paper filters cannot be used to filter zinc chloride solutions. It is also the reason why a number of paper plies were used to build up to the desired vulcanized fibre thickness, rather treating a single paperboard thickness. For instance, the practice was to use 8 paper plies of 4 mm thickness each, as opposed to a single paperboard ply of 32 mm.\n\nOnce the paper plies were saturated with the gelatinizing zinc chloride, they were pressed together. The pressing allowed intimate contact of the cellulose fibres, thus promoting bonding between the cellulose chains. Once the bonding was established, the process of leaching out the zinc chloride from the vulcanized fibre could begin. The leaching (removal by diffusion out) of the zinc chloride was accomplished by subjecting the vulcanized fibre to successively less concentrated baths of zinc chloride. The rate at which this could occur was constrained by osmotic forces. If the rate at which the vulcanized fibre was subjected to lower and lower concentrations of zinc chloride solution were too rapid, the osmotic forces could result in ply separations. The final leaching bath concentration was 0.05% zinc chloride.Thicknesses up to 0.093”, can be made on continuous lines that stretch up to 1,000 feet (305m) in length.\n\nFor thickness above 0.093” and up to 0.375”, a discrete laminated sheet (similar in size (l x w) to plywood) was produced by the cutdown process. The cutdown sheets were racked and moved from vat to vat by overhead tracked cranes. Each vat was successively less concentrated until the desired 0.05% was reached. The thicker the material, the longer it took to leach the zinc chloride to 0.05%. For the thickest products, times of 18 months to 2 years were needed. The zinc chloride used in these processes was for the most part not consumed in achieving the desired bonding. Indeed any dilution of the zinc chloride resulting from the leaching was dealt with by using evaporators to bring the zinc chloride solution back to the 70 Baume needed for using it again for saturating. In a sense, zinc chloride can be thought of as a catalyst in the making of the vulcanized fibre.\n\nOnce the vulcanized fibre is leached free of the zinc chloride, it is dried to 5 to 6 percent moisture, and pressed or calendared to flatness. The continuous process-made vulcanized fibre could then be sheeted or wound up into rolls. The density of the finished vulcanized fibre is 2 to 3 times greater than the paper from which it starts. The density increase is the result of 10% machine direction shrinkage, 20% cross machine direction shrinkage, and 30% shrinkage in thickness.\n\nThe final product is a homogeneous nearly 100%-cellulose mass free from any artificial glues, resins, or binders. The finished vulcanized fibre has useful mechanical and electrical properties. It offers high tear and tensile strength, while in the thinner thicknesses allowing flexibility to conform to curves and bends. In thicker thicknesses, it can be moulded to shape with steam and pressure. One application for vulcanized fibre that attests to its physical strength is that it is the preferred material for heavy sanding discs. The electrical properties exhibited by vulcanized fibre are high insulating value, and arc and track resistance with service temperature of up to 110 to 120°C. Vulcanized fibre shows high resistance to penetration by most organic solvents, oils, and petroleum derivatives\n\n"}
