{"id": "57399031", "url": "https://en.wikipedia.org/wiki?curid=57399031", "title": "1KUNS-PF", "text": "1KUNS-PF\n\n1KUNS-PF is the first Kenyan satellite to be launched into space. The 10 centimetre square satellite was developed and assembled by the University of Nairobi. Technical support was provided by Japan's Aerospace Exploration Agency and it was launched from the International Space Station.\n\nThe idea to have a Kenyan built satellite in space began in September 2015 with the planning and design of the space module. Financial support was obtained for the project when the University of Nairobi won a competitive grant from the United Nations Office for Outer Space Affairs (UNOOSA) in 2016. The University of Nairobi was the first institution to benefit from a joint project between the United Nations and JAXA. The satellite was given the acronym 1KUNS-PF which in full is First Kenya University Nano Satellite-Precursor Flight. External technical support was provided by Sapienza University together with two Italian companies. The cost of the programme was about a million dollars. The satellite orbits 400 kilometers above the earth.\n\nOn 2 April 2018, the satellite was carried on the International Space Station onboard a SpaceX CRS-14 which was launched on a Falcon 9 rocket with help from the National Aeronautic and Space Administration. It was deployed from the space station into its orbit from the Kibō module on 11 May 2018. Its signal was successfully received from the Ground Station in Rome by the students of Sapienza University of Rome. Its launch is the third for an African country after GhanaSat-1 and Nigeria EduSat-1 which went into service in 2017. In addition to 1KUNS-PF two other nano satellites, Ubakusat and Proyecto Irazú were also onboard the Falcon-9 rocket to the ISS. All three satellites were deployed into space from the ISS by Japanese astronaut Norishige Kanai.\n\nThe 1KUNS-PF is a nano-satellite with the size of a coffee cup. Its operation will include the mapping of Kenya's land mass, the monitoring of the coastline and helping combat illegal logging activities. The satellite will orbit the earth for about 12 to 18 months before de-orbiting and burning up in the atmosphere.\n"}
{"id": "193939", "url": "https://en.wikipedia.org/wiki?curid=193939", "title": "Alternative energy", "text": "Alternative energy\n\nAlternative energy is any energy source that is an alternative to fossil fuel. These alternatives are intended to address concerns about fossil fuels, such as its high carbon dioxide emissions, an important factor in global warming. Marine energy, hydroelectric, wind, geothermal and solar power are all alternative sources of energy.\n\nThe nature of what constitutes an alternative energy source has changed considerably over time, as have controversies regarding energy use. Because of the variety of energy choices and differing goals of their advocates, defining some energy types as \"alternative\" is considered very controversial.\n\nHistorians of economies have examined the key transitions to alternative energies and regard the transitions as pivotal in bringing about significant economic change. Prior to the shift to an alternative energy, supplies of the dominant energy type became erratic, accompanied by rapid increases in energy prices.\n\nIn the late medieval period, coal was the new alternative fuel to save the society from overuse of the dominant fuel, wood. The deforestation had resulted in shortage of wood, at that time soft coal appeared as a savior. Historian Norman F. Cantor describes how:\n\nEuropeans had lived in the midst of vast forests throughout the earlier medieval centuries. After 1250 they became so skilled at deforestation that by 1500 AD they were running short of wood for heating and cooking... By 1500 Europe was on the edge of a fuel and nutritional disaster, [from] which it was saved in the sixteenth century only by the burning of soft coal and the cultivation of potatoes and maize.\n\nWhale oil was the dominant form of lubrication and fuel for lamps in the early 19th century, but the depletion of the whale stocks by mid century caused whale oil prices to skyrocket setting the stage for the adoption of petroleum which was first commercialized in Pennsylvania in 1859.\n\nIn 1917, Alexander Graham Bell advocated ethanol from corn, wheat and other foods as an alternative to coal and oil, stating that the world was in measurable distance of depleting these fuels. For Bell, the problem requiring an alternative was lack of renewability of orthodox energy sources. Since the 1970s, Brazil has had an ethanol fuel program which has allowed the country to become the world's second largest producer of ethanol (after the United States) and the world's largest exporter. Brazil's ethanol fuel program uses modern equipment and cheap sugar cane as feedstock, and the residual cane-waste (bagasse) is used to process heat and power. There are no longer light vehicles in Brazil running on pure gasoline. By the end of 2008 there were 35,000 filling stations throughout Brazil with at least one ethanol pump.\n\nCellulosic ethanol can be produced from a diverse array of feedstocks, and involves the use of the whole crop. This new approach should increase yields and reduce the carbon footprint because the amount of energy-intensive fertilizers and fungicides will remain the same, for a higher output of usable material. As of 2008, there are nine commercial cellulosic ethanol plants which are either operating, or under construction, in the United States.\n\nSecond-generation biofuels technologies are able to manufacture biofuels from inedible biomass and could hence prevent conversion of food into fuel.\" As of July 2010, there is one commercial second-generation (2G) ethanol plant Inbicon Biomass Refinery, which is operating in Denmark.\n\nIn the 1970s, President Jimmy Carter's administration advocated coal gasification as an alternative to expensive imported oil. The program, including the Synthetic Fuels Corporation was scrapped when petroleum prices plummeted in the 1980s. The carbon footprint and environmental impact of coal gasification are both very high.\n\n\nIce storage air conditioning and thermal storage heaters are methods of shifting consumption to use low cost off-peak electricity. When compared to resistance heating, heat pumps conserve electrical power (or in rare cases mechanical or thermal power) by collecting heat from a cool source such as a body of water, the ground or the air.\n\nThermal storage technologies allow heat or cold to be stored for periods of time ranging from diurnal to interseasonal, and can involve storage of sensible energy (i.e. by changing the temperature of a medium) or latent energy (e.g. through phase changes of a medium (i.e. changes from solid to liquid or vice versa), such as between water and slush or ice). Energy sources can be natural (via solar-thermal collectors, or dry cooling towers used to collect winter's cold), waste energy (such as from HVAC equipment, industrial processes or power plants), or surplus energy (such as seasonally from hydropower projects or intermittently from wind farms). The Drake Landing Solar Community (Alberta, Canada) is illustrative. Borehole thermal energy storage allows the community to get 97% of its year-round heat from solar collectors on the garage roofs. The storages can be insulated tanks, borehole clusters in substrates ranging from gravel to bedrock, deep aquifers, or shallow pits that are lined and insulated. Some applications require inclusion of a heat pump.\n\nRenewable energy is generated from natural resources—such as sunlight, wind, rain, tides and geothermal heat—which are renewable (naturally replenished). When comparing the processes for producing energy, there remain several fundamental differences between renewable energy and fossil fuels. The process of producing oil, coal, or natural gas fuel is a difficult and demanding process that requires a great deal of complex equipment, physical and chemical processes. On the other hand, alternative energy can be widely produced with basic equipment and natural processes. Wood, the most renewable and available alternative fuel, emits the same amount of carbon when burned as would be emitted if it degraded naturally. Nuclear power is an alternative to fossil fuels that is non-renewable, like fossil fuels, nuclear ones are a finite resource.\n\nA renewable energy source such as biomass is sometimes regarded as a good alternative to providing heat and electricity with fossil fuels. Biofuels are not inherently ecologically friendly for this purpose, while burning biomass is carbon-neutral, air pollution is still produced. For example, the Netherlands, once leader in use of palm oil as a biofuel, has suspended all subsidies for palm oil due to the scientific evidence that their use \"may sometimes create more environmental harm than fossil fuels\". The Netherlands government and environmental groups are trying to trace the origins of imported palm oil, to certify which operations produce the oil in a responsible manner. Regarding biofuels from foodstuffs, the realization that converting the entire grain harvest of the US would only produce 16% of its auto fuel needs, and the decimation of Brazil's absorbing tropical rain forests to make way for biofuel production has made it clear that placing energy markets in competition with food markets results in higher food prices and insignificant or negative impact on energy issues such as global warming or dependence on foreign energy. Recently, alternatives to such undesirable sustainable fuels are being sought, such as commercially viable sources of cellulosic ethanol.\n\nCarbon-neutral fuels are synthetic fuels (including methane, gasoline, diesel fuel, jet fuel or ammonia) produced by hydrogenating waste carbon dioxide recycled from power plant flue-gas emissions, recovered from automotive exhaust gas, or derived from carbonic acid in seawater. Commercial fuel synthesis companies suggest they can produce synthetic fuels for less than petroleum fuels when oil costs more than $55 per barrel. Renewable methanol (RM) is a fuel produced from hydrogen and carbon dioxide by catalytic hydrogenation where the hydrogen has been obtained from water electrolysis. It can be blended into transportation fuel or processed as a chemical feedstock.\n\nThe George Olah carbon dioxide recycling plant operated by Carbon Recycling International in Grindavík, Iceland has been producing 2 million liters of methanol transportation fuel per year from flue exhaust of the Svartsengi Power Station since 2011. It has the capacity to produce 5 million liters per year. A 250 kilowatt methane synthesis plant was constructed by the Center for Solar Energy and Hydrogen Research (ZSW) at Baden-Württemberg and the Fraunhofer Society in Germany and began operating in 2010. It is being upgraded to 10 megawatts, scheduled for completion in autumn, 2012. Audi has constructed a carbon-neutral liquefied natural gas (LNG) plant in Werlte, Germany. The plant is intended to produce transportation fuel to offset LNG used in their A3 Sportback g-tron automobiles, and can keep 2,800 metric tons of CO out of the environment per year at its initial capacity. Other commercial developments are taking place in Columbia, South Carolina, Camarillo, California, and Darlington, England.\n\nSuch fuels are considered carbon-neutral because they do not result in a net increase in atmospheric greenhouse gases. To the extent that synthetic fuels displace fossil fuels, or if they are produced from waste carbon or seawater carbonic acid, and their combustion is subject to carbon capture at the flue or exhaust pipe, they result in negative carbon dioxide emission and net carbon dioxide removal from the atmosphere, and thus constitute a form of greenhouse gas remediation.\n\nSuch renewable fuels alleviate the costs and dependency issues of imported fossil fuels without requiring either electrification of the vehicle fleet or conversion to hydrogen or other fuels, enabling continued compatible and affordable vehicles. Carbon-neutral fuels offer relatively low cost energy storage, alleviating the problems of wind and solar intermittency, and they enable distribution of wind, water, and solar power through existing natural gas pipelines.\n\nNighttime wind power is considered the most economical form of electrical power with which to synthesize fuel, because the load curve for electricity peaks sharply during the day, but wind tends to blow slightly more at night than during the day, so, the price of nighttime wind power is often much less expensive than any alternative. Germany has built a 250 kilowatt synthetic methane plant which they are scaling up to 10 megawatts.\n\nAlgae fuel is a biofuel which is derived from algae. During photosynthesis, algae and other photosynthetic organisms capture carbon dioxide and sunlight and convert it into oxygen and biomass. This is usually done by placing the algae between two panes of glass. The algae creates three forms of energy fuel: heat (from its growth cycle), biofuel (the natural \"oil\" derived from the algae), and biomass (from the algae itself, as it is harvested upon maturity).\n\nThe heat can be used to power building systems (such as heat process water) or to produce energy. Biofuel is oil extracted from the algae upon maturity, and used to create energy similar to the use of biodiesel. The biomass is the matter left over after extracting the oil and water, and can be harvested to produce combustible methane for energy production, similar to the warmth felt in a compost pile or the methane collected from biodegradable materials in a landfill. Additionally, the benefits of algae biofuel are that it can be produced industrially, as well as vertically (i.e. as a building facade), thereby obviating the use of arable land and food crops (such as soy, palm, and canola).\n\nBiomass briquettes are being developed in the developing world as an alternative to charcoal. The technique involves the conversion of almost any plant matter into compressed briquettes that typically have about 70% the calorific value of charcoal. There are relatively few examples of large scale briquette production. One exception is in North Kivu, in eastern Democratic Republic of Congo, where forest clearance for charcoal production is considered to be the biggest threat to Mountain Gorilla habitat. The staff of Virunga National Park have successfully trained and equipped over 3500 people to produce biomass briquettes, thereby replacing charcoal produced illegally inside the national park, and creating significant employment for people living in extreme poverty in conflict affected areas.\nBiogas digestion harnesses the methane gas that is released when organic waste breaks down in an anaerobic environment. This gas can be retrieved from landfill sites or sewage systems. The gas can be used as a fuel for heat or, more commonly, electricity generation.\n\nThe methane gas that is collected and refined can be used as an energy source for various products.\n\nHydrogen gas is a completely clean burning fuel; its only by-product is water. It also contains relatively high amount of energy compared with other fuels due to its chemical structure.\n\n2H + O → 2HO + High Energy\n\nHigh Energy + 2HO → 2H + O\n\nThis requires a high-energy input, making commercial hydrogen very inefficient. Use of a biological vector as a means to split water, and therefore produce hydrogen gas, would allow for the only energy input to be solar radiation. Biological vectors can include bacteria or more commonly algae. This process is known as biological hydrogen production. It requires the use of single celled organisms to create hydrogen gas through fermentation. Without the presence of oxygen, also known as an anaerobic environment, regular cellular respiration cannot take place and a process known as fermentation takes over. A major by-product of this process is hydrogen gas. If this could be implemented on a large scale, then sunlight, nutrients and water could create hydrogen gas to be used as a dense source of energy. Large-scale production has proven difficult. Not until 1999, was it even possible to induce these anaerobic conditions by sulfur deprivation. Since the fermentation process is an evolutionary back up, turned on during stress, the cells would die after a few days. In 2000, a two-stage process was developed to take the cells in and out of anaerobic conditions and therefore keep them alive. For the last ten years, finding a way to do this on a large-scale has been the main goal of research. Careful work is being done to ensure an efficient process before large-scale production, however once a mechanism is developed, this type of production could solve our energy needs.\n\nHydroelectricity provided 75% of the worlds renewable electricity in 2013. Much of the electricity used today is a result of the heyday of conventional hydroelectric development between 1960 and 1980, which has virtually ceased in Europe and North America due to environmental concerns. Globally there is a trend towards more hydroelectricity. From 2004 to 2014 the installed capacity rose from 715 to 1,055 GW. A popular alternative to the large dams of the past is run-of-the-river where there is no water stored behind a dam and generation usually varies with seasonal rainfall. Using run-of-the-river in wet seasons and solar in dry seasons can balance seasonal variations for both. Another move away from large dams is small hydro, these tend to be situated high up on tributaries, rather than on main rivers in valley bottoms.\n\nOffshore wind farms are similar to land-based wind farms, but are located on the ocean. Offshore wind farms can be placed in water up to deep, whereas floating wind turbines can float in water up to deep.\nThe advantage of having a floating wind farm is to be able to harness the winds from the open ocean. Without any obstructions such as hills, trees and buildings, winds from the open ocean can reach up to speeds twice as fast as coastal areas.\n\nSignificant generation of offshore wind energy already contributes to electricity needs in Europe and Asia and now the first offshore wind farms are under development in U.S. waters. While the offshore wind industry has grown dramatically over the last several decades, especially in Europe, there is still uncertainty associated with how the construction and operation of these wind farms affect marine animals and the marine environment.\n\nTraditional offshore wind turbines are attached to the seabed in shallower waters within the nearshore marine environment. As offshore wind technologies become more advanced, floating structures have begun to be used in deeper waters where more wind resources exist.\n\nMarine and Hydrokinetic (MHK) or marine energy development includes projects using the following devices:\n\nIn the year 2015 ten new reactors came online and 67 more were under construction including the first eight new Generation III+ AP1000 reactors in the US and China and the first four new Generation III EPR reactors in Finland, France and China. Reactors are also under construction in Belarus, Brazil, India, Iran, Japan, Pakistan, Russia, Slovakia, South Korea, Turkey, Ukraine and United Arab Emirates.\n\nThorium is a fissionable material for possible future use in a thorium-based reactor. Proponents of thorium reactors claims several potential advantages over a uranium fuel cycle, such as thorium's greater abundance, better resistance to nuclear weapons proliferation, and reduced plutonium and actinide production. Thorium reactors can be modified to produce Uranium-233, which can then be processed into highly enriched uranium, which has been tested in low yield weapons, and is unproven on a commercial scale.\n\nAs an emerging economic sector, there are limited stock market investment opportunities in alternative energy available to the general public. The public can buy shares of alternative energy companies from various stock markets, with wildly volatile returns. The recent IPO of SolarCity demonstrates the nascent nature of this sector- within a few weeks, it already had achieved the second highest market cap within the alternative energy sector.\n\nInvestors can also choose to invest in ETFs (exchange-traded funds) that track an alternative energy index, such as the WilderHill New Energy Index. Additionally, there are a number of mutual funds, such as Calvert's Global Alternative Energy Mutual Fund that are a bit more proactive in choosing the selected investments.\n\nRecently, Mosaic Inc. launched an online platform allowing residents of California and New York to invest directly in solar. Investing in solar projects had previously been limited to accredited investors, or a small number of willing banks.\n\nOver the last three years publicly traded alternative energy companies have been very volatile, with some 2007 returns in excess of 100%, some 2008 returns down 90% or more, and peak-to-trough returns in 2009 again over 100%. In general there are three sub-segments of \"alternative\" energy investment: solar energy, wind energy and hybrid electric vehicles. Alternative energy sources which are renewable and have lower carbon emissions than fossil fuels are hydropower, wind energy, solar energy, geothermal energy, and bio fuels. Each of these four segments involve very different technologies and investment concerns.\n\nFor example, photovoltaic solar energy is based on semiconductor processing and accordingly, benefits from steep cost reductions similar to those realized in the microprocessor industry (i.e., driven by larger scale, higher module efficiency, and improving processing technologies). PV solar energy is perhaps the only energy technology whose electricity generation cost could be reduced by half or more over the next five years. Better and more efficient manufacturing process and new technology such as advanced thin film solar cell is a good example of that helps to reduce industry cost.\n\nThe economics of solar PV electricity are highly dependent on silicon pricing and even companies whose technologies are based on other materials (e.g., First Solar) are impacted by the balance of supply and demand in the silicon market. In addition, because some companies sell completed solar cells on the open market (e.g., Q-Cells), this creates a low barrier to entry for companies that want to manufacture solar modules, which in turn can create an irrational pricing environment.\n\nIn contrast, because wind power has been harnessed for over 100 years, its underlying technology is relatively stable. Its economics are largely determined by siting (e.g., how hard the wind blows and the grid investment requirements) and the prices of steel (the largest component of a wind turbine) and select composites (used for the blades). Because current wind turbines are often in excess of 100 meters high, logistics and a global manufacturing platform are major sources of competitive advantage. These issues and others were explored in a research report by Sanford Bernstein.\n\nDue to steadily rising gas prices in 2008 with the US national average price per gallon of regular unleaded gas rising above $4.00 at one point, there has been a steady movement towards developing higher fuel efficiency and more alternative fuel vehicles for consumers. In response, many smaller companies have rapidly increased research and development into radically different ways of powering consumer vehicles. Hybrid and battery electric vehicles are commercially available and are gaining wider industry and consumer acceptance worldwide.\n\nFor example, Nissan USA introduced the world's first mass-production electric vehicle, the Nissan Leaf. A plug-in hybrid car, the Chevrolet Volt also has been produced, using an electric motor to drive the wheels, and a small four-cylinder engine to generate additional electricity.\n\nBefore alternative energy becomes mainstream there are a few crucial obstacles that it must overcome. First there must be increased understanding of how alternative energies are beneficial; secondly the availability components for these systems must increase; and lastly the pay-back period must be decreased.\n\nFor example, electric vehicles (EV) and plug-in hybrid electric vehicles (PHEV) are on the rise. The continue adoption of these vehicles depend on investment in public charging infrastructure, as well as implementing much more alternative energy for future transportation.\n\nThere are numerous organizations within the academic, federal, and commercial sectors conducting large scale advanced research in the field of alternative energy. This research spans several areas of focus across the alternative energy spectrum. Most of the research is targeted at improving efficiency and increasing overall energy yields.\n\nIn the US, multiple federally supported research organizations have focused on alternative energy in recent years. Two of the most prominent of these labs are Sandia National Laboratories and the National Renewable Energy Laboratory (NREL), both of which are funded by the United States Department of Energy and supported by various corporate partners. Sandia has a total budget of $2.4 billion while NREL has a budget of $375 million.\n\nWith the increasing consumption levels of energy, it is projected that the levels would increase by 21% in 2030. The cost of the renewables was relatively cheaper at $2.5m/MW as compared to the non-renewables & 2.7m/MW. Evidently, the use of renewable energy is a cost effective method of obtaining energy. Additionally, their use also dispenses with the trade-off that has existed between environmental conservation and economic growth.\n\nMechanical energy associated with human activities such as blood circulation, respiration, walking, typing and running, is ubiquitous but usually wasted. It has attracted tremendous attention from researchers around the globe to find methods to scavenge such mechanical energies. The best solution currently is to use piezoelectric materials, which can generate flow of electrons when deformed. Various devices using piezoelectric materials have been built to scavenge mechanical energy. Considering that the piezoelectric constant of the material plays a critical role in the overall performance of a piezoelectric device, one critical research direction to improve device efficiency is to find new material of large piezoelectric response. Lead Magnesium Niobate-Lead Titanate (PMN-PT) is a next-generation piezoelectric material with super high piezoelectric constant when ideal composition and orientation are obtained. In 2012, PMN-PT Nanowires with a very high piezoelectric constant were fabricated by a hydro-thermal approach and then assembled into an energy-harvesting device. The record-high piezoelectric constant was further improved by the fabrication of a single-crystal PMN-PT nanobelt, which was then used as the essential building block for a piezoelectric nanogenerator.\n\nSolar energy can be used for heating, cooling or electrical power generation using the sun.\n\nSolar heat has long been employed in passively and actively heated buildings, as well as district heating systems. Examples of the latter are the Drake Landing Solar Community is Alberta, Canada, and numerous district systems in Denmark and Germany. In Europe, there are two programs for the application of solar heat: the Solar District Heating (SDH) and the International Energy Agency's Solar Heating and Cooling (SHC) program.\n\nThe obstacles preventing the large-scale implementation of solar powered energy generation is the inefficiency of current solar technology and the cost. Currently, photovoltaic (PV) panels only have the ability to convert around 16% of the sunlight that hits them into electricity.\n\nBoth Sandia National Laboratories and the National Renewable Energy Laboratory (NREL), have heavily funded solar research programs. The NREL solar program has a budget of around $75 million and develops research projects in the areas of photovoltaic (PV) technology, solar thermal energy, and solar radiation. The budget for Sandia's solar division is unknown, however it accounts for a significant percentage of the laboratory's $2.4 billion budget.\n\nSeveral academic programs have focused on solar research in recent years. The Solar Energy Research Center (SERC) at University of North Carolina (UNC) has the sole purpose of developing a cost-effective solar technology. In 2008, researchers at Massachusetts Institute of Technology (MIT) developed a method to store solar energy by using it to produce hydrogen fuel from water. Such research is targeted at addressing the obstacle that solar development faces of storing energy for use during nighttime hours when the sun is not shining. The Zhangebei National Wind and Solar Energy Storage and Transmission Demonstration Project northwest of Beijing, uses batteries to store 71 MWh, integrating wind and solar energy on the grid with frequency and voltage regulation.\n\nIn February 2012, North Carolina-based Semprius Inc., a solar development company backed by German corporation Siemens, announced that they had developed the world's most efficient solar panel. The company claims that the prototype converts 33.9% of the sunlight that hits it to electricity, more than double the previous high-end conversion rate.\n\nWind energy research dates back several decades to the 1970s when NASA developed an analytical model to predict wind turbine power generation during high winds. Today, both Sandia National Laboratories and National Renewable Energy Laboratory have programs dedicated to wind research. Sandia's laboratory focuses on the advancement of materials, aerodynamics, and sensors. The NREL wind projects are centered on improving wind plant power production, reducing their capital costs, and making wind energy more cost effective overall.\n\nThe Field Laboratory for Optimized Wind Energy (FLOWE) at Caltech was established to research alternative approaches to wind energy farming technology practices that have the potential to reduce the cost, size, and environmental impact of wind energy production.\n\nRenewable energies such as wind, solar, biomass and geothermal combined, supplied 1.3% of global final energy consumption in 2013.\n\nBiomass can be regarded as \"biological material\" derived from living, or recently living organisms. It most often refers to plants or plant-derived materials which are specifically called lignocellulosic biomass. As an energy source, biomass can either be used directly via combustion to produce heat, or indirectly after converting it to various forms of biofuel. Conversion of biomass to biofuel can be achieved by different methods which are broadly classified into: \"thermal\", \"chemical\", and \"biochemical\" methods. Wood remains the largest biomass energy source today; examples include forest residues (such as dead trees, branches and tree stumps), yard clippings, wood chips and even municipal solid waste. In the second sense, biomass includes plant or animal matter that can be converted into fibers or other industrial chemicals, including biofuels. Industrial biomass can be grown from numerous types of plants, including miscanthus, switchgrass, hemp, corn, poplar, willow, sorghum, sugarcane, bamboo, and a variety of tree species, ranging from eucalyptus to oil palm (palm oil).\n\nBiomass, biogas and biofuels are burned to produce heat/power and in doing so harm the environment. Pollutants such as sulphurous oxides (SO), nitrous oxides (NO), and particulate matter (PM) are produced from this combustion. The World Health Organisation estimates that 7 million premature deaths are caused each year by air pollution, and biomass combustion is a major contributor of it. The use of biomas is carbon neutral over time, but is otherwise similar to burning fossil fuels.\n\nAs the primary source of biofuels in North America, many organizations are conducting research in the area of ethanol production. On the Federal level, the USDA conducts a large amount of research regarding ethanol production in the United States. Much of this research is targeted toward the effect of ethanol production on domestic food markets.\n\nThe National Renewable Energy Laboratory has conducted various ethanol research projects, mainly in the area of cellulosic ethanol. Cellulosic ethanol has many benefits over traditional corn based-ethanol. It does not take away or directly conflict with the food supply because it is produced from wood, grasses, or non-edible parts of plants. Moreover, some studies have shown cellulosic ethanol to be more cost effective and economically sustainable than corn-based ethanol. Sandia National Laboratories conducts in-house cellulosic ethanol research and is also a member of the Joint BioEnergy Institute (JBEI), a research institute founded by the United States Department of Energy with the goal of developing cellulosic biofuels.\n\nFrom 1978 to 1996, the National Renewable Energy Laboratory experimented with using algae as a biofuels source in the \"Aquatic Species Program.\" A self-published article by Michael Briggs, at the University of New Hampshire Biofuels Group, offers estimates for the realistic replacement of all motor vehicle fuel with biofuels by utilizing algae that have a natural oil content greater than 50%, which Briggs suggests can be grown on algae ponds at wastewater treatment plants. This oil-rich algae can then be extracted from the system and processed into biofuels, with the dried remainder further reprocessed to create ethanol.\n\nThe production of algae to harvest oil for biofuels has not yet been undertaken on a commercial scale, but feasibility studies have been conducted to arrive at the above yield estimate. In addition to its projected high yield, algaculture— unlike food crop-based biofuels — does not entail a decrease in food production, since it requires neither farmland nor fresh water. Many companies are pursuing algae bio-reactors for various purposes, including scaling up biofuels production to commercial levels.\n\nSeveral groups in various sectors are conducting research on Jatropha curcas, a poisonous shrub-like tree that produces seeds considered by many to be a viable source of biofuels feedstock oil. Much of this research focuses on improving the overall per acre oil yield of Jatropha through advancements in genetics, soil science, and horticultural practices. SG Biofuels, a San Diego-based Jatropha developer, has used molecular breeding and biotechnology to produce elite hybrid seeds of Jatropha that show significant yield improvements over first generation varieties. The Center for Sustainable Energy Farming (CfSEF) is a Los Angeles-based non-profit research organization dedicated to Jatropha research in the areas of plant science, agronomy, and horticulture. Successful exploration of these disciplines is projected to increase Jatropha farm production yields by 200-300% in the next ten years.\n\nGeothermal energy is produced by tapping into the heat within the earths crust. It is considered sustainable because that thermal energy is constantly replenished. However, the science of geothermal energy generation is still young and developing economic viability. Several entities, such as the National Renewable Energy Laboratory and Sandia National Laboratories are conducting research toward the goal of establishing a proven science around geothermal energy. The International Centre for Geothermal Research (IGC), a German geosciences research organization, is largely focused on geothermal energy development research.\n\nOver $1 billion has been spent on the research and development of hydrogen fuel in the United States. Both the National Renewable Energy Laboratory and Sandia National Laboratories have departments dedicated to hydrogen research. Much of this work centers on hydrogen storage and fuel cell technologies\n\nThe generation of alternative energy on the scale needed to replace fossil energy, in an effort to reverse global climate change, is likely to have significant negative environmental impacts. For example, biomass energy generation would have to increase 7-fold to supply current primary energy demand, and up to 40-fold by 2100 given economic and energy growth projections. Humans already appropriate 30 to 40% of all photosynthetically fixed carbon worldwide, indicating that expansion of additional biomass harvesting is likely to stress ecosystems, in some cases precipitating collapse and extinction of animal species that have been deprived of vital food sources. The total amount of energy capture by vegetation in the United States each year is around 58 quads (61.5 EJ), about half of which is already harvested as agricultural crops and forest products. The remaining biomass is needed to maintain ecosystem functions and diversity. Since annual energy use in the United States is ca. 100 quads, biomass energy could supply only a very small fraction. To supply the current worldwide energy demand solely with biomass would require more than 10% of the Earth's land surface, which is comparable to the area use for all of world agriculture (i.e., ca. 1500 million hectares), indicating that further expansion of biomass energy generation will be difficult without precipitating an ethical conflict, given current world hunger statistics, over growing plants for biofuel versus food.\n\nGiven environmental concerns (e.g., fish migration, destruction of sensitive aquatic ecosystems, etc.) about building new dams to capture hydroelectric energy, further expansion of conventional hydropower in the United States is unlikely. Windpower, if deployed on the large scale necessary to substitute fossil energy, is likely to face public resistance. If 100% of U.S. energy demand were to be supplied by wind power, about 80 million hectares (i.e., more than 40% of all available farmland in the United States) would have to be covered with wind turbines (50m hub height and 250 to 500 m apart). It is therefore not surprising that the major environmental impact of wind power is related to land use and less to wildlife (birds, bats, etc.) mortality. Unless only a relatively small fraction of electricity is generated by wind turbines in remote locations, it is unlikely that the public will tolerate large windfarms given concerns about blade noise and aesthetics.\n\nBiofuels are different from fossil fuels in regard to net greenhouse gases but are similar to fossil fuels in that biofuels contribute to air pollution. Burning produces airborne carbon particulates, carbon monoxide and nitrous oxides.\n\nRenewable alternative forms of energy have faced opposition from multiple groups, including conservatives and liberals. Around twelve states have passed proposals written to inhibit the alternative energy movement. Kansas lawmakers struck down a bill to phase out renewable energy mandates but face the possibility of the bill reappearing.\n\nThe opposition cites the potentially high cost of branching out to these alternatives in order to support the continuation and reliance on fossil fuels. Ohio's mandate to phase in alternative energy faces opposition who believe higher electricity prices will result, while supporters fear the loss of economic development and jobs that alternative energy could bring.\n\nWith nuclear meltdowns in Chernobyl and Fukushima, nuclear power presents a constant danger and is more unlikely to be a popular alternative source. The costs of maintaining nuclear facilities, the potential risk of meltdowns, and the cost of cleaning up meltdowns are cited as reasons behind the movement away from the use of nuclear energy. In some countries nuclear power plants cannot compete with fossil fuels currently due to the latter's lower price and availability. Nuclear power plants also face competition from the increasing renewable energy subsidies.\n\nSolar panels are an icon of the 'green power' movement, however the process of manufacturing the quartz based panels can be detrimental to the environment. Raw quartz (silica) is used to create solar cells must be mined using harsh chemicals that harm the surrounding environment, as well as those working in the mines. Silicosis is a form of lung disease that is caused by the inhalation of crystalline silica dust resulting in nodule lesions in the lungs. The silica must be cultivated into metallurgical-grade silicon, the process requiring a massive amount of energy as the quartz is placed into electric arc furnaces. The metallurgical grade silica must be processed into polysilicon. This process also produces tetrachloride, a toxic substance that, if not disposed of correctly, can be harmful to the surrounding environment. Hydrochloric acid is formed when tetrachloride interacts with water, lowering water and soil pH. Incidents of tetrachloride spills are common in China, as the production of solar panels has shifted from Europe and the United States to Asian countries within the early 2000s. Because of such, the villagers of Gaolong are unable to leave their homes due to air and soil becoming toxic. This was due to Luoyang Zhongui High-Technology Co. repeatedly dumped tetrachloride in a nearby field for almost a year.\n\n\n"}
{"id": "54299994", "url": "https://en.wikipedia.org/wiki?curid=54299994", "title": "Ampd Energy", "text": "Ampd Energy\n\nAmpd Energy is a company in the energy industry, which designs and manufactures energy storage systems (ESS). It is a member of the Incu-Tech Programme, a Hong Kong Government and HK Science and Technology Park initiative. Their mission is to provide clean, affordable and stable electricity to people and enterprises that lack access to a reliable electric supply.\n\nAmpd was founded in December 2014. Its founders, Brandon Ng and Luca Valente created an electric motorcycle company during a blackout in the city, they reasoned that the batteries of their motorcycles could be redesigned to power buildings.\n\nIn February 2015, Ampd Energy was accepted into the Incu-Tech program. As of June 2017, the company has raised a total of $3.7 million in seed investment.\n\nIn December 2016, the company launched an energy storage system called Ampd Silo, which aims to replace lead acid battery, Uninterruptable Power Supply systems (UPSs) and diesel generators primarily in countries affected by frequent blackouts.\n\nUnlike many related companies such as Tesla and Sonnen, The Ampd Silo is primarily aimed at on-grid energy storage.\n\nThe Ampd Silo uses 1,792 rechargeable lithium-ion batteries cells, with each Ampd Silo offering a storage capacity of 16.8 kWh. Ampd Energy has patented novel methods of assembling the cells.\n\nIn early 2017, Ampd Silo received the \"CES2017 Innovation Award Honoree\" in the \"Tech For a Better World\" category. The company was also shortlisted as one of the finalists in Engadgets best CES 2017 in the start-up category.\n\nIn May 2017, co-founder and CEO, Brandon Ng, was named as a \"Forbes Asia 30 Under 30 honouree\" in the \"Industry, Manufacturing & Energy\" category.\n\n\n\n\n\n\n"}
{"id": "46504493", "url": "https://en.wikipedia.org/wiki?curid=46504493", "title": "Australian Ecology Research Award (AERA)", "text": "Australian Ecology Research Award (AERA)\n\nThe Australian Ecology Research Award (AERA) is an award presented by the Ecological Society of Australia for a specific body of recent ecological work by a mid-career researcher. Initiated in 2008, the AERA was inspired, in part, by the Robert H. MacArthur Award of the Ecological Society of America. The AERA is not restricted to any particular sector, and aims to recognize outstanding ecological research; nominations of researchers from academia, and the public and private sector agencies are invited annually.\n\nThe successful nominee is presented the AERA at the Annual Conference of the Ecological Society of Australia.\n\n2016: Jane Elith\n2015: Saul Cunningham\n2014: Melodie McGeoch \n2013: David Keith\n2012: Chris Johnson\n2011: Lesley Hughes\n2010: Corey Bradshaw\n2009: David Lindenmayer\n2008: Bob Pressey\n"}
{"id": "25531187", "url": "https://en.wikipedia.org/wiki?curid=25531187", "title": "Banni Grasslands Reserve", "text": "Banni Grasslands Reserve\n\nBanni Grasslands Reserve or Banni grasslands form a belt of arid grassland ecosystem on the outer southern edge of the desert of the marshy salt flats of Rann of Kutch in Kutch District, Gujarat State, India. They are known for rich wildlife and biodiversity and are spread across an area of 3,847 square kilometres. They are currently legally protected under the status as a protected or reserve forest in India. Though declared a protected forest more than half a century ago Gujarat state's forest department has recently proposed a special plan to restore and manage this ecosystem in the most efficient way. Wildlife Institute of India (WII) has identified this grassland reserve as one of the last remaining habitats of the cheetah in India and a possible reintroduction site for the species.\n\nThe word ‘Banni’ comes from Hindi word ‘banai’, meaning made. The land here was formed from the sediments that were deposited by the Indus and other rivers over thousands of years. Old villagers from this region say that before the 1819 Rann of Kutch earthquake, the river Indus flowed right through banni and the local farmers reaped a rich harvest of crops like red rice and sindhi chookha etc., red rice was the staple diet of the people of the region and it was even recommended by medical practitioners as a 'light diet' for ailing people. However, since the earthquake of 1819 the river Indus changed its course and now flows through Sindh in neighbouring country of Pakistan effectively turning this entire region arid.\n\nBanni grassland is peculiar to the Rann of Kutch, it has some forty Sindhi speaking Maldhari (cattle breeders) hamlets, home to the Halaypotra, Hingora, Hingorja, Jat and Mutwa tribes\n. It was first declared a \"protected forest\" in May 1955, using the nomenclature of the Indian Forest Act, 1927. Since then, the actual transfer of the land from the Revenue department to the Forest department has not been completed.\n\nVegetation in Banni is sparse and highly dependent on year-to-year variations in rainfall.\nBanni is dominated by low-growing forbs and graminoids, many of which are halophiles (salt tolerant), as well scatted tree cover and scrub. The tree cover is primarily composed of \"Salvadora\" spp. and the invasive \"Prosopis juliflora\". Dominant species include \"Cressa cretica\", \"Cyperus\" spp., grasses in the genera \"Sporobolus\", \"Dichanthium\", and \"Aristida\".\n\nThe grasslands are home to mammals such as the nilgai (\"Boselaphus tragocamelus\"), chinkara (\"Gazella bennettii\"), blackbuck (\"Antilope cervicapra\"), wild boar (\"Sus scrofa\"), golden jackal (\"Canis aureus\"), Indian hare (\"Lepus nigricollis\"), Indian wolf (\"Canis lupus pallipes\"), caracal (\"Caracal caracal\"), Asiatic wildcat (\"Felis silvestris ornata\") and desert fox (\"Vulpes vulpes pusilla\") etc. among others. The last Indian wild ass (\"Equus hemionus khur\") population, which had become confined to nearby Little Rann of Kutch, has been increasing in numbers since 1976 and has recently started spilling over into adjoining areas including Greater Rann of Kutch, Banni and the adjoining villages of the neighbouring Indian state of Rajasthan.\n\nBanni grasslands also have a rich diversity of avifauna, herpatofauna and invertebrates. During good rainfall years the seasonal water bodies of Banni form important staging grounds for thousands of flamingos, migratory cranes and also support large numbers of over 150 species of migratory and resident birds.\n\n\"Banni Grasslands Reserve\" and \"Narayan Sarovar Sanctuary\", both in Kutch, have been classified by Wildlife Institute of India (WII) as the last remaining habitats of the cheetah (\"Acinonyx jubatus\") in India and are proposed as some of the possible sites for the reintroduction of the species in India. Asiatic cheetah (\"Acinonyx jubatus venaticus\") that used to occur here are now locally extinct in India and elsewhere, except a very small critically endangered and fragmented population of last few, estimated to be below 100, thought to be surviving only in the central desert of Iran. Thus cheetah experts from around the world have advised India to import and introduce the cheetah from Africa as genetically it is identical to the ones found is Asia, as latest genetic studies have revealed that the Asian population had separated from the African relatively recently only 5000 years ago which is not enough for a subspecies level differentiation.\nHowever, the plan has been on hold in 2012, after the discovery that Asiatic cheetahs are genetically different and have been diverged from the Southern African population (\"Acinonyx jubatus jubatus\") between 32,000 and 67,000 years ago.\n\nMonsoon rains each year form several marshy wetlands which dot the Banni grasslands and the areas adjacent to it, all being ephemeral or seasonal in nature. Some better-known examples are: Vekario-Dhand, Kheerjog, Vinzar varo Thathh, Hodko Thathh, Servo-Dhand, Bhagadio Thathh, Kar near Kirro, Kunjevari Thathh, Hanjtal, and Chari-Dhand – the biggest in size among all of them. In the local Kutchhi-Sindhi language there are four terms used for wetlands in Banni and across the border in Pakistan, they are Kar (smallest), Chhachh (bigger than Kar), Thathh (bigger than Chhach) and Dhand (the biggest of the wetlands). The area of each of these seasonal freshwater wetlands during any given year depends upon the amount of rainfall received during that year.\n\nThese wetlands are located on the flyway of palaearctic migratory birds and play a very important role as foraging, roosting, resting and staging grounds for millions of waders, waterfowl, cranes and other feathered migrants that visit the area from August and staying until March every year. Thousands of flamingos in their breeding plumage, common cranes (\"Grus grus\") and other wetland birds including hundreds of painted storks (\"Mycteria leucocephala\") and Eurasian spoonbills (\"Platalea leucorodia\") among others can be spotted in the larger of these seasonal wetlands of the Banni.\n\nOne of the largest of these seasonal wetlands in the Banni is Chari-Dhand Wetland Conservation Reserve which has been accorded special protected status as a protected or reserve forest to conserve its wildlife and visiting migratory birds.\n\nThe Banni grasslands are under pressure due to man-made factors which are overgrazing, invasion by \"Prosopis juliflora\", an exotic thorny tree, and natural factors which are recurring droughts and salinity ingress.\n\nThe main sources of income of Maldharis pastoralist communities such as the Halaypotra, Hingora, Hingorja, Jat, Ker, and Mutwa who live here with their livestock are sale of high quality ghee, milk, wool, animals and handicrafts. Due to several reasons, the traditional occupational pattern is changing from livestock breeding to livestock grazing.\n\nThe climate is arid with an average rainfall of only 315 mm per year between June and September. The number of days during which rain falls in a year usually does not exceed 4 or 5. Banni has almost no rivers or natural streams however, about 100 rivers and rivulets flowing northwards from the Kutch mainland drain into the grasslands of Banni along its southern boundary. This area near the boundary gets flooded during the rainy season mainly by the water brought by these rivers and local rainfall. It is this annual flooding and the old silt deposits that formed Banni, which were deposited here when the Indus River used to flow through the region until it changed its course due to the 1819 earthquake. This gave rise to what is often called Asia's finest natural grasslands.\n\nThis entire region of Kutch, Gujarat is however drought prone due to erratic monsoons with cattle breeding pastoralist tribes (Maldharis) living here having to move out with their livestock as the region turns into a desert in bad rainfall years.\n\n\"Prosopis juliflora\", a non-native, thorny, shrubby species of mesquite locally known as ganda bawal, was planted in the area to help the Gujarat State forest department fight salinity ingress and barrenness in the Banni region of Kutch. A ban was placed on the tree's harvest in the 1980s, at which time it covered less than 10 per cent of the Banni grasslands. However, it quickly became an invasive species, occupying over 40 per cent of the land by the late 1990s. This worried the forest department, as \"P. juliflora\" is known for harming biodiversity and it was clear that it was destroying the grassland ecosystem, so the state government lifted the ban in early 2004, liberalising \"Prosopis\" cutting under Section 32 of the Indian Forest Act. The idea, on paper, had been to make charcoal from it and thus help improve the economic conditions of the people of Banni. This was aimed at containing the brazen spread of the wild weed, the decision however backfired with an equally mindless chopping for profit where often native trees were also cut down under the garb resulting in the crucial green cover in the region getting reduced to less than 10 per cent in 2004. In 2008, the Gujarat state government has re-imposed the ban on the cutting of ganda bawal in the Banni region of Kutch and a consensus has been reached on this at a joint meeting of the Forest department and Kutch legislators.\n\nMass cutting of ganda bawal trees and the air pollution from charcoal making has unexpectedly also vastly brought down the wild honey bee populations and has had a disastrous effect on wild bee honey collection, crop pollination and crop yields in the Kutch region. The number of dwarf bee hives in one square kilometre area has reduced to only 20–25 from the earlier 60–70 colonies in and around the Banni grasslands after the large scale tree felling. Local honey hunters (Koli community) who used to harvest about 300 tonnes of wild honey annually from Kutch after a blank for two years could only collect just 50 tonnes in 2008.\n\nIn dark nights an unexplained strange dancing light phenomena known locally as Chir Batti (Ghost lights) is known to occur here in the banni grasslands, its seasonal marshy wetlands and in the adjoining desert of the marshy salt flats of Rann of Kutch.\n\nThe Gujarat State government is developing Chari-Dhand Wetland Conservation Reserve, along with the surrounding areas in and around the Banni grasslands in the district of Kutch, for ecotourism.\n\nTo boost tourism in the area a few of the local villages in Banni are being developed as village resorts, showcasing local arts, crafts, ancient architecture of Kutch and traditional Kutchi cuisine; these mini resorts are being run by the villagers themselves in collaboration with the formal tourism infrastructure. A 270 km stretch has also been specially created in the grasslands of the Banni for the Adani Desert Car Rally organised by Kutch Infrastructure Development Society.\n\nFrom the city of Bhuj various ecologically rich and wildlife conservation areas of the Kutch / Kachchh district can be visited such as Indian Wild Ass Sanctuary, Kutch Desert Wildlife Sanctuary, Narayan Sarovar Sanctuary, Kutch Bustard Sanctuary, Banni Grasslands Reserve and Chari-Dhand Wetland Conservation Reserve etc..\n\n\n\n"}
{"id": "45584729", "url": "https://en.wikipedia.org/wiki?curid=45584729", "title": "Biosphere Technology", "text": "Biosphere Technology\n\nBioSphere Technology was invented, developed and is owned by Dr C A McCormack and the McCormack Design Bureau Nassau. From 2000 to 2013 BioSphere was marketed under licenses granted to Global Environmental Energy Corp., a US public company Chaired by the former Irish Prime Minister and Nobel Peace Prize Nominee, Albert Reynolds Under Reynolds Chairmanship BioSpheres manufactured under license in Russia, China, Thailand, Mexico and the USA were deployed worldwide. Reynolds personally spearheaded the company's involvement in North Africa, Nigeria, Iraq and North & South East Asia, before retiring due to ill health prior to his death in 2014 BioSphere and its military and marine variants BioSphere-Neo and BioSphere-Marine, is a starved oxygen gasification process that harnesses the combustibility of solid and liquid wastes and a variety of traditional fuels in a limited–oxygen environment which significantly limits atmospheric emissions creating a clean green heat source and generating electricity in a gas/steam turbine. Examples of wastes that can be converted into green energy using this technology are municipal solid waste, agricultural waste, forestry surpluses or wastes, industrial waste, medical waste materials, and traditional fossil fuels.\n\nThe manufacturing and marketing rights to the patented BioSphere technology were licensed to a Shenzhen-based Chinese consortium in 2013 and expire in 2034. BioSphere is one of several waste to energy technologies invented, patented, designed, developed and commercialised by McCormack. McCormack is a BSc & PhD graduate and former faculty member at University College Dublin, at University of California at Berkeley and at the University of Wales , a Medical Research Council Scholar, Nuffield Fellow Wellcome Fellow , Fulbright Scholar, Perkins Fellow, Fogarty Fellow and an Elected Member of The Physiological Society. \nBioSphere superseded DiGenter which was first patented in 1986 and developed with the support of the Ford Motor Company and The University of California-Riverside and Energy Resource Institute of California. DiGenter technology (the subject of 22 patents) was deployed widely in China and India and was one of the first successful commercial scale sustainable cellulosic waste to ethanol technologies; a forerunner to the now successful DuPont technologies manufacturing renewable fuel grade Bio-ethanol and gasoline from non-food feedstock's, waste materials, bagasse and biomass. The manufacturing and marketing rights to the patented DiGenter technology were re-licensed to a Hong Kong consortium in 2014 and expire in 2035. \n\n"}
{"id": "22423953", "url": "https://en.wikipedia.org/wiki?curid=22423953", "title": "Central Zambezian miombo woodlands", "text": "Central Zambezian miombo woodlands\n\nThe densely forested Central Zambezian miombo woodlands that cut across southern central Africa are one of the largest ecozones on the continent and home to a great variety of wildlife, including many large mammals.\n\nThe region covers a large area stretching northeast from Angola, including the southeast section of the Democratic Republic of the Congo, the northern half of Zambia, a large section of western Tanzania, southern Burundi, and northern and western Malawi. In the Congo the ecoregion is almost conterminous with Katanga Province. In Zambia it covers the northern half of the country above Lusaka, including the eastern and western \"ears\" and the Copperbelt. In Tanzania it covers the western inland provinces between Lake Victoria, Lake Tanganyika and Lake Malawi.\n\nThe area is mostly flat plateau, and the soils are poor. There is a tropical climate with a long dry season, up to seven months, which leaves the forest vulnerable to fires, and a rainy season from November to March. The woodland is interspersed with riverside dambos (grassy wetlands), which may constitute up to thirty percent of the region.\n\nThe woodlands contain much typical miombo flora of high trees with shrub and grassland underneath, but has much other plant life too. There are typically more evergreen trees than in most miombo woodlands. The classic miombo trees \"Brachystegia\", \"Julbernardia\", and \"Isoberlinia\" dominate the woodlands with other tree species such as \"Pterocapus angolensis\", Albizia sp. and \"Afzelia quanzensis\". Under the trees lie important areas of plants such as the herbaceous \"Crotalaria\" and \"Indigofera\".\n\nThe fauna is diverse. The grasses, shrubs and trees sustain many large mammals including black rhino, Cape buffalo, African elephants, and antelopes such as elands, sable antelope, roan antelope, Lichtenstein's hartebeest, and sitatunga. Large carnivores include lion (\"Panthera leo\"), leopard (\"Panthera pardus\"), cheetah (\"Acinonyx jubatus\"), spotted hyena (\"Crocuta crocuta\"), striped hyena (\"Hyaena hyaena\"), African wild dog (\"Lycaon pictus\") and side-striped jackal (\"Canis adustus\"). There are also many primates in the woodlands, particularly in Uganda and the Democratic Republic of the Congo, including yellow baboon and chimpanzee. The Gombe Stream National Park chimpanzee reserve is in this ecoregion. The only endemic mammals are Monard's dormouse (\"Graphiurus monardi\"), Rosevear's lemniscomys (\"Lemniscomys roseveari\"), Ansell's shrew (\"Crocidura ansellorum\") and Upemba shrew (\"Crocidura zimmeri\").\n\nThe woodlands are also home to many reptiles and birds. There are two endemic bird species, both found in the area of the ecoregion within the Democratic Republic of the Congo: the Lake Lufira weaver (\"Ploceus ruweti\") and the black-lored waxbill (\"Estrilda nigriloris\").\n\nThere are nineteen endemic reptiles and thirteen endemic amphibians; a particularly rich area for these is Upemba National Park.\n\nAlthough the ecoregion does include large areas of wilderness and national parkland, there are also areas with quite heavy population densities, particularly in Malawi and Burundi and in densely populated urban areas such as Lusaka in Zambia, around which the forest has been largely cleared for planting, firewood and charcoal production. Most of Zambia north of Lusaka is in this ecoregion, including the Copperbelt cities of Ndola, Kitwe, Chingola, location of the huge Nchanga Mines, Luanshya, and the Central Province former mining town of Kabwe (Broken Hill). In Congo, Katanga has a copper and cobalt mining industry, and there is a railway line north through the province from the capital Lubumbashi. In Tanzania the Lake Tanganyika port of Kigoma is the starting point for a visit to Gombe Stream National Park.\n\nThreats to the area include bushmeat hunting, in particular the poaching of elephant and rhino. Fire is damaging throughout the region and where there are human populations fire is more common, including the burning of woodland to create agricultural land or for charcoal, for example in Central Zambia. In the highly urbanised Copperbelt areas of Zambia and the Congo the forest has been extensively cleared for charcoal and farming, and waterways polluted by the mining industry, while the civil war in the Congo has led to considerable damage to the environment there.\n\nThe ecoregion includes large areas of national park, game reserves and other protected areas, including:\n\n\n"}
{"id": "41996379", "url": "https://en.wikipedia.org/wiki?curid=41996379", "title": "Disco Cuttlefish", "text": "Disco Cuttlefish\n\nStobie the Disco Cuttlefish was the mascot of the 2014 Adelaide Fringe Festival in South Australia. Its design was inspired by the cephalopod species \"Sepia apama\", the Giant Australian Cuttlefish. Stobie was a 13 metre long mechanised parade float which featured blinking eyes, waving tentacles and an elaborate sound, light and dance show. It made appearances each Saturday night during the festival, accompanied by a professional dance troupe which performed a set routine. This was followed by an original dance called 'The Cuttlefish' during which crowd participation was invited. Stobie also played a pre-recorded soundtrack of disco hits including a megamix of Stayin' Alive, Billie Jean, You Should be Dancing and Le Freak. The opening set routine was performed to the theme-song from the 1980 dance movie, Fame.\n\nAdelaide Fringe director Greg Clarke claimed responsibility for suggesting the adoption of a giant Australian cuttlefish as a festival mascot. He said that the festival team \"wanted to create a creature unique to South Australia and were inspired by the uniqueness and beauty of the giant cuttlefish that inhabits the gulf waters of South Australia.\" The idea was first announced with the release of the event program in November 2013.\n\nThe concept was realised by a team of South Australian artists and technicians accredited as follows:\nA ute provided the scaffold around which the cuttlefish was constructed. Sound and lighting systems were placed in the tray of the vehicle. The finished cuttlefish changed colour, featured blinking eyes and emitted smoke as a proxy for ink.\n\nThe Disco Cuttlefish was sponsored by SA Power Networks. Adelaide Fringe Festival's 2014 Principal Partner was BankSA, and its government partners were ArtsSA, the South Australian Tourism Commission and the Adelaide City Council.\n\nThe Adelaide Fringe Festival attracted criticism for using the image of the cuttlefish without referring to the decline of the Northern Spencer Gulf population of \"Sepia apama\" in South Australia. The decline has been described as 'catastrophic' by conservationists, as their population has dropped from over 170,000 in the late 1990s to 13,500 in 2013. Scientists believe the Whyalla aggregation of Giant Australian cuttlefish may represent a separate and new species, and this population collapse, if continued, may lead to extinction as soon as 2016.\n\nThe festival organisers responded by acknowledging that they were aware of the decline via Twitter and Facebook. The Twitter announcement was made on 17 February and read:\n\n\"@SaveCuttlefish Adelaide Fringe as an organisation is aware of the Giant Cuttlefish plight and we hope that by highlighting Stobie the Giant Cuttlefish in this way, we will make more people aware of the issues these creatures are facing...\"\n\nThe Adelaide Fringe Festival also added a footer to the Disco Cuttlefish event page on their website, linking to additional information from the Conservation Council of South Australia.\n\n"}
{"id": "3910661", "url": "https://en.wikipedia.org/wiki?curid=3910661", "title": "Draft (boiler)", "text": "Draft (boiler)\n\nThe difference between atmospheric pressure and the pressure existing in the furnace or flue gas passage of a boiler is termed as draft. Draft can also be referred to the difference in pressure in the combustion chamber area which results in the motion of the flue gases and the air flow.\n\nDrafts are produced by the rising combustion gases in the stack, flue, or by mechanical means. For example, a blower can be put into four categories: natural, induced, balanced, and forced.\n\nFor the proper and the optimized heat transfer from the flue gases to the boiler tubes draft holds a relatively high amount of significance. The combustion rate of the flue gases and the amount of heat transfer to the boiler are both dependent on the movement and motion of the flue gases. A boiler equipped with a combustion chamber which has a strong current of air (draft) through the fuel bed will increase the rate of combustion (which is the efficient utilization of fuel with minimum waste of unused fuel). The stronger movement will also increase the heat transfer rate from the flue gases to the boiler (which improves efficiency and circulation).\n\nSince the stack of a locomotive is too short to provide natural draft, during normal running forced draft is achieved by directing the exhaust steam from the cylinders through a cone (“blast pipe”) upwards and into a skirt at the bottom of the stack. When the locomotive is stationary or in a restricted space “live” steam from the boiler is directed through an annular ring surrounding the blast pipe to produce the same effect.\n\n"}
{"id": "54422295", "url": "https://en.wikipedia.org/wiki?curid=54422295", "title": "Erato (mythology)", "text": "Erato (mythology)\n\nIn Greek mythology, Erato (; Ancient Greek: Ἐρατώ \"desired\" or \"lovely\") was the name of the following individuals.\n"}
{"id": "84920", "url": "https://en.wikipedia.org/wiki?curid=84920", "title": "Faun", "text": "Faun\n\nThe faun (, , \"phaunos\", ) is a mythological half human–half goat creature appearing in Ancient Rome.\n\nThe goat man, more commonly affiliated with the Satyrs of Greek mythology or Fauns of Roman, is a bipedal creature with the legs and tail of a goat and the head, arms and torso of a man and is often depicted with goat's horns and pointed ears. These creatures in turn borrowed their appearance from the god Pan of the Greek pantheon. They were a symbol of fertility, and their chieftain was Silenus, a minor deity of Greek mythology.\n\nRomans believed fauns inspired fear in men traveling in lonely, remote or wild places. They were also capable of guiding humans in need, as in the fable of The Satyr and the Traveller, in the title of which Latin authors substituted the word \"Faunus\". Fauns and satyrs were originally quite different creatures: whereas fauns are half-man and half-goat, satyrs originally were depicted as stocky, hairy, ugly dwarves or woodwoses with the ears and tails of horses or asses. Satyrs also were more woman-loving than fauns, and fauns were rather foolish where satyrs had more knowledge.\n\nAncient Roman mythological belief also included a god named Faunus often associated with enchanted woods and the Greek god Pan and a goddess named Fauna who were goat people.\n\nThe \"Barberini Faun\" (located in the Glyptothek in Munich, Germany) is a Hellenistic marble statue from about 200 BCE, found in the Mausoleum of the Emperor Hadrian (the Castel Sant'Angelo) and installed at by Cardinal Maffeo Barberini (later Pope Urban VIII). Gian Lorenzo Bernini restored and refinished the statue.\n\nThe House of the Faun in Pompei, dating from the 2nd century BCE, was so named because of the dancing faun statue that was the centerpiece of the large garden. The original now resides in the National Museum in Naples and a copy stands in its place.\n\nThe French symbolist Stéphane Mallarmé's famous masterpiece ' (published in 1876) describes the sensual experiences of a faun who has just woken up from his afternoon sleep and discusses his encounters with several nymphs during the morning in a dreamlike monologue. The composer Claude Debussy based his symphonic poem ' (1894) on the poem, which also served as the scenario for a ballet entitled \"\" (or \"Afternoon of a Faun\") choreographed to Debussy's score in 1912 by Vaslav Nijinsky.\n\n"}
{"id": "17574685", "url": "https://en.wikipedia.org/wiki?curid=17574685", "title": "History of geophysics", "text": "History of geophysics\n\nThe historical development of geophysics has been motivated by two factors. One of these is the research curiosity of humankind related to Planet Earth and its several components, its events and its problems. The second is economical usage of Earth's resources (ore deposits, petroleum, water resources, etc.) and Earth-related hazards such as earthquakes, volcanoes, tsunamis, tides, and floods.\n\nIn circa 240 BC, Eratosthenes of Cyrene measured the circumference of the Earth, using trigonometry and the angle of the Sun at more than one latitude in Egypt.\n\nThere is some information about earthquakes in Aristotle's \"Meteorology\", in \"Naturalis Historia\" by Pliny the Elder, and in Strabo's \"Geographica\". Aristotle and Strabo recorded observations on tides.\n\nA natural explanation of volcanoes was first undertaken by the Greek philosopher Empedocles (c. 490-430 B.C.), who considered the world to be divided into four elemental forces: earth, air, fire and water. He maintained that volcanoes were manifestation of elemental fire. Winds and earthquakes would play a key role in explanations of volcanoes. Lucretius claimed Mount Etna was completely hollow and the fires of the underground driven by a fierce wind circulating near sea level. Pliny the Elder noted that the presence of earthquakes preceded an eruption. Athanasius Kircher (1602–1680) witnessed eruptions of Mount Etna and Stromboli, then visited the crater of Vesuvius and published his view of an Earth with a central fire connected to numerous others caused by the burning of sulfur, bitumen and coal.\n\nArguably the first modern experimental treatise was William Gilbert's \"De Magnete\" (1600), in which he deduced that compasses point north because the Earth itself is magnetic. In 1687 Isaac Newton published his \"Principia\", which not only laid the foundations for classical mechanics and gravitation but also explained a variety of geophysical phenomena such as tides and the precession of the equinox.\n\nThese experimental and mathematical analyses were applied to several areas of geophysics: Earth’s shape, density, and gravity field (Pierre Bouguer, Alexis Clairaut and Henry Cavendish), Earth’s magnetic field (Alexander von Humboldt, Edmund Halley and Carl Friedrich Gauss), seismology (John Milne and Robert Mallet), and the Earth's age, heat and radioactivity (Arthur Holmes and William Thomson, 1st Baron Kelvin).\n\nThere are several descriptions and discussions about a philosophical theory of the water cycle by Marcus Vitruvius, Leonardo da Vinci and Bernard Palissy. Pioneers in hydrology include Pierre Perrault, Edme Mariotte and Edmund Halley in studies of such things as rainfall, runoff, drainage area, velocity, river cross-section measurements and discharge. Advances in the 18th century included Daniel Bernoulli's piezometer and Bernoulli's equation as well as the Pitot tube by Henri Pitot. In the 19th century, groundwater hydrology was furthered by Darcy's law, the Dupuit-Thiem well formula, and the Hagen-Poiseuille equation for flows through pipes. \"Physical Geography of the Sea\", the first textbook of oceanography, was written by Matthew Fontaine Maury in 1855.\n\nThe thermoscope, or Galileo thermometer, was constructed by Galileo Galilei in 1607. In 1643, Evangelista Torricelli invented the mercury barometer. Blaise Pascal (in 1648) rediscovered that atmospheric pressure decreases with height, and deduced that there is a vacuum above the atmosphere.\n\nThe first known use of the word \"geophysics\" was by Julius Fröbel in 1834 (in German). It was used occasionally in the next few decades, but did not catch on until journals devoted to the subject began to appear, beginning with \"Beiträge zur Geophysik\" in 1887. The future \"Journal of Geophysical Research\" was founded in 1896 with the title \"Terrestrial Magnetism\". In 1898, a Geophysical Institute was founded at the University of Göttingen, and Emil Wiechert became the world's first Chair of Geophysics. An international framework for geophysics was provided by the founding of the International Union of Geodesy and Geophysics in 1919.\n\nThe 20th century was a revolutionary age for geophysics. As an international scientific effort between 1957 and 1958, the International Geophysical Year or IGY was one of the most important for scientific activity of all disciplines of geophysics: aurora and airglow, cosmic rays, geomagnetism, gravity, ionospheric physics, longitude and latitude determinations (precision mapping), meteorology, oceanography, seismology and solar activity.\n\nDetermining the physics of Earth's interior was enabled by the development of the first seismographs in the 1880s. Based on the behavior of the waves reflected off the internal layers of the Earth, several theories developed as to what would cause variances in wave speed or loss of certain frequencies. This led to scientists like Inge Lehmann discovering the presence of the Earth's core in 1936. Beno Gutenberg and Harold Jeffreys worked at explaining the difference in Earth's density due to compression and the shear velocity of waves. Since seismology is based on elastic waves, the speed of waves could help determine density and therefore the behavior of the layers within the Earth.\n\nNomenclature for the behavior of seismic waves was produced based on these findings. P-waves and S-waves were used to describe two types of elastic body waves possible. Love waves and Rayleigh waves were used to describe two types of surface waves possible.\n\nScientists who have contributed to advances in knowledge about the Earth's interior and seismology include Emil Wiechert, Beno Gutenberg, Andrija Mohorovičić, Harold Jeffreys, Inge Lehmann, Edward Bullard, Charles Francis Richter, Francis Birch, Frank Press, Hiroo Kanamori and Walter Elsasser.\n\nOne highly debated topic about Earth's interior is mantle plumes. These are theorized to be rising magma, which is responsible for the hotspots in the world, like Hawaii. Originally the theory was that mantle plumes rose up in a direct path, but now there is evidence that the plumes may deflect by small degrees as they rise. It was also found that the proposed hotspot underneath Yellowstone may not be related to a rising mantle plume. This theory has not been fully researched.\n\nIn the second half of the 20th century, plate tectonics theory was developed by several contributors including Alfred Wegener, Maurice Ewing, Robert S. Dietz, Harry Hammond Hess, Hugo Benioff, Walter C. Pitman, III, Frederick Vine, Drummond Matthews, Keith Runcorn, Bryan L. Isacks, Edward Bullard, Xavier Le Pichon, Dan McKenzie, W. Jason Morgan and John Tuzo Wilson. Prior to this, people had ideas of continental drift, but no real evidence came until the late 20th century. Alexander von Humboldt observed in the early 19th century the geometry and geology of the shores of continents of the Atlantic Ocean. James Hutton and Charles Lyell brought about the idea of gradual change, uniformitarianism, which helped people cope with the slow drift of the continents. Alfred Wegener spearheaded the original theory of continental drift and spent much of his life devoted to this theory. He proposed \"Pangaea\", one unified giant continent.\n\nDuring the development of continental drift theory, there was not much exploration of the oceanic part of the world, only continental. Once people began to pay attention to the ocean, geologists found that the floor was spreading, and in different rates at different spots. There are three different main ways in which plates can move: transform, divergent, and Convergent. As well, there can be Rifts, areas where the land is beginning to spread apart.\n\nAdvances in physical oceanography occurred in the 20th century. Sea depth by acoustic measurements was first made in 1914. The German \"Meteor\" expedition gathered 70,000 ocean depth measurements using an echo sounder, surveying the Mid-Atlantic Ridge between 1925 and 1927. The Great Global Rift was discovered by Maurice Ewing and Bruce Heezen in 1953, and the mountain range under the Arctic was found in 1954 by the Arctic Institute of the USSR. The theory of seafloor spreading was developed in 1960 by Harry Hammond Hess. The Ocean Drilling Program started in 1966. There has been much emphasis on the application of large scale computers to oceanography to allow numerical predictions of ocean conditions and as a part of overall environmental change prediction.\n\nThe motion of the conductive molten metal beneath the Earth's crust, or the Earth's dynamo, is responsible for the existence of the magnetic field. The interaction of the magnetic field and solar radiation has an impact on how much radiation reaches the surface of Earth and the integrity of the atmosphere. It has been found that the magnetic poles of the Earth have reversed several times, allowing researchers to get an idea of the surface conditions of the planet at that time. The cause of the magnetic poles being reversed is unknown, and the intervals of change vary and do not show a consistent interval. It is believed that the reversal is correlated to the Earth's mantle, although exactly how is still debated.\n\nDistortions to the Earth's magnetic field cause the phenomenon Aurora Borealis, commonly called the Northern Lights. The magnetic field stores energy given by cosmic particles known as solar wind, which causes the magnetic field lines to expand. When the lines contract, they release this energy, which can be seen as the Northern Lights.\n\nThe Earth's climate changes over time due to the planet's atmospheric composition, the sun's luminosity, and the occurrence of catastrophic events.\n\nAtmospheric composition affects and is affected by the biological mechanisms active on the Earth's surface. Organisms effect the amount of oxygen vs. carbon dioxide through respiration and photosynthesis. They also affect the levels of nitrogen through fixation, nitrification, and denitrification. The ocean is capable of absorbing carbon dioxide from the atmosphere, but this varies based on the levels of nitrogen and phosphorus present in the water. Humans have also played a role in changing the atmospheric composition of the Earth through industrial byproducts, deforestation, and motor vehicles.\n\nThe luminosity of the Sun increases as it progresses through its life cycle and are visible over the course of millions of years. Sunspots can form on the Sun's surface, which can cause greater variability in the emissions that Earth receives.\n\nVolcanoes form when two plates meet and one subducts underneath the other. They thus form along most plate boundaries; the Ring of Fire is an example of this. The study of volcanoes along plate boundaries has shown a correlation between eruptions and climate. Alan Robock theorizes that volcanic activity can influence climate and can lead to global cooling for years. The leading idea, based on volcanic eruptions, is that sulfur dioxide released from volcanoes has a major effect on the cooling of the atmosphere following the eruption.\n\nImpacts from large celestial bodies, commonly asteroids, create shock waves that push air and distribute dust into the atmosphere, blocking sunlight. This causes global cooling, which can lead to the death and possible extinction of many species.\n\nIndustrial applications of geophysics were developed by demand of petroleum exploration and recovery in the 1920s. Later, petroleum, mining and groundwater geophysics were improved. Earthquake hazard minimization and soil/site investigations for earthquake-prone areas were new applications of geophysical engineering in the 1990s.\n\nSeismology is used in the mining industry to read and build models of events that may have been caused or contributed to by the process of mining. This allows scientists to predict the hazards associated with mining in the area.\n\nMuch like mining, seismic waves are used to create models of the Earth's subsurface. Geological features, called traps, that commonly indicate the presence of oil, can be identified from the model and used to determine suitable sites to drill.\n\nGroundwater is highly vulnerable to the pollution produced from industry and waste disposal. In order to preserve the quality of fresh water sources, maps of groundwater depth are created and compared to the locations of pollutant sources.\n\n\n\n"}
{"id": "47375881", "url": "https://en.wikipedia.org/wiki?curid=47375881", "title": "Hot dry rock geothermal energy", "text": "Hot dry rock geothermal energy\n\nHot dry rock (HDR) is an abundant source of geothermal energy available for use. A vast store of thermal energy is contained within hot - but essentially dry - impervious crystalline basement rocks found almost everywhere deep beneath the earth’s surface. A concept for the extraction of useful amounts of geothermal energy from HDR originated at the Los Alamos National Laboratory in 1970, and Laboratory researchers were awarded a U.S. patent covering it.\nAlthough often confused with the relatively limited hydrothermal resource already commercialized to a large extent, HDR geothermal energy is very different. Whereas hydrothermal energy production can only exploit hot fluids already in place in the earth’s crust, an HDR system (consisting of the pressurized HDR reservoir, the boreholes drilled from the surface, and the surface injection pumps and associated plumbing) recovers the earth’s heat from hot but dry regions via the closed-loop circulation of pressurized fluid. This fluid, injected from the surface under high pressure, opens pre-existing joints in the basement rock, creating a man-made reservoir which can be as much as a cubic kilometer in size. The fluid injected into the reservoir absorbs thermal energy from the high-temperature rock surfaces and then serves as the conveyor for transporting the heat to the surface for practical use.\n\nAs the reservoir is formed by the pressure-dilation of the joints, the elastic response of the surrounding rock mass results in a region of tightly compressed, sealed rock at the periphery—making the HDR reservoir totally confined and contained. Such a reservoir is therefore fully engineered, in that the physical characteristics (size, depth at which it is created) as well as the operating parameters (injection and production pressures, production temperature, etc.) can be pre-planned and closely controlled.\n\nAs described by Brown, an HDR geothermal energy system is developed, first, by using conventional drilling to access a region of deep, hot basement rock. Once it has been determined the selected region contains no open faults or joints (by far the most common situation), an isolated section of the first borehole is pressurized at a level high enough to open several sets of previously sealed joints in the rock mass. By continuous pumping (hydraulic stimulation), a very large region of stimulated rock is created (the HDR reservoir) which consists of an interconnected array of joint flow paths within the rock mass. The opening of these flow paths causes movement along the pressure-activated joints, generating seismic signals (microearthquakes). Analysis of these signals yields information about the location and dimensions of the reservoir being developed.\n\nTypically, an HDR reservoir forms in the shape of an ellipsoid, with its longest axis orthogonal to the least principal earth stress. This pressure-stimulated region is then accessed by two production wells, drilled to intersect the HDR reservoir near the elongated ends of the stimulated region. In most cases, the initial borehole becomes the injection well for the three-well, pressurized water-circulating system.\n\nIn operation, fluid is injected at pressures high enough to hold open the interconnected network of joints against the earth stresses, and to effectively circulate fluid through the HDR reservoir at a high rate. During routine energy production, the injection pressure is maintained at just below the level that would cause further pressure-stimulation of the surrounding rock mass, in order to maximize energy production while limiting further reservoir growth.\n\nThe volume of the newly created array of opened joints within the HDR reservoir is much less than 1% of the volume of the pressure-stimulated rock mass. As these joints continue to pressure-dilate, the overall flow impedance across the reservoir becomes very low, leading to a high thermal productivity.\n\nThe feasibility of mining heat from the deep earth was proven in two separate HDR reservoir flow demonstrations—each involving about one year of circulation—conducted by the Los Alamos National Laboratory between 1978 and 1995. These groundbreaking tests took place at the Laboratory’s Fenton Hill HDR test site in the Jemez Mountains of north-central New Mexico, at depths of over 8000 ft and rock temperatures in excess of 180 °C. The results of these tests demonstrated conclusively the viability of the revolutionary new HDR geothermal energy concept. The two separate reservoirs created at Fenton Hill are still the only truly confined HDR geothermal energy reservoirs flow-tested anywhere in the world.\n\nThere have been numerous reports of the testing of unconfined geothermal systems pressure-stimulated in crystalline basement rock: for instance at the Rosemanowes quarry in Cornwall, England; at the Hijiori and Ogachi calderas in Japan; at the faulted Soultz-sous-Forêts “HDR” system in France; and in the Cooper Basin, Australia. However, all these “engineered” geothermal systems, while developed under programs directed toward the investigation of HDR technologies, have proven to be open—as evidenced by the high water losses observed during pressurized circulation. In essence, they are all EGS or hydrothermal systems, not true HDR reservoirs.\n\nThe EGS concept was first described by Los Alamos researchers in 1990, at a geothermal symposium sponsored by the United States Department of Energy (DOE)—many years before the DOE coined the term EGS in an attempt to emphasize the geothermal aspect of heat mining rather than the unique characteristics of HDR.\n\nThe first HDR reservoir tested at Fenton Hill, the Phase I reservoir, was created in June 1977 and then flow-tested for 75 days, from January to April 1978, at a thermal power level of 4 MW. The final water loss rate, at a surface injection pressure of 900 psi, was 2 gpm (2% of the injection rate). This initial reservoir was shown to essentially consist of a single pressure-dilated, near-vertical joint, with a vanishingly small flow impedance of 0.5 psi/gpm.\n\nThe initial Phase I reservoir was enlarged in 1979 and further flow-tested for almost a year in 1980. Of greatest importance, this flow test confirmed that the enlarged reservoir was also confined, and exhibited a low water loss rate of 6 gpm. This reservoir consisted of the single near-vertical joint of the initial reservoir (which, as noted above, had been flow-tested for 75 days in early 1978)augmented by a set of newly pressure-stimulated near-vertical joints that were somewhat oblique to the strike of the original joint.\n\nA deeper and hotter HDR reservoir (Phase II) was created during a massive hydraulic fracturing (MHF) operation in late 1983. It was first flow-tested in the spring of 1985, by an initial closed-loop flow test (ICFT) that lasted a little over a month. Information garnered from the ICFT provided the basis for a subsequent long-term flow test (LTFT), carried out from 1992 to 1995.\n\nThe LTFT comprised several individual steady-state flow runs, interspersed with numerous additional experiments. In 1992–1993, two steady-state circulation periods were implemented, the first for 112 days and the second for 55 days. During both tests, water was routinely produced at a temperature of over 180 °C and a rate of 90–100 gpm, resulting in continuous thermal energy production of approximately 4 MW. Over this time span, the reservoir pressure was maintained (even during shut-in periods) at a level of about 15 MPa.\n\nBeginning in mid-1993, the reservoir was shut in for a period of nearly two years and the applied pressure was allowed to drop to essentially zero. In the spring of 1995, the system was re-pressurized and a third continuous circulation run of 66 days was conducted. Remarkably, the production parameters observed in the two earlier tests were rapidly re-established, and steady-state energy production resumed at the same level as before. Observations during both the shut-in and operational phases of all these flow-testing periods provided clear evidence that the rock at the boundary of this man-made reservoir had been compressed by the pressurization and resultant expansion of the reservoir region.\n\nAs a result of the LTFT, water loss was eliminated as a major concern in HDR operations. Over the period of the LTFT, water consumption fell to just 7% of the quantity of water injected; and data indicated it would have continued to decline under steady-state circulation conditions. Dissolved solids and gases in the produced fluid rapidly reached equilibrium values at low concentrations (about one-tenth the salinity of sea water), and the fluid remained geochemically benign throughout the test period. \nRoutine operation of the automated surface plant showed that HDR energy systems could be run using the same economical staffing schedules that a number of unmanned commercial hydrothermal plants already employ.\n\nThe Fenton Hill tests clearly demonstrated the advantages of a fully engineered HDR reservoir over naturally occurring hydrothermal resources, including EGS. With all the essential physical characteristics of the reservoir—including rock volume, fluid capacity, temperature, etc.—established during the engineered creation of the reservoir zone, and the entire reservoir volume enclosed by a hyperstressed periphery of sealed rock, any variations in operating conditions are totally determined by intentional changes made at the surface. In contrast, a natural hydrothermal “reservoir”—which is essentially open and therefore unconfined(having boundaries that are highly variable)—is inherently subject to changes in natural conditions.\n\nAnother advantage of an HDR reservoir is that its confined nature makes it highly suitable for load-following operations, whereby the rate of energy production is varied to meet the varying demand for electric power—a process that can greatly increase the economic competitiveness of the technology. This concept was evaluated near the end of the Phase II testing period, when energy production was increased by 60% for 4 hours each day, by a programmed vent-down of the high-pressure reservoir regions surrounding the production borehole. Within two days it became possible to computerize the process, such that production was automatically increased and decreased according to the desired schedule for the rest of the test period. The transitions between the two production levels took less than 5 minutes, and at each level steady-state production was consistently maintained. Such load-following operations could not be implemented in a natural hydrothermal system or even in an EGS system because of the unconfined volume and boundary conditions.\n\nThe experiments at Fenton Hill have clearly demonstrated that HDR technology is unique, not only with respect to how the pressurized reservoir is created and then circulated, but also because of the management flexibility it offers. It has in common with hydrothermal technology only that both are “geothermal.”\n\nHot Wet Rock (HWR) hydrothermal technology makes use of hot fluids found naturally in basement rock; but such HWR conditions are rare. By far the bulk of the world’s geothermal resource base (over 98%) is in the form of basement rock that is hot but dry—with no naturally available water. This means that HDR technology is applicable almost everywhere on earth (hence the claim that HDR geothermal energy is ubiquitous).\n\nTypically, the temperature in those vast regions of the accessible crystalline basement rock increases with depth. This geothermal gradient, which is the principal HDR resource variable, ranges from less than 20 °C/km to over 60 °C/km, depending upon location. The concomitant HDR economic variable is the cost of drilling to depths at which rock temperatures are sufficiently high to permit the development of a suitable reservoir. The advent of new technologies for drilling hard crystalline basement rocks, such as new PDC (polycrystalline diamond compact) drill bits, drilling turbines or fluid-driven percussive technologies (such as Mudhammer ) may significantly improve HDR economics in the near future.\n\nAs noted above, in the late 1990s the DOE began referring to all attempts to extract geothermal energy from basement rock as “EGS,” which has led to both biographical and technical confusion. Biographically, a large number of publications exist that discuss work to extract energy from HDR without any mention of the term EGS. Thus, an internet search using the term EGS would not identify these publications.\n\nBut the technical distinction between HDR and EGS, as clarified in this article, may be even more important. Some sources describe the permeability of the earth’s basement rock as a continuum ranging from totally impermeable HDR to slightly permeable HWR to highly permeable conventional hydrothermal. However, this continuum concept is not technically correct. A more appropriate view would be to consider impermeable HDR rock as a separate state from that of the continuum of permeable rock—just as one would consider a completely closed faucet as distinct from one that is open to any degree, whether the flow be a trickle or a flood. In the same way, HDR technology should be regarded as totally distinct from EGS.\n\nA definitive book on HDR development, including a full account of the experiments at Fenton Hill, was published by Springer-Verlag in April 2012.\n\n"}
{"id": "2317035", "url": "https://en.wikipedia.org/wiki?curid=2317035", "title": "Iron(II) hydroxide", "text": "Iron(II) hydroxide\n\nIron(II) hydroxide or ferrous hydroxide is an inorganic compound with the formula Fe(OH). It is produced when iron(II) salts, from a compound such as iron(II) sulfate, are treated with hydroxide ions. Iron(II) hydroxide is a white solid, but even traces of oxygen impart a greenish tinge. The air-oxidized solid is sometimes known as \"green rust\".\n\nIron(II) hydroxide is poorly soluble in water (1.43 × 10 g/L), or 1.59 × 10 mol/L. It precipitates from the reaction of iron(II) and hydroxide salts:\n\nIf the solution is not deoxygenated and the iron reduced, the precipitate can vary in color starting from green to reddish brown depending on the iron(III) content. Iron(II) ions are easily substituted by iron(III) ions produced by its progressive oxidation.\n\nIt is also easily formed as a by-product of other reactions, a.o., in the synthesis of siderite, an iron carbonate (FeCO), if the crystal growth conditions are imperfectly controlled.\n\nFe(OH) is a layer double hydroxide (LDH).\n\nGreen rust is a recently discovered mineralogical form. All forms of green rust (including fougerite) are more complex and variable than the ideal iron(II) hydroxide compound. The natural analogue of iron(II) hydroxide compound is a very rare mineral amakinite, (Fe,Mg)(OH).\n\nUnder anaerobic conditions, the iron(II) hydroxide can be oxidized by the protons of water to form magnetite (iron(II,III) oxide) and molecular hydrogen.\nThis process is described by the Schikorr reaction:\n\nAnions such as selenite and selenate can be easily adsorbed on the positively charged surface of iron(II) hydroxide, where they are subsequently reduced by Fe. The resulting products are poorly soluble (Se, FeSe, or FeSe).\n\nIron(II) hydroxide has also been investigated as an agent for the removal of toxic selenate and selenite ions from water systems such as wetlands. The iron(II) hydroxide reduces these ions to elemental selenium, which is insoluble in water and precipitates out.\n\nIn a basic solution iron(II) hydroxide is the electrochemically active material of the negative electrode of the nickel-iron battery.\n\n"}
{"id": "22513715", "url": "https://en.wikipedia.org/wiki?curid=22513715", "title": "Jacobs Ranch", "text": "Jacobs Ranch\n\nJacobs Ranch is a large open-pit coal mine located 15 miles southeast of Wright, Wyoming in the coal-rich Powder River Basin. In 2007, the mine produced 38.1 million short tons of coal, making it the fourth-largest coal mine by production in the United States.\n\nUntil 2009 the Jacobs Ranch mine was owned by Rio Tinto Energy America, an American-based subsidiary of the international mining giant Rio Tinto Group. In early 2009, Rio Tinto reached an agreement to sell the Jacobs Ranch mine to Arch Coal for $761 million.\n"}
{"id": "55563387", "url": "https://en.wikipedia.org/wiki?curid=55563387", "title": "Kuyuluk Nature Park", "text": "Kuyuluk Nature Park\n\nKuyuluk Nature Park is a nature park in Turkey.\n\nThe park is in the Mezitli second level municipality of Mersin at . It is situated on the road to Fındıkpınarı in the southern slopes of the Toros Mountains. Its distance to Mersin is only . It was declared a picnic area in 1984 and a nature park in 2011.\n\nThe area of the nature park is . It is surrounded by Turkish pine (\"Pinus brutia\") and Mediterranean type scrup.\n"}
{"id": "409589", "url": "https://en.wikipedia.org/wiki?curid=409589", "title": "List of British natural gas companies", "text": "List of British natural gas companies\n\nThis is a list of British natural gas companies.\n\nExploration and production:\n\nUtilities:\n\nOther companies:\n\n"}
{"id": "31816263", "url": "https://en.wikipedia.org/wiki?curid=31816263", "title": "List of European tornadoes in 2011", "text": "List of European tornadoes in 2011\n\nThis is a list of all tornadoes that were confirmed throughout Europe by the European Severe Storms Laboratory and local meteorological agencies during 2011. Unlike the United States, the original Fujita Scale and the TORRO scale are used to rank tornadoes across the continent.\n\n"}
{"id": "4877817", "url": "https://en.wikipedia.org/wiki?curid=4877817", "title": "List of Martian canals", "text": "List of Martian canals\n\nThe supposed Martian canals were named, by Schiaparelli and others, after real and legendary rivers of various places on Earth or the mythological underworld. Some of those names are listed below, with the regions that the canals were thought to connect.\n"}
{"id": "39088121", "url": "https://en.wikipedia.org/wiki?curid=39088121", "title": "List of Ramsar sites in Mexico", "text": "List of Ramsar sites in Mexico\n\nThis list of Ramsar sites in Mexico includes wetlands that are considered to be of international importance under the Ramsar Convention. Mexico currently has 138 sites designated as \"Wetlands of International Importance\" with a surface area of . For a full list of all Ramsar sites worldwide, see List of Ramsar wetlands of international importance.\n\n"}
{"id": "6098955", "url": "https://en.wikipedia.org/wiki?curid=6098955", "title": "List of Sites of Special Scientific Interest in Annandale and Eskdale", "text": "List of Sites of Special Scientific Interest in Annandale and Eskdale\n\nThe following is a list of Sites of Special Scientific Interest in the Annandale and Eskdale Area of Search. For other areas, see List of SSSIs by Area of Search.\n\n"}
{"id": "18507109", "url": "https://en.wikipedia.org/wiki?curid=18507109", "title": "List of intergovernmental organizations", "text": "List of intergovernmental organizations\n\nThe following is a list of the major existing intergovernmental organizations (IGOs).\n\nFor a more complete listing, see the \"Yearbook of International Organizations\", which includes 25,000 international non-governmental organizations (INGOs), excluding for-profit enterprises, about 5,000 IGOs, and lists dormant and dead organizations as well as those in operation (figures as of the 400th edition, 2012/13).\n\nThe UN has six principal organs:\n\nThe UN also includes various Funds, Programmes and specialized agencies:\n\nThe UN also includes subsidiary organs:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n (disbanded)\n\n\n"}
{"id": "3449607", "url": "https://en.wikipedia.org/wiki?curid=3449607", "title": "List of lakes of the Faroe Islands", "text": "List of lakes of the Faroe Islands\n\nThe most important lakes in the Faroe Islands are Leitisvatn on Vágar, Fjallavatn also on Vágar, Sandsvatn on Sandoy, Lake Eiði on Eysturoy and Lake Toftir on Eysturoy. There are many other smaller lakes across the whole country, most of them used for leisure fishing. Some of the lakes are also used for electricity production, and especially Lake Eiði near Eiði and the water systems around Vestmanna are important in this context. Also in Strond on Borðoy and at Botnur in Suðuroy there are important power-plants.\n\n\nLakes referred to on the islands of Suðuroy, Sandoy, Vágoy, Nólsoy, Eysturoy, Streymoy and Hestur: \n"}
{"id": "12280396", "url": "https://en.wikipedia.org/wiki?curid=12280396", "title": "List of mountains in Norway by prominence", "text": "List of mountains in Norway by prominence\n\nThis is a list of the mountains of Norway, ordered by their topographic prominence. On the island Jan Mayen, a Norwegian administered island northeast of Iceland, the volcano Beerenberg has a height and prominence , and on the island Spitsbergen in Svalbard, the mountain Newtontoppen has height and prominence . For a list by height, see list of mountains in Norway by height.\n\n\n"}
{"id": "37904029", "url": "https://en.wikipedia.org/wiki?curid=37904029", "title": "List of nature centers in Iowa", "text": "List of nature centers in Iowa\n\nThis is a list of nature centers and environmental education centers in the state of Iowa.\n\nTo use the sortable tables: click on the icons at the top of each column to sort that column in alphabetical order; click again for reverse alphabetical order.\n\n\n"}
{"id": "18064308", "url": "https://en.wikipedia.org/wiki?curid=18064308", "title": "List of offshore wind farms", "text": "List of offshore wind farms\n\nThis page lists the largest offshore wind farms that are currently operational rated by nameplate capacity. It also lists the largest offshore wind farms currently under construction, the largest proposed offshore wind farms, and offshore wind farms with notability other than size.\n\nAs of September 2018 the Walney Extension in the United Kingdom is the largest offshore wind farm in the world at 659 MW.\n\nThis is a list of offshore wind farms with at least 200 MW nameplate capacity that are currently operational.\n\nThis is a list of wind farms with a nameplate capacity of more than 300MW currently under construction. \n\nThe following table lists largest offshore wind farm areas (by nameplate capacity) that are only at a \"proposal\" stage, and have achieved at least some of the formal consents required before construction can begin.\n\n"}
{"id": "19770805", "url": "https://en.wikipedia.org/wiki?curid=19770805", "title": "List of rivers of Sri Lanka", "text": "List of rivers of Sri Lanka\n\nThe following table lists most rivers of Sri Lanka. Since Sri Lanka is a trilingual country, some rivers may have a Sinhala name (i.e. Kalu Ganga), while other have an English name (i.e. Kelani River). In the Sinhala language, Ganga (ගඟ) translates to \"river\", where as Oya (ඔය) translates to \"smaller river\". At , the Mahaweli River is the longest river in the island, and has drainage basin covering more than one-fifth of the island.\n\n"}
{"id": "42284438", "url": "https://en.wikipedia.org/wiki?curid=42284438", "title": "MarkWest Energy", "text": "MarkWest Energy\n\nMarkWest Energy Partners, L.P. is a wholly owned subsidiary of MPLX LP (NYSE: MPLX), is engaged in the gathering, processing, and transportation of natural gas; the transportation, fractionation, storage and marketing of NGLs; and the gathering and transportation of crude oil. The company divides the business into four parts: Southwest, Northeast, Marcellus and Utica. Since 2007, the company develops natural gas supplies in emerging resource plays, now it has become the majority of its growth.\n\nIn 1988, the company was founded in Denver, Colorado.\n"}
{"id": "84526", "url": "https://en.wikipedia.org/wiki?curid=84526", "title": "Melanippe", "text": "Melanippe\n\nIn Greek mythology, Melanippe (Ancient Greek: Μελανίππη, \"black mare\") referred to several different people:\n\n\n"}
{"id": "44962413", "url": "https://en.wikipedia.org/wiki?curid=44962413", "title": "Moorish oven", "text": "Moorish oven\n\nA moorish oven (\"hornos morunos\" in Spanish) is a traditional clay wood-fired oven.\n\nCalera is a type of Moorish traditional oven for the manufacture of plaster. It is commonly used in Morocco and environs. During the Moorish period, the plaster produced was intended to erect walls on the humble homes of Madrid. Its use dates back to the early Middle Ages, and the popularity of plaster as a building material caused the demand to surge so much that more than hundred moorish ovens were built within the Madrid community. The demand for plaster was local so all of the ovens are located near the historic centers of Madrid municipalities.\n\nThe name comes from the use made during the Al-Andalus era of \"Aljezares\" in the production of plaster. These furnaces were in production until mid-century.\n"}
{"id": "9230761", "url": "https://en.wikipedia.org/wiki?curid=9230761", "title": "Mount Bruce Wildlife Centre", "text": "Mount Bruce Wildlife Centre\n\nPukaha Mount Bruce Wildlife Centre is a wildlife restoration centre at a protected forest area on State Highway 2 in New Zealand's Tararua district.\n\nThe forest was acquired by the government in the 1870s as part of the “Seventy Mile Bush”, which covered the area from Masterton to Central Hawkes Bay before European settlement. Most of the bush was destroyed and converted to farmland, but the 942 hectare Mount Bruce block was protected as a Forest Reserve. Some 55 ha of this were further protected as a Native Bird Reserve, administered by the Wildlife Service.\n\nLocal man Elwyn Welch became an expert in captive raising of birds, including endangered birds, leading to successes with Takahē (Porphyrio hochstetteri) in the 1950s.\n\nIn 1962, the centre was established to breed and release endangered native birds on these 55 hectares. Takahe (a very rare bird, thought extinct, but rediscovered in Fiordland) were the first species introduced. In the same decade, a large number of brown teal, buff weka and kākāriki were released.\n\nIn 2001 the entire forest became part of the wildlife reserve, extending the area from 55 to 942 hectares, increasing capacity to breed birds and diversified species. About 100 km of tracks were cut and thousands of traps and bait stations were scattered, setting up an area for wildlife with low predator pressure.\n\nThe main objective of is to help restore native wildlife. Currently restoration mostly concerns birds, but will expand to bats and reptiles such as the tuatara. Controlling invasive pest populations is an important means to ensuring the successful rejuvenation of native wildlife in the area. Currently the Goodnature A24 traps are being used in conjunction with other pest control methods with the aim of bringing the rat, stoat, and possum populations down to reduce the threat these animals pose to native birds in particular. \n\nBird releases started in 1996 with nine kākā, a kind of parrot. There are now approximately 160 kākā in the forest, and the goal is to have a population of 600 in a few years. North Island brown kiwi and North Island kōkako translocations followed in 2003. Over 15 kiwi are currently living in the forest and two in the nocturnal house, including some chicks. For the breeding programme, they incubate kiwi eggs to protect chicks and thus give them the chance to become adult.\n\nMany schools visit the centre. Some sponsor a kiwi, so they can follow its progress since the release. They participate in the LEOTC (Learning experiences Outside the Classroom) education programme, giving them the chance to see the kiwi and to learn about environmental problems facing New Zealand.\n\nThe second biggest mission of the centre is to welcome tourists and to educate them about environmental things and the protection of the wildlife. There are about 50,000 visitors per year.\n\nThere are several tourist facilities: a café, aviaries to discover the native birds and the nocturnal house where they can see the shy kiwi. There are guided visits and a daily feeding demonstrations for kākā and eels.\n\nThere is a staff of about 15: two work in reception, seven work with the birds or on forest regeneration, two on marketing and communication, and three or four at the café. There are also many volunteers from all over the world, who help the rangers with various tasks including preparing for the bird feeding and maintaining the aviaries and the park for the tourists.\n\nIt is closed only on Christmas Day.\n\n"}
{"id": "440959", "url": "https://en.wikipedia.org/wiki?curid=440959", "title": "Mpemba effect", "text": "Mpemba effect\n\nThe Mpemba effect is a process in which hot water can freeze faster than cold water. The phenomenon is temperature-dependent. There is disagreement about the parameters required to produce the effect and about its theoretical basis.\nThe Mpemba effect is named after Erasto Batholomeo Mpemba (b.1950) who discovered it in 1963. There were preceding ancient accounts of similar phenomena, but lacking sufficient detail to attempt verification.\n\nThe phenomenon, when taken to mean \"hot water freezes faster than cold\", is difficult to reproduce or confirm because this statement is ill-defined. Monwhea Jeng proposes as a more precise wording:\n\nThere exists a set of initial parameters, and a pair of temperatures, such that given two bodies of water identical in these parameters, and differing only in initial uniform temperatures, the hot one will freeze sooner.\n\nHowever, even with this definition it is not clear whether \"freezing\" refers to the point at which water forms a visible surface layer of ice; the point at which the entire volume of water becomes a solid block of ice; or when the water reaches . A quantity of water can be at and not be ice; after enough heat has been removed to reach more heat must be removed before the water changes to solid state (ice), so water can be liquid or solid at .\n\nWith the above definition there are simple ways in which the effect might be observed: For example, if the hotter temperature melts the frost on a cooling surface and thus increases the thermal conductivity between the cooling surface and the water container. On the other hand, there may be many circumstances in which the effect is not observed.\n\nVarious effects of heat on the freezing of water were described by ancient scientists such as Aristotle: \"The fact that the water has previously been warmed contributes to its freezing quickly: for so it cools sooner. Hence many people, when they want to cool water quickly, begin by putting it in the sun. So the inhabitants of Pontus when they encamp on the ice to fish (they cut a hole in the ice and then fish) pour warm water round their reeds that it may freeze the quicker, for they use the ice like lead to fix the reeds.\" Aristotle's explanation involved \"antiperistasis\", \"the supposed increase in the intensity of a quality as a result of being surrounded by its contrary quality.\"\n\nEarly modern scientists such as Francis Bacon noted that, \"slightly tepid water freezes more easily than that which is utterly cold.\" In the original Latin, \"aqua parum tepida facilius conglacietur quam omnino frigida.\"\n\nRené Descartes wrote in his \"Discourse on the Method\", \"One can see by experience that water that has been kept on a fire for a long time freezes faster than other, the reason being that those of its particles that are least able to stop bending evaporate while the water is being heated.\" This relates to Descartes' vortex theory.\n\nThe Scottish scientist Joseph Black investigated a special case of this phenomenon comparing previously-boiled with unboiled water; the previously-boiled water froze more quickly. Evaporation was controlled for. He discussed the influence of stirring on the results of the experiment, noting that stirring the unboiled water led to it freezing at the same time as the previously-boiled water, and also noted that stirring the very-cold unboiled water led to immediate freezing. Joseph Black then discussed Fahrenheit's description of supercooling of water (although the term supercooling had not then been coined), arguing, in modern terms, that the previously-boiled water could not be as readily supercooled.\n\nThe effect is named after Tanzanian Erasto Mpemba. He described it in 1963 in Form 3 of Magamba Secondary School, Tanganyika, when freezing ice cream mix that was hot in cookery classes and noticing that it froze before the cold mix. He later became a student at Mkwawa Secondary (formerly High) School in Iringa. The headmaster invited Dr. Denis G. Osborne from the University College in Dar es Salaam to give a lecture on physics. After the lecture, Erasto Mpemba asked him the question, \"If you take two similar containers with equal volumes of water, one at and the other at , and put them into a freezer, the one that started at freezes first. Why?\", only to be ridiculed by his classmates and teacher. After initial consternation, Osborne experimented on the issue back at his workplace and confirmed Mpemba's finding. They published the results together in 1969, while Mpemba was studying at the College of African Wildlife Management.\n\nMpemba and Osborne describe placing samples of water in beakers in the ice box of a domestic refrigerator on a sheet of polystyrene foam. They showed the time for \"freezing to start\" was longest with an initial temperature of and that it was much less at around . They ruled out loss of liquid volume by evaporation as a significant factor and the effect of dissolved air. In their setup most heat loss was found to be from the liquid surface.\nDavid Auerbach describes an effect that he observed in samples in glass beakers placed into a liquid cooling bath. In all cases the water supercooled, reaching a temperature of typically before spontaneously freezing. Considerable random variation was observed in the time required for spontaneous freezing to start and in some cases this resulted in the water which started off hotter (partially) freezing first.\n\nJames Brownridge, a radiation safety officer at the State University of New York, has said that he believes that supercooling is involved. Several molecular dynamics simulations have also supported that changes in hydrogen bonding during supercooling takes a major role in the process.\n\nIn 2016, Burridge and Linden defined the criterion as the time to reach , carried out experiments and reviewed published work to date. They noted that the large difference originally claimed had not been replicated, and that studies showing a small effect could be influenced by variations in the positioning of thermometers. They say, \"We conclude, somewhat sadly, that there is no evidence to support meaningful observations of the Mpemba effect\".\n\nHowever, in 2017, two research groups independently and simultaneously found theoretical evidence of the Mpemba effect and also predicted a new \"inverse\" Mpemba effect in which heating a cooled, far-from-equilibrium system takes less time than another system that is initially closer to equilibrium. Lu and Raz yield a general criterion based on Markovian statistical mechanics, predicting the appearance of the inverse Mpemba effect in the Ising model and diffusion dynamics. Lasanta and co-workers predict also the direct and inverse Mpemba effects for a granular gas in a far-from-equilibrium initial state. In this last work, it is suggested that a very generic mechanism leading to both Mpemba effects is due to a particle velocity distribution function that significantly deviates from the Maxwell-Boltzmann distribution.\n\nThe following explanations have been proposed:\n\n\nA reviewer for \"Physics World\" writes, \"Even if the Mpemba effect is real — if hot water can sometimes freeze more quickly than cold — it is not clear whether the explanation would be trivial or illuminating.\" He pointed out that investigations of the phenomenon need to control a large number of initial parameters (including type and initial temperature of the water, dissolved gas and other impurities, and size, shape and material of the container, and temperature of the refrigerator) and need to settle on a particular method of establishing the time of freezing, all of which might affect the presence or absence of the Mpemba effect. The required vast multidimensional array of experiments might explain why the effect is not yet understood.\n\n\"New Scientist\" recommends starting the experiment with containers at to maximize the effect. In a related study, it was found that freezer temperature also affects the probability of observing the Mpemba phenomenon as well as container temperature.\n\nIn 2012, the Royal Society of Chemistry held a competition calling for papers offering explanations to the Mpemba effect. More than 22,000 people entered and Erasto Mpemba himself announced Nikola Bregović as the winner. Bregović suggests two reasons for the effect — a colder sample gets supercooled rather than frozen, and enhanced convection in the warmer sample speeds up cooling by maintaining the heat gradient on the container walls.\n\nTao and co-workers proposed yet another possible explanation in 2016. On the basis of results from vibrational spectroscopy and modeling with density functional theory-optimized water clusters, they suggest that the reason might lie in the vast diversity and peculiar occurrence of different hydrogen bonds. Their key argument is that the number of strong hydrogen bonds increases as temperature is elevated. The existence of the small strongly-bonded clusters facilitates in turn the nucleation of hexagonal ice when warm water is rapidly cooled down.\n\nOther phenomena in which large effects may be achieved faster than small effects are\n\n\nNotes\n\n"}
{"id": "1624776", "url": "https://en.wikipedia.org/wiki?curid=1624776", "title": "Oceanium", "text": "Oceanium\n\nThe Oceanium is a public aquarium that opened in 2001 in Diergaarde Blijdorp, a zoo in Rotterdam, Netherlands.\nThe Oceanium lies in the expansion area of the zoo, which includes a new entrance and parking area, and was the biggest project to date for the zoo. The area around the Oceanium is home to projects depicting the Americas.\n\nThe Oceanium is also home to scientific research into the conservation of coral.\n"}
{"id": "3581788", "url": "https://en.wikipedia.org/wiki?curid=3581788", "title": "Opacity (optics)", "text": "Opacity (optics)\n\nOpacity is the measure of impenetrability to electromagnetic or other kinds of radiation, especially visible light. In radiative transfer, it describes the absorption and scattering of radiation in a medium, such as a plasma, dielectric, shielding material, glass, etc. An opaque object is neither transparent (allowing all light to pass through) nor translucent (allowing some light to pass through). When light strikes an interface between two substances, in general some may be reflected, some absorbed, some scattered, and the rest transmitted (also see refraction). Reflection can be diffuse, for example light reflecting off a white wall, or specular, for example light reflecting off a mirror. An opaque substance transmits no light, and therefore reflects, scatters, or absorbs all of it. Both mirrors and carbon black are opaque. Opacity depends on the frequency of the light being considered. For instance, some kinds of glass, while transparent in the visual range, are largely opaque to ultraviolet light. More extreme frequency-dependence is visible in the absorption lines of cold gases. Opacity can be quantified in many ways; for example, see the article mathematical descriptions of opacity.\n\nDifferent processes can lead to opacity including absorption, reflection, and scattering.\n\n\"Radiopacity\" is preferentially used to describe opacity of X-rays. In modern medicine, radiodense substances are those that will not allow X-rays or similar radiation to pass. Radiographic imaging has been revolutionized by radiodense contrast media, which can be passed through the bloodstream, the gastrointestinal tract, or into the cerebral spinal fluid and utilized to highlight CT scan or X-ray images. Radiopacity is one of the key considerations in the design of various devices such as guidewires or stents that are used during radiological intervention. The radiopacity of a given endovascular device is important since it allows the device to be tracked during the interventional procedure.\n\nThe words \"opacity\" and \"opaque\" are often used as colloquial terms for objects or media with the properties described above. However, there is also a specific, quantitative definition of \"opacity\", used in astronomy, plasma physics, and other fields, given here.\n\nIn this use, \"opacity\" is another term for the mass attenuation coefficient (or, depending on context, mass absorption coefficient, the difference is described here) formula_1 at a particular frequency formula_2 of electromagnetic radiation.\n\nMore specifically, if a beam of light with frequency formula_2 travels through a medium with opacity formula_1 and mass density formula_5, both constant, then the intensity will be reduced with distance \"x\" according to the formula\nwhere\n\nFor a given medium at a given frequency, the opacity has a numerical value that may range between 0 and infinity, with units of length/mass.\n\nOpacity in air pollution work refers to the percentage of light blocked instead of the attenuation coefficient (aka extinction coefficient) and varies from 0% light blocked to 100% light blocked:\n\nformula_10\n\nIt is customary to define the average opacity, calculated using a certain weighting scheme. Planck opacity uses the normalized Planck black body radiation energy density distribution, formula_11, as the weighting function, and averages formula_1 directly:\nwhere formula_14 is the Stefan-Boltzmann constant.\n\nRosseland opacity (after Svein Rosseland), on the other hand, uses a temperature derivative of the Planck distribution, formula_15, as the weighting function, and averages formula_16,\nThe photon mean free path is formula_18. The Rosseland opacity is derived in the diffusion approximation to the radiative transport equation. It is valid whenever the radiation field is isotropic over distances comparable to or less than a radiation mean free path, such as in local thermal equilibrium.\nIn practice, the mean opacity for Thomson electron scattering is:\nwhere formula_20 is the hydrogen mass fraction.\nFor nonrelativistic thermal bremsstrahlung, or free-free transitions, assuming solar metallicity, it is:\nThe Rosseland mean attenuation coefficient is:\n\n"}
{"id": "267233", "url": "https://en.wikipedia.org/wiki?curid=267233", "title": "Peijaiset", "text": "Peijaiset\n\nPeijaiset (in dialectal forms peijahaiset, peijaat or peijaajaiset) is a Finnish word that in its modern usage often refers to the celebrations following a successful elk hunt (or the hunting season), but may mean celebrating also other things that have come to an end (figurative references to \"pejaiset\" over e.g. bankrupt companies occur in newspapers). Traditionally, it referred to burials, but also to other celebrations in some dialects. \n\nKarhunpeijaiset is a celebration after a bear hunt. A bear was never \"hunted\"; it was merely \"brought down\". A single man could claim to have hunted and killed a bear, but in a community effort, the bear simply died. The ceremony was always a much more elaborate affair than the most influential member of the community would have merited. In eastern Finland it would have copious mourners and wailers, and the people would address the bear as a relative or as the son of a god. Its flesh was not eaten — that would have been cannibalism — or, if it was, an elaborate show was made to symbolically render the meat into that of another animal, e.g. venison. The bear's head was usually mounted on the top of a young tree, or on a pike. Carrion-eaters would then eat it, leaving only the skull, which would then become an object of veneration. A courtyard would also be cleared around the skull. Traditionally, only bears were sanctified thus.\n\nSometimes the ceremony was held as a sacred marriage rather than a burial. In such cases the bear was either propped up inside a frame or strapped to a cross. With all due ceremony, the chosen bride or groom would symbolically marry the bear. \n\nNowadays \"peijaiset\" usually means the festivities ending a successful hunt or hunting season, usually only for moose and bear. On many occasions these include making a meal of the latest kill for hunters in the evening.\n\nSimilar customs have been reported from many other northern people who share their habitat with bears.\n\n"}
{"id": "24714", "url": "https://en.wikipedia.org/wiki?curid=24714", "title": "Precession", "text": "Precession\n\nPrecession is a change in the orientation of the rotational axis of a rotating body. In an appropriate reference frame it can be defined as a change in the first Euler angle, whereas the third Euler angle defines the rotation itself. In other words, if the axis of rotation of a body is itself rotating about a second axis, that body is said to be precessing about the second axis. A motion in which the second Euler angle changes is called \"nutation\". In physics, there are two types of precession: torque-free and torque-induced.\n\nIn astronomy, \"precession\" refers to any of several slow changes in an astronomical body's rotational or orbital parameters. An important example is the steady change in the orientation of the axis of rotation of the Earth, known as the precession of the equinoxes.\n\nTorque-free precession implies that no external moment (torque) is applied to the body. In torque-free precession, the angular momentum is a constant, but the angular velocity vector changes orientation with time. What makes this possible is a time-varying moment of inertia, or more precisely, a time-varying inertia matrix. The inertia matrix is composed of the moments of inertia of a body calculated with respect to separate coordinate axes (e.g. , , ). If an object is asymmetric about its principal axis of rotation, the moment of inertia with respect to each coordinate direction will change with time, while preserving angular momentum. The result is that the component of the angular velocities of the body about each axis will vary inversely with each axis' moment of inertia.\n\nThe torque-free precession rate of an object with an axis of symmetry, such as a disk, spinning about an axis not aligned with that axis of symmetry can be calculated as follows:\n\nwhere is the precession rate, is the spin rate about the axis of symmetry, is the moment of inertia about the axis of symmetry, is moment of inertia about either of the other two equal perpendicular principal axes, and is the angle between the moment of inertia direction and the symmetry axis.\n\nWhen an object is not perfectly solid, internal vortices will tend to damp torque-free precession, and the rotation axis will align itself with one of the inertia axes of the body.\n\nFor a generic solid object without any axis of symmetry, the evolution of the object's orientation, represented (for example) by a rotation matrix that transforms internal to external coordinates, may be numerically simulated. Given the object's fixed internal moment of inertia tensor and fixed external angular momentum , the instantaneous angular velocity is\nPrecession occurs by repeatedly recalculating and applying a small rotation vector for the short time ; e.g.:\nfor the skew-symmetric matrix . The errors induced by finite time steps tend to increase the rotational kinetic energy:\nthis unphysical tendency can be counteracted by repeatedly applying a small rotation vector perpendicular to both and , noting that\n\nAnother type of torque-free precession can occur when there are multiple reference frames at work. For example, Earth is subject to local torque induced precession due to the gravity of the sun and moon acting on Earth's axis, but at the same time the solar system is moving around the galactic center. As a consequence, an accurate measurement of Earth's axial reorientation relative to objects outside the frame of the moving galaxy (such as distant quasars commonly used as precession measurement reference points) must account for a minor amount of non-local torque-free precession, due to the solar system’s motion.\n\nTorque-induced precession (gyroscopic precession) is the phenomenon in which the axis of a spinning object (e.g., a gyroscope) describes a cone in space when an external torque is applied to it. The phenomenon is commonly seen in a spinning toy top, but all rotating objects can undergo precession. If the speed of the rotation and the magnitude of the external torque are constant, the spin axis will move at right angles to the direction that would intuitively result from the external torque. In the case of a toy top, its weight is acting downwards from its center of mass and the normal force (reaction) of the ground is pushing up on it at the point of contact with the support. These two opposite forces produce a torque which causes the top to precess.\nThe device depicted on the right (or above on mobile devices) is gimbal mounted. From inside to outside there are three axes of rotation: the hub of the wheel, the gimbal axis, and the vertical pivot.\n\nTo distinguish between the two horizontal axes, rotation around the wheel hub will be called \"spinning\", and rotation around the gimbal axis will be called \"pitching\". Rotation around the vertical pivot axis is called \"rotation\".\n\nFirst, imagine that the entire device is rotating around the (vertical) pivot axis. Then, spinning of the wheel (around the wheelhub) is added. Imagine the gimbal axis to be locked, so that the wheel cannot pitch. The gimbal axis has sensors, that measure whether there is a torque around the gimbal axis.\n\nIn the picture, a section of the wheel has been named . At the depicted moment in time, section is at the perimeter of the rotating motion around the (vertical) pivot axis. Section , therefore, has a lot of angular rotating velocity with respect to the rotation around the pivot axis, and as is forced closer to the pivot axis of the rotation (by the wheel spinning further), because of the Coriolis effect, with respect to the vertical pivot axis, tends to move in the direction of the top-left arrow in the diagram (shown at 45°) in the direction of rotation around the pivot axis. Section of the wheel is moving away from the pivot axis, and so a force (again, a Coriolis force) acts in the same direction as in the case of . Note that both arrows point in the same direction.\n\nThe same reasoning applies for the bottom half of the wheel, but there the arrows point in the opposite direction to that of the top arrows. Combined over the entire wheel, there is a torque around the gimbal axis when some spinning is added to rotation around a vertical axis.\n\nIt is important to note that the torque around the gimbal axis arises without any delay; the response is instantaneous.\n\nIn the discussion above, the setup was kept unchanging by preventing pitching around the gimbal axis. In the case of a spinning toy top, when the spinning top starts tilting, gravity exerts a torque. However, instead of rolling over, the spinning top just pitches a little. This pitching motion reorients the spinning top with respect to the torque that is being exerted. The result is that the torque exerted by gravity – via the pitching motion – elicits gyroscopic precession (which in turn yields a counter torque against the gravity torque) rather than causing the spinning top to fall to its side.\n\nPrecession or gyroscopic considerations have an effect on bicycle performance at high speed. Precession is also the mechanism behind gyrocompasses.\n\nPrecession is the change of angular velocity and angular momentum produced by a torque. The general equation that relates the torque to the rate of change of angular momentum is:\n\nwhere formula_7 and formula_8 are the torque and angular momentum vectors respectively.\n\nDue to the way the torque vectors are defined, it is a vector that is perpendicular to the plane of the forces that create it. Thus it may be seen that the angular momentum vector will change perpendicular to those forces. Depending on how the forces are created, they will often rotate with the angular momentum vector, and then circular precession is created.\n\nUnder these circumstances the angular velocity of precession is given by:\n\nwhere is the moment of inertia, is the angular velocity of spin about the spin axis, is the mass, is the acceleration due to gravity and is the perpendicular distance of the spin axis about the axis of precession. The torque vector originates at the center of mass. Using , we find that the period of precession is given by:\n\nWhere is the moment of inertia, is the period of spin about the spin axis, and is the torque. In general, the problem is more complicated than this, however.\n\nThere is an easy way to understand why gyroscopic precession occurs without using any mathematics. The behavior of a spinning object simply obeys laws of inertia by resisting any change in direction. A spinning object possesses a property known as rigidity in space, meaning the spin axis resists any change in orientation. It is the inertia of matter comprising the object as it resists any change in direction that provides this property. Of course, the direction this matter travels constantly changes as the object spins, but any further change in direction is resisted. If a force is applied to the surface of a spinning disc, for example, matter experiences no change in direction at the place the force was applied (or 180 degrees from that place). But 90 degrees before and 90 degrees after that place, matter is forced to change direction. This causes the object to behave as if the force was applied at those places instead. When a force is applied to anything, the object exerts an equal force back but in the opposite direction. Since no actual force was applied 90 degrees before or after, nothing prevents the reaction from taking place, and the object causes itself to move in response. A good way to visualize why this happens is to imagine the spinning object to be a large hollow doughnut filled with water, as described in the book \"Thinking Physics\" by Lewis Epstein. The doughnut is held still while water circulates inside it. As the force is applied, the water inside is caused to change direction 90 degrees before and after that point. The water then exerts its own force against the inner wall of the doughnut and causes the doughnut to rotate as if the force was applied 90 degrees ahead in the direction of rotation. Epstein exaggerates the vertical and horizontal motion of the water by changing the shape of the doughnut from round to square with rounded corners. \n\nNow imagine the object to be a spinning bicycle wheel, held at both ends of its axle in the hands of a subject. The wheel is spinning clock-wise as seen from a viewer to the subject’s right. Clock positions on the wheel are given relative to this viewer. As the wheel spins, the molecules comprising it are traveling exactly horizontal and to the right the instant they pass the 12-o'clock position. They then travel vertically downward the instant they pass 3 o'clock, horizontally to the left at 6 o'clock, vertically upward at 9 o’clock and horizontally to the right again at 12 o'clock. Between these positions, each molecule travels components of these directions. Now imagine the viewer applying a force to the rim of the wheel at 12 o’clock. For this example’s sake, imagine the wheel tilting over when this force is applied; it tilts to the left as seen from the subject holding it at its axle. As the wheel tilts to its new position, molecules at 12 o’clock (where the force was applied) as well as those at 6 o’clock, still travel horizontally; their direction did not change as the wheel was tilting. Nor is their direction different after the wheel settles in its new position; they still move horizontally the instant they pass 12 and 6 o’clock. BUT, molecules passing 3 and 9 o’clock were forced to change direction. Those at 3 o’clock were forced to change from moving straight downward, to downward and to the right as viewed from the subject holding the wheel. Molecules passing 9 o’clock were forced to change from moving straight upward, to upward and to the left. This change in direction is resisted by the inertia of those molecules. And when they experience this change in direction, they exert an equal and opposite force in response AT THOSE LOCATIONS-3 AND 9 O’CLOCK. At 3 o’clock, where they were forced to change from moving straight down to downward and to the right, they exert their own equal and opposite reactive force to the left. At 9 o’clock, they exert their own reactive force to the right, as viewed from the subject holding the wheel. This makes the wheel as a whole react by momentarily rotating counter-clockwise as viewed from directly above. Thus, as the force was applied at 12 o’clock, the wheel behaved as if that force was applied at 3 o’clock, which is 90 degrees ahead in the direction of spin. Or, you can say it behaved as if a force from the opposite direction was applied at 9 o'clock, 90 degrees prior to the direction of spin.\n\nIn summary, when you apply a force to a spinning object to change the direction of its spin axis, you are not changing the direction of the matter comprising the object at the place you applied the force (nor at 180 degrees from it); matter experiences zero change in direction at those places. Matter experiences the maximum change in direction 90 degrees before and 90 degrees beyond that place, and lesser amounts closer to it. The equal and opposite reaction that occurs 90 degrees before and after then causes the object to behave as it does. This principle is demonstrated in helicopters. Helicopter controls are rigged so that inputs to them are transmitted to the rotor blades at points 90 degrees prior to and 90 degrees beyond the point at which the change in aircraft attitude is desired. The effect is dramatically felt on motorcycles. A motorcycle will suddenly lean and turn in the opposite direction the handle bars are turned. \n\nGyro precession causes another phenomenon for spinning objects such as the bicycle wheel in this scenario. If the subject holding the wheel removes a hand from one end of its axle, the wheel will not topple over, but will remain upright, supported at just the other end. However, it will immediately take on an additional motion; it will begin to rotate about a vertical axis, pivoting at the point of support as it continues spinning. If you allowed the wheel to continue rotating, you would have to turn your body in the same direction as the wheel rotated. If the wheel was not spinning, it would obviously topple over and fall when one hand is removed. The initial action of the wheel beginning to topple over is equivalent to applying a force to it at 12 o'clock in the direction toward the unsupported side (or a force at 6 o’clock toward the supported side). When the wheel is spinning, the sudden lack of support at one end of its axle is equivalent to this same force. So, instead of toppling over, the wheel behaves as if a continuous force is being applied to it at 3 or 9 o’clock, depending on the direction of spin and which hand was removed. This causes the wheel to begin pivoting at the one supported end of its axle while remaining upright. Although it pivots at that point, it does so only because of the fact that it is supported there; the actual axis of precessional rotation is located vertically through the wheel, passing through its center of mass. Also, this explanation does not account for the effect of variation in the speed of the spinning object; it only illustrates how the spin axis behaves due to precession. More correctly, the object behaves according to the balance of all forces based on the magnitude of the applied force, mass and rotational speed of the object. Once it is visualized why the wheel remains upright and rotates, it can easily be seen why the axis of a spinning top slowly rotates while the top spins as shown in the illustration on this page. A top behaves exactly like the bicycle wheel due to the force of gravity pulling downward. The point of contact with the surface it spins on is equivalent to the end of the axle the wheel is supported at. As the top's spin slows, the reactive force that keeps it upright due to inertia is overcome by gravity. Once the reason for gyro precession is visualized, the mathematical formulas start to make sense. \n\nThe special and general theories of relativity give three types of corrections to the Newtonian precession, of a gyroscope near a large mass such as Earth, described above. They are:\n\nIn astronomy, precession refers to any of several gravity-induced, slow and continuous changes in an astronomical body's rotational axis or orbital path. Precession of the equinoxes, perihelion precession, changes in the tilt of Earth's axis to its orbit, and the eccentricity of its orbit over tens of thousands of years are all important parts of the astronomical theory of ice ages. \"(See Milankovitch cycles.)\"\n\nAxial precession is the movement of the rotational axis of an astronomical body, whereby the axis slowly traces out a cone. In the case of Earth, this type of precession is also known as the \"precession of the equinoxes\", \"lunisolar precession\", or \"precession of the equator\". Earth goes through one such complete precessional cycle in a period of approximately 26,000 years or 1° every 72 years, during which the positions of stars will slowly change in both equatorial coordinates and ecliptic longitude. Over this cycle, Earth's north axial pole moves from where it is now, within 1° of Polaris, in a circle around the ecliptic pole, with an angular radius of about 23.5°.\n\nThe ancient Greek astronomer Hipparchus (c. 190-120 BC) is claimed to be the earliest known astronomer to recognize and assess the precession of the equinoxes at about 1° per century (which is not far from the actual value for antiquity, 1.38°). Caltech's Swerdlow disputes Hipparchus's knowledge of precession because Hipparchus apparently did not necessarily indicate anything like a motion of the entire sphere of the fixed stars with respect to the equinoxes. In ancient China, the Jin-dynasty scholar-official Yu Xi (fl. 307-345 AD) made a similar discovery centuries later, noting that the position of the Sun during the winter solstice had drifted roughly one degree over the course of fifty years relative to the position of the stars. The precession of Earth's axis was later explained by Newtonian physics. Being an oblate spheroid, Earth has a non-spherical shape, bulging outward at the equator. The gravitational tidal forces of the Moon and Sun apply torque to the equator, attempting to pull the equatorial bulge into the plane of the ecliptic, but instead causing it to precess. The torque exerted by the planets, particularly Jupiter, also plays a role.\n\nThe orbits of planets around the Sun do not really follow an identical ellipse each time, but actually trace out a flower-petal shape because the major axis of each planet's elliptical orbit also precesses within its orbital plane, partly in response to perturbations in the form of the changing gravitational forces exerted by other planets. This is called perihelion precession or apsidal precession.\n\nIn the adjunct image, Earth's apsidal precession is illustrated. As the Earth travels around the Sun, its elliptical orbit rotates gradually over time. The eccentricity of its ellipse and the precession rate of its orbit are exaggerated for visualization. Most orbits in the Solar System have a much smaller eccentricity and precess at a much slower rate, making them nearly circular and stationary.\n\nDiscrepancies between the observed perihelion precession rate of the planet Mercury and that predicted by classical mechanics were prominent among the forms of experimental evidence leading to the acceptance of Einstein's Theory of Relativity (in particular, his General Theory of Relativity), which accurately predicted the anomalies. Deviating from Newton's law, Einstein's theory of gravitation predicts an extra term of , which accurately gives the observed excess turning rate of 43″ every 100 years.\n\nThe gravitational force between the Sun and moon induces the precession in Earth's orbit, which is the major cause of the climate oscillation of Earth that has a period of 19,000 to 23,000 years. It follows that changes in Earth's orbital parameters (e.g., orbital inclination, the angle between Earth's rotation axis and its plane of orbit) is important to the study of Earth's climate, in particular to the study of past ice ages.\n\nOrbital nodes also precess over time.\n\n\n"}
{"id": "41565982", "url": "https://en.wikipedia.org/wiki?curid=41565982", "title": "Proof mass", "text": "Proof mass\n\nA proof mass or test mass is a known quantity of mass used in a measuring instrument as a reference for the measurement of an unknown quantity.\n\nA mass used to calibrate a weighing scale is sometimes called a \"calibration mass\" or \"calibration weight\".\n\nA proof mass that deforms a spring in an accelerometer is sometimes called the \"seismic mass\". In a convective accelerometer, a fluid proof mass may be employed.\n\n"}
{"id": "18680831", "url": "https://en.wikipedia.org/wiki?curid=18680831", "title": "Puerto Rico–Virgin Islands pipeline", "text": "Puerto Rico–Virgin Islands pipeline\n\nThe Puerto Rico–Virgin Islands pipeline is a proposed offshore natural gas pipeline to connect Puerto Rico and the United States Virgin Islands. The pipeline would start from the Puerto Rican natural gas grid on the island of Culebra and run to Saint Thomas, U.S. Virgin Islands. Gas would be supplied from the EcoEléctrica's LNG regasification plant in Peñuelas, Puerto Rico to replace diesel fuel for electric generation.\n\n"}
{"id": "43941671", "url": "https://en.wikipedia.org/wiki?curid=43941671", "title": "Regulation on Wholesale Energy Market Integrity and Transparency", "text": "Regulation on Wholesale Energy Market Integrity and Transparency\n\nRegulation on Wholesale Energy Market Integrity and Transparency (REMIT) is an EU regulation designed to increase the transparency and stability of the European energy markets while combating insider trading and market manipulation. REMIT was adopted in the European Union in 2011. Part of the regulation went into immediate effect in all EU member states, with some obligations around registration and transaction reporting only coming into effect once the REMIT Implementing Acts have been passed. The EU agency ACER has been tasked with the supervision and regulation of energy markets in accordance with REMIT.\n\nIn October 2011, the European Parliament and the European Council adopted Regulation (EU) No. 1227/2011, Regulation on Wholesale Energy Market Integrity and Transparency (REMIT). REMIT entered into force on 28 December 2011, twenty days after publication in the EU Official Journal.\n\nThe collection of transactions on the wholesale energy market as prescribed in Article 8 data is not yet implemented. Transaction reporting will begin nine months after the publication of the REMIT Implementing Acts, which happened on December 18, 2014. Thus initially, REMIT is limited to the prohibition and mandatory reporting of insider trading and market manipulation, as well as the obligation to publish inside information.\n\nIn preparation of the Implementing Acts, the EU Directorate-General responsible for the regulation, DG Energy, commissioned a market consultation and report on the implementation of a data and transaction reporting Framework for REMIT in 2012.\n\nREMIT has four main elements:<ref name=\"Regulation-1227/2011\">Regulation (EU) No. 1227/2011 of the European Parliament and of the Council of 25 October 2011 on wholesale energy market integrity and transparency (REMIT), \"Official Journal of the European Union\", L326 (December 08, 2011), p. 1-16.</ref>\n\nThe REMIT definition of a \"market participant\" applies to any legal or natural person carrying out transactions in wholesale energy products. In particular, the definition encompasses energy traders, transmission system operators, regulated exchanges for electricity or gas markets and energy brokers.\n\nThe REMIT definition of \"wholesale energy products\" includes physical and financial contracts for electricity or for natural gas where delivery is in the European Union. In particular, the definition includes the supply, transportation contracts and derivative transactions, such as options or swaps. Supply and distribution contracts to consumers and industrial clients consuming less than 600 GWh per year are excluded from REMIT. Transactions already reported under EMIR do not have to be reported again under REMIT.\n\nThe reporting of wholesale energy market transactions is mandatory for both parties to the transaction, seller and buyer. The resulting records can be linked by ACER using the Unique Transaction Identifier (UTI), which is a field in the report being unique in the market and identical for both sides of the report. In practice, this requirement is difficult to fulfill for purely bilateral transactions.\n\nAs an EU regulation, REMIT comes into force directly in all EU Member States. However, enforcement and sanctioning of possible violations remains the responsibility of Member States. Accordingly, the competencies of national regulatory authorities (NRAs) and the nature of sanctions have to be legislated in national law. In 2014, the Council of European Energy Regulators (CEER) issued a report on the implementation of REMIT at national level. The report concluded that no more than ″a majority of Member States have completed the transposition of Articles 13 and 18 of REMIT into national law in order to ensure that NRAs have the foreseen investigatory and enforcement powers, and that a sanctioning regime is in place.″\n\n"}
{"id": "9271131", "url": "https://en.wikipedia.org/wiki?curid=9271131", "title": "Samuel Rowbotham", "text": "Samuel Rowbotham\n\nSamuel Birley Rowbotham (; 1816 – 23 December 1884, London) was an English inventor and writer who wrote \"Zetetic Astronomy: Earth Not a Globe\" under the pseudonym Parallax. His work was originally published as a 16-page pamphlet (1849), which he later expanded into a 430-page book (1881). He dropped out of school at the age of 9.\n\nRowbotham's method, which he called zetetic astronomy, models the Earth as an enclosed plane centered at the North Pole and bounded along its perimeter by a wall of ice, with the Sun, Moon, planets, and stars moving only several hundred miles above the surface of Earth.\n\nRowbotham started out as an organiser of an Owenite commune in The Fens, where he formulated his theories about the Earth. After measuring a lack of curvature on the long straight drainage ditches of the Bedford Levels in his first Bedford Level experiment, he was convinced of the flatness of the Earth and began to lecture on the topic. He took a little time to learn his trade, running away from a lecture in Blackburn when he couldn't explain why the hulls of ships disappeared before their masts when sailing out to sea. However, as he persisted in filling halls by charging sixpence a lecture, his quick-wittedness and debating skills were honed so much that he could \"counter every argument with ingenuity, wit and consummate skill\".\n\nWhen finally pinned down to a challenge in Plymouth in 1864 by allegations that he wouldn't agree to a test, Parallax appeared on Plymouth Hoe at the appointed time, witnessed by Richard A. Proctor, a writer on astronomy, and proceeded to the beach where a telescope had been set up. His opponents had claimed that only the lantern of the Eddystone Lighthouse, some 14 miles out to sea, would be visible. In fact, only half the lantern was visible, yet Rowbotham claimed his opponents were wrong and that it proved the Earth was indeed flat so that many Plymouth folk left the Hoe agreeing that \"some of the most important conclusions of modern astronomy had been seriously invalidated\".\n\nIn 1861, Rowbotham married for a second time (to the 16-year-old daughter of his laundress) and settled in London, producing 14 children, of whom four survived. He was also alleged to be using the name \"Dr. Samuel Birley\", living in a beautiful 12-roomed house, selling the secrets for prolonging human life and curing every disease imaginable. Augustus De Morgan refers to him as S. Goulden. He patented a number of inventions, including a \"life-preserving cylindrical railway carriage\".\n\nHis book \"Zetetic Astronomy: The Earth not a Globe\" appeared in 1864. His lectures continued and concerned citizens addressed letters to the Astronomer Royal seeking rebuttals for his claims. A correspondent to the Leeds Times observed that \"One thing he did demonstrate was that scientific dabblers unused to platform advocacy are unable to cope with a man, a charlatan if you will (but clever and thoroughly up in his theory), thoroughly alive to the weakness of his opponents\".\n\nOne of Rowbotham's followers, John Hampden, a Christian polemicist, gained notoriety by engaging in raucous public debates with leading scientists of the day. A bet involving the prominent naturalist Alfred Russel Wallace in the famous Bedford Level experiment led to several lawsuits for fraud and libel and Hampden's imprisonment.\n\nAfter Rowbotham's death, Lady Elizabeth Blount founded the \"Universal Zetetic Society\", which attracted thousands of followers, published a magazine entitled \"The Earth Not a Globe Review\" and remained active well into the early part of the 20th century. After World War I, the movement underwent a slow decline, but it was revived in 1956 as The Flat Earth Society.\n\nIn the United States, Rowbotham's ideas were taken up by the Christian Catholic Apostolic Church and promoted widely on their radio station. His work in the United States was continued by William Carpenter. Carpenter, a printer originally from Greenwich, England, a supporter of Rowbotham and published \"Theoretical Astronomy Examined and Exposed — Proving the Earth not a Globe\" in eight parts from 1864 under the name \"Common Sense\". He later emigrated to Baltimore where he published \"A hundred proofs the Earth is not a Globe\" in 1885.\nHe argues that:\n\n\n"}
{"id": "33390054", "url": "https://en.wikipedia.org/wiki?curid=33390054", "title": "Second voyage of James Cook", "text": "Second voyage of James Cook\n\nThe second voyage of James Cook, from 1772 to 1775, commissioned by the British government with advice from the Royal Society, was designed to circumnavigate the globe as far south as possible to finally determine whether there was any great southern landmass, or Terra Australis. On his first voyage, Cook had demonstrated by circumnavigating New Zealand that it was not attached to a larger landmass to the south, and he charted almost the entire eastern coastline of Australia, yet Terra Australis was believed to lie further south. Alexander Dalrymple and others of the Royal Society still believed that this massive southern continent should exist. After a delay brought about by the botanist Joseph Banks' unreasonable demands, the ships \"Resolution\" and \"Adventure\" were fitted for the voyage and set sail for the Antarctic in July 1772.\n\nOn 17 January 1773, \"Resolution\" was the first ship to cross the Antarctic Circle which she crossed twice more on the voyage. The third crossing, on 3 February 1774, was to be the most southerly penetration, reaching latitude 71°10′ South at longitude 106°54′ West. Cook undertook a series of vast sweeps across the Pacific, finally proving there was no Terra Australis by sailing over most of its predicted locations.\n\nIn the course of the voyage he visited Easter Island, the Marquesas, Tahiti, the Society Islands, Niue, the Tonga Islands, the New Hebrides, New Caledonia, Norfolk Island, Palmerston Island, South Sandwich Islands, and South Georgia, many of which he named in the process. Cook proved the Terra Australis Incognita to be a myth and predicted that an Antarctic land would be found beyond the ice barrier.\n\nOn this voyage the Larcum Kendall K1 chronometer was successfully employed by William Wales to calculate longitude. Wales compiled a log book of the voyage, recording locations and conditions, the use and testing of various instruments, as well as making many observations of the people and places encountered on the voyage.\n\nIn 1752 a member of the Royal Society of London, Alexander Dalrymple, had found Luis Váez de Torres' testimony proving the existence of a passage south of New Guinea now known as Torres Strait, whilst translating some Spanish documents captured in the Philippines. This discovery led Dalrymple to publish \"An Historical Collection of the Several Voyages and Discoveries in the South Pacific Ocean.\" in 1770–1771 which aroused widespread interest in his claim of the existence of an unknown continent. Soon after his return from his first voyage in 1771, Commander Cook was commissioned by the Royal Society to make a second voyage in search of the supposed southern continent, Terra Australis Incognita.\n\nCook commanded HMS \"Resolution\" on this voyage, while Tobias Furneaux commanded its companion ship, HMS \"Adventure\". \"Resolution\" began her career as the 462 ton North Sea collier \"Marquis of Granby\", launched at Whitby in 1770, purchased by the Royal Navy in 1771 for £4,151, and converted to naval specifications for a cost of £6,565. She was long and abeam. She was originally registered as HMS \"Drake\", but fearing this would upset the Spanish, she was renamed \"Resolution\", on 25 December 1771. She was fitted out at Deptford with the most advanced navigational aids of the day, including an azimuth compass made by Henry Gregory, ice anchors and the latest apparatus for distilling fresh water from sea water. Twelve light 6-pounder guns and twelve swivel guns were carried. At his own expense Cook had brass door-hinges installed in the great cabin.\n\nHMS \"Adventure\" began her career as the 340 ton North Sea collier \"Marquis of Rockingham\", launched at Whitby in 1771. She was purchased by the Navy that year for £2,103 and named \"Rayleigh\", then renamed \"Adventure\". She was long, abeam and her draft was and carried ten guns. Both were built at the Fishburn yard at Whitby and purchased from Captain William Hammond of Hull.\n\nCook was asked to test the Larcum Kendall K1 chronometer on this voyage. The Board of Longitude had asked Kendall to copy and develop John Harrison's fourth model of a clock (H4) useful for navigation at sea. The first model finished by Kendall in 1769 was an accurate copy of H4, cost £450, and is known today as K1. Although constructed like a watch, the chronometer had a diameter of 13 cm and weighed 1.45 kg. Three other clocks, constructed by John Arnold were carried but did not withstand the rigors of the journey. The performance of the clocks was recorded in the logbooks of astronomers William Wales and William Bayly and as early as 1772 Wales had noted that the watch by Kendall was 'infinitely more to be depended on'.\n\nProvisions loaded onto the vessels for the voyage included of biscuit, 7,637 four-lb (appox 1,8 kg) pieces of salt beef, 14,214 two-lb (approx 1 kg) pieces of salt pork, 19 tuns of beer, of spirits of suet and 210 gallons of 'Oyle Olive'. As anti-scorbutics they took nearly of 'Sour Krout' and of 'Mermalade of Carrots'. Both ships carried livestock, including bullocks, sheep, goats (for milk), hogs and poultry (including geese). The crews had fishing gear (supplied by Onesimus Ustonson) and a water purification system was carried for distilling sea-water or purifying foul fresh-water. Various pieces of hardware (such as knives and axes) and trinkets (beads, ribbons, medallions) to be used for barter or as gifts for the natives were also taken aboard.\n\nFurneaux, commander of \"Adventure\", was an experienced explorer, having served on Samuel Wallis's circumnavigation in \"Dolphin\" in 1766–1768. He headed a crew of 81 which included Joseph Shank as first lieutenant, and Arthur Kempe as second lieutenant. There were also twelve marines headed by Lieutenant James Scott, Furneaux's personal servant, James Tobias Swilley, and, as master's mate John Rowe who was a relation of Furneaux. The ship's astronomer was William Bayly.\n\nIt was originally planned that the naturalist Joseph Banks and what he considered to be an appropriate entourage would sail with Cook, so a heightened waist, an additional upper deck and a raised poop deck were built on \"Resolution\" to suit Banks. This refit cost . However, in sea trials the ship was found to be top-heavy, and under Admiralty instructions the offending structures were removed in a second refit at Sheerness, at a further cost of . Banks subsequently refused to travel under the resulting \"adverse conditions.\" The writer Samuel Johnson was briefly considered as a replacement, but declined the offer. Instead the position was taken by Johann Reinhold Forster and his son, Georg, who were taken on as Royal Society scientists for the voyage. \"Resolution\" carried a crew of 112; as senior lieutenants Robert Cooper and Charles Clerke and among the midshipmen George Vancouver and James Burney. The master was Joseph Gilbert and Isaac Smith, a relation of Cook's wife was also aboard. William Wales was the astronomer and William Hodges the artist. In all, there were 90 seamen and 18 royal marines as well as the supernumeraries.\n\nCook’s second voyage of discovery departed Plymouth Sound on Monday 13 July 1772. His first port of call was at Funchal in the Madeira Islands, which he reached on 1 August. Cook gave high praise to his ship's sailing qualities in a report to the Admiralty from Funchal Roads, writing that she \"steers, works, sails well and is remarkably stiff and seems to promise to be a dry and very easy ship in the sea.\" The ship was re-provisioned with fresh water, beef, fruit and onions, and after a further provisioning stop in the Cape Verde Islands two weeks later, set sail due south toward the Cape of Good Hope. The \"Resolution\" anchored in Table Bay on 30 October with the crew all in good health because of Cook's imposition of a strict dietary and cleanliness regime. It was here that a Swede, Anders Sparrman joined the expedition as a botanist.\n\nThe ships left the Cape on 22 November 1772 and headed for the area of the South Atlantic where the French navigator Bouvet claimed to have spotted land that he named Cape Circumcision. Shortly after leaving they experienced severe cold weather and early on 23 November 1772 the crew were issued with \"fearnaught\" jackets and trousers at the expense of the government. By early December they were sailing in thick fog and seeing 'ice islands'. Cook had not found the island that Bouvet claimed to be in latitude 54°. Pack ice soon surrounded the ships but in the second week in January, in the southern mid-summer, the weather abated and Cook was able to take the ships southwards through the ice to reach the Antarctic Circle on 17 January. The next day, being severely impeded by the ice, they changed course and headed away to the north-east.\n\nOn 8 February 1773 \"Resolution\" and \"Adventure\" became separated in the Antarctic fog. Furneaux directed \"Adventure\" towards the prearranged meeting point of Queen Charlotte Sound (New Zealand), charted by Cook in 1770. On the way to the rendezvous, \"Adventure\" surveyed the southern and eastern coasts of Tasmania (then known as \"Van Diemen's Land\"), where Adventure Bay was named for the ship. Furneaux made the earliest British chart of this shore, but as he did not enter Bass Strait he assumed Tasmania to be part of Australia. \"Adventure\" arrived at Queen Charlotte Sound on 7 May 1773.\nCook continued his explorations south-eastwards, reaching 61°21′s on 24 February then, in mid-March he decided to head for Dusky Bay (now Dusky Sound) in the South Island of New Zealand where the ship rested until 30 April. The \"Resolution\" reached the rendezvous at Queen Charlotte Sound on 17 May.\nFrom June to October the two ships explored the southern Pacific, reaching Tahiti on 15 August, where Omai of Ra'iatea embarked on \"Adventure\". (Omai later became one of the first Pacific Islanders to visit Europe, before returning to Tahiti with Cook in 1776.)\n\nAfter calling at Tonga in the Friendly Islands the ships returned to New Zealand but were separated by a storm on 22 October. This time the rendezvous at Queen Charlotte Sound was missed — \"Resolution\" departed on 26 November, four days before \"Adventure\" arrived. Cook had left a message buried in the sand setting out his plan to explore the South Pacific and return to New Zealand. Furneaux decided to return home and buried a reply to that effect. In New Zealand Furneaux lost some of his men during an encounter with Māori, and eventually sailed back to Britain, setting out for home on 22 December 1773 via Cape Horn, arriving in England on 14 July 1774.\n\nCook continued to explore the Antarctic, heading south into the summer sea ice, icebergs and fog until he reached 67°31′ South before hauling north again for . The third crossing of the Antarctic Circle, on 26 January 1774, was the precursor to the most southerly penetration, reaching latitude 71°10′ South at longitude 106°54′ West on 30 January when they could go no further because of the solid sea ice. On this occasion, Cook wrote:\nI who had ambition not only to go farther than anyone had been before, but as far as it was possible for man to go, was not sorry in meeting with this interruption...\nThe vessel was then launched north to complete a huge arc in the Pacific Ocean, reaching latitudes just below the Equator then New Guinea. He had landed at the Friendly Islands, Easter Island, Norfolk Island, New Caledonia, and Vanuatu before returning to Queen Charlotte Sound in New Zealand.\n\nOn 10 November 1774 the expedition sailed east over the Pacific and sighted the western end of the Strait of Magellan on 17 December. They spent Christmas in a bay they named \"Christmas Sound\" on the western side of Tierra del Fuego. After passing Cape Horn, Cook explored the vast South Atlantic looking for another coastline that had been predicted by Dalrymple. When this failed to materialize they turned north and discovered an island that they named South Georgia. In a last vain attempt to find Bouvet Island Cook discovered the South Sandwich Islands. Here he correctly predicted that: ...there is a tract of land near the Pole, which is the Source of most of the ice which is spread over this vast Southern Ocean.\n\nLater, in February 1775, he called the existence of such a polar continent \"probable\" and in another copy of his journal he wrote:\"[I] firmly believe it and its more than probable that we have seen a part of it\".\n\nOn 21 March 1775 \"Resolution\" anchored in Table Bay, there to spend five weeks as her rigging was refitted. She arrived home at Spithead, Portsmouth on 30 July 1775 having visited St Helena and Fernando de Noronha on the way.\n\nHis reports upon his return home put to rest the popular myth of Terra Australis. Another accomplishment of the second voyage was the successful employment of the Larcum Kendall K1 chronometer, which enabled Cook to calculate his longitudinal position with much greater accuracy. Cook's log was full of praise for the watch which he used to make charts of the southern Pacific Ocean that were so remarkably accurate that copies of them were still in use in the mid-20th century.\nCook was promoted to the rank of captain and given an honorary retirement from the Royal Navy, as an officer in the Greenwich Hospital. His acceptance of the post was reluctant, insisting that he be allowed to quit the post if the opportunity for active duty presented itself. His fame now extended beyond the Admiralty and he was also made a Fellow of the Royal Society and awarded the Copley Gold Medal, painted by Nathaniel Dance-Holland, dined with James Boswell and described in the House of Lords as \"the first navigator in Europe\".\n\nOn his return to England, Forster claimed that he had been granted exclusive publication rights to the history of the voyage by the First Lord of the Admiralty, Lord Sandwich—a claim that Sandwich vehemently denied. Cook was writing his own account assisted by Dr John Douglas, Canon of Windsor. Eventually, Sandwich agreed that Forster and his son could add a scientific section to Cook's account of the voyage. This led to so much animosity between Forster and Sandwich that Sandwich banned him from writing or publishing anything about the voyage. Forster then avoided the ban by writing a book in his son's name using a draft of Cook's journal which he had earlier acquired. When Forster published his book, six weeks before Cook's own account, it was found to be highly critical of Cook and other members of the voyage, inaccurate and somewhat fabricated. Cook never read the book because it was published after he left on his third voyage.\n\nCook's accounts of the large seal and whale populations helped influence further exploration of the Southern Ocean from sealers in search of the mammals' valued skins. In the 19th century over one thousand sealing ships travelled to the Antarctic regions and its shoreline.\n\n\n\n"}
{"id": "34221331", "url": "https://en.wikipedia.org/wiki?curid=34221331", "title": "Shatsky Rise", "text": "Shatsky Rise\n\nThe Shatsky Rise is Earth's third largest oceanic plateau, (after Ontong Java and Kerguelen) located in the north-west Pacific Ocean east of Japan. It is one of a series of Pacific Cretaceous large igneous provinces (LIPs) together with Hess Rise, Magellan Rise, and Ontong Java-Manihiki-Hikurangi.\nIt was named for Nikolay Shatsky (1895-1960), a Soviet geologist, expert in tectonics of ancient platforms.\n\nThe rise consists of three large volcanic massifs, Tamu, Ori, and Shirshov, but, in contrast, there are few traces of magmatism on the surrounding ocean floor. The Tamu Massif may be the largest volcano yet discovered on Earth.\n\nIt covers an area that has been estimated to (roughly the size of California or Sumatra) and a volume of .\nBeneath Shatsky rise, however, the Mohorovičić discontinuity (Moho, the mantle-crust boundary) disappears at a depth of whereas it is normally observed at a depth of . Furthermore, the crustal thickness between the massifs of the Shatsky Rise is almost twice that of normal crust thickness. This considered, the area covered by the rise, assuming the crust was also formed by the Shatsky Rise volcanism, has been estimated to and the volume to .\n\nAfter its formation Shatsky Rise was uplifted and it then subsided , which, in both cases, is considerably more than in the case the Ontong-Java Plateau. There was least subsidence at the centre of the Tamu Massif ( ), subsidence increased at the northern flank of the Tamu Massif and at the Ori Massif ( ), and it becomes greatest at the flank of Ori Massif. The cause of this gradual increase in subsidence can be underplating beneath Tamu Massif. There was much less subsidence at Shirshov Massif farther north ( ) which probably represents a later, different phase of volcanism.\n\nScientific studies of the size, shape, and eruption rate of the Shatsky Rise have concluded that the rise originated from a mantle plume, whereas studies of magnetic lineations and plate tectonic reconstructions have shown that it must have originated near a triple junction and drifted up to during the Early Cretaceous (140–100 Ma). A 2016 study concluded that the Tamu Massif formed at a mid-ocean ridge that interacted with a plume head and that the Ori Massif formed off-axis probably from a plume tail.\n\nShatsky Rise formed at a triple junction, but the thickness of the plateau coupled with the depth and intensity of melting is different from those of MORB (mid-ocean ridge basalt), making a recycled mantle slab a more likely source. A decrease in magma volume with time is more consistent with the involvement of a mantle plume.\n\nIt formed during the Late Jurassic and Early Cretaceous at the Pacific–Farallon–Izanagi triple junction, probably making it the oldest unaltered ocean plateau. Because this occurred before the so-called Cretaceous silent period, a long period without magnetic reversals, its formation can be precisely dated.\nMagnetic lineations on and surrounding Shatsky Rise range from M21 (147 Ma) at the south-western edge to M1 (124 Ma) at the northern tip.\n\nThe Shatsky Rise LIP erupted at the location of the Pacific–Farallon–Izanagi triple junction 147–143 Ma either because a mantle plume reached the surface or because of decompression melting at a mid-ocean ridge. The eruption coincided with a , nine-stage jump in the location of the triple junction and a configuration change from ridge-ridge-ridge to ridge-ridge-transform.\n\nA set of magnetic lineations, called the Hawaiian lineations, between Shatsky Rise, Hess Rise, and the Mid-Pacific Mountains, formed during the spreading between the Pacific and Farallon plates 156–120 Ma. North of Shatsky Rise the so-called Japanese lineations are oriented in another direction and the differences in orientations trace the path of the Pacific–Farallon–Izanagi triple junction.\n\nThe triple junction moved north-west before M22 (150 Ma) after-which it started to reorganise, a microplate formed and the triple junction made an eastward jump to the oldest part of the rise, the TAMU Massif. The remainder of Shatsky Rise formed before M3 (126 Ma) along the trace of the triple junction. Shaktsky volcanism was episodic and tied to at least nine ridge jumps from this episode.\n\nThe volume of the rise decreases along the trace of the triple junction. The TAMU Massif at the southern end has an estimated volume of whereas both ORI and Shirshov (136 Ma) attained . Papanin Ridge, the north end of the rise, has a volume of but was probably emplaced over a longer period (131–124 Ma).\n\nThe conjugates of the Shatsky and Hess rises on the Farallon Plate were most likely involved in the Laramide orogeny; the former subducted beneath North America and the latter below northern Mexico.\n\n"}
{"id": "46462106", "url": "https://en.wikipedia.org/wiki?curid=46462106", "title": "Siva Power", "text": "Siva Power\n\nSiva Power, Inc. is an American solar power company that develops thin-film technology. The company designs and manufactures copper indium gallium deselenide (CIGS) photovoltaics. Siva Power is based in San Jose, California. Bruce Sohn is CEO and Brad Mattson is Chairman.\n\nThe company was founded in 2006 as Solexant (one of the founders was Paul Alivisatos) and was developing cadmium telluride (CdTe) technology to create solar panels. After the Chinese began dumping silicon solar panels and the American thin-film market worsened, the company decided to pivot and in 2011 hired Brad Mattson to take Solexant in a new direction, which he restarted in 2012. Mattson put the company into stealth mode and ramped up the R&D budget to find a different technology and process for manufacturing solar modules. In 2013 the company reemerged as Siva Power, developing CIGS photovoltaic technology.\n\nSiva Power's deposition technique is co-evaporation, which deposits the CIGS compound alloy film by direct reactive condensation of vapors of its constituent elements (copper, indium, gallium, and selenium) onto the substrate, for which Siva Power uses glass. Siva Power has said its initial production capacity of a single line would be 300 megawatts of modules per annum, whereas typical pilot production lines tend to be 5 to 30 megawatts.\n\nIn 2014, Siva Power won the US Department of Energy SunShot Initiative award under the SunShot SolarMat2 category for driving down the cost of manufacturing and implementing efficiency-increasing technology in manufacturing processes for solar modules. The manufacturing component Siva Power plans to build through the SunShot award are designed to have a 12x higher areal (e.g.: m2/min) manufacturing throughput than other CIGS deposition tools, enabling a fully automated CIGS deposition system at a 3x reduction in capex, labor, and overhead cost.\n\nWhile in stealth mode, Robert Wendt, former CTO of XsunX and one of the early pioneers of coevaporated CIGS technology, joined as Vice President of Process and Equipment Development. He began Siva’s CIGS program establishing the company's initial CIGS process capability. When Siva Power came out of stealth, it also announced that energy-security expert and former Director of the CIA James Woolsey had joined its board of directors. Shortly after, Siva Power hired Dr. Markus Beck, former Chief Scientist of First Solar and Solyndra, to be Siva Power’s CTO. Dr. Billy J. \"BJ\" Stanbery, former CEO and founder of HelioVolt, joined Siva Power as President in March 2015. Siva Power announced in May 2015 that Chris McDonald, a factory operations expert from Applied Materials' Solar Business Group and Intel, would become its COO. In 2016, Siva hired Bruce Sohn, former President of First Solar who was instrumental in driving First Solar from startup to billion-dollar company, as its Vice Chairman of the Board. Sohn became CEO in 2017.\n\nSiva Power has also assembled a technical advisory board composed of Dr. Rommel Noufi, Dr. Charlie Gay, Dr. Markus Beck, John Benner, Dr. Bulent Basol, Scott Thomsen, Dr. BJ Stanbery and Dr. Vivek Lall, all photovoltaic and materials experts. Noufi was the Principal Scientist of the CIGS technology group at the US National Renewable Energy Laboratory (NREL) and is a consulting professor at Stanford University. Gay was the former director of NREL and served as President of Applied Materials Solar Division. Dr. Gay left Siva's advisory board in 2016 when he joined the DOE as Director of the SunShot Program, so as to avoid conflict of interest Benner is the Executive Director of the Bay Area Photovoltaic Consortium, an industry-led consortium managed by Stanford and UC Berkeley charged with funding next-generation PV research at US universities and national laboratories with funds from both the US DoE and its industry members, and Basol has more than 30 years of experience in photovoltaics, beginning with Monosolar, one of the first solar companies. Scott Thomsen is a glass materials expert who ran Guardian Industries global glass group as the company President, having previously served as the company's Head of North American glass operations and CTO.\n\nWhen the company was Solexant and developing CdTe technology, it had raised $60M over 3 rounds of venture funding from X Seed Capital, Acero Capital (formerly Olympus Capital), Medley Partners, Birchmere Ventures, Trident Capital, Firelake Capital, and DBL Investors. In 2011, Solexant acquired Wakonda, a solar startup based in New York.\n\nThe company was recapitalized with $7M in 2015 in series D preferred equity investments from DBL, Medley, Acero and the city of Wuxi, China. In 2016, the company added $5M more to the series D investment. A $25M Series E round, led by Renaissance Technologies hedge fund manager Jim Simons, was closed in May 2017 for the purpose of funding a pilot line.\n"}
{"id": "5086358", "url": "https://en.wikipedia.org/wiki?curid=5086358", "title": "Ski season", "text": "Ski season\n\nA ski season is a period when skiing, snowboarding and other alpine sports are viable in an alpine resort. The season corresponds to when ski lifts are running and lift passes are available. Depending on the latitude and altitude of the resort, the season will typically run from early to mid-winter until mid- to late spring. Ideally the season will be over before the thaw begins.\n\nA typical ski season has three stages, and therefore three levels of lift ticket pricing:\n\n\nTypically in the United States, a ski season lasts from late November to early April, however larger resorts in Colorado and California are known to spin the lifts as late as the 4th of July.\n"}
{"id": "10778657", "url": "https://en.wikipedia.org/wiki?curid=10778657", "title": "South American Energy Summit", "text": "South American Energy Summit\n\nThe South American Energy Summit is the name for one of a sequence of summits bringing together delegations from the countries of the Union of South American Nations to discuss energy related issues.\n\nThe first South American Energy Summit took place on April 16–17 2007, on Isla Margarita, in the Venezuelan state of Nueva Esparta. Ten of the 12 South American presidents attend in person, the exceptions being Tabaré Vázquez of Uruguay and Alan García of Peru. Among the issues discussed was the production of ethanol fuel in the region, over which the countries have differing views. It was agreed that a new South American Energy Council, headed by the energy ministers of the 12 countries, would be created to co-ordinate energy policy, while the prospect of a future South American Energy Treaty was also raised. \n\nOn non-energy topics, the presidents also agreed that the South American Community of Nations should be renamed the Union of South American Nations.\n\n\n \n"}
{"id": "5413154", "url": "https://en.wikipedia.org/wiki?curid=5413154", "title": "Subtropical ridge", "text": "Subtropical ridge\n\nThe subtropical ridge, also known as the subtropical high or horse latitudes, is a significant belt of atmospheric high pressure situated around the latitudes of 30°N in the Northern Hemisphere and 30°S in the Southern Hemisphere. It is the product of the global air circulation cell known as the Hadley Cell. The subtropical ridge is characterized by mostly calm winds, which act to reduce air quality under its axis by causing fog overnight, and haze during daylight hours as a result of the stable atmosphere found near its location. The air descending from the upper troposphere flows out from its center at surface level toward the upper and lower latitudes of each hemisphere, creating both the trade winds and the westerlies. The subtropical ridge moves poleward during the summer, reaching its most northern latitude in early fall, before moving equatorward during the cold season. The El Niño southern climate oscillation (ENSO) can displace the northern hemisphere subtropical ridge, with La Niñas allowing for a more northerly axis for the ridge, while El Niños show flatter, more southerly ridges. The change of the ridge position during ENSO cycles changes the tracks of tropical cyclones that form around their equatorward and western peripheries. As the subtropical ridge varies in position and strength, it can enhance or depress monsoon regimes around their low-latitude periphery.\n\nHeating of the earth near the equator leads to large amounts of convection along the monsoon trough or Intertropical convergence zone. This air mass rises to the lower stratosphere where it diverges, moving away from the equator in the upper troposphere in both northerly and southerly directions. As it moves towards the mid-latitudes on both sides of the equator, the air cools and sinks. The resulting air mass subsidence creates a subtropical ridge of high pressure near the 30th parallel in both hemispheres. At the surface level, the sinking air diverges again with some returning to the equator, completing the Hadley circulation. This circulation on each side of the equator is known as the Hadley cell and leads to the formation of the subtropical ridge. Many of the world's deserts are caused by these climatological high-pressure areas.\n\nThe subtropical ridge starts migrating poleward in late spring reaching its zenith in early autumn before retreating equatorward during the late fall, winter, and early spring. The equatorward migration of the subtropical ridge during the cold season is due to increasing north-south temperature differences between the poles and tropics. The latitudinal movement of the subtropical ridge is strongly correlated with the progression of the monsoon trough or Intertropical Convergence Zone.\n\nMost tropical cyclones form on the side of the subtropical ridge closer to the equator, then move poleward past the ridge axis before recurving into the main belt of the Westerlies. When the subtropical ridge shifts due to ENSO, so will the preferred tropical cyclone tracks. Areas west of Japan and Korea tend to experience many fewer September–November tropical cyclone impacts during El Niño and neutral years, while mainland China experiences much greater landfall frequency during La Niña years. During El Niño years, the break in the subtropical ridge tends to lie near 130°E, which would favor the Japanese archipelago, while in La Niña years the formation of tropical cyclones, along with the subtropical ridge position, shift west, which increases the threat to China. In the Atlantic basin, the subtropical ridge position tends to lie about 5 degrees farther south during El Niño years, which leads to a more southerly recurvature for tropical cyclones during those years.\n\nWhen the Atlantic Multidecadal Oscillation's mode is favorable to tropical cyclone development (1995–present), it amplifies the subtropical ridge across the central and eastern Atlantic.\n\nWhen the subtropical ridge in the northwest Pacific is stronger than normal, it leads to a wet monsoon season for Asia. The subtropical ridge position is linked to how far northward monsoon moisture and thunderstorms extend into the United States. The subtropical ridge across North America typically migrates far enough northward to begin monsoon conditions across the Desert Southwest from July to September. When the subtropical ridge is farther north than normal towards the Four Corners, monsoon thunderstorms can spread northward into Arizona. When the high pressure moves south, its circulation cuts off the moisture and the atmosphere dries out across the Desert Southwest, causing a break in the monsoon regime.\n\nOn the subtropical ridges western edge (eastern coast of continents), the high pressure cell creates a southerly flow of tropical air toward the lower east sides of continents in the summer months. In the United States the subtropical ridge Bermuda High helps create the hot, sultry summers with daily thunderstorms typical of the Gulf and Atlantic states. This flow pattern also occurs on the eastern coasts of continents in other subtropical climates such as South China, southern Japan, central-eastern South America, southern Queensland and KwaZulu Natal province in South Africa.\n\nWhen surface winds become light, the subsidence produced directly under the subtropical ridge can lead to a buildup of particulates in urban areas under the ridge, leading to widespread haze. If the low level relative humidity rises towards 100 percent overnight, fog can form.\n\n"}
{"id": "1247702", "url": "https://en.wikipedia.org/wiki?curid=1247702", "title": "Teleomorph, anamorph and holomorph", "text": "Teleomorph, anamorph and holomorph\n\nIn mycology, the terms teleomorph, anamorph, and holomorph apply to portions of the life cycles of fungi in the phyla Ascomycota and Basidiomycota:\n\n\nFungi are classified primarily based on the structures associated with sexual reproduction, which tend to be conserved. However, many fungi reproduce only asexually, and cannot be easily placed in a classification based on sexual characters; some produce both asexual and sexual states. These problematic species are often members of the Ascomycota, but a few of them belong to the Basidiomycota. Even among fungi that reproduce both sexually and asexually, often only one method of reproduction can be observed at a specific point in time or under specific conditions. Additionally, fungi typically grow in mixed colonies and sporulate amongst each other. These facts have made it very difficult to link the various states of the same fungus.\n\nFungi that are not known to produce a teleomorph were historically placed into an artificial phylum, the \"Deuteromycota\", also known as \"Fungi Imperfecti\", simply for convenience. Some workers hold that this is an obsolete concept, and that molecular phylogeny allows accurate placement of species which are only known from part of their life cycle. Others retain the term deuteromycetes, but give it a lowercase \"d\" and no taxonomic rank.\n\nHistorically, Article 59 of the International Code of Botanical Nomenclature permitted mycologists to give asexually reproducing fungi (anamorphs) separate names from their sexual states (teleomorphs) but this practice was discontinued as of 1 January 2013.\n\nThe dual naming system can be confusing for novices. It is essential for workers in plant pathology, mold identification, medical mycology, and food microbiology, fields in which asexually reproducing fungi are commonly encountered.\n\nThe separate names for anamorphs of fungi with a pleomorphic life-cycle has been an issue of debate since the phenomenon was recognized in the mid-19th century. This was even before the first international rules for botanical nomenclature were issued in 1867. Special provisions are to be found in the earliest \"Codes\", which were then modified several times, and often substantially. The rules have been updated regularly and become increasingly complex, and by the mid-1970s they were being interpreted in different ways by different mycologists – even ones working on the same genus. Following intensive discussions under the auspices of the International Mycological Association, drastic changes were made at the International Botanical Congress in 1981 to clarify and simplify the procedures – and the now terms anamorph, teleomorph, and holomorph entered general use. An unfortunate effect of the simplification was that many name changes had to be made, including for some well-known and economically important species; at that date, the conservation of species names.\n\nUnforeseen in the 1970s, when the 1981 provisions were crafted, was the impact of molecular systematics. A decade later, it was starting to become obvious that fungi with no known sexual stage could confidently be placed in genera which were typified by species in which the sexual stage was known. This possibility of abandoning the dual nomenclatural system was debated at subsequent International Mycological Congresses and on other occasions, and the need for change was increasingly recognized. At the International Botanical Congress in Vienna in 2005, some minor modifications were made which allowed anamorph-typified names to be epitypified by material showing the sexual stage when it was discovered, and for that anamorph name to continue to be used.\n\nThe 1995 edition of the influential \"Ainsworth and Bisby’s Dictionary of the Fungi\" sought to replace the term anamorph with \"mitosporic fungus\" and teleomorph with \"meiosporic fungus\", based on the idea that the fundamental distinction is whether mitosis or meiosis preceded sporulation. This is a controversial choice because it is not clear that the morphological differences which traditionally define anamorphs and teleomorphs line up completely with sexual practices, or whether those sexual practices are sufficiently well understood in some cases.\n\nThe Vienna Congress (2005) established a Special Committee to investigate the issue further, but it was unable to reach a consensus. Matters were becoming increasingly desperate as mycologists using molecular phylogenetic approaches started to ignore the provisions, or interpret them in different ways.\n\nThe International Botanical Congress in Melbourne in July 2011 made a change in the \"International Code of Nomenclature for algae, fungi, and plants\" and adopted the principle \"one fungus, one name\". After 1 January 2013, one fungus can only have one name; the system of permitting separate names to be used for anamorphs then ended. This means that all legitimate names proposed for a species, regardless of what stage they are typified by, can serve as the correct name for that species.\n\nAll names now compete on an equal footing for priority, but in order not to render names for separate morphs as illegitimate, it was agreed that these should not be treated as superfluous alternative names in the sense of the \"Code\". It was further decided that anamorph-typified names should not be taken up to displace widely used teleomorph-typified names until the case has been considered by the General Committee established by the Congress. It was decided that lists of names can be submitted to the General Committee and, after due scrutiny, names accepted on those lists are to be treated as conserved over competing synonyms (and listed as appendices to the \"Code\"). Lichen-forming fungi (but not lichenicolous fungi) had always been excluded from the provisions permitting dual nomenclature.\n\n\nThis article incorporates CC-BY-3.0 text from the reference\n"}
{"id": "55120110", "url": "https://en.wikipedia.org/wiki?curid=55120110", "title": "Topaz marsh", "text": "Topaz marsh\n\nTopaz Slough Wildlife Management Area (Topaz Slough WMA) or Topaz Marsh is a marsh system that is part of the old Sevier River bed.\n\nThe water source stems from springs and irrigation runoff.\n\nTopaz Slough WMA is a historical use area for waterfowl and marsh dwelling species. Many different species of birds such as ducks, coots, geese, kestrels, great blue herons, and various shoreline birds can be found during the year.\n\nOccasionally, big animals like antelope use water on the west side of the slough. Furbearers include coyote, badger, red fox, and muskrat. Small game species that inhabit the WMA include cottontail rabbits, ring-necked pheasants, and mourning doves.\n"}
{"id": "2307244", "url": "https://en.wikipedia.org/wiki?curid=2307244", "title": "Urban climate", "text": "Urban climate\n\nThe climate in urban areas differs from that in neighboring rural areas, as a result of urban development. Urbanization greatly changes the form of the landscape, and also produces changes in an area's air.\n\nIn 1950 Åke Sundborg published one of the first theories on the climate of cities.\n\nTemperatures are higher in cities than the surrounding rural areas—this area is called the urban heat island. With rapid growth of megacities in the world, more sensible heat fluxes go to the atmosphere and thus making higher air temperature around urban area and its surrounding rural area. There are a number of causes of the urban heat island:\n\nThe urban heat island effect tends to be stronger in winter because the colder air above the city is less able to rise by convection to allow the hot air inside the city to escape into the atmosphere. The effect is greater at night for the same reason.\n\nBecause cities are warmer, the hot air is more likely to rise and if it has a high humidity it will cause convectional rainfall – short intense bursts of rain and thunderstorms. \nUrban areas produce particles of dust (notably soot) and these act as hygroscopic nuclei which encourages rain production.\nBecause of the warmer temperatures there is less snow in the city than surrounding areas.\n\nWind speeds are often lower in cities than the countryside because the buildings act as barriers (wind breaks). On the other hand, long streets with tall buildings can act as wind tunnels – winds funnelled down the street – and can be gusty as winds are channelled round buildings (eddying).\n\nCities usually have a lower relative humidity than the surrounding air because cities are hotter, and rainwater in cities is unable to be absorbed into the ground to be released into the air by evaporation, and transpiration does not occur because cities have little vegetation. Surface runoff is usually taken up directly into the subterranean sewage water system and thus vanishes from the surface immediately.\n\n"}
{"id": "2790692", "url": "https://en.wikipedia.org/wiki?curid=2790692", "title": "Vietnam Petroleum Institute", "text": "Vietnam Petroleum Institute\n\nVietnam Petroleum Institute (VPI) was established on May 22, 1978. It is based in Hanoi, with a branch in Ho Chi Minh City.\n\nThe Institute's assigned functions and tasks are as follows:\n"}
