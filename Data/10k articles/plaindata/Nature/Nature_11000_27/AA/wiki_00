{"id": "47602799", "url": "https://en.wikipedia.org/wiki?curid=47602799", "title": "1-Hydroxypyrene", "text": "1-Hydroxypyrene\n\n1-Hydroxypyrene is a human metabolite. It can be found in urine of outdoor workers exposed to air pollution.\n\nExperiments in pig show that urinary 1-hydroxypyrene is a metabolite of pyrene, when given orally.\n\nA \"Mycobacterium sp.\" strain isolated from mangrove sediments produced 1-hydroxypyrene during the degradation of pyrene.\n\nHighly significant differences and dose-response relationships with regard to cigarettes smoked per day were found for 2-, 3- and 4-hydroxyphenanthrene and 1-hydroxypyrene, but not for 1-hydroxyphenanthrene.\n"}
{"id": "7786686", "url": "https://en.wikipedia.org/wiki?curid=7786686", "title": "2340 Hathor", "text": "2340 Hathor\n\n2340 Hathor (), provisional designation , is an eccentric stony asteroid, classified as near-Earth object and potentially hazardous asteroid. It belongs to the Aten group of asteroids and measures approximately 210 meters in diameter. Discovered by Charles Kowal in 1976, it was later named after Hathor from Egyptian mythology.\n\n\"Hathor\" was discovered on 22 October 1976, by American astronomer Charles Kowal at Palomar Observatory, California, United States. It was independently discovered by Eleanor Helin and is named for the ancient Egyptian deity Hathor.\n\nOn 25 October 1976, \"Hathor\" was independently discovered by Eleanor Helin during the Palomar Planet-Crossing Asteroid Survey (PCAS), and by William Lawrence Sebok, who photographed the same field almost simultaneously using Palomars 1.22-meter Schmidt telescope. On the same day, the official discoverer Charles Kowal found that \"Hathor\" had already been imaged three days earlier by Palomars 0.46-meter telescope (the same instrument used by PCAS). A forth independent discovery was made several days later by Nikolai Chernykh at CrAO on the Crimean peninsula.\n\nThe multiple discoveries were probably due to its very close approach distance to Earth. After 2062 Aten, \"Hathor\" was the second discovery of an Aten asteroid. In 1978, the third Aten, 2100 Ra-Shalom was discovered. The Aten was already identified at Palomar in 1954, but its discovery date was later assigned to a 2003-observation at Lincoln Laboratory ETS, and is now known as .\n\nBeing a member of the Aten asteroids, \"Hathor\" orbits the Sun at a distance of 0.5–1.2 AU once every 0 years and 9 months (283 days). Its orbit has an eccentricity of 0.45 and an inclination of 6° with respect to the ecliptic. Its observation arc begins 3 days after its official discovery at Palomar, with no precoveries taken and no prior identifications made. Its orbital solution includes non-gravitational forces.\n\n\"Hathor\" has an \"Earth Minimum orbit intersection distance\" , which corresponds to 2.7 lunar distances (LD).\n\nWhen it was discovered in 1976, \"Hathor\" had one of its closest approaches to Earth at . On 21 October 2014, when it passed Earth at 0.048 AU, or 18.8 LD, it was observed 22 times by the Goldstone Deep Space Network using radar astronomy over a period of 21 days from 10 to 31 October. It will pass Earth again at on 21 October 2069.\n\nIn the Tholen and SMASS taxonomy, \"Hathor\" has a CSU and Sq spectral type, respectively.\n\nIn the 1990s, Dutch–American astronomer Tom Gehrels estimated \"Hathor\"s diameter to measure approximately 300 meters, assuming an albedo of 0.15. During its close approach to Earth in October 2014, a team of astronomer published a revised estimate of meters for its diameter. The \"Collaborative Asteroid Lightcurve Link\" adopts this diameter and derives an albedo of 0.3331 with an absolute magnitude of 20.2.\n\nIn November 2014, American astronomer Brian Warner obtained a rotational lightcurve of \"Hathor\" from photometric observations taken at the Palmer Divide Station in Colorado \"(also see )\". Light-curve analysis gave a well-defined rotation period of 3.350 hours with a brightness variation of 0.11 magnitude ().\n\nIn accordance with the custom to name all members of the Aten group after Ancient Egyptian deities, this minor planet is named for Hathor, sky-goddess and daughter of Ra, who personified the principles of joy, feminine love, and motherhood. The Ancient Greeks sometimes identified Hathor with the goddess Aphrodite. Naming was proposed by Eleanor Helin who also participated in the 1981-recovery. The minor planet 161 Athor is also named for Hathor. The official naming citation was published by the Minor Planet Center on 1 June 1981 ().\n\n"}
{"id": "16601110", "url": "https://en.wikipedia.org/wiki?curid=16601110", "title": "B Aquarii", "text": "B Aquarii\n\nb Aquarii refers to several different astronomical objects:\n\n"}
{"id": "880868", "url": "https://en.wikipedia.org/wiki?curid=880868", "title": "Büyük Menderes River", "text": "Büyük Menderes River\n\nThe Büyük Menderes River (historically the Maeander or Meander, from Ancient Greek: Μαίανδρος, \"Maíandros\"; ), is a river in southwestern Turkey. It rises in west central Turkey near Dinar before flowing west through the Büyük Menderes graben until reaching the Aegean Sea in the proximity of the ancient Ionian city Miletus. The word \"meander\" is used to describe a winding pattern, after the river.\n\nThe river rises in a spring near Dinar and flows to Lake Işıklı. After passing the Adıgüzel Dam and the Cindere Dam, the river flows past Nazilli, Aydın and Söke before it drains into the Aegean Sea.\n\nThe Maeander was a celebrated river of Caria in Asia Minor. It appears earliest in the Catalog of Trojans of Homer's Iliad along with Miletus and Mycale.\n\nThe river has its sources not far from Celaenae in Phrygia (now Dinar), where it gushed forth in a park of Cyrus. According to some its sources were the same as those of the river Marsyas; but this is irreconcilable with Xenophon, according to whom the sources of the two rivers were only near each other, the Marsyas rising in a royal palace. Others state that the Maeander flowed out of a lake on Mount Aulocrene. William Martin Leake reconciles all these apparently different statements by the remark that both the Maeander and the Marsyas have their origin in the lake on Mount Aulocrene, above Celaenae, but that they issue at different parts of the mountain below the lake.\n\nThe Maeander was so celebrated in antiquity for its numerous windings, that its classical name \"Maeander\" became, and still is, proverbial. Its whole course has a southwesterly direction on the south of the range of Mount Messogis. South of Tripolis it receives the waters of the Lycus, whereby it becomes a river of some importance. Near Carura it passes from Phrygia into Caria, where it flows in its tortuous course through the Maeandrian plain, and finally discharges itself in the Gulf of Icaros (an arm of the Aegean Sea), between Priene and Myus, opposite to the Ionian city of Miletus, from which its mouth is only 10 stadia distant.\n\nThe tributaries of the Maeander include the Orgyas, Marsyas, Cludrus, Lethaeus, and Gaeson, in the north; and the Obrimas, Lycus, Harpasus, and a second Marsyas in the south.\n\nThe Maeander is a deep river, but not very broad. In many parts its depth equals its breadth and, so, it is navigable only by small craft. It frequently overflows its banks and, as a result of the quantity of mud it deposits at its mouth, the coast has been pushed about 20 or 30 stadia further into the sea and several small islands off the coast have become united with the mainland.\n\nThe associated river god was also called Meander, one of the sons of Oceanus and Tethys.\n\nThere was a legend about a subterranean connection between the Maeander and the Alpheus River in Elis.\n\n\n"}
{"id": "3038633", "url": "https://en.wikipedia.org/wiki?curid=3038633", "title": "Camanchaca", "text": "Camanchaca\n\nCamanchacas are cloud banks that form on the Chilean coast, by the Earth's driest desert, the Atacama Desert, and move inland. (In Peru, camanchaca is called garúa) On the side of the mountains where these cloud banks form, the camanchaca is a dense fog that does not produce rain. The moisture that makes up the cloud measure between 1 and 40 microns across, too fine to form rain droplets.\n\nIn 1985, scientists devised a fog collection system of polyolefin netting to capture the water droplets in the fog to produce running water for villages in these otherwise desert areas. The Camanchacas Project installed 50 large fog-collecting nets on a mountain ridge, which capture some 2% of the water in the fog.\n\nIn 2005, another installation of panels of producing per square meter per day.\n"}
{"id": "75820", "url": "https://en.wikipedia.org/wiki?curid=75820", "title": "Cork (material)", "text": "Cork (material)\n\nCork is an impermeable buoyant material, the phellem layer of bark tissue that is harvested for commercial use primarily from \"Quercus suber\" (the cork oak), which is endemic to southwest Europe and northwest Africa. Cork is composed of suberin, a hydrophobic substance. Because of its impermeable, buoyant, elastic, and fire retardant properties, it is used in a variety of products, the most common of which is wine stoppers. The montado landscape of Portugal produces approximately half of cork harvested annually worldwide, with Corticeira Amorim being the leading company in the industry. Cork was examined microscopically by Robert Hooke, which led to his discovery and naming of the cell.\n\nThere are about 2,200,000 hectares of cork forest worldwide; 34% in Portugal and 27% in Spain.\nAnnual production is about 200,000 tons; 49.6% from Portugal, 30.5% from Spain, 5.8% from Morocco, 4.9% from Algeria, 3.5% from Tunisia, 3.1% Italy, and 2.6% from France.\nOnce the trees are about 25 years old the cork is traditionally stripped from the trunks every nine years, with the first two harvests generally producing lower quality cork. The trees live for about 300 years.\n\nThe cork industry is generally regarded as environmentally friendly. Cork production is generally considered sustainable because the cork tree is not cut down to obtain cork; only the bark is stripped to harvest the cork. The tree continues to live and grow. The sustainability of production and the easy recycling of cork products and by-products are two of its most distinctive aspects. Cork oak forests also prevent desertification and are a particular habitat in the Iberian Peninsula and the refuge of various endangered species.\n\nCarbon footprint studies conducted by Corticeira Amorim, Oeneo Bouchage of France and the Cork Supply Group of Portugal concluded that cork is the most environmentally friendly wine stopper in comparison to other alternatives. The Corticeira Amorim’s study, in particular (\"Analysis of the life cycle of Cork, Aluminum and Plastic Wine Closures\"), was developed by PricewaterhouseCoopers, according to ISO 14040. Results concluded that, concerning the emission of greenhouse gases, each plastic stopper released 10 times more CO, whilst an aluminium screw cap releases 26 times more CO than does a cork stopper.\n\nThe cork oak is unrelated to the \"cork trees\" (\"Phellodendron\"), which have corky bark but are not used for cork production.\n\nCork is extracted only from early May to late August, when the cork can be separated from the tree without causing permanent damage. When the tree reaches 25–30 years of age and about 24 in (60 cm) in circumference, the cork can be removed for the first time. However, this first harvest almost always produces poor quality or \"virgin\" cork (Portuguese \"cortiça virgem\"; Spanish \"corcho bornizo\" or \"corcho virgen\"). Bark from initial harvests can be used to make flooring, shoes, insulation and other industrial products. Subsequent extractions usually occur at intervals of 9 years, though it can take up to 13 for the cork to reach an acceptable size. If the product is of high quality it is known as \"gentle\" cork (Portuguese \"cortiça amadia\", but also \"cortiça secundeira\" only if it is the second time; Spanish \"corcho segundero\", also restricted to the \"second time\"), and, ideally, is used to make stoppers for wine and champagne bottles.\n\nThe workers who specialize in removing the cork are known as extractors. An extractor uses a very sharp axe to make two types of cuts on the tree: one horizontal cut around the plant, called a crown or necklace, at a height of about 2–3 times the circumference of the tree, and several vertical cuts called rulers or openings. This is the most delicate phase of the work because, even though cutting the cork requires significant force, the extractor must not damage the underlying phellogen or the tree will be harmed.\nTo free the cork from the tree, the extractor pushes the handle of the axe into the rulers. A good extractor needs to use a firm but precise touch in order to free a large amount of cork without damaging the product or tree.\n\nThese freed portions of the cork are called planks. The planks are usually carried off by hand since cork forests are rarely accessible to vehicles. The cork is stacked in piles in the forest or in yards at a factory and traditionally left to dry, after which it can be loaded onto a truck and shipped to a processor.\n\nCork's elasticity combined with its near-impermeability makes it suitable as a material for bottle stoppers, especially for wine bottles. Cork stoppers represent about 60% of all cork based production. Cork has an almost zero Poisson's ratio, which means the radius of a cork does not change significantly when squeezed or pulled.\n\nCork is an excellent gasket material. Some carburetor float bowl gaskets are made of cork, for example.\n\nCork is also an essential element in the production of badminton shuttlecocks.\n\nCork's bubble-form structure and natural fire retardant make it suitable for acoustic and thermal insulation in house walls, floors, ceilings and facades. The by-product of more lucrative stopper production, corkboard is gaining popularity as a non-allergenic, easy-to-handle and safe alternative to petrochemical-based insulation products.\n\nSheets of cork, also often the by-product of stopper production, are used to make bulletin boards as well as floor and wall tiles.\n\nCork's low density makes it a suitable material for fishing floats and buoys, as well as handles for fishing rods (as an alternative to neoprene).\n\nGranules of cork can also be mixed into concrete. The composites made by mixing cork granules and cement have lower thermal conductivity, lower density and good energy absorption. Some of the property ranges of the composites are density (400–1500 kg/m³), compressive strength (1–26 MPa) and flexural strength (0.5–4.0 MPa).\n\nAs late as the mid-17th century, French vintners did not use cork stoppers, using instead oil-soaked rags stuffed into the necks of bottles.\n\nWine corks can be made of either a single piece of cork, or composed of particles, as in champagne corks; corks made of granular particles are called \"agglomerated corks\".\n\nNatural cork closures are used for about 80% of the 20 billion bottles of wine produced each year. After a decline in use as wine-stoppers due to the increase in the use of synthetic alternatives, cork wine-stoppers are making a comeback and currently represent approximately 60% of wine-stoppers today.\nBecause of the cellular structure of cork, it is easily compressed upon insertion into a bottle and will expand to form a tight seal. The interior diameter of the neck of glass bottles tends to be inconsistent, making this ability to seal through variable contraction and expansion an important attribute. However, unavoidable natural flaws, channels, and cracks in the bark make the cork itself highly inconsistent. In a 2005 closure study, 45% of corks showed gas leakage during pressure testing both from the sides of the cork as well as through the cork body itself.\n\nSince the mid-1990s, a number of wine brands have switched to alternative wine closures such as plastic stoppers, screw caps, or other closures. During 1972 more than half of the Australian bottled wine went bad due to corking. A great deal of anger and suspicion was directed at Portuguese and Spanish cork suppliers who were suspected of deliberately supplying bad cork to non-EEC wine makers to help prevent cheap imports. Cheaper wine makers developed the aluminium \"Spelvin\" cap with a polypropylene stopper wad. More expensive wines and carbonated varieties continued to use cork, although much closer attention was paid to the quality. Even so, some high premium makers prefer the Spelvin as it is a guarantee that the wine will be good even after many decades of ageing. Some consumers may have conceptions about screw caps being representative of lower quality wines, due to their cheaper price; however, in Australia, for example, much of the non-sparkling wine production now uses these Spelvin caps as a cork alternative, although some have recently switched back to cork citing issues using screw caps. These alternatives to cork have both advantages and disadvantages. For example, screwtops are generally considered to offer a trichloroanisole (TCA) free seal, but they also reduce the oxygen transfer rate between the bottle and the atmosphere to almost zero, which can lead to a reduction in the quality of the wine. TCA is the main documented cause of cork taint in wine. However, some in the wine industry say natural cork stoppers are important because they allow oxygen to interact with wine for proper aging, and are best suited for wines purchased with the intent to age. Stoppers which resemble natural cork very closely can be made by isolating the suberin component of the cork from the undesirable lignin, mixing it with the same substance used for contact lenses and an adhesive, and molding it into a standardized product, free of TCA or other undesirable substances. Composite corks with real cork veneers are used in cheaper wines.\n\nThe study \"Analysis of the life cycle of Cork, Aluminum and Plastic Wine Closures,\" conducted by PricewaterhouseCoopers and commissioned by a major cork manufacturer, Amorim, concluded that cork is the most environmentally responsible stopper, in a one-year life cycle analysis comparison with plastic stoppers and aluminum screw caps.\n\nCork is used in musical instruments, particularly woodwind instruments, where it is used to fasten together segments of the instrument, making the seams airtight. Low quality conducting baton handles are also often made out of cork.\n\nIt is also used in shoes, especially those using welt construction to improve climate control and comfort.\n\nBecause it is impermeable and moisture-resistant, cork is often used as an alternative to leather in handbags, wallets and other fashion items.\n\nCork can be used to make bricks for the outer walls of houses, as in Portugal's pavilion at Expo 2000.\n\nOn November 28, 2007, the Portuguese national postal service CTT issued the world's first postage stamp made of cork.\n\nCork is used as the core of both baseballs and cricket balls. A corked bat is made by replacing the interior of a baseball bat with cork – a practice known as \"corking\". It was historically a method of cheating at baseball; the efficacy of the practice is now discredited.\n\nCork is often used, in various forms, in spacecraft heat shields and fairings.\n\nCork can be used in the paper pick-up mechanisms in inkjet and laser printers.\n\nCork is used to make later-model pith helmets.\n\nCorks are also hung from hats to keep insects away. (\"See \"cork hat)\n\nCork has been used as a core material in sandwich composite construction.\n\nCork can be used as the friction lining material of an automatic transmission clutch, as designed in certain mopeds.\n\nCork can be used instead of wood or aluminium in automotive interiors.\n\nCork can also be used to make watch bands and faces as seen with Sprout Watches.\n\nCork slabs are sometimes used by orchid growers as a natural mounting material.\n\nCork paddles are used by glass blowers to manipulate and shape hot molten glass.\n\n\n\n"}
{"id": "302252", "url": "https://en.wikipedia.org/wiki?curid=302252", "title": "Council communism", "text": "Council communism\n\nCouncil communism (also councilism) is a current of socialist thought that emerged in the 1920s. Inspired by the November Revolution, councilism was characterized by its opposition to state capitalism/state socialism and its advocacy of workers' councils and soviet democracy as the basis for dismantling the class state. Strong in Germany and the Netherlands during the 1920s, council communism continues to exist today within the greater socialist and communist movements.\n\nChief among the tenets of council communism is its opposition to the party vanguardism and democratic centralism of Leninist ideologies and its contention that democratic workers' councils arising in the factories and municipalities are the natural form of working class organization and authority. Council Communism also stands in contrast to social democracy through its formal rejection of both reformism and \"parliamentarism\" (i.e. the compromises and \"quids pro quo\", such as logrolling, normally found in legislative politics).\n\nCouncil Communists maintain that the working class should not rely on Leninist vanguard parties or hope for reforms of the capitalist system to bring socialism. It is viewed that worker's revolution will not be led by a \"revolutionary\" political party since these parties will only later create a party dictatorship, many point to the Bolshevik party in the October Revolution as an example, claiming that the party only became the capitalist class that replaced the old aristocratic feudal class. Revolutionary political parties will only agitate for revolution and worker's councils. These worker's councils which form during periods of struggle are believed to be the natural organizations of the working class. Democratic worker's councils will coordinate the functions of a society rather than a bureaucracy found in state socialist societies. Because of these beliefs Council Communists have been compared to Anarchists and Syndicalists.\n\nAs the Second International decayed at the beginning of World War I, socialists who opposed nationalism and supported proletarian internationalism regrouped. In Germany, two major communist trends emerged. First, the Spartacus League was created by the radical socialist Rosa Luxemburg. The second trend emerged among German rank-and-file trade unionists who opposed their unions and organized increasingly radical strikes towards the end of 1917 and the beginning of 1918. This second trend created the German Left Communist movement that would become the KAPD after the abortive German Revolution of 1918–19.\n\nAs the Communist International inspired by the Bolshevik revolution in Russia formed, a Left Communist tendency developed in the Comintern's German, Dutch and Bulgarian sections. Key figures in this milieu were Anton Pannekoek, Otto Rühle and Herman Gorter. In the United Kingdom, Sylvia Pankhurst's group, the Communist Party (British Section of the Third International), also identified with the Left Communist tendency.\n\nAlongside these formal Left Communist tendencies, the Italian group led by Amadeo Bordiga is often commonly recognized as a Left Communist party, although both Bordiga and the Italian Communist Left disputed this and qualified their politics as separate, distinct and more in line with the Third International's positions than the politics of Left Communism. Bordiga himself did not advocate abstention from the unions, although later Italian Left currents developed a critique of the \"regime unions\", positing that most or all unions had become tools of capitalism by submitting themselves to bourgeois interests and were no longer viable organs of class struggle. Nevertheless, those \"Bordigists\" who put forward this critique still held out the necessity of \"red unions\" or \"class unions\" re-emerging, outside and against the regime unions, which would openly advocate class struggle and allow the participation of communist militants.\n\nThese various assorted groups were all criticized by Vladimir Lenin in his booklet \"\".\n\nDespite a common general direction, and despite sharing the criticism of Lenin, there were few politics held in common between these movements. An example of this divergence is that the Italians supported the \"Right of Nations to Self Determination\", while the Dutch and Germans rejected this policy (seeing it as a form of bourgeois nationalism). However, all of the Left Communist tendencies opposed what they called \"Frontism\". Frontism was a tactic endorsed by Lenin, where Communists sought tactical agreements with reformist (social democratic) parties in pursuit of a definite, usually defensive, goal. In addition to opposing \"Frontism\", the Dutch-German tendency, the Bulgarians and British also refused to participate in bourgeois elections, which they denounced as parliamentarism.\n\nIn Germany, the Left Communists were expelled from the Communist Party of Germany, and they formed the Communist Workers Party (KAPD). Similar parties were formed in the Netherlands, Bulgaria and Britain. The KAPD rapidly lost most of its members and it eventually dissolved. However, some of its militants had been instrumental in organising factory-based unions like the AAUD and AAUD-E, the latter being opposed to separate party organisation (see: Syndicalism).\n\nThe leading theoreticians of the KAPD had developed a new series of ideas based on their opposition to party organisation, and their conception of the Bolshevik revolution in Russia as having been a bourgeois revolution. Their leading figures were Anton Pannekoek, Herman Gorter and Otto Rühle. Rühle later left the KAPD, and was one of the founders of the AAUD-E. Another leading theoretician of Council Communism was Paul Mattick, who later emigrated to the US. A minor figure in the Council Communist movement in the Netherlands was Marinus van der Lubbe, who was accused of the burning of the Reichstag in 1933 and consequently executed by the nazis after a show trial that marked the beginning of the persecution of socialist and communists in Nazi Germany.\n\nThe early councilists are followed later by the Group of Internationalist Communists, Henk Meijer, Cajo Brendel and Paul Mattick, Sr. There was a resurgence of councilist groups and ideas in the 1960s, through the Situationist International, Root and Branch in the United States, Socialisme ou Barbarie in France, and Solidarity in the UK.\n\nAlongside and sometimes connected to the councilists were the early Western Marxists, György Lukács (a council communist himself from 1918–21 or 22) and Karl Korsch (who turned to council communism in the 1930s).\n\nDuring the Russian Revolution of 1917, councils akin to those advocated by Council Communists were a significant political and organizational force; the Russian word \"soviet\" itself means council. After the success of the February Revolution, the Bolsheviks sought to capitalize on the influence of the soviets in order to boost their own popularity. Bolshevik leaders advocated the transference of authority to the soviets and the dissolution of Russian Provisional Government by means of a second revolution. When this campaign succeeded and the October Revolution occurred, the creation of the Congress of Soviets marked the beginning of a process of diminishing workers' control of the soviets, and the decisions of the Bolshevik Party acquired the full authority of the State. Thus, the new regime had developed into a one-party system, the Supreme Soviet (successor to the Congress of Soviets) had been relegated to the role of a rubber-stamp parliament, meeting just once a year to ratify decisions already made at higher levels, in most cases with no dissenting votes. Real power was concentrated in the hands of the Communist Party of the Soviet Union.\n\n\n\n"}
{"id": "4396171", "url": "https://en.wikipedia.org/wiki?curid=4396171", "title": "Earth's rotation", "text": "Earth's rotation\n\nEarth's rotation is the rotation of Planet Earth around its own axis. Earth rotates eastward, in prograde motion. As viewed from the north pole star Polaris, Earth turns counter clockwise.\n\nThe North Pole, also known as the Geographic North Pole or Terrestrial North Pole, is the point in the Northern Hemisphere where Earth's axis of rotation meets its surface. This point is distinct from Earth's North Magnetic Pole. The South Pole is the other point where Earth's axis of rotation intersects its surface, in Antarctica.\n\nEarth rotates once in about 24 hours with respect to the Sun, but once every 23 hours, 56 minutes, and 4 seconds with respect to the stars (see below). Earth's rotation is slowing slightly with time; thus, a day was shorter in the past. This is due to the tidal effects the Moon has on Earth's rotation. Atomic clocks show that a modern day is longer by about 1.7 milliseconds than a century ago, slowly increasing the rate at which UTC is adjusted by leap seconds. Analysis of historical astronomical records shows a slowing trend of about 2.3 milliseconds per century since the 8th century BCE.\n\nAmong the ancient Greeks, several of the Pythagorean school believed in the rotation of the earth rather than the apparent diurnal rotation of the heavens. Perhaps the first was Philolaus (470–385 BCE), though his system was complicated, including a counter-earth rotating daily about a central fire.\n\nA more conventional picture was that supported by Hicetas, Heraclides and Ecphantus in the fourth century BCE who assumed that the earth rotated but did not suggest that the earth revolved about the sun. In the third century BCE, Aristarchus of Samos suggested the sun's central place.\n\nHowever, Aristotle in the fourth century BCE criticized the ideas of Philolaus as being based on theory rather than observation. He established the idea of a sphere of fixed stars that rotated about the earth. This was accepted by most of those who came after, in particular Claudius Ptolemy (2nd century CE), who thought the earth would be devastated by gales if it rotated.\n\nIn 499 CE, the Indian astronomer Aryabhata wrote that the spherical earth rotates about its axis daily, and that the apparent movement of the stars is a relative motion caused by the rotation of the Earth. He provided the following analogy: \"Just as a man in a boat going in one direction sees the stationary things on the bank as moving in the opposite direction, in the same way to a man at Lanka the fixed stars appear to be going westward.\"\n\nIn the 10th century, some Muslim astronomers accepted that the Earth rotates around its axis. According to al-Biruni, Abu Sa'id al-Sijzi (d. circa 1020) invented an astrolabe called \"al-zūraqī\" based on the idea believed by some of his contemporaries \"that the motion we see is due to the Earth's movement and not to that of the sky.\" The prevalence of this view is further confirmed by a reference from the 13th century which states: \"According to the geometers [or engineers] (\"muhandisīn\"), the earth is in constant circular motion, and what appears to be the motion of the heavens is actually due to the motion of the earth and not the stars.\" Treatises were written to discuss its possibility, either as refutations or expressing doubts about Ptolemy's arguments against it. At the Maragha and Samarkand observatories, the Earth's rotation was discussed by Tusi (b. 1201) and Qushji (b. 1403); the arguments and evidence they used resemble those used by Copernicus.\n\nIn medieval Europe, Thomas Aquinas accepted Aristotle's view and so, reluctantly, did John Buridan and Nicole Oresme in the fourteenth century. Not until Nicolaus Copernicus in 1543 adopted a heliocentric world system did the contemporary understanding of earth's rotation begin to be established. Copernicus pointed out that if the movement of the earth is violent, then the movement of the stars must be very much more so. He acknowledged the contribution of the Pythagoreans and pointed to examples of relative motion. For Copernicus this was the first step in establishing the simpler pattern of planets circling a central sun.\n\nTycho Brahe, who produced accurate observations on which Kepler based his laws, used Copernicus's work as the basis of a system assuming a stationary earth. In 1600, William Gilbert strongly supported the earth's rotation in his treatise on the earth's magnetism and thereby influenced many of his contemporaries. Those like Gilbert who did not openly support or reject the motion of the earth about the sun are often called \"semi-Copernicans\". A century after Copernicus, Riccioli disputed the model of a rotating earth due to the lack of then-observable eastward deflections in falling bodies; such deflections would later be called the Coriolis effect. However, the contributions of Kepler, Galileo and Newton gathered support for the theory of the rotation of the Earth.\n\nEarth's rotation implies that the Equator bulges and the geographical poles are flattened. In his \"Principia\", Newton predicted this flattening would occur in the ratio of 1:230, and pointed to the pendulum measurements taken by Richer in 1673 as corroboration of the change in gravity, but initial measurements of meridian lengths by Picard and Cassini at the end of the 17th century suggested the opposite. However, measurements by Maupertuis and the French Geodesic Mission in the 1730s established the oblateness of Earth, thus confirming the positions of both Newton and Copernicus.\n\nIn the Earth's rotating frame of reference, a freely moving body follows an apparent path that deviates from the one it would follow in a fixed frame of reference. Because of the Coriolis effect, falling bodies veer slightly eastward from the vertical plumb line below their point of release, and projectiles veer right in the Northern Hemisphere (and left in the Southern) from the direction in which they are shot. The Coriolis effect is mainly observable at a meteorological scale, where it is responsible for the opposite directions of cyclone rotation in the Northern and Southern hemispheres (anticlockwise and clockwise, respectively).\n\nHooke, following a suggestion from Newton in 1679, tried unsuccessfully to verify the predicted eastward deviation of a body dropped from a height of , but definitive results were obtained later, in the late 18th and early 19th century, by Giovanni Battista Guglielmini in Bologna, Johann Friedrich Benzenberg in Hamburg and Ferdinand Reich in Freiberg, using taller towers and carefully released weights. A ball dropped from a height of departed by from the vertical compared with a calculated value of .\n\nThe most celebrated test of Earth's rotation is the Foucault pendulum first built by physicist Léon Foucault in 1851, which consisted of a lead-filled brass sphere suspended from the top of the Panthéon in Paris. Because of the Earth's rotation under the swinging pendulum, the pendulum's plane of oscillation appears to rotate at a rate depending on latitude. At the latitude of Paris the predicted and observed shift was about clockwise per hour. Foucault pendulums now swing in museums around the world.\n\nEarth's rotation period relative to the Sun (solar noon to solar noon) is its \"true solar day\" or \"apparent solar day\". It depends on the Earth's orbital motion and is thus affected by changes in the eccentricity and inclination of Earth's orbit. Both vary over thousands of years, so the annual variation of the true solar day also varies. Generally, it is longer than the mean solar day during two periods of the year and shorter during another two. The true solar day tends to be longer near perihelion when the Sun apparently moves along the ecliptic through a greater angle than usual, taking about longer to do so. Conversely, it is about shorter near aphelion. It is about longer near a solstice when the projection of the Sun's apparent motion along the ecliptic onto the celestial equator causes the Sun to move through a greater angle than usual. Conversely, near an equinox the projection onto the equator is shorter by about . Currently, the perihelion and solstice effects combine to lengthen the true solar day near by solar seconds, but the solstice effect is partially cancelled by the aphelion effect near when it is only longer. The effects of the equinoxes shorten it near and by and , respectively.\n\nThe average of the true solar day during the course of an entire year is the \"mean solar day\", which contains solar seconds. Currently, each of these seconds is slightly longer than an SI second because Earth's mean solar day is now slightly longer than it was during the 19th century due to tidal friction. The average length of the mean solar day since the introduction of the leap second in 1972 has been about 0 to 2 ms longer than 86,400 SI seconds. Random fluctuations due to core-mantle coupling have an amplitude of about 5 ms. The mean solar second between 1750 and 1892 was chosen in 1895 by Simon Newcomb as the independent unit of time in his Tables of the Sun. These tables were used to calculate the world's ephemerides between 1900 and 1983, so this second became known as the ephemeris second. In 1967 the SI second was made equal to the ephemeris second.\n\nThe apparent solar time is a measure of the Earth's rotation and the difference between it and the mean solar time is known as the equation of time.\n\nEarth's rotation period relative to the fixed stars, called its \"stellar day\" by the International Earth Rotation and Reference Systems Service (IERS), is seconds of mean solar time (UT1) , mean solar days). Earth's rotation period relative to the precessing mean vernal equinox, named \"sidereal day\", is seconds of mean solar time (UT1) , mean solar days). Thus, the sidereal day is shorter than the stellar day by about .\n\nBoth the stellar day and the sidereal day are shorter than the mean solar day by about . The mean solar day in SI seconds is available from the IERS for the periods and .\n\nRecently (1999–2010) the average annual length of the mean solar day in excess of 86,400 SI seconds has varied between and , which must be added to both the stellar and sidereal days given in mean solar time above to obtain their lengths in SI seconds (see Fluctuations in the length of day).\n\nThe angular speed of Earth's rotation in inertial space is radians per SI second (mean solar second). Multiplying by (180°/π radians) × (86,400 seconds/mean solar day) yields 360.9856°/mean solar day, indicating that Earth rotates more than 360° relative to the fixed stars in one solar day. Earth's movement along its nearly circular orbit while it is rotating once around its axis requires that Earth rotate slightly more than once relative to the fixed stars before the mean Sun can pass overhead again, even though it rotates only once (360°) relative to the mean Sun. Multiplying the value in rad/s by Earth's equatorial radius of (WGS84 ellipsoid) (factors of 2π radians needed by both cancel) yields an equatorial speed of , or . Some sources state that Earth's equatorial speed is slightly less, or . This is obtained by dividing Earth's equatorial circumference by . However, the use of only one circumference unwittingly implies only one rotation in inertial space, so the corresponding time unit must be a sidereal hour. This is confirmed by multiplying by the number of sidereal days in one mean solar day, , which yields the equatorial speed in mean solar hours given above of .\n\nThe tangential speed of Earth's rotation at a point on Earth can be approximated by multiplying the speed at the equator by the cosine of the latitude. For example, the Kennedy Space Center is located at latitude 28.59° N, which yields a speed of: cos 28.59° × = \n\nThe Earth's rotation axis moves with respect to the fixed stars (inertial space); the components of this motion are precession and nutation. It also moves with respect to the Earth's crust; this is called polar motion.\n\nPrecession is a rotation of the Earth's rotation axis, caused primarily by external torques from the gravity of the Sun, Moon and other bodies. The polar motion is primarily due to free core nutation and the Chandler wobble.\n\nOver millions of years, the Earth's rotation slowed significantly by tidal acceleration through gravitational interactions with the Moon. In this process, angular momentum is slowly transferred to the Moon at a rate proportional to formula_1, where formula_2 is the orbital radius of the Moon. This process gradually increased the length of day to its current value and resulted in the Moon being tidally locked with the Earth.\n\nThis gradual rotational deceleration is empirically documented with estimates of day lengths obtained from observations of tidal rhythmites and stromatolites; a compilation of these measurements found the length of day to increase steadily from about 21 hours at 600Myr ago to the current 24 hour value. By counting the microscopic lamina that form at higher tides, tidal frequencies (and thus day lengths) can be estimated, much like counting tree rings, though these estimates can be increasingly unreliable at older ages.\n\nThe current rate of tidal deceleration is anomalously high, implying the Earth's rotational velocity must have decreased more slowly in the past. Empirical data tentatively shows a sharp increase in rotational deceleration about 600Myr ago. Some models suggest that the Earth maintained a constant day length of 21 hours throughout much of the Precambrian. This day length corresponds to the semidiurnal resonant period of the thermally-driven atmospheric tide; at this day length, the decelerative lunar torque could have been canceled by an accelerative torque from the atmospheric tide, resulting in no net torque and a constant rotational period. This stabilizing effect could have been broken by a sudden change in global temperature. Recent computational simulations support this hypothesis and suggest the Marinoan or Sturtian glaciations broke this stable configuration about 600Myr ago, citing the resemblance of simulated results and existing paleorotational data.\n\nAdditionally, some large-scale events, such as the 2004 Indian Ocean earthquake, have caused the length of a day to shorten by 3 microseconds by affecting the Earth's moment of inertia. Post-glacial rebound, ongoing since the last Ice age, is also changing the distribution of the Earth's mass thus affecting the moment of inertia of the Earth and, by the conservation of angular momentum, the Earth's rotation period.\n\nThe primary monitoring of the Earth's rotation is performed with very-long-baseline interferometry coordinated with the Global Positioning System, satellite laser ranging, and other satellite techniques. This provides an absolute reference for the determination of universal time, precession, and nutation.\n\nThere are recorded observations of solar and lunar eclipses by Babylonian and Chinese astronomers beginning in the 8th century BCE, as well as from the medieval Islamic world and elsewhere. These observations can be used to determine changes in the Earth's rotation over the last 27 centuries, since the length of the day is a critical parameter in the calculation of the place and time of eclipses. A change in day length of milliseconds per century shows up as a change of hours and thousands of kilometers in eclipse observations. The ancient data are consistent with a shorter day, meaning the Earth was turning faster throughout the past.\n\nThe Earth's original rotation was a vestige of the original angular momentum of the cloud of dust, rocks, and gas that coalesced to form the Solar System. This primordial cloud was composed of hydrogen and helium produced in the Big Bang, as well as heavier elements ejected by supernovas. As this interstellar dust is heterogeneous, any asymmetry during gravitational accretion resulted in the angular momentum of the eventual planet.\n\nHowever, if the giant-impact hypothesis for the origin of the Moon is correct, this primordial rotation rate would have been reset by the Theia impact 4.5 billion years ago. Regardless of the speed and tilt of the Earth's rotation before the impact, it would have experienced a day some five hours long after the impact. Tidal effects would then have slowed this rate to its modern value.\n\n"}
{"id": "10531", "url": "https://en.wikipedia.org/wiki?curid=10531", "title": "El Niño", "text": "El Niño\n\nEl Niño (; ) is the warm phase of the El Niño Southern Oscillation (commonly called ENSO) and is associated with a band of warm ocean water that develops in the central and east-central equatorial Pacific (between approximately the International Date Line and 120°W), including off the Pacific coast of South America. El Niño Southern Oscillation refers to the cycle of warm and cold temperatures, as measured by sea surface temperature (SST) of the tropical central and eastern Pacific Ocean. El Niño is accompanied by high air pressure in the western Pacific and low air pressure in the eastern Pacific. The cool phase of ENSO is called \"La Niña\" with SST in the eastern Pacific below average and air pressures high in the eastern and low in western Pacific. The ENSO cycle, both El Niño and La Niña, cause global changes of both temperatures and rainfall.\n\nDeveloping countries that are dependent upon agriculture and fishing, particularly those bordering the Pacific Ocean, are usually most affected. In American Spanish, the capitalized term \"El Niño\" refers to \"the boy\", so named because the pool of warm water in the Pacific near South America is often at its warmest around Christmas. The original name, \"El Niño de Navidad\", traces its origin centuries back to Peruvian fishermen, who named the weather phenomenon in reference to the newborn Christ. \"La Niña\", chosen as the 'opposite' of El Niño, literally translates to \"the girl\".\n\nOriginally the term \"El Niño\" applied to an annual weak warm ocean current that ran southwards along the coast of Peru and Ecuador at about Christmas time. However, over time the term has evolved and now refers to the warm and negative phase of the El Niño Southern Oscillation and is the warming of the ocean surface or above-average sea surface temperatures in either the central and eastern tropical Pacific Ocean. This warming causes a shift in the atmospheric circulation with rainfall becoming reduced over Indonesia and Australia, while rainfall and tropical cyclone formation increases over the tropical Pacific Ocean. The low-level surface trade winds, which normally blow from east to west along the equator, either weaken or start blowing from the other direction.\nHistorically, El Niño events are thought to have been occurring for thousands of years. For example, it is thought that El Niño affected the Moche in modern-day Peru, who sacrificed humans in order to try to prevent the rains. Scientists have also found the chemical signatures of warmer sea surface temperatures and increased rainfall caused by El Niño in coral specimens that are around 13,000 years old. In around 1525 when Francisco Pizarro made landfall on Peru, he noted rainfall occurring in the deserts which subsequently became the first written record of the impacts of El Niño. Modern day research and reanalysis techniques have managed to find at least 26 El Niño events since 1900, with the 1982-83, 1997–98 and 2014–16 events among the strongest on record.\n\nCurrently, each country has a different threshold for what constitutes an El Niño event, which is tailored to their specific interests. For example, the Australian Bureau of Meteorology looks at the trade winds, SOI, weather models and sea surface temperatures in the Nino 3 and 3.4 regions, before declaring an El Niño. The United States Climate Prediction Center (CPC) and the International Research Institute for Climate and Society (IRI) looks at the sea surface temperatures in the Niño 3.4 region, the tropical Pacific atmosphere and forecasts that NOAA's Oceanic Niño Index will equal or exceed +0.5 °C for several seasons in a row. However, the Japan Meteorological Agency declares that an El Niño event has started when the average 5 month sea surface temperature deviation for the NINO.3 region, is over warmer for 6 consecutive months or longer. The Peruvian government declares that a coastal El Niño is under way if the sea surface temperatures in the Niño 1 and 2 regions equal or exceed +0.4 °C for at least 3 months.\n\nThere is no consensus on if climate change will have any influence on the occurrence, strength or duration of El Niño events, as research supports El Niño events becoming stronger, longer, shorter and weaker.\n\nA timeline of all the El Niño episodes between 1900 and 2016.\nEl Niño events are thought to have been occurring for thousands of years. For example, it is thought that El Niño affected the Moche in modern-day Peru, who sacrificed humans in order to try to prevent the rains.\n\nIt is thought that there have been at least 30 El Niño events since 1900, with the 1982–83, 1997–98 and 2014–16 events among the strongest on record. Since 2000, El Niño events have been observed in 2002–03, 2004–05, 2006–07, 2009–10 and 2014–16.\n\nMajor ENSO events were recorded in the years 1790–93, 1828, 1876–78, 1891, 1925–26, 1972–73, 1982–83, 1997–98, and 2014–16.\n\nTypically, this anomaly happens at irregular intervals of two to seven years, and lasts nine months to two years. The average period length is five years. When this warming occurs for seven to nine months, it is classified as El Niño \"conditions\"; when its duration is longer, it is classified as an El Niño \"episode\".\n\nThere is no consensus on whether climate change will have any influence on the occurrence, strength or duration of El Niño events, as research supports El Niño events becoming stronger, longer, shorter and weaker.\n\nDuring strong El Niño episodes, a secondary peak in sea surface temperature across the far eastern equatorial Pacific Ocean sometimes follows the initial peak.\n\nENSO conditions have occurred at two- to seven-year intervals for at least the past 300 years, but most of them have been weak. Evidence is also strong for El Niño events during the early Holocene epoch 10,000 years ago.\n\nEl Niño may have led to the demise of the Moche and other pre-Columbian Peruvian cultures. A recent study suggests a strong El-Niño effect between 1789 and 1793 caused poor crop yields in Europe, which in turn helped touch off the French Revolution. The extreme weather produced by El Niño in 1876–77 gave rise to the most deadly famines of the 19th century. The 1876 famine alone in northern China killed up to 13 million people.\n\nAn early recorded mention of the term \"El Niño\" to refer to climate occurred in 1892, when told the geographical society congress in Lima that Peruvian sailors named the warm north-flowing current \"El Niño\" because it was most noticeable around Christmas. The phenomenon had long been of interest because of its effects on the guano industry and other enterprises that depend on biological productivity of the sea.\n\nCharles Todd, in 1888, suggested droughts in India and Australia tended to occur at the same time; Norman Lockyer noted the same in 1904. An El Niño connection with flooding was reported in 1894 by (1852–1919) and in 1895 by Federico Alfonso Pezet (1859–1929). In 1924, Gilbert Walker (for whom the Walker circulation is named) coined the term \"Southern Oscillation\". He and others (including Norwegian-American meteorologist Jacob Bjerknes) are generally credited with identifying the El Niño effect.\n\nThe major 1982–83 El Niño led to an upsurge of interest from the scientific community. The period 1991–1995 was unusual in that El Niños have rarely occurred in such rapid succession. An especially intense El Niño event in 1998 caused an estimated 16% of the world's reef systems to die. The event temporarily warmed air temperature by 1.5 °C, compared to the usual increase of 0.25 °C associated with El Niño events. Since then, mass coral bleaching has become common worldwide, with all regions having suffered \"severe bleaching\".\n\nIt is thought that there are several different types of El Niño events, with the canonical eastern Pacific and the Modoki central Pacific types being the two that receive the most attention. These different types of El Niño events are classified by where the tropical Pacific sea surface temperature (SST) anomalies are the largest. For example, the strongest sea surface temperature anomalies associated with the canonical eastern Pacific event are located off the coast of South America. The strongest anomalies associated with the Modoki central Pacific event are located near the International Dateline. However, during the duration of a single event, the area with the greatest sea surface temperature anomalies can change.\n\nThe traditional Niño, also called Eastern Pacific (EP) El Niño, involves temperature anomalies in the Eastern Pacific. However, in the last two decades, nontraditional El Niños were observed, in which the usual place of the temperature anomaly (Niño 1 and 2) is not affected, but an anomaly arises in the central Pacific (Niño 3.4). The phenomenon is called Central Pacific (CP) El Niño, \"dateline\" El Niño (because the anomaly arises near the dateline), or El Niño \"Modoki\" (Modoki is Japanese for \"similar, but different\").\n\nThe effects of the CP El Niño are different from those of the traditional EP El Niño—e.g., the recently discovered El Niño leads to more hurricanes more frequently making landfall in the Atlantic.\n\nThere is also a scientific debate on the very existence of this \"new\" ENSO. Indeed, a number of studies dispute the reality of this statistical distinction or its increasing occurrence, or both, either arguing the reliable record is too short to detect such a distinction, finding no distinction or trend using other statistical approaches, or that other types should be distinguished, such as standard and extreme ENSO.\n\nThe first recorded El Niño that originated in the central Pacific and moved toward the east was in 1986. Recent Central Pacific El Niños happened in 1986–87, 1991–92, 1994–95, 2002–03, 2004–05 and 2009–10. Furthermore, there were \"Modoki\" events in 1957–59, 1963–64, 1965–66, 1968–70, 1977–78 and 1979–80. Some sources say that the El Niños of 2006-07 and 2014-16 were also Central Pacific El Niños.\n\nEl Nino affects the global climate and disrupts normal weather patterns, which as a result can lead to intense storms in some places and droughts in others.<ref name=\"NIWA El Niño/La Niña\"></ref>\n\nMost tropical cyclones form on the side of the subtropical ridge closer to the equator, then move poleward past the ridge axis before recurving into the main belt of the Westerlies. Areas west of Japan and Korea tend to experience much fewer September–November tropical cyclone impacts during El Niño and neutral years. During El Niño years, the break in the subtropical ridge tends to lie near 130°E, which would favor the Japanese archipelago.\n\nWithin the Atlantic Ocean vertical wind shear is increased, which inhibits tropical cyclone genesis and intensification, by causing the westerly winds in the atmosphere to be stronger. The atmosphere over the Atlantic Ocean can also be drier and more stable during El Niño events, which can also inhibit tropical cyclone genesis and intensification. Within the Eastern Pacific basin: El Niño events contribute to decreased easterly vertical wind shear and favours above-normal hurricane activity. However, the impacts of the ENSO state in this region can vary and are strongly influenced by background climate patterns. The Western Pacific basin experiences a change in the location of where tropical cyclones form during El Niño events, without a major change in how many develop each year. A change in the location of where tropical cyclones form also occurs within the Southern Pacific Ocean between 135°E and 120°W, with tropical cyclones more likely to occur within the Southern Pacific basin than the Australian region. As a result of this change tropical cyclones are 50% less likely to make landfall on Queensland, while the risk of a tropical cyclone is elevated for island nations like Niue, French Polynesia, Tonga, Tuvalu and the Cook Islands.\n\nA study of climate records has shown that El Niño events in the equatorial Pacific are generally associated with a warm tropical North Atlantic in the following spring and summer. About half of El Niño events persist sufficiently into the spring months for the Western Hemisphere Warm Pool to become unusually large in summer. Occasionally, El Niño's effect on the Atlantic Walker circulation over South America strengthens the easterly trade winds in the western equatorial Atlantic region. As a result, an unusual cooling may occur in the eastern equatorial Atlantic in spring and summer following El Niño peaks in winter. Cases of El Niño-type events in both oceans simultaneously have been linked to severe famines related to the extended failure of monsoon rains.\n\nMany ENSO linkages exist in the high southern latitudes around Antarctica. Specifically, El Niño conditions result in high-pressure anomalies over the Amundsen and Bellingshausen Seas, causing reduced sea ice and increased poleward heat fluxes in these sectors, as well as the Ross Sea. The Weddell Sea, conversely, tends to become colder with more sea ice during El Niño. The exact opposite heating and atmospheric pressure anomalies occur during La Niña. This pattern of variability is known as the Antarctic dipole mode, although the Antarctic response to ENSO forcing is not ubiquitous.\n\nObservations of El Niño events since 1950, show that impacts associated with El Niño events depend on what season it is. However, while certain events and impacts are expected to occur during events, it is not certain or guaranteed that they will occur. The impacts that generally do occur during most El Niño events include below-average rainfall over Indonesia and northern South America, while above average rainfall occurs in southeastern South America, eastern equatorial Africa, and the southern United States.\n\nDuring El Niño events, the shift in rainfall away from the Western Pacific may mean that rainfall across Australia is reduced. Over the southern part of the continent, warmer than average temperatures can be recorded as weather systems are more mobile and fewer blocking areas of high pressure occur. The onset of the Indo-Australian Monsoon in tropical Australia is delayed by two to six weeks, which as a consequence means that rainfall is reduced over the northern tropics. The risk of a significant bushfire season in south-eastern Australia is higher following an El Niño event, especially when it is combined with a positive Indian Ocean Dipole event. During an El Niño event, New Zealand tends to experience stronger or more frequent westerly winds during their summer, which leads to an elevated risk of drier than normal conditions along the east coast. There is more rain than usual though on New Zealand's West Coast, because of the barrier effect of the North Island mountain ranges and the Southern Alps.\n\nFiji generally experiences drier than normal conditions during an El Niño, which can lead to drought becoming established over the Islands. However, the main impacts on the island nation is felt about a year after the event becomes established. Within the Samoan Islands, below average rainfall and higher than normal temperatures are recorded during El Niño events, which can lead to droughts and forest fires on the islands. Other impacts include a decrease in the sea level, possibility of coral bleaching in the marine environment and an increased risk of a tropical cyclone affecting Samoa.\n\nIn Africa, East Africa — including Kenya, Tanzania, and the White Nile basin — experiences, in the long rains from March to May, wetter-than-normal conditions. Conditions are also drier than normal from December to February in south-central Africa, mainly in Zambia, Zimbabwe, Mozambique, and Botswana.\n\nAs warm water spreads from the west Pacific and the Indian Ocean to the east Pacific, it takes the rain with it, causing extensive drought in the western Pacific and rainfall in the normally dry eastern Pacific. Singapore experienced the driest February in 2014 since records began in 1869, with only 6.3 mm of rain falling in the month and temperatures hitting as high as 35 °C on 26 February. The years 1968 and 2005 had the next driest Februaries, when 8.4 mm of rain fell. \n\nEl Niño's effects on Europe are controversial, complex and difficult to analyse, as it is one of several factors that influence the weather over the continent and other factors can overwhelm the signal.\nOver North America, the main temperature and precipitation impacts of El Niño, generally occur in the six months between October and March. In particular the majority of Canada generally has milder than normal winters and springs, with the exception of eastern Canada where no significant impacts occur. Within the United States, the impacts generally observed during the six-month period include; wetter-than-average conditions along the Gulf Coast between Texas and Florida, while drier conditions are observed in Hawaii, the Ohio Valley, Pacific Northwest and the Rocky Mountains. Over California and the South-Western United States, there is a weak relationship between El Nino and above-average precipitation, as it strongly depends on the strength of the El Niño event and other factors.\n\nThe synoptic condition for the Tehuantepecer is associated with high-pressure system forming in Sierra Madre of Mexico in the wake of an advancing cold front, which causes winds to accelerate through the Isthmus of Tehuantepec. Tehuantepecers primarily occur during the cold season months for the region in the wake of cold fronts, between October and February, with a summer maximum in July caused by the westward extension of the Azores High. Wind magnitude is greater during El Niño years than during La Niña years, due to the more frequent cold frontal incursions during El Niño winters. Its effects can last from a few hours to six days. Some El Niño events were recorded in the isotope signals of plants, and that had helped centifics to study his impact.\n\nBecause El Niño's warm pool feeds thunderstorms above, it creates increased rainfall across the east-central and eastern Pacific Ocean, including several portions of the South American west coast. The effects of El Niño in South America are direct and stronger than in North America. An El Niño is associated with warm and very wet weather months in April–October along the coasts of northern Peru and Ecuador, causing major flooding whenever the event is strong or extreme. The effects during the months of February, March, and April may become critical along the west coast of South America, El Niño reduces the upwelling of cold, nutrient-rich water that sustains large fish populations, which in turn sustain abundant sea birds, whose droppings support the fertilizer industry. The reduction in upwelling leads to fish kills off the shore of Peru.\n\nThe local fishing industry along the affected coastline can suffer during long-lasting El Niño events. The world's largest fishery collapsed due to overfishing during the 1972 El Niño Peruvian anchoveta reduction. During the 1982–83 event, jack mackerel and anchoveta populations were reduced, scallops increased in warmer water, but hake followed cooler water down the continental slope, while shrimp and sardines moved southward, so some catches decreased while others increased. Horse mackerel have increased in the region during warm events. Shifting locations and types of fish due to changing conditions provide challenges for fishing industries. Peruvian sardines have moved during El Niño events to Chilean areas. Other conditions provide further complications, such as the government of Chile in 1991 creating restrictions on the fishing areas for self-employed fishermen and industrial fleets.\n\nThe ENSO variability may contribute to the great success of small, fast-growing species along the Peruvian coast, as periods of low population removes predators in the area. Similar effects benefit migratory birds that travel each spring from predator-rich tropical areas to distant winter-stressed nesting areas.\n\nSouthern Brazil and northern Argentina also experience wetter than normal conditions, but mainly during the spring and early summer. Central Chile receives a mild winter with large rainfall, and the Peruvian-Bolivian Altiplano is sometimes exposed to unusual winter snowfall events. Drier and hotter weather occurs in parts of the Amazon River Basin, Colombia, and Central America.\n\nWhen El Niño conditions last for many months, extensive ocean warming and the reduction in easterly trade winds limits upwelling of cold nutrient-rich deep water, and its economic effect on local fishing for an international market can be serious.\n\nMore generally, El Niño can affect commodity prices and the macroeconomy of different countries. It can constrain the supply of rain-driven agricultural commodities; reduce agricultural output, construction, and services activities; create food-price and generalised inflation; and may trigger social unrest in commodity-dependent poor countries that primarily rely on imported food. A University of Cambridge Working Paper shows that while Australia, Chile, Indonesia, India, Japan, New Zealand and South Africa face a short-lived fall in economic activity in response to an El Niño shock, other countries may actually benefit from an El Niño weather shock (either directly or indirectly through positive spillovers from major trading partners), for instance, Argentina, Canada, Mexico and the United States. Furthermore, most countries experience short-run inflationary pressures following an El Niño shock, while global energy and non-fuel commodity prices increase. The IMF estimates a significant El Niño can boost the GDP of the United States by about 0.5% (due largely to lower heating bills) and reduce the GDP of Indonesia by about 1.0%.\n\nExtreme weather conditions related to the El Niño cycle correlate with changes in the incidence of epidemic diseases. For example, the El Niño cycle is associated with increased risks of some of the diseases transmitted by mosquitoes, such as malaria, dengue, and Rift Valley fever. Cycles of malaria in India, Venezuela, Brazil, and Colombia have now been linked to El Niño. Outbreaks of another mosquito-transmitted disease, Australian encephalitis (Murray Valley encephalitis—MVE), occur in temperate south-east Australia after heavy rainfall and flooding, which are associated with La Niña events. A severe outbreak of Rift Valley fever occurred after extreme rainfall in north-eastern Kenya and southern Somalia during the 1997–98 El Niño.\n\nENSO conditions have also been related to Kawasaki disease incidence in Japan and the west coast of the United States, via the linkage to tropospheric winds across the north Pacific Ocean.\n\nENSO may be linked to civil conflicts. Scientists at The Earth Institute of Columbia University, having analyzed data from 1950 to 2004, suggest ENSO may have had a role in 21% of all civil conflicts since 1950, with the risk of annual civil conflict doubling from 3% to 6% in countries affected by ENSO during El Niño years relative to La Niña years.\n\n\n"}
{"id": "54441065", "url": "https://en.wikipedia.org/wiki?curid=54441065", "title": "Elk Island Provincial Park", "text": "Elk Island Provincial Park\n\nElk Island Provincial Park was designated a provincial park by the Government of Manitoba in 1974. The park is in size. The park is considered to be a Class III protected area under the IUCN protected area management categories.\n\n"}
{"id": "9848870", "url": "https://en.wikipedia.org/wiki?curid=9848870", "title": "Energy efficiency in transport", "text": "Energy efficiency in transport\n\nThe energy efficiency in transport is the useful travelled distance, of passengers, goods or any type of load; divided by the total energy put into the transport propulsion means. The energy input might be rendered in several different types depending on the type of propulsion, and normally such energy is presented in liquid fuels, electrical energy or food-energy. The energy efficiency is also occasionally known as energy intensity. The inverse of the energy efficiency in transport, is the energy consumption in transport.\n\nEnergy efficiency in transport is often, and confusingly, described in terms of fuel consumption, fuel consumption being the reciprocal of fuel economy. Nonetheless, fuel consumption is linked with a means of propulsion which uses liquid fuels, whilst energy efficiency is applicable to any sort of propulsion. To avoid said confusion, and to be able to compare the energy efficiency in any type of vehicle, experts tend to measure the energy in the International System of Units, i.e., Joules.\n\nTherefore, in the International System of Units, the energy efficiency in transport is measured in terms of metre per Joule, or m/J, whilst the energy consumption in transport is measured in terms of Joules per meter, or J/m. The more efficient the vehicle, the more metres it covers with one Joule (more efficiency), or the less Joules it uses to travel over one meter (less consumption). The energy efficiency in transport largely varies by means of transport. Different types of transport range from some hundred kilojoules per kilometre (kJ/km) for a bicycle to tens of megajoules per kilometer (MJ/km) for a helicopter.\n\nVia type of fuel used and rate of fuel consumption, energy efficiency is also often related to operating cost ($/km) and environmental emissions (e.g. CO2/km).\n\nIn the International System of Units, the energy efficiency in transport is measured in terms of metre per Joule, or m/J. Nonetheless, several conversions are applicable, depending on the unit of distance and on the unit of energy. For liquid fuels, normally the quantity of energy input is measured in terms of the liquid's volume, such as litres or gallons. For propulsion which runs on electricity, normally kW·h is used, whilst for any type of human-propelled vehicle, the energy input is measured in terms of Calories. Conversions between different types of energy and units, are very well known in the art.\n\nFor passenger transport, the energy efficiency is normally measured in terms of passengers times distance per unit of energy, in the SI, passengers metres per Joule (pax.m/J); whilst for cargo transport the energy efficiency is normally measured in terms of mass of transported cargo times distance per unit of energy, in the SI, kilograms metres per Joule (kg.m/J).\n\nEnergy efficiency is expressed in terms of fuel economy:\n\nEnergy consumption (reciprocal efficiency) is expressed terms of fuel consumption:\n\nEnergy consumption:\n\nEnergy consumption:\n\nIn the following table the energy efficiency and energy consumption for different types of passenger land vehicles and modes of transport, as well as standard occupancy rates, are presented. The sources for these figures are in the correspondent section for each vehicle, in the following article. The conversions amongst different types of units, are well known in the art.\n\nFor the conversion amongst units of energy in the following table, 1 litre of gasoline amounts to 34.2 MJ, 1 kWh amounts to 3.6 MJ and 1 kilocalorie amounts to 4184 J. For the car occupation ratio, the value of 1.2 passengers per automobile was considered. Nonetheless in Europe this value slightly increases to 1.4. The sources for conversions amongst units of measurements appear only of the first row.\n\nA person walking at requires approximately of food energy per hour, which is equivalent to 4.55 km/MJ. of gasoline contains about of energy, so this is approximately equivalent to .\n\nVelomobiles have the highest energy efficiency of any known mode of personal transport because of their small frontal area and aerodynamic shape. At a speed of , the velomobile manufacturer WAW claims that only 0.5 kW·h (1.8 MJ) of energy per 100 km is needed to transport the passenger (= 18 J/m). This is around (20%) of what is needed to power a standard upright bicycle without aerodynamic cladding at same speed, and (2%) of that which is consumed by an average fossil fuel or electric car (the velomobile efficiency corresponds to 4700 miles per US gallon, 2000 km/L, or 0.05 L/100 km). Real energy from food used by human is 4-5 times more. Unfortunately their energy use advantage over bicycles is smaller with decrease of speed and disappear at around 10 km/h where power needed for velomobile and triathlon bike is almost the same \n\nA standard lightweight, moderate-speed bicycle is one of the most energy-efficient forms of transport. Compared with walking, a cyclist riding at requires about half the food energy per unit distance: 27 kcal/km, per 100 km, or 43 kcal/mi. This converts to about . This means that a bicycle will use between 10-25 times less energy per distance traveled than a personal car, depending on fuel source and size of the car. This figure does depend on the speed and mass of the rider: greater speeds give higher air drag and heavier riders consume more energy per unit distance. In addition, because bicycles are very lightweight (usually between 7–15 kg) this means they consume very low amounts of materials and energy to manufacture. In comparison to an automobile weighing 1500 kg or more, a bicycle typically requires 100-200 times less energy to produce than an automobile.\n\nA motorized bicycle allows human power and the assistance of a engine, giving a range of . Electric pedal-assisted bikes run on as little as per 100 km, while maintaining speeds in excess of . These best-case figures rely on a human doing 70% of the work, with around per 100 km coming from the motor. This makes an electric bicycle one of the most efficient possible motorized vehicles, behind only a motorized velomobile.\n\nElectric kick scooters, such as those used by scooter-sharing systems like Bird or Lime, typically have a maximum range of under and a maximum speed of roughly . Intended to fit into a last mile niche and be ridden in bike lanes, they require little skill from the rider. Because of their light weight and small motors, they're extremely energy-efficient.\n\nTo be thorough, a comparison must also consider the energy costs of producing, transporting and packaging of fuel (food or fossil fuel), the energy incurred in disposing of exhaust waste, and the energy costs of manufacturing the vehicle. This last can be significant given that walking requires little or no special equipment, while automobiles, for example, require a great deal of energy to produce and have relatively short lifespans. In addition, any comparison of electric vehicles and liquid-fuelled vehicles must include the fuel consumed in the power station to generate the electricity. In the UK for instance the efficiency of the electricity generation and distribution system is around 0.40.\n\nThe automobile is an inefficient vehicle compared to other modes of transport.\nThis is because the ratio between the mass of the vehicle and the mass of the passengers is much higher when compared to other modes of transport.\n\nAutomobile fuel efficiency is most commonly expressed in terms of the volume of fuel consumed per one hundred kilometres (L/100 km), but in some countries (including the United States, the United Kingdom and India) it is more commonly expressed in terms of the distance per volume fuel consumed (km/L or miles per gallon). \nThis is complicated by the different energy content of fuels such as petrol and diesel. \nThe Oak Ridge National Laboratory (ORNL) states that the energy content of unleaded gasoline is 115,000 British thermal unit (BTU) per US gallon (32 MJ/L) compared to 130,500 BTU per US gallon (36.4 MJ/L) for diesel.\n\nA second important consideration is the energy costs of producing energy. \nBio-fuels, electricity and hydrogen, for instance, have significant energy inputs in their production. \nHydrogen production efficiency are 50-70% when produced from natural gas, and 10-15% from electricity.\nThe efficiency of hydrogen production, as well as the energy required to store and transport hydrogen, must to be combined with the vehicle efficiency to yield net efficiency. \nBecause of this, hydrogen automobiles are one of the least efficient means of passenger transport, generally around 50 times as much energy must be put into the production of hydrogen compared to how much is used to move the car.\n\nA third consideration to take into account when calculating energy efficiency of automobiles is the occupancy rate of the vehicle. \nAlthough the consumption per unit distance per vehicle increases with increasing number of passengers, this increase is slight compared to the reduction in consumption per unit distance per passenger. \nThis means that higher occupancy yields higher energy efficiency per passenger. \nAutomobile occupancy varies across regions. For example, the estimated average occupancy rate is about 1.3 passengers per car in the San Francisco Bay Area, while the 2006 UK estimated average is 1.58.\n\nFourth, the energy needed to build and maintain roads is an important consideration, as is the energy returned on energy invested (EROEI). \nBetween these two factors, roughly 20% must be added to the energy of the fuel consumed, to accurately account for the total energy used.\n\nFinally, vehicle energy efficiency calculations would be misleading without factoring the energy cost of producing the vehicle itself. \nThis initial energy cost can of course be depreciated over the life of the vehicle to calculate an average energy efficiency over its effective life span. In other words, vehicles that take a lot of energy to produce and are used for relatively short periods will require a great deal more energy over their effective lifespan than those that do not, and are therefore much less energy efficient than they may otherwise seem. Hybrid and electric cars use less energy in their operation than comparable petroleum-fueled cars but more energy is used to manufacture them, so the overall difference would be less than immediately apparent. Compare, for example, walking, which requires no special equipment at all, and an automobile, produced in and shipped from another country, and made from parts manufactured around the world from raw materials and minerals mined and processed elsewhere again, and used for a limited number of years.\nAccording to the French energy and environment agency ADEME, an average motor car has an embodied energy content of 20,800 kWh and an average electric vehicle amounts to 34,700 kWh. The electric car requires nearly twice as much energy to produce, primarily due to the large amount of mining and purification necessary for the rare earth metals and other materials used in lithium-ion batteries and in the electric drive motors. This represents a significant portion of the energy used over the life of the car (in some cases nearly as much as energy that is used through the fuel that is consumed, effectively doubling the car's per-distance energy consumption), and cannot be ignored when comparing automobiles to other transport modes. It is important to note, also, that as these are average numbers for French automobiles and they are likely to be significantly larger in more auto-centric countries like the United States and Canada, where much larger and heavier cars are more common.\n\nDriving practices and vehicles can be modified to improve their energy efficiency by about 15%.\n\nOn a percentage basis, if there is one occupant in an automobile, between 0.4-0.6% of the total energy used is used to move the person in the car, while 99.4-99.6% (about 165 to 250 times more) is used to move the car.\n\n\nTrains are in general one of the most efficient means of transport for freight and passengers. Efficiency varies significantly with passenger loads, and losses incurred in electricity generation and supply (for electrified systems), and, importantly, end-to-end delivery, where stations are not the originating final destinations of a journey.\n\nActual consumption depends on gradients, maximum speeds, and loading and stopping patterns. Data produced for the European MEET project (Methodologies for Estimating Air Pollutant Emissions) illustrate the different consumption patterns over several track sections. The results show the consumption for a German ICE high-speed train varied from around . The data also reflects the weight of the train per passenger. For example, TGV double-deck Duplex trains use lightweight materials, which keep axle loads down and reduce damage to track and also save energy.\n\nThe specific energy consumption of the trains worldwide amounts to about 150 kJ/pkm and 150 kJ/tkm (i.e. 4,2 kWh/100 pkm and 4,2 kWh/100 tkm) in terms of final energy. Passenger transportation by rail requires less than one-tenth of the energy needed to move a person by car or plane. This is the reason why, although accounting for 9% of world passenger transportation activity (expressed in pkm) in 2015, rail passenger services represented only 1% of final energy demand in passenger transportation.\n\nEnergy consumption estimates for rail freight vary widely, and many are provided by interested parties. Some are tabulated below.\n\nStopping is a considerable source of inefficiency. Modern electric trains like the \"Shinkansen\" (the \"Bullet Train\") use regenerative braking to return current into the catenary while they brake. A Siemens study indicated that regenerative braking might recover 41.6% of the total energy consumed. The Passenger Rail (Urban and Intercity) and Scheduled Intercity and All Charter Bus Industries Technological and Operational Improvements – FINAL REPORT states that \"Commuter operations can dissipate more than half of their total traction energy in braking for stops.\" and that \"We estimate head-end power to be 35 percent (but it could possibly be as high as 45 percent) of total energy consumed by commuter railways.\" Having to accelerate and decelerate a heavy train load of people at every stop is inefficient despite regenerative braking which can recover typically around 20% of the energy wasted in braking. Weight is a determinant of braking losses.\n\n\n\nA principal determinant of energy consumption in aircraft is drag, which must be opposed by thrust for the aircraft to progress.\n\nPassenger airplanes averaged 4.8 l/100 km per passenger (1.4 MJ/passenger-km) (49 passenger-miles per gallon) in 1998. Note that on average 20% of seats are left unoccupied. Jet aircraft efficiencies are improving: Between 1960 and 2000 there was a 55% overall fuel efficiency gain (if one were to exclude the inefficient and limited fleet of the DH Comet 4 and to consider the Boeing 707 as the base case). Most of the improvements in efficiency were gained in the first decade when jet craft first came into widespread commercial use. Compared to advanced piston engine airliners of the 1950s, current jet airliners are only marginally more efficient per passenger-mile. Between 1971 and 1998 the fleet-average annual improvement per available seat-kilometre was estimated at 2.4%. Concorde the supersonic transport managed about 17 passenger-miles to the Imperial gallon; similar to a business jet, but much worse than a subsonic turbofan aircraft. Airbus puts the fuel rate consumption of their A380 at less than 3 l/100 km per passenger (78 passenger-miles per US gallon).\nThe mass of an aircraft can be reduced by using light-weight materials such as titanium, carbon fiber and other composite plastics. Expensive materials may be used, if the reduction of mass justifies the price of materials through improved fuel efficiency. The improvements achieved in fuel efficiency by mass reduction, reduces the amount of fuel that needs to be carried. This further reduces the mass of the aircraft and therefore enables further gains in fuel efficiency. For example, the Airbus A380 design includes multiple light-weight materials.\n\nAirbus has showcased wingtip devices (sharklets or winglets) that can achieve 3.5 percent reduction in fuel consumption. There are wingtip devices on the Airbus A380. Further developed Minix winglets have been said to offer 6 percent reduction in fuel consumption. Winglets at the tip of an aircraft wing smooth out the wing-tip vortex (reducing the aircraft's wing drag) and can be retrofitted to any airplane.\n\nNASA and Boeing are conducting tests on a \"blended wing\" aircraft. This design allows for greater fuel efficiency since the whole craft produces lift, not just the wings. The blended wing body (BWB) concept offers advantages in structural, aerodynamic and operating efficiencies over today's more conventional fuselage-and-wing designs. These features translate into greater range, fuel economy, reliability and life cycle savings, as well as lower manufacturing costs. NASA has created a cruise efficient STOL (CESTOL) concept.\n\nFraunhofer Institute for Manufacturing Engineering and Applied Materials Research (IFAM) have researched a shark skin imitating paint that would reduce drag through a riblet effect. Aircraft are a major potential application for new technologies such as aluminium metal foam and nanotechnology such as the shark skin imitating paint.\n\nPropfan propulsors are a more fuel efficient technology than jets or turboprops, but turboprops have an optimum speed below about 450 mph (700 km/h). This speed is less than used with jets by major airlines today. However, the decrease in speed reduces drag. With the current high price for jet fuel and the emphasis on engine/airframe efficiency to reduce emissions, there is renewed interest in the propfan concept for jetliners that might come into service beyond the Boeing 787 and Airbus A350XWB. For instance, Airbus has patented aircraft designs with twin rear-mounted counter-rotating propfans. NASA has conducted an Advanced Turboprop Project (ATP), where they researched a variable pitch propfan that produced less noise and achieved high speeds.\n\nRelated to fuel efficiency is the impact of aviation emissions on climate.\n\n\nCunard stated that Queen Elizabeth 2 travelled 49.5 feet per imperial gallon of diesel oil (3.32 m/l or 41.2 ft/US gal), and that it had a passenger capacity of 1777. Thus carrying 1777 passengers we can calculate an efficiency of 16.7 passenger miles per imperial gallon (16.9 l/100 p·km or 13.9 p·mpg).\n\n has a capacity of 6,296 passengers and a fuel efficiency of 14.4 passenger miles per US gallon. Voyager-class cruise ships have a capacity of 3,114 passengers and a fuel efficiency of 12.8 passenger miles per US gallon.\n\nEmma Maersk uses a Wärtsilä-Sulzer RTA96-C, which consumes 163 g/kW·h and 13,000 kg/h. If it carries 13,000 containers then 1 kg fuel transports one container for one hour over a distance of 45 km. The ship takes 18 days from Tanjung (Singapore) to Rotterdam (Netherlands), 11 from Tanjung to Suez, and 7 from Suez to Rotterdam, which is roughly 430 hours, and has 80 MW, +30 MW. 18 days at a mean speed of gives a total distance of .\n\nAssuming the Emma Maersk consumes diesel (as opposed to fuel oil which would be the more precise fuel) then 1 kg diesel = 0.832 litres = 0.317 US gallons. This corresponds to 46,525 kJ. Assuming a standard 14 tonnes per container (per teu) this yields 74 kJ per tonne-km at a speed of 45 kmh (24 knots).\n\nA sailboat, much like a solar car, can locomote without consuming any fuel. A sail boat such as a Dinghy using just wind power requires no input energy in terms of fuel. However some manual energy is required by the crew to steer the boat and adjust the sails using lines. In addition energy will be needed for demands other than propulsion, such as cooking, heating or lighting. The fuel efficiency of a single-occupancy boat is highly dependent on the size of its engine, the speed at which it travels, and its displacement. Due to the high viscosity of water, with a single passenger, the equivalent energy efficiency will be lower than in a car, train, or plane.\n\nRail and bus are generally required to serve 'off peak' and rural services, which by their nature have lower loads than city bus routes and inter city train lines. Moreover, due to their 'walk on' ticketing it is much harder to match daily demand and passenger numbers. As a consequence, the overall load factor on UK railways is 35% or 90 people per train:\n\nConversely, airline services generally work on point-to-point networks between large population centres and are 'pre-book' in nature. Using yield management, overall load factors can be raised to around 70–90%. Intercity train operators have begun to use similar techniques, with loads reaching typically 71% overall for TGV services in France and a similar figure for the UK's Virgin Trains services.\n\nFor emissions, the electricity generating source needs to be taken into account.\nThe US transport Energy Data Book states the following figures for passenger transport in 2009:\n\nThe US transport Energy book states the following figures for freight transport in 2010:\nFrom 1960 to 2010 the efficiency of air freight has increased 75%, mostly due to more efficient jet engines.\n\n1 gal (3.785 l, 0.833 gal) of fuel can move a ton of cargo 857 km or 462 nmi by barge, or by rail, or by truck.\n\nCompare:\n\nNatural Resources Canada's Office of Energy Efficiency publishes annual statistics regarding the efficiency of the entire Canadian fleet. For researchers, these fuel consumption estimates are more realistic than the fuel consumption ratings of new vehicles, as they represent the real world driving conditions, including extreme weather and traffic. The annual report is called Energy Efficiency Trends Analysis. There are dozens of tables illustrating trends in energy consumption expressed in energy per passenger km (passengers) or energy per tonne km (freight).\n\nThe environmental calculator of the French environment and energy agency (ADEME) published in 2007 using data from 2005 enables one to compare the different means of transport as regards the CO emissions (in terms of carbon dioxide equivalent) as well as the consumption of primary energy. In the case of an electric vehicle, the ADEME makes the assumption that 2.58 toe as primary energy are necessary for producing one toe of electricity as end energy in France (see Embodied energy#In the energy field).\n\nThis computer tool devised by the ADEME shows the importance of public transport from an environmental point of view. It highlights the primary energy consumption as well as the CO emissions due to transport. Due to the relatively low environmental impact of radioactive waste, compared to that of fossil fuel combustion emissions, this is not a factor in the tool. Moreover, intermodal passenger transport is probably a key to sustainable transport, by allowing people to use less polluting means of transport.\n\nDeutsche Bahn calculates their energy consumption for their various routes they take.\n"}
{"id": "41693315", "url": "https://en.wikipedia.org/wiki?curid=41693315", "title": "European Multidisciplinary Seafloor and water column Observatory", "text": "European Multidisciplinary Seafloor and water column Observatory\n\nEuropean Multidisciplinary Seafloor and water-column Observatory (EMSO) is a large-scale European distributed Research Infrastructure (RI) for ocean observation, enabling real-time interactive long term monitoring of ocean processes. EMSO allows study of the interaction between the geosphere, the biosphere, the hydrosphere, and the lithosphere; including natural hazards, climate change, and marine ecosystems. EMSO nodes have been deployed at key sites in European seas, starting from the Arctic, through the Atlantic and Mediterranean, to the Black Sea.\n\nEMSO is one of the environmental RIs on the Roadmap of the European Strategy Forum on Research Infrastructures(ESFRI). The ESRFI Roadmap identifies RIs of pan-European importance that correspond to the long term needs of European research communities.\n\nThe different EMSO nodes were designed to address topics of regional importance: the biodiversity of mid ocean hot vents in the Azores region, the rapidly changing environmental conditions affecting the geosphere and biosphere of the Arctic, the deep-water ventilation in the eastern Mediterranean, the active seismicity and the associated geo-hazards of the Anatolian region.\nEMSO infrastructure has the capacity to observe the deep and open ocean, below, at and above the seafloor, at the European scale, utilizing both stand-alone observing systems, and nodes connected to shore stations through high throughput fibre optic cable.\nThe mission of EMSO is to unite these regional observatories into a common research infrastructure, to implement more generic sensor packages to collect synoptic data series on oceanographic features of more than regional interest, to bring these data together in a uniform format accessible to the general public, and to ensure maintenance of this research infrastructure over a longer time-span than easily maintained by national funding programs.\n\nThe global oceans cover 70% of the surface of the globe, consist of 95% of the living space, and are the core momentum of our planet’s physical, chemical, and biological cycles. As underlined in recent policy documents such as the Galway Statement and Belmont Challenge, in order to understand the changes predicted in the coming decades, EMSO aims to have a continuous presence in the oceans; and in order to understand both the slow moving and rapid catastrophes, EMSO seeks to have continuous real-time data from which to learn and to derive adaptation and early warning systems. Ocean observatories provide power and communications to allow a sustained interactive presence in the ocean. This challenge can only be addressed as part of an international cooperation between USA, Canada, Japan, Australia, Europe and other interested countries where EMSO takes a role for the European side.\n\nThe deployment of the EMSO distributed observatory nodes allows researchers to get useful data in order to understand the behaviour of the oceans and their impact on human society. \nIn particular EMSO collects data concerning the following main scientific fields:\n\nThe Preparatory Phase of EMSO was funded by the European Seventh Framework Programme (FP7), involving 12 countries of the European area (Italy, France, Germany, Ireland, Spain, Sweden, Greece, UK, Norway, Portugal, Turkey, the Netherlands), and Romania, that was involved as external interested country from 2010. The Preparatory Phase prepared the foundation for the adoption of the ERIC (European Research Infrastructure Consortium), that will be the legal entity in charge of coordinating and facilitating access to these nodes of open ocean fixed point observatory distributed infrastructure.\n\nThe EMSO-ERIC will be the central point of contact for observatory initiatives in other parts of the world to set up and promote cooperation in this field. EMSO-ERIC will also integrate research, training, and information dissemination activities for ocean observatory nodes in Europe and to enable scientists and other stakeholders to make efficient use of the EMSO distributed infrastructure around Europe.\n\n\n"}
{"id": "3244602", "url": "https://en.wikipedia.org/wiki?curid=3244602", "title": "Garden ornament", "text": "Garden ornament\n\nA Garden ornament is an item used for garden, landscape, and park enhancement and decoration.<br>\n\"The category can include:\"\n\nEarly examples of the use of garden ornaments in western culture were seen in Ancient Roman gardens such as those excavated at Pompeii and Herculaneum. The Italian Renaissance garden and French formal garden styles were the peak of using created forms in the garden and landscape, with high art and kitsch interpretations ever since. The English landscape garden expanded the scale of some garden ornaments to temple follies\n\nThe Asian tradition of making garden ornaments, often functioning in association with Feng Shui principles, has a nearly timeless history. Chinese gardens with Chinese scholar's rocks, Korean stone art, and Japanese gardens with Suiseki and Zen rock gardens have and symbolic meaning and natural ornamental qualities.\n\n\n"}
{"id": "13312969", "url": "https://en.wikipedia.org/wiki?curid=13312969", "title": "Global mental health", "text": "Global mental health\n\nGlobal mental health is the international perspective on different aspects of mental health. It is 'the area of study, research and practice that places a priority on improving mental health and achieving equity in mental health for all people worldwide'. There is a growing body of criticism of the global mental health movement, and has been widely criticised as a neo-colonial or \"missionary\" project and as primarily a front for pharmaceutical companies seeking new clients for psychiatric drugs.\n\nIn theory, taking into account cultural differences and country-specific conditions, it deals with the epidemiology of mental disorders in different countries, their treatment options, mental health education, political and financial aspects, the structure of mental health care systems, human resources in mental health, and human rights issues among others.\n\nThe overall aim of the field of global mental health is to strengthen mental health all over the world by providing information about the mental health situation in all countries, and identifying mental health care needs in order to develop cost-effective interventions to meet those specific needs.\n\nMental, neurological, and substance use disorders make a substantial contribution to the global burden of disease (GBD). This is a global measure of so-called disability-adjusted life years (DALY's) assigned to a certain disease/disorder, which is a sum of the years lived with disability and years of life lost due to this disease within the total population. Neuropsychiatric conditions account for 14% of the global burden of disease. Among non-communicable diseases, they account for 28% of the DALY's – more than cardiovascular disease or cancer. However, it is estimated that the real contribution of mental disorders to the global burden of disease is even higher, due to the complex interactions and co-morbidity of physical and mental illness.\n\nAround the world, almost one million people die due to suicide every year, and it is the third leading cause of death among young people. The most important causes of disability due to health-related conditions worldwide include unipolar depression, alcoholism, schizophrenia, bipolar depression and dementia. In low- and middle-income countries, these conditions represent a total of 19.1% of all disability related to health conditions.\n\nIt is estimated that one in four people in the world will be affected by mental or neurological disorders at some point in their lives. Although many effective interventions for the treatment of mental disorders are known, and awareness of the need for treatment of people with mental disorders has risen, the proportion of those who need mental health care but who do not receive it remains very high. This so-called \"treatment gap\" is estimated to reach between 76–85% for low- and middle-income countries, and 35–50% for high-income countries.\n\nDespite the acknowledged need, for the most part there have not been substantial changes in mental health care delivery during the past years. Main reasons for this problem are public health priorities, lack of a mental health policy and legislation in many countries, a lack of resources – financial and human resources – as well as inefficient resource allocation.\n\nIn 2011, the World Health Organization estimated a shortage of 1.18 million mental health professionals, including 55,000 psychiatrists, 628,000\nnurses in mental health settings, and 493,000 psychosocial care providers needed to treat mental disorders in 144 low- and middle-income countries. The annual wage bill to remove this health workforce shortage was estimated at about US$4.4 billion.\n\nInformation and evidence about cost-effective interventions to provide better mental health care are available. Although most of the research (80%) has been carried out in high-income countries, there is also strong evidence from low- and middle-income countries that pharmacological and psychosocial interventions are effective ways to treat mental disorders, with the strongest evidence for depression, schizophrenia, bipolar disorder and hazardous alcohol use.\n\nRecommendations to strengthen mental health systems around the world have been first mentioned in the WHO's \"World Health Report 2001\", which focused on mental health:\n\n\nBased on the data of 12 countries, assessed by the \"WHO Assessment Instrument for Mental Health Systems\" (WHO-AIMS), the costs of scaling up mental health services by providing a core treatment package for schizophrenia, bipolar affective disorder, depressive episodes and hazardous alcohol use have been estimated. Structural changes in mental health systems according to the WHO recommendations have been taken into account.\n\nFor most countries, this model suggests an initial period of investment of US$0.30 – 0.50 per person per year. The total expenditure on mental health would have to rise at least ten-fold in low-income countries. In those countries, additional financial resources will be needed, while in middle- and high-income countries the main challenge will be the reallocation of resources within the health system to provide better mental health service.\n\nPrevention is beginning to appear in mental health strategies, including the 2004 WHO report \"Prevention of Mental Disorders\", the 2008 EU \"Pact for Mental Health\" and the 2011 US National Prevention Strategy. NIMH or the National Institute of Mental Health has over 400 grants.\n\nTwo of WHO's core programmes for mental health are WHO MIND (Mental health improvements for Nations Development) and Mental Health Gap Action Programme (mhGAP).\n\nWHO MIND focuses on 5 areas of action to ensure concrete changes in people's daily lives. These are:\n\nMental Health Gap Action Programme (mhGAP) is WHO’s action plan to scale up services for mental, neurological and substance use disorders for countries especially with low and lower middle incomes. The aim of mhGAP is to build partnerships for collective action and to reinforce the commitment of governments, international organizations and other stakeholders.\n\nThe mhGAP Intervention Guide (mhGAP-IG) was launched in October 2010. It is a technical tool for the management of mental, neurological and substance use disorders in non-specialist health settings. The priority conditions included are: depression, psychosis, bipolar disorders, epilepsy, developmental and behavioural disorders in children and adolescents, dementia, alcohol use disorders, drug use disorders, self-harm/suicide and other significant emotional or medically unexplained complaints.\n\nOne of the most prominent critics of the Movement for Global Mental Health has been China Mills, author of the book \"Decolonizing Global Mental Health: The Psychiatrization of the Majority World\".\n\nMills writes that:\n\nAnother prominent critic is Ethan Watters, author of \"Crazy Like Us: The Globalization of the American Psyche\".\n\n\n"}
{"id": "1411028", "url": "https://en.wikipedia.org/wiki?curid=1411028", "title": "Humin", "text": "Humin\n\nHumins are a class of organic compounds that are insoluble in water at all pH's. The term is used in two related contexts, in soil chemistry and saccharide chemistry. These dark brown solids are inhomogenous and their structures are often vaguely described.\n\nSoil consists of both mineral (inorganic) and organic components. The organic components can be subdivided into fractions that are soluble, largely humic acids, and insoluble, the humins. Humins comprise about 50% of the organic matter in soil.\n\nHumins also produced during the dehydration of sugars, as is conducted in the conversion of cellulose to smaller organic compounds. These humins arise by the condensation of sugars with furfural and hydroxymethylfurfural, products of the dehydration of pentoses and hexoses. Since they are generally undesirable for chemical purposes, efforts are made to avoid their formation. Otherwise, humins are used for fuel.\n\nHumic substances, including humin, have not been observed in soils using modern analytical techniques.\n\n\nSinger, Michael J., and Donald N. Munns. Soils An Introduction (6th Edition). Upper Saddle River: Prentice Hall, 2005. \n"}
{"id": "25754996", "url": "https://en.wikipedia.org/wiki?curid=25754996", "title": "Invasive Species Forecasting System", "text": "Invasive Species Forecasting System\n\nThe Invasive Species Forecasting System, or simply ISFS, is a modeling environment for creating predictive habitat suitability maps for invasive species. It was developed by the National Aeronautics and Space Administration (NASA) in cooperation with various Department of Interior bureaus, including the United States Geological Survey (USGS), the Bureau of Land Management (BLM), and the National Park Service (NPS). \n"}
{"id": "160177", "url": "https://en.wikipedia.org/wiki?curid=160177", "title": "Joachim Barrande", "text": "Joachim Barrande\n\nJoachim Barrande (11 August 1799 – 5 October 1883) was a French geologist and palaeontologist.\n\nBarrande was born at Saugues, Haute Loire, and educated in the École Polytechnique and École Nationale des Ponts et Chaussées at Paris. Although he had received the training of an engineer, his first appointment was that of tutor to the duc de Bordeaux (afterwards known as the comte de Chambord), grandson of Charles X, and when the king abdicated in 1830, Barrande accompanied the royal exiles to England and Scotland, and afterwards to Prague. Settling in that city in 1831, he became occupied in engineering works, and his attention was then attracted to the fossils from the Lower Palaeozoic rocks of Bohemia.\n\nThe publication in 1839 of \"Murchison's Silurian System\" incited Barrande to carry on systematic researches on the equivalent strata in Bohemia. For ten years (1840–1850) he made a detailed study of these rocks, engaging workmen specially to collect fossils, and in this way he obtained upwards of 3500 species of graptolites, brachiopoda, mollusca, trilobites and fishes. The first volume of his great work, \"Système silurien du centre de la Bohême\" (dealing with trilobites, several genera, including \"Deiphon\", which he personally described), appeared in 1852; and from that date until 1881, he issued twenty-one quarto volumes of text and plates. Two other volumes were issued after his death in 1887 and 1894. It is estimated that he spent nearly £10,000 on these works. In addition he published a large number of separate papers. In recognition of his important researches the Geological Society of London in 1857 awarded to him the Wollaston medal. In 1870, he was elected a foreign member of the Royal Swedish Academy of Sciences. He was elected a Foreign Honorary Member of the American Academy of Arts and Sciences in 1875.\n\nBarrande died at Frohsdorf on 5 October 1883. His extensive collection has been stored in National Museum in Prague on grounds of his testament. \n\nIn 1884 the Barrande Rocks in Prague was named in honor of the scientist and massive plaque with Barrande's name was added at the rocks. On 24 February 1928, a district of Prague, \"Barrandov\", was named after him.\n\nBarrande was an advocate of the theory of catastrophes (as taught by Georges Cuvier), thus opposing Charles Darwin's theory of evolution. He rejected transmutation of species. He also wrote a five-volume book on the defense of his theory of so-called \"colonies\", presuming that the cause of the presence of fossils typical for one layer surrounded by those typical for another is atectonical. He tended to name those colonies with names of his scientific adversaries.\n\nHis anti-evolutionary views were criticized by geologist Henry Hicks who commented that \"Barrande is well known to be a determined opponent to the theory of evolution, and doubtless this strong bias has prevented him from seeing and accepting many facts which would otherwise, to so keen and careful an observer, have seemed inconsistent with such strong views.\"\n\n\n\n"}
{"id": "50091049", "url": "https://en.wikipedia.org/wiki?curid=50091049", "title": "KPTT Agricultural Training Center", "text": "KPTT Agricultural Training Center\n\nKPTT Agricultural Training Center \"(Kursus Pertanian Taman Tani)\" is a Jesuit-run boarding and day school that teaches organic farming. It was founded in 1965 in Salatiga, Central Java, Indonesia.\n\n \n"}
{"id": "5879882", "url": "https://en.wikipedia.org/wiki?curid=5879882", "title": "List of Gnaphosidae species", "text": "List of Gnaphosidae species\n\nThis page lists all described species of the spider family Gnaphosidae as of Dec. 29, 2016.\n\n\"Allomicythus\" \n\n\"Allozelotes\" \n\n\"Amazoromus\" \n\n\"Amusia\" \n\n\"Anagraphis\" \n\n\"Aneplasa\" \n\n\"Anzacia\" \n\n\"Aphantaulax\" \n\n\"Apodrassodes\" \n\n\"Apodrassus\" \n\n\"Apopyllus\" \n\n\"Aracus\" \n\n\"Arauchemus\" \n\n\"Asemesthes\" \n\n\"Asiabadus\" \n\n\"Australoechemus\" \n\n\"Benoitodes\" \n\n\"Berinda\" \n\n\"Berlandina\" \n\n\"Cabanadrassus\" \n\n\"Callilepis\" \n\n\"Camillina\" \n\n\"Canariognapha\" \n\n\"Ceryerda\" \n\n\"Cesonia\" \n\n\"Chatzakia\" \n\"Civizelotes\" \n\n\"Cladothela\" \n\n\"Coillina\" \n\n\"Coreodrassus\" \n\n\"Cryptodrassus\" \n\n\"Cubanopyllus\" \n\n\"Diaphractus\" \n\n\"Drassodes\" \n\n\"Drassodex\" \n\n\"Drassyllus\" \n\n\"Echemella\" \n\n\"Echemographis\" \n\n\"Echemoides\" \n\n\"Echemus\" \n\n\"Eilica\" \n\n\"Encoptarthria\" \n\n\"Epicharitus\" \n\n\"Fedotovia\" \n\n\"Gertschosa\" \n\n\"Gnaphosa\" \n\n\"Haplodrassus\" \n\n\"Hemicloea\" \n\n\"Herpyllus\" \n\n\"Heser\" \n\n\"Hitobia\" \n\n\"Homoeothele\" \n\n\"Hongkongia\" \n\n\"Hypodrassodes\" \n\n\"Ibala\" \n\n\"Intruda\" \n\n\"Kaitawa\" \n\n\"Kishidaia\" \n\n\"Ladissa\" \n\n\"Laronius\" \n\n\"Latonigena\" \n\n\"Leptodrassex\" \n\n\"Leptodrassus\" \n\n\"Leptopilos\" \n\n\"Litopyllus\" \n\n\"Macarophaeus\" \n\n\"Matua\" \n\n\"Megamyrmaekion\" \n\n\"Micaria\" \n\n\"Microdrassus\" \n\n\"Microsa\" \n\n\"Micythus\" \n\n\"Minosia\" \n\n\"Minosiella\" \n\n\"Montebello\" \n\n\"Nauhea\" \n\n\"Neodrassex\" \n\n\"Nodocion\" \n\n\"Nomisia\" \n\n\"Nopyllus\" \n\n\"Notiodrassus\" \n\n\"Odontodrassus\" \n\n\"Orodrassus\" \n\n\"Parabonna\" \n\n\"Parasyrisca\" \n\n\"Phaeocedus\" \n\n\"Poecilochroa\" \n\n\"Pseudodrassus\" \n\n\"Pterotricha\" \n\n\"Pterotrichina\" \n\n\"Sanitubius\" \n\n\"Scopoides\" \n\n\"Scotocesonia\" \n\n\"Scotognapha\" \n\n\"Scotophaeus\" \n\n\"Sergiolus\" \n\n\"Sernokorba\" \n\n\"Setaphis\" \n\n\"Shaitan\" \n\n\"Shiragaia\" \n\n\"Sidydrassus\" \n\n\"Smionia\" \n\n\"Sosticus\" \n\n\"Symphanodes\" \n\n\"Synaphosus\" \n\n\"Talanites\" \n\n\"Talanitoides\" \n\n\"Titus\" \n\n\"Trachyzelotes\" \n\n\"Trephopoda\" \n\n\"Trichothyse\" \n\n\"Turkozelotes\" \n\n\"Urozelotes\" \n\n\"Vectius\" \n\n\"Verita\" \n\n\"Xenoplectus\" \n\n\"Xerophaeus\" \n\n\"Xizangia\" \n\n\"Zelanda\" \n\n\"Zelominor\" \n\n\"Zelotes\" \n\n\"Zelotibia\" \n\n\"Zelowan\" \n\n\"Zimiromus\" \n\n"}
{"id": "3301644", "url": "https://en.wikipedia.org/wiki?curid=3301644", "title": "List of Lepidoptera that feed on elms", "text": "List of Lepidoptera that feed on elms\n\nElms (\"Ulmus\" species) are used as food plants by the larvae of a number of Lepidoptera species including:\n\nSpecies that feed exclusively on \"Ulmus\"\n\n\nSpecies that feed on \"Ulmus\" among other plants\n\n\n"}
{"id": "29845492", "url": "https://en.wikipedia.org/wiki?curid=29845492", "title": "List of New Mexico hurricanes", "text": "List of New Mexico hurricanes\n\nThe inland U.S. state of New Mexico has experienced impacts from 81 known tropical cyclones and their remnants. There have been 68 known tropical cyclones from the Eastern Pacific that affected the state, compared to only 13 such Atlantic hurricanes. The biggest threat from such storms in the state is their associated rainfall and flooding. The wettest storm was from the remnants of an Atlantic storm in 1941 that produced of precipitation. Since 1950, the highest rainfall total recorded was in Canton, also associated with an Atlantic storm in 1954. The rains in 1954 resulted in flooding in six towns that killed at least four people. Other deadly rainfall events from tropical cyclone remnants include Hurricane Dolly in 2008 and Tropical Storm Georgette in 2010. The former swept away a person along the Rio Ruidoso and caused $25 million in damage (2008 USD), which was the most damaging storm event. The floods damaged 500 buildings and destroyed 13 bridges. The latter caused heavy rains that resulted in one death in the Rio Grande.\n\nDue to New Mexico's position in the southwestern United States, there are three primary causes for flooding within the state in the autumn months: the effects of tropical cyclones, the North American Monsoon, or an approaching cold front. Tropical cyclones from both the Atlantic and Pacific oceans can affect New Mexico, usually in southern regions. Storms that originate from the Atlantic Ocean usually move through the Gulf of Mexico on a northwest trajectory, typically during the month of September. Tropical cyclones from the eastern Pacific usually maintain a fast north-northeast track, occasionally through the Gulf of California, and generally affect the state from the middle of September to the middle of October. The accompanying moisture of the Pacific storms encounter the mountains of the state, and through orographic lift, produce heavy rainfall. Due to the storms' fast movement, the rainfall events accompanying Pacific storms are generally shorter lasting and more concentrated than other such storms in the state.\n\nIn an average year, the remnants of 3.1 Pacific tropical cyclones affect the southwestern United States. In some locations, the systems provide 30% of the annual rainfall. In New Mexico, however, stations' annual rainfall from tropical cyclones range from 10% in the western portion to less than 5% in the eastern. Rainfall from tropical cyclones affect the Albuquerque International Sunport an average of 3.9 days per year. Storms affect New Mexico less than Arizona, and are generally weaker. The greatest threat from such storms are accompanying floods. Damage from tropical cyclones is expected to increase as the population increases in the southwestern United States.\n\n\n\n\n\n\n\n\n\n"}
{"id": "51484929", "url": "https://en.wikipedia.org/wiki?curid=51484929", "title": "List of chainsaw carving competitions", "text": "List of chainsaw carving competitions\n\nChainsaw carving competitions are competitive events held all over the world where contestants display their skills in the act of chainsaw carving.\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "31744838", "url": "https://en.wikipedia.org/wiki?curid=31744838", "title": "List of dates predicted for apocalyptic events", "text": "List of dates predicted for apocalyptic events\n\nPredictions of apocalyptic events that would result in the extinction of humanity, a collapse of civilization, or the destruction of the planet have been made since at least the beginning of the Common Era. Most predictions are related to Abrahamic religions, often standing for or similar to the eschatological events described in their scriptures. Christian predictions typically refer to events like the Rapture, Great Tribulation, Last Judgment, and the Second Coming of Christ. End-time events are usually predicted to occur within the lifetime of the person making the prediction, and are usually made using the Bible, and in particular the New Testament, as either the primary or exclusive source for the predictions. Often this takes the form of mathematical calculations, such as trying to calculate the point where it will have been 6000 years since the supposed creation of the Earth by the Abrahamic God, which according to the Talmud marks the deadline for the Messiah to appear. Predictions of the end from natural events have also been theorised by various scientists and scientific groups. While these predictions are generally accepted as plausible within the scientific community, the events and phenomena are not expected to occur for hundreds of thousands or even billions of years from now.\n\nLittle research has been done into why people make apocalyptic predictions. Historically, it has been done for reasons such as diverting attention from actual crises like poverty and war, pushing political agendas, and promoting hatred of certain groups; antisemitism was a popular theme of Christian apocalyptic predictions in medieval times, while French and Lutheran depictions of the apocalypse were known to feature English and Catholic antagonists respectively. According to psychologists, possible explanations for why people believe in modern apocalyptic predictions include mentally reducing the actual danger in the world to a single and definable source, an innate human fascination with fear, personality traits of paranoia and powerlessness and a modern romanticism involved with end-times due to its portrayal in contemporary fiction. The prevalence of Abrahamic religions throughout modern history is said to have created a culture which encourages the embracement of a future that will be drastically different from the present. Such a culture is credited with the rise in popularity of predictions that are more secular in nature, such as the 2012 phenomenon, while maintaining the centuries-old theme that a powerful force will bring the end of humanity.\n\nPolls conducted in 2012 across 20 countries found over 14% of people believe the world will end in their lifetime, with percentages ranging from 6% of people in France to 22% in the US and Turkey. Belief in the apocalypse is most prevalent in people with lower rates of education, lower household incomes, and those under the age of 35. In the UK in 2015, 23% of the general public believed the apocalypse was likely to occur in their lifetime, compared to 10% of experts from the Global Challenges Foundation. The general public believed the likeliest cause would be nuclear war, while experts thought it would be artificial intelligence. Only 3% of Britons thought the end would be caused by the Last Judgement, compared to 16% of Americans. Between one and three percent of people from both countries thought the apocalypse would be caused by zombies or alien invasion.\n\nThis section lists eschatological predictions, mostly by religious individuals or groups. Most predictions are related to Abrahamic religions, with numerous predictions standing for the eschatological events described in their scriptures. Christian predictions typically refer to events like the Rapture, Great Tribulation, Last Judgment, and the Second Coming of Christ.\n\n\n"}
{"id": "31104560", "url": "https://en.wikipedia.org/wiki?curid=31104560", "title": "List of mountains in Poland", "text": "List of mountains in Poland\n\nThe Sudetes and Carpathian Mountains mountain ranges are located on either side of Poland's southern border. Within Poland, neither of these ranges is forbidding enough to prevent substantial habitation; the Carpathians are especially densely populated. The rugged form of the Sudeten range derives from the geological shifts that formed the later Carpathian uplift. The Carpathians in Poland, formed as a discrete topographical unit in the relatively recent Tertiary Era, are the highest mountains in the country. They are the northernmost edge of a much larger range that extends into the Czech Republic, Slovakia, Ukraine, Hungary, and Romania.\n\nThe Świętokrzyskie Mountains, one of the oldest mountain ranges in Europe, are located in central Poland, in the vicinity of the city of Kielce. The mountain range consists of a number of separate ranges, the highest of which is Łysogóry (lit. \"bald mountains\"). Together with the Jura Krakowsko-Częstochowska the mountains form a region called the Lesser Poland Upland (\"Wyżyna Małopolska\"). They were formed during the Caledonian orogeny of the Silurian period and then rejuvenated in the Hercynian orogeny of the Upper Carboniferous period.\n\nThe most known and significant mountain in Poland are the Tatra Mountains which attract many tourists from Poland and its neighbouring states.\n\n\n\n\nhttp://www.pieniny.com/uploaded/mapapienin.png\n"}
{"id": "3569632", "url": "https://en.wikipedia.org/wiki?curid=3569632", "title": "List of national parks of Indonesia", "text": "List of national parks of Indonesia\n\nThis is the list of the national parks of Indonesia. Of the 53 national parks, 6 are World Heritage Sites, 9 are part of the World Network of Biosphere Reserves and 5 are wetlands of international importance under the Ramsar convention. A total of 9 parks are largely marine.\n\nThe first group of five Indonesian national parks were established in 1980. This number increased constantly reaching 41 in 2003. In a major expansion in 2004, nine more new national parks were created, raising the total number to 50. Mount Tambora was added in 2015.\n\n\n"}
{"id": "35813824", "url": "https://en.wikipedia.org/wiki?curid=35813824", "title": "List of pipeline accidents in the United States", "text": "List of pipeline accidents in the United States\n\n<nowiki> </nowiki>This List of pipeline accidents in the United States provides access to links for various timeframes, which are sometimes quite extensive.\n\n"}
{"id": "22492033", "url": "https://en.wikipedia.org/wiki?curid=22492033", "title": "List of rivers of Mozambique", "text": "List of rivers of Mozambique\n\nThis is a list of rivers in Mozambique. This list is arranged by drainage basin, with respective tributaries indented under each larger stream's name.\n\n"}
{"id": "369027", "url": "https://en.wikipedia.org/wiki?curid=369027", "title": "List of rivers of Sweden", "text": "List of rivers of Sweden\n\nThis is a list of the rivers of Sweden.\n\nFrom south to north\n\n\nA-Z of Swedish rivers. The lengths in miles are also given:\n"}
{"id": "8591394", "url": "https://en.wikipedia.org/wiki?curid=8591394", "title": "List of stars in Capricornus", "text": "List of stars in Capricornus\n\nThis is the list of notable stars in the constellation Capricornus, sorted by decreasing brightness.\n\n\n"}
{"id": "10597220", "url": "https://en.wikipedia.org/wiki?curid=10597220", "title": "Mount Miwa", "text": "Mount Miwa\n\nThe kami generally associated with Mount Miwa is Ōmononushi (\"Ōmono-nushi-no-kami\"), a rain kami. However, the \"Nihon Shoki\" notes that there was a degree of uncertainly when it came to naming the principal kami of Mount Miwa, but he is often linked to Ōkuninushi.\n\nMount Miwa was first described in the Kojiki as Mount Mimoro (三諸山). Both names were in common use until the reign of Emperor Yūryaku, after which \"Miwa\" was preferred. \"Mimoro\" has been held to mean something like \"august, beautiful\" (\"mi\") and \"room\", or \"hall\" (\"moro\" corruption of \"muro\"). The current kanji 三 (\"mi\") and 輪 (\"wa\") are purely phonetic. It has also been written 三和, another a phonetic spelling with the same pronunciation.\n\nReligious worship surrounding Mount Miwa has been deemed the oldest and more primitive of its kind in Japan, dating to pre-history. The very mountain itself is designated sacrosanct, and today's Ōmiwa Shrine still considers the mountain to be its \"shintai\", or kami-body. The kami residing on Mount Miwa was judged the most powerful by the Fujiwara clan, and consequently palaces and roads were built in the vicinity.\n\nThe \"Nihon Shoki\", Book V, (Chronicle of Emperor Sujin, 10th emperor) records that when the country was crippled by pesitilence and subsequent mayhem, the emperor consulted the gods. The god Ōmononushi (whom some sources to the chronicle identify with the Mount Miwa deity) spoke through the mouth of an elder princess of the imperial house named (daughter of 7th emperor Emperor Kōrei and Sujin's aunt) and revealed himself to be the deity residing in the borders of Yamato on Mount Miwa, and promised to bring end to chaos if he were properly worshipped. The emperor propitiated to the god but the effects were not immediate. Later, the same god appeared in a dream to and instructed him to seek out a man named , said to be the child of the god, and to install him as head priest of his cult. Subsequently, normal order was restored and crops no longer failed.\n\nThe \"Nihon shoki\" records that the first priest of the shrine, Ōtataneko declares himself the son born between the god and Ikutama yori-hime (or Ikudama-yori-bime). However, in the \"Kojiki\", Ōtataneko identifies himself as the great-grandson (Ōtataneko and Iku-tama-yori-bime begat Kushi-mikata, who begat Iikatasumi, who begat Takemikazuchi who begat the priest Ōtataneko).\n\n\"Kojiki\" tells how it became known Ikutama yori-hime was divinely conceived. The beautiful girl was found to be pregnant, and claimed a handsome being had come to her at night. Her parents, in order to discover the identity of the man, instructed her to sprinkle red earth by her bedside, and thread a hemp cord (or skein) with a needle through the hem of his garment. In the morning, the hemp went through the hole of the door-hook so that only \"three loops\" (\"miwa\") were left. They trailed the remaining hemp thread to the shrine in the mountain, and that was how they discovered the visitation was divine.\n\nThe \"Kojiki\" also records another divine birth from an earlier period (under Emperor Jimmu). It tells how a maiden named Seya-datara was squatting in the toilet when the god transformed into the shape of a red-painted arrow and poked her in her privates. In astonishment, she picked up the arrow and placed it by the floor, whereby it transformed into a fine youth, who wound up marrying her. The girl child then born was named Hoto-tatara-i-susuki-hime(or Hotota-tara-i-susugi-hime), \"hoto\" being an old word for a woman's private parts.\n\nBook V in the \"Nihon Shoki\", adds the following quaint episode. Suijin's aunt, the aforementioned Yamato-to-to-hi-momoso-hime, was later appointed the consort or wife of Ōmononushi (Mount Miwa). The kami however, would only appear to her at night, and the princess pleaded to reveal his true form. The kami warned her not to be shocked, and agreed to show himself inside her comb box () or toiletry case. The next day she opened the box and discovered a magnificent snake inside. She shrieked out in surprise, whereby the deity transformed into human form, promised her payback for shaming him so, and took off to Mount Mimoro (Mount Miwa). The princess was so distraught at this, that she flopped herself on the seat stabbed herself in the pudenda with chopsticks, which ensued in her death. She is supposedly buried at one of the six mounds near Mount Miwa, the \"Hashihaka\" (lit. \"chopstick-grave\") mound. The \"Kojiki\" version of this myth describes a union between a woman from the Miwa clan and Ōmononushi, resulting in the birth of an early Yamato king. Scholars note that this is a clear effort to strengthen Yamato authority by identifying and linking their lineage to the established worship surrounding Mount Miwa.\n\nIn \"Nihon Shoki\", Book XIV, under Emperor Yūryaku year 7 (purportedly 463 A.D.), it is recorded that the emperor expressed the desire to get a glimpse of the deity of Mount Mimoro (Mount Miwa) and ordered a man known for his brute strength, named to go capture it. (A scholium in the codices writes the identity of the god of this mountain is said to by by some sources, and ). Sugaru thereby climbed the mountain and captured and presented it to the emperor. But Yūryaku neglected to purify himself (by religious fasting, etc.) so that the great serpent made thunderous noise and made its eyes glare. The frightened emperor retreated to the palace, and had the snake released in the hill. He gave the hill a new name, \"Ikazuchi\" (lit. \"thunderbolt\").\n\nRecords say that Yamato Takeru's sword Kusanagi was later placed in the keeping of the Atsuta Shrine, and that Takeru also presented a number of Emishi (\"barbarians\") hostages he quelled to the same shrine. The priestess however found them rowdy and mannerless, and so she handed them over to the imperial court. The court settled them around the Mount Miwa area at first, but they would chopped down its trees, or make great hollers and frightened the villages. So their numbers were scattered and settled in five provinces, and they became the ancestors of the Saeki clans (Recorded in \"Nihongi\" Book VII, Emperor Keikō year 51 (purportedly 121 A.D.).\n\nMuch later in time, during Emperor Bidatsu year 10 (581 A.D.), \"Nihon Shoki\", Book XX), the Emishi were harassing the borderlands. The emperor summoned their leader named Ayakasu, and threatened the ringleaders of the ruffians with death. Whereby Ayakasu and the others entered Hatsuse-gawa (upper stream of Yamato River, sipped its water, and facing towards Mount Mimoro (Mt. Miwa), swore allegiance unto their descendants to the Yamato court.\n\nYamato leaders often ruled from palaces near sacred mountains, and built burial mounds around them, as it was a prominent sanctuary for both locals and Yamato kings alike.\n\nSix tumuli have been found in the Shiki area at the base of Mount Miwa. These earthen mounds were built between 250 AD to 350 AD, and all display the same keyhole shape and stone chambers found in earlier mounds. However, the tumuli found at Mount Miwa hint at the beginning of a more centralized Yamato state. All six mounds are exceptionally large, twice as large as any similar mounds found in Korea, and contain prolific amounts of mirrors, weapons, ornaments, as well as finely built wood and bamboo coffins.\n\nThey are as follows, in order of discovery:\n\nReligious objects and pottery have also been found on and around the mountain.\n\n\nJapanese cedar (\"Cryptomeria\", jp. \"sugi\") grows all over the mountain and is considered a holy tree.\n\n\n\n"}
{"id": "44649", "url": "https://en.wikipedia.org/wiki?curid=44649", "title": "National parks of New Zealand", "text": "National parks of New Zealand\n\nThe national parks of New Zealand are protected areas administered by the Department of Conservation. Although the national parks contain some of New Zealand's most beautiful scenery, the first few established were all focused on mountain scenery. Since the 1980s the focus has been on developing a more diverse representation of New Zealand landscapes. The parks are all culturally significant and many also contain historic features. Tongariro National Park is one of the World Heritage Sites that are of both cultural and natural significance, while four of the South Island national parks form Te Wahipounamu, another World Heritage Site. There are currently 13 national parks; a 14th, Te Urewera National Park, was disestablished in 2014.\n\nThe national parks are administered by the Department of Conservation \"for the benefit, use, and enjoyment of the public\". They are popular tourist destinations, with three-tenths of overseas tourists visiting at least one national park during their stay in New Zealand.\n\nThe National Parks Act of 1980 was established in order to codify the purpose, governance and selection of national parks. It begins by establishing the definition of a national park:\nThe National Parks Act goes on to state that the public will have freedom of entry and access to the parks, though this is subject to restrictions to ensure the preservation of native plants and animals and the welfare of the parks in general. Access to specially protected areas (550 km²) constituted under the act is by permit only.\nUnder the Act, national parks are to be maintained in their natural state as far as possible to retain their value as soil, water and forest conservation areas. Native plants and animals are to be preserved and introduced plants and animals removed if their presence interferes with the natural wildlife. Development in wilderness areas established under the act is restricted to foot tracks and huts used for wild animal control or scientific research.\n\nThe Act allows the Department of Conservation to provide hostels, huts, camping grounds, ski tows and similar facilities, parking areas, roading and tracks within the parks. In addition to these, the department also provides some accommodation, transport and other services at entry points to the parks, but these are also offered by other government agencies, voluntary organisations and private firms. More comprehensive services within the parks, such as guided walks and skiing tutorials, are privately provided with concessions from the department.\n\nThis table lists the current and former national parks from north to south.\n\n<nowiki>*</nowiki> – World Heritage Site or part thereof.\n\n<div style=\"float: centre\">\nThe area centred on Waipoua Forest, north of Dargaville, has been proposed as a possible Kauri National Park. The area contains most of New Zealand's remaining kauri, including the largest known kauri, Tāne Mahuta. These stands of kauri are also valuable as havens for endangered species including the North Island brown kiwi. This proposal is currently being investigated by the Department of Conservation.\n\nIn response to a DoC proposal to upgrade the protection of Great Barrier Island (Aotea), Forest and Bird launched a campaign in 2014 to designate it as a National Park.\n\nIn 2010 the New Zealand Government proposed removing some national park and conservation areas from Schedule 4 protection of the Crown Minerals Act which prohibits mining in those areas. In July the government abandoned the proposal after receiving a large number of submissions, most of which opposed mining.\n\n\n"}
{"id": "26584221", "url": "https://en.wikipedia.org/wiki?curid=26584221", "title": "Pachamama", "text": "Pachamama\n\nPachamama is a goddess revered by the indigenous people of the Andes. She is also known as the earth/time mother. In Inca mythology, Pachamama is a fertility goddess who presides over planting and harvesting, embodies the mountains, and causes earthquakes. She is also an ever-present and independent deity who has her own self-sufficient and creative power to sustain life on this earth. Her shrines are hallowed rocks, or the boles of legendary trees, and her artists envision her as an adult female bearing harvests of potatoes and coca leaves. The four cosmological Quechua principles – Water, Earth, Sun, and Moon – claim Pachamama as their prime origin. Priests sacrifice llamas, cuy (guinea pigs), and elaborate, miniature, burned garments to her. After the conquest by Spain, which forced conversion to Roman Catholicism, the figure of the Virgin Mary became united with that of the Pachamama for many of the indigenous people. In pre-Hispanic culture, Pachamama is often a cruel goddess eager to collect her sacrifices. As Andes cultures form modern nations, Pachamama remains benevolent, giving, and a local name for Mother Nature. Thus, many in South America believe that problems arise when people take too much from nature because they are taking too much from Pachamama. Pachamama is the mother of Inti the sun god and Mama Killa the moon goddess. Pachamama is said to also be the wife of Inti, her son.\n\n\"Pachamama\" is usually translated as Mother Earth, but a more literal translation would be \"World Mother\" (in Aymara and Quechua).\nThe Inca goddess can be referred to in multiple ways; the primary way being Pachamama. Other names for her are: Mama Pacha, La Pachamama, and Mother Earth. La Pachamama differs from Pachamama because the \"La\" signifies the interwoven connection that the goddess has with nature, whereas Pachamama – without the \"La\" – refers to only the goddess.\n\nPachamama and Inti are worshiped as benevolent deities in the area known as Tawantinsuyu. Tawantinsuyu is the name of the former Inca Empire, and the region stretches through the Andean mountains in present day Bolivia, Ecuador, Chile, Peru, and northern Argentina. People usually give a toast to honor Pachamama before meetings and festivities. In some regions, a special kind of libation known as a \"challa\" is performed on a daily basis. The challa is performed by spilling a small amount of \"chicha\" on the floor, and then drinking the rest. \n\nPachamama has a special worship day called \"Martes de challa\" (Challa's Tuesday), when people bury food, throw candies, and burn incense to thank Pachamama for their harvests. In some cases, celebrants assist traditional priests, known as \"yatiris\" in Aymara, in performing ancient rites to bring good luck or the good will of the goddess, such as sacrificing guinea pigs or burning llama fetuses (although this is rare today). The festival coincides with Shrove Tuesday, also celebrated as \"Carnevale\" or \"Mardi Gras\". \n\nThe central ritual to Pachamama is the \"Challa\" or \"Pago\" (Payment). It is carried out during all of August, and in many places also on the first Friday of each month. Other ceremonies are carried out in special times, as upon leaving for a trip or upon passing an \"\". According to Mario Rabey and Rodolfo Merlino, Argentine anthropologists who studied the Andean culture from the 1970s to the 1990s, \"The most important ritual is the challaco. Challaco is a deformation of the Quechua words 'ch'allay' and 'ch'allakuy', that refer to the action to insistently sprinkle. In the current language of the campesinos of the southern Central Andes, the word \"challar\" is used in the sense of \"to feed and to give drink to the land'. The challaco covers a complex series of ritual steps that begin in the family dwellings the night before. They cook a special food, the \"tijtincha\". The ceremony culminates at a pond or stream, where the people offer a series of tributes to Pachamama, including \"food, beverage, leaves of coca and cigars.\n\nRituals to honor Pachamama take place all year, but are especially abundant in August, right before the sowing season. Because August is the coldest month of the winter in the southern Andes, people feel more vulnerable to illness. August is therefore regarded as a \"tricky month.\" During this time of mischief, Andeans believe that they must be on very good terms with nature to keep themselves and their crops and livestock healthy and protected. In order to do this, families perform cleansing rituals by burning plants, wood and other items in order to scare evil spirits who are thought to be more abundant at this time. People also drink mate (a South American hot beverage), which is thought to give good luck.\n\nOn the night before August 1, families prepare to honor Pachamama by cooking all night. The host of the gathering then makes a hole in the ground If the soil comes out nicely, this means that it will be a good year; if not, the year will not be bountiful. Before any of the guests are allowed to eat, the host must first give a plate of food to Pachamama. Food that was left aside is poured onto the ground and a prayer to Pachamama is recited.\n\nA main attraction of the Pachamama festival is the Sunday parade. The organizational committee of the festival searches for the oldest woman in the community and elects her the \"Pachamama Queen of the Year.\" This election first occurred in 1949. Indigenous women, in particular senior women, are seen as incarnations of tradition and as living symbols of wisdom, life, fertility, and reproduction. The Pachamama queen who is elected is escorted by the Gauchos who circle the plaza on their horses and salute her during the Sunday parade. The Sunday parade is considered to be the climax of the festival.\n\nThere has been a recent rise in a New Age practice among white and Andean mestizo peoples. There is a weekly ritual worship which takes place on Sundays and includes invocations to Pachamama in Quechua, although there are some references in Spanish. Inside the temple, there is a large stone with a medallion on it, symbolizing the New Age group and its beliefs. A bowl of dirt on the right of the stone is there to represent Pachamama, because of her status as a Mother Earth. Many rituals related to the Pachamama are practiced in conjunction with those of Christianity, to the point that many families are simultaneously Christian and pachamamistas. Pachamama is sometimes syncretized as the Virgin of Candelaria. Certain travel agencies have drawn upon the emerging New Age movement in Andean communities (drawn from Quechua ritual practices) to urge tourists to come to visit Inca sites. Tourists visiting these sites, such as Machu Picchu and Cusco, are offered the chance to participate in ritual offerings to Pachamama. The tourist market has been using Pachamama to increase its draw to tourists.\n\nBelief in Pachamama features prominently in the Peruvian national narrative. Former President, Alejandro Toledo, held a symbolic inauguration on 28 July 2001 atop Machu Picchu which featured a Quechua religious elder giving an offering to Pachamama. Pachamama has also been used as an example of autochthony by some Andean intellectuals.\n\n\n"}
{"id": "27365903", "url": "https://en.wikipedia.org/wiki?curid=27365903", "title": "Per Thomsen", "text": "Per Thomsen\n\nPer Berle Thomsen (17 August 1905 – 22 October 1983) was a Norwegian journalist and newspaper editor.\n\nHe was born in Bergen.\n\nThomsen was a journalist for \"Stavanger Aftenblad\" from 1926 to 1941. He was editor-in-chief for \"Stavanger Aftenblad\" from 1955 to 1973.\n\nFrom November 1944 to May 1945 he was incarcerated in the Grini concentration camp.\n\n"}
{"id": "51794642", "url": "https://en.wikipedia.org/wiki?curid=51794642", "title": "Permafrost and Periglacial Processes", "text": "Permafrost and Periglacial Processes\n\nPermafrost and Periglacial Processes is a quarterly peer-reviewed scientific journal covering research on permafrost and periglacial geomorphology. It covers the subject from various points of views including engineering, hydrology, process geomorphology, and quaternary geology. It is the official journal of the International Permafrost Association and is published by John Wiley & Sons. The editor-in-chief is J.B. Murton (University of Sussex). According to the \"Journal Citation Reports\", the journal has a 2015 impact factor of 2.000.\n\n"}
{"id": "10121045", "url": "https://en.wikipedia.org/wiki?curid=10121045", "title": "Pore space in soil", "text": "Pore space in soil\n\nThe pore space of soil contains the liquid and gas phases of soil, i.e., everything but the solid phase that contains mainly minerals of varying sizes as well as organic compounds.\n\nIn order to understand porosity better a series of equations have been used to express the quantitative interactions between the three phases of soil.\n\nMacropores or fractures play a major role in infiltration rates in many soils as well as preferential flow patterns, hydraulic conductivity and evapotranspiration. Cracks are also very influential in gas exchange, influencing respiration within soils. Modeling cracks therefore helps understand how these processes work and what the effects of changes in soil cracking such as compaction, can have on these processes.\n\nformula_1\n\nThe bulk density of soil depends greatly on the mineral make up of soil and the degree of compaction. The density of quartz is around 2.65 g/cm but the bulk density of a soil may be less than half that density.\n\nMost soils have a bulk density between 1.0 and 1.6 g/cm but organic soil and some friable clay may have a bulk density well below 1 g/cm\n\nCore samples are taken by driving a metal core into the earth at the desired depth and soil horizon. \nThe samples are then oven dried and weighed.\n\nBulk density = (mass of oven dry soil)/volume\n\nThe bulk density of soil is inversely related to the porosity of the same soil. The more pore space in a soil the lower the value for bulk density.\n\nformula_2 or formula_3\n\nPorosity is a measure of the total pore space in the soil. This is measured as a volume or percent. The amount of porosity in a soil depends on the minerals that make up the soil and the amount of sorting that occurs within the soil structure. For example, a sandy soil will have larger porosity than silty sand, because the silt will fill in the gaps between the sand particles.\n\nHydraulic conductivity (K) is a property of soil that describes the ease with which water can move through pore spaces. It depends on the permeability of the material (pores, compaction) and on the degree of saturation. Saturated hydraulic conductivity, K, describes water movement through saturated media. Where hydraulic conductivity has the capability to be measured at any state. It can be estimated by numerous kinds of equipment. To calculate hydraulic conductivity, Darcy's law is used. The manipulation of the law depends on the Soil saturation and instrument used.\n\nInfiltration is the process by which water on the ground surface enters the soil. The water enters the soil through the pores by the forces of gravity and capillary action. The largest cracks and pores offer a great reservoir for the initial flush of water. This allows a rapid infiltration. The smaller pores take longer to fill and rely on capillary forces as well as gravity. The smaller pores have a slower infiltration as the soil becomes more saturated.\n\nA pore is not simply a void in the solid structure of soil. The various pore size categories have different characteristics and contribute different attributes to soils depending on the number and frequency of each type. A widely used classification of pore size is that of Brewer (1964):\n\nThe pores that are too large to have any significant capillary force. Unless impeded, water will drain from these pores, and they are generally air-filled at field capacity. Macropores can be caused by cracking, division of peds and aggregates, as well as plant roots, and zoological exploration.\nSize >75 μm.\n\nThe largest pores filled with water at field capacity. Also known as storage pores because of the ability to store water useful to plants. They do not have capillary forces too great so that the water does not become limiting to the plants. The properties of mesopores are highly studied by soil scientists because of their impact on agriculture and irrigation. \nSize 30 μm–75 μm.\n\nThese are \"pores that are sufficiently small that water within these pores is considered immobile, but available for plant extraction.\" Because there is little movement of water in these pores, solute movement is mainly by the process of diffusion.\nSize 5-30 μm.\n\nThese pores are suitable for habitation by microorganisms. Their distribution is determined by soil texture and soil organic matter, and they are not greatly affected by compaction\nSize 0.1-30 μm.\n\nPores that are too small to be penetrated by most microorganisms. Organic matter in these pores is therefore protected from microbial decomposition. They are filled with water unless the soil is very dry, but little of this water is available to plants, and water movement is very slow. Size <0.1 μm.\n\nBasic crack modeling has been undertaken for many years by simple observations and measurements of crack size, distribution, continuity and depth. These observations have either been surface observation or done on profiles in pits. Hand tracing and measurement of crack patterns on paper was one method used prior to advances in modern technology. Another field method was with the use of string and a semicircle of wire. The semi circle was moved along alternating sides of a string line. The cracks within the semicircle were measured for width, length and depth using a ruler. The crack distribution was calculated using the principle of Buffon's needle.\n\nThis method relies on the fact that crack sizes have a range of different water potentials. At zero water potential at the soil surface an estimate of saturated hydraulic conductivity is produced, with all pores filled with water. As the potential is decreased progressively larger cracks drain. By measuring at the hydraulic conductivity at a range of negative potentials, the pore size distribution can be determined. While this is not a physical model of the cracks, it does give an indication to the sizes of pores within the soil.\n\nHorgan and Young (2000) produced a computer model to create a two-dimensional prediction of surface crack formation. It used the fact that once cracks come within a certain distance of one another they tend to be attracted to each other. Cracks also tend to turn within a particular range of angles and at some stage a surface aggregate gets to a size that no more cracking will occur. These are often characteristic of a soil and can therefore be measured in the field and used in the model. However it was not able to predict the points at which cracking starts and although random in the formation of crack pattern, in many ways, cracking of soil is often not random, but follows lines of weaknesses.\n\nA large core sample is collected. This is then impregnated with araldite and a fluorescent resin. The core is then cut back using a grinding implement, very gradually (~1 mm per time), and at every interval the surface of the core sample is digitally imaged. The images are then loaded into a computer where they can be analysed. Depth, continuity, surface area and a number of other measurements can then be made on the cracks within the soil.\n\nUsing the infinite resistivity of air, the air spaces within a soil can be mapped. A specially designed resistivity meter had improved the meter-soil contact and therefore the area of the reading.\nThis technology can be used to produce images that can be analysed for a range of cracking properties.\n\n"}
{"id": "7028188", "url": "https://en.wikipedia.org/wiki?curid=7028188", "title": "Rauno Aaltonen", "text": "Rauno Aaltonen\n\nRauno August Aaltonen (born 7 January 1938), also known as \"The Rally Professor\", is a Finnish former professional rally driver who competed in the World Rally Championship throughout the 1970s.\n\nBefore WRC was established Aaltonen competed in the European Rally Championship. He won the championship in 1965, with Tony Ambrose as his co-driver. He also won the Finnish Rally Championship in 1961 and 1965. In 1966, he partnered Bob Holden in Australia to win the premier touring car race, the Gallaher 500, in a Mini Cooper S at Mount Panorama in New South Wales.\n\nAaltonen finished second on six occasions in the Safari Rally, which is considered one of the most difficult courses in rallying. In 1985, he was leading the rally by two hours when his engine broke down before the last few special stages. His other merits include winning the 1000 Lakes Rally in 1961, the RAC Rally in 1965, the Monte Carlo Rally in 1967, the Southern Cross Rally in 1977, and a \"Coupe des Alpes\" at the Alpine Rally in 1963 and 1964.\n\nDespite now being remembered as one of the Flying Finns of rallying, Aaltonen started his career on speed boats and later moved on to motorcycles competing in road racing, speedway and motocross. Before he became the first Finnish European Rally Champion, he was the first Finn to win a Grand Prix motorcycle racing competition.\n\nAaltonen was a proponent of left-foot braking. In 2010, he was among the first four inductees into the Rally Hall of Fame, along with Erik Carlsson, Paddy Hopkirk and Timo Mäkinen.\n\n"}
{"id": "41447596", "url": "https://en.wikipedia.org/wiki?curid=41447596", "title": "Renewable energy in Brunei", "text": "Renewable energy in Brunei\n\nRenewable energy in Brunei includes the photovoltaic power generation in Brunei.\n\nBrunei opened its first solar power plant, the 1.2 MW Tenaga Suria Brunei photovoltaic power plant, on 26 May 2011 by Sultan Hassanal Bolkiah. The plant powers up around 200 houses in the nation.\n\n"}
{"id": "21413029", "url": "https://en.wikipedia.org/wiki?curid=21413029", "title": "Saklıkent National Park", "text": "Saklıkent National Park\n\nSaklıkent National Park ()), established on June 6, 1996, is a national park in southwestern Turkey. The national park is a canyon, and is located in Muğla Province, far from Fethiye. Thé Canyon is 65km from Kaş in the Antalya province.\n\nThe canyon is deep and long, being one of the deepest in the world. It was formed through abrasion of the rocks by flowing waters over thousands of years. As the level of water rises during winter months, visitors can enter the canyon all year around only the deeper parts in the summer. of the canyon are walkable after April, when most of the snow in the Taurus Mountains has melted and passed through on its way to the Mediterranean Sea. Saklıkent means \"hidden city\" in Turkish.\nThe full length of 16km is only possible to discover with professional equipment and knowledge of advanced canyoning. Some adventure centers offer guided tours with an overnight Biwak camp and about 30 waterfalls to rappel.\n\n"}
{"id": "2603845", "url": "https://en.wikipedia.org/wiki?curid=2603845", "title": "Sapele", "text": "Sapele\n\nEntandrophragma cylindricum is a tree of the genus \"Entandrophragma\" of the family \"Meliaceae\". It is commonly known as sapele or sapelli ( ) or sapele mahogany, as well as aboudikro, assi and muyovu.\n\nThe name \"sapele\" comes from the city Sapele in Nigeria, where a preponderance of the tree exists. The African Timber and Plywood (AT&P) a division of the United Africa Company factory was based at this location and where the wood, along with \"Triplochiton scleroxylon\", \"Obeche\", mahogany and Khaya was processed into timber which was then exported from the Port of Sapele worldwide.\nThe origin of the name, itself is said to be an anglicized derivation of the Urhobo word Uriapele, named after a local deity. It is believed the British colonial authorities changed the name of the then hamlet to Sapele as it was easier to pronounce.\n\n\"Entandrophragma cylindricum\" is native to tropical Africa. There are protected populations and felling restrictions in place in various countries.\n\nThe species grows to a height of up to 45 m (rarely 60 m). The leaves are deciduous in the dry season, alternately arranged, pinnate, with 5-9 pairs of leaflets, each leaflet about 10 cm long. The flowers are produced in loose inflorescences when the tree is leafless, each flower about 5 mm diameter, with five yellowish petals. The fruit is a pendulous capsule about 10 cm long and 4 cm broad; when mature it splits into five sections to release the 15-20 seeds.\n\nThe commercially important hardwood is reminiscent of mahogany, and is a part of the same \"Meliaceae\" family. It is darker in tone and has a distinctive figure, typically applied where figure is important. Sapele is particularly prized for a lustrous iridescence with colors that range from light pink to brown and gold to red. It has a high density of 640 kg/m and interlocked grain, which can make machining somewhat difficult. Demand for sapele increased as a mahogany substitute in recent years due to \"genuine mahogany\" becoming a CITES Appendix II listed species. It is used in the manufacture of furniture, joinery, veneer, luxury flooring, and boat building.\n\nAmong its more exotic uses is that in musical instruments. It is used for the back and sides of acoustic guitar bodies, as well as the bodies of electric guitars. It is also used in manufacturing the neck piece of ukuleles and 26- and 36-string harps. In the late 90s, it started to be used as a board for Basque percussion instruments txalaparta.\n"}
{"id": "3838003", "url": "https://en.wikipedia.org/wiki?curid=3838003", "title": "Seismic anisotropy", "text": "Seismic anisotropy\n\nSeismic anisotropy is a term used in seismology to describe the directional dependence of the velocity of seismic waves in a medium (rock) within the Earth.\n\nA material is said to be anisotropic if the value of one or more of its properties varies with direction. Anisotropy differs from the property called heterogeneity in that anisotropy is the variation in values with direction at a point while heterogeneity is the variation in values between two or more points.\n\nSeismic Anisotropy can be defined as the dependence of seismic velocity on direction or upon angle. General anisotropy is described by a 4th order elasticity tensor with 21 independent elements. However, in practice observational studies are unable to distinguish all 21 elements, and anisotropy is usually simplified. In the simplest form, there are two main types of anisotropy, both of them are called transverse isotropy (it is called transverse isotropy because there is isotropy in the horizontal or vertical plane) or polar anisotropy. The difference between them is in their axis of symmetry, which is an axis of rotational invariance such that if we rotate the formation about the axis, the material is still indistinguishable from what it was before. The symmetry axis is usually associated with regional stress or gravity.\n\nThe transverse anisotropic matrix has the same form as the isotropic matrix, except that it has five non-zero values distributed among 12 non-zero elements.\n\nTrasverse isotropy is sometimes called transverse anisotropy or anisotropy with hexagonal symmetry. In many cases the axis of symmetry will be neither horizontal nor vertical, in which case it is often called \"tilted\". \n\nAnisotropy dates back to the 19th century following the theory of Elastic wave propagation. Green (1838) and Lord Kelvin (1856) took anisotropy into account in their articles on wave propagation. Anisotropy entered seismology in the late 19th century and was introduced by Maurice Rudzki. From 1898 till his death in 1916, Rudzki attempted to advance the theory of anisotropy, he attempted to determine the wavefront of a transversely isotropic medium (TI) in 1898 and in 1912 and 1913 he wrote on surface waves in transversely isotropic half space and on Fermat’s principle in anisotropic media respectively.\n\nWith all these, the advancement of anisotropy was still slow and in the first 30 years (1920-1950) of exploration seismology only a few papers were written on the subject. More work was done by several scientists such as Helbig (1956) who observed while doing seismic work on Devonian schists that velocities along the foliation were about 20% higher than those across the foliation. However the appreciation of anisotropy increased with the proposition of a new model for the generation of anisotropy in an originally isotropic background and a new exploration concept by Crampin (1987). One of the main points by Crampin was that the polarization of three component shear waves carries unique information about the internal structure of the rock through which they pass, and that shear wave splitting may contain information about the distribution of crack orientations.\n\nWith these new developments and the acquisition of better and new types of data such as three component 3D seismic data, which clearly show the effects of shear wave splitting, and wide Azimuth 3D data which show the effects of Azimuthal anisotropy, and the availability of more powerful computers, anisotropy began to have great impact in exploration seismology in the past three decades.\n\nSince the understanding of seismic anisotropy is closely tied to the shear wave splitting, we begin this section by discussing shear wave splitting.\n\nShear waves have been observed to split into two or more fixed polarizations which can propagate in the particular ray direction when entering an anisotropic medium. These split phases propagate with different polarizations and velocities. Crampin (1984) amongst others gives evidence that many rocks are anisotropic for shear wave propagation. In addition, shear wave splitting is almost routinely observed in three-component VSPs. Such shear wave splitting can be directly analyzed only on three component geophones recording either in the subsurface, or within the effective shear window at the free surface if there are no near surface low-velocity layers. Observation of these shear waves show that measuring the orientation and polarization of the first arrival and the delay between these split shear waves reveal the orientation of cracks and the crack density . This is particularly important in reservoir characterization.\n\nIn a linearly elastic material, which can be described by Hooke's law as one in which each component of stress is dependent on every component of strain, the following relationship exists:\n\nwhere \"σ\" is the stress, \"C\" is the elastic moduli or stiffness constant, and \"e\" is the strain.\n\nThe elastic modulus matrix for an anisotropic case is\nThe above is the elastic modulus for a vertical transverse isotropic medium (VTI), which is the usual case. The elastic modulus for a horizontal transverse isotropic medium (HTI) is;\nFor an anisotropic medium, the directional dependence of the three phase velocities can be written by applying the elastic moduli in the wave equation is; The direction dependent wave speeds for elastic waves through the material can be found by using the Christoffel equation and are given by\nwhere formula_5 is the angle between the axis of symmetry and the wave propagation direction, formula_6 is mass density and the formula_7 are elements of the elastic stiffness matrix. The Thomsen parameters are used to simplify these expressions and make them easier to understand.\n\nSeismic anisotropy has been observed to be weak, and Thomsen (1986) rewrote the velocities above in terms of their deviation from the vertical velocities as follows;\nwhere\nare the P and S wave velocities in the direction of the axis of symmetry (formula_10) (in geophysics, this is usually, but not always, the vertical direction). Note that formula_11 may be further linearized, but this does not lead to further simplification.\n\nThe approximate expressions for the wave velocities are simple enough to be physically interpreted, and sufficiently accurate for most geophysical applications. These expressions are also useful in some contexts where the anisotropy is not weak.\n\nThe Thomsen parameters are anisotropic and are three non-dimensional combinations which reduce to zero in isotropic cases, and are defined as\n\nAnisotropy has been reported to occur in the Earth's three main layers; the crust, mantle and the core.\n\nThe origin of seismic anisotropy is non-unique, a range of phenomena may cause Earth materials to display seismic anisotropy. The anisotropy may be strongly dependent on wavelength if it is due to the average properties of aligned or partially aligned heterogeneity. A solid has intrinsic anisotropy when it is homogeneously and sinuously anisotropic down to the smallest particle size, which may be due to crystalline anisotropy. Relevant crystallographic anisotropy can be found in the upper mantle. When an otherwise isotropic rock contains a distribution of dry or liquid-filled cracks which have preferred orientation it is named crack induced anisotropy. The presence of aligned cracks, open or filled with some different material, is an important mechanism at shallow depth, in the crust. It is well known that the small-scale, or microstructural, factors include (e.g. Kern & Wenk 1985; Mainprice et al. 2003): (1) crystal lattice preferred orientation (LPO) of constituent mineral phases; (2) variations in spatial distribution of grains and minerals; (3) grain morphology and (4) aligned fractures, cracks and pores, and the nature of their infilling material (e.g. clays, hydrocarbons, water, etc.). Because of the overall microstructural control on seismic anisotropy, it follows that anisotropy can be diagnostic for specific rock types. Here, we consider whether seismic anisotropy can be used as an indicator of specific sedimentary lithologies within the Earth’s crust.\nIn sedimentary rocks, anisotropy develops during and after deposition. For anisotropy to develop, there needs to be some degree of homogeneity or uniformity from point to point in the deposited clastics. During deposition, anisotropy is caused by the periodic layering associated with changes in sediment type which produces materials of different grain size, and also by the directionality of the transporting medium which tends to order the grains under gravity by grain sorting. Fracturing and some diagenetic processes such as compaction and dewatering of clays, and alteration etc. are post depositional processes that can cause anisotropy .\n\nIn the past two decades, the seismic anisotropy has dramatically been gaining attention from academic and industry, due to advances in anisotropy parameter estimation, the transition from post stack imaging to pre stack depth migration, and the wider offset and azimuthal coverage of 3D surveys. Currently, many seismic processing and inversion methods utilize anisotropic models, thus providing a significant enhancement over the seismic imaging quality and resolution. The integration of anisotropy velocity model with seismic imaging has reduced uncertainty on internal and bounding-fault positions, thus greatly reduce the risk of investment decision based heavily on seismic interpretation.\n\nIn addition, the establishment of correlation between anisotropy parameters, fracture orientation, and density, lead to practical reservoir characterization techniques. The acquisition of such information, fracture spatial distribution and density, the drainage area of each producing well can be dramatically increased if taking the fractures into account during the drilling decision process. The increased drainage area per well will result in fewer wells, greatly reducing the drilling cost of exploration and production (E&P) projects.\n\nAmong several applications of seismic anisotropy, the following are the most important: anisotropic parameter estimation, prestack depth anisotropy migration, and fracture characterization based on anisotropy velocity models.\n\nThe anisotropy parameter is most fundamental to all other anisotropy application in E&P area. In the early days of seismic petroleum exploration, the geophysicists were already aware of the anisotropy-induced distortion in P-wave imaging (the major of petroleum exploration seismic surveys). Although the anisotropy-induced distortion is less significant since the poststack processing of narrow-azimuth data is not sensitive to velocity. The advancement of seismic anisotropy is largely contributed by the Thomsen's work on anisotropy notation and also by the discovery of the P-wave time-process parameter formula_13 . These fundamental works enable to parametrize the transverse isotropic (TI) models with only three parameters, while there are five full independent stiff tensor element in transverse isotropic (VTI or HTI) models. This simplification made the measurement of seismic anisotropy a plausible approach.\n\nMost anisotropy parameter estimation work is based on shale and silts, which may be due to the fact that shale and silts are the most abundant sedimentary rocks in the earth’s crust. Also in the context of petroleum geology, organic shale is the source rock as well as seal rocks that trap oil and gas. In seismic exploration, shales represent the majority of the wave propagation medium overlying the petroleum reservoir. In conclusion, seismic properties of shale are important for both exploration and reservoir management.\n\nSeismic velocity anisotropy in shale can be estimated from several methods, including deviated-well sonic logs, walkway VSP, and core measurement. These methods have their own advantages and disadvantages: the walkway VSP method suffers from scaling issues, and core measure is impractical for shale, since shale is hard to be cored during drilling.\n\nThe Walkway VSP array several seismic surface sources at different offset from the well. Meanwhile, a vertical receiver array with constant interval between receivers is mounted in a vertical well. The sound arrival times between multiple surface sources and receivers at multiple depths are recorded during measurement. These arrival times are used to derive the anisotropy parameter based on the following equations\nWhere formula_15 is the arrival time from source with formula_16 offset, formula_17 is the arrival time of zero offset, formula_18 is NMO velocity, formula_13 is Thompson anisotropy parameter.\n\nThe layout of surface sources and receivers positions is shown in the following diagram.\n\nAnother technique used to estimate the anisotropy parameter is directly measure them from the core which is extracted through a special hollow drill bit during drill process. Since coring a sample will generate large extra cost, only limited number of core samples can be obtained for each well. Thus the anisotropy parameter obtained through core measurement technique only represent the anisotropy property of rock near the borehole at just several specific depth, rending this technique often provides little help on the field seismic survey application. The measurements on each shale plug require at least one week.\nFrom the context of this article, wave propagation in a vertically transverse medium can be described with five elastic constants, and ratios among these parameters define the rock anisotropy . This anisotropy parameter can be obtained in the laboratory by measuring the velocity travel speed with transducer ultrasonic systems at variable saturation and pressure conditions. Usually, three directions of wave propagation on core samples are the minimum requirement to estimate the five elastic coefficients of the stiffness tensor. Each direction in core plug measurement yields three velocities (one P and two S).\n\nThe variation of wave propagation direction can be achieved by either cutting three samples at 0°, 45° and 90° from the cores or by using one core plug with transducers attached at these three angles. Since most shales are very friable and fissured, it is often difficult to cut shale core plug. Its edges break off easily. Thus the cutting sample method can only be used for hard, competent rocks. The cutting position of samples can be explained by the following diagram.\n\nAnother way to get the wave propagation velocity at three directions is to arrange the ultrasonic transducer onto several specific location of the core sampler. This method avoids the difficulties encounter during the cutting of shale core sample. It also reduces the time of measurement by two thirds since three pairs of ultrasonic transducer work at the same time. The following diagram gives us a clear image on the arrangement of the transducers.\n\nOnce the velocities at three directions are measured by one of the above two methods, the five independent elastic constants are given by the following equations:\nThe P-wave anisotropy of a VTI medium can be described by using Thomsen's parameters formula_21. formula_22 quantifies the velocity difference for wave propagation along and perpendicular to the symmetry axis, while formula_11 controls the P-wave propagation for angles near the symmetry axis.\n\nThe last technique can be used to measure the seismic anisotropy is related to the sonic logging information of a deviated well. In a deviated well, the wave propagation velocity is higher than the wave propagation velocity in a vertical well at the same depth. This difference in velocity between deviated well and vertical well reflects the anisotropy parameters of the rocks near the borehole. The detail of this technique will be shown on an example of this report.\n\nIn the situation of complex geology, e.g. faulting, folding, fracturing, salt bodies, and unconformities, pre-stack migration (PreSM) is used due to better resolution under such complex geology. In PreSM, all traces are migrated before being moved to zero-offset. As a result, much more information is used, which results in a much better image, along with the fact that PreSM honours velocity changes more accurately than post-stack migration. The PreSM is extremely sensitive to the accuracy of the velocity field. Thus the inadequacy of isotropic velocity models is not suitable for the pre stack depth migration. P-wave anisotropic prestack depth migration (APSDM) can produce a seismic image that is very accurate in depth and space. As a result, unlike isotropic PSDM, it is consistent with well data and provides an ideal input for reservoir characterization studies. However, this accuracy can only be achieved if correct anisotropy parameters are used. These parameters cannot be estimated from seismic data alone. They can only be determined with confidence through analysis of a variety of geoscientific material – borehole data and geological history.\n\nDuring recent years, the industry has started to see the practical use of anisotropy in seismic imaging. We show case studies that illustrate this integration of the geosciences. We show that much better accuracy is being achieved. The logical conclusion is that, this integrated approach should extend the use of anisotropic depth imaging from complex geology only, to routine application on all reservoirs.\n\nAfter considering applications of anisotropy that improved seismic imaging, two approaches for exploiting anisotropy for the analysis of fractures in the formation are worthy of discussing. Ones uses azimuthal variations in the amplitude versus offset (AVO) signature when the wave is reflected from the top or base of an anisotropic material, and a second exploits the polarizing effect that the fractures have on a transmitted shear-wave. In both cases, the individual fractures are below the resolving power of the seismic signal and it is the cumulative effect of the fracturing that is recorded. Based on the idea behind them, both approaches can be divided into two steps. The first step is to get the anisotropy parameters from seismic signals, and the second steps is to retreat the information of fractures from anisotropy parameters based on the fracture induce anisotropy model.\n\nAligned subseismic-scale fracturing can produce seismic anisotropy (i.e., seismic velocity varies with direction) and leads to measurable directional differences in traveltimes and reflectivity.\nIf the fractures are vertically aligned, they will produce azimuthal anisotropy (the simplest case being horizontal transverse isotropy, or HTI) such that reflectivity of an interface depends on azimuth as well as offset. If either of the media bounding the interface is azimuthally anisotropic, the AVO will have an azimuthal dependence . The P-P wave reflection coefficient have the following relation with the azimutal if anisotropy exist in the layers:\n\nWhere formula_25 is the azimuth from data acquisition grid, the terms formula_26 are coefficients describing anisotropy parameter.\n\nThe behavior of shear waves as they pass through anisotropic media has been recognized for many years, with laboratory and field observations demonstrating how the shear wave splits into two polarized components with their planes aligned parallel and perpendicular to the anisotropy. For a fractured medium, the faster shear wave is generally aligned with the strike direction and the time delay between the split shear waves related to the fracture density and path length traveled. For layered medium, the shear wave polarized parallel to the layering arrives first.\n\nTwo examples will be discussed in there to show the anisotropy application in Petroleum E&P area. The first related to anisotropy parameter estimation via deviated well sonic logging tool. And the second example reflects the image quality improvement by PreStack Depth Migration technology.\n\nIn this case, the sonic velocity in a deviated well is obtained by dipole sonic logging tool . The formation is mostly composed of shale. In order to use the TI model, several assumptions are made:\n\nSatisfying the above conditions, the following equation hold for a TI model:\nWhere formula_25 is the deviated angle of the well, and formula_11, formula_30 are anisotropy parameter.\n\nThe following plot shows typical velocity distribution vs density in a deviated well. The color of each data point represents the frequency of this data point. The red color means a high frequency while the blue color represents a low frequency. The black line shows a typical velocity trend without the effect of anisotropy. Since the existence of anisotropy effect, the sound velocity is higher than the trend line.\n\nFrom the well logging data, the velocity vs formula_25 plot can be drawn. On the basis of this plot, a no liner regression will give us an estimate of formula_11 and formula_30. The following plot show the non-linear regression and its result.\n\nPut the estimated formula_11 and formula_30 into the following equation, the correct formula_36 can be obtained.\nBy doing the above correction calculation, the corrected formula_36 is plot vs density in the following plot. As be seen in the plot, most of the data point falls on the trend line. It validate the correctness of the estimate of anisotropy parameter.\n\nIn this case, the operator conducted several seismic surveys on a gas field in the north sea over the period of 1993-1998 . The early survey does not take into anisotropy into account, while the later survey employs the PreStack Depth Migration imaging. This PSDM was done on a commercial seismic package developed by Total. The following two plots clearly reveal the resolution improvement of the PSDM method. The top plot is a convention 3D survey without anisotropy effect. The bottom plot used PSDM method. As can be seen in the bottom plot, more small structure features are revealed due to the reduce of error and improved resolution.\n\nSeismic anisotropy relies on shear waves, shear waves carry rich information which can sometimes impede its utilization. Shear waves survey for anisotropy requires multi component (usually 3 component) geophones which are oriented at angles, these are more expensive than the widely used vertical oriented single component geophones. However, while expensive 3 component seismometers are much more powerful in their ability to collect valuable information about the Earth that vertical component seismometers simply cannot. While seismic waves do attenuate, large earthquakes (moment magnitude > 5) have the ability to produce observable shear waves. The second law of thermodynamics ensures a higher attenuation of shear wave reflected energy, this tends to impede the utilization of shear wave information for smaller earthquakes.\n\nIn the Earth's crust, anisotropy may be caused by preferentially aligned joints or microcracks, by layered bedding in sedimentary formations, or by highly foliated metamorphic rocks. Crustal anisotropy resulting from aligned cracks can be used to determine the state of stress in the crust, since in many cases, cracks are preferentially aligned with their flat faces oriented in the direction of minimum compressive stress. In active tectonic areas, such as near faults and volcanoes, anisotropy can be used to look for changes in preferred orientation of cracks that may indicate a rotation of the stress field.\n\nBoth seismic P-waves and S-waves may exhibit anisotropy. For both, the anisotropy may appear as a (continuous) dependence of velocity upon the direction of propagation. For S-waves, it may also appear as a (discrete) dependence of velocity upon the direction of polarization. For a given direction of propagation in any homogeneous medium, only two polarization directions are allowed, with other polarizations decomposing trigonometrically into these two. Hence, shear waves naturally \"split\" into separate arrivals with these two polarizations; in optics this is called birefringence.\n\nCrustal anisotropy is very important in the production of oil reservoirs, as the seismically fast directions can indicate preferred directions of fluid flow.\n\nIn crustal geophysics, the anisotropy is usually weak; this enables a simplification of the expressions for seismic velocities and reflectivities, as functions of propagation (and polarization) direction. In the simplest geophysically plausible case, that of polar anisotropy, the analysis is most conveniently done in terms of Thomsen Parameters.\n\nIn the mantle, anisotropy is normally associated with crystals (mainly olivine) aligned with the mantle flow direction called lattice preferred orientation (LPO). Due to their elongate crystalline structure, olivine crystals tend to align with the flow due to mantle convection or small scale convection. Anisotropy has long been used to argue whether plate tectonics is driven from below by mantle convection or from above by the plates, i.e. slab pull and ridge push.\n\nIn the transition zone, wadsleyite and/or ringwoodite could be aligned in LPO. Below the transition zone, the three main minerals, periclase, silicate perovskite (bridgmanite), and post-perovskite are all anisotropic and could be generating anisotropy observed in the D\" region (a couple hundred kilometer thick layer about the core-mantle boundary).\n\n\n"}
{"id": "2934609", "url": "https://en.wikipedia.org/wiki?curid=2934609", "title": "Soil functions", "text": "Soil functions\n\nSoil functions are general capabilities of soils that are important for various agricultural, environmental, nature protection, landscape architecture and urban applications. Six key soil functions are:,\n\n\nSoil maps can depict soil properties and functions in the context of specific soil functions such as agricultural food production, environmental protection, and civil engineering considerations. Maps can depict functional interpretations of specific properties such as critical nutrient levels, heavy-metal levels or can depict interpretation of multiple properties such as a map of erosion risk index.\n\nMapping of function specific soil properties is an extension of soil survey, using maps of soil components together with auxiliary information (including pedotransfer functions and soil inference models) to depict inferences about the specific performance of soil mapping units.\nOther function of soil in our ecosystem:\n\n"}
{"id": "24750606", "url": "https://en.wikipedia.org/wiki?curid=24750606", "title": "Sumapaz Páramo", "text": "Sumapaz Páramo\n\nSumapaz Páramo (Spanish: Páramo de Sumapaz - meaning \"Utterly peaceful moorland\" ) is a large páramo ecosystem located in the Altiplano Cundiboyacense mountain range, considered the largest páramo ecosystem in the world. It was declared a National Park of Colombia in 1977 because of its importance as a biodiversity hotspot and main source of water for the most densely populated area of the country, the Bogotá savanna.\n\nSumapaz Páramo was considered a sacred place for the Muisca indigenous people. It was associated with the divine forces of creation and the origin of mankind, a domain where humans were not supposed to enter. \n\nDuring the 16th century, German adventurer and conquistador Nikolaus Federmann conducted an expedition crossing the Sumapaz, searching for El Dorado mythic treasure, with heavy casualties. The place was named by the Spaniards \"País de la Niebla\" (\"Country of Fog\") because of the dense clouds at ground level, with great decrease in visibility.\n\nIn 1783, José Celestino Mutis led the Botanic Expedition, with the purpose of studying the flora and fauna of the region. However, the páramo was not visited because of its harsh climatic conditions. The German naturalist Alexander von Humboldt made the first description of the páramo and the local plants in 1799. He also described the presence of glacier valleys and associated the geologic features of the region, comparing them with those seen in the geomorphology of the Alps. \n\nDuring the early 20th century, the Spanish naturalist José Cuatrecasas made important research of the páramo and the tree line. Other scientists that described and studied Sumapaz páramo were Ernesto Guhl, who conducted a long-term 3-decade research of the vegetal communities, and Thomas van der Hammen.\n\nSumapaz Páramo has an inhospitable, cold climate with temperatures averaging below 10 °C (50 °F).(ranging from −10 °C to 17 °C) with quick changes from short periods of warm climate to freezing cold. The average altitude oscillates between 3500 and 4000 m. AMSL. The highest point is the Nevado del Sumapaz peak (4306 m AMSL). The precipitation is about 700–1000 mm/year. The rainy season lasts almost the entire year, except from December to February, when the sunlight reaches a peak, with intense ultraviolet radiation (adaptations such as white, glassy coloration help the local plants to survive). The humidity is usually high, (from 50 to 90%), and the ground remains soaked, and covered by shallow bodies of water and sticky mud, often covered with dense, flat vegetation difficult to spot by the unexperienced visitor, with danger of falling into them, and risk of drowning or other injuries. These places are called \"Chupaderos\" or \"Chucuas\" (\"Drainages\").\n\nSumapaz lies between the Orinoco River basin and the Magdalena River basin, the two main fluvial systems of Colombia, and provides tributaries to both. \nAll but one of the tributaries of the Sumapaz River originate in the páramo.\nIts location on the Thermal equator generates high rates of precipitation, which together with its endemic flora that regulate the soil moisture acting like sponges for the rain waters, contribute to the high amount of surface water and its role as source of water reservoirs.\n\nThe eastern part of Sumapaz consists of Devonian metamorphic rock formations, with fault scarped configuration, and alpine landscapes. Its western part consists of Oligocene sedimentary rocks, with softer landscapes. The different stages of the Quaternary glaciation left plenty of glacier debris, and glacier lakes such as Chisaca lake. During the Last Glacial Maximum, the glacier motion of the ice sheets through the Tunjuelo valley reached as far as Usme (today part of Bogotá).\n\nThe soil of this region is acidic, with high levels of sodium and potassium. This is a coarse-grained soil, with high permeability favoring the formation of groundwater in aquifers. The composition of the soil and the low temperatures contribute to the low amount of humus and poor decomposing of the organic matter making this soil largely unsuitable for agriculture.\n\nOver 200 species of vascular plants are native to the area with substantial amount of endemisms. The most representative plants of the area are the Espeletias. Several species have been described here, the most common being Espeletia grandiflora Humb. & Bonpl. The largest one is Espeletia uribei Cuatrec., with specimens up to 12 meters of height, other species are: Espeletia algodonosa Aristeg. Espeletia banksiifolia Sch.Bip. & Ettingsh. ex Wedd. Espeletia cuatrecasasii Ruíz-Terán & López-Fig. Espeletia formosa S.Díaz & Rodr.-Cabeza \nEspeletia glossophylla Mattf. Espeletia killipii Cuatrec. Espeletia picnophyla Cuatrec. Espeletia schultzii (Benth.) W.M.Curtis and Espeletia curialensis Cuatrec. The Sphagnum moss covers wide areas of Sumapaz, which increases the soil's capacity to hold water and nutrients by increasing capillary forces and cation exchange capacity. In the canyons areas, encenillo tree and tibouchina are the dominant species. The European plant Digitalis purpurea is an introduced species, the way of its introduction is not known, either deliberate or accidental.\n\nThe endangered spectacled bear lives in Sumapaz, its main source of food being the \"Puya boyacana\" fruits and the Espeletia plant stems, (known as caulirosule). Other animals described are: Little Red Brocket Deer, tapir, coati, golden eagle, torrent duck, Páramo duck (Anas georgica). An introduced species in the waterbodies is the rainbow trout.\n\nAlthough the soil and climate are adverse for agriculture and other economic activities, human settlements do exist in the Sumapaz Páramo, including the villages of San Juan de Sumapaz, Nazareth, Santa Rosa and El Hato (only the first two have road access) with an estimated 1200 families, most of them under the poverty threshold, living on less than $1.25 per day, without schools or sanitation. In consequence, the peasants often invade the protected area to grow potato crops. The natural forest line is severely altered by human activity (logging, intensive grazing), which makes the difference between natural and artificial grasslands difficult to distinguish. An estimated 10,000 heads of cattle live or feed within the protected area. In 1950, president Mariano Ospina Pérez ordered the Colombian banks not to approve loans destined to establishment of crops or cattle at altitudes higher than as an attempt to discourage such activities. \n\nIllegal armed groups such as FARC and ELN guerrillas used the area in recent years as a corridor for the transportation of kidnapping victims, weapon trafficking and drug trafficking. The Colombian government, in accordance with democratic security policies, established a center of military operations in 2002: the General Antonio Arredondo Military base, achieving the withdrawal of the illegal forces. However, the presence of the Colombian army has generated controversy over the environmental impact, with alleged destruction of the frailejones, whose leaves are supposedly collected by the soldiers for making rudimentary mattresses to sleep on.\n\n"}
{"id": "27973657", "url": "https://en.wikipedia.org/wiki?curid=27973657", "title": "Sumatran tropical pine forests", "text": "Sumatran tropical pine forests\n\nThe Sumatran tropical pine forests is a subtropical coniferous forest ecoregion on the island of Sumatra in Indonesia.\n\nThese pine forests are found on the higher slopes of Sumatra, especially in the north of the island near Lake Toba and along the Barisan Mountains including the tall Mount Leuser. With 2500mm of rainfall per year the pine forests have a tropical rainforest climate but are drier than the thick rainforest areas lower down the slopes, especially on the drier eastern aspects of the mountains.\n\nThis ecoregion is one of the rare areas of pine forest in the tropics with the dominant species Sumatran pine (\"Pinus merkusii\"), which has become established in areas where rainforest has been disturbed throughout history by events including landslides and forest fires as well as human clearance. Forest fire is an ongoing and continuous feature of the life-cycle of the forest.\n\nThere is less wildlife here than in the rainforest that covers most of the island and there are no endemic mammals but there are still a number of near-endemic species including birds like the Sunda robin.\n\nThe pine forests of the higher areas are less vulnerable to clearance than the valuable hardwood rainforests lower down and furthermore a third of them are within the Kerinci Seblat and other national parks.\n\n\nWikramanayake, Eric; Eric Dinerstein; Colby Loucks; et al. (2002). \"Terrestrial Ecoregions of the Indo-Pacific: a Conservation Assessment\". Island Press; Washington, DC.\n"}
{"id": "34459743", "url": "https://en.wikipedia.org/wiki?curid=34459743", "title": "Tantony", "text": "Tantony\n\nTantony is a shorted form of the name of St. Anthony the Abbot, a prominent figure among the Desert Fathers. It is used in reference to the attributes by which the saint is represented. These primarily are the \"Tantony Pouch\", \"Tantony Bell\" (a small church bell), \"Tantony Pig\" (the smallest pig of the litter).\n\nAnthony became the patron saint of swineherds, due to the reported relationship a pig had with him in keeping him attuned to the hours of the day for his prayers, one of several animals who played a role in his life. Thus he is most frequently represented as accompanied by a pig.\n\nIn Spain and its former colonies, this Anthony is honored as the patron saint of domestic animals. They would be blessed on his feast day (17 January), as today is done elsewhere for the feast of St. Francis of Assisi. Large bonfires are still lit on this day in Spain for the feast, with reveling held around it, focusing on the consumption of pork and sausages.\n"}
{"id": "5743225", "url": "https://en.wikipedia.org/wiki?curid=5743225", "title": "Themis (solar power plant)", "text": "Themis (solar power plant)\n\nThe THEMIS solar power tower is a research and development centre focused on solar energy. It is located near the village of Targassonne, in the department of Pyrénées-Orientales, south of France, 3 kilometres from the world's largest solar furnace in Odeillo.\n\nTHEMIS solar power tower, owned by the General Council of the Pyrénées-Orientales, was strategically located in the region of Cerdanya, in the Pyrénées-Orientales, because the conditions are excellent for solar energy. First, Cerdanya has almost 2400 hours of sunshine a year; second, there is a very low wind limiting the time of disruption of the plant; third, the site is at a high elevation providing stronger sunlight. The land chosen for the plant is on a slope between 6° and 18°, which is suitable for a power tower.\n\nResearch and development is focused on solar energy, and THEMIS is a research facility as well as a photovoltaic power facility and a solar thermal energy plant.\n\nThe plant had a power output of 2 MW in 1983. It was based on an array of 201 mirrors which heated a cavity receiver (a cavity lined with coolant tubes) at the top of a 100 m tower where the coolant (molten salts) carried the thermal energy to a steam generator, itself powering an electric turbine. The molten salts were potassium nitrate (53%), sodium nitrite (40%) and sodium nitrate (7%). The coolant entry temperature was 250 °C and the exit temperature 450 °C. Steam was produced in the generator at a pressure of 50 bar and a temperature of 430 °C.\n\nThe first THEMIS solar plant, was an experimental solar facility which produced power between 1983 and 1986, and then closed in part due to the difficulty of managing the cooling system, and in part due to a lack of political and financial support. Construction started in 1979 at a cost of 300 million French francs (about 45 million euros), and was operated and managed by Électricité de France (EDF).\n\nThe plant went into hibernation for more than twenty years, and turned into a scientific facility of the CERN, and the Commissariat à l'énergie atomique focusing on astrophysics, with an open air Cherenkov Telescope, measuring gamma rays hitting the atmosphere (see IACT).\n\nIn 2004, a rehabilitation program was devised by the General Council of the Pyrénées-Orientales, to produce power, and create a research and development centre on solar energy with the Centre national de la recherche scientifique (CNRS), and Tecsol, a local engineering office.\n\nTHEMIS still has its 201 sun-following devices called heliostats, most of them still equipped with mirrors (53.70 m² each) covering an area of 11,800 m² in total, able to send the solar energy towards the hot spot at the top of the 104 m central tower, where the receiver was originally placed (which is now part of the exhibition outside the Odeillo solar furnace). The heliostats are, however, no longer operational.\n\nThe three rows of heliostats furthest away from the tower are having their mirrors replaced by solar cells. Of these cells, some will follow the sun while others will not, so as to measure the difference in efficiency.\n\nThe CNRS rehabilitation project has repaired half of the heliostats in order to develop a power of 1 MWe, by means of a gas turbine installed at the top of the tower, fed either with hot air directly from an air receiver (PEGASE project) or a receiver with particles which also serves to store thermal energy: European project Next-CSP project website, 2017-2021.\n\n\n"}
{"id": "39221", "url": "https://en.wikipedia.org/wiki?curid=39221", "title": "Thermodynamic free energy", "text": "Thermodynamic free energy\n\nThe thermodynamic free energy is a concept useful in the thermodynamics of chemical or thermal processes in engineering and science. The change in the free energy is the maximum amount of work that a thermodynamic system can perform in a process at constant temperature, and its sign indicates whether a process is thermodynamically favorable or forbidden. Since free energy usually contains potential energy, it is not absolute but depends on the choice of a zero point. Therefore, only relative free energy values, or changes in free energy, are physically meaningful.\n\nThe free energy is a thermodynamic state function, like the internal energy, enthalpy, and entropy. \n\nFree energy is that portion of any first-law energy that is available to perform thermodynamic work at constant temperature, \"i.e.\", work mediated by thermal energy. Free energy is subject to irreversible loss in the course of such work. Since first-law energy is always conserved, it is evident that free energy is an expendable, second-law kind of energy. Several free energy functions may be formulated based on system criteria. Free energy functions are Legendre transforms of the internal energy. \n\nThe Gibbs free energy is given by , where \"H\" is the enthalpy, \"T\" is the absolute temperature, and \"S\" is the entropy. , where \"U\" is the internal energy, \"p\" is the pressure, and \"V\" is the volume. \"G\" is the most useful for processes involving a system at constant pressure \"p\" and temperature \"T\", because, in addition to subsuming any entropy change due merely to heat, a change in \"G\" also excludes the work needed to \"make space for additional molecules\" produced by various processes. Gibbs free energy change therefore equals work not associated with system expansion or compression, at constant temperature and pressure. (Hence its utility to solution-phase chemists, including biochemists.) \n\nThe historically earlier Helmholtz free energy is defined as . Its change is equal to the amount of reversible work done on, or obtainable from, a system at constant \"T\". Thus its appellation \"work content\", and the designation \"A\" from \"Arbeit\", the German word for work. Since it makes no reference to any quantities involved in work (such as \"p\" and \"V\"), the Helmholtz function is completely general: its decrease is the maximum amount of work which can be done \"by\" a system at constant temperature, and it can increase at most by the amount of work done \"on\" a system isothermally. The Helmholtz free energy has a special theoretical importance since it is proportional to the logarithm of the partition function for the canonical ensemble in statistical mechanics. (Hence its utility to physicists; and to gas-phase chemists and engineers, who do not want to ignore work.)\n\nHistorically, the term 'free energy' has been used for either quantity. In physics, \"free energy\" most often refers to the Helmholtz free energy, denoted by \"A\" or \"F\", while in chemistry, \"free energy\" most often refers to the Gibbs free energy. The values of the two free energies are usually quite similar and the intended free energy function is often implicit in manuscripts and presentations.\n\nThe basic definition of \"energy\" is a measure of a body's (in thermodynamics, the system's) ability to cause change. For example, when a person pushes a heavy box a few meters forward, that person exerts mechanical energy, also known as work, on the box over a distance of a few meters forward. The mathematical definition of this form of energy is the product of the force exerted on the object and the distance by which the box moved (Work=Force x Distance). Because the person changed the stationary position of the box, that person exerted energy on that box. The work exerted can also be called \"useful energy\". Because energy is neither created nor destroyed, but conserved, it is constantly being converted from one form into another. For the case of the person pushing the box, the energy in the form of internal (or potential) energy obtained through metabolism was converted into work in order to push the box. This energy conversion, however, is not linear. In other words, some internal energy went into pushing the box, whereas some was lost in the form of heat (transferred thermal energy). For a reversible process, heat is the product of the absolute temperature \"T\" and the change in entropy \"S\" of a body (entropy is a measure of disorder in a system). The difference between the change in internal energy, which is Δ\"U\", and the energy lost in the form of heat is what is called the \"useful energy\" of the body, or the work of the body performed on an object. In thermodynamics, this is what is known as \"free energy\". In other words, free energy is a measure of work (useful energy) a system can perform at constant temperature. Mathematically, free energy is expressed as:\n\nfree energy \"A\" = \"U\" - \"TS\"\n\nThis expression has commonly been interpreted to mean that work is extracted from the internal energy \"U\" while \"TS\" represents energy not available to perform work. However, this is incorrect. For instance, in an isothermal expansion of an ideal gas, the free energy change is Δ\"U\" = 0 and the expansion work \"w\" = -\"T\" Δ\"S\" is derived exclusively from the \"TS\" term supposedly not available to perform work.\n\nIn the 18th and 19th centuries, the theory of heat, i.e., that heat is a form of energy having relation to vibratory motion, was beginning to supplant both the caloric theory, i.e., that heat is a fluid, and the four element theory, in which heat was the lightest of the four elements. In a similar manner, during these years, heat was beginning to be distinguished into different classification categories, such as “free heat”, “combined heat”, “radiant heat”, specific heat, heat capacity, “absolute heat”, “latent caloric”, “free” or “perceptible” caloric (\"calorique sensible\"), among others.\n\nIn 1780, for example, Laplace and Lavoisier stated: “In general, one can change the first hypothesis into the second by changing the words ‘free heat, combined heat, and heat released’ into ‘vis viva, loss of vis viva, and increase of vis viva.’” In this manner, the total mass of caloric in a body, called \"absolute heat\", was regarded as a mixture of two components; the free or perceptible caloric could affect a thermometer, whereas the other component, the latent caloric, could not. The use of the words “latent heat” implied a similarity to latent heat in the more usual sense; it was regarded as chemically bound to the molecules of the body. In the adiabatic compression of a gas, the absolute heat remained constant but the observed rise in temperature implied that some latent caloric had become “free” or perceptible.\n\nDuring the early 19th century, the concept of perceptible or free caloric began to be referred to as “free heat” or heat set free. In 1824, for example, the French physicist Sadi Carnot, in his famous “Reflections on the Motive Power of Fire”, speaks of quantities of heat ‘absorbed or set free’ in different transformations. In 1882, the German physicist and physiologist Hermann von Helmholtz coined the phrase ‘free energy’ for the expression , in which the change in \"F\" (or \"G\") determines the amount of energy ‘free’ for work under the given conditions, specifically constant temperature.\n\nThus, in traditional use, the term “free” was attached to Gibbs free energy for systems at constant pressure and temperature, or to Helmholtz free energy for systems at constant temperature, to mean ‘available in the form of useful work.’ With reference to the Gibbs free energy, we need to add the qualification that it is the energy free for non-volume work.\n\nAn increasing number of books and journal articles do not include the attachment “free”, referring to \"G\" as simply Gibbs energy (and likewise for the Helmholtz energy). This is the result of a 1988 IUPAC meeting to set unified terminologies for the international scientific community, in which the adjective ‘free’ was supposedly banished. This standard, however, has not yet been universally adopted, and many published articles and books still include the descriptive ‘free’.\n\nJust like the general concept of energy, free energy has a few definitions suitable for different conditions. In physics, chemistry, and biology, these conditions are thermodynamic parameters (temperature \"T\", volume \"V\", pressure \"p\", etc.). Scientists have come up with several ways to define free energy. The mathematical expression of Helmholtz free energy is:\n\nThis definition of free energy is useful for gas-phase reactions or in physics when modeling the behavior of isolated systems kept at a constant volume. For example, if a researcher wanted to perform a combustion reaction in a bomb calorimeter, the volume is kept constant throughout the course of a reaction. Therefore, the heat of the reaction is a direct measure of the free energy change, \"q\" = Δ\"U\". In solution chemistry, on the other hand, most chemical reactions are kept at constant pressure. Under this condition, the heat \"q\" of the reaction is equal to the enthalpy change Δ\"H\" of the system. Under constant pressure and temperature, the free energy in a reaction is known as Gibbs free energy G.\n\nThese functions have a minimum in chemical equilibrium, as long as certain variables (\"T\", and \"V\" or \"p\") are held constant. In addition, they also have theoretical importance in deriving Maxwell relations. Work other than may be added, e.g., for electrochemical cells, or work in elastic materials and in muscle contraction. Other forms of work which must sometimes be considered are stress-strain, magnetic, as in adiabatic demagnetization used in the approach to absolute zero, and work due to electric polarization. These are described by tensors.\n\nIn most cases of interest there are internal degrees of freedom and processes, such as chemical reactions and phase transitions, which create entropy. Even for homogeneous \"bulk\" materials, the free energy functions depend on the (often suppressed) composition, as do all proper thermodynamic potentials (extensive functions), including the internal energy.\n\n\"N\" is the number of molecules (alternatively, moles) of type \"i\" in the system. If these quantities do not appear, it is impossible to describe compositional changes. The differentials for processes at uniform pressure and temperature are (assuming only \"pV\" work):\n\nwhere \"μ\" is the chemical potential for the \"i\"th component in the system. The second relation is especially useful at constant \"T\" and \"p\", conditions which are easy to achieve experimentally, and which approximately characterize living creatures. Under these conditions, it simplifies to\n\nAny decrease in the Gibbs function of a system is the upper limit for any isothermal, isobaric work that can be captured in the surroundings, or it may simply be dissipated, appearing as \"T\" times a corresponding increase in the entropy of the system and/or its surrounding.\n\nAn example is surface free energy, the amount of increase of free energy when the area of surface increases by every unit area.\n\nThe path integral Monte Carlo method is a numerical approach for determining the values of free energies, based on quantum dynamical principles.\n\nFor a reversible isothermal process, Δ\"S = q/T\" and therefore the definition of \"A\" results in\n\nThis tells us that the change in free energy equals the reversible or maximum work for a process performed at constant temperature. Under other conditions, free-energy change is not equal to work; for instance, for a reversible adiabatic expansion of an ideal gas, formula_7. Importantly, for a heat engine, including the Carnot cycle, the free-energy change after a full cycle is zero, formula_8, while the engine produces nonzero work.\n\nAccording to the second law of thermodynamics, for any process that occurs in a closed system, the inequality of Clausius, Δ\"S > q/T\", applies. For a process at constant temperature and pressure without non-\"PV\" work, this inequality transforms into formula_9. Similarly, for a process at constant temperature and volume, formula_10. Thus, a negative value of the change in free energy is a necessary condition for a process to be spontaneous; this is the most useful form of the second law of thermodynamics in chemistry. In chemical equilibrium at constant \"T\" and \"p\" without electrical work, d\"G\" = 0.\n\nThe quantity called \"free energy\" is a more advanced and accurate replacement for the outdated term \"affinity\", which was used by chemists in previous years to describe the \"force\" that caused chemical reactions. The term affinity, as used in chemical relation, dates back to at least the time of Albertus Magnus in 1250.\n\nFrom the 1998 textbook \"Modern Thermodynamics\" by Nobel Laureate and chemistry professor Ilya Prigogine we find: \"As motion was explained by the Newtonian concept of force, chemists wanted a similar concept of ‘driving force’ for chemical change. Why do chemical reactions occur, and why do they stop at certain points? Chemists called the ‘force’ that caused chemical reactions affinity, but it lacked a clear definition.\"\n\nDuring the entire 18th century, the dominant view with regard to heat and light was that put forth by Isaac Newton, called the \"Newtonian hypothesis\", which states that light and heat are forms of matter attracted or repelled by other forms of matter, with forces analogous to gravitation or to chemical affinity.\n\nIn the 19th century, the French chemist Marcellin Berthelot and the Danish chemist Julius Thomsen had attempted to quantify affinity using heats of reaction. In 1875, after quantifying the heats of reaction for a large number of compounds, Berthelot proposed the \"principle of maximum work\", in which all chemical changes occurring without intervention of outside energy tend toward the production of bodies or of a system of bodies which liberate heat.\n\nIn addition to this, in 1780 Antoine Lavoisier and Pierre-Simon Laplace laid the foundations of thermochemistry by showing that the heat given out in a reaction is equal to the heat absorbed in the reverse reaction. They also investigated the specific heat and latent heat of a number of substances, and amounts of heat given out in combustion. In a similar manner, in 1840 Swiss chemist Germain Hess formulated the principle that the evolution of heat in a reaction is the same whether the process is accomplished in one-step process or in a number of stages. This is known as Hess' law. With the advent of the mechanical theory of heat in the early 19th century, Hess's law came to be viewed as a consequence of the law of conservation of energy.\n\nBased on these and other ideas, Berthelot and Thomsen, as well as others, considered the heat given out in the formation of a compound as a measure of the affinity, or the work done by the chemical forces. This view, however, was not entirely correct. In 1847, the English physicist James Joule showed that he could raise the temperature of water by turning a paddle wheel in it, thus showing that heat and mechanical work were equivalent or proportional to each other, i.e., approximately, . This statement came to be known as the mechanical equivalent of heat and was a precursory form of the first law of thermodynamics.\n\nBy 1865, the German physicist Rudolf Clausius had shown that this equivalence principle needed amendment. That is, one can use the heat derived from a combustion reaction in a coal furnace to boil water, and use this heat to vaporize steam, and then use the enhanced high-pressure energy of the vaporized steam to push a piston. Thus, we might naively reason that one can entirely convert the initial combustion heat of the chemical reaction into the work of pushing the piston. Clausius showed, however, that we must take into account the work that the molecules of the working body, i.e., the water molecules in the cylinder, do on each other as they pass or transform from one step of or state of the engine cycle to the next, e.g., from (\"P\",\"V\") to (\"P\",\"V\"). Clausius originally called this the “transformation content” of the body, and then later changed the name to entropy. Thus, the heat used to transform the working body of molecules from one state to the next cannot be used to do external work, e.g., to push the piston. Clausius defined this \"transformation heat\" as .\n\nIn 1873, Willard Gibbs published \"A Method of Geometrical Representation of the Thermodynamic Properties of Substances by Means of Surfaces\", in which he introduced the preliminary outline of the principles of his new equation able to predict or estimate the tendencies of various natural processes to ensue when bodies or systems are brought into contact. By studying the interactions of homogeneous substances in contact, i.e., bodies, being in composition part solid, part liquid, and part vapor, and by using a three-dimensional volume-entropy-internal energy graph, Gibbs was able to determine three states of equilibrium, i.e., \"necessarily stable\", \"neutral\", and \"unstable\", and whether or not changes will ensue. In 1876, Gibbs built on this framework by introducing the concept of chemical potential so to take into account chemical reactions and states of bodies that are chemically different from each other. In his own words, to summarize his results in 1873, Gibbs states:\n\nIn this description, as used by Gibbs, \"ε\" refers to the internal energy of the body, \"η\" refers to the entropy of the body, and \"ν\" is the volume of the body.\n\nHence, in 1882, after the introduction of these arguments by Clausius and Gibbs, the German scientist Hermann von Helmholtz stated, in opposition to Berthelot and Thomas’ hypothesis that chemical affinity is a measure of the heat of reaction of chemical reaction as based on the principle of maximal work, that affinity is not the heat given out in the formation of a compound but rather it is the largest quantity of work which can be gained when the reaction is carried out in a reversible manner, e.g., electrical work in a reversible cell. The maximum work is thus regarded as the diminution of the free, or available, energy of the system (Gibbs free energy \"G\" at \"T\" = constant, \"P\" = constant or Helmholtz free energy \"F\" at \"T\" = constant, \"V\" = constant), whilst the heat given out is usually a measure of the diminution of the total energy of the system (Internal energy). Thus, \"G\" or \"F\" is the amount of energy “free” for work under the given conditions.\n\nUp until this point, the general view had been such that: “all chemical reactions drive the system to a state of equilibrium in which the affinities of the reactions vanish”. Over the next 60 years, the term affinity came to be replaced with the term free energy. According to chemistry historian Henry Leicester, the influential 1923 textbook \"Thermodynamics and the Free Energy of Chemical Reactions\" by Gilbert N. Lewis and Merle Randall led to the replacement of the term “affinity” by the term “free energy” in much of the English-speaking world.\n\n"}
{"id": "1008844", "url": "https://en.wikipedia.org/wiki?curid=1008844", "title": "Timeline of glaciation", "text": "Timeline of glaciation\n\nThere have been five or six major ice ages in the history of Earth over the past 3 billion years. \nThe Late Cenozoic Ice Age began 34 million years ago, its latest phase being the Quaternary glaciation, in progress since 2.58 million years ago.\n\nWithin ice ages, there exist periods of more severe glacial conditions and more temperate referred to as glacial periods and interglacial periods, respectively. The Earth is currently in such an interglacial period of the Quaternary glaciation, with the last glacial period of the Quaternary having ended approximately 11,700 years ago, the current interglacial being known as the Holocene epoch. \nBased on climate proxies, paleoclimatologists study the different climate states originating from glaciation.\n\n<br>\n10\n\n10\n\n10\n\n10\n\n10\n\n10\n\n10\nThe third ice age, and possibly most severe, is estimated to have occurred from 720 to 635 Ma (million years) ago, in the Neoproterozoic Era, and it has been suggested that it produced a second \"Snowball Earth\", in which the Earth iced over completely. It has been suggested also that the end of this second cold period was responsible for the subsequent Cambrian Explosion, a time of rapid diversification of multicelled life during the Cambrian Period. However, this hypothesis is still controversial, though is growing in popularity among researchers, as evidence in its favor has mounted.\n\nA minor series of glaciations occurred from 460 Ma to 430 Ma. There were extensive glaciations from 350 to 250 Ma. \n\nThe Late Cenozoic Ice Age has seen extensive ice sheets in Antarctica for the last 34 Ma. During the last 3 Ma ice sheets have also developed on the northern hemisphere. This phase is known as the Quaternary glaciation, and has seen more or less extensive glaciation on 40,000 and later, 100,000 year cycles.\n\nWhereas the first 30 million years of the Late Cenozoic Ice Age mostly involved Antarctica, the Quaternary has seen numerous ice sheets extending over parts of Europe and North America that are currently populated and easily accessible. Early geologists therefore named apparent sequences of glacial and interglacial periods of the Quaternary Ice Age after characteristic geological features, and these names varied from region to region. It is now more common for researchers to refer to the periods by their marine isotopic stage number. The marine record preserves all the past glaciations; the land-based evidence is less complete because successive glaciations may wipe out evidence of their predecessors. Ice cores from continental ice accumulations also provide a complete record, but do not go as far back in time as marine data. Pollen data from lakes and bogs as well as loess profiles provided important land-based correlation data. The \"names\" system has mostly been phased out by professionals, who instead use the marine isotopic stage indexes for all technical discussions. For example, there are five Pleistocene glacial/interglacial cycles recorded in marine sediments during the last half million years, but only three classic interglacials were originally recognized on land during that period (Mindel, Riss and Würm).\n\nLand-based evidence works acceptably well back as far as MIS 6, but it has been difficult to coordinate stages using just land-based evidence before that. Hence, the \"names\" system is incomplete and the land-based identifications of ice ages previous to that are somewhat conjectural. Nonetheless, land based data is essentially useful in discussing landforms, and correlating the known marine isotopic stage with them.\n\n\n\n\n\n\nIt has proved difficult to correlate the traditional regional names with the global marine and ice core sequences. The indexes of MIS often identify several distinct glaciations that overlap in time with a single traditional regional glaciation. Some modern authors use the traditional regional glacial names to identify such a sequence of glaciations, whereas others replace the word “glaciation” with “complex” to refer to a continuous period of time that also includes warmer stages. As shown in the table below, it is only during the last 200-300 thousand years that the time resolution of the traditional nomenclature allow for direct correspondence with MIS indexes. \n\nFor sources to the tables, see the individual linked articles.\n\n\n"}
{"id": "55036039", "url": "https://en.wikipedia.org/wiki?curid=55036039", "title": "Ultimate Fighting Championship rankings", "text": "Ultimate Fighting Championship rankings\n\nThe Ultimate Fighting Championship rankings, which were introduced in February 2013, are generated by a voting panel made up of media members. The media members are asked to vote for who they feel are the top fighters in the UFC by weight class and pound-for-pound. A fighter is only eligible to be voted on if they are in active status in the UFC. A fighter can appear in more than one weight division at a time. The champion and interim champion are considered to be in the top positions of their respective divisions and therefore are not eligible for voting by weight class. However, the champions can be voted on for the pound-for-pound rankings.\n\n\nChampion: Daniel Cormier [21–1 (1)]\nChampion: Daniel Cormier [21–1 (1)]\n\nChampion: Robert Whittaker [20–4]\nChampion: Tyron Woodley [19–3–1]\nChampion: Khabib Nurmagomedov [27–0]\nChampion: Max Holloway [19–3]\nChampion: T.J. Dillashaw [16–3]\nChampion: Henry Cejudo [13–2]\nChampion: Cris Cyborg [20–1 (1)]\n<br>\n\nChampion: Amanda Nunes [16–4]\nChampion: Vacant\nChampion: Rose Namajunas [8–3]\n\n"}
{"id": "4213165", "url": "https://en.wikipedia.org/wiki?curid=4213165", "title": "Weight distribution", "text": "Weight distribution\n\nWeight distribution is the apportioning of weight within a vehicle, especially cars, airplanes, and trains. Typically, it is written in the form \"x\"/\"y\", where \"x\" is the percentage of weight in the front, and \"y\" is the percentage in the back.\n\nIn a vehicle which relies on gravity in some way, weight distribution directly affects a variety of vehicle characteristics, including handling, acceleration, traction, and component life. For this reason weight distribution varies with the vehicle's intended usage. For example, a drag car maximizes traction at the rear axle while countering the reactionary pitch-up torque. It generates this counter-torque by placing a small amount of counterweight at a great distance forward of the rear axle.\n\nIn the airline industry, load balancing is used to evenly distribute the weight of passengers, cargo, and fuel throughout an aircraft, so as to keep the aircraft's center of gravity close to its center of pressure to avoid losing pitch control. In military transport aircraft, it is common to have a loadmaster as a part of the crew; their responsibilities include calculating accurate load information for center of gravity calculations, and ensuring cargo is properly secured to prevent its shifting.\n\nIn large aircraft and ships, multiple fuel tanks and pumps are often used, so that as fuel is consumed, the remaining fuel can be positioned to keep the vehicle balanced, and to reduce stability problems associated with the free surface effect.\n\nIn the trucking industry, individual axle weight limits require balancing the cargo when the gross vehicle weight nears the legal limit.\n\n\n"}
