{"id": "28917874", "url": "https://en.wikipedia.org/wiki?curid=28917874", "title": "AISSat-1", "text": "AISSat-1\n\nAISSat-1 is a satellite used to receive Automatic Identification System (AIS) signals. Launched on 12 June 2010 from \nSatish Dhawan Space Center as a secondary payload, AISSat-1 is in a sun-synchronous low-Earth orbit. Initially a development project, the satellite has since passed into ordinary operations. Via downlinks at Svalbard Satellite Station and at Vardø Vessel Traffic Service Centre it tracks vessels in the Norwegian Sea and Barents Sea for the Norwegian Coastal Administration, the Norwegian Coast Guard, the Norwegian Directorate of Fisheries and other public agencies.\n\nThe satellite was developed as a cooperation between the Norwegian Defence Research Establishment (NDRE), the Norwegian Space Centre and the Coastal Administration. The payload was developed by Kongsberg Seatex while the University of Toronto Institute for Aerospace Studies built the bus and completed manufacture. The satellite measures cube and weighs . Ownership and operation passed to Statsat in 2013. The satellite has since 2014 been supplemented with AISSat-2 and from 2015 by AISSat-3.\n\nThe Automatic Identification System was developed as a navigational aid for shipping traffic, initially primarily as a collision avoidance system. The system became mandatory most commercial ships from 2008. AIS was designed as a terrestrial system with AIS transponders operating on the very high frequency (VHF) range. In addition to ship-to-ship tracking, AIS could be monitored by a series of coastal base stations. The idea for satellite monitoring arose later and was mostly intended for maritime surveillance and control, as well as safety monitoring.\n\nThe Norwegian Defence Research Establishment took the first steps towards AIS satellite use in 2003 paper. A main concern was the low transmitting power of AIS, typically one to twelve watts. Simultaneous transmission could also result in data packet collision and thus make all transmissions unreadable. Research later concluded that satellite monitoring of heavily trafficked areas would be near impossible, but that satellite surveying of the sparsely used Arctic waters would be effective. The AIS infrastructure in Norway was built and is operated by the Norwegian Coastal Administration in cooperation with the Norwegian Armed Forces. It was this cooperation which led to the development of the satellite AIS system.\n\nIncreased focus on the High North arose following the appointment of Stoltenberg's Second Cabinet in 2005. The AISSats are part of a larger policy to strengthen Norway's grip on the Arctic areas. Norway's exclusive economic zone (EEZ) covers sea areas around Svalbard and Jan Mayen, which in addition to the continental EEZ gives it an area of . Eighty percent of all Arctic shipping traffic passes through Norway's EEZ.\n\nSpecific research into what became AISSat-1 began in 2005 as a cooperation between the Norwegian Space Centre, Norwegian Defence Research Establishment and Kongsberg Seatex. The initial research concluded that a satellite within an orbit of would be able to receive AIS communication. A particular challenge during this period was the need for an antenna with sufficient length to match the wavelength, which would be difficult with a small satellite profile. As part of the development, an AIS tranceiver was attached to the International Space Station to test out the receptivity of AIS signals in low-Earth orbit. AISSat-1 was Norway's first non-commercial satellite.\n\nThe satellite was regarded as experimental by the involved parties. Design of the payload and AIS components was carried out by Kongsberg Seatex. The project is owned by the Norwegian Space Centre while NDRE was responsible for technical aspects. Once operational, the data was fed to the Coastal Administration. Production was subcontracted to the University of Toronto Institute for Aerospace Studies (UTIAS) using their Generic Nanosatellite Bus as the satellite bus.\n\nAISSat-1 is built around the University of Toronto's Generic Nanosatellite Bus. The satellite measures cubed. It is solar-powered collected by thirty-six panels and stored in two batteries. There are three onboard computers, each with a ARM7 microcontroller. One operates household issues such as telemetry while one handled attitude control. The third handles payload operations. There are four ultra high frequency antennas for telemetry. Attitude is adjusted through three reaction wheels. The payload consists of a VHF antenna and an onboard computer for storing and processing of AIS data.\n\nAISSat was a secondary payload which was launched on 12 July 2010 at 03:52 UTC from the First Launch Pad at Satish Dhawan Space Center in India. The satellite was carried on board a Polar Satellite Launch Vehicle operated by the Indian Space Research Organisation. The main payload of the mission was the reconnaissance satellite Cartosat-2B. In addition to AISSat-1, secondary payloads were AlSat-2A, TISat-1 and StudSat.\n\nAISSat was placed into a sun-synchronous polar orbit with an altitude of . It received an inclination of 97.71° and a period of 97.4 minutes. UTIAS retained responsibility for launch and commissioning, after which operational responsibility was transferred to NDRE. The satellite was designed, built and launched within schedule and budget.\n\nAISSat-1 was built as an experimental satellite to investigate the feasibility of collecting AIS data from space. The satellite quickly proved to meet its expectations and has since been regarded as an operational satellite. Telemetry including data download is handled from Svalbard Satellite Station. From 2015 a second ground station, at Vardø Vessel Traffic Service Centre, was opened. Since 2013 operation of the satellite passed to Statsat.\n\nThe primary operational goal is the gathering of positioning and course information from fisheries and ship traffic within the Norwegian EEZ with the intent of environmental surveillance. The vessel data is used by the Coastal Administration and in particular Vardø Vessel Traffic Service Centre to monitor ship traffic. The data is stored and can also be used to collect accurate statistics on ship traffic in the Arctic. Unlike terrestrial data collection, the satellite information is not made publicly available. A contributing cause is that certain fishers many not want to reveal their fishing positions and could then have chosen to turn off their AIS instead.\n\nThe AIS data is used by the Norwegian Directorate of Fisheries for surveillance of the fisheries fleet to identify illegal fishing. Controls involve checking if ships rendezvous with other ships and when they dock. This information is then controlled with logs. This means of controlling has been an efficient preventative measure. Beyond use in the high north, AISSat-1 gradually sweeps above the entire globe in the course of a twenty-four hour period. This allows tracking of ship traffic in other areas under Norwegian jurisdiction around Bouvetøya and for instance data for combating piracy off Africa.\n\nData can also be used to identify any ship causing an oil spill. There are more than one thousand annual oil spills and illegal dumping in Norwegian waters. While observation satellites have previously been able to identify spills, the satellite AIS monitoring can normally identify the culprit and the data used as evidence. The Coast Guard, the Custom Service, the Police Service and the Armed Force can utilize the data from the data.\n\nA build-to-print copy of the satellite, AISSat-2, was launched on 8 July 2014. It is intended to be followed by a third satellite, AISSat-3, in 2015. The added satellites are intended to provide redundancy in case of data capturing failure in one satellite.\n"}
{"id": "9974280", "url": "https://en.wikipedia.org/wiki?curid=9974280", "title": "Alemayehu Tegenu", "text": "Alemayehu Tegenu\n\nAlemayehu Tegenu is the Ethiopian Minister of Water, Irrigation and Energy.\n\n"}
{"id": "23167450", "url": "https://en.wikipedia.org/wiki?curid=23167450", "title": "Chapel of the Holy Well", "text": "Chapel of the Holy Well\n\nChapel of the Holy well is a chapel located in Marianka, Slovakia. Rotunda Chapel of the Holy well was commissioned in 1696 for 1000 golden coins by earl Paul Eszterhazy and local baron Ján Macholány, who can be found pictured with his family on one of the ceiling frescos. In 1722, a renovated baroque altar was built, replacing the original one. In 1877, the statues of St. Anthony and St. Paul the Hermits were erected in front of the chapel. These statues came from the workshop of Rafael Donner, eventually being replaced by replicas created in Brno in 1981. Originals were transferred to the Slovak National Gallery, where they are a part of the original Baroque Art exhibition. The chapel was constructed directly above the stream, out of which the water is directed to the front of the chapel directly accessible to the visitors. The stream beneath was reported to have miraculous healing effect on physically handicapped pilgrims visiting the site. In the book of Kummer from the record of the local cloister, 140 recoveries were reported between 1634 - 1730.\n"}
{"id": "15545141", "url": "https://en.wikipedia.org/wiki?curid=15545141", "title": "Coastal Volcanic Belt", "text": "Coastal Volcanic Belt\n\nThe Coastal Volcanic Belt (CVB) is a major Late Silurian volcanic belt extending from Massachusetts, United States into southwestern New Brunswick, Canada. It is one of the largest bimodal volcanic provinces in world, ranging from basalts to rhyolites.\n\n\n"}
{"id": "7992", "url": "https://en.wikipedia.org/wiki?curid=7992", "title": "Devonian", "text": "Devonian\n\nThe Devonian is a geologic period and system of the Paleozoic, spanning 60 million years from the end of the Silurian, million years ago (Mya), to the beginning of the Carboniferous, Mya. It is named after Devon, England, where rocks from this period were first studied.\n\nThe first significant adaptive radiation of life on dry land occurred during the Devonian. Free-sporing vascular plants began to spread across dry land, forming extensive forests which covered the continents. By the middle of the Devonian, several groups of plants had evolved leaves and true roots, and by the end of the period the first seed-bearing plants appeared. Various terrestrial arthropods also became well-established.\n\nFish reached substantial diversity during this time, leading the Devonian to often be dubbed the \"Age of Fish\". The first ray-finned and lobe-finned bony fish appeared, while the placoderms began dominating almost every known aquatic environment. The ancestors of all four-limbed vertebrates (tetrapods) began adapting to walking on land, as their strong pectoral and pelvic fins gradually evolved into legs. In the oceans, primitive sharks became more numerous than in the Silurian and Late Ordovician.\n\nThe first ammonites, species of molluscs, appeared. Trilobites, the mollusc-like brachiopods and the great coral reefs, were still common. The Late Devonian extinction which started about 375 million years ago severely affected marine life, killing off all placodermi, and all trilobites, save for a few species of the order Proetida.\n\nThe palaeogeography was dominated by the supercontinent of Gondwana to the south, the continent of Siberia to the north, and the early formation of the small continent of Euramerica in between.\n\nThe period is named after Devon, a county in southwestern England, where a controversial argument in the 1830s over the age and structure of the rocks found distributed throughout the county was eventually resolved by the definition of the Devonian period in the geological timescale. The Great Devonian Controversy was a long period of vigorous argument and counter-argument between the main protagonists of Roderick Murchison with Adam Sedgwick against Henry De la Beche supported by George Bellas Greenough. Murchison and Sedgwick won the debate and named the period they proposed as the Devonian System.\n\nWhile the rock beds that define the start and end of the Devonian period are well identified, the exact dates are uncertain. According to the International Commission on Stratigraphy (Ogg, 2004), the Devonian extends from the end of the Silurian Mya, to the beginning of the Carboniferous Mya (in North America, the beginning of the Mississippian subperiod of the Carboniferous).\n\nIn nineteenth-century texts the Devonian has been called the \"Old Red Age\", after the red and brown terrestrial deposits known in the United Kingdom as the Old Red Sandstone in which early fossil discoveries were found. Another common term is \"Age of the Fishes\", referring to the evolution of several major groups of fish that took place during the period. Older literature on the Anglo-Welsh basin divides it into the Downtonian, Dittonian, Breconian and Farlovian stages, the latter three of which are placed in the Devonian.\n\nThe Devonian has also erroneously been characterised as a \"greenhouse age\", due to sampling bias: most of the early Devonian-age discoveries came from the strata of western Europe and eastern North America, which at the time straddled the Equator as part of the supercontinent of Euramerica where fossil signatures of widespread reefs indicate tropical climates that were warm and moderately humid but in fact the climate in the Devonian differed greatly during its epochs and between geographic regions. For example, during the Early Devonian, arid conditions were prevalent through much of the world including Siberia, Australia, North America, and China, but Africa and South America had a warm temperate climate. In the Late Devonian, by contrast, arid conditions were less prevalent across the world and temperate climates were more common.\n\nThe Devonian Period is formally broken into Early, Middle and Late subdivisions. The rocks corresponding to those epochs are referred to as belonging to the Lower, Middle and Upper parts of the Devonian System.\n\nThe Early Devonian lasted from and began with the Lochkovian stage, which lasted until the Pragian. It spanned from , and was followed by the Emsian, which lasted until the Middle Devonian began, . \nDuring this time, the first ammonoids appeared, descending from bactritoid nautiloids. Ammonoids during this time period were simple and differed little from their nautiloid counterparts. These ammonoids belong to the order Agoniatitida, which in later epochs evolved to new ammonoid orders, for example Goniatitida and Clymeniida. This class of cephalopod molluscs would dominate the marine fauna until the beginning of the Mesozoic era.\n\nThe Middle Devonian comprised two subdivisions: first the Eifelian, which then gave way to the Givetian .\nDuring this time the jawless agnathan fishes began to decline in diversity in freshwater and marine environments partly due to drastic environmental changes and partly due to the increasing competition, predation and diversity of jawed fishes. The shallow, warm, oxygen-depleted waters of Devonian inland lakes, surrounded by primitive plants, provided the environment necessary for certain early fish to develop such essential characteristics as well developed lungs, and the ability to crawl out of the water and onto the land for short periods of time.\n\nFinally, the Late Devonian started with the Frasnian, , during which the first forests took shape on land. The first tetrapods appeared in the fossil record in the ensuing Famennian subdivision, the beginning and end of which are marked with extinction events. This lasted until the end of the Devonian, .\n\nThe Devonian was a relatively warm period, and probably lacked any glaciers. The temperature gradient from the equator to the poles was not as large as it is today. The weather was also very arid, mostly along the equator where it was the driest. Reconstruction of tropical sea surface temperature from conodont apatite implies an average value of in the Early Devonian. levels dropped steeply throughout the Devonian period as the burial of the newly evolved forests drew carbon out of the atmosphere into sediments; this may be reflected by a Mid-Devonian cooling of around . The Late Devonian warmed to levels equivalent to the Early Devonian; while there is no corresponding increase in concentrations, continental weathering increases (as predicted by warmer temperatures); further, a range of evidence, such as plant distribution, points to a Late Devonian warming. The climate would have affected the dominant organisms in reefs; microbes would have been the main reef-forming organisms in warm periods, with corals and stromatoporoid sponges taking the dominant role in cooler times. The warming at the end of the Devonian may even have contributed to the extinction of the stromatoporoids.\n\nThe Devonian period was a time of great tectonic activity, as Euramerica and Gondwana drew closer together.\n\nThe continent Euramerica (or Laurussia) was created in the early Devonian by the collision of Laurentia and Baltica, which rotated into the natural dry zone along the Tropic of Capricorn, which is formed as much in Paleozoic times as nowadays by the convergence of two great air-masses, the Hadley cell and the Ferrel cell. In these near-deserts, the Old Red Sandstone sedimentary beds formed, made red by the oxidised iron (hematite) characteristic of drought conditions.\n\nNear the equator, the plate of Euramerica and Gondwana were starting to meet, beginning the early stages of the assembling of Pangaea. This activity further raised the northern Appalachian Mountains and formed the Caledonian Mountains in Great Britain and Scandinavia.\n\nThe west coast of Devonian North America, by contrast, was a passive margin with deep silty embayments, river deltas and estuaries, found today in Idaho and Nevada; an approaching volcanic island arc reached the steep slope of the continental shelf in Late Devonian times and began to uplift deep water deposits, a collision that was the prelude to the mountain-building episode at the beginning of the Carboniferous called the Antler orogeny.\n\nSea levels were high worldwide, and much of the land lay under shallow seas, where tropical reef organisms lived. The deep, enormous Panthalassa (the \"universal ocean\") covered the rest of the planet. Other minor oceans were the Paleo-Tethys Ocean, Proto-Tethys Ocean, Rheic Ocean, and Ural Ocean (which was closed during the collision with Siberia and Baltica).\n\nDuring the Devonian Chaitenia, an island arc, accreted to Patagonia.\n\nSea levels in the Devonian were generally high. Marine faunas continued to be dominated by bryozoa, diverse and abundant brachiopods, the enigmatic hederellids, microconchids and corals. Lily-like crinoids (animals, their resemblance to flowers notwithstanding) were abundant, and trilobites were still fairly common. Among vertebrates, jawless armored fish (ostracoderms) declined in diversity, while the jawed fish (gnathostomes) simultaneously increased in both the sea and fresh water. Armored placoderms were numerous during the lower stages of the Devonian Period and became extinct in the Late Devonian, perhaps because of competition for food against the other fish species. Early cartilaginous (Chondrichthyes) and bony fishes (Osteichthyes) also become diverse and played a large role within the Devonian seas. The first abundant genus of shark, \"Cladoselache\", appeared in the oceans during the Devonian Period. The great diversity of fish around at the time has led to the Devonian being given the name \"The Age of Fish\" in popular culture.\n\nThe first ammonites also appeared during or slightly before the early Devonian Period around 400 Mya.\n\nA now dry barrier reef, located in present-day Kimberley Basin of northwest Australia, once extended a thousand kilometres, fringing a Devonian continent. Reefs in general are built by various carbonate-secreting organisms that have the ability to erect wave-resistant frameworks close to sea level. The main contributors of the Devonian reefs were unlike modern reefs, which are constructed mainly by corals and calcareous algae. They were composed of calcareous algae and coral-like stromatoporoids, and tabulate and rugose corals, in that order of importance.\n\nBy the Devonian Period, life was well underway in its colonisation of the land. The moss forests and bacterial and algal mats of the Silurian were joined early in the period by primitive rooted plants that created the first stable soils and harbored arthropods like mites, scorpions, trigonotarbids and myriapods (although arthropods appeared on land much earlier than in the Early Devonian and the existence of fossils such as \"Climactichnites\" suggest that land arthropods may have appeared as early as the Cambrian). Also the first possible fossils of insects appeared around 416 Mya in the Early Devonian. Evidence for the earliest tetrapods takes the form of trace fossils in shallow lagoon environments within a marine carbonate platform/shelf during the Middle Devonian, although these traces have been questioned and an interpretation as fish feeding traces (Piscichnus) has been advanced.\n\nMany Early Devonian plants did not have true roots or leaves like extant plants although vascular tissue is observed in many of those plants. Some of the early land plants such as \"Drepanophycus\" likely spread by vegetative growth and spores. The earliest land plants such as \"Cooksonia\" consisted of leafless, dichotomous axes and terminal sporangia and were generally very short-statured, and grew hardly more than a few centimetres tall. By far the largest land organism during this period was the enigmatic \"Prototaxites\", which was possibly the fruiting body of an enormous fungus, rolled liverwort mat, or another organism of uncertain affinities that stood more than 8 metres tall, and towered over the low, carpet-like vegetation. By the Middle Devonian, shrub-like forests of primitive plants existed: lycophytes, horsetails, ferns, and progymnosperms had evolved. Most of these plants had true roots and leaves, and many were quite tall. The earliest-known trees, from the genus \"Wattieza\", appeared in the Late Devonian around 385 Mya. In the Late Devonian, the tree-like ancestral Progymnosperm \"Archaeopteris\" which had conifer-like true wood and fern-like foliage and the cladoxylopsids grew. (See also: lignin.) These are the oldest-known trees of the world's first forests. By the end of the Devonian, the first seed-forming plants had appeared. This rapid appearance of so many plant groups and growth forms has been called the \"Devonian Explosion\".\n\nThe 'greening' of the continents acted as a carbon sink, and atmospheric concentrations of carbon dioxide may have dropped. This may have cooled the climate and led to a massive extinction event. See Late Devonian extinction.\n\nPrimitive arthropods co-evolved with this diversified terrestrial vegetation structure. The evolving co-dependence of insects and seed-plants that characterised a recognisably modern world had its genesis in the Late Devonian period. The development of soils and plant root systems probably led to changes in the speed and pattern of erosion and sediment deposition. The rapid evolution of a terrestrial ecosystem that contained copious animals opened the way for the first vertebrates to seek out a terrestrial living. By the end of the Devonian, arthropods were solidly established on the land.\n\nA major extinction occurred at the beginning of the last phase of the Devonian period, the Famennian faunal stage (the Frasnian-Famennian boundary), about Mya, when all the fossil agnathan fishes, save for the psammosteid heterostraci, suddenly disappeared. A second strong pulse closed the Devonian period. The Late Devonian extinction was one of five major extinction events in the history of the Earth's biota, and was more drastic than the familiar extinction event that closed the Cretaceous.\n\nThe Devonian extinction crisis primarily affected the marine community, and selectively affected shallow warm-water organisms rather than cool-water organisms. The most important group to be affected by this extinction event were the reef-builders of the great Devonian reef systems.\n\nAmongst the severely affected marine groups were the brachiopods, trilobites, ammonites, conodonts, and acritarchs, as well as jawless fish, and all placoderms. Land plants as well as freshwater species, such as our tetrapod ancestors, were relatively unaffected by the Late Devonian extinction event (there is a counterargument that the Devonian extinctions nearly wiped out the tetrapods).\n\nThe reasons for the Late Devonian extinctions are still unknown, and all explanations remain speculative. Canadian paleontologist Digby McLaren suggested in 1969 that the Devonian extinction events were caused by an asteroid impact. However, while there were Late Devonian collision events (see the Alamo bolide impact), little evidence supports the existence of a large enough Devonian crater.\n\nCategories:\n\n\n"}
{"id": "26213761", "url": "https://en.wikipedia.org/wiki?curid=26213761", "title": "Dipole model of the Earth's magnetic field", "text": "Dipole model of the Earth's magnetic field\n\nThe dipole model of the Earth's magnetic field is a first order approximation of the rather complex true Earth's magnetic field. Due to effects of the interplanetary magnetic field, and the solar wind, the dipole model is particularly inaccurate at high L-shells (e.g., above L=3), but may be a good approximation for lower L-shells. For more precise work, or for any work at higher L-shells, a more accurate model that incorporates solar effects, such as the Tsyganenko magnetic field model, is recommended.\n\nThe following equations describe the dipole magnetic field.\n\nFirst, define formula_1 as the mean value of the magnetic field at the magnetic equator on the Earth's surface. Typically formula_2.\n\nThen, the radial and azimuthal fields can be described as\n\nformula_3\n\nformula_4\n\nformula_5\n\nwhere formula_6 is the mean radius of the Earth (approximately 6370 km), formula_7 is the radial distance from the center of the Earth (using the same units as used for formula_6), and formula_9 is the azimuth measured from the north magnetic pole.\n\nIt is sometimes more convenient to express the magnetic field in terms of magnetic latitude and distance in earth radii. The magnetic latitude formula_10 is measured northwards from the equator (analogous to geographic latitude) and is related to formula_9 by formula_12. In this case, the radial and azimuthal components of the magnetic field (the latter still in the formula_9 direction, measured from the axis of the north pole) are given by\n\nformula_14\n\nformula_15\n\nformula_16\n\nwhere formula_17 in this case has units of Earth radii (formula_18).\n\nInvariant latitude is a parameter that describes where a particular magnetic field line touches the surface of the Earth. It is given by\n\nformula_19\n\nor\n\nformula_20\n\nwhere formula_21 is the invariant latitude and formula_22 is the L-shell describing the magnetic field line in question.\n\nOn the surface of the earth, the invariant latitude (formula_21) is equal to the magnetic latitude (formula_10).\n\n\n"}
{"id": "38733580", "url": "https://en.wikipedia.org/wiki?curid=38733580", "title": "Eco funnel", "text": "Eco funnel\n\nThe ECO Funnel, also known as the \"Safety Ecological Funnel\", is a funnel intended to be affixed to a receptacle for liquid chemical waste. It is designed to reduce environmental contamination, in compliance with OSHA and EPA mandated safety protocols.\n\nThe ECO Funnel was first devised by Dr. Ron Najafi in 1996 while he was working at a facility in the San Francisco Bay Area. At that facility, complaints arose throughout the building of odorous fumes originating from the laboratory. This raised the alarm that contamination was not limited to the laboratory where chemical waste is stored, and an investigation was initiated. As a member of the company's Health and Safety Committee, Dr. Najafi conducted tests to determine the severity of emissions from waste bottles typically left unsealed beneath a fume hood. By marking the rapid decrease in waste bottle volume over a limited period, he was able to verify that significant amounts of potentially hazardous chemicals were evaporating into the surrounding environment.\n\nOnce the source and severity of the situation had been confirmed, a need was recognized to improve standard practice for the safety of lab personnel and to bring laboratories into adherence with public safety protocol such as those outlined by the Occupational Safety & Health Administration (OSHA) or the US Environmental Protection Agency (EPA):\n\n\nEnvironmental regulations regarding waste containers were at this time becoming more rigorously enforced than previously, making the situation urgent. After researching available options, it became clear that no solution existed to the problem of effectively containing chemical waste evaporation aside from re-capping original waste containers after each use. In response to his findings, Dr. Najafi created the ECO Funnel.\n\nThe current design of the ECO Funnel utilizes a latching lid with gasket, and two internal tubes. One larger and longer tube extends into the container from the base of the funnel into the liquid waste to restrict the surface area of the contents and inhibit fume emission. The other tube is designed to sit above the liquid level as a means to warn against overfilling. When the ECO Funnel is secured to a container, this second shorter and smaller tube provide the only ventilation. When the volume of liquid inside the container reaches the smaller tube opening, it will obstruct the air flow through the smaller tube, the waste fluid backs up into the funnel to signal the user that the container is at capacity. Detaching the ECO Funnel allows the remaining liquid to flow into the container and be safely sealed inside. So long as the ECO Funnel's contents do not exceed the maximum volume indicated, the liquid will drain into the container without overflowing.\n\nThe ECO Funnel resolves a long standing incongruity between federally mandated safety protocol and standard laboratory practice. The traditional method for handling fluid waste is to set a standard funnel onto the mouth of an open bottle, which is sometimes stored under a fume hood. This is out of compliance with OSHA 29 CFR 1910.1450 and EPA 40 CFR 264.173. Testing confirms that these methods allow chemicals to evaporate, contaminating the surrounding atmosphere and posing a health risk not only to laboratory workers, but to neighboring areas. Additionally, chemical contaminants in the atmosphere, in combination with normal oxygen levels in the lab, fulfill two of the three components necessary to complete the fire triangle. Further, chemical spills in the laboratory preclude procedures of safe disposal, leading these toxins to end up in landfills, and consequently, soil, rivers and oceans. As plant and animal life are effected, the consequences escalate exponentially.\n"}
{"id": "375314", "url": "https://en.wikipedia.org/wiki?curid=375314", "title": "Ecological yield", "text": "Ecological yield\n\nEcological yield is the harvestable population growth of an ecosystem. It is most commonly measured in forestry: sustainable forestry is defined as that which does not harvest more wood in a year than has grown in that year, within a given patch of forest.\n\nHowever, the concept is also applicable to water, soil, and any other aspect of an ecosystem which can be both harvested and renewed—called renewable resources. The carrying capacity of an ecosystem is reduced over time if more than the amount which is \"renewed\" (refreshed or regrown or rebuilt) is consumed.\n\nEcosystem services analysis calculates the global yield of the Earth's biosphere to humans as a whole. This is said to be greater in size than the entire human economy. However, it is more than just yield, but also the natural processes that increase biodiversity and conserve habitat which result in the total value of these services. \"Yield\" of ecological commodities like wood or water, useful to humans, is only a part of it.\n\nVery often an ecological yield in one place offsets an ecological load in another. Greenhouse gas released in one place, for instance, is fairly evenly distributed in the atmosphere, and so greenhouse gas control can be achieved by creating a carbon sink literally anywhere else.\n\nSome of the earliest academic papers on the subject were researching methods of sustainable fishing. Work of Russel et al. in 1931 observed in particular that ”it appears that the ideal of a stabilised fishery yielding a constant maximum value is impractical.” This work was mostly theoretical. Practical work would begin later, performed by industry and government agencies.\n\nEcological yield is a theoretical construct which aggregates information from several physically measurable quantities. It can be used to reason about other ecological indicators such as the footprint. It can also be used as a decision-making tool for governments and corporations.\n\nThe idea of ecological footprints is to measure the cost of economic activity in terms of the amount of ecologically productive land required to sustain it. Doing this accurately requires estimating how productive the land is; in other words, it requires measuring ecological yield. Conversely, one can extract ecological yield estimates from ecological footprint estimates.\n\nCorporations take out loans to buy equipment and land use rights. In order to pay back these loans, they must extract and sell resources from the land. If the corporation is ignorant of the yield of the land in question, then the debt instruments may demand a yield greater than the ecological capacity to renew. Green economics links this process with ecocide and poses solutions through monetary reform. \n\nEven well-meaning corporations may systematically overestimate the yield of an ecosystem. In the case of multiple corporations bidding for land rights, an economic phenomenon known as the winner's curse causes the winning party to systematically overestimate the economic value of the land. Typically the economic value comes mostly from the ecological yield, in which case the corporation will overestimate that as well.\n\nAnother form of overestimation may come from generalizing data from other ecosystems. For example, the same species of fish in two different systems may have significantly different diets. If its diet in one region consists mostly of algae but in another region consists largely of smaller fish, then it will be more expensive for the latter ecosystem to produce the fish. Yield will be correspondingly lower in the second region. This example illustrates the need for ecosystem-specific study and monitoring in order to reason about ecological yield.\n\nOne may define yearly ecological yield for a fixed ecological product as follows: the yield is the amount of the product which may be removed from the ecosystem so that it is capable of recovering in one year. As a theoretical property of ecosystems, it cannot be measured directly but only estimated. Note that definition is sensitive to the time period which is allowed for recovery: the amount of product one can remove which regenerates over 3 years is not necessarily 3 times that which one can remove and regenerate over 1 year. The yearly ecological yield is most useful because of the cycle of seasons and the commercial notion of the fiscal year. The seasons affect growth through temperature, sunlight, and rain, especially at the lowest trophic level. The fiscal year affects decisions by corporations to harvest resources: they may choose to harvest at or above ideal levels based on their need for short-term cash flow. \n\nIn 1986, Vitousek et al. estimated that humans made use of 50 petagrams (50 billion tons) per year of biomass produced from photosynthesis. They also estimated that these 50 billion tons comprised between 20% and 40% of photosynthetic activity on earth. Separately, the Global Footprint Network estimates the total human footprint as 1.6 times the total biosphere. This implies that ecosystems are overexploited by a factor of 1.6 on average.\n\nIn most biomes, the only form of primary production is photosynthesis. In other words, all new biomass can be traced back to photosynthetic plants and algae by a chain of predation. Therefore, one can predict the yield of one organism in an ecosystem as a function of the yield of its primary producers. When the biomass from prey is converted into biomass in its predator, some losses occur due to biological and thermodynamic inefficiency. The conversion rate is typically about 10%. In other words, 100 kg of plant matter may be converted into 10 kg of herbivores, which then may be converted to 1 kg of carnivores who exclusively eat herbivores. One can compute the trophic level of an organism as the weighted average of length of the predation chain from the organism to a primary producer. This trophic level determines an exponential multiplier to convert from primary producer biomass to the organism's biomass.\n\nOne can measure the amount of wood removed from a forest by asking the company who removed it; typically only one company has the logging rights to any given plot of land. In order to measure the regrowth of the forest in the coming year, typically one picks a representative subsample of the region and tracks every single tree in the subsample. \n\nOne such study measured growth in a section of the Tapajós National Forest for 13 years after logging activity. The loggers intended to harvest on a 30-year cycle. Logging in this region is restricted to mature trees measuring at least 45 cm DBH. Before logging, the region had somewhere between 150 m³ and 200 m³ of mature tree volume per hectare. Loggers removed about 75 m³ of tree per hectare, between 40% and 50% of the standing mass. \n\nThe authors show that growth rates in the region were elevated for up to 3 years after logging. After 13 years of growth, the basal area reached 75% of its original volume. They also show that logging makes substantial changes to the species composition and canopy structure of the forest. This introduces subjectivity into the notion of \"recovery\" for an ecosystem.\n\n"}
{"id": "14034429", "url": "https://en.wikipedia.org/wiki?curid=14034429", "title": "Electricity sector in Peru", "text": "Electricity sector in Peru\n\nThe electricity sector in Peru has experienced impressive improvements in the past 15 years. Access to electricity has increased from 45% in 1990 to 88.8% in July 2011, while service quality and efficiency of service provision improved. These improvements were made possible through privatizations following reforms initiated in 1992. At the same time, electricity tariffs have remained in line with the average for Latin America.\n\nHowever, several challenges remain. Chief among them are the still very low level of access in rural areas and the untapped potential of some renewable energies, in particular wind and solar energy, due to an inadequate regulatory framework.\n\nThe current electricity generation capacity is evenly divided between thermal and hydroelectric sources. A renewed recent dynamism of the electricity sector in the country is based on the shift to natural gas plants, which will be mainly fed from the production of the Camisea gas field in the Amazon Rainforest.\n\nThe National Interconnected System (SEIN) serves 85% of the connected population, with several “isolated” systems covering the rest of the country. While investment in generation, transmission and distribution in urban areas is predominantly private, resources for rural electrification come solely from public sources.\n\nInstalled generating capacity Peru is evenly divided between thermal and hydroelectric sources. In 2006, the country had 6.7 GW of installed capacity, 52% being thermal and 48% hydroelectric, with a negligible share of other renewable sources. Of the total capacity, 84% (5.63 GW) enters the electricity market, while the remaining 16% (1,03 GW) is generated for self-consumption.\n\nHowever, electricity generation is not evenly divided between the two dominating sources. In 2006, 72% of Peru’s total electricity generation came from hydroelectric plants (total generation was 27.4 TWh), with conventional thermal plants only in operation during peak load periods or when hydroelectric output is curtailed by weather events. This “underuse” of the country’s thermal capacity is due to the high variable costs of thermal generation. In 2004, the country’s reserve margin was estimated at 45%. However, when those high cost thermal plants were taken out of the equation, margins fell to as low as 15%.\n\nIn an attempt to reduce the country’s reliance upon hydroelectricity, the Peruvian government has encouraged greater investment in gas-fired power plants. The controversial Camisea Gas Project has opened up natural gas production in Peru, with the first new 140 MW gas-fired power plant in Tumbes to start operations by the end of 2007. The Camisea project is considered strategic for it is expected to contribute to reduce the existing deficit in Peru’s hydrocarbons trade balance by substituting imports (mainly of diesel and LPG) and allowing exports (naphta, LPG surpluses).\n\nThe dynamic nature of the electricity sector has continued during 2007, with an estimated 9.3% increase in generation, which is expected to reach 30 TWh. This increase is mainly due to the existing positive conditions for thermal generation through the use of natural gas in new plants and also to an increase in hydroelectric generation due to the availability of hydrological resources in the existing hydroelectric facilities.\n\nIn 2006, total electricity consumption in Peru was 24 TWh, which corresponds to 872 kWh per capita per year. The consumption share for the different economic sectors is as follows:\n\n\nIn terms of demand projections, the Ministry of Energy and Mines estimates that electricity demand will increase between 5.6% and 7.4% per year between 2007-2015. It is expected that per capita electricity demand will reach 1,632 kWh in 2030.\n\nTo meet this increasing demand, Peru will rely on natural gas, which is the most cost competitive option among all other fuel types. As such, it is expected that installed capacity of gas-fired electricity generation increases from 0.3 GW in 2002 to 6.0 GW in 2030. It is expected that, from 2026 onwards, natural gas will acquire the dominant share in the electricity generation mix, reaching 44% in 2030 compared with hydroelectricity's 37% share for the same year.\n\nIn 2006, 79% of the population in Peru had access to electricity, a percentage that is below the 94.6 average for the LAC region Peru has one of the lowest rural electrification rates in Latin America. Coverage in the predominantly poor rural areas is about 30%, with more than six million people without access to electricity. In its 2004 National Rural Electrification Plan (PNER), the Government of Peru reiterated its commitment to reduce the electrification gap, aiming to increase rural coverage from 30% to 75% by 2013.\n\nIn 2005, the average number of interruptions per subscriber was 14.5, while duration of interruptions per subscriber was 18.3 hours. Both numbers are very close to the weighted averages of 13 interruptions and 14 hours for the LAC region.\n\nLosses in 2006 amounted to 11% of total production. Distribution losses were 6.3%, down from 22% a decade before and below the 13.5% LAC average. Transmission losses for the same year have been estimated at 4.7%.\n\nThe National Electricity Office (DGE - \"Dirección General de la Electricidad\"), under the Ministry of Energy and Mines (MEM), is in charge of setting electricity policies and regulations and of granting concessions. It is also responsible for elaborating generation and transmission expansion plans and has to approve the relevant procedures for the operation of the electricity system.\n\nThe Energy and Mining Investment Supervisory Body (OSINERGMIN - \"Organismo Supervisor de Inversión en Energía y Miniería\"), created in 1996 as OSINERG (Mining competences were added recently, in January 2007), is in charge of enforcing compliance with the Electricity Concessions Law (LCE) of 1992 and is also in charge of ensuring the electricity public service. OSINERG is as well the body responsible for enforcing the fiscal obligations of the license holders as established by the law and its regulation. Finally, it is responsible for monitoring compliance of the System Economic Operation Committees (COES) functions and for determining biannually the percentages of market participation by the companies.\n\nIn 2000, OSINERG was merged with the Electricity Tariffs Commission (CTE), currently known as Adjunct Office for Tariff Regulation (GART). Together, they are in charge of fixing generation, transmission and distribution tariffs and the tariff adjustment conditions for the end consumers. They also determine the tariffs for transport and distribution of gas by pipeline.\n\nAs for rural electrification, the National Rural Electrification Office (DGER) is in charge of the National Rural Electrification Plan (PNER), which is framed under the policy guidelines set by the Ministry of Energy and Mines. DGER is in charge of the execution and coordination of projects in rural areas and regions of extreme poverty.\n\nFinally, the National Institute for Defense of Competition and the Protection of Intellectual Property (INDECOPI) is in charge of monitoring compliance with the Anti-monopoly and Anti-oligopoly Law of 1997.\n\nIn 2006, 38 companies generated electricity for the market, while 78 companies produced electricity for their own use. Among the 38 companies supplying energy to the market, four of them accounted for 70% of the total capacity:\n\n\nELP dominates hydroelectric production, with 32% of the total, while EDEGEL leads thermal generation also with 32% of the total.\n\nPrivate companies dominate the generation sector. In terms of participation, state companies hold 31% of generation capacity, with the remaining 69% in private hands. Production percentages are 40% and 60% for the public and private companies respectively.\n\nIn Peru, 100% of the transmission activities are in private hands. In 2006, there were 6 purely transmission companies that participated in electricity transmission in Perú: Red de Energía del Perú S.A. (REPSA), with 28% of the transmission lines; and Consorcio Energético Huancavelica (CONENHUA), Consorcio Transmantaro S.A. (S.A. Transmantaro), Eteselva S.R.L, Interconexión Eléctrica ISA Perú (ISAPERU) and Red Eléctrica del Sur.S.A. (REDESUR), with 15% of the lines. Generation and distribution utilities and the companies that generate electricity for their own consumption operate the remaining 57% of the transmission lines.\n\nIn 2006, 63% of the electricity was commercialized by 22 distribution companies, while the remaining 37% was commercialized directly by generation companies. The companies that stood out for their sales to end-consumers were: Luz del Sur (21%), Edelnor (21%), Enersur (9%), Edegel (8%), Electroperú (5%), Hidrandina (4%), Termoselva (4%) and Electroandes (4%).\n\nPublic distribution companies supply electricity to 55% of the existing clients, with the remaining 45% in hands of the private utilities. However, in terms of electricity distributed, private companies have the lead with 71% of the total as opposed to 29% for the public ones.\n\nThe National Environment Fund (FONAM) was created in 1997 and received the mandate from the Peruvian Congress to identify and promote projects that exploit renewable energy sources, introduce clean technologies, and promote energy efficiency and the substitution of highly polluting fuels. However, the contribution of renewable energy sources other than hydroelectricity is still very limited in Peru.\n\nHydroelectricity is the only renewable resource exploited in Peru. In 2006, it accounted for 48% of total installed capacity and 72% of electricity generated. The largest hydroelectric facility in the country is the 900 MW Mantaro Complex in southern Peru, which is operated by state-owned Electroperu. The two hydroelectric plants at the complex generate over one-third of Peru’s total electricity supply. In February 2006, Egecen S.A. completed construction of the 130-MW, Yuncán hydroelectric plant, located northeast of Lima. The plant will be operated by EnerSur, a subsidiary of Brussels-based Suez Energy International. \n\nConstruction on the multi-purpose Olmos Transandino Project has been underway since 2006 and in February 2010, the contract for its hydroelectricity power plant is expected to be issued. The power station in northwest Peru's Cajamarca province will have a 600 MW capacity and produce 4,000 GWh annually. Construction of the 406 MW dam on Marañón River in Chaglla District started in 2012. The 525 MW Cerro del Águila was inaugurated in 2016. In 2012, the Salcca-Pucara hydroelectric project received the last of several approvals it needed.\n\nStudies from the National Meteorological and Hydrological Service (SENAMHI) have estimated a total windpower potential of 19 GWh/year for Peru, or about 70% of current electricity consumption. The Departments of Talara, Laguna Grande, Marcona and Pta. Atico are the regions with the largest wind potential. However, the absence of a regulatory framework and of a reliable record of wind potential, together with the lack of human, financial, and technical resources, has so far hindered the exploitation of Peru’s windpower potential.\n\nThe contribution of wind power to the energy matrix in Peru was negligible in the first decade of the 21st century, with just 0.7 MW of installed capacity in 2006. In 2014 three large wind farms were inaugurated: the 32MW Marcona Wind Farm in the Ica region, the 83MW Cupisnique Wind Farm in Pacasmayo and the 30MW Talara Wind Farm in the Piura region. The 132 MW Wayra 1 in Marcona District is scheduled for completion in 2018.\n\nIt has been estimated that Peru has favorable conditions for the development of solar energy projects. However, the country’s solar potential has not been exploited yet. In the mountain ranges located in the South, solar energy reaches average levels above 6 kWh/m2/day, which are among the highest worldwide.\n\nA study by the Climate and Development Knowledge Network found that despite the high potential for energy cost savings across the private sector, a number of barriers prevent businesses in Peru from identifying and implementing energy efficiency opportunities in their premises and operations. \n\nEnergy demand in Peru is expected to continue to increase over the coming decades, largely fuelled by industrial expansion and increasing economic prosperity. The Peruvian Government has, however, recognised the importance of energy efficiency as a key element in climate change mitigation strategy and action, with energy efficiency featuring among the climate change mitigation actions in Peru’s Nationally Determined Contribution (NDC) under the Paris Agreement of the United Nations Framework Convention on Climate Change (UNFCCC), and also in Peru’s Planning for Climate Change project, known as ‘PlanCC’. \n\nFrom its inception, the Peruvian electricity system started to be developed by private initiative. In 1955, Law No. 12378 regulated the mechanisms of private participation, establishing a system of concessions with commitments to increase generation capacity by 10% annually. The National Tariff Commission and other mechanisms aiming at guaranteeing the profitability of the investments were then created. However, at the beginning of the 1970s, profound changes took place. In 1972, the \"de facto\" military government nationalized the electricity industry through Law No.19521, creating ELECTROPERU (Peru Electricity Company). ELECTROPERU became the owner of all the generation, transmission and distribution assets and came to be in charge of service provision and investment planning. Until the beginning of the 1980s there were large investments in hydroelectric and thermal projects. However, this dynamism started to fade during the 1980s mainly due to the debt crisis that started in 1982 and that precluded new financing in the region. By the beginning of the 1990s, the electricity sector in Peru showed an important deterioration due to low investment in infrastructure, the fact that tariffs did not cover production costs, the restricted investment in maintenance and the systematic destruction of infrastructures by terrorist activities. The results of this crisis were severe: in 1990 only 45% of the population had access to electricity, supply only covered 74% of the demand and distribution losses were above 20%.\n\nThe structural reform process that started in 1992 under the government of President Alberto Fujimori led to the privatization of the electricity sector in a decade in which most of the countries in the region underwent a similar process. The restructuring process, articulated in the Electricity Concessions Law (LCE) of 1992, unbundled the vertically integrated state monopoly into generation, transmission and distribution and led the basis for the introduction of private operators and competition for generation and commercialization, with transmission and distribution regulated on the basis of free entry and open access. The 1992 Law was modified by Law No. 26876 (Anti-monopoly and Anti-oligopoly Law) in 1997. The process of concessions and transfer of generation assets to private companies was started in 1994 and relaunched in 2002 as it had not been completed yet.\nPrivate companies emerging from the 1992 reforms made substantial investment commitments that were fulfilled in the following years. Investment figures reached their highest levels in the period 1996-1999, declining afterwards once the commitments had been fulfilled. The high level of investment led to annual average increases in installed capacity of 9.2%, a rate that was not matched by the increase in demand, which increased at only 4.7% per year on average. As a result, the level of reserves in the National Interconnected System (SEIN) increased at average rates of 23.2%. Investments in transmission and distribution led to increases in coverage from 53% in 1993 to 76% in 2004.\n\nIn September 2000, the Law for the Promotion of Energy Efficiency (Law No. 27345) was approved, declaring support for the efficient use of energy to be in the national interest. The regulation for this Law was approved in October 2007 (by Supreme Decree No. 053-2007-EM). The objectives of the Law are to contribute to energy security, improve the country’s competitiveness, generate surplus for exports, reduce environmental impacts, protect consumers and raise awareness about the importance of efficient energy use.\n\nAs for rural electrification, there have been several attempts to change the existing institutional and legal framework. In recent years, there have been two laws passed by Congress (the Law for Electrification of Rural and Isolated or Frontier Areas in 2002 and the Law to Regulate the Promotion of Private Investment in Rural Electrification in 2004) but neither of them has been implemented due to conflicts with provisions in other laws.\n\nIn 2006, the average residential tariff in Peru was US$0.1046 per kWh, LAC weighted average in 2005 was US$0.115.\n\nIn the unregulated market, the average tariff for final customers was US$0.0558 per kWh for the electricity supplied directly from the generators and US$0.0551 per kWh for the electricity supplied by distribution companies.\n\nLaw No. 275010 created, in November 2001, the Electricity Social Compensation Fund (FOSE). This Fund established a cross-subsidy system among consumers that benefits users with monthly consumption below 100kWh through fixed and proportional discounts. The fixed discount applies to consumers between 30 and 100 kWh and the proportional discount is targeted to those with consumptions below 30 kWh. The amount of the discounts is financed through a surcharge in the tariff paid by the regulated consumers with monthly consumptions above 100 kWh.\n\nThe number of households that benefit from this scheme is over 2.4 million (out of the 3.6 million connected households at the national level). In July 2004, the FOSE was extended to cover up to 50% of the bill in the National Interconnected System (SEIN) and 62.5% in the isolated systems for the users with consumption below 30kWh, including as well a special focus by geographic location (rural-urban).\n\nIn 2004, annual investment needs in the electricity sector up to 2016 were estimated at US$200 million, considering a projected annual demand increase of 5%.\n\nTotal investment in the electricity sector in 2006 was US$480.2 million, which was 22% higher than the amount for 2005. Investment in generation, transmission and distribution added up to US$446.2 million, while investment by the Executive Office for Projects (DEP) in the Rural Electrification was US$34 million. The table below summarizes the contribution of both the private and the public sector:\n\"Source\": Ministerio de Energia y Minas 2007\n\nInvestment by private companies has taken off after having reached very low numbers by 2003 (US$120 million, matching public investment for that year) after the general decline in investment that happened from 1999 onwards.\n\nTo meet expected demand, total investment needs in electricity generation and transmission between 2002 and 2030 are estimated to be US$16.2-20.7 million.\n\nAfter the power sector reform in the early 1990s, rural electrification in Peru has been limited to direct investment by the central government, without any additional funds from communities, regional governments or service providers. One important issue deterring electricity distribution companies from investing in rural electrification is the fact that they hold concession areas concentrated in small areas around urban centers and are only under the obligation to meet service requests within 100 meters of the existing network.\n\nTo expand coverage, the Government of Peru has been spending an average of US$40–50 million per year in the last ten years for electrification. These investments were carried out through social funds (e.g. FONCODES – Cooperation Fund for Social Development) and, to a larger extent, by the Executive Office for Projects (DEP), a division of the Ministry of Energy and Mines (MEM). The DEP, which is currently in the process of being absorbed by the National Rural Electrification Office (DGER), is in charge of planning, designing and constructing the rural electricity systems. Once they are finalized, the rural electricity systems are handed over for operation either to state-owned distribution companies or to a specially created state-owned asset-holding company that manages the systems under operation contracts with state-owned companies, or municipalities.\n\nThe structural reform process that started in 1992 unbundled the vertically integrated state monopoly and led to the privatization of the electricity sector. Today, private companies dominate the generation sector with almost 70% of capacity in their hands. Although there are about 40 companies that generate electricity for the market, just 4 of them (EDEGEL S.A.A., Electroperú S.A., Energía del Sur S.A. and EGENOR) account for 70% of the total capacity.\n\nAs for transmission, 100% is in the hands of several private companies, while 71% of electricity distributed and 45% of the existing clients were also controlled by private companies.\n\nThe National Environment Commission (CONAM), created in 1994, holds the Environmental responsibilities in Peru and promotes sustainable development. CONAM is a decentralized public agency under the Ministry of the Presidency. Its Management Committee consists of 10 members from the national, regional and local governments; economic sector representatives; NGOs; universities and professional associations. The National Environmental Agenda is the instrument that prioritizes environmental issues identified at the national level.\n\nIn 2002, CONAM created the National Climate Change Strategy, which was aimed at transmitting the relevance of Peru’s vulnerability to climate change. The main objective was to stress the need to incorporate in the country’s policies and programs the necessary adaptation measures and to make the population aware of the existing risks and the actions they can undertake to use resources responsibly. The Program to Strengthen the National Capacity to manage Climate Change and Air Pollution (PROCLIM) was created to implement the aforementioned Strategy. PROCLIM aims to contribute to poverty reduction by promoting the integration of climate change and air quality issues in sustainable development policies.\n\nOLADE (Latin American Energy Association) estimated that CO emissions from electricity production in 2003 were 3.32 million tons of CO, which corresponds to 13% of total emissions from the energy sector.\n\nCurrently (November 2007), there are seven registered CDM projects in the electricity sector in Peru, with overall estimated emission reductions of 800,020 tCOe per year.\n\"Source\": UNFCCC\n\nThe National Environment Fund FONAM is the focal point for CDM projects in Peru.\n\nThe Inter-American Development Bank is currently providing technical assistance for a Sustainable Energy Services project in Peru. This is a US$850,000 project of which the IDB is contributing US$750,000.\n\nCurrently, the World Bank is funding a Rural Electrification project in Peru. This is a 6-year (2006-2012), US$145 million project to which the World Bank is contributing US$50 million in lending and the Global Environment Facility (GEF) a US$10 million grant. This project is increasing access to efficient and sustainable electricity services and improving quality of life and income generation opportunities in rural areas.\n\n\n\n"}
{"id": "7082881", "url": "https://en.wikipedia.org/wiki?curid=7082881", "title": "Enactivism", "text": "Enactivism\n\nEnactivism argues that cognition arises through a dynamic interaction between an acting organism and its environment. It claims that our environment is one which we selectively create through our capacities to interact with the world. \"Organisms do not passively receive information from their environments, which they then translate into internal representations. Natural cognitive systems...participate in the generation of meaning ...engaging in transformational and not merely informational interactions: \"they enact a world\".\" These authors suggest that the increasing emphasis upon enactive terminology presages a new era in thinking about cognitive science. How the actions involved in enactivism relate to age-old questions about free will remains a topic of active debate.\n\nThe term 'enactivism' is close in meaning to 'enaction', defined as \"the manner in which a subject of perception creatively matches its actions to the requirements of its situation\". The introduction of the term \"enaction\" in this context is attributed to Francisco Varela, Evan Thompson, and Eleanor Rosch, who proposed the name to \"emphasize the growing conviction that cognition is not the representation of a pre-given world by a pre-given mind but is rather the enactment of a world and a mind on the basis of a history of the variety of actions that a being in the world performs\". This was further developed by Thompson and others, to place emphasis upon the idea that experience of the world is a result of mutual interaction between the sensorimotor capacities of the organism and its environment.\n\nThe initial emphasis of enactivism upon sensorimotor skills has been criticized as \"cognitively marginal\", but it has been extended to apply to higher level cognitive activities, such as social interactions. \"In the enactive view... knowledge is constructed: it is constructed by an agent through its sensorimotor interactions with its environment, co-constructed between and within living species through their meaningful interaction with each other. In its most abstract form, knowledge is co-constructed between human individuals in socio-linguistic interactions...Science is a particular form of social knowledge construction...[that] allows us to perceive and predict events beyond our immediate cognitive grasp...and also to construct further, even more powerful scientific knowledge.\"\n\nEnactivism is closely related to situated cognition and embodied cognition, and is presented as an alternative to cognitivism, computationalism, and Cartesian dualism.\n\nEnactivism is one of a cluster of related theories sometimes known as the \"4Es\". As described by Mark Rowlands, mental processes are:\n\nEnactivism proposes an alternative to dualism as a philosophy of mind, in that it emphasises the interactions between mind, body and the environment, seeing them all as inseparably intertwined in mental processes. The self arises as part of the process of an embodied entity interacting with the environment in precise ways determined by its physiology. In this sense, individuals can be seen to \"grow into\" or arise from their interactive role with the world.\n\nIn \"The Tree of Knowledge\" Maturana & Varela proposed the term \"enactive\" \"to evoke the view of knowledge that what is known is brought forth, in contraposition to the more classical views of either cognitivism or connectionism. They see enactivism as providing a middle ground between the two extremes of representationalism and solipsism. They seek to \"confront the problem of understanding how our existence-the praxis of our living- is coupled to a surrounding world which appears filled with regularities that are at every instant the result of our biological and social histories... to find a \"via media\": to understand the regularity of the world we are experiencing at every moment, but without any point of reference independent of ourselves that would give certainty to our descriptions and cognitive assertions. Indeed the whole mechanism of generating ourselves, as describers and observers tells us that our world, as the world which we bring forth in our coexistence with others, will always have precisely that mixture of regularity and mutability, that combination of solidity and shifting sand, so typical of human experience when we look at it up close.\"[\"Tree of Knowledge\", p. 241]\n\nEnactivism also addresses the hard problem of consciousness, referred to by Thompson as part of the \"explanatory gap\" in explaining how consciousness and subjective experience are related to brain and body. \"The problem with the dualistic concepts of consciousness and life in standard formulations of the hard problem is that they exclude each other by construction\". Instead, according to Thompson's view of enactivism, the study of consciousness or phenomenology as exemplified by Husserl and Merleau-Ponty is to complement science and its objectification of the world. \"The whole universe of science is built upon the world as directly experienced, and if we want to subject science itself to rigorous scrutiny and arrive at a precise assessment of its meaning and scope, we must begin by reawakening the basic experience of the world of which science is the second-order expression\" (Merleau-Ponty, \"The phenomenology of perception\" as quoted by Thompson, p. 165). In this interpretation, enactivism asserts that science is formed or enacted as part of humankind's interactivity with its world, and by embracing phenomenology \"science itself is properly situated in relation to the rest of human life and is thereby secured on a sounder footing.\"\n\nEnaction has been seen as a move to conjoin representationalism with phenomenalism, that is, as adopting a constructivist epistemology, an epistemology centered upon the active participation of the subject in constructing reality. However, 'constructivism' focuses upon more than a simple 'interactivity' that could be described as a minor adjustment to 'assimilate' reality or 'accommodate' to it. Constructivism looks upon interactivity as a radical, creative, revisionist process in which the knower \"constructs\" a personal 'knowledge system' based upon their experience and tested by its viability in practical encounters with their environment. Learning is a result of perceived anomalies that produce dissatisfaction with existing conceptions.\n\nHow does constructivism relate to enactivism? From the above remarks it can be seen that Glasersfeld expresses an interactivity between the knower and the known quite acceptable to an enactivist, but does not emphasize the structured probing of the environment by the knower that leads to the \"perturbation relative to some expected result\" that then leads to a new understanding. It is this probing activity, especially where it is not accidental but deliberate, that characterizes enaction, and invokes \"affect\", that is, the motivation and planning that lead to doing and to fashioning the probing, both observing and modifying the environment, so that \"perceptions and nature condition one another through generating one another.\" The questioning nature of this probing activity is not an emphasis of Piaget and Glasersfeld.\n\nSharing enactivism's stress upon both action and embodiment in the incorporation of knowledge, but giving Glasersfeld's mechanism of viability an evolutionary emphasis, is evolutionary epistemology. Inasmuch as an organism must reflect its environment well enough for the organism to be able to survive in it, and to be competitive enough to be able to reproduce at sustainable rate, the structure and reflexes of the organism itself embody knowledge of its environment. This biology-inspired theory of the growth of knowledge is closely tied to universal Darwinism, and is associated with evolutionary epistemologists such as Karl Popper, Donald T. Campbell, Peter Munz, and Gary Cziko. According to Munz, \"an organism is an \"embodied theory\" about its environment... Embodied theories are also no longer expressed in language, but in anatomical structures or reflex responses, etc.\"\n\nMcGann & others argue that enactivism attempts to mediate between the explanatory role of the coupling between cognitive agent and environment and the traditional emphasis on brain mechanisms found in neuroscience and psychology. In the interactive approach to social cognition developed by De Jaegher & others, the dynamics of interactive processes are seen to play significant roles in coordinating interpersonal understanding, processes that in part include what they call \"participatory sense-making\". Recent developments of enactivism in the area of social neuroscience involve the proposal of \"The Interactive Brain Hypothesis\" where social cognition brain mechanisms, even those used in non-interactive situations, are proposed to have interactive origins.\n\nIn the enactive view, perception \"is not conceived as the transmission of information but more as an exploration of the world by various means. Cognition is not tied into the workings of an 'inner mind', some cognitive core, but occurs in directed interaction between the body and the world it inhabits.\"\n\nAlva Noë in advocating an enactive view of perception sought to resolve how we perceive three-dimensional objects, on the basis of two-dimensional input. He argues that we perceive this solidity (or 'volumetricity') by appealing to patterns of sensorimotor expectations. These arise from our agent-active 'movements and interaction' with objects, or 'object-active' changes in the object itself. The solidity is perceived through our expectations and skills in knowing how the object's appearance would change with changes in how we relate to it. He saw all perception as an active exploration of the world, rather than being a passive process, something which happens to us.\n\nNoë's idea of the role of 'expectations' in three-dimensional perception has been opposed by several philosophers, notably by Andy Clark. Clark points to difficulties of the enactive approach. He points to internal processing of visual signals, for example, in the ventral and dorsal pathways, the two-streams hypothesis. This results in an integrated perception of objects (their recognition and location, respectively) yet this processing cannot be described as an action or actions. In a more general criticism, Clark suggests that perception is not a matter of expectations about sensorimotor mechanisms guiding perception. Rather, although the limitations of sensorimotor mechanisms constrain perception, this sensorimotor activity is drastically filtered to fit current needs and purposes of the organism, and it is these imposed 'expectations' that govern perception, filtering for the 'relevant' details of sensorimotor input (called \"sensorimotor summarizing\").\n\nThese sensorimotor-centered and purpose-centered views appear to agree on the general scheme but disagree on the dominance issue – is the dominant component peripheral or central. Another view, the closed-loop perception one, assigns equal a-priori dominance to the peripheral and central components. In closed-loop perception, perception emerges through the process of inclusion of an item in a motor-sensory-motor loop, i.e., a loop (or loops) connecting the peripheral and central components that are relevant to that item. The item can be a body part (in which case the loops are in steady-state) or an external object (in which case the loops are perturbed and gradually converge to a steady state). These enactive loops are always active, switching dominance by the need.\n\nAnother application of enaction to perception is analysis of the human hand. The many remarkably demanding uses of the hand are not learned by instruction, but through a history of engagements that lead to the acquisition of skills. According to one interpretation, it is suggested that \"the hand [is]...an organ of cognition\", not a faithful subordinate working under top-down instruction, but a partner in a \"bi-directional interplay between manual and brain activity.\" According to Daniel Hutto: \"Enactivists are concerned to defend the view that our most elementary ways of engaging with the world and others - including our basic forms of perception and perceptual experience - are mindful in the sense of being phenomenally charged and intentionally directed, despite being non-representational and content-free.\" Hutto calls this position 'REC' (Radical Enactive Cognition): \"According to REC, there is no way to distinguish neural activity that is imagined to be genuinely content involving (and thus truly mental, truly cognitive) from other non-neural activity that merely plays a supporting or enabling role in making mind and cognition possible.\"\n\nHanne De Jaegher and Ezequiel Di Paolo (2007) have extended the enactive concept of sense-making into the social domain. The idea takes as its departure point the process of interaction between individuals in a social encounter. De Jaegher and Di Paolo argue that the interaction process itself can take on a form of autonomy (operationally defined). This allows them to define social cognition as the generation of meaning and its transformation through interacting individuals.\n\nThe notion of participatory sense-making has led to the proposal that interaction processes can sometimes play constitutive roles in social cognition (De Jaegher, Di Paolo, Gallagher, 2010). It has been applied to research in social neuroscience\"\" and autism.\"\"\n\nIn a similar vein, \"an inter-enactive approach to agency holds that the behavior of agents in a social situation unfolds not only according to their individual abilities and goals, but also according to the conditions and constraints imposed by the autonomous dynamics of the interaction process itself\". According to Torrance, enactivism involves five interlocking themes related to the question \"What is it to be a (cognizing, conscious) agent?\" It is:\n\nTorrance adds that \"many kinds of agency, in particular the agency of human beings, cannot be understood separately from understanding the nature of the interaction that occurs between agents.\" That view introduces the social applications of enactivism. \"Social cognition is regarded as the result of a special form of action, namely \"social interaction\"...the enactive approach looks at the circular dynamic within a dyad of embodied agents.\"\nIn cultural psychology, enactivism is seen as a way to uncover cultural influences upon feeling, thinking and acting. Baerveldt and Verheggen argue that \"It appears that seemingly natural experience is thoroughly intertwined with sociocultural realities.\" They suggest that the social patterning of experience is to be understood through enactivism, \"the idea that the reality we have in common, and in which we find ourselves, is neither a world that exists independently from us, nor a socially shared way of representing such a pregiven world, but a world itself brought forth by our ways of communicating and our joint action...The world we inhabit is manufactured of 'meaning' rather than 'information'.\n\nLuhmann attempted to apply Maturana and Varela's notion of autopoiesis to social systems. \"A core concept of social systems theory is derived from biological systems theory: the concept of \"autopoiesis\". Chilean biologist Humberto Maturana come up with the concept to explain how biological systems such as cells are a product of their own production.\" \"Systems exist by way of operational closure and this means that they each construct themselves and their own realities.\"\n\nThe first definition of enaction was introduced by psychologist Jerome Bruner, who introduced enaction as 'learning by doing' in his discussion of how children learn, and how they can best be helped to learn. He associated enaction with two other ways of knowledge organization: Iconic and Symbolic.\n\nThe term 'enactive framework' was elaborated upon by Francisco Varela and Humberto Maturana.\n\nSriramen argues that enactivism provides \"a rich and powerful explanatory theory for learning and being.\" and that it is closely related to both the ideas of cognitive development of Piaget, and also the social constructivism of Vygotsky. Piaget focused on the child's immediate environment, and suggested cognitive structures like spatial perception emerge as a result of the child's interaction with the world. According to Piaget, children \"construct\" knowledge, using what they know in new ways and testing it, and the environment provides feedback concerning the adequacy of their construction. In a cultural context, Vygotsky suggested that the kind of cognition that can take place is not dictated by the engagement of the isolated child, but is also a function of social interaction and dialogue that is contingent upon a sociohistorical context. Enactivism in educational theory \"looks at each learning situation as a complex system consisting of teacher, learner, and context, all of which frame and co-create the learning situation.\" Enactivism in education is very closely related to situated cognition, which holds that \"knowledge is situated, being in part a product of the activity, context, and culture in which it is developed and used.\" This approach challenges the \"separating of what is learned from how it is learned and used.\"\n\nThe ideas of enactivism regarding how organisms engage with their environment have interested those involved in robotics and man-machine interfaces. The analogy is drawn that a robot can be designed to interact and learn from its environment in a manner similar to the way an organism does that, and a human can interact with a computer-aided design tool or data base using an interface that creates an enactive environment for the user, that is, all the user's tactile, auditory, and visual capabilities are enlisted in a mutually explorative engagement, capitalizing upon all the user's abilities, and not at all limited to cerebral engagement. In these areas it is common to refer to affordances as a design concept, the idea that an environment or an interface affords opportunities for enaction, and good design involves optimizing the role of such affordances.\n\nThe activity in the AI community also has influenced enactivism as whole. Referring extensively to modeling techniques for evolutionary robotics by Beer, the modeling of learning behavior by Kelso, and to modeling of sensorimotor activity by Saltzman, McGann, De Jaegher, and Di Paolo discuss how this work makes the dynamics of coupling between an agent and its environment, the foundation of enactivism, \"an operational, empirically observable phenomenon.\" That is, the AI environment invents examples of enactivism using concrete examples that, although not as complex as living organisms, isolate and illuminate basic principles.\n\n\n"}
{"id": "1061357", "url": "https://en.wikipedia.org/wiki?curid=1061357", "title": "History of Eurasia", "text": "History of Eurasia\n\nThe history of Eurasia is the collective history of a continental area with several distinct peripheral coastal regions: the Middle East, South Asia, East Asia, Southeast Asia, and Europe, linked by the interior mass of the Eurasian steppe of Central Asia and Eastern Europe. Perhaps beginning with the Steppe Route trade, the early Silk Road, the Eurasian view of history seeks establishing genetic, cultural, and linguistic links between European and Asian cultures of antiquity. In fact, much interest in this area lies with the presumed origin of the speakers of the Proto-Indo-European language and Chariot warfare in Central Eurasia.\n\nFossilized remains of Homo ergaster and Homo erectus between 1.8 and 1.0 million years old have been found in Europe (Georgia (Dmanisi), Spain), Indonesia (e.g., Sangiran and Trinil), Vietnam, and China (e.g., Shaanxi). (See also:Multiregional hypothesis.) The first remains are of Olduwan culture, later of Acheulean and Clactonian culture. Finds of later fossils, such as Homo cepranensis, are local in nature, so the extent of human residence in Eurasia during 1,000,000 - 300,000 ybp remains a mystery.\n\nGeologic temperature records indicate two intense ice ages dated around 650000 ybp and 450000 ybp. These would have presented any humans outside the tropics unprecedented difficulties. Indeed, fossils from this period are very few, and little can be said of human habitats in Eurasia during this period. The few finds are of Homo antecessor and Homo heidelbergensis, and [Lantian Man]] in China. \n\nAfter this,Homo neanderthalensis, with his Mousterian technology, emerged, in areas from Europe to western Asia and continued to be the dominant group of humans in Europe and the Middle East up until 40000-28000 ybp. Peking man has also been dated to this period. During the Eemian Stage, humans probably (e.g. Wolf Cave) spread wherever their technology and skills allowed. TBhe Sahara dried up, forming a difficult area for peoples to cross.\n\nThe birth of the first modern humans (Homo sapiens idaltu) has been dated between 200000 to 130000 ybp (see:Mitochondrial Eve, Single-origin hypothesis), that is, to the coldest phase of the Riss glaciation. Remains of Aterian culture appear in the archaeological evidence.\n\nIn the beginning of the last ice age a supervolcano erupted in Indonesia. Theory states the effects of the eruption caused global climatic changes for many years, effectively obliterating most of the earlier cultures. Y-chromosomal Adam (90000 - 60000 BP, dated data) was initially dated here. Neanderthals survived this abrupt change in the environment, so it's possible for other human groups too. According to the theory humans survived in Africa, and began to resettle areas north, as the effects of the eruption slowly vanished. Upper Paleolithic revolution began after this extreme event, the earliest finds are dated c.50000 BCE.\n\nA divergence in genetical evidence occurs during the early phase of the glaciation. Descendants of female haplogroups M, N and male CT are the ones found among Eurasian peoples today.\n\nThe Southern Dispersal scenario postulates the arrival of anatomically modern humans to Eurasia beginning about 70,000 BC. Moving along the southern coast of Asia, they reached Maritime Southeast Asia by about 65,000 years ago. \nThe establishment of population centers in Western Asia, the Indian subcontinent and in East Asia is attested by about 50,000 years ago.\nThe Eurasian Upper Paleolithic proper is taken after c. 45,000 years ago, with the Cro-Magnon expansion of into Europe (Mousterian), and the expansion into the Mammoth steppe of Northern Asia.\n\nTracing back minute differences in the genomes of modern humans by methods of genetic genealogy, can and have been used to produce models of historical migration. Though these give indications of the routes taken by ancestral humans, the dating of the various genetic markers is improving. The earliest migrations (dated c. 75.000 BP) from the Red Sea shores have been most likely along southern coast of Asia. After this, tracking and timing genetical markers gets increasingly difficult. What is known, is that on areas, of what is now Iraq, Iran, Pakistan and Afghanistan, genetic markers diversify (from about 60000 BCE), and subsequent migrations emerge to all directions (even back to the Levant) and North Africs. From the foothills of the Zagros, big game hunting cultures developed which spread across the Eurasian steppe. Crossing the Caucasus and the Ural Mountains were the ancestors of Samoyeds and the ancestors of Uralic peoples, developing sleds, skis and canoes. Through Kazakhstan moved the ancestors of the Indigenous Americans (dated 50000 - 40000 BCE). Eastbound (maybe through Dzungaria and the Tarim Basin went the ancestors of the northern Chinese and Koreans. It is possible that the routes taken by the Indo-European ancestors travelled across the Bosphorus. Genetic evidence suggests a number of separate migrations (1.Anatoleans 2.Tocharians, 3 Celto-Illyrians, 4.Germanic and Slav, - possibly in this order). Archaeological evidence has not been identified for a number of different groups. On historical linguistic evidence, see for example classification of Thracian. The traditional view of associating early Celts with the Hallstatt culture, and the Nordic Bronze Age with Germanic peoples. The Roman Empire spread after the first widespread use of iron outside Central Europe from the Villanovan culture area. Most likely there was trade also in these periods, e.g. with amber and salt being major products. \n\nInfluences from northern Africa via Gibraltar and Sicilia cannot be readily discounted. Many other questions remain open, too; for example, Neanderthals were still present at this time. More genetic data is being gathered by various research programs.\n\nAs the ice age ended, major environmental changes happened, such as sea level rise (est. 120m), vegetation changes, and animals disappearing in the Holocene extinction event. At the same time Neolithic revolution began and humans started to make pottery, began to cultivate crops and domesticated some animal species. \n\nNeolithic cultures in Eurasia are many, and best discussed in separate articles. Some of the articles on this subject include: Natufian culture, Jōmon culture, List of Neolithic cultures of China and Mehrgarh. European sites are many, they are discussed in Prehistoric Europe. The finding of Ötzi the Iceman (dated 3300 BC) provides an important insight to Chalcolithic period in Europe. Proto-languages of various peoples have been forming in this period, though no literal evidence can (by definition) be found. Later migrations further complicate the study of migrations in this period.\n\nOrigins of writing are dated to fourth millennium BC. Writing may have started independently on various areas of Eurasia. It appears the skill spread relatively fast, giving people a new way of communication.\n\nThe three regions of Western, Eastern and Southern Asia developed in a similar manner with each of the three regions developing early civilizations around fertile river valleys. The civilizations in Mesopotamia, the Indus Valley, and China (along the Yellow River and the Yangtze) shared many similarities and likely exchanged technologies and ideas such as mathematics and the wheel. Ancient Egypt also shared this model. These civilizations were most likely in more or less regular contact with each other by the early versions of the silk road. \n\nEurope was different, however. It was somewhat further north and contained no river systems to support agriculture. Thus Europe remained comparatively undeveloped, with only the southern tips of the region (Greece and Italy) being able to fully borrow crops, technologies, and ideas from the Middle East and North Africa. Similarly, civilization didn't arise in Southeast Asia until contact was made with ancient India, which gave rise to Indianized kingdoms in Indochina and the Malay archipelago. The steppe region had long been inhabited by mounted nomads, and from the central steppes they could reach all areas of the Asian continent. The northern part of the silk road traversed this region. \n\nOne such central expansion out of the steppe is that of the Proto-Indo-Europeans which spread their languages into the Middle East, India, Europe, and to the borders of China (with the Tocharians). Throughout their history, up to the development of gunpowder, all the areas of Eurasia would be repeatedly menaced by the Indo-Iranian, Turkic and Mongol nomads from the steppe.\n\nA difference between Europe and most of the regions of Eurasia is that each of the latter regions has few obstructions internally even though it is ringed by mountains and deserts. This meant that it was easier to establish unified control over the entire region, and this did occur with massive empires consistently dominating the Middle East, China, and at times, much of India. Europe, however, is riddled with internal mountain ranges: The Carpathians, the Alps, the Pyrenees and many others. Throughout its history, Europe has thus usually been divided into many small states, much like the Middle East and Indian subcontinent for much of their history.\n\nThe Iron Age made large stands of timber essential to a nation's success because smelting iron required so much fuel, and the pinnacles of human civilizations gradually moved as forests were destroyed. In Europe the Mediterranean region was supplanted by the German and Frankish lands. In the Middle East the main power center became Anatolia with the once dominant Mesopotamia its vassal. In China, the economical, agricultural, and industrial center moved from the northern Yellow River to the southern Yangtze, though the political center remained in the north. In part this is linked to technological developments, such as the mouldboard plough, that made life in once undeveloped areas more bearable.\n\nThe civilizations in China, India, and Mediterranean, connected by the silk road, became the principal civilizations in Eurasia in early CE times. Later development of Eurasian history of mankind is told in other articles.\n\n\n"}
{"id": "6314875", "url": "https://en.wikipedia.org/wiki?curid=6314875", "title": "Huon Peninsula", "text": "Huon Peninsula\n\nHuon Peninsula is a large rugged peninsula on the island of New Guinea in Morobe Province, eastern Papua New Guinea. It is named after French explorer Jean-Michel Huon de Kermadec. The peninsula is dominated by the steep Saruwaged and Finisterre and Cromwell Mountains. The nearest large town is the Morobe provincial capital Lae to the south, while settlements on the north coast include the former German town of Finschhafen, the district capital of Wasu, Malalamai and Saidor with its World War II era Saidor Airport.\n\nThe area was the site of the Huon Peninsula campaign of World War II, in 1943-44 as Japanese troops retreating from Lae fought their way over the Finisterre Mountains to Madang on the north coast.\n\nThe rainforests that cover these remote mountains provide habitats for many birds and animals and have been designated the Huon Peninsula Montane Rain Forests ecoregion. The rainforest of the hillsides consists of shorter trees and more herbs than you would find in lowland rainforests around the world, with predominant species of tree including \"Pometia\", \"Canarium\", \"Anisoptera\", \"Cryptocarya\" laurels, and \"Terminalia\", while the higher slopes have thicker forests of yet smaller trees and the higher slopes of the Cromwell Range in particular hold the best-preserved large forest of \"Dacrydium\" conifers in the southern hemisphere.\n\nMammals indigenous to the Huon region include the endangered Matschie's tree-kangaroo, while the birds include many of typical Australasian families such as bowerbirds, Australian robins, honeyeaters (including the endemic spangled honeyeater) and birds of paradise (including the endemic Huon astrapia). There are also endemic butterflies. Although some logging has taken place, the forests of the Huon Peninsula mountains are mostly undisturbed.\n\nThe raised beach coastal terraces of Huon were added to the UNESCO World Heritage Tentative List on June 6, 2006 in the Mixed (Cultural + Natural) category.\n\nIn 2009 the YUS Conservation Area was established in the northern part of the peninsula. YUS stretches over 760 km² and includes the three rivers, Yopno, Uruwa and Som, for which it was named.\n"}
{"id": "16585", "url": "https://en.wikipedia.org/wiki?curid=16585", "title": "Jell-O", "text": "Jell-O\n\nJell-O is a registered trademark of Kraft Foods for varieties of gelatin desserts (fruit gels), puddings, and no-bake cream pies. The original gelatin desserts are sometimes referred to as jello.\n\nJell-O is sold prepared (ready to eat) or in powder form, and is available in various colors and flavors. The powder contains powdered gelatin and flavorings, including sugar or artificial sweeteners. It is dissolved in hot water, then chilled and allowed to set. Fruit, vegetables, and whipped cream can be added to make elaborate snacks that can be molded into shapes. Jell-O must be put in a refrigerator until served, and once set, it can be eaten.\n\nSome nongelatin pudding and pie filling products are sold under the Jell-O brand. Pudding is cooked on the stove top with milk, then eaten warm or chilled until firmly set. Jell-O has an instant pudding product which is mixed with cold milk and chilled. To make pie fillings, the same products are prepared with less liquid.\n\nGelatin, a protein produced from collagen extracted from boiled bones, connective tissues, and other animal products, has been a component of food, particularly desserts, since the 15th century.\n\nGelatin was popularized in New York in the Victorian era with spectacular and complex jelly moulds. Gelatin was sold in sheets and had to be purified, which was time-consuming. Gelatin desserts were the province of royalty and the relatively well-to-do. In 1845, a patent for powdered gelatin was obtained by industrialist Peter Cooper, who built the first American steam-powered locomotive, the \"Tom Thumb\". This powdered gelatin was easy to manufacture and easier to use in cooking.\n\nIn 1897, in LeRoy, New York, carpenter and cough syrup manufacturer Pearle Bixby Wait trademarked a gelatin dessert, called Jell-O. His wife May and he added strawberry, raspberry, orange, and lemon flavoring to granulated gelatin and sugar. Then in 1899, Jell-O was sold to Orator Francis Woodward (1856–1906), whose Genesee Pure Food Company produced the successful Grain-O health drink. Part of the legal agreement between Woodward and Wait dealt with the similar Jell-O name.\n\nVarious elements were key to Jell-O becoming a mainstream product: new technologies, such as refrigeration, powdered gelatin and machine packaging, home economics classes, and the company's marketing.\n\nInitially Woodward struggled to sell the powdered product. Beginning in 1902, to raise awareness, Woodward's Genesee Pure Food Company placed advertisements in the \"Ladies' Home Journal\" proclaiming Jell-O to be \"America's Most Famous Dessert.\" Jell-O was a minor success until 1904, when Genesee Pure Food Company sent armies of salesmen into the field to distribute free Jell-O cookbooks, a pioneering marketing tactic. Within a decade, three new flavors, chocolate (discontinued in 1927), cherry and peach, were added, and the brand was launched in Canada. Celebrity testimonials and recipes appeared in advertisements featuring actress Ethel Barrymore and opera singer Ernestine Schumann-Heink. Some Jell-O illustrated advertisements were painted by Maxfield Parrish.\n\nIn 1923, the newly rechristened Jell-O Company launched D-Zerta, an artificially sweetened version of Jell-O. Two years later, Postum and Genesee merged, and in 1927 Postum acquired Clarence Birdseye's frozen foods company to form the General Foods Corporation.\nBy 1930, there appeared a vogue in American cuisine for congealed salads, and the company introduced lime-flavored Jell-O to complement the add-ins that cooks across the country were combining in these aspics and salads. Popular Jell-O recipes often included ingredients like cabbage, celery, green peppers, and even cooked pasta.\n\nBy the 1950s, salads would become so popular that Jell-O responded with savory and vegetable flavors such as celery, Italian, mixed vegetable and seasoned tomato. These flavors have since been discontinued.\n\nIn 1934, sponsorship from Jell-O made comedian Jack Benny the dessert's spokesperson. At this time Post introduced a jingle (\"featured\" by the agency Young & Rubicam) that would be familiar over several decades, in which the spelling \"J-E-L-L-O\" was (or could be) sung over a rising five-note musical theme. The jingle was written by Don Bestor, who was the bandleader for Jack Benny on his radio program.\n\nIn 1936, chocolate returned to the Jell-O lineup, as an instant pudding made with milk. It proved enormously popular, and over time other pudding flavors were added such as vanilla, tapioca, coconut, pistachio, butterscotch, egg custard, flan and rice pudding.\n\nThe baby boom saw a significant increase in sales for Jell-O. Young mothers didn't have the supporting community structures of earlier generations, so marketers were quick to promote easy-to-prepare prepackaged foods. By this time, creating a Jell-O dessert required simply boiling water, Jell-O and Tupperware molds.\n\nNew flavors were continually added and unsuccessful flavors were removed: in the 1950s and 1960s, apple, black cherry, black raspberry, grape, lemon-lime, mixed fruit, orange-banana, pineapple-grapefruit, blackberry, strawberry-banana, tropical fruit and more intense \"wild\" versions of the venerable strawberry, raspberry and cherry. In 1966, the Jell-O \"No-Bake\" dessert line was launched, which allowed a cheesecake to be made in 15 minutes. In 1969, Jell-O 1∗2∗3 (later Jell-O 1•2•3), a gelatin dessert that separated into three layers as it cooled, was unveiled. Until 1987, Jell-O 1•2•3 was readily found in grocery stores throughout most of the United States, but the dessert is now rare. In 1971 packaged prepared pudding called Jell-O Pudding Treats were introduced. Jell-O Whip 'n Chill, a mousse-style dessert, was introduced and widely promoted; it remains available in limited areas today. A similar dessert called Jell-O Soft Swirl was introduced in 1972, flavors included Chocolate Creme, Strawberry Creme, Vanilla Creme and Peach Creme. Florence Henderson appeared in TV ads for this product.\n\nIn 1964, the slogan \"There's always room for Jell-O\" was introduced, promoting the product as a \"light dessert\" that could easily be consumed even after a heavy meal.\n\nThroughout the 1960s through the 1980s, Jell-O's sales steadily decreased. Many Jell-O dishes, such as desserts and Jell-O salads, became special occasion foods rather than everyday items. Marketers blamed this decline on decreasing family sizes, a \"fast-paced\" lifestyle and women's increasing employment. By 1986, a market study concluded that mothers with young children rarely purchased Jell-O.\n\nTo turn things around, Jell-O hired Dana Gioia to stop the decline. The marketing team revisited the Jell-O recipes published in past cookbooks and rediscovered Jigglers, although the original recipe did not use that name. Jigglers are Jell-O snacks molded into fun shapes and eaten as finger food. Jell-O launched a massive marketing campaign, notably featuring Bill Cosby as spokesman. The campaign was a huge success, causing a significant gain.\n\nCosby became the company's pudding spokesperson in 1974, and continued as the voice of Jell-O for almost thirty years. Over his tenure as the mouthpiece for the company, he would help introduce new products such as frozen Jell-O Pops (in gelatin and pudding varieties); the new Sugar-Free Jell-O, which replaced D-Zerta in 1984 and was sweetened with NutraSweet; Jell-O Jigglers concentrated gummi snacks; and Sparkling Jell-O, a carbonated version of the dessert touted as the \"Champagne of Jell-O.\" In 2010, Cosby returned as Jell-O spokesperson in an on-line web series called \"OBKB.\"\n\nIn the 1980s, a Jell-O advertising campaign slogan reminded consumers, \"Don't forget—you have to remember to make it.\"\n\nIn 1990, General Foods merged into Kraft Foods by parent company Philip Morris (now the Altria Group). New flavors were continually introduced: watermelon, blueberry, cranberry, margarita and piña colada among others. In 2001, the state Senate of Utah recognized Jell-O as a favorite snack food of Utah and the Governor Michael O. Leavitt declared an annual \"Jell-O Week.\" During the 2002 Winter Olympics in Salt Lake City, the souvenir pins included one depicting green Jell-O.\n\nIn the late 1980s and early 1990s, Jell-O's family-friendly reputation was slightly tarnished by Jell-O shots and Jell-O wrestling.\n\n, there were over 420 million boxes of Jell-O gelatin and over 1 billion Jell-O cups sold in the United States each year. , there were more than 110 products sold under the Jell-O brand name.\n\nJell-O is used as a substantial ingredient in a well-known dessert, a \"Jell-O mold\" the preparation of which requires a mold designed to hold gelatin, and the depositing of small quantities of chopped fruit, nuts, and other ingredients before it hardens to its typical form. Fresh pineapple, papaya, kiwi, and ginger root cannot be used because they contain enzymes that prevent gelatin from \"setting\". In the case of pineapple juice and the enzyme bromelain that it contains though, the enzyme can be inactivated without denaturing through excessive heating and thus altering the flavor by the addition of a small measured amount of capsaicin sourced from hot chilies.\n\nAn alternative recipe calls for the addition of an alcoholic beverage to the mix, contributing approximately one third to one half of the liquid added after the gelatin has dissolved in the boil. A serving of the resulting mixture is called a \"Jell-O shot\" at parties. The quantity and timing of the addition of the alcohol are vital aspects; it is not possible to make Jell-O shots with alcohol alone, as the colloidal proteins in dry gelatin consist of chains which require a hot liquid to denature them before they can then reform as a semisolid colloidal suspension. Pure alcohol cannot be heated sufficiently to break down these proteins, as it evaporates.\n\nVodka or rum is used in Jell-O shots, but the shots can be made with almost any alcohol. It is important to adjust the proportions of alcohol and cold water to ensure that the mixture set when experimenting with different types of alcohol. The Jell-O shots can be served in shot glasses, small paper or plastic cups; the paper or plastic cups are easier to eat from, but shot glasses are more attractive. The alcohol in Jell-O shots is contained within the Jell-O, so the body absorbs it more slowly, causing people to underestimate how much alcohol they have consumed. Drinkers must monitor their intake because of this.\n\nAmerican singer-songwriter Tom Lehrer claims to have invented the Jell-O shot in the 1950s to circumvent restrictions on alcoholic beverages at the army base where he was stationed. An early published recipe for an alcoholic gelatin drink dates from 1862, found in \"How to Mix Drinks, or The Bon Vivant's Companion\" by Jerry Thomas: his recipe for \"Punch Jelly\" calls for the addition of isinglass or other gelatin to a punch made from cognac, rum, and lemon juice. Thomas warns that strength of the punch is \"artfully concealed\" by the gelatine.\n\n, LeRoy, New York, is known as the home of Jell-O and has the only Jell-O Museum in the world, located on the main road through the small town. Jell-O was manufactured here until General Foods closed the plant in 1964 and relocated manufacturing to Dover, Delaware. The Jell-O Gallery museum is operated by the Le Roy Historical Society at the Le Roy House and Union Free School, listed on the National Register of Historic Places in 1997.\n\nAt the museum, visitors can learn about the history of the dessert from its inception. Visitors starting on East Main Street, follow Jell-O Brick Road, whose stones are inscribed with the names of former factory employees. The museum offers looks at starting materials for Jell-O, such as sturgeon bladder and calves' hooves, and various molds.\n\nThe Jell-O plant in Mason City, Iowa, produces America's supply of ready to eat Jell-O gelatin dessert and pudding cups.\nJack Benny's top-rated radio show did not break for commercials. Instead, announcer Don Wilson incorporated speeches about Jell-O into the program at appropriate places, to Jack's feigned comic annoyance. Lucille Ball's \"My Favorite Husband\", the radio predecessor to TV's \"I Love Lucy\", was another popular program sponsored by Jell-O for much of its 124-episode run. Ball's character Liz Cooper often opened the program with the lively greeting \"jell-o everybody!\"\n\nComedian Bill Cosby is associated with Jell-O and, more famously, Jell-O pudding, and he appeared in many commercials promoting both. Shows like \"Mad TV\", \"The Simpsons\" and \"Saturday Night Live\" parody Cosby, using Jell-O references like \"pudding pop\". In the 1960s, the cast of the sitcom \"Hogan's Heroes\" did a commercial with Carol Channing featuring Colonel Hogan, his men, Kommandant Klink and Sergeant Shultz having Jell-O and Dream Whip for dessert. Also, in the first few seasons of the first of Lucille Ball's two 1960s television series, \"The Lucy Show\", cast members including Vivian Vance often did commercials for Jell-O.\n\nIn 1995, Jell-O carried the tagline \"It's alive!\" and had the phrase \"J-E-L-L-OOOOOOO!\".\n\nIn August 2018, Jell-O released an animated series on YouTube and Amazon Prime Video titled \"JELL-O Wobz\" in partnership with DreamWorksTV.\n\nJell-O is mentioned in the 1936 popular song \"A Fine Romance\" by Dorothy Fields (with music by Jerome Kern), where it is humorously referred to as a mundane alternative to the excitement of romantic love. In 1980, the American composer William Bolcom wrote a popular humorous song about Jell-O, \"Lime Jello Marshmallow Cottage Cheese Surprise\", satirising its use in combined sweet and savory dishes such as Jello salad.\n\nIn 1992, Ivette Bassa won the second ever Ig Nobel Prize in chemistry for inventing blue Jell-O.\n\nJell-O is especially popular among members of The Church of Jesus Christ of Latter-day Saints (LDS Church), often referred to as \"Mormons\". The Mormon Corridor region, which has the highest Mormon populations, was nicknamed the \"Jell-O Belt\", referring to the 20th-century Mormon cultural stereotype that Mormons have an affinity for Jell-O. In support of this image, Jell-O was designated as Utah's official state snack food in 2001. When drafting the resolution, the Utah Legislature gave many reasons to recognize Jell-O, including that Utah had had the highest per-capita consumption of Jell-O for many years, and how citizens of Utah had rallied to \"Take Back the Title\" after Des Moines, Iowa exceeded Utah in Jell-O consumption in 1999. The culture of Utah, petitions by Utahans, and campaigning by students of Brigham Young University were also mentioned as reasons for recognizing Jell-O. Bill Cosby, longtime spokesperson for the Jell-O brand, appeared before the Utah Legislature in support of the bill. \"He told the assembly that he believes the reason people in Utah love Jell-O is that the snack is perfect for families -- and the people of Utah are all about family.\" Jell-O is often served with homemade cookies or cakes and water at LDS Church socials. \n\nThe 2002 LDS Cinema romantic comedy \"The Singles Ward\", which is filled with inside Mormon jokes and stereotypes, has a scene where someone slips and falls in Jell-O at a church social for young, single Mormons.\n\nThe stereotype of Mormons loving Jell-O does not appear to have a long history. Media reports in 1969 and 1988 on foods popular among Mormons or in Utah make no mention of Jell-O, and a 1988 article mentions Jell-O as a Lutheran tradition. In 1997, Kraft foods released sales figures revealing Salt Lake City to have the highest per-capita Jell-O consumption.\n\nThe following are the flavors of Jell-O products that are currently being produced:\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "24813544", "url": "https://en.wikipedia.org/wiki?curid=24813544", "title": "List of Michigan rivers named Little Black River", "text": "List of Michigan rivers named Little Black River\n\nLittle Black River may refer to two rivers in the U.S. state of Michigan:\n\n\n"}
{"id": "5886151", "url": "https://en.wikipedia.org/wiki?curid=5886151", "title": "List of Sites of Special Scientific Interest in Cumbria", "text": "List of Sites of Special Scientific Interest in Cumbria\n\nThe following is a list of Sites of Special Scientific Interest (SSSIs) in Cumbria, England, United Kingdom. In England the body responsible for designating SSSIs is Natural England, which chooses a site because of its fauna, flora, geological or physiographical features.\n\nAs of July 2012 there were 278 SSSIs within Cumbria. Of these, 70 are listed purely for their geological interest and 170 for their biological interest. A further 38 have both geological and biological interest.\n\nThe county includes the whole of the Lake District National Park, as well as three Areas of Outstanding Natural Beauty: the Solway Coast, part of Arnside and Silverdale, and part of the North Pennines.\n\nFor SSSIs in other counties, see List of SSSIs by Area of Search.\n\n\n"}
{"id": "584855", "url": "https://en.wikipedia.org/wiki?curid=584855", "title": "List of U.S. Wilderness Areas", "text": "List of U.S. Wilderness Areas\n\nFour federal agencies of the Federal Government of the United States administer the National Wilderness Preservation System, which includes 765 Wildernesses and as of 2016 . These agencies are:\n\n\nThis is an area larger than Iraq or the state of California. In Alaska, there are of wilderness. This represents about 52% of the wilderness area in the United States. The National Park Service (NPS) has oversight of of wilderness at 60 locations. The U.S. Fish and Wildlife Service has responsibility for in 71 areas. The Bureau of Land Management (BLM) oversees at 222 unique sites. The Department of Agriculture, the U.S. Forest Service oversees of wilderness areas in 442 areas. Some wilderness areas are managed by multiple agencies, so the above totals exceed the actual number of units (759) in the system. In addition, some of the 60 NPS areas with wilderness have multiple units designated as such (for example, Lake Mead National Recreation Area).\n\nSome areas are designated wilderness by state or tribal governments. These are not governed by the Federal National Wilderness Preservation System.\n\nThe following sortable table lists all U.S. areas that have been designated by the United States Congress under the Wilderness Act. The listed designation date is the date that the wilderness was signed into law. Some areas have been expanded or otherwise changed since the original designation date. For more information about a specific area, see the wilderness name link.\n\n\n"}
{"id": "22685047", "url": "https://en.wikipedia.org/wiki?curid=22685047", "title": "List of astronomical catalogues", "text": "List of astronomical catalogues\n\nAn astronomical catalogue is a list or tabulation of astronomical objects, typically grouped together because they share a common type, morphology, origin, means of detection, or method of discovery. Astronomical catalogs are usually the result of an astronomical survey of some kind.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "1695753", "url": "https://en.wikipedia.org/wiki?curid=1695753", "title": "List of national parks and protected areas of Iran", "text": "List of national parks and protected areas of Iran\n\nThe National parks and protected areas, and wildlife refuges, of Iran\n\nThe national parks, protected areas, and wildlife refuges in Iran include:\n\n\n\n\n"}
{"id": "16074499", "url": "https://en.wikipedia.org/wiki?curid=16074499", "title": "List of rivers of Madagascar", "text": "List of rivers of Madagascar\n\nThis is a list of streams and rivers in Madagascar\n\nAcookto River - Ambalona River - Ambato River - Andranotsimisiamalona River - Antainambalana River\n\nBemarivo River - Besokatra River - Betsiboka River - Bombetoka River\n\nDes Pangalanes\n\nFanambana River - Faraony River - Fiherenana River\n\nIfasy River - Ihosy River - Ikopa River - Irodo River - Ivondro River\n\nLinta River - Loky River - Lokoho River\n\nMahajamba River - Mahajilo River - Mahavavy River - Manajeba River - Manambaho River - Manambolo River - Manampatrana River - Mananara River (south) - Mananara River (Analanjirofo) - Mananjary River - Mananjeba River - Mandrare River - Mangoky River - Mangoro River - Mania River - Maningory River - Marimbona River - Menarandra River - Morondava River\n\nNamorona River - Nosivolo River\n\nOnilahy River - Onive River - Onive River (Sava)\n\nRamena River - Rianila River\n\nSaharenana River - Sahatavy River - Sakaleona River - Sakanila River - Sahamaitso - Sakeni River - Sambirano River - Sandrananta River - Simianona River - Sofia River\n\nTahititnaloke River - Tsaratamana - Tsiribihina River\n\nZomandao River\n\n"}
{"id": "48213340", "url": "https://en.wikipedia.org/wiki?curid=48213340", "title": "List of star systems within 65–70 light-years", "text": "List of star systems within 65–70 light-years\n\nThis is a list of star systems within 65-70 light years of Earth.\n\n"}
{"id": "9578815", "url": "https://en.wikipedia.org/wiki?curid=9578815", "title": "List of tornado events by year", "text": "List of tornado events by year\n\nThe following is a list of tornado events by year.\n1970\n1971\n1972\n1973\n1974\n1975\n1976\n1977\n1978\n1979\n\n1980\n1981\n1982\n1983\n1984\n1985\n1986\n1987\n1988\n1989\n\n1990\n1991\n1992\n1993\n1994\n1995\n1996\n1997\n1998\n1999\n\n2000\n2001\n2002\n2003\n2004\n2005\n2006\n2007\n2008\n2009\n\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n\n\n"}
{"id": "41262001", "url": "https://en.wikipedia.org/wiki?curid=41262001", "title": "Luna Ring", "text": "Luna Ring\n\nLuna Ring is a speculative engineering project which consists in a series of solar generators, disposed around the equator of the Moon, that could send the generated electric energy back to the Earth via microwaves from the near side of the Moon. The project was proposed by Japanese construction firm Shimizu Corporation, after the 2011 Tōhoku earthquake and tsunami destroyed the Fukushima Daiichi Nuclear Power Plant, creating public opposition against nuclear electric energy. Until then, Japan had relied heavily on nuclear power.\n\nThe construction of a concrete ring on the Moon's equator to support the solar panels would be performed by robots, that would be teleguided back from Earth. Then, the solar panels would be placed on the concrete layer, and connected to microwave and laser transmitting stations. The energy sent to Earth that way could be captured by receiving stations all through the day. The fact that the ring would surround the entire moon would mean that at least half of it would always be lit by the sun, resulting in constant electric production.\n\nIn 2013, Shimizu Corporation stated that the construction of the Luna Ring could start as early as 2035. However, the practical drawbacks of putting such a technically vaste and complex project into place could hamp its construction, even if it could pave the way for simpler projects in clean energy production. \n\n\n"}
{"id": "17672471", "url": "https://en.wikipedia.org/wiki?curid=17672471", "title": "MIKE SHE", "text": "MIKE SHE\n\nMIKE SHE is an integrated hydrological modelling system for building and simulating surface water flow and groundwater flow. MIKE SHE can simulate the entire land phase of the hydrologic cycle and allows components to be used independently and customized to local needs. MIKE SHE emerged from Système Hydrologique Européen (SHE) as developed and extensively applied since 1977 onwards by a consortium of three European organizations: the Institute of Hydrology (the United Kingdom), SOGREAH (France) and DHI (Denmark). Since then, DHI has continuously invested resources into research and development of MIKE SHE. MIKE SHE can be used for the analysis, planning and management of a wide range of water resources and environmental problems related to surface water and groundwater, especially surface-water impact from groundwater withdrawal, conjunctive use of groundwater and surface water, wetland management and restoration, river basin management and planning, impact studies for changes in land use and climate.\n\nThe program is offered in both 32-bit and 64-bit versions for Microsoft Windows operating systems.\n\n\n"}
{"id": "5054933", "url": "https://en.wikipedia.org/wiki?curid=5054933", "title": "Molten-salt battery", "text": "Molten-salt battery\n\nMolten-salt batteries (including liquid-metal batteries) are a class of battery that uses molten salts as an electrolyte and offers both a high energy density and a high power density. Traditional \"use once\" thermal batteries can be stored in their solid state at room temperature for long periods of time before being activated by heating. Rechargeable liquid-metal batteries are used for electric vehicles and potentially also for grid energy storage, to balance out intermittent renewable power sources such as solar panels and wind turbines.\n\nThermal batteries originated during World War II when German scientist Georg Otto Erb developed the first practical cells using a salt mixture as an electrolyte. Erb developed batteries for military applications, including the V-1 flying bomb and the V-2 rocket, and artillery fuzing systems. None of these batteries entered field use during the war. Afterwards, Erb was interrogated by British intelligence. His work was reported in \"The Theory and Practice of Thermal Cells\". This information was subsequently passed on to the United States Ordnance Development Division of the National Bureau of Standards. When the technology reached the United States in 1946, it was immediately applied to replacing the troublesome liquid-based systems that had previously been used to power artillery proximity fuzes. They were used for ordnance applications (e.g., proximity fuzes) since WWII and later in nuclear weapons. The same technology was studied by Argonne National Laboratories and other researchers in the 1980s for use in electric vehicles.\n\nThermal batteries use an electrolyte that is solid and inactive at ambient temperatures. They can be stored indefinitely (over 50 years) yet provide full power in an instant when required. Once activated, they provide a burst of high power for a short period (a few tens of seconds to 60 minutes or more), with output ranging from watts to kilowatts. The high power is due to the high ionic conductivity of the molten salt, which is three orders of magnitude (or more) greater than that of the sulfuric acid in a lead–acid car battery.\n\nOne design uses a fuze strip (containing barium chromate and powdered zirconium metal in a ceramic paper) along the edge of the heat pellets to initiate burning. The fuze strip is typically fired by an electrical igniter or squib by application of electric current.\n\nAnother design uses a central hole in the middle of the battery stack, into which the high-energy electrical igniter fires a mixture of hot gases and incandescent particles. This allows much faster activation times (tens of milliseconds) vs. hundreds of milliseconds for the edge-strip design. Battery activation can be accomplished by a percussion primer, similar to a shotgun shell. The heat source should be gasless. The standard heat source typically consist of mixtures of iron powder and potassium perchlorate in weight ratios of 88/12, 86/14, or 84/16. The higher the potassium perchlorate level, the higher the heat output (nominally 200, 259, and 297 cal/g respectively). This property of unactivated storage has the double benefit of avoiding deterioration of the active materials during storage and eliminating capacity loss due to self-discharge until the battery is activated.\n\nIn the 1980s lithium-alloy anodes replaced calcium or magnesium anodes, with cathodes of calcium chromate, vanadium or tungsten oxides. Lithium–silicon alloys are favored over the earlier lithium–aluminium alloys. The corresponding cathode for use with the lithium-alloy anodes is mainly iron disulfide (pyrite) replaced by cobalt disulfide for high-power applications. The electrolyte is normally a eutectic mixture of lithium chloride and potassium chloride.\n\nMore recently, other lower-melting, eutectic electrolytes based on lithium bromide, potassium bromide, and lithium chloride or lithium fluoride have also been used to provide longer operational lifetimes; they are also better conductors. The so-called \"all-lithium\" electrolyte based on lithium chloride, lithium bromide, and lithium fluoride (no potassium salts) is also used for high-power applications, because of its high ionic conductivity. A radioisotope thermal generator, such as in the form of pellets of SrTiO, can be used for long-term delivery of heat for the battery after activation, keeping it in a molten state.\n\nThermal batteries are used almost exclusively for military applications, notably for guided missiles. They are the primary power source for many missiles such as the AIM-9 Sidewinder, MIM-104 Patriot, BGM-71 TOW, BGM-109 Tomahawk and others. In these batteries the electrolyte is immobilized when molten by a special grade of magnesium oxide that holds it in place by capillary action. This powdered mixture is pressed into pellets to form a separator between the anode and cathode of each cell in the battery stack. As long as the electrolyte (salt) is solid, the battery is inert and remains inactive. Each cell also contains a pyrotechnic heat source, which is used to heat the cell to the typical operating temperature of 400–550 °C.\n\nSince the mid-1960s much development work has been undertaken on rechargeable batteries using sodium (Na) for the negative electrodes. Sodium is attractive because of its high reduction potential of −2.71 volts, low weight, non-toxic nature, relative abundance, availability and low cost. In order to construct practical batteries, the sodium must be in liquid form. The melting point of sodium is . This means that sodium-based batteries operate at high temperatures between , with newer designs running at temperatures between .\n\nThe sodium–sulfur battery (NaS battery), along with the related lithium–sulfur battery employs cheap and abundant electrode materials. It was the first alkali-metal commercial battery. It used liquid sulfur for the positive electrode and a ceramic tube of beta-alumina solid electrolyte (BASE). Insulator corrosion was a problem because they gradually became conductive, and the self-discharge rate increased. \n\nBecause of their high specific power, NaS batteries have been proposed for space applications. A NaS battery for space use was successfully tested on the space shuttle mission STS-87 in 1997, but the batteries have not been used operationally in space. NaS batteries have been proposed for use in the high-temperature environment of Venus.\n\nA lower-temperature variant of NaS batteries was the development of the ZEBRA (originally, \"Zeolite Battery Research Africa\"; later, the \"Zero Emissions Batteries Research Activity\") battery in 1985, originally developed for electric vehicle applications. The battery uses NaAlCl with Na-beta-alumina ceramic electrolyte.\n\nThe battery operates at and uses molten sodium tetrachloroaluminate (), which has a melting point of , as the electrolyte. The negative electrode is molten sodium. The positive electrode is nickel in the discharged state and nickel chloride in the charged state. Because nickel and nickel chloride are nearly insoluble in neutral and basic melts, contact is allowed, providing little resistance to charge transfer. Since both and Na are liquid at the operating temperature, a sodium-conducting β-alumina ceramic is used to separate the liquid sodium from the molten . The primary elements used in the manufacture of these batteries have much higher worldwide reserves and annual production than lithium.\n\nIt was invented in 1985 by the Zeolite Battery Research Africa Project (ZEBRA) group at the Council for Scientific and Industrial Research (CSIR) in Pretoria, South Africa. It can be assembled in the discharged state, using NaCl, Al, nickel and iron powder. The positive electrode is composed mostly of materials in the solid state, which reduces the likelihood of corrosion, improving safety. Its specific energy is 90 Wh/kg; specific power is 150 W/kg. The β-alumina solid ceramic is unreactive to sodium metal and sodium aluminum chloride. Lifetimes of over 1,500 cycles and five years have been demonstrated with full-sized batteries, and over 3,000 cycles and eight years with 10- and 20-cell modules. For comparison, LiFePO lithium iron phosphate batteries store 90–110 Wh/kg, and the more common LiCoO lithium-ion batteries store 150–200 Wh/kg. A nano lithium-titanate battery stores 72 Wh/kg and can provide power of 760 W/kg.\n\nThe ZEBRA's liquid electrolyte freezes at , and the normal operating temperature range is . Adding iron to the cell increases its power response. ZEBRA batteries are currently manufactured by FIAMM Sonick and are used in the Modec Electric Van, the IVECO daily 3.5 ton delivery vehicle, the prototype Smart ED, and the Th!nk City. In 2011 the US Postal Service began testing all-electric delivery vans, one powered by a ZEBRA battery.\n\nIn 2010 General Electric announced a battery that it called a sodium–metal halide battery, with a 20-year lifetime. Its cathode structure consists of a conductive nickel network, molten salt electrolyte, metal current collector, carbon felt electrolyte reservoir and the active sodium–metal halide salts. In 2015, the company abandoned the project.\n\nSumitomo developed a battery using a salt that is molten at , far lower than sodium based batteries, and operational at . It offers energy densities as high as 290 Wh/L and 224 Wh/kg and charge/discharge rates of 1C with a lifetime of 100 - 1000 charge cycles. The battery employs only nonflammable materials and neither ignites on contact with air nor risks thermal runaway. This eliminates waste-heat storage or fire- and explosion-proof equipment, and allows closer cell packing. The company claimed that the battery required half the volume of lithium-ion batteries and one quarter that of sodium–sulfur batteries. The cell used a nickel cathode and a glassy carbon anode.\n\nIn 2014 researchers identified a liquid sodium–cesium alloy that operates at and produced 420 milliampere-hours per gram. The new material was able to fully coat, or \"wet,\" the electrolyte. After 100 charge/discharge cycles, a test battery maintained about 97% of its initial storage capacity. The lower operating temperature allowed the use of a less-expensive polymer external casing instead of steel, offsetting some of the increased cost of cesium.\n\nWhen not in use, batteries are typically kept molten and ready for use because if allowed to solidify they typically take 12 hours to reheat and charge. This reheating time varies depending on the battery-pack temperature, and power available for reheating. After shutdown a fully charged battery pack loses enough energy to cool and solidify in 3–4 days.\n\nProfessor Donald Sadoway at the Massachusetts Institute of Technology has pioneered the research of liquid-metal rechargeable batteries. Both Magnesium–antimony and more recently lead–antimony were used in experiments at MIT. The electrode and electrolyte layers are heated until they are liquid and self-segregate due to density and immiscibility. They may have longer lifetimes than conventional batteries, as the electrodes go through a cycle of creation and destruction during the charge–discharge cycle, which makes them immune to degradation affecting conventional battery electrodes.\n\nThe technology was proposed in 2009 based on magnesium and antimony separated by a molten salt. Magnesium was chosen as the negative electrode for its low cost and low solubility in the molten-salt electrolyte. Antimony was selected as the positive electrode due to its low cost and higher anticipated discharge voltage.\n\nIn 2011 the researchers demonstrated a cell with a lithium anode and a lead–antimony cathode, which had higher ionic conductivity and lower melting points (350–430 °C). The drawback of the Li chemistry is higher cost. A Li/LiF + LiCl + LiI/Pb-Sb cell with about 0.9 V open-circuit potential operating at 450 °C had electroactive material costs of 100 USD/kWh and 100 USD/kW and a projected 25-year lifetime. Its discharge power at 1.1 A/cm is only 44% (and 88% at 0.14 A/cm).\n\nExperimental data shows 69% storage efficiency, with good storage capacity (over 1000 mAh/cm), low leakage (< 1 mA/cm) and high maximal discharge capacity (over 200 mA/cm). By October 2014 the MIT team achieved an operational efficiency of approximately 70% at high charge/discharge rates (275 mA/cm), similar to that of pumped-storage hydroelectricity and higher efficiencies at lower currents. Tests showed that after 10 years of regular use, the system would retain about 85% of its initial capacity. In September 2014 a study described an arrangement using a molten alloy of lead and antimony for the positive electrode, liquid lithium for the negative electrode; and a molten mixture of lithium salts as the electrolyte.\n\nIn 2010, the Liquid Metal Battery Corporation (LMBC) was formed to commercialize the liquid-metal battery technology invented at MIT. LMBC was renamed Ambri in 2012; the name \"Ambri\" is derived from \"cAMBRIdge\" Massachusetts, where the company is headquartered and where MIT is located. In 2012 and 2014, Ambri received $40 million in funding from Bill Gates, Khosla Ventures, Total S.A., and GVB.\n\nIn September 2015 Ambri announced a layoff, pushing back commercial sales. but announced a return to the battery business with a redesigned battery in 2016.\n\nA recent innovation is the PbBi alloy which enables a very low melting point lithium based battery. It uses a molten salt electrolyte based on LiCl-LiI and operates at 410C.\n\n\n"}
{"id": "37080040", "url": "https://en.wikipedia.org/wiki?curid=37080040", "title": "Nang Tani", "text": "Nang Tani\n\nNang Tani (; \"Lady of Tani\") is a female spirit of the Thai folklore.\n\nAccording to folk tradition this ghost appears as a young woman that haunts wild banana trees (\"Musa balbisiana\"), known as in Thai language as \"Kluai Tani\" (กล้วยตานี).\n\n\"Nang Tani\" belongs to a type of female ghosts or fairies related to trees known generically as \"Nang Mai\" (นางไม้; \"Lady of the Wood\") in the Thai lore. There is a similar spirit in the Cambodian folklore, as well as in the Lao popular tradition.\n\n\"Nang Tani\" may also be called \"Phi Tani\" (ผีตานี; \"Ghost of Tani\") or \"Phrai Tani\" (พรายตานี; \"Nymph of Tani\").\n\nThis ghost inhabits the clumps of wild banana trees and is popularly represented as a beautiful young woman wearing a green traditional Thai costume. Most of the time \"Phi Tani\" remains hidden, but she comes out of the tree and becomes visible especially on full moon nights. She has a greenish complexion, blending with the tree, but her lips have the red color of the ripe ivy gourd. Her hair is black, abundant and untied. She generally appears in a standing position and her feet don't touch the ground, but hover slightly above it. In some modern representations the lower part of her body is represented with an immaterial quality, her waist cloth ending in a kind of wisp emanating from the tree trunk.\n\nIt is considered a bad omen to cut trees from the clump \"Tani\" inhabits. Offerings are made to her in the form of sweets, incense sticks and flowers. People also often tie a length of colored satin cloth around the trunk of the banana tree believed to be haunted by her. The \"Kluai Tani\" type of banana tree does not belong to the cultivated varieties. Owing to their connection with ghosts, people prefer not to have them growing near their homes and they are not found within village compounds. However, clumps of this tree are found not far from inhabited areas, often at the outskirts of villages or at the edge of cultivated fields by the roadside. They look very much like average banana trees, but their fruits are not edible. Their leaves are commonly used in Thailand to wrap locally produced sweets and the inflorescence for the treatment of ulcers in traditional medicine.\n\nLegends of the Thai oral tradition say that this spirit may harm men, especially those who have wronged women, but she is mostly considered benevolent. \"Nang Tani\" generally has a gentle disposition and may give food to passing Buddhist monks. Amulets featuring Nang Tani are popular and come in a variety of shapes and sizes. Some people tie lengths of colored silk around the trunks of the banana trees reputed to be haunted by \"Nang Tani\".\n\n\"Nang Tani\" is a popular folk spirit that has been featured in some Thai films such as \"Nang Phrai Tani\" (นางพรายตานี), a film made in 1967 that has become a classic. \"Nang Tani\" (นางตานี), Tani Thi Rak (ตานีที่รัก), Mon Rak Nang Phrai Patha Nang Tani (มนต์รักนางพรายปะทะนางตานี) and 2000 film Phrai Tani (พรายตานี), are less important movies in which the banana tree ghost plays the main role. She also plays a role in other films such as horror movie Nang Phrai Khanong Rak (นางพรายคะนองรัก), among others. \"Tani\" appeared as well as in a Sylvania light bulb commercial for Thai audiences.\n\nThis female ghost is much loved in Thai popular culture, representations and stories about her, sometimes humorous, are common in Thai comic books\nand story books. Tani has a role as well in the \"Nak\" animated movie.\n\nIn the PC MMORPG \"Ragnarok Online\", \"Nang Tani\" is the MVP boss character of the Ayothaya Dungeon.\n\n\n"}
{"id": "3432409", "url": "https://en.wikipedia.org/wiki?curid=3432409", "title": "North American Carbon Program", "text": "North American Carbon Program\n\nThe North American Carbon Program (NACP) is one of the major elements of the Strategic Plan for the U.S. Climate Change Science Program.\nThe central objective of NACP is to measure and understand carbon stocks and sources and sinks of carbon dioxide (CO), methane (CH), and carbon monoxide (CO) in North America and adjacent ocean regions.\n\nThis program consists of multiple agencies that focus on changes to the carbon cycle . Not only does this program rely on research conducted by multiple agencies, but it is also supported by federal agencies that assist in the funding of this program. To ensure the plan is effective, the \"Science Leadership Group (SLG)\" works to communicate among government program managers, independent research groups, and multiple institutions not affiliated to the government as well.\n\nAlongside the SLG, there is the Carbon Cycle Interagency Working Group (CCIWG), which interacts across agencies and ensures the efficiency of research about the carbon cycle in the United States. It works with 11 federal agencies like the National Aeronautics and Space Administration (NASA), National Institute of Standards and Technology (NIST), US Department of Agriculture (USDA), and the US Environmental Protection Agency (EPA). The CCIWG specializes in communicating among agencies for the proposal of new projects, secures resources for the various research programs available, and reports results to the public.\n\nThe specific program goals are as followed:\n\nDevelop quantitative scientific knowledge, robust observations, and models to determine the emissions and uptake of CO, CH, and CO, changes in carbon stocks, and the factors regulating these processes for North America and adjacent ocean basins.\n\nDevelop the scientific basis to implement full carbon accounting on regional and continental scales. This is the knowledge base needed to design monitoring programs for natural and managed CO sinks and emissions of CH.\n\nSupport long-term quantitative measurements of fluxes, sources, and sinks of atmospheric CO and CH, and develop forecasts for future trends.\n\nThe North American Carbon Program was designed to help with the process of providing data needed to model the synthesis activities.\n\nThe carbon cycle is subdivided into two sections, the rapid carbon exchange, and long-term cycling of carbon through the various geological processes.\n\nThis carbon cycle occurs in our biological processes, including respiration, photosynthesis, and is generated by autotrophs.\n\nAutotrophs (usually photosynthetic) are plants or algae, which capture the carbon dioxide from the air. In water, the compounds removed are bicarbonate ions. They are converted into organic compounds like glucose. Heterotrophs consume the glucose, which further breaks down the compounds and passes them through webs (this process is called cellular respiration). Living systems and decomposers release the carbons as carbon dioxide.This carbon is cycled quickly, with estimates of 1,000 to 1000,000 million metric tons of carbon cycling through these pathways in a single year.\n\nLike its name implies, this pathway takes longer than the pathway above, sometimes lasting millions of years. This form of carbon is found in rocks, in the ocean and other bodies of water, inside the core, and in fossil fuels. It is also found in the atmosphere, where it reacts with water and forms calcium carbonate- found in the shells of marine organisms which later become part of the sediment on the ocean floor.\n\nOver time, the remains of shells, plants, and animals form fossils that are later decomposed and released once again as carbon dioxide into the atmosphere.\n\nThrough volcanoes and eruptions, as well as hydrothermal vents, carbon dioxide is released.\n\n http://www.nacarbon.org/cgi-bin/google_maps/google_map_all.pl?\n\n"}
{"id": "2878755", "url": "https://en.wikipedia.org/wiki?curid=2878755", "title": "Nu Arae", "text": "Nu Arae\n\nNu Arae (ν Arae / ν Ara) is shared by two star systems in the constellation Ara:\nThey are separated by 0.49° on the heavens. The stars are also sometimes referred to Upsilon Arae (υ and υ Arae), but more generally unlettered.\n"}
{"id": "39639420", "url": "https://en.wikipedia.org/wiki?curid=39639420", "title": "Offshore geotechnical engineering", "text": "Offshore geotechnical engineering\n\nOffshore geotechnical engineering is a sub-field of geotechnical engineering. It is concerned with foundation design, construction, maintenance and decommissioning for human-made structures in the sea. Oil platforms, artificial islands and submarine pipelines are examples of such structures. The seabed has to be able to withstand the weight of these structures and the applied loads. Geohazards must also be taken into account. The need for offshore developments stems from a gradual depletion of hydrocarbon reserves onshore or near the coastlines, as new fields are being developed at greater distances offshore and in deeper water, with a corresponding adaptation of the offshore site investigations. Today, there are more than 7,000 offshore platforms operating at a water depth up to and exceeding 2000 m. A typical field development extends over tens of square kilometers, and may comprise several fixed structures, infield flowlines with an export pipeline either to the shoreline or connected to a regional trunkline.\n\nAn \"offshore\" environment has several implications for geotechnical engineering. These include the following:\n\nOffshore structures are exposed to various environmental loads: wind, waves, currents and, in cold oceans, sea ice and icebergs. Environmental loads act primarily in the horizontal direction, but also have a vertical component. Some of these loads get transmitted to the foundation (the seabed). Wind, wave and current regimes can be estimated from meteorological and oceanographic data, which are collectively referred to as metocean data. Earthquake-induced loading can also occur – they proceed in the opposite direction: from the foundation to the structure. Depending on location, other geohazards may also be an issue. All of these phenomena may affect the integrity or the serviceability of the structure and its foundation during its operational lifespan – they need to be taken into account in offshore design.\n\nFollowing are some to the features characterizing the soil in an offshore environment:\n\nWave forces induce motion of floating structures in all six degrees of freedom – they are a major design criterion for offshore structures. When a wave’s orbital motion reaches the seabed, it induces sediment transport. This only occurs to a water depth of about , which is the commonly adopted boundary between \"shallow water\" and \"deep water\".The reason is that the orbital motion only extends to a water depth that is half the wavelength, and the maximum possible wavelength is generally considered to be . In shallow water, waves may generate pore pressure build-up in the soil, which may lead to flow slide, and repeated impact on a platform may cause liquefaction, and loss of support.\n\nCurrents are a source of horizontal loading for offshore structures. Because of the Bernoulli effect, they may also exert upward or downward forces on structural surfaces and can induce the vibration of wire lines and pipelines. Currents are responsible for eddies around a structure, which cause scouring and erosion of the soil. There are various types of currents: oceanic circulation, geostrophic, tidal, wind-driven, and density currents.\n\nGeohazards are associated with geological activity, geotechnical features and environmental conditions. Shallow geohazards are those occurring at less than below the seafloor. Information on the potential risks associated with these phenomena is acquired through studies of the geomorphology, geological setting and tectonic framework in the area of interest, as well as with geophysical and geotechnical surveys of the seafloor. Examples of potential threats include tsunamis, landslides, active faults, mud diapirs and the nature of the soil layering (presence of karst, gas hydrates, carbonates). In cold regions, gouging ice features are a threat to subsea installations, such as pipelines. The risks associated with a particular type of geohazard is a function of how exposed the structure is to the event, how severe this event is and how often it occurs (for episodic events). Any threat has to be monitored, and mitigated for or removed.\n\nOffshore site investigations are not unlike those conducted onshore (see Geotechnical investigation). They may be divided into three phases:\n\nIn this phase, which may take place over a period of several months (depending on project size), information is gathered from various sources, including reports, scientific literature (journal articles, conference proceedings) and databases, with the purpose of evaluating risks, assessing design options and planning the subsequent phases. Bathymetry, regional geology, potential geohazards, seabed obstacles and metocean data are some of the information that are sought after during that phase.\n\nGeophysical surveys can be used for various purposes. One is to study the bathymetry in the location of interest and to produce an image of the seafloor (irregularities, objects on the seabed, lateral variability, ice gouges, …). Seismic refraction surveys can be done to obtain information on shallow seabed stratigraphy – it can also be used to locate material such as sand and gravel for use in the construction of artificial islands. Geophysical surveys are conducted from a research vessel equipped with sonar devices and related equipment, such as single-beam and multibeam echosounders, side-scan sonars, ‘towfish’ and remotely operated vehicles (ROVs). For the sub-bottom stratigraphy, the tools used include boomers, sparkers, pingers and chirp. Geophysical surveys are normally required before conducting the geotechnical surveys; in larger projects, these phases may be interwoven.\n\nGeotechnical surveys involve a combination of sampling, drilling, in situ testing as well as laboratory soil testing that is conducted offshore and, with samples, onshore. They serve to ground truth the results of the geophysical investigations; they also provide a detailed account of the seabed stratigraphy and soil engineering properties. Depending on water depth and metocean conditions, geotechnical surveys may be conducted from a dedicated geotechnical drillship, a semi-submersible, a jackup rig, a large hovercraft or other means. They are done at a series of specific locations, while the vessel maintains a constant position. Dynamic positioning and mooring with four-point anchoring systems are used for that purpose.\n\nShallow penetration geotechnical surveys may include soil sampling of the seabed surface or in situ mechanical testing. They are used to generate information on the physical and mechanical properties of the seabed. They extend to the first few meters below the mudline. Surveys done to these depths, which may be conducted at the same time as the shallow geophysical survey, may suffice if the structure to be deployed at that location is relatively light. These surveys are also useful for planning subsea pipeline routes.\n\nThe purpose of deep penetration geotechnical surveys is to collect information on the seabed stratigraphy to depths extending up to a few 100 meters below the mudline. These surveys are done when larger structures are planned at these locations. Deep drill holes require a few days during which the drilling unit has to remain exactly in the same position (see dynamic positioning).\n\nSeabed surface sampling can be done with a grab sampler and with a box corer. The latter provides undisturbed specimens, on which testing can be conducted, for instance, to determine the soil’s relative density, water content and mechanical properties. Sampling can also be achieved with a tube corer, either gravity-driven, or that can be pushed into the seabed by a piston or by means of a vibration system (a device called a vibrocorer).\n\nDrilling is another means of sampling the seabed. It is used to obtain a record of the seabed stratigraphy or the rock formations below it. The set-up used to sample an offshore structure's foundation is similar to that used by the oil industry to reach and delineate hydrocarbon reservoirs, with some differences in the types of testing. The drill string consists of a series of pipe segments in diameter screwed end to end, with a drillbit assembly at the bottom. As the dragbit (teeth extending downward from the drillbit) cut into the soil, soil cuttings are produced. Viscous drilling mud flowing down the drillpipe collects these cuttings and carry them up outside the drillpipe. As is the case for onshore geotechnical surveys, different tools can be used for sampling the soil from a drill hole, notably \"Shelby tubes\", \"piston samplers\" and \"split spoon samplers\".\n\nInformation on the mechanical strength of the soil can be obtained in situ (from the seabed itself as opposed to in a laboratory from a soil sample). The advantage of this approach is that the data are obtained from soil that has not suffered any disturbance as a result of its relocation. Two of the most commonly used instruments used for that purpose are the cone penetrometer (CPT) and the shear vane.\n\nThe CPT is a rod-shaped tool whose end has the shape of a cone with a known apex angle (\"e.g.\" 60 degrees). As it is pushed into the soil, the resistance to penetration is measured, thereby providing an indication of soil strength. A sleeve behind the cone allows the independent determination of the frictional resistance. Some cones are also able to measure pore water pressure. The shear vane test is used to determine the undrained shear strength of soft to medium cohesive soils. This instrument usually consists of four plates welded at 90 degrees from each other at the end of a rod. The rod is then inserted into the soil and a torque is applied to it so as to achieve a constant rotation rate. The torque resistance is measured and an equation is then used to determine the undrained shear strength (and the residual strength), which takes into account the vane’s size and geometry.\n\n\"Offshore structures\" are mainly represented by platforms, notably jackup rigs, steel jacket structures and gravity-based structures. The nature of the seabed has to be taken into account when planning these developments. For instance, a gravity-based structure typically has a very large footprint and is relatively buoyant (because it encloses a large open volume). Under these circumstances, vertical loading of the foundation may not be as significant as the horizontal loads exerted by wave actions and transferred to the seabed. In that scenario, sliding could be the dominant mode of failure. A more specific example is that of the Woodside \"North Rankin A\" steel jacket structure offshore Australia. The shaft capacity for the piles making up each of the structure's legs was estimated on the basis of conventional design methods, notably when driven into siliceous sands. But the soil at that site was a lower capacity calcareous sand. Costly remediation measures were required to correct this oversight.\n\nProper seabed characterization is also required for mooring systems. For instance, the design and installation of suction piles has to take into account the soil properties, notably its undrained shear strength. The same is true for the installation and capacity assessment of \"plate anchors\".\n\nSubmarine pipelines are another common type of man-made structure in the offshore environment. These structures either rest on the seabed, or are placed inside a trench to protect them from fishing trawlers, dragging anchors or fatigue due current-induced oscillations. Trenching is also used to protect pipelines from gouging by ice keels. In both cases, planning of the pipeline involves geotechnical considerations. Pipelines resting on the seabed require geotechnical data along the proposed pipeline route to evaluate potential stability issues, such as passive failure of the soil below it (the pipeline drops) due to insufficient bearing capacity, or sliding failure (the pipeline shift sideways), due to low sliding resistance. The process of trenching, when required, needs to take into account soil properties and how they would affect ploughing duration. Buckling potential induced by the axial and transverse response of the buried pipeline during its operational lifespan need to be assessed at the planning phase, and this will depend on the resistance of the enclosing soil.\n\n"}
{"id": "14015821", "url": "https://en.wikipedia.org/wiki?curid=14015821", "title": "Oil burden", "text": "Oil burden\n\nOil burden is the volume of petroleum consumed, multiplied by the average price, and divided by nominal gross domestic product.\nThis gives the proportion of the world economy devoted to buying oil. It is a concept developed by Veronique Riches-Flores of Société Générale.\n"}
{"id": "425753", "url": "https://en.wikipedia.org/wiki?curid=425753", "title": "Ordovician–Silurian extinction events", "text": "Ordovician–Silurian extinction events\n\nThe Ordovician–Silurian extinction events, when combined, are the second-largest of the five major extinction events in Earth's history in terms of percentage of genera that became extinct. This event greatly affected marine communities, which caused the disappearance of one third of all brachiopod and bryozoan families, as well as numerous groups of conodonts, trilobites, and graptolites. The Ordovician–Silurian extinction occurred during the Hirnantian stage of the Ordovician Period and the subsequent Rhuddanian stage of the Silurian Period. The last event is dated in the interval of 455–430 Ma ago, i.e., lasting from the Middle Ordovician to Early Silurian, thus including the extinction period. This event was the first of the big five Phanerozoic events and was the first to significantly affect animal-based communities.\n\nAlmost all major taxonomic groups were affected during this extinction event. Extinction was global during this period, eliminating 49-60% of marine genera and nearly 85% of marine species.\n\nBrachiopods, bivalves, echinoderms, bryozoans and corals were particularly affected. Before the late Ordovician cooling, temperatures were relatively warm and it is the suddenness of the climate changes and the elimination of habitats due to sea-level fall that are believed to have precipitated the extinctions. The falling sea level disrupted or eliminated habitats along the continental shelves. Evidence for the glaciation was found through deposits in the Sahara Desert. A combination of lowering of sea level and glacially driven cooling were likely driving agents for the Ordovician mass extinction.\n\nThe extinction occurred , during the Great Ordovician Biodiversification Event. It marks the boundary between the Ordovician and following Silurian period. During this extinction event there were several marked changes in biologically responsive carbon and oxygen isotopes. The spread of anoxia (the absence of oxygen) greatly affected the organisms that lived in this time period. This complexity may indicate several distinct closely spaced events, or particular phases within one event.\n\nAt the time, most complex multicellular organisms lived in the sea, and around 100 marine families became extinct, covering about 49% of faunal genera (a more reliable estimate than species). The brachiopods and bryozoans were decimated, along with many of the trilobite, conodont and graptolite families.\n\nStatistical analysis of marine losses at this time suggests that the decrease in diversity was mainly caused by a sharp increase in extinctions, rather than a decrease in speciation. Several groups of marine organisms with a planktonic lifestyle, more exposed to UV radiation than groups that lived in the benthos, suffered severely during the late Ordovician. Organisms that dwelled in the plankton were affected before benthic organisms during the mass extinction, and species dwelling in shallow water were more likely to become extinct than species dwelling in deep water.\n\nThe analysis of the available information reveals that the conditions during the mass extinction at the Ordovician–Silurian transition were considerably different as compared with the environments during the other four Phanerozoic mass extinction events, although all the main factors that were responsible for these processes were the same: the sea level and climate fluctuations, impact events, and volcanism, which should yield ejection of harmful gases, ashes, and aerosols into the atmosphere and, thus, provoke the greenhouse effect, atmosphere darkening, reduction of the photosynthesis and bio-productivity, the destruction of food chains, and anoxia.\n\nTwo environmental changes associated with the glaciation were responsible for much of the Late Ordovician extinction. First, the cooling global climate was probably especially detrimental because the biota was adapted to an intense greenhouse. Second, sea level decline, caused by sequestering of water in the ice cap, drained the vast epicontinental seaways and eliminated the habitat of many endemic communities.\n\nGlaciation pulses appear to correspond to the beginning and end of the most severe ice age of the Phanerozoic, which marked the end of a longer cooling trend in the Hirnantian faunal stage towards the end of the Ordovician, which had more typically experienced greenhouse conditions.\n\nAs the southern supercontinent Gondwana drifted over the South Pole, ice caps formed on it. Correlating rock strata have been detected in Late Ordovician rock strata of North Africa and then-adjacent northeastern South America, which were south-polar locations at the time. Glaciation locks up water from the world-ocean, and the interglacials free it, causing sea levels repeatedly to drop and rise; the vast shallow intra-continental Ordovician seas withdrew, which eliminated many ecological niches, then returned, carrying diminished founder populations lacking many whole families of organisms. Then they withdrew again with the next pulse of glaciation, eliminating biological diversity at each change (Emiliani 1992 p. 491). In the North African strata, five pulses of glaciation from seismic sections are recorded.\n\nThis incurred a shift in the location of bottom-water formation, shifting from low latitudes, characteristic of greenhouse conditions, to high latitudes, characteristic of icehouse conditions, which was accompanied by increased deep-ocean currents and oxygenation of the bottom-water. An opportunistic fauna briefly thrived there, before anoxic conditions returned. The breakdown in the oceanic circulation patterns brought up nutrients from the abyssal waters. Surviving species were those that coped with the changed conditions and filled the ecological niches left by the extinctions.\n\nSome scientists have suggested that the initial extinctions could have been caused by a gamma-ray burst originating from a hypernova within 6,000 light-years of Earth (in a nearby arm of the Milky Way galaxy). A ten-second burst would have stripped the Earth's atmosphere of half of its ozone almost immediately, exposing surface-dwelling organisms, including those responsible for planetary photosynthesis, to high levels of extreme ultraviolet radiation.\nAlthough the hypothesis is consistent with patterns at the onset of extinction, there is no unambiguous evidence that such a nearby gamma-ray burst ever happened.\n\nThe late Ordovician glaciation event was preceded by a fall in atmospheric carbon dioxide (from 7,000 ppm to 4,400 ppm). The dip is correlated with a burst of volcanic activity that deposited new silicate rocks, which draw CO out of the air as they erode. A major role of CO is implied by a 2009 paper. Atmospheric and oceanic CO levels may have fluctuated with the growth and decay of Gondwanan glaciation. Through the Late Ordovician, outgassing from major volcanism was balanced by heavy weathering of the uplifting Appalachian Mountains, which sequestered CO. In the Hirnantian Stage the volcanism ceased, and the continued weathering caused a significant and rapid draw down of CO. This coincides with the rapid and short ice age.\n\nToxic metals on the ocean floor may have dissolved into the water when the oceans' oxygen was depleted. An increase in available nutrients in the oceans may have been a factor. The toxic metals may have killed life forms in lower trophic levels of the food chain, causing a decline in population, and subsequently resulting in starvation for the dependent higher feeding life forms in the chain.\n\nThe end of the second event occurred when melting glaciers caused the sea level to rise and stabilize once more. The rebound of life's diversity with the sustained re-flooding of continental shelves at the onset of the Silurian saw increased biodiversity within the surviving orders.\n\nFollowing such a major loss of diversity, Silurian communities were initially less complex and broader niched. Highly endemic faunas, which characterized the Late Ordovician, were replaced by faunas that were amongst the most cosmopolitan in the Phanerozoic, biogeographic patterns that persisted throughout most of the Silurian.\n\nThese end Ordovician–Silurian events had nothing like the long-term impact of the Permian–Triassic and Cretaceous-Paleogene extinction events. Nevertheless, a large number of taxa disappeared from the Earth over a short time interval, eliminating and changing diversity.\n\n\n\n"}
{"id": "2870581", "url": "https://en.wikipedia.org/wiki?curid=2870581", "title": "Rossiter–McLaughlin effect", "text": "Rossiter–McLaughlin effect\n\nThe Rossiter–McLaughlin effect is a spectroscopic phenomenon observed when either an eclipsing binary's secondary star or an extrasolar planet is seen to transit across the face of the primary or parent star. As the main star rotates on its axis, one quadrant of its photosphere will be seen to be coming towards the viewer, and the other visible quadrant to be moving away. These motions produce blueshifts and redshifts, respectively, in the star's spectrum, usually observed as a broadening of the spectral lines. When the secondary star or planet transits the primary, it blocks part of the latter's disc, preventing some of the shifted light from reaching the observer. This causes the observed mean redshift of the primary star as a whole to vary from its normal value. As the transiting object moves across to the other side of the star's disc, the redshift anomaly will switch from being negative to being positive, or vice versa. This effect has been used to show that as many as 25% of hot Jupiters are orbiting in a retrograde direction with respect to their parent stars, strongly suggesting that dynamical interactions rather than planetary migration produce these objects.\n\nJ. R. Holt in 1893 proposed a method to measure the stellar rotation of stars using radial velocity measurements, he predicted that when one star of an eclipsing binary eclipsed the other it would first cover the advancing blueshifted half and then the receding redshifted half. This motion would create a redshift of the eclipsed star’s spectrum followed by a blueshift, thus appearing as a change in the radial velocity in addition to that caused by the orbital motion of the eclipsed star.\n\nThe effect is named after Richard Alfred Rossiter and Dean Benjamin McLaughlin.\n\n"}
{"id": "44595951", "url": "https://en.wikipedia.org/wiki?curid=44595951", "title": "Sand forest", "text": "Sand forest\n\nA sand forest is a type of rare subtropical forest region, distinctive due to its unique combination of plant (often rare plant) and animal species, and their restriction to ancient coastal dunes. The sand forests are endemic to Maputaland in South Africa, and found in parts of the Amazon (Brazil, Peru, and Colombia).\n\nThey are typically composed of clay soils and nutrient poor white sands. These forests are commonly referred to as \"sand forests\" in the South African regions. However, in Mozambique, they are known as the \"Lucauati forests\". In the Amazon, they are colloquially known as \"campinaranas\" and \"campinas\". Few studies have been carried out on the sand forests. Those studies that have been conducted, the majority of the research has been on the plant diversity within these forests.\n\nThere have been no comprehensive studies undertaken in the white sand forests. A full understanding of the process in which they have emerged is lacking. However, it is thought that the sand forests are the fragments of coastal dunes which were separated from the ocean over millions of years as the shoreline and water level slowly shifted. Researchers have speculated that these forest systems have been related to the Late Pleistocene and Holocene dry periods.\n\nPodsolization is a process in which the upper layer of soil becomes acidic due to the leaching of nutrients. This process combined with the lack of nutrients in the upper horizon results in only white sand being left behind. Because sandy soils can result from a number of different processes (nutrient leaching, tectonic activity, river dynamics, etc.), the sand forests are unique and, oftentimes, different from sand forests found elsewhere.\n\nSand forests are well known for their unique biodiversity and high levels of endemism. To date there have been more than 2,500 species of vascular plants and of those, 230 species are endemic within the Maputaland region. Distinct sclerophylly, or vegetation with thickened, hardened foliage that slows moisture loss, is a characteristic of a vast number of the plants found in this region. Due to nutrient and water restrictions, the vegetation that grows in the sand forests is very specialized. Many of the trees and shrubs have evolved anti-herbivorous defenses. Herbivores seemed to favor vegetation of plants in clay soils over the white sandy soils. In no presence of herbivores however, the clay soil vegetation survived just as well as white sand specialists, but grew much taller and produced more leaf area.\n\nSand forests have a thick humus layer due to the extremely low decomposition rates. Some sandy forests have even been recorded to have more than a meter of leaf litter on top of the white sand. Multiple reasons exist as to why there is such a large accumulation of organic matter including high acidity of the soil, high content of toxic compounds in the litter, and low nutrient quality of the litter.\n\nIt is very common to find epiphytes, such as the wiry orchid (\"Microcoelia exilis\"), and other lichens growing on the trees. They derive their moisture and nutrients from the air, rain, and debris that has accumulated over time.\n\nIn contrast to tropical rainforests, campinas are \"reduced in biomass and have relatively high light penetration.\" Due to nutrient deficiencies in campina soils, \"shrubs and small trees typically have a dwarfed and rachitic aspect with reduced quantities of foliage\" with many of the species being perennials and evergreens\n\n\nSand forests have vegetation gradients that closely coincide with saturation gradients. These saturation gradients are affected by rainfall and topography and, therefore, the vegetation is also affected by these same factors. Vegetation lower to the ground tends to congregate in areas that have been saturated for the longest period of time.\n\nBlackwater rivers (e.g. the Rio Negro) commonly begin in these sand forests due to their accumulation of humic matter that is easily washed downstream, particularly after heavy rainfall.\n\nThe majority of bird species found in sand forests is known to prefer sand forest habitat and are found rarely, if at all, in other types of habitats. There have been no studies, however, evaluating the habitat specialization of sand forest birds to determine whether these habitats have unique bird fauna.\n\nThe majority of these birds are small ground dwellers. They often forage in the underbrush to find food because of the thick organic layers found in the sand forests. Their sharp beaks allow them to break through the thick coating of fruits and obtain the seeds inside.\n\n\nThe exact role of fires in sand forests is not known and most studies disagree on the matter. However, evidence has shown that fires can help increase species diversity, particularly those that are specialized for such areas. For example, fire tolerant species may be more likely to colonize here.\n\nHistorically speaking, some sand forests may have been the result of anthropogenic burning done by indigenous people. They would burn swaths of sand forest to obtain better hunting and farming grounds.\n\nFires of any kind can have both positive and negative effects on the ecosystem. In some instances, a fire can cause one or just a few species to predominate when the forest begins to regenerate. However, fire can also promote more diversity within the regenerated forest.\n\nBecause sand forests are isolated in small patches, they have been mostly protected from the destruction of the Amazon rainforest. However, they are still at risk of disturbance. Sand forests are extremely sensitive to destruction due to the harsh growth and survival conditions. Since the conditions are so tough to thrive in once an ecosystem declines, it may take hundreds of years for it to develop again.\n\nSome sand forests overlap with protected areas and indigenous lands, but most do not. Although they are not immediately threatened, sand forests are still in danger from ever-increasing deforestation, hydroelectric projects, and mineral extractions.\n\nPeru has actively incorporated sand forests in three protected areas. Authors who have done research in campinas have also made recommendations for conservation in their work.\n\n"}
{"id": "1494524", "url": "https://en.wikipedia.org/wiki?curid=1494524", "title": "Scrutiny", "text": "Scrutiny\n\nScrutiny (French: \"scrutin\"; Late Latin: \"scrutinium\"; from \"scrutari\", meaning \"those who search through piles of rubbish in the hope of finding something of\" and originally from the Latin \"scruta,\" meaning \"broken things, rags, or rubbish.\"). In Roman times, the \"scrutari\" of cities and towns were those who laboriously searched for valuables amidst the waste and cast-offs of others. The modern English \"scrutiny\" is derived from this root, indicating a careful examination or inquiry (often implying the search for a hidden mistake, misstatement, or incongruity).\n\nThe word is specifically applied in the early Roman Catholic Church to the examination of the catechumens or those under instruction in the faith. They were taught the creed and the Lord's Prayer, examined therein, and exorcized prior to baptism. The days of scrutiny varied at different periods from three to seven. From about the beginning of the 12th century, when it became usual to baptize infants soon after their birth instead of at stated times (Easter and Pentecost), the ceremony of scrutiny was incorporated with that of the actual baptism.\n\nCurrently, there are three moments for the scrutinies to occur: the 3rd, 4th, and 5th Sundays of Lent. These are done in public in front of the entire congregation, and the candidates are dismissed before the Prayer of the Faithful. Only under grave circumstances can the scrutinies be dispensed, and only then by the local ordinary (who can dispense only two at most). The scrutinies are fully intended for the catechumens (i.e., those who are to receive Baptism, Communion, and Confirmation).\n\nScrutiny is also a term applied to a method of electing a pope in the Catholic Church, in contradistinction to two other methods, acclamation and accession. In the law of elections, scrutiny is the careful examination of votes cast after the unsuccessful candidate has lodged a petition claiming the seat, and alleging that he has the majority of legal votes. Each vote is dealt with separately, notice being given beforehand by one party to the other of the votes objected to and the grounds of objection.\n"}
{"id": "342657", "url": "https://en.wikipedia.org/wiki?curid=342657", "title": "Seven generation sustainability", "text": "Seven generation sustainability\n\nSeven generation stewardship is a concept that urges the current generation of humans to live and work for the benefit of the seventh generation into the future.\nIt originated with the Iroquois – Great Law of the Iroquois – which holds appropriate to think seven generations ahead (about 140 years into the future) and decide whether the decisions they make today would benefit their children seven generations into the future. It is frequently associated with the modern, popular concept of environmental stewardship or 'sustainability' but it is much broader in context (see the quotation below relative to \"in ALL of your deliberations ...\".\n\n\"In every deliberation, we must consider the impact on the seventh generation...\neven if it requires having skin as thick as the bark of a pine.\" This is an often repeated saying, and most who use it claim that it comes from \"The Constitution of the Iroquois Nations: The Great Binding Law.\"\n\nIn fact, the original language is as follows: \"In all of your deliberations in the Confederate Council, in your efforts at law making, in all your official acts, self-interest shall be cast into oblivion. Cast not over your shoulder behind you the warnings of the nephews and nieces should they chide you for any error or wrong you may do, but return to the way of the Great Law which is just and right. Look and listen for the welfare of the whole people and have always in view not only the past and present but also the coming generations, even those whose faces are yet beneath the surface of the ground – the unborn of the future Nation.\"\n\nOren Lyons, Chief of the Onondaga Nation, writes: \"We are looking ahead, as is one of the first mandates given us as chiefs, to make sure and to make every decision that we make relate to the welfare and well-being of the seventh generation to come. ... What about the seventh generation? Where are you taking them? What will they have?\"\n"}
{"id": "162404", "url": "https://en.wikipedia.org/wiki?curid=162404", "title": "Stratovolcano", "text": "Stratovolcano\n\nA stratovolcano, also known as a composite volcano, is a conical volcano built up by many layers (strata) of hardened lava, tephra, pumice and ash. Unlike shield volcanoes, stratovolcanoes are characterized by a steep profile with a summit crater and periodic intervals of explosive eruptions and effusive eruptions, although some have collapsed summit craters called calderas. The lava flowing from stratovolcanoes typically cools and hardens before spreading far due to high viscosity. The magma forming this lava is often felsic, having high-to-intermediate levels of silica (as in rhyolite, dacite, or andesite), with lesser amounts of less-viscous mafic magma. Extensive felsic lava flows are uncommon, but have travelled as far as .\n\nStratovolcanoes are sometimes called \"composite volcanoes\" because of their composite stratified structure built up from sequential outpourings of erupted materials. They are among the most common types of volcanoes, in contrast to the less common shield volcanoes. Two famous examples of stratovolcanoes are Krakatoa, best known for its catastrophic eruption in 1883 and Vesuvius, famous for its destruction of Pompeii and Herculaneum in 79 AD. Both eruptions claimed thousands of lives. In modern times, Mount Saint Helens and Mount Pinatubo have erupted catastrophically, with lesser losses of lives.\n\nExistence of stratovolcanoes has not been proved on other terrestrial bodies of the solar system with one exception. Their existence was suggested for some isolated massifs on Mars, e.g., Zephyria Tholus.\n\nStratovolcanoes are common at subduction zones, forming chains and clusters along plate tectonic boundaries where oceanic crust is drawn under continental crust (continental arc volcanism, e.g. Cascade Range, central Andes, Campania) or another oceanic plate (island arc volcanism, e.g. Japan, Philippines, Aleutian Islands). The magma forming stratovolcanoes rises when water trapped both in hydrated minerals and in the porous basalt rock of the upper oceanic crust is released into mantle rock of the asthenosphere above the sinking oceanic slab. The release of water from hydrated minerals is termed \"dewatering\", and occurs at specific pressures and temperatures for each mineral, as the plate descends to greater depths. The water freed from the rock lowers the melting point of the overlying mantle rock, which then undergoes partial melting and rises due to its lighter density relative to the surrounding mantle rock, and pools temporarily at the base of the lithosphere. The magma then rises through the crust, incorporating silica-rich crustal rock, leading to a final intermediate composition. When the magma nears the top surface, it pools in a magma chamber within the crust below the stratovolcano.\n\nThere, the relatively low pressure allows water and other volatiles (mainly CO, SO, Cl, and HO) dissolved in the magma to escape from solution, as occurs when a bottle of carbonated water is opened, releasing CO. Once a critical volume of magma and gas accumulates, the plug (solidified blockage) of the volcanic vent is broken, leading to a sudden explosive eruption.\n\nIn recorded history, explosive eruptions at subduction zone (convergent-boundary) volcanoes have posed the greatest hazard to civilizations. Subduction-zone stratovolcanoes, such as Mount St. Helens, Mount Etna and Mount Pinatubo, typically erupt with explosive force: the magma is too stiff to allow easy escape of volcanic gases. As a consequence, the tremendous internal pressures of the trapped volcanic gases remain and intermingle in the pasty magma. Following the breaching of the vent and the opening of the crater, the magma degasses explosively. The magma and gases blast out with high speed and full force.\n\nSince 1600 CE, nearly 300,000 people have been killed by volcanic eruptions. Most deaths were caused by pyroclastic flows and lahars, deadly hazards that often accompany explosive eruptions of subduction-zone stratovolcanoes. Pyroclastic flows are swift, avalanche-like, ground-sweeping, incandescent mixtures of hot volcanic debris, fine ash, fragmented lava and superheated gases that can travel at speeds in excess of . Around 30,000 people were killed by pyroclastic flows during the 1902 eruption of Mont Pelée on the island of Martinique in the Caribbean. In March to April 1982, three explosive eruptions of El Chichón Volcano in the State of Chiapas in southeastern Mexico, caused the worst volcanic disaster in that country's history. Villages within of the volcano were destroyed by pyroclastic flows, killing more than 2,000 people.\n\nTwo Decade Volcanoes that erupted in 1991 provide examples of stratovolcano hazards. On June 15, Mount Pinatubo spewed an ash cloud into the air and produced huge pyroclastic surges and lahar floods that devastated a large area around the volcano. Pinatubo, located in Central Luzon just west-northwest from Manila, had been dormant for 6 centuries before the 1991 eruption, which ranks as one of the largest eruptions in the 20th century. Also in 1991, Japan's Unzen Volcano, located on the island of Kyushu about east of Nagasaki, awakened from its 200-year slumber to produce a new lava dome at its summit. Beginning in June, repeated collapse of this erupting dome generated ash flows that swept down the mountain's slopes at speeds as high as . Unzen is one of more than 75 active volcanoes in Japan; an eruption in 1792 killed more than 15,000 people—the worst volcanic disaster in the nation's history.\n\nThe AD 79 Vesuvian eruption of Mount Vesuvius, a stratovolcano situated within Campania, completely smothered the nearby ancient cities of Pompeii and Herculaneum with thick deposits of pyroclastic surges and lava flows. Although death toll is estimated between 13,000 to 26,000 remains, the exact number still remains unknown. Vesuvius is recognized as one of the most dangerous volcanoes, due to its capacity for powerful explosive eruptions combined with the high population density of the surrounding Metropolitan Naples area (totaling about 3.6 million inhabitants).\n\nApart from possibly affecting the climate, volcanic clouds from explosive eruptions also pose a serious hazard to aviation safety. For example, during the 1982 eruption of Galunggung in Java, British Airways Flight 9 flew into the ash cloud, suffering temporary engine failure and structural damage. During the past two decades, more than 60 airplanes, mostly commercial airliners, have been damaged by in-flight encounters with volcanic ash. Some of these encounters have resulted in the power loss of all engines, necessitating emergency landings. Luckily, to date no crashes have happened because of jet aircraft flying into volcanic ash. Ashfalls are a threat to health when inhaled and is also a threat to property with enough accumulation. An accumulation of is sufficient to cause most buildings to collapse. Dense clouds of hot volcanic ash, caused by the collapse of an eruptive column or by being laterally expelled from the partial collapse of a volcanic edifice or lava dome during explosive eruptions, can generated devastating pyroclastic flows or surges, which can sweep off everything in their paths.\n\nLava flows from stratovolcanoes are generally not a significant threat to humans and animals because the highly viscous lava moves slowly enough for everyone to flee out of the path of flow. The lava flows are more of a threat to property. However, not all stratovolcanoes erupt viscous and sticky lava. Mount Nyiragongo is very dangerous because its magma has an unusually low silica content, making it quite fluid. Fluid lavas are typically associated with the formation of broad shield volcanoes such as those of Hawaii, but Nyiragongo has very steep slopes down which lava can flow at up to . Lava flows could melt down ice and glaciers that accumulated on the volcano's crater and upper slopes, generating massive lahar flows. Rarely, generally fluid lava could also generate a massive lava fountain, while lava of thicker viscosity can solidify within the vent, creating a block which can result in highly explosive eruptions.\n\nVolcanic bombs are extrusive igneous rocks ranging from the size of books to small cars, that are explosively ejected from stratovolcanoes during their climactic eruptive phases. These \"bombs\" can travel over away from the volcano, and present a risk to buildings and living beings while shooting at very high speeds (hundreds of kilometers/miles per hour) through the air. Most bombs do not themselves explode on impact, but rather carry enough force so as to have destructive effects as if they exploded.\n\nLahars (from a Javanese term for volcanic mudflows) are mixtures of volcanic debris and water. Lahars usually come from two sources: rainfall or the melting of snow and ice by hot volcanic elements, such as lava. Depending on the proportion and temperature of water to volcanic material, lahars can range from thick, gooey flows that have the consistency of wet concrete to fast-flowing, soupy floods. As lahars flow down the steep sides of stratovolcanoes, they have the strength and speed to flatten or drown everything in their paths. Hot ashes, lava flows and pyroclastic surges ejected during 1985 eruption of Nevado del Ruiz in Colombia melted snow and ice atop the high Andean volcano. The ensuing lahar flooded the city of Armero and nearby settlements, killing 25,000 people.\n\nAs per the above examples, while the Unzen eruptions have caused deaths and considerable local damage in the historic past, the impact of the June 1991 eruption of Mount Pinatubo was global. Slightly cooler-than-usual temperatures were recorded worldwide, with brilliant sunsets and intense sunrises attributed to the particulates; this eruption lofted particles high into the stratosphere. The aerosols that formed from the sulfur dioxide (SO), carbon dioxide (CO), and other gases dispersed around the world. The SO mass in this cloud—about 22 million tons—combined with water (both of volcanic and atmospheric origin) formed droplets of sulfuric acid, blocking a portion of the sunlight from reaching the troposphere and ground. The cooling in some regions is thought to have been as much as 0.5 °C (0.9 °F). An eruption the size of Mount Pinatubo tends to affect the weather for a few years; the material injected into the stratosphere gradually drops into the troposphere, where it is washed away by rain and cloud precipitation.\n\nA similar, but extraordinarily more powerful phenomenon occurred in the cataclysmic April 1815 eruption of Mount Tambora on Sumbawa Island in Indonesia. The Mount Tambora eruption is recognized as the most powerful eruption in recorded history. Its eruption cloud lowered global temperatures by as much as 3.5 °C (6.3 °F). In the year following the eruption, most of the Northern Hemisphere experienced sharply cooler temperatures during the summer. In parts of Europe, Asia, Africa and North America, 1816 was known as the \"Year Without a Summer\", which caused considerable agricultural crisis and a brief but bitter famine, which generated a series of distresses across much of the affected continents.\n\n"}
{"id": "762691", "url": "https://en.wikipedia.org/wiki?curid=762691", "title": "Supercritical fluid", "text": "Supercritical fluid\n\nA supercritical fluid (SCF) is any substance at a temperature and pressure above its critical point, where distinct liquid and gas phases do not exist. It can effuse through solids like a gas, and dissolve materials like a liquid. In addition, close to the critical point, small changes in pressure or temperature result in large changes in density, allowing many properties of a supercritical fluid to be \"fine-tuned\". Supercritical fluids are suitable as a substitute for organic solvents in a range of industrial and laboratory processes. Carbon dioxide and water are the most commonly used supercritical fluids, being used for decaffeination and power generation, respectively.\n\nIn general terms, supercritical fluids have properties between those of a gas and a liquid. In Table 1, the critical properties are shown for some substances that are commonly used as supercritical fluids.\n\nTable 2 shows density, diffusivity and viscosity for typical liquids, gases and supercritical fluids.\n\nIn addition, there is no surface tension in a supercritical fluid, as there is no liquid/gas phase boundary. By changing the pressure and temperature of the fluid, the properties can be \"tuned\" to be more liquid-like or more gas-like. One of the most important properties is the solubility of material in the fluid. Solubility in a supercritical fluid tends to increase with density of the fluid (at constant temperature). Since density increases with pressure, solubility tends to increase with pressure. The relationship with temperature is a little more complicated. At constant density, solubility will increase with temperature. However, close to the critical point, the density can drop sharply with a slight increase in temperature. Therefore, close to the critical temperature, solubility often drops with increasing temperature, then rises again.\n\nAll supercritical fluids are completely miscible with each other so for a mixture a single phase can be guaranteed if the critical point of the mixture is exceeded. The critical point of a binary mixture can be estimated as the arithmetic mean of the critical temperatures and pressures of the two components,\nFor greater accuracy, the critical point can be calculated using equations of state, such as the Peng Robinson, or group contribution methods. Other properties, such as density, can also be calculated using equations of state.\n\nFigures 1 and 2 show two-dimensional projections of a phase diagram. In the pressure-temperature phase diagram (Fig. 1) the boiling separates the gas and liquid region and ends in the critical point, where the liquid and gas phases disappear to become a single supercritical phase. \n\nThe appearance of a single phase can also be observed in the density-pressure phase diagram for carbon dioxide (Fig. 2). At well below the critical temperature, e.g., 280 K, as the pressure increases, the gas compresses and eventually (at just over 40 bar) condenses into a much denser liquid, resulting in the discontinuity in the line (vertical dotted line). The system consists of 2 phases in equilibrium, a dense liquid and a low density gas. As the critical temperature is approached (300 K), the density of the gas at equilibrium becomes higher, and that of the liquid lower. At the critical point, (304.1 K and 7.38 MPa (73.8 bar)), there is no difference in density, and the 2 phases become one fluid phase. Thus, above the critical temperature a gas cannot be liquefied by pressure. At slightly above the critical temperature (310 K), in the vicinity of the critical pressure, the line is almost vertical. A small increase in pressure causes a large increase in the density of the supercritical phase. Many other physical properties also show large gradients with pressure near the critical point, e.g. viscosity, the relative permittivity and the solvent strength, which are all closely related to the density. At higher temperatures, the fluid starts to behave like a gas, as can be seen in Figure 2. For carbon dioxide at 400 K, the density increases almost linearly with pressure.\n\nMany pressurized gases are actually supercritical fluids. For example, nitrogen has a critical point of 126.2 K (−147 °C) and 3.4 MPa (34 bar). Therefore, nitrogen (or compressed air) in a gas cylinder above this pressure is actually a supercritical fluid. These are more often known as permanent gases. At room temperature, they are well above their critical temperature, and therefore behave as a gas, similar to CO at 400 K above. However, they cannot be liquified by pressure unless cooled below their critical temperature.\n\nIn recent years, a significant effort has been devoted to investigation of various properties of supercritical fluids. This has been an exciting field with a long history since 1822 when Baron Charles Cagniard de la Tour discovered supercritical fluids while conducting experiments involving the discontinuities of the sound in a sealed cannon barrel filled with various fluids at high temperature. More recently, supercritical fluids have found application in a variety of fields, ranging from the extraction of floral fragrance from flowers to applications in food science such as creating decaffeinated coffee, functional food ingredients, pharmaceuticals, cosmetics, polymers, powders, bio- and functional materials, nano-systems, natural products, biotechnology, fossil and bio-fuels, microelectronics, energy and environment. Much of the excitement and interest of the past decade is due to the enormous progress made in increasing the power of relevant experimental tools. The development of new experimental methods and improvement of existing ones continues to play an important role in this field, with recent research focusing on dynamic properties of fluids.\n\nDima Bolmatov, V. V. Brazhkin and K. Trachenko discovered that specific heat shows a crossover between two different dynamic regimes of the low-temperature rigid-liquid and high temperature non-rigid gas-like fluid. Rigid liquids are rigid like a solid on short time scales, but flow like a liquid on long time scales; while a supercritical gas-like fluid has the dynamic motions of a gas but is able to dissolve materials, like a liquid. The crossover challenges the currently held belief that no difference can be made between a gas and a liquid above the critical point and that the supercritical state is homogeneous in terms of physical properties. Bolmatov and colleagues formulated a theory of system thermodynamics and heat capacity above the crossover. In that theory, energy and heat capacity are governed by the minimal length of the longitudinal mode in the system only, and do not depend on system-specific structure and interactions. Bolmatov and colleagues predicted the relationship between supercritical exponents of heat capacity and viscosity and derived a power law for the supercritical state.\n\nThe Fisher-Widom line allows to distinguish liquid-like and gas-like states within the supercritical fluid.\n\nHydrothermal circulation occurs within the Earth's crust wherever fluid becomes heated and begins to convect. These fluids are thought to reach supercritical conditions under a number of different settings, such as in the formation of porphyry copper deposits or high temperature circulation of seawater in the sea floor. At mid-ocean ridges, this circulation is most evident by the appearance of hydrothermal vents known as \"black smokers\". These are large (metres high) chimneys of sulfide and sulfate minerals which vent fluids up to 400 °C. The fluids appear like great black billowing clouds of smoke due to the precipitation of dissolved metals in the fluid. It is likely that at depth many of these vent sites reach supercritical conditions, but most cool sufficiently by the time they reach the sea floor to be subcritical. One particular vent site, Turtle Pits, has displayed a brief period of supercriticality at the vent site. A further site, Beebe, in the Cayman Trough, is thought to display sustained supercriticality at the vent orifice.\n\nThe atmosphere of Venus is 96.5% carbon dioxide and 3.5% nitrogen. The surface pressure is 9.3 MPa (93 bar) and the surface temperature is 735 K, above the critical points of both major constituents and making the surface atmosphere a supercritical fluid.\n\nThe interior atmospheres of the solar system's gas giant planets are composed mainly of hydrogen and helium at temperatures well above their critical points. The gaseous outer atmospheres of Jupiter and Saturn transition smoothly into the dense liquid interior, while the nature of the transition zones of Neptune and Uranus is unknown. Theoretical models of extrasolar planets 55 Cancri e and Gliese 876 d have posited an ocean of pressurized, supercritical fluid water with a sheet of solid high pressure water ice at the bottom.\n\nThe advantages of supercritical fluid extraction (compared with liquid extraction) are that it is relatively rapid because of the low viscosities and high diffusivities associated with supercritical fluids. The extraction can be selective to some extent by controlling the density of the medium, and the extracted material is easily recovered by simply depressurizing, allowing the supercritical fluid to return to gas phase and evaporate leaving little or no solvent residues. Carbon dioxide is the most common supercritical solvent. It is used on a large scale for the decaffeination of green coffee beans, the extraction of hops for beer production, and the production of essential oils and pharmaceutical products from plants. A few laboratory test methods include the use of supercritical fluid extraction as an extraction method instead of using traditional solvents.\n\nSupercritical water can be used to decompose biomass via supercritical water gasification of biomass. This type of biomass gasification can be used to produce hydrocarbon fuels for use in an efficient combustion device or to produce hydrogen for use in a fuel cell. In the latter case, hydrogen yield can be much higher than the hydrogen content of the biomass due to steam reforming where water is a hydrogen-providing participant in the overall reaction.\n\nSupercritical carbon dioxide (SCD) can be used instead of PERC (perchloroethylene) or other undesirable solvents for dry-cleaning. Supercritical carbon dioxide sometimes intercalates into buttons, and, when the SCD is depressurized, the buttons pop, or break apart. Detergents that are soluble in carbon dioxide improve the solvating power of the solvent.\n\nSupercritical fluid chromatography (SFC) can be used on an analytical scale, where it combines many of the advantages of high performance liquid chromatography (HPLC) and gas chromatography (GC). It can be used with non-volatile and thermally labile analytes (unlike GC) and can be used with the universal flame ionization detector (unlike HPLC), as well as producing narrower peaks due to rapid diffusion. In practice, the advantages offered by SFC have not been sufficient to displace the widely used HPLC and GC, except in a few cases such as chiral separations and analysis of high-molecular-weight hydrocarbons. For manufacturing, efficient preparative simulated moving bed units are available. The purity of the final products is very high, but the cost makes it suitable only for very high-value materials such as pharmaceuticals.\n\nChanging the conditions of the reaction solvent can allow separation of phases for product removal, or single phase for reaction. Rapid diffusion accelerates diffusion controlled reactions. Temperature and pressure can tune the reaction down preferred pathways, e.g., to improve yield of a particular chiral isomer. There are also significant environmental benefits over conventional organic solvents.\n\nAn electrochemical carboxylation of a para-isobutylbenzyl chloride to Ibuprofen is promoted under supercritical carbon dioxide.\n\nImpregnation is, in essence, the converse of extraction. A substance is dissolved in the supercritical fluid, the solution flowed past a solid substrate, and is deposited on or dissolves in the substrate. Dyeing, which is readily carried out on polymer fibres such as polyester using disperse (non-ionic) dyes, is a special case of this. Carbon dioxide also dissolves in many polymers, considerably swelling and plasticising them and further accelerating the diffusion process.\n\nThe formation of small particles of a substance with a narrow size distribution is an important process in the pharmaceutical and other industries. Supercritical fluids provide a number of ways of achieving this by rapidly exceeding the saturation point of a solute by dilution, depressurization or a combination of these. These processes occur faster in supercritical fluids than in liquids, promoting nucleation or spinodal decomposition over crystal growth and yielding very small and regularly sized particles. Recent supercritical fluids have shown the capability to reduce particles up to a range of 5-2000 nm.\n\nSupercritical fluids act as a new media for the generation of novel crystalline forms of APIs (Active Pharmaceutical Ingredients) named as pharmaceutical cocrystals. Supercritical fluid technology offers a new platform that allows a single-step generation of particles that are difficult or even impossible to obtain by traditional techniques. The generation of pure and dried new cocrystals (crystalline molecular complexes comprising the API and one or more conformers in the crystal lattice) can be achieved due to unique properties of SCFs by using different supercritical fluid properties: supercritical CO solvent power, anti-solvent effect and its atomization enhancement.\n\nSupercritical drying is a method of removing solvent without surface tension effects. As a liquid dries, the surface tension drags on small structures within a solid, causing distortion and shrinkage. Under supercritical conditions there is no surface tension, and the supercritical fluid can be removed without distortion. Supercritical drying is used for manufacture of aerogels and drying of delicate materials such as archeological samples and biological samples for electron microscopy. Carbon dioxide is used as a supercritical solvent in some dry cleaning processes.\n\nSupercritical water oxidation uses supercritical water as a medium in which to oxidize hazardous waste, eliminating production of toxic combustion products that burning can produce.\n\nThe waste product to be oxidised is dissolved in the supercritical water along with molecular oxygen (or an oxidising agent that gives up oxygen upon decomposition, e.g. hydrogen peroxide) at which point the oxidation reaction occurs.\n\nSupercritical hydrolysis is a method of converting all biomass polysaccharides as well the associated lignin into low molecular compounds by contacting with water alone under supercritical conditions. The supercritical water, acts as a solvent, a supplier of bond-breaking thermal energy, a heat transfer agent and as a source of hydrogen atoms. All polysaccharides are converted into simple sugars in near-quantitative yield in a second or less. The aliphatic inter-ring linkages of lignin are also readily cleaved into free radicals that are stabilized by hydrogen originating from the water. The aromatic rings of the lignin are unaffected under short reaction times so that the lignin-derived products are low molecular weight mixed phenols. To take advantage of the very short reaction times needed for cleavage a continuous reaction system must be devised. The amount of water heated to a supercritical state is thereby minimized.\n\nSupercritical water gasification is a process of exploiting the beneficial effect of supercritical water to convert aqueous biomass streams into clean water and gases like H, CH, CO, CO etc.\n\nThe efficiency of a heat engine is ultimately dependent on the temperature difference between heat source and sink (Carnot cycle). To improve efficiency of power stations the operating temperature must be raised. Using water as the working fluid, this takes it into supercritical conditions. Efficiencies can be raised from about 39% for subcritical operation to about 45% using current technology. Supercritical water reactors (SCWRs) are promising advanced nuclear systems that offer similar thermal efficiency gains. Carbon dioxide can also be used in supercritical cycle nuclear power plants, with similar efficiency gains. Many coal-fired supercritical steam generators are operational all over the world, and have enhanced the efficiency of traditional steam-power plants.\n\nConversion of vegetable oil to biodiesel is via a transesterification reaction, where the triglyceride is converted to the methyl ester plus glycerol. This is usually done using methanol and caustic or acid catalysts, but can be achieved using supercritical methanol without a catalyst. The method of using supercritical methanol for biodiesel production was first studied by Saka and his coworkers. This has the advantage of allowing a greater range and water content of feedstocks (in particular, used cooking oil), the product does not need to be washed to remove catalyst, and is easier to design as a continuous process.\n\nSupercritical carbon dioxide is used to enhance oil recovery in mature oil fields. At the same time, there is the possibility of using \"clean coal technology\" to combine enhanced recovery methods with carbon sequestration. The CO is separated from other flue gases, compressed to the supercritical state, and injected into geological storage, possibly into existing oil fields to improve yields.\n\nAt present, only schemes isolating fossil CO from natural gas actually use carbon storage, (e.g., Sleipner gas field), but there are many plans for future CCS schemes involving pre- or post- combustion CO. There is also the possibility to reduce the amount of CO in the atmosphere by using biomass to generate power and sequestering the CO produced.\n\nThe use of supercritical carbon dioxide, instead of water, has been examined as a geothermal working fluid.\n\nSupercritical carbon dioxide is also an important emerging refrigerant, being used in new, low-carbon solutions for domestic heat pumps. These systems are undergoing continuous development with supercritical carbon dioxide heat pumps already being successfully marketed in Asia. The EcoCute systems from Japan, developed by consortium of companies including Mitsubishi, develop high-temperature domestic water with small inputs of electric power by moving heat into the system from their surroundings. Their success makes a future use in other world regions possible.\n\nSupercritical fluids can be used to deposit functional nanostructured films and nanometer-size particles of metals onto surfaces. The high diffusivities and concentrations of precursor in the fluid as compared to the vacuum systems used in chemical vapour deposition allow deposition to occur in a surface reaction rate limited regime, providing stable and uniform interfacial growth. This is crucial in developing more powerful electronic components, and metal particles deposited in this way are also powerful catalysts for chemical synthesis and electrochemical reactions. Additionally, due to the high rates of precursor transport in solution, it is possible to coat high surface area particles which under chemical vapour deposition would exhibit depletion near the outlet of the system and also be likely to result in unstable interfacial growth features such as dendrites. The result is very thin and uniform films deposited at rates much faster than atomic layer deposition, the best other tool for particle coating at this size scale.\n\nCO at high pressures has antimicrobial properties. While its effectiveness has been shown for various applications, the mechanisms of inactivation have not been fully understood although they have been investigated for more than 60 years.\n\nIn 1822, Baron Charles Cagniard de la Tour discovered the critical point of a substance in his famous cannon barrel experiments. Listening to discontinuities in the sound of a rolling flint ball in a sealed cannon filled with fluids at various temperatures, he observed the critical temperature. Above this temperature, the densities of the liquid and gas phases become equal and the distinction between them disappears, resulting in a single supercritical fluid phase.\n\n\n"}
{"id": "1925328", "url": "https://en.wikipedia.org/wiki?curid=1925328", "title": "Taxonomy of Pachypodium", "text": "Taxonomy of Pachypodium\n\nThe taxonomy of the \"Pachypodium genus\" is the study of the species and subspecies in the genus \"Pachypodium\". There are currently 25 recognized species in the genus, of which 17 are shrubs and eight are trees.\n\nSpecies included in the genus \"Pachypodium\" are as follows:\n\n\n"}
{"id": "3458864", "url": "https://en.wikipedia.org/wiki?curid=3458864", "title": "Terai-Duar savanna and grasslands", "text": "Terai-Duar savanna and grasslands\n\nThe Terai-Duar savanna and grasslands is a narrow lowland ecoregion at the base of the Himalayas, about wide, and a continuation of the Gangetic Plain. It is colloquially called \"Terai\" in the Ganges Basin east to Nepal, then \"Dooars\" in West Bengal, Bangladesh, Bhutan and Assam east to the Brahmaputra River. The world's tallest grasslands are found in this ecoregion, which are the most threatened and rare worldwide.\n\nThis tropical and subtropical grasslands, savannas, and shrublands biome stretches from western Bhutan to southern Nepal's Terai, westward to Banke, covering the Dang and Deukhuri Valleys along the Rapti River to India's Bhabar and Doon Valleys. Each end crosses the border into India's states of Uttar Pradesh and Bihar. The eastern and central areas are wetter than the western end.\n\nIn Nepal, the wetlands of Koshi Tappu Wildlife Reserve, Beeshazar Tal in the bufferzone of Chitwan National Park, Jagdishpur Reservoir and Ghodaghodi Tal are designated Ramsar sites. The Sukla Phanta Wildlife Reserve is Nepal's largest patch of continuous grassland.\n\nThe Terai-Duar savanna and grasslands are a mosaic of tall riverside grasslands, savannas and evergreen and deciduous forests, depending on soil quality and the amount of rain each area receives. The grasslands of the Terai in Nepal are among the tallest in the world, and are maintained by silt deposited by the yearly monsoon floods. Important grasses include baruwa and kans grass (\"Saccharum spontaneum\"), which quickly establishes itself after the retreat of the monsoon waters. In the hillier areas the dominant tree is sal (\"Shorea robusta\"), which can grow to a height of . The belt also contains riverside tropical deciduous forest comprising \"Mallotus philippensis\", jamun, cotton tree, \"Trewia nudiflora\", and \"Garuga pinnata\".\n\nThe ecoregion is habitat for a huge number of mammalian and bird species. Notable are the large numbers of the endangered greater one-horned rhinoceros and Bengal tigers as well as Asian elephants, sloth bears, Indian leopards.\n\nIn Nepal's Chitwan National Park, more than 400 rhinos were sighted in 2008, and 125 adult tigers were recorded during a survey conducted from December 2009 to March 2010, which covered an area of . Nepal's Bardia National Park and Sukla Phanta Wildlife Reserve, and India's Valmiki and Dudhwa National Parks are home to nearly 100 tigers. Chitwan along with the adjoining Parsa National Park is of major importance, especially for tigers and clouded leopard. Grazing animals of the grasslands include five species of deer, barasingha, sambar, chital, hog deer and muntjac along with four large grazing animals, Asian elephant, rhinoceros, gaur and nilgai. Endangered mammals found here include the wild water buffalo and the near-endemic hispid hare (\"Caprolagus hispidus\").\n\nThe grasslands are also home to a number of reptiles including the gharial, mugger crocodile and soft-shelled turtles.\n\nThe grasslands partly cover two BirdLife International Endemic Bird Areas, the Central Himalayas EBA in western Nepal and the western end of the Assam Plains EBA south of Bhutan. There are three near-endemic bird species including the vulnerable Manipur bush quail (\"Perdicula manipurensis\"). The 44 threatened and declining bird species of the grasslands include the Bengal florican (\"Houbaropsis bengalensis\"), lesser florican (\"Sypheotides indica\"), sarus crane (\"Grus antigone\") and rufous-rumped grassbird (\"Graminicola bengalensis\").\n\nAlthough the population density has been low it is now growing, especially in the southern Terai belt and much of the ecoregion has been converted to farmland since the forest was cut down for timber, although Sukla Phanta Wildlife Reserve, Chitwan, Bardia and Dudhwa National Parks, some of which were once royal hunting grounds, preserve significant sections of habitat, and are home to some of the greatest concentrations of rhinoceros and tiger remaining in South Asia. The areas of tall grasslands, which in particular have been much reduced for farmland, are of special conservation importance. The remaining forest meanwhile is mostly found in the drier bhabar belt, with some forest patches in Chitwan National Park.\n\n\n"}
{"id": "11158362", "url": "https://en.wikipedia.org/wiki?curid=11158362", "title": "Thaw depth", "text": "Thaw depth\n\nIn soil science, the thaw depth or thaw line is the instantaneous level down to which the soil has warmed to zero degrees celcius. The active layer thickness is the maximum thaw depth over a period of two years.\n\nThe layer of soil over the thaw depth is called the active layer, while the soil below is called permafrost. \n\nThe term frost front refers to the varying position of the thaw line during the periods of freezing/thawing.\n\nThe knowledge of the thaw depth is important for the two major reasons: its influence on the ecology and on construction (buildings, pipelines, roads, etc.). These influences are mediated by the effects of the dynamics biological, pedologic, geomorphologic, biogeochemical, and hydrologic processes in permafrost.\n\nIn ecology, roots of plants cannot penetrate beyond the active layer, which places restrictions on which plants can grow in permafrost.\n\nIn construction, the thaw depth is a major factor in ensuring the structural integrity of the objects in question.\n\nThe primary factor that determines the thaw depth is the maximal air temperature. The soil type is another important factor: coarser textures of the parent material have higher thermal conductivity, and, e.g., sandy soils have much deeper thaw line than clays. Yet another factors are the vegetation and the percentage of the soil organic matter, which influence the bulk density of the soil, and hence thermal conductivity.\n"}
{"id": "10240860", "url": "https://en.wikipedia.org/wiki?curid=10240860", "title": "The EBCC Atlas of European Breeding Birds", "text": "The EBCC Atlas of European Breeding Birds\n\nThe EBCC Atlas of European Breeding Birds - their distribution and abundance () is an ornithological atlas published for the European Bird Census Council by T & A D Poyser in 1997. Its editors were Ward J. M. Hagemeijer and Michael J. Blair. The atlas was the first to present grid-square distribution maps for all breeding birds at a Europe-wide level. The bulk of the book is in English, although it also contains introductions in thirteen other European languages. The atlas presents the results of the European Bird Census Council's European Ornithological Atlas project, the fieldwork for which was carried out between 1985 and 1988.\n\nThe book has cxli + 903 pages. Its Foreword (by Karel H Voous) and Preface (by Goetz Rheinwald and Jeremy Greenwood) are followed by an English introduction and shorter introductions in Czech, German, Spanish, French, Finnish, Greek, Hungarian, Italian, Dutch, Portuguese, Polish, Russian and Swedish; indexes of bird names, in the same languages, are also included, at the end of the book. The introductions are followed by sections detailing the history of the European Ornithological Atlas project and the Evolution and History of the European Bird Fauna, and a section listing national and major regional published bird atlases.\n\nThe bulk of the book (772 pages) consists of species accounts. All birds with established breeding populations within Europe are covered in a single-page or two-page account which includes a map of breeding distribution, histograms showing those countries with the largest breeding populations, and a species text. Seventeen further species for which some breeding behaviour has been observed within the region are covered more briefly. Some species distributions are shown on maps of the whole survey area, but for those with more restricted distributions, base maps showing only a relevant subdivision are shown.\n\nThe book concludes with sections on the Conservation Status of Europe's Birds, a list of species with threat statuses, a set of derived maps depicting overall species richness and richness of threatened species, and a 65-page references section.\n\nThe book's dustjacket is illustrated by bird artist David Nurney - the front cover shows a common kingfisher (with a map of its distribution), while a European bee-eater is depicted on the spine.\n\nThe European Ornithological Atlas project was initiated in 1971. Its fieldwork took the form of surveys during the breeding season, in each of the countries within the region, between 1985 and 1988.\n\nThe survey area covered by the project extends to include Madeira, The Azores, Iceland, Svalbard, Novaya Zemlya, Franz Josef Land and Transcaucasia, as well as European Russia east to the Urals. However, Turkey, Cyprus, Greenland and the Canary Islands are excluded.\n\nThe maps from the atlas are available online at the SOVON website here: The EBCC Atlas of European Breeding Birds\n\nIn 2007, a follow-up publication, \"A climatic atlas of European breeding birds\" was published by Lynx Edicions. This consisted of maps of projected species distributions based on the predictions of the Hadley centre HadCM3 climate change model.\n\n"}
{"id": "31823498", "url": "https://en.wikipedia.org/wiki?curid=31823498", "title": "Transpolar Drift Stream", "text": "Transpolar Drift Stream\n\nThe Transpolar Drift Stream is a major ocean current of the Arctic Ocean, transporting sea ice from the Laptev Sea and the East Siberian Sea towards Fram Strait. Drift experiments with ships like Fram or Tara showed that the drift takes between two and four years.\n\nIn 1937, Pyotr Shirshov at the Soviet drift ice station North Pole-1 described this drift. The stream conveys water in roughly two major routes to the northern Atlantic Ocean at a rate of about per day. Primarily wind-driven, it flows roughly from the northern coast of Russia and Alaska, sometimes curving toward the Beaufort Sea before exiting to the Atlantic Ocean. It has been cited as a major factor in the North Atlantic Oscillation and Arctic oscillation atmospheric changes. The drift typically takes one of two paths before exiting into the northern Atlantic Ocean.\n\nOn decadal and longer timescales, the North Atlantic Oscillation (NAO) and the Arctic Oscillation (AO) indices affect the flow pattern of the transpolar drift stream. During times of positive NAO (NAO+) and positive AO (AO+), there is a weak Arctic high and the associated surface winds produce a cyclonic (anti-clockwise) ice drift motion in eastern Arctic Ocean. In this case, the drift flows from the Laptev Sea towards the Beaufort Sea before exiting the Arctic Ocean through the Fram Strait. Conversely, during periods of NAO- and AO-, there is a strong Arctic high and ice motion flows in an anticyclonic (clockwise) motion in the Eurasian Basin. In this phase, the drift flows directly from the Laptev Sea through the Fram Strait.\n\n"}
{"id": "203111", "url": "https://en.wikipedia.org/wiki?curid=203111", "title": "Tropical and subtropical coniferous forests", "text": "Tropical and subtropical coniferous forests\n\nTropical and subtropical coniferous forests are a tropical forest biome. They are located in regions of humid climate at tropical and subtropical latitudes. Most tropical and subtropical coniferous forest ecoregions are found in the Nearctic and Neotropic ecozones, from the Mid-Atlantic states to Nicaragua and on the Greater Antilles, Bahamas, and Bermuda. Other tropical and subtropical coniferous forests ecoregions occur in Asia.\n\nOutside the Americas and the Greater Ranges of Asia they are rare and predominate on islands.\n\n\n"}
{"id": "1379448", "url": "https://en.wikipedia.org/wiki?curid=1379448", "title": "Vacuum grease", "text": "Vacuum grease\n\nVacuum grease is a lubricant with low volatility and is used for applications in low pressure environments. Lubricants with higher volatility would evaporate, causing two problems:\n\nAs well as a lubricant, vacuum grease is also used as a sealant for joints in vacuum systems. This is usually limited to soft vacuums, as ultra high vacuum or high temperatures may give problems with the grease outgassing. Grease is most commonly used with glass vacuum systems. All metal systems usually use knife-edge seals in soft metals instead. Where O ring seals are used, these should \"not\" be greased (in static seals at least) as it can cause the O rings to become permanently distorted when compressed.\n\nIn electronics manufacturing processes, vacuum grease is often used to prevent corrosion.\n\nOne of the early vacuum greases is the Ramsay grease.\n"}
{"id": "10268864", "url": "https://en.wikipedia.org/wiki?curid=10268864", "title": "Volcano Number", "text": "Volcano Number\n\nThe Volcano Number (also Volcano Reference File Number, Volcano Numbering System, or VNUM) is a hierarchical geographical system to uniquely identify and tag volcanoes and volcanic features on Earth. The numbers consist of four numerals, a hyphen, then two or three more numerals. The first two numerals identify the region, the next two the subregion, and the last two or three the individual volcano.\n\nThe VNUM was developed by the \"Catalogue of the Active Volcanoes of the World\" project of the International Association of Volcanology, now the International Association of Volcanology and Chemistry of the Earth's Interior (IAVCEI), in the late 1930s. It is currently administered by the Global Volcanism Program at the Smithsonian Institution in Washington, D.C. (USA), in cooperation with the IAVCEI, and the World Organization of Volcano Observatories (WOVO), a commission of IAVCEI.\n\nThe number system was used in both editions of the book Volcanoes of the World in 1981 and 1994. \n\nIn September 2013, the Global Volcanism Program announced a new format for Volcano Number. The new format is a 6-digit number, and no longer includes non-numeric characters.\n\n"}
{"id": "7359182", "url": "https://en.wikipedia.org/wiki?curid=7359182", "title": "World Wetlands Day", "text": "World Wetlands Day\n\nWorld Wetlands Day occurs annually on February 2nd, marking the date of the adoption of the Convention on Wetlands on February 2, 1971.\n\nEstablished to raise awareness about the value of wetlands for humanity and the planet, WWD was celebrated for the first time in 1997 and has grown remarkably since then. Each year, government agencies, non-governmental organizations, and groups of citizens at all levels of the community, have taken advantage of the opportunity to undertake actions aimed at raising public awareness of wetland values and benefits. Some of these benefits include: biologically diverse ecosystems that provide habitat for many species, serve as buffers on the coast against storms and flooding, and naturally filter water by breaking down or transforming harmful pollutants.\n\nSince 1998 the Ramsar Secretariat, with financial support from the Danone Group Evian Fund for Water, a variety of outreach materials which include logos, posters, factsheets, handouts and guide documents to support country activities organized to celebrate WWD, have been produced. These materials are available for free download on the World Wetlands Day website in the three languages of the Convention: English, French, and Spanish (www.worldwetlandsday.org). All the materials are also available in their design files for event organizers to customize and adapt them to their local languages and contexts. A few print copies are available to countries upon request to the Secretariat.\n\nStarting in 2015, a month-long Wetlands Youth Photo Contest that starts on 2 February, was introduced as a part of a new approach to target young people and get them involved in WWD. Thanks to Star Alliance Biosphere Connections, the prize awarded to the winner of the photo contest is a chance to visit a wetland of their choice from the over 2200 Ramsar Sites all over the world.\n\nSince 1997 the Ramsar website has posted reports from about 100 countries of their WWD activities. In 2016 a map of events was introduced to help countries promote their activities and to facilitate reporting after WWD www.ramsar.org/activity/world-wetlands-day.\n\nEach year a theme is selected to focus attention and help raise public awareness about the value of wetlands. Countries organize a variety of events to raise awareness such as ; lectures, seminars, nature walks, children’s art contests, sampan races, community clean-up days, radio and television interviews, letters to newspapers, to the launch of new wetland policies, new Ramsar Sites and new programmes at the national level.\n\n"}
{"id": "18952479", "url": "https://en.wikipedia.org/wiki?curid=18952479", "title": "Xerophyte", "text": "Xerophyte\n\nA xerophyte (from Greek ξηρός \"xeros\" dry, φυτόν \"phuton\" plant) is a species of plant that has adaptations to survive in an environment with little liquid water, such as a desert or an ice- or snow-covered region in the Alps or the Arctic. Popular examples of xerophytes are cacti and pineapple plants.\n\nThe structural features (morphology) and fundamental chemical processes (physiology) of xerophytes are variously adapted to conserve water, also common to store large quantities of water, during dry periods. Other species are able to survive long periods of extreme dryness or desiccation of their tissues, during which their metabolic activity may effectively shut down. Plants with such morphological and physiological adaptations are . Xerophytes such as cacti are capable of withstanding extended periods of dry conditions as they have deep-spreading roots and capacity to store water. the leaves are waxy and thorny that prevents loss of water and moisture. Even their fleshy stems can store water.\n\nPlants absorb water from the soil, which then evaporates from their shoots and leaves; this process is known as transpiration. In dry environments, a typical mesophytic plant would evaporate water faster than the rate of water uptake from the soil, leading to wilting and even death.\n\nXerophytic plants exhibit a diversity of specialized adaptations to survive in such water-limiting conditions. They may use water from their own storage, allocate water specifically to sites of new tissue growth, or lose less water to the atmosphere and so channel a greater proportion of water from the soil to photosynthesis and growth. Different plant species possess different qualities and mechanisms to manage water supply, enabling them to survive.\n\nCacti and other succulents are commonly found in deserts, where there is little rainfall. Other xerophytes, such as certain bromeliads, can survive through both extremely wet and extremely dry periods and can be found in seasonally-moist habitats such as tropical forests, exploiting niches where water supplies are too intermittent for mesophytic plants to survive. Likewise, chaparral plants are adapted to Mediterranean climates, which have wet winters and dry summers. \n\nPlants that live under arctic conditions also have a need for xerophytic adaptations, since water is unavailable for uptake when the ground is frozen, such as the European resurrection plants \"Haberlea rhodopensis\" and \"Ramonda serbica\". \n\nIn an environment with very high salinity such as mangrove swamps and semi-deserts, water uptake by plants is a challenge due to the high salt ion levels. Besides that, such environments may cause an excess of ions to accumulate in the cells, which is very damaging. Halophytes and xerophytes evolved to survive in such environments. Some xerophytes may also be considered halophytes, however, halophytes are not necessarily xerophytes. The succulent xerophyte \"Zygophyllum xanthoxylum\", for example, have specialised protein transporters in their cells which allow storage of excess ions in their vacuole to maintain normal cytosolic pH and ionic composition.\n\nThere are many factors which affect water availability, which is the major limiting factor of seed germination, seedling survival, and plant growth. These factors include infrequent raining, intense sunlight and very warm weather leading to faster water evaporation. An extreme environmental pH and high salt content of water also disrupt plants' water uptake.\n\nSucculent plants store water in their stems or leaves. These include plants from the Cactaceae family, which have round stems and can store a lot of water. The leaves are often vestigial, as in the case of cacti, wherein the leaves are reduced to spines, or they do not have leaves at all. These include the C4 perennial woody plant, \"Haloxylon ammodendron\" which is a native of northwest China. \n\nNon-succulent perennials successfully endure long and continuous shortage of water in the soil. These are hence called 'true xerophytes' or euxerophytes. Water deficiency usually reaches 60–70% of their fresh weight, as a result of which the growth process of the whole plant is hindered during cell elongation. The plants which survive drought are, understandably, small and weak.\n\nEphemerals are the 'drought escaping' kind, and not true xerophytes. They do not really endure drought, only escape it. With the onset of rainfall, the plant seeds germinate, quickly grow to maturity, flower, and set seed, i.e., the entire life cycle is completed before the soil dries out again. Most of these plants are small, roundish, dense shrubs represented by species of Papilionaceae, some inconspicuous Compositae, a few Zygophyllaceae and some grasses. Water is stored in the bulbs of some plants, or at below ground level. They may be dormant during drought conditions and are, therefore, known as drought evaders.\n\nShrubs which grow in arid and semi-arid regions are also xeromorphic. For example, \"Caragana korshinskii, Artemisia sphaerocephala,\" and \"Hedysarum scoparium\" are shrubs potent in the semi-arid regions of the northwest China desert. These psammophile shrubs are not only edible to grazing animals in the area, they also play a vital role in the stabilisation of desert sand dunes. \n\nBushes, also called semi-shrubs often occur in sandy desert region, mostly in deep sandy soils at the edges of the dunes. One example is the \"Reaumuria soongorica\", a perennial resurrection semi-shrub. Compared to other dominant arid xerophytes, an adult \"R. soongorica\", bush has a strong resistance to water scarcity, hence, it is considered a super-xerophytes.\n\nIf the water potential (or strictly, water vapour potential) inside a leaf is higher than outside, the water vapour will diffuse out of the leaf down this gradient. This loss of water vapour from the leaves is called transpiration, and the water vapour diffuses through the open stomata. Transpiration is natural and inevitable for plants; a significant amount of water is lost through this process. However, it is vital that plants living in dry conditions are adapted so as to decrease the size of the open stomata, lower the rate of transpiration, and consequently reduce water loss to the environment. Without sufficient water, plant cells lose turgor. This is known as plasmolysis. If the plant loses too much water, it will pass its permanent wilting point, and die.\n\nIn brief, the rate of transpiration is governed by the number of stomata, stomatal aperture i.e. the size of the stoma opening, leaf area (allowing for more stomata), temperature differential, the relative humidity, the presence of wind or air movement, the light intensity, and the presence of a waxy cuticle. It is important to note, that whilst it is vital to keep stomata closed, they have to be opened for gaseous exchange in respiration and photosynthesis.\n\nXerophytic plants may have similar shapes, forms, and structures and look very similar, even if the plants are not very closely related, through a process called convergent evolution. For example, some species of cacti (members of the family Cactaceae), which evolved only in the Americas, may appear similar to Euphorbias, which are distributed worldwide. An unrelated species of caudiciforms plants with swollen bases that are used to store water, may also display some similarities. \n\nUnder conditions of water scarcity, the seeds of different xerophytic plants behave differently, which means that they have different rates of germination since water availability is a major limiting factor. These dissimilarities are due to natural selection and eco-adaptation as the seeds and plants of each species evolve to suit their surrounding.\n\nXerophytic plants can have less overall surface area than other plants, so reducing the area that is exposed to the air and reducing water loss by transpiration and evaporation. They can also have smaller leaves or fewer branches than other plants. An example of leaf surface reduction are the spines of a cactus, while the effects of compaction and reduction of branching can be seen in the barrel cacti. Other xerophytes may have their leaves compacted at the base, as in a basal rosette, which may be smaller than the plant's flower. This adaptation is exhibited by some \"Agave\" and \"Eriogonum\" species, which can be found growing near Death Valley.\n\nSome xerophytes have tiny hairs on their surfaces to provide a wind break and reduce air flow, thereby reducing the rate of evaporation. When a plant surface is covered with tiny hairs, it is called tomentose. Stomata are located in these hairs or in pits to reduce their exposure to wind. This enables them to maintain a humid environment around them. \n\nIn a still, windless environment, the areas under the leaves or spines where transpiration takes place form a small localised environment that is more saturated with water vapour than normal. If this concentration of water vapour is maintained, the external water vapour potential gradient near the stomata is reduced, thus, reducing transpiration. In a windier situation, this localisation is blown away and so the external water vapour gradient remains low, which makes the loss of water vapour from plant stomata easier. Spines and hairs trap a layer of moisture and slows air movement over tissues.\n\nThe color of a plant, or of the waxes or hairs on its surface, may serve to reflect sunlight and reduce transpiration. An example is the white chalky epicuticular wax coating of \"Dudleya brittonii\", which has the highest ultraviolet light (UV) reflectivity of any known naturally-occurring biological substance.\n\nMany xerophytic species have thick cuticles. Just like human skin, a plant's cuticles are the first line of defense for its aerial parts. As mentioned above, the cuticle contains wax for protection against biotic and abiotic factors. The ultrastructure of the cuticles varies in different species. Some examples are \"Antizoma miersiana\", \"Hermannia disermifolia\" and \"Galenia africana\" which are xerophytes from the same region in Namaqualand, but have different cuticle ultrastructures. \n\n\"A. miersiana\" has thick cuticle as expected to be found on xerophytes, but \"H. disermifolia\" and \"G. africana\" have thin cuticles. Since resources are scarce in arid regions, there is selection for plants having thin and efficient cuticles to limit the nutritional and energy costs for the cuticle construction.\n\nIn periods of severe water stress and stomata closure, the cuticle's low water permeability is considered as one of the most vital factor in ensuring the survival of the plant. The rate of transpiration of the cuticles of xerophytes is 25 times lower than that of stomatal transpiration. To give an idea of how low this is, the rate of transpiration of the cuticles of mesophytes is only 2 to 5 times lower than stomatal transpiration.\n\nThere are many changes that happen on the molecular level when a plant experiences stress. When in heat shock, for example, their protein molecule structures become unstable, unfold, or reconfigure to become less efficient. Membrane stability will decrease in plastids, which is why photosynthesis is the first process to be affected by heat stress. Despite the many stresses, xerophytes have the ability to survive and thrive in drought conditions due to their physiological and biochemical specialties.\n\nSome plants can store water in their root structures, trunk structures, stems, and leaves. Water storage in swollen parts of the plant is known as succulence. A swollen trunk or root at the ground level of a plant is called a caudex and plants with swollen bases are called caudiciforms.\n\nPlants may secrete resins and waxes (epicuticular wax) on their surfaces, which reduce transpiration. Examples are the heavily-scented and flammable resins (volatile organic compounds) of some chaparral plants, such as \"Malosma laurina\", or the chalky wax of \"Dudleya pulverulenta\". \n\nIn regions continuously exposed to sunlight, UV rays can cause biochemical damage to plants, and eventually lead to DNA mutations and damages in the long run. When one of the main molecules involved in photosynthesis, photosystem II (PSII) is damaged by UV rays, it induces responses in the plant, leading to the synthesis of protectant molecules such as flavonoids and more wax. Flavonoids are UV-absorbing and act like sunscreen for the plant. \n\nHeat shock proteins (HSPs) are a major class of proteins in plants and animals which are synthesised in cells as a response to heat stress. They help prevent protein unfolding and help re-fold denatured proteins. As temperature increases, the HSP protein expression also increases.\n\nEvaporative cooling via transpiration can delay the effects of heat stress on the plant. However, transpiration is very expensive if there is water scarcity, so generally this is not a good strategy for the plants to employ.\n\nMost plants have the ability to close their stomata at the start of water stress, at least partially, to restrict rates of transpiration. They use signals or hormones sent up from the roots and through the transpiration stream. Since roots are the parts responsible for water searching and uptake, they can detect the condition of dry soil. The signals sent are an early warning system - before the water stress gets too severe, the plant will go into water-economy mode.\n\nAs compared to other plants, xerophytes have an inverted stomatal rhythm. During the day and especially during mid-day when the sun is at its peak, most stomata of xerophytes are close. Not only do more stomata open at night in the presence of mist or dew, the size of stomatal opening or aperture is larger at night compared to during the day. This phenomenon was observed in xeromorphic species of Cactaceae, Crassulaceae, and Liliaceae.\n\nAs the epidermis of the plant is covered with water barriers such as lignin and waxy cuticles, the night opening of the stomata is the main channel for water movement for xerophytes in arid conditions. Even when water is not scarce, the xerophytes \"A. Americana\" and pineapple plant are found to utilise water more efficiently than mesophytes.\n\nThe plasma membrane of cells are made up of lipid molecules called phospholipids. These lipids become more fluid when temperature increases. Saturated lipids are more rigid than unsaturated ones i.e. unsaturated lipids becomes fluid more easily than saturated lipids. Plant cells undergo biochemical changes to change their plasma membrane composition to have more saturated lipids to sustain membrane integrity for longer in hot weather.\nIf the membrane integrity is compromised, there will be no effective barrier between the internal cell environment and the outside. Not only does this mean the plant cells are susceptible to disease-causing bacteria and mechanical attacks by herbivores, the cell could not perform its normal processes to continue living - the cells and thus the whole plant will die.\n\nLight stress can be tolerated by dissipating excess energy as heat through the xanthophyll cycle. Violaxanthin and zeaxanthin are carotenoid molecules within the chloroplasts called xanthophylls. Under normal conditions, violaxanthin channels light to photosynthesis. However, high light levels promote the reversible conversion of violaxanthin to zeaxanthin. These two molecules are photo-protective molecules. \n\nUnder high light, it is unfavourable to channel extra light into photosynthesis because excessive light may cause damage to the plant proteins. Zeaxanthin dissociates light-channelling from the photosynthesis reaction - light energy in the form of photons will not be transmitted into the photosynthetic pathway anymore.\n\nStomata closure not only restricts the movement of water out of the plant, another consequence of the phenomenon is that carbon dioxide influx or intake into the plant is also reduced. As photosynthesis requires carbon dioxide as a substrate to produce sugar for growth, it is vital that the plant has a very efficient photosynthesis system which maximises the utilisation of the little carbon dioxide the plant gets. \n\nMany succulent xerophytes employ the Crassulacean acid metabolism or better known as CAM photosynthesis. It is also dubbed the \"dark\" carboxylation mechanism because plants in arid regions collect carbon dioxide at night when the stomata open, and store the gases to be used for photosynthesis in the presence of light during the day. Although most xerophytes are quite small, this mechanism allows a positive carbon balance in the plants to sustain life and growth. Prime examples of plants employing the CAM mechanism are the pineapple, \"Agave Americana\", and \"Aeonium haworthii\". \n\nAlthough some xerophytes perform photosynthesis using this mechanism, the majority of plants in arid regions still employ the C and C photosynthesis pathways. A small proportion of desert plants even use a collaborated C-CAM pathway.\n\nThe surrounding humidity and moisture right before and during seed germination play an important role in the germination regulation in arid conditions.\nAn evolutionary strategy employed by desert xerophytes is to reduce the rate of seed germination. By slowing the shoot growth, less water is consumed for growth and transpiration. Thus, the seed and plant can utilise the water available from short-lived rainfall for a much longer time compared to mesophytic plants.\n\nDuring dry times, resurrection plants look dead, but are actually alive. Some xerophytic plants may stop growing and go dormant, or change the allocation of the products of photosynthesis from growing new leaves to the roots. These plants evolved to be able to coordinately switch off their photosynthetic mechanism without destroying the molecules involved in photosynthesis. When water is available again, these plants would \"resurrect from the dead\" and resume photosynthesis, even after they had lost more than 80% of their water content. A study has found that the sugar levels in resurrection plants increase when subjected to desiccation. This may be associated with how they survive without sugar production via photosynthesis for a relatively long duration. Some examples of resurrection plants include the \"Anastatica hierochuntica\" plant or more commonly known as the \"Rose of Jericho\", as well as one of the most robust plant species in East Africa, the \"Craterostigma pumilum\".\nSeeds may be modified to require an excessive amount of water before germinating, so as to ensure a sufficient water supply for the seedling's survival. An example of this is the California poppy, whose seeds lie dormant during drought and then germinate, grow, flower, and form seeds within four weeks of rainfall.\n\nIf the water supply is not enough despite the employment of other water-saving strategies, the leaves will start to collapse and wilt due to water evaporation still exceeding water supply. Leaf loss (abscission) will be activated in more severe stress conditions. Drought deciduous plants may drop their leaves in times of dryness. \n\nThe wilting of leaves is a reversible process, however, abscission is irreversible. Shedding leaves is not favourable to plants because when water is available again, they would have to spend resources to produces new leaves which are needed for photosynthesis.\n\nThe leaf litter on the ground around a plant can provide an evaporative barrier to prevent water loss. A plant’s root mass itself may also hold organic material that retains water, as in the case of the arrowweed (\"Pluchea sericea\").\n\nLand degradation is a major threat to many countries such as China and Uzbekistan. The major impacts include the loss of soil productivity and stability, as well as the loss of biodiversity due to reduced vegetation consumed by animals. In arid regions where water is scarce and temperatures are high, mesophytes will not be able to survive, due to the many stresses. Xerophytic plants are used widely to prevent desertification and for fixation of sand dunes. In fact, in northwest China, the seeds of three shrub species namely \"Caragana korshinskii, Artemisia sphaerocephala, and\" \"Hedysarum scoparium\" are dispersed across the region. These shrubs have the additional property of being palatable to grazing animals such as sheep and camels. \"H. scoparium\" is under protection in China due to it being a major endangered species. \"Haloxylon ammodendron\" and \"Zygophyllum xanthoxylum\" are also plants that form fixed dunes. \n\nA more well-known xerophyte is the succulent plant \"Agave americana\". It is cultivated as an ornamental plant popular across the globe. Agave nectar is garnered from the plant and is consumed as a substitute for sugar or honey. In Mexico, the plant's sap is usually fermented to produce an alcoholic beverage.\n\nMany xerophytic plants produce colourful vibrant flowers and are used for decoration and ornamental purposes in gardens and in homes. Although they have adaptations to live in stressful weather and conditions, these plants thrive when well-watered and in tropical temperatures. \"Phlox sibirica\" is rarely seen in cultivation and does not flourish in areas without long exposure to sunlight.\n\nA study has shown that xerophytic plants which employ the CAM mechanism can solve micro-climate problems in buildings of humid countries. The CAM photosynthetic pathway absorbs the humidity in small spaces, effectively making the plant such as \"Sansevieria trifasciatas\" a natural indoor humidity absorber. Not only will this help with cross-ventilation, but lowering the surrounding humidity increases the thermal comfort of people in the room. This is especially important in East Asian countries where both humidity and temperature are high. \nHerbal extracts from plants of the Craterostigma genus are traditionally used by Kenyan natives as remedies to reduce inflammation and to relieve pain relating to the muscle and joints. However, scientific research on this practice revealed that these herbal extracts could cause hyperglasia.\n\nRecent years has seen interests in resurrection plants other than their ability to withstand extreme dryness. The metabolites, sugar alcohols, and sugar acids present in these plants may be applied as natural products for medicinal purposes and in biotechnology. During desiccation, the levels of the sugars sucrose, raffinose, and galactinol increase; they may have a crucial role in protecting the cells against damage caused by reactive oxygen species (ROS) and oxidative stress. Besides having anti-oxidant properties, other compounds extracted from some resurrection plants showed anti-fungal and anti-bacterial properties. A glycoside found in \"Haberlea rhodopensis\" called myconoside is extracted and used in cosmetic creams as a source of anti-oxidant as well as to increase elasticity of the human skin. Although there are other molecules in these plants that may be of benefit, it is still much less studied than the primary metabolites mentioned above. \n\n\n"}
{"id": "52051185", "url": "https://en.wikipedia.org/wiki?curid=52051185", "title": "Yerköprü Waterfall (Mersin)", "text": "Yerköprü Waterfall (Mersin)\n\nYerköprü Waterfall () is a waterfall in Mersin Province, southern Turkey. It is a registered natural monument.\n\nThe waterfall is located to the north of Kuskan village of Gülnar ilçe (district) in Mersin Province, But it is accessible from Mut, Mersin|Mut ilçe. It is situated to the southeast of Gezende Dam. Its distance to Mut is .\n\nIn early March 2013, the road leading to the waterfall was blocked by rocks due to a landslide. After almost two years, construction works began for access to the waterfall on the other bank of the creek. It can be reached by a wooden staircase down from a bridge over the creek.\n\nThe site is a popular tourist attraction . Admission is collected for visitors and vehicles.\n\nThe waterfall is on the Ermenek Creek, a tributary of the Göksu River. It was formed in a deep canyon by a narrow spring emerged in faults of Cretaceous-period limestone formation rocks, which are about 110 million years old. It is fed by an underground water tunnel of about length, and width, a cave with stalactites and a rich vegetation. Waters from the Gezende Dam join the spring water in the cave.\n\nIn 2001, the waterfall, covering an area of , was registered as a natural monument by the Nature Reserve and Nature Parks Administration of the Ministry of Forest and Water Management.\n"}
{"id": "53284071", "url": "https://en.wikipedia.org/wiki?curid=53284071", "title": "Zirconian", "text": "Zirconian\n\nThe Zirconian is the second Era within the Hadean Eon in a proposed revision of the Precambrian time scale. It lasted 373 million years from the end of the Chaotian Era to the beginning of Eoarchean Era . The Zirconian follows the Chaotian Era and its beginning is chronometrically set at 4.404 ± 0.008 Gya. This corresponds to the age of the first occurrence of Hadean zircons in the Jack Hills in Western Australia (Yilgarn craton). The end of the Zirconian Era and the transition to the Acastan Period (the earliest period of the Archean Eon and Eoarchean Era) occurred with the appearance of the oldest rock at 4.031 ± 0.003 Gya.\n\nThe term \"Zirconian\" is derived from the mineral zircon, since it is the only substance that is preserved from this early period. An alternative proposal for the name of this era is \"Jack Hillsian Era\".\n\nThe first 196 million years after the formation of Earth, left no solid trace, except for the formation of the Moon. This time is termed the Chaotian Era. The first datable solid Earth minerals, zircons, mark the start of the Zirconian Era. The oldest known crystal could be dated to 4,404 ± 8 Mya. It was in metasedimentary rock on Erawandoo Hills in the Jack Hills of Narryer Gneiss Terrane in Western Australia. The 4.4 Gya is the limiting age, and zircons this old are very rare. But from 4.2 to 4.1 Gya they are already common. The bulk of the zircons in Narryer Gneiss Terrane date much later, from 3.75 to 3.5 Gya.\n\nThe zircons are limited only to Archean cratons. In the Yilgarn craton quartzites of Southern Cross Terrane also contains zircons with an age of 4.35-3.13 Gya).\n\nPossibly the Nuvvuagittuq Greenstone Belt of the Superior Craton is of Zirconian age, its proposed age of 4.28 Gya is however still controversial. The end of the Zirconian Era is at 4.031 Gya, marked by the Acasta Gneiss of Slave craton and it is likely that its protolith is older. In 2006, a single zircon crystal in Acasta Gneiss was dated to 4.2 Gya\n"}
