{"id": "33703683", "url": "https://en.wikipedia.org/wiki?curid=33703683", "title": "Aviation in World War II", "text": "Aviation in World War II\n\nDuring World War II, aviation firmly established itself as a critical component of modern warfare from the Battle of Britain in the early stages to the great aircraft carrier battles between American and Japanese Pacific fleets and the final delivery of nuclear weapons. The major combatants – Germany and Japan on the one side and Britain, the United States and the USSR on the other – manufactured huge air forces which engaged in pitched battles both with each other and with the opposing ground forces. Bombing established itself as a major strategic force, and this was also the first war in which the aircraft carrier played a significant role.\n\nAs with Aviation in World War I, military investment during World War II drove aviation forward in leaps and bounds. The streamlined cantilever monoplane quickly proved its worth in almost every role, although a few older biplanes remained in niche roles for much of the war. Engine power and aircraft performance increased steadily, with jet and rocket engines beginning to make their appearance by the end of the war. Avionics systems increased in sophistication and became more widespread, including power-assisted flight controls, blind flying instrumentation, radio communications and radar tracking.\n\nThe development of civil aviation stagnated until peace could be restored, and in the combatant countries many existing civilian aircraft were pressed into military service. However military technologies developed during the war would revolutionise postwar aviation. In particular, the widespread construction of aerodromes with serviceable runways would provide the basis for a postwar move of long-range passenger flights from flying boats to landplanes.\n\nThe streamlined cantilever monoplane quickly proved its worth in almost every role, although a few older biplanes and other obsolescent types remained in niche roles for much of the war. Key design features during this period included:\n\nThe retracting undercarriage gave landplanes a significant performance advantage over the equivalent seaplane, whose floats caused additional drag. In other respects, the evolution of seaplane design paralleled landplane developments. Seaplanes, typically flying boats, remained in use for long-range maritime operations. Smaller craft, typically floatplanes, remained in other niche areas such as mountain lakes where a runway was not feasible.\n\nExperiments on other configurations continued throughout the war, especially in Germany.\n\nA small number of twin tailboom types entered production, and some slower types designed for roles such as Army observation retained the older fixed undercarriage.\n\nTowards the end of the war the first jet aircraft entered service; the Arado Ar 234 reconnaissance bomber, Messerschmitt Me 262 \"Schwalbe\" (Swallow) fighter and Gloster Meteor fighter. The Messerschmitt Me 163 \"Komet\" (Comet) interceptor was both rocket powered and of tailless configuration. Both of the Messerschmitt types had swept wings to delay the onset of small shock waves and the accompanying drag at transonic speeds. Other German types delivered to front-line units in the last days of the war included the rocket-powered vertical-takeoff Bachem Ba 349 \"Natter\" (Adder) interceptor — the very first manned, rocket-powered aircraft to launch vertically as designed — and the jet-powered Heinkel He 162 \"Spatz\" (sparrow) light fighter.\n\nOther variations were flown but never entered production, sometimes independently by different countries. These included the canard or tail-first configuration in combination with a pusher propeller, the flying wing, the slip-wing which took off as a biplane and then discarded the upper wing, and twin centrally-mounted engines in push-pull configuration with a tractor installation at the front and a pusher installation in the rear.\n\nMilitary gliders such as the British Airspeed Horsa and specialised tugs such as the German Heinkel He 111Z were developed by a number of countries during World War II, for landing assault troops and equipment behind enemy lines. These gliders were characterised by a steep gliding angle and short landing run, allowing a short time in the air and precision landing. However they were highly vulnerable and their dependence on surprise severely limited their success. The British used them extensively in the battle of Arnhem and suffered huge losses.\n\nRotorcraft had been produced before the war in the form of the autogyro and many, such as the Avro Rota, a license-built Cierva design, continued in use throughout the war.\n\nThe Focke-Achgelis Fa 330 \"Bachstelze\" (Wagtail) unpowered rotor kite was towed behind submarines for use as an observation platform.\n\nIn 1942 the Flettner Fl 282 \"Kolibri\" (Hummingbird) observation platform became the first true helicopter with a power-driven rotor to enter production. Two years later it was followed in Germany by the twin-rotor Focke Achgelis Fa 223 \"Drache\" (Kite) transport helicopter and in America by the Sikorsky R-4. The R-4 was far the most-produced type and was introduced into RAF service as the Hoverfly I, where it was progressively replacing the Avro Rota autogyro by the end of hostilities.\n\nEngine power and aircraft performance increased steadily throughout the war, with liquid-cooled inline and vee engines competing with air-cooled radials much as they had competed with air-cooled rotary types in the First War. As an example, at the start of the war the Rolls-Royce Merlin III liquid-cooled V-12 engine developed just 1,000 hp while at the end its derivative the Rolls-Royce Griffon 61 offered 2,035 hp.\n\nIn the early stages of the war German fighters, especially the Messerschmitt Bf 109, were very fast and maneuverable and had the advantage over the British types of a fuel-injected engine. This allowed them to fly upside-down or to perform other negative-G manoeuvres without fear of the engine cutting out, as happened to the British types fitted with carburettors. On the other hand, the carburettor combined with a turbocharger gave better performance at altitude. However, as the war wore on, Germany's critical inability to produce piston aviation engines of at least 1,500 kW (2,000 PS) maximum power and above that possessed proven front-line reliability, prevented them from fully developing more advanced strategic and tactical combat aircraft designs that would require such powerplants.\n\nMeanwhile, jet and rocket engines had been under steady development, the rocket especially in Germany and the jet both there and in Britain. By the end of the war they were beginning to make their appearance in operational types. The German and British jet technologies differed significantly. The axial-flow jet, in which air passes continuously backwards through the engine, was recognized as the most efficient design but required highly advanced new technologies in both materials and precision manufacture. While the Germans opted for this approach, the British chose the simpler and more robust centrifugal compressor in which the air is first flung outwards, using centrifugal force to help compress it, before being burned and returned to the axial-flow turbine stage. This resulted in a shorter but wider engine for the same airflow and output power. The Hungarian Jendrassik Cs-1, in 1940 the world's first turboprop, was of axial-flow design with similarly revered-flow combustion but was cancelled due to other priorities. The pulse jet was a crude jet engine which produced too much vibration to be usable for manned aircraft but found a niche in the V-1 flying bomb.\n\nAt the start of the war, the British Hawker Hurricane and Supermarine Spitfire fighters had eight machine guns against typically four on the Messerschmitt Bf 109, giving them far greater firepower. The early marks of Spitfire and Hurricane had machine guns that were, however, of the .30 calibre (7.62mm) class, with less hitting power than heavier calibre weapons firing non-explosive bullets - the Germans' MG 131 machine gun, the Japanese Ho-103 machine gun, the Soviets' Berezin UB and particularly the \"light-barrel\" AN/M2 version of the American Browning M2 machine gun, all of the .50 calibre (12.7mm) size, became very widely used as primary offensive and defensive aircraft armament by the end of World War II.\n\nThe Bf 109 could also be fitted with a cannon, and later variants with up to three. These guns have exploding shells instead of solid bullets but are larger and heavier than ordinary machine guns and opinions differed on both sides as to which type was better. Some aircraft were made in both variants, while others could be modified in the field to fit either or both types. As the war progressed, higher aircraft speeds, cockpit armour and stronger airframes led to a steadily increasing preference for cannon.\n\nLater in the war the British and American forces made extensive use of unguided rockets for ground attack, while the German Bachem Natter point-defence interceptor had a battery of rockets in the nose intended to be fired into an oncoming bomber formation.\n\nAvionics systems increased in sophistication and became more widespread, including power-assisted flight controls, blind flying instrumentation, radio communications and radar tracking.\n\nAircraft manufacturing remained a high priority throughout the war for all the key combatants and was a major part of their economic output. Women, and in Germany slave labour, were extensively employed due to the military call-up of able-bodied men.\n\nDue to the threat of bombing, especially in Europe, manufacturing became progressively more dispersed. When British and American bombing raids started in earnest, Germany moved much of its production into underground factories.\n\nStrategic materials such as aluminium for airframes and petroleum oil for fuel were in limited supply and soon became scarce. Many manufacturers, especially in the Soviet Union and, later on, in Germany turned to more readily-available raw materials such as timber and coal. The de Havilland Mosquito fighter-bomber was a rare British example of a wooden aeroplane.\n\nAt the outbreak of war there were relatively few aerodromes capable of supporting military aircraft operations. Land-based aircraft with retracting undercarriage had superior performance to the equivalent seaplanes, leading to the widespread construction of aerodromes throughout all theatres of campaign. After the war many of these would become civil airports, providing the basis for a move of long-range passenger flights from flying boats to landplanes.\n\nThe increasing sophistication of warplanes meant that ground facilities also became more sophisticated. The high-powered engines in use could no longer be started by hand-swinging the propeller, but powered starting systems had to be provided, whether mechanical as with the Hucks starter or electrical as with the wheeled battery pack or trolley accumulator used for aircraft such as the Spitfire, which had an inbuilt electric starter motor.\n\nAviation during this period was dominated by the conduct of the war, and the war in turn by air power. Aviation was heavily involved in the development of military technologies, strategies, tactics and events throughout the war.\n\nAt the outbreak of the war in Europe in 1939, the German \"Luftwaffe\" had amassed an attack force of modern all-metal cantilever monoplanes designed to support the \"Blitzkrieg\" style of warfare at relatively short range and manufactured by a large and organised industry. Pilots had been well-trained, both in flying clubs and in some cases the Spanish Civil War. Other European air forces, especially the British RAF were struggling to re-equip with similarly modern types and to train up aircrew. Early German successes, with the notable assistance of the Junkers Ju 87 \"Stuka\" dive-bomber, overran Europe and left Britain open to attack.\n\nDuring the ensuing Battle of Britain, the British fighter squadrons had to quickly relearn the old tactical lessons from the first war. Initially the RAF fighters flew in a tight three-fighter arrowhead formation, soon changing to the looser four-aircraft arrangement which the Germans called the \"finger-four.\" They soon also relearned the value of climbing above your opponent before attacking. At the same time, the development of an early radar warning system by the British provided a new way to track German attack formations as they gathered over the European coast and flew across the English Channel. The radio communications, provided to every pilot, also required new protocols such as radio silence before engaging the enemy.\n\nLater on, the British and Americans developed large long-range heavy bombers, causing great damage to the German war effort and substantial casualties. While the British favoured unescorted night bombing, the Americans preferred to mount daytime raids, escorted by long-range fighters.\n\nIn the Pacific war, both sides made extensive use of aircraft carriers and carrier-to-carrier engagements became pivotal turning points in several campaigns.\n\nCivil aviation continued in non-combatant countries.\n\nIn countries engaged in warfare, many civilian aircraft were pressed into military service. Some civilian operations were maintained, for example BOAC continued making overseas passenger flights, often camouflaging its aircraft.\n\n"}
{"id": "1792063", "url": "https://en.wikipedia.org/wiki?curid=1792063", "title": "Aziza (African mythology)", "text": "Aziza (African mythology)\n\nThe Aziza (African) are a type of beneficent supernatural race in West African (specifically, Dahomey) mythology. Living in the forest, they provide good magic for hunters. They are also known to have given practical and spiritual knowledge to people (including knowledge of the use of fire).\nThe Aziza are described as little hairy people and are said to live in anthills and silk-cotton trees.\n\nWhile the Aziza are usually described as a people, some traditions also refer to a single individual by name \"Aziza\", with similar traits. For example, Jeje oral tradition has a divinity called \"Aziza\" (described as a small, single-legged man smoking a pipe).\n\nAccording to Ochuko Tonukari, Aziza is also a god of the Urhobo people of the Western Niger Delta of Nigeria.\n"}
{"id": "6730067", "url": "https://en.wikipedia.org/wiki?curid=6730067", "title": "BSI PAS 100", "text": "BSI PAS 100\n\nThe British Composting Association worked to establish an industry standard for composts, the BSI PAS 100 certified by the British Standards Institution. The specification covers the entire process; from raw materials and production methods, through quality control and lab testing ensuring certified composts are quality assured traceable safe and reliable. Description: Composting (waste), Biodegradability, Degradation, Waste disposal, Waste handling, Quality control, Biological hazards, Pathogens, Toxic materials, Toxins, Contaminants, Pollution control, Environmental management, Management operations, Performance, Marking, Labels, Fertilizers. \n\nPAS stands for Publicly Available Specification\n\n"}
{"id": "9853191", "url": "https://en.wikipedia.org/wiki?curid=9853191", "title": "Beijing Anomaly", "text": "Beijing Anomaly\n\nThe Beijing Anomaly is an observed seismic feature in the Earth's mantle at a depth of around 700–1400 km below Northeastern China where a high degree of seismic attenuation was discovered to exist. According to its discoverers, Jesse Lawrence (from Scripps Institute of Oceanography) and Michael Wysession (from Washington University), the Beijing Anomaly is evidence for large amounts of water contained within the mantle.\n\nThe effects of water on the attenuation of seismic waves are largely unknown, although theoretical expectations seem to indicate that a seismic Q·¹ factor change from 300 to 100 can be explained by an approximate tenfold increase in water content. This leads to the hypothesis that only small amounts of water, present at depth within the mantle (on the order of 0.1 wt%), will lead to significant seismic attenuation and could potentially explain the anomalously low Q·¹ values which extend over a broad region in the mantle below Northeast Asia.\n\nThe zone of low Q·¹ existing in the mantle below Northeastern China was found via seismic attenuation tomography and begins approximately 700 km below the surface and displays stark decreases in Q·¹ values, down to a minimum of Q·¹= 95 at around 1000 km in depth. The extent of the anomaly below a large portion of Northeast Asia would require the existence of contributing factors which would facilitate the distribution of water within the mantle itself. Factors which could facilitate the dispersion of water to such a degree include mantle-rock advection and water transport along grain boundaries.\n\nIt has therefore been suggested that the downgoing oceanic lithosphere can carry with it large amounts of water into deeper regions of the mantle, directly below the continental margins (to depths reaching far beyond 1400 km). The water contained within these subducting slabs is thought to remain unaffected by the surrounding high-temperature and pressure conditions, since it is contained within the centre of the cold lithospheric slab. The seismic attenuation anomalies in Northeast China are potentially explained by water having been extracted from the cold, deep-seated oceanic lithosphere and “flooding” the overlying lower mantle wedge at depth, giving rise to zones of high elasticity (and potentially fluid-rich) which would cause levels of seismic attenuation that were observed tomographically.\n\nRecent research has been able to better constrain the degree to which subducting oceanic mantle is hydrated using measurements of P-wave attenuation from localized earthquakes near the Wadati–Benioff zone. Estimates suggest an approximately 40 km thick layer of moderately serpentinized mantle would be sufficient to explain the observed seismic P-wave attenuations associated to events occurring at depths of around 5–35 km below the Wadati–Benioff zone. This would imply that the topmost zones of the oceanic lithosphere provide a significant source of water to deeper areas of the mantle during subduction, with an estimated flux of 170-318 Tg/ma of water per meter of down-going slab.\n"}
{"id": "52506181", "url": "https://en.wikipedia.org/wiki?curid=52506181", "title": "Beki River", "text": "Beki River\n\nBeki River (also known as the Kurissu River in Bhutan, ) is one of the right bank tributaries of the mighty Brahmaputra River, which flows down from the Bhutan region but a large portion flows in Indian state Assam. It touches flows from Bhutan touching Mathanguri, Narangur, Khusrabari, Valaguri, Mainamata, Odalguri, Barpeta Road, Nichukha, Sorbhog, Kalgachia, Balaipathar, Kharballi, Bardanga, Kamarpara, Srirampur, Daoukmari, Jania, Chanpur, Rubi,Sawpur, Gobindapur, Moinbari and Balikuri. Tourists can have beautiful view of the river and its natural surroundings from the bridges situated on NH no- 31.\n\nBeki River, also known as Kurissu River in Bhutan, lies between 26° 20' 00\" N; 90° 56' 00\" E which comes from Himalayan glacier. Bhutan has several major river systems flowing swiftly out of the Himalayas that are fed by glaciers in northern Bhutan. They flow south and join the Brahmaputra River basin in India. The Brahmaputra flows into Bangladesh and drains into the Bay of Bengal.\n\nThe soil erosion of Beki river has become a major problem of flowing two districts Barpeta and Baksa of Assam.\n"}
{"id": "37634973", "url": "https://en.wikipedia.org/wiki?curid=37634973", "title": "Bibliography of sustainability", "text": "Bibliography of sustainability\n\nThis is a bibliography of sustainability publications.\n\n"}
{"id": "147341", "url": "https://en.wikipedia.org/wiki?curid=147341", "title": "Bog", "text": "Bog\n\nA bog is a wetland that accumulates peat, a deposit of dead plant material—often mosses, and in a majority of cases, sphagnum moss. It is one of the four main types of wetlands. Other names for bogs include mire, quagmire, and muskeg; alkaline mires are called fens. They are frequently covered in ericaceous shrubs rooted in the sphagnum moss and peat. The gradual accumulation of decayed plant material in a bog functions as a carbon sink.\n\nBogs occur where the water at the ground surface is acidic and low in nutrients. In some cases, the water is derived entirely from precipitation, in which case they are termed ombrotrophic (cloud-fed). Water flowing out of bogs has a characteristic brown colour, which comes from dissolved peat tannins. In general, the low fertility and cool climate result in relatively slow plant growth, but decay is even slower owing to the saturated soil. Hence, peat accumulates. Large areas of the landscape can be covered in many meters deep in peat.\n\nBogs have distinctive assemblages of animal, fungal and plant species, and are of high importance for biodiversity, particularly in landscapes that are otherwise settled and farmed.\n\nBogs are widely distributed in cold, temperate climes, mostly in boreal ecosystems in the Northern Hemisphere. The world's largest wetland is the peat bogs of the Western Siberian Lowlands in Russia, which cover more than a million square kilometres. Large peat bogs also occur in North America, particularly the Hudson Bay Lowland and the Mackenzie River Basin. They are less common in the Southern Hemisphere, with the largest being the Magellanic moorland, comprising some 44,000 square kilometres. Sphagnum bogs were widespread in northern Europe but have often been cleared and drained for agriculture.\n\nA 2014 expedition leaving from Itanga village, Republic of the Congo, discovered a peat bog \"as big as England\" which stretches into neighboring Democratic Republic of Congo.\n\nThere are many highly specialised animals, fungi, and plants associated with bog habitat. Most are capable of tolerating the combination of low nutrient levels and waterlogging. Sphagnum is generally abundant, along with ericaceous shrubs. The shrubs are often evergreen, which is understood to assist in conservation of nutrients. In drier locations, evergreen trees can occur, in which case the bog blends into the surrounding expanses of boreal evergreen forest. Sedges are one of the more common herbaceous species. Carnivorous plants such as sundews (\"Drosera\") and pitcher plants (for example \"Sarracenia purpurea\") have adapted to the low-nutrient conditions by using invertebrates as a nutrient source. Orchids have adapted to these conditions through the use of mycorrhizal fungi to extract nutrients. Some shrubs such as \"Myrica gale\" (bog myrtle) have root nodules in which nitrogen fixation occurs, thereby providing another supplemental source of nitrogen.\n\nBogs are recognized as a significant/specific habitat type by a number of governmental and conservation agencies. They can provide habitat for mammals, such as caribou, moose, and beavers, as well as for species of nesting shorebirds, such as Siberian cranes and yellowlegs. The United Kingdom in its Biodiversity Action Plan establishes bog habitats as a priority for conservation. Russia has a large reserve system in the West Siberian Lowland. The highest protected status occurs in Zapovedniks (IUCN category IV); Gydansky and Yugansky are two prominent examples. Bogs even have distinctive insects; English bogs give a home to a yellow fly called the hairy canary fly (\"Phaonia jaroschewskii\"), and bogs in North America are habitat for a butterfly called the bog copper (\"Lycaena epixanthe\"). In Ireland, the viviparous lizard, the only known reptile in the country, dwells in bogland.\n\nBog habitats may develop in various situations, depending on the climate and topography (see also hydrosere succession).\n\nOne way of classifying bogs is based upon their location in the landscape, and their source of water.\n\nThese develop in gently sloping valleys or hollows. A layer of peat fills the deepest part of the valley, and a stream may run through the surface of the bog. Valley bogs may develop in relatively dry and warm climates, but because they rely on ground or surface water, they only occur on acidic substrates.\n\nThese develop from a lake or flat marshy area, over either non-acidic or acidic substrates. Over centuries there is a progression from open lake, to a marsh, to a fen (or on acidic substrates, valley bog), to a carr, as silt or peat accumulates within the lake. Eventually, peat builds up to a level where the land surface is too flat for ground or surface water to reach the center of the wetland. This part, therefore, becomes wholly rain-fed (ombrotrophic), and the resulting acidic conditions allow the development of bog (even if the substrate is non-acidic). The bog continues to form peat, and over time a shallow dome of bog peat develops into a raised bog. The dome is typically a few meters high in the center and is often surrounded by strips of fen or other wetland vegetation at the edges or along streamsides where groundwater can percolate into the wetland.\n\nThe various types of raised bog may be divided into:\n\nIn cool climates with consistently high rainfall (on more than c. 235 days a year), the ground surface may remain waterlogged for much of the time, providing conditions for the development of bog vegetation. In these circumstances, bog develops as a layer \"blanketing\" much of the land, including hilltops and slopes. Although a blanket bog is more common on acidic substrates, under some conditions it may also develop on neutral or even alkaline ones, if abundant acidic rainwater predominates over the groundwater. A blanket bog cannot occur in drier or warmer climates, because under those conditions hilltops and sloping ground dry out too often for peat to form – in intermediate climates a blanket bog may be limited to areas which are shaded from direct sunshine. In periglacial climates a patterned form of blanket bog may occur, known as a string bog. In Europe, these mostly very thin peat layers without significant surface structures are distributed over the hills and valleys of Ireland, Scotland, England and Norway. In North America, blanket bogs occur predominantly in Canada east of Hudson Bay. These bogs are often still under the influence of mineral soil water (groundwater). Blanket bogs do not occur north of the 65th latitude in the northern hemisphere.\n\nA \"quaking bog\" is a form of bog occurring in wetter parts of valley bogs and raised bogs and sometimes around the edges of acidic lakes. The bog vegetation, mostly sphagnum moss anchored by sedges (such as \"Carex lasiocarpa\"), forms a floating mat approximately half a meter thick on the surface of the water or on top of very wet peat. White spruces are also common in this bog regime. Walking on the surface causes it to move – larger movements may cause visible ripples on the surface, or they may even make trees sway. In the absence of disturbance from waves, the bog mat may eventually cover entire bays or even entire small lakes. Bogs at the edges of lakes may become detached and form floating islands.\n\nA cataract bog is a rare ecological community formed where a permanent stream flows over a granite outcropping. The sheeting of water keeps the edges of the rock wet without eroding the soil, but in this precarious location, no tree or large shrub can maintain a roothold. The result is a narrow, permanently wet habitat.\n\nBogs may also be classified by the nutrient content of the peat.\n\nA eutrophic bog, also called a minerotrophic bog, is one that lies on top of fen-peat. As a result, its water is rich in nutrients. They are found in temperate regions. Fens are an example of this kind of bog.\n\nA mesotrophic bog, also called a transitional peat bog, contains a moderate quantity of nutrients.\n\nOligotrophic bogs occur where the groundwater is poor in nutrients e.g. in wetlands with nutrient-poor soils. They occur in several variants: raised bogs, soligenic bogs and blanket bog.\n\nThe Great Kemeri Bog Boardwalk is a tourist destination in Ķemeri National Park, Jūrmala, Latvia, offering visitors a chance to explore the bog and its inhabitants. Short () and long () boardwalk trails are present, with an observation platform popular with photographers for sunrise and sunset scenes.\n\nAfter drying, peat is used as a fuel, and it has been used that way for centuries. More than 20% of home heat in Ireland comes from peat, and it is also used for fuel in Finland, Scotland, Germany, and Russia. Russia is the leading exporter of peat for fuel, at more than 90 million metric tons per year. Ireland's \"Bord na Móna\" (\"peat board\") was one of the first companies to mechanically harvest peat, which is being phased out.\n\nThe other major use of dried peat is as a soil amendment (sold as \"moss peat\" or \"sphagnum peat\") to increase the soil's capacity to retain moisture and enrich the soil. It is also used as a mulch. Some distilleries, notably in the Islay whisky-producing region, use the smoke from peat fires to dry the barley used in making Scotch whisky.\n\nOnce the peat has been extracted it can be difficult to restore the wetland, since peat accumulation is a slow process. More than 90% of the bogs in England have been damaged or destroyed. In 2011 plans for the elimination of peat in gardening products were announced by the UK government.\n\nThe peat in bogs is an important place for the storage of carbon. If the peat decays, carbon dioxide would be released to the atmosphere, contributing to global warming. Undisturbed, bogs function as a carbon sink. As one example, the peatlands of the former Soviet Union were calculated to be removing 52 Tg of carbon per year from the atmosphere.\n\nPeat bogs are also important in storing fresh water, particularly in the headwaters of large rivers. Even the enormous Yangtze River arises in the Ruoergai peatland near its headwaters in Tibet.\n\nBlueberries, cranberries, cloudberries, huckleberries, and lingonberries are harvested from the wild in bogs. Bog oak, wood that has been partially preserved by bogs, has been used in the manufacture of furniture.\n\nSphagnum bogs are also used for outdoor recreation, with activities including ecotourism and hunting. For example, many popular canoe routes in northern Canada include areas of peatland. Some other activities, such as all-terrain vehicle use, are especially damaging to bogs.\n\nThe anaerobic environment and presence of tannic acids within bogs can result in the remarkable preservation of organic material. Finds of such material have been made in Denmark, Germany, Ireland, Russia, and the United Kingdom. Some bogs have preserved bog-wood such as ancient oak logs useful in dendrochronology, and they have yielded extremely well preserved bog bodies, with hair, organs, and skin intact, buried there thousands of years ago after apparent Germanic and Celtic human sacrifice. Excellent examples of such human specimens are Haraldskær Woman and Tollund Man in Denmark, and Lindow man found at Lindow Common in England. At Céide Fields in County Mayo in Ireland, a 5,000-year-old neolithic farming landscape has been found preserved under a blanket bog, complete with field walls and hut sites. One ancient artifact found in various bogs is bog butter, large masses of fat, usually in wooden containers. These are thought to have been food stores, of both butter and tallow.\n\n\n\n"}
{"id": "19075690", "url": "https://en.wikipedia.org/wiki?curid=19075690", "title": "British Oceanographic Data Centre", "text": "British Oceanographic Data Centre\n\nThe British Oceanographic Data Centre (BODC) is a national facility for looking after and distributing data about the marine environment. BODC is the designated marine science data centre for the UK and part of the Natural Environment Research Council (NERC). The centre provides a resource for science, education and industry, as well as the general public. BODC is hosted by the National Oceanography Centre (NOC) — primarily at its facility in Liverpool, with small number of its staff in Southampton.\n\nThe origins of BODC go back to 1969 when NERC created the British Oceanographic Data Service (BODS). Located at the National Institute of Oceanography, Wormley in Surrey, its purpose was to: \n\nIn 1975 BODS was transferred to Bidston Observatory on the Wirral, near Liverpool, as part of the newly formed Institute of Oceanographic Sciences. The following year BODS became the Marine Information and Advisory Service (MIAS). Its primary activity was to manage the data collected from weather ships, oil rigs and data buoys.\nThe data banking component of MIAS was restructured to form BODC in April 1989. Its mission was to 'operate as a world-class data centre in support of UK marine science'. BODC pioneered a start to finish approach to marine data management. This involved:\nIn December 2004, BODC moved to the purpose-built Joseph Proudman Building on the campus of the University of Liverpool. A small number of its staff are based in the National Oceanography Centre (NOC), Southampton.\n\n\nBODC is one of six designated data centres that manage NERC's environmental data and has a number of national roles and responsibilities:\n\nBODC's international roles and responsibilities include:\n\nThe following are a selection of the projects that BODC is or has been involved with:\n\n"}
{"id": "12701886", "url": "https://en.wikipedia.org/wiki?curid=12701886", "title": "Brittle Power", "text": "Brittle Power\n\nBrittle Power: Energy Strategy for National Security is a 1982 book by Amory B. Lovins and L. Hunter Lovins, prepared originally as a Pentagon study and re-released in 2001 following the September 11 attacks. The book argues that U.S. domestic energy infrastructure is very vulnerable to disruption, by accident or malice, often even more so than imported oil. According to the authors, a resilient energy system is feasible, costs less, works better, is favoured in the market, but is rejected by U.S. policy. In the preface to the 2001 edition, Lovins explains that these themes are still very current.\n\nLovins argues that the United States has for decades been running on energy that is \"brittle\" (easily shattered by accident or malice) and that this poses a grave and growing threat to national security, life, and liberty.\n\nLovins explains that this danger comes not from hostile ideology but from misapplied technology. The size, complexity, pattern, and control structure of the electrical power system make it \"inherently\" vulnerable to large-scale failures. The same is true of the technologies that deliver coal, gas, and oil for running buildings, vehicles, and industries. Reliance on these delicately poised energy systems has unwittingly put at risk the entire American way of life.\n\nLovins' detailed research shows that these vulnerabilities are increasingly being exploited. \"Brittle Power\" documents many significant assaults on energy facilities, other than during a war, in 40 countries and within the United States, in some 24 states.\n\nLovins claims that most energy utilities and governments are unsuccessfully trying to build high technical reliability into power plants so large that their cost of failure is unacceptable. A resilient energy supply system may instead consist of numerous, relatively small sustainable energy sources, each with a low individual cost of failure. Potentially these individual energy source \"are \"renewable\": they harness the energy of sun, wind, water, or farm and forestry wastes, rather than that of depletable fuels\".\n\nAmory B. and L. Hunter Lovins reiterated the main message of \"Brittle Power\" in \"Terrorism and Brittle Technology\", Chapter 3 in Albert H. Teich's book \"Technology and the Future\" (2003):\n\nThe foundation of a secure energy system is to need less energy in the first place, then to get it from sources that are inherently invulnerable because they're diverse, dispersed, renewable, and mainly local. They're secure not because they're American but because of their design. Any highly centralised energy system – pipelines, nuclear plants, refineries – invite devastating attack. But invulnerable alternatives don't, and can't, fail on a large scale.\nIn his book \"Reinventing Fire\" (2011), Amory B. Lovins puts forward a vision of networked island-able microgrids where energy is generated locally from solar power, wind power, and other resources, and used by super-efficient buildings. When each building, or neighborhood, is generating its own power, with links to other “islands” of power, the security of the entire network is greatly enhanced. Lovins has said that in the face of hundreds of blackouts in 2005, Cuba reorganized its electricity transmission system into networked microgrids and cut the occurrence of blackouts to zero within two years, limiting damage even after two hurricanes. Denmark has performed tests of islanding a region (\"cell\") to maintain robustness with smaller local assets rather than large centralized ones.\n\n\"Brittle Power\" is 500 pages long and has 1,200 references. It has been summarised and referred to in several other publications:\n\n"}
{"id": "10522152", "url": "https://en.wikipedia.org/wiki?curid=10522152", "title": "Counterhegemony", "text": "Counterhegemony\n\nCounter-hegemony refers to attempts to critique or dismantle hegemonic power. In other words, it is a confrontation and/or opposition to existing status quo and its legitimacy in politics, but can also be observed in various other spheres of life, such as history, media, music, etc. Neo-Gramscian theorist Nicola Pratt (2004) has described counter-hegemony as \"a creation of an alternative hegemony on the terrain of civil society in preparation for political change\". \n\nAccording to Theodore H. Cohn, \"a counterhegemony is an alternative ethical view of society that poses a challenge to the dominant bourgeois-led view\".\n\nIf a counterhegemony grows large enough it is able to subsume and replace the \"historic bloc\" it was born in. Neo-Gramscians use the Machiavellian terms \"war of position\" and \"war of movement\" to explain how this is possible. In a war of position a counterhegemonic movement attempts, through persuasion or propaganda, to increase the number of people who share its view on the hegemonic order; in a war of movement the counterhegemonic tendencies which have grown large enough overthrow, violently or democratically, the current hegemony and establish themselves as a new historic bloc.\n\nAn example of counter-hegemony in politics is the \"anti-globalization movement\". An example of counter-hegemony in media could be a documentary questioning the government’s involvement in a war.\n\nThe term \"hegemony\" came from the writings of Karl Marx and was conceptualized by Antonio Gramsci, a Marxist social philosopher who lived in Mussolini's Italy. Because Gramsci was a Marxist, he subscribed to the basic Marxist premise of the historical dialectic. Therefore, according to classic Marxist theories, societies will transform over time from oppressive economic systems to more and more liberating ones, until finally the state of communism in society is reached. \n\nIn his writings Gramsci claims that intellectuals create both hegemony and counter-hegemony. He argues that \"there is no organization without intellectuals,\" for to be without them is to be without \"the theoretical aspect of the theory-practice nexus essential to all effective organizations\".\n\nCounter-hegemonic groups of people do not start off as radical or extremist groups; they encourage people to share their view against hegemony through the use of persuasion and/or propaganda whilst raising awareness. One view describes the possibility that once the counter-hegemonic group has gained enough support and consensus against the current powers, they would then attempt to overthrow them, whether through violence or democracy. Depending on whether full power is given to each individual, or if it is kept among a close few could be the deterministic factor between a decentralized government and a dictatorship.\n\n"}
{"id": "34783762", "url": "https://en.wikipedia.org/wiki?curid=34783762", "title": "Decoupling (meteorology)", "text": "Decoupling (meteorology)\n\nIn weather forecasting, decoupling is boundary-layer decoupling of atmospheric layers over land at night. During the day when the sun shines and warms the land, air at the surface of the earth is heated and rises. This rising air mixes the atmosphere near the earth. At night this process stops and air near the surface cools as the land loses heat by radiating in the infrared. If winds are light, air near the surface of the earth can become much colder, compared to the air above it, than if more mixing of air layers occurred.\n"}
{"id": "34045714", "url": "https://en.wikipedia.org/wiki?curid=34045714", "title": "Dumitru Dan", "text": "Dumitru Dan\n\nDumitru (Demetre) Dan (14 July 1890 – 4 December 1978) was a Romanian geographer, professor of geography and globe-trotter. He took the world record for walking around the world.\n\nIn 1908, the Touring club de France announced a contest for walking around the world, with an offered prize of 100,000 francs. Dumitru Dan and his countrymen Paul Pârvu, George Negreanu and Alexandru Pascu were all students in Paris at the time, and they decided to take up the challenge, using their own money and resources.\n\nThe group returned to Romania to prepare, learning additional languages, studying cartography, and doing weight training and exercises for two hours a day. They also walked 45 km per day, travelling plains, hills and mountains in all seasons. They had learned Romanian folk songs and dances and also to play the flute and accordion so that they could support their endeavor along the way by performing their native folklore.\n\nIn 1910, wearing native garb and walking in sandals, the group set out on their voyage with a dog, Harap. Much of the route was covered between 1910 and 1916, but Dan's companions did not survive to see the trip's completion in 1923.\n\nOn 17 July 1911, in India, the group were invited to the palace of the Rajah of Bombay. After sharing a meal there, the walkers went in group to purchase supplies for their journey in town. Alexandru Pascu remained to tell the Rajah about their adventures to that point. His companions returned to find Pascu and their hosts had indulged in and passed out from opium. Although their hosts felt Pascu would likely recover, he never woke. He died as a result of opium poisoning and was buried in India.\n\nGeorge Negreanu died two years into the voyage, when the then trio were crossing a narrow mountain pass in the Nanling Mountains in China. In a brief rain during which they were forced to feel their way with canes, Negreanu - at the head of the group - fell onto rocks from a sharp precipice. Dan and Pârvu carried him to a local settlement and then to a hospital but there was no doctor available there to assist. By the time medical care was found, Negreanu was dead.\n\nPârvu found himself unable to continue in Jacksonville, Florida. Several years earlier, he had wounded his legs during the more than 2600 km walk across Alaska (coming from China and Siberian Russia) on the route Kotzebue, Fairbanks, Anchorage, Yakutat, Gustavus, Skagway and Juneau, and he developed gangrene. Advised by doctors to stop, he remained in Florida with the dog Harap. Subsequently, both of his legs were amputated, but it was not sufficient to save him. In May 1915, he died.\n\nDumitru Dan was forced to put his trip on hold due to the outbreak of World War I, but he completed his voyage by 1923. The value of the prize was substantially diminished, given inflation over 15 years and from the War. According to a 2011 Romanian news article, it had been worth about 500,000 Euros in 1908, but was only worth 40,000 at the time it was awarded. He crossed five continents over three oceans, through 76 countries and over 1,500 cities, wearing out 497 pairs of shoes.\n\nDan died in Buzău, Romania in 1978. He is buried in the Heroes' Cemetery. In 1985, his record was entered into the Guinness Book.\n\n\n"}
{"id": "8453001", "url": "https://en.wikipedia.org/wiki?curid=8453001", "title": "Economy 10", "text": "Economy 10\n\nEconomy 10 is the name of a tariff provided by United Kingdom electricity suppliers created in 2004. Similar to the Economy 7 this is designed to be used with high thermal mass heating such as storage heaters, underfloor heating, and is also used with electrical boilers driving radiators or water-based heat stores.\n\nIn contrast to Economy 7, which only provides off-peak electricity during nighttime hours, Economy 10 tariffs provide ten hours of off-peak heating split between night, afternoon and evening. The advantage of this scheme is that by matching the storage periods better to the times when heat is required, less heat needs to be stored during the day, when there may be no demand for heating. The afternoon and evening periods also provide a top-up to heating systems at off-peak prices.\n\nOff-peak electricity costs can be half of the peak prices, but many Economy 10 tariffs levy an increased standing daily charge.\n\nThe structure of the ten off-peak hours is determined by the local distribution network operator rather than the electricity supply company and they vary across the fourteen regions in the UK. Times are switched automatically, and it is common for the switching times to be locked to either GMT or BST when the meter is installed. Some metering systems use a radio teleswitch controlled by the supplier to vary switching times, and effect the daylight saving time switchover twice a year. Economy 10 thus requires a special multi-tariff meter, different from an Economy 7 meter.\n\nFor example, customers in central Scotland (within the Scottish Power Energy Networks DNO area) are on the following times:\n\nAnother set of times (West Midlands) are:\n\n\n"}
{"id": "28812639", "url": "https://en.wikipedia.org/wiki?curid=28812639", "title": "Emmonsite", "text": "Emmonsite\n\nEmmonsite, also known as durdenite, is an iron tellurite mineral with the formula: Fe(TeO)·2(HO). Emmonsite forms triclinic crystals. It is of a yellowish-green color, with a vitreous luster, and a hardness of 5 on the Moh scale. \n\nEmmonsite was first described in 1885 for an occurrence in the Tombstone District, Cochise County, Arizona. It was named for the American geologist, Samuel Franklin Emmons, (1841–1911), of the United States Geological Survey.\n\nEmmonsite is found, often with quartz or cerussite in the Tombstone, Arizona area. It is also associated with native tellurium, tellurite, native gold, pyrite, rodalquilarite, mackayite, sonoraite, cuzticite and eztlite.\n\n"}
{"id": "7710657", "url": "https://en.wikipedia.org/wiki?curid=7710657", "title": "George Wharton James", "text": "George Wharton James\n\nGeorge Wharton James (27 September 1858 – 1923) was an American popular lecturer, photographer, journalist and editor. Born in Lincolnshire, England, he emigrated to the United States as a young man after being ordained as a Methodist minister.\n\nHe served in parishes in Nevada and Southern California, gradually beginning his journalism and writing career. An editor of two magazines, he also wrote more than 40 books and many articles and pamphlets on California and the American Southwest.\n\nGeorge Wharton James was born in Lincolnshire, England. He married and was ordained as a Methodist minister. He and his wife immigrated to the United States in 1881.\n\nHe served in parishes in Nevada and southern California. However, in 1889 his wife sued for divorce, accusing him of committing numerous acts of adultery. He was tried by the Methodist Church, charged with real estate fraud, using faked credentials, and sexual misconduct. He was defrocked, although he was later reinstated.\n\nIn addition to writing his own books, James was associate editor of \"The Craftsman\" (1904–05), and editor of \"Out West\" (1912–14). In the style of the times, he was a popular lecturer in the region. He also lectured at both the Panama-Pacific and Panama-California expositions 1915–16.\n\nJames had a long-running feud with Charles Fletcher Lummis, a California writer with similar regional interests. Both men also explored the American Southwest, becoming acquainted with Father Anton Docher, a French-born missionary priest who served at Pueblo of Isleta in New Mexico for 34 years.\n\nJames' books included the well-received \"The Wonders of the Colorado Desert\" (1906), \"Through Ramona's Country\" (1909), \"In and Out of the Old Missions of California\" (1905), and \"The Lake of the Sky\" (1915). Characteristics of his writing included romanticism, an enthusiasm for natural environments, idealization of aboriginal lifeways, and promotion of health fads.\n\nAfter his divorce, James married again, living in Pasadena, California with his second wife at 1098 North Raymond Avenue. Writer Lawrence Clark Powell later described James' home as serving as \"a kind of museum salon in the same way that El Alisal served as the center for his rival booster Lummis' Los Angeles followers. He founded the Pasadena Browning Society, and the Anti-Whispering Society. According to Powell, the Anti-Whispering Society was \"devoted to the suppression of (1) talking audiences, (2) peanut fiends, and (3) crying babies.\"\n\n\n\n\n"}
{"id": "7096967", "url": "https://en.wikipedia.org/wiki?curid=7096967", "title": "Geothermal heat pump", "text": "Geothermal heat pump\n\nA geothermal heat pump or ground source heat pump (GSHP) is a central heating and/or cooling system that transfers heat to or from the ground.\n\nIt uses the earth all the time, without any intermittency, as a heat source (in the winter) or a heat sink (in the summer). This design takes advantage of the moderate temperatures in the ground to boost efficiency and reduce the operational costs of heating and cooling systems, and may be combined with solar heating to form a geosolar system with even greater efficiency. They are also known by other names, including geoexchange, earth-coupled, earth energy systems. The engineering and scientific communities prefer the terms \"\"geoexchange\" or \"ground source heat pumps\"\" to avoid confusion with traditional geothermal power, which uses a high temperature heat source to generate electricity. Ground source heat pumps harvest heat absorbed at the Earth's surface from solar energy. The temperature in the ground below is roughly equal to the mean annual air temperature at that latitude at the surface.\n\nDepending on latitude, the temperature beneath the upper of Earth's surface maintains a nearly constant temperature between 10 and 16 °C (50 and 60 °F), if the temperature is undisturbed by the presence of a heat pump. Like a refrigerator or air conditioner, these systems use a heat pump to force the transfer of heat from the ground. Heat pumps can transfer heat from a cool space to a warm space, against the natural direction of flow, or they can enhance the natural flow of heat from a warm area to a cool one. The core of the heat pump is a loop of refrigerant pumped through a vapor-compression refrigeration cycle that moves heat. Air-source heat pumps are typically more efficient at heating than pure electric heaters, even when extracting heat from cold winter air, although efficiencies begin dropping significantly as outside air temperatures drop below 5 °C (41 °F). A ground source heat pump exchanges heat with the ground. This is much more energy-efficient because underground temperatures are more stable than air temperatures through the year. Seasonal variations drop off with depth and disappear below to due to thermal inertia. Like a cave, the shallow ground temperature is warmer than the air above during the winter and cooler than the air in the summer. A ground source heat pump extracts ground heat in the winter (for heating) and transfers heat back into the ground in the summer (for cooling). Some systems are designed to operate in one mode only, heating or cooling, depending on climate.\n\nGeothermal pump systems reach fairly high coefficient of performance (CoP), 3 to 6, on the coldest of winter nights, compared to 1.75–2.5 for air-source heat pumps on cool days. Ground source heat pumps (GSHPs) are among the most energy-efficient technologies for providing HVAC and water heating.\n\nSetup costs are higher than for conventional systems, but the difference is usually returned in energy savings in 3 to 10 years, and even shorter lengths of time with federal, state and utility tax credits and incentives. Geothermal heat pump systems are reasonably warranted by manufacturers, and their working life is estimated at 25 years for inside components and 50+ years for the ground loop. As of 2004, there are over one million units installed worldwide providing 12 GW of thermal capacity, with an annual growth rate of 10%.\n\nSome confusion exists with regard to the terminology of heat pumps and the use of the term \"\"geothermal\". \"Geothermal\" derives from the Greek and means \"Earth heat\" – which geologists and many laymen understand as describing hot rocks, volcanic activity or heat derived from deep within the earth. Though some confusion arises when the term \"geothermal\" is also used to apply to temperatures within the first 100 metres of the surface, this is \"Earth heat\"\" all the same, though it is largely influenced by stored energy from the sun.\n\nThe heat pump was described by Lord Kelvin in 1853 and developed by Peter Ritter von Rittinger in 1855. After experimenting with a freezer, Robert C. Webber built the first direct exchange ground-source heat pump in the late 1940s. The first successful commercial project was installed in the Commonwealth Building (Portland, Oregon) in 1948, and has been designated a National Historic Mechanical Engineering Landmark by ASME. The technology became popular in Sweden in the 1970s, and has been growing slowly in worldwide acceptance since then. Open loop systems dominated the market until the development of polybutylene pipe in 1979 made closed loop systems economically viable. As of 2004, there are over a million units installed worldwide providing 12 GW of thermal capacity. Each year, about 80,000 units are installed in the US (geothermal energy is used in all 50 U.S. states today, with great potential for near-term market growth and savings) and 27,000 in Sweden. In Finland, a geothermal heat pump was the most common heating system choice for new detached houses between 2006 and 2011 with market share exceeding 40%.\n\nHeat pumps provide winter heating by extracting heat from a source and transferring it into a building. Heat can be extracted from any source, no matter how cold, but a warmer source allows higher efficiency. A ground source heat pump uses the top layer of the earth's crust as a source of heat, thus taking advantage of its seasonally moderated temperature.\n\nIn the summer, the process can be reversed so the heat pump extracts heat from the building and transfers it to the ground. Transferring heat to a cooler space takes less energy, so the cooling efficiency of the heat pump gains benefits from the lower ground temperature.\n\nGround source heat pumps employ a heat exchanger in contact with the ground or groundwater to extract or dissipate heat. This component accounts for anywhere from a fifth to half of the total system cost, and would be the most cumbersome part to repair or replace. Correctly sizing this component is necessary to assure long-term performance: the energy efficiency of the system improves with roughly 4% for every degree Celsius that is won through correct sizing, and the underground temperature balance must be maintained through proper design of the whole system. Incorrect design can result in the system freezing after a number of years or very inefficient system performance; thus accurate system design is critical to a successful system \n\nShallow horizontal heat exchangers experience seasonal temperature cycles due to solar gains and transmission losses to ambient air at ground level. These temperature cycles lag behind the seasons because of thermal inertia, so the heat exchanger will harvest heat deposited by the sun several months earlier, while being weighed down in late winter and spring, due to accumulated winter cold. Deep vertical systems deep rely on migration of heat from surrounding geology, unless they are recharged annually by solar recharge of the ground or exhaust heat from air conditioning systems.\n\nSeveral major design options are available for these, which are classified by fluid and layout. Direct exchange systems circulate refrigerant underground, closed loop systems use a mixture of anti-freeze and water, and open loop systems use natural groundwater.\n\nThe direct exchange geothermal heat pump (DX) is the oldest type of geothermal heat pump technology. The ground-coupling is achieved through a single loop, circulating refrigerant, in direct thermal contact with the ground (as opposed to a combination of a refrigerant loop and a water loop). The refrigerant leaves the heat pump cabinet, circulates through a loop of copper tube buried underground, and exchanges heat with the ground before returning to the pump. The name \"direct exchange\" refers to heat transfer between the refrigerant loop and the ground without the use of an intermediate fluid. There is no direct interaction between the fluid and the earth; only heat transfer through the pipe wall. Direct exchange heat pumps are not to be confused with \"water-source heat pumps\" or \"water loop heat pumps\" since there is no water in the ground loop. ASHRAE defines the term ground-coupled heat pump to encompass closed loop and direct exchange systems, while excluding open loops.\n\nDirect exchange systems are more efficient and have potentially lower installation costs than closed loop water systems. Copper's high thermal conductivity contributes to the higher efficiency of the system, but heat flow is predominantly limited by the thermal conductivity of the ground, not the pipe. The main reasons for the higher efficiency are the elimination of the water pump (which uses electricity), the elimination of the water-to-refrigerant heat exchanger (which is a source of heat losses), and most importantly, the latent heat phase change of the refrigerant in the ground itself.\n\nHowever, in case of leakage there is virtually no risk of contaminating the ground or the ground water. Contrary to water-source geothermal systems, direct exchange systems do not contain antifreeze. So, in case of a refrigerant leakage, the refrigerant currently used in most systems – R-410A – would immediately vaporize and seek the atmosphere. This is due to the low boiling point of R-410A: . R-410A refrigerant replaces larger volumes of antifreeze mixtures used in water-source geothermal systems and presents no threat to aquifers or to the ground itself.\n\nWhile they require more refrigerant and their tubing is more expensive per foot, a direct exchange earth loop is shorter than a closed water loop for a given capacity. A direct exchange system requires only 15 to 40% of the length of tubing and half the diameter of drilled holes, and the drilling or excavation costs are therefore lower. Refrigerant loops are less tolerant of leaks than water loops because gas can leak out through smaller imperfections. This dictates the use of brazed copper tubing, even though the pressures are similar to water loops. The copper loop must be protected from corrosion in acidic soil through the use of a sacrificial anode or other cathodic protection.\n\nThe U.S. Environmental Protection Agency conducted field monitoring of a direct geoexchange heat pump water heating system in a commercial application. The EPA reported that the system saved 75% of the electrical energy that would have been required by an electrical resistance water heating unit. According to the EPA, if the system is operated to capacity, it can avoid the emission of up to 7,100 pounds of CO and 15 pounds of NO each year per ton of compressor capacity (or 42,600 lbs. of CO and 90 lbs. of NO for a typical 6 ton system).\n\nIn Northern climates, although the earth temperature is cooler, so is the incoming water temperature, which enables the high efficiency systems to replace more energy than would otherwise be required of electric or fossil fuel fired systems. Any temperature above is sufficient to evaporate the refrigerant, and the direct exchange system can harvest energy through ice.\n\nIn extremely hot climates with dry soil, the addition of an auxiliary cooling module as a second condenser in line between the compressor and the earth loops increases efficiency and can further reduce the amount of earth loop to be installed.\n\nMost installed systems have two loops on the ground side: the primary refrigerant loop is contained in the appliance cabinet where it exchanges heat with a secondary water loop that is buried underground. The secondary loop is typically made of high-density polyethylene pipe and contains a mixture of water and anti-freeze (propylene glycol, denatured alcohol or methanol). Monopropylene glycol has the least damaging potential when it might leak into the ground, and is therefore the only allowed anti-freeze in ground sources in an increasing number of European countries. After leaving the internal heat exchanger, the water flows through the secondary loop outside the building to exchange heat with the ground before returning. The secondary loop is placed below the frost line where the temperature is more stable, or preferably submerged in a body of water if available. Systems in wet ground or in water are generally more efficient than drier ground loops since water conducts and stores heat better than solids in sand or soil. If the ground is naturally dry, soaker hoses may be buried with the ground loop to keep it wet.\nClosed loop systems need a heat exchanger between the refrigerant loop and the water loop, and pumps in both loops. Some manufacturers have a separate ground loop fluid pump pack, while some integrate the pumping and valving within the heat pump. Expansion tanks and pressure relief valves may be installed on the heated fluid side. Closed loop systems have lower efficiency than direct exchange systems, so they require longer and larger pipe to be placed in the ground, increasing excavation costs.\n\nClosed loop tubing can be installed horizontally as a loop field in trenches or vertically as a series of long U-shapes in wells (see below). The size of the loop field depends on the soil type and moisture content, the average ground temperature and the heat loss and or gain characteristics of the building being conditioned. A rough approximation of the initial soil temperature is the average daily temperature for the region.\n\nA vertical closed loop field is composed of pipes that run vertically in the ground. A hole is bored in the ground, typically deep. Pipe pairs in the hole are joined with a U-shaped cross connector at the bottom of the hole. The borehole is commonly filled with a bentonite grout surrounding the pipe to provide a thermal connection to the surrounding soil or rock to improve the heat transfer. Thermally enhanced grouts are available to improve this heat transfer. Grout also protects the ground water from contamination, and prevents artesian wells from flooding the property. Vertical loop fields are typically used when there is a limited area of land available. Bore holes are spaced at least 5–6 m apart and the depth depends on ground and building characteristics. For illustration, a detached house needing 10 kW (3 ton) of heating capacity might need three boreholes deep. (A ton of heat is 12,000 British thermal units per hour (BTU/h) or 3.5 kilowatts.) During the cooling season, the local temperature rise in the bore field is influenced most by the moisture travel in the soil. Reliable heat transfer models have been developed through sample bore holes as well as other tests.\n\nA horizontal closed loop field is composed of pipes that run horizontally in the ground. A long horizontal trench, deeper than the frost line, is dug and U-shaped or slinky coils are placed horizontally inside the same trench. Excavation for shallow horizontal loop fields is about half the cost of vertical drilling, so this is the most common layout used wherever there is adequate land available. For illustration, a detached house needing 10 kW (3 ton) of heating capacity might need three loops long of NPS 3/4 (DN 20) or NPS 1.25 (DN 32) polyethylene tubing at a depth of .\n\nThe depth at which the loops are placed significantly influences the energy consumption of the heat pump in two opposite ways: shallow loops tend to indirectly absorb more heat from the sun, which is helpful, especially when the ground is still cold after a long winter. On the other hand, shallow loops are also cooled down much more readily by weather changes, especially during long cold winters, when heating demand peaks. Often, the second effect is much greater than the first one, leading to higher costs of operation for the more shallow ground loops. This problem can be reduced by increasing both the depth and the length of piping, thereby significantly increasing costs of installation. However, such expenses might be deemed feasible, as they may result in lower operating costs. Recent studies show that utilization of a non-homogeneous soil profile with a layer of low conductive material above the ground pipes can help mitigate the adverse effects of shallow pipe burial depth. The intermediate blanket with lower conductivity than the surrounding soil profile demonstrated the potential to increase the energy extraction rates from the ground to as high as 17% for a cold climate and about 5–6% for a relatively moderate climate.\n\nA slinky (also called coiled) closed loop field is a type of horizontal closed loop where the pipes overlay each other (not a recommended method). The easiest way of picturing a slinky field is to imagine holding a slinky on the top and bottom with your hands and then moving your hands in opposite directions. A slinky loop field is used if there is not adequate room for a true horizontal system, but it still allows for an easy installation. Rather than using straight pipe, slinky coils use overlapped loops of piping laid out horizontally along the bottom of a wide trench. Depending on soil, climate and the heat pump's run fraction, slinky coil trenches can be up to two thirds shorter than traditional horizontal loop trenches. Slinky coil ground loops are essentially a more economical and space efficient version of a horizontal ground loop.\n\nAs an alternative to trenching, loops may be laid by mini horizontal directional drilling (mini-HDD). This technique can lay piping under yards, driveways, gardens or other structures without disturbing them, with a cost between those of trenching and vertical drilling. This system also differs from horizontal & vertical drilling as the loops are installed from one central chamber, further reducing the ground space needed. Radial drilling is often installed retroactively (after the property has been built) due to the small nature of the equipment used and the ability to bore beneath existing constructions.\n\nA closed pond loop is not common because it depends on proximity to a body of water, where an open loop system is usually preferable. A pond loop may be advantageous where poor water quality precludes an open loop, or where the system heat load is small. A pond loop consists of coils of pipe similar to a slinky loop attached to a frame and located at the bottom of an appropriately sized pond or water source.\n\nIn an open loop system (also called a groundwater heat pump), the secondary loop pumps natural water from a well or body of water into a heat exchanger inside the heat pump. ASHRAE calls open loop systems \"groundwater heat pumps\" or \"surface water heat pumps\", depending on the source. Heat is either extracted or added by the primary refrigerant loop, and the water is returned to a separate injection well, irrigation trench, tile field or body of water. The supply and return lines must be placed far enough apart to ensure thermal recharge of the source. Since the water chemistry is not controlled, the appliance may need to be protected from corrosion by using different metals in the heat exchanger and pump. Limescale may foul the system over time and require periodic acid cleaning. This is much more of a problem with cooling systems than heating systems. Also, as fouling decreases the flow of natural water, it becomes difficult for the heat pump to exchange building heat with the groundwater. If the water contains high levels of salt, minerals, iron bacteria or hydrogen sulfide, a closed loop system is usually preferable.\n\nDeep lake water cooling uses a similar process with an open loop for air conditioning and cooling. Open loop systems using ground water are usually more efficient than closed systems because they are better coupled with ground temperatures. Closed loop systems, in comparison, have to transfer heat across extra layers of pipe wall and dirt.\n\nA growing number of jurisdictions have outlawed open-loop systems that drain to the surface because these may drain aquifers or contaminate wells. This forces the use of more environmentally sound injection wells or a closed loop system.\n\nA standing column well system is a specialized type of open loop system. Water is drawn from the bottom of a deep rock well, passed through a heat pump, and returned to the top of the well, where traveling downwards it exchanges heat with the surrounding bedrock. The choice of a standing column well system is often dictated where there is near-surface bedrock and limited surface area is available. A standing column is typically not suitable in locations where the geology is mostly clay, silt, or sand. If bedrock is deeper than from the surface, the cost of casing to seal off the overburden may become prohibitive.\n\nA multiple standing column well system can support a large structure in an urban or rural application. The standing column well method is also popular in residential and small commercial applications. There are many successful applications of varying sizes and well quantities in the many boroughs of New York City, and is also the most common application in the New England states. This type of ground source system has some heat storage benefits, where heat is rejected from the building and the temperature of the well is raised, within reason, during the summer cooling months which can then be harvested for heating in the winter months, thereby increasing the efficiency of the heat pump system. As with closed loop systems, sizing of the standing column system is critical in reference to the heat loss and gain of the existing building. As the heat exchange is actually with the bedrock, using water as the transfer medium, a large amount of production capacity (water flow from the well) is not required for a standing column system to work. However, if there is adequate water production, then the thermal capacity of the well system can be enhanced by discharging a small percentage of system flow during the peak Summer and Winter months.\n\nSince this is essentially a water pumping system, standing column well design requires critical considerations to obtain peak operating efficiency. Should a standing column well design be misapplied, leaving out critical shut-off valves for example, the result could be an extreme loss in efficiency and thereby cause operational cost to be higher than anticipated.\n\nThe heat pump is the central unit that becomes the heating and cooling plant for the building. Some models may cover space heating, space cooling, (space heating via conditioned air, hydronic systems and / or radiant heating systems), domestic or pool water preheat (via the desuperheater function), demand hot water, and driveway ice melting all within one appliance with a variety of options with respect to controls, staging and zone control. The heat may be carried to its end use by circulating water or forced air. Almost all types of heat pumps are produced for commercial and residential applications.\n\n\"Liquid-to-air\" heat pumps (also called \"water-to-air\") output forced air, and are most commonly used to replace legacy forced air furnaces and central air conditioning systems. There are variations that allow for split systems, high-velocity systems, and ductless systems. Heat pumps cannot achieve as high a fluid temperature as a conventional furnace, so they require a higher volume flow rate of air to compensate. When retrofitting a residence, the existing duct work may have to be enlarged to reduce the noise from the higher air flow.\n\"Liquid-to-water\" heat pumps (also called \"water-to-water\") are hydronic systems that use water to carry heating or cooling through the building. Systems such as radiant underfloor heating, baseboard radiators, conventional cast iron radiators would use a liquid-to-water heat pump. These heat pumps are preferred for pool heating or domestic hot water pre-heat. Heat pumps can only heat water to about efficiently, whereas a boiler normally reaches . Legacy radiators designed for these higher temperatures may have to be doubled in numbers when retrofitting a home. A hot water tank will still be needed to raise water temperatures above the heat pump's maximum, but pre-heating will save 25–50% of hot water costs.\n\nGround source heat pumps are especially well matched to underfloor heating and baseboard radiator systems which only require warm temperatures 40 °C (104 °F) to work well. Thus they are ideal for open plan offices. Using large surfaces such as floors, as opposed to radiators, distributes the heat more uniformly and allows for a lower water temperature. Wood or carpet floor coverings dampen this effect because the thermal transfer efficiency of these materials is lower than that of masonry floors (tile, concrete). Underfloor piping, ceiling or wall radiators can also be used for cooling in dry climates, although the temperature of the circulating water must be above the dew point to ensure that atmospheric humidity does not condense on the radiator.\n\nCombination heat pumps are available that can produce forced air and circulating water simultaneously and individually. These systems are largely being used for houses that have a combination of air and liquid conditioning needs, for example central air conditioning and pool heating.\n\nThe efficiency of ground source heat pumps can be greatly improved by using seasonal thermal energy storage and interseasonal heat transfer. Heat captured and stored in thermal banks in the summer can be retrieved efficiently in the winter. Heat storage efficiency increases with scale, so this advantage is most significant in commercial or district heating systems.\n\nGeosolar combisystems have been used to heat and cool a greenhouse using an aquifer for thermal storage. In summer, the greenhouse is cooled with cold ground water. This heats the water in the aquifer which can become a warm source for heating in winter. The combination of cold and heat storage with heat pumps can be combined with water/humidity regulation. These principles are used to provide renewable heat and renewable cooling to all kinds of buildings.\n\nAlso the efficiency of existing small heat pump installations can be improved by adding large, cheap, water filled solar collectors. These may be integrated into a to-be-overhauled parking lot, or in walls or roof constructions by installing one-inch PE pipes into the outer layer.\n\nThe net thermal efficiency of a heat pump should take into account the efficiency of electricity generation and transmission, typically about 30%. Since a heat pump moves three to five times more heat energy than the electric energy it consumes, the total energy output is much greater than the electrical input. This results in net thermal efficiencies greater than 300% as compared to radiant electric heat being 100% efficient. Traditional combustion furnaces and electric heaters can never exceed 100% efficiency.\n\nGeothermal heat pumps can reduce energy consumption— and corresponding air pollution emissions—up to 44% compared to air source heat pumps and up to 72% compared to electric resistance heating with standard air-conditioning equipment.\n\nThe dependence of net thermal efficiency on the electricity infrastructure tends to be an unnecessary complication for consumers and is not applicable to hydroelectric power, so performance of heat pumps is usually expressed as the ratio of heating output or heat removal to electricity input. Cooling performance is typically expressed in units of BTU/hr/watt as the energy efficiency ratio (EER), while heating performance is typically reduced to dimensionless units as the coefficient of performance (COP). The conversion factor is 3.41 BTU/hr/watt. Performance is influenced by all components of the installed system, including the soil conditions, the ground-coupled heat exchanger, the heat pump appliance, and the building distribution, but is largely determined by the \"lift\" between the input temperature and the output temperature.\n\nFor the sake of comparing heat pump appliances to each other, independently from other system components, a few standard test conditions have been established by the American Refrigerant Institute (ARI) and more recently by the International Organization for Standardization. Standard ARI 330 ratings were intended for closed loop ground-source heat pumps, and assume secondary loop water temperatures of for air conditioning and for heating. These temperatures are typical of installations in the northern US. Standard ARI 325 ratings were intended for open loop ground-source heat pumps, and include two sets of ratings for groundwater temperatures of and . ARI 325 budgets more electricity for water pumping than ARI 330. Neither of these standards attempt to account for seasonal variations. Standard ARI 870 ratings are intended for direct exchange ground-source heat pumps. ASHRAE transitioned to ISO 13256-1 in 2001, which replaces ARI 320, 325 and 330. The new ISO standard produces slightly higher ratings because it no longer budgets any electricity for water pumps.\n\nEfficient compressors, variable speed compressors and larger heat exchangers all contribute to heat pump efficiency. Residential ground source heat pumps on the market today have standard COPs ranging from 2.4 to 5.0 and EERs ranging from 10.6 to 30. To qualify for an Energy Star label, heat pumps must meet certain minimum COP and EER ratings which depend on the ground heat exchanger type. For closed loop systems, the ISO 13256-1 heating COP must be 3.3 or greater and the cooling EER must be 14.1 or greater.\n\nActual installation conditions may produce better or worse efficiency than the standard test conditions. COP improves with a lower temperature difference between the input and output of the heat pump, so the stability of ground temperatures is important. If the loop field or water pump is undersized, the addition or removal of heat may push the ground temperature beyond standard test conditions, and performance will be degraded. Similarly, an undersized blower may allow the plenum coil to overheat and degrade performance.\n\nSoil without artificial heat addition or subtraction and at depths of several metres or more remains at a relatively constant temperature year round. This temperature equates roughly to the average annual air-temperature of the chosen location, usually at a depth of in the northern US. Because this temperature remains more constant than the air temperature throughout the seasons, geothermal heat pumps perform with far greater efficiency during extreme air temperatures than air conditioners and air-source heat pumps.\n\nStandards ARI 210 and 240 define Seasonal Energy Efficiency Ratio (SEER) and Heating Seasonal Performance Factors (HSPF) to account for the impact of seasonal variations on air source heat pumps. These numbers are normally not applicable and should not be compared to ground source heat pump ratings. However, Natural Resources Canada has adapted this approach to calculate typical seasonally adjusted HSPFs for ground-source heat pumps in Canada. The NRC HSPFs ranged from 8.7 to 12.8 BTU/hr/watt (2.6 to 3.8 in nondimensional factors, or 255% to 375% seasonal average electricity utilization efficiency) for the most populated regions of Canada. When combined with the thermal efficiency of electricity, this corresponds to net average thermal efficiencies of 100% to 150%.\n\nThe US Environmental Protection Agency (EPA) has called ground source heat pumps the most energy-efficient, environmentally clean, and cost-effective space conditioning systems available. Heat pumps offer significant emission reductions potential, particularly where they are used for both heating and cooling and where the electricity is produced from renewable resources.\n\nGSHPs have unsurpassed thermal efficiencies and produce zero emissions locally, but their electricity supply includes components with high greenhouse gas emissions, unless the owner has opted for a 100% renewable energy supply. Their environmental impact therefore depends on the characteristics of the electricity supply and the available alternatives. \n\nThe GHG emissions savings from a heat pump over a conventional furnace can be calculated based on the following formula:\n\nformula_1\n\nGround-source heat pumps always produce fewer greenhouse gases than air conditioners, oil furnaces, and electric heating, but natural gas furnaces may be competitive depending on the greenhouse gas intensity of the local electricity supply. In countries like Canada and Russia with low emitting electricity infrastructure, a residential heat pump may save 5 tons of carbon dioxide per year relative to an oil furnace, or about as much as taking an average passenger car off the road. But in cities like Beijing or Pittsburgh that are highly reliant on coal for electricity production, a heat pump may result in 1 or 2 tons more carbon dioxide emissions than a natural gas furnace. For areas not served by utility natural gas infrastructure, however, no better alternative exists.\n\nThe fluids used in closed loops may be designed to be biodegradable and non-toxic, but the refrigerant used in the heat pump cabinet and in direct exchange loops was, until recently, chlorodifluoromethane, which is an ozone depleting substance. Although harmless while contained, leaks and improper end-of-life disposal contribute to enlarging the ozone hole. For new construction, this refrigerant is being phased out in favor of the ozone-friendly but potent greenhouse gas R410A. The EcoCute water heater is an air-source heat pump that uses carbon dioxide as its working fluid instead of chlorofluorocarbons. Open loop systems (i.e. those that draw ground water as opposed to closed loop systems using a borehole heat exchanger) need to be balanced by reinjecting the spent water. This prevents aquifer depletion and the contamination of soil or surface water with brine or other compounds from underground.\n\nBefore drilling, the underground geology needs to be understood, and drillers need to be prepared to seal the borehole, including preventing penetration of water between strata. The unfortunate example is a geothermal heating project in Staufen im Breisgau, Germany which seems the cause of considerable damage to historical buildings there. In 2008, the city centre was reported to have risen 12 cm, after initially sinking a few millimeters. The boring tapped a naturally pressurized aquifer, and via the borehole this water entered a layer of anhydrite, which expands when wet as it forms gypsum. The swelling will stop when the anhydrite is fully reacted, and reconstruction of the city center \"is not expedient until the uplift ceases.\" By 2010 sealing of the borehole had not been accomplished. By 2010, some sections of town had risen by 30 cm.\n\nGround-source heat pump technology, like building orientation, is a natural building technique (bioclimatic building).\n\nGround source heat pumps are characterized by high capital costs and low operational costs compared to other HVAC systems. Their overall economic benefit depends primarily on the relative costs of electricity and fuels, which are highly variable over time and across the world. Based on recent prices, ground-source heat pumps currently have lower operational costs than any other conventional heating source almost everywhere in the world. Natural gas is the only fuel with competitive operational costs, and only in a handful of countries where it is exceptionally cheap, or where electricity is exceptionally expensive. In general, a homeowner may save anywhere from 20% to 60% annually on utilities by switching from an ordinary system to a ground-source system.\n\nCapital costs and system lifespan have received much less study until recently, and the return on investment is highly variable. The most recent data from an analysis of 2011–2012 incentive payments in the state of Maryland showed an average cost of residential systems of $1.90 per watt, or about $26,700 for a typical (4 ton) home system. An older study found the total installed cost for a system with 10 kW (3 ton) thermal capacity for a detached rural residence in the US averaged $8000–$9000 in 1995 US dollars. More recent studies found an average cost of $14,000 in 2008 US dollars for the same size system. The US Department of Energy estimates a price of $7500 on its website, last updated in 2008. One source in Canada placed prices in the range of $30,000–$34,000 Canadian dollars. The rapid escalation in system price has been accompanied by rapid improvements in efficiency and reliability. Capital costs are known to benefit from economies of scale, particularly for open loop systems, so they are more cost-effective for larger commercial buildings and harsher climates. The initial cost can be two to five times that of a conventional heating system in most residential applications, new construction or existing. In retrofits, the cost of installation is affected by the size of living area, the home's age, insulation characteristics, the geology of the area, and location of the property. Proper duct system design and mechanical air exchange should be considered in the initial system cost.\n\nCapital costs may be offset by government subsidies; for example, Ontario offered $7000 for residential systems installed in the 2009 fiscal year. Some electric companies offer special rates to customers who install a ground-source heat pump for heating or cooling their building. Where electrical plants have larger loads during summer months and idle capacity in the winter, this increases electrical sales during the winter months. Heat pumps also lower the load peak during the summer due to the increased efficiency of heat pumps, thereby avoiding costly construction of new power plants. For the same reasons, other utility companies have started to pay for the installation of ground-source heat pumps at customer residences. They lease the systems to their customers for a monthly fee, at a net overall saving to the customer.\n\nThe lifespan of the system is longer than conventional heating and cooling systems. Good data on system lifespan is not yet available because the technology is too recent, but many early systems are still operational today after 25–30 years with routine maintenance. Most loop fields have warranties for 25 to 50 years and are expected to last at least 50 to 200 years. Ground-source heat pumps use electricity for heating the house. The higher investment above conventional oil, propane or electric systems may be returned in energy savings in 2–10 years for residential systems in the US. If compared to natural gas systems, the payback period can be much longer or non-existent. The payback period for larger commercial systems in the US is 1–5 years, even when compared to natural gas. Additionally, because geothermal heat pumps usually have no outdoor compressors or cooling towers, the risk of vandalism is reduced or eliminated, potentially extending a system's lifespan.\n\nGround source heat pumps are recognized as one of the most efficient heating and cooling systems on the market. They are often the second-most cost effective solution in extreme climates (after co-generation), despite reductions in thermal efficiency due to ground temperature. (The ground source is warmer in climates that need strong air conditioning, and cooler in climates that need strong heating.) The financial viability of these systems depends on the adequate sizing of ground heat exchangers (GHEs), which generally contribute the most to the overall capital costs of GSHP systems.\n\nCommercial systems maintenance costs in the US have historically been between $0.11 to $0.22 per m per year in 1996 dollars, much less than the average $0.54 per m per year for conventional HVAC systems.\n\nGovernments that promote renewable energy will likely offer incentives for the consumer (residential), or industrial markets. For example, in the United States, incentives are offered both on the state and federal levels of government. In the United Kingdom the Renewable Heat Incentive provides a financial incentive for generation of renewable heat based on metered readings on an annual basis for 20 years for commercial buildings. The domestic Renewable Heat Incentive is due to be introduced in Spring 2014 for seven years and be based on deemed heat.\n\nBecause of the technical knowledge and equipment needed to design and size the system properly (and install the piping if heat fusion is required), a GSHP system installation requires a professional's services. Several installers have published real-time views of system performance in an online community of recent residential installations. The International Ground Source Heat Pump Association (IGSHPA), Geothermal Exchange Organization (GEO), the Canadian GeoExchange Coalition and the Ground Source Heat Pump Association maintain listings of qualified installers in the US, Canada and the UK. Furthermore, detailed analysis of Soil thermal conductivity for horizontal systems and formation thermal conductivity for vertical systems will generally result in more accurately design systems with a higher efficiency.\n\n\n"}
{"id": "29868842", "url": "https://en.wikipedia.org/wiki?curid=29868842", "title": "Green Power Partnership", "text": "Green Power Partnership\n\nThe United States Environmental Protection Agency's Green Power Partnership is a voluntary program that supports the organizational procurement of green power by offering expert advice, technical support, tools and resources. Green Power Partnership provides public health and environmental benefits by expanding U.S. renewable energy markets through the voluntary use of green power.\n\nThe EPA defines 'green power' as a subset of renewable energy and \"represents those renewable energy resources and technologies that provide the highest environmental benefit\", with electricity produced from solar, wind, geothermal, biogas, biomass, and low-impact small hydroelectric sources listed as types of green power.\n\nKey program elements include:\n\nGreen Power Communities are defined by the United States Environmental Protection Agency (EPA) as \"towns, villages, cities, counties, or tribal governments in which the local government, businesses, and residents collectively buy green power in amounts that meet or exceed EPA's Green Power Community purchase requirements.\" \n\n\nOfficial website\n"}
{"id": "30840139", "url": "https://en.wikipedia.org/wiki?curid=30840139", "title": "Himalayan silver fir forests", "text": "Himalayan silver fir forests\n\nThe forest's vegetation is dominated by the \"Abies pindrow\" (Himalayan Silver Fir) species of conifer trees.\n\nIt is located in the Western Himalayas region of the Himalayan Range System in southern Asia.\n\nHimalayan Silver Fir Forests are found from the Gandak River region in Nepal, through the higher elevations of northwestern India, to montane northern Pakistan.\n\n\n"}
{"id": "20616378", "url": "https://en.wikipedia.org/wiki?curid=20616378", "title": "INOGATE", "text": "INOGATE\n\nINOGATE was an international energy co-operation programme between the European Union (EU), the littoral states of the Black and Caspian seas and their neighbouring countries. The programme was operational from 1996 to 2016.\n\nINOGATE was one of the longest running energy technical assistance programmes funded by the EU. Up to 2006, it was funded by the Tacis Regional Cooperation Programme, and as of 2007, it was funded by the European Neighbourhood and Partnership Instrument (ENPI) under the ENPI-East Regional Indicative Programme 2007-2010 and 2010-2013. EuropeAid supported the programme through the ENPI and the Development Cooperation Instrument. The coordinating INOGATE Technical Secretariat was discontinued in April 2016.\n\nINOGATE originated in 1995 as an EU support mechanism dealing with INterstate Oil and GAs Transportation to Europe (whence it derived its name as an acronym). It was particularly concerned initially with oil and gas pipelines running from and through Eastern Europe and the Caucasus to the EU. In 2001, a formal 'Umbrella Agreement' was signed by twenty one countries in Kiev, to cooperate on pipeline development and enhancement. Following conferences in Baku, Azerbaijan in 2004 and in Astana, Kazakhstan, INOGATE evolved into a broader energy partnership between the EU and countries of the former Soviet Union, (excluding the Russian Federation and the Baltic States, but including Turkey), and concentrating on four key topics:\n\nThe INOGATE partner countries are:\n\nTurkey is an INOGATE partner country and was, therefore, regularly invited to attend INOGATE meetings but it is not a beneficiary country.\n\nThe Baku Initiative resulted from a policy dialogue on energy cooperation between the European Union and the INOGATE partner countries. The initiative was announced on 13 November 2004 at the Energy Ministerial Conference in Baku. A second Ministerial Conference was held in Astana on 30 November 2006 which confirmed the broader remit of the INOGATE programme to cover a range of energy issues in the Partner Countries and their cooperation in these issues with the EU.\n\nOne objective of the Baku Initiative is to enhance integration of the energy markets of participating countries with the EU energy market, so as to create transparent energy markets, capable of attracting investment and enhancing security of energy supply. (Other aspects of the Initiative were concerned with transport issues: see main article). The partner countries agreed objectives to harmonise legal and technical standards so as to create a functioning integrated energy market in accordance with EU and international legal and regulatory frameworks; to increase the safety and security of energy supplies by extending and modernising existing infrastructure, substituting outdated power generation infrastructures with environmentally friendly systems; the development of new infrastructures and implementation of modern monitoring systems; improvement of energy supply and demand management through the integration of efficient and sustainable energy systems; and promotion of the financing of commercially and environmentally viable energy projects of common interest. A 'road-map' towards achievement of these and allied objectives was adopted at the Astana Ministerial Conference.\n\nThe coordinating body of INOGATE was the Technical Secretariat based in Kiev, Ukraine, with regional offices in Tbilisi, Georgia, and in Tashkent, Uzbekistan. The INOGATE Secretariat had an important role in ensuring the visibility, the coordination,coherence and sustainability of the programme as a whole as well as the individual projects.\n\nThe Baku Initiative is implemented through the four working groups, each of which contains members from all partner countries:\n\n\nA number of specific projects, financed by the ENPI, were associated with the INOGATE programme, and are listed at the INOGATE webportal. These projects included initiatives to improve safety and security of oil and gas transit structures, to develop coordinated national energy policies in Central Asia, and to support market integration and sustainable energy initiatives in partner countries.\n\nINOGATE was closely involved with the development of the Eastern European Regional Centre for Hydrocarbon Metrology in Boyarka, Ukraine, which 'provides state-of-the-art calibration services for gas transfer within the region and to other countries'. This should form an important element in the development of a regional energy market operating to Western European standards.\n\nThe INOGATE programme hadan ongoing schedule of conferences and publications. Its webportal contained regularly updated information and news on energy developments in partner countries and relevant EU initiatives and policies.\n\nThe programme supported reduction in Partner Countries, their dependency on fossil fuels and imports, improvement of the security of their energy supply, and climate change mitigation.\n\nAn 'INOGATE Project' was any EU-funded regional energy project supporting energy policy cooperation in the INOGATE Partner countries in the topics of the four INOGATE areas of cooperation. Since the inception of the Programme in 1996, approximately 61 INOGATE projects have been implemented.\n\nIn the period 2010-2015 there were three active INOGATE projects:\n\n\nIn addition to the INOGATE projects, there was also the following INOGATE-related project (not directly promoted as an INOGATE project but using INOGATE funding and encouraging the same objectives):\n\n\nEach of the individual projects was responsible for ensuring visibility of their actions. However the ITS project promoted their results in the wider framework of the INOGATE Programme.\n\n\n\n"}
{"id": "15490", "url": "https://en.wikipedia.org/wiki?curid=15490", "title": "Indus River", "text": "Indus River\n\nThe Indus River (locally called Darya-e-Sindh) is one of the longest rivers in Asia. Originating in the Tibetan Plateau in the vicinity of Lake Manasarovar, the river runs a course through the Ladakh region of Kashmir, towards Gilgit-Baltistan and the Hindukush ranges, and then flows in a southerly direction along the entire length of Pakistan to merge into the Arabian Sea near the port city of Karachi in Sindh. It is the longest river and national river of Pakistan.\n\nThe river has a total drainage area exceeding . Its estimated annual flow stands at around , twice that of the Nile River and three times that of the Tigris and Euphrates rivers combined, making it the twenty-first largest river in the world in terms of annual flow. The Zanskar is its left bank tributary in Ladakh. In the plains, its left bank tributary is the Panjnad which itself has five major tributaries, namely, the Chenab, Jhelum, the Ravi, the Beas, and the Sutlej. Its principal right bank tributaries are the Shyok, the Gilgit, the Kabul, the Gomal, and the Kurram. Beginning in a mountain spring and fed with glaciers and rivers in the Himalayas, the river supports ecosystems of temperate forests, plains and arid countryside.\n\nThe northern part of the Indus Valley, with its tributaries, forms the Punjab region, while the lower course of the Indus is known as Sindh and ends in a large delta. The river has historically been important to many cultures of the region. The 3rd millennium BC saw the rise of a major urban civilization of the Bronze Age. During the 2nd millennium BC, the Punjab region was mentioned in the hymns of the Hindu Rigveda as \"Sapta Sindhu\" and the Zoroastrian Avesta as \"Hapta Hindu\" (both terms meaning \"seven rivers\"). Early historical kingdoms that arose in the Indus Valley include Gandhāra, and the Ror dynasty of Sauvīra. The Indus River came into the knowledge of the West early in the Classical Period, when King Darius of Persia sent his Greek subject Scylax of Caryanda to explore the river, ca. 515 BC.\n\nThis river was known to the ancient Indians in Sanskrit as \"Sindhu\" and the Achaemenids as \"Hiduš\" (, \"H-i-du-u-š\"), which was regarded by both of them as \"the border river\". The variation between the two names is explained by the Old Iranian sound change \"*s\" > \"h\", which occurred between 850–600 BCE according to Asko Parpola. From the Achaemenid Empire, the name passed to the Greeks as \"Indós\" (Ἰνδός). It was adopted by the Romans as \"Indus\".\n\nThe meaning of \"Sindhu\" as a \"large body of water, sea, or ocean\" is a later meaning in Classical Sanskrit. A later Persian name for the river was \"Darya\", which similarly has the connotations of large body of water and sea.\n\nOther variants of the name \"Sindhu\" include Assyrian \"Sinda\" (as early as the 7th century BC), Persian \"Ab-e-sind\", Pashto \"Abasind\", Arab \"Al-Sind\", Chinese \"Sintow\", and Javanese \"Santri\".\n\n\"India\" is a Greek and Latin term for \"the country of the River Indus\". The region through which the river drains into sea is called Sindh and owes its name to the river (Sanskrit: \"Sindhu\").\n\nMegasthenes' book \"Indica\" derives its name from the river's Greek name, \"Indós\" (\"Ἰνδός\"), and describes Nearchus's contemporaneous account of how Alexander the Great crossed the river. The ancient Greeks referred to the Indians (people of present-day northwest India and Pakistan) as \"Indói\" (\"Ἰνδοί\"), literally meaning \"the people of the Indus\".\n\nRigveda also describes several mythical rivers, including one named \"Sindhu\". The Rigvedic \"Sindhu\" is thought to be the present-day Indus river and is attested 176 times in its text – 95 times in the plural, more often used in the generic meaning. In the Rigveda, notably in the later hymns, the meaning of the word is narrowed to refer to the Indus river in particular, as in the list of rivers mentioned in the hymn of \"Nadistuti sukta\". The Rigvedic hymns apply a feminine gender to all the rivers mentioned therein but \"Sindhu\" is the only river attributed the masculine gender which means Sindhu is the warrior and greatest among all other rivers in whole world.\n\nIn other languages of the region, the river is known as सिन्धु \"(Sindhu)\" in Hindi and Nepali, سنڌو (\"Sindhu\") in Sindhi, (\"Sindh\") in Shahmukhi Punjabi, ਸਿੰਧ ਨਦੀ (\"Sindh Nadī\") in Gurmukhī Punjabi, اباسين (\"Abāsin\" lit. \"Father of Rivers\") in Pashto, نهر السند (\"Nahar al-Sind\") in Arabic, སེང་གེ་གཙང་པོ། (\"singi khamban\" lit. \"Lion River\" or \"Lion Spring\") in Tibetan, (\"Yìndù\") in Chinese, and \"Nilab\" in Turki.\n\nThe Indus River provides key water resources for Pakistan's economy – especially the \"breadbasket\" of Punjab province, which accounts for most of the nation's agricultural production, and Sindh. The word Punjab means \"land of five rivers\" and the five rivers are Jhelum, Chenab, Ravi, Beas and Sutlej, all of which finally flow into the Indus. The Indus also supports many heavy industries and provides the main supply of potable water in Pakistan.\n\nThe ultimate source of the Indus is in Tibet; the river begins at the confluence of the Sengge Zangbo and Gar Tsangpo rivers that drain the Nganglong Kangri and Gangdise Shan (Gang Rinpoche, Mt. Kailas) mountain ranges. The Indus then flows northwest through Ladakh and Baltistan into Gilgit, just south of the Karakoram range. The Shyok, Shigar and Gilgit rivers carry glacial waters into the main river. It gradually bends to the south, coming out of the hills between Peshawar and Rawalpindi. The Indus passes gigantic gorges deep near the Nanga Parbat massif. It flows swiftly across Hazara and is dammed at the Tarbela Reservoir. The Kabul River joins it near Attock. The remainder of its route to the sea is in the plains of the Punjab and Sindh, where the flow of the river becomes slow and highly braided. It is joined by the Panjnad at Mithankot. Beyond this confluence, the river, at one time, was named the \"Satnad River\" (\"sat\" = \"seven\", \"nadī\" = \"river\"), as the river now carried the waters of the Kabul River, the Indus River and the five Punjab rivers. Passing by Jamshoro, it ends in a large delta to the east of Thatta.\n\nThe Indus is one of the few rivers in the world to exhibit a tidal bore. The Indus system is largely fed by the snows and glaciers of the Himalayas, Karakoram and the Hindu Kush ranges of Tibet, the Indian states of Jammu and Kashmir and Himachal Pradesh and Gilgit-Baltistan region of Pakistan. The flow of the river is also determined by the seasons – it diminishes greatly in the winter, while flooding its banks in the monsoon months from July to September. There is also evidence of a steady shift in the course of the river since prehistoric times – it deviated westwards from flowing into the Rann of Kutch and adjoining Banni grasslands after the 1816 earthquake. Presently, Indus water flows in to the Rann of Kutch during its floods breaching flood banks.\n\nThe traditional source of the river is the \"Senge Khabab\" or \"Lion's Mouth\", a perennial spring, not far from the sacred Mount Kailash marked by a long low line of Tibetan chortens. There are several other tributaries nearby, which may possibly form a longer stream than Senge Khabab, but unlike the Senge Khabab, are all dependent on snowmelt. The Zanskar River, which flows into the Indus in Ladakh, has a greater volume of water than the Indus itself before that point.\n\nThe major cities of the Indus Valley Civilisation, such as Harappa and Mohenjo-daro, date back to around 3300 BC, and represent some of the largest human habitations of the ancient world. The Indus Valley Civilisation extended from across northeast Afghanistan to Pakistan and northwest India, with an upward reach from east of Jhelum River to Ropar on the upper Sutlej. The coastal settlements extended from Sutkagan Dor at the Pakistan, Iran border to Kutch in modern Gujarat, India. There is an Indus site on the Amu Darya at Shortughai in northern Afghanistan, and the Indus site Alamgirpur at the Hindon River is located only from Delhi. To date, over 1,052 cities and settlements have been found, mainly in the general region of the Ghaggar-Hakra River and its tributaries. Among the settlements were the major urban centres of Harappa and Mohenjo-daro, as well as Lothal, Dholavira, Ganeriwala, and Rakhigarhi. Only 90–96 of more than 800 known Indus Valley sites have been discovered on the Indus and its tributaries. The Sutlej, now a tributary of the Indus, in Harappan times flowed into the Ghaggar-Hakra River, in the watershed of which were more Harappan sites than along the Indus.\n\nMost scholars believe that settlements of Gandhara grave culture of the early Indo-Aryans flourished in Gandhara from 1700 BC to 600 BC, when Mohenjo-daro and Harappa had already been abandoned.\n\nThe word \"India\" is derived from the Indus River. In ancient times, \"India\" initially referred to those regions immediately along the east bank of the Indus, but by 300 BC, Greek writers including Herodotus and Megasthenes were applying the term to the entire subcontinent that extends much farther eastward.\n\nThe lower basin of the Indus forms a natural boundary between the Iranian Plateau and the Indian subcontinent; this region embraces all or parts of the Pakistani provinces Balochistan, Khyber Pakhtunkhwa, Punjab and Sindh and the countries Afghanistan and India. It was crossed by the invading armies of Alexander, but after his Macedonians conquered the west bank—joining it to the Hellenic Empire, they elected to retreat along the southern course of the river, ending Alexander's Asian campaign. The Indus plains were later dominated by the Persian empire and then the Kushan empire. Over several centuries Muslim armies of Muhammad bin Qasim, Mahmud of Ghazni, Mohammed Ghori, Tamerlane and Babur crossed the river to invade the inner regions of the Punjab and points farther south and east\n\nThe Indus river feeds the Indus submarine fan, which is the second largest sediment body on the Earth. It consists of around 5 million cubic kilometres of material eroded from the mountains. Studies of the sediment in the modern river indicate that the Karakoram Mountains in northern Pakistan and India are the single most important source of material, with the Himalayas providing the next largest contribution, mostly via the large rivers of the Punjab (Jhelum, Ravi, Chenab, Beas and Sutlej). Analysis of sediments from the Arabian Sea has demonstrated that prior to five million years ago the Indus was not connected to these Punjab rivers which instead flowed east into the Ganga and were captured after that time. Earlier work showed that sand and silt from western Tibet was reaching the Arabian Sea by 45 million years ago, implying the existence of an ancient Indus River by that time. The delta of this proto-Indus river has subsequently been found in the Katawaz Basin, on the Afghan-Pakistan border.\n\nIn the Nanga Parbat region, the massive amounts of erosion due to the Indus river following the capture and rerouting through that area is thought to bring middle and lower crustal rocks to the surface.\n\nIn November 2011, satellite images showed that the Indus river had re-entered India, feeding Great Rann of Kutch, Little Rann of Kutch and a lake near Ahmedabad known as Nal Sarovar. Heavy rains had left the river basin along with the Lake Manchar, Lake Hemal and Kalri Lake (all in modern-day Pakistan) inundated. This happened two centuries after the Indus river shifted its course westwards following the 1819 Rann of Kutch earthquake.\n\nThe Induan Age at start of the Triassic Period of geological time is named for the Indus region.\n\nAccounts of the Indus valley from the times of Alexander's campaign indicate a healthy forest cover in the region, which has now considerably receded. The Mughal Emperor Babur writes of encountering rhinoceroses along its bank in his memoirs (the Baburnama). Extensive deforestation and human interference in the ecology of the Shivalik Hills has led to a marked deterioration in vegetation and growing conditions. The Indus valley regions are arid with poor vegetation. Agriculture is sustained largely due to irrigation works.\nThe Indus river and its watershed has a rich biodiversity. It is home to around 25 amphibian species and 147 fish species, 22 of which are only found in the Indus.\n\nThe Indus River Dolphin (\"Platanista indicus minor\") is a sub-species of dolphin found only in the Indus River. It formerly also occurred in the tributaries of the Indus river. According to the World Wildlife Fund it is one of the most threatened cetaceans with only about 1,000 still existing.\n\nPalla fish, Tenualosa ilisha, of the river is a delicacy for people living along the river. The population of fish in the river is moderately high, with Sukkur, Thatta, and Kotri being the major fishing centres – all in the lower Sindh course. As a result, damming and irrigation has made fish farming an important economic activity. Located southeast of Karachi, the large delta has been recognized by conservationists as one of the world's most important ecological regions. Here, the river turns into many marshes, streams and creeks and meets the sea at shallow levels. Here, marine fishes are found in abundance, including pomfret and prawns.\n\nThe Indus is the most important supplier of water resources to the Punjab and Sindh plains – it forms the backbone of agriculture and food production in Pakistan. The river is especially critical since rainfall is meagre in the lower Indus valley. Irrigation canals were first built by the people of the Indus Valley Civilisation, and later by the engineers of the Kushan Empire and the Mughal Empire. Modern irrigation was introduced by the British East India Company in 1850 – the construction of modern canals accompanied with the restoration of old canals. The British supervised the construction of one of the most complex irrigation networks in the world. The Guddu Barrage is long – irrigating Sukkur, Jacobabad, Larkana and Kalat. The Sukkur Barrage serves over .\n\nAfter Pakistan came into existence, a water control treaty signed between India and Pakistan in 1960 guaranteed that Pakistan would receive water from the Indus River and its two tributaries the Jhelum River & the Chenab River independently of upstream control by India.\n\nThe Indus Basin Project consisted primarily of the construction of two main dams, the Mangla Dam built on the Jhelum River and the Tarbela Dam constructed on the Indus River, together with their subsidiary dams. The Pakistan Water and Power Development Authority undertook the construction of the Chashma-Jhelum link canal – linking the waters of the Indus and Jhelum rivers – extending water supplies to the regions of Bahawalpur and Multan. Pakistan constructed the Tarbela Dam near Rawalpindi – standing long and high, with an long reservoir. It supports the Chashma Barrage near Dera Ismail Khan for irrigation use and flood control and the Taunsa Barrage near Dera Ghazi Khan which also produces 100,000 kilowatts of electricity. The Kotri Barrage near Hyderabad is long and provides additional water supplies for Karachi. The extensive linking of tributaries with the Indus has helped spread water resources to the valley of Peshawar, in the Khyber Pakhtunkhwa. The extensive irrigation and dam projects provide the basis for Pakistan's large production of crops such as cotton, sugarcane and wheat. The dams also generate electricity for heavy industries and urban centers.\n\nThe inhabitants of the regions are mainly Muslim as Pakistan is an Islamic country through which the Indus river passes and forms a major natural feature and resource are diverse in ethnicity, religion, national and linguistic backgrounds. On the northern course of the river in the state of Jammu and Kashmir in India, live the Buddhist people of Ladakh, of Tibetan stock, and the Dards of Indo-Aryan or Dardic stock and practising Islam. Then it descends into Baltistan, northern Pakistan passing the main Balti city of Skardu. A river from Dubair Bala also drains into it at Dubair Bazar. People living in this area are mainly Kohistani and speak the Kohistani language. Major areas through which the Indus river passes in Kohistan are Dasu, Pattan and Dubair. As it continues through Pakistan, the Indus river forms a distinctive boundary of ethnicity and cultures – upon the western banks the population is largely Pashtun, Baloch, and of other Iranian stock. The eastern banks are largely populated by people of Indo-Aryan stock, such as the Punjabis and the Sindhis. In northern Punjab and the Khyber Pakhtunkhwa, ethnic Pashtun tribes live alongside Dardic people in the hills (Khowar, Kalash, Shina, etc.), Burushos (in Hunza), and Punjabi people.\n\nThe people living along the Indus river speak Punjabi and Sindhi on the eastern side (in Punjab and Sindh provinces respectively), Pushto plus Balochi as well as Barohi (in Khyber Pakhtoonkha and Baluchistan provinces). In the province of Sindh, the upper third of the river is inhabited by people speaking Saraiki; which is a somewhat transitional dialect of the Punjabi and Sindhi languages.\n\nThe Indus is a strategically vital resource for Pakistan's economy and society. After Pakistan and India declared Independence from the British Raj, the use of the waters of the Indus and its five eastern tributaries became a major dispute between India and Pakistan. The irrigation canals of the Sutlej valley and the Bari Doab were split – with the canals lying primarily in Pakistan and the headwork dams in India disrupting supply in some parts of Pakistan. The concern over India building large dams over various Punjab rivers that could undercut the supply flowing to Pakistan, as well as the possibility that India could divert rivers in the time of war, caused political consternation in Pakistan. Holding diplomatic talks brokered by the World Bank, India and Pakistan signed the Indus Waters Treaty in 1960. The treaty gave India control of the three easternmost rivers of the Punjab, the Sutlej, the Beas and the Ravi, while Pakistan gained control of the three western rivers, the Jhelum, the Chenab and the Indus. India retained the right to use of the western rivers for non-irrigation projects.\nThere are concerns that extensive deforestation, industrial pollution and global warming are affecting the vegetation and wildlife of the Indus delta, while affecting agricultural production as well. There are also concerns that the Indus River may be shifting its course westwards – although the progression spans centuries. On numerous occasions, sediment clogging owing to poor maintenance of canals has affected agricultural production and vegetation. In addition, extreme heat has caused water to evaporate, leaving salt deposits that render lands useless for cultivation.\n\nOriginally, the delta used to receive almost all of the water from the Indus river, which has an annual flow of approximately , and is accompanied by 400 million tonnes of silt. Since the 1940s, dams, barrages and irrigation works have been constructed on the river Indus. The Indus Basin Irrigation System is the \"largest contiguous irrigation system developed over the past 140 years\" anywhere in the world. This has reduced the flow of water and by 2018, the average annual flow of water below the Kotri barrage was , and annual amount of silt discharged was estimated to be . Substantial annual chemical/dissolved load of 46 million tons in the river basin is unable to reach the sea as it is getting trapped in the irrigated areas by increasing the ground water salinity. Nearly 90% of the water from Indus River System has been harnessed already in Pakistan and India leaving rest of water to join the sea. Any further utilization of the river basin water is not economically feasible. The result has been catastrophic for both the environment and the local population. The reduction of freshwater due to the dams also increases salinity, making the surface and ground water available in the delta area unsuitable for the freshwater species and crops. As a result, the 2010 Pakistan floods were considered \"good news\" for the ecosystem and population of the river delta as they brought much needed fresh water. In case of the Indus dolphin, the damming of the river has isolated the delta dolphin population from those dolphins upstream.\n\nThe Tibetan Plateau contains the world's third-largest store of ice. Qin Dahe, the former head of the China Meteorological Administration, said the recent fast pace of melting and warmer temperatures will be good for agriculture and tourism in the short term, but issued a strong warning:\n\n\"There is insufficient data to say what will happen to the Indus,\" says David Grey, the World Bank's senior water advisor in South Asia. \"But we all have very nasty fears that the flows of the Indus could be severely, severely affected by glacier melt as a consequence of climate change,\" and reduced by perhaps as much as 50 percent. \"Now what does that mean to a population that lives in a desert [where], without the river, there would be no life? I don't know the answer to that question,\" he says. \"But we need to be concerned about that. Deeply, deeply concerned.\"\n\nU.S. diplomat Richard Holbrooke said, shortly before his death in 2010, that he believed that falling water levels in the Indus River \"could very well precipitate World War III.\"\n\nOver the years factories on the banks of the Indus River have increased levels of water pollution in the river and the atmosphere around it. High levels of pollutants in the river have led to the deaths of endangered Indus River Dolphin. The Sindh Environmental Protection Agency has ordered polluting factories around the river to shut down under the Pakistan Environmental Protection Act, 1997. Death of the Indus River Dolphin has also been attributed to fishermen using poison to kill fish and scooping them up. As a result, the government banned fishing from Guddu Barrage to Sukkur.\n\nComing second after the Yangtze, and together with 9 other rivers, the Indus transports 90 % of all the plastic that reaches the oceans.\n\nFrequently, Indus river is prone to moderate to severe flooding. In July 2010, following abnormally heavy monsoon rains, the Indus River rose above its banks and started flooding. The rain continued for the next two months, devastating large areas of Pakistan. In Sindh, the Indus burst its banks near Sukkur on 8 August, submerging the village of Mor Khan Jatoi. In early August, the heaviest flooding moved southward along the Indus River from severely affected northern regions toward western Punjab, where at least of cropland was destroyed, and the southern province of Sindh. , over two thousand people had died and over a million homes had been destroyed since the flooding began.\n\nThe 2011 Sindh floods began during the Pakistani monsoon season in mid-August 2011, resulting from heavy monsoon rains in Sindh, eastern Balochistan, and southern Punjab. The floods caused considerable damage; an estimated 434 civilians were killed, with 5.3 million people and 1,524,773 homes affected. Sindh is a fertile region and often called the \"breadbasket\" of the country; the damage and toll of the floods on the local agrarian economy was said to be extensive. At least of arable land were inundated. The flooding followed the previous year's floods, which devastated a large part of the country. Unprecedented torrential monsoon rains caused severe flooding in 16 districts of Sindh.\n\nIn Pakistan currently there are three barrages on the Indus: Guddu barrage, Sukkur Barrage, and Kotri barrage (also called Ghulam Muhammad barrage). There are some bridges on river Indus, such as, Dadu Moro Bridge, Larkana Khairpur Indus River Bridge, Thatta-Sujawal bridge, Jhirk-Mula Katiar bridge and recently planned Kandhkot-Ghotki bridge.\n\nKala Bagh Barrage, Chasma Barrage, and Taunsa Barrage are also built in Punjab on the Indus.\n\nTarbela Dam in Pakistan is constructed on the Indus River, while the controversial Kalabagh dam is also being constructed on Indus river.\n\n"}
{"id": "8297429", "url": "https://en.wikipedia.org/wiki?curid=8297429", "title": "Karni Mata Temple", "text": "Karni Mata Temple\n\nKarni Mata Temple (Hindi: करणी माता मंदिर) is a Hindu temple dedicated to Karni Mata at Deshnoke, 30 km from Bikaner, in Rajasthan, India. It is also known as the Temple of Rats.\n\nThe temple is famous for the approximately 25,000 black rats that live, and are revered, in the temple. These holy rats are called kabbas, and many people travel great distances to pay their respects. The temple draws visitors from across the country for blessings, as well as curious tourists from around the world.\n\nLegend has it that Laxman, son of Karni mata , drowned in a pond in Kapil Sarovar in Kolayat Tehsil while he was attempting to drink from it. Karni Mata implored Yama, the god of death, to revive him. First refusing, Yama eventually relented, permitting Laxman and all of Karni Mata's male children to be reincarnated as rats.\n\nEating food that has been nibbled on by the rats is considered to be a \"high honour\". If one of them is killed, it must be replaced with another one made of solid silver.\n\nThe building was completed in its current form in the early 20th century in the late Mughal style by Maharaja Ganga Singh of Bikaner.\n\nIn front of the temple is a beautiful marble facade, which has solid silver doors built by Maharaja Ganga Singh. Across the doorway are more silver doors with panels depicting the various legends of the Goddess. The image of the Goddess is enshrined in the inner sanctum.\n\nOut of all of the thousands of rats in the temple, there are a few white rats, which are considered to be especially holy. They are believed to be the manifestations of Karni Mata herself and her four sons. Sighting them is a special blessing and visitors put in extensive efforts to bring them forth, offering prasad, a sweet holy food.\n\nThe temple is opened to the public early in the morning at 04:00. Charan priests perform Mangla-Ki-Aarti and offer bhog (special food) in worship. Devotees make offerings to the rats, which roam about the temple in large numbers and are considered auspicious. There are two kinds of offerings made: the 'dwar-bhent' is attributed to the priests and the workers, while the 'kalash-bhent' is utilised for the temple maintenance and development.\n\nKarni Mata Fair is held twice a year at Deshnok:\n\n\nDuring Navratri thousands of people travel to the temple by foot.\n\nThe temple appeared on the first season of the American reality television series \"The Amazing Race\".\n\nBecause of its revered rat population, the temple was featured in the 2016 documentary film \"Rats\", directed by Morgan Spurlock.\n\n"}
{"id": "17125", "url": "https://en.wikipedia.org/wiki?curid=17125", "title": "Kerguelen Islands", "text": "Kerguelen Islands\n\nThe Kerguelen Islands ( or ; in French commonly ' but officially ', ), also known as the Desolation Islands (\"\" in French), are a group of islands in the Antarctic constituting one of the two exposed parts of the Kerguelen Plateau, a large igneous province mostly submerged by the southern Indian Ocean. They are among the most isolated places on Earth, located northwest of the uninhabited Heard Island and McDonald Islands and more than from Madagascar, the nearest populated location (excluding the Alfred Faure scientific station in Île de la Possession, about from there, and the non-permanent station located in Île Amsterdam, away). The islands, along with Adélie Land, the Crozet Islands, Amsterdam, and Saint Paul Islands, and France's Scattered Islands in the Indian Ocean are part of the French Southern and Antarctic Lands and are administered as a separate district.\n\nThe main island, Grande Terre, is in area and is surrounded by a further 300 smaller islands and islets, forming an archipelago of . The climate is raw and chilly with frequent high winds throughout the year. The surrounding seas are generally rough and they remain ice-free year-round. There are no indigenous inhabitants, but France maintains a permanent presence of 45 to 100 scientists, engineers and researchers. There are no airports on the islands, so all travel and transport from the outside world is conducted by ship.\n\nKerguelen Islands appear as the \"Ile de Nachtegal\" on Philippe Buache's map from 1754 before the island was officially discovered in 1772. The Buache map has the title \"Carte des Terres Australes comprises entre le Tropique du Capricorne et le Pôle Antarctique où se voyent les nouvelles découvertes faites en 1739 au Sud du Cap de Bonne Esperance\" ('Map of the Southern Lands contained between the Tropic of Capricorn and the Antarctic Pole, where the new discoveries made in 1739 to the south of the Cape of Good Hope may be seen'). It is possible this early name was after Abel Tasman's ship \"De Zeeuwsche Nachtegaal.\" On the Buache map, \"Ile de Nachtegal\" is located at 43°S, 72°E, about 6 degrees north and 2 degrees east of the accepted location of Grande Terre.\n\nThe islands were officially discovered by the French navigator Yves-Joseph de Kerguelen-Trémarec on 12 February 1772. The next day Charles de Boisguehenneuc landed and claimed the island for the French crown. Yves de Kerguelen organised a second expedition in 1773 and arrived at the \"baie de l'Oiseau\" by December of the same year. On 6 January 1774 he commanded his lieutenant, Henri Pascal de Rochegude, to leave a message notifying any passers-by of the two passages and of the French claim to the islands. Thereafter, a number of expeditions briefly visited the islands, including that of Captain James Cook in December 1776 during his third voyage, who verified and confirmed the passage of de Kerguelen by discovering and annotating the message left by the French navigator.\nSoon after their discovery, the archipelago was regularly visited by whalers and sealers (mostly British, American and Norwegian) who hunted the resident populations of whales and seals to the point of near extinction, including fur seals in the 18th century and elephant seals in the 19th century. The sealing era lasted from 1781 to 1922 during which time 284 sealing visits are recorded, nine of which ended when the vessel was wrecked. Modern industrial sealing, associated with whaling stations, occurred intermittently between 1908 and 1956. Since the end of the whaling and sealing era, most of the islands' species have been able to increase their population again. Relics of the sealing period include trypots, hut ruins, graves and inscriptions.\n\nIn 1800, spent eight months sealing and whaling around the islands. During this time Captain Robert Rhodes, her master, prepared a chart of the islands. That vessel returned to London in April 1801 with 450 tuns of sea elephant oil.\n\nIn 1825, the British sealer John Nunn and three crew members from \"Favourite\" were shipwrecked on Kerguelen until they were rescued in 1827 by Captain Alexander Distant during his hunting campaign.\nThe islands were not completely surveyed until the Ross expedition of 1840.\n\nThe Australian James Kerguelen Robinson (1859-1914) was the first human born south of the Antarctic Convergence, on board the sealing ship \"Offley\" in Gulf of Morbihan (Royal Sound then), Kerguelen Island on 11 March 1859.\n\nFor the 1874 transit of Venus, George Biddell Airy at the Royal Observatory of the UK organised and equipped five expeditions to different parts of the world. Three of these were sent to the Kerguelen Islands. The Reverend Stephen Joseph Perry led the British expeditions to the Kerguelen Islands. He set up his main observation station at Observatory Bay and two auxiliary stations, one at Thumb Peak () led by Sommerville Goodridge, and the second at Supply Bay () led by Cyril Corbet. Observatory Bay was also used by the German Antarctic Expedition led by Erich Dagobert von Drygalski in 1902–03. In January 2007, an archaeological excavation of this site was carried out.\n\nIn 1874–1875, British, German and U.S. expeditions visited Kerguelen to observe the transit of Venus.\n\nIn 1877 the French started a coal mining operation; however, this was abandoned soon after.\n\nThe Kerguelen Islands, along with the islands of Amsterdam and St Paul, and the Crozet archipelago were officially annexed by France in 1893, and were included as possessions in the French constitution in 1924 (in addition to that portion of Antarctica claimed by France and known as Adélie Land; as with all Antarctic territorial claims, France's possession on the continent is held in abeyance until a new international treaty is ratified that defines each claimant's rights and obligations).\n\nThe German auxiliary cruiser \"Atlantis\" called at Kerguelen during December 1940. During their stay the crew performed maintenance and replenished their water supplies. This ship's first fatality of the war occurred when a sailor, Bernhard Herrmann, fell while painting the funnel. He is buried in what is sometimes referred to as \"the most southerly German war grave\" of World War II.\n\nKerguelen has been continually occupied since 1950 by scientific research teams, with a population of 50 to 100 frequently present. There is also a French satellite tracking station.\n\nUntil 1955, the Kerguelen Islands were administratively part of the French Colony of Madagascar and Dependencies. That same year they collectively became known as ' (French Southern and Antarctic Lands) and were administratively part of the French '. In 2004 they were permanently transformed into their own entity (keeping the same name) but having inherited another group of five very remote tropical islands, \"\", which are also owned by France and are dispersed widely throughout the southern Indian Ocean.\n\nThe main island of the archipelago is called \"\". It measures east to west and north to south.\n\nPort-aux-Français, a scientific base, is along the eastern shore of the Gulf of Morbihan on La Grande Terre at . Facilities there include scientific-research buildings, a satellite tracking station, dormitories, a hospital, a library, a gymnasium, a pub, and the chapel of Notre-Dame des Vents.\n\nThe highest point is Mont Ross in the Gallieni Massif, which rises along the southern coast of the island and has an elevation of . The Cook Ice Cap (), France's largest glacier with an area of about , lies on the west-central part of the island. Overall, the glaciers of the Kerguelen Islands cover just over . Grande Terre has also numerous bays, inlets, fjords, and coves, as well as several peninsulas and promontories. The most important ones are listed below:\n\nThere are also a number of notable localities, all on La Grande Terre (see also the main map):\n\n\nFrom 1968 to 1981, just east of Port-aux-Français was a launching site for sounding rockets, some for French (Dragon rockets), American (Arcas) or French-Soviet (Eridans) surveys, but at the end mainly for a Soviet program (M-100).\n\nThe following is a list of the most important adjacent islands:\n\n\nPrincipal activities on the Kerguelen Islands focus on scientific research – mostly earth sciences and biology.\n\nThe former sounding rocket range to the east of Port-aux-Français is currently the site of a SuperDARN radar.\n\nSince 1992, the French Centre National d'Études Spatiales (CNES) has operated a satellite and rocket tracking station which is located east of Port-aux-Français. CNES needed a tracking station in the Southern Hemisphere, and the French government required that it be located on French territory, rather than in a populated, but foreign, place like Australia or New Zealand.\n\nAgricultural activities were limited until 2007 to raising sheep (about 3,500 Bizet sheep – a breed of sheep that is rare in mainland France) on Longue Island for consumption by the occupants of the base, as well as small quantities of vegetables in a greenhouse within the immediate vicinity of the main French base. There are also feral rabbits and sheep that can be hunted, as well as wild birds.\n\nThere are also 5 fishing boats and vessels, owned by fishermen on Réunion Island (a \"department\" of France about to the north) who are licensed to fish within the archipelago's Exclusive Economic Zone.\n\nThe Kerguelen islands form an emerged part of the submerged Kerguelen Plateau, which has a total area nearing . The plateau was built by volcanic eruptions associated with the Kerguelen hotspot, and now lies on the Antarctic plate.\n\nThe major part of the volcanic formations visible on the islands is characteristic of an effusive volcanism, which caused a trap rock formation to start emerging above the level of the ocean 35 million years ago. The accumulation is of a considerable amount; basalt flows, each with a thickness of three to ten metres, stacked on top of each other, sometimes up to a depth of . This form of volcanism creates a monumental relief shaped as stairs of pyramids.\n\nOther forms of volcanism are present locally, such as the strombolian volcano Mont Ross, and the volcano-plutonic complex on the Rallier du Baty peninsula. Various veins and extrusions of lava such as trachytes, trachyphonolites and phonolites are common all over the islands.\n\nNo eruptive activity has been recorded in historic times, but some fumaroles are still active in the South-West of the Grande-Terre island.\n\nA few lignite strata, trapped in basalt flows, reveal fossilised araucarian fragments, dated at about 14 million years of age.\n\nGlaciation caused the depression and tipping phenomena which created the gulfs at the north and east of the archipelago. Erosion caused by the glacial and fluvial activity carved out the valleys and fjords; erosion also created conglomerate detrital complexes, and the plain of the Courbet Peninsula.\n\nThe islands are part of a submerged microcontinent called the Kerguelen sub-continent. The microcontinent emerged substantially above sea level for three periods between 100 million years ago and 20 million years ago. The so-called Kerguelen sub-continent may have had tropical flora and fauna about 50 million years ago. The Kerguelen sub-continent finally sank 20 million years ago and is now below sea level. Kerguelen's sedimentary rocks are similar to ones found in Australia and India, indicating they were all once connected. Scientists hope that studying the Kerguelen sub-continent will help them discover how Australia, India, and Antarctica broke apart.\n\nKerguelen's climate is oceanic, cold and extremely windswept. Under the Köppen climate classification, Kerguelen's climate is considered to be an \"ET\" or tundra climate, which is technically a form of polar climate, as the average temperature in the warmest month is below . Comparable climates include the Aleutian Islands, Campbell Island (New Zealand), the Crozet Islands, Iceland, northern Kamchatka Peninsula, Labrador and Tierra del Fuego.\n\nAll climate readings come from the Port-aux-Français base, which has one of the more favourable climates in Kerguelen due to its proximity to the coast and its location in a gulf sheltered from the wind.\n\nThe average annual temperature is with an annual range of around . The warmest months of the year include January and February, with average temperatures between .The coldest month of the year is August with an average temperature of . Annual high temperatures rarely surpass , while temperatures in winter have never been recorded below at sea level.\n\nKerguelen receives frequent precipitation, with snow throughout the year as well as rain. Port-aux-Français receives a modest amount of precipitation ( per year) compared to the west coast which receives an estimated three times as much precipitation per year.\n\nThe mountains are frequently covered in snow but can thaw very quickly in rain. Over the course of several decades, many permanent glaciers have shown signs of retreat, with some smaller ones having disappeared completely.\n\nThe west coast receives almost continuous wind at an average speed of , due to the islands' location in between the Roaring Forties and the Furious Fifties. Wind speeds of are common and can even reach .\n\nWaves up to high are common, but there are many sheltered places where ships can dock.\n\nDue to the island's southern latitude, it experiences a couple of weeks from December to early January in which there is no true night, only astronomical twilight, meaning that sun illumination is barely distinguishable at nighttime.\n\nThe islands are part of the Southern Indian Ocean Islands tundra ecoregion that includes several subantarctic islands. Plant life is mainly limited to grasses, mosses and lichens, although the islands are also known for the indigenous, edible Kerguelen cabbage, a good source of vitamin C to mariners. The main indigenous animals are insects along with large populations of ocean-going seabirds, seals and penguins.\n\nThe wildlife is particularly vulnerable to introduced species and one particular problem has been cats. The main island is the home of a well-established feral cat population, descended from ships' cats. They survive on sea birds and the feral rabbits that were introduced to the islands. There are also populations of wild sheep (\"Ovis orientalis orientalis\") and reindeer.\n\nIn the 1950s and 1960s, French geologist Edgar Albert de la Rue began to introduce several species of salmonids. Of the seven species introduced, only brook trout and brown trout survived to establish wild populations.\n\n\nThe islands appear in a number of fictional works. The title character in Edgar Allan Poe's 1838 novel, \"The Narrative of Arthur Gordon Pym of Nantucket\", visits the islands. French writer Jules Verne's 1897 novel \"An Antarctic Mystery\" offers a follow up to Poe's book, and revisits the Kerguelen Islands. The 1874 short story \"The Tachypomp\" by Edward Page Mitchell tells of a hole through the center of the Earth with one end in the United States and the other in \"Kerguellen's Land\" (which is roughly antipodal to the United States and Canada). In Kipling's poem \"McAndrew's Hymn\" - about a ship's engineer - there are the lines: \"Fra' Cape Town east to Wellington - ye need an engineer. \nFail there - ye've time to weld your shaft - ay, eat it, ere ye're spoke,\nOr make Kerguelen under sail - three jiggers burned wi' smoke!\" \n\nHenry De Vere Stacpoole set his 1919 novel \"The Beach of Dreams\" on the islands. The Kerguelen Islands were the setting for a post-Second World War confrontation between W. E. Johns's recurring hero, Biggles and the crew of a gold bullion-bearing German U-boat, in the 1948 novel \"Biggles' Second Case\". The fifth book in Patrick O'Brian's Aubrey–Maturin series, published in 1978, is entitled \"Desolation Island\".\n\nFrench author Jean-Paul Kauffmann produced a non-fiction account of his 1991 journey to the islands, titled \"The Arch of Kerguelen: Voyage to the Islands of Desolation\".\n\nIn 2000 British journalist and former Conservative MP Matthew Parris spent four months on Kerguelen, staying with the researchers at Port-aux-Français. A series of articles were published in \"The Times\" in which Parris charted his visit, and a documentary \"To The Ends of Earth: Dreaming on Desolation Island\" was produced for UK television, which aired on Channel 4.\n\nThe islands inspired the 2008 song \"The Loneliest Place on the Map\" by singer Al Stewart.\n\nKerguelen and island based research facilities are an important part of Craig A. Falconer's 2015 science fiction novel, \"Not Alone\" and sequel \"Not Alone: Second Contact\".\n\n\n"}
{"id": "42293681", "url": "https://en.wikipedia.org/wiki?curid=42293681", "title": "Lake Stubbe", "text": "Lake Stubbe\n\nLake Stubbe (Stubbe Sø, in Danish) is a nature conservation area, and a former fjord, which in the Stone Age entered the sea, Kattegat, at the entrance to the Baltic Sea between Denmark and Sweden in Northern Europe. The lake is the largest lake in Djursland and is located about 6 km north of Ebeltoft. \n\n150 years ago the lake was surrounded by moor land as the original oak forest had been depleted due to human intervention. To prevent sand drift the area was afforested, and 62 square kilometers of forest surrounds the lake today, consisting primarily of Norwegian fir and spruce, with some alder in low-lying areas.\n\nOtters, which are rare in Denmark, live by the lake, just as the lake is a resting and foraging grounds for migrating birds to and from the rest of Scandinavia. Several large species of birds of prey can be seen by the lake.\n\nIn 1963 Lake Stubbe and adjacent woodland (381 ha) was reserved as protected in order preserve the landscape. Lake Stubbe is also an EU-habitat area, just as the area is part of Mols Bjerge National Park established in 2009.\nPublic access to the lake is hampered by a wall of reed surrounding most of the lake. There is a 2 km unpaved public road close to the southern shore through woodland. This road is a suitable starting point for walks including mushroom picking in the woods in late summer and autumn. On the North-eastern shore of Lake Stubbe the Danish Ornithological Society has built a public bird watching tower, that can be reached by foot via a former railroad track passing the lakes eastern side leading to the town of Ebeltoft 5 km south of the lake. The Society also owns a bird sanctuary on the north side of the lake. Here public access is possible by foot to a part of the lake where there are openings in the reed belt along the shore.\n\nStreams leading to the lake are spawning grounds for sea trout. Lake fishlife includes perch, pike, bream, roach and walleye. \n\nAround year 2000 a seemingly failed state-initiated biomanipulation project was carried out, in an attempt to make the lake water clearer. In summer Lake Stubbe can be green with algae. Part of the project was to fish up in the order of 1000 tons of roach and bream with nets, as these fish eat daphnia and copepods, small crustaceans, that devour the algae that make the water green. The idea was, that with less algae, the water would become clearer. Algae blossom is an indication of nutrient rich water. A large number of pike fry was also released, as pike eat bream and roach. Ditches with nutrient rich water from a summer cottage area north of the lake was also cut off. \n\nIn 2014 + 10 years after the biomanipulation effort, the lake still becomes green with algae in summer, indicating the attempt was in vain. The tipping balance between a clear lake type and a murky green lake type can be part of a natural cycle in lakes over years, alternating between the two separate biological states. A biologist has suggested that a missing factor might have been to raise the level of the lake in spring by some decimeters, thus flooding low-lying land, in order to increase the amount of suitable breeding grounds for pike, that eat bream and roach. This would have involved flooding privately owned land.\n\n"}
{"id": "2477282", "url": "https://en.wikipedia.org/wiki?curid=2477282", "title": "Lambda transition", "text": "Lambda transition\n\nThe λ (lambda) universality class is a group in condensed matter physics. It regroups several systems possessing strong analogies, namely, superfluids, superconductors and smectics (liquid crystals). All these systems are expected to belong to the same universality class for the thermodynamic critical properties of the phase transition. While these systems are quite different at the first glance, they all are described by similar formalisms and their typical phase diagrams are identical.\n\n\n\n"}
{"id": "57593772", "url": "https://en.wikipedia.org/wiki?curid=57593772", "title": "Lisa Tauxe", "text": "Lisa Tauxe\n\nLisa Tauxe is a geophysicist, professor and department chair at the Scripps Institute of Oceanography at UC San Diego. Tauxe is a researcher and international authority on the behavior of the ancient geomagnetic field and applications of paleomagnetism to geological problems.\n\nTauxe's contributions include the study of remnant magnetism in geological and archaeological materials, as well as co-founding a collaborative data system for compiling and sharing geological magnetic data from around the globe, the Magnetics Information Consortium (MagIC). To facilitate paleomagnetic measurements, Tauxe oversaw the creation in 1984 of potentially the world's largest demagnetized space in San Diego. Tauxe is the leader in research that estimates when the earth's magnetic poles will reverse. Because technology and electrical grids depend on the earth's magnetic field to protect it from the sun's magnetic storms, Tauxe's work has global significance. She pioneered paleointensity analysis of undersea basaltic glasses and copper slag residues found in archaeological sites, fundamentally changing the process of collecting magnetic field data and the volume of data available to study.\n\nTauxe graduated from Columbia University with a Ph.D. in 1983.\n\nIn 2014, Tauxe was awarded the prestigious Ben Franklin Medal for Earth and Environmental Science \"[f]or the development of observational techniques and theoretical models providing an improved understanding of the behavior of, and variations in intensity of, the Earth's magnetic field through geologic time.\" , Tauxe was the general secretary of the American Geophysical Union.\n\nDr Tauxe has authored two textbooks, over 150 academic papers, including 44 in AGU journals.\n\n\n\nTauxe has a brother, Dr. Robert Tauxe, who works at the Centers for Disease Control and Prevention.\n\n"}
{"id": "31266100", "url": "https://en.wikipedia.org/wiki?curid=31266100", "title": "List of botanical gardens in South Africa", "text": "List of botanical gardens in South Africa\n\nThis list of botanical gardens in South Africa is intended to include all significant botanical gardens and arboretums in South Africa.\n\n\n"}
{"id": "902664", "url": "https://en.wikipedia.org/wiki?curid=902664", "title": "List of nearest bright stars", "text": "List of nearest bright stars\n\nThis list of nearest bright stars is a table of stars found within 15 parsecs (48.9 light-years) of the Sun that have an absolute magnitude of +8.5 or brighter, which is approximately comparable to a listing of stars more luminous than a red dwarf. Right ascension and declination coordinates are for the epoch J2000. The distance measurements are based on the Hipparcos Catalogue and other astrometric data. In the event of a spectroscopic binary, the combined spectral type and absolute magnitude are listed in \"italics\".\n\nThe list is ordered by increasing distance.\n\nThese stars are estimated to be within 32.6 light years of the Sun.\n\nThese stars are estimated to be from 32.7 to 42.4 light years distant from the Sun.\n\nThese stars are estimated to be from 42.5 to 48.9 light years distant from the Sun. A value of 48.9 light years corresponds to a minimum parallax of 66.7 mas.\n\n\n"}
{"id": "51082607", "url": "https://en.wikipedia.org/wiki?curid=51082607", "title": "List of off-season South Pacific tropical cyclones", "text": "List of off-season South Pacific tropical cyclones\n\nAn off-season South Pacific tropical cyclone is a tropical cyclone that exists in the South Pacific basin outside of the official tropical cyclone season. The World Meteorological Organization currently defines the season as occurring between November 1 and April 30, of the following year, with approximately 96% of all activity occurring between these months. If a tropical cyclone should develop during the off-season, it is more likely to develop during May or October than any other month of the off-season. As of 2018, there have been 91 tropical cyclones known to have occurred off-season, with 54 of these occurring since the satellite era started during the 1969–70 season.\n\nOff-season cyclones are most likely to occur in the Coral Sea, with most impacting either the Solomon Islands, Vanuatu and New Caledonia. The most recent off-season storm is Tropical Depression 01F, which developed during September 2018 and impacted the Solomon Islands. The strongest tropical cyclone to exist during the off-season in terms of wind speed was Severe Tropical Cyclone Donna of 2017, with maximum of 125 mph (205 km/h), while the most intense by central pressure was Severe Tropical Cyclone Xavier of 2006, with an estimated value of . The deadliest and most damaging system was Severe Tropical Cyclone Namu, which caused over 100 deaths, when it impacted the Solomon Islands during May 1986.\n\nTropical cyclones are considered to be non-frontal, low pressure systems that develop, within an environment of warm sea surface temperatures and little vertical wind shear aloft. \n\nWithin the South Pacific basin to the east of 160°E, the cyclone season is defined as running between November 1 and April 30 of the following year. During the off-season a total of 90 tropical cyclones have been recorded during the off-season, with a total of 53 of these systems occurring after satellite imagery became regularly available during the 1969–70 season.\n\nThe first system to exist in the off-season on record was active during May 17, 1868 and was located near Vanuatu, according to records compiled by Stephen Sargent Visher. The most recent systems to exist in the off-season were Severe Tropical Cyclone Donna and Tropical Cyclone Ella, which both developed during May 2017. The strongest tropical cyclone to exist during the off-season in terms of wind speed was Severe Tropical Cyclone Donna of 2017, with maximum of 125 mph (205 km/h), while the most intense by central pressure was Severe Tropical Cyclone Xavier of 2006, with a minimum value of 930 hPa (27.46 inHg). The deadliest and most damaging system was Severe Tropical Cyclone Namu, which caused over 100 deaths, when it impacted the Solomon Islands during May 1986.\n\nOf the 57 known tropical systems that have been recorded during the off-season, a total of 33 tropical cyclones have been recorded in May or have persisted into the off-season after developing in April. A total of 18 tropical cyclones have developed during October, while three systems have been recorded in both June and July, while two have been recorded in September. The three systems recorded in June were all named tropical cyclones, with Gina and Keli considered to be a Category 3 severe tropical cyclones on the Australian tropical cyclone intensity scale. In addition Tropical Cyclone Raquel developed in June 2015 and was considered, to be the latest system to develop in the South Pacific Ocean. In addition as the system persisted into July 2015, it was unofficially considered to be a part of two tropical cyclone years. Off the two systems recorded in July, Tropical Depression 17F in July 2002 was designated 17F, despite being the first system of the 2002–03 season. Tropical Depression 01F/01P of July 2015, which subsequently persisted into August 2015. The first system that developed during September, was an unnamed system that occurred in 1924, which according to a report published by Greenpeace, caused damage to some ships to the west of New Caledonia. The other system was declared to be a tropical depression by the Fiji Meteorological Service during September 1999, but was more likely to be a hybrid system rather than a proper tropical depression.\n\nThe season with the most off-season systems was 1999–2000, which had a total of six tropical depressions existing during the off-season. The 1997–98, 2004–05 and 2013–14 seasons had four tropical cyclones during the off-season each, while the 1988–89 and the 1996–97 season had three off-season tropical cyclones.\n\nThe wind speeds listed are maximum ten-minute average sustained winds, while the pressure is the minimum barometric pressure, both of which are estimates taken from the archives of either the Australian Bureau of Meteorology, the Fiji Meteorological Service, and New Zealand's MetService. If there are no known estimates of either the winds or pressure then the system is listed as \"Not specified\" under winds or pressure, if there is no known estimated winds or pressure. For deaths and damages \"None\" indicates that there were no reports of fatalities or damages, although such storms may have impacted land. Where it The damage totals are the United States dollar of the year of the storm.\n! colspan=9|\n\nAccording to records compiled by Stephen Sargent Visher and the Vanuatu Meteorology Service, the first tropical cyclone on record to occur outside of the current season was located near Vanuatu and was active during May 17, 1868. However, the official database provided by the International Best Track Archive for Climate Stewardship, which dates back to 1892, shows that the first storm to occur in the basin outside of the current season was in 1912. The database shows that 55 tropical cyclones have existed in the basin during the off-season between May and October, while in addition 36 other systems that are not included in IBTRACS have monitored by the warning centres.\n\nOff-season systems are most likely to occur in May with a total of 43 systems developing or persisting into the month, compared with 20 developing during October. Out of all systems recorded only four systems have developed in either August or September, and were all recorded before the satellite era started during the 1969-70 season. The latest tropical cyclone to exist in the basin on record was Tropical Cyclone Raquel, which was named during June 30, 2015. Raquel was also unofficially considered to be the earliest a tropical cyclone, after it persisted into July 2015 when the tropical cyclone year changed. \n\nThe charts below both show during which month a tropical cyclone developed during the off-season. The statistics for April are not complete and only show those systems that formed during the month and either dissipated on April 30, or persisted into May and the off-season. The charts are also split with red showing those systems that developed within the satellite era, while those in red developed, before the satellite era started during the 1969-70 season.\n\n"}
{"id": "1502835", "url": "https://en.wikipedia.org/wiki?curid=1502835", "title": "Magnetohydrodynamic generator", "text": "Magnetohydrodynamic generator\n\nA magnetohydrodynamic generator (MHD generator) is a magnetohydrodynamic converter that transforms thermal energy and kinetic energy into electricity. MHD generators are different from traditional electric generators in that they operate at high temperatures without moving parts. MHD was developed because the hot exhaust gas of an MHD generator can heat the boilers of a steam power plant, increasing overall efficiency. MHD was developed as a topping cycle to increase the efficiency of electric generation, especially when burning coal or natural gas. MHD dynamos are the complement of MHD accelerators, which have been applied to pump liquid metals, seawater and plasmas.\n\nAn MHD generator, like a conventional generator, relies on moving a conductor through a magnetic field to generate electric current. The MHD generator uses hot conductive ionized gas (a plasma) as the moving conductor. The mechanical dynamo, in contrast, uses the motion of mechanical devices to accomplish this. MHD generators are technically practical for fossil fuels, but have been overtaken by other, less expensive technologies, such as combined cycles in which a gas turbine's or molten carbonate fuel cell's exhaust heats steam to power a steam turbine.\n\nNatural MHD dynamos are an active area of research in plasma physics and are of great interest to the geophysics and astrophysics communities, since the magnetic fields of the earth and sun are produced by these natural dynamos.\n\nThe Lorentz Force Law describes the effects of a charged particle moving in a constant magnetic field. The simplest form of this law is given by the vector equation.\nwhere \n\nThe vector F is perpendicular to both v and B according to the right hand rule.\n\nTypically, for a large-scale power station to approach the operational efficiency of computer models, steps must be taken to increase the electrical conductivity of the conductive substance. The heating of a gas to its plasma state or the addition of other easily ionizable substances like the salts of alkali metals can accomplish this increase. In practice, a number of issues must be considered in the implementation of an MHD generator: generator efficiency, economics, and toxic byproducts. These issues are affected by the choice of one of the three MHD generator designs: the Faraday generator, the Hall generator, and the disc generator.\n\nThe Faraday generator is named after the man who first looked for the effect in the Thames river (see history). A simple Faraday generator would consist of a wedge-shaped pipe or tube of some non-conductive material. When an electrically conductive fluid flows through the tube, in the presence of a significant perpendicular magnetic field, a voltage is induced in the field, which can be drawn off as electrical power by placing the electrodes on the sides at 90 degree angles to the magnetic field.\n\nThere are limitations on the density and type of field used. The amount of power that can be extracted is proportional to the cross sectional area of the tube and the speed of the conductive flow. The conductive substance is also cooled and slowed by this process. MHD generators typically reduce the temperature of the conductive substance from plasma temperatures to just over 1000 °C.\n\nThe main practical problem of a Faraday generator is that differential voltages and currents in the fluid short through the electrodes on the sides of the duct. The most powerful waste is from the Hall effect current. This makes the Faraday duct very inefficient. Most further refinements of MHD generators have tried to solve this problem. The optimal magnetic field on duct-shaped MHD generators is a sort of saddle shape. To get this field, a large generator requires an extremely powerful magnet. Many research groups have tried to adapt superconducting magnets to this purpose, with varying success.\n\nThe most common solution is to use the Hall effect to create a current that flows with the fluid. The normal scheme is to place arrays of short, vertical electrodes on the sides of the duct. The first and last electrodes in the duct power the load. Each other electrode is shorted to an electrode on the opposite side of the duct. These shorts of the Faraday current induce a powerful magnetic field within the fluid, but in a chord of a circle at right angles to the Faraday current. This secondary, induced field makes current flow in a rainbow shape between the first and last electrodes.\n\nLosses are less than a Faraday generator, and voltages are higher because there is less shorting of the final induced current. However, this design has problems because the speed of the material flow requires the middle electrodes to be offset to \"catch\" the Faraday currents. As the load varies, the fluid flow speed varies, misaligning the Faraday current with its intended electrodes, and making the generator's efficiency very sensitive to its load.\n\nThe third and, currently, the most efficient design is the Hall effect disc generator. This design currently holds the efficiency and energy density records for MHD generation. A disc generator has fluid flowing between the center of a disc, and a duct wrapped around the edge. The magnetic excitation field is made by a pair of circular Helmholtz coils above and below the disk. The Faraday currents flow in a perfect dead short around the periphery of the disk. The Hall effect currents flow between ring electrodes near the center and ring electrodes near the periphery.\n\nAnother significant advantage of this design is that the magnet is more efficient. First, it has simple parallel field lines. Second, because the fluid is processed in a disk, the magnet can be closer to the fluid, and magnetic field strengths increase as the 3rd power of distance. Finally, the generator is compact for its power, so the magnet is also smaller. The resulting magnet uses a much smaller percentage of the generated power.\n\nThe efficiency of the direct energy conversion in MHD power generation increases with the magnetic field strength and the plasma conductivity, which depends directly on the plasma temperature, and more precisely on the electron temperature. As very hot plasmas can only be used in pulsed MHD generators (for example using shock tubes) due to the fast thermal material erosion, it was envisaged to use nonthermal plasmas as working fluids in steady MHD generators, where only free electrons are heated a lot (10,000–20,000 kelvins) while the main gas (neutral atoms and ions) remains at a much lower temperature, typically 2500 kelvins. The goal was to preserve the materials of the generator (walls and electrodes) while improving the limited conductivity of such poor conductors to the same level as a plasma in thermodynamic equilibrium, i.e. completely heated to more than 10,000 degrees, a temperature that no material could stand.\n\nBut Evgeny Velikhov first discovered theoretically in 1962 and experimentally in 1963 that an ionization instability, later called the Velikhov instability or electrothermal instability, quickly arises in any MHD converter using magnetized nonthermal plasmas with hot electrons, when a critical Hall parameter is reached, hence depending on the degree of ionization and the magnetic field. Such an instability greatly degrades the performance of nonequilibrium MHD generators. The prospects about this technology, which initially predicted awesome efficiencies, crippled MHD programs all over the world as no solution to mitigate the instability was found at that time.\n\nConsequently, without implementing solutions to master the electrothermal instability, practical MHD generators had to limit the Hall parameter or use moderately heated thermal plasmas instead of cold plasmas with hot electrons, which severely lowers efficiency.\n\nAs of 1994, the 22% efficiency record for closed-cycle disc MHD generators was held by Tokyo Technical Institute. The peak enthalpy extraction in these experiments reached 30.2%. Typical open-cycle Hall & duct coal MHD generators are lower, near 17%. These efficiencies make MHD unattractive, by itself, for utility power generation, since conventional Rankine cycle power plants easily reach 40%.\n\nHowever, the exhaust of an MHD generator burning fossil fuel is almost as hot as the flame of a conventional steam boiler. By routing its exhaust gases into a boiler to make steam, MHD and a steam Rankine cycle can convert fossil fuels into electricity with an estimated efficiency up to 60 percent, compared to the 40 percent of a typical coal plant.\n\nA magnetohydrodynamic generator might also be the first stage of a gas-cooled nuclear reactor.\n\nMHD generators have not been employed for large scale mass energy conversion because other techniques with comparable efficiency have a lower lifecycle investment cost. Advances in natural gas turbines achieved similar thermal efficiencies at lower costs, by having the turbine's exhaust drive a Rankine cycle steam plant. To get more electricity from coal, it is cheaper to simply add more low-temperature steam-generating capacity.\n\nA coal-fueled MHD generator is a type of Brayton power cycle, similar to the power cycle of a combustion turbine. However, unlike the combustion turbine, there are no moving mechanical parts; the electrically conducting plasma provides the moving electrical conductor. The side walls and electrodes merely withstand the pressure within, while the anode and cathode conductors collect the electricity that is generated. All Brayton cycles are heat engines. Ideal Brayton cycles also have an ideal efficiency equal to ideal Carnot cycle efficiency. Thus, the potential for high energy efficiency from an MHD generator. All Brayton cycles have higher potential for efficiency the higher the firing temperature. While a combustion turbine is limited in maximum temperature by the strength of its air/water or steam-cooled rotating airfoils; there are no rotating parts in an open-cycle MHD generator. This upper bound in temperature limits the energy efficiency in combustion turbines. The upper bound on Brayton cycle temperature for an MHD generator is not limited, so inherently an MHD generator has a higher potential capability for energy efficiency.\n\nThe temperatures at which linear coal-fueled MHD generators can operate are limited by factors that include: (a) the combustion fuel, oxidizer, and oxidizer preheat temperature which limit the maximum temperature of the cycle; (b) the ability to protect the sidewalls and electrodes from melting; (c) the ability to protect the electrodes from electrochemical attack from the hot slag coating the walls combined with the high current or arcs that impinge on the electrodes as they carry off the direct current from the plasma; and (d) by the capability of the electrical insulators between each electrode. Coal-fired MHD plants with oxygen/air and high oxidant preheats would probably provide potassium seeded plasmas of about 4200 deg. F, 10 atmospheres pressure, and begin expansion at Mach 1.2. These plants would recover MHD exhaust heat for oxidant preheat, and for combined cycle steam generation. With aggressive assumptions, one DOE-funded feasibility study of where the technology could go, 1000 MWe Advanced Coal-Fired MHD/Steam Binary Cycle Power Plant Conceptual Design, published in June 1989, showed that a large coal-fired MHD combined cycle plant could attain a HHV energy efficiency approaching 60 percent—well in excess of other coal-fueled technologies, so the potential for low operating costs exists.\n\nHowever, no testing at those aggressive conditions or size has yet occurred, and there are no large MHD generators now under test. There is simply an inadequate reliability track record to provide confidence in a commercial coal-fueled MHD design.\n\nU25B MHD testing in Russia using natural gas as fuel used a superconducting magnet, and had an output of 1.4 megawatts. A coal-fired MHD generator series of tests funded by the U.S. Department of Energy (DOE) in 1992 produced MHD power from a larger superconducting magnet at the Component Development and Integration Facility (CDIF) in Butte, Montana. None of these tests were conducted for long-enough durations to verify the commercial durability of the technology. Neither of the test facilities were in large-enough scale for a commercial unit.\n\nSuperconducting magnets are used in the larger MHD generators to eliminate one of the large parasitic losses: the power needed to energize the electromagnet. Superconducting magnets, once charged, consume no power, and can develop intense magnetic fields 4 teslas and higher. The only parasitic load for the magnets are to maintain refrigeration, and to make up the small losses for the non-supercritical connections.\n\nBecause of the high temperatures, the non-conducting walls of the channel must be constructed from an exceedingly heat-resistant substance such as yttrium oxide or zirconium dioxide to retard oxidation. Similarly, the electrodes must be both conductive and heat-resistant at high temperatures. The AVCO coal-fueled MHD generator at the CDIF was tested with water-cooled copper electrodes capped with platinum, tungsten, stainless steel, and electrically conducting ceramics.\n\nMHD reduces overall production of hazardous fossil fuel wastes because it increases plant efficiency. In MHD coal plants, the patented commercial \"Econoseed\" process developed by the U.S. (see below) recycles potassium ionization seed from the fly ash captured by the stack-gas scrubber. However, this equipment is an additional expense. If molten metal is the armature fluid of an MHD generator, care must be taken with the coolant of the electromagnetics and channel. The alkali metals commonly used as MHD fluids react violently with water. Also, the chemical byproducts of heated, electrified alkali metals and channel ceramics may be poisonous and environmentally persistent.\n\nThe first practical MHD power research was funded in 1938 in the U.S. by Westinghouse in its Pittsburgh, Pennsylvania laboratories, headed by Hungarian Bela Karlovitz. The initial patent on MHD is by B. Karlovitz, U.S. Patent No. 2,210,918, \"Process for the Conversion of Energy\", August 13, 1940.\n\nWorld War II interrupted development. In 1962, the First International Conference on MHD Power was held in Newcastle upon Tyne, UK by Dr. Brian C. Lindley of the International Research and Development Company Ltd. The group set up a steering committee to set up further conferences and disseminate ideas. In 1964, the group set up a second conference in Paris, France, in consultation with the European Nuclear Energy Agency.\n\nSince membership in the ENEA was limited, the group persuaded the International Atomic Energy Agency to sponsor a third conference, in Salzburg, Austria, July 1966. Negotiations at this meeting converted the steering committee into a periodic reporting group, the ILG-MHD (international liaison group, MHD), under the ENEA, and later in 1967, also under the International Atomic Energy Agency. Further research in the 1960s by R. Rosa established the practicality of MHD for fossil-fueled systems.\n\nIn the 1960s, AVCO Everett Aeronautical Research began a series of experiments, ending with the Mk. V generator of 1965. This generated 35 MW, but used about 8MW to drive its magnet. In 1966, the ILG-MHD had its first formal meeting in Paris, France. It began issuing a periodic status report in 1967. This pattern persisted, in this institutional form, up until 1976. Toward the end of the 1960s, interest in MHD declined because nuclear power was becoming more widely available.\n\nIn the late 1970s, as interest in nuclear power declined, interest in MHD increased. In 1975, UNESCO became persuaded the MHD might be the most efficient way to utilise world coal reserves, and in 1976, sponsored the ILG-MHD. In 1976, it became clear that no nuclear reactor in the next 25 years would use MHD, so the International Atomic Energy Agency and ENEA (both nuclear agencies) withdrew support from the ILG-MHD, leaving UNESCO as the primary sponsor of the ILG-MHD.\n\nOver more than a ten-year span, engineers in former Yugoslavian Institute of Thermal and Nuclear Technology (ITEN), Energoinvest Co., Sarajevo, had built the first experimental Magneto-Hydrodynamic facility power generator in 1989. It was here it was first patented.\nIn the 1980s, the U.S. Department of Energy began a vigorous multiyear program, culminating in a 1992 50MW demonstration coal combustor at the Component Development and Integration Facility (CDIF) in Butte, Montana. This program also had significant work at the Coal-Fired-In-Flow-Facility (CFIFF) at University of Tennessee Space Institute.\n\nThis program combined four parts:\n\n\nInitial prototypes at the CDIF were operated for short durations, with various coals: Montana Rosebud, and a high-sulphur corrosive coal, Illinois No. 6. A great deal of engineering, chemistry and material science was completed. After final components were developed, operational testing completed with 4,000 hours of continuous operation, 2,000 on Montana Rosebud, 2,000 on Illinois No. 6. The testing ended in 1993.\n\nThe Japanese program in the late 1980s concentrated on closed-cycle MHD. The belief was that it would have higher efficiencies, and smaller equipment, especially in the clean, small, economical plant capacities near 100 megawatts (electrical) which are suited to Japanese conditions. Open-cycle coal-powered plants are generally thought to become economical above 200 megawatts.\n\nThe first major series of experiments was FUJI-1, a blow-down system powered from a shock tube at the Tokyo Institute of Technology. These experiments extracted up to 30.2% of enthalpy, and achieved power densities near 100 megawatts per cubic meter. This facility was funded by Tokyo Electric Power, other Japanese utilities, and the Department of Education. Some authorities believe this system was a disc generator with a helium and argon carrier gas and potassium ionization seed.\n\nIn 1994, there were detailed plans for FUJI-2, a 5MW (electrical) continuous closed-cycle facility, powered by natural gas, to be built using the experience of FUJI-1. The basic MHD design was to be a system with inert gases using a disk generator. The aim was an enthalpy extraction of 30% and an MHD thermal efficiency of 60%. FUJI-2 was to be followed by a retrofit to a 300 MWe natural gas plant.\n\nIn 1986, Professor Hugo Karl Messerle at The University of Sydney researched coal-fueled MHD. This resulted in a 28 MWe topping facility that was operated outside Sydney. Messerle also wrote one of the most recent reference works (see below), as part of a UNESCO education program.\n\nA detailed obituary for Hugo is located on the Australian Academy of Technological Sciences and Engineering (ATSE) website.\n\nThe Italian program began in 1989 with a budget of about 20 million $US, and had three main development areas:\n\n\nA joint U.S.-China national programme ended in 1992 by retrofitting the coal-fired No. 3 plant in Asbach. A further eleven-year program was approved in March 1994. This established centres of research in:\n\n\nThe 1994 study proposed a 10 MW (electrical, 108 MW thermal) generator with the MHD and bottoming cycle plants connected by steam piping, so either could operate independently.\n\nIn 1971 the natural-gas fired U-25 plant was completed near Moscow, with a designed capacity of 25 megawatts. By 1974 it delivered 6 megawatts of power. By 1994, Russia had developed and operated the coal-operated facility U-25, at the High-Temperature Institute of the Russian Academy of Science in Moscow. U-25's bottoming plant was actually operated under contract with the Moscow utility, and fed power into Moscow's grid. There was substantial interest in Russia in developing a coal-powered disc generator.\n\n\n"}
{"id": "14901447", "url": "https://en.wikipedia.org/wiki?curid=14901447", "title": "Maritime Continent", "text": "Maritime Continent\n\nMaritime Continent is the name given primarily by meteorologists and oceanographers to the region of Southeast Asia which comprises, amongst other countries, Indonesia, Philippines and Papua New Guinea. Located between the Indian and Pacific Oceans, it is situated within a warm ocean region known as the Tropical Warm Pool.\n\nThe maritime continent consists of many islands, peninsulas and shallow seas. The region is significant in meteorology because it is considered the most important energy source region in the entire global circulation system owing to a number of coincident factors, the most significant being geographic location and topography, both of which contribute to the development of the Tropical Warm Pool, which is the warmest large area of ocean on Earth.\n\nCoined by Colin Ramage in 1968, the name combines the terms \"maritime\" and \"continent\" normally used as opposites in the description of climate. Maritime air is humid, and continental air is dry.\n\nIn the Southeast Asia region, land masses and bodies of water are, roughly-speaking, evenly distributed. Moreover, the land masses are characterized by high mountains, and the seas are among the warmest on the earth. This produces a widespread area of diurnal thunderstorms which pump huge quantities of moisture and heat high up into the atmosphere which in turn feeds the upper level winds which circle the globe.\n\n"}
{"id": "38027034", "url": "https://en.wikipedia.org/wiki?curid=38027034", "title": "Music of World War I", "text": "Music of World War I\n\nThe music of World War I is the music which was composed during the war or which is associated with it.\n\nIn 1914, music hall was by far the most popular form of popular song. It was listened to and sung along to in theatres which were getting ever larger (three thousand seaters were not uncommon) and in which the musical acts were gradually overshadowing all other acts (animal imitators, acrobats, human freaks, conjurors, etc.) The industry was more and more dominated by chains of theatres like Moss, and by music publishers, since selling sheet music was very profitable indeed—a real hit could sell over a million copies.\n\nThe seats at the music hall could be very cheap and attracted a largely working class audience, for whom a gramophone would generally be too expensive. Although many ordinary people had heard gramophones in seaside resorts or in park concerts organized by local councils, many more would discover the gramophone while in the army, since gramophone manufacturers produced large numbers of portable gramophones \"for our soldiers in France\".\n\nThe repertoire of songs was dominated by the jauntily comic. Humorous stereotypes of domineering wivese or mothers-in-law, the bourgeoisie, foreigners, Blacks and Jews were often subjects of songs. Many more songs were made up of tongue-twisters or other comic elements. Sentimental love songs and dreams of an ideal land (Ireland or Dixie in particular) made up another major category. Practically all the songs of the era are unknown today; several thousand music hall songs were published in the UK alone during the war years.\n\nThe singers moved from town to town, many just scraping together a living, but a few making a lot of money. The key stars at the time included Marie Lloyd, Vesta Tilley, George Formby, Sr., Harry Lauder, Gertie Gitana and Harry Champion.\n\nAt the outbreak of war, many songs were produced which called for young men to join up. Examples included \"We Don't Want to Lose You, but We Think You Ought to Go\", \"Now You've Got the Khaki On\" or \"Kitcheners' Boys\". After a few months of war and rising numbers of deaths, the recruitment songs all but disappeared, and the 1915 \"Greatest hits\" collection published by Francis and Day contains no recruitment songs at all. The music hall songs which mentioned the war (about a third of the total produced) were more and more dreams about the end of the war—\"When the Boys Come Home\" and \"Keep the Home Fires Burning\" are two well-known examples.\n\nPopular, patriotic songs that were composed during the war also served to raise the morale of soldiers and civilians alike. These hit songs covered a variety of themes, such as separation of loved ones, boot camp, war as an adventure, and humorous songs about the military life.\n\nBecause there were no radios or televisions that reported the conditions of the battlefields, Americans had a romantic view of war. Not only were many of the songs patriotic, but they were also romantic. These songs portrayed soldiers as brave and noble, while the women were portrayed as fragile and loyal as they waited for their loved ones.\n\nIt was almost impossible to sing anti-war songs on the music-hall stage. The managers of music halls would be worried about their license, and the singalong nature of music hall songs meant that one needed to sing songs which had the support of the vast majority of the audience. In the music hall, dissent about the war drive was therefore limited to sarcastic songs such as \"Oh It's a Lovely War\" or bitter complaints about the stupidity of conscription tribunals (for example \"The Military Representative\").\nWhen the anti-war movement had, for a few months in 1916, a mass audience, anti-war music hall songs from the United States such as \"I Didn't Raise My Boy to Be a Soldier\" were sung at anti-war meetings, but not on the music hall stage.\n\n\n\n\n"}
{"id": "2937664", "url": "https://en.wikipedia.org/wiki?curid=2937664", "title": "Nanophase material", "text": "Nanophase material\n\nNanophase materials are materials that have grain sizes under 100 nanometres. They have different mechanical and optical properties compared to the large grained materials of the same chemical composition.\n\nTransparency and different transparent colours can be achieved with nanophase materials by varying the grain size.\n\n\n"}
{"id": "49887558", "url": "https://en.wikipedia.org/wiki?curid=49887558", "title": "Net metering in Nevada", "text": "Net metering in Nevada\n\nNet metering in Nevada is a public policy and political issue surrounding the rates that Nevada public utilities are required to pay to purchase excess energy produced by electric customers who generate their own electricity, such as through rooftop solar panels. The issue centers around two policies: paying solar customers the \"retail\" rate versus the \"wholesale\" rate.\n\nBefore 2016, Nevada utilities paid people with solar panels the “retail” rate for electricity. At the end of 2015, the Nevada Public Utilities Commission (NPUC) approved new rules that changed that rate to the \"wholesale\" rate.\n\nUnder a setup known as net metering, utilities must buy excess electricity generated by people with solar panels. For example, on a sunny day, a home with solar panels generates power. If the homeowners are away and the home is using less electricity than the solar panels are generating, that excess electricity flows to the electric grid. The utility company purchases that electricity.\n\nUtility companies buy power at a “wholesale” rate and sell to consumers at a “retail rate.” The retail rate is higher than the wholesale rate. The retail electric rate charged by utilities are the final rates that customers get charged for using electricity. The utilities calculate these rates by taking into consideration the cost of the purchasing the power (the wholesale rate) and the fixed costs the utility incurs. Fixed costs are items are such as installing electric poles and wires, putting meters on people’s homes and businesses, and implementing technology to maintain the electric grid. The wholesale rate is what the utility pays electricity providers to buy their electricity. A competitive marketplace exists from which utilities can purchase electricity. Throughout a single day, the price of electricity on the wholesale market can change.\n\nNevada's net metering rules began in 1997. Tim Webb, a manager at Robco Electric (one of the main sellers of rooftop solar panels in Nevada), said, \"It was kind of like the solar gold rush here. All these companies flocked into town, set up an office and sold systems.\"\n\nNet metering customers in southern Nevada receive an average subsidy of $623 per year for their excess electricity. Northern Nevada net metering customers receive an average of $471 per year.\n\nIn 2015, the Nevada legislature passed a law that required the commission to examine the electric rate structure and to look at cost shifting. KRNV Channel 4 (NBC) of Reno, Nevada, defined cost shifting as “when non-solar residential customers are said to be subsidizing those without solar for using NV Energy’s grid.” (NV Energy is the major utility company in the state of Nevada, and it is owned by Berkshire Hathaway Energy, a subsidiary of Warren Buffett’s Berkshire Hathaway.) Specifically, the law requiring NPUC’s review was Senate Bill 374, passed during the legislature’s 2015 session. The bill set a deadline of December 31, 2015 for the commission to decide on new electric rates.\n\nOn December 22, 2015, regulators in Nevada created major controversy following their decision to reduce rooftop solar incentives in addition to creating new fees for solar owners. As a result, a lawsuit was filed against the Nevada Public Utilities Commission (PUC) based on the fact that lowering rates constituted as a violation of contracts made with installers.\n\nCommissioner David Noble, who had presided over the PUC's net metering docket, recently had decided to break his usual silence and speak out on the regulatory body's decision at the National Association of Regulatory Utility Commissioners (NARUC) meeting held in Nashville, Tennessee over the summer. He proceeded to explain the events leading up to the decision, making it out to be a cautionary about the unethical actions and reckless comments that may ensue in such dealings.\n\nNoble began with the May 2015 signing of Senate Bill 374, which gave the PUC the responsibility to find a fair and suitable replacement for retail rates given to net metering solar owners, which had caused significant cost shifts negatively impacting non-solar consumers.\n\nThe new order calls for higher monthly rates for solar-owning customers from $12.75 to $17.90. In addition to increase in charges, the volumetric rate was lowered from $0.111/kWh to $0.108/kWh. Also, it was determined that over a 12-year period, the rate will reach $38.51 while the volumetric rate will drop to $0.999/kWh. The new rates and conditions apply to both new and existing customers.\n\nNoble insisted that all the terms of this new reform were created to be more than fair and free of any bias or favoritism. He also pointed out the solar industry's lack of evidence supporting their claim to out-benefiting utility grids.\n\nThere was also much backlash regarding the omission of a grandfather clause for existing customers. Noble eventually compromised extending the initiation of rate changes for existing solar customers, yet denying them lower rates altogether. This was not taken well by Governor Brian Sandoval, who felt the Commission neglected to protect the interests of those customers. Shortly following this decision, it was announced that the governor will not be reappointing Noble to his post.\n\nNoble was also faced with further backlash in a much more aggressive nature when protestors attempted to barge in a PUC meeting carrying firearms. Although Nevada is an open carry state, the meeting had to be canceled with the commissioners being escorted out.\n\nIn regard to accusations from media sources that his decision was made in retaliation to the intimiditative tactics of rooftop solar groups, Noble maintained he was not swayed by anything other than facts and evidence. He also disapproved of the media's blows to his character.\n\nAs far as the future of rooftop solar companies in Nevada, Noble acknowledged the collective effort to ease tensions among all the groups with more focus on respect and collaboration. Although the solar companies and groups are making an effort to be cooperative, they still do support a referendum to overturn the commission's decision and go back to retail rates.\n\nIn January 2017, NV Energy asked the Public Utilities Commission to reverse a decision regarding net metering rates for future solar customers. The decision restored \"more favorable net metering rates\" for some rooftop solar customers in northern Nevada. NV Energy argued that the energy savings should have been directed to its Sierra Pacific customers, not redirected to rooftop solar residents in northern Nevada. In October 2016, NV Energy decreased rates for customers by approximately $2.92 million annually, which would amount to $8.77 million over a three-year period. Its intention was to make these cost savings shared among all Northern Nevada electric customers.\n\nIn 2016, the rooftop solar industry faced various obstacles when the PUC established less favorable rates for rooftop solar customers in Nevada. Solar energy supporters support PUC’s new decision. PUC has 40 days to act on the NV Energy request.\n\nOn March 8, 2017, Las Vegas-based Democratic state Assemblyman Justin Watkins introduced a bill, AB 270 to restore net metering to all utility customers. The bill sets a floor rate customers would be paid when they sell their excess energy back to utilities.\n\nTo put the bill in context, according to Utility Dive: \"Following public outcry over the PUCN's 2015 net metering decision, NV Energy collaborated with solar interests, including SolarCity, to come up with a grandfathering proposal. Then, regulators voted to restore retail rate net metering to customers in Sierra Pacific Power's service territory.\"\n\nTesla plans to introduce an amendment to the bill \"addressing variable rates for energy storage.\" Meanwhile, NV Energy plans to add an amendment \"detailing a competitive bidding process addressing private generation in some locations.\"\n\nNV Energy, which is the largest public utility in Nevada, announced in April 2017 that it would exceed Nevada’s renewable energy mandate. The current law requires utilities to get 25 percent of their power from renewable sources by 2025. As of 2017, the state gets around 23 percent of its electricity from renewables such as wind, solar and geothermal. However, some legislators are introducing bills to require utilities to generate more of their power from renewables.\n\nOne bill, AB206 sponsored by Assemblyman Chris Brooks (D-Las Vegas), would change the state’s goal of 25 percent by 2025 to 80 percent by 2040. Another bill, AB270, would reverse the Public Utilities Commission’s decision in 2015 that eliminated retail rates for net metering. According to KUNR (a Reno NPR affiliate station), the commission’s decision “gutted” the rooftop solar industry. The sponsor of AB270, Assemblyman Justin Watkins (D-Las Vegas), said, “We have enough sun in this state that provides for enough energy to be put back into the grid that gives enough credits to residents to make it financially beneficial as long as the rates are right.”\n\nOverall, 15 bills have been introduced regarding renewable energy. The Nevada Resort Association told state lawmakers during a committee hearing that increasing the renewable standards too quickly could hurt the resort industry, which is the state’s largest industry. NV Energy has worked to offset concerns and suggested alternative measures. “Through amendments to both the energy portfolio and net-metering bills, the utility would not face increased mandates, but could voluntarily increase green energy production. They would also offer consumers a 20-year net-metering bill credit that has yet to be defined.”\n\nIn the last days of the 2017 legislative session, the Nevada legislature took a number of actions to \"reopen the state to the development of and investment in distributed generation and community solar projects,\" according to the \"National Law Review\". The most recent development of solar power in Nevada has been utility-scale projects (vs. home-based solar panels). This is due to the decision in 2015 by the Public Utility Commission of Nevada that changed net metering rates \"paid by utilities for electricity produced by distributed generation systems to wholesale, rather than retail, rates.\"\n\nOne bill that was passed, AB405, makes it so that utility companies must buy back energy from rooftop solar customers at 95 percent of the retail rate. This rate would be significantly higher than the current wholesale rate that utilities pay such customers. The legislature also passed SB392, \"which governs and promotes the development of community solar gardens in the state. As in other states, there is a variety of requirements and limits on the capacity, ownership and subscription of community solar gardens. For example, the legislation limits community solar gardens to a nameplate capacity of twelve megawatts or less and requires that they have at least twenty subscribers.\" One of the requirements of the legislation is that at least ten percent of the power-producing capacity of each solar garden must be made available to people with low incomes. According to \"National Law Review\", Nevada Governor Brian Sandoval plans to sign both bills.\n\nAt the end of 2015, the Nevada Public Utilities Commission changed the rates that utility companies pay to rooftop solar customers for the excess electricity that those customers sell to the utilities. The commission argued that paying rooftop solar customers the full retail rate for their excess electricity puts unreasonable cost increases on utility customers who do not have solar power.\n\nAccording to the commission, the new rules:\n\nPaul Thomsen, the chairman of NPUC, said, \"If I reduce the burden for one individual or group of people, I have to increase it for someone else. It is that simple.\"\n\nA deal between NV Energy and SolarCity was finally approved on September 16, 2016, allowing rooftop solar customers to retain the use of their original retail rates, which were eradicated following the Nevada Public Utilities Commission's (PUC) actions which \"reduced net metering incentives for both existing and future solar customers- and made the unprecedented decision not to 'grandfather' existing solar customers into the new rate structure.\"\n\nThe decision prompted Nevada solar groups and the Nevada Attorney General's Bureau of Consumer Protection to intervene and challenge the PUC's decision based on the fact \"it violated the contract clause of the Constitution because many rooftop solar contracts were predicated on retail rate net metering.\" Under the terms of the new agreement, 32,000 rooftop solar customers will be grandfathered in under the original retail net metering rates over a period of 20 years. These customers applied or installed a rooftop solar system before December 31, 2015. \"The agreement also makes eligible customers who withdrew an application, or had a RenewableGenerations application expire between December 23 and December 31, 2015, for the more favorable rates.\"\n\nNV Energy denies ever requesting the exclusion of a \"grandfather\" clause in its original proposal, and there were attempts to include such provisions. \"NV Energy's intent with its grandfathering proposal was to offer a solution for customers who installed or had valid applications to install rooftop solar systems in the most efficient and timely manner.\" The clause was eventually denied by the regulators, then presided by Commissioner David Noble, who also authored the net metering decision. This controversy led to Nevada Governor Brian Sandoval's decision to not reappoint Noble to his position and take further action as he was \"critical of the net metering decision and convened New Energy Task Force to resolve net metering issues.\" Both NV Energy and the solar industry applaud Gov. Sandoval's intervention in the matter in order to reach a fair and reasonable outcome.\n\nIn August 2017, the Nevada Public Utilities Commission held three days of hearings \"including multiple hours of cross examinations and increasingly technical questions on the application last week\" on the issue of net metering. The commission drafted and voted on an order that would implement regulations and rates for net metering customers, something that has been \"long-awaited\" by stakeholders (the vote was mandated by a bill passed by the state legislature during the 2017 legislative session). The purpose of that bill was to restore \"favorable reimbursement rates for net metering customers,\" people who use rooftop solar panels. Net metering customers receive a credit from the utility company when they generate more energy than they are using, and that energy gets sent back into the electric grid. The summer of 2017, and the year leading up to it, was a period of \"contentious back and forth between NV Energy, solar companies and state officials.\"\n\nThe previous version of net metering in Nevada was shut down by the state legislature and carried out by the Public Utilities Commission in December 2015. Under the new system, new rooftop solar customers will receive a reimbursement equal to a percentage of the retail price of electricity whenever they send their own energy back to the electric grid. That percentage rates are known as \"tranches\" and will start at 95% of the retail price for the first 80 MW of installed capacity.\n\nIn July 2017, NV Energy submitted a 376-page application that proposed to modify the rate structure for all of their customers. The application was criticized by rooftop solar companies including Tesla, Vivint Solar and a pro-solar political group called Vote Solar. The solar companies said that the rate structure would \"undercut their business model.\"\n\nAccording to Paul Thomsen, the chairman of NPUC, Governor Sandoval was disappointed at the commission’s decision.\n\nGreenpeace opposed NPUC’s action. Greenpeace called the Nevada Governor, Brian Sandoval, corrupt. The group wrote a post in its website with the headline \"What a Bought Politician in Nevada Means for the 2016 Presidential Race.\" The Nevada newspaper \"Reno News & Review\" wrote in an article that Greenpeace’s article does not support the headline. The article also states, \"Greenpeace lets the state legislators—who directed the PUC to act—off the hook entirely.\"\n\nHarry Reid, the Democratic leader of the U.S. Senate, condemned the commission’s decision during a political visit at the Washoe County Democratic headquarters. Reid said, \"Warren Buffett said it all. He said, 'People don’t buy utilities to get rich, they buy utilities to stay rich.'\"\n\nOn February 8, 2016, the commission held a hearing around whether to finalize or reconsider the commission’s new net metering rates. Outside the commission’s office, protesters gathered and a senior vice president of SolarCity spoke in support of the protesters. Three people carrying guns tried to enter the building to attend the hearing, but were turned away by security guards. The three people were wearing T-shirts that said “Bring Back Solar” and had wheelbarrows with petition cards. After they were turned away, they said they would be back at the next commission meeting with their guns.\n\nAccording to the \"Las Vegas Review-Journal\", an email was being circulated that contained the home addresses of the members of the commission.\n\nIn public statements, both Governor Sandoval and SolarCity requested civility and respect.\n\n\n"}
{"id": "21195964", "url": "https://en.wikipedia.org/wiki?curid=21195964", "title": "Oceanic physical-biological process", "text": "Oceanic physical-biological process\n\nDue to the higher density of sea water (1,030 kg m) than air (1.2 kg m), the force exerted by the same velocity on an organism is 827 times stronger in the ocean. When waves crash on the shore, the force exerted on littoral organisms can be equivalent to several tons.\n\nWater forms the ocean, produces the high density fluid environment and greatly affects the oceanic organisms.\n\n\nWater flow can be described as laminar or turbulent. Laminar flow is characterized by smooth motion: neighboring particles advected by such a flow will follow similar paths. Turbulent flow is dominated by re-circulation, whorls, eddies and apparent randomness. In such a flow particles which are neighbors at one moment can find themselves widely separated later.\n\nReynolds number is the ratio of inertial forces to viscous forces. As the size of an organism and the strength of the current increases, inertial forces will eventually dominate, and the flow becomes turbulent (large Re). As the size and strength decrease, viscous forces eventually dominate and the flow becomes laminar (small Re). \n\nBiologically there is an important distinction between plankton and nekton. Plankton are the aggregate of relatively passive organisms which float or drift with the currents, such as tiny algae and bacteria, small eggs and larvae of marine organisms, and protozoa and other minute predators. Nekton are the aggregate of actively swimming organisms which are able to move independently of water currents, such as shrimps, forage fish and sharks. \n\nAs a rule of thumb, plankton are small and, if they swim at all, do so at biologically low Reynolds numbers (0.001 to 10), where the viscous behaviour of water dominates and reversible flows are the rule. Nekton, on the other hand, are larger and swim at biologically high Reynolds numbers (10 to 10), where inertial flows are the rule and eddies (vortices) are easily shed. Many organisms, such as jellyfish and most fish, start life as larva and other tiny members of the plankton community, swimming at low Reynolds numbers, but become nekton as they grow large enough to swim at high Reynolds numbers.\n\nBernoulli's Principle states that for an inviscid (frictionless) flow, an increase in the speed of the fluid occurs simultaneously with a decrease in pressure or a decrease in the fluid's potential energy. \n\nOne result of Bernoulli's Principle is that slower moving current has higher pressure. This principle is used, for example, by some benthic suspension feeders. These smart guys dig holes like U tubes with one end higher than the other end. Because of bottom drag, as water flows over the bottom the lower tube opening has a lower fluid speed and thus a higher pressure than the upper tube opening. The benthic suspension feeder can hide in the tube as the pressure difference between the tube ends drives water and suspended particles through the tube. \n\nThe body shapes of many benthic creatures also exploit Bernoulli's Principle not only decrease the friction and drag but also to create lift when they move through the current.\n\nDrag is the tendency of an object to move in the direction of the flow. The magnitude of drag depends on the current velocity, the shape and size of the organism and the density of the fluid. Drag is a dissipative process which generally results in the generation of heat.\n\nIn sea water, drag can be decomposed into two different forms: skin friction and pressure drag.\n\nBesides being soft and flexible, organisms have other methods to minimize drag.\n"}
{"id": "27386754", "url": "https://en.wikipedia.org/wiki?curid=27386754", "title": "Paleosalinity", "text": "Paleosalinity\n\nPaleosalinity (or palaeosalinity) is the salinity of the global ocean or of an ocean basin at a point in geological history.\n\nFrom Bjerrum plots, it is found that a decrease in the salinity of an aqueous fluid will act to increase the value of the carbon dioxide-carbonate system equilibrium constants, (pK*). This means that the relative proportion of carbonate with respect to carbon dioxide is higher in more saline fluids, e.g. seawater, than in fresher waters. Of crucial importance for paleoclimatology is the observation that an increase in salinity will thus reduce the solubility of carbon dioxide in the oceans. Since there is thought to have been a 120 m depression in sea level at the last glacial maximum due to the extensive formation of ice sheets (which are solely freshwater), this represents a significant fractionation towards saltier seas during glacial periods. Correspondingly, this will cause a net outgassing of carbon dioxide into the atmosphere because of its reduced solubility, acting to increase atmospheric carbon dioxide by 6.5‰. This is thought to partly offset the net decrease of 80-100‰ observed during glacial periods.\n\nIn addition, it is thought that extensive salinity stratification can lead to a reduction in the meridional overturning circulation (MOC) through the slowing of thermohaline circulation. Increased stratification means that there is effectively a barrier to subduction of parcels of water; isopycnals effectively do not outcrop at the surface and are parallel to the surface. The ocean, in this case, can be described as \"less ventilated\", and this has been implicated in the slowing down of the MOC.\n\nThere may exist proxies for salinity, but to date the main way that salinity has been measured has been by directly measuring chlorinity in pore fluids. Adkins et al. (2002) used pore fluid chlorinity in ODP cores, with the paleo-depth estimated from nearby coral horizons. Chlorinity was measured rather than pure salinity because the major ions in seawater are not constant with depth in the sediment column; for example, sulfate reduction and cation-clay interactions can change overall salinity, whereas chlorinity is not heavily affected.\n\nAdkins' study found that global salinity increased with a global sea level drop of 120 m. Analyzing O data they also found that deep waters were within error of the freezing point, with oceanic waters exhibiting a greater degree of homogeneity in temperatures. In contrast, variations in salinity were much greater than they are today. Modern day salinities are all within 0.5 psu of the global average salinity of 34.7 psu, whereas salinities during the last glacial maximum (LGM) ranged from 35.8 psu in the North Atlantic to 37.1 in the Southern Ocean.\n\nThere are some notable differences in the hydrography at the LGM and present day. Today the North Atlantic Deep Water (NADW) is observed to be more saline than Antarctic Bottom Water (AABW), whereas at the last glacial maximum it was observed that the AABW was in fact more saline; a complete reversal. Today the NADW is more salty because of the Gulf Stream; this could thus indicate a reduction of flow through the Florida Straits due to lowered sea level.\n\nAnother observation is that the Southern Ocean was vastly more salty at the LGM than today. This is particularly intriguing given the assumed importance of the Southern Ocean in oceanic dynamical regulation of ice ages. The extreme value of 37.1 psu is assumed to be a consequence of an increased degree of sea ice formation and export. This would account for the increased salinity, but would also account for the lack of oxygen isotopic fractionation; brine rejection without oxygen isotopic fractionation is thought to be highly characteristic of sea ice formation.\n\nThe presence of waters near the freezing point alters the balance of the relative effects of contrasts in salinity and temperature on sea water density. This is described in the equation,\nwhere formula_2 is the thermal expansion coefficient and formula_3 is the haline contraction coefficient. In particular, the ratio formula_4 is crucial. Using the observed temperatures and salinities, in the modern ocean, formula_4 is about 10 whilst at the LGM formula_4 it is estimated to have been closer to 25. The modern thermohaline circulation is thus more controlled by density contrasts due to thermal differences, whereas during the LGM the oceans were more than twice as sensitive to differences in salinity rather than temperature. In this way, the thermohaline circulation can be considered to have been less \"thermo\" and more \"haline\".\n\n\n"}
{"id": "1276772", "url": "https://en.wikipedia.org/wiki?curid=1276772", "title": "Papa (mythology)", "text": "Papa (mythology)\n\nIn Cook Islands mythology of the southern Cook Islands group, the earth goddess Papa was created when Varima-te-takere, the primordial mother goddess, plucked her out from the left side of her body. Papa married her brother, the sky god Vatea. They had twin sons, the sea god Tangaroa and the vegetation god Rongo.\n\n"}
{"id": "1722616", "url": "https://en.wikipedia.org/wiki?curid=1722616", "title": "Physical body", "text": "Physical body\n\nIn physics, a physical body or physical object (or simply a body or object) is an identifiable collection of matter, which may be constrained by an identifiable boundary, and may move as a unit by translation or rotation, in 3-dimensional space.\n\nIn common usage, an object is a collection of matter within a defined contiguous boundary in 3-dimensional space. The boundary must be defined and identified by the properties of the material. The boundary may change over time. The boundary is usually the visible or tangible surface of the object. The matter in the object is constrained (to a greater or lesser degree) to move as one object. The boundary may move in space relative to other objects that it is not attached to (through translation and rotation). An object's boundary may also deform and change over time in other ways.\n\nAlso in common usage, an object is not constrained to consist of the same collection of matter. Atoms or parts of an object may change over time. An object is defined by the simplest representation of the boundary consistent with the observations. However the laws of Physics only apply directly to objects that consist of the same collection of matter.\n\nEach object has a unique identity, independent of any other properties. Two objects may be identical, in all properties except position, but still remain distinguishable. In most cases the boundaries of two objects may not overlap at any point in time. The property of identity allows objects to be counted.\n\nExamples of models of physical bodies include, but are not limited to a particle, several interacting smaller bodies (particles or other), and continuous media.\n\nThe common conception of physical objects includes that they have extension in the physical world, although there do exist theories of quantum physics and cosmology which may challenge this. In modern physics, \"extension\" is understood in terms of the spacetime: roughly speaking, it means that for a given moment of time the body has some location in the space, although not necessarily a point. A physical body as a whole is assumed to have such quantitative properties as mass, momentum, electric charge, other conserving quantities, and possibly other quantities.\n\nA body with known composition and described in an adequate physical theory is an example of physical system.\n\nAn object is known by the application of senses. The properties of an object are inferred by learning and reasoning based on the information perceived. Abstractly, an object is a construction of our mind consistent with the information provided by our senses, using Occam's razor.\n\nIn common usage an object is the material inside the boundary of an object, in 3-dimensional space. The boundary of an object is a contiguous surface which may be used to determine what is inside, and what is outside an object. An object is a single piece of material, whose extent is determined by a description based on the properties of the material. An imaginary sphere of granite within a larger block of granite would not be considered an identifiable object, in common usage. A fossilized scull encased in a rock may be considered an object because it is possible to determine the extent of the scull based on the properties of the material.\n\nFor a rigid body, the boundary of an object may change over time by continuous translation and rotation. For a deformable body the boundary may also be continuously deformed over time in other ways.\n\nAn object has an identity. In general two objects with identical properties, other than position at an instance in time, may be distinguished as two objects and may not occupy the same space at the same time (excluding component objects). An object's identity may be tracked using the continuity of the change in its boundary over time. The identity of objects allows objects to be arranged in sets and counted.\n\nThe material in an object may change over time. For example, a rock may wear away or have pieces broken off it. The object will be regarded as the same object after the addition or removal of material, if the system may be more simply described with the continued existence of the object, than in any other way. The addition or removal of material may discontinuously change the boundary of the object. The continuation of the objects identity is then based on the description of the system by continued identify being simpler than without continued identity.\n\nFor example, a particular car might have all its wheels changed, and still be regarded as the same car.\n\nThe identity of an object may not split. If an object is broken into two pieces at most one of the pieces has the same identity. An object's identity may also be destroyed if the simplest description of the system at a point in time changes from identifying the object to not identifying it. Also an object's identity is created at the first point in time that the simplest model of the system consistent with perception identifies it.\n\nAn object may be composed of components. A component is an object completely within the boundary of a containing object.\n\nIn classical mechanics a physical body is collection of matter having properties including mass, velocity, momentum and energy. The matter exists in a volume of three-dimensional space. This space is its extension.\n\nUnder Newtonian gravity the gravitational field further away than the furthest extent of an object is determined only by the mass and the position of the center of mass.\n\nInteractions between objects are partly described by orientation and external shape.\n\nIn continuum mechanics an object may be described as a collection of sub objects, down to an infinitesimal division, which interact with each other by forces which may be described internally by pressure and mechanical stress.\n\nIn quantum mechanics an object is a particle or collection of particles. Until measured, a particle does not have a physical position. A particle is defined by a probability distribution of finding the particle at a particular position. There is a limit to the accuracy with which the position and velocity may be measured. A particle or collection of particles is described by a quantum state.\n\nThese ideas vary from the common usage understanding of what an object is.\n\nIn particle physics, there is a debate as to whether some elementary particles are not bodies, but are points without extension in physical space within space-time, or are always extended in at least one dimension of space as in string theory or M theory.\n\nIn some branches of psychology, depending on school of thought, a physical body is a physical object with physical properties, as compared to mental objects. In (reductionistic) behaviorism, a physical body and its properties are the (only) meaningful objects of study.\nWhile in the modern day behavioral psychotherapy it is still only the means for goal oriented behavior modifications, in Body Psychotherapy it is not a means only anymore, but its felt sense is a goal of its own. In cognitive psychology, physical bodies as they occur in biology are studied in order to understand the mind, which may not be a physical body, as in functionalist schools of thought.\n\nA physical body is an enduring object that exists throughout a particular trajectory of space and orientation over a particular duration of time, and which is located in the world of physical space (i.e., as studied by physics). This contrasts with abstract objects such as mathematical objects which do not exist at any particular time or place.\nExamples are a cloud, a human body, a weight, a billiard ball, a table, or a proton. This is contrasted with abstract objects such as mental objects, which exist in the mental world, and mathematical objects. Other examples that are not physical bodies are emotions, the concept of \"justice\", a feeling of hatred, or the number \"3\". In some philosophies, like the Idealism of George Berkeley, a physical body is a mental object, but still has extension in the space of a visual field.\n\n"}
{"id": "50310130", "url": "https://en.wikipedia.org/wiki?curid=50310130", "title": "Polar Science", "text": "Polar Science\n\nPolar Science is a quarterly peer-reviewed scientific journal covering research related to the polar regions of the Earth and other planets. It is published by Elsevier on behalf of the National Institute of Polar Research (Japan). It covers a wide range of fields, including atmospheric science, oceanography, glaciology and environmental science. The editor-in-chief is Takashi Yamanouchi (National Institute of Polar Research and Sōkendai).\n\nThe journal is hybrid open access. By paying a fee, authors can choose to make their papers open access at the time of publication. All papers that are over 24 months old are freely available to the community via ScienceDirect.\n\nThe journal is abstracted and indexed in the Science Citation Index Expanded, Scopus, EBSCOhost, GEOBASE, GeoRef, ProQuest and Referativnyi Zhurnal (VINITI Database RAS). According to the \"Journal Citation Reports\", the journal has a 2016 impact factor of 1.118.\n"}
{"id": "1725699", "url": "https://en.wikipedia.org/wiki?curid=1725699", "title": "Radura", "text": "Radura\n\nThe Radura is the international symbol indicating a food product has been irradiated. The Radura is usually green and resembles a plant in circle. The top half of the circle is dashed. Graphical details and colours vary between countries.\n\nThe word \"Radura\" is derived from radurization, in itself a portmanteau combining the initial letters of the word \"radiation\" with the stem of \"durus\", the Latin word for hard, lasting.\n\nThe inventors of the symbol Radura - knowing this proposal for a new terminology - came from the former Pilot Plant for Food Irradiation, Wageningen, Netherlands, which was the nucleus for the later Gammaster today known as Isotron. The director at the time, R.M. Ulmann, introduced this symbol to the international community. Dr. Ulmann in his lecture also provided the interpretation of this symbol: denoting food - as an agricultural product - i.e., a plant (dot and two leaves) in a closed package (the circle) - irradiated from top through the package by penetrating ionizing rays (the breaks in the upper part of the circle).\n\nThe Radura was originally used in the 1960s exclusively by a pilot plant for food irradiation in Wageningen, Netherlands that owned the copyright. Jan Leemhorst, then president of Gammaster, untiringly propagated the use of this logo internationally. The use of the logo was permitted to everybody adhering to the same rules of quality. The symbol was also widely used by Atomic Energy of South Africa, including the labelling by the term 'radurized' instead of irradiated. By his intervention, the new logo was also included in the Codex Alimentarius Standard on irradiated food as an option to label irradiated food. Today it is found in the Codex Alimentarius Standard on Labelling of Prepacked Food.\n\nThe symbol Radura was originally used as a symbol of quality for food processed by ionizing radiation. The Dutch pilot plant used the logo as an identification of irradiated products and as a promotion tool for a high quality product with extended shelf life. In supermarkets where the irradiated mushrooms were on sale the logo was dominantly shown and buyers received a leaflet with information about the process and the advantages of the treated products. In clearances for other products granted by the Dutch authorities at later dates, application of the logo on the product or a clearly visible logo near treated bulk product was even demanded.\n\nFollowing the later interpretation by some food and process engineers, the symbol may also be read the following way:\n\n- The central dot is the radiation source. - The two circle segments ('leaves') are the biological shield to protect the workers and the environment. - The outer ring is the transport system, the lower half of it is shielded from radiation by the biological shield and resembles also the loading area, the upper broken half symbolizes the rays hitting the target goods on the transport system.\n\nPerceptions of the Radura are often intertwined with common perceptions of irradiation. Irradiation of food has not been widely adopted due to negative public perceptions, concerns expressed by some consumer groups and the reluctance of many food producers. Proponents of food irradiation have been frustrated by proposals to use international warning symbols for radiation hazard or bio-hazard since irradiated food does not pose any radiological or biological hazards.\n\nThe European Community does not provide for the use of the Radura logo and relies exclusively on labeling by the appropriate phrases in the respective languages of the Member States. Furthermore, irradiated ingredients have to be labeled even down to the last molecule contained in the final product; it is also required that restaurant food is labeled according to the same rule. Other countries and regions have varying regulations.\n\nAs part of its approval, the U.S. Food and Drug Administration (FDA) requires since 1986 that irradiated foods include labeling with either the statement \"treated with radiation\" or \"treated by irradiation,\" along with the Radura. In the USA, irradiation labeling requirements apply only to foods sold in stores. For example, irradiated spices or fresh strawberries should be labeled. Irradiation labeling does not apply to restaurant foods or processed foods. (NOTE: The Radura symbol as compulsory under FDA-rule has a design slightly different from the Codex Alimentarius version; the 'leaves' being empty areas.)\n\nSuch requirements are seen by consumer groups as helpful information to consumers concerned about food irradiation.\n\n"}
{"id": "1397700", "url": "https://en.wikipedia.org/wiki?curid=1397700", "title": "Rectilinear propagation", "text": "Rectilinear propagation\n\nElectromagnetic waves (light). Even though a wave front may be bent, (e.g. the waves created by a rock hitting a pond) the individual waves are moving in straight lines. In the sense of the scattering of waves by an inhomogeneous medium, this situation corresponds to the case n ≠ 1, where n is the refractive index of the material.\n\nAn experiment can be set up to prove this. Three cardboard squares are aligned with a small hole in the centre of each. A light is set up behind the cardboard. The light appears through all three holes from the other side. The light is blocked if any one of the cardboard squares are moved even a tiny bit. This proves that waves travel in straight lines and this helps to explain how humans see things, among other uses. It has a number of applications in real life as well.\n\n"}
{"id": "10403956", "url": "https://en.wikipedia.org/wiki?curid=10403956", "title": "Robert Garside", "text": "Robert Garside\n\nRobert Garside (born 6 January 1967), calling himself The Runningman, is a British runner who is credited by Guinness World Records as the first person to run around the world. Garside began his record-setting run following two aborted attempts from Cape Town, South Africa and London, England. Garside set off from New Delhi, India on 20 October 1997, completing his run back at the same point on 13 June 2003.\n\nWhile his run has been challenged by some ultra distance runners and some members of the press, subsequent publications clarified a number of the points raised, such as anomalies in his online diary, and his running of the Nullarbor plain without a support crew - a feat believed impossible according to classic ultrarunning methodologies but achieved using lateral thinking and relying upon passing traffic and local people to drop off water for him instead - and highlighted the clashes of personality, running approach, and actions, that had engendered the concerns.\n\nIn assessing his feat, \"Trailrunner\" senior editor Monique Cole stated he had clearly run more of the world than anyone else, while former media critic Dan Koeppel, who became one of the few journalists outside Guinness to discuss and examine his full records at length, became convinced by 2005 that Garside had indeed run around the world and expressed great remorse and \"haunting\" guilt at his past part in fuelling a media frenzy that, as he felt, \"screwed one of the greatest runners ever\" and \"erased... one of the most incredible things a runner had ever done\".\n\nGuinness World Records, who spent several years evaluating evidence, declared it authentic and the record was officially bestowed on Garside on 27 March 2007 at a ceremony in Piccadilly Circus, London, England.\n\nBorn in Stockport, Cheshire, England, Robert Garside attended Hillcrest Grammar School, where he was an all-round sportsman and captain of the soccer team, and after school, switched between several academic courses and jobs (including the Merchant Navy and police). Garside had become obsessed with running in the late 1980s, and while studying psychology at London's Royal Holloway University he described how he came across a copy of Guinness World Records in January 1995, and noticed that there was a record listed for walking the world, but not for running it. He decided to attempt to set a record as the first person to run around the world, an extreme ultramarathon feat.\n\nHe credited as part of his motive, his mother's happiness at leaving his father to return to her native country, Slovakia, following their divorce, when he was a teenager, and also finding that the state of mind he reached when running as an adult brought back some of his \"best times\" from childhood, where he ran and played in the \"huge forests\" near his house.\n\nGarside stated that his aim was to run for his own satisfaction as well as the record, therefore he set about running each continent the longest way possible, rather than the easiest way to gain the record. His run covered around 40,000 miles across 6 continents and 29 countries. Koeppel notes that attempting to run Africa was entirely voluntary, and that the renowned first walk around the world, by David Kunst 30 years earlier covered barely a third of that distance (14,452 miles) and skipped South America and Africa. In a 1998 video interview Garside added that he was motivated because it was \"so challenging\", clarifying that he meant it was challenging to himself.\nGarside's first effort from Cape Town, South Africa, in early 1996 was abandoned in Namibia, and his second attempt, begun on 7 December 1996, started from London's Piccadilly Circus but was abandoned at the Russia-Kazakhstan border around June 1997; Garside initially covered up the break in running with fabricated diary entries (see below), for which he later apologised saying that he had not wanted potential competitors to know of the lapsed progress. He recommenced his run some weeks later, from New Delhi, India.\n\nIt was therefore his third attempt, initiated on 20 October 1997 from the monument of India Gate in New Delhi, that was eventually authenticated by Guinness as a successful record.\n\nDuring his run, Garside updated his website with a portable computer, describing an arduous journey complicated by human and natural hurdles that included physical attacks and imprisonment as well as grueling climate extremes. He met with considerable assistance, as he was offered lodgings around the globe in such diverse settings as five-star hotels and private homes to prison cells and police stations. In addition to corporate sponsorship of £50,000, he indicated he received £120,000 in donations from individuals. One donor in Hong Kong agreed to back Garside in return for a share in future profits. Along the way, Garside also met his future wife, then Endrina Perez, in Venezuela.\n\nGarside indicated in 2001 that it was his habit to jog seven to eight hours a day, covering an average of forty miles a day when running on flat ground, outfitted with a video camera to record his journey and a fifteen-pound backpack. On his third run, he used his video camera every 20 minutes while running to take a four-minute clip of his location, and routinely requested signed, dated documents from local officials.\n\nGarside completed his world-traversing journey on 13 June 2003 at the monument of India Gate, at which time \"The Independent\" reported the total miles run over five and a half years at 35,000 (approximately 56,000 kilometers), covering territory in 30 countries. Near the end of his run, Garside indicated that the worst experiences he'd encountered were three days spent running without any food and five days spent in jail in China because he lacked proper documentation. He described running over the Himalayas as \"fantastic\" in spite of freezing temperatures, \"the most spiritual of mind journeys.\"\n\nGarside's equipment and funding was described by Koeppel and also in an August 2000 article, when he was in Central America. Koeppel states that Garside ran with around 15 pounds (6.8 kg) of equipment in a backpack, and started with around £20 ($30) of money; the latter report broadly reports similarly. His possessions were a palmtop computer, digital video camera, map, toothbrush, change of clothing, and a hat - the article comments that to reduce weight he did not keep a water bottle, instead finding water sources as he ran, including at times natural sources such as rivers and puddles. Other reports add to these, a cellphone, a music player or music playing phone, a camera, and passport visas and paperwork.\n\nHe had also learned, Garside stated, to raise sponsorship as he ran; the August 2000 article states he had raised around $90,000 by selling interviews and his story to media as he travelled, by the time of the interview.\n\nThere had also been running companions and girlfriends on the journey, as well as support and help from the public: \"[P]eople always help you out\" Garside commented.\n\nRegarding the physical toll of ultra-distance running, Garside stated that his choice of approach was key, although commenting it took him two years to recover afterwards:\n\nHe also commented on the experience of running itself:\n\nA number of people who casually ran alongside Garside for a time, or testified to his running, were also quoted in various media, for example:\n\nThe record breaking run was originally commenced in 1996 at London's Piccadilly Circus (although this is now generally considered his second attempt). According to Garside, at the Russia-Kazakhstan border he received a plea from a former girlfriend to be with her during a family medical emergency involving her mother. Returning to London, he resumed his run in October 1997 from what he deemed his new starting point, in New Delhi, India. In 2001 faced with questions about his records, he admitted that some diary entries from around June 1997, prior to his restart in New Delhi, describing colorful adventures in Kazakhstan, Uzbekistan, Tajikistan, Pakistan and Afghanistan had been fabricated to hide his diversion home from competitors. He stated of the matter that the break it covered was intended to be brief and within rules: \"It's a tactic ... I suppose it's a lie. I'm sorry about that\". He had also previously explained the matter as abandonment due to the 1992-1996 civil war in Afghanistan.\n\nOther controversies highlighted on his verified run mainly related to flights omitted from his online diary - one of which made Garside appear to have run at world record speeds in Mexico - and being found in locations which contradicted his online information. Garside commented on these that he had not always updated his online diary promptly, and at times had made repeated or unplanned brief air trips which were not reflected in the online record.\n\nOne such incident was his meeting with Ronnie Biggs, a famous British criminal, on the coast of Rio de Janeiro, when according to his diary he was supposed to be in the Amazon rainforest. Garside quipped that he had decided to go and see Biggs because \"he's on the run - and so am I!\"  and that his 3-month visa had expired by Manaus, requiring back-and-forth flights to several cities to rectify the problem before resuming, which had not been documented online. The \"New York Press\" reported that the \"Guardian\"'s Brazilian correspondent Alex Bellos had confirmed the visa expiry as being genuine. Other incidents, such as the flight in Mexico, did not damage his record attempt as they were allowed by Guinness' criteria, but were seized on by an already skeptical audience. Garside - often running thousands of miles away or in isolated places, poorly disposed towards much of the broader running community, solitary, secretive about adverse events affecting his progress, and at times abrasive in his responses - was often poorly placed to handle these appropriately, and at times responded with invective or numerous phone calls instead.\n\nEven before Garside completed his third run, some ultra distance runners and press media had questioned his achievement, in particular because he seemed to be an individual without recognized prior ultrarunning experience and who had lacked the usual help, and some of his claims seemed too remarkable to be plausible. Some of these were addressed in dialog between Garside and Dan Koeppel, a former critic, after the completion of the run.\n\nA 2002 article in \"Sports Illustrated\" described media and the running community's concerns in depth, saying that \"[His] 'little white lies' have led to bigger and grayer ones (he has been forced to retract [other claims]), so that now nobody knows what, if anything, he says is true\", and characterised him as a \"self-mythologizer\"; a former ally was quoted in the same article as opining that Garside was being \"destroyed\" before he had finished the run, by his \"readiness to deceive\". The article's author considered this to have created a problem that, as of 2002, while \"no one dispute[d]\" Garside had run a great distance, equally nobody could be certain how many of those miles he claimed to run but had not. Garside himself said only that the records and evidence he was sending home periodically would bear out his side of the dispute in the end. According to Andy Milroy, an ultramarathon claim authenticator of 25 years experience, this was an especially severe concern in a world run, as \"one bit of jungle, one bit of shrub, one bit of road looks like any other\", for a runner lacking a support team or stipulated route, the usual means of validation.\n\nOne major critic of Garside was David Blaikie, editor of now-defunct Canadian website \"Ultramarathon World\" and former president of the Association of Canadian Ultramarathoners, who according to Koeppel \"wielded huge influence\" and as a critic became Garside's \"primary nemesis\". He expressed disbelief stating \"I do not believe ... that he has fully run any of the major sections of the world he has claimed, or even a substantial portion of any section.\" Blaikie also cited the lack of any support team or helpers to help him carry food and water and his lack of experience with ultramarathons as reasons to doubt Garside's claims. Steven Seaton, then editor of \"Runner's World\", also pointed out Garside's lack of previous experience with ultramarathons, saying, \"Some of the things he has claimed to have achieved would constitute world records for ultrarunning, which is nonsense for somebody who is claiming to have run almost every day. He went into this with no outstanding ultra-credentials, which makes it difficult to believe what he claims to have done.\" A demonstration for \"Richard and Judy\" on Channel 4 in the UK, for which Garside agreed to rerun the 130 miles he stated he had run in 24 hours (a routine distance for an ultrarunner), observed by witnesses including Ian Champion of the London to Brighton Run and UK Road Runners Club, resulted in Garside pulling out after 72 miles. As well as hiding his 1997 restart, Garside had also apparently admitted to shortening his route by 1300 km by taking an airplane from Mexico City to the United States border; his diary left the impression of 10 days to run 1300 km, a world record if true. Jesse Dale Riley of the Trans-American Footrace expressed concern that Garside's records showed him crossing 746 miles of the Nullarbor Plain without a support crew, stating that \"I know a lot of people who have crossed the Nullarbor but I've never heard of anyone doing it alone. The issue of water supply alone casts serious doubt. It's totally inconceivable to me how anyone could do such a thing and survive\".\nKoeppel, investigating the latter, traced the discrepancy to a matter of running philosophy: Garside, who ran for pleasure and took a far longer route than he needed to, had not approached his run as a competitive athlete would, and had used strategies that formal athletic approaches would not have conceived. Where Blaikie, Riley and other ultrarunners saw the Nullarbor as unrunnable without support, Garside explained to Koeppel that the Nullarbor was \"no tougher than anywhere else\", because he obtained support from \"passing traffic\" who would leave water cached ahead for him at agreed drop-offs, or give him transport to sleep elsewhere after a day's running and take him back to resume running the next day from the same place he had stopped. He commented in his diary that \"the key to running the Nullarbor turned out to be Australian hospitality\", a statement confirmed by Koeppel in 2010 when he succeeded in vindicating Garside's strategy by running its 200-mile driest zone himself the same way, and contacted others who saw Garside run it.\n\nKoeppel also found that contrary to prior claims, Garside indeed had a prior record as a runner and in particular as a sub-3 hour marathon runner, including three well-known marathons where he had \"done well\" in 1994 with times of 3.01 (London Marathon), 2.48 (Brussels Marathon) and 3.10 (Amsterdam Marathon) respectively; Garside's comment on his televised demonstration (ended at 72 miles of 130) was that mentally and emotionally, \"running in circles\" round a track - which he had not done before - had not been at all like long distance cross-country running, and was \"demoraliz[ing]\"; Ian Champion softened his opinion on the matter as a result, commenting that it could indeed have been \"situational\".\n\nThe \"New York Press\" commented on the controversies upon Garside's 2001 arrival in New York. They stated that Guinness did not require running where roads did not exist, or unreasonable feats, but noted that his undisclosed use of air flight at times - notably in parts of Central and South America - led to \"the British press... ripping into him\". Lengthy and vitriolic animosity between Garside and Canadian ultrarunner and reporter David Blaikie, who had become a \"huge critic\", was also noted, as were statements by third party runners and businesses who paced Garside and supported his claims. The article quotes \"Trailrunner\" senior editor Monique Cole:\n\nGarside's former manager or patron, photo agent Mike Soulsby, agreed with the assessment, stating to Dan Koeppel that he had no financial interest in Garside, who owed him money. Looking back, he provided what Koeppel felt might be \"the definitive statement\" on Garside, apart from Garside's own:\n\nUpon announcement of authentication in 2007, there was a measure of concern that the feat should be scrutinized carefully or seemed dubious. \"The Guardian\" quoted Ian Champion of the UK Road Runners Club, who had been called upon to judge Garside's uncompleted supervised 24-hour road test in April 2004, as saying he was \"stunned\" at the decision, with the paper noting his non-completion of the 130-mile 24-hour run in near-ideal conditions under observation. \"Reuters\" described the award as \"a major vindication for Garside\".\n\nIn August 2012's \"Runner's World\", journalist and runner Dan Koeppel published a lengthy \"apologia\" over his role in helping discredit Garside's world run, for which he felt great remorse. Titled \"Redemption of the Runningman\" and subsequently anthologized in \"The Best American Sports Writing 2013\", it tells the story of how, not long after the run, Koeppel had come to regret the attacks upon Garside as a \"media lynch mob\" that he himself had helped to ignite, and the erasure of \"one of the most incredible things a runner had ever done\", his changed belief that \"Garside did run the Nullarbor\", and that he wanted to \"make amends\" to Garside for the \"haunting\" sense of guilt he felt for having \"screwed one of the greatest runners ever\".\n\nIn his article, Koeppel recounted how he had favoured Blaikie's style, as a reputed and smooth-mannered reporter and runner who \"seemed credible\", to Garside's abrasive style, and had not paused to consider both sides fairly, thereby making \"a classic journalistic error\" when Blaikie \"built a perfect journalistic campaign against Garside\". Eventually he stated, \"When I got the chance to see the evidence, he'd clearly been to all the places he claimed to have been - and he'd moved at a runner's pace\". Later, meeting Garside in London, he was given full access to copy the runner's logs, photographs and records as well as confirming Garside's past running record (including three \"well run\" and well-known marathons from 1994 timed between 2:48 and 3:10), and contacted people worldwide who confirmed following Garside for many tens of miles at a time, and in places \"arguably more inhospitable than the Nullarbor\". Finding that Guinness, having accepted the record as genuine in 2007, had \"rested\" it (removed it from their public records), Koeppel began attempting to reverse the decision, and, when Garside became uncontactable in 2010, he decided to go further and challenge Blaikie's premise that the Nullarbor was unrunnable, by successfully running 200 miles across the heart of the Nullarbor - its \"loneliest, driest, emptiest\" zone - himself using Garside's strategy, and relying on support from passing drivers rather than a formal crew.\n\nGuinness' criterion for a recognized world record required Garside to run the equivalent distance of around the world, covering both North and South hemispheres and all but the Arctic and Antarctic continents:\n\nAccording to 2009's \"Getting Into Guinness\", Guinness permits rest days and ship or plane travel across bodies of water in epic journeys, and according to Canada's \"The Globe and Mail\" an average speed of no less than 10 km/h (6.25 mph) is required when running, to avoid being classified as 'walking'.\n\nGuinness World Records began considering evidence of Garside's record, evaluating the journey that began in New Delhi on 20 October 1997, after his detour to spend time in the UK with his girlfriend, including China, Japan, Australia, South America, North America, Africa, southern Europe, and the Middle East. In 2007, Guinness authenticated and recognized Garside's run, formally listing him as the first person to run around the world, declaring they were quite satisfied with the evidence evaluated, and that their conclusion was that:\n\nThe \"Daily Mail\" quoted Guinness' Head of Records in 2007 as commenting that, in authenticating the record, \"\"his team has spent more than three years \"[Note: press release says \"5 years\"]\" examining 15 boxes of credit card receipts, 300 time-coded tapes and many random calls to independent witnesses ...[and]... was in close contact during every leg of the journey\"\". They also quote Guinness on verifying other points, such as his 2001 diversion from Mozambique to Morocco.\n\nThe record was officially observed on 27 March 2007 at a ceremony in Piccadilly Circus, where representatives of Guinness endorsed the record. Garside said, \"I'm really happy about this, this run cost me everything.\" \n\nIn 2003, Garside indicated his intention to follow up his record-setting run by running across the Antarctic and swimming around the globe, with intentions to embark on the latter in June 2004. Garside married his girlfriend in London in 2004, having met her in Venezuela in 2000.\n\n\n"}
{"id": "19760932", "url": "https://en.wikipedia.org/wiki?curid=19760932", "title": "Sprengel pump", "text": "Sprengel pump\n\nThe Sprengel zinti pump is a vacuum pump that uses drops of mercury falling through a small-bore capillary tube to trap air from the system to be evacuated. It was invented by Hanover-born chemist Hermann Sprengel in 1865 while he was working in London. The pump created the highest vacuum achievable at that time, less than 1 mPa (approximately 1×10 atm).\nThe supply of mercury is contained in the reservoir on the left. It flows over into the bulb \"B\", where it falls in drops into the long tube on the right. These drops entrap between them the air in \"B\". The mercury which runs out is collected and poured back into reservoir on the left. In this manner practically all the air can be removed from the bulb \"B\", and hence from any vessel \"R\", which may be connected with \"B\". At \"M\" is a manometer which indicates the pressure in the vessel \"R\", which is being exhausted.\n\nFalling mercury drops compress the air to atmospheric pressure which is released when the stream reaches a container at the bottom of the tube. As the pressure drops, the cushioning effect of trapped air between the droplets diminishes, so a hammering or knocking sound can be heard, accompanied by flashes of light within the evacuated vessel due to electrostatic effects on the mercury.\n\nThe speed, simplicity and efficiency of the Sprengel pump made it a popular device with experimenters. Sprengel's earliest model could evacuate a half litre vessel in 20 minutes.\n\nWilliam Crookes used the pumps in series in his studies of electric discharges. William Ramsay used them to isolate the noble gases, and Joseph Swan and Thomas Edison used them to evacuate their new carbon filament lamps. The Sprengel pump was the key tool which made it possible in 1879 to sufficiently exhaust the air from a light bulb so a carbon filament incandescent electric light bulb lasted long enough to be commercially practical. Sprengel himself moved on to investigating explosives and was eventually elected as a Fellow of the Royal Society.\n\n"}
{"id": "1166646", "url": "https://en.wikipedia.org/wiki?curid=1166646", "title": "Storage heater", "text": "Storage heater\n\nA storage heater or heat bank (Australia) is an electrical heater which stores thermal energy during the evening, or at night when base load electricity is available at lower cost, and releases the heat during the day as required.\n\nStorage heaters are typically composed of clay bricks or other ceramic material (grog), of concrete walls, or of water containers. There are also special materials such as feolite. This material serves as a heat storage medium. There are electrical heating elements embedded in the material which can be switched on to heat the storage medium and thus to store energy.\n\nThe stored heat is given off continuously (through thermal radiation and convection). To speed up the heat transfer, storage heaters may come equipped with mechanical fans that can move air through the heater; see the section on fan-assisted storage heaters.\n\nHigh heat retention storage heaters (HHRSH) are the newest and most advanced storage heaters on the market. Dimplex pioneered this technology with the Quantum, which led to a new category being created in SAP. The UK government approved software which calculates the heating system that can be used in a new building or refurbishment. High heat retention storage heaters are able to retain more heat than traditional storage heaters, with a minimum of 45% heat retention 24 hours after a full charge. This significantly reduces the heat wasted during the day, and is achieved through improved insulation. High heat retention storage heaters also include smart controls and monitor climatic conditions to estimate future heat demand, making them much more responsive to changes in their environment than traditional storage heaters. All high heat retention storage heaters are also Lot 20 compliant, in line with the EcoDesign regulations which came into force on the 1 January 2018 (please see below).\n\nLot 20 is an EU legislation that was designed to remove inefficient heating technologies from the market and reduce the energy used by products that heat our homes. The goal of the legislation is to achieve the UK's overall carbon reduction targets and has been in place since 1 January 2018. It states that all installed electric heating products manufactured from 1 January 2018 must have an electronic thermostat with a 24-hour, 7-day timer with either adaptive start or an open window sensor. Storage heaters must include: an electronic heat charge control with room and/or outdoor temperature feedback or controlled by energy supplier, an electronic room temperature control plus week timer and a fan assisted output. Optional additional compliance features are distance control, adaptive start and open window detection. Non-compliant stock that was manufactured before this date may still be sold.\n\nStorage heaters are usually used in conjunction with a two-tariff electricity meter which records separately the electricity used during the off-peak period so that it can be billed at a lower rate. In order to enjoy the lower rates, the house must be on a special electricity tariff. In most countries, storage heaters are only economical (compared to other forms of heating) when used with such a special tariff. In the United Kingdom the Economy 7 tariff is appropriate.\n\nStorage heaters usually have two controls: a charge control (often called \"input\"), which controls the amount of heat stored, and the draught control (often called \"output\"), which controls the rate at which heat is released. These controls may be controlled by the user, or may operate automatically once the user selects the target room temperature on a thermostat.\n\nStorage heaters may also incorporate an electric heater (utilizing either resistance heaters or heat pumps), which can be used to increase heat output. Such added heating, if it is resistance heating, is expensive, as it occurs during the high-tariff time of day.\n\n\n\nStorage heaters can be cost-effective if used properly, but they require more attention than fuel-fired systems.\n\nStorage heaters can provide two power circuits, one for on-peak and one for off-peak electricity, and two power switches, which are switched off during the summer when heat is not required. During other months the off-peak switch can be left on at all times, with the on-peak switch being used when insufficient energy has been stored during off-peak times. The amount of heat that is stored can be altered using the controls on the storage heater unit.\nNormally the on-peak will have a fuse as it is part of another circuit. The off-peak will just be a switch as it has a dedicated circuit.\n\nBasic storage heaters have an input switch, and an output switch, called \"heat boost\" on some models.\n\nThe position of the input switch may be changed to reflect how cold the next day is predicted to be. The input switch is normally thermostatic, cutting off the charge when the room reaches a certain temperature overnight. The exact setting needed will depend on the size of the storage heater, the desired room temperature during the day, the number of hours that this needs to be maintained, and the room's rate of heat loss under a given set of circumstances. Some experimenting may be needed to find the relationship between forecast outside temperature and best input setting for a particular room. Most storage heater users follow simpler guidelines; for example, in the middle of winter, it is often appropriate to turn the input switch to its maximum setting. There is no need to touch the input switch on a daily basis if the same sort of weather prevails for weeks at a time. There is no need to touch the input switch during the day, as storage heaters only use electricity at night.\n\nThe output switch may require attention throughout the day. Before going to bed, the operator should switch the output to its minimum setting. This keeps as much heat in the bricks as possible. Enough will leak out into the room to make it warm in the morning. Only in exceptionally cold circumstances will the operator require output overnight. The operator may wish to slowly increase the output switch during the day to try to maintain the temperature in the house. Increasing the output will allow the heat to convect out of the heater. If the house is empty during the day, the output should be left at a minimum all day and then switched up when returning to the house in order to let more heat escape.\n\nMany storage heaters also have a mechanically-controlled automatic output switch. In this case, if the manual output switch is not set to minimum overnight, the damper will automatically close (as if the output switch had been set to minimum), and then the damper will re-open after a time delay; this time delay is measured by the gradual drop in the heater's core temperature, and is therefore longer if the core temperature started higher due to more charge. The delay can also be biased by the output switch's setting. Some output switches that are set up this way are marked \"early\" and \"late\" as well as \"closed\" and \"open\"; the minimum \"closed\" setting corresponds with \"early\" and the maximum \"open\" setting corresponds with \"late\". These output switches can be controlled manually by ensuring they are closed at night and opened when desired, or they can be left to automatic operation by not closing at night.\n\nA thermostatic storage heater will automatically regulate the temperature in a room throughout the day. However, the operator may wish to switch the thermostatic switch to the minimum setting overnight to lower the room temperature. If the room is empty during the day, it is better to keep the thermostat at the minimum setting and then increase the setting when the room is occupied in the evening. Some thermostatic heaters also make use of on-peak electricity when there is not enough stored heat to maintain the requested temperature; the user may wish to be aware of this and lower the settings.\n\nFan-assisted storage heaters employ an electric fan to drive air through the heater rather than relying on convection. The fan is usually controlled by a thermostat which allows the user to set the desired room temperature. The use of the fan means these heaters can be insulated more than other models, and therefore lose less heat due to heat transfer at times when heat is not required (such as when the room is not occupied, or at night).\n\nIn common with other forms of direct electric heating, storage heaters are not necessarily environmentally friendly because the source of electricity may be generated using fossil fuels, with up to two-thirds of energy in the fuel lost at the power station and in transmission losses. In Sweden the use of direct electric heating has been restricted since the 1980s for this reason, and there are plans to phase it out entirely—see Oil phase-out in Sweden—while Denmark and Germany have banned the installation of electric space heating in new buildings for similar reasons (though in Germany the ban was lifted in 2013).\n\nIn the UK, a storage heater earns a \"Poor\" rating for Environmental Performance on an Energy Performance Certificate. However many progressive countries are developing their electricity generating system, principally, to incorporate \"greener\", more sustainable and renewable energy sources; so how \"green\" a storage heater system is would in principle depend on how the electricity used is generated. Of course this argument applies to all forms of electric heating, but the ability of a storage heater system to use electricity at times when, for example, wind generated electricity could not otherwise be used, may in conjunction with a smart grid give storage heating a new role in the future.\n\nIn some countries, the current design of the electrical generating system may result in a surplus of electricity from base load power stations during off-peak periods, and storage heaters may then be able to make use of this surplus to increase the net efficiency of the system as a whole. However, future changes in supply and demand—for example as a result of energy conservation measures or a more responsive generating system—may then reverse this situation, with storage heaters preventing a reduction in the national base load. Other technologies may incorporate demand response electronics to sense when there is a change in supply and demand. Thereby, they ensure that these loads only use off-peak electricity. Further advances in supply technology could provide for a more bespoke 'supply and demand' tariff system to make smart grid sensing technologies like dynamic demand a more viable financial prospect.\n\nCompared to other forms of electric heating, storage heaters are cheaper to run and they impose lower peak loads. The highest peak loads come from instantaneous electric heating, such as immersion water heaters, which create heavy loads for short durations, although instantaneous water heaters may use less electricity overall. High-efficiency ground source heat pumps are able to use up to 66% less electricity than storage heaters in heating by recovering heat from the ground, and are regarded as preferable even though they use electricity throughout the day. These are not to be confused with air conditioning (A/C) heat pumps which are now considered to be an environmental liability in some, (in particular hotter climate), countries.\n\nWhere alternatives to electricity exist, hot-water central heating systems can use water heated in or close to the building using high-efficiency condensing boilers, biofuels, heat pumps or district heating. Ideally hot water heating should be used. This can be converted in the future to use other technologies such as solar panels, so also providing future-proofing. In the case of new buildings, low-energy buildings such as those built to the Passive House standard can eliminate almost all the need for conventional space heating systems.\n\n"}
{"id": "34208684", "url": "https://en.wikipedia.org/wiki?curid=34208684", "title": "Tanzania Craton", "text": "Tanzania Craton\n\nThe Tanzania Craton is an old and stable part of the continental lithosphere in central Tanzania.\nSome of the rocks are over 3 billion years old.\n\nThe Tanzania Craton forms the highest part of the East African Plateau.\nThe craton is surrounded by Proterozoic mobile belts of various ages and grades of metamorphism.\nThese include the Ubendian, Usagaran, Karagwe-Ankolean and Bukoban systems. The Mozambique Belt lies to the east.\nThe craton divides the east and west branches of the East African Rift.\nThe southern end of the Gregory Rift Valley terminates against the craton. The volcanic area of this rift covers the surface interface between the Mozambique orogenic fold belt and the Tanzania Craton. A superplume exists beneath the craton. An indirect effect of rift and plume associated volcanism in the Tanzania Craton is the high levels of soil nutrients in Serengeti provided by volcanic ash from Ol Doinyo Lengai.\n\nThe craton is a composite of several different terranes of Archaean metasediments. The Dodoman is the oldest, and others include the Nyanzian and Kavirondian. Some of the greenstone belts are more than 3 billion years old. They were intruded by granites and migmatized in different events that date back to 2.9, 2.7, 2.4 and 1.85 billion years ago.\nThe craton mainly consists of Archaean granitic complexes, but also includes rocks from the Dodoma System in the central area, and belts of greenstone to the south and east of Lake Victoria.\nGneisses, schists, quartzites, migmatites, amphibolites and granulite are also found.\nThere was widespread intrusion of kimberlites in the Cretaceous Period, mostly in the part of the craton that lies south of Lake Victoria.\nThis includes the Mwadui kimberlite pipe in the center of Tanzania.\n\n"}
{"id": "3377322", "url": "https://en.wikipedia.org/wiki?curid=3377322", "title": "Viking (satellite)", "text": "Viking (satellite)\n\nViking was Sweden's first satellite. It was launched on an Ariane 1 rocket as a piggyback payload together with the French satellite SPOT 1, on February 22, 1986. Operations ended on May 12, 1987. Viking was used to explore plasma processes in the magnetosphere and the ionosphere.\n\nSpace was limited underneath the SPOT 1 satellite, and Viking had to be quite sturdy in order to withstand the stress of launch. The basic shape of the Swedish satellite was a flat octagonal disc, 0.5 metres thick and 1.9 metres across. The mechanical interface of the payload adapter from the Ariane rocket was duplicated on top of Viking. This enabled it to be added to the launch with a minimum of redesign of the SPOT satellite. After SPOT had been released, Viking fired its own rocket engine and was sent into its proper polar orbit.\n\nOnce in orbit, 4 wire segments of 40 metre length each were spooled out in a radial direction from the edge of the spinning satellite disc. Also, 2 stiff rods of 4 metre length were extended in the axial direction. A sensor pod was stationed at the end of each of these, forming three orthogonal pairs. Together they could measure the electric field of the earth in all three dimensions. Stiff booms were also extended for other types of sensors and antennas.\n\nThe mission produced a large amount of extremely useful scientific data, and was deemed a great success. An initial discussion of what scientists learned from these measurements, which included \"global distribution of magnetosphere-ionosphere interaction; auroral morphology and substorm dynamics; heating and expulsion of ionospheric plasma into the magnetosphere; field aligned acceleration into the ionosphere; and electron and ion wave generation\" can be found in the article \"Scientific results from the Swedish Viking satellite\", 1988 Swedish Inst. of Space Physics, Kiruna. Since then, many other scientific articles have been written using this important data set.\n\nAfter the scientific mission ended, both \"Viking\" and the upper stage of the rocket used to launch the satellite became objects that would continue to orbit Earth for many years. , both objects remain in orbit.\n\n"}
{"id": "396275", "url": "https://en.wikipedia.org/wiki?curid=396275", "title": "Wilderness", "text": "Wilderness\n\nWilderness or wildland is a natural environment on Earth that has not been significantly modified by human activity. It may also be defined as: \"The most intact, undisturbed wild natural areas left on our planet—those last truly wild places that humans do not control and have not developed with roads, pipelines or other industrial infrastructure.\" The term has traditionally referred to terrestrial environments, though growing attention is being placed on marine wilderness. Recent maps of wilderness suggest it covers roughly one quarter of Earth's terrestrial surface, but is being rapidly degraded by human activity. Even less wilderness remains in the ocean, with only 13.2% free from intense human activity. \n\nSome governments establish them by law or administrative acts, usually in land tracts that have not been modified by human action in great measure. The main feature of them is that human motorized activity is significantly restricted. These actions seek not only to preserve what already exists, but also to promote and advance a natural expression and development. Wilderness areas can be found in preserves, conservation preserves, National Forests, National Parks and even in urban areas along rivers, gulches or otherwise undeveloped areas. These areas are considered important for the survival of certain species, biodiversity, ecological studies, conservation, solitude, and recreation. Wilderness is deeply valued for cultural, spiritual, moral, and aesthetic reasons. Some nature writers believe wilderness areas are vital for the human spirit and creativity. They may also preserve historic genetic traits and provide habitat for wild flora and fauna that may be difficult to recreate in zoos, arboretums or laboratories.\n\nThe word \"wilderness\" derives from the notion of \"wildness\"—in other words, that which is not controlled by humans. The mere presence or activity of people does not disqualify an area from being \"wilderness.\" Many ecosystems that are, or have been, inhabited or influenced by activities of people may still be considered \"wild.\" This way of looking at wilderness includes areas within which natural processes operate without human interference.\n\nThe WILD Foundation states that wilderness areas have two dimensions: they must be biologically intact and legally protected. The World Conservation Union (IUCN) classifies wilderness at two levels, Ia (Strict Nature Reserves) and Ib (Wilderness Areas). Activities on the margins of specific wilderness areas, such as fire suppression and the interruption of animal migration also affect the interior of wildernesses.\n\nEspecially in wealthier, industrialized nations, it has a specific legal meaning as well: as land where development is prohibited by law. Many nations have designated wilderness, including the United States, Australia, Canada, New Zealand, and South Africa. Many new parks are currently being planned and legally passed by various Parliaments and Legislatures at the urging of dedicated individuals around the globe who believe that \"in the end, dedicated, inspired people empowered by effective legislation will ensure that the spirit and services of wilderness will thrive and permeate our society, preserving a world that we are proud to hand over to those who come after us.\"\nLooked at through the lens of the visual arts, nature and wildness have been important subjects in various epochs of world history. An early tradition of landscape art occurred in the Tang Dynasty (618-907). The tradition of representing nature \"as it is\" became one of the aims of Chinese painting and was a significant influence in Asian art. Artists in the tradition of Shan shui (lit. \"mountain-water-picture\"), learned to depict mountains and rivers \"from the perspective of nature as a whole and on the basis of their understanding of the laws of nature… as if seen through the eyes of a bird.\" In the 13th century, Shih Erh Chi recommended avoiding painting \"scenes lacking any places made inaccessible by nature.\"\n\nFor most of human history, the greater part of the Earth's terrain was wilderness, and human attention was concentrated in settled areas. The first known laws to protect parts of nature date back to the Babylonian Empire and Chinese Empire. Ashoka, the Great Mauryan King, defined the first laws in the world to protect flora and fauna in Edicts of Ashoka around 3rd Century B.C. In the Middle Ages, the Kings of England initiated one of the world’s first conscious efforts to protect natural areas. They were motivated by a desire to be able to hunt wild animals in private hunting preserves rather than a desire to protect wilderness. Nevertheless, in order to have animals to hunt they would have to protect wildlife from subsistence hunting and the land from villagers gathering firewood. Similar measures were introduced in other European countries.\n\nThe idea of wilderness having intrinsic value emerged in the Western world in the 19th century. British artists John Constable and J. M. W. Turner turned their attention to capturing the beauty of the natural world in their paintings. Prior to that, paintings had been primarily of religious scenes or of human beings. William Wordsworth’s poetry described the wonder of the natural world, which had formerly been viewed as a threatening place. Increasingly the valuing of nature became an aspect of Western culture.\n\nBy the mid-19th century, in Germany, \"Scientific Conservation,\" as it was called, advocated \"the efficient utilization of natural resources through the application of science and technology.\" Concepts of forest management based on the German approach were applied in other parts of the world, but with varying degrees of success. Over the course of the 19th century wilderness became viewed not as a place to fear but a place to enjoy and protect, hence came the conservation movement in the latter half of the 19th century. Rivers were rafted and mountains were climbed solely for the sake of recreation, not to determine their geographical context.\n\nIn 1861, following an intense lobbying by artists (painters), the French Waters and Forests Military Agency set an « artistic reserve » in Fontainebleau State Forest. With a total of 1 097 hectares, it is known to be the first World nature reserve.\n\nGlobal conservation became an issue at the time of the dissolution of the British Empire in Africa in the late 1940s. The British established great wildlife preserves there. As before, this interest in conservation had an economic motive: in this case, big game hunting. Nevertheless, this led to growing recognition in the 1950s and the early 1960s of the need to protect large spaces for wildlife conservation worldwide. The World Wildlife Fund (WWF), founded in 1961, grew to be one of the largest conservation organizations in the world.\n\nEarly conservationists advocated the creation of a legal mechanism by which boundaries could be set on human activities in order to preserve natural and unique lands for the enjoyment and use of future generations. This profound shift in wilderness thought reached a pinnacle in the US with the passage of the Wilderness Act of 1964, which allowed for parts of U.S. National Forests to be designated as \"wilderness preserves\". Similar acts, such as the 1975 Eastern Wilderness Act, followed.\n\nNevertheless, initiatives for wilderness conservation continue to increase. There are a growing number of projects to protect tropical rainforests through conservation initiatives. There are also large-scale projects to conserve wilderness regions, such as Canada's Boreal Forest Conservation Framework. The Framework calls for conservation of 50 percent of the 6,000,000 square kilometres of boreal forest in Canada's north. In addition to the World Wildlife Fund, organizations such as the Wildlife Conservation Society, the WILD Foundation, The Nature Conservancy, Conservation International, The Wilderness Society (United States) and many others are active in such conservation efforts.\n\nThe 21st century has seen another slight shift in wilderness thought and theory. It is now understood that simply drawing lines around a piece of land and declaring it a wilderness does not necessarily make it a wilderness. All landscapes are intricately connected and what happens outside a wilderness certainly affects what happens inside it. For example, air pollution from Los Angeles and the California Central Valley affects Kern Canyon and Sequoia National Park. The national park has miles of \"wilderness\" but the air is filled with pollution from the valley. This gives rise to the paradox of what a wilderness really is; a key issue in 21st century wilderness thought.\n\nThe creation of National Parks, beginning in the 19th century, preserved some especially attractive and notable areas, but the pursuits of commerce, lifestyle, and recreation combined with increases in human population have continued to result in human modification of relatively untouched areas. Such human activity often negatively impacts native flora and fauna. As such, to better protect critical habitats and preserve low-impact recreational opportunities, legal concepts of \"wilderness\" were established in many countries, beginning with the United States (see below).\n\nThe first National Park was Yellowstone, which was signed into law by U.S. President Ulysses S. Grant on 1 March 1872. The Act of Dedication declared Yellowstone a land \"hereby reserved and withdrawn from settlement, occupancy, or sale under the laws of the United States, and dedicated and set apart as a public park or pleasuring ground for the benefit and enjoyment of the people.\"\n\nThe world's second national park, the Royal National Park, located just 32 km to the south of Sydney, Australia, was established in 1879.\n\nThe U.S. concept of national parks soon caught on in Canada, which created Banff National Park in 1885, at the same time as the transcontinental Canadian Pacific Railway was being built. The creation of this and other parks showed a growing appreciation of wild nature, but also an economic reality. The railways wanted to entice people to travel west. Parks such as Banff and Yellowstone gained favor as the railroads advertised travel to \"the great wild spaces\" of North America. When outdoorsman Teddy Roosevelt became president of the United States, he began to enlarge the U.S. National Parks system, and established the National Forest system.\n\nBy the 1920s, travel across North America by train to experience the \"wilderness\" (often viewing it only through windows) had become very popular. This led to the commercialization of some of Canada's National Parks with the building of great hotels such as the Banff Springs Hotel and Chateau Lake Louise.\n\nDespite their similar name, national parks in England and Wales are quite different from national parks in many other countries. Unlike most other countries, in England and Wales, designation as a national park may include substantial settlements and human land uses which are often integral parts of the landscape, and land within a national park remains largely in private ownership. Each park is operated by its own national park authority.\n\nBy the later 19th century it had become clear that in many countries wild areas had either disappeared or were in danger of disappearing. This realization gave rise to the conservation movement in the United States, partly through the efforts of writers and activists such as John Burroughs, Aldo Leopold, and John Muir, and politicians such as U.S. President Teddy Roosevelt.\nThe idea of protecting nature for nature's sake began to gain more recognition in the 1930s with American writers like Aldo Leopold, calling for a \"land ethic\" and urging wilderness protection. It had become increasingly clear that wild spaces were disappearing rapidly and that decisive action was needed to save them. Wilderness preservation is central to deep ecology; a philosophy that believes in an inherent worth of all living beings, regardless of their instrumental utility to human needs.\n\nTwo different groups had emerged within the US environmental movement by the early 20th century: the conservationists and the preservationists. The initial consensus among conservationists was split into \"utilitarian conservationists\" later to be referred to as conservationists, and \"aesthetic conservationists\" or preservationists. The main representative for the former was Gifford Pinchot, first Chief of the United States Forest Service, and they focused on the \"proper use of nature,\" whereas the preservationists sought the \"protection of nature from use.\" Put another way, conservation sought to regulate human use while preservation sought to eliminate human impact altogether. The management of US public lands during the years 1960s and 70s reflected these dual visions, with conservationists dominating the Forest Service, and preservationists the Park Service\n\nThe World Conservation Union (IUCN) classifies wilderness at two levels, Ia (Strict Nature Preserves) and Ib (Wilderness areas). For the global standard of wilderness (1b) protection, governance and management, read Wilderness Protected Areas: Management Guidelines for IUCN Category 1b Protected Areas.\n\nThere have been recent calls for the World Heritage Convention to better protect wilderness and to include the word wilderness in their selection criteria for Natural Heritage Sites\n\nForty-eight countries have wilderness areas established via legislative designation as IUCN protected area management Category 1b sites that do not overlap with any other IUCN designation. They are: Australia, Austria, Bahamas, Bangladesh, Bermuda, Bosnia and Herzegovina, Botswana, Canada, Cayman Islands, Costa Rica, Croatia, Cuba, Czech Republic, Democratic Republic of Congo, Denmark, Dominican Republic, Equatorial Guinea, Estonia, Finland, French Guyana, Greenland, Iceland, India, Indonesia, Japan, Latvia, Liechtenstein, Luxembourg, Malta, Marshall Islands, Mexico, Mongolia, Nepal, New Zealand, Norway, Northern Mariana Islands, Portugal, Seychelles, Serbia, Singapore, Slovakia, Slovenia, Spain, Sri Lanka, Sweden, Tanzania, United States of America, and Zimbabwe. At publication, there are 2,992 marine and terrestrial wilderness areas registered with the IUCN as solely Category 1b sites.\n\nTwenty-two other countries have wilderness areas. These wilderness areas are established via administrative designation or wilderness zones within protected areas. Whereas the above listing contains countries with wilderness exclusively designated as Category 1b sites, some of the below-listed countries contain protected areas with multiple management categories including Category 1b. They are: Argentina, Bhutan, Brazil, Chile, Honduras, Germany, Italy, Kenya, Malaysia, Namibia, Nepal, Pakistan, Panama, Peru, Philippines, the Russian Federation, South Africa, Switzerland, Uganda, Ukraine, the United Kingdom of Great Britain and Northern Ireland, Venezuela, and Zambia.\n\nSince 1861, the French Waters and Forests Military Agency (Administration des Eaux et Forêts) put a strong protection on what was called the « artistic reserve » in Fontainebleau State Forest. With a total of 1 097 hectares, it is known to be the first World nature reserve.\n\nThen in the 1950s, Integral Biological Reserves (Réserves Biologiques Intégrales, RBI) are dedicated to man free ecosystem evolution, on the contrary of Managed Biological reserves (Réserves Biologiques Dirigées, RBD) where a specific management is applied to conserve vulnerable species or threatened habitats.\n\nIntegral Biological Reserves occurs in French State Forests or City Forests and are therefore managed by the National Forests Office. In such reserves, all harvests coupe are forbidden excepted exotic species elimination or track safety works to avoid fallen tree risk to visitors (already existing tracks in or on the edge of the reserve).\n\nAt the end of 2014, there were 60 Integral Biological Reserves in French State Forests for a total area of 111 082 hectares and 10 in City Forests for a total of 2 835 hectares.\n\nIn Greece there are some parks called \"ethniki drimoi\" (εθνικοί δρυμοί, national forests) that are under protection of the Greek government. Such parks include: Olympus, Parnassos and Parnitha National Parks.\n\nDue to Russia's size and in comparison non-dense population settlement, as well as of lack of infrastucture and the decades-long iron curtain the country is considered as one of the least explored areas and most natural places in the world.\n\nThere are seven wilderness areas in New Zealand as defined by the National Parks Act 1980 and the Conservation Act 1987 that fall well within the IUCN definition. Wilderness areas cannot have any human intervention and can only have indigenous species re-introduced into the area if it is compatible with conservation management strategies.\n\nIn New Zealand wilderness areas are remote blocks of land that have high natural character. The Conservation Act 1987 prevents any access by vehicles and livestock, the construction of tracks and buildings, and all indigenous natural resources are protected. They are generally over 40,000 ha in size.\n\nIn the United States, a Wilderness Area is an area of federal land set aside by an act of Congress. Human activities in wilderness areas are restricted to scientific study and non-mechanized recreation; horses are permitted but mechanized vehicles and equipment, such as cars and bicycles, are not.\n\nThe United States was the first country to officially designate land as \"wilderness\" through the Wilderness Act of 1964. The Wilderness Act was—and is still—an important part of wilderness designation because it created the legal definition of wilderness and founded the National Wilderness Preservation System. The Wilderness Act defines wilderness as \"an area where the earth and its community of life are untrammelled by man, where man himself is a visitor who does not remain.\"\n\nWilderness designation helps preserve the natural state of the land and protects flora and fauna by prohibiting development and providing for non-mechanized recreation only.\n\nThe first administratively protected wilderness area in the United States was the Gila National Forest. In 1922, Aldo Leopold, then a ranking member of the U.S. Forest Service, proposed a new management strategy for the Gila National Forest. His proposal was adopted in 1924, and 750 thousand acres of the Gila National Forest became the Gila Wilderness.\n\n'The Great Swamp in New Jersey was the first formally designated wilderness refuge in the United States. It was declared a wildlife refuge on 3 November 1960. In 1966 it was declared a National Natural Landmark and, in 1968, it was given wilderness status. Properties in the swamp had been acquired by a small group of residents of the area, who donated the assembled properties to the federal government as a park for perpetual protection. Today the refuge amounts to that are within thirty miles of Manhattan.\nWhile wilderness designations were originally granted by an Act of Congress for Federal land that retained a \"primeval character\", meaning that it had not suffered from human habitation or development, the Eastern Wilderness Act of 1975 extended the protection of the NWPS to areas in the eastern States that were not initially considered for inclusion in the Wilderness Act. This act allowed lands that did not meet the constraints of size, roadlessness, or human impact to be designated as wilderness areas under the belief that they could be returned to a \"primeval\" state through preservation.\n\nApproximately are designated as wilderness in the United States. This accounts for 4.82% of the country's total land area; however, 54% of that amount is found in Alaska (recreation and development in Alaskan wilderness is often less restrictive), while only 2.58% of the lower continental United States is designated as wilderness. Following the Omnibus Public Land Management Act of 2009 there are 756 separate wilderness designations in the United States ranging in size from Florida's Pelican Island at to Alaska's Wrangell-Saint Elias at .\n\nIn Western Australia, a Wilderness Area is an area that has a wilderness quality rating of 12 or greater and meets a minimum size threshold of 8,000 hectares in temperate areas or 20,000 hectares in arid and tropical areas. A wilderness area is gazetted under section 62(1)(a) of the Conservation and Land Management Act 1984 by the Minister on any land that is vested in the Conservation Commission of Western Australia.\n\nAt the forefront of the international wilderness movement has been The WILD Foundation, its founder Ian Player and its network of sister and partner organizations around the globe. The pioneer World Wilderness Congress in 1977 introduced the wilderness concept as an issue of international importance, and began the process of defining the term in biological and social contexts. Today, this work is continued by many international groups who still look to the World Wilderness Congress as the international venue for wilderness and to The WILD Foundation network for wilderness tools and action. The WILD Foundation also publishes the standard references for wilderness professionals and others involved in the issues: \"Wilderness Management: Stewardship and Protection of Resources and Values\", the \"International Journal of Wilderness\", \"A Handbook on International Wilderness Law and Policy\" and \"Protecting Wild Nature on Native Lands\" are the backbone of information and management tools for international wilderness issues.\n\nThe Wilderness Specialist Group within the World Commission on Protected Areas (WTF/WCPA) of the International Union for the Conservation of Nature (IUCN) plays a critical role in defining legal and management guidelines for wilderness at the international level and is also a clearing-house for information on wilderness issues. The IUCN Protected Areas Classification System defines wilderness as \"A large area of unmodified or slightly modified land, and/or sea retaining its natural character and influence, without permanent or significant habitation, which is protected and managed so as to preserve its natural condition (Category 1b).\" The WILD Foundation founded the WTF/WCPA in 2002 and remains co-chair.\n\nThe most recent efforts to map wilderness show that less than one quarter (~23%) of the world's wilderness area now remains, and that there have been catastrophic declines in wilderness extent over the last two decades. Over 3 million square kilometers (10 percent) of wilderness was converted to human land-uses. The Amazon and Congo rain forests suffered the most loss. Human pressure is starting to extend into almost every corner of the planet. The loss of wilderness could have serious implications for biodiversity conservation.\n\nA previous study, \"Wilderness: Earth's Last Wild Places,\" carried out by Conservation International, 46% of the world's land mass is wilderness. For purposes of this report, \"wilderness\" was defined as an area that \"has 70% or more of its original vegetation intact, covers at least and must have fewer than five people per square kilometer.\" However, an IUCN/UNEP report published in 2003, found that only 10.9% of the world's land mass is currently a Category 1 Protected Area, that is, either a \"strict nature reserve\" (5.5%) or \"protected wilderness\" (5.4%). Such areas remain relatively untouched by humans. Of course, there are large tracts of lands in National Parks and other protected areas that would also qualify as wilderness. However, many protected areas have some degree of human modification or activity, so a definitive estimate of true wilderness is difficult.\n\nThe Wildlife Conservation Society generated a \"human footprint\" using a number of indicators, the absence of which indicate wildness: human population density, human access via roads and rivers, human infrastructure for agriculture and settlements and the presence of industrial power (lights visible from space). The society estimates that 26% of the Earth's land mass falls into the category of \"Last of the wild.\" The wildest regions of the world include the Arctic Tundra, the Siberia Taiga, the Amazonia Rainforest, the Tibetan Plateau, the Australia Outback and deserts such as the Sahara, and the Gobi. However, from the 1970s, numerous geoglyphs have been discovered on deforested land in the Amazon rainforest, leading to claims about Pre-Columbian civilizations. The BBC's \"Unnatural Histories\" claimed that the Amazon rainforest, rather than being a pristine wilderness, has been shaped by man for at least 11,000 years through practices such as forest gardening and terra preta.\n\nIt should be noted that the percentage of land area designated \"wilderness\" does not necessarily reflect a measure of its biodiversity. Of the last natural wilderness areas, the taiga—which is mostly wilderness—represents 11% of the total land mass in the Northern Hemisphere. Tropical rainforest represent a further 7% of the world's land base. Estimates of the Earth's remaining wilderness underscore the rate at which these lands are being developed, with dramatic declines in biodiversity as a consequence.\nThe American concept of wilderness has been criticized by some nature writers. For example, William Cronon writes that what he calls a wilderness ethic or cult may \"teach us to be dismissive or even contemptuous of such humble places and experiences\", and that \"wilderness tends to privilege some parts of nature at the expense of others\", using as an example \"the mighty canyon more inspiring than the humble marsh.\" This is most clearly visible with the fact that nearly all U.S. National Parks preserve spectacular canyons and mountains, and it was not until the 1940s that a swamp became a national park—the Everglades. In the mid-20th century national parks started to protect biodiversity, not simply attractive scenery.\n\nCronon also believes the passion to save wilderness \"poses a serious threat to responsible environmentalism\" and writes that it allows people to \"give ourselves permission to evade responsibility for the lives we actually lead...to the extent that we live in an urban-industrial civilization but at the same time pretend to ourselves that our real home is in the wilderness\".\n\nMichael Pollan has argued that the wilderness ethic leads people to dismiss areas whose wildness is less than absolute. In his book \"Second Nature\", Pollan writes that \"once a landscape is no longer 'virgin' it is typically written off as fallen, lost to nature, irredeemable.\" Another challenge to the conventional notion of wilderness comes from Robert Winkler in his book, \"Going Wild: Adventures with Birds in the Suburban Wilderness\". \"On walks in the unpeopled parts of the suburbs,\" Winkler writes, \"I’ve witnessed the same wild creatures, struggles for survival, and natural beauty that we associate with true wilderness.\" Attempts have been made, as in the Pennsylvania Scenic Rivers Act, to distinguish \"wild\" from various levels of human influence: in the Act, \"wild rivers\" are \"not impounded\", \"usually not accessible except by trail\", and their watersheds and shorelines are \"essentially primitive\".\nAnother source of criticism is that the criteria for wilderness designation is vague and open to interpretation. For example, the Wilderness Act states that wilderness must be roadless. The definition given for roadless is \"the absences of roads which have been improved and maintained by mechanical means to insure relatively regular and continuous use.\" However, there have been added sub-definitions that have, in essence, made this standard unclear and open to interpretation.\n\nComing from a different direction, some criticism from the Deep Ecology movement argues against conflating \"wilderness\" with \"wilderness reservations\", viewing the latter term as an oxymoron that, by allowing the law as a human construct to define nature, unavoidably voids the very freedom and independence of human control that defines wilderness. True wilderness requires the ability of life to undergo speciation with as little interference from humanity as possible. Anthropologist and scholar on wilderness Layla Abdel-Rahim argues that it is necessary to understand the principles that govern the economies of mutual aid and diversification in wilderness from a non-anthropocentric perspective.\n\n\n\n\n"}
{"id": "6835893", "url": "https://en.wikipedia.org/wiki?curid=6835893", "title": "World of Ghost in the Shell", "text": "World of Ghost in the Shell\n\nMasamune Shirow's manga and anime series \"Ghost in the Shell\" takes place in a (post)cyberpunk version of Earth in the near future. The series focuses on Japan, but several other nations figure prominently in some stories. The world of \"Ghost in the Shell\" features significant advances in technology, the most significant of which is the cyberbrain, a mechanical casing for the human brain that allows mental interface with the Internet and other networks.\n\nA is a device in the fictional universe of \"Ghost in the Shell\" by Masamune Shirow (and also in Shirow's later work \"Real Drive\") that acts as a self-contained module containing, protecting, and interfacing an artificially augmented brain. The \"brain\" includes the brain stem but excludes the eyes, optic nerves, and most of the spine. By being physically self-contained, the cyberbrain allows the artificially augmented brain inside to function or be physically stored inside a body, to be physically transferred between bodies, or to be temporarily stored or transported outside any body. Cyberbrain implants, in conjunction with micromachines, allow the brain to initiate and maintain a connection to computer networks or other individuals who also possess a cyberbrain. This capability results in a number of unforeseen psychosocial phenomena whose emergence is a major plot element of the various \"Ghost in the Shell\" stories.\n\nThe process of augmentation of the brain in this fashion is referred to in the series as . It is not necessary for a subject to undergo complete cyberization, acquiring a full-body prosthesis, to support the cyberized brain; an individual may choose to only have their brain cyberized. Cyberization is imagined to take three distinct forms:\n\n\nIn the series, \"super-class-A hackers\" are able to take over people's cyberbrains, to the extent that they can alter memories or \"steal eyes\", altering the victim's sensory input. The Laughing Man character was able to alter the vision of an entire crowd and superimpose his logo over his face to disguise his identity. is a criminal technique wherein the victim's entire memory is replaced with false memories. This is notably used in the series by the \"Puppet Master\" in the first \"Ghost in the Shell\" film. \"Cyberbrain hijacking\" involves the replacement of a victim's cyberbrain with the criminal's own, while the same criminal retains another body in storage. Alternatively, a criminal may choose to place their cyberbrain into a copy of the victim's body, and then impersonate the victim. Both of these methods were portrayed in the episode \"Face\".\n\nIn the series, is a form of cyberbrain-induced reclusiveness, occurring when users of cyberbrain technology shut themselves off from the outside world to avoid harming others or themselves. It can also be a psychological barrier induced by the subconscious to protect the ego from being overwhelmed by the depth and connective nature of the Internet. CSS patients have the appearance of savants with extremely high computer skills.\n\nIn \"Ghost in the Shell\", a is a human who has had , possessing a partly or entirely prosthetic body which has parts that can be exchanged or replaced when damaged. Cyborgs do not eat normal food; rather, they eat special processed protein bars which provide nutrients for their remaining organic parts. This \"food\" does not taste good to those who still have organic taste buds due to an inability to manipulate taste input cybernetically. The most heavily cyberized individuals, such as Major Kusanagi, have only their soul or \"ghost\" remaining as proof of their humanity - even their physical brains are made of synthetic materials. As Kusanagi muses in the original film, she doesn't even have tangible physical proof of even being human anymore.\n\nAn important technology used in the series is . Members of Section 9 as well as their Tachikoma tanks have the ability to activate a special camouflage technology which enables them to blend in with the environment, making them near-invisible across all visible spectrum and thermal imaging. It is an active stealth system which projects ambient conditions of the opposing side, thus rendering the masked object transparent by transmission. The system is not perfect, as it seems unable to compensate for sudden changes and physical impacts and not impervious to close observation. It also has difficulty working in rain or if walking through shallow water. A faint translucent distortion is shown as the limitations of the technology. In the legal landscape of the series, usage of the technology without a warrant is heavily restricted. The use of this technology by Section 9 is the exception, and not the norm - further highlighting their extraordinary legal standing.\n\nIn the \"Stand Alone Complex\" alternate timeline, however, this technology seems to have been perfected and is capable of operating in highly illuminated environment, as evidenced in the episode \"Android and I\". The visual distortions are therefore purely for the benefit of the viewers.\n\nThere is present-day research into the active optic camouflage at the University of Tokyo inspired by the technology's fictional portrayal in the series.\n\nThink tanks are robotic weapons platforms that make use of artificial intelligence (AI) to enhance their abilities. In the Japanese version, they are officially referred to as , but they are more commonly referred to as . The most prominent think tanks are those used by Section 9 in its various incarnations, including the , , , and . Section 9's think tanks are spider-like in appearance, having four walking legs, a pair of front-mounted manipulators, and a segmented body. The pilot sits in a posterior pod attached to the main body, but the tanks can operate independently using their AI. The AI of the tanks exhibit learning algorithms to help them adapt to new situations and these experiences are synchronized among the tanks after each mission. Fuchikoma and Tachikoma AIs have childlike personalities and the latter's AI develop rapidly over the course of \"Stand Alone Complex\", almost to the point of sentience and individuality. Uchikoma are seen at the end of \"2nd GIG\" after the destruction of the satellite containing the Tachikoma AIs. Their AIs have been considerably limited, and their subsequent failure during a mission in \"Solid State Society\" leads to the return of the Tachikoma.\n\nThe think tanks in each series often exhibit a childlike demeanor. During the development of \"Stand Alone Complex\", the creators had wanted to include Fuchikoma into the new series, but could not due to copyright issues. Instead, they designed Tachikoma using Fuchikoma as inspiration. There are several Tachikoma models seen in video, manga and after-market toys. There is a brown one with a more squat rear end, a yellow \"construction\" version, as well as yellow and red \"cyber\" versions named and . A prototype Tachikoma is seen in the PSP . In \"Ghost in the Shell: Solid State Society\", a 2006 anime television movie continuation of the anime series, the Tachikoma have their own names.\n\n\"Ghost in the Shell\" takes place on the cusp of the mid-21st century. World War III was a nuclear war primarily involving wealthy and powerful developed countries whose destruction resulted in fundamental changes in the global balance of power, as long-established national boundaries and concentrations of population were broken. World War IV was a non-nuclear war (also known as the Second Vietnam War) triggered by the ensuing collapse of many developing countries.\n\nAs a result of these wars the global geopolitical landscape has become heavily balkanized, with even formerly strong developed countries like the United States divided into smaller, less stable competitors. Civil wars and non-state revolutionary movements pose a constant security threat worldwide, and the millions of refugees displaced by the global tumult have become a major problem. The continuing advance of technology, particularly cyberization technology and the Internet, has also drastically increased the frequency of cyber-terrorism. The island nation of Japan emerged from these conflicts politically intact, and occupies a position of relative privilege.\n\nJapan is a major world power, having gained equal footing with the descendant countries of the former United States. This status is very much the result of its scientific and technological prowess; in particular, it is currently the only country with access to \"radiation scrubber\" technology. Also known as the , these nanomachines can eliminate nuclear fallout, although they must be distributed before a blast occurs to work effectively.\n\nThe Japanese economy is one of the world's strongest, and the Japanese as a whole enjoy a high standard of living. However, Japanese society is far from perfect.\n\nThe geography has since changed. The Kantō region has been destroyed by nuclear bombs in the nuclear World War III (in the original manga it is by an attack from the Ameri-Soviet Union, in \"S.A.C.\" it is China) as well as in the non-nuclear World War IV. The land that Tokyo, Yokohama, Kawasaki, and Yokosuka once stood on is gone, having been submerged due to crustal deformations from the bombings in the World Wars. The Kansai region has been left relatively unscathed. The Seto Inland Sea off the coast of Kobe has been reclaimed, with the land becoming the new , sometimes referred to as . Niihama briefly served as the provisional capital for Japan until the Prime Minister's offices and home were moved to Fukuoka. Niihama remains a population center, with Section 9's headquarters located in the city. It is connected to Kobe by the . The island of Iturup has been returned to Japan, and it has become a bustling city; however its ambiguity in the international sphere has made it become a special economic zone where criminal organizations like to keep their headquarters. The Nemuro Peninsula has become occupied by foreign powers during the various wars. Japan plans on recapturing it through cyber warfare. In the \"Stand Alone Complex\" universe, the Tōhoku region is devastated, with the cities of Sendai and Niigata destroyed, remaining solely as craters from a nuclear bomb during World War III. Maizuru in Kyoto has also been destroyed, and the entirety of Okinawa Island has been submerged due to nuclear bombardment by China in World War III.\n\nWhile the government of Japan is a parliamentary democracy, political corruption is an issue, often driven by the competition of corporate, military, and bureaucratic interests. These groups make use of considerable and indiscriminate surveillance and espionage.\n\nThe Self-Defence Forces continue to be legally bound by Article 9 of the Constitution of Japan. They, along with a variety of police organizations, help to maintain order, often via methods that greatly compromise individual and press freedoms.\n\nThe demographics of Japan have been shaken by the post-war influx of Asian refugees; their illegality confines them to ghettoes such as the one on Dejima, leaves them vulnerable to exploitation, encourages identity fraud and other crime, and leads to ethnic tensions. Their presence and status constitute a major national political issue.\n\nThe trade and abuse of illegal drugs, including \"cyber-drugs\", human trafficking, and other activities of Japanese Yakuza and Chinese organized crime syndicates also pose a threat to internal security.\n\nThe , also known as Imperial Americana, is a fictional country appearing in Masamune Shirow's anime and manga series' \"Ghost in the Shell\" and \"Appleseed\", featuring most prominently in \"\". The AE exists from before 2020 and at least until 2147, and consists primarily of the American states that had formed the Confederate States of America, plus parts of the Great Plains and Southwest.\n\nAfter the end of World War III, the United States is partitioned by an unspecified process into three rump states: the American Empire, the , and the rump state . The American Empire is the only successor state to play a major role in world politics. Its government seems driven by a desire to restore its diminished power and prestige, towards which end it adopts a policy of militarist aggression and open imperialism, directed primarily against Latin America.\n\nDue to war damage inflicted on its economy and its weakened political position, the American Empire enters into a security pact with Japan, which had escaped World War III largely unscathed. One of the principal reasons the American Empire seeks the treaty is its desire to co-opt the \"Japanese Miracle\"; this technology makes the \"Ghost in the Shell\" universe's nuclear weaponry somewhat less apocalyptic in its implications than the real-world article, since the radioactive particulates created by an atomic event can be safely contained, reducing the weapons' deterrence potential.\n\nThe pact also reaffirms Article 9 of the Japanese Constitution, thus helping to prevent Japanese superpower status.\n\nIn 1999, Beijing was destroyed by a massive meteor impact, causing the fall of the Communist Party of China. China and Taiwan have since reunited, and the unified is now a multi-party democracy with a higher degree of political and personal freedom than before.\n\nSino-Japanese relations remain strong throughout the series, as the nations have moved on from the differences of World War II. This pan-Asian rapprochement is a serious concern to the American Empire, which is engaged in a Second Cold War with China.\n\nThe world's largest economy, China is part of a trading bloc which includes most of Southeast Asia and India (the second-largest economy). Its considerable economic growth has not been of equal benefit to its citizens, and poverty is much in evidence; however, at least part of this can be attributed to the collateral damage of war.\n\nIn the \"Stand Alone Complex\" universe, China remains known as the , and it is they who are the primary combatants against Japan in World War III. It is their bombings that destroy the cities of Tokyo, Sendai, Niigata, and Maizuru, as well as Okinawa Island.\n\nIn the original manga, North Korea and South Korea have reunited into a single . Additionally, the American Army has set up a military base on reclaimed land.\n\nHowever, in \"S.A.C. 2nd GIG\", it is revealed that the Korean civil war has continued well through the fourth non-nuclear World War, with the United Nations' peacekeeping forces requesting assistance from the JSDF to help end a conflict.\n\nAfter the third and fourth World Wars, the American Empire was left militarily and economically devastated and in an effort to regain prestige as a major power engaged in open imperialism in Mexico and other parts of Latin America under the pretext of overthrowing the corrupt governments of those nations. In the late 2020s, the American Empire invaded and occupied much of Mexico; the invading forces used psychological warfare and intimidation techniques such as killing women in smaller villages and torturing prisoners in an attempt to break the spirit of the Mexican resistance. This was compounded by a relentless carpet bombing campaign against major cities and a nuclear weapons strike on Monterrey. Large groups of American, British, and Japanese mechanized infantry and armored divisions were then sent in to mop up all remaining Mexican forces, which led to extended bouts of tank-versus-tank fighting and constant guerrilla warfare which lasted for months.\n\nWhile the American Empire was initially successful in its campaign and occupied large parts of northern Mexico, they were unable to move further into the country and suffered massive casualties at the hands of both the Mexican military and mercenary forces hired by the Mexican government. Eventually the American-led coalition was forced to withdraw and as a result the American Empire's prestige further deteriorated and it lost much of the goodwill of the international community.\n\nIt is not specified what happened to the Mexican state after the war but it is known that it claimed victory over the American Empire and later expanded to absorb the countries of Central America.\n\n, a major manufacturer of cybernetics, electronics and medical technology featured prominently in the series, is based in Mexico; it is the fourth largest company in the world, boasts offices in the Netherlands, Germany, and Japan and is powerful enough to exert influence over the Japanese government, suggesting that Mexico has become a major technological and economic power. It is part of a single large trading bloc comprising all of South and Central America.\n\nThe is a fictional state in the \"\" universe, referred to in the latest film, \"\".\n\nThe Siak Republic is presumed to occupy the present territory of the Republic of Singapore and the surrounding territory of Riau, Indonesia. At some point within this fictional universe these territories must have merged to become a new state, adopting the system of government of the old Singaporean republic. The name is derived from the Siak Regency of Riau. At some point before the film, the Siak Republic's ruling regime, led by General Ka Rum, was deposed, and Ka Rum lived in exile in Japan.\n"}
{"id": "1486283", "url": "https://en.wikipedia.org/wiki?curid=1486283", "title": "Árbol del Tule", "text": "Árbol del Tule\n\nEl Árbol del Tule (Spanish for The Tree of Tule) is a tree located in the church grounds in the town center of Santa María del Tule in the Mexican state of Oaxaca, approximately east of the city of Oaxaca on the road to Mitla. It is a Montezuma cypress (\"Taxodium mucronatum\"), or \"ahuehuete\" (meaning \"old man of the water\" in Nahuatl). It has the stoutest trunk of any tree in the world. In 2001, it was placed on a UNESCO tentative list of World Heritage Sites.\n\nIn 2005, its trunk had a circumference of , equating to a diameter of , an increase from a measurement of m in 1982. However, the trunk is heavily buttressed, giving a higher diameter reading than the true cross-sectional of the trunk represents; when this is taken into account, the diameter of the 'smoothed out' trunk is . This is slightly wider than the next most stout tree known, a giant sequoia with a diameter.\n\nThe height is difficult to measure due to the very broad crown; the 2005 measurement, made by laser, is , shorter than previous measurements of .\nIt is so large that it was originally thought to be multiple trees, but DNA tests have proven that it is only one tree. This does not rule out another hypothesis, which states that it comprises multiple trunks from a single individual.\n\nThe age is unknown, with estimates ranging between 1,200 and 3,000 years, and even one claim of 6,000 years; the best scientific estimate based on growth rates is 1,433-1,600 years. Local Zapotec legend holds that it was planted about 1,400 years ago by Pecocha, a priest of Ehecatl, the Aztec wind god, in broad agreement with the scientific estimate; its location on a sacred site (later taken over by the Roman Catholic Church) would also support this.\n\nThe tree is occasionally nicknamed the \"Tree of Life\" from the images of animals that are reputedly visible in the tree's gnarled trunk. As part of an official project local schoolchildren give tourists a tour of the tree and point out shapes of creatures on the trunk, including jaguars and elephants.\n\nIn 1990, it was reported that the tree is slowly dying because its roots have been damaged by water shortages, pollution, and traffic, with 8,000 cars travelling daily on a nearby highway.\n"}
