{"id": "28162705", "url": "https://en.wikipedia.org/wiki?curid=28162705", "title": "Acorn tube", "text": "Acorn tube\n\nAn acorn tube, or acorn valve, refers to any member of a family of VHF/UHF vacuum tubes starting just before World War II. They were named after their resemblance to the acorn, specifically due to the glass cap at one end of the tube that looked similar to the cap on an acorn. The acorn tubes found widespread use in radios and radar systems.\n\nHigh-frequency performance is limited by (1) parasitic lead inductance and capacitance and skin effect, and (2) electron transit time (the time required to travel from cathode to anode). Transit time effects are complicated, but one simple effect is the phase margin; another one is input conductance, also known as grid loading. At extremely high frequencies, electrons arriving at the grid may become out of phase with those departing towards the anode. This imbalance of charge causes the grid to exhibit a reactance that is much less than its low-frequency \"open circuit\" characteristic. Acorn- as well as Lighthouse tubes and Nuvistors attempt to minimize this effect by arranging cathode, grid(s) and anode as closely spaced together as possible.\n\nThe original range included about half a dozen tubes, designed to work in the VHF range. The 955 is a triode. The 954 and 956 types are sharp and remote cut-off pentodes, respectively, all with indirect 6.3 V, 150 mA heaters. Types 957, 958 and 959 are for portable equipment and have 1.25 V NiCd battery heaters. The 957 is a medium-μ signal triode, the 958 is a transmitting triode with dual, paralleled filaments for increased emission, and the 959 is a sharp cut-off pentode like the 954. The 957 and 959 draw 50 mA heater current, the 958 twice as much. In 1942, the 958A with tightened emission specifications was introduced after it turned out that 958s with excessively high emission kept working after the filament power was turned off, the filament still sufficiently heating on the anode current alone. After the introduction of the miniature 7-pin base, the 954, 955 and 956 were made available with this base as 9001, 9002 and 9003. Other acorn tubes include:\n\nLarger, higher-power types such as the 316A, 368A, 388A, and 703A triodes and the 713A and 717A pentodes were referred to as \"Doorknob\" tubes.\n\nThe introduction of the EF50 was the first serious competition for the acorn design, and replaced the acorns in many roles, especially post-war when millions of surplus EF50s were dumped on the market.\n\n"}
{"id": "54132651", "url": "https://en.wikipedia.org/wiki?curid=54132651", "title": "Aganippe (naiad)", "text": "Aganippe (naiad)\n\nAganippe (; ) was the name of both a spring and the Naiad (a Crinaea) associated with it. The spring is in Boeotia, near Thespiae, at the base of Mount Helicon, and was associated with the Muses who were sometimes called Aganippides. Drinking from it was considered to be a source of poetic inspiration. The nymph is called a daughter of the river-god Permessus (called Termessus by Pausanias). Ovid associates Aganippe with Hippocrene.\n\n"}
{"id": "4397693", "url": "https://en.wikipedia.org/wiki?curid=4397693", "title": "Alberta Energy", "text": "Alberta Energy\n\nThe Ministry of Energy is a Cabinet-level agency of the government of the Canadian province of Alberta responsible for coordinating policy relating to the development of mineral and energy resources. It is also responsible for assessing and collecting non-renewable resource (NRR) royalties, freehold mineral taxes, rentals, and bonuses. The Alberta Petroleum Marketing Commission, which is fully integrated with the Department of Energy within the ministry, and fully funded by the Crown, accepts delivery of the Crown's royalty share of conventional crude oil and sells it at the current market value.\n\nThe Alberta Energy and Utilities Board regulated energy resource development, pipelines, transmission lines, and investor-owned electric, water, and natural gas utilities, as well as certain municipality-owned utilities. It reported to the Executive Council through the Ministry of Energy, although it operated and made its formal decisions independently and autonomously. On January 1, 2008 the Alberta Energy and Utilities Board (EUB) was realigned into two separate regulatory bodies:\n\nIn 1984, the Alberta Department of Energy and Natural Resources (ENR), was a complex multi-divisional organization, with a permanent staff of 2, 605 and a budget of $499 million, that was responsible for the management of energy, mineral, forest and fish and wildlife resources as well as public (crown owned lands) which constituted 62% of Alberta's land base. ENR policy was based on the premise that with proper planning and management, land can support a variety of uses, such as, timber, recreation and wildlife. However few were ideally compatible creating a climate of competition and conflict.\n\nIn 1986 the Department of Energy and the Department of Forestry, Lands and Wildlife were created. The original resource agencies continued and interdepartmental planning took place under Resource Evaluation and Planning (REAP). The Resource Evaluation and Planning (REAP) division was created in 1976 to provide coordination and data gathering services.\n\nIn the 1980s REAP oversaw an integrative planning system using a team approach to decision-making. It was a challenging time of transition. More established agencies like the Alberta Forest Service supported preservation of traditional attitudes and behaviour and felt threatened. By the 1980s Alberta Forest Service had a strong authority system with a military style chain of command and system of ranks. Fish and Wildlife Division were more flexible and less formally structured. Public Lands were more bureaucratic and mechanistic.\n\nThe Fish and Wildlife division who emphasized long-term research and monitoring are under the auspices of the Fish and Wildlife Act. Fish and Wildlife division were with the Department of Recreation and Parks before joining Energy and Natural Resources (ENR) in 1979.\n\nThe Mineral Resources division had very high status and power because of their client groups, which included the oil and gas industry, who are \"powerful actors on the Alberta scene.\"\n\nIn 1982 the Alberta Forest Service had a staff of 765 and a budget of $123 million and the Fish and Wildlife division whose clients were often environmental groups, had 414 positions and $20 million.\n\nRoyalty rates in Alberta are based on the price of WTI. That royalty rate is applied to a project's Net Revenue if the project has reached payout or Gross Revenue if the project has not yet reached payout. A project's revenue is a direct function of the price it is able to sell its crude for. Since WCS is a benchmark for oil sands crudes, revenues in the oil sands are discounted when the price of WCS is discounted. Those price discounts flow through to the royalty payments.\n\nThe Province of Alberta receives a portion of benefits from the development of energy resources in the form of royalties that fund in part programs like health, education and infrastructure.\n\nIn 2006-7 the oil sands royalty revenue was $2.411 billion. In 2007/08 it rose to $2.913 billion and it continued to rise in 2008/09 to $2.973 billion.\n\nIn their response to the 2010 competitive review with input from the Canadian Association of Petroleum Producers (CAPP) and the Small Explorers and Producers Association of Canada, Alberta Energy lowered non-renewable resource (NRR) royalty rates.\n\nThe rate cuts included, \n\nIn 2010 the oil and gas industry accounted for 30 percent of Alberta's GDP and 147,000 direct jobs. The decision to lower royalty rates to make the NRR industries more competitive was based on the economic argument that the decrease in royalties revenue would be offset by an increase in land sales and tax revenue.\n\nFollowing the revised Alberta Royalty Regime it fell in 2009/10 to $1.008 billion. In that year Alberta's total resource revenue \"fell below $7 billion...when the world economy was in the grip of recession.\"\n\nIn February 2012 the Province of Alberta \"expected $13.4 billion in revenue from non-renewable resources in 2013-14. By January 2013 the province was anticipating only $7.4 billion. \"30 per cent of Alberta’s approximately $40-billion budget is funded through oil and gas revenues. Bitumen royalties represent about half of that total.\" In 2009/10 royalties from the oil sands amounted to $1.008 billion (Budget 2009 cited in Energy Alberta 2009.\n\nIn order to accelerate development of the oil sands, the federal and provincial governments more closely aligned taxation of the oil sands with other surface mining resulting in \"charging one per cent of a project’s gross revenues until the project’s investment costs are paid in full at which point rates increased to 25 per cent of net revenue. These policy changes and higher oil prices after 2003 had the desired effect of accelerating the development of the oil sands industry. \"A revised Alberta Royalty Regime was implemented in January 1, 2009. through which each oil sands project pays a gross revenue royalty rate of 1% (Oil and Gas Fiscal Regimes 2011:30). Oil and Gas Fiscal Regimes 2011 summarizes the petroleum fiscal regimes for the western provinces and territories. The Oil and Gas Fiscal Regimes described how royalty payments were calculated:\n\nWhen the price of oil per barrel is less than or equal to $55/bbl indexed against West Texas Intermediate (WTI) (Oil and Gas Fiscal Regimes 2011:30)(Indexed to the Canadian dollar price of West Texas Intermediate (WTI) (Oil and Gas Fiscal Regimes 2011:30) to a maximum of 9%). When the price of oil per barrel is less than or equal to $120/ bbl indexed against West Texas Intermediate (WTI) \"payout.\"\n\nPayout refers \"the first time when the developer has recovered all the allowed costs of the project, including a return allowance on those costs equal to the Government of Canada long-term bond rate [\"LTBR\"].\n\nIn order to encourage growth and prosperity and due to the extremely high cost of exploration, research and development, oil sands and mining operations pay no corporate, federal, provincial taxes or government royalties other than personal income taxes as companies often remain in a loss position for tax and royalty purposes for many years. Defining a loss position becomes increasingly complex when vertically-integrated multi-national energy companies are involved. Suncor claims their realized losses were legitimate and that Canada Revenue Agency (CRA) is unfairly claiming \"$1.2-billion\" in taxes which is jeopardizing their operations.\n\n\"Bitumen Valuation Methodology (BVM) is a method to determine for royalty purposes a value for bitumen produced in oil sands projects and either upgraded on-site or sold or transferred to affiliates. The BVM ensures that Alberta receives market value for its bitumen production, taken in cash or bitumen royalty-in-kind, through the royalty formula. Western Canadian Select (WCS), a grade or blend of Alberta bitumens, diluents (a product such as naphtha or condensate which is added to increase the ability of the oil to flow through a pipeline) and conventional heavy oils, developed by Alberta producers and stored and valued at Hardisty, AB was determined to be the best reference crude price in the development of a BVM.\"\n\nBy 2014 NRR revenue dropped to 21% of total revenue from 30% in 2010. The 2014 Provincial Budget reported that future anticipated NRR revenue is \"far less than in 2011-2012, less than the 30% recorded in 2010 and in the four year period from 2005-06 to 2008-09.\"\n\nBudget 2014 forecast that the 2014-2015 West Texas Intermediate (WTI) - Western Canadian Select (WCS)- differential, would be 26% with the WTI price at US$95.22. By December 2014 4 December 2014 WTI had dropped to $US67.25 bbl and WCS to US$50.70 with a differential of 16%.\n\n"}
{"id": "1262184", "url": "https://en.wikipedia.org/wiki?curid=1262184", "title": "Andrew Crosse", "text": "Andrew Crosse\n\nAndrew Crosse (17 June 1784 – 6 July 1855) was a British amateur scientist who was born and died at Fyne Court, Broomfield, Somerset. Crosse was an early pioneer and experimenter in the use of electricity. He became widely known after press reports of an electrocrystallization experiment he conducted in 1836, during which insects \"appeared\".\n\nCrosse was the first son of Richard Crosse and Susannah Porter. In 1788 he accompanied them on a trip to France, where he went to school for a time in Orléans. From the age of six until he was eight he stayed with a tutor, the Reverend Mr White, in Dorchester, where he learned Greek. On 1 February 1792 he was sent to boarding school in Bristol.\n\nAround the age of 12 Crosse persuaded one of his teachers to let him attend a series of lectures on the natural sciences, the second of which was on the subject of electricity. This led to his lifelong interest in the subject. Crosse first started experimenting with electricity during his time in the sixth form, when he built a Leyden jar. After leaving school he studied at Brasenose College, Oxford.\n\nHaving lost his parents, his father in 1800 and his mother in 1805, Crosse took over the management of the family estates at the age of 21. After abandoning his studies for the Bar, he increasingly devoted his spare time to studying electricity at Fyne Court, where he built his own laboratory. He also studied mineralogy and became interested in the formation of crystalline deposits in caves. Around 1807 he started to experiment with electrocrystallization, forming crystalline lime carbonate from water taken from Holwell Cavern. He returned to the subject again from around 1817 and in subsequent years produced a total of 24 electrocrystallized minerals.\n\nAmong his experiments Crosse erected \"an extensive apparatus for examining the electricity of the atmosphere,\" incorporating at one point an insulated wire some long, later shortened to , suspended from poles and trees. Using this wire he was able to determine the polarity of the atmosphere under various weather conditions. His results were published by his friend George Singer in 1814, as part of Singer's \"Elements of Electricity and Electro-Chemistry\".\n\nAlong with Sir Humphry Davy (who visited Fyne Court in 1827), Crosse was one of the first to develop large voltaic piles. Although it was not the largest he built, Henry Minchin Noad's \"Manual of Electricity\" describes a battery consisting of 50 jars containing of coated surface. Using his wires Crosse was able to charge and discharge it some 20 times a minute, \"accompanied by reports almost as loud as those of a cannon\". He became known locally as \"the thunder and lightning man\". In 1836 Sir Richard Phillips described seeing a wide variety of voltaic piles at Fyne Court, totalling 2,500, of which 1,500 were in use when he visited.\n\nIn 1836 Crosse was persuaded to attend a meeting of the British Association for the Advancement of Science in Bristol. After describing his discoveries over dinner at the house of a friend in Bristol, he was further persuaded to recount them to both the chemical and the geological sections of the meeting. They included his electrocrystallization and atmospheric experiments, and his improvements to the voltaic battery.\n\nCrosse went on to separate copper from its ores using electrolysis, experimented with the electrolysis of sea water, wine and brandy to purify them, and examined the effects of electricity on vegetation. He was also interested in the practical uses of electricity and magnetism, including the development of loudspeakers and telegraphy although he did not do research in these areas himself.\n\nA few months after the meeting of the British Association for the Advancement of Science Crosse was conducting another electrocrystallization experiment when, on the 26th day of the experiment, he saw what he described as \"the perfect insect, standing erect on a few bristles which formed its tail\". More creatures appeared and two days later they began moving their legs. Over the next few weeks hundreds more appeared. They crawled around the table and hid themselves wherever they could find shelter. Crosse identified them as being members of the genus \"Acarus\".\n\nPuzzled, Crosse mentioned the incident to a couple of friends. A local newspaper learned of the incident and published an article about the \"extraordinary experiment,\" naming the insects \"Acarus crossii\". The article was subsequently picked up by other newspapers across the country and elsewhere in Europe. Some readers apparently gained the impression that Crosse had somehow \"created\" the insects, or at least claimed to have done so. He received angry letters in which he was accused of blasphemy and trying to take God's place as a creator. Some of them included death threats.\n\nOther scientists tried to repeat the experiment. W. H. Weeks took extensive measures to assure a sealed environment by placing his experiment inside a bell jar. He obtained the same results as Crosse, but due to the controversy that Crosse's experiment had sparked his work was never published. In February 1837 many newspapers reported that Michael Faraday had also replicated Crosse's results. However, this was not true. Faraday had not even attempted the experiment. Later researchers, such as fellow members of the London Electrical Society Henry Noad and Alfred Smee, were unable to replicate Crosse's results.\n\nCrosse did not claim that he had created the insects. He assumed that there were insect eggs embedded in his samples. Later commentators agreed that the insects were probably cheese mites or dust mites that had contaminated Crosse's instruments.\n\nIt has been suggested that this episode was a source of inspiration for Mary Shelley's novel \"Frankenstein\", but this cannot have been the case, since Crosse's experiments took place almost 20 years after the novel was first published. The idea appears to have originated in the book \"The Man Who Was Frankenstein\" (1979) by Peter Haining. Mary Shelley did, however, know Crosse through a mutual friend, the poet Robert Southey. Percy Bysshe Shelley and Mary Wollstonecraft Godwin reportedly attended a lecture by Crosse in London in December 1814, in which he allegedly explained his experiments with atmospheric electricity. However, Mary Shelley's diary speaks only of \"Garnerin\" as the lecturer. Similarly dubious is a claim that Edward W. Cox wrote a report of their visits to Fyne Court to see Crosse's work in the \"Taunton Courier\" in Autumn 1836. Percy had been dead for over a dozen years by then.\n\nCrosse also wrote a great many poems and enjoyed walking on the Quantock Hills, in which Fyne Court is set, \"at all hours of day and night, in all seasons\".\n\nCrosse advocated the benefits of education for the lower classes, argued against emigration, and supported a campaign by local farmers against falling food prices and high taxes during the 1820s. He was also active in party politics, speaking in support of friends at election meetings. Following the Battle of Waterloo Crosse boarded a ship at Exeter to see the captured Napoleon Bonaparte on the deck of \"HMS Bellerophon\" near Plymouth. Crosse also served as a magistrate.\n\nCrosse married Mary Anne Hamilton in 1809. They had seven children, although three died in childhood. Mary died in 1846 following several years of ill health.\n\nOn 22 July 1850 Crosse married again, aged 66. His second wife was the 23-year-old Cornelia Augusta Hewett Berkeley. They went on to have three children.\n\nCrosse suffered a stroke while dressing on the morning on 26 May 1855. He died on 6 July 1855, in the same room in which he had been born.\n\nThe laboratory table on which Crosse carried out experiments stands in the aisle of the Church of St Mary & All Saints, Broomfield, and an obelisk in his memory is in the churchyard.\n\nCrosse's home, Fyne Court, was largely destroyed by fire in 1894. The garden and the estate are now owned by the National Trust, and are open to visitors.\n\nA number of documents related to Andrew Crosse and his work are held in the Somerset Record Office. In December 2008 Somerset County Council acquired a further two letters for the sum of 400 pounds to add to the collection.\n"}
{"id": "56302927", "url": "https://en.wikipedia.org/wiki?curid=56302927", "title": "B. D. Porritt", "text": "B. D. Porritt\n\nBenjamin Dawson Porritt FRSE FIC FIP FIRI (26 January 1884 – 28 January 1940) was an early 20th century British chemist and academic author, specialising in the use of rubber. In authorship he is known as B. D. Porritt. He was a keen yachtsman and rugby player.\n\nHe was born on 26 January 1884 at Turtle Mountain in Canada. He was the son of Herbert Thomas Porritt, from Armley, Yorkshire in Britain. His family returned to Britain soon after his birth and he was educated at Whitgift Grammar School in Croydon in London. In 1903 he entered University College, London studying Chemistry. He graduated BSc in 1906.He then proceeded to gain an MSc and began research with Sir William Ramsay and Sir Norman Collie.\n\nAround 1909 he moved to Edinburgh as a chemist to the North British Rubber Company who had huge works in the Fountainbridge district (mainly making wellington boots and rubber hot water bottles). He was promoted to Senior Chemist in 1912. In 1916 he became Research Superintendent.\n\nIn 1919 he was elected a Fellow of the Royal Society of Edinburgh. His proposers were Sir James Walker, Alexander Lauder, Sir Edmund Taylor Whittaker and Cargill Gilston Knott.\n\nIn 1920 he moved back to Croydon taking on the role of Director of the newly formed Research Association of the British Rubber and Tyre Manufacturers. He received the Colwyn Medal for services to Chemistry in 1938.\n\nHe remained in this post until his death, which occurred in Croydon, London, on 28 January 1940, a few days after his 56th birthday.\n\nHe was married with two daughters.\n\n"}
{"id": "58036987", "url": "https://en.wikipedia.org/wiki?curid=58036987", "title": "Bahamas Plastic Movement", "text": "Bahamas Plastic Movement\n\nThe Bahamas Plastic Movement is a Nonprofit organization based out of the South Eleuthera, Bahamas that focuses on reducing plastic pollution. The organization was founded in 2014 by Kristal Ambrose. The organization specializes in using youth to end plastic pollution. \n\nIn 2018 a youth delegation from the Bahamas Plastic Movement traveled to Nassau, Bahamas to meet with Romauld Ferreira, Minister of the Environment and Housing, to ban plastic bags in the Bahamas. The Bahamas Plastic Movement claimed that if plastic pollution on beaches increased that $8.5 million in tourism would be lost. Fereira and his cabinet have since approved plans to phase out plastic bags by 2020.\n"}
{"id": "515382", "url": "https://en.wikipedia.org/wiki?curid=515382", "title": "Beamline", "text": "Beamline\n\nIn accelerator physics, a beamline refers to the trajectory of the beam of accelerated particles, including the overall construction of the path segment (vacuum tube, magnets, diagnostic devices) along a specific path of an accelerator facility. This part is either\n\nBeamlines usually end in experimental stations that utilize particle beams or synchrotron light obtained from a synchrotron, or neutrons from a spallation source or research reactor. Beamlines are used in experiments in particle physics, materials science, chemistry, and molecular biology.\n\nIn particle accelerators the beamline is usually housed in a tunnel and/or underground, cased inside a concrete housing. The beamline is usually a cylindrical metal pipe, typically called a \"beam pipe\", and/or a \"drift tube\", evacuated to a high vacuum so there are few gas molecules in the path for the beam of accelerated particles to hit, which would scatter them before they reach their destination.\n\nThere are specialized devices and equipment on the beamline that are used for producing, maintaining, monitoring, and accelerating the particle beam. These devices may be in proximity or attached to the beamline. These devices include sophisticated transducers, diagnostics (position monitors and wire scanners), lenses, collimators, thermocouples, ion pumps, ion gauges, ion chambers (sometimes called \"beam loss monitors\"), vacuum valves (\"isolation valves\"), and gate valves, to mention a few. There are also water cooling devices to cool the dipole and quadrupole magnets. Positive pressure, such as that provided by compressed air, regulates and controls the vacuum valves and manipulators on the beamline.\n\nIt is imperative to have all beamline sections, magnets, etc., aligned by a survey and alignment crew by using a laser tracker. All beamlines must be within micrometre tolerance. Good alignment helps to prevent beam loss, and beam from colliding with the pipe walls, which creates secondary emissions and/or radiation.\n\nRegarding synchrotrons, \"beamline\" may also refer to the instrumentation that carries beams of synchrotron radiation to an experimental end station, which uses the radiation produced by the bending magnets and insertion devices in the storage ring of a synchrotron radiation facility. A typical application for this kind of beamline is crystallography, although many other utilising synchrotron light exist.\n\nAt a large synchrotron facility there will be many beamlines, each optimised for a particular field of research. The differences will depend on the type of insertion device (which, in turn, determines the intensity and spectral distribution of the radiation); the beam conditioning equipment; and the experimental end station. A typical beamline at a modern synchrotron facility will be 25 to 100 m long from the storage ring to the end station, and may cost up to millions of US dollars. For this reason, a synchrotron facility is often built in stages, with the first few beamlines opening on day one of operation, and other beamlines being added later as the funding permits.\n\nThe beamline elements are located in radiation shielding enclosures, called hutches, which are the size of a small room (cabin). A typical beamline consists of two hutches, an optical hutch for the beam conditioning elements and an experimental hutch, which houses the experiment. Between hutches, the beam travels in a transport tube. Entrance to the hutches is forbidden when the beam shutter is open and radiation can enter the hutch. This is enforced by the use of elaborate safety systems with redundant interlocking functions, which make sure that no one is inside the hutch when the radiation is turned on. The safety system will also shut down the radiation beam if the door to the hutch is accidentally opened when the beam is on. In this case, the beam is dumped, meaning the stored beam is diverted into a target designed to absorb and contain its energy.\n\nElements that are used in beamlines by experimenters for conditioning the radiation beam between the storage ring and the end station include the following:\n\n\nThe combination of beam conditioning devices controls the thermal load (heating caused by the beam) at the end station; the spectrum of radiation incident at the end station; and the focus or collimation of the beam. Devices along the beamline which absorb significant power from the beam may need to be actively cooled by water, or liquid nitrogen. The entire length of a beamline is normally kept under ultra high vacuum conditions.\n\nAlthough the design of a synchrotron radiation beamline may be seen as an application of X-ray optics, there are dedicated tools for modeling the x-ray propagation down the beamline and their interaction with various components. There are ray-tracing codes such as Shadow and McXTrace that treat the x-ray beam in the geometric optics limit, and then there are wave propagation software that takes into account diffraction, and the intrinsic wavelike properties of the radiation. For the purposes of understanding full or partial coherence of the synchrotron radiation, the wave properties need to be taken into account. The codes SRW and Spectra include this possibility.\n\nAn experimental end station in a neutron facility is called a neutron beamline. Superficially, neutron beamlines differ from synchrotron radiation beamlines mostly by the fact that they use neutrons from a research reactor or a spallation source instead of photons. The experiments usually measure neutron scattering from the sample under study.\n\n\n"}
{"id": "8230217", "url": "https://en.wikipedia.org/wiki?curid=8230217", "title": "Birds of North America", "text": "Birds of North America\n\nBirds of North America is a comprehensive encyclopedia of bird species in the United States and Canada, with substantial articles about each species. It was first published as a series of 716 printed booklets, prepared by 863 authors, and made available as the booklets were completed from 1992 through 2003. The project was overseen by the American Ornithologists' Union in partnership with the Academy of Natural Sciences of Philadelphia.\nIn 2004, an online version of the encyclopedia, including audio and video resources, was produced and released by the Cornell Lab of Ornithology. Access is by personal or institutional subscription.\n"}
{"id": "47943235", "url": "https://en.wikipedia.org/wiki?curid=47943235", "title": "Broken Ridge", "text": "Broken Ridge\n\nBroken Ridge or Broken Plateau is an oceanic plateau in the south-eastern Indian Ocean. Broken Ridge once formed a large igneous province (LIP) together with the Kerguelen Plateau. When Australia and Antarctica started to separate, Broken Ridge and the Kerguelen Plateau got separated by the Southeast Indian Ridge. Alkalic basalt from Broken Ridge has been dated to 95 Ma.\n\nBroken Ridge stretches from the southern end of Ninety East Ridge towards the south-western corner of Australia. It is up to wide and reaches below sea level. It is separated from the Diamantina Fracture Zone on its southern side by a escarpment, while on the northern side the ridge slopes gently towards the abyssal Wharton Basin. The sediment cover on the ridge reaches and the Moho is found at about . It is separated from the Naturaliste Plateau by the Dirck Hartog Ridge.\n\nThe Kerguelen LIP covered making it the second largest LIP on Earth (after the Ontong Java Plateau in the Pacific). Both these enormous LIPs reaches above the surrounding ocean floor and have a crustal thickness of (compared to oceanic crust typically around thick.)\nThe Broken Ridge and Kerguelen Plateau are now separated by . When they broke-up, the southern flank of Broken Ridge was uplifted some and reached above sea level.\n\nThe Kerguelen LIP has a long and complicated history, however, and is probably the least \"typical\" of all LIPs.\nRocks from both the Broken Ridge and the Kerguelen Plateau contain a continental component or \"fingerprint\". In the Early Cretaceous, the Kerguelen hotspot was split into several diapirs of various sizes, composition, and ascent rates. These separate diapirs created the Bunbury Basalt, the Southern Kerguelen Plateau, the Rajmahal Traps/Indian lamprophyres, Antarctic lamprophyres, and the Central Kerguelen Plateau/Broken Ridge. In the late Cretaceous, activity in the mantle slowed and the Kerguelen hotspot was reduced to a single plume which created the Ninety East Ridge.\n120-95 Ma when the Southern and Central Kerguelen Plateau formed together with the Broken Ridge, the Kerguelen hotspot produced /year, but 95-25 Ma the output decreased to .\n\n"}
{"id": "30669133", "url": "https://en.wikipedia.org/wiki?curid=30669133", "title": "Central Canadian Shield forests", "text": "Central Canadian Shield forests\n\nThe Central Canadian Shield forests are a taiga ecoregion of Canada.\n\nThis ecoregion consists of rolling hills, lakes, bogs and rocky outcrops covering a large curved swathe on the Canadian Shield from eastern Manitoba and Northern Ontario running southeastwards through Thunder Bay District to the north shore of Lake Superior and then northeastwards into western Quebec. The area has a cool climate with summer tempearatures averaging around 13°C dropping to a very cold -17°C in winter, with the hills around Lac Seul in northwestern Ontario being warmer and wetter than most of the area. Average rainfall in the eastern side of the ecoregion is 550mm per year, rising to 750mm in the centre (near Lake Nipigon for example), up to 900mm in the west. Specific areas include Lake Nipigon and Big Trout Lake (Ontario). These forest contrast with more severe boreal forest ecoregions such as the Eastern Canadian Shield taiga to the north, which covers most of Northern Quebec and Labrador.\n\nThese hills support a large area of rich taiga forest dominated by black spruce (\"Picea mariana\") along with jack pine and some paper birch (\"Betula papyrifera\") and in the warmer south-facing areas some trembling aspen (\"Populus tremuloides\"), white spruce (\"Picea glauca\"), Ontario balsam poplar (\"Populus balsamifera\") and balsam fir (\"Abies balsamea\"). Many of the rocks of the area are covered with colourful lichens. The ecoregion contains large areas of wetland especially in the south east, and a greater variety of plant life on the north shore of Lake Superior.\n\nMammals of the taiga include moose (\"Alces alces\"), the southernmost populations of migratory woodland caribou (\"Rangifer tarandus caribou\"), American black bear (\"Ursus americanus\"), Canada lynx (\"Lynx canadensis\"), snowshoe hare (\"Lepus americanus\") and grey wolf (\"Canis lupus\"). Birds of the area include northern hawk owl (\"Surnia ulula\"), great horned owl (\"Bubo virginianus\"), sharp-tailed grouse (\"Tympanuchus phasianellus\"), ruffed grouse (\"Bonasa umbellus\"), American black duck (\"Anas rubripes\"), wood duck (\"Aix sponsa\"), Canada goose (\"Branta canadensis\") in the north, and hooded merganser (\"Lophodytes cucullatus\") and pileated woodpecker (\"Dryocopus pileatus\") in the west.\n\nThis is fairly well-preserved ecoregion with 40% of original forest intact (less so in Manitoba) while most has been heavily changed by logging and in some areas by mining and hydroelectric power generation. Large blocks of undisturbed forest do remain, for example north of Lake Nipigon in Ontario and around Lake Mistassini in Quebec, and in the following protected areas: Wabakimi Provincial Park northwest of Lake Nipigon, Pukaskwa National Park near Marathon, Ontario on the Lake Superior shore, a number of rivers and lakes in Northern Ontario (Winisk River, Missinaibi River, Albany River north of Wabakimi Park, Severn River (northern Ontario), Attawapiskat Lake/Attawapiskat River and Brightsand River), Kesagami Provincial Park, Michipicoten Island in Lake Superior, all in Ontario; and Aiguebelle National Park in Quebec.\n"}
{"id": "9041863", "url": "https://en.wikipedia.org/wiki?curid=9041863", "title": "ChaNGa", "text": "ChaNGa\n\nChaNGa (Charm N-body GrAvity solver) is a computer program to perform collisionless \"N\"-body simulations. It can perform cosmological simulations with periodic boundary conditions in comoving coordinates or simulations of isolated stellar systems. It is based on the Barnes–Hut algorithm and uses Ewald summation for periodic forces.\n\nChaNGa makes use of the Charm++ parallel programming system, including its dynamic load balancing schemes, in order to scale to large processor configurations. Simulation results have been reported on up to 20,000 IBM Bluegene/L processors .\n\nFor more information on obtaining, building and running ChaNGa, please see the Wiki documentation at\n\n\n"}
{"id": "49755248", "url": "https://en.wikipedia.org/wiki?curid=49755248", "title": "Charleshatchettite", "text": "Charleshatchettite\n\nCharleshatchettite is a very rare, complex, niobium oxide mineral with the formula CaNbO(OH)•8HO. It was discovered in the mineral-rich site Mont Saint-Hilaire, Montérégie, Québec, Canada.\n\nCharleshatchettite is chemically similar to hochelagaite.\n"}
{"id": "13717508", "url": "https://en.wikipedia.org/wiki?curid=13717508", "title": "Cirrus castellanus cloud", "text": "Cirrus castellanus cloud\n\nCirrus castellanus is a species of cirrus cloud. Its name comes from the word \"castellanus\", which means \"of a fort\", \"of a castle\" in Latin. Like all cirrus, this species occurs at high altitudes. It appears as separate turrets rising from a lower-level cloud base. Often these cloud turrets form in lines, and they can be taller than they are wide. This cloud species is usually dense in formation.\n\n\n"}
{"id": "31546082", "url": "https://en.wikipedia.org/wiki?curid=31546082", "title": "Coastal flood", "text": "Coastal flood\n\nCoastal flooding occurs when normally dry, low-lying land is flooded by seawater. The extent of coastal flooding is a function of the elevation inland flood waters penetrate which is controlled by the topography of the coastal land exposed to flooding. The seawater can inundate the land via several different paths:\n\n\nCoastal flooding is largely a natural event, however human influence on the coastal environment can exacerbate coastal flooding. Extraction of water from groundwater reservoirs in the coastal zone can enhance subsidence of the land increasing the risk of flooding. Engineered protection structures along the coast such as sea walls alter the natural processes of the beach, often leading to erosion on adjacent stretches of the coast which also increases the risk of flooding.\n\nCoastal flooding can result from a variety of different causes including storm surges created by storms like hurricanes and tropical cyclones, rising sea levels due to climate change and by tsunamis.\n\nStorms can cause flooding through storm surges which are waves significantly larger than normal and if a storm event corresponds with the high astronomical tide extensive flooding can occur. Storm surges occur during storm events, including hurricanes and tropical cyclones due to three processes:\n\n\nWinds blowing in an onshore direction (from the sea towards the land) can cause the water to 'pile up' against the coast. This is known as wind setup. Low atmospheric pressure is associated with storm systems and this tends to increase the surface sea level, this is barometric setup. Finally increased wave break height results in a higher water level in the surf zone which is wave setup. These three processes interact to create waves that can overtop natural and engineered coastal protection structures thus penetrating seawater further inland than normal.\n\nThe Intergovernmental Panel on Climate Change (IPCC) estimate global mean sea-level rise from 1990 to 2100 to be between nine and eighty eight centimetres. It is also predicted that with climate change there will be an increase in the intensity and frequency of storm events such as hurricanes. This suggests that coastal flooding from storm surges will become more frequent with sea level rise. A rise in sea level alone threatens increased levels of flooding and permanent inundation of low-lying land as sea level simply may exceed the land elevation. This therefore indicates that coastal flooding associated with sea level rise will become a significant issue into the next 100 years especially as human populations continue to grow and occupy the coastal zone.\n\nCoastal areas can be significantly flooded as the result of tsunami waves which propagate through the ocean as the result of the displacement of a significant body of water through earthquakes, landslides, volcanic eruptions and glacier calvings. There is also evidence to suggest that significant tsunami have been caused in the past by meteor impact into the ocean. Tsunami waves are so destructive due to the velocity of the approaching waves, the height of the waves when they reach land and the debris the water entrains as it flows over land can cause further damage.\n\nIt has been said that one way to prevent significant flooding of coastal areas now and into the future is by reducing global sea level rise. This could be minimised by further reducing greenhouse gas emissions. However, even if significant emission decreases are achieved, there is already a substantial commitment to sea level rise into the future. International climate change policies like the Kyoto Protocol are seeking to mitigate the future effects of climate change, including sea level rise.\n\nIn addition, more immediate measures of engineered and natural defences are put in place to prevent coastal flooding.\n\nThere are a variety of ways in which humans are trying to prevent the flooding of coastal environments, gypically through so called hard engineering structures such as seawalls and levees. That armouring of the coast is typically to protect towns and cities which have developed right up to the beachfront. Enhancing depositional processes along the coast can also help prevent coastal flooding. Structures such as groynes (Figure 1), breakwaters and artificial headlands promote the deposition of sediment on the beach thus helping to buffer against storm waves and surges as the wave energy is spent on moving the sediments in the beach than on moving water inland.\n\nThe coast does provide natural protective structures to guard against coastal flooding. These include physical features like gravel bars and sand dune systems, but also ecosystems such as salt marshes and mangrove forests (Figure 2) have a buffering function. Mangroves and wetlands are often considered to provide significant protection against storm waves, tsunamis and shoreline erosion through their ability to attenuate wave energy. To protect the coastal zone from flooding, the natural defences should therefore be protected and maintained.\n\nAs coastal flooding is typically a natural process, it is inherently difficult to prevent flood occurrence. If human systems are affected by flooding, an adaption to how that system operates on the coast through behavioural and institutional changes is required, these changes are the so-called \"non-structural\" mechanisms of coastal flooding response. Building regulations, coastal hazard zoning, urban development planning, spreading the risk through insurance and enhancing public awareness are some ways of achieving this. Adapting to the risk of flood occurrence, can be the best option if the cost of building defence structures outweighs any benefits or if the natural processes in that stretch of coastline add to its natural character and attractiveness. A more extreme and often difficult to accept response to coastal flooding is abandoning the area (also known as managed retreat) prone to flooding. This however raises issues for where the people and infrastructure affected would go and what sort of compensation should/could be paid.\n\nThe coastal zone (the area both within 100 kilometres distance of the coast and 100 metres elevation of sea level) is home to a large and growing proportion of the global population. Over 50 percent of the global population and 65 percent of cities with populations over five million people are in the coastal zone. In addition to the significant number of people at risk of coastal flooding, these coastal urban centres are producing a considerable amount of the global Gross Domestic Product (GDP). People's lives, homes, businesses and city infrastructure like roads, railways and industrial plants are all at risk of coastal flooding with massive potential social and economic costs. The recent earthquakes and tsunami in Indonesia in 2004 and in Japan in March 2011 clearly illustrate the devastation coastal flooding can produce. Indirect economic costs can be incurred if economically important sandy beaches are eroded away resulting in a loss of tourism in areas dependent on the attractiveness of those beaches.\n\nCoastal flooding can result in a wide variety of environmental impacts on different spatial and temporal scales. Flooding can destroy coastal habitats such as coastal wetlands and estuaries and can erode dune systems. These places are characterised by their high biological diversity therefore coastal flooding can cause significant biodiversity loss and potentially species extinctions. In addition to this, these coastal features are the coasts natural buffering system against storm waves; consistent coastal flooding and sea level rise can cause this natural protection to be reduced allowing waves to penetrate greater distances inland exacerbating erosion and furthering coastal flooding. Prolonged inundation of seawater after flooding can also cause salination of agriculturally productive soils thus resulting in a loss of productivity for long periods of time. Food crops and forests can be completely killed off by salination of soils or wiped out by the movement of flood waters. Coastal freshwater bodies including lakes, lagoons and coastal freshwater aquifers can also be affected by saltwater intrusion. This can destroy these water bodies as habitats for freshwater organisms and sources of drinking water for towns and cities.\n\nExamples of existing coastal flooding issues include:\n\nThe Thames Barrier (Fig. 3) is one of the world's largest flood barriers and serves to protect London from flooding during exceptionally high tides and storm surges. The Barrier can be lifted at high tide to prevent sea waters flooding London and can be lowered to release storm water runoff from the Thames catchment (for more information see Thames Barrier)\n\nFlooding of this low-lying coastal zone can result in prolonged inundation, which can affect the productivity of the affected pastoral agriculture for several years.\n\nHurricane Katrina made landfall as a category 3 cyclone on the Saffir–Simpson hurricane wind scale, indicating that it had become an only moderate level storm. However the catastrophic damage caused by the extensive flooding (Fig. 4) was the result of the highest recorded storm surges in North America. For several days prior to landfall of Katrina, wave set up was generated by the persistent winds of the cyclonic rotation of the system. This prolonged wave set up coupled with the very low central pressure level meant massive storm surges were generated. Storm surges overtopped and breached the levees and flood walls intended to protect the city from inundation. Unfortunately New Orleans is inherently prone to coastal flooding for a number of factors. Firstly, much of New Orleans is below sea level and is bordered by the Mississippi River therefore protection against flooding from both the sea and the river has become dependent on engineered structures. Land use change and modification to natural systems in the Mississippi River have rendered the natural defences for the city less effective. Wetland loss has been calculated to be around since 1930. This is a significant amount as four miles of wetland are estimated to reduce the height of a storm surge by one foot (30 centimetres).\n\nAn earthquake of approximately magnitude 9.0 struck off the coast of Sumatra, Indonesia causing the propagation of a massive tsunami throughout the Indian Ocean. This tsunami caused significant loss of human life, an estimate of 280,000 – 300,000 people has been reported and caused extensive damage to villages, towns and cities (Fig. 5) and to the physical environment. The natural structures and habitats destroyed or damaged included coral reefs, mangroves, beaches and seagrass beds. The more recent earthquake and tsunami in Japan in March 2012 also clearly illustrates the destructive power of tsunamis and the turmoil of coastal flooding.\n\nThere is a need for future research into:\n\n\n\n"}
{"id": "2160676", "url": "https://en.wikipedia.org/wiki?curid=2160676", "title": "Coastal geography", "text": "Coastal geography\n\nCoastal geography is the study of the constantly changing region between the ocean and the land, incorporating both the physical geography (i.e. coastal geomorphology, geology and oceanography) and the human geography (sociology and history) of the coast. It includes understanding coastal weathering processes, particularly wave action, sediment movement and weather, and the ways in which humans interact with the coast\n\nThe waves of different strengths that constantly hit against the shoreline are the primary movers and shapers of the coastline. Despite the simplicity of this process, the differences between waves and the rocks they hit result in hugely varying shapes.\n\nThe effect that waves have depends on their strength. Strong waves, also called destructive waves, occur on high-energy beaches and are typical of winter. They reduce the quantity of sediment present on the beach by carrying it out to bars under the sea. Constructive, weak waves are typical of low-energy beaches and occur most during summer. They do the opposite to destructive waves and increase the size of the beach by piling sediment up onto the berm.\n\nOne of the most important transport mechanisms results from wave refraction. Since waves rarely break onto a shore at right angles, the upward movement of water onto the beach (swash) occurs at an oblique angle. However, the return of water (backwash) is at right angles to the beach, resulting in the net movement of beach material laterally. This movement is known as beach drift (Figure 3). The endless cycle of swash and backwash and resulting beach drift can be observed on all beaches. This may differ between coasts.\nProbably the most important effect is longshore drift (LSD)(Also known as Littoral Drift), the process by which sediment is continuously moved along beaches by wave action. LSD occurs because waves hit the shore at an angle, pick up sediment (sand) on the shore and carry it down the beach at an angle (this is called swash). Due to gravity, the water then falls back perpendicular to the beach, dropping its sediment as it loses energy (this is called backwash). The sediment is then picked up by the next wave and pushed slightly further down the beach, resulting in a continual movement of sediment in one direction. This is the reason why long strips of coast are covered in sediment, not just the areas around river mouths, which are the main sources of beach sediment. LSD is reliant on a constant supply of sediment from rivers and if sediment supply is stopped or sediment falls into a submarine canals at any point along a beach, this can lead to bare beaches further along the shore.\n\nLSD helps create many landforms including barrier islands, bay beaches and spits. In general LSD action serves to straighten the coast because the creation of barriers cuts off bays from the sea while sediment usually builds up in bays because the waves there are weaker (due to wave refraction), while sediment is carried away from the exposed headlands. The lack of sediment on headlands removes the protection of waves from them and makes them more vulnerable to weathering while the gathering of sediment in bays (where longshore drift is unable to remove it) protects the bays from further erosion and makes them pleasant recreational beaches.\n\n\nIn tropical regions in particular, plants and animals not only affect the weathering of rocks but are a source of sediment themselves. The shells and skeletons of many organisms are of calcium carbonate and when this is broken down it forms sediment, limestone and clay.\n\nThe main physical Weathering process on beaches is salt-crystal growth. Wind carries salt spray onto rocks, where it is absorbed into small pores and cracks within the rocks. There the water evaporates and the salt crystallises, creating pressure and often breaking down the rock. In some beaches calcium carbonate is able to bind together other sediments to form beachrock and in warmer areas dunerock. Wind erosion is also a form of erosion, dust and sand is carried around in the air and slowly erodes rock, this happens in a similar way in the sea were the salt and sand is washed up onto the rocks.\n\nThe sea level on earth regularly rises and falls due to climatic changes. During cold periods more of the Earth’s water is stored as ice in glaciers while during warm periods it is released and sea levels rise to cover more land. Sea levels are currently quite high, while just 18,000 years ago during the Pleistocene ice age they were quite low. Global warming may result in further rises in the future, which presents a risk to coastal cities as most would be flooded by only small rises. As sea levels rise, fjords and rias form. Fjords are flooded glacial valleys and rias are flooded river valleys. Fjords typically have steep rocky sides, while rias have dendritic drainage patterns typical of drainage zones. As tectonic plates move about the Earth they can rise and fall due to changing pressures and the presence of glaciers. If a beach is moving upwards relative to other plates this is known as isostatic change and raised beaches can be formed.\nThis is found in the U.K. as above the line from the Wash to the Severn estuary, the land was covered in ice sheets during the last ice age. The weight of the ice caused northeast Scotland to sink, displacing the southeast and forcing it to rise. As the ice sheets receded the reverse process happened, as the land was released from the weight. At current estimates the southeast is sinking at a rate of about 2 mm per year, with northeast Scotland rising by the same amount.\n\nIf the coast suddenly changes direction, especially around an estuary, spits are likely to form. Long shore drift pushes the sediment along the beach but when it reaches a turn as in the diagram, the long shore drift does not always easily turn with it, especially near an estuary where the outward flow from a river may push sediment away from the coast. The area may be also be shielded from wave action, preventing much long shore drift. On the side of the headland receiving weaker waves, shingle and other large sediments will build up under the water where waves are not strong enough to move them along. This provides a good place for smaller sediments to build up to sea level. The sediment, after passing the headland will accumulate on the other side and not continue down the beach, sheltered both by the headland and the shingle.\n\nSlowly over time sediment simply builds on this area, extending the spit outwards, forming a barrier of sand. Once in a while, the wind direction will change and come from the other direction. During this period the sediment will be pushed along in the other direction. The spit will start to grow backwards, forming a 'hook'. After this time the spit will grow again in the original direction. Eventually the spit will not be able to grow any further because it is no longer sufficiently sheltered from erosion by waves, or because the estuary current prevents sediment resting. Usually in the salty but calm waters behind the spit there will form a salt marshland. Spits often form around the breakwater of artificial harbours requiring dredging.\n\nOccasionally, if there is no estuary then it is possible for the spit to grow across to the other side of the bay and form what is called a bar, or barrier. Barriers come in several varieties, but all form in a manner similar to spits. They usually enclose a bay to form a lagoon. They can join two headlands or join a headland to the mainland. When an island is joined to the mainland with a bar or barrier it is known as a tombolo. This usually occurs due to wave refraction, but can also be caused by isostatic change, a change in the level of the land (e.g. Chesil Beach). An example of this is along the Holderness coastline.\n\n\n\n"}
{"id": "9731591", "url": "https://en.wikipedia.org/wiki?curid=9731591", "title": "Colin Skinner", "text": "Colin Skinner\n\nDr. Colin Skinner (born 1965) is a British author, adventurer and molecular biologist who is attempting to walk around the world. As of mid-2014, he has walked over and has crossed Great Britain, Iceland, America and New Zealand. He has used the walks to raise money and awareness for various causes, including conservation biology, people with disabilities, cancer relief, AIDS, and hospice.\n\nSkinner earned his Bachelor of Science (B.Sc.) combined honours degree in biochemistry and genetics from the University of Leeds. He earned his PhD in molecular biology from University College London. He earned his Postgraduate Certificate in Education (PGCE) in secondary science from Canterbury Christ Church University.\n\nHe began at the age of 18 at John o' Groats (at the northern tip of Scotland) in 1984, and walked to Land's End in England. On this journey, which he carried out with three other people, he pushed a wheelchair and raised £3,500 for The Forelands School for handicapped children. In 1983, he had already run around a 400-metre track to raise further money for The Forelands School for handicapped children, at Broadstairs in Kent.\n\nIn 1986, at the age of 20, whilst at the University of Leeds, he crossed Iceland, together with three other people, from Seyðisfjörður in the east, through the interior to the north of the Vatnajökull ice fields, and then west to Reykjavík. The team encountered an 'ash storm', where storm force winds had whipped up fine black volcanic ash, and had to wear goggles and face masks to push on into the winds. In the rain shadow of the Vatnajökull, they ran out of water, then encountered a flash flood, as mud rushed down from the melting glaciers. They also had to survive on food contaminated with petrol that had leaked from their petrol stoves. This journey of raised £2,000 for the Royal Association for Disability and Rehabilitation. As part of the training for the walk across Iceland he ran the Leeds Marathon, in a time of 3 hours and 41 minutes.\n\nOn the Icelandic trek, he came up with the idea of walking , across Britain and America to raise money for Macmillan Cancer Support in Britain and hospice in America and Canada.\n\nOn 1 May 1988, he set off again from John o' Groats, this time walking through the West Highlands, down the Pennine Way and then south to Land's End: a distance of in seven weeks. As part of the training for this walk he ran the gruelling Snowdonia Marathon, in a time of 4 hours and 40 minutes. The walk through Scotland and England raised £2,000 for Macmillan Cancer Support.\n\nThe journey across the United States began on 15 July 1988. On the journey he slept in bushes beneath the World Trade Center, camped outside Kennedy Airport in a tent, then headed west. On Staten Island he collapsed from heat exhaustion at 105 degrees Fahrenheit. In Utah the temperatures went down to minus 30 Fahrenheit. Carrying a tent and a backpack, with no backup, he walked alone to Niagara Falls, through Ontario in Canada, to Detroit, between the Great Lakes, across the Great Plains, through the Rockies in winter, to Yellowstone National Park, then south to the Grand Canyon, on to Las Vegas, through Death Valley and then snowshoed over the Sierras to reach San Francisco. In Death Valley, down to his last $13, Skinner found $200 in the desert, and he had $1 left when he crossed the Sierras to reach Yosemite Valley. The total distance he walked from New York City to San Francisco, was .\n\nOn the journey he visited 70 hospices and appeared on television, radio and in newspapers to encourage support for hospices across the U.S. and Canada. The mayor of San Francisco, Art Agnos, proclaimed 21 March 1989, \"Colin Skinner Day,\" in recognition of the attention he brought to the work of hospices with AIDS patients in the city.\n\nReturning to Britain after this walk, he obtained a job as a research assistant in Chemical Pathology at the Middlesex Hospital and went on to obtain a PhD in Molecular Biology at University College London. In 1992, whilst studying for his PhD, he also ran the London Marathon, in a time of 4 hours and 41 minutes, to raise money to buy a computer for a young boy with physical disabilities. Skinner's PhD involved developing genetic tests to detect congenital adrenal hyperplasia in children. He had work published in a number of scientific journals. In 1994 he had his work published in \"Human Molecular Genetics\".\n\nIn 1994, Skinner married Dr. Monica Schneider (also a molecular biologist), and in 1996 their son James was born. From 1994 to 1996 Skinner worked as a Post-Doctoral Research Fellow at Vanderbilt University Medical Center, in Nashville, Tennessee. The work he carried out there involved gene sequencing and protein purification of cytochrome P450 enzymes. His work was published in the \"Journal of Biological Chemistry\". From 1996 to 1997 Skinner took care of his infant son, James, whilst his wife continued to work at Vanderbilt University Medical Center.\n\nIn 1998 he walked from Cape Reinga in the North Island, to Bluff, at the southern tip of New Zealand; a distance of . On the journey he walked through the active volcano at White Island, experienced earthquakes up to 4.9 on the Richter Scale, clambered over glaciers, swam with seals and reported on conservation biology projects involving endangered species. Information from the journey was posted on the Internet for schoolchildren in the U.S. via the Scholastic Corporation Scholastic Network. In December 2010 Skinner completed a book about the journey and conservation biology, \"New Zealand - 1500 miles on foot through - The Land Of The Long White Cloud\" on an Internet website.\n\nIn 1999 he obtained a PGCE (Postgraduate Certificate in Education) from Canterbury Christ Church University in Canterbury, England. In 2000 he worked as a secondary school science teacher at St. Edmund's School in Dover, teaching 11 to 16-year-olds.\n\nIn 2001 he worked as a volunteer at a wildlife park, working on enrichment activities for animals. From 2001 until 2003 he worked part-time at a post office. During this time, he also taught science to primary school children, in a 'Link-Scientist' scheme run by the pharmaceutical company Pfizer, and took care of his son.\n\nIn 2003 his mother died from pancreatic cancer, at the age of 59. This prompted him to write the story of the journey across Britain and America. In 2006 he finished the book \"Beyond the Setting Sun\", with an introduction by Ranulph Fiennes, the renowned polar explorer and adventurer. The book was written to raise money for hospices in Britain, Canada and America.\n\nOn 29 April 2007, he began walking again at John o' Groats and arrived at Land's End on 8 June, having covered in 6 weeks. On the walk in Britain he visited 20 hospices and raised £10,000 for hospice through sales of his \"Beyond the Setting Sun\" book.\n\nStarting on 22 August 2009, he walked from Kennedy Airport in New York City to within 15 miles of Devil's Lake, North Dakota. This was a distance of and Skinner stopped his journey on 3 December 2009, after 3 days with windchills down to -30 Fahrenheit. During the trip Skinner had to make incisions in his feet to relieve the pressure from blisters, suffered food poisoning, met up with a wolf in Upper Michigan, had to face down two wild dogs, and had ski masks frozen to his beard in North Dakota. On the journey he appeared on television, radio and in newspaper articles. He also wrote a daily blog for the National Hospice Foundation. He met hospice patients, including one woman with a terminal illness, who said that at times she could forget she was ill, thanks to the care she received in a hospice house in Buffalo, New York. He also met a man with lung cancer who could not sleep in hospitals, where there was always someone coming to check on him. In the hospice house in Windsor, Ontario, the man had a peaceful room to himself, where he could finally get some rest.\n\nOn 10 September 2011, Skinner set off from mile marker 283 on U.S. Route 2, before Devil's Lake, North Dakota and walked to Tuolumne Meadows in Yosemite National Park, California. The walk took him through North Dakota, Montana, Idaho, Utah, Arizona, Nevada and into California. Skinner snow-shoed over Tioga Pass and camped in a tent in the Sierra Mountains for several nights, but was forced to stop walking after suffering from frostbite in both feet. On the journey he appeared on television, radio and in newspapers and encouraged support for hospices taking care of people with serious illnesses. Skinner is now writing a book about this journey, entitled \"America- 12000 miles on foot, a wing and a prayer\".\n\nIn September 2012, Skinner completed a short story, entitled \"Chenga\", and published this on an Internet website. In October 2012, Skinner completed the second part of a science-fiction fantasy trilogy, entitled \"Djara\", and published this on an Internet website.\n\nSkinner has now begun writing the third part of the science-fiction fantasy trilogy, entitled \"Tau\". The \"Chenga\", \"Djara\", \"Tau\" trilogy includes the themes of time travel, parallel universes, vampires, shapeshifters, angels, demons and descendants of the fabled giants known as the Nephilim.\n\nIn 2012 Skinner published four short poems: \"Gaia\", \"The Dreaming\", \"New Zealand Water Torture\" and \"Life Jim But Not As We Know It\" on an Internet website.\n\nHe is planning future walks through Australia, Japan, China, Tibet, Afghanistan, Iran, Iraq, Israel, Egypt and Europe.\n\n\n\n"}
{"id": "31918503", "url": "https://en.wikipedia.org/wiki?curid=31918503", "title": "Debate between Winter and Summer", "text": "Debate between Winter and Summer\n\nThe Debate between Winter and Summer or Myth of Emesh and Enten is a Sumerian creation myth, written on clay tablets in the mid to late 3rd millennium BC.\n\nSeven \"debate\" topics are known from the Sumerian literature, falling in the category of 'disputations'; some examples are: the debate between sheep and grain; the debate between bird and fish; the tree and the reed; and the dispute between silver and copper, etc. These topics came some centuries after writing was established in Sumerian Mesopotamia. The debates are philosophical and address humanity's place in the world.\n\nThe first lines of the myth were discovered on the University of Pennsylvania Museum of Archaeology and Anthropology, catalogue of the Babylonian section (CBS), tablet number 8310 from their excavations at the temple library at Nippur. This was translated by George Aaron Barton in 1918 and first published as \"Sumerian religious texts\" in \"Miscellaneous Babylonian Inscriptions\", number seven, entitled \"A Hymn to Ibbi-Sin\". The tablet is by by at its thickest point. Barton describes Ibbi-Sin as an \"inglorious King\" suggesting the text to have been composed during his lifetime, he commented \"The hymn provides a powerful statement for emperor worship in Ur at the time of composition.\" Ibbi-Sin is still mentioned in the modern translation \"For my king named by Nanna, the son of Enlil, Ibbi-Sin, when he is arrayed in the 'cutur' garment and the 'hursag' garment.\"\n\nAnother tablet from the same collection, number 8886 was documented by Edward Chiera in \"Sumerian Epics and Myths\", number 46. Samuel Noah Kramer included CBS tablets 3167, 10431, 13857, 29.13.464, 29.16.142 (which forms a join with 8310), 29.16.232, 29.16.417, 29.16.427, 29.16.446 and 29.16.448. He also included translations from tablets in the Nippur collection of the Museum of the Ancient Orient in Istanbul, catalogue numbers 2705, 3167 and 4004. Further tablets from Nippur were added by Jane Heimerdinger. Other tablets were added from the \"Ur excavations texts\" in 1928 along with several others to bring it to its present form. A later edition of the text were published by Miguel Civil in 1996.\n\nThe story takes the form of a contest poem between two cultural entities first identified by Kramer as vegetation gods, Emesh and Enten. These were later identified with the natural phenomena of Winter and Summer. The location and occasion of the story is described in the introduction with the usual creation sequence of day and night, food and fertility, weather and seasons and sluice gates for irrigation.\n\nThe two seasons are personified as brothers, born after Enlil copulates with a \"hursag\" (hill). The destinies of Summer and Winter are then described, Summer founding towns and villages with plentiful harvests, Winter to bring the Spring floods.\n\nThe two brothers soon decide to take their gifts to Enlil's \"house of life\", the E-namtila, where they begin a debate about their relative merits. Summer argues:\n\nTo which Winter replies:\n\nEnlil eventually intervenes and declares Winter the winner of the debate and there is a scene of reconciliation. Bendt Alster explains \"Winter prevails over Summer, because Winter provides the water that was so essential to agriculture in the hot climate of ancient Mesopotamia.\"\n\nJohn Walton wrote that \"people in the Ancient Near East did not think of creation in terms of making material things – instead, everything is function oriented. Creation thus constituted bringing order to the cosmos from an originally nonfunctional condition. Consequently, to create something (cause it to exist) in the ancient world means to give it a function, not material properties.\" Samuel Noah Kramer has noted this myth \"is the closest extant Sumerian parallel to the Biblical Cain and Abel story\" in the Book of Genesis (). This connection has been made by other scholars. The disputation form has also been suggested to have similar elements to the discussions between Job and his friends in the Book of Job. M. L. West noted similarities with Aesop's fable \"a debate between Winter and Spring\" along with another similar work by Bion of Smyrna.\n\nJ.J.A. van Dijk analysed the myth and determined the following common elements with other Sumerian debates \"(1) Introduction, presenting the disputants and the occasion of the dispute; (2) the dispute itself, in which each party praises himself and attacks the other; (3) judgement uttered by a god, followed by reconciliation; (4) a formula of praise.\" Bendt Alster suggests a link to harvest festivals, saying \"It is definitely conceivable that summer and winter contests may have belonged to festivals celebrating the harvest among the peasants.\" Herman Vanstiphout has suggested the lexical listing of offerings was used in scribal training, quoting the example from the myth \"Wild Animals, cattle and sheep from the mountains, Wild rams, mountain rams, deer and full-grown ibex, Mountain sheep, first class sheep, and fat tailed sheep he brings.\"\n\nEliade and Adams note that in the story, the water flows through the \"hursag\" (foothills), Enlil is identified as a \"kurgal\" (mountain) and his main temple being the \"eKur\" (mountain house), they link this mountain aspect with Enlil being the \"Lord of the winds\" by suggesting the ancients believed the winds originated in the mountains. Piotr Michalowski makes the connection in the story that \"E-hursag\" is a structure \"named as the residence of the king\" and \"E-namtilla\" \"as the residence of Enlil\", suspecting the two words refer to the same place and that \"E-namtilla is simply another name for E-hursag\" and that it was a royal palace.\n\n\n\n"}
{"id": "34299995", "url": "https://en.wikipedia.org/wiki?curid=34299995", "title": "Department of Energy (South Africa)", "text": "Department of Energy (South Africa)\n\nThe Department of Energy is the department of the South African government responsible for energy policy. It was established in 2009 when the former Department of Minerals and Energy was divided into the Department of Energy and the Department of Mineral Resources.\n\nFrom 2012 to 2014, the Minister of Energy was Ben Martins and his deputy was Barbara Thompson. Tina Joemat-Pettersson MP has been the Minister of Energy since 25 May 2014 . She was previously the Minister of Agriculture, Forestry, and Fisheries from 2009-2014. After Tine Joemat-Petterson was asked to leave, Mmamoloko Kubayi was appointed. This only lasted 7 months before the next reshuffle and the appointment of David Mahlobo. His appointment was potentially linked to securing the planned Russian nuclear deal - a country he had just visited as Minister of State Security.link\n\nPresident Cyril Ramaphosa appointed Jeff Radebe as Minister as part of his cabinet reshuffle on 26 January 2018.\n\nIn the 2016/2017 budget the department had a budget of R7,545 million and a staff complement of 622 civil servants.\n\nIn August 2018, the Department of Energy released a draft of South Africa's updated Integrated Resource Plan (IRP), the plan which seeks to meet the country's energy consumption demands, for public comment. The current plan dropped proposals for expansion of the number of nuclear plants in the country, focusing instead on expanding the production of renewable energy and creating two new coal power plants.\n\n"}
{"id": "43139769", "url": "https://en.wikipedia.org/wiki?curid=43139769", "title": "Direct energy conversion", "text": "Direct energy conversion\n\nDirect energy conversion (DEC) or simply direct conversion converts a charged particle's kinetic energy into a voltage. It is a scheme for power extraction from nuclear fusion.\n\nIn the middle of the 1960s direct energy conversion was proposed as a method for capturing the energy from the exhaust gas in a fusion reactor. This would generate a direct current of electricity. Richard F. Post at the Lawrence Livermore National Laboratory was an early proponent of the idea. Post reasoned that capturing the energy would require five steps: (1) Ordering the charged particles into linear beam. (2) Separation of positives and negatives. (3) Separating the ions into groups, by their energy. (4) Gathering these ions as they touch collectors. (5) Using these collectors as the positive side in a circuit. Post argued that the efficiency was theoretically determined by the number of collectors.\n\nDesigns in the early 1970s by William Barr and Ralph Moir used metal ribbons at an angle to collect these ions. This was called the Venetian Blind design, because the ribbons look like window blinds. Those metal ribbon-like surfaces are more transparent to ions going forward than to ions going backward. Ions pass through surfaces of successively increasing potential until they turn and start back, along a parabolic trajectory. They then see opaque surfaces and are caught. Thus ions are sorted by energy with high-energy ions being caught on high-potential electrodes.\n\nWilliam Barr and Ralph Moir then ran a group which did a series of direct energy conversion experiments through the late 1970s and early 1980s. The first experiments used beams of positives and negatives as fuel, and demonstrated energy capture at a peak efficiency of 65 percent and a minimum efficiency of 50 percent. The following experiments involved a true plasma direct converter that was tested on the Tandem Mirror Experiment (TMX), an operating magnetic mirror fusion reactor. In the experiment, the plasma moved along diverging field lines, spreading it out and converting it into a forward moving beam with a Debye length of a few centimeters.<ref name=\"doi10.1088/0741-3335/36/8/003\"></ref> Suppressor grids then reflect the electrons, and collector anodes recovered the ion energy by\nslowing them down and collecting them at high-potential plates. This machine demonstrated an energy capture efficiency of 48 percent. However, Marshall Rosenbluth argued that keeping the plasma's neutral charge over the very short Debye length distance would be very challenging in practice, though he said that this problem would not occur in every version of this technology.\n\nThe Venetian Blind converter can operate with 100 to 150 keV D-T plasma, with an efficiency of about 60% under conditions compatible with economics, and an upper technical conversion efficiency up to 70% ignoring economic limitations.\n\nA second type of electrostatic converter initially proposed by Post, then developed by Barr and Moir, is the Periodic Electrostatic Focusing concept. Like the Venetian Blind concept, it is also a direct collector, but the collector plates are disposed in many stages along the longitudinal axis of an electrostatic focusing channel. As each ion is decelerated along the channel toward zero energy, the particle becomes \"over-focused\" and is deflected sideways from the beam, then collected. The Periodic Electrostatic Focusing converter typically operates with a 600 keV D-T plasma (as low as 400 keV and up to 800 keV) with efficiency of about 60% under conditions compatible with economics, and an upper technical conversion efficiency up to 90% ignoring economic limitations.\n\nFrom the 1960s through the 1970s, methods have been developed to extract electrical energy directly from a hot gas (a plasma) in motion within a channel fitted with electromagnets (producing a transverse magnetic field), and electrodes (connected to load resistors). Charge carriers (free electrons and ions) incoming with the flow are then separated by the Lorentz force and an electric potential difference can be retrieved from pairs of connected electrodes. Shock tubes used as pulsed MHD generators were for example able to produce several megawatts of electricity in channels the size of a beverage can.\n\nIn addition to converters using electrodes, pure inductive magnetic converters have also been proposed by Lev Artsimovich in 1963, then Alan Frederic Haught and his team from United Aircraft Research Laboratories in 1970, and Ralph Moir in 1977.\n\nThe magnetic compression-expansion direct energy converter is analogous to the internal combustion engine. As the hot plasma expands against a magnetic field, in a manner similar to hot gases expanding against a piston, part of the energy of the internal plasma is inductively converted to an electromagnetic coil, as an EMF (voltage) in the conductor.\n\nThis scheme is best used with pulsed devices, because the converter then works like a \"magnetic four-stroke engine\":\n\nIn 1973, a team from Los Alamos and Argonne laboratories stated that the thermodynamic efficiency of the magnetic direct conversion cycle from alpha-particle energy to work is 62%.\n\nIn 1992, a Japan–U.S. joint-team proposed a novel direct energy conversion system for 14.7 MeV protons produced by D-He fusion reactions, whose energy is too high for electrostatic converters.\n\nThe conversion is based on a Traveling-Wave Direct Energy Converter (TWDEC). A gyrotron converter first guides fusion product ions as a beam into a 10-meter long microwave cavity filled with a 10-tesla magnetic field, where 155 MHz microwaves are generated and converted to a high voltage DC output through rectennas.\n\nThe Field-Reversed Configuration reactor ARTEMIS in this study was designed with an efficiency of 75%. The traveling-wave direct converter has a maximum projected efficiency of 90%.\n\nOriginal direct converters were designed to extract the energy carried by 100 to 800 keV ions produced by D-T fusion reactions. Those electrostatic converters are not suitable for higher energy product ions above 1 MeV generated by other fusion fuels like the D-He or the \"p\"-B aneutronic fusion reactions.\n\nA much shorter device than the Traveling-Wave Direct Energy Converter has been proposed in 1997 and patented by Tri Alpha Energy, Inc. as an Inverse Cyclotron Converter (ICC).\n\nThe ICC is able to decelerate the incoming ions based on experiments made in 1950 by Felix Bloch and Carson D. Jeffries, in order to extract their kinetic energy. The converter operates at 5 MHz and requires a magnetic field of only 0.6 tesla. The linear motion of fusion product ions is converted to circular motion by a magnetic cusp. Energy is collected from the charged particles as they spiral past quadrupole electrodes. More classical electrostatic collectors would also be used for particles with energy less than 1 MeV. The Inverse Cyclotron Converter has a maximum projected efficiency of 90%.\n\nA significant amount of the energy released by fusion reactions is composed of electromagnetic radiations, essentially X-rays due to Bremsstrahlung. Those X-rays can not be converted into electric power with the various electrostatic and magnetic direct energy converters listed above, and their energy is lost.\n\nWhereas more classical thermal conversion has been considered with the use of a radiation/boiler/energy exchanger where the X-ray energy is absorbed by a working fluid at temperatures of several thousand degrees, more recent research done by companies developing nuclear aneutronic fusion reactors, like Lawrenceville Plasma Physics (LPP) with the Dense Plasma Focus, and Tri Alpha Energy, Inc. with the Colliding Beam Fusion Reactor (CBFR), plan to harness the photoelectric and Auger effects to recover energy carried by X-rays and other high-energy photons. Those photoelectric converters are composed of X-ray absorber and electron collector sheets nested concentrically in an onion-like array. Indeed, since X-rays can go through far greater thickness of material than electrons can, many layers are needed to absorb most of the X-rays. LPP announces an overall efficiency of 81% for the photoelectric conversion scheme.\n\nIn the early 2000s, research was undertaken by Sandia National Laboratories, Los Alamos National Laboratory, The University of Florida, Texas A&M University and General Atomics to use direct conversion to extract energy from fission reactions, essentially, attempting to extract energy from the linear motion of charged particles coming off a fission reaction.\n"}
{"id": "400679", "url": "https://en.wikipedia.org/wiki?curid=400679", "title": "Ecological crisis", "text": "Ecological crisis\n\nAn ecological crisis occurs when changes to the environment of a species or population destabilizes its continued survival. A few possible causes include:\n\n\nThe evolutionary theory of punctuated equilibrium sees infrequent ecological crises as a potential driver of rapid evolution.\n\nClimate change is starting to have major impacts on ecosystems. With global temperature rising, there is a decrease in snow-fall, and sea levels are rising. Ecosystems will change or evolve to cope with the increase in temperature. Consequently, many species are being driven out of their habitats.\n\nPolar bears are being threatened. They need ice for hunting seals, their primary prey. However, the ice caps are melting, making their hunting periods shorter each year. As a result, the polar bears are not developing enough fat for the winter; therefore, they are not able to reproduce at a healthy rate.\n\nFresh water and wetland ecosystems are dealing with extreme effects of the increase of temperature. The climate change could be devastating to salmon and trout and to other aquatic life. The increase in temperature will disrupt the current life patterns of the salmon and trout. The cold-water fish will eventually leave their natural geographical range to live in cooler waters by migrating to higher elevations.\n\nWhile many species have been able to adapt to the new conditions by moving their range further towards the poles, other species are not as fortunate. The option to move is not available for polar bears and for some aquatic life.\n\nDue to increase in ecological crisis,vast numbers of species are being annihilated. Every year between 17,000 and 100,000 species vanish from the planet. The speed in which species are becoming extinct is much faster than in the past. The last mass extinction was caused by a meteor collision 66 million years ago.\n\nThe loss of new species in an ecosystem will eventually affect all living creatures. In the U.S. and Canada, there was a dramatic reduction of shark population along the U.S. east coast. Since then, there has been an increase in population of rays and skates, which in turn has decimated the population of shellfish. The loss of shellfish has reduced the water quality and the size of sea grass beds. Biodiversity is being lost at a fast rate. The more species there are in an ecosystem, the more resilient it is to evolution.\n\nSeven million square kilometers of tropical forest have vanished in the last 50 years. About two million square kilometers were used for crops, while the remaining five million square kilometers is poor quality land. Turning these unproductive lands back into native forest could capture an estimated five billion metric tons of carbon from the atmosphere every year for 10 to 20 or more years. Reforestation will have enormous benefits on biodiversity.\n\nIn the wilderness, the problem of animal overpopulation is solved by predators. Predators tend to look for signs of weakness in their prey, and therefore usually first eat the old or sick animals. This has the side effects of ensuring a strong stock among the survivors and controlling the population.\n\nIn the absence of predators, animal species are bound by the resources they can find in their environment, but this does not necessarily control overpopulation. In fact, an abundant supply of resources can produce a \"population boom\" that ends up with more individuals than the environment can support. In this case, starvation, thirst, and sometimes violent competition for scarce resources may effect a sharp reduction in population, and in a very short lapse, a population crash. Lemmings, as well as other less popular species of rodents, are known to have such cycles of rapid population growth and subsequent decrease.\n\nIn an ideal setting, when animal populations grow, so do the number of predators that feed on that particular animal. Animals that have birth defects or weak genes (such as the runt of the litter) also die off, unable to compete over food with stronger, healthier animals.\n\nIn reality, an animal that is not native to an environment may have advantages over the native ones, such being unsuitable for the local predators. If left uncontrolled, such an animal can quickly overpopulate and ultimately destroy its environment.\n\nExamples of animal overpopulation caused by introduction of a foreign species abound.\n\n\nmore examples=\nSome common examples of ecological crises are:\n\n\n\n"}
{"id": "10290", "url": "https://en.wikipedia.org/wiki?curid=10290", "title": "Emulsion", "text": "Emulsion\n\nAn emulsion is a mixture of two or more liquids that are normally immiscible (unmixable or unblendable). Emulsions are part of a more general class of two-phase systems of matter called colloids. Although the terms \"colloid\" and \"emulsion\" are sometimes used interchangeably, \"emulsion\" should be used when both phases, dispersed and continuous, are liquids. In an emulsion, one liquid (the dispersed phase) is dispersed in the other (the continuous phase). Examples of emulsions include vinaigrettes, homogenized milk, and some cutting fluids for metal working. Graphene and its modified forms are also a good example of recent unconventional surfactants helping in stabilizing emulsion systems.\n\nThe word \"emulsion\" comes from the Latin mulgeo, mulgere \"to milk\", as milk is an emulsion of fat and water, along with other components.\n\nTwo liquids can form different types of emulsions. As an example, oil and water can form, first, an oil-in-water emulsion, wherein the oil is the dispersed phase, and water is the dispersion medium. (Lipoproteins, used by all complex living organisms, are one example of this.) Second, they can form a water-in-oil emulsion, wherein water is the dispersed phase and oil is the external phase. Multiple emulsions are also possible, including a \"water-in-oil-in-water\" emulsion and an \"oil-in-water-in-oil\" emulsion.\n\nEmulsions, being liquids, do not exhibit a static internal structure. The droplets dispersed in the liquid matrix (called the “dispersion medium”) are usually assumed to be statistically distributed.\n\nThe term \"emulsion\" is also used to refer to the photo-sensitive side of photographic film. Such a photographic emulsion consists of silver halide colloidal particles dispersed in a gelatin matrix. Nuclear emulsions are similar to photographic emulsions, except that they are used in particle physics to detect high-energy elementary particles.\n\nEmulsions contain both a dispersed and a continuous phase, with the boundary between the phases called the \"interface\". Emulsions tend to have a cloudy appearance because the many phase interfaces scatter light as it passes through the emulsion. Emulsions appear white when all light is scattered equally. If the emulsion is dilute enough, higher-frequency (low-wavelength) light will be scattered more, and the emulsion will appear bluer – this is called the \"Tyndall effect\". If the emulsion is concentrated enough, the color will be distorted toward comparatively longer wavelengths, and will appear more yellow. This phenomenon is easily observable when comparing skimmed milk, which contains little fat, to cream, which contains a much higher concentration of milk fat. One example would be a mixture of water and oil.\n\nTwo special classes of emulsions – microemulsions and nanoemulsions, with droplet sizes below 100 nm – appear translucent. This property is due to the fact that light waves are scattered by the droplets only if their sizes exceed about one-quarter of the wavelength of the incident light. Since the visible spectrum of light is composed of wavelengths between 390 and 750 nanometers (nm), if the droplet sizes in the emulsion are below about 100 nm, the light can penetrate through the emulsion without being scattered. Due to their similarity in appearance, translucent nanoemulsions and microemulsions are frequently confused. Unlike translucent nanoemulsions, which require specialized equipment to be produced, microemulsions are spontaneously formed by “solubilizing” oil molecules with a mixture of surfactants, co-surfactants, and co-solvents. The required surfactant concentration in a microemulsion is, however, several times higher than that in a translucent nanoemulsion, and significantly exceeds the concentration of the dispersed phase. Because of many undesirable side-effects caused by surfactants, their presence is disadvantageous or prohibitive in many applications. In addition, the stability of a microemulsion is often easily compromised by dilution, by heating, or by changing pH levels.\n\nCommon emulsions are inherently unstable and, thus, do not tend to form spontaneously. Energy input – through shaking, stirring, homogenizing, or exposure to power ultrasound – is needed to form an emulsion. Over time, emulsions tend to revert to the stable state of the phases comprising the emulsion. An example of this is seen in the separation of the oil and vinegar components of vinaigrette, an unstable emulsion that will quickly separate unless shaken almost continuously. There are important exceptions to this rule – microemulsions are thermodynamically stable, while translucent nanoemulsions are kinetically stable.\n\nWhether an emulsion of oil and water turns into a \"water-in-oil\" emulsion or an \"oil-in-water\" emulsion depends on the volume fraction of both phases and the type of emulsifier (surfactant) (see \"Emulsifier\", below) present. In general, the Bancroft rule applies. Emulsifiers and emulsifying particles tend to promote dispersion of the phase in which they do not dissolve very well. For example, proteins dissolve better in water than in oil, and so tend to form oil-in-water emulsions (that is, they promote the dispersion of oil droplets throughout a continuous phase of water).\n\nThe geometric structure of an emulsion mixture of two lyophobic liquids with a large concentration of the secondary component is fractal: Emulsion particles unavoidably form dynamic inhomogeneous structures on small length scale. The geometry of these structures is fractal. The size of elementary irregularities is governed by a universal function which depends on the volume content of the components. The fractal dimension of these irregularities is 2.5.\n\nEmulsion stability refers to the ability of an emulsion to resist change in its properties over time. There are four types of instability in emulsions: flocculation, creaming/sedimentation, coalescence, and Ostwald ripening. Flocculation occurs when there is an attractive force between the droplets, so they form flocs, like bunches of grapes. Coalescence occurs when droplets bump into each other and combine to form a larger droplet, so the average droplet size increases over time. Emulsions can also undergo creaming, where the droplets rise to the top of the emulsion under the influence of buoyancy, or under the influence of the centripetal force induced when a centrifuge is used. Creaming is a common phenomenon in dairy and non-dairy beverages (i.e. milk, coffee milk, almond milk, soy milk) and usually does not change the droplet size. Sedimentation is the opposite phenomenon of creaming and normally observed in water-in-oil emulsions. Sedimentation happens when the dispersed phase is denser than the continuous phase and the gravitational forces pull the denser globules towards the bottom of the emulsion. Similar to creaming, sedimentation follows Stoke’s law.\n\nAn appropriate \"surface active agent\" (or \"surfactant\") can increase the kinetic stability of an emulsion so that the size of the droplets does not change significantly with time. It is then said to be stable. For example, oil-in-water emulsions containing mono- and diglycerides and milk protein as surfactant showed that stable oil droplet size over 28 days storage at 25°C. \n\nThe stability of emulsions can be characterized using techniques such as light scattering, focused beam reflectance measurement, centrifugation, and rheology. Each method has advantages and disadvantages.\n\nThe kinetic process of destabilization can be rather long – up to several months, or even years for some products. Often the formulator must accelerate this process in order to test products in a reasonable time during product design. Thermal methods are the most commonly used – these consist of increasing the emulsion temperature to accelerate destabilization (if below critical temperatures for phase inversion or chemical degradation). Temperature affects not only the viscosity but also the inter-facial tension in the case of non-ionic surfactants or, on a broader scope, interactions of forces inside the system. Storing an emulsion at high temperatures enables the simulation of realistic conditions for a product (e.g., a tube of sunscreen emulsion in a car in the summer heat), but also to accelerate destabilization processes up to 200 times.\n\nMechanical methods of acceleration, including vibration, centrifugation, and agitation, can also be used.\n\nThese methods are almost always empirical, without a sound scientific basis.\n\nAn emulsifier (also known as an \"emulgent\") is a substance that stabilizes an emulsion by increasing its kinetic stability. One class of emulsifiers is known as \"surface active agents\", or surfactants. Emulsifiers are compounds that typically have a polar or hydrophilic (i.e. water-soluble) part and a non-polar (i.e. hydrophobic or lipophilic) part. Because of this, emulsifiers tend to have more or less solubility either in water or in oil. Emulsifiers that are more soluble in water (and conversely, less soluble in oil) will generally form oil-in-water emulsions, while emulsifiers that are more soluble in oil will form water-in-oil emulsions.\n\nExamples of food emulsifiers are:\n\nDetergents are another class of surfactant, and will interact physically with both oil and water, thus stabilizing the interface between the oil and water droplets in suspension. This principle is exploited in soap, to remove grease for the purpose of cleaning. Many different emulsifiers are used in pharmacy to prepare emulsions such as creams and lotions. Common examples include emulsifying wax, polysorbate 20, and ceteareth 20. \n\nSometimes the inner phase itself can act as an emulsifier, and the result is a nanoemulsion, where the inner state disperses into \"nano-size\" droplets within the outer phase. A well-known example of this phenomenon, the \"Ouzo effect\", happens when water is poured into a strong alcoholic anise-based beverage, such as ouzo, pastis, absinthe, arak, or raki. The anisolic compounds, which are soluble in ethanol, then form nano-size droplets and emulsify within the water. The resulting color of the drink is opaque and milky white.\n\n\"See also: explained, in the Simple English Wikipedia\"\n\nA number of different chemical and physical processes and mechanisms can be involved in the process of emulsification:\n\n\nOil-in-water emulsions are common in food products:\n\nWater-in-oil emulsions are less common in food, but still exist:\n\nOther foods can be turned into products similar to emulsions, for example meat emulsion is a suspension of meat in liquid that is similar to true emulsions.\n\nIn pharmaceutics, hairstyling, personal hygiene, and cosmetics, emulsions are frequently used. These are usually oil and water emulsions but dispersed, and which is continuous depends in many cases on the pharmaceutical formulation. These emulsions may be called creams, ointments, liniments (balms), pastes, films, or liquids, depending mostly on their oil-to-water ratios, other additives, and their intended route of administration. The first 5 are topical dosage forms, and may be used on the surface of the skin, transdermally, ophthalmically, rectally, or vaginally. A highly liquid emulsion may also be used orally, or may be injected in some cases. Popular medications occurring in emulsion form include cod liver oil, Polysporin, cortisol cream, Canesten, and Fleet.\n\nMicroemulsions are used to deliver vaccines and kill microbes. Typical emulsions used in these techniques are nanoemulsions of soybean oil, with particles that are 400–600 nm in diameter. The process is not chemical, as with other types of antimicrobial treatments, but mechanical. The smaller the droplet the greater the surface tension and thus the greater the force required to merge with other lipids. The oil is emulsified with detergents using a high-shear mixer to stabilize the emulsion so, when they encounter the lipids in the cell membrane or envelope of bacteria or viruses, they force the lipids to merge with themselves. On a mass scale, in effect this disintegrates the membrane and kills the pathogen. The soybean oil emulsion does not harm normal human cells, or the cells of most other higher organisms, with the exceptions of sperm cells and blood cells, which are vulnerable to nanoemulsions due to the peculiarities of their membrane structures. For this reason, these nanoemulsions are not currently used intravenously (IV). The most effective application of this type of nanoemulsion is for the disinfection of surfaces. Some types of nanoemulsions have been shown to effectively destroy HIV-1 and tuberculosis pathogens on non-porous surfaces.\n\nEmulsifying agents are effective at extinguishing fires on small, thin-layer spills of flammable liquids (class B fires). Such agents encapsulate the fuel in a fuel-water emulsion, thereby trapping the flammable vapors in the water phase. This emulsion is achieved by applying an aqueous surfactant solution to the fuel through a high-pressure nozzle. Emulsifiers are not effective at extinguishing large fires involving bulk/deep liquid fuels, because the amount of emulsifier agent needed for extinguishment is a function of the volume of the fuel, whereas other agents such as aqueous film-forming foam need cover only the surface of the fuel to achieve vapor mitigation.\n\nEmulsions are used to manufacture polymer dispersions – polymer production in an emulsion 'phase' has a number of process advantages, including prevention of coagulation of product. Products produced by such polymerisations may be used as the emulsions – products including primary components for glues and paints. Synthetic latexes (rubbers) are also produced by this process.\n\n"}
{"id": "46672907", "url": "https://en.wikipedia.org/wiki?curid=46672907", "title": "Gustavo Orcés V. Natural History Museum", "text": "Gustavo Orcés V. Natural History Museum\n\nGustavo Orcés V. Natural History Museum () is a natural history museum in Quito, Ecuador. It was established in 2005.\n\nFrom the eighteenth century European naturalists came to Ecuador for scientific expeditions, during which they collected specimens of flora, fauna, rocks and fossils. In the early twentieth century, Franz Spillmann brought together a collection of fossils, which formed the \"Cabinet of Natural Sciences\" of the Central University. Later, in 1946, Robert Hoffstetter and Gustavo Orcés founded the Department of Biology at the National Polytechnic School, and made numerous paleontological expeditions, particularly in the Santa Elena peninsula. Hoffstetter's extensive work and organization formed the basis of the modern museum.\n"}
{"id": "39742421", "url": "https://en.wikipedia.org/wiki?curid=39742421", "title": "John Alden Loring", "text": "John Alden Loring\n\nJ. (John) Alden Loring (March 31, 1871 – May 8, 1947) was a mammalogist and field naturalist who served with the Bureau of Biological Survey, United States Department of Agriculture, the Bronx Zoological Park, the Smithsonian Institution and numerous expeditions collecting specimens in North America, Europe and Africa. A voluminous and careful traveling collector, Loring was recognized early in his career for 900 specimens collected, prepared and sent to the United States National Museum over a three-month period during an 1898 expedition through Scandinavia and northwestern Europe.\n\nLoring's work and professional relationships spanned several continents focusing on collecting and documenting species of mammals. He served on the Smithsonian-Roosevelt African Expedition (1909–1910) as the Smithsonian specialist designated to preserve small mammals collected during the year-long expedition. In 1916, he was sent as a joint envoy of the New York Zoological Park, Philadelphia Zoological Gardens and the National Zoological Park to South Africa to collect animals and if possible to arrange for a supply of future living specimens.\n\nJ. Alden Loring's personal papers are held by the Smithsonian Institution Archives. His collection of field books are part of the Smithsonian's Field Book Registry.\n\nLoring's rat was named for him (Heller, 1909).\n\n"}
{"id": "22589893", "url": "https://en.wikipedia.org/wiki?curid=22589893", "title": "Limiting similarity", "text": "Limiting similarity\n\nLimiting similarity (informally \"limsim\") is a concept in theoretical ecology and community ecology that proposes the existence of a maximum level of niche overlap between two given species that will allow continued coexistence.\n\nThis concept is a corollary of the competitive exclusion principle, which states that, controlling for all else, two species competing for exactly the same resources cannot stably coexist. It assumes normally-distributed resource utilization curves ordered linearly along a resource axis, and as such, it is often considered to be an oversimplified model of species interactions. Moreover, it has theoretical weakness, and it is poor at generating real-world predictions or falsifiable hypotheses. Thus, the concept has fallen somewhat out of favor except in didactic settings (where it is commonly referenced), and has largely been replaced by more complex and inclusive theories.\n\nIn 1932, Georgii Gause created the competitive exclusion principle based on experiments with cultures of yeast and paramecium. The principle maintains that two species with the same ecological niches cannot stably coexist. That is to say, when two species compete for identical resource access, one will be competitively superior and it will ultimately supplant the other. Over the next half century, limiting similarity slowly emerged as a natural outgrowth of this principle, aiming (but not necessarily succeeding) to be more quantitative and specific.\n\nNoted ecologist and evolutionary biologist David Lack said retrospectively that he had already begun to mull around with the ideas of limiting similarity as early as the 1940s, but it wasn't until the end of the 1950s that the theory began to be built up and articulated. G. Evelyn Hutchinson's famous \"Homage to Santa Rosalia\" was the next foundational paper in the history of the theory. Its subtitle famously asks, \"Why are there so many kinds of animals?\", and the address attempts to answer this question by suggesting theoretical bounds to speciation and niche overlap. For the purposes of understanding limiting similarity, the key portion of Hutchinson's address is the end where he presents the observation that a seemingly ubiquitous ratio (1.3:1) defines the upper bound of morphological character similarity between closely related species. While this so-called \"Hutchinson ratio\" and the idea of a universal limit have been overturned by later research, the address was still foundational to the theory of limiting similarity.\n\nMacArthur and Levins were the first to introduce the term 'limiting similarity' in their 1967 paper. They attempted to lay out a rigorous quantitative basis for the theory using probability theory and the Lotka–Volterra competition equations. In doing so, they provided the ultimate theoretical framework on which many subsequent studies were based.\n\nAs proposed by MacArthur and Levins in 1967, the theory of limiting similarity is rooted in the Lotka–Volterra competition model. This model describes two or more populations with logistic dynamics, adding in an additional term to account for their biological interactions. Thus for two populations, \"x\" and \"x\":\n\nwhere\n\n\nMacArthur and Levins examine this system applied to three populations, also visualized as resource utilization curves, depicted below. In this model, at some upper limit of competition \"α\", between two species \"x\" and \"x\", the survival of a third species \"x\" between the other two is not possible. This phenomenon is termed limiting similarity. Evolutionary, if two species are more similar than some limit \"L\", a third species will converge towards the nearer of the two competitors. If the two species are less similar than some limit \"L\", a third species will evolve an intermediate phenotype.\n[embedded graph: U v R. x1, x2, x3 curves.]\n\nFor each resource R, U represents the probability of utilization per unit time by an individual. At some level of overlap between species \"x\" and \"x\", the survival of a third species \"x\" is no longer possible.\nMay extended this theory when considering species with different carrying capacities, concluding that coexistence was unlikely if the distance between the modes of competing resource utilization curves \"d\" was less than the standard deviation of the curves \"w\".\n\nIt is of note that the theory of limiting similarity does not easily generate falsifiable predictions about natural phenomenon. However, many studies have tried to test the theory by making the highly suspect assumption that character displacement can be used as a close proxy for niche incongruence. One recent paleoecological study, for example, used fossil proxies of gastropod body size to determine levels of character displacement over 42,500 years during the Quaternary. They found little evidence of character displacement, and they concluded that \"limiting similarity, as seen in both ecological character displacement and community-wide character displacement, is a transient ecological phenomenon rather than a long-term evolutionary process\". Other theoretical and empirical studies tend to find results that similarly play down the strength and role of limiting similarity in ecology and evolution. For example, Abrams (who is prolific on the subject of limiting similarity) and Rueffler find in 2009 that \"there is no absolute limit to similarity; there is always some range of mortality rates of one species allowing coexistence, given a fixed mortality of the other species\".\n\nWhat a lot of studies examining limiting similarity find are the weaknesses in the original theory that are addressed below.\n\nThe key weakness of the theory of limiting similarity is that it is highly system specific and thus difficult to test in practice. In actual environments, one resource axis is inadequate and a specific analysis must be done for each given pair of species. In practice it is necessary to take into account:\nWhile these complications don't invalidate the concept, they render limiting similarity exceedingly difficult to test in practice and useful for little more than didacticism. \n\nFurthermore, Hubbell and Foster point out that extinction via competition can take an extremely long time and the importance of limiting similarity in extinction may even be superseded by speciation. Also, from a theoretical standpoint, small changes in carrying capacities can allow for nearly completely overlapping resource utilization curves and in practice carrying capacity can be difficult to determine. Many studies that attempt to explore limiting similarity (including Huntley et al. 2007) resort to examining character displacement as a proxy for niche overlap, which is suspect at best. While a useful-if simple-model, limiting similarity is nearly untestable in reality.\n\n"}
{"id": "18993825", "url": "https://en.wikipedia.org/wiki?curid=18993825", "title": "Liquid", "text": "Liquid\n\nA liquid is a nearly incompressible fluid that conforms to the shape of its container but retains a (nearly) constant volume independent of pressure. As such, it is one of the four fundamental states of matter (the others being solid, gas, and plasma), and is the only state with a definite volume but no fixed shape. A liquid is made up of tiny vibrating particles of matter, such as atoms, held together by intermolecular bonds. Water is, by far, the most common liquid on Earth. Like a gas, a liquid is able to flow and take the shape of a container. Most liquids resist compression, although others can be compressed. Unlike a gas, a liquid does not disperse to fill every space of a container, and maintains a fairly constant density. A distinctive property of the liquid state is surface tension, leading to wetting phenomena.\n\nThe density of a liquid is usually close to that of a solid, and much higher than in a gas. Therefore, liquid and solid are both termed condensed matter. On the other hand, as liquids and gases share the ability to flow, they are both called fluids. Although liquid water is abundant on Earth, this state of matter is actually the least common in the known universe, because liquids require a relatively narrow temperature/pressure range to exist. Most known matter in the universe is in gaseous form (with traces of detectable solid matter) as interstellar clouds or in plasma form within stars.\n\nLiquid is one of the four primary states of matter, with the others being solid, gas and plasma. A liquid is a fluid. Unlike a solid, the molecules in a liquid have a much greater freedom to move. The forces that bind the molecules together in a solid are only temporary in a liquid, allowing a liquid to flow while a solid remains rigid.\n\nA liquid, like a gas, displays the properties of a fluid. A liquid can flow, assume the shape of a container, and, if placed in a sealed container, will distribute applied pressure evenly to every surface in the container. If liquid is placed in a bag, it can be squeezed into any shape. Unlike a gas, a liquid is nearly incompressible, meaning that it occupies nearly a constant volume over a wide range of pressures; it does not generally expand to fill available space in a container but forms its own surface, and it may not always mix readily with another liquid. These properties make a liquid suitable for applications such as hydraulics.\n\nLiquid particles are bound firmly but not rigidly. They are able to move around one another freely, resulting in a limited degree of particle mobility. As the temperature increases, the increased vibrations of the molecules causes distances between the molecules to increase. When a liquid reaches its boiling point, the cohesive forces that bind the molecules closely together break, and the liquid changes to its gaseous state (unless superheating occurs). If the temperature is decreased, the distances between the molecules become smaller. When the liquid reaches its freezing point the molecules will usually lock into a very specific order, called crystallizing, and the bonds between them become more rigid, changing the liquid into its solid state (unless supercooling occurs).\n\nOnly two elements are liquid at standard conditions for temperature and pressure: mercury and bromine. Four more elements have melting points slightly above room temperature: francium, caesium, gallium and rubidium. Metal alloys that are liquid at room temperature include NaK, a sodium-potassium metal alloy, galinstan, a fusible alloy liquid, and some amalgams (alloys involving mercury).\n\nPure substances that are liquid under normal conditions include water, ethanol and many other organic solvents. Liquid water is of vital importance in chemistry and biology; it is believed to be a necessity for the existence of life.\n\nInorganic liquids include water, magma, inorganic nonaqueous solvents and many acids.\n\nImportant everyday liquids include aqueous solutions like household bleach, other mixtures of different substances such as mineral oil and gasoline, emulsions like vinaigrette or mayonnaise, suspensions like blood, and colloids like paint and milk.\n\nMany gases can be liquefied by cooling, producing liquids such as liquid oxygen, liquid nitrogen, liquid hydrogen and liquid helium. Not all gases can be liquified at atmospheric pressure, for example carbon dioxide can only be liquified at pressures above 5.1 atm.\n\nSome materials cannot be classified within the classical three states of matter; they possess solid-like and liquid-like properties. Examples include liquid crystals, used in LCD displays, and biological membranes.\n\nLiquids have a variety of uses, as lubricants, solvents, and coolants. In hydraulic systems, liquid is used to transmit power.\n\nIn tribology, liquids are studied for their properties as lubricants. Lubricants such as oil are chosen for viscosity and flow characteristics that are suitable throughout the operating temperature range of the component. Oils are often used in engines, gear boxes, metalworking, and hydraulic systems for their good lubrication properties.\n\nMany liquids are used as solvents, to dissolve other liquids or solids. Solutions are found in a wide variety of applications, including paints, sealants, and adhesives. Naphtha and acetone are used frequently in industry to clean oil, grease, and tar from parts and machinery. Body fluids are water based solutions.\n\nSurfactants are commonly found in soaps and detergents. Solvents like alcohol are often used as antimicrobials. They are found in cosmetics, inks, and liquid dye lasers. They are used in the food industry, in processes such as the extraction of vegetable oil. \nLiquids tend to have better thermal conductivity than gases, and the ability to flow makes a liquid suitable for removing excess heat from mechanical components. The heat can be removed by channeling the liquid through a heat exchanger, such as a radiator, or the heat can be removed with the liquid during evaporation. Water or glycol coolants are used to keep engines from overheating. The coolants used in nuclear reactors include water or liquid metals, such as sodium or bismuth. Liquid propellant films are used to cool the thrust chambers of rockets. In machining, water and oils are used to remove the excess heat generated, which can quickly ruin both the work piece and the tooling. During perspiration, sweat removes heat from the human body by evaporating. In the heating, ventilation, and air-conditioning industry (HVAC), liquids such as water are used to transfer heat from one area to another.\n\nLiquid is the primary component of hydraulic systems, which take advantage of Pascal's law to provide fluid power. Devices such as pumps and waterwheels have been used to change liquid motion into mechanical work since ancient times. Oils are forced through hydraulic pumps, which transmit this force to hydraulic cylinders. Hydraulics can be found in many applications, such as automotive brakes and transmissions, heavy equipment, and airplane control systems. Various hydraulic presses are used extensively in repair and manufacturing, for lifting, pressing, clamping and forming.\n\nLiquids are sometimes used in measuring devices. A thermometer often uses the thermal expansion of liquids, such as mercury, combined with their ability to flow to indicate temperature. A manometer uses the weight of the liquid to indicate air pressure.\n\nQuantities of liquids are measured in units of volume. These include the SI unit cubic metre (m) and its divisions, in particular the cubic decimeter, more commonly called the litre (1 dm = 1 L = 0.001 m), and the cubic centimetre, also called millilitre (1 cm = 1 mL = 0.001 L = 10 m).\n\nThe volume of a quantity of liquid is fixed by its temperature and pressure. Liquids generally expand when heated, and contract when cooled. Water between 0 °C and 4 °C is a notable exception.\nLiquids have little compressibility. Water, for example, will compress by only 46.4 parts per million for every unit increase in atmospheric pressure (bar). At around 4000 bar (58,000 psi) of pressure, at room temperature, water only experiences an 11% decrease in volume. In the study of fluid dynamics, liquids are often treated as incompressible, especially when studying incompressible flow. This incompressible nature makes a liquid suitable for transmitting hydraulic power, because very little of the energy is lost in the form of compression. However, the very slight compressibility does lead to other phenomena. The banging of pipes, called water hammer, occurs when a valve is suddenly closed, creating a huge pressure-spike at the valve that travels backward through the system at just under the speed of sound. Another phenomenon caused by liquid's incompressibility is cavitation. Because liquids have little elasticity they can literally be pulled apart in areas of high turbulence or a dramatic change in direction, such as the trailing edge of a boat propeller or a sharp corner in a pipe. A liquid in an area of low pressure (vacuum) vaporizes and forms bubbles, which then collapse as they enter high pressure areas. This causes liquid to fill the cavities left by the bubbles with tremendous localized force, eroding any adjacent solid surface.\n\nIn a gravitational field, liquids exert pressure on the sides of a container as well as on anything within the liquid itself. This pressure is transmitted in all directions and increases with depth. If a liquid is at rest in a uniform gravitational field, the pressure, \"p\", at any depth, \"z\", is given by\nwhere:\nNote that this formula assumes that the pressure \"at\" the free surface is zero, and that surface tension effects may be neglected.\n\nObjects immersed in liquids are subject to the phenomenon of buoyancy. (Buoyancy is also observed in other fluids, but is especially strong in liquids due to their high density.)\n\nUnless the volume of a liquid exactly matches the volume of its container, one or more surfaces are observed. The surface of a liquid behaves like an elastic membrane in which surface tension appears, allowing the formation of drops and bubbles. Surface waves, capillary action, wetting, and ripples are other consequences of surface tension. In a confined liquid, defined by geometric constraints on a nanoscopic scale, most molecules sense some surface effects, which can result in physical properties grossly deviating from those of the bulk liquid.\n\nA free surface is the surface of a fluid that is subject to both zero perpendicular normal stress and parallel shear stress, such as the boundary between, e.g., liquid water and the air in the Earth's atmosphere.\n\nThe liquid level (as in, e.g., water level) is the height associated with the liquid free surface, especially when it's the top-most surface. It may be measured with a level sensor.\n\nViscosity measures the resistance of a liquid which is being deformed by either shear stress or extensional stress. In other words, viscosity is the resistance of a liquid to flow.\n\nWhen a liquid is supercooled towards the glass transition, the viscosity increases dramatically. The liquid then becomes a viscoelastic medium that shows both the elasticity of a solid and the fluidity of a liquid, depending on the time scale of observation or on the frequency of perturbation.\n\nThe speed of sound in a fluid is given by\nformula_4 where \"K\" is the bulk modulus of the fluid, and \"ρ\" the density. To give a typical value, in fresh water \"c\"=1497 m/s at 25 °C.\n\nAt a temperature below the boiling point, any matter in liquid form will evaporate until the condensation of gas above reach an equilibrium. At this point the gas will condense at the same rate as the liquid evaporates. Thus, a liquid cannot exist permanently if the evaporated liquid is continually removed. A liquid at its boiling point will evaporate more quickly than the gas can condense at the current pressure. A liquid at or above its boiling point will normally boil, though superheating can prevent this in certain circumstances.\n\nAt a temperature below the freezing point, a liquid will tend to crystallize, changing to its solid form. Unlike the transition to gas, there is no equilibrium at this transition under constant pressure, so unless supercooling occurs, the liquid will eventually completely crystallize. Note that this is only true under constant pressure, so e.g. water and ice in a closed, strong container might reach an equilibrium where both phases coexist. For the opposite transition from solid to liquid, see melting.\n\nThe phase diagram explains why liquids do not exist in space or any other vacuum. Since the pressure is zero (except on surfaces or interiors of planets and moons) water and other liquids exposed to space will either immediately boil or freeze depending on the temperature. In regions of space near the earth, water will freeze if the sun is not shining directly on it and vapourize (sublime) as soon as it is in sunlight. If water exists as ice on the moon, it can only exist in shadowed holes where the sun never shines and where the surrounding rock doesn't heat it up too much. At some point near the orbit of Saturn, the light from the sun is too faint to sublime ice to water vapour. This is evident from the longevity of the ice that composes Saturn's rings.\n\nLiquids can display immiscibility. The most familiar mixture of two immiscible liquids in everyday life is the vegetable oil and water in Italian salad dressing. A familiar set of miscible liquids is water and alcohol. Liquid components in a mixture can often be separated from one another via fractional distillation.\n\nIn a liquid, atoms do not form a crystalline lattice, nor do they show any other form of long-range order. This is evidenced by the absence of Bragg peaks in X-ray and neutron diffraction. Under normal conditions, the diffraction pattern has circular symmetry, expressing the isotropy of the liquid. In radial direction, the diffraction intensity smoothly oscillates. This is usually described by the static structure factor \"S(q)\", with wavenumber \"q\"=(4π/λ)sinθ given by the wavelength λ of the probe (photon or neutron) and the Bragg angle θ. The oscillations of \"S(q)\" express the \"near order\" of the liquid, i.e. the correlations between an atom and a few shells of nearest, second nearest, ... neighbors.\n\nA more intuitive description of these correlations is given by the radial distribution function \"g(r)\", which is basically the Fourier transform of \"S(q)\". It represents a spatial average of a temporal snapshot of pair correlations in the liquid.\n\nThe above expression for the sound velocity formula_4 contains the bulk modulus \"K\". If \"K\" is frequency independent then the liquid behaves as a linear medium, so that sound propagates without dissipation and without mode coupling. In reality, any liquid shows some dispersion: with increasing frequency, \"K\" crosses over from the low-frequency, liquid-like limit formula_6 to the high-frequency, solid-like limit formula_7. In normal liquids, most of this cross over takes place at frequencies between GHz and THz, sometimes called hypersound.\n\nAt sub-GHz frequencies, a normal liquid cannot sustain shear waves: the zero-frequency limit of the shear modulus is formula_8. This is sometimes seen as the defining property of a liquid. \nHowever, just as the bulk modulus \"K\", the shear modulus \"G\" is frequency dependent,\nand at hypersound frequencies it shows a similar cross over from the liquid-like limit formula_9 to a solid-like, non-zero limit formula_10.\n\nAccording to the Kramers-Kronig relation, the dispersion in the sound velocity (given by the real part of \"K\" or \"G\") goes along with a maximum in the sound attenuation (dissipation, given by the imaginary part of \"K\" or \"G\"). According to linear response theory, the Fourier transform of \"K\" or \"G\" describes how the system returns to equilibrium after an external perturbation; for this reason, the dispersion step in the GHz..THz region is also called structural relaxation. According to the fluctuation-dissipation theorem, relaxation \"towards\" equilibrium is intimately connected to fluctuations \"in\" equilibrium. The density fluctuations associated with sound waves can be experimentally observed by Brillouin scattering.\n\nOn supercooling a liquid towards the glass transition, the crossover from liquid-like to solid-like response moves from GHz to MHz, kHz, Hz, ...; equivalently, the characteristic time of structural relaxation increases from ns to μs, ms, s, ... This is the microscopic explanation for the above-mentioned viscoelastic behaviour of glass-forming liquids.\n\nThe mechanisms of atomic/molecular diffusion (or particle displacement) in solids are closely related to the mechanisms of viscous flow and solidification in liquid materials. Descriptions of viscosity in terms of molecular \"free space\" within the liquid\nwere modified as needed in order to account for liquids whose molecules are known to be \"associated\" in the liquid state at ordinary temperatures. When various molecules combine together to form an associated molecule, they enclose within a semi-rigid system a certain amount of space which before was available as free space for mobile molecules. Thus, increase in viscosity upon cooling due to the tendency of most substances to become \"associated\" on cooling.\n\nSimilar arguments could be used to describe the effects of pressure on viscosity, where it may be assumed that the viscosity is chiefly a function of the volume for liquids with a finite compressibility. An increasing viscosity with rise of pressure is therefore expected. In addition, if the volume is expanded by heat but reduced again by pressure, the viscosity remains the same.\n\nThe local tendency to orientation of molecules in small groups lends the liquid (as referred to previously) a certain degree of association. This association results in a considerable \"internal pressure\" within a liquid, which is due almost entirely to those molecules which, on account of their temporary low velocities (following the Maxwell distribution) have coalesced with other molecules. The internal pressure between several such molecules might correspond to that between a group of molecules in the solid form.\n"}
{"id": "2448150", "url": "https://en.wikipedia.org/wiki?curid=2448150", "title": "List of New Hampshire state forests", "text": "List of New Hampshire state forests\n\nA list of all State Forests in the state of New Hampshire in the United States.\n\nThe former Gay State Forest was transferred to the Society for the Protection of New Hampshire Forests in 2009.\n\n\n"}
{"id": "2442430", "url": "https://en.wikipedia.org/wiki?curid=2442430", "title": "List of Vermont state forests", "text": "List of Vermont state forests\n\nThis is a list of forests and natural areas that are protected and managed by the state of Vermont.\n\n"}
{"id": "46550291", "url": "https://en.wikipedia.org/wiki?curid=46550291", "title": "List of countries by aircraft and spacecraft exports", "text": "List of countries by aircraft and spacecraft exports\n\nThe following is a list of countries by exports of aircraft, including helicopters, and spacecraft (Harmonized System code 8802). Data is for 2016, in billions of United States dollars, as reported by The Observatory of Economic Complexity. Currently the top twenty countries are listed.\n\nNote: Export realized under secret code are not counted. The use of secret code is particularly frequent for military equipment, including military aircraft. Example: Russia exported 14 fighter aircraft Su-30 in the year 2016, as well as other types of military aircrft. Depending on sources, the Su-30 are sold at more or less 50 million USD per unit.\n\nLatest data on\n"}
{"id": "24695021", "url": "https://en.wikipedia.org/wiki?curid=24695021", "title": "List of invasive species in North America", "text": "List of invasive species in North America\n\nThis is a list of invasive species in North America. A species is regarded as invasive if it has been introduced by human action to a location, area, or region where it did not previously occur naturally (i.e., is not a native species), becomes capable of establishing a breeding population in the new location without further intervention by humans, and becomes a pest in the new location, directly threatening human industry, such as agriculture, or the local biodiversity.\n\nThe term \"invasive species\" refers to a subset of those species defined as introduced species. If a species has been introduced, but remains local, and is not problematic for human industry or the local biodiversity, then it is not considered invasive, and does not belong on this list.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "7197674", "url": "https://en.wikipedia.org/wiki?curid=7197674", "title": "List of longest rivers of Ukraine", "text": "List of longest rivers of Ukraine\n\nThis is a list of the major rivers that flow through Ukraine.\n\nUkraine has around 23,000 rivers. Most of rivers of Ukraine drain into Black Sea and Azov Sea belonging to the bigger Mediterranean basin. Those rivers mostly flow in southern direction, except for some Pripyat tributaries in Volhynia and Dniester tributaries in Prykarpattia. A small portion of rivers is part of Western Bug drainage basin located in Western Ukraine near the border with Poland. Those rivers usually have northern direction and are part of bigger Baltic basin. The most notable rivers of Ukraine include: Dnieper, Dniester, Southern Buh, and Siversky Donets. The longest river is Dnieper, the longest tributary is the Dnieper's tributary Desna. Two of the Danube's tributaries in Ukraine Prut and Tysa are longer than the main river within Ukraine.\n\nThe territory of Ukraine is conditionally divided into 9 hydrographic zone according to major river basins including basins of rivers Wisla (Western Bug and San), Danube, Dniester, Southern Bug, Dnieper, Don, rivers of Black Sea littoral, Sea of Azov littoral, and separately rivers of Crimea. The biggest river basin by area is Dnieper which is subdivided into Prypiat basin, Desna basin, basin of Middle Dnieper, basin of Lower Dnieper. Beside Dnieper, basin of the Danube zoning is also subdivided into basin of Tysa, basin of Prut and Siret, and basin of the Lower Danube.\n\nListed are rivers over long. Length is in kilometers.\n\nThe most water in Ukraine is carried by Dnieper. Its annual drainage volume is . The only other river that has higher drainage volume is the Danube which running through the Central Europe within Ukraine stretches only for . The average annual drainage of the Danube is around .\n\nThe deepest river of Ukraine is Dniester. In its mid stream between Pyzhniv village and Mohyliv-Podilsky through the Dniester canyon (Podillia Upland) the river narrows to in width and deepens up to .\n\nThe biggest water amount among distributaries is carried by the Chilia branch.\n\nThe biggest river delta in Ukraine belongs to Dnieper and has area of , while the Danube Delta within Ukraine is only .\n\n\n\n"}
{"id": "365106", "url": "https://en.wikipedia.org/wiki?curid=365106", "title": "List of national parks of Argentina", "text": "List of national parks of Argentina\n\nThe National Parks of Argentina make up a network of 36 national parks in Argentina. The parks cover a very varied set of terrains and biotopes, from Baritú National Park on the northern border with Bolivia to Tierra del Fuego National Park in the far south of the continent. The Administración de Parques Nacionales (National Parks Administration) is the agency that preserves and manages these national parks along with Natural monuments and National Reserves within the country.\n\nThe headquarters of the National Service are in downtown Buenos Aires, on Santa Fe Avenue. A library and information centre are open to the public. The administration also covers the national monuments, such as the Jaramillo Petrified Forest, and natural and educational reserves.\n\nThe creation of the National Parks dates back to the 1903 donation of of land in the Lake District in the Andes foothills by Francisco Moreno. This formed the nucleus of a larger protected area in Patagonia around San Carlos de Bariloche. In 1934, a law was passed creating the National Parks system, formalising the protected area as the Nahuel Huapi National Park and creating the Iguazú National Park. Thus, Argentina was the third country in the Americas, after United States and Canada to establish a national parks system. The National Park Police Force was born, enforcing the new laws preventing tree-felling and hunting. Their early task was largely to establish national sovereignty over these disputed areas and to protect borders. Five further national parks were declared in 1937 in Patagonia and the service planned new towns and facilities to promote tourism and education. Six more were declared by 1970.\n\nIn 1970 a new law established new categories of protection: National Parks, National Monuments, Educational Reserves, and Natural Reserves. Three national parks were designated in the 1970s. In 1980, another new law affirmed the status of national parks - this law is still in place. The 1980s saw the service reaching out to local communities and local government to help in the running and development of the national parks. Ten more national parks were created with local co-operation, sometimes at local instigation. In 2000, Mburucuyá and Copo National Parks were declared, and El Leoncito natural reserve was upgraded to a national park. Currently, there are 41 protected areas in Argentina, which cover an area of or about 1.5% of the total land area in Argentina.\n\n\n"}
{"id": "3357133", "url": "https://en.wikipedia.org/wiki?curid=3357133", "title": "List of rice varieties", "text": "List of rice varieties\n\nThis is a list of rice varieties, also known as rice cultivars. There are several grains called rice, which have been cultivated for thousands of years. Asian rice (\"Oryza sativa)\" is most widely known and most widely grown, with two major subspecies and over 90,000 varieties. Also included in this list are varieties of African rice (\"Oryza glaberrima\") and wild rice (genus \"Zizania\"). Rice may vary in genetics, grain length, color, thickness, stickiness, aroma, growing method, and other characteristics, leading to a vast preponderance of varieties. For instance, over nine major varieties of rice exist for the purpose of making sake alone.\n\nRice can be divided into different categories on the basis of each of its major characteristics. The two subspecies of Asian rice, indica and japonica, can generally be distinguished by length and stickiness. Indica rice is long-grained and unsticky, while japonica is short-grained and glutinous.\n\nOn the basis of processing type, rice can be divided into the two broad categories of brown and white. Brown rice is whole grain, with only the inedible hull of the seed removed, while white (milled) rice additionally has the bran and germ removed through the process of milling. Milled rice may not necessarily actually be white in color; there are also purple, black, and red variants of rice, which can be eaten whole grain or milled.\n\nThe varieties listed in this article may vary in any number of these characteristics, and most can be eaten whole grain or milled (brown or white), although there are often strong cultural preferences for one or the other, depending on variety and region.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBasmati and premium non-basmati rice grows in Punjab and Haryana region of India, such as: \n\n\nAndhra Pradesh and Telangana are home to hundreds of rice varieties, such as: \n\n\n\n\n\n\n\n\nThere are possibly up to 82,700 varieties of rice extant in India, and of those more than 5000 were found in West Bengal. However, only 150 of them are commonly grown. Many are grown organically to compete with more modern cultivars. The Agricultural Training Centre of West Bengal exists to conserve and promote the use of folk rice varieties, including many listed below.\n\nIn Indonesia, there are at least 45 varieties of rice for wet-field production (sawah) and 150 varieties of rice for dry-field production.\n\nMany varieties of rice are cultivated in Iran. A few of them are listed below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "52750108", "url": "https://en.wikipedia.org/wiki?curid=52750108", "title": "Michel Gordillo", "text": "Michel Gordillo\n\nMichel Gordillo is a world record aviator, and the first person to circumpolar navigate the world in an experimental aircraft with a mass of less than 1750 kilograms. He was commander of Iberia Airlines piloting Airbus A319, A320, and A321. In total he has more than 15,000 hours of flight. Gordillo is also a glider pilot, hold the title C silver, and is a member of the Spanish national glider team. He is fluent in Spanish, French and English.\n\nMichel Gordillo, born on 2 May 1955 as Miguel Ángel Gordillo Urquia in French Cameroun at Douala near the volcano Mount Cameroon, and lived there until the age of seven. The family moved to the capital, Yaoundé, during the independence movement of Cameroon. In 1967, the family moved to Cannes, France and then to Madrid, Spain. After he completed the Baccalaureate, he entered the Spanish Air Force Academy selection group after passing a public examination. At present, Gordillo is retired and lives with his family in Spain.\n\nHe gained admission to the Spanish Air Force to train to be a future pilot. At San Javier in Murcia, Spain, he went through military training and after two years started flight training. The first aircraft was a Beechcraft T-34 Mentor, and he flew solo and also received aerobatics training. He flew Beechcraft Bonanza planes and received navigation training (both visual and radio navigation). During his last year at the Academia General del Aire (AGA), he received advanced flight training with T-6 Texan aircraft. Gordillo completed flight training that included formation flying, aerobatics, combat flying and Instrument Flight Rules (IFR) flying. He received the fighter pilot aptitude, but took slot 20 of 24 and the fighter school only took the first 18 slots. Next came IFR multiengine training for one year at Salamanca where he flew the Spanish designed CASA C-212 Aviocar airplane. Gordillo was then assigned to the P3 Orion anti-submarine warfare aircraft. (Note: in Spain the P-3 Orion is assigned to the Air Force, not the Navy.) He became a fighter pilot, but was assigned as a ship and submarine fighter pilot. He flew the P-3 for seven years and became a crew leader. This is where he learned the challenges of locating someone in the water, as he also served in search and rescue aircraft and maritime patrol aircraft capacities. This experience would later serve him for his long distance oceanic flights. While assigned to the P-3 squadron, Gordillo received orders for one year to attend undergraduate and advanced navigator training with the U.S. Air Force at Mather Air Force Base in Sacramento, California. After his P-3 Orion assignment, he was transferred to the 45th Air Force Group. The Group is responsible for transporting VIP people, like the King of Spain and his family, government ministers and dignitaries, and other government officials. Gordillo flew the Falcon 20 aircraft. Gordillo made the decision to retire rather than advance to major in the Air Force and receive an assignment as a desk jockey rather than a flight jockey.\n\nIn 1987, Gordillo went to work for Iberia Airlines and flew the DC-9, as copilot. Then he moved up to the MD-87 aircraft for Iberia and was trained at Los Angeles. Next came long range aircraft, the four engine Airbus A340 for flights from Miami. In 1998, he was promoted to captain, and assigned to fly the twin engine Airbus A320, and later, Airbus A319 and Airbus A321. Gordillo retired at the age of 58.\n\nOn 5 January 2006, acting as captain of Flight 161 of Iberia Airlines, Gordillo refused to take off for safety and security reasons due to faulty fire detector that had not been repaired. This resulted in his termination. During the judicial proceedings that ensued, a hearing was held on 22 June 2006 and his dismissal was declared inadmissible. However, by 15 November 2006, Gordillo was not reinstated to Iberia Airlines.\n\nAs a boy, Gordillo was given a control line Air Cobra airplane, with a .049 engine. Unfortunately, he was never able to start the engine. Gordillo bought his first magazine: \"Le modèle reduit d´avion\", a model aircraft magazine. The magazine changed his life and set the course for his calling in life. The magazine remains in his possession. \nGordillo developed an interest in model airplanes, first with U-Line models, with .15 engines (combat and aerobatics). He entered the Aeromodelling School. He moved on to free flight models. First gliders and then powered models called FAI F1C (International Aeronautics Federation: F1 stands for free flight and C fuel powered engines) He also flies F1B, rubber powered models.\nAfter he completed the Baccalaureate, he entered the Spanish Air Force Academy selection group after passing a public examination. Gordillo spent one year at Granada waiting for a chance to become an Air Force pilot.\n\nWhile in the Spanish Air Force, Gordillo was able to attend glider school at Jerez.\nGordillo received his initial glider instruction at Monflorite in Huesca, Spain. He flew the Blaník type glider for three or four flights and then upgraded to the Pirate type glider. For flight number 8 he flew for 5.5 hours and qualified for Silver C. On flight 9 he achieved Silver C distance and altitude and finally flew for flight number 10, to actually receive the C glider rating together with the Silver C glider rating, something that no one had ever achieved in Spain.\n\nEver since Gordillo was in the Air Force Academy, he wanted to build his own airplane. Not until he became a commercial airline pilot was Gordillo able to build his first real aircraft, the Denny Kitfox IV. The Kitfox was powered by an 80-horsepower (60 kW) Rotax 912. A good aircraft to fly slow and low. The next plane he built was a Dyn'Aéro MCR01 aircraft with a Rotax 912S engine. The most recent aircraft built was a Van's Aircraft RV-8.\n\nThe idea of \"Madosh\" (a portmanteau of the Madrid to Oshkosh flight) came about in 1996, since Gordillo wanted to fly into Oshkosh, Wisconsin for EAA AirVenture Oshkosh. Oshkosh is known as the Mecca of experimental aircraft builders. His plan was to take a different approach and fly eastward, into the sun, instead of flying west, from Spain. The Spanish Air Force and Iberia Airlines provided him support and sponsorship, and the dream became reality in 1998.\n\nHis next project was to build a Dyn'Aéro MCR01 aircraft with a Rotax 912S engine. The flight plan this time called for another around the world flight and was called \"Into the Sunrise.\" Iberia Airlines and the Spanish Air Force provided support and sponsorship for the flight in 2001. Gordillo flew east from Madrid and arrived in Oshkosh safely.\n\nAfter taking some time off from building experimental aircraft, in 2003 Gordillo began making plans to build and fly the plane of his dreams, a Van's Aircraft RV-8. The project had three main goals. One goal was to collect atmospheric pollution data over remote area such as the desert areas, oceans, and the Arctic and Antarctic poles where no other small aircraft have previously flown. Another goal was to establish a new record maintained by the Fédération Aéronautique Internationale (FAI), the C-1b record category for flight distance by light aircraft. The final goal was for Gordillo to complete a third circumnavigation in an experimental aircraft, the RV-8. Previous flights were completed in 1998 in a Denney Kitfox Model IV, and then in 2001 in an Dyn'Aéro MCR01.\nGordillo utilized an aethalometer to collect samples. The data collected during the flight will be analyzed in a project promoted by the Interuniversity Research Institute of the Earth System in Andalusia and the University of Granada. The object of the study will be to increase the body of evidence and knowledge regarding the role of the solid pollutants in global climate change. Previously Matevž Lenarčič utilized an aethalometer to collect samples during his circumnavigation flight.\n\nFor the circumpolar navigational trek, Gordillo flew a Van's Aircraft RV-8, an experimental aircraft. The RV-8 is a high performance craft with a maximum airspeed of 356 km/h (221 mph). The aircraft is powered by a Superior XP IO-360 engine with 180-horsepower.\n\nMadrid to North Pole to Madrid... (Miles noted are approximate flight distances in kilometers (km)).\nJerez, Spain to Dakar, Senegal; 2700 km. Dakar, Senegal to Natal, Brazil; 3010 km. Natal to Belem, Brazil; 1600 km. Belem to Santarem, Brazil; 720 km. Santarem to Manaus, Brazil; 590 km. Manuas to Boa Vista, Roraima, Brazil; 670 km. Boa Vista, Brazil to Angel Falls, Venezuela; 410 km. Auyan Tepui in Venezuela then west to Medellín, Colombia; 1450 km. Medellin to Guatemala City, Guatemala; 1900 km. Flew over Panama, Costa Rica, Nicaragua, El Salvador and Guatemala. He landed at La Aurora International Airport, in Guatemala City, Guatemala. Guatemala to Huatulco, Mexico; 630 km. Huatulco to Toluca, Mexico; 540 km. Toluca, Mexico to Reynosa, Mexico; 770 km. Reynosa, Mexico to Freeport, Grand Bahama; 1950 km. Freeport, Grand Bahama to Windsor, Ontario, Canada; 1800 km. Windsor, Ontario, Canada to Red Lake, Ontario, Canada; 1270 km. Red Lake, Ontario to Rankin Inlet, Nunavut; 1320 km. Rankin Inlet, Nunavut, MB to Resolute, Nunavut, Canada; 1330 km. Resolute, Nunavut, Canada to the North Pole; 650 km. North Pole to Longyearbyen, Norway; 1050 km. Longyearbyen, Norway to Ålesund Airport, Vigra, Norway; 115 km. Ålesund Vigra, Norway to Wurzburg, Germany; 1450 km. Wurzburg to Friedrichshafen, Germany; 240 km. Friedrichshafen, Germany to Lyon, France; 420 km. Lyon, France to Madrid, Spain; 920 km.\n\nMadrid to South Pole to Madrid... (Miles noted are approximate flight distances in kilometers (km)).\nCuatro Vientos Airport, Madrid to Minorca Mahón Airport, Balearic Islands, Mediterranean Sea; 682 km. Minorca Mahón Airport, Balearic Islands to Malta International Airport, Luqa, Malta; 1008 km. Malta International Airport, Luqa, Malta to Marsa Matruh International Airport, Egypt; 1282 km. Mearsa Matruh International Airport, Egypt to Khartoum, Sudan; 1845 km. Khartoum, Sudan to Malindi Airport, Kenya; 2244 km. Malindi Airport, Kenya to Seychelles; 1720 km. Seychelles to Gan International Airport, Gan (Addu Atoll), Maldives; 2010 km. Gan, Maldives to Cocos (Keeling) Islands. 2911 km. Cocos Islans to Port Hedland, Western Australia, Australia; 2491 km. Port Hedland, Western Australia to Ayres Rock, Northern Territory; 1382 km. Ayres Rock, Northern Territory to Mildura, Victoria; 1471 km. Mildura, Victoria to Cambridge Aerodrome, Hobart, Tasmania; 1068 km. Cambridge Aerodrome, Hobart, Tasmania to Mario Zucchelli Station, Terra Nova Bay, Antarctica; 3643 km. From Mario Zucchelli Station to South Pole, 90 degrees South; 1710 km. South Pole, 90 degrees South to Marambio Base, Antarctica; 2876 km. Marambio, Antarctica to Ushuaia, Argentina; 1236 km. Ushuaia, Argentina to Comodoro Rivadavia, Argentina; 1000 km. Comodoro Rivadavia, Argentina to General Rodriguez Aerodrome, Buenos Aires, Argentina; 1200 km. General Rodriguez Aerodrome, Buenos Aires to Iguazu Falls, Argentina; 1401 km. Iguazu Falls, Argentina to Iguazu Falls, Brazil; 1 km. Iguazu Falls, Brazil to Campo dos Amarais Airport, Sao Paulo, Brazil; 808 km. Campo dos Amarais Airport, Sao Paulo, Brazil to Brasillia, Brazil; 223 km. Brasillia to São José de Mipibu, Natal, Brazil; 2392 km. São José de Mipibu, Natal, Brazil to Aeroporto Internacional de Natal, Sao Gonçalo; 31 km. Aeroporto Internacional de Natal, Sao Gonçalo to Praia Airport, Praia, Cabo Verde; 2652 km. Praia Airport, Praia, Cabo Verde to Lanzarote, Canary Islands; 1661 km. Lanzarote, Canary Islands to La Axarquía, Spain; 1461 km. La Axarquía, Spain to Cuatro Vientos Airport, Madrid, Spain; 394 km.\n\nspidertracks.com, FlightAware\n"}
{"id": "984216", "url": "https://en.wikipedia.org/wiki?curid=984216", "title": "Mount Rogers", "text": "Mount Rogers\n\nMount Rogers is the highest natural point in the Commonwealth of Virginia, United States, with a summit elevation of above mean sea level. The summit straddles the border of Grayson and Smyth Counties, Virginia, about WSW of Troutdale, Virginia. Most of the mountain is contained within the Lewis Fork Wilderness, while the entire area is part of the Mount Rogers National Recreation Area, which itself is a part of the Jefferson National Forest.\n\nThe mountain is named for William Barton Rogers, a Virginian educated at the College of William & Mary, who taught at William & Mary and the University of Virginia, became Virginia's first State Geologist, and went on to found the Massachusetts Institute of Technology.\n\nThe summit is most easily accessed from Grayson Highlands State Park by following the Appalachian Trail southbound for to a blue-blazed trail leading to the summit, which is covered by trees and marked with four National Geodetic Survey triangulation station disks; a standard station disk marked with an equilateral triangle and three standard reference disks marked with arrows pointing towards the station disk. One reference disk has been obscured by dense overgrowth. Because the Appalachian Trail passes within a half mile of the summit, the area is especially popular with hikers.\nThe Mount Rogers area contains a unique record of the geohistory of Virginia. There is evidence from the rocks that volcanoes were part of the landscape. Roughly 750 million years ago, rift-related (divergent) volcanoes erupted along the axis of what later became the Appalachians, and one remnant of that volcanic zone, with its volcanic rocks, still can be seen at Mount Rogers. Massive rhyolite lava flows erupted at the mountain during the Precambrian rifting event. Mount Rogers is also the only place in Virginia that preserves evidence of ancient Proterozoic glaciation.\n\n Mount Rogers is the northernmost habitat of the high-altitude Southern Appalachian spruce-fir forests, which are found in only five other locations in the United States: the Great Smoky Mountains, the Black Mountains, the Great Balsam Mountains, Grandfather Mountain, and Roan Mountain. This forest type is one of the few remaining habitats of the Fraser fir, which is only found at high elevations, typically above , in the southern Appalachian Mountains.\n\nThese forests have suffered recent declines due to infestations by the balsam woolly adelgid (\"Adelges piceae\"), a non-native insect that originated in Europe. It first infested Mount Rogers in 1962 and the entire U.S. population of Fraser firs suffered a 67% mortality rate since, although Mount Rogers was not affected as severely as other locations.\n\nSome researchers have proposed that air pollution in the form of nitrogen and sulfur compounds originating from power plants has been a source of stress to the Fraser firs, resulting in an increased susceptibility to the balsam woolly adelgid, but this relationship has not been confirmed.\n\n\nhttp://www.radford.edu/~fldsch/RUFieldschool/formationpages/Mt.%20Rogers%20Fm/MtRogers.html\n"}
{"id": "14030953", "url": "https://en.wikipedia.org/wiki?curid=14030953", "title": "Nanoionic supercapacitor", "text": "Nanoionic supercapacitor\n\nNanoionic supercapacitors belong to new class of nanoionic devices, i.e. devices operating due to fast ion transport at nano-scale. An example of such a device is a nanoionic switch with quantum conductance. All solid state micrometre sized supercapacitors; based on advanced superionic conductors i.e. nanoionic supercapacitors, have of late gained recognition as critical electron components of future sub-voltage and deep sub-voltage nanoelectronics and related technologies.\n\nSupercapacitors based on advanced superionic conductors were developed several decades ago, however\nthe capacity density of their functional heterojunctions (disordered crystal structure) is 100 -10 microF/cm2\nat frequencies ~0,01–1000 Hz. Low operation frequencies of such heterojunctions are the result of violations of\nfast ion transport conditions in molecular-thin double electric layer on the heterojunctions. The major approach to nanoionics of advanced superionic conductors is to retain the concentration and potential barrier heights to mobile\nion jumps on functional heterojunctions at the level of those in the volume. Nanoionic supercapacitors should be devices with fast ion transport in double electric layer and record high capacity-frequency characteristics.\n\n\n"}
{"id": "22991819", "url": "https://en.wikipedia.org/wiki?curid=22991819", "title": "Norwegian Forest and Landscape Institute", "text": "Norwegian Forest and Landscape Institute\n\nThe Norwegian Forest and Landscape Institute () was a research institute based in Norway.\n\nOrganizationally subordinate to the Norwegian Ministry of Agriculture and Food, it is autonomous in its research. It was established on 1 July 2006 through a merger of the Norwegian Forest Research Institute () and the Norwegian Institute of Land Inventory (). Headquarters are in Ås, and regional offices are in Bergen, Steinkjer and Tromsø.\n\nThe director is Arne Bardalen.\n\nOn June 30, 2015 the Institute was merged into the Norwegian Institute of Bioeconomy Research.\n"}
{"id": "6048590", "url": "https://en.wikipedia.org/wiki?curid=6048590", "title": "Nuclear War Survival Skills", "text": "Nuclear War Survival Skills\n\nNuclear War Survival Skills or NWSS, by Cresson Kearny, is a civil defense manual. It contains information gleaned from research performed at Oak Ridge National Laboratory during the Cold War, as well as from Kearny's extensive jungle living and international travels.\n\n\"Nuclear War Survival Skills\" aims to provide a general audience with advice on how to survive conditions likely to be encountered in the event of a nuclear catastrophe, as well as encouraging optimism in the face of such a catastrophe by asserting the survivability of a nuclear war.\n\nThe main chapters are preceded by forewords from Edward Teller and Eugene Wigner. Following this is an introduction which explains that even the fruition of the Strategic Defense Initiative program would not make \"self-help civil defense\" obsolete. A comparison is made of the civil defense preparations of Switzerland, Russia, and the United States, where it is concluded that: \"Switzerland has the best civil defense system\"; \"The rulers of the Soviet Union... continue to prepare the Russians to fight, survive, and win all types of wars\"; and that \"the United States has advocated... a strategy that purposely leaves its citizens unprotected hostages to its enemies.\" Thus, \"The emphasis in this book is on survival preparations that can be made in the last few days of a worsening crisis.\"\n\nThe first chapter aims to give background information to dispel various demoralizing myths and reaffirm the potential survivability and reality of nuclear weapons. \"An all-out nuclear war between Russia and the United States would... be far from the end of human life on earth.\" Myths listed include: \"Fallout radiation from a nuclear war would poison the air and all parts of the environment. It would kill everyone.\"; \"Fallout radiation penetrates everything; there is no escaping its deadly effects.\"; and \"Unsurvivable \"nuclear winter\" surely will follow a nuclear war.\"\n\nThis chapter provides information on the immediate effects of thermonuclear explosions, and peoples' likely reactions to them, in an attempt to lessen the terror and confusion that would be prevalent after an unexpected nuclear attack. \"Some people would think the end of the world was upon them if they happened to be in an area downwind from surface bursts of nuclear weapons that sucked millions of tons of pulverized earth into the air.\"\n\nIllustrates the limitations of the National Warning System (NAWAS) and the Attack Warning Signal sirens, concluding that \"In an all-out attack, the early explosions would give sufficient warning for most people to reach nearby shelter in time.\"\n\nDue to the replacement of large warheads on inaccurate missiles with smaller warheads on more accurate missiles, \"you may logically conclude that unless your home is closer than 10 miles from the nearest probable target, you need not evacuate to avoid blast and fire dangers.\" Evacuation relevant to fallout radiation risk is thoroughly discussed, where it is noted that most available fallout risk-area maps are inaccurate, outdated, and misleading.\n\nProvides information on fallout protection and basic structures; complete designs for \"6 types of earth-covered expedient shelters\" are provided in Appendix A.\n\n\"Some shelters will become dangerously hot in a few hours.\" The Kearny air pump (for which a design is included in the appendix) is recommended, with natural ventilation considered typically inadequate, and electric pumps considered unreliable and prone to heating the air. Filters are considered unnecessary, \"the hazards from fallout particles carried into shelters by unfiltered ventilating air are minor compared to the dangers from inadequate ventilation.\"\n\nFire is considered the third-most-dangerous hazard, after direct blast effects and fallout radiation. It is noted that during the Bombing of Dresden, \"Most casualties were caused by the inhalation of hot gases and carbon monoxide\"\n\nThe importance of water to basic survival is discussed, with the recommendation that \"four or five quarts of drinking water per day are essential.\" Methods of storing, transporting, and purifying water are also explained, with plastic-lined earthen storage pits recommended for storing large quantities.\n\nA basic diet, vegetarian and consisting only of bulk staples, is presented, along with basic nutrition facts and special advice for the very young, very old, and sick. \"And because of the remarkable productivity of American agriculture, there usually would be enough grain and beans in storage to supply surviving Americans with sufficient food for at least a year following a heavy nuclear attack.\n\nReviews the best dose-rate meters and dosimeters available in 1987 and details the Kearny fallout meter, a \"Homemakeable dose-rate meter\", which can be built from a correctly scaled copy of the plans, such as those provided in hardcopies of the book; photocopies and printouts of digital copies may not be to scale.\n\n\"Under wartime conditions, even a faint light that shows only the shapes of nearby people and things can make the difference between an endurable situation, and a black ordeal.\" Plans for an expedient cooking-oil powered lamp are included.\n\nTopics covered include: Clean Water and Food; Control of Insects; Prevention of Skin Diseases; Disposal of Human Wastes; Disposal of Dead Bodies; and Respiratory Diseases.\n\n\"Where There Is No Doctor\" is recommended to supplement to this chapter. Excerpts are provided, along with additional information specific to the health effects of radiation and the use of potassium iodide.\n\nImprovised furniture is discussed, including a hammock made from bedsheets, and a hanging chair made from the hammock.\n\nThe basic principles of thermal insulation are explained; ideas for expedient cold- and wet-weather clothing are provided.\n\nA list of recommended preparations are given for: shelter, shelter ventilation, water, food, fallout meters, sanitation, medicines, light, communications, etc.\n\nConsiderations for permanent fallout shelters are given, with emphasis on maximizing habitability and minimizing cost.\n\nDetails the potential fallout dangers to the United States of a limited nuclear exchange between other countries.\n\n\nIn an article sharply critical of the whole genre, the \"Bulletin of the Atomic Scientists\" (Vol 39, 1983) characterizes the volume as being one of the two more \"substantial\" books on surviving nuclear war out of the four reviewed. The other \"substantial\" book, \"Life After Doomsday: A Survivalist Guide to Nuclear War and Other Major Disasters\" by Bruce D. Clayton, itself is stated to praise and borrow from \"Nuclear War Survival Skills\". The BAS article backhandedly compliments NWSS on its inclusion of features such as \"elaborate diagrams for building shelter; testing for radiation with homemade meters; providing for ventilation; filtration of water and sanitation,\" but goes on to say that the basic flaw with NWSS and the other books reviewed is that they deal only with short-term survival, and sidestep putting heavy thought into the long-term ramifications of nuclear conflict for the continued survival of both their assiduous readers and the balance of the human race. The article suggests that \"the best and certainly most honest publication [in the post-apocalyptic survival guide genre] is \"The Official Government Nuclear Survivors' Manual: Everything that is Known about Effective Procedures in Case of Nuclear Attack\"\" (published by Farrar, Straus & Giroux), which \"has 192 pages, all blank.\"\n\n\"Nuclear War Survival Skills\" was released into the public domain by the author, and is available in digital format for free from several sources online. In the printed form, a modest charge will generally be incurred.\n\nOriginally released September 1979, it was updated and republished in May 1987 with a significant addition on nuclear winter, consisting largely of detailing the shaky assumptions used by nuclear winter models. In 2001 a one-page addendum on radiation hormesis was added.\n\n\n\n"}
{"id": "18842359", "url": "https://en.wikipedia.org/wiki?curid=18842359", "title": "Ocean", "text": "Ocean\n\nAn ocean () is a body of saline water that composes much of a planet's hydrosphere. On Earth, an ocean is one of the major conventional divisions of the World Ocean. These are, in descending order by area, the Pacific, Atlantic, Indian, Southern (Antarctic), and Arctic Oceans. The word \"ocean\" is often used interchangeably with \"sea\" in American English. Strictly speaking, a \"sea\" is a body of saline water (generally a division of the world ocean) partly or fully enclosed by land, though \"the sea\" refers also to the oceans.\n\nSaline water covers approximately and is customarily divided into several principal oceans and smaller seas, with the ocean covering approximately 71% of Earth's surface and 90% of the Earth's biosphere. The ocean contains 97% of Earth's water, and oceanographers have stated that less than 5% of the World Ocean has been explored. The total volume is approximately 1.35 billion cubic kilometers (320 million cu mi) with an average depth of nearly .\n\nAs the world ocean is the principal component of Earth's hydrosphere, it is integral to life, forms part of the carbon cycle, and influences climate and weather patterns. The World Ocean is the habitat of 230,000 known species, but because much of it is unexplored, the number of species that exist in the ocean is much larger, possibly over two million. The origin of Earth's oceans is unknown; oceans are thought to have formed in the Hadean eon and may have been the impetus for the emergence of life.\n\nExtraterrestrial oceans may be composed of water or other elements and compounds. The only confirmed large stable bodies of extraterrestrial surface liquids are the lakes of Titan, although there is evidence for the existence of oceans elsewhere in the Solar System. Early in their geologic histories, Mars and Venus are theorized to have had large water oceans. The Mars ocean hypothesis suggests that nearly a third of the surface of Mars was once covered by water, and a runaway greenhouse effect may have boiled away the global ocean of Venus. Compounds such as salts and ammonia dissolved in water lower its freezing point so that water might exist in large quantities in extraterrestrial environments as brine or convecting ice. Unconfirmed oceans are speculated beneath the surface of many dwarf planets and natural satellites; notably, the ocean of Europa is estimated to have over twice the water volume of Earth. The Solar System's giant planets are also thought to have liquid atmospheric layers of yet to be confirmed compositions. Oceans may also exist on exoplanets and exomoons, including surface oceans of liquid water within a circumstellar habitable zone. Ocean planets are a hypothetical type of planet with a surface completely covered with liquid.\n\nThe word « ocean » comes from the figure in classical antiquity, Oceanus (; \"Ōkeanós\", ), the elder of the Titans in classical Greek mythology, believed by the ancient Greeks and Romans to be the divine personification of the sea, an enormous river encircling the world.\n\nThe concept of Ōkeanós has an Indo-European connection. Greek Ōkeanós has been compared to the Vedic epithet ā-śáyāna-, predicated of the dragon Vṛtra-, who captured the cows/rivers. Related to this notion, the Okeanos is represented with a dragon-tail on some early Greek vases.\n\nThough generally described as several separate oceans, the global, interconnected body of salt water is sometimes referred to as the World Ocean or global ocean. The concept of a continuous body of water with relatively free interchange among its parts is of fundamental importance to oceanography.\n\nThe major oceanic divisions – listed below in descending order of area and volume – are defined in part by the continents, various archipelagos, and other criteria.\n\nOceans are fringed by smaller, adjoining bodies of water such as seas, gulfs, bays, bights, and straits.\n\nThe mid-ocean ridges of the world are connected and form a single global mid-oceanic ridge system that is part of every ocean and the longest mountain range in the world. The continuous mountain range is long (several times longer than the Andes, the longest continental mountain range).\n\nThe total mass of the hydrosphere is about 1.4 quintillion metric tons ( long tons or short tons), which is about 0.023% of Earth's total mass. Less than 3% is freshwater; the rest is saltwater, almost all of which is in the ocean. The area of the World Ocean is about 361.9 million square kilometers (139.7 million square miles), which covers about 70.9% of Earth's surface, and its volume is approximately 1.335 billion cubic kilometers (320.3 million cubic miles). This can be thought of as a cube of water with an edge length of . Its average depth is about , and its maximum depth is at the Mariana Trench. Nearly half of the world's marine waters are over deep. The vast expanses of deep ocean (anything below ) cover about 66% of Earth's surface. This does not include seas not connected to the World Ocean, such as the Caspian Sea.\n\nThe bluish color of water is a composite of several contributing agents. Prominent contributors include dissolved organic matter and chlorophyll. Mariners and other seafarers have reported that the ocean often emits a visible glow which extends for miles at night. In 2005, scientists announced that for the first time, they had obtained photographic evidence of this glow. It is most likely caused by bioluminescence.\n\nOceanographers divide the ocean into different vertical zones defined by physical and biological conditions. The pelagic zone includes all open ocean regions, and can be divided into further regions categorized by depth and light abundance. The photic zone includes the oceans from the surface to a depth of 200 m; it is the region where photosynthesis can occur and is, therefore, the most biodiverse. Because plants require photosynthesis, life found deeper than the photic zone must either rely on material sinking from above (see marine snow) or find another energy source. Hydrothermal vents are the primary source of energy in what is known as the aphotic zone (depths exceeding 200 m). The pelagic part of the photic zone is known as the epipelagic.\n\nThe pelagic part of the aphotic zone can be further divided into vertical regions according to temperature.\nThe mesopelagic is the uppermost region. Its lowermost boundary is at a thermocline of , which, in the tropics generally lies at . Next is the bathypelagic lying between , typically between and , lying along the top of the abyssal plain is the abyssopelagic, whose lower boundary lies at about . The last zone includes the deep oceanic trench, and is known as the hadalpelagic. This lies between and is the deepest oceanic zone.\n\nThe benthic zones are aphotic and correspond to the three deepest zones of the deep-sea. The bathyal zone covers the continental slope down to about . The abyssal zone covers the abyssal plains between 4,000 and 6,000 m. Lastly, the hadal zone corresponds to the hadalpelagic zone, which is found in oceanic trenches.\n\nThe pelagic zone can be further subdivided into two subregions: the neritic zone and the oceanic zone. The neritic zone encompasses the water mass directly above the continental shelves whereas the oceanic zone includes all the completely open water.\n\nIn contrast, the littoral zone covers the region between low and high tide and represents the transitional area between marine and terrestrial conditions. It is also known as the intertidal zone because it is the area where tide level affects the conditions of the region.\n\nIf a zone undergoes dramatic changes in temperature with depth, it contains a thermocline. The tropical thermocline is typically deeper than the thermocline at higher latitudes. Polar waters, which receive relatively little solar energy, are not stratified by temperature and generally lack a thermocline because surface water at polar latitudes are nearly as cold as water at greater depths. Below the thermocline, water is very cold, ranging from −1 °C to 3 °C. Because this deep and cold layer contains the bulk of ocean water, the average temperature of the world ocean is 3.9 °C. \nIf a zone undergoes dramatic changes in salinity with depth, it contains a halocline. If a zone undergoes a strong, vertical chemistry gradient with depth, it contains a chemocline.\n\nThe halocline often coincides with the thermocline, and the combination produces a pronounced pycnocline.\n\nThe deepest point in the ocean is the Mariana Trench, located in the Pacific Ocean near the Northern Mariana Islands. Its maximum depth has been estimated to be (plus or minus 11 meters; see the Mariana Trench article for discussion of the various estimates of the maximum depth.) The British naval vessel \"Challenger II\" surveyed the trench in 1951 and named the deepest part of the trench the \"Challenger Deep\". In 1960, the Trieste successfully reached the bottom of the trench, manned by a crew of two men.\n\nOceanic maritime currents have different origins. Tidal currents are in phase with the tide, hence are quasiperiodic, they may form various knots in certain places, most notably around headlands. Non periodic currents have for origin the waves, wind and different densities.\n\nThe wind and waves create surface currents (designated as « drift currents »). These currents can decompose in one quasi permanent current (which varies within the hourly scale) and one movement of Stokes drift under the effect of rapid waves movement (at the echelon of a couple of seconds).). The quasi permanent current is accelerated by the breaking of waves, and in a lesser governing effect, by the friction of the wind on the surface.\n\nThis acceleration of the current takes place in the direction of waves and dominant wind. Accordingly, when the sea depth increases, the rotation of the earth changes the direction of currents, in proportion with the increase of depth while friction lowers their speed. At a certain sea depth, the current changes direction and is seen inverted in the opposite direction with speed current becoming nul: known as the Ekman spiral. The influence of these currents is mainly experienced at the mixed layer of the ocean surface, often from 400 to 800 meters of maximum depth. These currents can considerably alter, change and are dependent on the various yearly seasons. If the mixed layer is less thick (10 to 20 meters), the quasi permanent current at the surface adopts an extreme oblique direction in relation to the direction of the wind, becoming virtually homogeneous, until the Thermocline.\n\nIn the deep however, maritime currents are caused by the temperature gradients and the salinity between water density masses.\n\nIn littoral zones, breaking wave is so intense and the depth measurement so low, that maritime currents reach often 1 to 2 knots.\n\nOcean currents greatly affect Earth's climate by transferring heat from the tropics to the polar regions. Transferring warm or cold air and precipitation to coastal regions, winds may carry them inland. Surface heat and freshwater fluxes create global density gradients that drive the thermohaline circulation part of large-scale ocean circulation. It plays an important role in supplying heat to the polar regions, and thus in sea ice regulation. Changes in the thermohaline circulation are thought to have significant impacts on Earth's energy budget. In so far as the thermohaline circulation governs the rate at which deep waters reach the surface, it may also significantly influence atmospheric carbon dioxide concentrations.\n\nFor a discussion of the possibilities of changes to the thermohaline circulation under global warming, see shutdown of thermohaline circulation.\n\nIt is often stated that the thermohaline circulation is the primary reason that the climate of Western Europe is so temperate. An alternate hypothesis claims that this is largely incorrect, and that Europe is warm mostly because it lies downwind of an ocean basin, and because atmospheric waves bring warm air north from the subtropics.\n\nThe Antarctic Circumpolar Current encircles that continent, influencing the area's climate and connecting currents in several oceans.\n\nOne of the most dramatic forms of weather occurs over the oceans: tropical cyclones (also called \"typhoons\" and \"hurricanes\" depending upon where the system forms).\n\nThe ocean has a significant effect on the biosphere. Oceanic evaporation, as a phase of the water cycle, is the source of most rainfall, and ocean temperatures determine climate and wind patterns that affect life on land. Life within the ocean evolved 3 billion years prior to life on land. Both the depth and the distance from shore strongly influence the biodiversity of the plants and animals present in each region.\n\nAs it is thought that life evolved in the ocean, the diversity of life is immense, including:\n\nIn addition, many land animals have adapted to living a major part of their life on the oceans. For instance, seabirds are a diverse group of birds that have adapted to a life mainly on the oceans. They feed on marine animals and spend most of their lifetime on water, many only going on land for breeding. Other birds that have adapted to oceans as their living space are penguins, seagulls and pelicans. Seven species of turtles, the sea turtles, also spend most of their time in the oceans.\n\nA zone of rapid salinity increase with depth is called a halocline. The temperature of maximum density of seawater decreases as its salt content increases. Freezing temperature of water decreases with salinity, and boiling temperature of water increases with salinity. Typical seawater freezes at around −1.9 °C at atmospheric pressure. If precipitation exceeds evaporation, as is the case in polar and temperate regions, salinity will be lower. If evaporation exceeds precipitation, as is the case in tropical regions, salinity will be higher. Thus, oceanic waters in polar regions have lower salinity content than oceanic waters in temperate and tropical regions.\n\nSalinity can be calculated using the chlorinity, which is a measure of the total mass of halogen ions (includes fluorine, chlorine, bromine, and iodine) in seawater. By international agreement, the following formula is used to determine salinity:\n\nSalinity (in ‰) = 1.80655 × Chlorinity (in ‰)\n\nThe average chlorinity is about 19.2‰, and, thus, the average salinity is around 34.7‰ \n\nMany of the world's goods are moved by ship between the world's seaports. Oceans are also the major supply source for the fishing industry. Some of the major harvests are shrimp, fish, crabs, and lobster.\n\nThe motions of the ocean surface, known as undulations or \"waves\", are the partial and alternate rising and falling of the ocean surface. The series of mechanical waves that propagate along the interface between water and air is called swell. \n\nAlthough Earth is the only known planet with large stable bodies of liquid water on its surface and the only one in the Solar System, other celestial bodies are thought to have large oceans.\n\nThe gas giants, Jupiter and Saturn, are thought to lack surfaces and instead have a stratum of liquid hydrogen; however their planetary geology is not well understood. The possibility of the ice giants Uranus and Neptune having hot, highly compressed, supercritical water under their thick atmospheres has been hypothesised. Although their composition is still not fully understood, a 2006 study by Wiktorowicz and Ingersall ruled out the possibility of such a water \"ocean\" existing on Neptune, though some studies have suggested that exotic oceans of liquid diamond are possible.\n\nThe Mars ocean hypothesis suggests that nearly a third of the surface of Mars was once covered by water, though the water on Mars is no longer oceanic (much of it residing in the ice caps). The possibility continues to be studied along with reasons for their apparent disappearance. Astronomers think that Venus had liquid water and perhaps oceans in its very early history. If they existed, all later vanished via resurfacing.\n\nA global layer of liquid water thick enough to decouple the crust from the mantle is thought to be present on the natural satellites Titan, Europa, Enceladus and, with less certainty, Callisto, Ganymede and Triton. A magma ocean is thought to be present on Io. Geysers have been found on Saturn's moon Enceladus, possibly originating from about deep ocean beneath an ice shell. Other icy moons may also have internal oceans, or may once have had internal oceans that have now frozen.\n\nLarge bodies of liquid hydrocarbons are thought to be present on the surface of Titan, although they are not large enough to be considered oceans and are sometimes referred to as \"lakes\" or seas. The Cassini–Huygens space mission initially discovered only what appeared to be dry lakebeds and empty river channels, suggesting that Titan had lost what surface liquids it might have had. Cassini's more recent fly-by of Titan offers radar images that strongly suggest hydrocarbon lakes exist near the colder polar regions. Titan is thought to have a subsurface liquid-water ocean under the ice and hydrocarbon mix that forms its outer crust.\n\nCeres appears to be differentiated into a rocky core and icy mantle and may harbour a liquid-water ocean under its surface.\n\nNot enough is known of the larger trans-Neptunian objects to determine whether they are differentiated bodies capable of supporting oceans, although models of radioactive decay suggest that Pluto, Eris, Sedna, and Orcus have oceans beneath solid icy crusts approximately thick.\n\nSome planets and natural satellites outside the Solar System are likely to have oceans, including possible water ocean planets similar to Earth in the habitable zone or \"liquid-water belt\". The detection of oceans, even through the spectroscopy method, however is likely extremely difficult and inconclusive.\n\nTheoretical models have been used to predict with high probability that GJ 1214 b, detected by transit, is composed of exotic form of ice VII, making up 75% of its mass,\nmaking it an ocean planet.\n\nOther possible candidates are merely speculated based on their mass and position in the habitable zone include planet though little is actually known of their composition. Some scientists speculate Kepler-22b may be an \"ocean-like\" planet. Models have been proposed for Gliese 581 d that could include surface oceans. Gliese 436 b is speculated to have an ocean of \"hot ice\". Exomoons orbiting planets, particularly gas giants within their parent star's habitable zone may theoretically have surface oceans.\n\nTerrestrial planets will acquire water during their accretion, some of which will be buried in the magma ocean but most of it will go into a steam atmosphere, and when the atmosphere cools it will collapse on to the surface forming an ocean. There will also be outgassing of water from the mantle as the magma solidifies—this will happen even for planets with a low percentage of their mass composed of water, so \"super-Earth exoplanets may be expected to commonly produce water oceans within tens to hundreds of millions of years of their last major accretionary impact.\"\n\nOceans, seas, lakes and other bodies of liquids can be composed of liquids other than water, for example the hydrocarbon lakes on Titan. The possibility of seas of nitrogen on Triton was also considered but ruled out. There is evidence that the icy surfaces of the moons Ganymede, Callisto, Europa, Titan and Enceladus are shells floating on oceans of very dense liquid water or water–ammonia. Earth is often called \"the\" ocean planet because it is 70% covered in water. Extrasolar terrestrial planets that are extremely close to their parent star will be tidally locked and so one half of the planet will be a magma ocean. It is also possible that terrestrial planets had magma oceans at some point during their formation as a result of giant impacts. Hot Neptunes close to their star could lose their atmospheres via hydrodynamic escape, leaving behind their cores with various liquids on the surface. Where there are suitable temperatures and pressures, volatile chemicals that might exist as liquids in abundant quantities on planets include ammonia, argon, carbon disulfide, ethane, hydrazine, hydrogen, hydrogen cyanide, hydrogen sulfide, methane, neon, nitrogen, nitric oxide, phosphine, silane, sulfuric acid, and water.\n\nSupercritical fluids, although not liquids, do share various properties with liquids. Underneath the thick atmospheres of the planets Uranus and Neptune, it is expected that these planets are composed of oceans of hot high-density fluid mixtures of water, ammonia and other volatiles. The gaseous outer layers of Jupiter and Saturn transition smoothly into oceans of supercritical hydrogen. The atmosphere of Venus is 96.5% carbon dioxide, which is a supercritical fluid at its surface.\n\n\nOn other bodies:\n\n"}
{"id": "203875", "url": "https://en.wikipedia.org/wiki?curid=203875", "title": "Orders of magnitude (mass)", "text": "Orders of magnitude (mass)\n\nTo help compare different orders of magnitude, the following lists describe various mass levels between 10 kg and 10 kg.\n\nThe table below is based on the kilogram (kg), the base unit of mass in the International System of Units (SI). The kilogram is the only standard unit to include an SI prefix (\"kilo-\") as part of its name. The \"gram\" (10 kg) is an SI derived unit of mass. However, the \"names\" of all SI mass units are based on \"gram\", rather than on \"kilogram\"; thus 10 kg is a \"megagram\" (10 g), not a \"kilokilogram\".\n\nThe \"tonne\" (t) is a SI-compatible unit of mass equal to a megagram, or 10 kg. The unit is in common use for masses above about 10 kg and is often used with SI prefixes. For example, a gigagram or 10 g is 10 tonne, commonly called a \"kilotonne\".\nOther units of mass are also in use. Historical units include the stone, the pound, the carat, and the grain.\n\nFor subatomic particles, physicists use the mass equivalent to the energy represented by an electronvolt (eV). At the atomic level, chemists use the mass of one-twelfth of a carbon-12 atom (the dalton). Astronomers use the mass of the sun ().\n\nUnlike other physical quantities, mass-energy does not have an \"a priori\" expected minimal quantity, as is the case with time or length, or an observed basic quantum as in the case of electric charge. Planck's law allows for the existence of photons with arbitrarily low energies. Consequently, there can only ever be an experimental lower bound on the mass of a supposedly massless particle; in the case of the photon, this confirmed lower bound is of the order of 3×10 eV = 10 kg.\n\n\n"}
{"id": "3759820", "url": "https://en.wikipedia.org/wiki?curid=3759820", "title": "Physis", "text": "Physis\n\nPhysis (Greek: \"phusis\") is a Greek theological, philosophical, and scientific term usually translated into English as \"nature\".\n\nThe term is central to Greek philosophy, and as a consequence to Western philosophy as a whole.\nIn pre-Socratic usage, \"phusis\" contrasted with \"nomos\" \"law, human convention\"\nSince Aristotle, however, the \"physical\" (the subject matter of \"physics\", properly \"natural things\") has more typically been juxtaposed to the \"metaphysical\".\n\nThe word φύσις is a verbal noun based on φύω \"to grow, to appear\" (cognate with English \"to be\"). In Homeric Greek it is used quite literally, of the manner of growth of a particular species of plant. \n\nIn pre-Socratic philosophy, beginning with Heraclitus, \"phusis\" in keeping with its etymology of \"growing, becoming\" is always used in the sense of the \"natural\" \"development\", although the focus might lie either with the origin, or the process, or the end result of the process. There is some evidence that by the 6th century BC, beginning with the Ionian School, the word could also be used \nin the comprehensive sense, as referring to \"\"all\" things\", as it were \"Nature\" in the sense of \"Universe\".\n\nIn the Sophist tradition, the term stood in opposition to \"nomos\" (), \"law\" or \"custom\", in the debate on which parts of human existence are natural, and which are due to convention. \nThe contrast of \"phisis\" vs. \"nomos\" could be applied to any subject, much like the modern contrast of \"nature vs. nurture\".\n\nIn book 10 of \"Laws\", Plato criticizes those who write works \"peri phuseōs\". The criticism is that such authors tend to focus on a purely \"naturalistic\" explanation of the world, ignoring the role of \"intention\" or \"technē\", and thus becoming prone to the error of naive atheism. Plato accuses even Hesiod of this, for the reason that the gods in Hesiod \"grow\" out of primordial entities after the physical universe had been established.\n\n\"Because those who use the term mean to say that nature is the first creative power; but if the soul turns out to be the primeval element, and not fire or air, then in the truest sense and beyond other things the soul may be said to exist \"by\" nature; and this would be true if you proved that the soul is older than the body, but not otherwise.\"\n\nAristotle sought out the definition of \"physis\" to prove that there was more than one definition of \"physis\", and more than one way to interpret nature. \"Though Aristotle retains the ancient sense of \"physis\" as growth, he insists that an adequate definition of \"physis\" requires the different perspectives of the four causes (aitia): material, efficient, formal, and final.\" Aristotle believed that nature itself contained its own source of matter (material), power/motion (efficiency), form, and end (final). A unique feature about Aristotle's definition of \"physis\" was his relationship between art and nature. Aristotle said that \"physis\" (nature) is dependent on techne (art). \"The critical distinction between art and nature concerns their different efficient causes: nature is its own source of motion, whereas techne always requires a source of motion outside itself.\" What Aristotle was trying to bring to light, was that art does not contain within itself its form or source of motion. Consider the process of an acorn becoming an oak tree. This is a natural process that has its own driving force behind it. There is no external force pushing this acorn to its final state, rather it is progressively developing towards one specific end (telos).\nThough φύσις was often used in Hellenistic philosophy, it is used only 14 times in the New Testament (10 of those in the writings of Paul). Its meaning varies throughout Paul's writings. One usage refers to the established or natural order of things, as in \"Romans 2:14\" where Paul writes \"For when Gentiles, who do not have the law, by \"nature\" do what the law requires, they are a law to themselves, even though they do not have the law.\" Another use of φύσις in the sense of \"natural order\" is \"Romans 1:26\" where he writes \"the men likewise gave up \"natural\" relations with women and were consumed with passion for one another\". In \"1 Corinthians 11:14\", Paul asks \"Does not nature itself teach you that if a man wears long hair it is a disgrace for him?\"\n\nThis use of φύσις as referring to a \"natural order\" in \"Romans 1:26\" and \"1 Corinthians 11:14\" may have been influenced by Stoicism. The Greek philosophers, including Aristotle and the Stoics are credited with distinguishing between man-made laws and a natural law of universal validity, but Gerhard Kittel states that the Stoic philosophers were not able to combine the concepts of νόμος (law) and φύσις (nature) to produce the concept of \"natural law\" in the sense that was made possible by Judeo-Christian theology.\n\nAs part of the Pauline theology of salvation by grace, Paul writes in \"Ephesians 2:3\" that \"we all once lived in the passions of our flesh, carrying out the desires of the body and the mind, and were by \"nature\" children of wrath, like the rest of mankind. In the next verse he writes, \"by grace you have been saved.\" \n\nTheologians of the early Christian period differed in the usage of this term. In Antiochene circles, it connoted the humanity or divinity of Christ conceived as a concrete set of characteristics or attributes. In Alexandrine thinking, it meant a concrete individual or independent existent and approximated to hypostasis without being a synonym. While it refers to much the same thing as ousia it is more empirical and descriptive focussing on function while ousia is metaphysical and focuses more on reality. Although found in the context of the Trinitarian debate, it is chiefly important in the Christology of Cyril of Alexandria.\n\nThe Greek adjective \"phusikos\" is represented in various forms in modern English:\nAs \"physics\" \"the study of nature\", as \"physical\" (via Middle Latin \"physicalis\") referring both to physics (the study of nature, the material universe) and to the human body. The term physiology (\"physiologia\") is of 16th-century coinage (Jean Fernel). The term \"physique\", for \"the bodily constitution of a person\", is a 19th-century loan from French. \n\nIn medicine the suffix \"-physis\" occurs in such compounds as \"symphysis\", \"epiphysis\", and a few others, in the sense of \"a growth\". The physis also refers to the \"growth plate\", or site of growth at the end of long bones.\n\n"}
{"id": "1724417", "url": "https://en.wikipedia.org/wiki?curid=1724417", "title": "Pollen count", "text": "Pollen count\n\nPollen count is the measurement of the number of grains of pollen in a cubic meter of air. High pollen counts can sometimes lead to increased rates of an allergic reaction for those with allergic disorders. Usually, the counts are announced for specific plants such as grass, ash, or olive. These are tailored to common plants in the measured areas. Mild winters with warmer days lead to an increase in pollen counts while colder winters lead to delayed pollen release.\n\nIn the UK, the public announcement of the pollen count was popularised by Dr. William Frankland, an immunologist.\n\nAccording to a study by Leonard Bielory, M.D. that was presented to the American College of Allergy, Asthma & Immunology, climate changes are expected to cause pollen counts to more than double by 2040.\n\nOne method of taking the sample uses a silicone grease-covered rod, rotated in the air to collect the pollen. The rod is rotated periodically during the collection period to gather samples through an entire 24-hour day. The rod is then removed and taken to a lab where the collected material is analyzed for the type of pollen and its concentration.\n\nAnother method for measuring the pollen count is by using a Burkard Trap, also known as the seven-day volumetric spore trap. This device works by facing towards the wind and drawing in air by using a pump. The pollen particles drawn in by the pump are then trapped by a sticky wax film that is attached to a rotating drum. The drum slowly rotates one turn over the course of seven days while collecting particles. The tape is removed after one week and cut into day-length sections.\n\n"}
{"id": "31541398", "url": "https://en.wikipedia.org/wiki?curid=31541398", "title": "SafeMed II Project", "text": "SafeMed II Project\n\nThe SafeMed Project on maritime safety and prevention of pollution from ships is a European Union (EU) funded regional project run by the Regional Marine Pollution Emergency Response Centre for the Mediterranean Sea (REMPEC) on behalf of the International Maritime Organization (IMO). \n\nCurrently in its second phase of operation, the SafeMed II Project (2009–2011) builds on the achievements of SafeMed I (2006–2009).\n\nThe aim of the SafeMed II Project is to develop cooperation in the fields of maritime safety and security and in the protection of the marine environment between the EU and the Mediterranean partners. Its main objectives are:\n\n\nThe SafeMed II Project assists the Mediterranean partner Beneficiaries with the further implementation of the 2007 adopted Regional Transport Action Plan (RTAP) for the Mediterranean 2007-2013. The Project offers training programmes and assistance, promotes a common platform for best practices to regulate maritime traffic in the Mediterranean, and seeks to achieve improved access to information for all.\n\nThe Project funds scholarships to internationally recognized maritime universities (such as the World Maritime University), organises seminars and workshops and publishes its own quarterly newsletter, the SafeMed Beacon.\n\n\n"}
{"id": "53037512", "url": "https://en.wikipedia.org/wiki?curid=53037512", "title": "Shapley attractor", "text": "Shapley attractor\n\nThe Shapley attractor is an attractor located about the Shapley Supercluster.\n\nIt is opposed to the Dipole Repeller, in the CMB Dipole of local galactic flow. It is thought to be the composite contributions of the Shapley Concentration and the Great Attractor.\n"}
{"id": "2401559", "url": "https://en.wikipedia.org/wiki?curid=2401559", "title": "Sonnets to Orpheus", "text": "Sonnets to Orpheus\n\nThe Sonnets to Orpheus () are a cycle of 55 sonnets written in 1922 by the Bohemian-Austrian poet Rainer Maria Rilke (1875–1926). It was first published the following year. Rilke, who is \"widely recognized as one of the most lyrically intense German-language poets,\" wrote the cycle in a period of three weeks experiencing what he described a \"savage creative storm.\" Inspired by the news of the death of Wera Ouckama Knoop (1900–1919), a playmate of Rilke's daughter Ruth, he dedicated them as a memorial, or \"\" (literally \"grave-marker\"), to her memory.\n\nAt the same time in February 1922, Rilke had completed work on his deeply philosophical and mystical ten-poem collection entitled \"Duino Elegies\" which had taken ten years to complete. The \"Sonnets to Orpheus\" and the \"Duino Elegies\" are considered Rilke's masterpieces and the highest expressions of his talent.\n\nThrough most of the 1910s, Rilke had suffered from a severe depression that had kept him from writing. He had begun his \"Duino Elegies\" in 1912, and completed parts of it in 1913 and 1915 before being rendered silent by a psychological crisis caused by the events of World War I and his brief conscription into the Austro-Hungarian army. Only in 1920 was he motivated to focus toward completing the \"Elegies.\" However, for the next two years, his mode of life was unstable and did not permit him the time or mental state he needed for his writing.\nIn 1921, Rilke journeyed to Switzerland, hoping to immerse himself among French culture near Geneva and to find a place to live permanently. At the time, he was romantically involved with Baladine Klossowska. At the invitation of Werner Reinhart, Rilke moved into the Château de Muzot, a thirteenth-century manor that lacked gas and electricity, located near Veyras, Rhone Valley, Switzerland. Reinhart, a Swiss merchant and amateur clarinetist, used his wealth to act as a patron to many 20th Century writers and composers. He purchased Muzot to allow Rilke to live there rent-free and focus on his work. Rilke and Klossowska moved in in July 1921 and during the fall Rilke translated writings by Paul Valéry and Michelangelo into German.\n\nWith news of the death of his daughter's friend, Wera Knoop, Rilke was inspired to create and set to work on \"Sonnets to Orpheus\". Within a few days, between 2 February and 5 February 1922, he had completed the first section of 26 sonnets. For the next few days, he focused on the \"Duino Elegies\", completing them on the evening of 11 February. Immediately after, he returned to work on the \"Sonnets\" and completed the following section of 29 sonnets in less than two weeks. In letters to friends, Rilke referred to this three-week period as a \"savage creative storm.\" Rilke considered both collections to be \"of the same birth.\" Writing to his former lover, Lou Andreas-Salomé, on 11 February, he described this period as \"...a boundless storm, a hurricane of the spirit, and whatever inside me is like thread and webbing, framework, it all cracked and bent. No thought of food.\"\n\nThroughout the \"Sonnets\", Wera appears in frequent references to her, both direct where he addresses her by name and indirect as allusions to a \"dancer\" or the mythical Eurydice. Later, Rilke wrote to the young girl's mother stating that Wera's ghost was \"commanding and impelling\" him to write.\n\nThere are 55 sonnets in the sequence, divided into two sections: the first of 26 and the second of 29. The sonnets follow certain trends, but they include many different forms.\n\nAll of the sonnets are composed of two quatrains followed by two tercets.\n\nThe sonnet tradition is not as pronounced in German literature as it is, for example, in English and Italian literature. A possible model for Rilke might have been Charles Baudelaire's \"Les Fleurs du Mal\". To fashion poems in entire cycles was quite common in contemporary practice, the works of Stefan George, Arthur Rimbaud and Stéphane Mallarmé being examples of this. Rilke does not at all stick to the formal standards of the German sonnet fashioned by August Wilhelm Schlegel. The rhyme schemes vary, and are generally ABAB CDCD or ABBA CDDC in the quartets, and EEF GGF, EFG EFG or EFG GFE in the triplets. The sonnets are also all metered, but their meters vary more greatly between poems; dactylic and trochaic are the most common feet, with line length varying greatly, sometimes even within a particular sonnet. Due to the frequent use of enjambment Rilke even breaks through the verse structure. Difficulties in understanding the text arise from pronouns lacking clear reference. So begins, for example, the third sonnet of the first part:\n\n<poem>\nEin Gott vermags. Wie aber, sag mir, soll\nein Mann ihm folgen durch die schmale Leier?\nSein Sinn ist Zwiespalt. An der Kreuzung zweier\nHerzwege steht kein Tempel für Apoll.</poem>\n\n<poem>\n\"A God is able. But tell me, how shall\"\n\"a man follow him through the narrow lyre?\"\n\"His mind is divided. At the crossing of two\"\n\"heart roads there is no temple for Apollo.\" </poem>\nIt is left to interpretation whether \"his mind\" refers to the God or the man.\n\nThe content of the sonnets is, as is typical of Rilke, highly metaphorical. The work is based on the myth of Orpheus and Eurydice. The character of Orpheus (whom Rilke refers to as the \"god with the lyre\") appears several times in the cycle, as do other mythical characters such as Daphne. Sources for this are primarily Ovid's \"Metamorphoses\" and to a lesser extent Virgil's \"Georgics\". The principle of Ovidian transformations can also be found in and especially between the sonnets. During the first sonnet of Orphic singing, the speech of the forest and the animals is \"transformed\" into a girl in the second sonnet: \"And almost a girl it was who emerged / from this joyful unity of song and lyre...\" During the second sonnet, the focus shifts from the girl to the world: \"She slept the world...\" The cycle also contains biblical allusions, including a reference to Esau. Other themes involve animals, peoples of different cultures, and time and death.\n\nWhile Rilke invokes the original poet Orpheus, a poetic self-reflection takes place at the same time. It frequently addresses the conditions of poetry, the nature of art: \"Song is being. For the god, a simple matter. / But when are we?\" (I,3) A solution to these problems can be found in the fifth sonnet of the first part, where Rilke exclaims: \"Once and forever it's Orpheus, whenever there's song\" (I,5). This means that the poem always possesses a divine quality, as the poet stands in direct succession to the son of the Muses.\n\nAlthough Rilke claimed that the entire cycle was inspired by Wera, she appears as a character in only one of the poems. He insisted, however, that \"Wera's own figure [...] nevertheless governs and moves the course of the whole\".\n\nIn May 1922, after deciding he could afford the cost of considerable necessary renovation, the Swiss philanthropist Werner Reinhart bought Muzot so that Rilke could live there rent-free, and became Rilke's patron. He completed the \"Duino Elegies\" while Reinhart's tenant. During this time, Reinhart introduced Rilke to his protégée, the Australian violinist Alma Moodie. Rilke was so impressed with her playing that he wrote in a letter: \"\"What a sound, what richness, what determination. That and the \"Sonnets to Orpheus\", those were two strings of the same voice. And she plays mostly Bach! Muzot has received its musical christening...\"\"\n\nFrom early on, there was criticism of Rilke's Sonnets. Thus, already in 1927 Robert Musil described Rilke as the poet who \"did nothing but perfect the German poem for the first time\", but he limited this judgement to the Duino Elegies as the pinnacle of artistic creation, and described Rilke's \"Sonnets to Orpheus\" as an \"exceptional falling off that Rilke's work suffers\". What Wolfram Groddeck referred to in his afterword to the Reclam edition as a \"dilemma of critical reading\", was a result of the uncompromising text, which resists simple interpretation. At the same time the quality of lyrical expression undoubtedly constitutes a highlight of German poetic history. Thus, the criticism of the sonnets often fluctuates between the assumption of a sonic primacy over the semantic level and an unconditional affirmation of the cycle.\n\n<poem>\nDa stieg ein Baum. O reine Übersteigung!\nO Orpheus singt! O hoher Baum im Ohr.\nUnd alles schwieg. Doch selbst in der verschweigung\nging neuer Anfang, Wink und Wandlung vor.\n\nTiere aus Stille drangen aus dem klaren\ngelösten wald von Lager und Genist;\nund da ergab sich, daß sie nicht aus List\nund nicht aus Angst in sich so leise waren,\n\nsondern aus Hören. Brüllen, Schrei, Geröhr\nschien klein in ihren Herzen. Und wo eben\nkaum eine Hütte war, dies zu empfangen,\n\nein Unterschlupf aus dunkelstem Verlangen\nmit einem Zugang, dessen Pfosten beben, -\nda schufst du ihnen Tempel im Gehör.</poem>\n<poem>\n\"There rose a tree. O pure transcendence!\"\n\"O Orpheus sings! O tall tree in the ear!\"\n\"And all was silent. Yet still in this silence\"\n\"proceeded new beginning, sign and transformation.\"\n\n\"Creatures of stillness pressed out of the clear\"\n\"unravelled forest from lair and nest;\"\n\"and it came to pass, that not by cunning\"\n\" and not out of fear were they made so quiet,\"\n\n\"but simply out of hearing. Bellow, scream, roar\"\n\"seemed small in their hearts. And just where\"\n\"there was scarcely a hut to receive this,\"\n\n\"a shelter of darkest longing\"\n\"with an entrance, whose posts shook, -\"\n\"you built for them a temple in hearing.\"</poem>\n\nNeuman, Claude, \"The Sonnets to Orpheus\", English and French rhymed and metered translations, trilingual German-English-French edition, Editions www.ressouvenances.fr , 2017\n\n"}
{"id": "30845824", "url": "https://en.wikipedia.org/wiki?curid=30845824", "title": "Southern Afrotemperate Forest", "text": "Southern Afrotemperate Forest\n\nSouthern Afrotemperate Forest (the Southern Cape Forests) is a kind of tall, shady, multilayered indigenous South African forest.\nThis is the main forest-type in the south-western part of South Africa, naturally extending from the Cape Peninsula in the west, as far as Port Elizabeth in the east. In this range (apart from the massive Knysna-tsitsikamma forest complex), it usually occurs in small forest pockets, surrounded by fynbos vegetation.\n\nThis forest ecosystem is a subtype of the general Afromontane forest, which can be found across Africa as far north as Ethiopia. However, it is distinguished from other types of forests in southern Africa by its relatively distinct range of species and its being confined to the far south-western corner of Africa – separated from the other forested areas to the east and north. Southern Afrotemperate Forest tends to grow on soils derived from sandstone and granite which are the dominant rock formations in the south-western Cape.\n\nThe Western Cape is prone to seasonal fires and the various types of fynbos vegetation that dominate here are all governed by the fire cycles. However, Southern Afrotemperate Forest is not adapted to fire, so is always restricted to \"fire refugia\" such as gorges, wet riverine areas, or rocky scree slopes where fires cannot reach. In the absence of veld fires, the taller forests tend to expand at the expense of the fynbos.\n\nIt is conventionally divided into three closely related subtypes:\n\nThis is a type of medium-height scree forest usually only found in small patches, growing on steep, rocky slopes and by mountain streams. It is endemic to the Western Cape. <br>\nBased on location and the species composition of the forests, this type is often informally divided into riverine forests (\"oewerbos\" in Afrikaans) and scree forests (\"dasbos\" in Afrikaans). The species composition of these two subtypes differs slightly, but they are still similar enough to be classed together as an ecosystem. The dominant, largest, and most obvious tree species are \"Metrosideros angustifolia\", \"Brabejum stellatifolium\", \"Cassine schinoides\", \"Apodytes dimidiata\", \"Cunonia capensis\", \"Ilex mitis\", \"Kiggelaria africana\", \"Rapanea melanophloeos\", \"Olinia ventosa\", and \"Podocarpus elongatus\".<br>\nWestern Cape Talus forests naturally undergo periodic disturbance, flooding in the case of riverine forests, and rock-slides in the case of scree forests. Swift regeneration immediately follows. The natural cycle of disturbance of the surrounding fynbos vegetation is fire-driven, but this has little effect on the sheltered talus forests.\n\nThe main threats to this ecosystem are from invasive alien plants, especially black wattle trees. The natural habitat of Western Cape Talus is usually within catchment areas, and they thus perform an important function in regulating the water-systems and preventing erosion. They are also host to many scenic hiking trails and have value as a source of medicinal plants. In addition, several species (such as \"Clivia mirabilis\" and \"Cryptocarya angustifolia\") are endemic to these forests and occur nowhere else in the world.\n\nThe thicker, deeper, denser \"South-western Cape forests\" are dominated by larger afromontane trees. These tall woodlands are typically found in sheltered gorges and mountainous areas in the Western Cape. \n\nThey also include the Cape Peninsula Forests such as those at Newlands Forest, Kirstenbosch, and Orangekloof, which are all located around Table Mountain, within the city of Cape Town.\n\nTypical species include massive trees such as yellowwoods, \"Ilex mitis\", \"Kiggelaria africana\", Assegai trees, ironwoods, \"Cunonia capensis\", \"Cassine\" species, \"Olinia ventosa\", and \"Rapanea melanophloeos\", which form the highest canopy; smaller trees such as \"Halleria lucida\", \"Diospyros whyteana\", and \"Maytenus acuminata\", which form a medium layer; as well as a variety of ferns, herbs, bushes, vines, and lianas (e.g. \"Asparagus scandens\", \"Rhoicissus tomentosa\"). Though not as rich in biodiversity as the Southern Cape Afrotemperate forests, these woodlands still contain a variety of endemic plants and animals which occur nowhere else in the world.\n\nThe major threats come from invasive alien plants such as Australian cheesewood, bugweed, black wattle, lantana, privet, and pine trees. \nWestern Cape Afrotemperate forests have a high socioeconomic value, due to their use for recreation such as hiking, their role in preserving the Western Cape's water supply, and their natural production of an enormous range of medicinal plants.\n\nBy far, the largest portion of Southern Afrotemperate Forest includes the enormous Knysna-Tsitsikamma forests. It extends from Mossel Bay in the Western Cape, eastwards into the Eastern Cape, nearly as far as Port Elizabeth.\n\nThis is a tall, dense, species-rich forest in a moist and warm temperate climate. The highest canopy is formed from the intermeshed crowns of the most massive trees. Understories are formed from medium-sized, multitrunked trees and the forest floor is home to a range of indigenous shrubs, bushes, ferns, and flowers. Enormous lianas and vines reach up to the canopy and between the branches and a variety of animals inhabit these woods. \nThis forest is very similar to Western Cape Afrotemperate Forest with a very high species overlap; however, it also has some lesser similarities with the Amatole mistbelt forests that lie further to the east in the Drakensberg mountain range.\nPreviously, large game was abundant, but today it is largely exterminated. A small population of elephants survives at Knysna.\n\nIt is often subdivided into three smaller vegetation types: the Southern Cape Mountain forest, Coastal-Platform, and Scarp forests.\n\nSome of the major indigenous tree species:\n\n\n\n"}
{"id": "1114533", "url": "https://en.wikipedia.org/wiki?curid=1114533", "title": "Tobias Furneaux", "text": "Tobias Furneaux\n\nCaptain Tobias Furneaux (21 August 1735 – 18 September 1781) was an English navigator and Royal Navy officer, who accompanied James Cook on his second voyage of exploration. He was one of the first men to circumnavigate the world in both directions, and later commanded a British vessel during the American Revolutionary War.\n\nFurneaux was born at Swilly House near Stoke Damerel, Plymouth Dock, son of William Furneaux (1696-1748) of Swilly, and Susanna Wilcocks (1698-1775). He entered the Royal Navy and was employed on the French and African coasts and in the West Indies during the latter part of the Seven Years' War (1760–1763). He served as second lieutenant of under Captain Samuel Wallis on the latter's voyage round the globe (August 1766 – May 1768) and due to Wallis being ill and confined to his cabin, Furneaux was the first to set foot on Tahiti, hoisting a pennant, turning a turf, and taking possession of the land in the name of His Majesty (25 June 1767).\n\nIn November 1771, Furneaux was given command of , which accompanied James Cook (in ) on his second voyage. On this expedition Furneaux was twice separated from his leader (8 February 1773 to 19 May 1773; and 22 October 1773 to 14 July 1774, the date of his return to England). On the former occasion he explored a great part of the south and east coasts of Van Diemen's Land (now Tasmania), and made the earliest British chart of the same. Most of his names here survive; Cook, visiting the shore-line on his third voyage, confirmed Furneaux's account and delineation of it, with certain minor criticisms and emendations, and named after him the Furneaux Group at the eastern entrance to Bass Strait, and the group now known as the Low Archipelago.\n\nAfter \"Adventure\" was finally separated from \"Resolution\" off New Zealand in October 1773, Furneaux returned home alone, bringing with him Omai of Ulaietea (Raiatea). This first South Sea Islander to travel to Great Britain returned to Tahiti with Cook on 12 August 1777.\n\nFurneaux was made a navigator in 1775. During the American Revolutionary War, he commanded HMS \"Syren\" in the British attack of 28 June 1776 upon Charleston, South Carolina. \"Syren\", with Furneaux in command, was wrecked near Point Judith, Rhode Island on 6 November 1777. The Rhode Island Marine Archaeology Project (RIMAP) has published a detailed history of the \"Syren\"'s activities in the American Revolution, as well as some of the original documents related to her loss, confirming 6 November as the correct date. By 10 November Furneaux and his crew were prisoners in Providence, Rhode Island, awaiting later exchange. RIMAP has also noted that the \"Syren\" is one of at least five ships associated with Captain Cook and his circumnavigating men with an historical connection to the State of Rhode Island.\n\nFurneaux died unmarried in 1781 and was buried in Stoke Damerel church where he had been christened.\n\n\n\n"}
{"id": "28750597", "url": "https://en.wikipedia.org/wiki?curid=28750597", "title": "Utilities Intermediaries Association", "text": "Utilities Intermediaries Association\n\nThe Utilities Intermediaries Association (UIA) is a trade body for UK Third Party Intermediaries (TPIs) in the business energy sector. Although it is a limited company, it runs as a not-for-profit organisation. TPIs are also known as energy brokers or consultants (Energy switching services in the UK), which facilitate the purchasing of electricity and gas between their clients and energy suppliers. They work on behalf of businesses, providing procurement services in exchange for commissions from the energy suppliers. This had led some TPIs to compromise their neutrality so they can earn the most commission through bias toward the suppliers that pay the most.\n\nThe UIA exists to work with the various organisations involved in the uk energy industry, with the aim of improving consumer confidence when undergoing a purchasing program for their energy. Membership is open to all TPIs and takes a minimum of 3 months to complete, providing they meet the standards of operation set by the association itself.\n\nWhilst the assessment process is rigorous, the UIA mark is not necessarily a 100% guarantee that a member's charges will remain totally transparent and unbiased. To do this would require a rigorous and ongoing assessment process in which members' websites are continually monitored and customers contacted on a random basis to ensure their energy broker is complying with the UIA Code of Practice. In the absence of any complaint by the public, the degree in which members' websites are actively monitoring for compliance to UIA Code of Practice is uncertain. Prior to engaging an energy broker it is important to request that the energy broker confirm they are independent, unbiased and exactly what their charges are.\n\nMembership fees for the UIA are currently not transparent and currently unpublished on the UIA website. Some smaller brokers find the cost of joining the UIA onerous and although smaller brokers may have transparent client fees and operate fairly independently, some choose not to join the UIA due to prohibitively expensive membership fees.\n\nFrom the UIA website:\n\nAll members of the UIA agree to adhere to a strict code of practice regarding the way in which they operate. Being a member allows the TPI to display the UIA 'boatmark' on their website or other communications, to give confidence to their clients and customers that they will follow the Code of Practice.\n\nThere are currently 30 full members as listed on the UIA's website.\n\nThe UIA along with the Association of Convenience Stores (ACS) and the Forum of Private Business condemned the decision of Ofgem not to outlaw rollover energy contracts, which would tie business users into another contract if they did not terminate their current contract within a specified period. On 18 January 2010, Ofgem introduced new rules to protect smaller businesses, known as micro-businesses. The rule, known as Condition 7A, protects these businesses in 3 ways:\n\n"}
{"id": "32496", "url": "https://en.wikipedia.org/wiki?curid=32496", "title": "Vacuum tube", "text": "Vacuum tube\n\nIn electronics, a vacuum tube, an electron tube, or valve (British usage) or, colloquially, a tube (North America), is a device that controls electric current flow in a high vacuum between electrodes to which an electric potential difference has been applied.\n\nThe type known as a thermionic tube or thermionic valve uses the phenomenon of thermionic emission of electrons from a heated cathode and is used for a number of fundamental electronic functions such as signal amplification and current rectification.\nNon-thermionic types, such as a vacuum phototube however, achieve electron emission through the photoelectric effect, and are used for such as the detection of light levels. In both types, the electrons are accelerated from the cathode to the anode by the electric field in the tube.\n\nThe simplest vacuum tube, the diode invented in 1904 by John Ambrose Fleming, contains only a heated electron-emitting cathode and an anode. Current can only flow in one direction through the device from the cathode to the anode. Adding one or more control grids within the tube allows the current between the cathode and anode to be controlled by the voltage on the grid or grids. These devices became a key component of electronic circuits for the first half of the twentieth century. They were crucial to the development of radio, television, radar, sound recording and reproduction, long distance telephone networks, and analogue and early digital computers. Although some applications had used earlier technologies such as the spark gap transmitter for radio or mechanical computers for computing, it was the invention of the thermionic vacuum tube that made these technologies widespread and practical, and created the discipline of electronics.\n\nIn the 1940s the invention of semiconductor devices made it possible to produce solid-state devices, which are smaller, more efficient, reliable and durable, and cheaper than thermionic tubes. From the mid-1960s, thermionic tubes were then being replaced by the transistor. However, the cathode-ray tube (CRT) remained the basis for television monitors and oscilloscopes until the early 21st century. Thermionic tubes still have some applications, such as the magnetron used in microwave ovens, and certain high-frequency amplifiers.\n\nNot all electronic circuit valves/electron tubes are vacuum tubes. Gas-filled tubes are similar devices, but containing a gas, typically at low pressure, which exploit phenomena related to electric discharge in gases, usually without a heater.\n\nOne classification of thermionic vacuum tubes is by the number of active electrodes. A device with two active elements is a diode, usually used for rectification. Devices with three elements are triodes used for amplification and switching. Additional electrodes create tetrodes, pentodes, and so forth, which have multiple additional functions made possible by the additional controllable electrodes.\n\nOther classifications are:\n\nTubes have different functions, such as cathode ray tubes which create a beam of electrons for display purposes (such as the television picture tube) in addition to more specialized functions such as electron microscopy and electron beam lithography. X-ray tubes are also vacuum tubes. Phototubes and photomultipliers rely on electron flow through a vacuum, though in those cases electron emission from the cathode depends on energy from photons rather than thermionic emission. Since these sorts of \"vacuum tubes\" have functions other than electronic amplification and rectification they are described in their own articles.\n\nA vacuum tube consists of two or more electrodes in a vacuum inside an airtight envelope. Most tubes have glass envelopes with a glass-to-metal seal based on kovar sealable borosilicate glasses, though ceramic and metal envelopes (atop insulating bases) have been used. The electrodes are attached to leads which pass through the envelope via an airtight seal. Most vacuum tubes have a limited lifetime, due to the filament or heater burning out or other failure modes, so they are made as replaceable units; the electrode leads connect to pins on the tube's base which plug into a tube socket. Tubes were a frequent cause of failure in electronic equipment, and consumers were expected to be able to replace tubes themselves. In addition to the base terminals, some tubes had an electrode terminating at a top cap. The principal reason for doing this was to avoid leakage resistance through the tube base, particularly for the high impedance grid input. The bases were commonly made with phenolic insulation which performs poorly as an insulator in humid conditions. Other reasons for using a top cap include improving stability by reducing grid-to-anode capacitance, improved high-frequency performance, keeping a very high plate voltage away from lower voltages, and accommodating one more electrode than allowed by the base. There was even an occasional design that had two top cap connections.\n\nThe earliest vacuum tubes evolved from incandescent light bulbs, containing a filament sealed in an evacuated glass envelope. When hot, the filament releases electrons into the vacuum, a process called thermionic emission, originally known as the \"Edison Effect\". A second electrode, the anode or \"plate\", will attract those electrons if it is at a more positive voltage. The result is a net flow of electrons from the filament to plate. However, electrons cannot flow in the reverse direction because the plate is not heated and does not emit electrons. The filament (\"cathode\") has a dual function: it emits electrons when heated; and, together with the plate, it creates an electric field due to the potential difference between them. Such a tube with only two electrodes is termed a diode, and is used for rectification. Since current can only pass in one direction, such a diode (or \"rectifier\") will convert alternating current (AC) to pulsating DC. Diodes can therefore be used in a DC power supply, as a demodulator of amplitude modulated (AM) radio signals and for similar functions.\n\nEarly tubes used the filament as the cathode; this is called a \"directly heated\" tube. Most modern tubes are \"indirectly heated\" by a \"heater\" element inside a metal tube that is the cathode. The heater is electrically isolated from the surrounding cathode and simply serves to heat the cathode sufficiently for thermionic emission of electrons. The electrical isolation allows all the tubes' heaters to be supplied from a common circuit (which can be AC without inducing hum) while allowing the cathodes in different tubes to operate at different voltages. H. J. Round invented the indirectly heated tube around 1913.\n\nThe filaments require constant and often considerable power, even when amplifying signals at the microwatt level. Power is also dissipated when the electrons from the cathode slam into the anode (plate) and heat it; this can occur even in an idle amplifier due to quiescent currents necessary to ensure linearity and low distortion. In a power amplifier, this heating can be considerable and can destroy the tube if driven beyond its safe limits. Since the tube contains a vacuum, the anodes in most small and medium power tubes are cooled by radiation through the glass envelope. In some special high power applications, the anode forms part of the vacuum envelope to conduct heat to an external heat sink, usually cooled by a blower, or water-jacket.\n\nKlystrons and magnetrons often operate their anodes (called collectors in klystrons) at ground potential to facilitate cooling, particularly with water, without high-voltage insulation. These tubes instead operate with high negative voltages on the filament and cathode.\n\nExcept for diodes, additional electrodes are positioned between the cathode and the plate (anode). These electrodes are referred to as grids as they are not solid electrodes but sparse elements through which electrons can pass on their way to the plate. The vacuum tube is then known as a triode, tetrode, pentode, etc., depending on the number of grids. A triode has three electrodes: the anode, cathode, and one grid, and so on. The first grid, known as the control grid, (and sometimes other grids) transforms the diode into a \"voltage-controlled device\": the voltage applied to the control grid affects the current between the cathode and the plate. When held negative with respect to the cathode, the control grid creates an electric field which repels electrons emitted by the cathode, thus reducing or even stopping the current between cathode and anode. As long as the control grid is negative relative to the cathode, essentially no current flows into it, yet a change of several volts on the control grid is sufficient to make a large difference in the plate current, possibly changing the output by hundreds of volts (depending on the circuit). The solid-state device which operates most like the pentode tube is the junction field-effect transistor (JFET), although vacuum tubes typically operate at over a hundred volts, unlike most semiconductors in most applications.\n\nThe 19th century saw increasing research with evacuated tubes, such as the Geissler and Crookes tubes. The many scientists and inventors who experimented with such tubes include Thomas Edison, Eugen Goldstein, Nikola Tesla, and Johann Wilhelm Hittorf. With the exception of early light bulbs, such tubes were only used in scientific research or as novelties. The groundwork laid by these scientists and inventors, however, was critical to the development of subsequent vacuum tube technology.\n\nAlthough thermionic emission was originally reported in 1873 by Frederick Guthrie, it was Thomas Edison's apparently independent discovery of the phenomenon in 1883 that became well known. Although Edison was aware of the unidirectional property of current flow between the filament and the anode, his interest (and patent) concentrated on the sensitivity of the anode current to the current through the filament (and thus filament temperature). Little practical use was ever made of this property (however early radios often implemented volume controls through varying the filament current of amplifying tubes). It was only years later that John Ambrose Fleming utilized the rectifying property of the diode tube to detect (demodulate) radio signals, a substantial improvement on the early cat's-whisker detector already used for rectification.\n\nHowever actual amplification by a vacuum tube only became practical with Lee De Forest's 1907 invention of the three-terminal \"audion\" tube, a crude form of what was to become the triode. Being essentially the first electronic amplifier, such tubes were instrumental in long-distance telephony (such as the first coast-to-coast telephone line in the US) and public address systems, and introduced a far superior and versatile technology for use in radio transmitters and receivers. The electronics revolution of the 20th century arguably began with the invention of the triode vacuum tube.\n\nThe English physicist John Ambrose Fleming worked as an engineering consultant for firms including Edison Swan, Edison Telephone and the Marconi Company. In 1904, as a result of experiments conducted on Edison effect bulbs imported from the United States, he developed a device he called an \"oscillation valve\" (because it passes current in only one direction). The heated filament, was capable of thermionic emission of electrons that would flow to the \"plate\" (or \"anode\") when it was at a positive voltage with respect to the heated cathode. Electrons, however, could not pass in the reverse direction because the plate was not heated and thus not capable of thermionic emission of electrons.\n\nLater known as the Fleming valve, it could be used as a rectifier of alternating current and as a radio wave detector. This greatly improved the crystal set which rectified the radio signal using an early solid-state diode based on a crystal and a so-called cat's whisker, an adjustable point contact. Unlike modern semiconductors, such a diode required painstaking adjustment of the contact to the crystal in order for it to rectify.\n\nThe tube was relatively immune to vibration, and thus vastly superior on shipboard duty, particularly for navy ships with the shock of weapon fire commonly knocking the sensitive but delicate galena off its sensitive point (the tube was in general no more sensitive as a radio detector, but was adjustment free). The diode tube was a reliable alternative for detecting radio signals.\n\nAs electronic engineering advanced, notably during World War II, this function of a diode came to be considered as one type of demodulation. While firmly established by history, the term \"detector\" is not of itself descriptive, and should be considered outdated.\n\nHigher power diode tubes or \"power rectifiers\" found their way into power supply applications until they were eventually replaced first by selenium, and later, by silicon rectifiers in the 1960s.\n\nOriginally, the only use for tubes in radio circuits was for rectification, not amplification. In 1906, Robert von Lieben filed for a patent for a cathode ray tube which included magnetic deflection. This could be used for amplifying audio signals and was intended for use in telephony equipment. He would later help refine the triode vacuum tube.\n\nHowever, Lee De Forest is credited with inventing the triode tube in 1907 while experimenting to improve his original (diode) Audion. By placing an additional electrode between the filament (cathode) and plate (anode), he discovered the ability of the resulting device to amplify signals. As the voltage applied to the control grid (or simply \"grid\") was lowered from the cathode's voltage to somewhat more negative voltages, the amount of current from the filament to the plate would be reduced.\n\nThe negative electrostatic field created by the grid in the vicinity of the cathode would inhibit passage of emitted electrons and reduce the current to the plate. Thus, a few volt difference at the grid would make a large change in the plate current and could lead to a much larger voltage change at the plate; the result was voltage and power amplification. In 1908, De Forest was granted a patent () for such a three-electrode version of his original Audion for use as an electronic amplifier in radio communications. This eventually became known as the triode.\nDe Forest's original device was made with conventional vacuum technology. The vacuum was not a \"hard vacuum\" but rather left a very small amount of residual gas. The physics behind the device's operation was also not settled. The residual gas would cause a blue glow (visible ionization) when the plate voltage was high (above about 60 volts). In 1912, De Forest brought the Audion to Harold Arnold in AT&T's engineering department. Arnold recommended that AT&T purchase the patent, and AT&T followed his recommendation. Arnold developed high-vacuum tubes which were tested in the summer of 1913 on AT&T's long distance network. The high-vacuum tubes could operate at high plate voltages without a blue glow.\n\nFinnish inventor Eric Tigerstedt significantly improved on the original triode design in 1914, while working on his sound-on-film process in Berlin, Germany. Tigerstedt's innovation was to make the electrodes concentric cylinders with the cathode at the centre, thus greatly increasing the collection of emitted electrons at the anode.\n\nIrving Langmuir at the General Electric research laboratory (Schenectady, New York) had improved Wolfgang Gaede's high-vacuum diffusion pump and used it to settle the question of thermionic emission and conduction in a vacuum. Consequently, General Electric started producing hard vacuum triodes (which were branded Pliotrons) in 1915. Langmuir patented the hard vacuum triode, but De Forest and AT&T successfully asserted priority and invalidated the patent.\n\nPliotrons were closely followed by the French type 'TM' and later the English type 'R' which were in widespread use by the allied military by 1916. Historically, vacuum levels in production vacuum tubes typically ranged from 10 µPa down to 10 nPa.\n\nThe triode and its derivatives (tetrodes and pentodes) are transconductance devices, in which the controlling signal applied to the grid is a \"voltage\", and the resulting amplified signal appearing at the anode is a \"current\". Compare this to the behavior of the bipolar junction transistor, in which the controlling signal is a current and the output is also a current.\n\nFor vacuum tubes, transconductance or mutual conductance () is defined as the change in the plate(anode)/cathode current divided by the corresponding change in the grid to cathode voltage, with a constant plate(anode) to cathode voltage. Typical values of for a small-signal vacuum tube are 1 to 10 millisiemens. It is one of the three 'constants' of a vacuum tube, the other two being its gain μ and plate resistance or . The Van der Bijl equation defines their relationship as follows: formula_1\n\nThe non-linear operating characteristic of the triode caused early tube audio amplifiers to exhibit harmonic distortion at low volumes. Plotting plate current as a function of applied grid voltage, it was seen that there was a range of grid voltages for which the transfer characteristics were approximately linear.\n\nTo use this range, a negative bias voltage had to be applied to the grid to position the DC operating point in the linear region. This was called the idle condition, and the plate current at this point the \"idle current\". The controlling voltage was superimposed onto the bias voltage, resulting in a linear variation of plate current in response to both positive and negative variation of the input voltage around that point.\n\nThis concept is called \"grid bias\". Many early radio sets had a third battery called the \"C battery\" (unrelated to the present-day C cell, for which the letter denotes its size and shape). The C battery's positive terminal was connected to the cathode of the tubes (or \"ground\" in most circuits) and whose negative terminal supplied this bias voltage to the grids of the tubes.\n\nLater circuits, after tubes were made with heaters isolated from their cathodes, used cathode biasing, avoiding the need for a separate negative power supply. For cathode biasing, a relatively low-value resistor is connected between the cathode and ground. This makes the cathode positive with respect to the grid, which is at ground potential for DC.\n\nHowever C batteries continued to be included in some equipment even when the \"A\" and \"B\" batteries had been replaced by power from the AC mains. That was possible because there was essentially no current draw on these batteries; they could thus last for many years (often longer than all the tubes) without requiring replacement.\n\nWhen triodes were first used in radio transmitters and receivers, it was found that tuned amplification stages had a tendency to oscillate unless their gain was very limited. This was due to the parasitic capacitance between the plate (the amplifier's output) and the control grid (the amplifier's input), known as the Miller capacitance.\n\nEventually the technique of \"neutralization\" was developed whereby the RF transformer connected to the plate (anode) would include an additional winding in the opposite phase. This winding would be connected back to the grid through a small capacitor, and when properly adjusted would cancel the Miller capacitance. This technique was employed and led to the success of the Neutrodyne radio during the 1920s.\nHowever, neutralization required careful adjustment and proved unsatisfactory when used over a wide range of frequencies.\n\nTo combat the stability problems and limited voltage gain due to the Miller effect, the physicist Walter H. Schottky invented the tetrode tube in 1919. He showed that the addition of a second grid, located between the control grid and the plate (anode), known as the \"screen grid\", could solve these problems. (\"Screen\" in this case refers to electrical \"screening\" or shielding, not physical construction: all \"grid\" electrodes in between the cathode and plate are \"screens\" of some sort rather than solid electrodes since they must allow for the passage of electrons directly from the cathode to the plate). A positive voltage slightly lower than the plate (anode) voltage was applied to it, and was bypassed (for high frequencies) to ground with a capacitor. This arrangement decoupled the anode and the control grid, essentially eliminating the Miller capacitance and its associated problems. Consequently, higher voltage gains from a single tube became possible, reducing the number of tubes required in many circuits. This two-grid tube is called a \"tetrode\", meaning four active electrodes, and was common by 1926.\nHowever, the tetrode had one new problem. In any tube, electrons strike the anode with sufficient energy to cause the emission of electrons from its surface. In a triode this so-called secondary emission of electrons is not important since they are simply re-captured by the more positive anode (plate). But in a tetrode they can be captured by the screen grid (thus also acting as an anode) since it is also at a high voltage, thus robbing them from the plate current and reducing the amplification of the device. Since secondary electrons can outnumber the primary electrons, in the worst case, particularly as the plate voltage dips below the screen voltage, the plate current can decrease with increasing plate voltage. This is the so-called \"tetrode kink\" and is an example of negative resistance which can itself cause instability. The otherwise undesirable negative resistance was exploited to produce a simple oscillator circuit only requiring connection of the plate to a resonant LC circuit to oscillate; this was effective over a wide frequency range. The so-called dynatron oscillator thus operated on the same principle of negative resistance as the tunnel diode oscillator many years later. Another undesirable consequence of secondary emission is that in extreme cases enough charge can flow to the screen grid to overheat and destroy it. Later tetrodes had anodes treated to reduce secondary emission; earlier ones such as the type 77 sharp-cutoff pentode connected as a tetrode made better dynatrons.\n\nThe solution was to add another grid between the screen grid and the main anode, called the suppressor grid (since it suppressed secondary emission current toward the screen grid). This grid was held at the cathode (or \"ground\") voltage and its negative voltage (relative to the anode) electrostatically repelled secondary electrons so that they would be collected by the anode after all. This three-grid tube is called a pentode, meaning five electrodes. The pentode was invented in 1926 by Bernard D. H. Tellegen and became generally favored over the simple tetrode. Pentodes are made in two classes: those with the suppressor grid wired internally to the cathode (e.g. EL84/6BQ5) and those with the suppressor grid wired to a separate pin for user access (e.g. 803, 837). An alternative solution for power applications is the beam tetrode or \"beam power tube\", discussed below.\n\nSuperheterodyne receivers require a local oscillator and mixer, combined in the function of a single pentagrid converter tube. Various alternatives such as using a combination of a triode with a hexode and even an octode have been used for this purpose. The additional grids include both control grids (at a low potential) and screen grids (at a high voltage). Many designs used such a screen grid as an additional anode to provide feedback for the oscillator function, whose current was added to that of the incoming radio frequency signal. The pentagrid converter thus became widely used in AM receivers, including the miniature tube version of the \"All American Five\". Octodes, such as the 7A8, were rarely used in the United States, but much more common in Europe, particularly in battery operated radios where the lower power consumption was an advantage.\n\nTo further reduce the cost and complexity of radio equipment, two separate structures (triode and pentode for instance) could be combined in the bulb of a single \"multisection tube\". An early example was the Loewe 3NF. This 1920s device had three triodes in a single glass envelope together with all the fixed capacitors and resistors required to make a complete radio receiver. As the Loewe set had only one tube socket, it was able to substantially undercut the competition, since, in Germany, state tax was levied by the number of sockets. However, reliability was compromised, and production costs for the tube were much greater. In a sense, these were akin to integrated circuits. In the United States, Cleartron briefly produced the \"Multivalve\" triple triode for use in the Emerson Baby Grand receiver. This Emerson set also had a single tube socket, but because it used a four-pin base, the additional element connections were made on a \"mezzanine\" platform at the top of the tube base.\n\nBy 1940 multisection tubes had become commonplace. There were constraints, however, due to patents and other licensing considerations (see British Valve Association). Constraints due to the number of external pins (leads) often forced the functions to share some of those external connections such as their cathode connections (in addition to the heater connection). The RCA Type 55 was a double diode triode used as a detector, automatic gain control rectifier and audio preamplifier in early AC powered radios. These sets often included the 53 Dual Triode Audio Output. Another early type of multi-section tube, the 6SN7, is a \"dual triode\" which performs the functions of two triode tubes, while taking up half as much space and costing less.\nThe 12AX7 is a dual \"high mu\" (high voltage gain) triode in a miniature enclosure, and became widely used in audio signal amplifiers, instruments, and guitar amplifiers.\n\nThe introduction of the miniature tube base (see below) which could have 9 pins, more than previously available, allowed other multi-section tubes to be introduced, such as the 6GH8/ECF82 triode-pentode, quite popular in television receivers. The desire to include even more functions in one envelope resulted in the General Electric Compactron which had 12 pins. A typical example, the 6AG11, contained two triodes and two diodes.\n\nSome otherwise conventional tubes do not fall into standard categories; the 6AR8, 6JH8 and 6ME8 had several common grids, followed by a pair of beam deflection electrodes which deflected the current towards either of two anodes. It was sometimes known as the 'sheet beam' tube, and was used in some color TV sets for color demodulation. The similar 7360 was popular as a balanced SSB (de)modulator.\n\nThe beam power tube is usually a tetrode with the addition of beam-forming electrodes, which take the place of the suppressor grid. These angled plates (not to be confused with the \"anode\") focus the electron stream onto certain spots on the anode which can withstand the heat generated by the impact of massive numbers of electrons, while also providing pentode behavior. The positioning of the elements in a beam power tube uses a design called \"critical-distance geometry\", which minimizes the \"tetrode kink\", plate to control grid capacitance, screen grid current, and secondary emission from the anode, thus increasing power conversion efficiency. The control grid and screen grid are also wound with the same pitch, or number of wires per inch. The two grids are positioned so that the control grid creates \"sheets\" of electrons which pass between the screen-grid wires. They're aligned to be equidistant from, say, the bottom of the tube.\n\nAligning the grid wires also helps to reduce screen current, which represents wasted energy. This design helps to overcome some of the practical barriers to designing high-power, high-efficiency power tubes. EMI engineers Cabot Bull and Sidney Rodda developed the design which became the 6L6, the first popular beam power tube, introduced by RCA in 1936 and later corresponding tubes in Europe the KT66, KT77 and KT88 made by the Marconi-Osram Valve subsidiary of GEC (the KT standing for \"Kinkless Tetrode\").\n\n\"Pentode operation\" of beam power tubes is often described in manufacturers' handbooks and data sheets, resulting in some confusion in terminology.\nThey are not pentodes, of course.\n\nVariations of the 6L6 design are still widely used in tube guitar amplifiers, making it one of the longest-lived electronic device families in history. Similar design strategies are used in the construction of large ceramic power tetrodes used in radio transmitters.\n\nBeam power tubes can be connected as triodes for improved audio tonal quality but in triode mode deliver significantly reduced power output.\n\nGas-filled tubes such as discharge tubes and cold cathode tubes are not \"hard\" vacuum tubes, though are always filled with gas at less than sea-level atmospheric pressure. Types such as the voltage-regulator tube and thyratron resemble hard vacuum tubes and fit in sockets designed for vacuum tubes. Their distinctive orange, red, or purple glow during operation indicates the presence of gas; electrons flowing in a vacuum do not produce light within that region. These types may still be referred to as \"electron tubes\" as they do perform electronic functions. High-power rectifiers use mercury vapor to achieve a lower forward voltage drop than high-vacuum tubes.\n\nEarly tubes used a metal or glass envelope atop an insulating bakelite base. In 1938 a technique was developed to use an all-glass construction with the pins fused in the glass base of the envelope. This was used in the design of a much smaller tube outline, known as the miniature tube, having 7 or 9 pins. Making tubes smaller reduced the voltage where they could safely operate, and also reduced the power dissipation of the filament. Miniature tubes became predominant in consumer applications such as radio receivers and hi-fi amplifiers. However the larger older styles continued to be used especially as higher power rectifiers, in higher power audio output stages and as transmitting tubes.Subminiature tubes with a size roughly that of half a cigarette were used in hearing-aid amplifiers. These tubes did not have pins plugging into a socket but were soldered in place. The \"acorn tube\" (named due to its shape) was also very small, as was the metal-cased RCA nuvistor from 1959, about the size of a thimble. The nuvistor was developed to compete with the early transistors and operated at higher frequencies than those early transistors could. The small size supported especially high-frequency operation; nuvistors were used in aircraft radio transceivers, UHF television tuners, and some HiFi FM radio tuners (Sansui 500A) until replaced by high-frequency capable transistors.\n\nThe earliest vacuum tubes strongly resembled incandescent light bulbs and were made by lamp manufacturers, who had the equipment needed to manufacture glass envelopes and the vacuum pumps required to evacuate the enclosures. De Forest used Heinrich Geissler's mercury displacement pump, which left behind a partial vacuum. The development of the diffusion pump in 1915 and improvement by Irving Langmuir led to the development of high-vacuum tubes. After World War I, specialized manufacturers using more economical construction methods were set up to fill the growing demand for broadcast receivers. Bare tungsten filaments operated at a temperature of around 2200 °C. The development of oxide-coated filaments in the mid-1920s reduced filament operating temperature to a dull red heat (around 700 °C), which in turn reduced thermal distortion of the tube structure and allowed closer spacing of tube elements. This in turn improved tube gain, since the gain of a triode is inversely proportional to the spacing between grid and cathode. Bare tungsten filaments remain in use in small transmitting tubes but are brittle and tend to fracture if handled roughly – e.g. in the postal services. These tubes are best suited to stationary equipment where impact and vibration is not present.\n\nThe desire to power electronic equipment using AC mains power faced a difficulty with respect to the powering of the tubes' filaments, as these were also the cathode of each tube. Powering the filaments directly from a power transformer introduced mains-frequency (50 or 60 Hz) hum into audio stages. The invention of the \"equipotential cathode\" reduced this problem, with the filaments being powered by a balanced AC power transformer winding having a grounded center tap.\n\nA superior solution, and one which allowed each cathode to \"float\" at a different voltage, was that of the indirectly heated cathode: a cylinder of oxide-coated nickel acted as electron-emitting cathode, and was electrically isolated from the filament inside it. Indirectly heated cathodes enable the cathode circuit to be separated from the heater circuit. The filament, no longer electrically connected to the tube's electrodes, became simply known as a \"heater\", and could as well be powered by AC without any introduction of hum. In the 1930s indirectly heated cathode tubes became widespread in equipment using AC power. Directly heated cathode tubes continued to be widely used in battery-powered equipment as their filaments required considerably less power than the heaters required with indirectly heated cathodes.\n\nTubes designed for high gain audio applications may have twisted heater wires to cancel out stray electric fields, fields that could induce objectionable hum into the program material.\n\nHeaters may be energized with either alternating current (AC) or direct current (DC). DC is often used where low hum is required.\n\nVacuum tubes used as switches made electronic computing possible for the first time, but the cost and relatively short mean time to failure of tubes were limiting factors. \"The common wisdom was that valves—which, like light bulbs, contained a hot glowing filament—could never be used satisfactorily in large numbers, for they were unreliable, and in a large installation too many would fail in too short a time\". Tommy Flowers, who later designed \"Colossus\", \"discovered that, so long as valves were switched on and left on, they could operate reliably for very long periods, especially if their 'heaters' were run on a reduced current\". In 1934 Flowers built a successful experimental installation using over 3,000 tubes in small independent modules; when a tube failed, it was possible to switch off one module and keep the others going, thereby reducing the risk of another tube failure being caused; this installation was accepted by the Post Office (who operated telephone exchanges). Flowers was also a pioneer of using tubes as very fast (compared to electromechanical devices) electronic switches. Later work confirmed that tube unreliability was not as serious an issue as generally believed; the 1946 ENIAC, with over 17,000 tubes, had a tube failure (which took 15 minutes to locate) on average every two days. The quality of the tubes was a factor, and the diversion of skilled people during the Second World War lowered the general quality of tubes. During the war Colossus was instrumental in breaking German codes. After the war, development continued with tube-based computers including, military computers ENIAC and Whirlwind, the Ferranti Mark 1 (the first commercially available electronic computer), and UNIVAC I, also available commercially.\n\nFlowers's Colossus and its successor Colossus Mk2 were built by the British during World War II to substantially speed up the task of breaking the German high level Lorenz encryption. Using about 1,500 vacuum tubes (2,400 for Mk2), Colossus replaced an earlier machine based on relay and switch logic (the Heath Robinson). Colossus was able to break in a matter of hours messages that had previously taken several weeks; it was also much more reliable. Colossus was the first use of vacuum tubes \"working in concert\" on such a large scale for a single machine.\n\nOnce Colossus was built and installed, it ran continuously, powered by dual redundant diesel generators, the wartime mains supply being considered too unreliable. The only time it was switched off was for conversion to Mk2, which added more tubes. Another nine Colossus Mk2s were built. Each Mk2 consumed 15 kilowatts; most of the power was for the tube heaters.\n\nA Colossus reconstruction was switched on in 1996; it was upgraded to Mk2 configuration in 2004; it found the key for a wartime German ciphertext in 2007.\n\nTo meet the reliability requirements of the 1951 US digital computer Whirlwind, \"special-quality\" tubes with extended life, and a long-lasting cathode in particular, were produced. The problem of short lifetime was traced to evaporation of silicon, used in the tungsten alloy to make the heater wire easier to draw. Elimination of silicon from the heater wire alloy (and more frequent replacement of the wire drawing dies) allowed production of tubes that were reliable enough for the Whirlwind project. The tubes developed for Whirlwind were later used in the giant SAGE air-defense computer system. SAGE computers were dual installations, with one operating, and the other in standby. To locate potential tube failures in the standby computer, heater voltages were reduced, which caused failures of tubes which would otherwise fail in service. These computers continued in service years after other tube computers had been superseded.\n\nHigh-purity nickel tubing and cathode coatings free of materials that can poison emission (such as silicates and aluminum) also contribute to long cathode life. The first such \"computer tube\" was Sylvania's 7AK7 of 1948. Computers were the first tube devices to run tubes at cutoff (enough negative grid voltage to make them cease conduction) for quite-extended periods of time. When their grids became less negative, they failed to conduct. While hot but non-conductive, an insulating layer (\"cathode interface\") developed between the nickel sleeve and the oxide coating. What was described above cured this problem.\n\nBy the late 1950s it was routine for special-quality small-signal tubes to last for hundreds of thousands of hours, if operated conservatively. This increased reliability also made mid-cable amplifiers in submarine cables possible.\n\nA considerable amount of heat is produced when tubes operate, both from the filament (heater) but also from the stream of electrons bombarding the plate. In power amplifiers this source of heat will exceed the power due to cathode heating.\nA few types of tube permit operation with the anodes at a dull red heat; in other types, red heat indicates severe overload.\n\nThe requirements for heat removal can significantly change the appearance of high-power vacuum tubes. High power audio amplifiers and rectifiers required larger envelopes to dissipate heat. Transmitting tubes could be much larger still.\n\nHeat escapes the device by black body radiation from the anode (plate) as infrared radiation, and by convection of air over the tube envelope. Convection is not possible inside most tubes since the anode is surrounded by vacuum.\n\nTubes which generate relatively little heat, such as the 1.4-volt filament directly heated tubes designed for use in battery-powered equipment, often have shiny metal anodes. 1T4, 1R5 and 1A7 are examples. Gas-filled tubes such as thyratrons may also use a shiny metal anode, since the gas present inside the tube allows for heat convection from the anode to the glass enclosure.\n\nThe anode is often treated to make its surface emit more infrared energy. High-power amplifier tubes are designed with external anodes which can be cooled by convection, forced air or circulating water. The water-cooled 80 kg, 1.25 MW 8974 is among the largest commercial tubes available today.\n\nIn a water-cooled tube, the anode voltage appears directly on the cooling water surface, thus requiring the water to be an electrical insulator to prevent high voltage leakage through the cooling water to the radiator system. Water as usually supplied has ions which conduct electricity; deionized water, a good insulator, is required. Such systems usually have a built-in water-conductance monitor which will shut down the high-tension supply if the conductance becomes too high.\n\nThe screen grid may also generate considerable heat. Limits to screen grid dissipation, in addition to plate dissipation, are listed for power devices. If these are exceeded then tube failure is likely.\n\nMost modern tubes have glass envelopes, but metal, fused quartz (silica) and ceramic have also been used. A first version of the 6L6 used a metal envelope sealed with glass beads, while a glass disk fused to the metal was used in later versions. Metal and ceramic are used almost exclusively for power tubes above 2 kW dissipation. The nuvistor was a modern receiving tube using a very small metal and ceramic package.\n\nThe internal elements of tubes have always been connected to external circuitry via pins at their base which plug into a socket. Subminiature tubes were produced using wire leads rather than sockets, however these were restricted to rather specialized applications. In addition to the connections at the base of the tube, many early triodes connected the grid using a metal cap at the top of the tube; this reduces stray capacitance between the grid and the plate leads. Tube caps were also used for the plate (anode) connection, particularly in transmitting tubes and tubes using a very high plate voltage.\n\nHigh-power tubes such as transmitting tubes have packages designed more to enhance heat transfer. In some tubes, the metal envelope is also the anode. The 4CX1000A is an external anode tube of this sort. Air is blown through an array of fins attached to the anode, thus cooling it. Power tubes using this cooling scheme are available up to 150 kW dissipation. Above that level, water or water-vapor cooling are used. The highest-power tube currently available is the Eimac , a forced water-cooled power tetrode capable of dissipating 2.5 megawatts. By comparison, the largest power transistor can only dissipate about 1 kilowatt.\n\nThe generic name \"[thermionic] valve\" used in the UK derives from the unidirectional current flow allowed by the earliest device, the thermionic diode emitting electrons from a heated filament, by analogy with a non-return valve in a water pipe. The US names \"vacuum tube\", \"electron tube\", and \"thermionic tube\" all simply describe a tubular envelope which has been evacuated (\"vacuum\"), has a heater, and controls electron flow.\n\nIn many cases manufacturers and the military gave tubes designations which said nothing about their purpose (e.g., 1614). In the early days some manufacturers used proprietary names which might convey some information, but only about their products; the KT66 and KT88 were \"Kinkless Tetrodes\". Later, consumer tubes were given names which conveyed some information, with the same name often used generically by several manufacturers. In the US, Radio Electronics Television Manufacturers' Association (RETMA) designations comprise a number, followed by one or two letters, and a number. The first number is the (rounded) heater voltage; the letters designate a particular tube but say nothing about its structure; and the final number is the total number of electrodes (without distinguishing between, say, a tube with many electrodes, or two sets of electrodes in a single envelope—a double triode, for example). For example, the 12AX7 is a double triode (two sets of three electrodes plus heater) with a 12.6V heater (which, as it happens, can also be connected to run from 6.3V). The \"AX\" has no meaning other than to designate this particular tube according to its characteristics. Similar, but not identical, tubes are the 12AD7, 12AE7...12AT7, 12AU7, 12AV7, 12AW7 (rare!), 12AY7, and the 12AZ7.\n\nA system widely used in Europe known as the Mullard–Philips tube designation, also extended to transistors, uses a letter, followed by one or more further letters, and a number. The type designator specifies the heater voltage or current (one letter), the functions of all sections of the tube (one letter per section), the socket type (first digit), and the particular tube (remaining digits). For example, the ECC83 (equivalent to the 12AX7) is a 6.3V (E) double triode (CC) with a miniature base (8). In this system special-quality tubes (e.g., for long-life computer use) are indicated by moving the number immediately after the first letter: the E83CC is a special-quality equivalent of the ECC83, the E55L a power pentode with no consumer equivalent.\n\nSome special-purpose tubes are constructed with particular gases in the envelope. For instance, voltage-regulator tubes contain various inert gases such as argon, helium or neon, which will ionize at predictable voltages. The thyratron is a special-purpose tube filled with low-pressure gas or mercury vapor. Like vacuum tubes, it contains a hot cathode and an anode, but also a control electrode which behaves somewhat like the grid of a triode. When the control electrode starts conduction, the gas ionizes, after which the control electrode can no longer stop the current; the tube \"latches\" into conduction. Removing anode (plate) voltage lets the gas de-ionize, restoring its non-conductive state.\n\nSome thyratrons can carry large currents for their physical size. One example is the miniature type 2D21, often seen in 1950s jukeboxes as control switches for relays. A cold-cathode version of the thyratron, which uses a pool of mercury for its cathode, is called an ignitron; some can switch thousands of amperes. Thyratrons containing hydrogen have a very consistent time delay between their turn-on pulse and full conduction; they behave much like modern silicon-controlled rectifiers, also called thyristors due to their functional similarity to thyratrons. Hydrogen thyratrons have long been used in radar transmitters.\n\nA specialized tube is the krytron, which is used for rapid high-voltage switching. Krytrons are used to initiate the detonations used to set off a nuclear weapon; krytrons are heavily controlled at an international level.\n\nX-ray tubes are used in medical imaging among other uses. X-ray tubes used for continuous-duty operation in fluoroscopy and CT imaging equipment may use a focused cathode and a rotating anode to dissipate the large amounts of heat thereby generated. These are housed in an oil-filled aluminium housing to provide cooling.\n\nThe photomultiplier tube is an extremely sensitive detector of light, which uses the photoelectric effect and secondary emission, rather than thermionic emission, to generate and amplify electrical signals. Nuclear medicine imaging equipment and liquid scintillation counters use photomultiplier tube arrays to detect low-intensity scintillation due to ionizing radiation.\n\nBatteries provided the voltages required by tubes in early radio sets. Three different voltages were generally required, using three different batteries designated as the A, B, and C battery. The \"A\" battery or LT (low-tension) battery provided the filament voltage. Tube heaters were designed for single, double or triple-cell lead-acid batteries, giving nominal heater voltages of 2 V, 4 V or 6 V. In portable radios, dry batteries were sometimes used with 1.5 or 1 V heaters. Reducing filament consumption improved the life span of batteries. By 1955 towards the end of the tube era, tubes using only 50 mA down to as little as 10 mA for the heaters had been developed.\n\nThe high voltage applied to the anode (plate) was provided by the \"B\" battery or the HT (high-tension) supply or battery. These were generally of dry cell construction and typically came in 22.5-, 45-, 67.5-, 90-, 120- or 135-volt versions.\nEarly sets used a grid bias battery or \"C\" battery which was connected to provide a \"negative\" voltage. Since virtually no current flows through a tube's grid connection, these batteries had very low drain and lasted the longest. Even after AC power supplies became commonplace, some radio sets continued to be built with C batteries, as they would almost never need replacing. However more modern circuits were designed using cathode biasing, eliminating the need for a third power supply voltage; this became practical with tubes using indirect heating of the cathode.\n\nThe \"C battery\" for bias is a designation having no relation to the \"C cell\" battery size.\n\nBattery replacement was a major operating cost for early radio receiver users. The development of the battery eliminator, and, in 1925, batteryless receivers operated by household power, reduced operating costs and contributed to the growing popularity of radio. A power supply using a transformer with several windings, one or more rectifiers (which may themselves be vacuum tubes), and large filter capacitors provided the required direct current voltages from the alternating current source.\n\nAs a cost reduction measure, especially in high-volume consumer receivers, all the tube heaters could be connected in series across the AC supply using heaters requiring the same current and with a similar warm-up time. In one such design, a tap on the tube heater string supplied the 6 volts needed for the dial light. By deriving the high voltage from a half-wave rectifier directly connected to the AC mains, the heavy and costly power transformer was eliminated. This also allowed such receivers to operate on direct current, a so-called AC/DC receiver design. Many different US consumer AM radio manufacturers of the era used a virtually identical circuit, given the nickname All American Five.\n\nWhere the mains voltage was in the 100-120V range, this limited voltage proved suitable only for low-power receivers. Television receivers either required a transformer or could use a voltage doubling circuit. Where 230 V nominal mains voltage was used, television receivers as well could dispense with a power transformer.\n\nTransformer-less power supplies required safety precautions in their design to limit the shock hazard to users, such as electrically insulated cabinets and an interlock tying the power cord to the cabinet back, so the line cord was necessarily disconnected if the user or service person opened the cabinet. A \"cheater cord\" was a power cord ending in the special socket used by the safety interlock; servicers could then power the device with the hazardous voltages exposed.\n\nTo avoid the warm-up delay, \"instant on\" television receivers passed a small heating current through their tubes even when the set was nominally off. At switch on, full heating current was provided and the set would play almost immediately.\n\nOne reliability problem of tubes with oxide cathodes is the possibility that the cathode may slowly become \"poisoned\" by gas molecules from other elements in the tube, which reduce its ability to emit electrons. Trapped gases or slow gas leaks can also damage the cathode or cause plate (anode) current runaway due to ionization of free gas molecules. Vacuum hardness and proper selection of construction materials are the major influences on tube lifetime. Depending on the material, temperature and construction, the surface material of the cathode may also diffuse onto other elements. The resistive heaters that heat the cathodes may break in a manner similar to incandescent lamp filaments, but rarely do, since they operate at much lower temperatures than lamps.\n\nThe heater's failure mode is typically a stress-related fracture of the tungsten wire or at a weld point and generally occurs after accruing many thermal (power on-off) cycles. Tungsten wire has a very low resistance when at room temperature. A negative temperature coefficient device, such as a thermistor, may be incorporated in the equipment's heater supply or a ramp-up circuit may be employed to allow the heater or filaments to reach operating temperature more gradually than if powered-up in a step-function. Low-cost radios had tubes with heaters connected in series, with a total voltage equal to that of the line (mains). Some receivers made before World War II had series-string heaters with total voltage less than that of the mains. Some had a resistance wire running the length of the power cord to drop the voltage to the tubes. Others had series resistors made like regular tubes; they were called ballast tubes.\n\nFollowing World War II, tubes intended to be used in series heater strings were redesigned to all have the same (\"controlled\") warm-up time. Earlier designs had quite-different thermal time constants. The audio output stage, for instance, had a larger cathode, and warmed up more slowly than lower-powered tubes. The result was that heaters that warmed up faster also temporarily had higher resistance, because of their positive temperature coefficient. This disproportionate resistance caused them to temporarily operate with heater voltages well above their ratings, and shortened their life.\n\nAnother important reliability problem is caused by air leakage into the tube. Usually oxygen in the air reacts chemically with the hot filament or cathode, quickly ruining it. Designers developed tube designs that sealed reliably. This was why most tubes were constructed of glass. Metal alloys (such as Cunife and Fernico) and glasses had been developed for light bulbs that expanded and contracted in similar amounts, as temperature changed. These made it easy to construct an insulating envelope of glass, while passing connection wires through the glass to the electrodes.\n\nWhen a vacuum tube is overloaded or operated past its design dissipation, its anode (plate) may glow red. In consumer equipment, a glowing plate is universally a sign of an overloaded tube. However, some large transmitting tubes are designed to operate with their anodes at red, orange, or in rare cases, white heat.\n\n\"Special quality\" versions of standard tubes were often made, designed for improved performance in some respect, such as a longer life cathode, low noise construction, mechanical ruggedness via ruggedized filaments, low microphony, for applications where the tube will spend much of its time cut off, etc. The only way to know the particular features of a special quality part is by reading the data sheet. Names may reflect the standard name (12AU7==>12AU7A, its equivalent ECC82==>E82CC, etc.), or be absolutely anything (standard and special-quality equivalents of the same tube include 12AU7, ECC82, B329, CV491, E2163, E812CC, M8136, CV4003, 6067, VX7058, 5814A and 12AU7A).\n\nThe longest recorded valve life was earned by a Mazda AC/P pentode valve (serial No. 4418) in operation at the BBC's main Northern Ireland transmitter at Lisnagarvey. The valve was in service from 1935 until 1961 and had a recorded life of 232,592 hours. The BBC maintained meticulous records of their valves' lives with periodic returns to their central valve stores.\n\nA vacuum tube needs an extremely good (\"hard\") vacuum to avoid the consequences of generating positive ions within the tube. With a small amount of residual gas, some of those atoms may ionize when struck by an electron and create fields that adversely affect the tube characteristics. Larger amounts of residual gas can create a self-sustaining visible glow discharge between the tube elements. To avoid these effects, the residual pressure within the tube must be low enough that the mean free path of an electron is much longer than the size of the tube (so an electron is unlikely to strike a residual atom and very few ionized atoms will be present). Commercial vacuum tubes are evacuated at manufacture to about .\n\nTo prevent gases from compromising the tube's vacuum, modern tubes are constructed with \"getters\", which are usually small, circular troughs filled with metals that oxidize quickly, barium being the most common. While the tube envelope is being evacuated, the internal parts except the getter are heated by RF induction heating to evolve any remaining gas from the metal parts. The tube is then sealed and the getter is heated to a high temperature, again by radio frequency induction heating, which causes the getter material to vaporize and react with any residual gas. The vapor is deposited on the inside of the glass envelope, leaving a silver-colored metallic patch which continues to absorb small amounts of gas that may leak into the tube during its working life. Great care is taken with the valve design to ensure this material is not deposited on any of the working electrodes. If a tube develops a serious leak in the envelope, this deposit turns a white color as it reacts with atmospheric oxygen. Large transmitting and specialized tubes often use more exotic getter materials, such as zirconium. Early gettered tubes used phosphorus-based getters, and these tubes are easily identifiable, as the phosphorus leaves a characteristic orange or rainbow deposit on the glass. The use of phosphorus was short-lived and was quickly replaced by the superior barium getters. Unlike the barium getters, the phosphorus did not absorb any further gases once it had fired.\n\nGetters act by chemically combining with residual or infiltrating gases, but are unable to counteract (non-reactive) inert gases. A known problem, mostly affecting valves with large envelopes such as cathode ray tubes and camera tubes such as iconoscopes, orthicons, and image orthicons, comes from helium infiltration. The effect appears as impaired or absent functioning, and as a diffuse glow along the electron stream inside the tube. This effect cannot be rectified (short of re-evacuation and resealing), and is responsible for working examples of such tubes becoming rarer and rarer. Unused (\"New Old Stock\") tubes can also exhibit inert gas infiltration, so there is no long-term guarantee of these tube types surviving into the future.\n\nLarge transmitting tubes have carbonized tungsten filaments containing a small trace (1% to 2%) of thorium. An extremely thin (molecular) layer of thorium atoms forms on the outside of the wire's carbonized layer and, when heated, serve as an efficient source of electrons. The thorium slowly evaporates from the wire surface, while new thorium atoms diffuse to the surface to replace them. Such thoriated tungsten cathodes usually deliver lifetimes in the tens of thousands of hours. The end-of-life scenario for a thoriated-tungsten filament is when the carbonized layer has mostly been converted back into another form of tungsten carbide and emission begins to drop off rapidly; a complete loss of thorium has never been found to be a factor in the end-of-life in a tube with this type of emitter.\nWAAY-TV in Huntsville, Alabama achieved 163,000 hours (18.6 years) of service from an Eimac external cavity klystron in the visual circuit of its transmitter; this is the highest documented service life for this type of tube. \nIt has been said that transmitters with vacuum tubes are better able to survive lightning strikes than transistor transmitters do. While it was commonly believed that at RF power levels above approximately 20 kilowatts, vacuum tubes were more efficient than solid-state circuits, this is no longer the case, especially in medium wave (AM broadcast) service where solid-state transmitters at nearly all power levels have measurably higher efficiency. FM broadcast transmitters with solid-state power amplifiers up to approximately 15 kW also show better overall power efficiency than tube-based power amplifiers.\n\nCathodes in small \"receiving\" tubes are coated with a mixture of barium oxide and strontium oxide, sometimes with addition of calcium oxide or aluminium oxide. An electric heater is inserted into the cathode sleeve, and insulated from it electrically by a coating of aluminium oxide. This complex construction causes barium and strontium atoms to diffuse to the surface of the cathode and emit electrons when heated to about 780 degrees Celsius.\n\nA catastrophic failure is one which suddenly makes the vacuum tube unusable. A crack in the glass envelope will allow air into the tube and destroy it. Cracks may result from stress in the glass, bent pins or impacts; tube sockets must allow for thermal expansion, to prevent stress in the glass at the pins. Stress may accumulate if a metal shield or other object presses on the tube envelope and causes differential heating of the glass. Glass may also be damaged by high-voltage arcing.\n\nTube heaters may also fail without warning, especially if exposed to over voltage or as a result of manufacturing defects. Tube heaters do not normally fail by evaporation like lamp filaments, since they operate at much lower temperature. The surge of inrush current when the heater is first energized causes stress in the heater, and can be avoided by slowly warming the heaters, gradually increasing current with a NTC thermistor included in the circuit. Tubes intended for series-string operation of the heaters across the supply have a specified controlled warm-up time to avoid excess voltage on some heaters as others warm up. Directly heated filament-type cathodes as used in battery-operated tubes or some rectifiers may fail if the filament sags, causing internal arcing. Excess heater-to-cathode voltage in indirectly heated cathodes can break down the insulation between elements and destroy the heater.\n\nArcing between tube elements can destroy the tube. An arc can be caused by applying voltage to the anode (plate) before the cathode has come up to operating temperature, or by drawing excess current through a rectifier, which damages the emission coating. Arcs can also be initiated by any loose material inside the tube, or by excess screen voltage. An arc inside the tube allows gas to evolve from the tube materials, and may deposit conductive material on internal insulating spacers.\n\nTube rectifiers have limited current capability and exceeding ratings will eventually destroy a tube.\n\nDegenerative failures are those caused by the slow deterioration of performance over time.\n\nOverheating of internal parts, such as control grids or mica spacer insulators, can result in trapped gas escaping into the tube; this can reduce performance. A getter is used to absorb gases evolved during tube operation, but has only a limited ability to combine with gas. Control of the envelope temperature prevents some types of gassing. A tube with an unusually high level of internal gas may exhibit a visible blue glow when plate voltage is applied. The getter (being a highly reactive metal) is effective against many atmospheric gases, but has no (or very limited) chemical reactivity to inert gases such as helium. One progressive type of failure, especially with physically large envelopes such as those used by camera tubes and cathode-ray tubes, comes from helium infiltration. The exact mechanism is not clear: the metal-to-glass lead-in seals are one possible infiltration site.\n\nGas and ions within the tube contribute to grid current which can disturb operation of a vacuum tube circuit. Another effect of overheating is the slow deposit of metallic vapors on internal spacers, resulting in inter-element leakage.\n\nTubes on standby for long periods, with heater voltage applied, may develop high cathode interface resistance and display poor emission characteristics. This effect occurred especially in pulse and digital circuits, where tubes had no plate current flowing for extended times. Tubes designed specifically for this mode of operation were made.\n\nCathode depletion is the loss of emission after thousands of hours of normal use. Sometimes emission can be restored for a time by raising heater voltage, either for a short time or a permanent increase of a few percent. Cathode depletion was uncommon in signal tubes but was a frequent cause of failure of monochrome television cathode-ray tubes. Usable life of this expensive component was sometimes extended by fitting a boost transformer to increase heater voltage.\n\nVacuum tubes may develop defects in operation that make an individual tube unsuitable in a given device, although it may perform satisfactorily in another application. \"Microphonics\" refers to internal vibrations of tube elements which modulate the tube's signal in an undesirable way; sound or vibration pick-up may affect the signals, or even cause uncontrolled howling if a feedback path develops between a microphonic tube and, for example, a loudspeaker. Leakage current between AC heaters and the cathode may couple into the circuit, or electrons emitted directly from the ends of the heater may also inject hum into the signal. Leakage current due to internal contamination may also inject noise. Some of these effects make tubes unsuitable for small-signal audio use, although unobjectionable for other purposes. Selecting the best of a batch of nominally identical tubes for critical applications can produce better results.\n\nTube pins can develop non-conducting or high resistance surface films due to heat or dirt. Pins can be cleaned to restore conductance.\n\nVacuum tubes can be tested outside of their circuitry using a vacuum tube tester.\n\nMost small signal vacuum tube devices have been superseded by semiconductors, but some vacuum tube electronic devices are still in common use. The magnetron is the type of tube used in all microwave ovens. In spite of the advancing state of the art in power semiconductor technology, the vacuum tube still has reliability and cost advantages for high-frequency RF power generation.\n\nSome tubes, such as magnetrons, traveling-wave tubes, carcinotrons, and klystrons, combine magnetic and electrostatic effects. These are efficient (usually narrow-band) RF generators and still find use in radar, microwave ovens and industrial heating. Traveling-wave tubes (TWTs) are very good amplifiers and are even used in some communications satellites. High-powered klystron amplifier tubes can provide hundreds of kilowatts in the UHF range.\n\nThe cathode ray tube (CRT) is a vacuum tube used particularly for display purposes. Although there are still many televisions and computer monitors using cathode ray tubes, they are rapidly being replaced by flat panel displays whose quality has greatly improved even as their prices drop. This is also true of digital oscilloscopes (based on internal computers and analog to digital converters), although traditional analog scopes (dependent upon CRTs) continue to be produced, are economical, and preferred by many technicians. At one time many radios used \"magic eye tubes\", a specialized sort of CRT used in place of a meter movement to indicate signal strength, or input level in a tape recorder. A modern indicator device, the vacuum fluorescent display (VFD) is also a sort of cathode ray tube.\n\nThe X-ray tube is a type of cathode ray tube that generates X-rays when high voltage electrons hit the anode.\n\nGyrotrons or vacuum masers, used to generate high-power millimeter band waves, are magnetic vacuum tubes in which a small relativistic effect, due to the high voltage, is used for bunching the electrons. Gyrotrons can generate very high powers (hundreds of kilowatts).\nFree-electron lasers, used to generate high-power coherent light and even X-rays, are highly relativistic vacuum tubes driven by high-energy particle accelerators. Thus, these are sorts of cathode ray tubes.\n\nA photomultiplier is a phototube whose sensitivity is greatly increased through the use of electron multiplication. This works on the principle of secondary emission, whereby a single electron emitted by the photocathode strikes a special sort of anode known as a dynode causing more electrons to be released from that dynode. Those electrons are accelerated toward another dynode at a higher voltage, releasing more secondary electrons; as many as 15 such stages provide a huge amplification. Despite great advances in solid-state photodetectors, the single-photon detection capability of photomultiplier tubes makes this vacuum tube device excel in certain applications. Such a tube can also be used for detection of ionizing radiation as an alternative to the Geiger–Müller tube (itself not an actual vacuum tube). Historically, the image orthicon TV camera tube widely used in television studios prior to the development of modern CCD arrays also used multistage electron multiplication.\n\nFor decades, electron-tube designers tried to augment amplifying tubes with electron multipliers in order to increase gain, but these suffered from short life because the material used for the dynodes \"poisoned\" the tube's hot cathode. (For instance, the interesting RCA 1630 secondary-emission tube was marketed, but did not last.) However, eventually, Philips of the Netherlands developed the EFP60 tube that had a satisfactory lifetime, and was used in at least one product, a laboratory pulse generator. By that time, however, transistors were rapidly improving, making such developments superfluous.\n\nOne variant called a \"channel electron multiplier\" does not use individual dynodes but consists of a curved tube, such as a helix, coated on the inside with material with good secondary emission. One type had a funnel of sorts to capture the secondary electrons. The continuous dynode was resistive, and its ends were connected to enough voltage to create repeated cascades of electrons. The microchannel plate consists of an array of single stage electron multipliers over an image plane; several of these can then be stacked. This can be used, for instance, as an image intensifier in which the discrete channels substitute for focussing.\n\nTektronix made a high-performance wideband oscilloscope CRT with a channel electron multiplier plate behind the phosphor layer. This plate was a bundled array of a huge number of short individual c.e.m. tubes that accepted a low-current beam and intensified it to provide a display of practical brightness. (The electron optics of the wideband electron gun could not provide enough current to directly excite the phosphor.)\n\nAlthough vacuum tubes have been largely replaced by solid-state devices in most amplifying, switching, and rectifying applications, there are certain exceptions. In addition to the special functions noted above, tubes have some niche applications.\n\nIn general, vacuum tubes are much less susceptible than corresponding solid-state components to transient overvoltages, such as mains voltage surges or lightning, the electromagnetic pulse effect of nuclear explosions, or geomagnetic storms produced by giant solar flares. This property kept them in use for certain military applications long after more practical and less expensive solid-state technology was available for the same applications, as for example with the MiG-25. In that aircraft, output power of the radar is about one kilowatt and it can burn through a channel under interference.\n\nVacuum tubes are still practical alternatives to solid-state devices in generating high power at radio frequencies in applications such as industrial radio frequency heating, particle accelerators, and broadcast transmitters. This is particularly true at microwave frequencies where such devices as the klystron and traveling-wave tube provide amplification at power levels unattainable using semiconductor devices. The household microwave oven uses a magnetron tube to efficiently generate hundreds of watts of microwave power.\n\nIn military applications, a high-power vacuum tube can generate a 10–100 megawatt signal that can burn out an unprotected receiver's frontend. Such devices are considered non-nuclear electromagnetic weapons; they were introduced in the late 1990s by both the U.S. and Russia.\n\nEnough people prefer tube sound to make tube amplifiers commercially viable in three areas: musical instrument (e.g., guitar) amplifiers, devices used in recording studios, and audiophile equipment.\n\nMany guitarists prefer using valve amplifiers to solid-state models, often due to the way they tend to distort when overdriven. Any amplifier can only accurately amplify a signal to a certain volume; past this limit, the amplifier will begin to distort the signal. Different circuits will distort the signal in different ways; some guitarists prefer the distortion characteristics of vacuum tubes. Most popular vintage models use vacuum tubes.\n\nA modern display technology using a variation of cathode ray tube is often used in videocassette recorders, DVD players and recorders, microwave oven control panels, and automotive dashboards. Rather than raster scanning, these vacuum fluorescent displays (VFD) switch control grids and anode voltages on and off, for instance, to display discrete characters. The VFD uses phosphor-coated anodes as in other display cathode ray tubes. Because the filaments are in view, they must be operated at temperatures where the filament does not glow visibly. This is possible using more recent cathode technology, and these tubes also operate with quite low anode voltages (often less than 50 volts) unlike cathode ray tubes. Their high brightness allows reading the display in bright daylight. VFD tubes are flat and rectangular, as well as relatively thin. Typical VFD phosphors emit a broad spectrum of greenish-white light, permitting use of color filters, though different phosphors can give other colors even within the same display. The design of these tubes provides a bright glow despite the low energy of the incident electrons. This is because the distance between the cathode and anode is relatively small. (This technology is distinct from fluorescent lighting, which uses a discharge tube.)\n\nIn the early years of the 21st century there has been renewed interest in vacuum tubes, this time with the electron emitter formed on a flat silicon substrate, as in integrated circuit technology. This subject is now called vacuum nanoelectronics. The most common design uses a cold cathode in the form of a large-area field electron source (for example a field emitter array). With these devices, electrons are field-emitted from a large number of closely spaced individual emission sites.\n\nSuch integrated microtubes may find application in microwave devices including mobile phones, for Bluetooth and Wi-Fi transmission, and in radar and satellite communication. , they were being studied for possible applications in field emission display technology, but there were significant production problems.\n\nAs of 2014, NASA's Ames Research Center was reported on working on vacuum-channel transistors produced using CMOS techniques.\n\n\n\n"}
{"id": "57974713", "url": "https://en.wikipedia.org/wiki?curid=57974713", "title": "Vanesa Magar Brunner", "text": "Vanesa Magar Brunner\n\nVanesa Magar Brunner (also known as Vanesa Magar) is a Mexican scientist, a chartered mathematician, and an associate professor at the Physical Oceanography Department, Ensenada Center for Scientific Research and Higher Education, Mexico. She studies the effects of climate change on coastal erosion and evolution of coastlines, and wind and tidal energy. She is the Vice President of the (Mexican Geophysical Union), and editor of PLOS ONE, of PLOS \"Responding to Climate Change Channel\", of \"Frontiers in Marine Science - Ocean and Coastal Processes\".\n\nIn 1992, Magar was selected by NASA to take part in a space life sciences training programme at Kennedy Space Center, to celebrate International Space Year. Magar completed her Bachelors in physics at National Autonomous University of Mexico in 1996. She moved to the UK for her Masters, graduating from the Faculty of Mathematics, University of Cambridge in 1997. She remained here for her PhD, working on fluid dynamics with Tim Pedley. Together they studied the uptake of nutrients by swimming microorganisms. She graduated in 2001.\n\nMagar remained at the Department of Applied Mathematics and Theoretical Physics (DAMTP), Cambridge University, England, between 2001 and 2002, for her postdoctoral studies. She joined Bangor University in 2002, researching the transport of sediment above rippled beds. In 2005, she won a Research Councils UK Fellowship to work at the University of Plymouth.\n\nIn 2008 she was made a Chartered Mathematician. She is a Fellow of the Software Sustainability Institute, and Fellow of the Institute of Mathematics and its Applications since 2011. She was appointed a lecturer at the University of Plymouth in 2010. In 2014 she moved to the Ensenada Center for Scientific Research and Higher Education, where she is an associate professor of Physical Oceanography. She researches ways to mitigate climate change in the Sonoran Desert and projects related to renewable energy. Vanesa is concerned that Mexico are not best exploiting marine energy.\n\nShe is the Vice President of the for 2018 and 2019. She contributed to the books \"Sustainable Energy Technologies\" and \"Wave and Tidal Energy.\"\n\nMagar was born in 1971 to Roger Bernard Daniel Louis Magar Vincent (1936- ), a physicist, and Palmira Brunner Liebshard (1940-2018), a biologist turned palaeontologist. She was educated at the Lycée Franco-Mexicain, the National Autonomous University of Mexico, and at Clare College and Wolfson College, Cambridge University. \n\nIn 2009, she married Markus S. Gross, an engineer turned geoscientist who also works at the Physical Oceanography Department, Ensenada Center for Scientific Research and Higher Education. They have one son, Damián Suré Gross-Magar (born 14 April 2010). \n"}
{"id": "5112286", "url": "https://en.wikipedia.org/wiki?curid=5112286", "title": "Xi Centauri", "text": "Xi Centauri\n\nThe Bayer designation Xi Centauri (ξ Cen / ξ Centauri) is shared by two star systems, in the constellation Centaurus:\nThey are separated by 0.66° on the sky.\n"}
