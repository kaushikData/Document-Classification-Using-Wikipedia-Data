{"id": "28913046", "url": "https://en.wikipedia.org/wiki?curid=28913046", "title": "All-Blacks Nunataks", "text": "All-Blacks Nunataks\n\nAll-Blacks Nunataks () is a group of conspicuous nunataks lying midway between Wallabies Nunataks and Wilhoite Nunataks at the southeast margin of the Byrd Névé in Antarctica. Named by the New Zealand Geological Survey Antarctic Expedition (1960–61) for the well known New Zealand national rugby union team.\n"}
{"id": "17118072", "url": "https://en.wikipedia.org/wiki?curid=17118072", "title": "Aqua Kids (TV series)", "text": "Aqua Kids (TV series)\n\nAqua Kids is a K thru 12 program that educates young people about ecology, wildlife, and science as well as how it all relates to them. Aqua Kids was created by George A. Stover III, a professional TV producer, videographer and scuba diver. He and his wife Carol gathered the first group of \"Aqua Kids\" and shot a pilot for the new TV show in 2000. In 2005, full seasons of Aqua Kids began airing.\n\nAqua Kids TV, Inc. is a charitable and educational 501c3 non-profit organization based in Baltimore, Maryland. The series is currently seen in syndication to meet educational programming mandates.\n"}
{"id": "33938351", "url": "https://en.wikipedia.org/wiki?curid=33938351", "title": "Barrier layer (Oceanography)", "text": "Barrier layer (Oceanography)\n\nThe Barrier layer in the ocean is a layer of water separating the well-mixed surface layer from the thermocline.\n\nThe thickness of the barrier layer is defined as the difference between mixed layer depth (MLD) calculated from temperature minus the mixed layer depth calculated using density. The first reference to this difference as the barrier layer was in a paper describing observations in the western Pacific as part of the Western Equatorial Pacific Ocean Circulation Study. In regions where the barrier layer is present, stratification is stable because of strong buoyancy forcing associated with a fresh lens sitting on top of the water column. \nIn the past, a typical criterion for MLD was the depth at which the surface temperature cools by 0.2C (see e.g. D in the Figure). Prior to the subsurface salinity available from Argo, this was the main methodology for calculating the oceanic MLD. More recently, a density criterion has been used to define the MLD, defined as the depth where the density increases from the surface value due to a prescribed temperature decrease of 0.2C from the surface value while maintaining constant surface salinity value. In the Figure, this is defined by D and corresponds to an isothermal/isohaline layer. The BLT is the difference of the temperature-defined MLD minus the density-defined value (i.e. D - D).\n\nLarge values of the BLT are typically found in the equatorial regions and can be as high as 50 m. Above the barrier layer, the well mixed layer may be due to local precipitation exceeding evaporation (e.g. in the western Pacific), monsoon related river runoff (e.g. in the northern Indian Ocean), or advection of salty water subducted in the subtropics (found in all subtropical ocean gyres). BLT formation in the subtropics is associated with seasonal change in the mixed layer depth, a sharper gradient in sea surface salinity (SSS) than normal, and subduction across this SSS front. In particular, BLT is formed in winter season in the equatorward flank of subtropical salinity maxima. During early winter, the atmosphere cools the surface and strong wind and negative buoyancy forcing mixes temperature to a deep layer. At this same time, fresh surface salinity is advected from the rainy regions in the tropics. The deep temperature layer along with strong stratification in the salinity gives the conditions for BLT formation.\nFor the western Pacific, the mechanism for BLT formation is different. Along the equator, the eastern edge of the warm pool (typically 28C isotherm - see SST plot in the western Pacific) is a demarcation region between warm fresh water to the west and cold, salty, upwelled water in the central Pacific. A barrier layer is formed in the isothermal layer when salty water is subducted from the east into the warm pool due to local convergence and warm fresh water overrides denser water to the east. Here, weak winds, heavy precipitation, eastward advection of low salinity water, westward subduction of salty water and downwelling equatorial Kelvin or Rossby waves are factors that contribute to deep BLT formation.\n\nPrior to El Nino, the warm pool stores heat and is confined to the far western Pacific. During the El Nino, the warm pool migrates eastward along with the concomitant precipitation and current anomalies. The fetch of the westerlies is increased during this time, reinforcing the event. Using data from ship of opportunity and Tropical Atmosphere – Ocean (TAO) moorings in the western Pacific, the east and west migration of the warm pool was tracked over 1992-2000 using sea surface salinity (SSS), sea surface temperature (SST), currents, and subsurface data on conductivity as a function of temperature and depth taken on various research cruises. This work showed that during westward flow, the BLT in the western Pacific along the equator (138E-145E, 2N-2S) was between 18 m – 35 m corresponding with warm SST and serving as an efficient storage mechanism for heat. BLT formation is driven by westward (i.e. converging and subducting) currents along the equator near the eastern edge of the salinity front that defines the warm pool. These westward currents are driven by downwelling Rossby waves and represent either a westward advection of BLT or a preferential deepening of the deeper thermocline versus the shallower halocline due to Rossby wave dynamics (i.e. these waves favor vertical stretching of the upper water column). During El Nino, westerly winds drive the warm pool eastward allowing fresh water to ride on top of the local colder/saltier/denser water to the east. \nUsing coupled, atmospheric/ocean models and tuning the mixing to eliminate BLT for one year prior to El Nino, it was shown that the heat buildup associated with BLT is a requirement for big El Nino. It has been shown that there is a tight relationship between SSS and SST in the western Pacific and the barrier layer is instrumental in maintaining heat and momentum in the warm pool within the salinity stratified layer. Later work, including Argo drifters, confirm the relationship between eastward migration of the warm pool during El Nino and BLT heat storage in the western Pacific. The main impact of BLT is to maintain a shallow mixed layer allowing an enhanced air-sea coupled response. In addition, the BLT is the key factor in establishing the mean state that is perturbed during El Nino/La Nina \n"}
{"id": "52991942", "url": "https://en.wikipedia.org/wiki?curid=52991942", "title": "Bığbığ Ivy", "text": "Bığbığ Ivy\n\nBığbığ Ivy () is a very old climbing plant located in Adana Province, southern Turkey.\n\nBığbığ Ivy is located at Meydan Yaylası (highland) of Aladağ district in Adana Province. Situated in Bığbığı location at high above main sea level, the age of the ivy (\"Hedera helix\") is dated to be around 4000 years old. It is long with a trunk circumference of and a canopy size of .\n\nThe plant was registered a natural monument on June 6, 1994. The protected area of the plant covers .\n"}
{"id": "50617726", "url": "https://en.wikipedia.org/wiki?curid=50617726", "title": "CASA-1000", "text": "CASA-1000\n\nThe Central Asia-South Asia power project, commonly known by the acronym CASA-1000, is a $1.16 billion project currently under construction that will allow for the export of surplus hydroelectricity from Tajikistan and Kyrgyzstan to Pakistan and Afghanistan. Groundbreaking for the project took place on May 12, 2016 in Tajikistan in a ceremony attended by the Kyrgyz, Tajik, and Pakistani Prime Ministers, and is expected to be completed by the end of 2018. The project initially also included transfer of electricity to Afghanistan, however the country abandoned its share of electricity due to dearth of demand, hence Pakistan will receive 1,300 megawatts of electricity.\n\nThe project will allow for the export of 1,300 megawatts of electricity during the summer months when both Tajikistan and Kyrgyzstan experience surplus electricity generation from hydroelectric dams.\n\nHigh voltage direct current (HVDC) converter stations will also be included as part of the project, as well as a 477 kilometer long, 500 kilovolt alternating current transmission line between Datka, Kyrgyzstan and Khujand, Tajikistan. A 1,300 MW AC/DC converter station will be constructed in the city of Sangtuda, Tajikistan, as well as a 300 MW converter station in Kabul, Afghanistan. A 750km HVDV line will be constructed between Sangtuda, and the city of Peshawar, Pakistan, via the Salang Pass and Kabul. In Peshawar, a 1,300 MW converter station will be built and connected to Pakistan's electric grid.\n\nTransmission lines are designed to transmit 1,300 MW of electricity, with Afghanistan allotted 300 MW of electricity and Pakistan 1000 MW of electricity. Initially, however, Pakistan will receive 700 MW, while Afghanistan will be supplied 300 MW.\n"}
{"id": "11376541", "url": "https://en.wikipedia.org/wiki?curid=11376541", "title": "Carbon Emission Reduction Target", "text": "Carbon Emission Reduction Target\n\nThe Carbon Emission Reduction Target (CERT) in the United Kingdom (formerly the Energy Efficiency Commitment) is a target imposed on the gas and electricity transporters and suppliers under Section 33BC of the Gas Act 1986 and Section 41A of the Electricity Act 1989, as modified by the Climate Change and Sustainable Energy Act 2006\n\nThe original Energy Efficiency Commitment 1 (2002–2005) program required that all electricity and gas suppliers with 15,000 or more domestic customers must achieve a combined energy saving of 62 TWh by 2005 by assisting their customers to take energy-efficiency measures in their homes: suppliers had to achieve at least half of their energy savings in households on income-related benefits and tax credits. \n\nIn the second phase of the Energy Efficiency Commitment (2005–2008) scheme, energy saving targets were raised to 130 TWh suppliers, and here suppliers with at least 50,000 domestic customers (including affiliated licenses) were eligible for an obligation.\n\nThe third phase of CERT (previously known as Energy Efficiency Commitment 3) originally ran from 2008 to 2011 and increased the previous targets to 154 MtC. A consultation document was published alongside the 2007 Energy White Paper, and responses were invited by 15 August 2007. The new scheme is regulated by Electricity and Gas (Carbon Emissions Reduction) Order 2008 (S.I. 2008/188). In 2009 the UK Government increased the emission reduction target by a further 20% to 185 MtC. In 2010 the Government increased the target to 293 MtC, to be achieved over an extended period running until the end of 2012 (see The Electricity and Gas (Carbon Emissions Reduction) (Amendment) Order 2010: S.I.2010/1958).\n\nFrom 2013 CERT will be superseded by the Energy Company Obligation (ECO)\n\n\n"}
{"id": "484858", "url": "https://en.wikipedia.org/wiki?curid=484858", "title": "Chatterton's compound", "text": "Chatterton's compound\n\nChatterton’s compound was an adhesive waterproof insulating compound that was used in early submarine telegraph cables. It was patented in 1859 by John Chatterton and Willoughby Smith.\n\nIts constitution is as follows: \n\nChatterton's Compound was also used to stick insulating paper to armatures, for example those on Synchronome clocks, which were the most accurate clocks made in the early part of the 20th century. If the paper appeared to be in bad condition it could be removed by warming up the armature near a flame and replacing it.\n\nChatterton's Compound was used in the manufacture of pneumatic pipe organs to seal lead tubing into wooden blocks for the pneumatic action, or, particularly in the UK, to affix reed weights to the reed tongues. It is still used in historical restoration jobs for this purpose but has been replaced with modern materials on more recent builds.\n\nChatterton's Compound was much used by submarine cable technicians for various purposes around oversea telegraph offices prior to the 1960s; it could be used as an adhesive or in jointing gutta-percha-insulated submarine telegraph cables. It was available in round black sticks of about 3/4 inch diameter which responded to heat from a spirit lamp. It was known to telegraph technicians as 'chats'.\n\nIn France, the normal black PVC electrical insulating tape is often referred to as \"chatterton\".\n"}
{"id": "42806", "url": "https://en.wikipedia.org/wiki?curid=42806", "title": "Cyclone", "text": "Cyclone\n\nIn meteorology, a cyclone is a large scale air mass that rotates around a strong center of low atmospheric pressure. Cyclones are characterized by inward spiraling winds that rotate about a zone of low pressure. The largest low-pressure systems are polar vortices and extratropical cyclones of the largest scale (the synoptic scale). Warm-core cyclones such as tropical cyclones and subtropical cyclones also lie within the synoptic scale. Mesocyclones, tornadoes and dust devils lie within the smaller mesoscale. Upper level cyclones can exist without the presence of a surface low, and can pinch off from the base of the tropical upper tropospheric trough during the summer months in the Northern Hemisphere. Cyclones have also been seen on extraterrestrial planets, such as Mars and Neptune. Cyclogenesis is the process of cyclone formation and intensification. Extratropical cyclones begin as waves in large regions of enhanced mid-latitude temperature contrasts called baroclinic zones. These zones contract and form weather fronts as the cyclonic circulation closes and intensifies. Later in their life cycle, extratropical cyclones occlude as cold air masses undercut the warmer air and become cold core systems. A cyclone's track is guided over the course of its 2 to 6 day life cycle by the steering flow of the subtropical jet stream.\n\nWeather fronts mark the boundary between two masses of air of different temperature, humidity, and densities, and are associated with the most prominent meteorological phenomena. Strong cold fronts typically feature narrow bands of thunderstorms and severe weather, and may on occasion be preceded by squall lines or dry lines. Such fronts form west of the circulation center and generally move from west to east; warm fronts form east of the cyclone center and are usually preceded by stratiform precipitation and fog. Warm fronts move poleward ahead of the cyclone path. Occluded fronts form late in the cyclone life cycle near the center of the cyclone and often wrap around the storm center.\n\nTropical cyclogenesis describes the process of development of tropical cyclones. Tropical cyclones form due to latent heat driven by significant thunderstorm activity, and are warm core. Cyclones can transition between extratropical, subtropical, and tropical phases. Mesocyclones form as warm core cyclones over land, and can lead to tornado formation. Waterspouts can also form from mesocyclones, but more often develop from environments of high instability and low vertical wind shear. In the Atlantic and the northeastern Pacific oceans, a tropical cyclone is generally referred to as a hurricane (from the name of the ancient Central American deity of wind, Huracan), in the Indian and south Pacific oceans it is called a cyclone, and in the northwestern Pacific it is called a typhoon.\nThe growth of instability in the vortices is not universal. For example, the size, intensity, moist-convection, surface evaporation, the value of potential temperature at each potential height can affect the nonlinear evolution of a vortex. \n\nHenry Piddington published 40 papers dealing with tropical storms from Calcutta between 1836 and 1855 in \"The Journal of the Asiatic Society\". He also coined the term \"cyclone\", meaning the coil of a snake. In 1842, he published his landmark thesis, \"Laws of the Storms\".\n\nThere are a number of structural characteristics common to all cyclones. A cyclone is a low-pressure area. A cyclone's center (often known in a mature tropical cyclone as the eye), is the area of lowest atmospheric pressure in the region. Near the center, the pressure gradient force (from the pressure in the center of the cyclone compared to the pressure outside the cyclone) and the force from the Coriolis effect must be in an approximate balance, or the cyclone would collapse on itself as a result of the difference in pressure.\n\nBecause of the Coriolis effect, the wind flow around a large cyclone is counterclockwise in the Northern Hemisphere and clockwise in the Southern Hemisphere. In the Northern Hemisphere, the fastest winds relative to the surface of the Earth therefore occur on the eastern side of a northward-moving cyclone and on the northern side of a westward-moving one; the opposite occurs in the Southern Hemisphere. In contrast to low pressure systems, the wind flow around high pressure systems are clockwise (anticyclonic) in the northern hemisphere, and counterclockwise in the southern hemisphere.\n\nCyclogenesis is the development or strengthening of cyclonic circulation in the atmosphere. Cyclogenesis is an umbrella term for several different processes that all result in the development of some sort of cyclone. It can occur at various scales, from the microscale to the synoptic scale.\n\nExtratropical cyclones begin as waves along weather fronts before occluding later in their life cycle as cold-core systems. However, some intense extratropical cyclones can become warm-core systems when a warm seclusion occurs.\n\nTropical cyclones form as a result of significant convective activity, and are warm core. Mesocyclones form as warm core cyclones over land, and can lead to tornado formation. Waterspouts can also form from mesocyclones, but more often develop from environments of high instability and low vertical wind shear. Cyclolysis is the opposite of cyclogenesis, and is the high-pressure system equivalent, which deals with the formation of high-pressure areas—Anticyclogenesis.\n\nA surface low can form in a variety of ways. Topography can create a surface low. Mesoscale convective systems can spawn surface lows that are initially warm core. The disturbance can grow into a wave-like formation along the front and the low is positioned at the crest. Around the low, the flow becomes cyclonic. This rotational flow moves polar air towards the equator on the west side of the low, while warm air move towards the pole on the east side. A cold front appears on the west side, while a warm front forms on the east side. Usually, the cold front moves at a quicker pace than the warm front and \"catches up\" with it due to the slow erosion of higher density air mass out ahead of the cyclone. In addition, the higher density air mass sweeping in behind the cyclone strengthens the higher pressure, denser cold air mass. The cold front over takes the warm front, and reduces the length of the warm front. At this point an occluded front forms where the warm air mass is pushed upwards into a trough of warm air aloft, which is also known as a trowal.\n\nTropical cyclogenesis is the development and strengthening of a tropical cyclone. The mechanisms by which tropical cyclogenesis occurs are distinctly different from those that produce mid-latitude cyclones. Tropical cyclogenesis, the development of a warm-core cyclone, begins with significant convection in a favorable atmospheric environment. There are six main requirements for tropical cyclogenesis: \n\nAn average of 86 tropical cyclones of tropical storm intensity form annually worldwide, with 47 reaching hurricane/typhoon strength, and 20 becoming intense tropical cyclones (at least Category 3 intensity on the Saffir–Simpson Hurricane Scale).\n\nThe following types of cyclones are identifiable in synoptic charts.\n\nThere are three main types of surface-based cyclones: Extratropical cyclones, Subtropical cyclones and Tropical cyclones\n\nAn extratropical cyclone is a synoptic scale of low-pressure weather system that does not have tropical characteristics, as it is connected with fronts and horizontal gradients (rather than vertical) in temperature and dew point otherwise known as \"baroclinic zones\".\n\n\"Extratropical\" is applied to cyclones outside the tropics, in the middle latitudes. These systems may also be described as \"mid-latitude cyclones\" due to their area of formation, or \"post-tropical cyclones\" when a tropical cyclone has moved (extratropical transition) beyond the tropics. They are often described as \"depressions\" or \"lows\" by weather forecasters and the general public. These are the everyday phenomena that, along with anti-cyclones, drive weather over much of the Earth.\n\nAlthough extratropical cyclones are almost always classified as baroclinic since they form along zones of temperature and dewpoint gradient within the westerlies, they can sometimes become barotropic late in their life cycle when the temperature distribution around the cyclone becomes fairly uniform with radius. An extratropical cyclone can transform into a subtropical storm, and from there into a tropical cyclone, if it dwells over warm waters sufficient to warm its core, and as a result develops central convection. A particularly intense type of extratropical cyclone that strikes during winter is known colloquially as a \"nor'easter\".\n\nA polar low is a small-scale, short-lived atmospheric low-pressure system (depression) that is found over the ocean areas poleward of the main polar front in both the Northern and Southern Hemispheres. Polar lows were first identified on the meteorological satellite imagery that became available in the 1960s, which revealed many small-scale cloud vortices at high latitudes. The most active polar lows are found over certain ice-free maritime areas in or near the Arctic during the winter, such as the Norwegian Sea, Barents Sea, Labrador Sea and Gulf of Alaska. Polar lows dissipate rapidly when they make landfall. Antarctic systems tend to be weaker than their northern counterparts since the air-sea temperature differences around the continent are generally smaller . However, vigorous polar lows can be found over the Southern Ocean. During winter, when cold-core lows with temperatures in the mid-levels of the troposphere reach move over open waters, deep convection forms, which allows polar low development to become possible. The systems usually have a horizontal length scale of less than and exist for no more than a couple of days. They are part of the larger class of mesoscale weather systems. Polar lows can be difficult to detect using conventional weather reports and are a hazard to high-latitude operations, such as shipping and gas and oil platforms. Polar lows have been referred to by many other terms, such as polar mesoscale vortex, Arctic hurricane, Arctic low, and cold air depression. Today the term is usually reserved for the more vigorous systems that have near-surface winds of at least 17 m/s.\n\nA subtropical cyclone is a weather system that has some characteristics of a tropical cyclone and some characteristics of an extratropical cyclone. They can form between the equator and the 50th parallel. As early as the 1950s, meteorologists were unclear whether they should be characterized as tropical cyclones or extratropical cyclones, and used terms such as quasi-tropical and semi-tropical to describe the cyclone hybrids. By 1972, the National Hurricane Center officially recognized this cyclone category. Subtropical cyclones began to receive names off the official tropical cyclone list in the Atlantic Basin in 2002. They have broad wind patterns with maximum sustained winds located farther from the center than typical tropical cyclones, and exist in areas of weak to moderate temperature gradient.\n\nSince they form from extratropical cyclones, which have colder temperatures aloft than normally found in the tropics, the sea surface temperatures required is around 23 degrees Celsius (73 °F) for their formation, which is three degrees Celsius (5 °F) lower than for tropical cyclones. This means that subtropical cyclones are more likely to form outside the traditional bounds of the hurricane season. Although subtropical storms rarely have hurricane-force winds, they may become tropical in nature as their cores warm.\n\nA tropical cyclone is a storm system characterized by a low-pressure center and numerous thunderstorms that produce strong winds and flooding rain. A tropical cyclone feeds on heat released when moist air rises, resulting in condensation of water vapour contained in the moist air. They are fueled by a different heat mechanism than other cyclonic windstorms such as nor'easters, European windstorms, and polar lows, leading to their classification as \"warm core\" storm systems.\n\nThe term \"tropical\" refers to both the geographic origin of these systems, which form almost exclusively in tropical regions of the globe, and their dependence on Maritime Tropical air masses for their formation. The term \"cyclone\" refers to the storms' cyclonic nature, with counterclockwise rotation in the Northern Hemisphere and clockwise rotation in the Southern Hemisphere. Depending on their location and strength, tropical cyclones are referred to by other names, such as hurricane, typhoon, tropical storm, cyclonic storm, tropical depression, or simply as a cyclone.\n\nWhile tropical cyclones can produce extremely powerful winds and torrential rain, they are also able to produce high waves and a damaging storm surge. Their winds increase the wave size, and in so doing they draw more heat and moisture into their system, thereby increasing their strength. They develop over large bodies of warm water, and hence lose their strength if they move over land. This is the reason coastal regions can receive significant damage from a tropical cyclone, while inland regions are relatively safe from strong winds. Heavy rains, however, can produce significant flooding inland. Storm surges are rises in sea level caused by the reduced pressure of the core that in effect \"sucks\" the water upward and from winds that in effect \"pile\" the water up. Storm surges can produce extensive coastal flooding up to from the coastline. Although their effects on human populations can be devastating, tropical cyclones can also relieve drought conditions. They also carry heat and energy away from the tropics and transport it toward temperate latitudes, which makes them an important part of the global atmospheric circulation mechanism. As a result, tropical cyclones help to maintain equilibrium in the Earth's troposphere.\n\nMany tropical cyclones develop when the atmospheric conditions around a weak disturbance in the atmosphere are favorable. Others form when other types of cyclones acquire tropical characteristics. Tropical systems are then moved by steering winds in the troposphere; if the conditions remain favorable, the tropical disturbance intensifies, and can even develop an eye. On the other end of the spectrum, if the conditions around the system deteriorate or the tropical cyclone makes landfall, the system weakens and eventually dissipates. A tropical cyclone can become extratropical as it moves toward higher latitudes if its energy source changes from heat released by condensation to differences in temperature between air masses. A tropical cyclone is usually not considered to become subtropical during its extratropical transition.\n\nA polar, sub-polar, or Arctic cyclone (also known as a polar vortex) is a vast area of low pressure that strengthens in the winter and weakens in the summer. A polar cyclone is a low-pressure weather system, usually spanning to , in which the air circulates in a counterclockwise direction in the northern hemisphere, and a clockwise direction in the southern hemisphere. The Coriolis acceleration acting on the air masses moving poleward at high altitude, causes a counterclockwise circulation at high altitude. The poleward movement of air originates from the air circulation of the Polar cell. The polar low is not driven by convection as are tropical cyclones, nor the cold and warm air mass interactions as are extratropical cyclones, but is an artifact of the global air movement of the Polar cell. The base of the polar low is in the mid to upper troposphere. In the Northern Hemisphere, the polar cyclone has two centers on average. One center lies near Baffin Island and the other over northeast Siberia. In the southern hemisphere, it tends to be located near the edge of the Ross ice shelf near 160 west longitude. When the polar vortex is strong, its effect can be felt at the surface as a westerly wind (toward the east). When the polar cyclone is weak, significant cold outbreaks occur.\n\nUnder specific circumstances, upper level cold lows can break off from the base of the Tropical Upper Tropospheric Trough (TUTT), which is located mid-ocean in the Northern Hemisphere during the summer months. These upper tropospheric cyclonic vortices, also known as TUTT cells or TUTT lows, usually move slowly from east-northeast to west-southwest, and their bases generally do not extend below in altitude. A weak inverted surface trough within the trade wind is generally found underneath them, and they may also be associated with broad areas of high-level clouds. Downward development results in an increase of cumulus clouds and the appearance of a surface vortex. In rare cases, they become warm-core tropical cyclones. Upper cyclones and the upper troughs that trail tropical cyclones can cause additional outflow channels and aid in their intensification. Developing tropical disturbances can help create or deepen upper troughs or upper lows in their wake due to the outflow jet emanating from the developing tropical disturbance/cyclone.\n\nThe following types of cyclones are not identifiable in synoptic charts.\n\nA mesocyclone is a vortex of air, to in diameter (the mesoscale of meteorology), within a convective storm. Air rises and rotates around a vertical axis, usually in the same direction as low-pressure systems in both northern and southern hemisphere. They are most often cyclonic, that is, associated with a localized low-pressure region within a supercell. Such storms can feature strong surface winds and severe hail. Mesocyclones often occur together with updrafts in supercells, where tornadoes may form. About 1,700 mesocyclones form annually across the United States, but only half produce tornadoes.\n\nA tornado is a violently rotating column of air that is in contact with both the surface of the earth and a cumulonimbus cloud or, in rare cases, the base of a cumulus cloud. Also referred to as twisters, a colloquial term in America, or cyclones, although the word cyclone is used in meteorology, in a wider sense, to name any closed low-pressure circulation.\n\nA dust devil is a strong, well-formed, and relatively long-lived whirlwind, ranging from small (half a metre wide and a few metres tall) to large (more than 10 metres wide and more than 1000 metres tall). The primary vertical motion is upward. Dust devils are usually harmless, but can on rare occasions grow large enough to pose a threat to both people and property.\n\nA waterspout is a columnar vortex forming over water that is, in its most common form, a non-supercell tornado over water that is connected to a cumuliform cloud. While it is often weaker than most of its land counterparts, stronger versions spawned by mesocyclones do occur.\n\nA gentle vortex over calm water or wet land made visible by rising water vapour.\n\nA fire whirl – also colloquially known as a fire devil, fire tornado, firenado, or fire twister – is a whirlwind induced by a fire and often made up of flame or ash.\n\nScientists warn that climate change could increase the intensity of typhoons as climate change projections show that the difference in temperature between the ocean – the heat source for cyclones – and the storm tops – the cold parts of cyclones – are likely to increase. Climate change is predicted to increase the frequency of high-intensity storms in selected ocean basins. While the effect changing climate is having on tropical storms remains largely unresolved scientists and president of Vanuatu Baldwin Lonsdale say the devastation caused by Pam, was aggravated by climate change.\n\nCyclones are not unique to Earth. Cyclonic storms are common on Jovian planets, such as the Small Dark Spot on Neptune. It is about one third the diameter of the Great Dark Spot and received the nickname \"Wizard's Eye\" because it looks like an eye. This appearance is caused by a white cloud in the middle of the Wizard's Eye. Mars has also exhibited cyclonic storms. Jovian storms like the Great Red Spot are usually mistakenly named as giant hurricanes or cyclonic storms. However, this is inaccurate, as the Great Red Spot is, in fact, the inverse phenomenon, an anticyclone.\n\n"}
{"id": "15152510", "url": "https://en.wikipedia.org/wiki?curid=15152510", "title": "Don Stephens", "text": "Don Stephens\n\nDon Stephens is a futurist, eco-home sustainable designer and author. He has published books in the field of what he terms \"optimized self-sufficiency\" for a range of uncertain-future scenarios, that is also labeled survivalism by others.\n\nIn the 1960s, Stephens popularized the term \"retreater\" to describe those in the survivalist movement who were making preparations to avoid conflict by leaving populated areas for a pre-established remote survival retreat when or if society broke down. (At that time, this trend was primarily motivated by concerns about monetary collapse arising out of anticipated accelerating monetary inflation.) He also wrote on this topic for \"Innovator\" magazine, \"Inflation Survival Letter\", \"Atlantis Quarterly\" and \"Harry Browne's Newsletter\", among others. Stephens also contributed the \"Safe Shelter & Independent Energy\" chapter for \"The Complete Survival Guide\" (1983), which was edited by ark Thiffault. In public speaking venues, newsletters, and magazines, Stephens has been associated with Harry Browne, Robert D. Kephart, James McKeever, and Mel Tappan.\n\nStephens studied architecture at the University of Idaho, and worked in a range of architectural and engineering firms in Idaho, Washington and southern California, before establishing his own eco-home design and consulting practice. From his university days onward, he has explored and evolved techniques of active solar and passive solar heating, solar power, earth-integrated design (earth sheltering), and pioneered uses of a range of \"alternative\", \"natural\", and salvaged materials and building techniques in home and retreat construction. These include straw bale, high-density recompressed strawblocks, earth-rammed tires, tire bales, urbanite, rammed earth, rice hulls, salvaged/used carpeting, and the pre-stressed \"self-filling\" of cement-bonded polystyrene bead insulating concrete forms (ICFs).\n\nStephens developed (and originated the name for) the Annualized Geo-Solar (AGS) technique for simply and inexpensively capturing/storing the summer sun's heat, for predictable, delayed return six months later, to maintain up to 100% of needed winter warmth, which has drawn particular interest. This grew out of his pioneering work with solar and earth-sheltering, beginning in 1960. On these topics, he has written for \"Earth-Shelter Digest\", \"Earthtone\" magazine and \"The Last Straw\". He also prepared a requested paper on AGS for The Global Sustainable Building Conference 2005, in Tokyo, Japan. Stephens also presented this material at a number of conferences and workshops over the years for The American Underground-Space Association, Sol-West, the Northwest Renewable Energy Festival, the International Strawbale Association and the Northwest EcoBuilding Guild, and others.\n\nIn keeping with his professional advocacies, he lives in a retrofitted century-old home heated by solar and biomass, utilizes photovoltaics for power generation, drives an electric vehicle, and practices organic gardening, composting and rainwater catchment.\n\nDon Stephens passed away in February 2018.\n\n\n"}
{"id": "7731295", "url": "https://en.wikipedia.org/wiki?curid=7731295", "title": "Epic of evolution", "text": "Epic of evolution\n\nIn social, cultural and religious studies, the \"epic of evolution\" is a narrative that blends religious and scientific views of cosmic, biological and sociocultural evolution in a mythological manner. According to \"Taylor's Encyclopedia of Religion and Nature\", an \"epic of evolution\" encompasses\n\"Epic of evolution\" seems to have originated from the sociobiologist Edward O. Wilson's use of the phrase \"evolutionary epic\" in 1978. Wilson was not the first to use the term but his prominence prompted its usage as the morphed phrase 'epic of evolution'. In later years, he also used the latter term.\n\nNaturalistic and liberal religious writers have picked up on Wilson's term and have used it in a number of texts. These authors however have at times used other terms to refer to the idea: \"Universe Story\" (Brian Swimme, John F. Haught), \"Great Story\" (Connie Barlow, Michael Dowd), \"Everybody's Story\" (Loyal Rue), \"New Story\" (Thomas Berry, Al Gore, Brian Swimme) and \"Cosmic Evolution\" (Eric Chaisson).\n\nEvolution generally refers to biological evolution, but here it means a process in which the whole universe is a progression of interrelated phenomena, a gradual process in which something changes into a different and usually more complex form (emergence). It should not be 'biologized' as it includes many areas of science. In addition, outside of the scientific community, the term evolution is frequently used differently from scientists' usage. Even respected dictionaries may present questionable definitions that are not scientific or only address genetic changes. Unfortunately it is common for the general public to enter into a discussion about evolution with a wrong definition in mind. This often leads to misunderstanding since scientists are viewing evolution from a different perspective. The same applies to the use of the term theory as used in the theory of evolution (see references for Evolution as theory and fact).\n\nThis epic is not a long narrative poem but a series of events that form the proper subject for a laudable kind of tale. It is mythic in that it is a story of ostensibly historical events that serves to unfold part of the worldview of a people and explains a natural phenomenon. It is a form of myth that has an approach to investigation that is empirical or scientific. According to Joseph Campbell myths serve two purposes—provide meaning for a maturing individual (an individuate) and how to be part of a community. This Epic does both.\n\nDr. William Grassie of Temple University writes that the word \"myth\" in common usage is usually misunderstood. In academia it defines \"a story that serves to define the fundamental world view of a culture by explaining aspects of the natural world and delineating the psychological and social practices and ideals of a society\". He suggests that the Greek term \"mythos\" would be a better term to apply to the Epic as it is more all-encompassing. He concluded that there is not yet an interpretive tradition within science and society about this epic of evolution. If anything, there is an anti-interpretation tradition. Consequently, this is dangerous as it is a powerful revelation at this time. Grassie says the Epic is complex and multifaceted, not simple or easy to understand. It takes a romantic vision, philosophical rigor, and artistic interpretations. It requires a consilience of modern disciplines and acceptance of social diversity. The ancient wisdoms of the world's spiritual traditions must be adapted to make the framework to weave the Epic.\n\nE. O. Wilson explained that humans had a need for the epic of evolution because they must have a mythical story or a sublime account of how the world was created and how humanity became part of it. Religious epics fulfill a primal need in this respect as they verify that humans are part of something greater than themselves. The best empirical knowledge that science and history can provide is necessary in order to provide a comparable epic tale that will reliably unite a separated human spirituality. He believes the evolutionary epic can be as inherently noble as any religious epic when it is expressed in a poetic way. In a similar vein, biologist Ursula Goodenough sees the tale of natural emergence as far more magical than traditional religious miracles. It is a story that people can work with in a religious way if they elect to do so.\n\nPhilip Hefner uses the analogy of weaving to describe the Epic. The warp anchors the story and the weft creates the pattern and the tapestry. The Epic, as scientists see it, is the warp and the weft forms the pattern as each of us views it (we are all weavers) but the patterns all have the warp in common. Hefner writes that stories about the evolutionary epic are redolent with ultimacy. It is not science; it is scientifically informed myth, a myth driven by the refusal to give up on the insistence that the natural world and our lives in the world have meaning and purpose. It is a mythical tale of irony and hope that fills a large space in the domain of religion-and-science. Biologist Ursula Goodenough also makes use of Hefner's weaving metaphor.\n\nConnie Barlow and Michael Dowd's Great Story divides the epic into 8 phases eons or eras: the Great Radiance, the Galactic, Hadean, Archean, Proterozoic, Paleozoic, Mesozoic, and Cenozoic. Dowd uses the term 'epic of evolution' to help construct his viewpoint of evolution theology (a form of theistic evolution). His position is that science and religious faith are not mutually exclusive. He preaches that the epic of cosmic, biological, and human evolution, revealed by science, is a basis for an inspiring and meaningful view of our place in the universe. Evolution is viewed as a spiritual process that it is not meaningless blind chance.\n\nLoyal Rue states that there is nothing in the core of everybody's story to rule out belief in a personal deity. However belief in God is not an indispensable part of this narrative and here will be both theistic and non-theistic versions of it. He says it is the fundamental story of matter, created from energy, the organization of that matter into complex conditions, and then via self-organization into diverse life forms. Humans are as other living things—we are by nature star-born, earth-formed, fitness-maximized, biochemical systems. An aspect of the Epic is the evolution of behavior by that biochemical system.\n\nBrian Swimme sees the Epic as a way to gently maneuver a person into the magnificence of the Universe and as an antidote to the unhealthy consciousness of consumerism. It is the way into the future and enabled him to comprehend the cultural significance of this new story of science moving away from a materialistic worldview. It may move science away from its traditional abstractness to the uniqueness found in natural history. To him evolution and creativity are equivalent so it could be the Epic of Creativity (similar to Gordon Kaufman's thinking). Although the Epic is scientific, it is 'definitely mythic'—it has the fundamental nature of being mythic. \"You take hydrogen gas, and you leave it alone, and it turns into rosebuds, giraffes and humans.\"\n\nGordon D. Kaufman sees the Epic as a serendipitous creative process. He states that it is a notion that can interpret the enormous expansion and complexification of the physical universe (from the Big Bang outward), as well as the evolution of life here on earth and the gradual emergence of human historical existence. The whole vast process manifests (in varying degrees) serendipitous creativity, an everflowing coming into being of new modes of reality. In his book, \"In the beginning—creativity\", he says this creative process is God. Creativity, as metaphor, and as defined in the concept of evolution, has possibilities for constructing a new concept of God. The most foundational kind of creativity is found in that of cosmos/biological evolution—a paradigm that is now the organizing principle of all the sciences. It would seem as though he was equating God to the evolutionary story. This is similar to Dowd who sees the facts of Nature as God's native tongue.\n\nEric Chaisson orients his view of the epic around an \"arrow of time\", which he divides into 'Seven Ages of the Cosmos': particulate, galactic, stellar, planetary, chemical, biological, and cultural. However, such a thermodynamic arrow is not intended to be directional, a common misunderstanding; he sees no purpose, plan, or design in evolution, which he regards as unceasing, uncaring, and unpredictable. Chaisson’s strictly scientific analysis of evolution in its broadest sense—cosmic evolution)—implies an unidirectional, meandering process extending from the big bang to humankind on Earth, and continuing while likely producing increasing complexity, perhaps forever, to ends unknown. As a physicist, he is perhaps best known for attempting to quantify the epic of evolution, using the currently known laws of science and the scientific method (with its insistence on experimental tests of all ideas, principally those of energy rate density) in order to empirically discriminate between objective sense and subjective nonsense. What emerges is a grand and inspiring \"scientific\" narrative of who we are and whence we came—the most recent and up-to-date version of this work having been summarized in a long peer-review article.\n\nBron Taylor's \"Encyclopedia of Religion and Nature\" gives four primary categories: cosmic, planetary, life, cultural.\n\nThe creation-evolution controversy goes beyond the field of evolutionary biology and includes many fields of science (cosmology, geology, paleontology, thermodynamics, physics and sociobiology).\n\nAlthough in the scientific community there is essentially universal agreement that the evidence of evolution is overwhelming, and the consensus supporting the neo-Darwinian evolutionary synthesis is nearly absolute, creationists have asserted that there is a significant scientific controversy and disagreement over the validity of the evolution epic.\n\nThe debate is sometimes portrayed as being between science and religion. However, as the National Academy of Sciences states:\n\nDr.John Haught, Roman Catholic theologian, in his \"Science and Religion: from Conflict to Conversation\" suggests a theistic acceptance of the Epic. He says contemporary theology is being changed by evolutionary science. There are many versions undergoing constant revision. He considers evolution to be, at least provisionally, a most appropriate and fruitful scientific framework within which to think about God today and deplores that contemporary theology gets hung up in the creationism controversy. There are liberal congregations these days that may see the epic of evolution as a history about life and the universe that is both scientific and sacred. The profoundly sacred elements of the story warm up the cold technical facts with awe and reverence, giving Nature an inspiring beauty.\n\nEric Chaisson, in his book, \"Epic of Evolution\", concludes that the coherent story of cosmic evolution—a powerful and noble effort—may perhaps be the way to ethical evolution in the new millennium. Although his is a decidedly materialistic view of the evolutionary epic, he recognizes more than most scientists that humanity is part of this grand story and that we have a responsibility to survive as the only sentient, intelligent beings known in the universe.\n\nNot all of the Epic's advocates are distinguished scientists. Some are Christians who consider it a 'narrative of mythic proportions' that contain religious aspects. They see it as a multifaceted concept that has been in Christian theology implicitly for hundreds of years and is congenial to perspectives that include ultimacy, transcendence, purpose and morality. However, there are both humanists and creationists who dispute this stance, making unclear the many varied theological stories of our world.\n\nIn 1996 the Institute on Religion in an Age of Science held a conference on the epic of evolution. In 1997 the American Association for the Advancement of Science organized a conference on the epic of evolution as part of their program on Dialogue on Science, Ethics, and Religion. In July 1999, The Forum on Religion and Ecology with special support from the Center for Respect of Life and Environment sponsored a conference titled The Epic of Evolution and World Religions. It consisted of a small invitational gathering of scholars of the world's religious traditions as well as a number of scientists and educators. It explored how the creation stories of the world's religions intersect with or react to the epic of evolution. An Evolutionary Epic conference was held in Hawaii in January 2008. It was attended by scientists, artists, educators, and spiritual and religious leaders. \n\nWashington University in St. Louis offers a course on the epic of evolution. The Epic has also been taught at Northern Arizona University. The course engaged the task of formulating a new epic myth that is based on the physical, natural, social, and cultural sciences for which there are as yet few textbooks. The course was presented in three segments: the cosmos before humans appeared, the human phenomenon, and scenarios for the future of evolution. An annual undergraduate course on \"cosmic evolution\" has been taught at Harvard University for most of the past 35 years.\n\nEvolution Sunday, a Christian church event (1,044 Congregations observed it in 2009) arose from the Clergy Letter Project signed in 2004 by 10,500 American clergy. It is spreading internationally and across other faiths. It supports the story of evolution in a manner similar to the Epic (science and religion compatibility) promoting serious discussion and reflection on the relationship between religion and science. \"For far too long, strident voices, in the name of Christianity, have been claiming that people must choose between religion and modern science,\" says Michael Zimmerman, founder of Evolution Sunday and dean of the College of Liberal Arts and Sciences at Butler University in Indianapolis. \"We're saying you can have your faith, and you can also have science.\"\n\nMichael Dowd and his wife Connie Barlow have traveled the US since 2002 by van as nomads teaching his \"Gospel of Evolution\".\n\n\nReading lists – Books of the Epic of Evolution,\nCosmic Evolution\n\n"}
{"id": "1533051", "url": "https://en.wikipedia.org/wiki?curid=1533051", "title": "Eruption column", "text": "Eruption column\n\nAn eruption column is a cloud of super-heated ash and tephra suspended in gases emitted during an explosive volcanic eruption. The volcanic materials form a column that may rise many kilometers into the air above the vent of the volcano. In the most explosive eruptions, the eruption column may rise over , penetrating the stratosphere. Stratospheric injection of aerosols by volcanoes is a major cause of short-term climate change.\n\nA common occurrence in explosive eruptions is \"column collapse\" when the eruption column is or becomes too dense to be lifted high into the sky by air convection, and instead falls down the slopes of the volcano to form pyroclastic flows or surges (although the latter is less dense). On some occasions, if the material isn't dense enough to fall, it may create pyrocumulonimbus clouds.\n\nEruption columns form in explosive volcanic activity, when the high concentration of volatile materials in the rising magma causes it to be disrupted into fine volcanic ash and coarser tephra. The ash and tephra are ejected at speeds of several hundred metres per second, and can rise rapidly to heights of several kilometres, lifted by enormous convection currents.\n\nEruption columns may be transient, if formed by a discrete explosion, or sustained, if produced by a continuous eruption or closely spaced discrete explosions.\n\nThe solid and/or liquid materials in an eruption column are lifted by processes which vary as the material ascends:\n\n\nThe column will stop rising once it attains an altitude where it is more dense than the surrounding air. Several factors control the height that an eruption column can reach.\n\nIntrinsic factors include the diameter of the erupting vent, the gas content of the magma, and the velocity at which it is ejected. Extrinsic factors can be important, with winds sometimes limiting the height of the column, and the local thermal temperature gradient also playing a role. The atmospheric temperature in the troposphere normally decreases by about 6-7 K/km, but small changes in this gradient can have a large effect on the final column height. Theoretically, the maximum achievable column height is thought to be about . In practice, column heights ranging from about are seen.\n\nEruption columns with heights of over break through the tropopause and inject particulates into the stratosphere. Ashes and aerosols in the troposphere are quickly removed by precipitation, but material injected into the stratosphere is much more slowly dispersed, in the absence of weather systems. Substantial amounts of stratospheric injection can have global effects: after Mount Pinatubo erupted in 1991, global temperatures dropped by about . The largest eruptions are thought to cause temperature drops down to several degrees, and are potentially the cause of some of the known mass extinctions.\n\nEruption column heights are a useful way of measuring eruption intensity since for a given atmospheric temperature, the column height is proportional to the fourth root of the mass eruption rate. Consequently, given similar conditions, to double the column height requires an eruption ejecting 16 times as much material per second. The column height of eruptions which have not been observed can be estimated by mapping the \"maximum\" distance that pyroclasts of different sizes are carried from the vent—the higher the column the further ejected material of a particular mass (and therefore size) can be carried.\n\nThe approximate maximum height of an eruption column is given by the equation.\nWhere:\n\nEruption columns may become so laden with dense material that they are too heavy to be supported by convection currents. This can suddenly happen if, for example, the rate at which magma is erupted increases to a point where insufficient air is entrained to support it, or if the magma density suddenly increases as denser magma from lower regions in a stratified magma chamber is tapped.\n\nIf it does happen, then material reaching the bottom of the convective thrust region can no longer be adequately supported by convection and will fall under gravity, forming a pyroclastic flow or surge which can travel down the slopes of a volcano at speeds of over . Column collapse is one of the most common and dangerous volcanic hazards in column-creating eruptions.\n\nSeveral eruptions have seriously endangered aircraft which have encountered or passed by the eruption column. In two separate incidents in 1982, airliners flew into the upper reaches of an eruption column blasted off by Mount Galunggung, and the ash severely damaged both aircraft. Particular hazards were the ingestion of ash stopping the engines, the sandblasting of the cockpit windows rendering them largely opaque and the contamination of fuel through the ingestion of ash through pressurisation ducts. The damage to engines is a particular problem since temperatures inside a gas turbine are sufficiently high that volcanic ash is melted in the combustion chamber, and forms a glass coating on components further downstream of it, for example on turbine blades.\n\nIn the case of British Airways Flight 9, the aircraft lost power on all four engines, and in the other, nineteen days later, three of the four engines failed on a Singapore Airlines 747. In both cases, engines were successfully restarted but the aircraft were forced to make emergency landings in Jakarta.\n\nSimilar damage to aircraft occurred due to an eruption column over Redoubt volcano in Alaska in 1989. Following the eruption of Mount Pinatubo in 1991, aircraft were diverted to avoid the eruption column, but nonetheless, fine ash dispersing over a wide area in Southeast Asia caused damage to 16 aircraft, some as far as from the volcano.\n\nEruption columns are not usually visible on weather radar and may be obscured by ordinary clouds or night. Because of the risks posed to aviation by eruption columns, there is a network of nine Volcanic Ash Advisory Centers around the world which continuously monitor for eruption columns using data from satellites, ground reports, pilot reports and meteorological models.\n\n\n\n"}
{"id": "2446565", "url": "https://en.wikipedia.org/wiki?curid=2446565", "title": "Extra-low voltage", "text": "Extra-low voltage\n\nExtra-low voltage (ELV) is an electricity supply voltage in a range which carries a low risk of dangerous electrical shock. There are various standards that define extra-low voltage. The International Electrotechnical Commission member organizations and the UK IET (BS 7671:2008) define an ELV device or circuit as one in which the electrical potential between conductor or electrical conductor and earth (ground) does not exceed 50 V a.c. or 120 V d.c. (ripple free). EU's Low Voltage Directive applies from 50 V a.c. or 75 V d.c.\n\nThe IEC and IET go on to define actual types of extra-low voltage systems, for example SELV, PELV, FELV. These can be supplied using sources including motor / fossil fuel generator sets, transformers, switched PSU's or rechargeable battery. SELV, PELV, FELV, are distinguished by various safety properties, supply characteristics and design voltages.\n\nSome types of landscape lighting use SELV / PELV (extra-low voltage) systems. Modern battery operated hand tools fall in the SELV category. In more arduous conditions 25 volts RMS alternating current / 60 volts (ripple-free) direct current can be specified to further reduce hazard. Lower voltage can apply in wet or conductive conditions where there is even greater potential for electric shock. These systems should still fall under the SELV / PELV (ELV) safety specifications.\n\nIEC defines a SELV system as \"an electrical system in which the voltage cannot exceed ELV under normal conditions, and under single-fault conditions, \"including\" earth faults in other circuits\". It is generally accepted that the acronym: SELV stands for separated extra-low voltage (separated from earth) as defined in installation standards (e.g. BS 7671), though BS EN 60335 refers to it as safety extra-low voltage.\n\nA SELV circuit must have:\n\nThe safety of a SELV circuit is provided by\n\nThe design of a SELV circuit typically involves an isolating transformer, guaranteed minimum distances between conductors and electrical insulation barriers. The electrical connectors of SELV circuits should be designed such that they do not mate with connectors commonly used for non-SELV circuits.\n\nTypical examples for a SELV circuit: decorative out-door lighting, Class III battery charger, fed from a Class II power supply. Modern cordless hand tools are considered SELV equipment.\n\nIEC 61140 defines a PELV system as \"an electrical system in which the voltage cannot exceed ELV under normal conditions, and under single-fault conditions, \"except\" earth faults in other circuits\".\n\nA PELV circuit only requires protective-separation from all circuits other than SELV and PELV (i.e., all circuits that might carry higher voltages), but it may have connections to other PELV systems and earth (ground).\n\nIn contrast to a SELV circuit, a PELV circuit can have a protective earth (ground) connection. A PELV circuit, just as with SELV, requires a design that guarantees a low risk of accidental contact with a higher voltage. For a transformer, this can mean that the primary and secondary windings must be separated by an extra insulation barrier, or by a conductive shield with a protective earth connection.\n\nA typical example for a PELV circuit is a computer with a Class I power supply.\n\nThe term functional extra-low voltage (FELV) describes any other extra-low-voltage circuit that does not fulfill the requirements for an SELV or PELV circuit. Although the FELV part of a circuit uses an extra-low voltage, it is not adequately protected from accidental contact with higher voltages in other parts of the circuit. Therefore the protection requirements for the higher voltage have to be applied to the entire circuit.\n\nExamples for FELV circuits include those that generate an extra low voltage through a semiconductor device or a potentiometer or a transformer. A typical example is an electronically controlled toaster where the timer circuit runs off extra low voltage derived from a tap on the heating element. Another is the old door-bell circuit fed from a transformer.\n\nThe IET / BSI (BS 7671) also define Reduced low voltage (RLV) which can be either single phase or three phase A.C. This system has been used for many years on construction sites. Both single and three phase. The single phase voltage is 110 V a.c. though having a \"centre tapped Earth\" reducing the voltage to Earth to 55 V a.c. The three phase is 110 V phase to phase, 63 V to Neutral / Earth. This system is slightly outside ELV but still very commonly used for cord powered hand tools and temporary lighting in hazardous areas.\n\nCabling for extra-low voltage systems, such as in remote-area power systems (RAPS), is designed to minimise energy losses while maximising safety. Lower voltages require a higher current for the same power. The higher current results in greater resistive losses in the cabling. Cable sizing must therefore consider maximum demand, voltage drop over the cable, and current-carrying capacity. Voltage drop is usually the main factor considered, but current-carrying capacity is as important when considering short, high-current runs such as between a battery bank and inverter.\n\nArcing is a risk in DC ELV systems, and some fuse types which can cause undesired arcing include semi-enclosed, rewireable and automotive fuse types. Instead high rupturing capacity fuses and appropriately rated circuit breakers are the recommended type for RAPS. Cable termination and connections must be done properly to avoid arcing also, and soldering is not recommended.\n\nPrecise definitions of \"extra low voltage\" are given in applicable wiring regulations in a region.\n\nIn the European Union, the \"Low Voltage Directive\" defines low voltage starting from 50 V AC, and 75 V DC. The directive only covers electrical equipment and not voltages appearing inside equipment or voltages in electrical components. IEC 60364 defines it as 50 V AC and 120 V DC.\n\nAS/NZS 3000 \"Wiring Rules\" define \"extra low voltage\" as \"Not exceeding 50 V a.c or 120 V ripple-free d.c.\" However, AS/ACIF S009 Clause 3.1.78.1 Extra-Low Voltage (ELV)states: \"a voltage not exceeding 42.4 V peak or 60 V d.c.[AS/NZS 60950.1:2003]\" and adds a note: \"This definition differs from the ELV definition contained in AS/NZS 3000:2000\" which is more closely aligned to the Telecommunications Network Voltage (TNV) limits ... i.e. 120 V d.c. or 70.7 V a.c. peak (50 V a.c. r.m.s.)\" which accommodates telephone ringing voltage on the nominally −48 Vdc battery supply which could be encountered on a telephone line and was not considered hazardous, whereas 120 Vac without current limiting at its source can inject 115mA into individuals leading to fibrillation of the heart.\n\nIn Brazil, ELV (\"Extra-baixa tensão\" or \"EBT\" in Portuguese) is officially defined in Regulatory Standard no. 10 from the Brazilian Ministry of Labor and Employment as any voltage \"not exceeding 50 volts a.c. or 120 volts d.c.\". Although that standard defines safety rules for electricity, the Regulatory Standard no. 12 requires an even lower voltage for start and stop devices on machines and equipment made from March 2012 and on, stating it shall not exceed 25 volts a.c. or 60 volts d.c.\n\n\n"}
{"id": "22343163", "url": "https://en.wikipedia.org/wiki?curid=22343163", "title": "Flora of South Georgia", "text": "Flora of South Georgia\n\nThis is a list of the flora of South Georgia, an island in the subantarctic Atlantic Ocean, part of the British overseas territory of South Georgia and the South Sandwich Islands.\n\nSpore producing plants.\n\n\nSeed plants.\n\n\n"}
{"id": "1818327", "url": "https://en.wikipedia.org/wiki?curid=1818327", "title": "Freshet", "text": "Freshet\n\nThe term freshet is most commonly used to describe a spring thaw resulting from snow and ice melt in rivers located in the northern latitudes of North America. A spring freshet can sometimes last several weeks on large river systems, resulting in significant inundation of flood plains as the snowpack melts in the river's catchment area. Freshets occur with generally diminishing strength and duration depending upon the snowpacks having large accumulations and then the local average rates of warming temperatures; late spring melts allowing faster flooding from the relatively longer days and higher solar angle against more southerly latitudes and elevations reaching average melting temperatures sooner where earlier and generally lesser seasonal snow piles melt more gradually spread over a longer melt period. Serious flooding from southern freshets are more often related to rain storms of large tropical weather systems rolling in from the South Atlantic or Gulf of Mexico, to add their powerful heating capacity to lesser snow packs. Tropically induced rainfall influenced quick melts can also affect snow cover to latitudes as far north as southern Canada, so long as the generally colder air mass is not blocking northward movement of low pressure systems.\nIn the eastern part of the continent, annual freshets occur from the Canadian Taiga ranging along both sides of the Great Lakes then down through the heavily forested Appalachian mountain chain and St. Lawrence valley from Northern Maine into barrier ranges in North Carolina and Tennessee.\n\nIn the western part of the continent, freshets occur throughout the generally much higher elevations of the various west coast mountain ranges that extend southward down from Alaska even into the northern parts of Arizona and New Mexico.\n\n\n"}
{"id": "5077458", "url": "https://en.wikipedia.org/wiki?curid=5077458", "title": "George Colvocoresses", "text": "George Colvocoresses\n\nGeorge Musalas \"Colvos\" Colvocoresses (October 22, 1816 – June 3, 1872) was an American Navy officer who commanded the during the American Civil War. From 1838 up until 1842, he took part in the United States Exploring Expedition, which explored large regions of the Pacific Ocean.\n\nHe was born on the island of Chios in the Greek Archipelago on October 22, 1816. As a member of a prominent Greek family, he was kidnapped along with his mother and two sisters and ransomed from the Turks after the massacre of the Greek population of the island in 1822, during the Greek War of Independence. His family's fortunes were devastated by the massacre. Most close relatives, including six brothers, were killed. He was enslaved at 6 years of age, but his father managed to buy back his freedom.\n\nBy 1824, Colvocoresses was sent to Baltimore in the United States by his father. He became the adopted son of Captain Alden Partridge, the founder of the American Literary, Scientific, and Military Academy (later Norwich University) in Norwich, Vermont. Colvocoresses entered the Navy after graduating from the Academy in 1831. Several generations of his family have also graduated from Norwich and followed military careers.\n\nIn 1832, he was appointed a midshipman, and in 1836-1837 attached to the frigate on the Mediterranean Squadron. From 1838 up until 1842, he served in the United States Exploring Expedition, better known as the Wilkes Expedition of the Pacific Ocean. Colvocoresses authored a work on the Wilkes Expedition in 1852, entitled \"Four Years in a Government Exploring Expedition\". He was promoted to lieutenant on December 7, 1843 and to commander on July 2, 1861. \n\nOn January 29, 1862, the vessel , under his command, captured the Confederate schooner \"Stephen Hart\" off the south coast of Florida, carrying assorted cargo. In early August 1864 with 115 men in 7 boats from his sloop , he conducted an expedition to gather intelligence and capture enemy prisoners. Two week later, at South Newport, Georgia, Colvocoresses led 130 men in boats, capturing a lieutenant and 38 privates of the Third South Carolina Cavalry, six overseers of saltworks he had destroyed, and 71 slaves.\n\nHe was promoted to captain and placed on the retired list on April 4, 1867.\n\nColvocoresses was robbed and murdered in Bridgeport, Connecticut on June 3, 1872 while on his way to New York. The case was never solved.\n\nColvocoresses was married twice. From his first union, with Adeline Maria Swasey, he had four children, including Frank E. and George P. His second marriage was with Eliza T. Halsey.\n\n\n\n"}
{"id": "42324849", "url": "https://en.wikipedia.org/wiki?curid=42324849", "title": "Holitsynske gas field", "text": "Holitsynske gas field\n\nThe Holitsynske gas field natural gas field located on the continental shelf of the Black Sea. It was discovered in 1974 and developed by Chornomornaftogaz. It started commercial production in 1975. The total proven reserves of the Holitsynske gas field are around , and production is slated to be around in 2015.\n"}
{"id": "5571489", "url": "https://en.wikipedia.org/wiki?curid=5571489", "title": "Instability strip", "text": "Instability strip\n\nThe unqualified term instability strip usually refers to a region of the Hertzsprung–Russell diagram largely occupied by several related classes of pulsating variable stars: Delta Scuti variables, SX Phoenicis variables, and rapidly oscillating Ap stars (roAps) near the main sequence; RR Lyrae variables where it intersects the horizontal branch; and the Cepheid variables where it crosses the supergiants.\n\nRV Tauri variables are also often considered to lie on the instability strip, occupying the area to the right of the brighter Cepheids (at lower temperatures), since their pulsations are attributed to the same mechanism.\n\nThe instability strip intersects the main sequence in the region of A and F stars (1–2 solar mass ()) and extends to G and early K bright supergiants (early M if RV Tauri stars at minimum are included). Above the main sequence, the vast majority of stars in the instability strip are variable. Where the instability strip intersects the main sequence, the vast majority of stars are stable, but there are some variables, including the roAp stars.\n\nStars in the instability strip pulsate due to He III (doubly ionized helium). In normal A-F-G stars He is neutral in the stellar photosphere. Deeper below the photosphere, at about 25,000–30,000K, begins the He II layer (first He ionization). Second ionization (He III) starts at about 35,000–50,000K.\n\nWhen the star contracts, the density and temperature of the He II layer increases. He II starts to transform into He III (second ionization). This causes the opacity of the star to increase and the energy flux from the interior of the star is effectively absorbed. The temperature of the star rises and it begins to expand. After expansion, He III begins to recombine into He II and the opacity of the star drops. This lowers the surface temperature of the star. The outer layers contract and the cycle starts from the beginning.\n\nThe phase shift between a star's radial pulsations and brightness variations depends on the distance of He II zone from the stellar surface in the stellar atmosphere. For most Cepheids, this creates a distinctly asymmetrical observed light curve, rising rapidly to maximum and falling slowly back down to minimum.\n\nThere are several types of pulsating star not found on the instability strip and with pulsations driven by different mechanisms. At cooler temperatures are the long period variable AGB stars. At hotter temperatures are the Beta Cephei and PV Telescopii, variables. Right at the edge of the instability strip near the main sequence are Gamma Doradus variables. The band of White dwarfs has three separate regions types of variable: DOV, DBV, and DAV (= ZZ Ceti variables) white dwarfs. Each of these types of pulsating variable has an associated instability strip created by variable opacity partial ionisation regions other than helium.\n\nMost high luminosity supergiants are somewhat variable, including the Alpha Cygni variables. In the specific region of more luminous stars above the instability strip are found the yellow hypergiants which have irregular pulsations and eruptions. The hotter luminous blue variables may be related and show similar short- and long-term spectral and brightness variations with irregular eruptions.\n"}
{"id": "40789564", "url": "https://en.wikipedia.org/wiki?curid=40789564", "title": "International Association of Geoanalysts", "text": "International Association of Geoanalysts\n\nThe International Association of Geoanalysts (referred to as the IAG) is an international not-for-profit learned society. Its objectives are to: \"serve as an international forum for the advancement of geoanalytical science and to promote the interests and support the professional needs of those involved in the analysis of geological and environmental materials\".\n\nThe association aims to improve and promote the assessment of measurement uncertainty and data quality as fit for purpose through proficiency testing and the development and use of geochemical reference materials.\nThe IAG organises meetings, conferences, workshops and short courses, is associated with journal publishing, and operates two proficiency testing schemes: GeoPT, initiated in 1994 (or 1996), intended for analytical chemistry laboratories dealing with whole-rock geochemical analyses and G-Probe for microprobe and laser ablation ICP-MS geochemical laboratories. The IAG also carries out the certification of geological and environmental reference materials in accordance with its own Certification Protocol, and holds liaison membership of ISO-REMCO to represent the views of the geochemical research community particularly in matters of metrology.\n\nThe IAG has a legal-commercial arm, IAGeo Limited, which distributes, markets and develops reference materials for scientific research.\n\nThe International Association of Geoanalysts was founded on 2 June 1997 during the 'Geoanalysis 97' Conference held at Vail, Colorado, USA. It is a successor organisation to the International Working Group-Groupe International de Travail (IWG-GIT) on geochemical reference materials, which operated under the direction of Dr K. Govindaraju at the CRPG, Nancy (France) from 1977 to its dissolution in 1996, and which was closely associated with the journal Geostandards Newsletter.\n\nThe IAG is directed by a fifteen-member council, the current president of which is Professor Thomas C. Meisel (Montanuniversität Leoben, Austria). The past presidents of the association are: Douglas L. Miles (formerly of the British Geological Survey), Philip J. Potts (The Open University, UK) and Michael Wiedenbeck (Helmholtz-Zentrum Potsdam, Germany).\n\nThe IAG is an editorial partner of the quarterly journal \"Geostandards and Geoanalytical Research\" published by Wiley-Blackwell. The association is also a joint publishing partner of - an International Magazine of Mineralogy, Geochemistry and Petrology.\n\nThe association has a number of special interest groups dealing with its core activities. These are:\n\nThe IAG is the main sponsor of the triennial Geoanalysis conference – an international meeting series that has taken place since 1990. Scientific proceedings of the Geoanalysis conferences are published in Geostandards and Geoanalytical Research. The IAG also organises short courses on Quality Assurance in geoanalysis as well as workshops dealing with isotope determination in geochemistry.\n\nThe IAG dispenses an annual Early Career Researcher Award for excellence in analytical geochemistry. Eligibility is limited to scientists who are currently pursuing a higher degree in a field related to geoanalysis or who have completed their university education within the past five years. The award promotes the careers of young scientists who have either developed innovative analytical methods or provided new strategies to improve data quality as applied to the chemical analysis of geological or environmental samples.\n\n"}
{"id": "326386", "url": "https://en.wikipedia.org/wiki?curid=326386", "title": "Ion source", "text": "Ion source\n\nAn ion source is a device that creates atomic and molecular ions. Ion sources are used to form ions for mass spectrometers, optical emission spectrometers, particle accelerators, ion implanters and ion engines.\n\nElectron ionization is widely used in mass spectrometry, particularly for organic molecules. The gas phase reaction producing electron ionization is\n\nwhere M is the atom or molecule being ionized, <chem>e^-</chem> is the electron, and <chem>M^{+\\bullet}</chem> is the resulting ion.\n\nThe electrons may be created by an arc discharge between a cathode and an anode.\n\nAn electron beam ion source (EBIS) is used in atomic physics to produce highly charged ions by bombarding atoms with a powerful electron beam. Its principle of operation is shared by the electron beam ion trap.\n\nElectron capture ionization (ECI) is the ionization of a gas phase atom or molecule by attachment of an electron to create an ion of the form A. The reaction is\n\nwhere the M over the arrow denotes that to conserve energy and momentum a third body is required (the molecularity of the reaction is three).\n\nElectron capture can be used in conjunction with chemical ionization.\n\nAn electron capture detector is used in some gas chromatography systems.\n\nChemical ionization (CI) is a lower energy process than electron ionization because it involves ion/molecule reactions rather than electron removal. The lower energy yields less fragmentation, and usually a simpler spectrum. A typical CI spectrum has an easily identifiable molecular ion.\n\nIn a CI experiment, ions are produced through the collision of the analyte with ions of a reagent gas in the ion source. Some common reagent gases include: methane, ammonia, and isobutane. Inside the ion source, the reagent gas is present in large excess compared to the analyte. Electrons entering the source will preferentially ionize the reagent gas. The resultant collisions with other reagent gas molecules will create an ionization plasma. Positive and negative ions of the analyte are formed by reactions with this plasma. For example, protonation occurs by\n\nCharge-exchange ionization (also known as charge-transfer ionization) is a gas phase reaction between an ion and an atom or molecule in which the charge of the ion is transferred to the neutral species.\n\nChemi-ionization is the formation of an ion through the reaction of a gas phase atom or molecule with an atom or molecule in an excited state. Chemi-ionization can be represented by\nwhere G is the excited state species (indicated by the superscripted asterisk), and M is the species that is ionized by the loss of an electron to form the radical cation (indicated by the superscripted \"plus-dot\").\n\nAssociative ionization is a gas phase reaction in which two atoms or molecules interact to form a single product ion. One or both of the interacting species may have excess internal energy.\n\nFor example,\n\nwhere species A with excess internal energy (indicated by the asterisk) interacts with B to form the ion AB.\n\nPenning ionization is a form of chemi-ionization involving reactions between neutral atoms or molecules. The process is named after the Dutch physicist Frans Michel Penning who first reported it in 1927. Penning ionization involves a reaction between a gas-phase excited-state atom or molecule G and a target molecule M resulting in the formation of a radical molecular cation M, an electron e, and a neutral gas molecule G:\n\nPenning ionization occurs when the target molecule has an ionization potential lower than the internal energy of the excited-state atom or molecule.\n\nAssociative Penning ionization can proceed via\n\nSurface Penning ionization (also known as Auger deexcitation) refers to the interaction of the excited-state gas with a bulk surface S, resulting in the release of an electron according to\n\nIon-attachment ionization is similar to chemical ionization in which a cation is attached to the analyte molecule in a reactive collision:\n\nWhere M is the analyte molecule, X is the cation and A is a non-reacting collision partner.\n\nIn a radioactive ion source, a small piece of radioactive material, for instance Ni or Am, is used to ionize a gas. This is used in ionization smoke detectors and ion mobility spectrometers.\n\nThese ion sources use a plasma source or electric discharge to create ions.\n\nIons can be created in an inductively coupled plasma, which is a plasma source in which the energy is supplied by electrical currents which are produced by electromagnetic induction, that is, by time-varying magnetic fields.\n\nMicrowave induced plasma ion sources are capable of exciting electrodeless gas discharges to create ions for trace element mass spectrometry. A microwave plasma is a type of plasma, that has high frequency electromagnetic radiation in the GHz range. It is capable of exciting electrodeless gas discharges. If applied in surface-wave-sustained mode, they are especially well suited to generate large-area plasmas of high plasma density. If they are both in surface-wave and resonator mode, they can exhibit a high degree of spatial localization. This allows to spatially separate the location of plasma generations from the location of surface processing. Such a separation (together with an appropriate gas-flow scheme) may help reduce the negative effect, that particles released from a processed substrate may have on the plasma chemistry of the gas phase.\n\nThe ECR ion source makes use of the electron cyclotron resonance to ionize a plasma. Microwaves are injected into a volume at the frequency corresponding to the electron cyclotron resonance, defined by the magnetic field applied to a region inside the volume. The volume contains a low pressure gas.\n\nIons can be created in an electric glow discharge. A glow discharge is a plasma formed by the passage of electric current through a low-pressure gas. It is created by applying a voltage between two metal electrodes in an evacuated chamber containing gas. When the voltage exceeds a certain value, called the striking voltage, the gas forms a plasma.\n\nA duoplasmatron is a type of glow discharge ion source that consists of a hot cathode or cold cathode that produces a plasma that is used to ionize a gas. Duoplasmatrons can produce positive or negative ions. Duoplasmatrons are used for secondary ion mass spectrometry., ion beam etching, and high-energy physics.\n\nIn a flowing afterglow, ions are formed in a flow of inert gas, typically helium or argon. Reagents are added downstream to create ion products and study reaction rates. Flowing-afterglow mass spectrometry is used for trace gas analysis for organic compounds.\n\nElectric spark ionization is used to produce gas phase ions from a solid sample. When incorporated with a mass spectrometer the complete instrument is referred to as a spark ionization mass spectrometer or as a spark source mass spectrometer (SSMS).\n\nA closed drift ion source uses a radial magnetic field in an annular cavity in order to confine electrons for ionizing a gas. They are used for ion implantation and for space propulsion (Hall effect thrusters).\n\nPhotoionization is the ionization process in which an ion is formed from the interaction of a photon with an atom or molecule.\n\nIn multi-photon ionization (MPI), several photons of energy below the ionization threshold may actually combine their energies to ionize an atom.\n\nResonance-enhanced multiphoton ionization (REMPI) is a form of MPI in which one or more of the photons accesses a bound-bound transition that is resonant in the atom or molecule being ionized.\n\nAtmospheric pressure photoionization uses a source of photons, usually a vacuum UV (VUV) lamp, to ionize the analyte with single photon ionization process. Analogous to other atmospheric pressure ion sources, a spray of solvent is heated to relatively high temperatures (above 400 degrees Celsius) and sprayed with high flow rates of nitrogen for desolvation. The resulting aerosol is subjected to UV radiation to create ions. Atmospheric pressure laser ionization uses UV laser light sources to ionize the analyte via MPI.\n\nField desorption refers to an ion source in which a high-potential electric field is applied to an emitter with a sharp surface, such as a razor blade, or more commonly, a filament from which tiny \"whiskers\" have formed. This results in a very high electric field which can result in ionization of gaseous molecules of the analyte. Mass spectra produced by FI have little or no fragmentation. They are dominated by molecular radical cations <chem>M^{+.}</chem> and less often, protonated molecules <chem>[{M}+H]+</chem>.\n\nParticle bombardment with atoms is called fast atom bombardment (FAB) and bombardment with atomic or molecular ions is called secondary ion mass spectrometry (SIMS). Fission fragment ionization uses ionic or neutral atoms formed as a result of the nuclear fission of a suitable nuclide, for example the Californium isotope Cf.\n\nIn FAB the analytes is mixed with a non-volatile chemical protection environment called a matrix and is bombarded under vacuum with a high energy (4000 to 10,000 electron volts) beam of atoms. The atoms are typically from an inert gas such as argon or xenon. Common matrices include glycerol, thioglycerol, 3-nitrobenzyl alcohol (3-NBA), 18-crown-6 ether, 2-nitrophenyloctyl ether, sulfolane, diethanolamine, and triethanolamine. This technique is similar to secondary ion mass spectrometry and plasma desorption mass spectrometry.\n\nSecondary ion mass spectrometry (SIMS) is used to analyze the composition of solid surfaces and thin films by sputtering the surface of the specimen with a focused primary ion beam and collecting and analyzing ejected secondary ions. The mass/charge ratios of these secondary ions are measured with a mass spectrometer to determine the elemental, isotopic, or molecular composition of the surface to a depth of 1 to 2 nm.\n\nIn a liquid metal ion source (LMIS), a metal (typically gallium) is heated to the liquid state and provided at the end of a capillary or a needle. Then a Taylor cone is formed under the application of a strong electric field. As the cone's tip get sharper, the electric field becomes stronger, until ions are produced by field evaporation. These ion sources are particularly used in ion implantation or in focused ion beam instruments.\n\nPlasma desorption ionization mass spectrometry (PDMS), also called fission fragment ionization, is a mass spectrometry technique in which ionization of material in a solid sample is accomplished by bombarding it with ionic or neutral atoms formed as a result of the nuclear fission of a suitable nuclide, typically the californium isotope Cf.\n\nMatrix-assisted laser desorption/ionization (MALDI) is a soft ionization technique. The sample is mixed with a matrix material. Upon receiving a laser pulse, the matrix absorbs the laser energy and it is thought that primarily the matrix is desorbed and ionized (by addition of a proton) by this event. The analyte molecules are also desorbed. The matrix is then thought to transfer proton to the analyte molecules (e.g., protein molecules), thus charging the analyte.\n\nSurface-assisted laser desorption/ionization (SALDI) is a soft laser desorption technique used for analyzing biomolecules by mass spectrometry. In its first embodiment, it used graphite matrix. At present, laser desorption/ionization methods using other inorganic matrices, such as nanomaterials, are often regarded as SALDI variants. A related method named \"ambient SALDI\" - which is a combination of conventional SALDI with ambient mass spectrometry incorporating the DART ion source - has also been demonstrated.\n\nSurface-enhanced laser desorption/ionization (SELDI) is a variant of MALDI that is used for the analysis of protein mixtures that uses a target modified to achieve biochemical affinity with the analyte compound.\n\nDesorption ionization on silicon (DIOS) refers to laser desorption/ionization of a sample deposited on a porous silicon surface.\n\nA laser vaporization cluster source produces ions using a combination of laser desorption ionization and supersonic expansion. The Smalley source (or Smalley cluster source) was developed by Richard Smalley at Rice University in the 1980s and was central to the discovery of fullerenes in 1985.\n\nIn aerosol mass spectrometry with time-of-flight analysis, micrometer sized solid aerosol particles extracted from the atmosphere are simultaneously desorbed and ionized by a precisely timed laser pulse as they pass through the center of a time-of-flight ion extractor.\n\nSpray ionization methods involve the formation of aerosol particles from a liquid solution and the formation of bare ions after solvent evaporation.\n\nSolvent-assisted ionization (SAI) is a method in which charged droplets are produced by introducing a solution containing analyte into a heated inlet tube of an atmospheric pressure ionization mass spectrometer. Just as in Electrospray Ionization (ESI), desolvation of the charged droplets produces multiply charged analyte ions. Volatile and nonvolatile compounds are analyzed by SAI, and high voltage is not required to achieve sensitivity comparable to ESI. Application of a voltage to the solution entering the hot inlet through a zero dead volume fitting connected to fused silica tubing produces ESI-like mass spectra, but with higher sensitivity. The inlet tube to the mass spectrometer becomes the ion source.\n\nMatrix-Assisted Ionization [MAI] is similar to MALDI in sample preparation, but a laser is not required to convert analyte molecules included in a matrix compound into gas-phase ions. In MAI, analyte ions have charge states similar to electrospray ionization but obtained from a solid matrix rather than a solvent. No voltage or laser is required, but a laser can be used to obtain spatial resolution for imaging. Matrix-analyte samples are ionized in the vacuum of a mass spectrometer and can be inserted into the vacuum through an atmospheric pressure inlet. Less volatile matrices such as 2,5-dihydroxybenzoic acid require a hot inlet tube to produce analyte ions by MAI, but more volatile matrices such as 3-nitrobenzonitrile require no heat, voltage, or laser. Simply introducing the matrix:analyte sample to the inlet aperture of an atmospheric pressure ionization mass spectrometer produces abundant ions. Compounds at least as large as bovine serum albumin [66 kDa] can be ionized with this method. In this simple, low cost and easy to use ionization method, the inlet to the mass spectrometer can be considered the ion source.\n\nAtmospheric pressure chemical ionization is a form of chemical ionization using a solvent spray at atmospheric pressure. A spray of solvent is heated to relatively high temperatures (above 400 degrees Celsius), sprayed with high flow rates of nitrogen and the entire aerosol cloud is subjected to a corona discharge that creates ions with the evaporated solvent acting as the chemical ionization reagent gas. APCI is not as \"soft\" (low fragmentation) an ionization technique as ESI. Note that atmospheric pressure ionization (API) should not be used as a synonym for APCI.\n\nThermospray ionization is a form of atmospheric pressure ionization in mass spectrometry. It transfers ions from the liquid phase to the gas phase for analysis. It is particularly useful in liquid chromatography-mass spectrometry.\n\nIn electrospray ionization, a liquid is pushed through a very small, charged and usually metal, capillary. This liquid contains the substance to be studied, the analyte, dissolved in a large amount of solvent, which is usually much more volatile than the analyte. Volatile acids, bases or buffers are often added to this solution too. The analyte exists as an ion in solution either in its anion or cation form. Because like charges repel, the liquid pushes itself out of the capillary and forms an aerosol, a mist of small droplets about 10 μm across. The aerosol is at least partially produced by a process involving the formation of a Taylor cone and a jet from the tip of this cone. An uncharged carrier gas such as nitrogen is sometimes used to help nebulize the liquid and to help evaporate the neutral solvent in the droplets. As the solvent evaporates, the analyte molecules are forced closer together, repel each other and break up the droplets. This process is called Coulombic fission because it is driven by repulsive Coulombic forces between charged molecules. The process repeats until the analyte is free of solvent and is a bare ion. The ions observed are created by the addition of a proton (a hydrogen ion) and denoted <chem>[{M}+H]+</chem>, or of another cation such as sodium ion, <chem>[M + Na]+</chem>, or the removal of a proton, <chem>[M - H]^-</chem>. Multiply charged ions such as <chem>[{M}+2H]^2+</chem> are often observed. For large macromolecules, there can be many charge states, occurring with different frequencies; the charge can be as great as <chem>[M + 25H]^{25+}</chem>, for example.\n\nProbe electrospray ionization (PESI) is a modified version of electrospray, where the capillary for sample solution transferring is replaced by a sharp-tipped solid needle with periodical motion.\n\nContactless atmospheric pressure ionization is a technique used for analysis of liquid and solid samples by mass spectrometry. Contactless API can be operated without an additional electric power supply (supplying voltage to the source emitter), gas supply, or syringe pump. Thus, the technique provides a facile means for analyzing chemical compounds by mass spectrometry at atmospheric pressure.\n\nSonic spray ionization is method for creating ions from a liquid solution, for example, a mixture of methanol and water. A pneumatic nebulizer is used to turn the solution into a supersonic spray of small droplets. Ions are formed when the solvent evaporates and the statistically unbalanced charge distribution on the droplets leads to a net charge and complete desolvation results in the formation of ions. Sonic spray ionization is used to analyze small organic molecules and drugs and can analyze large molecules when an electric field is applied to the capillary to help increase the charge density and generate multiple charged ions of proteins.\n\nSonic spray ionization has been coupled with high performance liquid chromatography for the analysis of drugs. Oligonucleotides have been studied with this method. SSI has been used in a manner similar to desorption electrospray ionization for ambient ionization and has been coupled with thin layer chromatography in this manner.\n\nUltrasonication-assisted spray ionization (UASI) involves ionization through the application of ultrasound.\n\nThermal ionization (also known as surface ionization, or contact ionization) involves spraying vaporized, neutral atoms onto a hot surface, from which the atoms re-evaporate in ionic form. To generate positive ions, the atomic species should have a low ionization energy, and the surface should have a high work function. This technique is most suitable for alkali atoms (Li, Na, K, Rb, Cs) which have low ionization energies and are easily evaporated.\n\nTo generate negative ions, the atomic species should have a high electron affinity, and the surface should have a low work function. This second approach is most suited for halogen atoms Cl, Br, I, At.\n\nIn ambient ionization, ions are formed outside the mass spectrometer without sample preparation or separation. Ions can be formed by extraction into charged electrospray droplets, thermally desorbed and ionized by chemical ionization, or laser desorbed or ablated and post-ionized before they enter the mass spectrometer.\n\nSolid-liquid extraction based ambient ionization uses a charged spray to create a liquid film on the sample surface. Molecules on the surface are extracted into the solvent. The action of the primary droplets hitting the surface produces secondary droplets that are the source of ions for the mass spectrometer. Desorption electrospray ionization (DESI) uses an electrospray source to create charged droplets that are directed at a solid sample a few millimeters to a few centimeters away. The charged droplets pick up the sample through interaction with the surface and then form highly charged ions that can be sampled into a mass spectrometer.\n\nPlasma-based ambient ionization is based on an electrical discharge in a flowing gas that produces metastable atoms and molecules and reactive ions. Heat is often used to assist in the desorption of volatile species from the sample. Ions are formed by chemical ionization in the gas phase. A direct analysis in real time source operates by exposing the sample to a dry gas stream (typically helium or nitrogen) that contains long-lived electronically or vibronically excited neutral atoms or molecules (or \"metastables\"). Excited states are typically formed in the DART source by creating a glow discharge in a chamber through which the gas flows. A similar method called atmospheric solids analysis probe [ASAP] uses the heated gas from ESI or APCI probes to vaporize sample placed on a melting point tube inserted into an ESI/APCI source. Ionization is by APCI.\n\nLaser-based ambient ionization is a two-step process in which a pulsed laser is used to desorb or ablate material from a sample and the plume of material interacts with an electrospray or plasma to create ions. Electrospray-assisted laser desorption/ionization (ELDI) uses a 337 nm UV laser or 3 µm infrared laser to desorb material into an electrospray source. Matrix-assisted laser desorption electrospray ionization (MALDESI) is an atmospheric pressure ionization source for generation of multiply charged ions. An ultraviolet or infrared laser is directed onto a solid or liquid sample containing the analyte of interest and matrix desorbing neutral analyte molecules that are ionized by interaction with electrosprayed solvent droplets generating multiply charged ions. Laser ablation electrospray ionization (LAESI) is an ambient ionization method for mass spectrometry that combines laser ablation from a mid-infrared (mid-IR) laser with a secondary electrospray ionization (ESI) process.\n\nIn a mass spectrometer a sample is ionized in an ion source and the resulting ions are separated by their mass-to-charge ratio. The ions are detected and the results are displayed as spectra of the relative abundance of detected ions as a function of the mass-to-charge ratio. The atoms or molecules in the sample can be identified by correlating known masses to the identified masses or through a characteristic fragmentation pattern.\n\nIn particle accelerators an ion source creates a particle beam at the beginning of the machine, the \"source\". The technology to create ion sources for particle accelerators depends strongly on the type of particle that needs to be generated: electrons, protons, H ion or a Heavy ions.\n\nElectrons are generated with an electron gun, and there are many varieties of these.\n\nProtons are generated with a plasma-based device, like a duoplasmatron or a magnetron.\n\nH ions are generated with a magnetron or a Penning source. A magnetron consists of a central cylindrical cathode surrounded by an anode. The discharge voltage is typically greater than 150 V and the current drain is around 40 A. A magnetic field of about 0.2 tesla is parallel to the cathode axis. Hydrogen gas is introduced by a pulsed gas valve. Caesium is often used to lower the work function of the cathode, enhancing the amount of ions that are produced.\n\nFor a Penning source, a strong magnetic field parallel to the electric field of the sheath guides electrons and ions on cyclotron spirals from cathode to cathode. Fast H-minus ions are generated at the cathodes as in the magnetron. They are slowed down due to the charge exchange reaction as they migrate to the plasma aperture. This makes for a beam of ions that is colder than the ions obtained from a magnetron.\n\nHeavy ions can be generated with an electron cyclotron resonance ion source. The use of electron cyclotron resonance (ECR) ion sources for the production of intense beams of highly charged ions has immensely grown over the last decade. ECR ion sources are used as injectors into linear accelerators, Van-de-Graaff generators or cyclotrons in nuclear and elementary particle physics. In atomic and surface physics ECR ion sources deliver intense beams of highly charged ions for collision experiments or for the investigation of surfaces. For the highest charge states, however, Electron beam ion sources (EBIS) are needed. They can generate even bare ions of mid-heavy elements. The Electron beam ion trap (EBIT), based on the same principle, can produce up to bare uranium ions and can be used as an ion source as well.\n\nHeavy ions can also be generated with an Ion Gun which typically uses the thermionic emission of electrons to ionize a substance in its gaseous state. Such instruments are typically used for surface analysis.\n\nGas flows through the ion source between the anode and the cathode. A positive voltage is applied to the anode. This voltage, combined with the high magnetic field between the tips of the internal and external cathodes allow a plasma to start. Ions from the plasma are repelled by the anode electric field. This creates an ion beam. \n\n\n"}
{"id": "53972849", "url": "https://en.wikipedia.org/wiki?curid=53972849", "title": "Irmak Nature Park", "text": "Irmak Nature Park\n\nIrmak Nature Park () is a nature park located in Sarıyer district of Istanbul Province, Turkey.\n\nIrmak Nature Park is situated across Kirazlıbent Nature Park inside the Belgrad Forest and is west of Bahçeköy, Sarıyer. The area was declared a nature park by the Ministry of Environment and Forest in 2011, and is one of the nine nature parks inside the Belgrad Forest. It covers an area of about . The protected area is named \nfor the creek () running through the protected area.\n\nThe nature park offers outdoor recreation activities such as hiking, cycling and picnicing for visitors on daily basis.[2] There are playgrounds for children. Admission is charged for visitors and vehicles.\n\nVegetation of the nature park is dense wood of deciduous trees comprising sessile oak (\"Quercus petraea\"), Kasnak oak (\"Quercus vulcanica\"), hornbeam (\"Carpinus betulus\"), European alder (\"Alnus glutinosa\"), sweet chestnut (\"Castanea sativa\"), blackthorn (\"Prunus spinosa\"), laurestine (\"Viburnum tinus\"), common ivy (\"Hedera helix\"), butcher's-broom (\"Ruscus aculeatus\") and blackberry (Rubus).\n\nAnimals mainly observed in the Belgrad Forest are wild boar, golden jackal, deer, roe deer, fox, gray wolf, marten, hare, squirrel, porcupine, wood mouse, the reptile tortoise and the bird species buzzard, hawk, magpie, crow, woodpecker, passer, finch and goldfinch.\n\n"}
{"id": "52953824", "url": "https://en.wikipedia.org/wiki?curid=52953824", "title": "Koca Katran (Mersin)", "text": "Koca Katran (Mersin)\n\nKoca Katran (literalily \"big cedar\") is a monumental old prickly cedar (\"Juniperus oxycedrus\") in Mersin Province, southern Turkey. It is a registered natural monument of the country.\n\nThe tree is in Dümbelek Pass of Cocokdere Valley in Toros Mountains at an elevation of . Nearest settlements are Arslanköy town and Tırtar village of Toroslar district in Mersin Province. Its distance to Mersin city center is about . Entrance to the valley, where the tree is situated, is barred, and access is subject to permission from the forestry authority.\n\nThe tree is high and the circumference of its trunk is at diameter. The nameplate at site states the age of the cedar as 625 years old.\n\nThe old tree was registered a natural monument on September 27, 1994.\n"}
{"id": "698720", "url": "https://en.wikipedia.org/wiki?curid=698720", "title": "Landau damping", "text": "Landau damping\n\nIn physics, Landau damping, named after its discoverer, the eminent Soviet physicist Lev Davidovich Landau (1908–68), is the effect of damping (exponential decrease as a function of time) of longitudinal space charge waves in plasma or a similar environment. This phenomenon prevents an instability from developing, and creates a region of stability in the parameter space. It was later argued by Donald Lynden-Bell that a similar phenomenon was occurring in galactic dynamics, where the gas of electrons interacting by electrostatic forces is replaced by a \"gas of stars\" interacting by gravitation forces. Landau damping can be manipulated exactly in numerical simulations such as particle-in-cell simulation. It was proved to exist experimentally by Malmberg and Wharton in 1964, almost two decades after its prediction by Landau in 1946.\n\nLandau damping occurs because of the energy exchange between an electromagnetic wave with phase velocity formula_1 and particles in the plasma with velocity approximately equal to formula_1, which can interact strongly with the wave. Those particles having velocities slightly less than formula_1 will be accelerated by the electric field of the wave to move with the wave phase velocity, while those particles with velocities slightly greater than formula_1 will be decelerated losing energy to the wave: particles tend to synchronize with the wave. This is proved experimentally with a Traveling-wave tube.\n\nIn an ideal MHD plasma the particle velocities are often taken to be approximately a Maxwellian distribution function. \nIf the slope of the function is negative, the number of particles with velocities slightly less than the wave phase velocity is greater than the number of particles with velocities slightly greater. Hence, there are more particles gaining energy from the wave than losing to the wave, which leads to wave damping.\nIf, however, the slope of the function is positive, the number of particles with velocities slightly less than the wave phase velocity is smaller than the number of particles with velocities slightly greater. Hence, there are more particles losing energy to the wave than gaining from the wave, which leads to a resultant increase in the wave energy.\n\nThe mathematical theory of Landau damping is somewhat involved—see the section below. However, there is a simple physical interpretation [introduced in section 7.5 of with a caveat], which, though not strictly correct, helps to visualize this phenomenon.\n\nIt is possible to imagine Langmuir waves as waves in the sea, and the particles as surfers trying to catch the wave, all moving in the same direction. If the surfer is moving on the water surface at a velocity slightly less than the waves he will eventually be caught and pushed along the wave (gaining energy), while a surfer moving slightly faster than a wave will be pushing on the wave as he moves uphill (losing energy to the wave).\n\nIt is worth noting that only the surfers are playing an important role in this energy interactions with the waves; a beachball floating on the water (zero velocity) will go up and down as the wave goes by, not gaining energy at all. Also, a boat that moves very fast (faster than the waves) does not exchange much energy with the wave.\n\nA simple mechanical description of particle dynamics provides a quantitative estimate of the synchronization of particles with the wave [Equation (1) of ]. A more rigorous approach shows the strongest synchronization occurs for particles with a velocity in the wave frame proportional to the damping rate and independent of the wave amplitude [section 4.1.3 of ]. Since Landau damping occurs for waves with arbitrarily small amplitudes, this shows the most active particles in this damping are far from being trapped. This is natural, since trapping involves diverging time scales for such waves (specifically formula_5 for a wave amplitude formula_6).\n\nTheoretical treatment starts with the Vlasov equation in the non-relativistic zero-magnetic field limit, the Vlasov–Poisson set of equations. Explicit solutions are obtained in the limit of a small formula_7-field. The distribution function formula_8 and field formula_7 are expanded in a series: formula_10, formula_11 and terms of equal order are collected.\n\nTo first order the Vlasov–Poisson equations read\n\nLandau calculated the wave caused by an initial disturbance formula_13 and found by aid of Laplace transform and contour integration a damped travelling wave of the form formula_14 with wave number formula_15 and damping decrement\n\nHere formula_17 is the plasma oscillation frequency and formula_18 is the electron density. Later Nico van Kampen proved that the same result can be obtained with Fourier transform. He showed that the linearized Vlasov–Poisson equations have a continuous spectrum of singular normal modes, now known as van Kampen modes\n\nin which formula_20 signifies principal value, formula_21 is the delta function (see generalized function) and\n\nis the plasma permittivity. Decomposing the initial disturbance in these modes he obtained the Fourier spectrum of the resulting wave. Damping is explained by phase-mixing of these Fourier modes with slightly different frequencies near formula_17.\n\nIt was not clear how damping could occur in a collisionless plasma: where does the wave energy go? In fluid theory, in which the plasma is modeled as a dispersive dielectric medium, the energy of Langmuir waves is known: field energy multiplied by the Brillouin factor formula_24.\nBut damping cannot be derived in this model. To calculate energy exchange of the wave with resonant electrons, Vlasov plasma theory has to be expanded to second order and problems about suitable initial conditions and secular terms arise.\n\nThe rigorous mathematical theory is based on solving the Cauchy problem for the evolution equation (here the partial differential Vlasov–Poisson equation) and proving estimates on the solution.\n\nFirst a rather complete linearized mathematical theory has been developed since Landau.\n\nGoing beyond the linearized equation and dealing with the nonlinearity has been a longstanding problem in the mathematical theory of Landau damping. \nPreviously one mathematical result at the non-linear level was the existence of a class of exponentially damped solutions of the Vlasov–Poisson equation in a circle which had been proved in by means of a scattering technique (this result has been recently extended in). However these existence results do not say anything about \"which\" initial data could lead to such damped solutions.\n\nIn a recent paper the initial data issue is solved and Landau damping is mathematically established for the first time for the non-linear Vlasov equation. It is proved that solutions starting in some neighborhood (for the analytic or Gevrey topology) of a linearly stable homogeneous stationary solution are (orbitally) stable for all times and are damped globally in time. The damping phenomenon is reinterpreted in terms of transfer of regularity of formula_8 as a function of formula_26 and formula_27, respectively, rather than exchanges of energy. Large scale variations pass into variations of smaller and smaller scale in velocity space, corresponding to a shift of the Fourier spectrum of formula_8 as a function of formula_27. This shift, well known in linear theory, proves to hold in the non-linear case.\n\nAn expression of plasma permittivity analogous to the above one, but corresponding to the Laplace transform used by Landau, can be obtained simply in an N-body frame. One considers a (one-component) plasma where only electrons are present as particles, and ions just provide a uniform neutralizing background. The principle of the calculation is provided by considering the fictitious linearized motion of a single particle in a single Fourier component of its own electric field. The full calculation boils down to a summation of the corresponding result over all formula_18 particles and all Fourier components. The Vlasovian expression for the plasma permittivity is finally recovered by substituting an integral over a smooth distribution function for the discrete sum over the particles in the N-body plasma permittivity. Together with Landau damping, this mechanical approach also provides the calculation of Debye shielding, or Electric-field screening, in a plasma.\n\n"}
{"id": "9013017", "url": "https://en.wikipedia.org/wiki?curid=9013017", "title": "List of apple diseases", "text": "List of apple diseases\n\nThis article is a list of diseases of apples (\"Malus × domestica\").\n\n"}
{"id": "31181167", "url": "https://en.wikipedia.org/wiki?curid=31181167", "title": "List of hawthorn species with yellow fruit", "text": "List of hawthorn species with yellow fruit\n\nMost species of \"Crataegus\" (hawthorn) have red fruit, others can have black or purple fruit, and some have yellow or orange fruit.\n\n\n\n\n"}
{"id": "22933016", "url": "https://en.wikipedia.org/wiki?curid=22933016", "title": "List of rivers of Mongolia", "text": "List of rivers of Mongolia\n\nThis is a list of notable rivers of Mongolia, arranged geographically by river basin.\n\nThe Mongolian words for river are \"gol\" (') and \"mörön\" ('), with the latter usually used for larger rivers. The Mongolian names also occasionally have a genitive construction, with the name of the river having the suffix \"-iin\" (') or \"-yn\" ('). For example, Ider River is \"Ideriin Gol\" (), having the meaning \"River of Ider\".\n\n\n\n\n\n\n\nRivers draining into endorheic Uvs Lake, forming the drainage of the Uvs Lake Basin\n\n\n\n\n"}
{"id": "11700559", "url": "https://en.wikipedia.org/wiki?curid=11700559", "title": "List of scientists who disagree with the scientific consensus on global warming", "text": "List of scientists who disagree with the scientific consensus on global warming\n\nThis is a list of scientists who have made statements that conflict with the scientific consensus on global warming as summarized by the Intergovernmental Panel on Climate Change and endorsed by other scientific bodies. A minority are climatologists. Nearly all publishing climate scientists (97–98%) support the consensus on anthropogenic climate change, hence this list represents a minority viewpoint.\n\nThe scientific consensus is that the global average surface temperature has risen over the last century. Scientific opinion on climate change was summarized in the 2001 Third Assessment Report of the Intergovernmental Panel on Climate Change (IPCC). The main conclusions on global warming at that time were as follows:\n\nThese findings are recognized by the national science academies of all the major industrialized nations; the consensus has strengthened over time and is now virtually unanimous. The level of consensus correlates with expertise in climate science.\n\nThere have been several efforts to compile lists of dissenting scientists, including a 2008 US senate minority report, the Oregon Petition, and a 2007 list by the Heartland Institute, all three of which have been criticized on a number of grounds.\n\nFor the purpose of this list, a \"scientist\" is defined as an individual who has published at least one peer-reviewed research article in the broad field of natural sciences, although not necessarily in a field relevant to climatology. Since the publication of the IPCC Third Assessment Report, each has made a clear statement in his or her own words (as opposed to the name being found on a petition, etc.) disagreeing with one or more of the report's three main conclusions, and each has been described in reliable sources as a climate skeptic, denier, or in disagreement with any of the three main conclusions. Their views on climate change are usually described in more detail in their biographical articles. Few of the statements in the references for this list are part of the peer-reviewed scientific literature; most are from other sources such as interviews, opinion pieces, online essays and presentations.\n\n\"Nota bene\": Only individuals who have their own Wikipedia article may be included in the list.\n\nThese scientists have said that it is not possible to project global climate accurately enough to justify the ranges projected for temperature and sea-level rise over the 21st century. They may not conclude specifically that the current IPCC projections are either too high or too low, but that the projections are likely to be inaccurate due to inadequacies of current global climate modeling.\n\nThese scientists have said that the observed warming is more likely to be attributable to natural causes than to human activities. Their views on climate change are usually described in more detail in their biographical articles.\n\nThese scientists have said that no principal cause can be ascribed to the observed rising temperatures, whether man-made or natural.\n\nThese scientists have said that projected rising temperatures will be of little impact or a net positive for society or the environment.\n\n\nThese scientists published material indicating their opposition to the mainstream scientific assessment of global warming prior to their deaths. \n\n\n\n"}
{"id": "9017707", "url": "https://en.wikipedia.org/wiki?curid=9017707", "title": "List of sunflower diseases", "text": "List of sunflower diseases\n\nThis article is a list of diseases of sunflowers (\"Helianthus annuus\") and Jerusalem Artichoke (\"H. tuberosus\").\n\n"}
{"id": "315426", "url": "https://en.wikipedia.org/wiki?curid=315426", "title": "Mediocrity principle", "text": "Mediocrity principle\n\nThe mediocrity principle is the philosophical notion that \"if an item is drawn at random from one of several sets or categories, it's likelier to come from the most numerous category than from any one of the less numerous categories\". The principle has been taken to suggest that there is nothing very unusual about the evolution of the Solar System, Earth's history, the evolution of biological complexity, human evolution, or any one nation. It is a heuristic in the vein of the Copernican principle, and is sometimes used as a philosophical statement about the place of humanity. The idea is to assume mediocrity, rather than starting with the assumption that a phenomenon is special, privileged, exceptional, or even superior.\n\nThe mediocrity principle suggests, given the existence of life on Earth, that life typically exists on Earth-like planets throughout the universe.\n\nThe mediocrity principle is in contrast with the anthropic principle, which asserts that the presence of an intelligent observer (humans) limits the circumstances to bounds under which intelligent life can be observed to exist, no matter how improbable. Both stand in contrast to the fine-tuning hypothesis, which asserts that the natural conditions for intelligent life are implausibly rare.\n\nThe mediocrity principle implies that Earth-like environments are necessarily common, based in part on the evidence of any happening at all, whereas the anthropic principle suggests that no assertion can be made about the probability of intelligent life based on a sample set of one (self-described) example, who are necessarily capable of making such an assertion about themselves.\n\nIt is also possible to handle the Mediocrity Principle as a statistical problem, a case of a single Data point statistics, also present in the German tank problem.\n\nDavid Deutsch argues that the mediocrity principle is incorrect from a physical point of view, in reference to either humanity's part of the universe or to its species. Deutsch refers to Stephen Hawking's quote: \"The human race is just a chemical scum on a moderate-sized planet, orbiting around a very average star in the outer suburb of one among a hundred billion galaxies\". Deutsch wrote that Earth's neighborhood in the universe is not typical (80% of the universe's matter is dark matter) and that a concentration of mass such as the Solar System is an \"isolated, uncommon phenomenon\". He also disagrees with Richard Dawkins, who considers that humans, because of natural evolution, are limited to the capabilities of their species. Deutsch responds that even though evolution did not give humans the ability to detect neutrinos, scientists can currently detect them, which significantly expands their capabilities beyond what is available as a result of evolution.\n\n"}
{"id": "36395362", "url": "https://en.wikipedia.org/wiki?curid=36395362", "title": "Micronized rubber powder", "text": "Micronized rubber powder\n\nMicronized rubber powder (MRP) is classified as fine, dry, powdered elastomeric crumb rubber in which a significant proportion of particles are less than 100 µm and free of foreign particulates (metal, fiber, etc.). MRP particle size distributions typically range from 180 µm to 10 µm. Narrower distributions can be achieved depending on the classification technology.\n\nMRP is typically made from vulcanized elastomeric material, most often from end-of-life tire material, but can also be produced from post-industrial nitrile rubber, ethylene propylene diene monomer (EPDM), butyl and natural rubber compounds.\n\nMRP is a free flowing, black rubber powder that disperses into a multitude of systems and applications. Due to its micron size, MRP can be incorporated into multiple polymers, and provides a smooth surface appearance on finished products. In some cases, in order to improve compatibility with host materials, the MRP is given a chemical treatment to activate, or “make functional” the surface of the powder particles. This is referred to as functionalized MRP or FMRP.\n\nMRP represents an evolution over previous post-manufactured rubber technologies. The most basic rubber processing technology converts end-of-life tire and post-industrial rubber material into rubber chips that are typically one inch or larger in size. These chips are then used in tire-derived fuel and civil engineering projects. A second-generation processing technology converts end-of-life tire and rubber material into crumb rubber, also known as ground tire rubber (GTR). GTR typically comprises chips between one inch and 30 mesh in size, with the associated fiber and steel mostly removed. This material is used in asphalt, as garden mulch and in playgrounds. \n\nMRP is a micron-size material that is produced in various sizes, including 80 mesh and down to 300 mesh. MRP is virtually metal and fiber-free, enabling its use in a wide range of advanced products. \n\nMRP is used as a compound extender to offset the use of natural rubber and synthetic polymers as well as act as a process aid in material production. In some cases, MRP can reduce formulation costs, because it replaces commodity-priced rubber- and oil-based feedstocks, According to some estimates, MRP offers up to 50 percent cost savings over virgin raw materials. \n\nMRP also can improve the sustainability, and in some cases the performance, of the compounds in which it is used. For example, the smaller particle sizes of MRP are known to increase the impact strength of certain plastic compositions. However, in all applications the particle size and loading levels depend on the target application.\n\nDue to its size and composition, MRP can be incorporated into more advanced and higher-value applications than crumb rubber. Industries incorporating MRP into their products include tire, automotive, construction, industrial components and consumer products. It is also used as an additive in tires, plastics, asphalt, coatings, and sealants. MRP can also be incorporated into prime or recycled grade polypropylene (PP), high-density polyethylene (HDPE) and nylons. Additionally, the incorporation of MRPs in thermoplastic elastomers (TPE) and thermoplastic vulcanizates (TPV) makes it a feasible ingredient for automotive and building and construction applications.\n\nCurrently, the leading producer of MRP is Lehigh Technologies, which utilizes a cryogenic turbo mill process with more than 100 million pounds of annual production capacity. MRP produced by Lehigh has set high benchmarks for performance in a range of applications with customers and third-party research institutions, including several studies on increased asphalt performance. Lehigh claims more than 250 million tires on the road today have been made using its MRP.\n\nThere is an applicable American Society for Testing and Materials (ASTM) specification [ASTM D5603-01 (2008)] for the classification of rubber powder, including MRP.\n\nNumerous U.S. and European studies have found crumb rubber and MRP meet standards for human health and safety. Recently, an EPA study found that crumb rubber in field turfs and playgrounds contained concentrations of materials below harmful levels. \n"}
{"id": "2117852", "url": "https://en.wikipedia.org/wiki?curid=2117852", "title": "Middle Jurassic", "text": "Middle Jurassic\n\nThe Middle Jurassic is the second epoch of the Jurassic Period. It lasted from about 174 to 163 million years ago. \nFossil-bearing rocks from the Middle Jurassic are relatively rare, but some important formations include the Forest Marble Formation in England, the Kilmaluag Formation in Scotland, the Daohugou Beds in China, Itat Formation in Russia, and the Isalo III Formation of western Madagascar.\n\nDuring the Middle Jurassic epoch, Pangaea began to separate into Laurasia and Gondwana, and the Atlantic Ocean formed. Tectonic activities are active on eastern Laurasia as the Cimmerian plate continues to collide with Laurasia's southern coast, completely closing the Paleo-Tethys Ocean. A subduction zone on the coast of western North America continues to create the Ancestral Rocky Mountains.\n\nThe Middle Jurassic is one of the key periods in the evolution of life on earth. Many groups, including dinosaurs and mammals, diversified during this time.\n\nDuring this time, marine life (including ammonites and bivalves) flourished. Ichthyosaurs, although common, are reduced in diversity; while the top marine predators, the pliosaurs, grew to the size of killer whales and larger (e.g. \"Pliosaurus\", \"Liopleurodon\"). Plesiosaurs became common at this time, and metriorhynchid crocodilians first appeared.\n\nMany of the major groups of dinosaurs emerged during the Middle Jurassic, (including cetiosaurs, brachiosaurs, megalosaurs and primitive ornithopods).\n\nDescendants of the therapsids, the cynodonts were still flourishing along with the dinosaurs. These included the tritylodonts and mammals. Mammals remained quite small, but were diverse and numerous in faunas from around the world. Tritylodonts were larger, and also had an almost global distribution. The first crown group mammals appeared in the Middle Jurassic. A group of cynodonts, the trithelodonts, were becoming rare and eventually became extinct at the end of this epoch.\n\nConifers were dominant in the Middle Jurassic. Other plants, such as ginkgoes, cycads, and ferns were also common.\n"}
{"id": "27173352", "url": "https://en.wikipedia.org/wiki?curid=27173352", "title": "Mitigation of aviation's environmental impact", "text": "Mitigation of aviation's environmental impact\n\nAviation affects the environment due to aircraft engines emitting noise, particulates, and gases which contribute to climate change and global dimming. Despite emission reductions from automobiles and more fuel-efficient (and therefore less polluting) turbofan and turboprop engines, the rapid growth of air travel in recent years contributes to an increase in total pollution attributable to aviation. In the EU, greenhouse gas emissions from aviation increased by 87% between 1990 and 2006.\n\nAt present aviation accounts for 2.5% of global CO emissions. Due to projected growth in air travel, in the most technologically radical scenarios for having a better than 50% chance of keeping global warming below 2 degrees Celsius, in 2050 aviation will make up 15% of global CO emissions. In more conventional scenarios its emissions will exceed the entire global carbon budget before then. This presents governments and the operators of aircraft with a responsibility to reduce the aviation industry's emissions.\n\nMitigation of aviation's environmental impact can be achieved through a variety of measures, the most obvious and arguably the most economical of which is to reduce the fuel burn of the aircraft as this accounts for 28% of an airlines costs. However, there is a wide variety of other options available to minimise aviation's growing impact upon the environment as are listed below:\n\nAs stated previously, reducing the direct fuel burn of an aircraft is the most obvious and arguably the most economical way of reducing emissions attributable to aviation. Over the last 40 years, commercial jet airliners have become 70% more fuel efficient and are predicted to be another 25% more fuel efficient by 2025.\n\nThe next-generation of aircraft, including the Boeing 787 Dreamliner, Airbus A350 and Bombardier CSeries, are 20% more fuel efficient per passenger kilometre than current generation aircraft. This is primarily achieved through more fuel-efficient engines and lighter airframes & supporting structures made of composite materials but is also achieved through more aerodynamic shapes, winglets, a \"one-piece\" fuselage and more advanced computer systems for optimising routes and loading of the aircraft.\n\nCurrently, air traffic corridors that aircraft are forced to follow place unnecessary detours on an aircraft's route forcing higher fuel burn and an increase in emissions. An improved Air Traffic Management System with more direct routes and optimized cruising altitudes would allow airlines to reduce their emissions by up to 18%.\n\nIn the European Union, a Single European Sky has been proposed for the last 15 years so that there are no overlapping airspace restrictions between countries in the EU and so reduce emissions. As yet, the Single European Sky is still only a plan but progress has been made. If the Single European Sky had been created 15 years ago, 12 million tons of CO could have been saved.\n\nBiofuels are fuels derived from biomass material such as plants and waste. Plant derived biofuels offer large savings in CO emissions as they absorb Carbon Dioxide and release it as Oxygen when they grow and so in a life-cycle, emissions can be drastically reduced. A number of airlines have operated biofuel test flights including Virgin Atlantic Airways, which flew with one engine operating on a blend of 20% coconut oil and 80% traditional jet fuel, and Continental Airlines which flew with one engine operating on a blend of 44% Jatropha oil, 6% Algae oil and 50% traditional jet fuel. Other airlines to demonstrate biofuels include Air New Zealand and Japan Airlines.\nIn the Continental Airlines test, the engine running partly on biofuel burned 46 kg less fuel than the conventionally fuelled engine in 1 and a half hours while producing more thrust from the same volume of fuel. Continental Airlines' CEO, Larry Kellner, commented \"This is a good step forward, an opportunity to really make a difference to the environment\" citing jatropha's 50–80% lower CO emissions as opposed to Jet-A1 in its lifecycle.\n\nFrom 2014 onwards, British Airways, in co-operation with Solena, is going to turn half a million tonnes of waste annually that would normally go to landfill from the City of London into biofuel to be used in the British Airways fleet. Waste derived biofuel produces up to 95% less pollution in its life-cycle and so therefore this measure will reduce emissions by the equivalent of taking 42,000 cars off the road every year. \nAirlines and airports are looking at ways of reducing emissions and fuel burn through the use of improved operating procedures. Two of the more common ones in operation are a single-engine taxi to and from the runway and the use of a Continuous Descent Approach, or CDA, which can reduce emissions significantly during the operations in and around an airport. Scandinavian Airlines (SAS) is now operating its Boeing 737 fleet at a slower cruising speed to help reduce emissions by 7–8%.\n\nIn the EU, aviation will be including the European Emission Trading Scheme from 2012 onwards. The scheme places a cap on the emissions an aircraft operator can emit and forces the operator to either lower emissions through more efficient technology or to buy \"Carbon Credits\" from other companies who have produced fewer emissions than their cap. It is thought that this will reduce aviation's net environmental impact.\n\nAviation produces a number of other pollutants besides carbon dioxide including nitrogen oxides (NO), particulates, unburned hydrocarbons (UHC) and contrails. A number of methods to reduce the level of these pollutants follows:\n\nNitrogen oxides have a far stronger impact upon climate change than Carbon Oxides and are produced in small quantities from aircraft engines. Engine designers have worked since the start of the jet age to reduce NO emissions and the result is ever reducing levels of nitrogen oxide emissions. For example, between 1997 and 2003, NO emissions from jet engines fell by over 40%.\n\nParticulates and smoke were a problem with early jet engines at high power settings but modern engines are designed so that no smoke is produced at any point in the flight.\n\nUnburned hydrocarbons (UHC) are products of incomplete combustion of fuel and are produced in greater quantities in engines with low pressure gains in the compressors and/or relatively low temperatures in the combustor. As with particulates, UHC has all but been eliminated in modern jet engines through improved design and technology.\n\nAircraft flying at high altitude form condensation trails or contrails in the exhaust plume of their engines. While in the Troposphere these have very little climatic impact. However, jet aircraft cruising in the Stratosphere do create an impact from their contrails, although the extent of the damage to the environment is as yet unknown. Contrails can also trigger the formation of high-altitude Cirrus cloud thus creating a greater climatic effect. A 2015 study found that artificial cloudiness caused by contrail \"outbreaks\" reduce the difference between daytime and nighttime temperatures. The former are decreased and the latter are increased, in comparison to temperatures the day before and the day after such outbreaks. On days with outbreaks the day/night temperature difference was diminished by about 6F° in the U.S. South and 5F° in the Midwest.\n\nIn the three days following the September 11 attacks on the World Trade Center in New York City, when no commercial aircraft flew in the United States, climate scientists measured the daily temperature range over 5000 weather stations across the USA. The results showed a 1 ° Celsius change in the average daily temperature range for those days of the year, thus showing contrails do have a significant impact on climate. Potential ways of reducing the impact of contrails on our climate include reducing the maximum cruising altitude of aircraft so high-altitude contrails can not form. Cruising at lower altitudes would marginally increase flight time and increase fuel consumption by 4%.\n\nOne of the by-products of an aircraft's engine is noise and this has become an increasingly important issue which is being dealt with through many different methods:\n\nNext-Generation engines are not only more fuel-efficient but also tend to be quieter with Pratt & Whitney's PurePower PW1000G fitted to the Bombardier CSeries aircraft being 4 times quieter than aircraft currently in service. Engines can also incorporate serrated edges or 'chevrons' on the back of the nacelle to reduce noise impact as shown in this picture.\n\nA Continuous Descent Approach, or CDA, not only reduces fuel burn but also allows airlines to provide quieter approaches for part of the descent to a runway. As the engines are at close to idle power, less noise emissions are produced and combined with new engine technology, the reductions in noise emissions can be large.\n\nA carbon offset is a means of reducing emissions to zero by saving enough carbon to balance the carbon emitted by a particular action. Several airlines have begun offering carbon offsets to passengers to offset the emissions created by their proportion of the flight. Money generated is put to projects around the world to invest in green technology such as renewable energy and research into future technology. Airlines offering carbon offsets include British Airways, easyJet, Continental Airlines, Delta Air Lines, Lufthansa and Qantas although there are many more carriers participating in such schemes.\n\nBritish Airways' carbon offsetting scheme involves paying a fee dependant on aircraft type, class of travel and distance flown and therefore prices vary. Funds generated are currently awarded to three renewable energy projects around the world: Bayin'aobao wind farm in Inner Mongolia, Faxinal dos Guedes hydroelectric power plant in Brazil and Xiaohe hydroelctric power plant in Gansu Province, China.\n\nContinental Airlines' carbon offsetting scheme involves paying a fixed fee of $2 to cancel out emissions through reforestation. Passengers can also choose to pay $50 for offsetting emissions through renewable energy projects.\n\n\n"}
{"id": "1681984", "url": "https://en.wikipedia.org/wiki?curid=1681984", "title": "Molecular sieve", "text": "Molecular sieve\n\nA molecular sieve is a material with pores (very small holes) of uniform size. These pore diameters are similar in size to small molecules, and thus large molecules cannot enter or be adsorbed, while smaller molecules can. As a mixture of molecules migrate through the stationary bed of porous, semi-solid substance referred to as a sieve (or matrix), the components of highest molecular weight (which are unable to pass into the molecular pores) leave the bed first, followed by successively smaller molecules. Some molecular sieves are used in chromatography, a separation technique that sorts molecules based on their size. Other molecular sieves are used as desiccants (some examples include activated charcoal and silica gel).\n\nThe diameter of a molecular sieve is measured in ångströms (Å) or nanometres (nm). According to IUPAC notation, microporous materials have pore diameters of less than 2 nm (20 Å) and macroporous materials have pore diameters of greater than 50 nm (500 Å); the mesoporous category thus lies in the middle with pore diameters between 2 and 50 nm (20–500 Å).\n\nMolecular sieves can be microporous, mesoporous, or macroporous material.\n\n\n\n\nMolecular sieves are often utilized in the petroleum industry, especially for drying gas streams. For example, in the liquid natural gas (LNG) industry, the water content of the gas needs to be reduced to less than 1 ppmv to prevent blockages caused by ice.\n\nIn the laboratory, molecular sieves are used to dry solvent. \"Sieves\" have proven to be superior to traditional drying techniques, which often employ aggressive desiccants.\n\nUnder the term zeolites, molecular sieves are used for a wide range of catalytic applications. They catalyze isomerisation, alkylation, and epoxidation, and are used in large scale industrial processes, including hydrocracking and fluid catalytic cracking.\n\nThey are also used in the filtration of air supplies for breathing apparatus, for example those used by scuba divers and firefighters. In such applications, air is supplied by an air compressor and is passed through a cartridge filter which, depending on the application, is filled with molecular sieve and/or activated carbon, finally being used to charge breathing air tanks. \nSuch filtration can remove particulates and compressor exhaust products from the breathing air supply.\n\nThe U.S. FDA has as of April 1, 2012 approved sodium aluminosilicate for direct contact with consumable items under 21 CFR 182.2727. Prior to this approval Europe had used molecular sieves with pharmaceuticals and independent testing suggested that molecular sieves meet all government requirements but the industry had been unwilling to fund the expensive testing required for government approval.<ref name=\"desiccantpacks.net/molecular-sieve/\"></ref>\n\nMethods for regeneration of molecular sieves include pressure change (as in oxygen concentrators), heating and purging with a carrier gas (as when used in ethanol dehydration), or heating under high vacuum. Regeneration temperatures range from 175 °C to 315 °C depending on molecular sieve type. In contrast, silica gel can be regenerated by heating it in a regular oven to 120 °C (250 °F) for two hours. However, some types of silica gel will \"pop\" when exposed to enough water. This is caused by breakage of the silica spheres when contacting the water.\n\n\n3Å molecular sieves do not adsorb molecules whose diameters are larger than 3 Å. The characteristics of these molecular sieves include fast adsorption speed, frequent regeneration ability, good crushing resistance and pollution resistance. These features can improve both the efficiency and lifetime of the sieve. 3Å molecular sieves are the necessary desiccant in petroleum and chemical industries for refining oil, polymerization, and chemical gas-liquid depth drying.\n\n3Å molecular sieves are used to dry a range of materials, such as ethanol, air, refrigerants, natural gas and unsaturated hydrocarbons. The latter include cracking gas, acetylene, ethylene, propylene and butadiene.\n\n3Å molecular sieve is utilized to remove water from ethanol, which can later be used directly as a bio-fuel or indirectly to produce various products such as chemicals, foods, pharmaceuticals, and more. Since normal distillation cannot remove all the water (an undesirable byproduct from ethanol production) from ethanol process streams due to the formation of an azeotrope at around 95 percent concentration, molecular sieve beads are used to separate ethanol and water on a molecular level by adsorbing the water into the beads and allowing the ethanol to pass freely. Once the beads are full of water, temperature or pressure can be manipulated, allowing the water to be released from the molecular sieve beads.\n\n3Å molecular sieves are stored at room temperature, with a relative humidity not more than 90%. They are sealed under reduced pressure, being kept away from water, acids and alkalis.\n\n\n4Å molecular sieves are widely used to dry laboratory solvents. They can absorb water and other molecules with a critical diameter less than 4 Å such as NH, HS, SO, CO, CHOH, CH, and CH. It is widely used in the drying, refining and purification of liquids and gases (such as the preparation of argon).\n\nThese molecular sieves are used to assist detergents as they can produce demineralized water through calcium ion exchange, remove and prevent the deposition of dirt. They are widely used to replace phosphorus. The 4Å molecular sieve plays a major role to replace sodium tripolyphosphate as detergent auxiliary in order to mitigate the environmental impact of the detergent. It also can be used as a soap forming agent and in toothpaste.\n\n4Å molecular sieves can purify sewage of cationic species such as ammonium ions, Pb, Cu, Zn and Cd. Due to the high selectivity for NH they have been successfully applied in the field to combat eutrophication and other effects in waterways due to excessive ammonium ions. 4Å molecular sieves have also been used to remove heavy metal ions present in water due to industrial activities.\n\n\n5Å molecular sieves are often utilized in the petroleum industry, especially for the purification of gas streams and in the chemistry laboratory for separating compounds and drying reaction starting materials. They contain tiny pores of a precise and uniform size, and are mainly used as an adsorbent for gases and liquids.\n\n5Å molecular sieves are used to dry natural gas, along with performing desulfurization and decarbonation of the gas.\nThey can also be used to separate mixtures of oxygen, nitrogen and hydrogen, and oil-wax n-hydrocarbons from branched and polycyclic hydrocarbons.\n\n5Å molecular sieves are stored at room temperature, with a relative humidity less than 90% in cardboard barrels or carton packaging. The molecular sieves should not be directly exposed to the air and water, acids and alkalis should be avoided.\n\nSelecting Molecular Sieve:\n\nMolecular Sieves are available in different shape and sizes. But the spherical beads have advantage over other shape as they offer lower pressure drop, attrition resistant as they do not have any sharp edges, also have good strength i.e. crush force required per unit area is higher. Certain beaded molecular sieves offer lower heat capacity thus less energy requirement during regeneration.\n\nThe other advantage of using beaded molecular sieve is bulk density is usually higher than other shape, thus for same adsorption requirement molecular sieve volume required is less. Thus while doing de-bottlenecking you may use beaded molecular sieve and load more adsorbent in same volume and avoid any vessel modifications.\n\n\n"}
{"id": "7654820", "url": "https://en.wikipedia.org/wiki?curid=7654820", "title": "Myrmecophily", "text": "Myrmecophily\n\nMyrmecophily ( ; literally \"ant-love\") is the term applied to positive interspecies associations between ants and a variety of other organisms such as plants, other arthropods, and fungi. Myrmecophily refers to mutualistic associations with ants, though in its more general use the term may also refer to commensal or even parasitic interactions.\n\nThe term \"myrmecophile\" is used mainly for animals that associate with ants. There are an estimated 10,000 species of ants (Formicidae), with a higher diversity in the tropics. In most terrestrial ecosystems ants are ecologically and numerically dominant, being the main invertebrate predators. As a result, ants play a key role in controlling arthropod richness, abundance, and community structure. There is evidence that the evolution of myrmecophilous interactions has contributed to the abundance and ecological success of ants, by ensuring a dependable and energy-rich food supply and thus providing a competitive advantage for ants over other invertebrate predators. Most myrmecophilous associations are opportunistic, unspecialized, and facultative (meaning both species are capable of surviving without the interaction), though obligate mutualisms (those in which one or both species are dependent on the interaction for survival) have also been observed for many species.\n\nA myrmecophile is an organism that lives in association with ants.\n\nMyrmecophiles may have various roles in their host ant colony. Many consume waste materials in the nests, such as dead ants, dead larvae, or fungi growing in the nest. Some myrmecophiles, however, feed on the stored food supplies of ants, and a few are predatory on ant eggs, larvae, or pupae. Others benefit the ants by providing a food source for them. Most associations are facultative, benefiting one or both participants but not being necessary to their survival, but many myrmecophilous relationships are obligate, meaning one or the other participant requires the relationship for survival.\n\nMyrmecophilous associations are best known in butterflies of the family Lycaenidae. Many lycaenid caterpillars produce nectar by specialized organs and communicate with the ants through sound and vibrations. The association with ants is believed to reduce the parasitisation of the butterfly caterpillars.\n\nThere are myrmecophilous beetles in the families Coccinellidae (e.g. the ladybird \"Thalassa saginata\"), Aphodiidae, Scarabaeidae, Lucanidae, Cholevidae, Pselaphidae, Staphylinidae, Histeridae and Ptiliidae (some treated here as subfamilies). In ant-beetle associations, the myrmecophilous Staphylinids are the most diverse of the beetle families. Myrmecophilous associations are also seen in various other insects such as aphids and treehoppers, as well as the hoverfly genus \"Microdon\" and several other groups of flies.\n\nSome mites and spiders are also myrmecophilous, particularly some oribatid mites, which have been found to be obligate myrmecophiles.\n\nOther myrmecophile groups include\n\nThe first major work in cataloguing British myrmecophiles was done by Horace Donisthorpe in his 1927 book \"The Guests of British Ants\".\n\nAnt-plant interactions are geographically widespread, with hundreds of species of myrmecophytic plants in several families including the Leguminosae, Euphorbiaceae, and Orchidaceae. In general, myrmecophytes (or ant plants) usually provide some form of shelter and food in exchange for ant \"tending,\" which may include protection, seed dispersal (see myrmecochory), reduced competition from other plants, hygienic services, and/or nutrient supplementation.\n\nThree of the most common and important structural adaptations of ant plants are extrafloral nectaries, domatia, and (least commonly) Beltian bodies. Plant domatia are pre-formed nesting sites provided by the plant in the form of hollow stems, petioles, thorns, or curled leaves. The production of ant-specialized domatia has been documented in over 100 genera of tropical plants. Beltian bodies provide a high-energy food source to ants in the form of nutritive corpuscles produced on leaflet tips, and they have been described in at least 20 plant families. Extrafloral nectaries (EFN's) are known to occur in at least 66 families of angiosperm plants in both temperate and tropical regions, as well as some ferns, but are absent in all gymnosperms and are most abundant in the tropics. EFN's being outside of the plant flowers are not employed in pollination; their primary purpose is to attract and sustain tending ants. Many plants can control the flow of nectar from the EFN's so that the availability of nectar varies according to daily and seasonal cycles. Because ants can respond quickly to changes in flow rate from EFN's, this may be possible mechanism by which plants can induce greater ant activity during times of peak herbivory and minimize overall costs of nectar production. The combined nutritional output of EFN's and Beltian bodies can be a significant food source for tending ants, and in some cases can provide the total nutritive needs for an ant colony.\n\nIn exchange for nesting sites and food resources, ants protect plants from herbivores. One the best-known examples of ant-plant mutualism is in bullhorn acacias (\"Acacia cornigera\") and their tending \"Pseudomyrmex\" ants in Central America. This system was studied by Dan Janzen in the late 1960s, who provided some of the first experimental evidence that ants significantly reduce herbivory rates of myrmecophytes. Since then many other studies have demonstrated similar results in other systems. In the Bullhorn acacia system, in exchange for protection the acacias provide domatia, Beltian bodies, and EFN's, and there is evidence that the \"Pseudomyrmex\" ants can survive exclusively on these food resources without having to forage elsewhere. For many plants, including the bullhorn acacias, ants can significantly reduce herbivory from both phytophagous insects as well as larger organisms, such as large grazing mammals. Obligately associated ant species are some of most aggressive ants in the world, and can defend a plant against herbivory by large mammals by repeatedly biting their attacker and spraying formic acid into the wound.\n\nMyrmecophily is considered a form of indirect plant defense against herbivory, though ants often provide other services in addition to protection. Some ants provide hygienic services to keep leaf surfaces clean and deter disease, and defense against fungal pathogens has also been demonstrated. It is common for ants to prune epiphytes, vines, and parasitic plants from their host plant, and they sometimes thin the shoots of neighboring plants as well. In doing so, ants reduce plant-plant competition for space, light, nutrients, and water. Finally, current work focusing on ants' role in nutrient supplementation for plants has shown that in many ant-plant relationships nutrient flow is bidirectional. One study has estimated that while 80% of the carbon in the bodies of \"Azteca\" spp. workers is supplied by the host tree (\"Cecropia\" spp.), 90% of the \"Cecropia\" tree's nitrogen was supplied by ant debris carried to the tree as a result of external foraging. In light of these services, myrmecophily has been consdiered advantageous in ensuring a plant's survival and ecological success, although the costs to the plant of providing for the ants can sufficiently high to offsets benefits.\n\nAnts tend a wide variety of insect species, most notably lycaenid butterfly caterpillars and hemipterans. Forty-one percent of all ant genera include species that associate with insects. In all ant-insect associations the ants provide some service in exchange for nutrients in the form of honeydew, a sugary fluid excreted by many phytophagous insects. Interactions between honeydew-producing insects and ants is often called trophobiosis, a term which merges notions of trophic relationships with symbioses between ants and insects. This term has been criticized, however, on the basis that myrmecophilous interactions are often more complex than simple trophic interactions, and the use of symbiosis is inappropriate for describing interactions among free-living organisms.\n\nSome of the most well-studied myrmecophilous interactions involve ants and hemipterans (earlier grouped in the order Homoptera which included the Auchenorrhyncha and Sternorrhyncha), especially aphids. There are around 4000 described species of aphids, and they are the most abundant myrmecophilous organisms in the northern temperate zones. Aphids feed on the phloem sap of plants, and as they feed they excrete honeydew droplets from their anus. The tending ants ingest these honeydew droplets then return to their nest to regurgitate the fluid for their nestmates (see trophallaxis). Between 90-95% of the dry weight of aphid honeydew is various sugars, while the remaining matter includes vitamins, minerals, and amino acids. Aphid honeydew can provide an abundant food source for ants (aphids in the genus \"Tuberolachnus\" can secrete more honeydew droplets per hour than their body weight) and for some ants aphids may be their only source of food. In these circumstances, ants may supplement their honeydew intake by preying on the aphids once the aphid populations have reached certain densities. In this way ants can gain extra protein and ensure efficient resource extraction by maintaining honeydew flow rates that do not exceed the ants' collection capabilities. Even with some predation by ants, aphid colonies can reach larger densities with tending ants than colonies without. Ants have been observed to tend large \"herds\" of aphids, protecting them from predators and parasitoids. Aphid species that are associated with ants often have reduced structural and behavioral defense mechanisms, and are less able to defend themselves from attack than aphid species that are not associated with ants.\n\nAnts engage in associations with other honeydew-producing hemipterans such as scale insects (Coccidae), mealybugs (Pseudococcidae), and treehoppers (Membracidae), and most of these interaction are facultative and opportunistic with some cases of obligate associations, such as hemipterans that are inquiline, meaning they can only survive inside ant nests. In addition to protection, ants may provide other services in exchange for hemipteran honeydew. Some ants bring hemipteran larvae into the ant nests and rear them along with their own ant brood. Additionally, ants may actively aid in hemipteran dispersal: queen ants have been observed transporting aphids during their dispersive flights to establish a new colony, and worker ants will often carry aphids to a new nesting site if the previous ant nest has been disturbed. Ants may also carry hemipterans to different parts of a plant or to different plants in order to ensure a fresh food source and/or adequate protection for the herd.\n\nMyrmecophily among lycaenid caterpillars differs from the associations of hemipterans because caterpillars feed on plant tissues, not phloem sap, and therefore do not continually excrete honeydew. Caterpillars of lycaenid butterflies have therefore evolved specialized organs that secrete chemicals to feed and appease ants. The secretions are a mixture of sugar and amino acid which in synergy is more attractive to the ants than either component in its own. Because caterpillars do not automatically pass honeydew, they must be stimulated to secrete droplets and do so in response to ant antennation, which is the drumming or stroking of the caterpillar's body by the ants' antennae. Some caterpillars possess specialized receptors that allow them to distinguish between ant antennation and contact from predators and parasites, and others produce acoustic signals that agitate ants, making them more active and likely better defenders of the larvae. As with homopteran myrmecophiles, ants protect Lycaenid larvae from predatory insects (including other ants) and parasitoid wasps, which lay their eggs in the bodies of many species of Lepidoptera larvae. For example, one study conducted by Pierce and colleagues in Colorado experimentally found that for the larvae of \"Glaucopsyche lygdamus\" that were tended by a certain ant species (\"Formica podzolica\"), compared to untended larvae, the percentage of larvae disappearing from plants before late final instar decreased (not statistically significantly, though) and the percentage of larvae infected by parasitoids significantly decreased (from 33% to 9%–12%). These interactions do not come without an energetic cost to the butterfly, however, and it has been shown that ant-tended individuals reach smaller adult sizes than non-tended individuals due to the costs of appeasing ants during the larval stage. Interactions with ants are not limited to the butterfly's larval stage, and in fact ants can be important partners for butterflies at all stages of their life cycle. For example, adult females of many lycaenid butterflies, such as \"J. evagoras\"\",\" preferentially oviposit on plants where ant partners are present, possibly by using ants' own chemical cues in order to locate sites where juvenile butterflies will likely be tended by ants. Finally, while ant attendance has been most widely documented in Lycaenid butterflies and to some extent in riodinid butterflies such as \"E. elvina\", many other lepidopteran species are known to associate with ants, including many moths.\n\nMany trophobiotic ants can simultaneously maintain associations with multiple species. Ants that interact with myrmecophilous insects and myrmecophytes are highly associated: species that are adapted to interact with one of these myrmecophiles may switch among them depending on resource availability and quality. Of the ant genera that include species that associate with ant plants, 94% also include species that associate with trophobionts. In contrast, ants that are adapted to cultivate fungus (leaf cutter ants, tribe Attini) do not possess the morphological or behavioral adaptations to switch to trophobiotic partners. Many ant mutualists can exploit these multi-species interactions to maximize the benefits of myrmecophily. For example, some plants will host aphids instead of investing in EFN's, which may be more energetically costly depending on local food availability. The presence of multiple interactors can strongly influence the outcomes of myrmecophily, often in unexpected ways.\n\nMutualisms are geographically ubiquitous, found in all organismic kingdoms, and play a major role in all ecosystems. Combined with the fact that ants are one of the most dominant lifeforms on earth, it is clear that myrmecophily plays a significant role in the evolution and ecology of diverse organisms, and in the community structure of many terrestrial ecosystems.\n\nQuestions of how and why species coevolve are of great interest and significance. In many myrmecophilous organisms it is clear that ant associations have been influential in the ecological success, diversity, and persistence of species. Analyses of phylogenetic information for myrmecophilous organisms as well as ant lineages have demonstrated that myrmecophily has arisen independently in most groups multiple times. Because there have been multiple gains (and perhaps losses) of myrmecophilous adaptations, the evolutionary sequence of events in most lineages is unknown. Exactly how these associations evolve also remains unclear.\n\nIn studying the coevolution of myrmecophilous organisms, many researchers have addressed the relative costs and benefits of mutualistic interactions, which can vary drastically according to local species composition and abundance, variation in nutrient requirements and availability, host plant quality, presence of alternative food sources, abundance and composition of predator and parasitoid species, and abiotic conditions. Because of the large amounts of variation in some of these factors, the mechanisms that support the stable persistence of myrmecophily are still unknown. In many cases, variation in external factors can result in interactions that shift along a continuum of mutualism, commensalism, and even parasitism. In almost all mutualisms, the relative costs and benefits of interactions are asymmetrical; that is, one partner experiences greater benefits and/or fewer costs than the other partner. This asymmetry leads to \"cheating,\" in which one partner evolves strategies to receive benefits without providing services in return. As with many other mutualisms, cheating has evolved in interactions between ants and their partners. For example, some lycaenid larvae are taken into ant nests where they prey on ant brood and offer no services to the ants. Other lycaenids may parasitize ant-plant relationships by feeding on plants that are tended by ants, apparently immune to ant attack because of their own appeasing secretions. Hemipterophagous lycaenids engage in a similar form of parasitism in ant-hemipteran associations. In light of the variability in outcomes of mutualistic interactions, and also the evolution of cheating in many systems, much remains to be learned about the mechanisms that maintain mutualism as an evolutionarily stable interaction.\n\nIn addition to leading to coevolution, mutualisms also play an important role in structuring communities. One of the most obvious ways in which myrmecophily influences community structure is by allowing for the coexistence of species which might otherwise be antagonists or competitors. For many myrmecophiles, engaging in ant associations is first and foremost a method of avoiding predation by ants. For example, the caterpillars of lycaenid butterflies are an ideal source of food for ants: they are slow-moving, soft-bodied, and highly nutritious, yet they have evolved complex structures to not only appease ant aggression but to elicit protective services from the ants. In order to explain why ants cooperate with other species as opposed to preying on them, two related hypotheses have been proposed: cooperation either provides ants with resources that are otherwise difficult to find, or it ensures the long-term availability of those resources.\n\nAt both small and large spatiotemporal scales, mutualistic interactions influence patterns of species richness, distribution, and abundance. Myrmecophilous interactions play an important role in determining community structure by influencing inter- and intraspecific competition; regulating population densities of arthropods, fungi, and plants; determining arthropod species assemblages; and influencing trophic dynamics. Recent work in tropical forests has shown that ant mutualisms may play key roles in structuring food webs, as ants can control entire communities of arthropods in forest canopies. Myrmecophily has also been key in the ecological success of ants. Ant biomass and abundance in many ecosystems exceeds that of their potential prey, suggesting a strong role of myrmecophily in supporting larger populations of ants than would otherwise be possible. Furthermore, by providing associational refugia and habitat amelioration for many species, ants are considered dominant ecosystem engineers.\n\nMyrmecophilous interactions provide an important model system for exploring ecological and evolutionary questions regarding coevolution, plant defense theory, food web structure, species coexistence, and evolutionarily stable strategies. Because many myrmecophilous relationships are easily manipulable and tractable, they allow for testing and experimentation that may not be possible in other interactions. Therefore, they provide ideal model systems in which to explore the magnitude, dynamics, and frequency of mutualism in nature.\n\n"}
{"id": "18434026", "url": "https://en.wikipedia.org/wiki?curid=18434026", "title": "Natural gas in Canada", "text": "Natural gas in Canada\n\nAt per day in 2011, Canada was the fourth-largest producer of natural gas in the world. Its proven reserves were at the close of 2006. A large portion of Canada's gas is exported to the United States; in 2006, per day.\n\nIn 2016 natural gas was used to provide 35% of all energy in Canada, double the amount supplied by electricity. Electricity generated by natural gas was 8.5% of the nations total. Natural gas is used to supply 50% of space heating, and 65% of water heating in homes, similarly 80% of businesses use natural gas for space and water heating. The industrial sector uses natural gas as a source of process heat, as a fuel for the generation of steam and as a feedstock in the production of petrochemicals and fertilizers. Provincially Alberta is the largest consumer of natural gas at 3.9 billion cubic feet per day, followed by BC at 2.3 billion cubic feet per day.\n\n\n"}
{"id": "89344", "url": "https://en.wikipedia.org/wiki?curid=89344", "title": "Nemesis (hypothetical star)", "text": "Nemesis (hypothetical star)\n\nNemesis is a hypothetical red dwarf or brown dwarf, originally postulated in 1984 to be orbiting the Sun at a distance of about 95,000 AU (1.5 light-years), somewhat beyond the Oort cloud, to explain a perceived cycle of mass extinctions in the geological record, which seem to occur more often at intervals of 26 million years. , more than 1800 brown dwarfs have been identified. There are actually fewer brown dwarfs in our cosmic neighborhood than previously thought. Rather than one star for every brown dwarf, there may be as many as six stars for every brown dwarf. The majority of solar-type stars are single. The previous idea stated half or perhaps most stellar systems were binary, trinary, or multiple-star systems associated with clusters of stars, rather than the single-star systems that tend to be seen most often. In a 2017 paper, Sarah Sadavoy and Steven Stahler argued that the Sun was likely part of a binary system at the time of its formation, leading them to suggest \"there probably was a Nemesis, a long time ago.” Such a star would have separated from this binary system over four billion years ago, meaning it could not be responsible for the more recent perceived cycle of mass extinctions, Douglas Vakoch told \"Business Insider\", adding that \"If the sun really was part of a binary star system in its early days, its early twin deserves a benign name like Companion, rather than the threatening Nemesis.\" \n\nMore recent theories suggest that other forces, like close passage of other stars, or the angular effect of the galactic gravity plane working against the outer solar orbital plane, may be the cause of orbital perturbations of some outer Solar System objects. In 2011, Coryn Bailer-Jones analyzed craters on the surface of the Earth and reached the conclusion that the earlier findings of simple periodic patterns (implying periodic comet showers dislodged by a hypothetical Nemesis star) were statistical artifacts, and found that the crater record shows no evidence for Nemesis. However, in 2010, A.L. Melott and R.K. Bambach found evidence in the fossil record confirming the extinction event periodicity originally claimed by Raup & Sepkoski in 1984, but at a higher confidence level and over a time period nearly twice as long. The Infrared Astronomical Satellite (IRAS) failed to discover Nemesis in the 1980s. The 2MASS astronomical survey, which ran from 1997 to 2001, failed to detect an additional star or brown dwarf in the Solar System.\n\nUsing newer and more powerful infrared telescope technology which is able to detect brown dwarfs as cool as 150 kelvins out to a distance of 10 light-years from the Sun, the Wide-field Infrared Survey Explorer (WISE survey) has not detected Nemesis. In 2011, David Morrison, a senior scientist at NASA known for his work in risk assessment of near Earth objects, has written that there is no confidence in the existence of an object like Nemesis, since it should have been detected in infrared sky surveys.\n\nIn 1984, paleontologists David Raup and Jack Sepkoski published a paper claiming that they had identified a statistical periodicity in extinction rates over the last 250 million years using various forms of time series analysis. They focused on the extinction intensity of fossil families of marine vertebrates, invertebrates, and protozoans, identifying 12 extinction events over the time period in question. The average time interval between extinction events was determined as 26 million years. At the time, two of the identified extinction events (Cretaceous–Paleogene and Eocene–Oligocene) could be shown to coincide with large impact events. Although Raup and Sepkoski could not identify the cause of their supposed periodicity, they suggested a possible non-terrestrial connection. The challenge to propose a mechanism was quickly addressed by several teams of astronomers.\n\nIn 2010, Melott & Bambach re-examined the fossil data, including the now-improved dating, and using a second independent database in addition to that Raup & Sepkoski had used. They found evidence for a signal showing an excess extinction rate with a 27-million-year periodicity, now going back 500 million years, and at a much higher statistical significance than in the older work. They also determined that this periodicity is inconsistent with the Nemesis hypothesis. The change from 26 to 27 million years is expected based on a 3% \"stretch\" in the geological timescale since the 1980s.\n\nTwo teams of astronomers, Daniel P. Whitmire and Albert A. Jackson IV, and Marc Davis, Piet Hut, and Richard A. Muller, independently published similar hypotheses to explain Raup and Sepkoski's extinction periodicity in the same issue of the journal \"Nature\". This hypothesis proposes that the Sun may have an undetected companion star in a highly elliptical orbit that periodically disturbs comets in the Oort cloud, causing a large increase of the number of comets visiting the inner Solar System with a consequential increase of impact events on Earth. This became known as the \"Nemesis\" or \"Death Star\" hypothesis.\n\nIf it does exist, the exact nature of Nemesis is uncertain. Muller suggests that the most likely object is a red dwarf with an apparent magnitude between 7 and 12, while Daniel P. Whitmire and Albert A. Jackson argue for a brown dwarf. If a red dwarf, it would exist in star catalogs, but it would only be confirmed by measuring its parallax; due to orbiting the Sun it would have a low proper motion and would escape detection by older proper motion surveys that have found stars like the 9th-magnitude Barnard's star. (The proper motion of Barnard's star was detected in 1916.) Muller expects Nemesis to be discovered by the time parallax surveys reach the 10th magnitude.\n\nMuller, referring to the date of a recent extinction at 11 million years before the present day, posits that Nemesis has a semi-major axis of about and suggests it is located (supported by Yarris, 1987) near Hydra, based on a hypothetical orbit derived from original aphelions of a number of atypical long-period comets that describe an orbital arc meeting the specifications of Muller's hypothesis. Richard Muller's most recent paper relevant to the Nemesis theory was published in 2002. In 2002, Muller speculated that Nemesis was perturbed 400 million years ago by a passing star from a circular orbit into an orbit with an eccentricity of 0.7.\n\nThe trans-Neptunian object Sedna has an extra-long and unusual elliptical orbit around the Sun, ranging between 76 and 937 AU. Sedna's orbit is estimated to last between 10.5 and 12 thousand years. Its discoverer, Michael Brown of Caltech, noted in a \"Discover\" magazine article that Sedna's location seemed to defy reasoning: \"Sedna shouldn't be there\", Brown said. \"There's no way to put Sedna where it is. It never comes close enough to be affected by the Sun, but it never goes far enough away from the Sun to be affected by other stars.\" Brown therefore postulated that a massive unseen object may be responsible for Sedna's anomalous orbit. This line of inquiry eventually led to the hypothesis of Planet Nine.\n\nBrown has stated that it is more likely that one or more non-companion stars, passing near the Sun billions of years ago, could have pulled Sedna out into its current orbit. In 2004, Kenyon forwarded this explanation after analysis of Sedna's orbital data and computer modeling of possible ancient non-companion star passes.\n\nSearches for Nemesis in the infrared are important because cooler stars comparatively shine more brightly in infrared light. The University of California's Leuschner Observatory failed to discover Nemesis by 1986. The Infrared Astronomical Satellite (IRAS) failed to discover Nemesis in the 1980s. The 2MASS astronomical survey, which ran from 1997 to 2001, failed to detect a star, or brown dwarf, in the Solar System. If Nemesis exists, it may be detected by Pan-STARRS or the planned LSST astronomical surveys.\n\nIn particular, if Nemesis is a red dwarf or a brown dwarf, the WISE mission (an infrared sky survey that covered most of our solar neighborhood in movement-verifying parallax measurements) was expected to be able to find it. WISE can detect 150-kelvin brown dwarfs out to 10 light-years. But the closer a brown dwarf is, the easier it is to detect. Preliminary results of the WISE survey were released on April 14, 2011. On March 14, 2012, the entire catalog of the WISE mission was released. In 2014 WISE data ruled out a Saturn or larger-sized body in the Oort cloud out to ten thousand AU.\n\nCalculations in the 1980s suggested that a Nemesis object would have an irregular orbit due to perturbations from the galaxy and passing stars. The Melott & Bambach work shows an extremely regular signal, inconsistent with the expected irregularities in such an orbit. Thus, while supporting the extinction periodicity, it appears to be inconsistent with the Nemesis hypothesis, though of course not inconsistent with other kinds of substellar objects. According to a 2011 NASA news release, \"recent scientific analysis no longer supports the idea that extinctions on Earth happen at regular, repeating intervals, and thus, the Nemesis hypothesis is no longer needed.\"\n\n\n\n"}
{"id": "4360893", "url": "https://en.wikipedia.org/wiki?curid=4360893", "title": "Niðafjöll", "text": "Niðafjöll\n\nNiðafjöll (pronounced , also written Niðvellir), which means \"dark mountains\", are located in the northern underworld. Niðafjöll is the site from which the dragon Níðhöggr comes. According to Snorri Sturluson, the good and virtuous people will live here in a golden palace after the Ragnarök, despite its proximity to Hel.\n\nNiðafjöll is mentioned in \"Völuspá\" from the Poetic Edda.\n\n"}
{"id": "43669588", "url": "https://en.wikipedia.org/wiki?curid=43669588", "title": "NowCast (air quality index)", "text": "NowCast (air quality index)\n\nThe NowCast is a weighted average of hourly air monitoring data used by the United States Environmental Protection Agency for real-time reporting of the Air Quality Index (AQI) for PM (PM or PM) or Ozone data.\n\nThe PM NowCast is computed from the most recent 12 hours of PM monitoring data, but the NowCast weights the most recent hours of data more heavily than an ordinary 12-hour average when pollutant levels are changing. The PM NowCast is used in lieu of a 24-hour average PM concentration in the calculation of the AQI until an entire calendar day of hourly concentrations has been monitored.\n\nSimilarly, the Ozone NowCast is computed from the most recent 8 hours of ozone monitoring data, but weights the most recent hours of data more heavily than an eight-hour average when pollutant levels are changing. The Ozone NowCast is used in lieu of a forward rolling 8-hour average for the calculation of the AQI until an entire calendar day of data hourly data is available. \nLet c,c...c represent the hourly PM concentrations for the most recent 12-hour period, with c the most recent hourly value, and let c and c represent the minimum and maximum hourly concentration for the 12-hour period.\nDefine:\n\nand let\n\nWith these definitions the PM NowCast is given by:\n\nFor the special case where there is no variability in the hourly values, c = c, w = 1, and the NowCast reduces to the twelve-hour average:\n\nFor the special case where w=1/2:\n\nBut 1/(1-x)=1 + x + x+ ... , x < 1, so to a good approximation, when w = 1/2:\nBecause the most recent hours of data are weighted so heavily in the NowCast when PM levels are changing, EPA does not report the NowCast when data is missing for c or c.\n\nLet c,c...c represent the hourly ozone concentrations for the most recent 8-hour period, with c the most recent hourly value, and let c and c represent the minimum and maximum hourly concentration for the 8-hour period.\nDefine:\n\nWith these definitions the Ozone NowCast is given by\n\nNote: USEPA generally truncates ozone values to whole PPB, so a value of 13.78 PPB would use a value of 13.0 in the NowCast calculation, and a value of 0.01378 PPM would use a value 0.013 PPM. Using un-truncated numbers may yield discrepancies with forecast data presented at the AIRNow site.\n\nConsider a day when the hourly average PM concentration is zero for all hours of the day, except for a single hour from noon to 1 pm, where a monitor records a concentration pulse of 71 micrograms per cubic meter (µg/m). According to the equation above, the Nowcast is 71/2 µg/m=35.5 µg/m the hour after the pulse, two hours later it is 71/4 µg/m=17.8 µg/m and three hours later it is 71/8 µg/m= 8.9 µg/m. To calculate the corresponding AQI values, each NowCast concentration is substituted into the AQI equation in place of the 24-hour average PM concentration:\n\nwhere:\n\nand:\n\nThus, the three NowCast concentrations correspond to air quality indices of 101, (AQI Color Code Orange, Air Quality: Unhealthy for Sensitive Groups), 63 (AQI Color Code Yellow, Air Quality: Moderate ), and 37 (AQI Color Code Green, Air Quality: Good) respectively. \nAfter the day is over and all of the hourly data is available, the AQI for the day is calculated from the 24-hr average; 71/24 µg/m= 3.0 µg/m, an AQI of 12 (Color Code Green, Air Quality: Good). EPA has developed a calculator to compute the PM NowCast, AQI, and AQI category and color from the most recent 12 hours of monitoring data.\n"}
{"id": "19654281", "url": "https://en.wikipedia.org/wiki?curid=19654281", "title": "Pangaea", "text": "Pangaea\n\nPangaea or Pangea () was a supercontinent that existed during the late Paleozoic and early Mesozoic eras. It assembled from earlier continental units approximately 335 million years ago, and it began to break apart about 175 million years ago. In contrast to the present Earth and its distribution of continental mass, much of Pangaea was in the southern hemisphere and surrounded by a superocean, Panthalassa. Pangaea was the most recent supercontinent to have existed and the first to be reconstructed by geologists.\n\nThe name \"Pangaea/Pangea\" is derived from Ancient Greek \"pan\" (, \"all, entire, whole\") and \"Gaia\" (, \"Mother Earth, land\"). The concept that the continents once formed a continuous land mass was first proposed by Alfred Wegener, the originator of the scientific theory of continental drift, in his 1912 publication \"The Origin of Continents\" (\"Die Entstehung der Kontinente\"). He expanded upon his hypothesis in his 1915 book \"The Origin of Continents and Oceans\" (\"Die Entstehung der Kontinente und Ozeane\"), in which he postulated that, before breaking up and drifting to their present locations, all the continents had formed a single supercontinent that he called the \"Urkontinent\".\n\nThe name \"Pangea\" occurs in the 1920 edition of \"Die Entstehung der Kontinente und Ozeane\", but only once, when Wegener refers to the ancient supercontinent as \"the Pangaea of the Carboniferous\". Wegener used the Germanized form \"Pangäa\", but the name entered German and English scientific literature (in 1922 and 1926, respectively) in the Latinized form \"Pangaea\" (of the Greek \"Pangaia\"), especially due to a symposium of the American Association of Petroleum Geologists in November 1926.<ref name=\"Waterschoot/Gracht\">Willem A. J. M. van Waterschoot van der Gracht (and 13 other authors): \"Theory of Continental Drift: a Symposium of the Origin and Movements of Land-masses of both Inter-Continental and Intra-Continental, as proposed by Alfred Wegener.\" X + 240 S., Tulsa, Oklahoma, USA, The American Association of Petroleum Geologists & London, Thomas Murby & Co.</ref>\n\nThe forming of supercontinents and their breaking up appears to have been cyclical through Earth's history. There may have been many others before Pangaea. The fourth-last supercontinent, called Columbia or Nuna, appears to have assembled in the period 2.0–1.8 Ga. Columbia/Nuna broke up and the next supercontinent, Rodinia, formed from the accretion and assembly of its fragments. Rodinia lasted from about 1.1 billion years ago (Ga) until about 750 million years ago, but its exact configuration and geodynamic history are not nearly as well understood as those of the later supercontinents, Pannotia and Pangaea.\n\nWhen Rodinia broke up, it split into three pieces: the supercontinent of Proto-Laurasia, the supercontinent of Proto-Gondwana, and the smaller Congo craton. Proto-Laurasia and Proto-Gondwana were separated by the Proto-Tethys Ocean. Next Proto-Laurasia itself split apart to form the continents of Laurentia, Siberia and Baltica. Baltica moved to the east of Laurentia, and Siberia moved northeast of Laurentia. The splitting also created two new oceans, the Iapetus Ocean and Paleoasian Ocean. Most of the above masses coalesced again to form the relatively short-lived supercontinent of Pannotia. This supercontinent included large amounts of land near the poles and, near the equator, only a relatively small strip connecting the polar masses. Pannotia lasted until 540 Ma, near the beginning of the Cambrian period and then broke up, giving rise to the continents of Laurentia, Baltica, and the southern supercontinent of Gondwana.\n\nIn the Cambrian period, the continent of Laurentia, which would later become North America, sat on the equator, with three bordering oceans: the Panthalassic Ocean to the north and west, the Iapetus Ocean to the south and the Khanty Ocean to the east. In the Earliest Ordovician, around 480 Ma, the microcontinent of Avalonia – a landmass incorporating fragments of what would become eastern Newfoundland, the southern British Isles, and parts of Belgium, northern France, Nova Scotia, New England, South Iberia and northwest Africa – broke free from Gondwana and began its journey to Laurentia. Baltica, Laurentia, and Avalonia all came together by the end of the Ordovician to form a minor supercontinent called Euramerica or Laurussia, closing the Iapetus Ocean. The collision also resulted in the formation of the northern Appalachians. Siberia sat near Euramerica, with the Khanty Ocean between the two continents. While all this was happening, Gondwana drifted slowly towards the South Pole. This was the first step of the formation of Pangaea.\n\nThe second step in the formation of Pangaea was the collision of Gondwana with Euramerica. By the Silurian, 440 Ma, Baltica had already collided with Laurentia, forming Euramerica. Avalonia had not yet collided with Laurentia, but as Avalonia inched towards Laurentia, the seaway between them, a remnant of the Iapetus Ocean, was slowly shrinking. Meanwhile, southern Europe broke off from Gondwana and began to move towards Euramerica across the newly formed Rheic Ocean. It collided with southern Baltica in the Devonian, though this microcontinent was an underwater plate. The Iapetus Ocean's sister ocean, the Khanty Ocean, shrank as an island arc from Siberia collided with eastern Baltica (now part of Euramerica). Behind this island arc was a new ocean, the Ural Ocean.\n\nBy the late Silurian, North and South China split from Gondwana and started to head northward, shrinking the Proto-Tethys Ocean in their path and opening the new Paleo-Tethys Ocean to their south. In the Devonian Period, Gondwana itself headed towards Euramerica, causing the Rheic Ocean to shrink. In the Early Carboniferous, northwest Africa had touched the southeastern coast of Euramerica, creating the southern portion of the Appalachian Mountains, the Meseta Mountains and the Mauritanide Mountains. South America moved northward to southern Euramerica, while the eastern portion of Gondwana (India, Antarctica and Australia) headed toward the South Pole from the equator. North and South China were on independent continents. The Kazakhstania microcontinent had collided with Siberia. (Siberia had been a separate continent for millions of years since the deformation of the supercontinent Pannotia in the Middle Carboniferous.)\n\nWestern Kazakhstania collided with Baltica in the Late Carboniferous, closing the Ural Ocean between them and the western Proto-Tethys in them (Uralian orogeny), causing the formation of not only the Ural Mountains but also the supercontinent of Laurasia. This was the last step of the formation of Pangaea. Meanwhile, South America had collided with southern Laurentia, closing the Rheic Ocean and forming the southernmost part of the Appalachians and Ouachita Mountains. By this time, Gondwana was positioned near the South Pole and glaciers were forming in Antarctica, India, Australia, southern Africa and South America. The North China block collided with Siberia by the Late Carboniferous, completely closing the Proto-Tethys Ocean.\n\nBy the early Permian, the Cimmerian plate split from Gondwana and headed towards Laurasia, thus closing the Paleo-Tethys Ocean, but forming a new ocean, the Tethys Ocean, in its southern end. Most of the landmasses were all in one. By the Triassic Period, Pangaea rotated a little and the Cimmerian plate was still travelling across the shrinking Paleo-Tethys, until the Middle Jurassic. The Paleo-Tethys had closed from west to east, creating the Cimmerian Orogeny. Pangaea, which looked like a \"C\", with the new Tethys Ocean inside the \"C\", had rifted by the Middle Jurassic, and its deformation is explained below.\n\nFossil evidence for Pangaea includes the presence of similar and identical species on continents that are now great distances apart. For example, fossils of the therapsid \"Lystrosaurus\" have been found in South Africa, India and Antarctica, alongside members of the \"Glossopteris\" flora, whose distribution would have ranged from the polar circle to the equator if the continents had been in their present position; similarly, the freshwater reptile \"Mesosaurus\" has been found in only localized regions of the coasts of Brazil and West Africa.\n\nAdditional evidence for Pangaea is found in the geology of adjacent continents, including matching geological trends between the eastern coast of South America and the western coast of Africa. The polar ice cap of the Carboniferous Period covered the southern end of Pangaea. Glacial deposits, specifically till, of the same age and structure are found on many separate continents that would have been together in the continent of Pangaea.\n\nPaleomagnetic study of apparent polar wandering paths also support the theory of a supercontinent. Geologists can determine the movement of continental plates by examining the orientation of magnetic minerals in rocks; when rocks are formed, they take on the magnetic properties of the Earth and indicate in which direction the poles lie relative to the rock. Since the magnetic poles drift about the rotational pole with a period of only a few thousand years, measurements from numerous lavas spanning several thousand years are averaged to give an apparent mean polar position. Samples of sedimentary rock and intrusive igneous rock have magnetic orientations that are typically an average of the \"secular variation\" in the orientation of magnetic north because their remanent magnetizations are not acquired instantaneously. Magnetic differences between sample groups whose age varies by millions of years is due to a combination of true polar wander and the drifting of continents. The true polar wander component is identical for all samples, and can be removed, leaving geologists with the portion of this motion that shows continental drift and can be used to help reconstruct earlier continental positions.\n\nThe continuity of mountain chains provides further evidence for Pangaea. One example of this is the Appalachian Mountains chain, which extends from the southeastern United States to the Caledonides of Ireland, Britain, Greenland, and Scandinavia.\n\nThere were three major phases in the break-up of Pangaea. The first phase began in the Early-Middle Jurassic (about 175 Ma), when Pangaea began to rift from the Tethys Ocean in the east to the Pacific in the west. The rifting that took place between North America and Africa produced multiple failed rifts. One rift resulted in a new ocean, the North Atlantic Ocean.\n\nThe Atlantic Ocean did not open uniformly; rifting began in the north-central Atlantic. The South Atlantic did not open until the Cretaceous when Laurasia started to rotate clockwise and moved northward with North America to the north, and Eurasia to the south. The clockwise motion of Laurasia led much later to the closing of the Tethys Ocean and the widening of the \"Sinus Borealis\", which later became the Arctic Ocean. Meanwhile, on the other side of Africa and along the adjacent margins of east Africa, Antarctica and Madagascar, new rifts were forming that would lead to the formation of the southwestern Indian Ocean that would open up in the Cretaceous.\n\nThe second major phase in the break-up of Pangaea began in the Early Cretaceous (150–140 Ma), when the minor supercontinent of Gondwana separated into multiple continents (Africa, South America, India, Antarctica, and Australia). The subduction at Tethyan Trench probably caused Africa, India and Australia to move northward, causing the opening of a \"South Indian Ocean\". In the Early Cretaceous, Atlantica, today's South America and Africa, finally separated from eastern Gondwana (Antarctica, India and Australia). Then in the Middle Cretaceous, Gondwana fragmented to open up the South Atlantic Ocean as South America started to move westward away from Africa. The South Atlantic did not develop uniformly; rather, it rifted from south to north.\n\nAlso, at the same time, Madagascar and India began to separate from Antarctica and moved northward, opening up the Indian Ocean. Madagascar and India separated from each other 100–90 Ma in the Late Cretaceous. India continued to move northward toward Eurasia at 15 centimeters (6 in) a year (a plate tectonic record), closing the eastern Tethys Ocean, while Madagascar stopped and became locked to the African Plate. New Zealand, New Caledonia and the rest of Zealandia began to separate from Australia, moving eastward toward the Pacific and opening the Coral Sea and Tasman Sea.\n\nThe third major and final phase of the break-up of Pangaea occurred in the early Cenozoic (Paleocene to Oligocene). Laurasia split when North America/Greenland (also called Laurentia) broke free from Eurasia, opening the Norwegian Sea about 60–55 Ma. The Atlantic and Indian Oceans continued to expand, closing the Tethys Ocean.\n\nMeanwhile, Australia split from Antarctica and moved quickly northward, just as India had done more than 40 million years before. Australia is currently on a collision course with eastern Asia. Both Australia and India are currently moving northeast at 5–6 centimeters (2–3 in) a year. Antarctica has been near or at the South Pole since the formation of Pangaea about 280 Ma. India started to collide with Asia beginning about 35 Ma, forming the Himalayan orogeny, and also finally closing the Tethys Seaway; this collision continues today. The African Plate started to change directions, from west to northwest toward Europe, and South America began to move in a northward direction, separating it from Antarctica and allowing complete oceanic circulation around Antarctica for the first time. This motion, together with decreasing atmospheric carbon dioxide concentrations, caused a rapid cooling of Antarctica and allowed glaciers to form. This glaciation eventually coalesced into the kilometers-thick ice sheets seen today. Other major events took place during the Cenozoic, including the opening of the Gulf of California, the uplift of the Alps, and the opening of the Sea of Japan. The break-up of Pangaea continues today in the Red Sea Rift and East African Rift.\n\nPangaea's formation is now commonly explained in terms of plate tectonics. The involvement of plate tectonics in Pangaea's separation helps to show how it did not separate all at once, but at different times, in sequences. Additionally, after these separations, it has also been discovered that the separated land masses may have also continued to break apart multiple times. The formation of each environment and climate on Pangaea is due to plate tectonics, and thus, it is as a result of these shifts and changes different climatic pressures were placed on the life on Pangaea. Although plate tectonics was paramount in the formation of later land masses, it was also essential in the placement, climate, environments, habitats, and overall structure of Pangaea.\n\nWhat can also be observed in relation to tectonic plates and Pangaea, is the formations to such plates. Mountains and valleys form due to tectonic collisions as well as earthquakes and chasms. Consequentially, this shaped Pangaea and animal adaptations. Furthermore, plate tectonics can contribute to volcanic activity, which is responsible for extinctions and adaptations that have evidently affected life over time, and without doubt on Pangaea.\n\nFor the approximately 160 million years Pangaea existed, many species did well, whereas others struggled. The Traversodonts were an example of such successful animals. Plants dependent on spore reproduction were largely replaced by the gymnosperms, which reproduce through the use of seeds. Later on, insects (including beetles and cicadas) also thrived, during the Permian period 299 to 252 million years ago. However, the Permian extinction at 252 Mya greatly impacted these insects in mass extinction, being the only mass extinction to affect insects. When the Triassic Period came, many reptiles were able to also thrive, including Archosaurs, which were an ancestor to modern-day crocodiles and birds.\n\nLittle is known about marine life during the existence of Pangaea. Scientists are unable to find substantial evidence or fossilized remains in order to assist them in answering such questions. However, a couple of marine animals have been determined to have existed at the time - the Ammonites and Brachiopods. Additionally, evidence pointing towards massive reefs with varied ecosystems, especially in the species of sponges and coral, have also been discovered.\n\nThe reconfiguration of continents and oceans after the breakup of Pangea changed the world's climate. There is scientific evidence that this change was drastic. When the continents separated and reformed themselves, it changed the flow of the oceanic currents and winds. The scientific reasoning behind all of the changes is Continental Drift. The theory of Continental Drift, created by Alfred Wegener, explained how the continents shifted Earth’s surface and how that affected many aspects such as climate, rock formations found on different continents and plant and animal fossils. Wegener studied plant fossils from the frigid Arctic of Svalbard, Norway. He determined that such plants were not adapted to a glacial climate. The fossils he found were from tropical plants that were adapted to thrive in warmer and tropical climates. Because he would not assume that the plant fossils were capable of traveling to a different place, he suspected that Svalbard had had a warmer, less frigid climate in the past.\n\nWhen Pangaea separated, the reorganization of the continents changed the function of the oceans and seaways. The restructuring of the continents, changed and altered the distribution of warmth and coolness of the oceans. When North America and South America connected, it stopped equatorial currents from passing from the Atlantic Ocean to the Pacific Ocean. Researchers have found evidence by using computer hydrological models to show that this strengthened the Gulf Stream by diverting more warm currents towards Europe. Warm waters at high latitudes led to an increased evaporation and eventually atmospheric moisture. Increased evaporation and atmospheric moisture resulted in increased precipitation. Evidence of increased precipitation is the development of snow and ice that covers Greenland, which led to an accumulation of the icecap. Greenland’s growing ice cap led to further global cooling. Scientists also found evidence of global cooling through the separation of Australia and Antarctica and the formation of the Antarctic Ocean. Ocean currents in the newly formed Antarctic or Southern Ocean created a circumpolar current. The creation of the new ocean that caused a circumpolar current eventually led to atmospheric currents that rotated from west to east. Atmospheric and oceanic currents stopped the transfer of warm, tropical air and water to the higher latitudes. As a result of the warm air and currents moving northward, Antarctica cooled down so much that it became frigid.\n\nAlthough many of Alfred Wegener’s theories and conclusions were valid, scientists are constantly coming up with new innovative ideas or reasoning behind why certain things happen. Wegener’s theory of Continental Drift was later replaced by the theory of tectonic plates.\n\nThere is evidence to suggest that the deterioration of northern Pangaea contributed to the Permian Extinction, one of Earth’s five major mass extinction events, which resulted in the loss of over 90% of marine and 70% of terrestrial species. There were three main sources of environmental deterioration that are believed to have had a hand in the extinction event.\n\nThe first of these sources is a loss of oxygen concentration in the ocean, which caused deep water regions called the lysocline to grow shallower. With the lysocline shrinking, there were fewer places for calcite to dissolve in the ocean, considering calcite only dissolves at deep ocean depths. This led to the extinction of carbonate producers such as brachiopods and corals that relied on dissolved calcite to survive. The second source is the eruption of the Siberian Traps, a large volcanic event that is argued to be the result of Pangaean tectonic movement. This had several negative repercussions on the environment, including metal loading and excess atmospheric carbon. Metal loading, the release of toxic metals from volcanic eruptions into the environment, led to acid rain and general stress on the environment. These toxic metals are known to infringe on vascular plants’ ability to photosynthesize, which may have resulted in the loss of Permian-era flora. Excess carbon dioxide in the atmosphere is believed to be the main cause of the shrinking of lysocline areas.\n\nThe third cause of this extinction event that can be attributed to northern Pangaea is the beginnings of anoxic ocean environments, or oceans with very low oxygen concentrations. The mix of anoxic oceans and ocean acidification due to metal loading led to increasingly acidic oceans, which ultimately led to the extinction of benthic species.\n\n\n"}
{"id": "922972", "url": "https://en.wikipedia.org/wiki?curid=922972", "title": "Paste (rheology)", "text": "Paste (rheology)\n\nIn physics, a paste is a substance that behaves as a solid until a sufficiently large load or stress is applied, at which point it flows like a fluid. In rheological terms, a paste is an example of a Bingham plastic fluid.\n\nPastes typically consist of a suspension of granular material in a background fluid. The individual grains are jammed together like sand on a beach, forming a disordered, glassy or amorphous structure, and giving pastes their solid-like character. It is this \"jamming together\" that gives pastes some of their most unusual properties; this causes paste to demonstrate properties of fragile matter. \n\nIn pharmacology, paste is basic pharmaceutical form. It consists of fatty base (e.g., petroleum jelly) and at least 25% solid substance (e.g., zinc oxide).\nPastes are the semisolid preparations intended for external application to the skin. Usually they are thick and do not melt at normal temperature. Remain on the area for longer duration.\n\nExamples include starch pastes, toothpaste, mustard, and putty.\n\n"}
{"id": "41809901", "url": "https://en.wikipedia.org/wiki?curid=41809901", "title": "Privacy and Civil Liberties Oversight Board report on mass surveillance", "text": "Privacy and Civil Liberties Oversight Board report on mass surveillance\n\nThe Privacy and Civil Liberties Oversight Board report on mass surveillance was issued in January 2014 in light of the global surveillance disclosures of 2013, recommending the US end bulk data collection.\n\nThe Privacy and Civil Liberties Oversight Board was first chartered under the Intelligence Reform and Terrorism Prevention Act of 2004. The role of the board is to provide advice and review of whether adequate supervision, guidelines, and oversight exist and to \"continually review\" regulations, policies, procedures, and information sharing practices to ensure privacy and civil liberties considerations are protected. To carry out these roles, the board does not have subpoena power, but is able to request subpoenas subject to the U.S. Attorney General's discretion \"to protect sensitive law enforcement or counterterrorism information or ongoing operations.\" The U.S. Director of National Intelligence also has the power to override requests \"to protect the national security interests of the United States\"\n\nA report by former members of the 9/11 commission in December 2005 noted there was \"little urgency\" in creation of the board, whose first meeting was in 2006. It was initially composed of a chair, vice chair, and three other members. As these members served at the pleasure of the President, \"Critics... maintained that the board appeared to be a presidential appendage, devoid of the capability to exercise independent judgment and assessment or to provide impartial findings and recommendations\", according to the Congressional Research Service. Subsequently member Lanny Davis resigned in protest over the board's lack of independence, citing \"extensive redlining by Administration officials of the board's first report to Congress\" that was accepted by the other members. The board was then reconstituted under the Implementing Recommendations of the 9/11 Commission Act of 2007 (H.R. 1), beginning in January 2008, as an independent agency with appointments subject to Senate confirmation. Four members of the board were nominated by President Barack Obama in 2011, and confirmed by the Senate in August 2012. Board chairman David Medine was finally confirmed in May 2013 in the wake of the Snowden disclosures in a party-line vote with 53 Democrats supporting and 45 Republicans opposing.\n\nThe Board's report follows a series of highly publicized leaks about the operations of the global surveillance program conducted by the National Security Agency in the United States working with a number of other countries (see Five Eyes). While the program's nominal focus was on foreign nationals, the disclosures also revealed the large-scale surveillance of communications by United States citizens. These leaks were largely the work of Edward Snowden, a Booz Allen Hamilton employee with access to a wide range of top secret documents. Publications of the documents by \"The Washington Post\" and \"The Guardian\" began in June 2013.\n\nOn January 23, 2014, the board released its report, recommending the US end bulk data collection. Instead federal agencies would be able to obtain phone and other records under court orders in cases containing an individualized suspicion of wrongdoing. But there would be no storehouse, private or public, of telephone data beyond what the phone companies keep in the course of their normal business activities. The report concluded that the program \"lacks a viable legal foundation\". It concluded \"we see little evidence that the unique capabilities provided by the NSA's bulk collection of telephone records actually have yielded material counterterrorism results that could not have been achieved without the NSA's Section 215 program.\" The report concluded: “Cessation of the program would eliminate the privacy and civil liberties concerns associated with bulk collection without unduly hampering the government’s efforts, while ensuring that any governmental requests for telephone calling records are tailored to the needs of specific investigations.”\n\nThe report called for a \"Special Advocate\" to be involved in some cases before the FISA court judge. The Board also contained the recommendation to release future and past FISC decisions “that involve novel interpretations of FISA or other significant questions of law, technology or compliance.”\n\nThe report also recommended against an alternate proposal, which President Barack Obama had ordered Attorney General Eric Holder to formulate within 60 days, which would force third parties including telephone companies to conduct data retention. The panel's majority spokesman said this was due to cost and legal exposure for the companies involved, and that it was not an \"easy out\".\n\nSen. Ron Wyden, a senior member of the intelligence committee and long-time critic of the programs, released a statement in reaction to the report. Said Wyden: \"The privacy board's findings closely mirror many of the criticisms made by surveillance reform advocates. The bulk collection program was built on a murky legal foundation that raises many constitutional questions and has been proven to be an ineffective tool for collecting unique intelligence information. Moreover, as the board wrote in its report, a program where the government collects the telephone records of millions of law-abiding Americans 'fundamentally shifts the balance of power between the state and its citizens.' The board goes on to say that with the government's 'powers of compulsion and criminal prosecution,' collection of data on its own citizens 'poses unique threats to privacy,' and is expected to have a 'chilling effect on the free exercise of speech and association.'\"\n\nAsked by Patrick Mächler to what extent the report will have an impact, Edward Snowden stated: I don’t see how Congress could ignore it, as it makes it clear there is no reason at all to maintain the 215 program. Let me quote from the official report:\n\n“Cessation of the program would eliminate the privacy and civil liberties concerns associated with bulk collection without unduly hampering the government’s efforts, while ensuring that any governmental requests for telephone calling records are tailored to the needs of specific investigations.”\n\n\n"}
{"id": "11027261", "url": "https://en.wikipedia.org/wiki?curid=11027261", "title": "RazakSAT", "text": "RazakSAT\n\nRazakSAT is a Malaysian Earth observation satellite carrying a high-resolution camera. It was launched into low Earth orbit on 14 July 2009. It was placed into a near-equatorial orbit that presents many imaging opportunities for the equatorial region. It weighs over three times a much as TiungSAT-1 and carries a high resolution Earth observation camera. Developed in conjunction with Satrec Initiative, the satellite's low inclination orbit (9 degrees) brought it over Malaysia a dozen or more times per day. This was intended to provide greatly increased coverage of Malaysia, compared to most other Earth observation satellites. An audit report released in October 2011 revealed that the satellite had failed after only 1 year of operation.\n\nRazakSAT was the first satellite to be put in orbit by a completely developed private liquid-fueled rocket: SpaceX's Falcon 1 .\n\nThis satellite is Malaysia's second remote sensing satellite after TiungSAT-1.\n\nOriginally called MACSAT, RazakSAT's payload is mainly electro-optical, carrying a Medium-sized Aperture Camera (MAC) which is a pushbroom camera with five linear detectors (one panchromatic, four multi-spectral) weighing approximately 50 kg. The entire satellite weighs at about 180 kg.\n\nSpaceX launched RazakSAT at 03:35 UTC on 14 July 2009 using a Falcon 1 rocket. This was the fifth flight of a Falcon 1, and like the previous flights lift-off was from Omelek Island in the Kwajalein Atoll. At 05:25 UTC Elon Musk, founder and CEO of SpaceX, told a reporter the launch had been a success. \"We nailed the orbit to well within target parameters...pretty much a bullseye.\" Musk said.\n\nRazakSAT's mission plan was carried out by engineers from ATSB.\n\nThis was especially important because Malaysia is usually covered by the equatorial cloud bands. Normal sun-synchronous optical satellites, which may re-visit an area only once every 7 days, will almost never be able to see the ground during their pass. As a result, much optical satellite imagery of Malaysia have more than 50% cloud cover within the image's footprint.\n\nRazaksat, on the other hand, revisited some parts of Malaysian territory every 90 minutes, potentially maximising its ability to exploit gaps in the clouds.\n\nRazakSAT, equipped with a high resolution Medium-Sized Aperture Camera (MAC), achieved the intended Near-Equatorial Low Earth Orbit (NEqO) at 685 km altitude and a 9 degree inclination. It was expected to provide high resolution images of Malaysia that can be applied to land management, resource development and forestry.\n\nBe that as it may, the NEqO orbit does have 3 distinct disadvantages. Analysis through the commercially available Satellite Tool Kit (STK) software had shown that the orbit revisit over Malaysia was found to have a maximum peak of 2-4 overpasses per day during daylight hours (from 8 am to 6:30 pm) to a period of 6 consecutive days without any usable daylight overpasses. The second disadvantage is that most imagery acquired through the NEqO orbit are not usable through a remote sensing perspective as the NEqO orbit is not a sun-synchronous orbit, a vital criterion for monitoring and analysis work. Thirdly, it was found that the NEqO orbit exposes the satellite to the South Atlantic Anomaly (SAA) phenomenon on every orbit it takes around the earth unlike the polar orbit, near-polar or sun-synchronous orbits, thereby further increasing the risk of radiation damage to the satellite.\n\nOriginally slated for commercial purposes in 2009,\nthe aim of the RazakSAT project was then changed into a Research & Development (R&D) Project in 2010.\n\nDuring its operations after its launch in 2009, it was discovered that the RazakSAT satellite could not achieve its targeted pointing accuracy of within 1 km of its intended target. The Malaysian English newspaper The Star, quoting from the Malaysian Government Auditor-General's Report 2010, reported that the images that were acquired by the RazakSAT satellite was found to be 37 km off their intended target.\n\nAs a result of the pointing error, all of the over 1,328 images acquired by the satellite were rendered unusable. Despite a promise of delivering MAC images by 2010, as of the end of 2011, the operators have not released any images. Efforts to fix the problems with the satellite were terminated in December 2010.\n\nThe major specification of RazakSAT, as provided by the satellites' builder and operator (ATSB) in February 2010, are:\nThe satellite bus used for RazakSAT was jointly developed by ATSB and Satrec Initiative, a commercial satellite manufacturer in Korea. Satrec Initiative markets the bus system as the \"SI-200\".\n\n\n"}
{"id": "24808357", "url": "https://en.wikipedia.org/wiki?curid=24808357", "title": "Roger Revelle Prize", "text": "Roger Revelle Prize\n\nThe Roger Revelle Prize is a prize given out by the Scripps Institution of Oceanography to people who have made outstanding contributions that advance or promote scientific research in fields such as oceanography, climatology and other planetary sciences. This prize is named for Roger Revelle, a scientist who served as director of the Scripps Institution of Oceanography and was instrumental in the founding of the University of California, San Diego. He was an important oceanographer and a pioneer in climate change research. \n\nThe inaugural recipient of this prize was Al Gore, the former Vice President of the United States. He was awarded the prize on 6 March 2009 for his efforts to bring climate change and environmental issues to a worldwide audience. The second recipient of this prize was Prince Albert of Monaco. He was awarded the prize on 23 October 2009 for his efforts to promote scientific research and protection of the environment.\n"}
{"id": "1196186", "url": "https://en.wikipedia.org/wiki?curid=1196186", "title": "Superhard material", "text": "Superhard material\n\nA superhard material is a material with a hardness value exceeding 40 gigapascals (GPa) when measured by the Vickers hardness test. They are highly incompressible solids with high electron density and high bond covalency. As a result of their unique properties, these materials are of great interest in many industrial areas including, but not limited to, abrasives, polishing and cutting tools and wear-resistant and protective coatings.\n\nDiamond is the hardest known material to date, with a Vickers hardness in the range of 70–150 GPa. Diamond demonstrates both high thermal conductivity and electrically insulating properties and much attention has been put into finding practical applications of this material. However, diamond has several limitations for mass industrial application, including its high cost and oxidation at temperatures above 800 °C. In addition, diamond dissolves in iron and forms iron carbides at high temperatures and therefore is inefficient in cutting ferrous materials including steel. Therefore, recent research of superhard materials has been focusing on compounds which would be thermally and chemically more stable than pure diamond.\n\nSuperhard materials can be generally classified into two categories: intrinsic compounds and extrinsic compounds. The intrinsic group includes diamond, cubic boron nitride (c-BN), carbon nitrides and ternary compounds such as B-N-C, which possess an innate hardness. Conversely, extrinsic materials are those that have superhardness and other mechanical properties that are determined by their microstructure rather than composition. An example of extrinsic superhard material is nanocrystalline diamond known as aggregated diamond nanorods.\n\nThe hardness of a material is directly related to its incompressibility, elasticity and resistance to change in shape. A superhard material has high shear modulus, high bulk modulus and does not deform plastically. Ideally superhard materials should have a defect-free, isotropic lattice. This greatly reduces structural deformations that can lower the strength of the material. However, defects can actually strengthen some covalent structures. Traditionally, high-pressure and high-temperature (HPHT) conditions have been used to synthesize superhard materials, but recent superhard material syntheses aim at using less energy and lower cost materials.\n\nHistorically, hardness was first defined as the ability of one material to scratch another and quantified by an integer (sometimes half-integer) from 0 to 10 on the Mohs scale. This scale was however quickly found too discrete and non-linear. Measuring the mechanical hardness of materials changed to using a nanoindenter (usually made of diamond) and evaluating bulk moduli, and the Brinell, Rockwell, Knoop and Vickers scales have been developed. Whereas the Vickers scale is widely accepted as a most common test, there remain controversies on the weight load to be applied during the test. Bulk moduli, shear moduli, and elasticity are the key factors in the superhard classification process.\n\nThe incompressibility of a material is quantified by the bulk modulus B, which measures the resistance of a solid to volume compression under hydrostatic stress as B = −Vdp/dV. Here V is the volume, p is pressure, and dp/dV is the partial derivative of pressure with respect to the volume. The bulk modulus test uses an indenter tool to form a permanent deformation in a material. The size of the deformation depends on the material’s resistance to the volume compression made by the tool. Elements with small molar volumes and strong interatomic forces usually have high bulk moduli. Bulk moduli was the first major test of hardness and originally shown to be correlated with the molar volume (V) and cohesive energy (E) as B ~ E/V Bulk modulus was believed to be a direct measure of a material’s hardness but this no longer remains the dominant school of thought. For example, some alkali and noble metals (Pd, Ag) have anomalously high ratio of the bulk modulus to the Vickers of Brinell hardness. In the early 2000s, a direct relationship between bulk modulus and valence electron density was found as the more electrons were present the greater the repulsions within the structure were. Bulk modulus is still used as a preliminary measure of a material as superhard but it is now known that other properties must be taken into account.\n\nIn contrast to bulk modulus, shear modulus measures the resistance to shape change at a constant volume, taking into account the crystalline plane and direction of shear. The shear modulus G is defined as ratio of shear stress to shear strain: G = stress/strain = F·L/(A·dx), where F is the applied force, A is the area upon which the force acts, dx is the resulting displacement and L is the initial length. The larger the shear modulus, the greater the ability for a material to resist shearing forces. Therefore, the shear modulus is a measure of rigidity. Shear modulus is related to bulk modulus as , where v is the Poisson’s ratio, which is typically ~0.1 in covalent materials. If a material contains highly directional bonds, the shear modulus will increase and give a low Poisson ratio.\n\nA material is also considered hard if it resists plastic deformation. If a material has short covalent bonds, atomic dislocations that lead to plastic deformation are less likely to occur than in materials with longer, delocalized bonds. If a material contains many delocalized bonds it is likely to be soft. Somewhat related to hardness is another mechanical property fracture toughness, which is a material's ability to resist breakage from forceful impact (note that this concept is distinct from the notion of toughness). A superhard material is not necessarily \"supertough\". For example, the fracture toughness of diamond is about 7–10 MPa·m, which is high compared to other gemstones and ceramic materials, but poor compared to many metals and alloys – common steels and aluminium alloys have the toughness values at least 5 times higher.\n\nSeveral properties must be taken into account when evaluating a material as (super)hard. While hard materials have high bulk moduli, a high bulk modulus does not mean a material is hard. Inelastic characteristics must be considered as well, and shear modulus might even provide a better correlation with hardness than bulk modulus. Covalent materials generally have high bond-bending force constants and high shear moduli and are more likely to give superhard structures than, for example, ionic solids.\n\nDiamond is an allotrope of carbon where the atoms are arranged in a modified version of face-centered cubic (\"fcc\") structure known as \"diamond cubic\". It is known for its hardness (see table above) and incompressibility and is targeted for some potential optical and electrical applications. The properties of individual natural diamonds or carbonado vary too widely for industrial purposes, and therefore synthetic diamond became a major research focus.\n\nThe high-pressure synthesis of diamond in 1953 in Sweden and in 1954 in the US, made possible by the development of new apparatus and techniques, became a milestone in synthesis of artificial superhard materials. The synthesis clearly showed the potential of high-pressure applications for industrial purposes and stimulated growing interest in the field. Four years after the first synthesis of artificial diamond, cubic boron nitride c-BN was obtained and found to be the second hardest solid.\n\nSynthetic diamond can exist as a single, continuous crystal or as small polycrystals interconnected through the grain boundaries. The inherent spatial separation of these subunits causes the formation of grains, which are visible by the unaided eye due to the light absorption and scattering properties of the material.\n\nThe hardness of synthetic diamond (70–150 GPa) is very dependent on the relative purity of the crystal itself. The more perfect the crystal structure, the harder the diamond becomes. It has recently been reported that HPHT single crystals and nanocrystalline diamond aggregates (aggregated diamond nanorods) can be harder than natural diamond.\n\nHistorically, it was thought that synthetic diamond should be structurally perfect to be useful. This is because diamond was mainly preferred for its aesthetic qualities, and small flaws in structure and composition were visible by naked eye. Although this is true, the properties associated with these small changes has led to interesting new potential applications of synthetic diamond. For example, nitrogen doping can enhance mechanical strength of diamond, and heavy doping with boron (several atomic percent) makes it a superconductor.\n\nCubic boron nitride or c-BN was first synthesized in 1957 by Robert H. Wentorf at General Electric, shortly after the synthesis of diamond. The general process for c-BN synthesis is the dissolution of hexagonal boron nitride (h-BN) in a solvent-catalyst, usually alkali or alkaline earth metals or their nitrides, followed by spontaneous nucleation of c-BN under high pressure, high temperature (HPHT) conditions. The yield of c-BN is lower and substantially slower compared to diamond's synthetic route due to the complicated intermediate steps. Its insolubility in iron and other metal alloys makes it more useful for some industrial applications than diamond.\nPure cubic boron nitride is transparent or slightly amber. Different colors can be produced depending on defects or an excess of boron (less than 1%). Defects can be produced by doping solvent-catalysts (i.e. Li, Ca, or Mg nitrides) with Al, B, Ti, or Si. This induces a change in the morphology and color of c-BN crystals.\nThe result is darker and larger (500 μm) crystals with better shapes and a higher yield.\n\nCubic boron nitride adopts a sphalerite crystal structure, which can be constructed by replacing every two carbon atoms in diamond with one boron atom and one nitrogen atom. The short B-N (1.57 Å) bond is close to the diamond C-C bond length (1.54 Å), that results in strong covalent bonding between atoms in the same fashion as in diamond. The slight decrease in covalency for B-N bonds compared to C-C bonds reduces the hardness from ~100 GPa for diamond down to 48 GPa in c-BN. As diamond is less stable than graphite, c-BN is less stable than h-BN, but the conversion rate between those forms is negligible at room temperature.\n\nCubic boron nitride is insoluble in iron, nickel, and related alloys at high temperatures, but it binds well with metals due to formation of interlayers of metal borides and nitrides. It is also insoluble in most acids, but is soluble in alkaline molten salts and nitrides, such as LiOH, KOH, NaOH/NaCO, NaNO which are used to etch c-BN. Because of its stability with heat and metals, c-BN surpasses diamond in mechanical applications. The thermal conductivity of BN is among the highest of all electric insulators. In addition, c-BN consists of only light elements and has low X-ray absorptivity, capable of reducing the X-ray absorption background.\n\nDue to its great chemical and mechanical robustness, c-BN has widespread application as an abrasive, such as on cutting tools and scratch resistant surfaces. Cubic boron nitride is also highly transparent to X-rays. This, along with its high strength, makes it possible to have very thin coatings of c-BN on structures that can be inspected using X-rays. Several hundred tonnes of c-BN are produced worldwide each year. By modification, Borazon, a US brand name of c-BN, is used in industrial applications to shape tools, as it can withstand temperatures greater than 2,000 °C. Cubic boron nitride-coated grinding wheels, referred to as Borazon wheels, are routinely used in the machining of hard ferrous metals, cast irons, and nickel-base and cobalt-base superalloys. Other brand names, such as Elbor and Cubonite, are marketed by Russian vendors.\n\nNew approaches in research focus on improving c-BN pressure capabilities of the devices used for c-BN synthesis. At present, the capabilities for the production of c-BN are restricted to pressures of about 6 GPa. Increasing the pressure limit will permit synthesis of larger single crystals than from the present catalytic synthesis. However, the use of solvents under supercritical conditions for c-BN synthesis has been shown to reduce pressure requirements. The high cost of c-BN still limits its application, which motivates the search for other superhard materials.\n\nThe structure of beta carbon nitride (β-CN) was first proposed by Marvin Cohen and Amy Liu in 1989. It is isostructural with SiN and was predicted to be harder than diamond. The calculated bond length was 1.47 Å, 5% shorter than the C-C bond length in diamond. Later calculations indicated that the shear modulus is 60% of that of diamond, and carbon nitride is less hard than c-BN.\n\nDespite two decades of pursuit of this compound, no synthetic sample of CN has validated the hardness predictions; this has been attributed to the difficulty in synthesis and the instability of CN. Carbon nitride is only stable at a pressure that is higher than that of the graphite-to-diamond transformation. The synthesis conditions would require extremely high pressures because carbon is four- and sixfold coordinated. In addition, CN would pose problems of carbide formation if they were to be used to machine ferrous metals. Although publications have reported preparation of CN at lower pressures than stated, synthetic CN was not proved superhard.\n\nThe similar atomic sizes of boron, carbon and nitrogen, as well as the similar structures of carbon and boron nitride polymorphs, suggest that it might be possible to synthesize diamond-like phase containing all three elements. It is also possible to make compounds containing B-C-O, B-O-N, or B-C-O-N under high pressure, but their synthesis would expect to require a complex chemistry and in addition, their elastic properties would be inferior to that of diamond.\n\nBeginning in 1990, a great interest has been put in studying the possibility to synthesize dense B-C-N phases. They are expected to be thermally and chemically more stable than diamond, and harder than c-BN, and would therefore be excellent materials for high speed cutting and polishing of ferrous alloys. These characteristic properties are attributed to the diamond-like structure combined with the sp3 σ-bonds among carbon and the heteroatoms. BCN thin films were synthesized by chemical vapor deposition in 1972. However, data on the attempted synthesis of B-C-N dense phases reported by different authors have been contradictory. It is unclear whether the synthesis products are diamond-like solid solutions between carbon and boron nitride or just mechanical mixtures of highly dispersed diamond and c-BN. In 2001, a diamond-like-structured c-BCN was synthesized at pressures >18 GPa and temperatures >2,200 K by a direct solid-state phase transition of graphite-like (BN)C. The reported Vickers and Knoop hardnesses were intermediate between diamond and c-BN, making the new phase the second hardest known material. Ternary B–C–N phases can also be made using shock-compression synthesis. It was further suggested to extend the B–C–N system to quaternary compounds with silicon included.\n\nUnlike carbon-based systems, metal borides can be easily synthesized in large quantities under ambient conditions, an important technological advantage. Most metal borides are hard; however, a few stand out among them for their particularly high hardnesses (for example, WB, RuB, OsB and ReB). \nThese metal borides are still metals and not semiconductors or insulators (as indicated by their high electronic density of states at the Fermi Level); however, the additional covalent B-B and M-B bonding (M = metal) lead to high hardness. Dense heavy metals, such as osmium, rhenium, tungsten etc., are particularly apt at forming hard borides because of their high electron densities, small atomic radii, high bulk moduli, and ability to bond strongly with boron.\n\nOsmium diboride (OsB) has a high bulk modulus of 395 GPa and therefore is considered as a candidate superhard material, but the maximum achieved Vickers hardness is 37 GPa, slightly below the 40 GPa limit of superhardness. A common way to synthesize OsB is by a solid-state metathesis reaction containing a 2:3 mixture of OsCl:MgB. After the MgCl product is washed away, X-ray diffraction indicates products of OsB, OsB and Os. Heating this product at 1,000 °C for three days produces pure OsB crystalline product. OsB has an orthorhombic structure (space group P\"mmn\") with two planes of osmium atoms separated by a non-planar layer of hexagonally coordinated boron atoms; the lattice parameters are \"a\" = 4.684 Å, \"b\" = 2.872 Å and \"c\" = 4.096 Å. The \"b\" direction of the crystal is the most compressible and the \"c\" direction is the least compressible. This can be explained by the orthorhombic structure. When looking at the boron and osmium atoms in the \"a\" and \"b\" directions, they are arranged in a way that is offset from one another. Therefore, when they are compressed they are not pushed right up against one another. Electrostatic repulsion is the force that maximizes the materials incompressibility and so in this case the electrostatic repulsion is not taken full advantage of. When compressed in the \"c\" direction, the osmium and boron atoms are almost directly in line with one another and the electrostatic repulsion is therefore high, causing direction \"c\" to be the least compressible. This model implies that if boron is more evenly distributed throughout the lattice then incompressibility could be higher. Electron backscatter diffraction coupled with hardness measurements reveals that in the (010) plane, the crystal is 54% harder in the <100> than <001> direction. This is seen by looking at how long the indentation is along a certain direction (related to the indentations made with a Vickers hardness test). Along with the alignment of the atoms, this is also due to the short covalent B-B (1.80 Å) bonds in the <100> direction, which are absent in the <001> direction (B-B = 4.10 Å).\n\nRhenium was targeted as a candidate for superhard metal borides because of its desirable physical and chemical characteristics. It has a high electron density, a small atomic radius and a high bulk modulus. When combined with boron, it makes a crystal with highly covalent bonding allowing it to be incompressible and potentially very hard. A wide array of rhenium borides have been investigated including ReB, ReB, ReB, ReB, ReB, ReB, ReB, ReB and ReB. Each of these materials has their own set of properties and characteristics. Some show promise as superconductors and some have unique elastic and electronic properties, but the most relevant to superhard materials is ReB.\n\nRhenium diboride (ReB) is a refractory compound which was first synthesized in the 1960s, using arc melting, zone melting, or optical floating zone furnaces. An example synthesis of this material is the flux method, which is conducted by placing rhenium metal and amorphous boron in an alumina crucible with excess aluminium. This can be run with a ratio of 1:2:50 for Re:B:Al, with the excess aluminum as a growth medium. The crucible is placed in an alumina tube, inserted into a resistively heated furnace with flowing argon gas and sintered at 1,400 °C for several hours. After cooling, the aluminium is dissolved in NaOH. Each ReB synthesis route has its own drawbacks, and this one gives small inclusions of aluminum incorporated into the crystal lattice.\n\nRhenium diboride has a very high melting point approaching 2,400 °C and a highly anisotropic, layered crystal structure. Its symmetry is either hexagonal (space group P6\"mc\") or orthorhombic (C\"mcm\") depending on the phase. There, close-packed Re layers alternate with puckered triangular boron layers along the (001) plane. This can be seen above on the example of osmium diboride. The density of states for ReB has one of the lowest values among the metal borides, indicating strong covalent bonding and high hardness.\n\nOwing to the anisotropic nature of this material, the hardness depends on the crystal orientation. The (002) plane contains the most covalent character and exhibits a maximum Vickers hardness value of 40.5 GPa, while the perpendicular planes were 6% lower at 38.1 GPa. These values decrease with increased load, settling at around 28 GPa each. The nanoindentation values were found to be 36.4 GPa and 34.0 GPa for the (002) and perpendicular planes respectively. The hardness values depend on the material purity and composition – the more boron the harder the boride – and the above values are for a Re:B ratio of approximately 1.00:1.85. Rhenium diboride also has a reported bulk modulus of 383 GPa and a shear modulus of 273 GPa. The hardness of rhenium diboride, and most other materials also depends on the load during the test. The above values of about 40 GPa were all measured with an effective load of 0.5–1 N. At such low load, the hardness values are also overestimated for other materials, for example it exceeds 100 GPa for c-BN. Other researchers, while having reproduced the high ReB hardness at low load, reported much lower values of 19–17 GPa at a more conventional load of 3–49 N, that makes ReB a hard, but not a superhard material.\n\nRhenium diboride exhibits metallic conductivity which increases as temperature decreases and can be explained by a nonzero density of states due to the d and p overlap of rhenium and boron respectively. At this point, it is the only superhard material with metallic behavior. The material also exhibits relatively high thermal stability. Depending on the heating method, it will maintain its mass up to temperatures of 600–800 °C, with any drop being due to loss of absorbed water. A small loss of mass can then be seen at temperatures approaching 1,000 °C. It performs better when a slower heat ramp is utilized. Part of this small drop at around 1,000 °C was explained by the formation of a dull BO coating on the surface as boron is leached out of the solid, which serves as a protective coating, thereby reducing additional boron loss. This can be easily dissolved by methanol to restore the material to it native shiny state.\n\nAluminum magnesium boride or BAM is a chemical compound of aluminium, magnesium and boron. Whereas its nominal formula is AlMgB, the chemical composition is closer to AlMgB. It is a ceramic alloy that is highly resistive to wear and has a low coefficient of sliding friction.\n\nOther hard boron-rich compounds include BC and BO. Amorphous a-BC has a hardness of about 50 GPa, which is in the range of superhardness. It can be looked at as consisting of boron icosahedra-like crystals embedded in an amorphous medium. However, when studying the crystalline form of BC, the hardness is only about 30 GPa. This crystalline form has the same stoichiometry as BC, which consists of boron icosahedra connected by boron and carbon atoms. Boron suboxide (BO) has a hardness of about 35 GPa. Its structure contains eight B icosahedra units, which are sitting at the vertices of a rhombohedral unit cell. There are two oxygen atoms located along the (111) rhombohedral direction.\n\nNanosuperhard materials fall into the extrinsic category of superhard materials. Because molecular defects affect the superhard properties of bulk materials it is obvious that the microstructure of superhard materials give the materials their unique properties. Focus on synthesizing nano superhard materials is around minimizing microcracks occurring within the structure through grain boundary hardening. The elimination of microcracks can strengthen the material by 3 to 7 times its original strength. Grain boundary strengthening is described by the Hall-Petch equation\n\nHere σ is the critical fracture stress, d the crystallite size and σ and k are constants.\n\nIf a material is brittle its strength depends mainly on the resistance to forming microcracks. The critical stress which causes the growth of a microcrack of size a is given by a general formula\n\nHere E is the Young's modulus, k is a constant dependent on the nature and shape of the microcrack and the stress applied and γ the surface cohesive energy.\n\nThe average hardness of a material decreases with d (crystallite size) decreasing below 10 nm. There have been many mechanisms proposed for grain boundary sliding and hence material softening, but the details are still not understood. Besides grain boundary strengthening, much attention has been put into building microheterostructures, or nanostructures of two materials with very large differences in elastic moduli. Heterostructures were first proposed in 1970 and contained such highly ordered thin layers that they could not theoretically be separated by mechanical means. These highly ordered heterostructures were believed to be stronger than simple mixtures. This theory was confirmed with Al/Cu and Al/Ag structures. After the formation of Al/Cu and Al/Ag, the research was extended to multilayer systems including Cu/Ni, TiN/VN, W/WN, Hf/HfN and more. In all cases, decreasing the lattice period increased the hardness. One common form of a nanostructured material is aggregated diamond nanorods, which is harder than bulk diamond and is currently the hardest (~150 GPa) material known.\n\n"}
{"id": "53004017", "url": "https://en.wikipedia.org/wiki?curid=53004017", "title": "Sweet chestnut of Mount Tota", "text": "Sweet chestnut of Mount Tota\n\nSweet chestnut of Mount Tota () is an old chestnut tree in Isparta Province, western Turkey. It is a registered natural monument of the country.\n\nThe chestnut tree is located on Mount Tota at Kasımlar village in Sütçüler district of Isparta Province. It is a sweet chestnut (\"Castanea sativa\"). The tree is high, has a circumference of at diameter. Its age is dated to be about 190 years old.\n\nThe tree was registered a natural monument on September 6, 2002. The protected area of the plant covers .\n"}
{"id": "27185226", "url": "https://en.wikipedia.org/wiki?curid=27185226", "title": "Tarball (oil)", "text": "Tarball (oil)\n\nA tarball is a blob of petroleum which has been weathered after floating in the ocean. Tarballs are an aquatic pollutant in most environments, although they can occur naturally and as such are not always associated with oil spills.\n\nTarball concentration and features have been used to assess the extent of oil spills and their composition can also be used to identify their sources of origin. They are slowly decomposed by microorganisms such as \"Chromobacterium violaceum\", \"Cladosporium resinae\", \"Bacillus submarinus\", \"Micrococcus varians\", \"Pseudomonas aeruginosa\", \"Candida marina\", and \"Saccharomyces estuari\".\n\nTarballs may be dispersed over long distances by deep sea currents. The density of tarballs depends on the solids picked up in the weathering process. They can range in density with some being more dense than seawater, which, at 1.025 g/ml, is more dense than the density of fresh water. When the tarballs are less dense than seawater, they can travel over great distances.\n\nThey can also be contained like oil and picked up using a variety of methods. Containment booms can be used to isolate tarballs similar to methods used to isolate oil. \n\n"}
{"id": "18676889", "url": "https://en.wikipedia.org/wiki?curid=18676889", "title": "Tropical rainforest climate", "text": "Tropical rainforest climate\n\nA tropical rainforest climate is a tropical climate usually found within 10 to 15 degrees latitude of the equator, and has at least 60 mm (2.4 inches) of rainfall every month of the year. Regions with this climate are typically designated \"Af\" by the Köppen climate classification. A tropical rainforest climate is typically hot and wet. \n\nTropical rainforests have a type of tropical climate in which there is no dry season—all months have an average precipitation value of at least . In rainforest climates the dry season is very short, and rainfall is normally heavy throughout the year. One day in an equatorial climate can be very similar to the next, while the change in temperature between day and night may be larger than the average change in temperature during the year. \n\n A tropical rainforest climate is usually found at latitudes within 15 degrees North and South of the equator, which are dominated by the Intertropical Convergence Zone. The climate is most commonly found in South America, Central Africa, Southeast Asia and Oceania. These rainforests are uniformly and monotonously wet throughout the year. Locations in South Pacific, areas along the coast of South and Central America, from Ecuador to Belize, parts of Central Africa, and much of Indonesia have this type of climate.\n\n"}
{"id": "2173107", "url": "https://en.wikipedia.org/wiki?curid=2173107", "title": "Viridios", "text": "Viridios\n\nViridios, or Viridius, is a god of ancient Roman Britain. \n\nInscribed stones dedicated to Viridios have been recovered in Ancaster, in Lincolnshire, England. So far, Ancaster is the only place where inscriptions to this god have been found. \n\nThe first inscribed stone was discovered in 1961 in a grave in Ancaster. R.P. Wright reports the find thus: \n\nThe Latin inscription may be translated in English as: \"For the god Viridius, Trenico made this arch, donated from his own funds\".\n\nA second inscribed stone was discovered in 2001 by British Channel 4 Television's archaeological programme \"Time Team\". The stone was discovered as part of a late Roman or early Sub-Roman \"cist\" burial, being used as a slab in the grave. Located near the grave where the first inscribed stone was found, the inscription on this second stone reads:\n\nDEO VRID\n\nSANCT\n\nwhich has been interpreted as: \"To the holy god Viridios\".\n\nThe Ancaster inscriptions are in Latin, suggesting that the name of the god is also in Latin. If so, the name is used in the dative form, meaning \"to (the deity)\". The nominative form, and therefore the name of the god, would be Viridius or Viridios. Because of the Latin word viridis (\"green, fresh, vigorous\"), the simplest etymology proposed is that the god's name derives from this root word and refers to a local, tribal Roman god possessing these characteristics. The Green Man is a well-known pagan mythic personage whose human face sprouting green leaves or vines is found in some medieval churches.\n\nSignificant speculation exists as to the possible Celtic origins of the deity. Wright specifically addresses such speculations, but notes that such a connection cannot be affirmatively made as there are no firm historico-linguistic connections in evidence between the modern conception of Viridios as a Celtic deity and the god Viridios referenced on the inscriptions: \n\nAn additional limestone figurative carving, probably used as an altarpiece, was also found near Ancaster and dated to the late Iron Age. The carving depicts a naked man holding an axe, standing beneath an archway. While no inscriptions appear on the stone, the location of the find near Ancaster, and the features displayed on the god that may depict suggested meanings of the name Viridios (\"mighty, virile, verdant, fertile\"), have led some to speculate that the altarpiece is a depiction of the god Viridios.\n\n\n"}
{"id": "24713528", "url": "https://en.wikipedia.org/wiki?curid=24713528", "title": "West of Shetland pipeline", "text": "West of Shetland pipeline\n\nThe West of Shetland Pipeline (WOSP) is a pipeline system which transports natural gas from three offshore fields in the West of Shetland area to Sullom Voe Terminal on the Shetland Isles of Scotland.\n\nThe West of Shetland area is a region in the northern Atlantic Ocean, where there are currently three developed offshore fields; Schiehallion, Foinaven and Clair. A number of further hydrocarbon discoveries have been made in the area, but these are not currently considered to be economic due to the high costs of development. The Laggan-Tormore and Rosebank-Lochnagar discoveries are in the process of being further appraised for development, but the WOSP is considered to be unable to carry the additional gas production from these fields.\n\nWOSP is owned collectively by the owners of the Schiehallion, Loyal, Foinaven and East Foinaven fields; each owner company holds divided capacity rights in accordance with its ownership interest. It is operated by BP.\n\nThe system consists of a trunkline which begins near Schiehallion and has length of . It has a maximum capacity of of gas per year.\n\nNatural gas from the Clair field is delivered into the pipeline at the Clair \"tee\" midway along the pipeline from Sullom Voe.\n\nNatural gas from WOSP is further transported by East of Shetland Pipeline (EOSP), NLGP and FLAGS/SEGAL systems for the delivery into the UK National Transmission System at St Fergus in the northeast of Scotland, and for the delivery of separated natural gas liquids into either the Shell operated SEGAL system or the onshore section of the BP-operated Forties pipeline system. The gas delivered from the pipeline is also part of the Magnus oilfield enhanced oil recovery scheme where gas and natural gas liquids are piped to the Magnus platform and injected into the reservoir to increase the recoverable reserves from the development.\n"}
{"id": "2839446", "url": "https://en.wikipedia.org/wiki?curid=2839446", "title": "Yushui (solar term)", "text": "Yushui (solar term)\n\nThe traditional East Asian calendars divide a year into 24 solar terms. Yǔshuǐ, \"Usui\", \"Usu\", or \"Vũ thủy\" is the 2nd solar term. It begins when the Sun reaches the celestial longitude of 330° and ends when it reaches the longitude of 345°. It more often refers in particular to the day when the Sun is exactly at the celestial longitude of 330°. In Gregorian calendar, it usually begins around 18 February (19 February East Asia time) and ends around 5 March.\n\nEach solar term can be divided into 3 pentads (候). They are first pentad (初候), second pentad (次候) and last pentad (末候). Pentads in Yushui including:\n\n\n"}
