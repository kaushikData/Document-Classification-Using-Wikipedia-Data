{"id": "78731", "url": "https://en.wikipedia.org/wiki?curid=78731", "title": "Adrasteia", "text": "Adrasteia\n\nIn Greek mythology, Adrasteia (; Greek: Ἀδράστεια (Ionic Greek: Ἀδρήστεια), \"inescapable\"; also spelled Adrastia, Adrastea, Adrestea, Adastreia or Adrasta) was a nymph who was charged by Rhea with nurturing the infant Zeus in secret in the Dictaean cave, to protect him from his father Cronus.\n\nShe is known to have been worshipped in hellenised Phrygia (north-western Turkey), probably derived from a local Anatolian mountain deity. She is known from inscriptions in Greece from around 400 BCE as a deity who defends the righteous.\n\nAdrastea may be interchangeable with Cybele, a goddess also associated with childbirth. The Greeks cultivated a patronic system of gods who served specific human needs, conditions or desires to whom one would give praise or tribute for success in certain arenas such as childbirth.\n\nBoth the early 3rd-century BC poet Callimachus, and the mid 3rd-century BC poet Apollonius of Rhodes, name\nAdrasteia (here possibly another name for Nemesis) as a nurse of the infant Zeus. According to Callimachus, Adrasteia, along with the ash-tree nymphs, the Meliae, laid Zeus \"to rest in a cradle of gold\", and fed him with honeycomb, and the milk of the goat Amaltheia. Apollonius of Rhodes, describes a wondrous toy ball which Adrasteia gave the child Zeus, when she was his nurse in the \"Idean cave\".\nHyginus says that Adrasteia, along with her sisters Ida and Amalthea, were daughters of Oceanus, or that according to \"others\" they were Zeus's nurses, \"the ones that are called Dodonian Nymphys (others call them the Naiads)\". According to Apollodorus, Adrasteia and Ida were daughters of Melisseus, who nursed Zeus, feeding him on the milk of Amalthea.\n\nThe tragedy \"Rhesus\" (possibly by Euripides) makes Adrasteia the daughter of Zeus, rather than his nurse.\n\nAt Cirrha, the port that served Delphi, Pausanias noted \"\"a temple of Apollo, Artemis and Leto, with very large images of Attic workmanship. Adrasteia has been set up by the Cirrhaeans in the same place, but she is not so large as the other images\".\"\n\n\"Adrasteia\" was also an epithet of Nemesis, a primordial goddess of the archaic period. (Her name appears as \"a-da-ra-te-ja\" in Mycenaean Pylos.) The epithet is derived by some writers from Adrastus, who is said to have built the first sanctuary of Nemesis on the river Asopus, and by others from the Greek verb (\"didraskein\"), according to which it would signify the goddess whom none can escape.\n\n\"Adrasteia\" was also an epithet applied to Rhea herself, to Cybele, and to Ananke, as her daughter. As with Adrasteia, these four were especially associated with the dispensation of rewards and punishments.\n\nIn \"The Dialogue of the Sea-Gods\", Poseidon remarks to a Nereid that Adrasteia is a great deal stronger than Nephele, who was unable to prevent the fall of her daughter Helle from the ram of the Golden Fleece.\n\n\n\n"}
{"id": "47299712", "url": "https://en.wikipedia.org/wiki?curid=47299712", "title": "Börde", "text": "Börde\n\nA börde (plural: \"börden\") is a region of highly fertile lowland in North Germany, a \"fertile plain\". These landscapes often cover great areas and are particularly important for arable farming on account of their rich soils.\n\nThese regions coincide closely with areas of flat, fertile loess soil and few trees. \"Börden\" are found in Germany, especially in the North German Old Drift region on the northern edges of Central Uplands. The resulting black earth soils are some of the best soils in Germany.\n\nThese landscapes are restricted to, or concentrated on, those regions where the Eastphalian and Westphalian dialects are spoken. There are two opposing explanations for the name. According to one, the term is connected with the Old High German word \"giburida\" (\"judicial district\") or the plattdeutsch word \"bören\" (\"to bear\"). The \"börde\" in this context is seen as a district in which the inhabitants had to \"bear\" public charges, i.e. it was effectively a taxation district. This may thus be a dialect variation of \"\" (\"burden\", \"heavy load\").\n\nBecause of the fertility of the \"börden\" another connexion has been made to the word \"bören\", whose primary meaning was \"yield\" or \"output\", thus a \"börde\" would be a \"land that produced a rich yield\". Semasiological comparisons are made in this regard to the word \"tragen\" (\"to carry\", \"to bear\"): the related words \"Ertrag\" (\"yield\") and \"Getreide\" (\"grain\") stem from the Middle High German \"getregede\" (\"that which is borne\"). In the heraldic motto of the House of Alsleben is the term \"Vorborde\", which is translated today as \"for the ancestral land\".\n\n\"See also:\" Beauce and the terms Gau and Gäu as part of the name of various landscapes or administrative districts.\n\n\"Börden\" extend from the North German geest to the perimeter of the German Central Uplands and consist of loess that has been predominantly deposited by east winds. In some places the loess lies over boulder clay (on the rivers Weser, Leine and Oker), in others over Mesozoic and Tertiary sedimentary rocks (in the Hellwegbörden and the foreland of the Harz Mountains). The loess layers are up to 10 metres thick and tend to attenuate differences in relief. In the (sub-)oceanic climatic region the loess has been largely decalcified and loamified.\nThe northern edge of the loess region is not only a soil and vegetation boundary, but also a settlement zone - cities like Minden, Hanover or Magdeburg lie on the loess boundary.\n\n\n\n\n\n\n"}
{"id": "49172235", "url": "https://en.wikipedia.org/wiki?curid=49172235", "title": "Camp Thomas A. Scott", "text": "Camp Thomas A. Scott\n\nCamp Thomas A. Scott, located in Fort Wayne, Indiana, was a Railway Operating Battalion training center for the Pennsylvania Railroad from 1942 to 1944 and a prisoner of war camp during World War II. It was named for Thomas A. Scott, who served as the fourth president of the Pennsylvania Railroad from 1874-1880. As the United States Assistant Secretary of War in 1861, Scott was instrumental in using railroads for military purposes during the American Civil War. \n\nCamp Scott was built in August 1942.\n\nAs a prisoner of war camp, it operated under the command of Camp Perry, Ohio.\nWith the war over and the prisoners gone, Camp Scott officially closed on November 16, 1945.\n\nCamp Scott sat dormant until January 1946, when the Fort Wayne Housing Authority began the process of converting camp buildings into much-needed housing for returning American veterans and their families. In the years following, more housing was built in Fort Wayne, and the families living at Camp Scott gradually relocated to other homes. Camp Scott served as a temporary housing facility until August 1949. Over the next decades, the buildings were torn down, with the last building being demolished in 1977.\n\nThe City of Fort Wayne converted the land on which Camp Scott stood into a constructed wetland.\n\n"}
{"id": "53273587", "url": "https://en.wikipedia.org/wiki?curid=53273587", "title": "Carbon rift", "text": "Carbon rift\n\nCarbon rift is a theory attributing the input and output of carbon into the environment to human capitalistic systems. This is a derivative of Karl Marx's concept of metabolic rift. In practical terms, increased commodity production demands that greater levels of carbon dioxide (or CO) be emitted into the biosphere via fossil fuel consumption. Carbon rift theory states that this ultimately disrupts the natural carbon cycle and that this \"rift\" has adverse effects on nearly every aspect of life. Many of the specifics regarding how this metabolic carbon rift interacts with capitalism are proposed by Brett Clark and Richard York in a 2005 article titled \"Carbon Metabolism: Global capitalism, climate change, and the biospheric rift\" in the journal \"Theory and Society\". Researchers such as Jean P. Sapinski of the University of Oregon claim that, despite increased interest in closing the carbon rift, it is projected that as long as capitalism continues, there is little hope of reducing the rift.\n\nBoth deforestation and the emission of greenhouse gases have been linked to increased atmospheric CO levels. Carbon rift theory states that these are the result of human production through capitalistic systems. There are proposed solutions to climate change such as geoengineering proposed in the December 2015 Paris Agreement. However, some argue that the capitalist mode of production is at fault for the emission of greenhouse gas and that solutions must be found to this issue before climate change itself can be addressed.\n\nCarbon rift theory, while rarely criticized directly, often indirectly receives criticism regarding the underlying causes of climate change and attributing the stated effects to alternative explanations for climate change, instead of as a result of human activity. Such explanations include the Chaotic Solar System Theory and that increased water vapor is responsible for climate change.\n\nCarbon rift is a result of CO gas being released into the environment by human sources, with the theory focusing specifically on capitalistic ones. In 2014, fossil fuel consumption resulted in nearly 36 billion metric tons of CO finding its way into natural sinks such as the atmosphere, land, and oceans. This transfer of carbon from the burning of fossil fuels into the biosphere is the primary human-driven cause of greenhouse gas emissions and is closely related to the unchecked behavior of capitalism.\n\nAnother contributing factor to carbon rift is the continual deforestation of the Earth's forests. In doing so, humankind is not only releasing carbon into the biosphere but removing one of the primary ways that carbon is naturally re-absorbed into the carbon cycle. Deforestation can both be tied to having large effects on greenhouse gas emissions (specifically, carbon dioxide) and to capitalism's continual disregard for its use of the truly limited resource represented by the forests. Thus, we have a tie between capitalism, deforestation, and carbon. This is the metabolic pathway defined by carbon rift.\n\nAs the carbon rift continues to grow, the ecosystems of the biosphere continue to experience detrimental effects. One of the readily observable examples is the acidification of the world's oceans. This occurs when carbon dioxide is absorbed by seawater, lowering its pH. Since the start of the Industrial Revolution, which Marx explicitly ties to capitalism, oceans have experienced a 30% increase in acidity. This acidification and resulting calcification of biological organisms are in part responsible for a decline of fishing as an industry and viable food source. The enlarging carbon rift could result in poorer conditions for human society over time.\n\nCarbon rift plays into a larger discussion of climate change caused by humans—a topic with stark political division. In the United States, the right end of the political spectrum tends to either deny/downplay climate change or attribute it to non-human causes, while people on the left stress the dangerous effects it has on the planet and society. Even scientists are divided on the issue—papers have been written claiming humans are and are not the cause of climate change. While the theory of carbon rift is not particularly well-known, these political divisions transfer to opinions on carbon rift because the theory operates under the belief that reliance on capitalist modes of production is the cause of increased carbon dioxide emissions.\n\nThe small amount of political and economic analysis that has been done on carbon rift discusses the theory’s relation to geoengineering. While geoengineering is still in the development stage as both a topic and solution to climate change, the December 2015 Paris Agreement highlighted “negative emissions technologies”. These technologies aim to either “remove carbon dioxide from the atmosphere” or “reduce the amount of solar radiation that hits the earth’s surface.\n\nSome scientists and advocacy groups warn that geoengineering will have dangerous, irreversible effects on human society. Furthermore, there is no way to fully test the accuracy of these technologies before launch, making the risk even greater. The 2013 film Snowpiercer offers a grim, politicized portrayal of the possible negative effects of climate engineering. However, other researchers support the develop of such technologies, as they believe their necessity is inevitable. These researchers claim that the climate of capitalistic growth will not falter and greenhouse gas emissions will continue to rise.\n\nCritics of geoengineering emphasize that development of such technologies does not address the cause of carbon rift. Jean Sapinski from the University of Oregon defines the root cause as the “capitalist mode of production and the growth imperative it entails”. The extent of carbon rift relates directly to the dominant economic system and the political institutions that reinforce said system. Essentially, those who find fault in the capitalist system are more likely to believe carbon rift cannot be treated effectively without tackling capitalism first.\n\nCarbon rift theory, as a subtopic of both Marxist metabolic theory and climate change, has inherited dissenting viewpoints from both its parent topics. Detractors claim exactly the opposite of carbon rift theory: human production does not have an appreciable effect on the carbon emissions in the biosphere. Since carbon rift theory has not yet made it into the mainstream lexicon, it is not often attacked directly by its detractors, but its concepts are. A notable individual that believes that climate change and human carbon emissions are unrelated is Patrick Moore, of Greenpeace fame. Other theories that explain the growing carbon rift (but exclude capitalism as a contributing factor) are the Chaotic Solar System theory, the claim that carbon is wrongly blamed for the greenhouse effects of water vapor and that the sun is causing global warming. These together are referred to as Non-Consensus views, and lack reliable scientific evidence.\n\n\n"}
{"id": "4276335", "url": "https://en.wikipedia.org/wiki?curid=4276335", "title": "Christopher Camuto", "text": "Christopher Camuto\n\nChristopher Camuto is an American nature writer, scholar and poet. He is the author of three books focused on the southern Appalachians--\"A Fly Fisherman's Blue Ridge \"(Henry Holt, 1990), \"Another Country: Journeying Toward the Cherokee Mountains\" (Henry Holt, 1997), \"Hunting from Home: A Year Afield in the Blue Ridge\" (W. W. Norton, 2003) and of \"Time and Tide in Acadia: Seasons on Mount Desert Island\" (W. W. Norton, 2009). He worked under the editorship of William Strachan at Henry Holt and of Amy Cherry at Norton.\n\nHis second book, \"Another Country\", is perhaps his most complex, interweaving historical accounts of the southern Appalachians, reflections on the Cherokee language and its relationship to the landscape, and an account of efforts to reintroduce the endangered red wolf into Great Smoky Mountains National Park.\n\nSince 1995 Camuto has been the book review columnist for \"Gray's Sporting Journal\", where he comments six times a year on sporting literature and art. Since 1998 he had written the quarterly \"Watersheds\" column, which he created, for Trout Unlimited's \"Trout\". He was the book review columnist for \"Audubon\" from 1999-2002 and has written for a wide range of periodicals devoted to nature and the environment, including \"American Rivers\", \"Audubon\", \"The Boston Globe\", \"Chesapeake Bay Journal\", \"Field & Stream\", \"Flyfishing\", \"Fly Fisherman\", \"Gray's Sporting Journal\", \"Sewanee Review\", \"Sierra\", \"Sports Afield\", \"Trout\", \"Weber Studies in the Environment\", and \"Wilderness\".\n\nCamuto's work has been anthologized in a number of books devoted to distinguished nature writing, including \"The Gift of Trout\" (Lyons and Burford, 1996), \"The Height of Our Mountains: Nature Writing from Virginia's Blue Ridge Mountains and Shenandoah Valley\" (The Johns Hopkins University Press, 1998), \"In Praise of Wild Trout\" (The Lyons Press, 1998), \"The Woods Stretched for Miles: Contemporary Southern Nature Writing\" (The University of Georgia Press, 1999), \"Uncommon Wealth: Essays on Virginia's Wild Places\" (Falcon Press, 1999), \"The Greatest Fishing Stories Ever Told\" (The Lyons Press, 2000), \"Into the Backing\" (The Lyons Press, 2001), \"Elemental South: Earth, Air, Fire and Water\" (The University of Georgia Press, 2004), \"Bartram's Living Legacy\" (Mercer University Press, 2010), and \"Afield: American Writers on Bird Dogs\" (Skyhorse Press, 2010). He wrote the introduction for the West Virginia University Press reprint, published in 2011, of Julia Davis' 1945 \"The Shenandoah\", one of the titles in the celebrated Rivers of America series (1937-1974).\n\nIn the 1990s, Camuto was instrumental in publicizing the acidification of southern Appalachian headwater streams, most notably in an extended feature in the Winter 1991 \"Trout\": \"Dropping Acid in the Southern Appalachians: A Wild Trout Resource at Considerable Risk.\" During the 1980s and 1990s, he worked, as a writer and activist, on the controversies surrounding management of public forest lands in the southeast, especially the protection, preservation and restoration of coldwater streams and the preservation of roadless areas on national forest land. He is associated with the Southern Nature Project (www.southernnature.org), has done work on behalf of American Rivers, the Izaak Walton League, The Nature Conservancy, the Sierra Club, the Southern Appalachian Forest Coalition, Trout Unlimited, The Wilderness Society, and other environmental organizations. In the 1970s, he worked on behalf of migrant and seasonal farmworkers on the eastern shore of Virginia.\n\nA native New Yorker and a long-time resident of Virginia (the Eastern Shore, Albemarle and Rockbridge counties), Camuto currently lives at Wolftree Farm in Union County, Pennsylvania, a hard-used but biologically diverse 78-acre woodland he acquired in 2005 and which has become an eco-restoration project in progress. Located near the foot of Buffalo Mountain, between state-owned forest land and private farmland, the property is an instructive example of the condition of private woodlands in central Pennsylvania. Despite its name, Wolftree Farm is for the most part re-grown woodland rather than tillable land, its succession toward mature woods having been defeated repeatedly by select (high-grade) logging in past decades. Working on his own, Camuto is striving to mitigate the effects of this logging, inroads of invasive species, and ill-conceived white and red pine plantings from the 1960s. He is trying to encourage a naturally diverse mix of native hardwoods—walnut, oak, hickory, black cherry—along with a healthy understory of native shrubs. Despite its problems, the property is diverse in birdlife (over a hundred species) and wildlife (including black bear, wild turkey, ruffed grouse). Camuto hopes to establish the property as a nature preserve. He has established a modest homestead on land he cleared, including a cedar log house he partly built himself near the abandoned home site of one James Glover (1824-1898), grandson of John Glover, Sr., an eighteenth-century Irish immigrant to America and an early (1772) white settler in what became Hartley Township, Pennsylvania. This homestead and woodland are now the center of Camuto's new work in nonfiction, fiction and poetry, including \"Works and Days: Notes on a Woodland Farm\" (nonfiction), \"A Hunter's Book of Hours\" (poetry), \"Sympathy for the Settler\" (poetry), \"Amygdala: Stories\" (fiction) and \"A Dream of Darwin\" (prose/poetry).\n\nIn recent years, Camuto has also been exploring his ancestral roots in Italy and the Mediterranean world. He is at work on a book of essays about his grandparents' connections to the Italian landscape—in Sicily (Bronte), in Basilicata (Melfi, Potenza, San Costantino Albanese) and Como—and a volume of poetry, \"Learning to Travel\", about travel related to Magna Graecia, pre-Socratic philosophy, and classical Western literature, all of which are strong influences on his life, writing and teaching. He maintains close ties to the relations of his paternal grandmother, Delores Scutari, who live in Potenza, Senise and San Costantino Albanese, one of Basilicata's Arberesche mountain villages. Through his maternal grandmother, Mary Bocchetta Zanini, he is related to Vittore Bocchetta—Italian sculptor, painter, scholar and anti-Fascist resistance fighter in World War II.\n\n"}
{"id": "2423894", "url": "https://en.wikipedia.org/wiki?curid=2423894", "title": "Coal pollution mitigation", "text": "Coal pollution mitigation\n\nCoal pollution mitigation, often referred to by the term clean coal, is a series of systems and technologies that seek to mitigate the pollution and other environmental effects normally associated with the burning (though not the mining or processing) of coal, which is widely regarded as the dirtiest of the common fuels for industrial processes and power generation.\n\nApproaches attempt to mitigate emissions of carbon dioxide (CO) and other greenhouse gases, and radioactive materials, that arise from the use of coal, mainly for electrical power generation, using various technologies. Historical efforts to reduce coal pollution focused on flue-gas desulfurization starting in the 1850s and clean burn technologies. These efforts have been very successful in countries with strong environmental regulation, such as the US, where emissions of acid-rain causing compounds and particulates have been reduced by up to 90% since 1995. More recent developments include carbon capture and storage, which pumps and stores CO emissions underground, and integrated gasification combined cycle (IGCC) involve coal gasification, which provides a basis for increased efficiency and lower cost in capturing CO emissions.\n\nThere are seven technologies deployed or proposed by the National Mining Association for deployment in the United States:\n\nOf the 22 clean coal demonstration projects funded by the U.S. Department of Energy since 2003, none are in operation as of February 2017, having been abandoned or delayed due to capital budget overruns or discontinued because of excessive operating expenses.\n\nSince the 1970s, various policy and regulatory measures have driven coal pollution mitigation. In the US, the Clean Air Act was the primary driving force in reducing particulate emissions and acid rain from coal combustion. As regulations have increased the demand for coal pollution mitigation technologies, costs have fallen and performance has improved.\n\nThe widespread deployment of pollution-control equipment to reduce sulphur dioxide, NO and dust emissions is just one example that brought cleaner air to many countries. The desire to tackle rising CO emissions to address climate change later introduced Carbon Capture and Storage (CCS).\n\nWithin the United States, Carbon Capture and Storage technologies, also sometimes referred to as carbon capture and sequestration, are mainly being developed in response to regulations by the Environmental Protection Agency—most notably the Clean Air Act—and in anticipation of legislation that seeks to mitigate climate change.\n\nLoan guarantees and tax incentives have a long history of use in Australia, EU countries and the US to encourage the introduction of coal pollution mitigation and other technologies to reduce environmental impact.\n\nCombustion of coal--which is mostly carbon--produces carbon dioxide as a product of combustion. According to the United Nations Intergovernmental Panel on Climate Change, the burning of coal, a fossil fuel, is a significant contributor to global warming. (See the UN IPCC Fourth Assessment Report). For 1 ton of coal burned, 2.86 tons of carbon dioxide is created.\n\nCarbon sequestration technology has yet to be tested on a large scale and may not be safe or successful. Sequestered CO may eventually leak up through the ground, may lead to unexpected geological instability or may cause contamination of aquifers used for drinking water supplies.\n\nAs 25.5% of the world's electrical generation in 2004 was from coal-fired generation (see world energy consumption), reaching the carbon dioxide reduction targets of the Kyoto Protocol will require modifications to how coal is used.\n\nBy-products of coal combustion are compounds which are released into the atmosphere as a result of burning coal. Coal includes contaminants such as sulfur compounds and non-combustible minerals. When coal is burned, the minerals become ash (i.e particulate matter or PM) and the sulfur forms sulfur dioxide (SO). Since air is mostly nitrogen, combustion of coal often leads to production of nitrogen oxides. Sulfur dioxide and nitrogen oxides are primary causes of acid rain. For many years--before greenhouse gasses were widely understood to be a threat-- it was thought that these by-products were the only drawback to using coal. These by-products are still a problem, but they have been greatly diminished in most advanced countries due to clean air regulations.\nIt is possible to remove most of the sulfur dioxide (SO), nitrogen oxides (NO), and particulate matter (PM) emissions from the coal-burning process. For example, various techniques are used in a coal preparation plant to reduce the amount of non-combustible matter (i.e. ash) in the coal prior to burning. During combustion, fluidized bed combustion is used to reduce sulfur dioxide emissions. After burning, particulate matter (i.e. ash and dust) can be reduced using an electrostatic precipitator and sulfur dioxide emissions can be further reduced with Flue-gas desulfurization. Trace amounts of radionuclides are more difficult to remove.\n\nCoal-fired power plants are the largest aggregate source of the toxic heavy metal mercury: 50 tons per year come from coal power plants out of 150 tons emitted nationally in the USA and 5000 tons globally. However, according to the United States Geological Survey, the trace amounts of mercury in coal by-products do not pose a threat to public health. A study in 2013 found that Mercury found in the fish in the Pacific Ocean could possibly be linked to coal-fired plants in Asia.\n\nWhether carbon capture and storage technology is adopted worldwide will “…depend less on science than on economics. Cleaning coal is very expensive.” \n\nConversion of a conventional coal-fired power plant is done by injecting the into ammonium carbonate after which it is then transported and deposited underground (preferably in soil beneath the sea). This injection process however is by far the most expensive. Besides the cost of the equipment and the ammonium carbonate, the coal-fired power plant also needs to use 30% of its generated heat to do the injection (parasitic load). A test-setup has been done in the American Electric Power Mountaineer coal-burning power plant.\n\nOne solution to reduce this thermal loss/parasitic load is to burn the pulverised load with pure oxygen instead of air.\n\nNewly built coal-fired power plants can be made to immediately use gasification of the coal prior to combustion. This makes it much easier to separate off the from the exhaust fumes, making the process cheaper. This gasification process is done in new coal-burning power plants such as the coal-burning power plant at Tianjin, called \"GreenGen\".\n\nThe projected nationwide costs for the implementing of carbon capture and storage (CCS) in coal-fired power plants in the USA (presumably using a conventional tactic, see above) can be found in the Wall Street Journal article. Credit Suisse Group says $15 billion needs to be invested in CCS over the next 10 years for it to play an important role in climate change. The International Energy Agency says $20 billion is needed. The Pew Center on Global Climate Change says the number is as high as $30 billion. Those figures dwarf the actual investments to date.\n\nIn the US, the Bush administration spent about $2.5 billion on a range of mitigation technologies — a large amount, but far less than the amounts previously suggested. CCS proponents say both the government and the private sector need to step up their investments.\n\nThe coal industry in the US has the potential to make billions of dollars if clean coal technologies are pursued. It is estimated that from 2000 to 2020 the industry could make up to $15 billion in reduced fuel costs, $25 billion in avoided environmental costs, and $32 billion from exporting the equipment and licensing for use in other countries.\n\nIn Australia, carbon capture and storage was often referred to by then Prime Minister Kevin Rudd as a possible way to reduce greenhouse gas emissions. (The previous Prime Minister John Howard had stated that nuclear power was a better alternative, as CCS technology may not prove to be economically feasible.)\n\nIn 2014 SaskPower a provincial-owned electric utility finished renovations on Boundary Dam’s boiler number 3 making it the worlds first post-combustion carbon capture storage facility. The renovation project ended up costing a little over $1.2 billion and can scrub out CO2 and other toxin from up to 90 percent of the flue gas that it emits.\n\nSince 2006, China keeps releasing more than any other country. Researchers in China are focusing on increasing efficiency of burning coal so they can get more power out of less coal. It is estimated that new high efficiency power plants could reduce CO2 emission by 7% because they won't have to burn as much coal to get the same amount of power.\n\nFollowing the catastrophic failure of the Fukushima I Nuclear Power Plant in Japan that resulted from the 2011 Tōhoku earthquake and tsunami, and the subsequent widespread public opposition against nuclear power, high energy, lower emission (HELE) coal power plants were increasingly favored by the Shinzō Abe-led government to recoup lost energy capacity from the partial shutdown of nuclear power plants in Japan and to replace aging coal and oil-fired power plants, while meeting 2030 emission targets of the Paris Agreement. 45 HELE power plants have been planned, purportedly to employ integrated gasification fuel cell cycle, a further development of integrated gasification combined cycle.\n\nJapan had adopted prior pilot projects on IGCC coal power plants in the early-1990s and late-2000s.\n\nIn the United States, \"clean coal\" was mentioned by former President George W. Bush on several occasions, including his 2007 State of the Union Address. Bush's position was that carbon capture and storage technologies should be encouraged as one means to reduce the country's dependence on foreign oil.\n\nDuring the US Presidential campaign for 2008, both candidates John McCain and Barack Obama expressed interest in the development of CCS technologies as part of an overall comprehensive energy plan. The development of pollution mitigation technologies could also create export business for the United States or any other country working on it.\n\nThe American Reinvestment and Recovery Act, signed in 2009 by President Obama, allocated $3.4 billion for advanced carbon capture and storage technologies, including demonstration projects.\n\nFormer Secretary of State Hillary Clinton has said that \"we should strive to have new electricity generation come from other sources, such as clean coal and renewables,” and former Energy Secretary Dr. Steven Chu has said that “It is absolutely worthwhile to invest in carbon capture and storage\", noting that even if the U.S. and Europe turned their backs on coal, developing nations like India and China would likely not.\n\nDuring the first 2012 United States presidential election debate, Mitt Romney expressed his support for clean coal, and claimed that current federal policies were hampering the coal industry.\n\nEnvironmentalists such as Dan Becker, director of the Sierra Club's Global Warming and Energy Program, believes that the term \"clean coal\" is misleading: \"There is no such thing as clean coal and there never will be. It's an oxymoron.\" The Sierra Club's Coal Campaign has launched a site refuting the clean coal statements and advertising of the coal industry.\n\nComplaints focus on the environmental impacts of coal extraction, high costs to sequester carbon, and uncertainty of how to manage end result pollutants and radionuclides. In reference to sequestration of carbon, concerns exist about whether geologic storage of CO in reservoirs, aquifers, \"etc.\", is indefinite/permanent.\n\nThe palaeontologist and influential environmental activist Tim Flannery made the assertion that the concept of clean coal might not be viable for all geographical locations.\n\nCritics also believe that the continuing construction of coal-powered plants (whether or not they use carbon sequestration techniques) encourages unsustainable mining practices for coal, which can strip away mountains, hillsides, and natural areas. They also point out that there can be a large amount of energy required and pollution emitted in transporting the coal to the power plants.\n\nThe Reality Coalition, a US non-profit climate organization composed of the Alliance for Climate Protection, the Sierra Club, the National Wildlife Federation, the Natural Resources Defense Council and the League of Conservation Voters, ran a series of television commercials in 2008 and 2009. The commercials were highly critical of attempts to mitigate coal's pollution, stating that without capturing CO emissions and storing it safely that it cannot be called \"clean coal\".\n\nGreenpeace is a major opponent of the concept, because they view emissions and wastes as not being avoided but instead transferred from one waste stream to another. According to Greenpeace USA's Executive Director Phil Radford speaking in 2012, \"even the industry figures it will take 10 or 20 years to arrive, and we need solutions sooner than that. We need to scale up renewable energy; “clean coal” is a distraction from that.\"\n\nThe term Clean Coal in modern society often refers to the carbon capture and storage process. The term has been used by advertisers, lobbyists, and politicians such as Donald Trump.\n\nThe industry term \"clean coal\" is increasingly used in reference to carbon capture and storage, an advanced theoretical process that would eliminate or significantly reduce carbon dioxide emissions from coal-based plants and permanently sequester them. More generally, the term has been found in modern usage to describe technologies designed to enhance both the efficiency and the environmental acceptability of coal extraction, preparation, and use.\n\nU.S. Senate Bill 911 in April, 1987, defined clean coal technology as follows:\n\n\"The term clean coal technology means any technology...deployed at a new or existing facility which will achieve significant reductions in air emissions of sulfur dioxide or oxides of nitrogen associated with the utilization of coal in the generation of electricity.\"\n\nBefore being adopted in this fashion, historically \"clean coal\" was used to refer to clean-burning coal with low levels of impurities, though this term faded after rates of domestic coal usage dropped. The term appeared in a speech to mine workers in 1918, in context indicating coal that was \"free of dirt and impurities.\" In the early 20th century, prior to World War II, clean coal (also called \"smokeless coal\") generally referred to anthracite and high-grade bituminous coal, used for cooking and home heating.\n\n"}
{"id": "440246", "url": "https://en.wikipedia.org/wiki?curid=440246", "title": "Corporate average fuel economy", "text": "Corporate average fuel economy\n\nThe Corporate Average Fuel Economy (CAFE) standards are regulations in the United States, first enacted by the United States Congress in 1975, after the 1973–74 Arab Oil Embargo, to improve the average fuel economy of cars and light trucks (trucks, vans and sport utility vehicles) produced for sale in the United States.\n\nThe Energy Policy and Conservation Act (EPCA), as amended by the 2007 Energy Independence and Security Act (EISA), requires that the U.S. Department of Transportation (DOT) establish standards separately for passenger automobiles (passenger cars) and nonpassenger automobiles (light trucks) at the maximum feasible levels in each model year, and requires that DOT enforce compliance with the standards. DOT has delegated the responsibilities to the National Highway Traffic Safety Administration (NHTSA). Through EPCA and EISA, U.S. law (49 U.S. Code § 32919) also preempts state or local laws: \"a State or a political subdivision of a State may not adopt or enforce a law or regulation related to fuel economy standards or average fuel economy standards.\"\n\nThe CAFE achieved by a given fleet of vehicles in a given model year is the production-weighted harmonic mean fuel economy, expressed in miles per USgallon (mpg), of a manufacturer's fleet of current model year passenger cars or light trucks with a gross vehicle weight rating (GVWR) of 8,500 pounds (3,856 kg) or less (but also including medium-duty passenger vehicles, such as large sport-utility vehicles and passenger vans, with GVWR up to 10,000 pounds), produced for sale in the United States. The CAFE standards in a given model year define the CAFE levels that manufacturers' fleets are required to meet in that model year, specific levels depending on the characteristics and mix of vehicles produced by each manufacturer. If the average fuel economy of a manufacturer's annual fleet of vehicle production falls below the applicable requirement, the manufacturer must either apply sufficient CAFE credits (see below) to cover the shortfall or pay a penalty, currently $5.50 per 0.1 mpg under the standard, multiplied by the manufacturer's total production for the U.S. domestic market. Congress established both of these provisions explicitly in EPCA, as amended in 2007 by EISA. In addition, a Gas Guzzler Tax is levied on individual passenger car models (but not trucks, vans, minivans, or SUVs) that get less than .\n\nStarting in 2011, the CAFE standards are newly expressed as mathematical functions depending on vehicle footprint, a measure of vehicle size determined by multiplying the vehicle’s wheelbase by its average track width. A complicated 2011 mathematical formula was replaced starting in 2012 with a simpler inverse-linear formula with cutoff values.\n\nCAFE footprint requirements are set up such that a vehicle with a larger footprint has a lower fuel economy requirement than a vehicle with a smaller footprint. For example, the fuel economy target for the 2012 Honda Fit with a footprint of is , equivalent to a published fuel economy of (see #Calculations of MPG overestimated for information regarding the difference), and a Ford F-150 with its footprint of has a fuel economy target of , i.e., published. Individual vehicles do not have to meet their fuel economy targets; CAFE compliance is enforced at the fleet level. CAFE 2016 target fuel economy of 34.0 MPG (44 sq. ft. footprint) compares to 2012 advanced vehicle performance of Prius hybrid on the compliance test cycles: 70.7 MPG, Plug-in Prius hybrid: 69.8 MPGe and LEAF electric vehicle: 141.7 MPGe. The compliance fuel economy of plug-in electric vehicles such as the Plug-in Prius or LEAF is complicated by accounting for the energy used in generating electricity. In 2012, the LEAF's compliance fuel consumption was considered to be 0 gal/mi, or infinite fuel economy, as it uses no liquid fuel on board, and that of the Plug-in Prius was also adjusted for the portion of vehicle energy use originating from the electric grid.\n\nCAFE has separate standards for \"passenger cars\" and \"light trucks\" even if the majority of \"light trucks\" are being used as passenger vehicles. The market share of \"light trucks\" grew steadily from 9.7% in 1979 to 47% in 2001 and remained in 50% numbers up to 2011.\nMore than 500,000 vehicles in the 1999 model year exceeded the GVWR cutoff and were thus omitted from CAFE calculations. More recently, coverage of medium duty trucks has been added to the CAFE regulations starting in 2012, and heavy duty commercial trucks starting in 2014.\n\nThe National Highway Traffic Safety Administration (NHTSA) regulates CAFE standards and the U.S. Environmental Protection Agency (EPA) measures vehicle fuel efficiency. Congress specifies that CAFE standards must be set at the \"maximum feasible level\" given consideration for:\n\nHistorically, the EPA has encouraged consumers to buy more fuel efficient vehicles, while the NHTSA expressed concerns that smaller, more fuel efficient vehicles may lead to increased traffic fatalities.\nThus higher fuel efficiency was associated with lower traffic safety, intertwining the issues of fuel economy, road-traffic safety, air pollution, and climate change. In the mid-2000s, increasing safety of smaller cars and the poor safety record of light trucks began to reverse this association. Nevertheless, in 2008, the on-road vehicle fleets in the United States and Canada had the lowest overall average fuel economy among first world nations: in North America, versus in the European Union and was even higher in Japan, according to data as of 2008. Furthermore, despite general opinion that larger and heavier (and therefore relatively fuel-uneconomical) vehicles are safer, the U.S. traffic fatality rate—and its trend over time—is higher than some other western nations, although it has recently started to gradually decline at a faster rate than in previous years.\n\nIn 2002, a committee of the National Academy of Sciences wrote a report on the effects of the CAFE standard. The report's conclusions include a finding that in the absence of CAFE, and with no other fuel economy regulation substituted, motor vehicle fuel consumption would have been approximately 14 percent higher than it actually was in 2002. One cost of this increase in fuel economy is a possible increase in fatalities, estimated to be 1,300 to 2,600 increased fatalities in 1993, albeit with certain of the committee members dissenting.\n\nA plot of average overall vehicle fuel economy (CAFE) for new model year passenger cars, the required by law CAFE standard target fuel economy value (CAFE standard) for new model year passenger cars, and fuel prices, adjusted for inflation, shows that there has been little variation over the past 20 years. Within this period, there are three distinct periods of fuel economy change:\nbefore returning to 1986 levels in 1990. These are following by an extended period during which the passenger car CAFE standard, the observed average passenger car fuel economy, and the price of gasoline remained stable, and finally a period starting about 2003 when prices rose dramatically and fuel economy has slowly responded.\n\nThe law of supply and demand would predict that an increase in gasoline prices would lead in the long run to an increase in the average fuel economy of the U.S. passenger car fleet, and that a drop in gasoline prices would be associated with a reduction in the average fuel economy of the entire U.S. fleet. There is some evidence that this happened with an increase in market share of lower fuel economy light trucks and SUVs and decline in passenger car sales, as a percentage of total fleet sales, as car buying trends changed during the 1990s, the impact of which is not reflected in this chart.\nIn the case of passenger cars, U.S. average fuel economy did not fall as economic theory would predict, suggesting that CAFE standards maintained the higher fuel economy of the passenger car fleet during the long period from the end of the 1979 energy crisis to the rise of gasoline prices in the early 2000s. Most recently, fuel economy has increased about one mpg from 2006 to 2007. This increase is due primarily to increased fuel efficiency of imported cars. Similarly, the law of supply and demand predicts that due to the United States' large percentage consumption of the world's oil supply, that increasing fuel economy would drive down the gasoline prices that U.S. consumers would otherwise have to pay. Reductions in petroleum demand in the United States helped create the collapse of OPEC market power in 1986.\n\nThe \"CAFE\" and \"CAFE standard\" shown here only regards new model passenger car fuel economy and target fuel economy (respectively) rather than the overall U.S. fuel economy average which tends to be dominated by used vehicles manufactured in previous years, new model light truck CAFE standards, light truck CAFE averages, or aggregate data.\n\nFleet fuel economy is calculated using a harmonic mean, not a simple arithmetic mean (average) – namely, the reciprocal of the average of the reciprocal values. For a fleet composed of four different kinds of vehicle A, B, C and D, produced in numbers n, n, n and n, with fuel economies f, f, f and f, the CAFE would be:\n\nFor example, a fleet of 4 vehicles getting 15, 13, 17, and 100 mpg has a CAFE of slightly less than 19 mpg:\n\nWhile the arithmetic mean fuel economy of the fleet is just over 36 mpg:\n\nThe harmonic mean captures the fuel economy of driving each car in the fleet for the same number of miles, while the arithmetic mean captures the fuel economy of driving each car using the same amount of gas (i.e., the 13 mpg vehicle would travel with one gallon while the 100 mpg vehicle would travel 100 miles).\n\nFor the purposes of CAFE, a manufacturer's car output is divided into a domestic fleet (vehicles with more than 75 percent U.S., Canadian or post-NAFTA Mexican content) and a foreign fleet (everything else). Each of these fleets must separately meet the requirements. The two-fleet requirement was developed by the United Automobile Workers (UAW) as a means to ensure job creation in the United States. The UAW successfully lobbied Congress to write this provision into the enabling legislation – and continues to advocate this position. The two fleet rule for light trucks was removed in 1996.\n\nFor the fuel economy calculation for alternative fuel vehicles, a gallon of alternative fuel is deemed to contain 15% fuel (which is approximately the amount of gasoline in a gallon of E85) \nas an incentive to develop alternative fuel vehicles. The mileage for dual-fuel vehicles, such as E85 capable models and plug-in hybrid electric vehicles, is computed as the average of its alternative fuel rating—divided by 0.15 (equal to multiplying by 6.666)—and its gasoline rating. Thus an E85-capable vehicle that gets 15 mpg on E-85 and 25 mpg on gasoline might logically be rated at 20 mpg. But in fact the average, for CAFE purposes, despite perhaps only one percent of the fuel used in E85-capable vehicles is actually E85, is computed as 100 mpg for E-85 and the standard 25 mpg for gasoline, or 62.5 mpg. However, the total increase in a manufacturer's average fuel economy rating due to dual-fueled vehicles cannot exceed 1.2mpg. Section 32906 reduces the increase due to dual-fueled vehicles to 0 through 2020. Electric vehicles are also incentivized by the 0.15 fuel divisor, but are not subject to the 1.2 mpg cap like dual-fuel vehicles.\n\nManufacturers are also allowed to earn CAFE \"credits\" in any year they exceed CAFE requirements, which they may use to offset deficiencies in other years. CAFE credits can be applied to the three years before or the five years after the year in which they are earned. The reason for this flexibility is so manufacturers are penalized only for persistent failure to meet the requirements, not for transient non-compliance due to market conditions.\n\nFuel economy regulations were first introduced in 1978, only for passenger vehicles. The next year, a second category was defined for light trucks. These were distinguished from heavy duty vehicles by a gross vehicle weight rating (GVWR) of 6000 pounds or less. The GVWR threshold was raised to 8500 pounds in 1980 and has remained at that level through 2010. Thus certain large trucks and SUV's were exempt, such as the Hummer and the Ford Excursion. From 1979 to 1991, separate standards were established for two-wheel drive (2WD) and four-wheel drive (4WD) light trucks, but for most of this period, car makers were allowed to choose between these separate standards or a combined standard to be applied to the entire fleet of light trucks they sold that model year. In 1980 and 1981, respectively, a manufacturer whose light truck fleet was powered exclusively by basic engines which were not also used in passenger cars could meet standards of 14 mpg and 14.5 mpg. After 2010, new rules set varying targets based on truck size footprint.\n\nSince 1980, the traditional Japanese manufacturers have increased their combined fleet average fuel economy by 1.6 miles per gallon according to the March 30, 2009 Summary of Fuel Economy Performance published annually by NHTSA. During this time, they also increased their sales in the United States by 221%. The traditional European manufacturers actually decreased their fleet average fuel economy by 2 miles per gallon while increasing their sales volume by 91%. The traditional U.S. manufacturers, Chrysler, Ford, and General Motors, increased their fleet average fuel economy by 4.1 miles per gallon since 1980 according to the latest government figures. During this time the sales of U.S. manufacturers decreased by 29%.\n\nA number of manufacturers choose to pay CAFE penalties rather than attempt to comply with the regulations. These tend to be companies with small U.S. market share and expensive, high performance vehicles, such as Porsche, Mercedes, and Fiat. In model year 2012, Jaguar (Land Rover) and Volvo did not meet CAFE requirements. They paid fines totaling 15 million dollars for the year.\n\nFor the 2011 model year, Spyker followed by GM light trucks had the lowest fleet average while Tesla followed by Honda had the highest.\n\nBefore the oil price increases of the 2000s, overall fuel economy for both cars and light trucks in the U.S. market reached its highest level in 1987, when manufacturers managed 26.2 mpg (8.98 L/100 km). The average in 2004 was 24.6 mpg. In that time, vehicles increased in size from an average of 3,220 pounds to 4,066 pounds (1,461 kg to 1,844 kg), in part due to an increase in truck ownership from 28% to 53%.\n\nThe CAFE rules for trucks were officially amended at the end of March 2006. However, the 9th Circuit Court of Appeals has overturned the rules, returning them to NHTSA, as discussed below. These changes would have segmented truck fleets by vehicle size and class as of 2011. All SUVs and passenger vans up to 10,000 pounds GVWR would have had to comply with CAFE standards regardless of size, but pickup trucks and cargo vans over 8500 pounds gross vehicle weight rating (GVWR) would have remained exempt.\n\nUnder the new final light truck CAFE standard 2008–2011, fuel economy standards would have been restructured so that they are based on a measure of vehicle size called \"footprint\", the product of multiplying a vehicle's wheelbase by its track width. A target level of fuel economy would have been established for each increment in footprint using a continuous mathematical formula. Smaller footprint light trucks had higher fuel economy targets and larger trucks lower targets. Manufacturers who made more large trucks would have been allowed to meet a lower overall CAFE target, manufacturers who make more small trucks would have needed to meet a higher standard. Unlike previous CAFE standards there was no requirement for a manufacturer or the industry as a whole to meet any particular overall actual MPG target, since that will depend on the mix of sizes of trucks manufactured and ultimately purchased by consumers. Some critics pointed out that this might have had the unintended consequence of pushing manufacturers to make ever-larger vehicles to avoid strict economy standards. However, the equation used to calculate the fuel economy target had a built in mechanism that provides an incentive to reduce vehicle size to about 52 square feet (the approximate midpoint of the current light truck fleet.)\n\nThe United States Court of Appeals for the Ninth Circuit agreed with NHTSA that economic benefit-cost analysis (maximizing net economic benefits to the Nation) is, under the Energy Policy and Conservation Act (EPCA), an appropriate method to select the maximum feasible stringency of CAFE standards, but nonetheless found that NHTSA incorrectly set a value of zero dollars to the global warming damage caused by CO emissions; failed to set a \"backstop\" to prevent trucks from emitting more CO than in previous years; failed to set standards for vehicles in the 8,500 to range; and failed to prepare a full Environmental Impact Statement (EIS) rather than a more abbreviated environmental impact assessment. The Court directed NHTSA to prepare a new standard as quickly as possible and to fully evaluate that new standard's impact on the environment.\n\nIn 2007, the House and Senate passed the Energy Independence and Security Act (EISA) with broad support, setting a goal for the national fuel economy standard of 35 miles per gallon (mpg) by 2020 and rendering the court judgment obsolete. On December 19, 2007, President George W. Bush signed the bill. The bill's standard would increase the fuel economy standards by 40 percent and save the United States billions of gallons of fuel. This was the first legislative change to the CAFE standard since it was created in 1975. The requirement applies to all passenger automobiles, including \"light trucks.\" President Bush faced serious pressure to reduce the Nation's dependency on oil and this was part of his initiative to do so.\n\nIn 2006, the rule making for light trucks for model years 2008–2011 included a reform to the structure for CAFE standards for light trucks and gave manufacturers the option for model years 2008–2010 to comply with the reformed standard or to comply with the unreformed standard. The reformed standard was based on the vehicle footprint. The unreformed standard for 2008 was set to be 22.5mpg.\n\nTo achieve the target of 35mpg authorized under EISA for the combined fleet of passenger cars and light truck for MY2020, NHTSA is required to continue raising the CAFE standards. In determining a new CAFE standard, NHTSA must assess the environmental impacts of each new standard and the effect of this standard on employment. With the EISA, NHTSA needed to take new analysis including taking a fresh look at the potential impacts under the National Environmental Policy Act (NEPA) and assessing whether or not the impacts are significant within the meaning of NEPA.\n\nNHTSA has to issue its new standards eighteen months before the model year for fleet. According to NHTSA report, in order to achieve this industry wide combined fleet of at least 35mpg, NHTSA must set new standards well in advance of the model year so as to provide the automobile manufacturers with lead time enough to make extensive necessary changes in their automobiles. The EISA also called for a reform where the standards set by the Transportation Department would be are “attribute based” so as to ensure that the safety of vehicles is not compromised for higher standards.\n\nThe 2007 Energy Independence and Security Act also instructed NHTSA to establish a credit trading and transferring scheme to allow manufacturers to transfer credits between categories, as well as sell them to other manufacturers or non-manufacturers. In addition, the period over which credits could be carried forward was extended from three years to five. Traded or transferred credits may not be used to meet the minimum standard in the domestic passenger car fleet, however they may be used to meet the \"attribute standard\". This latter allowance has drawn criticism from the UAW which fears it will lead manufacturers to increase the importation of small cars to offset shortfalls in the domestic market.\n\nThese new flexibilities were implemented by regulation on March 23, 2009 in the Final Rule for 2011 Model Year Passenger Cars and Light Trucks.\n\nCalculations using official CAFE data, and the newly proposed credit trading flexibility contained in the September 28, 2009 Notice of Proposed Rulemaking show that ninety-eight percent of the benefit derived from just the cross fleet credit trading provision flows to Toyota. According to these calculations 75% of the benefit from the two new CAFE credit trading provisions, cross fleet trading and 5-year carry-forward, falls to foreign manufacturers. Toyota can use the provision to avoid or reduce compliance on average by 0.69 mpg per year through 2020,\n\nThe estimated value of the CAFE exemption gained by Toyota is $2.5 billion; Honda’s benefit is worth $800 million, and Nissan’s benefit is valued at $900 million in reduced CAFE compliance costs. Foreign companies gained $5.5 billion in benefits compared with the $1.8 billion that went to the Detroit Three.\n\nIn the years 2021 to 2030, the standards requires MPG to be the \"maximum feasible\" fuel economy. The law allows NHTSA to issue additional requirements for cars and trucks based on the footprint model or other mathematical standard. Additionally, each manufacturer must meet a minimum standard of the higher of either 27.5 mpg for passenger automobiles or 92% of the projected average for all manufacturers. National Highway Traffic Safety Administration (NHTSA) is directed based on National Academy of Sciences studies to set medium and heavy-duty truck MPG standards to the \"maximum feasible\". Additionally, the law phases out the mpg credit previously granted to E85 flexible-fuel vehicle manufacturers and adds in one for biodiesel, and it adds a requirement that NHTSA publish replacement tire fuel efficiency ratings. The bill also adds support for initial state and local infrastructure for plug-in electric vehicles.\n\nOn April 22, 2008 NHTSA responded to the Energy Independence and Security Act of 2007 with proposed new fuel economy standards for cars and trucks effective model year 2011. The new rules also introduce the footprint model for cars as well as trucks, where if a manufacturer makes more large cars and trucks they will be allowed to meet a lower standard for fuel economy. This means that an overall fuel efficiency for a particular manufacturer nor the fleet as a whole cannot be predicted with certainty since it will depend on the actual product mix manufactured. However, if the product mix is as NHTSA predicts, car fuel economy would increase from a current standard of to in 2011. The new regulations are designed to be \"optimized\" with respect to a certain set of assumptions which include: gas prices in 2016 will be $2.25 a U.S. gallon (59.4¢/L), all new car purchasers will pay 7% interest rates on their vehicles purchases, and only care about fuel costs for the first 5 years of a vehicle's life, and that the value of global warming is $7 per ton CO. This corresponds to a global warming value of $4.31 savings a year per car under the new regulations. Further, the new regulations assume that no advanced hybrids (Toyota Prius), plug-in hybrids and extended range electric vehicles (Chevrolet Volt), electric cars (Th!nk City), nor alternative fuel vehicles (Honda Civic GX) will be used to achieve these fuel economies. The proposal again explained that U.S. law (49 U.S. Code § 32919) requires that \"a State or a political subdivision of a State may not adopt or enforce a law or regulation related to fuel economy standards or average fuel economy standards\", and explained that laws or regulations applicable to motor vehicle greenhouse gas emissions are related to fuel economy standards.\n\nIn mid-October 2008, DOT completed and released a final environmental impact statement in anticipation of issuing standards for model years 2011–2015. Based on its consideration of the public comments and other available information, including information on the financial condition of the automotive industry, the agency adjusted its analysis and the standards and prepared a final rule and Final Regulatory Impact Analysis (FRIA) for MYs 2011–2015. On November 14, 2008, the Office of Management and Budget concluded review of the rule and FRIA. However, issuance of the final rule was held in abeyance. On January 7, 2009, the Department of Transportation announced that the final rule would not be issued, writing: \"The Bush Administration will not finalize its rulemaking on Corporate Fuel Economy Standards. The recent financial difficulties of the automobile industry will require the next administration to conduct a thorough review of matters affecting the industry, including how to effectively implement the Energy Independence and Security Act of 2007 (EISA). The National Highway Traffic Safety Administration has done significant work that will position the next Transportation Secretary to finalize a rule before the April 1, 2009 deadline.\"\n\nOn January 27, 2009, President Barack Obama directed the Department of Transportation to review relevant legal, technological, and scientific considerations associated with establishing more stringent fuel economy standards, and to finalize the 2011 model year standard by the end of March. This single-model year standard was issued March 27, 2009, and is about one mpg lower than the fuel economy standards previously recommended under the Bush Administration. \"These standards are important steps in the nation's quest to achieve energy independence and bring more fuel efficient vehicles to American families\", said Secretary LaHood. The new standards will raise the industry-wide combined average to (a increase over the 2010 model year average), as estimated by the National Highway Traffic Safety Administration (NHTSA). It will save about of fuel and reduce carbon dioxide emissions by 8.3 million metric tons. This 2011 single-year standard will use an attribute-based system, which sets fuel economy standards for individual vehicle models, based on the footprint model. Secretary LaHood also noted that work on the multi-year fuel economy plan for model years after 2011 is already well underway. The review will include an evaluation of fuel-saving technologies, market conditions and future product plans from the manufacturers. The effort will be coordinated with interested stakeholders and other federal agencies, including the Environmental Protection Agency. The new rules were immediately challenged in court again by the Center for Biological Diversity as not addressing the inadequacies found by the previous court rulings.\n\nOn May 19, 2009, President Barack Obama proposed a new national fuel economy program which adopts uniform federal standards to regulate both fuel economy and greenhouse gas emissions while preserving the legal authorities of DOT, EPA and California. The program covered model year 2012 to model year 2016 and ultimately required an average fuel economy standard of in 2016 (of 39 miles per gallon for cars and 30 mpg for trucks), a jump from the 2009 average for all vehicles of 25 miles per gallon. Obama said, \"The status quo is no longer acceptable.\" The higher fuel economy was projected to reduce oil consumption by approximately over the life of the program and reduce greenhouse gas emissions by approximately 900 million metric tons; the expected consumer costs in terms of higher car prices was unknown. Ten car companies and the UAW embraced the national program because it provided certainty and predictability to 2016 and included flexibilities that would significantly reduce the cost of compliance. Stated goals for the program included: saving consumers money over the long term in increased fuel efficiency, preserving consumer choice (the new rules do not dictate the size of cars, trucks and SUVs that manufacturers can produce; rather it requires that all sizes of vehicles become more energy efficient), reduced air pollution in the form of greenhouse gas emissions and other conventional pollutants, one national policy for all automakers instead of three standards (a DOT standard, an EPA standard and a California standard that would apply to 13 other states), and industry desires: clarity, predictability and certainty concerning the rules while giving them flexibility on how to meet the expected outcomes and the lead time they need to innovate. The policy was expected to result in yearly 5% increases in efficiency from 2012 through 2016, of oil saved cumulatively over the lifetime of the program and significant reductions in greenhouse gas emissions equivalent to taking 177 million of today's cars off the road.\n\nBy model year 2014, many of the program's goals were being met. The average new vehicle fuel economy was 30.7 mpg (35.6 mpg for cars and 25.5 mpg for trucks) and for the years 2012–2015, auto industry outperformed the GHG standard by a substantial margin. Consumers are expected to save an estimated 16.6 billion gallons of fuel over the lifetime of model year 2011 to 2014 vehicles due to the manufacturers exceeding the CAFE standards in those years.\n\nOn July 29, 2011, President Obama announced an agreement with thirteen large automakers to increase fuel economy to 54.5 miles per gallon for cars and light-duty trucks by model year 2025. He was joined by Ford, GM, Chrysler, BMW, Honda, Hyundai, Jaguar/Land Rover, Kia, Mazda, Mitsubishi, Nissan, Toyota, and Volvo—which together account for over 90% of all vehicles sold in the United States—as well as the United Auto Workers (UAW), and the State of California, who were all participants in the deal. The agreement resulted in new CAFE regulations for model year 2017–2025 vehicles, which were finalized on August 28, 2012. The major increases in stringency and the changes in the structure of CAFE create a need for research that incorporates the demand and supply sides of the new vehicle market in a more detailed manner than was needed with static fuel economy standards.\n\nVolkswagen responded to the July 29, 2011 agreement with the following statement:\n\"Volkswagen does not endorse the proposal under discussion. It places an unfairly high burden on passenger cars, while allowing special compliance flexibility for heavier light trucks. Passenger cars would be required to achieve 5% annual improvements, and light trucks 3.5% annual improvements. The largest trucks carry almost no burden for the 2017–2020 timeframe, and are granted numerous ways to mathematically meet targets in the outlying years without significant real-world gains. The proposal encourages manufacturers and customers to shift toward larger, less efficient vehicles, defeating the goal of reduced greenhouse gas emissions.\" Additionally, Volkswagen has since approached U.S. lawmakers about lowering their proposal to double fuel efficiency for passenger cars by 2025. Volkswagen at the time claimed that the new plan was unfair, but the company was later revealed to have been systematically cheating emissions tests. As a result, Volkswagen is one of the only major auto manufacturers to not sign the agreement that has led to the current proposal from the Obama administration.\n\nThe 2011 agreement set up requirements for a mid-term review to look at how the industry was progressing with the new standards. On July 18, 2016, the EPA, NHTSA and the California Air Resources Board (CARB) released a technical paper assessing whether or not the auto industry will be able to reach the 2022 to 2025 mpg standards. The Draft Technical Assessment Report, as the paper is called, is the first step in the mid-term evaluation process.\n\nThe government groups found that the auto industry has been doing a good job innovating and pushing towards lowering greenhouse gas emissions. The paper says the technology is cheaper or about what was expected in terms of cost, and that automakers are adopting new technologies quicker than expected. Still, the paper says that the 54.5 mpg goal is unrealistic. That goal was based off a market that was 67 percent cars and 33 percent trucks and SUVs. American customers aren't buying that many cars—the market is still about 50/50 and will likely stay that way. The paper says more realistic goals are 50 mpg to 52.6.\n\nIn early August 2018, the EPA and Department of Transportation, now operating under the Presidency of Donald Trump, issued a proposed ruling that, if enacted, would rollback some of the goals set in 2012 under President Obama. The ruling would freeze the fuel economy goals to the 2021 target of 37 mpg, would halt requirements on the production of hybrid and electric cars, and would eliminate the legal waiver that allows states like California to set more stringent standards. The EPA acting administrator Andrew R. Wheeler and the Transportation Secretary Elaine Chao issued a joint statement stating that the rule change was needed as the current rules \"impose significant costs on American consumers and eliminate jobs\", while the new rules \"give consumers greater access to safer, more affordable vehicles, while continuing to protect the environment\". The proposal issues a withdraw of the waiver that granted California for setting its own GHG and ZEV (Zero Emission Vehicle) standards and that allow other States to adopt the standard instead of the Federal standard. The goal of the withdraw is to address national regulatory efficiency .Following publication of the proposed rule changes, California and eighteen other states announced that should the rule be enacted, they will sue the government to reject the rule.\n\nThe new ruling proposed by the EPA and NHTSA is named the \"Safer Affordable Fuel-Efficient (SAFE) Vehicle Rules\" that would replace the original CAFE standard regulations set for MY 2022-2025 passenger car and light trucks, while the 2021 MY vehicles will maintain the CAFE rules. The safety reason provided by the government is to shift people to buying new vehicles once the vehicles become more affordable under SAFE standards, with a government study conducted to show new model year vehicles result in lower fatality rates. After releasing the proposal on August 2 2018, NHTSA and EPA held a comment hearing period for 60 days.\n\nAccording to the TRB committee, the weakening of 2022-2025 CAFE standards would make it harder for the U.S. to meet the two-degree-Celsius global warming scenario, meaning more effort would have to be met in 2050 if if the SAFE standard is administrated to halt the original CAFE regulations.\n\nA study has found that the adoption of CAFE standards, if supported together by government incentives, would accelerate the Electric Vehicle Market. The U.S. could be less dependent on fossil fuels from the shift to EV market adoption. Economic research in 2015 concludes that firms are shown to be more incentivized toward innovations on fuel economy while the expenses of other safety considerations are undetermined.\n\nCAFE neither directly offers incentives for customers to choose fuel efficient vehicles nor directly affects fuel prices. Rather, it attempts to accomplish the goals indirectly, by making it more expensive for automakers to build inefficient vehicles by introducing penalties.\nThe conservative Heartland Institute contends that CAFE standards do not work economically to consumers' benefit, smaller cars are more likely to be damaged in a collision, and insurance premiums for them are higher than for many larger cars. However, the Insurance Companies' Highway Loss Data Institute publishes data showing that larger vehicles are more expensive to insure.\n\nCAFE advocates assert that most of the gains in fuel economy over the past 30 years can be attributed to the standard itself. Opponents assert that economic forces are responsible for fuel economy gains, and that higher fuel prices drove customers to seek more fuel-efficient vehicles. CAFE standards have come under attack by some conservative thinktanks, along with safety experts, car and truck manufacturers, some consumer and environment groups, and organized labor.\n\nHistorically, NHTSA has expressed concerns that automotive manufacturers would increase mileage by reducing vehicle weight, which might lead to weight disparities in the vehicle population and increased danger for occupants of lighter vehicles. However, vehicle safety ratings are now made available to consumers by NHTSA and by the Insurance Institute for Highway Safety. A National Research Council report found that the standards implemented in the 1970s and 1980s \"probably resulted in an additional 1,300 to 2,600 traffic fatalities in 1993. A Harvard Center for Risk Analysis study found that CAFE standards led to \"2,200 to 3,900 additional fatalities to motorists per year. The Insurance Institute for Highway Safety's 2007 data show a correlation of about 250–500 fatalities per year per MPG. Proponents of higher CAFE standards argue that it is the \"Footprint\" model of CAFE for trucks that encourages production of larger trucks with concomitant increases in vehicle weight disparities, and point out that some small cars such as the Mini Cooper and Toyota Matrix are four times safer than SUVs like the Chevrolet S-10 Blazer. They argue that the quality of the engineering design is the prime determinant of vehicular safety, not the vehicle's mass. In 2006, IIHS found that some of the smallest cars have good crash safety, and others do not, depending upon the engineering design. In a 2007 analysis, IIHS found that 50 percent of fatalities in small four-door vehicles were single vehicle crashes, compared to 83 percent in very large SUVs. The Mini Cooper had a fatality rate of 68 per million vehicle-years compared to 115 for the Ford Excursion. The analysis' conclusions include findings that death rates generally are higher in lighter vehicles, but cars almost always have lower death rates than SUVs or pickup trucks of comparable weight. A 2005 IIHS plot shows that in collisions between SUVs weighing and cars, the car driver is more than 4X more likely to be killed, and if the SUV weighs over the car driver is 9 times more likely to be killed, with 16 percent of deaths occurring in car-to-car crashes and 18 percent in car-to-truck crashes. Recent studies find about 75 percent of two-vehicle fatalities involve a truck, and about half these fatalities involve a side-impact crash. Risk to the driver of the other vehicle is almost 10 times higher when the vehicle is a one ton pickup compared to an imported car. And a 2003 Transportation Research Board study show greater safety disparities among vehicles of differing price, country of origin, and quality than among vehicles of different size and weight. These more recent studies tend to discount the importance of vehicle mass to traffic safety, pointing instead to the quality of engineering design as the primary factor.\n\nAs fuel efficiency rises, people drive their cars more, which can mitigate some of the gains in carbon dioxide emissions from the higher standards. According to the National Academies Report (Page 19) a 10% improvement in fuel efficiency leads to an average increase in travel distance of 1–2%. This phenomenon is referred to as the \"rebound effect\". The report stated (page 20) that the fuel efficiency improvements of light-duty vehicles have reduced the overall U.S. emissions of CO2 by 7%.\n\nIt is also possible that auto buyers may choose to keep their older cars (some of which are less efficient) for longer before making a new purchase.\n\nHowever, associated costs, such as increased deaths, may be more than offset by savings on a global scale, because increased CAFE standards reduce reliance on increasingly expensive and unreliable sources of imported petroleum and lower the probability of global climate change by reducing U.S. emissions of carbon dioxide.\n\nIn the May 6, 2007 edition of \"Autoline Detroit\", Bob Lutz, an automobile designer/executive of BMW and Big Three fame, asserted that the CAFE standard was a failure and said it was like trying to fight obesity by requiring tailors to make only small-sized clothes.\n\nProponents state that automobile-purchasing decisions that may have global effects should not be left entirely up to individuals operating in a free market.\n\nAutomakers have said that small, fuel-efficient vehicles cost the auto industry billions of dollars. They cost almost as much to design and market but cannot be sold for as much as larger vehicles such as SUVs, because consumers expect small cars to be inexpensive. In 1999, \"USA Today\" reported small cars tend to depreciate faster than larger cars, so they are worth less in value to the consumer over time. However, 2007 Edmunds depreciation data show that some small cars, primarily premium models, are among the best in holding their value.\n\nThere are a large number of technologies that manufacturers can apply to improve fuel efficiency short of implementing hybrid or plug-in hybrid technologies. Applied aggressively, at a cost of several thousand dollars per vehicle, the Union of Concerned Scientists estimates that these technologies can almost double MPG.\n\nSome technologies, such as four valves per cylinder, are already widely applied in cars, but not trucks. Manufacturers dispute how effective these technologies are, their retail price, and how willing customers are to pay for these improvements. Payback on these improvements is highly dependent on fuel prices.\n\nHistorically, automakers and some conservative groups have believed consumers do not prioritize fuel economy. In 2003, Alliance of Automobile Manufacturers spokesman Eron Shosteck asserted automakers produce more than 30 models rated at 30 mpg or more for the U.S. market, and they are poor sellers. In 2004, GM retiree Charles Amann said statistically, consumers do not pick the weak-performing vehicle when given a choice of engines. However, after a spike in gas prices, a 2006 Consumer Reports survey concluded fuel economy is the most important consideration in consumers' choice of vehicle and a 2007 Pew Charitable Trusts survey found that nine out of ten Americans favor tougher CAFE standards, including 91% of Democrats and 85% of Republicans. In 2007, the 55 mpg Toyota Prius outsold the top-selling SUV, the 17 mpg Ford Explorer. In late 2007, GM Vice Chairman Bob Lutz called hybrid gasoline-electric vehicles the \"ideal solution\". In 2008, GM advertised fuel economy improvements and their upcoming Chevrolet Volt Extended Range Electric Vehicle, and developed corporate branding for their fuel economy technologies, and though GM Chairman Rick Wagoner admitted not knowing which fuel efficiency technologies consumers really want, he said \"we are moving fast with technologies like E‑85 (ethanol), all-electric, fuel cells, and a wide range of hybrid offers\". NHTSA's public record shows the automakers publicly express opposition to CAFE increases.\n\nThe definitions for cars and trucks are not the same for fuel economy and emission standards. For example, a Chrysler PT Cruiser is defined as a car for emissions purposes and a truck for fuel economy purposes. Under the current light truck fuel economy rules, the PT Cruiser will have a higher fuel economy target (28.05 mpg beginning in 2011) than it would if it were classified as a passenger car. CAFE standards signaled the end of the traditional long station wagon, but Chrysler CEO Lee Iacocca developed the idea of marketing the minivan as a station wagon alternative, while certifying it in the separate truck category to allow compliance with less-strict emissions standards. Eventually, this same idea led to the promotion of the SUV. This trend has reversed itself since the crossover has eroded SUV sales during the mid-2000s, SUVs must conform with emission standards and a crossover is defined as a car for fuel economy purposes.\n\nNew York, New Jersey, Pennsylvania, Connecticut, and California disagreed with NHTSA that U.S. law (49 U.S. Code § 32919) precludes state-level light vehicle greenhouse gas regulations because such regulations are related to fuel economy standards. While NHTSA indicated that fuel economy is calculated by measuring vehicular carbon emissions and that nearly all of the engineering options to reduce vehicular GHG emissions also improve fuel economy, these states argued that because that the use of alternative fuels could allow greenhouse gas emissions to be reduced somewhat independently of fuel efficiency, greenhouse gas standards are not related to fuel economy standards.\n\nThe United States Environmental Protection Agency (EPA) laboratory measurements of MPG have consistently overestimated fuel economy of gasoline vehicles and underestimated diesel vehicles. John DeCicco, the automotive expert for the Environmental Defense Fund (EDF), estimated that this results in about 20% higher actual consumption than measured CAFE goals. Starting with 2008-model vehicles, the EPA has adopted a new protocol for estimating the MPG figures presented to consumers. The new protocol includes driving cycles more closely representative of today's traffic and road conditions, as well as increased air conditioner usage. This change does not affect how the EPA calculates CAFE ratings; the new protocol changes only the mileage estimates provided for consumer information.\n\nNHTSA spends one-third of one percent of its budget on CAFE, or $0.014 per U.S. citizen.\n\nSome critics argue that CAFE fines do not seem to be having much impact in the fuel economy drive.\n\nCurrently, the CAFE penalty is $55 USD per vehicle for every 1 mpg under the standard. For the year 2006 Mercedes-Benz drew a $30.3 million penalty for violating fuel economy standards by 2.2 MPG, or $122 per vehicle. According to the government \"fueleconomy.gov\" website violating CAFE by 2.42 MPG means consuming extra () of mostly imported fuel in 10 years which is worth $3,490 (Based on 45% highway, 55% city driving, 15,000 annual miles and a fuel price of $2.95 per gallon) that is 13.4% more and also it means emitting extra 14 Tons of CO in 10 years that is 12.7% more. These numbers are based on comparison of 2010 Mercedes ML 350 4MATIC with CAFE Unadjusted Average Fuel Economy of 21.64 MPG (this model meets 2006 CAFE requirements of 21.6 MPG) and 2010 Mercedes ML 550 4MATIC with CAFE Unadjusted Average Fuel Economy of 19.22 MPG. So consuming extra $3,490 worth of mostly imported fuel and emitting extra 14 Tons of CO draws a penalty of only $122 for a single luxury car buyer. $122 is only 0.3% of the price of $40,000 car (average 2010 price of a luxury car). Several experts stated that this is not enough of a monetary incentive to comply with CAFE.\n\nCAFE penalty have increased only 10% since 1983, while cumulative inflation was 119%. Thus, the CAFE penalty in 2010 is actually less than half of what it was in 1983. NHTSA officials stated that in addition to the authority the Federal Civil Penalties Inflation Adjustment Act of 1990 under the EPCA, the NHTSA has the authority to raise CAFE penalties to $100 per mpg shortfall. However, the NHTSA currently does not exercise this authority.\n\nThe Kingdom of Saudi Arabia announced new light-duty vehicle fuel economy standards in November 2014 which became effective January 1, 2016 and will be fully phased in by January 1, 2018 <Saudi Standards regulation (SASO-2864)>. A review of the targets will be carried by December 2018, at which time targets for 2021–2025 will be set.\n\n\n"}
{"id": "1565136", "url": "https://en.wikipedia.org/wiki?curid=1565136", "title": "Crest and trough", "text": "Crest and trough\n\nA crest is the point on a wave with the maximum value of upward displacement within a cycle. A crest is a point on a surface wave where the displacement of the medium is at a maximum. A trough is the opposite of a crest, so the minimum or lowest point in a cycle.\n\nWhen the crests and troughs of two sine waves of equal amplitude and frequency intersect or collide, while being in phase with each other, the result is called \"constructive\" interference and the magnitudes double (above and below the line). When in antiphase – 180° out of phase – the result is \"destructive\" interference: the resulting wave is the undisturbed line having zero amplitude.\n\n\n"}
{"id": "15160635", "url": "https://en.wikipedia.org/wiki?curid=15160635", "title": "Gordon Walker (professor)", "text": "Gordon Walker (professor)\n\nGordon P. Walker is professor in the Department of Geography and Lancaster Environment Centre at Lancaster University. Professor Walker is the author of many publications on environmental justice and inequality, and community energy initiatives which involve embedding renewable energy at the local level.\n\n"}
{"id": "12633703", "url": "https://en.wikipedia.org/wiki?curid=12633703", "title": "Hibernaculum (zoology)", "text": "Hibernaculum (zoology)\n\nA hibernaculum \"plural form: hibernacula\" (Latin, \"tent for winter quarters\") is a place in which a creature seeks refuge, such as a bear using a cave to overwinter. The word can be used to describe a variety of shelters used by many kinds of animals, including insects, toads, lizards, snakes, bats, rodents, and primates of various species.\n\nInsects range in their size, structure, and general appearance but most use hibernacula. All insects are primarily exothermic. For this reason, extremely cold temperatures, such as those experienced in the winter season, outside of tropical locations, cause their metabolic systems to shut down; long exposure may lead to death. Insects survive colder winters through the process of overwintering, which occurs at all stages of development and may include migration or hibernation for different insects, the latter of which must be done in hibernacula. Insects that do not migrate must halt their growth to avoid freezing to death, in a process called diapause. Insects prepare to overwinter through a variety of mechanisms, such as using anti-freeze proteins or cryoprotectants in freeze-avoidant insects, like Soybean aphids. Cryoprotectants are toxic, with high concentrations only tolerated at low temperatures. Thus, hibernacula are used to avoid sporadic warming and the risk of death due high concentrations of cryoprotectants at warmer temperatures. Freeze-tolerant insects, like second-generation corn-borers, can survive being frozen and therefore, undergo inoculative freezing. Hibernacula range in size and structure depending on the insects using them.\n\nHowever, insect hibernacula are generally required to be:\n\nSome insects, like convergent lady beetles, reuse the same hibernacula, year after year. They converge with other lady beetles and migrate to hibernacula used by prior generations. They are able to find old hibernacula due to hydrocarbons released by lady beetle feet which create a lasting path. This allows lady beetles to retrace their footsteps to previously used hibernacula. Their tendency to aggregate and overwinter in groups is likely due to their attraction to similar environments and conspecifics. Beetles use rock crevices as hibernacula, often clumping in them, in groups. These rock crevices are found in rock fields the beetle are attracted to for high levels of vegetation and greenery.\n\nOther types of insect hibernacula include self-spun silk hibernacula, such as those made and used by spruce budworms as they moult and overwinter in their second instars. An example is the eastern spruce budworm which creates hibernacula after dispersing during its first instar then overwinter before emerging from the hibernacula in early May. Woolly bear caterpillars overwinter as caterpillars and grow to be Isabella tiger moths. They use plant debris as makeshift hibernacula, to protect themselves from extreme elements. Some butterflies, like the White admiral butterfly also only mature halfway as a caterpillar before hibernating for the winter. For freeze-avoidant insects, ideal hibernacula are dry, as freeze-avoidant insects are less likely to dampen and freeze in them, however moist hibernacula promote inoculative freezing for freeze-tolerant insects.\n\nAmphibians that hibernate include several species of frogs and salamanders from the northern continental climates of North America and Eurasia and also from extreme southern hemisphere climates. These amphibians slow their metabolism during winter to avoid unsuitable conditions, such as freezing. Most freeze avoidance strategies include overwintering in aquatic situations or burrowing in the soil to depths below the frostline. A hibernaculum for amphibians should provide the following:\nSpecies from cool continental climates hibernate at temperatures from 0 to 4 °C. Some species will not survive hibernation at temperatures that exceed 4 °C.\n\nGenerally, for amphibians that hibernate under ice, it is necessary for the animal to be submerged in water that is 10 to 15 cm deep and to maintain the temperature between 2 and 3 °C and not above 4 °C. Water should be well aerated, with maintained low-intensity light levels and minimal disturbance of the amphibians.\n\nLike other amphibians, frogs show minimal capacities for freezing tolerance and survive winter by using terrestrial hibernacula where they avoid freezing. However, frogs may exhibit greater freeze-tolerance capacity at high latitude range limits, where winter climate is more severe. For example, data suggests that while cricket frogs in South Dakota survive winter by locating hibernacula that prevent freezing, their toleration of short freezing bouts may expand the range of suitable hibernacula. However, overwinter mortality may be high at the northern range boundary due to colder temperatures and might limit cricket frogs from expanding their range northward.\n\nThe microclimate refers to the climate of a very small or restricted area, like the hibernaculum, especially when this differs from the climate of the surrounding area. Overwinter survival in these cricket frogs among other frogs is dependent on using hibernacula with appropriate physical microclimate characteristics, such as moist soil, that buffer frogs from temperatures that drop below the freezing point of the body fluids for extended periods. Although, determining if frogs can identify sites with appropriate microclimates to support overwinter survival and what factors might inform such choices are still unknown and will require further study. Therefore, it is still not known to what extent various types of prospective hibernacula for frogs might be suitable in the years to come, especially factoring in climate change.\nAs part the Highways Agency Biodiversity Action Plan (HABAP) in the UK, the Species Action Plan (SAP) for great crested newts aims to maintain and enhance existing newt populations through appropriate management of suitable habitat. As part of steps to implement the HABAP, newt hibernacula (e.g. log piles) have been constructed to improve the quality of the terrestrial habitat through increasing the number of potential overwintering sites. It was also determined that habitat surrounding breeding ponds with plenty of cover and suitable overwintering sites may have less need for provision of artificial hibernacula than landscapes with less woodland, hedgerows, scrub etc. Because great crested newts show high loyalty to over-wintering locations, returning to such established areas year after year, artificial hibernacula could be important in future years to conserve newts and other amphibians. Although, monitoring in the vicinity of these hibernacula in autumn using felt roofing tiles did not reveal the presence of any great crested newts even though they are known to breed in nearby ponds. Common toads and frogs did surround the area however. Therefore, further studies need to be conducted in order to create species-appropriate artificial hibernacula.\n\nMany reptiles undergo hibernation or a process called brumation, which is similar to hibernation; both processes require usage of a hibernaculum. Staying inside an insulated hibernaculum is a strategy to avoid the harsh winter months when the frigid outside temperatures may kill an ectothermic reptile. They depress their metabolism and heart rates to reduce energy consumption so they don't need to exit their hibernacula. Hibernating reptiles are also safer from predation inside of their concealed and protected hibernacula. Various species of turtles, snakes, and lizards all use hibernacula, the forms of which can vary greatly.Hibernacula are typically:\n\nCommon snapping turtles generally hibernate for about 6 months from early October to mid April. They live in lakes during their active months, then travel to small offshoot streams to hibernate. Hibernacula are about 100–150 meters away from the main body of the home lake. Most snapping turtles hibernate by burrowing into the banks of alder streams or vegetated streams, but some use other structures such as abandoned beaver dens. These streams are typically less than 0.3 m deep and 0.7 m wide, covered by sunken alder roots or fallen trees, and not covered by ice in the winter. Many animals return to the same stream to hibernate in subsequent years.\n\nUnlike more solitary snapping turtles, snakes may either hibernate alone or in large aggregations of up to several thousand individuals of the same or different species. They use a wide variety of hibernacula, including: rock piles, debris-filled wells, caves, crevices, unused burrows made by other animals, and ant mounds. The common European viper has actually been observed using all of the hibernacula listed above. Most species seem to prefer finding an already-present suitable site rather than constructing one of their own, but they do expand upon present structures and may make their own burrows if there aren't any quality sites available. Pine snakes and the closely related Louisiana pine snakes are two of the most well-studied hibernating snake species, and share similar hibernacula characteristics. These species sometimes constructs their own burrows, or use tunnels formed from the decay of tree roots or by gophers. The tunnels form complex networks, and have side chambers which each house one snake.\n\nMesquite lizards in Mexico and the southern United States have been found hibernating in groups of 2-8 in cracks or under slabs of bark in mesquite trees. Common collared lizards spend about 6 months hibernating, almost always solitarily, though pairs of juvenile females have been observed within the same hibernacula. They use the undersides of rock slabs as hibernacula, digging a small chamber in the dirt just large enough for their bodies with a small tunnel for outside access. Adults use larger rock slabs, dig deeper chambers, and have longer tunnels than juveniles. Perhaps the most extreme example is seen in the viviparous lizard, the most northern of all lizards. They can burrow into the soil, go under leaf litter, or use shelters like rocks as hibernacula. Although the air temperature in West Siberia can drop to −10 °C, the soil temperature at the depths where these lizards hibernate remains higher than −10 °C. This enables them to survive the harshest temperatures of any lizard.\n\nLike other animals, mammals hibernate during seasons of harsh environmental conditions and resource scarcity. As it requires less energy to maintain homeostasis and survive when an individual is hibernating, this is a cost-effective strategy to increase survival rates. Hibernation is usually perceived as taking place during winter, as in the most well-known hibernators bears and bats, but can also occur during the dry season when there is little food or water, as in the mouse lemurs of Madagascar. Given that mammals can spend anywhere from 1–9 months hibernating, their choice in hibernaculum in essential in determining their survival.\n\nHibernacula vary greatly, but are typically:\n\nMany bears occupy similar hibernacula to smaller mammals, but theirs are, of course, much larger and can vary greatly across and within species. Most black bears excavate dens into a hillside or at the base of a tree, stump, or shrub, but some make dens at the bases of hollow trees, in hollow logs, or in rock caves or cavities. Den reuse is observed in this species, but very rarely. There were no significant den size differences between age or sex classes, except adult males creating larger entrances. Grizzly bears likewise don't show age or sex class differences in den dimensions. Grizzlies prefer hibernacula sites with abundant ground and canopy cover, and abundant sweet-vetch. Polar bears differ from black bears, grizzlies, and other bear species where both sexes hibernate in that only females use hibernacula. Like other female bears, polar bears use hibernacula as maternity dens. Also like other species, they tend to dig dens into the earth, although their arctic hibernacula are usually covered with snow by the time they emerge.\n\nBats favor larger hibernacula where large groups may roost together, including natural caves, mines, cellars, and other kinds of underground sites and man-made structures, like ice-houses. Within these hibernacula, the bats are still highly tuned to environmental factors. Little brown bats in northern latitudes hibernate for up to 8 months during the winter, and leave their roosts in the warm spring weather when insect prey is plentiful again. Bats gauge the outside temperature by being attuned to the airflow at the hibernacula entrance, which is driven by temperature differences between inside and outside the hibernacula, allowing bats to leave when the temperature begins to warm. Some hibernacula are shared between multiple species, such as common pipistrelles roosting with soprano pipistrelles. Behavior other than hibernating can also occur at hibernacula; common pipistrelles produce most of their mating calls and mate with each other in and near their hibernacula.\n\nMany hibernating, small-bodied mammals hibernate in simple holes in the ground, though some use complex systems of tunnels and burrows. Mountain pygmy possums in New South Wales, Australia dig holes in the ground to form hibernacula, with the preferred location being in boulder fields under a layer of snow. During the first few months of hibernation, possums awaken occasionally and leave one hibernaculum in favor of another, seemingly in an effort to find the hibernaculum with the most suitable microclimate. The reddish-grey mouse lemur also wakes and leaves the hibernaculum spontaenously and for brief periods of time. Their hibernacula are located in holes in large trees with varying levels of insulation. However, the range of insulation levels is relatively narrow, as there are often small numbers of suitably large trees. There can be hibernacula differences even within a species. In Columbian ground squirrels, hibernacula size is proportional to the weight of the individual occupying it, with adults having deeper hibernacula than juveniles, unlike black bears. Most juveniles choose to hibernate within 20 meters of their mother's burrow; those that don't have higher mortality rates.\n\n"}
{"id": "33038899", "url": "https://en.wikipedia.org/wiki?curid=33038899", "title": "Hydrogen clathrate", "text": "Hydrogen clathrate\n\nA hydrogen clathrate is a clathrate containing hydrogen in a water lattice. This substance is interesting due to its possible use to store hydrogen in a hydrogen economy. Another unusual characteristic is that multiple hydrogen molecules can occur at each cage site in the ice, one of only a very few guest molecule that forms clathrates with this property. The maximum ratio of hydrogen to water is 6 H to 17 HO. It can be formed at 250K in a diamond anvil at a pressure of 300MPa (3000 Bars). It takes about 30 minutes to form, so this method is impractical for rapid manufacture. The percent of weight of hydrogen is 3.77%. The cage compartments are hexakaidecahedral and hold from two to four molecules of hydrogen. At temperatures above 160K the molecules rotate around inside the cage. Below 120K the molecules stop racing around the cage, and below 50K are locked into a fixed position. This was determined with deuterium in a neutron scattering experiment.\n\nUnder higher pressures a 1:1 ratio clathrate can form. It crystallises in a cubic structure, where H and HO are both arranged in a diamond lattice. It is stable above 2.3 GPa.\n\nUnder even higher pressures (over 38 GPa) there is a prediction of the existence of a clathrate with a cubic structure and a 1:2 ration: 2H•HO.\n\nMore complex clathrates can occur with hydrogen, water and other molecules such as methane, and tetrahydrofuran.\n\nSince hydrogen and water ice are common constituents of the universe, it is very likely that under the right circumstances natural hydrogen clathrates will be formed. This could occur in icy moons for example. Hydrogen clathrate was likely to be formed in the high pressure nebulae that formed the gas giants, but not to have formed in comets.\n"}
{"id": "17509637", "url": "https://en.wikipedia.org/wiki?curid=17509637", "title": "Hydrothermal explosion", "text": "Hydrothermal explosion\n\nHydrothermal explosions occur when superheated water trapped below the surface of the earth rapidly converts from liquid to steam, violently disrupting the confining rock. Boiling water, steam, mud, and rock fragments called breccia are ejected over an area of a few meters up to several kilometers in diameter. Although the energy inherently comes from a deep igneous source, this energy is transferred to the surface by circulating meteoric water rather than by magma, as occurs in volcanic eruptions. The energy is stored as heat in hot water and rock within a few hundred feet of the surface.\n\nHydrothermal explosions are caused by the same instability and chain reaction mechanism as geysers but are so violent that rocks and mud are expelled along with water and steam.\n\nHydrothermal explosions occur where shallow interconnected reservoirs of water at temperatures as high as 250° Celsius underlie thermal fields. Water usually boils at 100 °C, but under pressure its boiling point increases, causing the water to become superheated. A sudden reduction in pressure causes a rapid phase transition from liquid to steam, resulting in an explosion of water and rock debris. During the last Ice Age, many hydrothermal explosions were triggered by the release of pressure as glaciers receded. Other causes are seismic activity, erosion, or hydraulic fracturing.\n\nYellowstone National Park is a thermally active area with an extensive system of hot springs, fumaroles, geysers, and mudpots. There are also several hydrothermal explosion craters, which are not to be confused with calderas, which are collapse features. Eight of these hydrothermal explosion craters are in hydrothermally cemented glacial deposits, and two are in Pleistocene ash-flow tuff. Each is surrounded by a rim composed of debris derived from the crater, 30 to 100 feet high.\n\nMore than 20 large hydrothermal explosions have occurred at Yellowstone, approximately one every 700 years. The temperature of the magma reservoir below Yellowstone is believed to exceed 800° Celsius causing the heating of rocks in the region. If so, the average heat flow supplied by convection currents is 30 times greater than anywhere in the Rocky Mountains. Snowmelt and rainfall seep into the ground at a rapid rate and can conduct enough heat to raise the temperature of ground water to almost boiling.\n\nThe phenomena of geyser basins are the product of hot ground water rising close to the surface and occasionally bubbling through. Water temperatures of 238° Celsius at 332 meters have been recorded at Norris Geyser Basin. \nPocket Basin was originally an ice-dammed lake over a hydrothermal system. Melting ice during the last glacial period caused the lake to rapidly drain, causing a sudden change in pressure triggering a massive hydrothermal explosion.\n\nA hydrothermal explosion is similar to a geyser's eruption except that includes surrounding rock and mud.\n\nOne well-known hydrothermal geyser is Old Faithful which throws up plumes of steam and water approximately every hour and a half on average. Rarely has any steam explosion violently hurled water and rock thousands of feet above the ground; however in Yellowstone’s geological history these colossal events have been recorded numerous times and have been found to have created new hills and shaped parts of the landscape.\n\nThe largest hydrothermal explosion ever documented was located near the northern edge of Yellowstone Lake, on an embankment commonly known as “Mary Bay”. Now consisting of a 1.5 mile crater, it was formed relatively recently, approximately 13,800 years ago. It is believed this crater was formed by a sequence of several hydrothermal explosions in a short time. What triggered this series of events has not yet been clearly established, but volcanologists believe a large earthquake could have played a role by accelerating the melting of nearby glaciers and thus depressurizing the hydrothermal system. Alternatively, rapid changes in the level of Yellowstone Lake may have been responsible.\n\nMost of Yellowstone’s recent large hydrothermal explosions have been the consequence of sudden changes of pressure deep within the hydrothermal system. Generally, these larger explosions have created craters in a north-south pattern (between Norris and Mammoth Hot Springs). It is estimated that all of the known hydrothermal craters were created between 14,000 and 3,000 years ago. Volcanologists believe no magma has ever broken through the fragile crust of Yellowstone Park or stirred the movement of magma in the reservoir beneath Yellowstone. These phenomena are now considered to be mutually exclusive events; hydrothermal explosions are not correlated with volcanism, although throughout the world all hydrothermal systems are heated and caused by magma.\n\n"}
{"id": "13680406", "url": "https://en.wikipedia.org/wiki?curid=13680406", "title": "Ion vibration current", "text": "Ion vibration current\n\nThe ion vibration current (IVI) and the associated ion vibration potential is an electric signal that arises when an acoustic wave propagates through a homogeneous fluid.\n\nHistorically, the IVI was the first known electroacoustic phenomenon. It was predicted by Peter Debye in 1933.\n\nWhen a longitudinal sound wave travels through a solvent, the associated pressure gradients push the fluid particles back and forth, and it is easy in practice to create such accelerations that measure thousands or millions of g's. If a solute molecule is more dense or less dense than the surrounding liquid, then in this accelerating environment, the molecule will move relative to the surrounding liquid. This relative motion is essentially the same phenomenon that occurs in a centrifuge, or more simply, it is essentially the same phenomenon that occurs when low-density objects float to the top of a glass of water, and high-density particles sink to the bottom (see the equivalence principle, which states that gravity is just like any other acceleration). The amount of relative motion depends on the balance between the molecule's effective mass (which includes both the mass of the molecule itself and any solvent molecules that are so tightly bound to the molecule that they follow along with the molecule's motion), its effective volume (related to buoyant force), and the viscous drag (friction) between the molecule and the surrounding fluid.\n\nIVI concerns the case where the particles in question are anions and cations. In general, they will have different amounts of motion relative to the fluid during the sound wave oscillations, and that discrepancy creates an alternating electric potential between various points in a sound wave.\n\nThis effect was extensively used in the 1950s and 1960s for characterizing ion solvation. These works are mostly associated with the names of Zana and Yaeger, who published a review of their studies in 1982.\n"}
{"id": "10181708", "url": "https://en.wikipedia.org/wiki?curid=10181708", "title": "Ionospheric heater", "text": "Ionospheric heater\n\nAn ionospheric heater, or an ionospheric HF pump facility, is a powerful radio wave transmitter with an array of antennas which is used for research of plasma turbulence, the ionosphere and upper atmosphere. These transmitters operate in the high frequency (HF) range (3-30 MHz) at which radio waves are reflected from the ionosphere back to the ground. With such facilities a range of plasma turbulence phenomena can be excited in a semi-controlled fashion from the ground, during conditions when the ionosphere is naturally quiet and not perturbed by for example aurora. This stimulus-response type of research complements passive observations of naturally excited phenomena to learn about the ionosphere and upper atmosphere.\n\nThe plasma turbulence phenomena that are studied include different types on nonlinear wave interactions, in which different waves in the plasma couple and interact with the transmitted radio wave, formation and self organization of filamentary plasma structures, as well as electron acceleration. The turbulence is diagnosed by for example incoherent scatter radar, by detecting the weak electromagnetic emissions from the turbulence and optical emissions. The optical emissions result from the excitation of atmospheric atoms and molecules by electrons that have been accelerated in the plasma turbulence. As this process is the same as for the aurora, the optical emission excited by HF waves have sometimes been referred to as artificial aurora, although sensitive cameras are needed to detect these emissions, which is not the case for the real aurora.\n\nIonospheric HF pump facilities need to be sufficiently powerful to provide the possibility for plasma turbulence studies, although any radio wave that propagates in the ionosphere affects it by heating the electrons. That radio waves affect the ionosphere was discovered already in the 1930s with the Luxemburg effect. Although the research facilities need to have powerful transmitters, the power flux in the ionosphere for the most powerful facility (HAARP) is below 0.03 W/m. This gives an energy density in the ionosphere that is less than 1/100 of the thermal energy density of the ionospheric plasma itself. The power flux may also be compared with the solar flux at the Earth's surface of about 1.5 kW/m. During aurora generally no ionospheric effects can be observed with the HF pump facilities as the radio wave power is strongly absorbed by the naturally heated ionosphere.\n\n"}
{"id": "990247", "url": "https://en.wikipedia.org/wiki?curid=990247", "title": "KT66", "text": "KT66\n\nKT66 is the designator for a beam tetrode vacuum tube introduced by Marconi-Osram Valve Co. Ltd. (M-OV) of Britain in 1937.\n\nThe KT66 is the direct descendant of the \"Harries Valve\" developed by British engineer J. Owen Harries and marketed by the Hivac Co. Ltd. in 1935. Harries is believed to be the first engineer to discover the \"critical distance\" effect, which maximized the efficiency of a power tetrode by positioning its anode at a distance which is a specific multiple of the screen grid-cathode distance. This design also minimized interference of secondary emission electrons dislodged from the anode.\n\nEMI engineers Cabot Bull and Sidney Rodda improved the Harries design with a pair of beam plates, connected to the cathode, which directed the electron streams into two narrow areas and also acted like a suppressor grid to deflect some secondary electrons back to the anode. The beam tetrode design was also undertaken to avoid the patents which the giant Philips firm held on power pentodes in Europe. Because this overall design eliminated the \"tetrode kink\" in the lower parts of the tetrode's voltage-current characteristic curves (which sometimes caused tetrode amplifiers to become unstable), M-OV marketed this tube family as the \"KT\" series, standing for \"kinkless tetrode\".\n\nA number of different KT tubes were later marketed by M-OV. Some, but not all, were versions of existing American beam tetrode tubes or European power pentodes, such as the KT66 (6L6), KT77 (EL34 and 6CA7), KT88 (6550), and KT63 (6V6).\n\nAlthough the RCA 6L6 of 1936 (the result of a license agreement between RCA and M-OV) was the first true \"beam power tube\" on the market, the later KT66 became almost equally famous, at least in Europe. The two tubes were nearly interchangeable, except that the KT66 was somewhat more rugged than the early metal 6L6.\n\nThe KT66 was very popular in European radios and audio amplifiers. It was the standard output tube in the classic Quad II (1952, a version of which is still being manufactured today) and in the LEAK Type 15 (1945) and TL/12 (1948), both among the earliest British hi-fi amplifiers. Because of their excellent electrical characteristics and overload tolerance, KT66s are preferred by some guitar players for use in guitar amps in place of 6L6GC. However, the plate dissipation of the 6L6GC, at 30W, exceeds the KT66's 25W, and adjustment of the amplifier's bias is necessary.\n\nM-OV ceased glass vacuum tube manufacturing in 1988; their old audio tube types became valuable collectibles. In 2004 original M-OV KT66 tubes (bearing the official \"Genalex\" marketing brand that M-OV used outside the UK), unused in original carton, sold for US$250. KT66 tubes continued to be manufactured at Reflektor Saratov in Russia, JJ Electronic in Slovakia, and at Liuzhou in China.\n\nSome modern Russian manufacture Sovtek KT66 tubes are actually 6L6GC tubes in a KT66 style bottle. While these tubes have the same pinout and minimum tolerances required of a KT66 tube, they do not have the performance characteristics of a true kinkless tetrode KT66 tube.\n\nBy contrast the very latest Russian manufactured tubes (2012) not only carry the same internal electrode structure as the original KT66 (they now look the same) they also have the same rugged electrical characteristics and can withstand a high voltage on grid 2 comparable to the anode voltage rating, allowing greater power output afforded by higher voltage capability when run in ultralinear connection.\n\n\n\n"}
{"id": "46728440", "url": "https://en.wikipedia.org/wiki?curid=46728440", "title": "Lake Gala National Park", "text": "Lake Gala National Park\n\nThe Lake Gala National Park (), established on March 5, 2005, is a national park located within Edirne Province in Marmara Region of Turkey.\n\nThe national park covers an area consisting of Lake Pamuklu and Lake Küçük Gala within the boundaries of İpsala and Enez districts. In 1991, of land was declared a nature reserve. In 2002, biologists from Trakya University in Edirne demanded transformation of the protected area into a national park status due to pollution of lakes by pesticide waste and fertilizer, used in agriculture, as well as uncontrolled fishing activities and bird poaching that reach a level of massacre. In 2005, the area was enlarged to , and it was established as a national park. Lake Gala National Park is an ecosystem of wetland, lake and forest. It is a habitat for various plant and animal species.\n\nThe protected area is administered by the Directorate-General of Nature Protection and National Parks () of the Ministry of Environment and Forest.\n\nThe park is quite rich of bird genera. Some 163 bird species are observed in the area, of which 46 are resident, 27 winter migratory and 90 summer migratory birds. Best time to observe most of the bird species all together is between April and May in the spring, and September and October in the fall.\n\nThe two lakes are home to 16 fish genera, including European eel (\"Anguilla anguilla, L.\"), pike perch (\"Stizostedion lucioperca L.\"), common carp (\"Cyprinus carpio L.\") and northern pike (\"Esox lucius\"), which are of high economic value.\n"}
{"id": "43024", "url": "https://en.wikipedia.org/wiki?curid=43024", "title": "Levee", "text": "Levee\n\nA levee (), dike, dyke, embankment, floodbank or stopbank is an elongated naturally occurring ridge or artificially constructed fill or wall, which regulates water levels. It is usually earthen and often parallel to the course of a river in its floodplain or along low-lying coastlines.\n\nSpeakers of American English (notably in the Midwest and Deep South), use the word \"levee\", from the French word \"levée\" (from the feminine past participle of the French verb \"lever\", \"to raise\"). It originated in New Orleans a few years after the city's founding in 1718 and was later adopted by English speakers. The name derives from the trait of the levee's ridges being \"raised\" higher than both the channel and the surrounding floodplains.\n\nThe modern word \"dike\" or \"dyke\" most likely derives from the Dutch word \"dijk\", with the construction of dikes in Frisia (now part of the Netherlands and Germany) well attested as early as the 11th century. The long Westfriese Omringdijk, completed by 1250, formed by connecting existing older dikes. The Roman chronicler Tacitus mentions that the rebellious Batavi pierced dikes to flood their land and to protect their retreat (AD 70). The word \"dijk\" originally indicated both the trench and the bank. It closely parallels the English verb \"to dig\".\n\nIn Anglo-Saxon, the word \"dic\" already existed and was pronounced as \"dick\" in northern England and as \"ditch\" in the south. Similar to Dutch, the English origins of the word lie in digging a trench and forming the upcast soil into a bank alongside it. This practice has meant that the name may be given to either the excavation or to the bank. Thus Offa's Dyke is a combined structure and Car Dyke is a trench - though it once had raised banks as well. In the midlands and north of England, and in the United States, a dike is what a ditch is in the south, a property-boundary marker or small drainage-channel. Where it carries a stream, it may be called a running dike as in \"Rippingale Running Dike\", which leads water from the catchwater drain, Car Dyke, to the South Forty Foot Drain in Lincolnshire (TF1427). The Weir Dike is a soak dike in Bourne North Fen, near Twenty and alongside the River Glen, Lincolnshire. In the Norfolk and Suffolk Broads, a dyke may be a drainage ditch or a narrow artificial channel off a river or broad for access or mooring, some longer dykes being named, e.g. Candle Dyke.\n\nIn parts of Britain, particularly Scotland, a dyke may be a field wall, generally made with dry stone.\n\nThe main purpose of artificial levees is to prevent flooding of the adjoining countryside and to slow natural course changes in a waterway to provide reliable shipping lanes for maritime commerce over time; they also confine the flow of the river, resulting in higher and faster water flow. Levees can be mainly found along the sea, where dunes are not strong enough, along rivers for protection against high-floods, along lakes or along polders. Furthermore, levees have been built for the purpose of empoldering, or as a boundary for an inundation area. The latter can be a controlled inundation by the military or a measure to prevent inundation of a larger area surrounded by levees. Levees have also been built as field boundaries and as military defences. More on this type of levee can be found in the article on dry-stone walls.\n\nLevees can be permanent earthworks or emergency constructions (often of sandbags) built hastily in a flood emergency. When such an emergency bank is added on top of an existing levee it is known as a \"cradge\".\n\nSome of the earliest levees were constructed by the Indus Valley Civilization (in Pakistan and North India from circa 2600 BC) on which the agrarian life of the Harappan peoples depended. Levees were also constructed over 3,000 years ago in ancient Egypt, where a system of levees was built along the left bank of the River Nile for more than , stretching from modern Aswan to the Nile Delta on the shores of the Mediterranean. The Mesopotamian civilizations and ancient China also built large levee systems. Because a levee is only as strong as its weakest point, the height and standards of construction have to be consistent along its length. Some authorities have argued that this requires a strong governing authority to guide the work, and may have been a catalyst for the development of systems of governance in early civilizations. However, others point to evidence of large scale water-control earthen works such as canals and/or levees dating from before King Scorpion in Predynastic Egypt, during which governance was far less centralized. \n\nAnother example of a historical levee that protected the growing city-state of Mēxihco-Tenōchtitlan and the neighbouring city of Tlatelōlco, was constructed during the early 1400s, under the supervision of the tlahtoani of the altepetl Texcoco, Nezahualcoyotl. Its function was to separate the brackish waters of Lake Texcoco (ideal for the agricultural technique \"Chināmitls\") from the fresh potable water supplied to the settlements. However, after the Europeans destroyed Tenochtitlan, the levee was also destroyed and flooding became a major problem, which resulted in the majority of The Lake to be drained in the 17th Century.\n\nLevees are usually built by piling earth on a cleared, level surface. Broad at the base, they taper to a level top, where temporary embankments or sandbags can be placed. Because flood discharge intensity increases in levees on both river banks, and because silt deposits raise the level of riverbeds, planning and auxiliary measures are vital. Sections are often set back from the river to form a wider channel, and flood valley basins are divided by multiple levees to prevent a single breach from flooding a large area. A levee made from stones laid in horizontal rows with a bed of thin turf between each of them is known as a \"spetchel\".\n\nArtificial levees require substantial engineering. Their surface must be protected from erosion, so they are planted with vegetation such as Bermuda grass in order to bind the earth together. On the land side of high levees, a low terrace of earth known as a \"banquette\" is usually added as another anti-erosion measure. On the river side, erosion from strong waves or currents presents an even greater threat to the integrity of the levee. The effects of erosion are countered by planting suitable vegetation or installing stones, boulders, weighted matting or concrete revetments. Separate ditches or drainage tiles are constructed to ensure that the foundation does not become waterlogged.\n\nProminent levee systems have been built along the Mississippi River and Sacramento River in the United States, and the Po, Rhine, Meuse River, Rhône, Loire, Vistula, the delta formed by the Rhine, Maas/Meuse and Scheldt in the Netherlands and the Danube in Europe. During the Chinese Warring States period, the Dujiangyan irrigation system was built by the Qin as a water conservation and flood control project. The system's infrastructure is located on the Minjiang (), which is the longest tributary of the Chang Jiang, in Sichuan, China.\n\nThe Mississippi levee system represents one of the largest such systems found anywhere in the world. It comprises over of levees extending some along the Mississippi, stretching from Cape Girardeau, Missouri, to the Mississippi Delta. They were begun by French settlers in Louisiana in the 18th century to protect the city of New Orleans. The first Louisiana levees were about high and covered a distance of about along the riverside. The U.S. Army Corps of Engineers, in conjunction with the Mississippi River Commission, extended the levee system beginning in 1882 to cover the riverbanks from Cairo, Illinois to the mouth of the Mississippi delta in Louisiana. By the mid-1980s, they had reached their present extent and averaged in height; some Mississippi levees are as high as . The Mississippi levees also include some of the longest continuous individual levees in the world. One such levee extends southwards from Pine Bluff, Arkansas, for a distance of some .\n\nThe United States Army Corps of Engineers (USACE) recommends and supports Cellular Confinement technology (geocells) as a best management practice. Particular attention is given to the matter of surface erosion, overtopping prevention and protection of levee crest and downstream slope. Reinforcement with geocells provides tensile force to the soil to better resist instability.\n\nArtificial levees can lead to an elevation of the natural river bed over time; whether this happens or not and how fast, depends on different factors, one of them being the amount and type of the bed load of a river. Alluvial rivers with intense accumulations of sediment tend to this behavior. Examples of rivers where artificial levees led to an elevation of the river bed, even up to a point where the river bed is higher than the adjacent ground surface behind the levees, are found for the Yellow River in China and the Mississippi in the USA.\n\nLevees are very common on the marshlands bordering the Bay of Fundy in New Brunswick and Nova Scotia Canada. The Acadians who settled the area can be credited with the original construction of many of the levees in the area, created for the purpose of farming the fertile tidal marshlands. These levees are referred to as dykes. They are constructed with hinged sluice gates that open on the falling tide to drain freshwater from the agricultural marshlands, and close on the rising tide to prevent seawater from entering behind the dyke. These sluice gates are called \"aboiteaux\". In the Lower Mainland around the city of Vancouver, British Columbia, there are levees (known locally as dikes, and also referred to as \"the sea wall\") to protect low-lying land in the Fraser River delta, particularly the city of Richmond on Lulu Island. There are also dikes to protect other locations which have flooded in the past, such as the Pitt Polder, land adjacent to the Pitt River and other tributary rivers.\n\nCoastal flood prevention levees are also common along the inland coastline behind the Wadden Sea, an area devastated by many historic floods. Thus the peoples and governments have erected increasingly large and complex flood protection levee systems to stop the sea even during storm floods. The biggest of these are of course the huge levees in the Netherlands, which have gone beyond just defending against floods, as they have aggressively taken back land that is below mean sea level. \n\nThese typically man-made hydraulic structures are situated to protect against erosion. They are typically placed in alluvial rivers perpendicular, or at an angle, to the bank of the channel or the revetment, and are used widely along coastlines. There are two common types of spur dyke, permeable and impermeable, depending on the materials used to construct them.\n\nNatural levees commonly form around lowland rivers and creeks without human intervention. They are elongate ridges of mud and/or silt that form on the river floodplains immediately adjacent to the cut banks. Like artificial levees, they act to reduce the likelihood of floodplain inundation.\n\nDeposition of levees is a natural consequence of the flooding of meandering rivers which carry high proportions of suspended sediment in the form of fine sands, silts, and muds. Because the carrying capacity of a river depends in part on its depth, the sediment in the water which is over the flooded banks of the channel is no longer capable of keeping the same amount of fine sediments in suspension as the main thalweg. The extra fine sediments thus settle out quickly on the parts of the floodplain nearest to the channel. Over a significant number of floods, this will eventually result in the building up of ridges in these positions, and reducing the likelihood of further floods and episodes of levee building.\n\nIf aggradation continues to occur in the main channel, this will make levee overtopping more likely again, and the levees can continue to build up. In some cases this can result in the channel bed eventually rising above the surrounding floodplains, penned in only by the levees around it; an example is the Yellow River in China near the sea, where oceangoing ships appear to sail high above the plain on the elevated river.\n\nLevees are common in any river with a high suspended sediment fraction, and thus are intimately associated with meandering channels, which also are more likely to occur where a river carries large fractions of suspended sediment. For similar reasons, they are also common in tidal creeks, where tides bring in large amounts of coastal silts and muds. High will cause flooding, and result in the building up of levees.\n\nBoth natural and man-made levees can fail in a number of ways. Factors that cause levee failure include overtopping, erosion, structural failures, and levee saturation. The most frequent (and dangerous) is a \"levee breach\". Here, a part of the levee actually breaks or is eroded away, leaving a large opening for water to flood land otherwise protected by the levee. A breach can be a sudden or gradual failure, caused either by surface erosion or by subsurface weakness in the levee. A breach can leave a fan-shaped deposit of sediment radiating away from the breach, described as a crevasse splay. In natural levees, once a breach has occurred, the gap in the levee will remain until it is again filled in by levee building processes. This increases the chances of future breaches occurring in the same location. Breaches can be the location of meander cutoffs if the river flow direction is permanently diverted through the gap.\n\nSometimes levees are said to fail when water \"overtops\" the crest of the levee. This will cause flooding on the floodplains, but because it does not damage the levee, it has fewer consequences for future flooding.\n\nAmong various failure mechanisms that cause levee breaches, soil erosion is found to be one of the most important factors. Predicting soil erosion and scour generation when overtopping happens is important in order to design stable levee and floodwalls. There have been numerous studies to investigate the erodibility of soils. Briaud et al. (2008) used Erosion Function Apparatus (EFA) test to measure the erodibility of the soils and afterwards by using Chen 3D software, numerical simulations were performed on the levee to find out the velocity vectors in the overtopping water and the generated scour when the overtopping water impinges the levee. By analyzing the results from EFA test, an erosion chart to categorize erodibility of the soils was developed. Hughes and Nadal in 2009 studied the effect of combination of wave overtopping and storm surge overflow on the erosion and scour generation in levees. The study included hydraulic parameters and flow characteristics such as flow thickness, wave intervals, surge level above levee crown in analyzing scour development. According to the laboratory tests, empirical correlations related to average overtopping discharge were derived to analyze the resistance of levee against erosion. These equations could only fit to the situation similar to the experimental tests while they can give a reasonable estimation if applied to other conditions.\n\nOsouli et al. (2014) and Karimpour et al. (2015) conducted lab scale physical modeling of levees to evaluate score characterization of different levees due to floodwall overtopping.\n\nAnother approach applied to prevent levee failures is electrical resistivity tomography (ERT). This non-destructive geophysical method can detect in advance critical saturation areas in embankments. ERT can thus be used in monitoring of seepage phenomena in earth structures and act as an early warning system, e.g. in critical parts of leeves or embankments.\n\n\n"}
{"id": "5893812", "url": "https://en.wikipedia.org/wiki?curid=5893812", "title": "List of Sites of Special Scientific Interest in West Sussex", "text": "List of Sites of Special Scientific Interest in West Sussex\n\nThis is a list of the Sites of Special Scientific Interest (SSSIs) in West Sussex, a county in South East England.\n\n, there are 78 sites designated within this Area of Search, of which have been designated for their biological interest, 19 for their geological interest, and 5 for both biological and geological interest.\n\nIn England, the body responsible for designating SSSIs is Natural England, which selects sites because of their flora, fauna, geological or physiographical features. Natural England took over the role of designating and managing SSSIs from English Nature in October 2006 when it was formed from the amalgamation of English Nature, parts of the Countryside Agency and the Rural Development Service.\n\nThe data in this table is taken from Natural England's website in the form of citation sheets for each SSSI.\n\n"}
{"id": "51919024", "url": "https://en.wikipedia.org/wiki?curid=51919024", "title": "List of coolest stars", "text": "List of coolest stars\n\nThis is a list of coolest stars discovered, arranged by decreasing temperature. The stars with temperatures lower than 3,000 K are included.\n\n"}
{"id": "644982", "url": "https://en.wikipedia.org/wiki?curid=644982", "title": "List of mountains in Nepal", "text": "List of mountains in Nepal\n\nNepal contains part of the Himalayas, the highest mountain range in the world. Eight of the fourteen eight-thousanders are located in the country, either in whole or shared across a border with China or India. Nepal has the highest mountain in the world, Mount Everest.\n\nNorth of the Greater Himalayas in western Nepal, ~6,000 metre \"Tibetan Border Ranges\" form the Ganges-Brahmaputra divide, which the international border generally follows. South of the Greater Himalayas, Nepal has a \"High Mountain\" region of ~4,000 metre summits, then the \"Middle Hills\" and Mahabharat Range with 1,500 to 3,000 metre summits. South of the Mahabharats, an outer range of foothills with ~1,000 metre summits is called the Siwaliks or \"Churiya Hills\".\n\n"}
{"id": "1888461", "url": "https://en.wikipedia.org/wiki?curid=1888461", "title": "List of national parks of Ethiopia", "text": "List of national parks of Ethiopia\n\nThe national parks of Ethiopia include:\n\nOther protected areas include the Babile Elephant, Senkele, and Deara Wildlife Sanctuaries and the Tama and Chelbi Wildlife Reserves.\n\n\n"}
{"id": "454979", "url": "https://en.wikipedia.org/wiki?curid=454979", "title": "List of national parks of Japan", "text": "List of national parks of Japan\n\nJapan established its first or public parks in 1873 (Asakusa Park, Asukayama Park, Fukagawa Park, Shiba Park, and Ueno Park). In 1911 local citizens petitioned that the shrines and forests of Nikkō be placed under public protection. In 1929 the National Parks Association was formed. In 1931 the first was passed. After much study and survey, in March 1934 the first parks were established — Setonaikai, Unzen and Kirishima — with five more in December and a further four two years later. Three further parks were established under the old National Parks Law, in colonial Taiwan in 1937: the Tatun National Park (the smallest in Japan); Tsugitaka-Taroko National Park, (the largest); and Niitaka-Arisan National Park (with the highest mountain in then Japan).\n\nIse-Shima was the first to be created after the war, and a further seven had been added by 1955.\n\nIn 1957 the Natural Parks Law replaced the earlier National Parks Law, allowing for three categories: the National, Quasi-National, and Prefectural Natural Parks. With minor amendments this established the framework that operates today.\n\nAs of 1 April 2014, there were 31 National Parks and 56 Quasi-National Parks, with the National Parks covering 20,996 km² (5.6% of the land area) and the Quasi-National Parks 13,592 km² (3.6% of the land area). In addition, there were 314 Prefectural Parks covering 19,726 km² (5.2% of the land area). On 27 March 2015, the 32nd National Park was established, Myōkō-Togakushi Renzan National Park, on 15 September 2016, the 33rd, Yanbaru National Park, and on 7 March 2017, the 34th, Amami Guntō National Park, subsuming Amami Guntō Quasi-National Park. On 25 March 2016, a further Quasi-National Park was established, Kyoto Tamba Kogen Quasi-National Park.\n\nThe area of each National and Quasi-National Park is divided into ordinary, special and marine park zones. Special zones are further subdivided into special protection and class I, II, and III special zones, restricting access and use for preservation purposes. The state owns only approximately half of the land in the parks.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "25315287", "url": "https://en.wikipedia.org/wiki?curid=25315287", "title": "List of open clusters", "text": "List of open clusters\n\nThis is a list of open clusters located in the Milky Way. An open cluster is a gravitationally bound association of up to a few thousand stars that all formed from the same giant molecular cloud. There are over 1,000 known open clusters in the Milky Way galaxy, but the actual total may be up to ten times higher. The estimated half lives of clusters, after which half the original cluster members will have been lost, range from 150 million to 800 million years, depending on the original density.\n\n"}
{"id": "26421705", "url": "https://en.wikipedia.org/wiki?curid=26421705", "title": "List of vulnerable plants", "text": "List of vulnerable plants\n\nAs of September 2016, the International Union for Conservation of Nature (IUCN) lists 5430 vulnerable plant species. 25% of all evaluated plant species are listed as vulnerable. \nThe IUCN also lists 244 subspecies and 235 varieties as vulnerable. No subpopulations of plants have been evaluated by the IUCN.\n\nFor a species to be assessed as vulnerable to extinction the best available evidence must meet quantitative criteria set by the IUCN designed to reflect \"a high risk of extinction in the wild\". \"Endangered\" and \"critically endangered\" species also meet the quantitative criteria of \"vulnerable\" species, and are listed separately. See: List of endangered plants, List of critically endangered plants. Vulnerable, endangered and critically endangered species are collectively referred to as \"threatened species\" by the IUCN.\n\nAdditionally 1674 plant species (7.6% of those evaluated) are listed as \"data deficient\", meaning there is insufficient information for a full assessment of conservation status. As these species typically have small distributions and/or populations, they are intrinsically likely to be threatened, according to the IUCN. While the category of \"data deficient\" indicates that no assessment of extinction risk has been made for the taxa, the IUCN notes that it may be appropriate to give them \"the same degree of attention as threatened taxa, at least until their status can be assessed.\"\n\nThis is a complete list of vulnerable plant species, subspecies and varieties evaluated by the IUCN.\nThere are 24 bryophyte species assessed as vulnerable.\nThere are 83 species and one subspecies of pteridophyte assessed as vulnerable.\nThere are 69 species in the class Polypodiopsida assessed as vulnerable.\nThere are 61 species in the order Polypodiales assessed as vulnerable.\n\n\nSpecies\n\nSubspecies\n\nThere are 156 species, seven subspecies, and 34 varieties of gymnosperm assessed as vulnerable.\n\nSpecies\n\nSubspecies\n\nSpecies\n\nSubspecies\n\nVarieties\nThere are 4545 species, 230 subspecies, and 197 varieties of dicotyledon assessed as vulnerable.\nThere are 24 species in the order Piperales assessed as vulnerable.\n\nThere are 51 species and two subspecies in Campanulales assessed as vulnerable.\n\nSpecies\n\nSubspecies\n\nThere are 197 species, three subspecies, and nine varieties in Theales assessed as vulnerable.\n\nSpecies\n\nSubspecies\n\nSpecies\n\nVarieties\n\nSpecies\n\nVarieties\n\nSpecies\n\nVarieties\nThere are 114 species, 13 subspecies, and one variety in the order Malvales assessed as vulnerable.\n\nSpecies\n\nSubspecies\n\n\nSpecies\n\nSubspecies\n\nVarieties\n\nSpecies\n\nSubspecies\nVarieties\n\nSpecies\n\nSubspecies\n\nSpecies\n\nSubspecies\nVarieties\n\nSpecies\n\nSubspecies\nVarieties\nThere are 26 species and one subspecies in the order Proteales assessed as vulnerable.\n\nSpecies\n\nSubspecies\n\n\nSpecies\n\nSubspecies\n\nVarieties\nThere are 86 species, one subspecies, and three varieties in Violales assessed as vulnerable.\n\nSpecies\n\nSubspecies\nVarieties\n\nSpecies\n\nVarieties\n\nThere are 236 species, five subspecies, and 33 varieties in Euphorbiales assessed as vulnerable.\n\n\nSpecies\n\nSubspecies\n\nVarieties\nThere are 136 species and three varieties in the order Laurales assessed as vulnerable.\n\nSpecies\n\nVarieties\nThere are 49 species in the order Cucurbitales assessed as vulnerable.\nThere are 226 species, eight subspecies, and five varieties in Ebenales assessed as vulnerable.\n\nSpecies\n\nSubspecies\nVarieties\n\nSpecies\n\nSubspecies\n\nVarieties\n\nThere are 102 species, three subspecies, and ten varieties in the order Celastrales assessed as vulnerable.\n\nSpecies\n\nSubspecies\nVarieties\n\nSpecies\n\nSubspecies\nVarieties\nThere are 357 species, 13 subspecies, and 11 varieties in the order Myrtales assessed as vulnerable.\n\nSpecies\n\nSubspecies\n\nVarieties\n\nSpecies\n\nSubspecies\n\nSpecies\n\nSubspecies\n\nSpecies\n\nSubspecies\nVarieties\n\nThere are 345 species, ten subspecies, and 14 varieties in the order Sapindales assessed as vulnerable.\n\nSpecies\n\nSubspecies\nVarieties\n\nSpecies\nSubspecies\n\nVarieties\n\nSpecies\n\nVarieties\n\nSpecies\n\nVarieties\n\nSpecies\n\nSubspecies\n\nSpecies\n\nSubspecies\nVarieties\n\nSpecies\n\nSubspecies\n\nVarieties\n\nThere are 270 species, 55 subspecies, and 28 varieties in the order Magnoliales assessed as vulnerable.\n\nSpecies\nSubspecies\n\nSpecies\n\nSubspecies\n\nVarieties\n\nSpecies\n\nSubspecies\n\nSpecies\n\nSubspecies\n\nVarieties\n\nThere are 41 species and five subspecies in Capparales assessed as vulnerable.\n\nSpecies\n\nSubspecies\n\nSubspecies\n\nThere are 108 species in the order Apiales assessed as vulnerable.\nThere are 108 species and one variety in the order Gentianales assessed as vulnerable.\n\nSpecies\n\nVarieties\n\nThere are 118 species, 13 subspecies, and nine varieties in the order Rosales assessed as vulnerable.\n\nSpecies\n\nSubspecies\nVarieties\n\nSpecies\n\nVarieties\n\nSpecies\n\nSubspecies\n\nSpecies\n\nVarieties\n\nSpecies\n\nSubspecies\n\nVarieties\n\nSpecies\n\nSubspecies\n\nThere are 50 species and one variety in Primulales assessed as vulnerable.\n\nSpecies\n\nVarieties\n\n\nSpecies\n\nSubspecies\n\nThere are 46 species, five subspecies, and three varieties in Urticales assessed as vulnerable.\n\nSpecies\n\nVarieties\n\nSpecies\n\nSubspecies\n\nSpecies\n\nSubspecies\n\nVarieties\n\nSpecies\n\nVarieties\n\nThere are 33 species and two subspecies in the order Solanales assessed as vulnerable.\n\nSpecies\n\nSubspecies\n\n\nThere are 220 species, 14 subspecies, and two varieties in the order Scrophulariales assessed as vulnerable.\n\nSpecies\n\nSubspecies\nVarieties\n\nSpecies\n\nSubspecies\n\nVarieties\n\nSpecies\n\nSubspecies\nThere are 111 species, two subspecies, and four varieties in the order Lamiales assessed as vulnerable.\n\nSpecies\n\nVarieties\n\nSpecies\n\nSubspecies\n\nSpecies\n\nVarieties\n\nThere are 41 species in Nepenthales assessed as vulnerable.\n\n\nSpecies\n\nVarieties\n\nThere are 36 species, one subspecies, and one variety in the order Ranunculales assessed as vulnerable.\n\nSpecies\n\nSubspecies\nVarieties\n\nThere are 17 species and one variety in the order Ericales assessed as vulnerable.\n\nSpecies\n\nVarieties\n\nSpecies\n\nVarieties\n\nSpecies\n\nSubspecies\n\nVarieties\nThere are 190 species, 12 subspecies, and one variety in the order Caryophyllales assessed as vulnerable.\n\nSpecies\n\nSubspecies\n\nSpecies\n\nSubspecies\n\nVarieties\n\n\nThere are 45 species and one subspecies in the order Fagales assessed as vulnerable.\n\nSpecies\n\nSubspecies\n\nSpecies\n\nSubspecies\n\nThere are 619 species, six subspecies, and four varieties of monocotyledon assessed as vulnerable.\n\nSpecies\n\nVarieties\n\nThere are 114 species and three subspecies in Orchidales assessed as vulnerable.\n\nSpecies\n\nSubspecies\n\nSpecies\n\nSubspecies\nThere are 42 species in the order Zingiberales assessed as vulnerable.\n\nThere are 78 species and two varieties in Cyperales assessed as vulnerable.\n\nSpecies\n\nVarieties\n\n"}
{"id": "5307536", "url": "https://en.wikipedia.org/wiki?curid=5307536", "title": "Lithoautotrophy", "text": "Lithoautotrophy\n\nLithoautotrophy is a special type of chemoautotrophy found exclusively in Archaea and Bacteria. Lithoautotrophic organisms utilize inorganic compounds as energy sources, typically from mineral in origin. The word \"lithoautotrophy\" means \"to feed it- or oneself from stone\". In ancient Greek, \"lithos\" means stone or rock, \"autos\" means self, and \"trophein\" to feed.\n\n"}
{"id": "5073565", "url": "https://en.wikipedia.org/wiki?curid=5073565", "title": "Mass distribution", "text": "Mass distribution\n\nIn physics and mechanics, mass distribution is the spatial distribution of mass within a solid body. In principle, it is relevant also for gases or liquids, but on earth their mass distribution is almost homogeneous.\n\nIn astronomy mass distribution has decisive influence on the development e.g. of nebulae, stars and planets.\nThe mass distribution of a solid defines its center of gravity and influences its dynamical behaviour - e.g. the oscillations and eventual rotation.\n\nA mass distribution can be modeled as a measure. This allows point masses, line masses, surface masses, as well as masses given by a volume density function. Alternatively the latter can be generalized to a distribution. For example, a point mass is represented by a delta function defined in 3-dimensional space. A surface mass on a surface given by the equation \"f(x,y,z)\" = 0 may be represented by a density distribution \"g(x,y,z) δ (f(x,y,z))\", where formula_1 is the mass per unit area.\n\nThe mathematical modelling can be done by potential theory, by numerical methods (e.g. a great number of mass points), or by theoretical equilibrium figures.\n\nIn geology the aspects of rock density are involved.\n\nRotating solids are affected considerably by the mass distribution, either if they are homogeneous or inhomogeneous - see Torque, moment of inertia, wobble, imbalance and stability.\n\n\n"}
{"id": "4693975", "url": "https://en.wikipedia.org/wiki?curid=4693975", "title": "MidSTAR-1", "text": "MidSTAR-1\n\nMidSTAR-1 is an artificial satellite produced by the United States Naval Academy Small Satellite Program. It was sponsored by the United States Department of Defense (DoD) Space Test Program (STP), and was launched on March 8, 2007 at 11:32 Eastern Standard Time, aboard an Atlas V expendable launch vehicle from Cape Canaveral Air Force Station. MidSTAR-1 flew along with FalconSat 3, STPSat 1, and CFESat as secondary payloads; the primary payload was Orbital Express.\n\nMidSTAR is a general-purpose satellite bus capable of supporting a variety of space missions by easily accommodating a wide range\nof space experiments and instruments. The integration of the experiments with the satellite bus must be accomplished with minimal changes to the satellite bus design. MidSTAR is intended to be a relatively low-cost, quick response platform accommodating small payloads approved by the DoD Space Experiments Review Board (SERB) and awaiting launch through STP.\"\n\nMidSTAR is designed for use on the EELV Secondary Payload Adapter (ESPA) Ring developed by Air Force Research Laboratory (AFRL) for placement on Delta IV or Atlas V expendable launch vehicles. MidSTAR is a Class D spacecraft, produced at minimum cost with a correspondingly higher technical risk in production and operation. It is intentionally simple in design and rugged in construction, using commercial off-the-shelf “plug-and-play” components to the greatest extent possible. Component development and circuit-board level design are accomplished only when necessary.\n\nMidSTAR-1 is the first implementation of the design. It was commissioned by STP to carry the Internet Communications Satellite (ICSat) Experiment for SSP and the Configurable Fault Tolerant Processor (CFTP) Experiment for Naval Postgraduate School (NPS). In addition, MidSTAR-1 carries the Nano Chem Sensor Unit (NCSU) for the National Aeronautics and Space Administration (NASA) Ames Research Center; Eclipse, built by Eclipse Energy Systems, Inc. for NASA Goddard Space Flight Center (GSFC); and the Micro Dosimeter Instrument (MiDN), sponsored by the National Space Biomedical Research Institute (NSBRI) and built by the USNA Department of Aerospace Engineering. The mission is intended to last two years.\n\nThe MidSTAR-1 mission includes a single spacecraft under the command and control of a single satellite ground station (SGS) located at the United States Naval Academy, Annapolis, Maryland. The ground station forwards downlinked data files to the principal investigators via the Internet. The launch segment for MidSTAR-1 utilized an Atlas V launch vehicle through the Space Test Program, placing the satellite in a circular orbit at 496 km altitude, 46 degrees inclination.\n\nThe satellite uses an uplink at 1.767 GHz with an intermediate frequency (IF) of 435 MHz, and a 2.20226 GHz downlink. By utilizing a Gaussian Mean Shift Key modulation, communications with the satellite are achieved at 68.4 kbit/s or higher data rate. The satellite also uses open source software based on the Linux operating system. MidSTAR-1 has no attitude control or determination, no active thermal control, and its mass is 120 kg.\n\nOne hundred percent success would be the successful launch and operation of the satellite with full support for the two primary experiments for two years. Fifty percent success was the successful launch and operation of the satellite with: Full support of one primary experiment for two years; Full support of both primary experiments for one year; or, partial support of both primary experiments for two years. Thirty-three percent success was successful launch of the satellite and full operation of the satellite bus with partial support of any combination of primary and secondary payloads for any length of time.\n\n9 March 2007: MidSTAR-1 flew as part of the STP-1 mission on a United Launch Alliance Atlas V from Cape Canaveral Air Force Station. Liftoff occurred at 0310 UTC; spacecraft separation occurred at 0332 UTC. USNA SGS successfully acquired communications with the spacecraft during the first pass over Annapolis MD at 0459 UTC. The spacecraft was operating nominally in safe mode.\n\n21 March 2007: CFTP turned on at 2217 UTC to add 6 W continuous to the electrical power system load and thus lessen charging stress on the batteries.\n\n28 March 2007: MiDN turned on at approximately 2400 UTC. Spacecraft stopped responding to all ground commands subsequent to this pass.\n\n4 April 2007: First use of \"firecode reset\" of spacecraft at approximately 2130 UTC. This command toggles the reset switch on the MIP-405 processor and reboots the operating system. This reset returned the CFTP and MiDN experiments to \"off\" and cleared all command buffers. At 2324 UTC the spacecraft responded to a \"transmitter on\" command. Telemetry confirmed that the reboot was successful.\n\n5 April 2007: CFTP and MiDN turned back on.\n\n6 April 2007: Selective download of MiDN files retrieved 71 files of 92 bytes each which were delivered to the Principal Investigator (PI). This was the first successful retrieval of science data from the spacecraft. With this milestone, MidSTAR-1 satisfied the criteria of 33% mission success.\n\n26 May 2007: NCSU turned on at approximately 1900 Z.\n\n29 May 2007: First data package delivered to NCSU PI. All four experiments are on and delivering data to the PIs.\n\n18 June 2007: NASA press release announces success of NCSU.\n\n5 September 2007: Spacecraft computer froze as a result of unknown influences, most likely radiation-induced upsets. This happened while the spacecraft was in full sun and with the power drains (30 W) on to prevent battery overcharging. Without the computer to cycle the drains off, the spacecraft remained in a continuous negative net power configuration which eventually drained the batteries. When the battery voltage dropped below 8 V, the electronic switches for the drains defaulted to off, returning the spacecraft to positive net power and allowing the batteries to recharge.\n\n7 September 2007: Once the batteries recharged sufficiently, the computer restarted successfully. Restart occurred 48 hours after the initial event. No telemetry from the spacecraft or any experiment is available for that 48-hour period. Telemetry indicates that normal operation resumed, but all experiments were left off pending post-event analysis and the development of a plan to bring them back online.\n\n12 September 2007: CFTP restarted.\n\n21 September 2007: MiDN restarted.\n\nApril 2009: Contact with MidSTAR-1 lost. Spacecraft ceased transmitting and failed to respond to ground command. Anomaly attributed to failure of battery packs. MidSTAR-1 declared non-operational. MidSTAR-1 fully supported all onboard experiments for two full years, fulfilling the 100% success criteria.\n\nThe MidSTAR-1 frame is an octagonal structure 32.5\" along the long axis, including separation system, and 21.2\"x21.2\" measured side-to-side in cross-section. The deployment mechanism is mounted on the negative x face. The positive x face is reserved for externally mounted experiments. Of the 38\" along the x-axis allowed in the ESPA envelope, 2-4\" are reserved for the deployment mechanism (15-in motorized lightband manufactured by Planetary Systems, Inc.), and 4-6\" are reserved for external experiments. The frame length is 30\". All eight sides of the spacecraft are covered with solar cells in order to maximize the power available. Eight dipole antennas are mounted on the four faces of the spacecraft which \"cut the corners\" of the ESPA envelope, and are therefore positioned within the ESPA envelope rather than coincident with the envelope surface. The remaining sides are mounted with remove-before-flight eyeholes for lifting and transport during ground support.\nMidSTAR-1 has three interior shelves which provide area inside the satellite for mounting of components and payloads. Their locations are determined by the dimensions of the payloads and components. These can be varied in future implementations of the MidSTAR model if necessary, as long as the structure remains within the center of gravity requirements.\nThe load-bearing structure of the octagon consists of the top and bottom decks, connected at the eight corners by stringers. The side panels of the spacecraft are 1/8\" aluminum panels mounted to the stringers with #10 bolts.\n\nThe mission of the Command and Data Handling System (C&DH) is to receive and execute commands; collect, store, and transmit house-keeping data; and support the onboard payloads. The flight computer is designed to control the satellite and manage telemetry and experiment data for a minimum of two years.\n\nThe C&DH system consists of a custom-modified MIP405-3X single board computer which included (i) 133 MHz PowerPC processor; (ii) 128 MB ECC; (iii) 4 RS-232 asynchronous serial ports; (iv) 1 Ethernet Port; (v) a PC/104 bus; (vi) a PC/104+ bus; and, (vi) a 202-D384-X Disc on Chip providing 384 MB of secondary storage. The computer board is supported by an ESCC-104 Synchronous Serial Card with 2 synchronous serial ports, and an EMM-8M-XT Serial Expansion Card with 8 RS-232/422/485 asynchronous serial ports and 8 digital I/O channels. A modified I0485 data acquisition board provides 22 analog telemetry channels and 32 digital I/O channels.\n\nThe decision to use the PowerPC based MIP405 over an x86 based board was based solely on the low power consumption of the board combined with the feature set. The choice was limited to x86, PowerPC, and ARM processor architectures because of a program decision to use the Linux operating system. The MIP405 integrates Ethernet, serial ports, and Disk-on-Chip interface on a single board while providing 128 MB of ECC memory and a powerful processor for under 2 watts. The closest x86 based system with comparable features found consumed 5 watts of power.\n\nThe M-Systems Disk-on-Chip was chosen because it was the \"de facto\" standard flash memory harddisk replacement. Flash memory was chosen over a traditional hard disk to increase reliability and reduce power. The 384 MB version was chosen to provide the storage required for the operating system and still maintain adequate margin.\n\nThe Diamond Systems Emerald-MM-8 was chosen for the asynchronous serial board based on its innate flexibility with any of the 8 ports capable of being configured as RS-232, RS-422, RS-485.\n\nRMV's IO485 data acquisition and control board was chosen for the distributed telemetry system because of built-in support for daisy chaining and handling a large number of boards. The integrated expandability is fundamental to addressing future telemetry issues in later versions of the MidSTAR line.\n\nThe C&DH uses the Linux operating system with a 2.4 series kernel. To create an open software architecture the IP protocol stack was chosen to provide inter process, intra-satellite, and satellite-ground communications. This allowed programs created at different facilities on different hardware to be integrated with minimum difficulty.\n\nAll internal and external communications use internet protocols. TCP is used for all internal satellite communications; UDP or MDP is used on the uplink and downlink.\n\n"}
{"id": "6230269", "url": "https://en.wikipedia.org/wiki?curid=6230269", "title": "Military meteorology", "text": "Military meteorology\n\nMilitary meteorology is meteorology applied to military purposes, by armed forces or other agencies. It is one of the most common fields of employment for meteorologists. \n\nWorld War II brought great advances in meteorology as large-scale military land, sea, and air campaigns were highly dependent on weather, particularly forecasts provided by the Royal Navy, Met Office and USAAF for the Normandy landing and strategic bombing. \n\nUniversity meteorology departments grew rapidly as the military services sent cadets to be trained as weather officers. Wartime technological developments such as radar also proved to be valuable meteorological observing systems. More recently, the use of satellites in space has contributed extensively to military meteorology.\n\nMilitary meteorologists currently operate with a wide variety of military units, from aircraft carriers to special forces.\n\n\nEnlisted meteorology and oceanography forecasters are called Aerographer's Mates.\n\nNaval meteorology and oceanography officers are restricted line officers in the Information Dominance Corps.\n\n\n\n\n"}
{"id": "3589686", "url": "https://en.wikipedia.org/wiki?curid=3589686", "title": "Movement of Animals", "text": "Movement of Animals\n\nMovement of Animals (or On the Motion of Animals; Greek Περὶ ζῴων κινήσεως; Latin \"De Motu Animalium\") is one of Aristotle's major texts on biology. It sets out the general principles of animal locomotion.\n\n"}
{"id": "621348", "url": "https://en.wikipedia.org/wiki?curid=621348", "title": "Natural Resources Canada", "text": "Natural Resources Canada\n\nThe Department of Natural Resources (), operating under the FIP applied title Natural Resources Canada (NRCan), is the ministry of the government of Canada responsible for natural resources, energy, minerals and metals, forests, earth sciences, mapping and remote sensing. It was created in 1995 by amalgamating the now-defunct Departments of Energy, Mines and Resources and Forestry. Natural Resources Canada (NRCan) works to ensure the responsible development of Canada's natural resources, including energy, forests, minerals and metals. NRCan also uses its expertise in earth sciences to build and maintain an up-to-date knowledge base of our landmass and resources. To promote internal collaboration, NRCan has implemented a departmental wide wiki based on MediaWiki. Natural Resources Canada also collaborates with American and Mexican government scientists, along with the Commission for Environmental Cooperation, to produce the North American Environmental Atlas, which is used to depict and track environmental issues for a continental perspective.\n\nUnder the Canadian constitution, responsibility for natural resources belongs to the provinces, not the federal government. However, the federal government has jurisdiction over off-shore resources, trade and commerce in natural resources, statistics, international relations, and boundaries. The current Minister of Natural Resources is Amarjeet Sohi as of July 18, 2018.\n\nThe department is governed by the Resources and Technical Surveys Act, R.S.C., c.R-7 and the Department of Natural Resources Act, S.C. 1994, c. 41.\n\nThe department currently has these sectors:\n\n\nThe following sub-agencies are attached to the department:\n\nActs for which Natural Resources Canada has responsibility\n\n\n\n\n"}
{"id": "9932703", "url": "https://en.wikipedia.org/wiki?curid=9932703", "title": "Nin-imma", "text": "Nin-imma\n\nNin-imma is a Sumerian, Babylonian, and Akkadian fertility goddess and deification of the female sex organs. Her parents are Enki and Ninkurra.\n\nHer name derives from the Sumerian words \"nin\" – goddess, and \"imma\" – water that created everything. \n\n\n"}
{"id": "27164953", "url": "https://en.wikipedia.org/wiki?curid=27164953", "title": "Numerical response", "text": "Numerical response\n\nThe numerical response in ecology is the change in predator density as a function of change in prey density. The term numerical response was coined by M. E. Solomon in 1949. It is associated with the functional response, which is the change in predator's rate of prey consumption with change in prey density. As Holling notes, total predation can be expressed as a combination of functional and numerical response. The numerical response has two mechanisms: the demographic response and the aggregational response. The numerical response is not necessarily proportional to the change in prey density, usually resulting in a time lag between prey and predator populations. For example, there is often a scarcity of predators when the prey population is increasing.\n\nThe demographic response consists of changes in the rates of predator reproduction or survival due to a changes in prey density. The increase in prey availability translates into higher energy intake and reduced energy output. This is different from an increase in energy intake due to increased foraging efficiency, which is considered a functional response. This concept can be articulated in the Lotka-Volterra Predator-Prey Model.\n\nformula_1\n\na = conversion efficiency: the fraction of prey energy assimilated by the predator and turned into new predators\nP = predator density\nV = prey density\nm = predator mortality\n\nDemographic response consists of a change in dP/dt due to a change in V and/or m. For example, if V increases, then predator growth rate (dP/dt) will increase. Likewise if the energy intake increases (due to greater food availability) and a decrease in energy output (from foraging), then predator mortality (m) will decrease and predator growth rate (dP/dt) will increase. In contrast, the functional response consists of a change in conversion efficiency (a) or capture rate (c).\n\nThe relationship between available energy and reproductive efforts can be explained with the life history theory in the trade-off between fecundity and growth/survival. If an organism has more net energy, then the organism will sacrifice less energy dedicated to survival per reproductive effort and will therefore increase its reproduction rate.\n\nIn parasitism, functional response is measured by the rate of infection or laying of eggs in host, rather than the rate of prey consumption as it is measured in predation. Numerical response in parasitism is still measured by the change in number of adult parasites relative to change in host density. Parasites can demonstrate a more pronounced numerical response to changes in host density since there is often a more direct connection (less time lag) between food and reproduction in that both needs are immediately satisfied by its interaction with the host. \n\nThe aggregational response, as defined by Readshaw in 1973, is a change in predator population due to immigration into an area with increased prey population. In an experiment conducted by Turnbull in 1964, he observed the consistent migration of spiders from boxes without prey to boxes with prey. He proved that hunger impacts predator movement.\n\nRiechert and Jaeger studied how predator competition interferes with the direct correlation between prey density and predator immigration. One way this can occur is through exploitation competition: the differential efficiency in use of available resources, for example, an increase in spiders' web size (functional response). The other possibility is interference competition where site owners actively prevent other foragers from coming in vicinity.\n\nThe concept of numerical response becomes practically important when trying to create a strategy for pest control. The study of spiders as a biological mechanism for pest control has driven much of the research on aggregational response. Antisocial predator populations that display territoriality, such as spiders defending their web area, may not display the expected aggregational response to increased prey density.\n\nA credible, simple alternative to the Lotka-Volterra predator-prey model and its common prey dependent generalizations is the ratio dependent or Arditi-Ginzburg model. The two are the extremes of the spectrum of predator interference models. According to the authors of the alternative view, the data show that true interactions in nature are so far from the Lotka-Volterra extreme on the interference spectrum that the model can simply be discounted as wrong. They are much closer to the ratio dependent extreme, so if a simple model is needed one can use the Arditi-Ginzburg model as the first approximation.\n"}
{"id": "8404180", "url": "https://en.wikipedia.org/wiki?curid=8404180", "title": "Puna grassland", "text": "Puna grassland\n\nThe Puna grassland ecoregion, of the montane grasslands and shrublands biome, is found in the central Andes Mountains of South America. It is considered one of the eight Natural Regions in Peru, but extends south, across Bolivia, as far as northern Argentina and Chile. The term puna encompasses diverse ecosystems of the high Central Andes above 3200–3400 m.\n\nThe puna is found above the treeline at 3200–3500 m elevation, and below the permanent snow line above 4500–5000 m elevation. It extends from central Peru in the north, across the Altiplano plateau of Peru and Bolivia, and south along the spine of the Andes into northern Argentina and Chile.\n\nOther sources claim that it goes on Suni (high plateaus and cliffs, some agriculture) and from 4000 m to the snow line (permafrost and alpine desert) of Puna grassland (mountain tops and slopes, much colder).\n\nThe puna is a diverse ecosystem that comprises varied ecoregions labeled wet/moist puna, dry puna and desert puna.\n\nThis ecoregion is a high elevation, wet, montane grassland in the southern high Andes, occurring from northern Peru to northern Bolivia. The wet puna shares its border on the west with the Sechura desert and the east with the wet Peruvian Yungas. The characteristically mountainous landscape contains high lakes, mountain valleys, snow-covered mountains, and plateaux.\nThe high elevation of the wet puna (4200 to 5000 m) causes the area to have large temperature differences between night and day. The average annual temperature is low, ranging from 5 to 7 °C; with night frost periods from March to October. Temperatures shift from characteristic summer highs in the day and drop to winter lows at night. This extreme temperature shift has caused selective adaptation to occur and many endemic plants such as the Culcitium, Perezia, and Polylepis center their diversity in the wet puna.\nThe ecoregion contains snow-capped peaks, glacial lakes, and several rivers that originate in the Cordilleras. The biggest lake in the ecoregion is Lake Titicaca, which is the highest navigable lake in the world, at an elevation of 3800 m (above sea level). The Suches and Tiwanacu rivers in Bolivia are the lakes tributaries. The areas in the north surrounding Lake Titicaca have eight wet months, and the areas in the south have one to two wet months. The average precipitation in this region ranges from 400 to 2000 mm.\n\nThis ecoregion is a very dry, high elevation montane grassland of the southern high Andes. It extends into northern Chile and Argentina and east into western Bolivia occurring above 3500 m between the tree and permanent snow lines. The vegetation of the dry puna consists of tropical alpine herbs with dwarf shrubs. Within the dry puna are salt flats, high plateaus, snow-covered peaks and volcanoes. Dry puna is distinguished from the other types of puna by its diminished annual rainfall. The dry puna has an 8-month long dry season and receives less than 400 mm of rainfall each year. The region lies at an elevation of 3500–5000 m above sea level. The dry puna is oligothermic as well. The average temperatures in this ecoregion range from 8 to 11 degrees Celsius and are lowest in the south. As a result of the elevation, varied temperatures and lack of rainfall, the Central Andean dry puna is a unique ecoregion with highly adapted flora and fauna. The southern region of the dry puna encompasses an even drier puna known as the desert puna. In the desert puna the average rainfall ranges from only 51–406 mm. The desert puna is dominated by the huge salt lakes and is known for the scattered halophytes around and in the depressions. These salt lakes are home to the endemic Andean flamingo.\n\nThe World Wildlife fund defines three distinct puna sub-ecoregions:\n\n\nPuna soils are composed of an organic rich layer and a stony layer. The average soil profile is 33 cm deep. The Puna ecosystem has a low diversity of bacteria in its soils. The rhizosphere of the grasses are dominated by the Bacillas species, these organisms are composed of dormant cells that enable them to survive in the extreme climatic conditions in the Puna ecosystem. The dormant bacterial community of Puna grasses is similar to those found in desert soils.\n\nThe puna flora is characterized by its unique assemblages of cushion and mat forming species. Many of these species, most notably the large \"Azorella compacta\" (Yareta) has been heavily harvested for fuel and medicinal use. The vegetation with the puna grassland displays complex patterns of spatial variation, despite the low cover and overall density. The puna belt which ranges from wet puna in the north of the Andes to dry puna to the southwestern Andes is composed mostly by poaceae (Grasses) and shrubs of the asteraceae (daisy) family. Other representative grasses include species \"Jarava ichu\" (“Paja Brava”), \"Calamagrostis vicunarum\" (“Crespillo”), and \"Festuca dolichophylla\" (“Chillihua”).\nThere are several main rock unit formations in the Puna with distinct soil conditions that can be used to identify the main flora of each area. Up to 3000 m above the desert, the arid vegetation of the mountainous steppe is characterized by columnar cacti, arid shrubs and herbs. Vegetation located between 3800–4000 m are sustained by brown andic soils on ash-fall deposits and includes many endemic plant species as \"Hersodoma arequipensis\", \"Piplostephium tacorense\" and \"Opuntia corotilla\".In the wettest area shrubby vegetation of families asteraceae, fabaceae and solanaceae dominate.\nThe puna is generally drier than the páramo montane grasslands of the northern Andes.\n\nNative mammals include llamas, alpacas, vicuñas and guanacos. Relatively few birds, such as the Darwin's rhea, Andean condor, and certain miners and yellow-finches, are frequently found in the vast expanses of puna grasslands, but numerous birds are associated with the highland lakes and marshes that are found in the puna grasslands, for example the Andean goose, Andean flamingo, Andean avocet, giant coot, puna teal and diademed sandpiper-plover. The highland puna is a biome that ecompases relatively large rserves. Some of the major species for its conservation are the \"Lama vicugna\" (vicuna) and \"Lama guanicoe\". The guanaco (\"Lama guanicoe\") is a camelid native to South America. This animal can grow up to four feet tall (1.2 metres).\n\nBird populations in the puna ecosystem are surprisingly diverse for such a harsh and extreme environment. For example, the Lauca National Park includes 148 species of birds, which represents about one third of the entire Chilean bird population. Many of these species are rare and attract visitors to the area. One example of this rare avifauna is the giant flightless Darwin's rhea (\"Rhea pennata\"), which is similar to the ostrich found in the Old World, reaching up to one meter in height and 20 kg in weight. The puna also includes a great variety of aquatic species particularly at Lago Chungara located in northern Chile. The puna ecosystem has a great diversity of fresh water fishes. Some of these include the giant coot, the silvery grebe, the Chilean teal, and the diademed sandpiper-plover (one of the rarest shorebirds in the world).\n\nPeople of this region cultivate barley, potatoes and maca. Alpacas, vicuñas, and guanacos are raised for wool, and llamas for wool and transport.Human habitation in the puna is widespread and tends to increase to the east, toward the moister areas. Native tubers and grains are cultivated over large areas of the central puna. The inhabitants of this region cultivate native tubers (potatoes and maca) along with non-native grains such as barley and native pseudocereals such as quinoa. Alpacas, vicuñas, llamas, and guanacos are raised for wool and, as a result, most of the entirety of the puna is under the effect of animal grazing. Cattle, horses, and donkeys are localized in the wet/humid puna while llama, sheep and alpaca can be raised in both the wet and drier areas of the puna. Fire often accompanies grazing as a management tool and is one of the main threats to the grasslands. The drier areas are being threatened with progression to desertification.\n\nPuna grasslands are being rapidly depleted by human activity, and as a result need much attention in the conservation realm. Numerous factors can lead to the cause of this destruction, but the preservation of it depends almost entirely on to what degree humans are populating the area. Humans dramatically shape the ecosystem through the conversion of much of the land to farming grounds and grazing areas. Due to the high demand for cooking and heating fuel among the residents of the area, much of the land is degraded. For example, trees of the polylepis genus used to be easily found throughout the ecosystem and now are scarce. The most widespread influence on the grasslands is extensive grazing combined with the effects of fire. Because grazing dries out the land, it is more susceptible to fire. Once a land has been exposed to fire, it makes it more likely to burn again, creating a feedback loop that leads to damage of the ecosystem. Despite the fact that the puna grasslands experience heavy grazing, as an ecosystem it is highly resilient. In these areas of high grazing, successional species of grass and forbs grow back thick, thereby preserving the soil which means its potential to rebound is higher. The grasslands are also influenced more locally by agriculture, mining, and waste disposal depending on the concentration of the population. \nThere are two predominant forms of management of the puna grazing lands. The first is communal. In this form of management, the community controls the land and every member of that community grazes livestock. This generally leads to overgrazing and degradation of the land. The second style of management is cooperative. This type of management originated from a movement that took land from large landholders and turned it over to council composed of workers. Agronomists and animal scientists see over the land and make sure the grazing is sustainable.\nThough there are a significant number of problems in puna grasslands, mostly being attributed to overgrazing, there are measures being taken to improve the current situation. These measures are minimal, the ratio of protected areas to the rest of the ecosystem is minute. A growing population, construction of new roads, and mining activities are all acting as hindrances to the conservation of the ecosystem. Luckily, awareness is being raised about the problem, and steps are being taken to help improve its preservation. Currently range management programs are being introduced in many of the neighboring universities to research new ideas that implement little technology and can help restore the ecosystem. With the right management, the puna grasslands can rebound and support the growing populations of the surrounding areas. Ultimately however, it is up to the local individuals of the area to coordinate other ways they can receive income in ways that does not harm the land.\n\nAndean Continental Divide\n\n\n"}
{"id": "28376129", "url": "https://en.wikipedia.org/wiki?curid=28376129", "title": "Rayleigh sky model", "text": "Rayleigh sky model\n\nThe Rayleigh sky model describes the observed polarization pattern of the daytime sky. Within the atmosphere Rayleigh scattering of light from air molecules, water, dust, and aerosols causes the sky's light to have a defined polarization pattern. The same elastic scattering processes cause the sky to be blue. The polarization is characterized at each wavelength by its degree of polarization, and orientation (the e-vector angle, or scattering angle).\n\nThe polarization pattern of the sky is dependent on the celestial position of the sun. While all scattered light is polarized to some extent, light is highly polarized at a scattering angle of 90° from the light source. In most cases the light source is the sun, but the moon creates the same pattern as well. The degree of polarization first increases with increasing distance from the sun, and then decreases away from the sun. Thus, the maximum degree of polarization occurs in a circular band 90° from the sun. In this band, degrees of polarization near 80% are typically reached.\n\nWhen the sun is located at the zenith, the band of maximal polarization wraps around the horizon. Light from the sky is polarized horizontally along the horizon. During twilight at either the Vernal or Autumnal equinox, the band of maximal polarization is defined by the North-Zenith-South plane, or meridian. In particular, the polarization is vertical at the horizon in the North and South, where the meridian meets the horizon. The polarization at twilight at an equinox is represented by the figure to the right. The red band represents the circle in the North-Zenith-South plane where the sky is highly polarized. The cardinal directions N, E, S, W are shown at 12-o'clock, 9 o'clock, 6 o'clock and 3 o'clock (counter-clockwise around the celestial sphere since the observer is looking up at the sky).\n\nNote that because the polarization pattern is dependent on the sun, it changes not only throughout the day but throughout the year. When the sun sets toward the South, in the winter, the North-Zenith-South plane is offset, with \"effective\" North actually located somewhat toward the West. Thus if the sun sets at an azimuth of 255° (15° South of West) the polarization pattern will be at its maximum along the horizon at an azimuth of 345° (15° West of North) and 165° (15° East of South).\n\nDuring a single day, the pattern rotates with the changing position of the sun. At twilight it typically appears about 45 minutes before local sunrise and disappears 45 minutes after local sunset. Once established it is very stable, showing change only in its rotation. It can easily be seen on any given day using polarized sunglasses.\n\nMany animals use the polarization patterns of the sky at twilight and throughout the day as a navigation tool. Because it is determined purely by the position of the sun, it is easily used as a compass for animal orientation. By orienting themselves with respect to the polarization patterns, animals can locate the sun and thus determine the cardinal directions.\n\nThe geometry for the sky polarization can be represented by a celestial triangle based on the sun, zenith, and observed pointing (or the point of scattering). In the model, γ is the angular distance between the observed pointing and the sun, Θ is the solar zenith distance (90° – solar altitude), Θ is the angular distance between the observed pointing and the zenith (90° – observed altitude), Φ is the angle between the zenith direction and the solar direction at the observed pointing, and ψ is the angle between the solar direction and the observed pointing at the zenith.\n\nThus, the spherical triangle is defined not only by the three points located at the sun, zenith, and observed point but by both the three interior angles as well as the three angular distances. In an altitude-azimuth grid the angular distance between the observed pointing and the sun and the angular distance between the observed pointing and the zenith change while the angular distance between the sun and the zenith remains constant at one point in time.\nThe figure to the left shows the two changing angular distances as mapped onto an altitude-azimuth grid (with altitude located on the x-axis and azimuth located on the y-axis). The top plot represents the changing angular distance between the observed pointing and the sun, which is opposite to the interior angle located at the zenith (or the scattering angle). When the sun is located at the zenith this distance is greatest along the horizon at every cardinal direction. It then decreases with rising altitude moving closer toward the zenith. At twilight the sun is setting in the west. Hence the distance is greatest when looking directly away from the sun along the horizon in the east, and lowest along the horizon in the west.\n\nThe bottom plot in the figure to the left represents the angular distance from the observed pointing to the zenith, which is opposite to the interior angle located at the sun. Unlike the distance between the observed pointing and the sun, this is independent of azimuth, i.e. cardinal direction. It is simply greatest along the horizon at low altitudes and decreases linearly with rising altitude.\nThe figure to the right represents the three angular distances. The left one represents the angle at the observed pointing between the zenith direction and the solar direction. This is thus heavily dependent on the changing solar direction as the sun moves across the sky. The middle one represents the angle at the sun between the zenith direction and the pointing. Again this is heavily dependent on the changing pointing. This is symmetrical between the North and South hemispheres. The right one represents the angle at the zenith between the solar direction and the pointing. It thus rotates around the celestial sphere.\nThe Rayleigh sky model predicts the degree of sky polarization as:\n\nAs a simple example one can map the degree of polarization on the horizon. As seen in the figure to the right it is high in the North (0° and 360°) and the South (180°). It then resembles a cosine function and decreases toward the East and West reaching zero at these cardinal directions.\n\nThe degree of polarization is easily understood when mapped onto an altitude-azimuth grid as shown below. As the sun sets due West, the maximum degree of polarization can be seen in the North-Zenith-South plane. Along the horizon, at an altitude of 0° it is highest in the North and South, and lowest in the East and West. Then as altitude increases approaching the zenith (or the plane of maximum polarization) the polarization remains high in the North and South and increases until it is again maximum at 90° in the East and West, where it is then at the zenith and within the plane of polarization.\n\nClick on the adjacent image to view an animation that represents the degree of polarization as shown on the celestial sphere. Black represents areas where the degree of polarization is zero, whereas red represents areas where the degree of polarization is much larger. It is approximately 80%, which is a realistic maximum for the clear Rayleigh sky during day time. The video thus begins when the sun is slightly above the horizon and at an azimuth of 120°. The sky is highly polarized in the effective North-Zenith-South plane. This is slightly offset because the sun's azimuth is not due East. The sun moves across the sky with clear circular polarization patterns surrounding it. When the sun is located at the zenith the polarization is independent of azimuth and decreases with rising altitude (as it approaches the sun). The pattern then continues as the sun approaches the horizon once again for sunset. The video ends with the sun below the horizon.\nThe scattering plane is the plane through the sun, the observer, and the point observed (or the scattering point). The scattering angle, γ, is the angular distance between the sun and the observed point. The equation for the scattering angle is derived from the law of cosines to the spherical triangle (refer to the figure above in the geometry section). It is given by:\n\nIn the above equation, ψ and θ are respectively the azimuth and zenith angle of the sun, and ψ and θ are respectively the azimuth and zenith angle of the observed point.\n\nThis equation breaks down at the zenith where the angular distance between the observed pointing and the zenith, θ is 0. Here the orientation of polarization is defined as the difference in azimuth between the observed pointing and the solar azimuth.\n\nThe angle of polarization (or polarization angle) is defined as the relative angle between a vector tangent to the meridian of the observed point, and an angle perpendicular to the scattering plane.\n\nThe polarization angles show a regular shift in polarization angle with azimuth. For example, when the sun is setting in the West the polarization angles proceed around the horizon. At this time the degree of polarization is constant in circular bands centered around the sun. Thus the degree of polarization as well as its corresponding angle clearly shifts around the horizon. When the sun is located at the zenith the horizon represents a constant degree of polarization. The corresponding polarization angle still shifts with different directions toward the zenith from different points.\n\nThe video to the right represents the polarization angle mapped onto the celestial sphere. It begins with the sun located in a similar fashion. The angle is zero along the line from the sun to the zenith and increases clockwise toward the East as the observed point moves clockwise toward the East. Once the sun rises in the East the angle acts in a similar fashion until the sun begins to move across the sky. As the sun moves across the sky the angle is both zero and high along the line defined by the sun, the zenith, and the anti-sun. It is lower South of this line and higher North of this line. When the sun is at the zenith, the angle is either fully positive or 0. These two values rotate toward the west. The video then repeats a similar fashion when the sun sets in the West.\n\nThe angle of polarization can be unwrapped into the Q and U Stokes parameters. Q and U are defined as the linearly polarized intensities along the position angles 0° and 45° respectively; -Q and -U are along the position angles 90° and −45°.\n\nIf the sun is located on the horizon due west, the degree of polarization is then along the North-Zenith-South plane. If the observer faces West and looks at the zenith, the polarization is horizontal with the observer. At this direction Q is 1 and U is 0. If the observer is still facing West but looking North instead then the polarization is vertical with him. Thus Q is −1 and U remains 0. Along the horizon U is always 0. Q is always −1 except in the East and West.\n\nThe scattering angle (the angle at the zenith between the solar direction and the observer direction) along the horizon is a circle. From the East through the West it is 180° and from the West through the East it is 90° at twilight. When the sun is setting in the West, the angle is then 180° East through West, and only 90° West through East. The scattering angle at an altitude of 45° is consistent.\n\nThe input stokes parameters q and u are then with respect to North but in the altitude-azimuth frame. We can easily unwrap q assuming it is in the +altitude direction. From the basic definition we know that +Q is an angle of 0° and -Q is an angle of 90°. Therefore, Q is calculated from a sine function. Similarly U is calculated from a cosine function. The angle of polarization is always perpendicular to the scattering plane. Therefore, 90° is added to both scattering angles in order to find the polarization angles. From this the Q and U Stokes parameters are determined:\n\nand\n\nThe scattering angle, derived from the law of cosines is with respect to the sun. The polarization angle is the angle with respect to the zenith, or positive altitude. There is a line of symmetry defined by the sun and the zenith. It is drawn from the sun through the zenith to the other side of the celestial sphere where the \"anti-sun\" would be. This is also the effective East-Zenith-West plane.\n\nThe first image to the right represents the q input mapped onto the celestial sphere. It is symmetric about the line defined by the sun-zenith-anti-sun. At twilight, in the North-Zenith-South plane it is negative because it is vertical with the degree of polarization. It is horizontal, or positive in the East-Zenith-West plane. In other words, it is positive in the ±altitude direction and negative in the ±azimuth direction. As the sun moves across the sky the q input remains high along the sun-zenith-anti-sun line. It remains zero around a circle based on the sun and the zenith. As it passes the zenith it rotates toward the south and repeats the same pattern until sunset.\n\nThe second image to the right represents the u input mapped onto the celestial sphere. The u stokes parameter changes signs depending on which quadrant it is in. The four quadrants are defined by the line of symmetry, the effective East-Zenith-West plane and the North-Zenith-South plane. It is not symmetric because it is defined by the angles ±45°. In a sense it makes two circles around the line of symmetry as opposed to only one.\n\nIt is easily understood when compared with the q input. Where the q input is halfway between 0° and 90°, the u input is either positive at +45° or negative at −45°. Similarly if the q input is positive at 90° or negative at 0° the u input is halfway between +45° and −45°. This can be seen at the non symmetric circles about the line of symmetry. It then follows the same pattern across the sky as the q input.\n\nAreas where the degree of polarization is zero (the skylight is unpolarized), are known as neutral points. Here the Stokes parameters Q and U also equal zero by definition. The degree of polarization therefore increases with increasing distance from the neutral points.\n\nThese conditions are met at a few defined locations on the sky. The Arago point is located above the antisolar point, while the Babinet and Brewster points are located above and below the sun respectively. The zenith distance of the Babinet or Arago point increases with increasing solar zenith distance. These neutral points can depart from their regular positions due to interference from dust and other aerosols.\n\nThe skylight polarization switches from negative to positive while passing a neutral point parallel to the solar or antisolar meridian. The lines that separate the regions of positive Q and negative Q are called neutral lines.\n\nThe Rayleigh sky causes a clearly defined polarization pattern under many different circumstances. The degree of polarization however, does not always remain consistent and may in fact decrease in different situations. The Rayleigh sky may undergo depolarization due to nearby objects such as clouds and large reflecting surfaces such as the ocean. It may also change depending on the time of the day (for instance at twilight or night).\n\nIn the night, the polarization of the moonlit sky is very strongly reduced in the presence of urban light pollution, because scattered urban light is not strongly polarized.\n\nExtensive research shows that the angle of polarization in a clear sky continues underneath clouds if the air beneath the cloud is directly lit by the sun. The scattering of direct sunlight on those clouds results in the same polarization pattern. In other words, the proportion of the sky that follows the Rayleigh Sky Model is high for both clear skies and cloudy skies. The pattern is also clearly visible in small visible patches of sky. The celestial angle of polarization is unaffected by clouds.\n\nPolarization patterns remain consistent even when the sun is not present in the sky. Twilight patterns are produced during the time period between the beginning of astronomical twilight (when the sun is 18° below the horizon) and sunrise, or sunset and the end of astronomical twilight. The duration of astronomical twilight depends on the length of the path taken by the sun below the horizon. Thus it depends on the time of year as well as the location, but it can last for as long as 1.5 hours.\n\nThe polarization pattern caused by twilight remains fairly consistent throughout this time period. This is because the sun is moving below the horizon nearly perpendicular to it, and its azimuth therefore changes very slowly throughout this time period.\n\nAt twilight, scattered polarized light originates in the upper atmosphere and then traverses the entire lower atmosphere before reaching the observer. This provides multiple scattering opportunities and causes depolarization. It has been seen that polarization increases by about 10% from the onset of twilight to dawn. Therefore, the pattern remains consistent while the degree changes slightly.\n\nNot only do polarization patterns remain consistent as the sun moves across the sky, but also as the moon moves across the sky at night. The moon creates the same polarization pattern. Thus it is possible to use the polarization patterns as a tool for navigation at night. The only difference is that the degree of polarization is not quite as strong.\n\nUnderlying surface properties can affect the degree of polarization of the daytime sky. The degree of polarization has a strong dependence on surface properties. As the surface reflectance or optical thickness increase, the degree of polarization decreases. The Rayleigh sky near the ocean can therefore be highly depolarized.\n\nLastly, there is a clear wavelength dependence in Rayleigh scattering. It is greatest at short wavelengths, whereas skylight polarization is greatest at middle to long wavelengths. Initially it is greatest in the ultraviolet, but as light moves to the Earth's surface and interacts via multiple-path scattering it becomes high at middle to long wavelengths. The angle of polarization shows no variation with wavelength.\n\nMany animals, typically insects, are sensitive to the polarization of light and can therefore use the polarization patterns of the daytime sky as a tool for navigation. This theory was first proposed by Karl von Frisch when looking at the celestial orientation of honeybees. The natural sky polarization pattern serves as an easily detected compass. From the polarization patterns, these species can orient themselves by determining the exact position of the sun without the use of direct sunlight. Thus under cloudy skies, or even at night, animals can find their way.\n\nUsing polarized light as a compass however is no easy task. The animal must be capable of detecting and analyzing polarized light. These species have specialized photoreceptors in their eyes that respond to the orientation and the degree of polarization near the zenith. They can extract information on the intensity and orientation of the degree of polarization. They can then incorporate this visually to orient themselves and recognize different properties of surfaces.\n\nThere is clear evidence that animals can even orient themselves when the sun is below the horizon at twilight. How well insects might orient themselves using nocturnal polarization patterns is still a topic of study. So far, it is known that nocturnal crickets have wide-field polarization sensors and should be able to use the night-time polarization patterns to orient themselves. It has also been seen that nocturnally migrating birds become disoriented when the polarization pattern at twilight is unclear.\n\nThe best example is the halicitid bee \"Megalopta genalis,\" which inhabits the rainforests in Central America and scavenges before sunrise and after sunset. This bee leaves its nest approximately 1 hour before sunrise, forages for up to 30 minutes, and accurately returns to its nest before sunrise. It acts similarly just after sunset.\n\nThus, this bee is an example of an insect that can perceive polarization patterns throughout astronomical twilight. Not only does this case exemplify the fact that polarization patterns are present during twilight, but it remains as a perfect example that when light conditions are challenging the bee orients itself based on the polarization patterns of the twilight sky.\n\nIt has been suggested that Vikings were able to navigate on the open sea in a similar fashion, using the birefringent crystal Iceland spar, which they called \"sunstone\", to determine the orientation of the sky's polarization. This would allow the navigator to locate the sun, even when it was obscured by cloud cover. An actual example of such a \"sunstone\" was found on a sunken (Tudor) ship dated 1592, in proximity to the ship's navigational equipment.\n\nBoth artificial and natural objects in the sky can be very difficult to detect using only the intensity of light. These objects include clouds, satellites, and aircraft. However, the polarization of these objects due to resonant scattering, emission, reflection, or other phenomena can differ from that of the background illumination. Thus they can be more easily detected by using polarization imaging. There is a wide range of remote sensing applications in which polarization is useful for detecting objects that are otherwise difficult to see.\n\n\n"}
{"id": "29003841", "url": "https://en.wikipedia.org/wiki?curid=29003841", "title": "Rhapso", "text": "Rhapso\n\nIn Greek mythology, Rhapso (Greek: Ῥαψώ) was a nymph or a minor goddess worshipped at Athens. She is known solely from an inscription of the 4th century BCE, found at Phaleron. Her name apparently derives from the Greek verb ῥάπτω \"to sew\" or \"to stitch\". \n\nAccording to some, she is associated with the Moirai (as a fate goddess) and Eileithyia (as a birth goddess); she somehow organized a man's thread of life, at birth, by some sort of stitching work (similar to Clotho of the Moirai). And according to others, she was possibly a patroness of seamstresses.\n\n"}
{"id": "42505850", "url": "https://en.wikipedia.org/wiki?curid=42505850", "title": "Rose Hall of Fame", "text": "Rose Hall of Fame\n\nThe Rose Hall of Fame contains roses considered world favourites by a vote of members of the World Federation of Rose Societies. Inductees are announced every three years at World Rose Conventions. Additionally, popular historical roses and roses of genealogical importance are inducted in the Old Rose Hall of Fame.\n\n"}
{"id": "1269807", "url": "https://en.wikipedia.org/wiki?curid=1269807", "title": "Sognefjord Span", "text": "Sognefjord Span\n\nThe Sognefjord Spans are the second, third, and fourth longest spans in the world situated east of Hermansverk, Norway and are part of different powerlines. As Sognefjord Span is a deep valley the pylons of these spans are not taller than ordinary pylons. In order to avoid any too close approaches or contacts between the conductors, each is mounted on a separate pylon at the end of the span, which is built as a steel framework tower.\n\nThis span is part of the powerline between Hermansverk and Vikøyri and was built in the 1950s. It has a length of 4850 metres and was until the inauguration of Ameralik Span the longest span in the world. The span, which crosses Sognefjord in North-South direction, has 4 conductors, whereby one is for reserve. Its northern ends are at<br>\n<br> \n<br> \n<br> \n<br>\n\nand its southern ends are at<br>\n\nJust a few hundred metres east of Span 1, the powerline from Stakaldefossen to Refsdal: crosses Sognefjord with a 4520 metres long spanin East-West direction. As Span 1 it has 4 conductors, whereby 1 is as reserve. The locations of the towers used for this span are at its werstern end: <br> <br> <br> <br> \n\nand at its eastern end:<br>\n\nApproximately 9 kilometres east-south east of these spans, there crosses the line from Aurland1 to Langedøla Sognefjord in a span with a length of 4500 metres in North-South direction. It has no reserve conductor and consists therefore of 3 ropes. Its towers are situated at its southern end at<br><br> <br> <br>\n\nand at its north end at<br>\n<br><br><br>.\n\n\n"}
{"id": "1973564", "url": "https://en.wikipedia.org/wiki?curid=1973564", "title": "Stadial", "text": "Stadial\n\nStadials and interstadials are phases dividing the Quaternary period, or the last 2.6 million years. Stadials are periods of colder climate while interstadials are periods of warmer climate. \n\nEach Quaternary climate phase is associated with a Marine Isotope Stage (MIS) number, which describe alternation between warmer and cooler temperatures as measured by oxygen isotope data. Stadials have even MIS numbers and interstadials odd MIS numbers. The current Holocene interstadial is MIS 1 and the Last glacial maximum stadial is MIS 2. \n\nMarine Isotope Stages are sometimes further subdivided into stadials and interstadials by minor climate fluctuations within the overall stadial or interstadial regime, which are indicated by letters. The odd-numbered interstadial MIS 5, also known as the Sangamonian interglacial, contains two periods of relative cooling, and so is subdivided into three interstadials (5a, 5c, 5e) and two stadials (5b, 5d). A stadial isotope stage like MIS 6 would be subdivided by periods of relative warming, and so in that case the first and last subdivisions would be stadials; MIS 6a, 6c and 6e are stadials while 6b and 6d are interstadials.\n\nGenerally, stadials endure for a thousand years or less, and interstadials for less than ten thousand years, while interglacials last for more than ten thousand and glacials for about one hundred thousand. \n\nWhile the MIS 1 interstadial encompasses the entirety of the present Holocene interglacial, the Wisconsin glaciation encompasses MIS 2, 3, and 4.\n\nGlacials and Interglacials refer to the 100kyr cycles associated with Milankovitch cycles, while stadials and interstadials are defined by the actual oxygen-isotope temperature record.\n\nThe Bølling Oscillation and the Allerød Oscillation, where they are not clearly distinguished in the stratigraphy, are taken together to form the Bølling/Allerød interstadial, and dated from about 14,700 to 12,700 years before the present.\n\nThe Oldest, Older, and Younger Dryas are three stadials that occurred during the warming since the Last Glacial Maximum. The Older Dryas occurred between the Bølling and Allerød interstadials. All three periods are named for the arctic plant species, Dryas octopetala, which proliferated during these cold periods.\n\nGreenland ice cores show 24 interstadials during the one hundred thousand years of the Wisconsin glaciation. Referred to as the Dansgaard-Oeschger events, they have been extensively studied, and in their northern European contexts are sometimes named after towns, such as the Brorup, the Odderade, the Oerel, the Glinde, the Hengelo, or the Denekamp.\n\n"}
{"id": "4739163", "url": "https://en.wikipedia.org/wiki?curid=4739163", "title": "Standby power", "text": "Standby power\n\nStandby power, also called vampire power, vampire draw, phantom load, ghost load or leaking electricity (\"phantom load\" and \"leaking electricity\" are defined technical terms with other meanings, adopted for this different purpose), refers to the way electric power is consumed by electronic and electrical appliances while they are switched off (but are designed to draw some power) or in standby mode. This only occurs because some devices claimed to be \"switched off\" on the electronic interface, but are in a different state from switching off at the plug, or disconnecting from the power point, which can solve the problem of standby power completely. In fact, switching off at the power point is effective enough, there is no need to disconnect all devices from the power point. Some such devices offer remote controls and digital clock features to the user, while other devices, such as power adapters for disconnected electronic devices, consume power without offering any features (sometimes called no-load power). All of the above examples, such as the remote control, digital clock functions and—in the case of adapters, no-load power—are switched off just by switching off at the power point. However, for some devices with built-in internal battery, such as a phone, the standby functions can be stopped by removing the battery instead.\n\nIn the past, standby power was largely a non-issue for users, electricity providers, manufacturers, and government regulators. In the first decade of the twenty-first century, awareness of the issue grew and it became an important consideration for all parties. Up to the middle of the decade, standby power was often several watts or even tens of watts per appliance. By 2010, regulations were in place in most developed countries restricting standby power of devices sold to one watt (and half that from 2013).\n\nStandby power is electrical power used by appliances and equipment while switched off or not performing their primary function, often waiting to be activated by a remote controller. That power is consumed by internal or external power supplies, remote control receivers, text or light displays, circuits energized when the device is plugged in even when switched off.\n\nWhile this definition is inadequate for technical purposes, there is as yet no formal definition; an international standards committee is developing a definition and test procedure.\n\nThe term is often used more loosely for any device that continuously must use a small amount of power even when not active; for example a telephone answering machine must be available at all times to receive calls, switching off to save power is not an option. Timers, powered thermostats, and the like are other examples. An uninterruptible power supply could be considered to be wasting standby power only when the computer it protects is off. Disconnecting standby power proper is at worst inconvenient; powering down completely, for example an answering machine not dealing with a call, renders it useless.\n\nStandby power is often consumed for a purpose, although in the past there was little effort to minimize power used.\n\nThe disadvantages of standby power mainly relate to the energy used. As standby power is reduced, the disadvantages become less. Older devices often used ten watts or more; with the adoption of the One Watt Initiative by many countries, standby energy use is much diminished.\n\nStandby power makes up a portion of homes' miscellaneous electric load, which also includes small appliances, security systems, and other small power draws. The U.S. Department of Energy said in 2008:\n\n\"Many appliances continue to draw a small amount of power when they are switched off. These \"phantom\" loads occur in most appliances that use electricity, such as VCRs, televisions, stereos, computers, and kitchen appliances. This can be avoided by unplugging the appliance or using a power strip and using the switch on the power strip to cut all power to the appliance.\"\nStandby power used by older devices can be as high as 10–15 W per device, while a modern HD LCD television may use less than 1 W in standby mode. Some appliances use no energy when turned off. Many countries adopting the One Watt Initiative now require new devices to use no more than 1 W starting in 2010, and 0.5 W in 2013.\n\nAlthough the power needed for functions such as displays, indicators, and remote control functions is relatively small, the large number of such devices and their being continuously plugged in resulted in energy usage before the One Watt regulations of 8 to 22 percent of all appliance consumption in different countries, 32 to 87 W, and around 3–10 percent of total residential consumption. In Britain in 2004 standby modes on electronic devices accounted for 8% of all British residential power consumption. A similar study in France in 2000 found that standby power accounted for 7% of total residential consumption.\n\nIn 2004, the California Energy Commission produced a report containing typical standby and operational power consumption for 280 different household devices, including baby monitors and toothbrush chargers.\n\nOver a decade ago some electronics, such as microwaves, CRTs and VHS players used more standby power than appliances manufactured in the last 5 years. For a historical reference please see this article from the Economist.\n\nIn the US the average home used an average of 11,040 kWh of electricity per year in 2010.\nEach watt of power consumed by a device running continuously consumes about 9kWh (1 W × 365.25 days/year × 24 hours/day) per year, a little less than one thousandth of the annual US household consumption. Unplugging a device constantly consuming standby power saves a yearly 9 kWh for each watt of continuous consumption (saving $1 per year at average US rates).\n\nDevices such as security systems, fire alarms, and digital video recorders require continuous power to operate properly (though in the case of electric timers used to disconnect other devices on standby, they actually reduce total energy usage). The Reducing Consumption section below provides information on reducing standby power.\n\nThere is a risk of fire from devices in standby mode. There are reports of televisions, in particular, catching fire in standby mode.\n\nBefore the development of modern semiconductor electronics it was not uncommon for devices, typically television receivers, to catch fire when plugged in but switched off, sometimes when fully switched off rather than on standby. This is much less likely with modern equipment, but not impossible. Older cathode-ray tube display equipment (television and computer displays) had high voltages and currents, and were far more of a fire risk than thin panel LCD and other displays.\n\nContributing factors for electrical fires include:\n\nThe One Watt Initiative was launched by the IEA in 1999 to ensure through international cooperation that by 2010 all new appliances sold in the world only use one watt in standby mode. This would reduce CO emissions by 50 million tons in the OECD countries alone by 2010.\n\nIn July 2001 U.S. President George W. Bush signed an Executive Order directing federal agencies to \"purchase products that use no more than one watt in their standby power consuming mode\".\n\nIn July 2007 California's 2005 appliance standards came into effect, limiting external power supply standby power to 0.5 watts.\n\nOn 6 January 2010 the European Commission (EC) Regulation No 1275/2008 came into force. The regulations mandate that from 6 January 2010 \"off mode\" and standby power for electrical and electronic household and office equipment shall not exceed 1W, \"standby plus\" power (providing information or status display in addition to possible reactivation function) shall not exceed 2W. Equipment must where appropriate provide off mode and/or standby mode when the equipment is connected to the mains power source. These figures were halved on 6 January 2013.\n\nThe following types of devices consume standby power.\n\nOther devices consume standby power which is required for normal functioning that cannot be saved by switching off when not in use. For these devices electricity can only be saved by choosing units with minimal permanent power consumption:\n\nStandby power consumption can be estimated using tables of standby power used by typical devices, although standby power used by appliances of the same class vary extremely widely (for a CRT computer display standby power is listed at a minimum of 1.6 W, maximum 74.5 W). Total standby power can be estimated by measuring total house power with all devices standing by, then disconnected, but this method is inaccurate and subject to large errors and uncertainties.\n\nThe power wasted in standby must go somewhere; it is dissipated as heat. The temperature, or simply perceived warmth, of a device on standby long enough to reach a stable temperature gives some idea of power wasted.\n\nFor most home applications, wattmeters give a good indication of energy used, and some indication of standby consumption.\n\nA wattmeter is used to measure electrical power. Inexpensive plugin wattmeters, sometimes described as energy monitors, are available from prices of around US$10. Some more expensive models for home use have remote display units. In the US wattmeters can often also be borrowed from local power authorities or a local public library. Although accuracy of measurement of low AC current and quantities derived from it, such as power, is often poor, these devices are nevertheless indicative of standby power, if sensitive enough to register it. Some home power monitors simply specify an error figure such as 0.2%, without specifying the parameter subject to this error (e.g., voltage, easy to measure), and without qualification. Errors of measurement at the low standby powers used from about 2010 (i.e., less than a few watts) may be a very large percentage of the actual value—accuracy is poor. Modification of such meters to read standby power has been described and discussed in detail (with oscilloscope waveforms and measurements). Essentially, the meter's shunt resistor, used to generate a voltage proportional to load current, is replaced by one of value typically 100 times larger, with protective diodes. Readings of the modified meter have to be multiplied by the resistance factor (e.g. 100), and maximum measurable power is reduced by the same factor.\n\nProfessional equipment capable of (but not specifically designed for) low-power measurements clarifies typically that the error is a percentage of \"full-scale\" value, or a percentage of reading plus a fixed amount, and valid only within certain limits.\n\nIn practice, accuracy of measurements by meters with poor performance at low power levels can be improved by measuring the power drawn by a fixed load such as an incandescent light bulb, adding the standby device, and calculating the difference in power consumption.\n\nLess expensive wattmeters may be subject to significant inaccuracy at low current (power). They are often subject to other errors due to their mode of operation:\nLaboratory-grade equipment designed for low power measurement, which costs from several hundreds of US dollars and is much larger than simple domestic meters, can measure power down to very low values without any of these effects. The US IEC 62301 recommendation for measurements of active power is that power of 0.5 W or greater shall be made with an uncertainty of 2%. Measurements of less than 0.5 W shall be made with an uncertainty of 0.01 W. The power measurement instrument shall have a resolution of 0.01 W or better.\n\nEven with laboratory-grade equipment measurement of standby power has its problems. There are two basic ways of connecting equipment to measure power; one measures the correct voltage, but the current is wrong; the error is negligibly small for relatively high currents, but becomes large for the small currents typical of standby—in a typical case a standby power of 100 mW would be overestimated by over 50%. The other connection gives a small error in the voltage but accurate current, and reduces the error at low power by a factor of 5000. A laboratory meter intended for measurement of higher powers may be susceptible to this error. Another issue is the possibility of measuring equipment damage if in a very sensitive range capable of measuring a few milliamps; if the device being measured comes out of standby and draws several amps, the meter can be damaged unless it is protected.\n\nSome equipment has a quick-start mode; standby power is eliminated if this mode is not used. Video game consoles often use power when they are turned off, but the standby power can be further reduced if the correct options are set. For example, a Wii console can go from 18 watts to 8 watts to 1 watt by turning off the WiiConnect24 and Standby Connection options.\n\nDevices that have rechargeable batteries and are always plugged in use standby power even if the battery is fully charged. Corded appliances such as vacuum cleaners, electric razors, and simple telephones do not need a standby mode and do not consume the standby power that cordless equivalents do.\n\nOlder devices with power adapters that are large and are warm to the touch use several watts of power. Newer power adapters that are lightweight and are not warm to the touch may use less than one watt.\n\nStandby power consumption can be reduced by unplugging or totally switching off, if possible, devices with a standby mode not currently in use; if several devices are used together or only when a room is occupied, they can be connected to a single power strip that is switched off when not needed. This may cause some electronic devices, particularly older ones, to lose their configuration settings.\n\nTimers can be used to turn off standby power to devices that are unused on a regular schedule. Switches that turn the power off when the connected device goes into standby, or that turn other outlets on or off when a device is turned on or off are also available. Switches can be activated by sensors. Home automation sensors, switches and controllers can be used to handle more complex sensing and switching. This produces a net saving of power so long as the control devices themselves use less power than the controlled equipment in standby mode.\n\nStandby power consumption of some computers can be reduced by turning off components that use power in standby mode. For instance, disabling Wake-on-LAN (WoL), \"wake on modem\", \"wake on keyboard\" or \"wake on USB\" may reduce power when in standby. Unused features may be disabled in the computer's BIOS setup to save power.\n\nDevices were introduced in 2010 that allow the remote controller for equipment to be used to totally switch off power to everything plugged into a power strip. It was claimed in the UK that this could save £30, more than the price of the device, in one year.\n\nAs users of energy and government authorities have become aware of the need not to waste energy, more attention is being paid to the electrical efficiency of devices (fraction of power consumed that achieves functionality, rather than waste heat); this affects all aspects of equipment, including standby power. Standby power use can be decreased both by attention to circuit design and by improved technology. Programs directed at consumer electronics have stimulated manufacturers to cut standby power use in many products. It is probably technically feasible to reduce standby power by 75% overall; most savings will be less than a watt, but other cases will be as large as 10 watts.\n\nFor example, a commercially available computer in Wake on LAN standby typically consumed 2 to 8 watts of standby power , but it was possible to design much more efficient circuitry: a purpose-designed microcontroller can reduce total system power to under 0.5W, with the microcontroller itself contributing 42 mW.\n\n\n"}
{"id": "1417149", "url": "https://en.wikipedia.org/wiki?curid=1417149", "title": "Structure of the Earth", "text": "Structure of the Earth\n\nThe internal structure of the Earth is layered in spherical shells: an outer silicate solid crust, a highly viscous asthenosphere and mantle, a liquid outer core that is much less viscous than the mantle, and a solid inner core. Scientific understanding of the internal structure of the Earth is based on observations of topography and bathymetry, observations of rock in outcrop, samples brought to the surface from greater depths by volcanoes or volcanic activity, analysis of the seismic waves that pass through the Earth, measurements of the gravitational and magnetic fields of the Earth, and experiments with crystalline solids at pressures and temperatures characteristic of the Earth's deep interior.\n\nThe force exerted by Earth's gravity can be used to calculate its mass. Astronomers can also calculate Earth's mass by observing the motion of orbiting satellites. Earth’s average density can be determined through gravimetric experiments, which have historically involved pendulums.\n\nThe mass of Earth is about .\n\nThe structure of Earth can be defined in two ways: by mechanical properties such as rheology, or chemically. Mechanically, it can be divided into lithosphere, asthenosphere, mesospheric mantle, outer core, and the inner core. Chemically, Earth can be divided into the crust, upper mantle, lower mantle, outer core, and inner core. The geologic component layers of Earth are at the following depths below the surface:\n\nThe layering of Earth has been inferred indirectly using the time of travel of refracted and reflected seismic waves created by earthquakes. The core does not allow shear waves to pass through it, while the speed of travel (seismic velocity) is different in other layers. The changes in seismic velocity between different layers causes refraction owing to Snell's law, like light bending as it passes through a prism. Likewise, reflections are caused by a large increase in seismic velocity and are similar to light reflecting from a mirror.\n\nThe Earth's crust ranges from in depth and is the outermost layer. The thin parts are the oceanic crust, which underlie the ocean basins (5–10 km) and are composed of dense (mafic) iron magnesium silicate igneous rocks, like basalt. The thicker crust is continental crust, which is less dense and composed of (felsic) sodium potassium aluminium silicate rocks, like granite. The rocks of the crust fall into two major categories – sial and sima (Suess,1831–1914). It is estimated that sima starts about 11 km below the Conrad discontinuity (a second order discontinuity). The uppermost mantle together with the crust constitutes the lithosphere. The crust-mantle boundary occurs as two physically different events. First, there is a discontinuity in the seismic velocity, which is most commonly known as the Mohorovičić discontinuity or Moho. The cause of the Moho is thought to be a change in rock composition from rocks containing plagioclase feldspar (above) to rocks that contain no feldspars (below). Second, in oceanic crust, there is a chemical discontinuity between ultramafic cumulates and tectonized harzburgites, which has been observed from deep parts of the oceanic crust that have been obducted onto the continental crust and preserved as ophiolite sequences.\n\nMany rocks now making up Earth's crust formed less than 100 million (1) years ago; however, the oldest known mineral grains are about 4.4 billion (4.4) years old, indicating that Earth has had a solid crust for at least 4.4 billion years.\n\nEarth's mantle extends to a depth of 2,890 km, making it the thickest layer of Earth. The mantle is divided into upper and lower mantle. The upper and lower mantle are separated by the transition zone. The lowest part of the mantle next to the core-mantle boundary is known as the D″ ( dee-double-prime) layer. The pressure at the bottom of the mantle is ≈140 GPa (1.4 Matm). The mantle is composed of silicate rocks that are rich in iron and magnesium relative to the overlying crust. Although solid, the high temperatures within the mantle cause the silicate material to be sufficiently ductile that it can flow on very long timescales. Convection of the mantle is expressed at the surface through the motions of tectonic plates. As there is intense and increasing pressure as one travels deeper into the mantle, the lower part of the mantle flows less easily than does the upper mantle (chemical changes within the mantle may also be important). The viscosity of the mantle ranges between 10 and 10 Pa·s, depending on depth. In comparison, the viscosity of water is approximately 10 Pa·s and that of pitch is 10 Pa·s. The source of heat that drives plate tectonics is the primordial heat left over from the planet’s formation as well as the radioactive decay of uranium, thorium, and potassium in Earth’s crust and mantle.\n\nThe average density of Earth is\n. Because the average density of surface material is only around , we must conclude that denser materials exist within Earth's core. \nThis result has been known since the Schiehallion experiment, performed in the 1770s.\nCharles Hutton in his 1778 report concluded that the mean density of the Earth must be about formula_1 that of surface rock, concluding that the interior of the Earth must be metallic. Hutton estimated this metallic portion to occupy some 65% of the diameter of the Earth.\nHutton's estimate on the mean density of the Earth was still about 20% too low, at \nHenry Cavendish in his torsion balance experiment of 1798 found a value of , within 1% of the modern value.\nSeismic measurements show that the core is divided into two parts, a \"solid\" inner core with a radius of ≈1,220 km and a liquid outer core extending beyond it to a radius of ≈3,400 km. The densities are between 9,900 and 12,200 kg/m in the outer core and 12,600–13,000 kg/m in the inner core.\n\nThe inner core was discovered in 1936 by Inge Lehmann and is generally believed to be composed primarily of iron and some nickel. Since this layer is able to transmit shear waves (transverse seismic waves), it must be solid. Experimental evidence has at times been critical of crystal models of the core. Other experimental studies show a discrepancy under high pressure: diamond anvil (static) studies at core pressures yield melting temperatures that are approximately 2000 K below those from shock laser (dynamic) studies. The laser studies create plasma, and the results are suggestive that constraining inner core conditions will depend on whether the inner core is a solid or is a plasma with the density of a solid. This is an area of active research.\n\nIn early stages of Earth's formation about 4.6 billion years ago, melting would have caused denser substances to sink toward the center in a process called planetary differentiation (see also the iron catastrophe), while less-dense materials would have migrated to the crust. The core is thus believed to largely be composed of iron (80%), along with nickel and one or more light elements, whereas other dense elements, such as lead and uranium, either are too rare to be significant or tend to bind to lighter elements and thus remain in the crust (see felsic materials). Some have argued that the inner core may be in the form of a single iron crystal.\n\nUnder laboratory conditions a sample of iron–nickel alloy was subjected to the corelike pressures by gripping it in a vise between 2 diamond tips (diamond anvil cell), and then heating to approximately 4000 K. The sample was observed with x-rays, and strongly supported the theory that Earth's inner core was made of giant crystals running north to south.\n\nThe liquid outer core surrounds the inner core and is believed to be composed of iron mixed with nickel and trace amounts of lighter elements.\n\nRecent speculation suggests that the innermost part of the core is enriched in gold, platinum and other siderophile elements.\n\nThe matter that comprises Earth is connected in fundamental ways to matter of certain chondrite meteorites, and to matter of outer portion of the Sun. There is good reason to believe that Earth is, in the main, like a chondrite meteorite. Beginning as early as 1940, scientists, including Francis Birch, built geophysics upon the premise that Earth is like ordinary chondrites, the most common type of meteorite observed impacting Earth, while totally ignoring another, albeit less abundant type, called enstatite chondrites. The principal difference between the two meteorite types is that enstatite chondrites formed under circumstances of extremely limited available oxygen, leading to certain normally oxyphile elements existing either partially or wholly in the alloy portion that corresponds to the core of Earth.\n\nDynamo theory suggests that convection in the outer core, combined with the Coriolis effect, gives rise to Earth's magnetic field. The solid inner core is too hot to hold a permanent magnetic field (see Curie temperature) but probably acts to stabilize the magnetic field generated by the liquid outer core. The average magnetic field strength in Earth's outer core is estimated to be 25 Gauss (2.5 mT), 50 times stronger than the magnetic field at the surface.\n\nRecent evidence has suggested that the inner core of Earth may rotate slightly faster than the rest of the planet; however, more recent studies in 2011 found this hypothesis to be inconclusive. Options remain for the core which may be oscillatory in nature or a chaotic system. In August 2005 a team of geophysicists announced in the journal \"Science\" that, according to their estimates, Earth's inner core rotates approximately 0.3 to 0.5 degrees per year faster relative to the rotation of the surface.\n\nThe current scientific explanation for Earth's temperature gradient is a combination of heat left over from the planet's initial formation, decay of radioactive elements, and freezing of the inner core.\n\n\n"}
{"id": "5478196", "url": "https://en.wikipedia.org/wiki?curid=5478196", "title": "Sudden ionospheric disturbance", "text": "Sudden ionospheric disturbance\n\nA sudden ionospheric disturbance (SID) is an abnormally high ionization/plasma density in the D region of the ionosphere caused by a solar flare. The SID results in a sudden increase in radio-wave absorption that is most severe in the upper medium frequency (MF) and lower high frequency (HF) ranges, and as a result often interrupts or interferes with telecommunications systems.\n\nThe \"Dellinger effect\", or sometimes \"Mögel–Dellinger effect\", is another name for a sudden ionospheric disturbance. The effect was discovered by John Howard Dellinger around 1935 and also described by the German physicist Hans Mögel (1900-1944) in 1930. The fadeouts are characterized by sudden onset and a recovery that takes minutes or hours.\n\nWhen a solar flare occurs on the Sun a blast of intense ultraviolet and x-ray radiation hits the dayside of the Earth after a propagation time of about 8 minutes. This high energy radiation is absorbed by atmospheric particles, raising them to excited states and knocking electrons free in the process of photoionization. The low altitude ionospheric layers (D region and E region) immediately increase in density over the entire dayside. The ionospheric disturbance enhances VLF radio propagation. Scientists on the ground can use this enhancement to detect solar flares; by monitoring the signal strength of a distant VLF transmitter, sudden ionospheric disturbances (SIDs) are recorded and indicate when solar flares have taken place. The small geomagnetic effect in the lower ionosphere appears as a small hook on magnetic records and is therefore called \"geomagnetic crochet effect\" or \"sudden field effect\".\n\nShort wave radio waves (in the HF range) are absorbed by the increased particles in the low altitude ionosphere causing a complete blackout of radio communications. This is called a short wave fading. These fadeouts last for a few minutes to a few hours and are most severe in the equatorial regions where the Sun is most directly overhead. The ionospheric disturbance enhances long wave (VLF) radio propagation. SIDs are observed and recorded by monitoring the signal strength of a distant VLF transmitter.\nA whole array of sub-classes of SIDs exist, detectable by different techniques at various wavelengths: the SPA (Sudden Phase Anomaly), SFD (Sudden Frequency Deviation), SCNA (Sudden Cosmic Noise Absorption), SEA (Sudden Enhancement of Atmospherics), etc.\n\n\n"}
{"id": "7000543", "url": "https://en.wikipedia.org/wiki?curid=7000543", "title": "The Last Dragon (2004 film)", "text": "The Last Dragon (2004 film)\n\nThe Last Dragon, known as Dragons: A Fantasy Made Real in the United States, and also known as Dragon's World in other countries, is a British docufiction made by Darlow Smithson Productions for Channel Four and broadcast on both Channel Four and Animal Planet that is described as the story of \"the natural history of the most extraordinary creature that never existed\".\n\nIt posits a speculative evolution of dragons from the Cretaceous period up to the 15th century, and suppositions about what dragon life and behavior might have been like if they had existed and evolved. It uses the premise that the ubiquity of dragons in world mythology suggests that dragons could have existed. They are depicted as a scientifically feasible species of reptile that could have evolved, somewhat similar to the depiction of dragons in the \"Dragonology\" series of books. The dragons featured in the show were designed by John Sibbick.\n\nThe program switches between two stories. The first uses CGI to show the dragons in their natural habitat throughout history. The second shows the story of a modern-day scientist at a museum, Dr. Tanner, who believes in dragons. When the frozen remains of an unknown creature are discovered in the Carpathian Mountains, Tanner, and two colleagues from the museum, undertake the task to examine the specimen to try to save his reputation. Once there, they discover that the creature is a dragon. Tanner and his colleagues set about working out how it lived and died.\n\nThe docufiction features two interwoven stories. Jack Tanner, an American paleontologist working for the Natural History Museum in London, suggests the theory that a carbonized Tyrannosaurus rex skeleton on display was killed by a dragon, causing him to believe that the legends were more than myth. This ruins Tanner's reputation. As viewed in a flashback, Tanner's theory is proven true, as said Tyrannosaurus battles a female dragon in the Cretaceous but is mortally wounded. The female dies from her wounds, forcing her infant to survive on his own, escaping a male dragon by learning how to fly for the first time. A later vignette shows the dragon, now an adult, trying to mate.\n\nThe museum is contacted by Romanian authorities, who discovered an alleged corpse of a dragon in the Carpathian Mountains, along with two carbonized human bodies from the 15th century. Tanner and two colleagues are sent to examine the bodies, moved to a warehouse. The scientists are baffled by the corpse, discovering despite being , it was capable of both flight and breathing fire by storing bacteria and hydrogen inside its body. Marine dragons survived the KT extinction, before evolving into other subspecies, such as a Chinese forest dragon, able to glide on its smaller wings and capable of camouflaging itself. The forest dragon hunts a wild boar and Bengal tiger, but the arrival of humans in the forest challenges its survival.\n\nBy analysing the dead dragon's reproductive system, Tanner concludes the corpse is that of an infant, killed by the humans. The scientists travel to the mountains to explore the caves where the corpses were found. In 1475, a lone female dragon on the verge of extinction lives in the Carpathian Mountains, looking for a mate. A male arrives from the Atlas Mountains and they perform an airborne courtship ritual, free falling from the sky at high speed. Tanner discovers a preserved dragon egg in the cave. It is revealed that the male dragon guards the nest, made from a cluster of rocks and the eggs are kept warm for preservation. However, the male is negligent, letting one of the eggs die, and is chased away by the female.\n\nSometime later, the dragon has had a lone daughter, hunting sheep from the local shepherds who hired dragon slayers to kill them if they manage to get too close. A hunter and his squire attack, slaying the daughter but are in turn killed by the mother. Tanner discovers more human corpses and then that of the female dragon, twice the size of the infant. In a final flashback, a larger group of hunters approach the cave, leading to the deaths of all involved. Tanner and his team take the dragons to the museum, reuniting mother and daughter. A year later, Tanner receives information of another discovery and rushes off to investigate. What he will find remains unknown.\n\n\"The Scotsman\" opined that \"The Last Dragon\"'s computer graphics made it \"awesome\", but ultimately the show gave the feeling of conveying the message \"Do not believe this slice of old hokum\" to the viewer. According to \"The New York Times\" \"it's easy to forget that [the film] isn't a serious documentary\" after the fiction disclaimer at the beginning, judging the computer graphics to be well made, sometimes beautiful, but not impressive \"to the point of wonder\".\n\n\n"}
{"id": "213682", "url": "https://en.wikipedia.org/wiki?curid=213682", "title": "Thermal depolymerization", "text": "Thermal depolymerization\n\nThermal depolymerization (TDP) is a depolymerization process using hydrous pyrolysis for the reduction of complex organic materials (usually waste products of various sorts, often biomass and plastic) into light crude oil. It mimics the natural geological processes thought to be involved in the production of fossil fuels. Under pressure and heat, long chain polymers of hydrogen, oxygen, and carbon decompose into short-chain petroleum hydrocarbons with a maximum length of around 18 carbons.\n\nThermal depolymerisation is similar to other processes which use superheated water as a major step to produce fuels, such as direct Hydrothermal Liquefaction.\nThese are distinct from processes using dry materials to depolymerize, such as pyrolysis. The term Thermochemical Conversion (TCC) has also been used for conversion of biomass to oils, using superheated water, although it is more usually applied to fuel production via pyrolysis.\nOther commercial scale processes include the \"SlurryCarb\" process operated by EnerTech, which uses similar technology to decarboxylate wet solid biowaste, which can then be physically dewatered and used as a solid fuel called E-Fuel. The plant in Rialto, California was designed to process 683 tons of waste per day. However, it failed to perform to design standards and was closed down. The Rialto facility defaulted on its bond payments and is in the process of being liquidated. \nThe Hydro Thermal Upgrading (HTU) process uses superheated water to produce oil from domestic waste.\nA demonstration plant is due to start up in The Netherlands said to be capable of processing 64 tons of biomass (dry basis) per day into oil. Thermal depolymerisation differs in that it contains a hydrous process followed by an anhydrous cracking / distillation process.\n\nThermal depolymerization is similar to the geological processes that produced the fossil fuels used today, except that the technological process occurs in a timeframe measured in hours. Until recently, the human-designed processes were not efficient enough to serve as a practical source of fuel—more energy was required than was produced.\n\nThe first industrial process to obtain gas, diesel fuels and other petroleum products through pyrolysis of coal, tar or biomass was designed and patented in the late 1920s by Fischer-Tropsch. In U. S. patent 2,177,557, issued in 1939, Bergstrom and Cederquist discuss a method for obtaining oil from wood in which the wood is heated under pressure in water with a significant amount of calcium hydroxide added to the mixture. In the early 1970s Herbert R. Appell and coworkers worked with hydrous pyrolysis methods, as exemplified by U. S. patent 3,733,255 (issued in 1973), which discusses the production of oil from sewer sludge and municipal refuse by heating the material in water, under pressure, and in the presence of carbon monoxide.\n\nAn approach that exceeded break-even was developed by Illinois microbiologist Paul Baskis in the 1980s and refined over the next 15 years (see U. S. patent 5,269,947, issued in 1993). The technology was finally developed for commercial use in 1996 by Changing World Technologies (CWT). Brian S. Appel (CEO of CWT) took the technology in 2001 and expanded and changed it into what is now referred to as TCP (Thermal Conversion Process), and has applied for and obtained several patents (see, for example, published patent 8,003,833, issued August 23, 2011). A Thermal Depolymerization demonstration plant was completed in 1999 in Philadelphia by Thermal Depolymerization, LLC, and the first full-scale commercial plant was constructed in Carthage, Missouri, about from ConAgra Foods' massive Butterball turkey plant, where it is expected to process about 200 tons of turkey waste into of oil per day.\n\nIn the method used by CWT, the water improves the heating process and contributes hydrogen to the reactions.\n\nIn the Changing World Technologies (CWT) process, the feedstock material is first ground into small chunks, and mixed with water if it is especially dry. It is then fed into a pressure vessel reaction chamber where it is heated at constant volume to around 250 °C. Similar to a pressure cooker (except at much higher pressure), steam naturally raises the pressure to 600 psi (4 MPa) (near the point of saturated water). These conditions are held for approximately 15 minutes to fully heat the mixture, after which the pressure is rapidly released to boil off most of the water (see: Flash evaporation). The result is a mix of crude hydrocarbons and solid minerals. The minerals are removed, and the hydrocarbons are sent to a second-stage reactor where they are heated to 500 °C, further breaking down the longer hydrocarbon chains. The hydrocarbons are then sorted by fractional distillation, in a process similar to conventional oil refining.\n\nThe CWT company claims that 15 to 20% of feedstock energy is used to provide energy for the plant. The remaining energy is available in the converted product. Working with turkey offal as the feedstock, the process proved to have yield efficiencies of approximately 85%; in other words, the energy contained in the end products of the process is 85% of the energy contained in the inputs to the process (most notably the energy content of the feedstock, but also including electricity for pumps and natural gas or woodgas for heating). If one considers the energy content of the feedstock to be free (i.e., waste material from some other process), then 85 units of energy are made available for every 15 units of energy consumed in process heat and electricity. This means the \"Energy Returned on Energy Invested\" (EROEI) is (6.67), which is comparable to other energy harvesting processes. Higher efficiencies may be possible with drier and more carbon-rich feedstocks, such as waste plastic.\n\nBy comparison, the current processes used to produce ethanol and biodiesel from agricultural sources have EROEI in the 4.2 range, when the energy used to produce the feedstocks is accounted for (in this case, usually sugar cane, corn, soybeans and the like). These EROEI values are not directly comparable, because these EROEI calculations include the energy cost to produce the feedstock, whereas the above EROEI calculation for thermal depolymerization process (TDP) does not.\n\nThe process breaks down almost all materials that are fed into it. TDP even efficiently breaks down many types of hazardous materials, such as poisons and difficult-to-destroy biological agents such as prions.\n\n\"(Note: Paper/cellulose contains at least 1% minerals, which was probably grouped under carbon solids.)\"\n\nAs reported on 04/02/2006 by Discover Magazine, a Carthage, Missouri plant was producing of oil made from 270 tons of turkey entrails and 20 tons of hog lard. This represents an oil yield of 22.3 percent. The Carthage plant produces API 40+, a high value crude oil. It contains light and heavy naphthas, a kerosene, and a gas oil fraction, with essentially no heavy fuel oils, tars, asphaltenes or waxes. It can be further refined to produce No. 2 and No. 4 fuel oils.\n\nThe fixed carbon solids produced by the TDP process have multiple uses as a filter, a fuel source and a fertilizer. It can be used as activated carbon in wastewater treatment, as a fertilizer, or as a fuel similar to coal.\n\nThe process can break down organic poisons, due to breaking chemical bonds and destroying the molecular shape needed for the poison's activity. It is likely to be highly effective at killing pathogens, including prions. It can also safely remove heavy metals from the samples by converting them from their ionized or organometallic forms to their stable oxides which can be safely separated from the other products.\n\nAlong with similar processes, it is a method of recycling the energy content of organic materials without first removing the water. It can produce liquid fuel, which separates from the water physically without need for drying. Other methods to recover energy often require pre-drying (e.g. burning, pyrolysis) or produce gaseous products (e.g. anaerobic digestion).\n\nThe United States Environmental Protection Agency estimates that in 2006 there were 251 million tons of municipal solid waste, or 4.6 pounds generated per day per person in the USA. Much of this mass is considered unsuitable for oil conversion.\n\nThe process only breaks long molecular chains into shorter ones, so small molecules such as carbon dioxide or methane cannot be converted to oil through this process. However, the methane in the feedstock is recovered and burned to heat the water that is an essential part of the process. In addition, the gas can be burned in a combined heat and power plant, consisting of a gas turbine which drives a generator to create electricity, and a heat exchanger to heat the process input water from the exhaust gas. The electricity can be sold to the power grid, for example under a feed-in tariff scheme. This also increases the overall efficiency of the process (already said to be over 85% of feedstock energy content).\n\nAnother option is to sell the methane product as biogas. For example, biogas can be compressed, much like natural gas, and used to power motor vehicles.\n\nMany agricultural and animal wastes could be processed, but many of these are already used as fertilizer, animal feed, and, in some cases, as feedstocks for paper mills or as boiler fuel. Energy crops constitute another potentially large feedstock for thermal depolymerization.\n\nReports in 2004 claimed that the Carthage facility was selling products at 10% below the price of equivalent oil, but its production costs were low enough that it produced a profit. At the time it was paying for turkey waste (see also below).\n\nThe plant then consumed 270 tons of turkey offal (the full output of the turkey processing plant) and 20 tons of egg production waste daily. In February 2005, the Carthage plant was producing about of crude oil.\n\nIn April 2005 the plant was reported to be running at a loss. Further 2005 reports summarized some economic setbacks which the Carthage plant encountered since its planning stages. It was thought that concern over mad cow disease would prevent the use of turkey waste and other animal products as cattle feed, and thus this waste would be free. As it turned out, turkey waste may still be used as feed in the United States, so that the facility must purchase that feed stock at a cost of $30 to $40 per ton, adding $15 to $20 per barrel to the cost of the oil. Final cost, as of January 2005, was $80/barrel ($1.90/gal).\n\nThe above cost of production also excludes the operating cost of the thermal oxidizer and scrubber added in May 2005 in response to odor complaints (see below).\n\nA biofuel tax credit of roughly $1 per US gallon (26 ¢/L) on production costs was not available because the oil produced did not meet the definition of \"biodiesel\" according to the relevant American tax legislation. The Energy Policy Act of 2005 specifically added thermal depolymerization to a $1 renewable diesel credit, which became effective at the end of 2005, allowing a profit of $4/barrel of output oil.\n\nThe company has explored expansion in California, Pennsylvania, and Virginia, and is presently examining projects in Europe, where animal products cannot be used as cattle feed. TDP is also being considered as an alternative means for sewage treatment in the United States.\n\nThe pilot plant in Carthage was temporarily shut down due to smell complaints. It was soon restarted when it was discovered that few of the odors were generated by the plant. Furthermore, the plant agreed to install an enhanced thermal oxidizer and to upgrade its air scrubber system under a court order. Since the plant is located only four blocks from the tourist-attracting town center, this has strained relations with the mayor and citizens of Carthage.\n\nAccording to a company spokeswoman, the plant has received complaints even on days when it is not operating. She also contended that the odors may not have been produced by their facility, which is located near several other agricultural processing plants.\n\nOn December 29, 2005, the plant was ordered by the state governor to shut down once again over allegations of foul odors as reported by MSNBC.\n\nAs of March 7, 2006, the plant has begun limited test runs to validate it has resolved the odor issue.\n\nAs of August 24, 2006, the last lawsuit connected with the odor issue has been dismissed and the problem is acknowledged as fixed. In late November, however, another complaint was filed over bad smells. This complaint was closed on January 11 of 2007 with no fines assessed.\n\nA May 2003 article in Discover magazine stated, \"Appel has lined up federal grant money to help build demonstration plants to process chicken offal and manure in Alabama and crop residuals and grease in Nevada. Also in the works are plants to process turkey waste and manure in Colorado and pork and cheese waste in Italy. He says the first generation of depolymerization centers will be up and running in 2005. By then it should be clear whether the technology is as miraculous as its backers claim.\"\n\nHowever, as of August 2008, the only operational plant listed at the company's website is the initial one in Carthage, Missouri.\n\nChanging World Technology applied for an IPO on August 12; 2008, hoping to raise $100 million.\n\nThe unusual Dutch Auction type IPO failed possibly because CWT has lost nearly $20 million with very little revenue.\n\nCWT, the parent company of Renewable Energy Solutions, filed for Chapter 11 bankruptcy. No details on plans for the Carthage plant have been released.\n\nIn April 2013, CWT was acquired by a Canadian firm, Ridgeline Energy Services, based in Calgary.\n\n\n\n"}
{"id": "442749", "url": "https://en.wikipedia.org/wiki?curid=442749", "title": "Washing and anointing", "text": "Washing and anointing\n\nWashing and anointing (also called the initiatory) is a temple ordinance practiced by The Church of Jesus Christ of Latter-day Saints (LDS Church) and Mormon fundamentalists as part of the faith's endowment ceremony. It is a purification ritual for adults, similar to chrismation, usually performed at least a year after baptism. The ordinance is performed by the authority of the Melchizedek priesthood by an officiator of the same sex as the participant.\n\nIn the ritual, a person is sprinkled with water to symbolically wash away the \"blood and sins of this generation.\" After the washing, the officiator anoints the person with consecrated oil while declaring blessings upon certain body parts. The officiator then declares that the person is anointed to become a \"king and priest\" or a \"queen and priestess\" in the afterlife.\n\nOnce washed and anointed, the participant is dressed in the temple garment, a religious white undergarment which the participant is instructed to wear throughout his or her life. (Since 2005, participants in the LDS Church version of the ritual already come clothed in this garment prior to the washing and anointing.) Finally, the participant is given a \"new name\" which he or she is instructed never to reveal except under certain conditions in the temple.\n\nMormons link the ritual to biblical washings and anointings. The temple garment symbolizes the skins of clothing given to Adam and Eve in the Garden of Eden, and the \"new name\" is linked to Revelation 2:17, which states that God will give those who overcome \"a white stone with a new name written on it, known only to him who receives it.\"\n\nAs the Mormons were completing their first temple in Kirtland, Ohio, founder Joseph Smith led many of the prominent Mormon males in a pre-endowment ritual patterned after similar washings and anointings described in the bible. This ritual took place beginning on 21 January 1836 in the attic of a printing office. Their bodies were washed with water and anointed with perfume, and then their heads were anointed with consecrated oil. Soon after the temple's dedication ceremony on 27 March 1836, about 300 Mormon men participated in a further ritual washing of feet and faces.\n\nSeveral years later, after Mormons moved to Nauvoo, Illinois, Joseph Smith revised the washing and anointing rituals as part of the new Nauvoo endowment. On 4–5 May 1842, nine prominent Mormon men were inducted into this endowment ceremony in the upper story of Smith's store. The first woman (Joseph Smith's first wife Emma) was inducted into the endowment ceremony on 28 September 1843.\n\nAs the washings and anointings were practiced in Nauvoo, men and women were taken to separate rooms, where they disrobed and, when called upon, passed through a canvas curtain to enter a tub where they were washed from head to foot while words of blessing were recited. Then oil from a horn was poured over the head of the participant, usually by another officiator, while similar words were repeated. As part of the ceremony, participants were ordained to become kings and queens in eternity. Men performed the ritual for men, and women performed the ritual for women. Also, as part of the ceremony, participants were given a new name and a ritual undergarment in which symbolic marks were snipped into the fabric.\n\nOriginally, the recipient of the washing and anointing was naked during the ceremony. Beginning in the 20th century, recipients were given a white poncho-like \"shield\" to wear during the washing and anointing. Since 2005, participants in the LDS Church version of the ritual already come clothed in the temple garment and wear it during the washing and anointing. In the original version of the ceremony, water and oil were applied to various parts of the body by the officiator as specific blessings related to the body parts were mentioned; since the early-21st century, the water and oil are applied only to the head and the symbolic nature of the washing and anointing is emphasized as the blessings for the body parts are related.\n\nRitual anointings were a prominent part of religious rites in the biblical world. Recipients of the anointing included temple officiants (e.g., Aaron), prophets (e.g., Elisha), and kings (e.g., Jehu, Solomon). In addition, sacral objects associated with the Israelite sanctuary were anointed. Of equal importance in the religion of the Israelites were ablutions (ceremonial washings). To ensure religious purity, Mosaic law required that designated individuals receive a ritual washing, sometimes in preparation for entering the temple.\n\nThe washings and anointings of the biblical period have a parallel today in the LDS Church. In response to a commandment to gather the saints and to build a house \"to prepare them for the ordinances and endowments, washings, and anointings\", these ordinances were introduced in the Kirtland Temple on January 21, 1836. The rites are in many respects similar in purpose to ancient Israelite practice and to the washing of feet by Jesus among his disciples. These modern LDS rites are performed only in temples set apart and dedicated for sacred purposes, according to a January 19, 1841 revelation said by Joseph Smith to be from Jesus Christ.\n\nThe ordinances are \"mostly symbolic in nature, but promising definite, immediate blessings as well as future blessings,\" contingent upon continued righteous living. Many symbolic meanings of washings and anointings are traceable in the scriptures. Ritual washings (Heb. 9:10) symbolize the cleansing of the soul from sins and iniquities. They signify the washing-away of the pollutions of the Lord's people (Isa. 4:4). Psalm 51:2 expresses the human longing and divine promise: \"Wash me thoroughly from mine iniquity, and cleanse me from my sin\". The anointing of a person or object with sacred ointment represents sanctification and consecration, so that both become \"most holy\" unto the Lord. In this manner, profane persons and things are sanctified in similitude of the \"messiah\" (Hebrew \"anointed one\"), who is \"Christ\" (Greek \"anointed one\").\n\nThe ordinances of washing and anointing are referred to often in the temple as \"initiatory ordinances\" since they precede the endowment and sealing ordinances. In connection with the initiatory ordinances, one is also clothed in the garment in the temple. Washings and anointings are also conducted on behalf of deceased individuals as a type of \"vicarious ordinance\".\n\n\n"}
{"id": "12469994", "url": "https://en.wikipedia.org/wiki?curid=12469994", "title": "Weisz–Prater criterion", "text": "Weisz–Prater criterion\n\nThe Weisz–Prater criterion is a method used to estimate the influence of pore diffusion on reaction rates in heterogeneous catalytic reactions. If the criterion is satisfied, pore diffusion limitations are negligible. The criterion is \nformula_1\nWhere formula_2 is the reaction rate per volume of catalyst, formula_3 is the catalyst particle radius, formula_4 is the reactant concentration at the particle surface, and formula_5 is the effective diffusivity. Diffusion is usually in the Knudsen regime when average pore radius is less than 100 nm.\nFor a given effectiveness factor,formula_6, and reaction order, n, the quantity formula_7 is defined by the equation:\nformula_8\nfor small values of beta this can be approximated using the binomial theorem:\nformula_9\nAssuming formula_10 with a reaction order formula_11 gives value of formula_7 equal to 0.1. Therefore for many conditions, if formula_13 then pore diffusion limitations can be excluded.\n"}
{"id": "9389691", "url": "https://en.wikipedia.org/wiki?curid=9389691", "title": "Worm cast", "text": "Worm cast\n\nA worm cast is a structure created by worms, typically on soils such as those on beaches that gives the appearance of multiple worms.\n\n"}
{"id": "970992", "url": "https://en.wikipedia.org/wiki?curid=970992", "title": "Yaksha", "text": "Yaksha\n\nYaksha (Sanskrit: यक्ष \"yakṣa\", Kannada: ಯಕ್ಷ \"yakṣa\", Tamil: யகன் \"yakan\", இயக்கன் \"iyakan\", Odia: ଯକ୍ଷ \"jôkhyô\", Pali: \"yakkha\") are a broad class of nature-spirits, usually benevolent, but sometimes mischievous or capricious caretakers of the natural treasures hidden in the earth and tree roots. They appear in Hindu, Jain and Buddhist texts, as well as ancient and medieval era temples of South Asia and Southeast Asia as guardian deities. The feminine form of the word is or Yakshini (\"\").\n\nIn Hindu, Jain, and Buddhist texts, the has a dual personality. On the one hand, a may be an inoffensive nature-fairy, associated with woods and mountains; but there is also a darker version of the , which is a kind of ghost (bhuta) that haunts the wilderness and waylays and devours travelers, similar to the .\n\nIn the Hindu Mythology Kubera (Sanskrit: कुबेर, Pali/later Sanskrit: Kuvera), also spelt Kuber, the Lord of Wealth and prosperity is considered as the god-king of the semi-divine Yakshas. He is regarded as the regent of the North (Dik-pala), and a protector of the world (Lokapala). His many epithets extol him as the overlord of numerous semi-divine species and the owner of the treasures of the world. Kubera is often depicted with a plump body, adorned with jewels, and carrying a money-pot and a club. His plump body denotes that Kubera the lord of Yakshas, is indeed a Yaksha. His (vehicle) is the viverrine (mongoose). He is often seen with another religious figure, Lakshmi (Sanskrit: लक्ष्मी, lakṣmī, ˈləkʂmiː) the Hindu goddess of wealth, fortune and prosperity.\nKubera was originally described as the chief of evil spirits in Vedic-era texts, Kubera acquired the status of a Deva (god) only in the Puranas and the Hindu epics.\n\nVaiśravaṇa is the Buddhist equivalent of Kubera.\n\nIn Kālidāsa's poem Meghadūta, for instance, the narrator is a romantic figure, pining with love for his missing beloved. By contrast, in the didactic Hindu dialogue of the \"Questions of the \", it is a tutelary spirit of a lake that challenges .\nThe may have originally been the tutelary gods of forests and villages, and were later viewed as the steward deities of the earth and the wealth buried beneath.\n\nIn Indian art, male are portrayed either as fearsome warriors or as portly, stout and dwarf-like. Female , known as , are portrayed as beautiful young women with happy round faces and full breasts and hips.\n\nSeveral monumental Yashas are known from the time the Mauryan Empire period. They are variously dated from around the 3rd century BCE to the 1st century BCE. These statues are monumental (usually around 2 meters tall), and often bear inscriptions related to their identification as Yakshas. They are considered as the first known monumental stone sculptures in India. Two of these monumental Yakshas are known from Patna, one from Vidisha and one from Parkham, as well as one female Yakshi from Besnagar.\n\nIn Buddhist literature, the are the attendants of Vaiśravaṇa, the guardian of the northern quarter, a beneficent god who protects the righteous. The term also refers to the Twelve Heavenly Generals who guard , the Medicine Buddha.\n\nThe-Mahamayuri-Vidyaraini-Sutra a Buddhist text that dates back to fourth cent or earlier (it was translated by Kumarajiva) gives a large list of Yakshas throughout the classical cities in India (including modern Afghanistan, Pakistan, Nepal, Bangladesh and Sri lanka) who are invoked to seek the protection of the BuddhaDharma:\nVaiśravaṇa (Kubera) is regarded to the chief of the Yakshas in Buddhism.\n\nIn \"Mahavamsa\" a local population is given the term Yakkhas. Prince Vijaya encountered the royalty of the yakkhas' queen, Kuveni, in her capital of Lankapura, and conquered them. The Yakkhas served as loyal subjects with the House of Vijaya and the yakkha chieftain sat on equal height to the Sri Lankan leaders on festival days.\n\nYakshas (, ) are an important element in Thai temple art and architecture. They are common as guardians of the gates in Buddhist temples throughout the country since at least the 14th century. Ceramic sculptures of guardian Yakshas were produced in Thailand, during the Sukhothai and Ayutthaya periods, between the 14th and 16th centuries, at several kiln complexes in northern Thailand. They are mostly depicted with a characteristic face, having big round bulging eyes and protruding fangs, as well as a green complexion. Yakshas and their female counterparts are common in the Buddhist literature of Thailand, such as in The Twelve Sisters and Phra Aphai Mani. As ogres, giants, and ogresses, yakshas are present as well in Thai folklore.\n\n\"ย ยักษ์\", \"(yo yak)\" is also used as an illustration in order to name the letter ย, the 34th consonant of the Thai alphabet, according to the traditional letter symbols Thai children use to memorize the alphabet.\n\nJains mainly maintain cult images of Arihants and Tirthankaras, who have conquered the inner passions and attained moksha. \"Yaksha\" and \"Yakshini\" are found in pair around the cult images of Jinas, serving as guardian deities. \"Yaksha\" is generally on the right-hand side of the Jina image and \"Yakshini\" on the left-hand side. They are regarded mainly as devotees of the Jina, and have supernatural powers. They are also wandering through the cycles of births and deaths just like the worldly souls, but have supernatural powers. \nThe Harivamsapurana (783 CE) refers to them as Shasandevatas. Initially among the yakshas, Manibhadra and Purnabadra yakshas and Bahuputrika yakshini were popular. The yaksha Manibhadra is worshipped by the Jains affiliated with the Tapa Gachchha. During tenth and thirteenth centuries yaksha Saarvanubhuti, or Sarvahna and yakshinis Chakreshvari, Ambika, Padmavati, and Jwalamalini became so popular that independent temples devoted to them were erected.\n\nThe Yakshas and Yakshinis are common among traditional Murtipujak Shvetambara and Bispanthi Digambar Jains. The Digambara Terapanth movement opposes their worship. Among the Murtipujak Shvetambaras, the Tristutik Gaccha sect (both historical founded by Silagana and Devabhadra, and the modern sect organized by Rajendrasuri) object to the worship of shruta-devatas.\n\nIn Jainism, there are twenty-four yakshas and twenty-four yakshais that serve as shasan-devatas for the twenty-four tirthankaras:. The Yakshas are\n\n\n"}
{"id": "9133008", "url": "https://en.wikipedia.org/wiki?curid=9133008", "title": "Zero Emission Resource Organisation", "text": "Zero Emission Resource Organisation\n\nZero Emission Resource Organisation or ZERO is a Norwegian environmental organisation that was founded in 2002 to work with reduction of greenhouse gases, primarily in Norway. The philosophy of the organisation is that if new facilities are made emission-free, then when existing plants and methods are phased out due to old age, society is left with emission-free facilities. The primary working areas include disposal, renewable energy, especially wind power, and new transportation fuels, including hydrogen and biofuel. ZERO is organised as foundation and was started by former activists and employees of Natur og Ungdom and Bellona. Funding sources include industrial associations and companies. ZERO is led by (director) and Erik Espeset (chairman).\n\nZERO promotes new technology that enables emission-free energy solutions without harming the environment. Important issues are electric cars and chargers, carbon capture and storage (CCS), renewable energy, electrification of offshore installations, climate friendly construction and buildings and fossile free plastic. It also promotes biofuel as an alternative to fossil fuels. \n\nZERO worked to establish an electricity certificate scheme to promote renewable energy in Norway and Sweden. The organisation also supported Hynor—A chain of hydrogen fuel stations that would span the South Coast from Oslo to Stavanger. It promotes CO-capture and storage from industrial plants and other industrial emission reductions and is a supporter of windmills.\n\nThe organisation receives financial support from a wide range og public and private donors. Among others, the partners include Elkem, Siemens, Coca-Cola Norway, Scatec Solar, Tesla and Statoil.\n\nZERO produces knowledge-based reports, writes op-eds and comments in the media and arranges various meetings, seminars and conferences. \n\nThe biggest event is the annual ZERO conference (Zero-konferansen), that takes place in Oslo every november. The conference, established in 2006, is the largest climate conference in Norway with more than 1000 attendants. Arnold Schwarzenegger, Kofi Annan, Jens Stoltenberg, Chelsea Clinton and former chairman of the IPCC Rajendra K. Pachauri have all been speakers at the conference.\n"}
