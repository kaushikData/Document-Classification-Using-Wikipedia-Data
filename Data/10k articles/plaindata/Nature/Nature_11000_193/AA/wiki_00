{"id": "42078014", "url": "https://en.wikipedia.org/wiki?curid=42078014", "title": "(192642) 1999 RD32", "text": "(192642) 1999 RD32\n\n, provisional designation , is an eccentric asteroid and suspected contact binary, classified as near-Earth object and potentially hazardous asteroid of the Apollo group, approximately 5 kilometers in diameter. It was discovered on 8 September 1999, at a magnitude of 18, by astronomers of the LINEAR program using its 1-meter telescope at the Lincoln Laboratory's Experimental Test Site near Socorro, New Mexico, United States. The asteroid is likely of carbonaceous composition and has a rotation period of 17.08 hours.\n\n orbits the Sun at a distance of 0.6–4.7 AU once every 4 years and 4 months (1,571 days; semi-major axis of 2.64 AU). Its orbit has a high eccentricity of 0.77 and an inclination of 7° with respect to the ecliptic.\n\nThe asteroid's observation arc begins with a precovery taken at Palomar Observatory in January 1995. It is known that passed from Earth on 27 August 1969. During the 1969 close approach the asteroid reached about apparent magnitude 8.8. The similarly-sized 4179 Toutatis also reached that brightness in September 2004. It passed less than from asteroid 29 Amphitrite on 17 January 1939.\n\nArecibo radar observations on 5–6 March 2012 showed that is approximately in diameter and has an estimated albedo of only 0.04. Other sources calculate a smaller diameter of 1.63 kilometers based on a dated assumption, that the object is a stony rather than a carbonaceous asteroid. The two visible lobes suggest that is a tight binary asteroid or contact binary. About 10–15% of near-Earth asteroids larger than 200 meters are expected to be contact binary asteroids with two lobes in mutual contact.\n\nThis minor planet was numbered by the Minor Planet Center on 13 November 2008. As of 2018, it has not been named.\n\n"}
{"id": "56248020", "url": "https://en.wikipedia.org/wiki?curid=56248020", "title": "2014 Dan River coal ash spill", "text": "2014 Dan River coal ash spill\n\nIn February 2014, an Eden, North Carolina facility owned by Duke Energy spilled 39,000 tons of coal ash into the Dan River. The company later pled guilty to criminal negligence in their handling of coal ash at Eden and elsewhere and paid substantial fines. The U.S. Environmental Protection Agency (EPA) has since been responsible for overseeing cleanup of the waste. EPA and Duke Energy signed an administrative order for the site cleanup.\n\nOn February 2, 2014 a drainage pipe burst at a coal ash containment pond owned by Duke Energy in Eden, North Carolina, sending 39,000 tons of coal ash into the Dan River. In addition to the coal ash, 27 million gallons of wastewater from the plant was also released into the river. The broken pipe was left unsealed for almost a week before the draining coal ash was stopped. The ash was deposited up to from the site of the spill and contained harmful metals and chemicals. This catastrophe occurred at the site of the Dan River Steam Station, a retired coal power plant which had ceased operation in 2012. Duke Energy apologized for the incident and announced detailed plans for removal of coal ash at the Dan River site. Workers were only able to remove about ten percent of the coal ash that was spilled into the river, but cleanup is ongoing and Duke Energy plans to spend around 3 million dollars to continue the cleanup efforts.\n\nCNN reported that the river was turned into an oily sludge. The river is a drinking water source for communities in North Carolina and Virginia. Immediate tests showed increased amounts of arsenic and selenium, but the river was deemed by state officials to be a safe source for drinking water. The coal ash immediately endangered animals and fish species that lived in or around the river. Six days after the spill Duke Energy announced that the leakage had been stopped and they pledged to clean up the coal ash.\n\nThe cause of the ash spill was described by EPA as a limited structural flaw. A storm pipe nearby the deposits of a coal ash slurry containment area broke and allowed for the leakage. Coal ash slurry is produced during the process of burning coal. It is the left over impurities that stick around after burning coal for electricity. Coal companies have found that the cheapest way to store this waste is to mix it with water and store it in a pond. These ponds have been found to have leaks that can dispose hazardous material into surface water among other things. This material was released into the Dan River because of the collapse of a 48 inch drain pipe. The pipe was made of concrete and corrugated metal and reason for the fracture cannot be identified. What resulted was 39 thousand tons of coal ash and 27 million gallons of ash pond water were deposited into the Dan River.\n\nEPA has been collecting dissolved contaminant concentration data in the Dan River (from the VA/NC state line to midway between Danville and South Boston) since the coal ash spill. The organization has been periodically comparing the retrieved water/sediment chemistry data to ecological risk screening levels (ERSLs) to assess risk to aquatic and plant life. Coal ash is made up of various materials after the burning of coal takes place. These include silica, arsenic, boron, cadmium, chromium, copper, lead, mercury, selenium, and zinc. Certain contaminants that were measured exceed the screening levels, necessitating that the water/sediment chemistry must continue to be monitored. Coal ash can coat and degrade the habitats of aquatic animals as well as cause direct harm to certain organisms.\n\nThe latest surface water sampling results were released by EPA in July 2014. All surface water chemical concentrations were found to be below the ERSLs except for lead. The latest sediment sampling results were also released in July 2014. All sediment chemical concentrations were found to be below the ERSLs except for aluminum, arsenic, barium, and iron. The latest soil sampling results were released in June 2014. All soil chemical concentrations were found to be below the ERSLs except for aluminum, barium, iron, and manganese.\n\nThe coal ash will never be fully removed from the river. This is due to samples passing human health screening, the potential for historical contamination to become re-suspended, and removal being more detrimental to certain endangered species than the coal ash itself. In addition, the coal ash is already mixed in with existing sediment, complicating its removal further.\n\nThe \"New York Times\" reported that the North Carolina Department of Environmental Quality (NCDEQ; formerly the Department of Environment and Natural Resources) was directed to minimize its regulatory role prior to the accident by Governor Pat McCrory. Prior to being Governor, McCrory had worked for Duke Energy for nearly three decades. At the time, it was the third largest coal ash spill to have occurred in the United States. Prior to the incident, environmental groups had attempted to sue Duke Energy three times in 2013 under the Clean Water Act to force the company to fix leaks in its coal ash dumps. Each time, the groups were blocked by NCDEQ, which eventually fined the company $99,111. Federal prosecutors found this fine to be suspiciously low, and investigated both Duke Energy and the state regulators. Many newspaper editorials alleged that Duke Energy's environmental safety controls were lax and that the company \"bullied\" regulators.\n\nAfter the incident, Duke Energy was prosecuted by a number of agencies, and substantial evidence was presented indicating that company officials knew about numerous coal ash leaks in various plants including the Eden facility and declined to resolve it or provide local plant administrators the funds they were requesting to monitor and mitigate the problems. At the federal level, Duke was prosecuted by the United States Department of Justice Environment and Natural Resources Division and pled guilty to nine charges of criminal negligence under the Clean Water Act. Duke agreed to pay $102 million in fines and restitution, the largest federal criminal fine in North Carolina history. Duke also agreed to pay fines to North Carolina and Virginia ($2.5 million).\n\nLargely as a result of the attention brought to Duke Energy's handling of coal ash ponds by the 2014 disaster, the North Carolina state legislature ordered Duke Energy to close its 32 ash ponds in the state by 2029. On May 2nd 2014, Duke Energy and the EPA agreed to a 3 million dollar cleanup agreement. Part of the agreement is having Duke Energy identify areas of necessary cleanup on the Dan River that is estimated to cost around 1 million dollars. The other 2 million dollars is allocated to the EPA to address future response methods needed in order to clean up the Dan River. Spokesperson with Duke Energy has announced that they plan on getting out of the coal ash business all together. Associates have said that well before the Dan River incident they had allocated 130 million dollars to transitioning plants to handle fly-ash dry and manage it in lined landfills. Duke Energy said that they have also created an advisory group of researchers to help with cleaner coal combustion in their facilities.\n\nIn February 2016 EPA proposed a $6.8 million settlement, which Duke Energy immediately appealed. In September the corporation accepted a settlement just shy of the original amount at $5,983,750 to be paid for fines, restitution, cleanup assessment, removal, and community action initiatives. Regarding the initial settlement cost listed above, EPA sends periodic bills to Duke Energy accounting for direct and indirect costs incurred by EPA, its contractors, and the Department of Justice.\n\nTo keep the energy provider accountable, under the Administrative Settlement Agreement & Order on Consent for Removal Action (AOC) as of May 2014, the Respondent, Duke Energy, was required to submit a number of plans to EPA, including a scope of work, public health, post-removal site control, and engineering plans.\n\nWithin these plans, Duke Energy is responsible for creating and implementing a Site Assessment that includes but is not limited to ecological analysis, surface water and sediment assessment as well as post-removal monitoring protocols to calculate the extent of pollution in the Dan River in North Carolina and the Kerr Reservoir and Schoolfield Dam in Danville, Virginia. These assessments were approved by the EPA in consultation with the affected states agencies including NCDEQ and the Virginia Department of Environmental Quality (VDEQ). Following the spill and written into the AOC are monitoring protocols in which the EPA will sporadically authorize the NCDEQ and VDEQ to take split or duplicate water samples to ensure consistent quality after removal of the coal ash.\n\n"}
{"id": "2492100", "url": "https://en.wikipedia.org/wiki?curid=2492100", "title": "Aeschynite-(Ce)", "text": "Aeschynite-(Ce)\n\nAeschynite-(Ce) (or Aschynite, Eschinite, Eschynite) is a rare earth mineral of cerium, calcium, iron, thorium, titanium, niobium, oxygen, and hydrogen with formula: (Ce,Ca,Fe,Th)(Ti,Nb)(O,OH). Its name comes from the Greek word for \"shame\". The \"-(Ce)\" means it has more cerium than the yttrium variety aeschynite-(Y). Its Mohs scale rating is 5-6.\n"}
{"id": "21656016", "url": "https://en.wikipedia.org/wiki?curid=21656016", "title": "Ang Trapaing Thmor", "text": "Ang Trapaing Thmor\n\nThe Ang Trapaing Thmor Crane Sanctuary is a protected area of Cambodia on the site of a large Khmer Rouge irrigation project built during the 1970s. The sanctuary was gazetted on 1 January 1999 and covers an area of 10,250 hectares. The reserve was set aside to protect the rare eastern sarus crane (\"Grus antigone sharpii\"). Prior to the discovery of the crane at Trapaing Thmor, there were thought to be fewer than 1,000 of the birds left alive in the world.\n\nThe crane sanctuary is located in north western Cambodia, not far from the border with Thailand. The entire 10,000 hectares of the protected area is contained within Phnom Srok District of Banteay Meanchey Province. Phnom Srok district shares a border with Siem Reap and Oddar Meanchey Provinces. Reading from the north clockwise, Phnom Srok borders with Banteay Ampil and Chong Kal districts of Oddar Meanchey province to the north. The eastern border of the district is shared with Srei Snam and Kralanh districts of Siem Reap province. To the south the district shares a border with Preah Net Preah District of Banteay Meanchey. The western border of the district joins with Svay Chek and Thmor Pouk districts also of Banteay Meanchey. The sanctuary can be accessed by road from Sisophon (70 km) or Siem Reap (city) (90 km) via National Highway 6 initially then via smaller unsealed district roads to Ang Trapaing Thmor.\n\nThe site of Ang Trapaing Thmor reservoir is also the site of an ancient Angkorian causeway.\nThe sarus crane, \"Grus antigone\" is an all-year resident breeding bird in northern Pakistan and India (especially Central India and the Gangetic plains), Nepal, Southeast Asia and Queensland, Australia. It is a very large crane, averaging 156 cm (5 ft) in length, which is found in freshwater marshes and plains.\n\nAng Trapaing Thmor sanctuary is also an important conservation area for a number of other threatened species. One globally threatened primate species, the long tailed macaque (\"Macaca fascicularis\") is found within the sanctuary. One globally threatened ungulate species—the Eld's deer (\"Rucervus eldii\")—and three globally threatened turtle species—the Asian box turtle (\"Cuora amboinensis\"), the Malayan snail-eating turtle (\"Malayemys subtrijuga\"), and the elongated tortoise (\"Indotestudo elongata\")—are also found within Ang Trapaing Thmor.\n"}
{"id": "11253981", "url": "https://en.wikipedia.org/wiki?curid=11253981", "title": "Anthedon (mythology)", "text": "Anthedon (mythology)\n\nIn Greek mythology, there were several people named Anthedon (Ancient Greek: Ἀνθηδών means \"rejoicing in flowers\") — at least two male and one female.\n\n"}
{"id": "87872", "url": "https://en.wikipedia.org/wiki?curid=87872", "title": "Antiproton", "text": "Antiproton\n\nThe antiproton, , (pronounced \"p-bar\") is the antiparticle of the proton. Antiprotons are stable, but they are typically short-lived, since any collision with a proton will cause both particles to be annihilated in a burst of energy.\n\nThe existence of the antiproton with −1 electric charge, opposite to the +1 electric charge of the proton, was predicted by Paul Dirac in his 1933 Nobel Prize lecture. Dirac received the Nobel Prize for his previous 1928 publication of his Dirac equation that predicted the existence of positive and negative solutions to the Energy Equation (formula_1) of Einstein and the existence of the positron, the antimatter analog to the electron, with positive charge and opposite spin.\n\nThe antiproton was first experimentally confirmed in 1955 at the Bevatron particle accelerator by University of California, Berkeley physicists Emilio Segrè and Owen Chamberlain, for which they were awarded the 1959 Nobel Prize in Physics. In terms of valence quarks, an antiproton consists of two up antiquarks and one down antiquark (). The properties of the antiproton that have been measured all match the corresponding properties of the proton, with the exception that the antiproton has electric charge and magnetic moment that are the opposites of those in the proton. The questions of how matter is different from antimatter, and the relevance of antimatter in explaining how our universe survived the Big Bang, remain open problems—open, in part, due to the relative scarcity of antimatter in today's universe.\n\nAntiprotons have been detected in cosmic rays for over 25 years, first by balloon-borne experiments and more recently by satellite-based detectors. The standard picture for their presence in cosmic rays is that they are produced in collisions of cosmic ray protons with nuclei in the interstellar medium, via the reaction, where A represents a nucleus:\n\nThe secondary antiprotons () then propagate through the galaxy, confined by the galactic magnetic fields. Their energy spectrum is modified by collisions with other atoms in the interstellar medium, and antiprotons can also be lost by \"leaking out\" of the galaxy.\n\nThe antiproton cosmic ray energy spectrum is now measured reliably and is consistent with this standard picture of antiproton production by cosmic ray collisions. These experimental measurements set upper limits on the number of antiprotons that could be produced in exotic ways, such as from annihilation of supersymmetric dark matter particles in the galaxy or from the Hawking radiation caused by the evaporation of primordial black holes. This also provides a lower limit on the antiproton lifetime of about 1-10 million years. Since the galactic storage time of antiprotons is about 10 million years, an intrinsic decay lifetime would modify the galactic residence time and distort the spectrum of cosmic ray antiprotons. This is significantly more stringent than the best laboratory measurements of the antiproton lifetime:\n\n\nThe magnitude of properties of the antiproton are predicted by CPT symmetry to be exactly related to those of the proton. In particular, CPT symmetry predicts the mass and lifetime of the antiproton to be the same as those of the proton, and the electric charge and magnetic moment of the antiproton to be opposite in sign and equal in magnitude to those of the proton. CPT symmetry is a basic consequence of quantum field theory and no violations of it have ever been detected.\n\n\nAntiprotons were routinely produced at Fermilab for collider physics operations in the Tevatron, where they were collided with protons. The use of antiprotons allows for a higher average energy of collisions between quarks and antiquarks than would be possible in proton-proton collisions. This is because the valence quarks in the proton, and the valence antiquarks in the antiproton, tend to carry the largest fraction of the proton or antiproton's momentum.\n\nTheir formation requires energy equivalent to a temperature of 10 trillion K (10 K) and this does not tend to happen naturally. However, at CERN, protons are accelerated in the Proton Synchrotron to an energy of 26 GeV, and then smashed into an iridium rod. The protons bounce off the iridium nuclei with enough energy for matter to be created. A range of particles and antiparticles are formed, and the antiprotons are separated off using magnets in vacuum.\n\nIn July 2011, the ASACUSA experiment at CERN determined the mass of the antiproton to be times more massive than an electron. This is the same as the mass of a proton, within the level of certainty of the experiment.\n\nAntiprotons have been shown within laboratory experiments to have the potential to treat certain cancers, in a similar method currently used for ion (proton) therapy. The primary difference between antiproton therapy and proton therapy is that following ion energy deposition the antiproton annihilates depositing additional energy in the cancerous region.\n\nIn October 2017, scientists working on the BASE experiment at CERN reported a measurement of the antiproton magnetic moment to a precision of 1.5 parts per billion. It is consistent with the most precise measurement of the proton magnetic moment (also made by BASE in 2014), which supports the hypothesis of CPT symmetry. This measurement represents the first time that a property of antimatter is known more precisely than the equivalent property in matter.\n\n"}
{"id": "41083788", "url": "https://en.wikipedia.org/wiki?curid=41083788", "title": "Aurivillius phases", "text": "Aurivillius phases\n\nAurivillius phases are a form of perovskite represented by the general formulae is (BiO)(ABO) (where A is a large 12 co-ordinate cation, and B is a small 6 co-ordinate cation). \n\nBasically, their structure is built by alternating layers of [BiO] and pseudo-perovskite blocks, with perovskite layers that are \"n\" octahedral layers in thickness. This crystal structure was first described in 1949 by B. Aurivillius. The first interest in Aurivillius phases arose from the observation of ferroelectricity even for the simplest member, BiWO (n=1) of this crystallographic family. The Mo-homologous Aurivillius phase BiMoO was recently investigated as a potential LTCC material. \nTheir oxide ion-conducting properties of Aurivillius phases were first discovered in the 1970s by Takahashi et al., and they have been used too for this purpose ever since.\n"}
{"id": "5736902", "url": "https://en.wikipedia.org/wiki?curid=5736902", "title": "Bluestone State Park", "text": "Bluestone State Park\n\nBluestone State Park is a state park in Summers County, West Virginia. The park is located along the western shore of Bluestone Lake, an impoundment of the New River built and managed by the U.S. Army Corps of Engineers. The park and lake are named after the Bluestone River, that flows into the New River at the park.\n\n\n"}
{"id": "44220410", "url": "https://en.wikipedia.org/wiki?curid=44220410", "title": "Butchers Gap Conservation Park", "text": "Butchers Gap Conservation Park\n\nButcher Gap Conservation Park is a protected area located in the Limestone Coast of South Australia overlooking Lacepede Bay about south of the town of Kingston SE. The conservation park was proclaimed under the \"National Parks and Wildlife Act 1972\" in 1990. \n\nThe following statements from the conservation park’s management plan summarises its conservation significance:The park comprises the foredune and associated swale system surrounding a wetland area (including Salt lake and Butchers lake), bisected by the Butcher Gap Drain… The wetland supports an association of dense South Australian swamp paperbark (\"Melaleuca halmaturorum\") over marine meadow, while the remainder of the Park is a coastal scrub association. A lease of the area of Butcher Gap drain that bisects the Park has been negotiated with the South Eastern Water Conservation and Drainage Board. The park is an important seasonal habitat for migratory bird species. The wetlands support a variety of waterfowl, including some migratory waders, however, its value to these species is not well understood… The park is recognised as suitable habitat for the endangered orange-bellied parrot (\"Neophema chrysogaster\"), which has been observed feeding on two-horned searocket (\"Cakile maritima\") near the beach and in the extensive samphire habitat around Salt Lake.\n\nThe conservation park is classified as an IUCN Category III protected area.\n\n"}
{"id": "3198504", "url": "https://en.wikipedia.org/wiki?curid=3198504", "title": "CHIPSat", "text": "CHIPSat\n\nCHIPSat (Cosmic Hot Interstellar Plasma Spectrometer satellite) is a now-decommissioned, but still-orbiting, microsatellite. It was launched on January 12, 2003 from Vandenberg Air Force Base aboard a Delta II with the larger ICESat, and had an intended mission duration of one year. CHIPSat was the first of NASA's University-Class Explorers (UNEX) mission class. It performed spectroscopy from 90 to 250 angstroms (9 to 26 nm), extreme ultraviolet light.\n\nThe primary objective of the science team, led by Principal Investigator Mark Hurwitz, was to study the million-degree gas in the local interstellar medium. CHIPSat was designed to capture the first spectra of the faint, extreme ultraviolet glow that is expected to be emitted by the hot interstellar gas within about 300 light-years of the Sun, a region often referred to as the Local Bubble. Surprisingly, these measurements produced a null result, with only very faint EUV emissions detected, despite theoretical expectations of much stronger emissions.\n\nIt was the first U.S. mission to use TCP/IP for end-to-end satellite operations control.\n\nThe University of California, Berkeley's Space Sciences Laboratory served as CHIPSat's primary groundstation and manufactured the CHIPS spectrograph, designed to perform all-sky spectroscopy. Other ground network support was provided by groundstations at Wallops Island, Virginia and Adelaide, Australia. CHIPSat's spacecraft platform was manufactured by SpaceDev.\n\nIn September 2005 the spacecraft was converted to a solar observatory. From April 3, 2006 to April 5, 2008 CHIPsat performed 1458 observations of the Sun.\n\nSatellite operations were terminated in April 2008.\n\n\n"}
{"id": "1724862", "url": "https://en.wikipedia.org/wiki?curid=1724862", "title": "Capsella bursa-pastoris", "text": "Capsella bursa-pastoris\n\nCapsella bursa-pastoris, known by its common name shepherd's purse because of its triangular flat fruits, which are purse-like, is a small annual and ruderal flowering plant in the mustard family Brassicaceae that grows up to tall. It is native to eastern Europe and Asia minor, but is naturalized and considered a common weed in many parts of the world, especially in colder climates, including British Isles, where it is regarded as an archaeophyte, North America and China, but also in the Mediterranean and North Africa. \"C. bursa-pastoris\" is the second most common weed in the world.\n\n\"C. bursa-pastoris\" plants grow from a rosette of lobed leaves at the base. From the base emerges a stem about tall, which bears a few pointed leaves which partly grasp the stem. The flowers, which appear in any month of the year in the British Isles, are white and small, in diameter, with four petals and six stamens. They are borne in loose racemes, and produce flattened, two-chambered seed pods known as siliques, which are triangular to heart-shaped, each containing several seeds.\n\nLike a number of other plants in several plant families, its seeds contain a substance known as mucilage, a condition known as myxospermy. The adaptive value of myxospermy is unknown, although the fact that mucilage becomes sticky when wet has led some to propose that \"C. bursa-pastoris\" traps insects which then provide nutrients to the seedling, which would make it protocarnivorous.\n\n\"Capsella bursa-pastoris\" is closely related to the model organism such as \"Arabidopsis thaliana\" and is also used as a model organism, due to the variety of genes expressed throughout its life cycle that can be compared to genes that are well studied in \"A. thaliana\". Unlike most flowering plants, it flowers almost all year round. Like other annual ruderals exploiting disturbed ground, \"C. bursa-pastoris\" reproduces entirely from seed, has a long soil seed bank, and short generation time, and is capable of producing several generations each year.\n\nIt was formally described by the Swedish botanist Carl Linnaeus in his seminal publication 'Species Plantarum' in 1753, and then published by Friedrich Kasimir Medikus in \"Pflanzen-Gattungen\" (Pfl.-Gatt.) on page 85 in 1792.\n\n\"Capsella bursa-pastoris\" subsp. \"thracicus\" (Velen.) Stoj. & Stef. is the only known subspecies.\n\nWilliam Coles (botanist) wrote in his book, 'Adam in Eden' (published in 1657), \"It is called Shepherd's purse or Scrip (wallet) from the likeness of the seed hath with that kind of leathearne bag, wherein Shepherds carry their Victualls [food and drink] into the field.\"\"\nIn England and Scotland, it was once commonly called 'mother's heart', which is derived from a child's game/trick of picking the seed pod, which then would burst and the child would be accused of 'breaking his mother's heart'.\n\nCommon on cultivated ground, waysides and meadows, disturbed land and roadworks.\n\n\"C. bursa-pastoris\" is gathered from the wild, or grown. It has many uses, including for food, to supplement animal feed, for cosmetics, and in traditional medicine. It is cultivated as a commercial food crop in Asia.\n\nIn Chinese, this plant is known as \"jìcài\" (; ). It is commonly used in food in Shanghai and the surrounding Jiangnan region, where it is stir-fried with rice cakes and other ingredients or as part of the filling in wontons.\n\nIt is one of the ingredients of the symbolic dish consumed in the Japanese spring-time festival, \"Nanakusa-no-sekku\".\n\nIn Korea, it is known as \"naengi\" () and used as a root vegetable in the characteristic Korean dish, namul (fresh greens and wild vegetables).\n\nFumaric acid is one chemical substance that has been isolated from \"C. bursa-pastoris\".\n\n\n\n"}
{"id": "10234843", "url": "https://en.wikipedia.org/wiki?curid=10234843", "title": "Ceto (disambiguation)", "text": "Ceto (disambiguation)\n\nCeto may refer to:\n\n\n\n"}
{"id": "79539", "url": "https://en.wikipedia.org/wiki?curid=79539", "title": "Charites", "text": "Charites\n\nIn Greek mythology, a Charis (; , ) or Grace is one of three or more minor goddesses of charm, beauty, nature, human creativity, and fertility, together known as the Charites ( ) or Graces. The usual list, from youngest to oldest, is Aglaea (\"Splendor\"), Euphrosyne (\"Mirth\"), and Thalia (\"Good Cheer\"). In Roman mythology they were known as the , the \"Graces\". In some variants, Charis was one of the Graces and was not the singular form of their name.\n\nThe Charites were usually considered the daughters of Zeus and Eurynome, though they were also said to be daughters of Dionysus and Aphrodite or of Helios and the naiad Aegle. Other possible names of their mother by Zeus are Eurydome, Eurymedousa, and Euanthe. Homer wrote that they were part of the retinue of Aphrodite. The Charites were also associated with the Greek underworld and the Eleusinian Mysteries.\n\nThe river Cephissus near Delphi was sacred to the three goddesses.\n\nAlthough the Graces usually numbered three, according to the Spartans, Cleta, not Thalia, was the third, and other Graces are sometimes mentioned, including Auxo, Hegemone, Peitho, Phaenna, Pasithea and Charis or Cale. An ancient vase painting attests the following names as five: Antheia (\"Blossoms\"), Eudaimonia (\"Happiness\"), Paidia (\"Play\"), Pandaisia (\"Banquet\"), Pannychis (\"Night Festivities\")—all referring to the Charites as patronesses of amusement and festivities.\n\nPausanias interrupts his \"Description of Greece\" (book 9.xxxv.1–7) to expand upon the various conceptions of the Graces that had developed in different parts of mainland Greece and Ionia:\n\nThe Charites was most commonly depicted in the sanctuaries of other gods, but they did have their own temples as well, and at least four temples exclusively to them are known from Greece. The two main cult centres of the Charites were the town of Orkhomenos (Orchomenus) in northern Boiotia, and the Aegean island of Paros. \n\nThere were temples to the Charites in Hermione, in Sparta and in Elis: \nThe temple regarded as their perhaps most important was the Temple of the Charites in Orkhomenos, where their cult was thought to have originated: \nStrabo wrote :\n\nThe river Cephissus near Delphi was sacred to the three goddesses.\n\nOn the representation of the Graces, Pausanias wrote,\n\nDuring the Renaissance, the Roman statue group of the three graces in the Piccolomini library in Duomo di Siena inspired most themes.\n\nThe Charites are depicted together with several other mythological figures in Sandro Botticelli's painting \"Primavera\". Raphael also pictured them in a painting now housed in Chantilly in France. Among other artistic depictions, they are the subject of famous sculptures by Antonio Canova and Bertel Thorvaldsen.\n\nA group of three trees in the Calaveras Big Trees State Park are named \"The Three Graces\" after the Charites.\n\n\n\n\"(The Imagebase links are all broken)\"\n\n"}
{"id": "57000621", "url": "https://en.wikipedia.org/wiki?curid=57000621", "title": "Charleston Conservation Park", "text": "Charleston Conservation Park\n\nCharleston Conservation Park is a protected area located in the Australian state of South Australia in the locality of Charleston in the Adelaide Hills state government region about east of the state capital of Adelaide and about north of the town centre in Lobethal.\n\nThe conservation park consists of land in section 3943 in the cadastral unit of the Hundred of Onkaparinga. It was proclaimed under the \"National Parks and Wildlife Act 1972\" on 8 April 1976. As of 2016, it covered an area of .\n\nIn 1980, it was described as follows:Charleston Conservation Park preserves a pristine remnant representative of the transition between the wetter stringy bark forests on the western side of the Mount Lofty Ranges and the drier mallee woodlands to the east. A large diversity of flora and fauna are represented in the park including at least seventy-six bird species. An area of gently undulating relief featuring three main woodland associations. These being, a \"Casuarina stricta\" association with scattered \"Eucalyptus leucoxylon\" / \"E. viminalis\", a \"E. leucoxylon\" association and a \"Banksia marginata\" association. The understorey is dominated by \"Acacia pycnantha\" with occasional thickets of \"Leptospermum myrsinoides\" and \"Xanthorrhoea semiplana\". Small, regenerating stands of \"Acacia melanoxylon\" and \"Callitris preissii\" are of interest. Charleston Conservation Park is in a near pristine condition despite its cultural surrounds, having never been grazed…\n\nThe conservation park is classified as an IUCN Category III protected area. In 1980, it was listed on the now-defunct Register of the National Estate.\n\n \n"}
{"id": "5069898", "url": "https://en.wikipedia.org/wiki?curid=5069898", "title": "Dynamic demand (electric power)", "text": "Dynamic demand (electric power)\n\nDynamic Demand is the name of a semi-passive technology for adjusting load demands on an electrical power grid. (It is also the name of an independent not-for-profit organization in the UK supported by a charitable grant from the Esmée Fairbairn Foundation dedicated to promoting this technology.) The concept is that by monitoring the frequency of the power grid, as well as their own controls, intermittent domestic and industrial loads switch themselves on/off at optimal moments to balance the overall grid load with generation, reducing critical power mismatches. As this switching would only advance or delay the appliance operating cycle by a few seconds, it would be unnoticeable to the end user. This is the foundation of dynamic demand control. In the United States, in 1982, a (now-lapsed) patent for this idea was issued to power systems engineer Fred Schweppe. Other patents have been issued based on this idea. \n\nDynamic demand is similar to demand response mechanisms to manage domestic and industrial consumption of electricity in response to supply conditions, for example, having electricity customers reduce their consumption at critical times or in response to prices. The difference is that dynamic demand devices passively shut off when stress in the grid is sensed, whereas demand response mechanisms respond to transmitted requests to shut off,\n\nThe power utilities are able to predict to a reasonable accuracy (generally to within one or two percent) the demand pattern throughout any particular day. This means that the free market in electricity is able to schedule just enough base load in advance. Any remaining imbalance would then be due either to inaccuracies in the prediction, or unscheduled changes in supply (such as a power station fault) and/or demand. Such imbalances are removed by requesting generators to operate in so called \"frequency response mode\" (also called \"frequency control mode\"), altering their output continuously to keep the frequency near the required value.\n\nThe grid frequency is a system-wide indicator of overall power imbalance. For example, it will drop if there is too much demand because generators will start to slow down slightly. A generator in frequency-response mode will, under nominal conditions, run at reduced output in order to maintain a buffer of spare capacity. It will then continually alter its output on a second-to-second basis to the needs of the grid with droop speed control.\n\nThis spinning reserve is a significant expense to the power utilities as often fuel must be burned or potential power sales lost to maintain it. The kind of generation used for fast response is usually fossil fuel powered which produces emissions of between 0.48 and 1.3 tonnes of CO equivalent for every megawatt hour (MWh) generated. Thus a significant environmental burden, in the form of increased greenhouse gas emissions, is associated with this imbalance.\n\nIn principle, any appliance that operates to a duty cycle (such as industrial or domestic air conditioners, water heaters, heat pumps and refrigeration) could be used to provide a constant and reliable grid balancing service by timing their duty cycles in response to system load.\n\nBecause it is possible to measure grid frequency from any power outlet on the grid, it is possible to design controllers for electrical appliances that detect any frequency imbalance in real time. Dynamic-demand enabled appliances would react to this same signal. When the frequency decreases they would be more likely to switch off, reducing the load on the grid and helping to restore the balance. When the frequency increases past the standard, they would be more likely to switch on, using up the excess power. Obviously, the controller must also ensure that at no point does the appliance stray out of its acceptable operating range. As line frequency is directly related to the speed of rotation of generators on the system, millions of such devices acting together would act like a huge, fast-reacting peaking power plant.\n\nThe dynamic controller could also provide other ancillary services, such as aiding blackstart recovery—the ability of a power grid to be brought back to service after a power outage – if programmed with that function. Generally blackstarts are made more difficult because of the large number of reactive loads attempting to draw power simultaneously at start up when voltages are low. This causes huge overloads that trip local breakers delaying full system recovery. The dynamic controller could have these loads \"wait their turn\", as it were, until full power had been restored.\n\nAnother vital balancing service is ‘fast reserve’ which is the use of standby plant to replace possible lost generation (e.g. due to a failed power generator or lost power line). By shedding load quickly while the running generators spin up, then switching back in to bring the frequency back to standard, dynamic controllers could spare the high cost of fast reserve generators. Also the fast response speed of this method would avoid possible brownouts occurring.\n\nThe technology could also help facilitate greater use of generation from variable sources, like wind power. Demand-side techniques could be an efficient and cost-effective way to help integrate this resource onto the grid. In particular it would allow these sources to work in conjunction with virtual power reserves like municipal water towers to provide a reasonably predictable dispatchable capacity.\n\nDynamic demand devices have the potential to save considerable amounts of energy by the services they provide. But before dynamic demand control can be widely incorporated regulation must be put in place to mandate installation on at least new appliances or an effective market mechanism must be created to reward installation of the technology fairly. One method contemplated is to enable the electricity meter that measures the electricity consumption also measure the grid frequency, and switch to a higher tariff if the frequency drops below a certain level. The monthly electricity bill will then say that so many hours (and so many kilowatt hours) were on the Regular tariff and a few hours on the Short Supply tariff. Those consumers without smart demand management have to pay the extra cost, but those who install smart technologies that adapt to the short supply periods will save money.\n\nOn 1 March 2011, RLtec launched its Dynamic Demand frequency response service in hot water and HVAC load devices distributed across one of the UK’s largest supermarket chains, Sainsbury's. This megawatt scale virtual power plant service provides commercial frequency regulating response to National Grid in the UK. The company is now called Open Energi.\n\nThe national grid in the UK already is a massive user of this technology at an industrial scale - up to 2 GW of load can be lost instantaneously by frequency sensitive relays switching off steelworks etc., which is matched over a 20-minute cycle by up to 2 GW of quite small emergency diesel generators. For a complete description of this complex system see for example \"Emergency Diesel Standby Generator’s Potential Contribution to Dealing With Renewable Energy Sources Intermittency And Variability\" - a talk by David Andrews of Wessex Water who works closely with the UK National Grid to provide this service, given at the Open University Seminar \"Coping with Variability - Integrating Renewables into the Electricity System\" 24 January 2006.\nUp to 5 GW of such diesel generation is used in France for similar purposes, but these technologies seem to be relatively unknown . There is no reason they should not be massively increased in scope to cope with even the intermittence introduced by wind power.\n\nIn August 2007, the UK government published a report outlining what potential it sees for dynamic demand technology. The report stops short of recommending the government encourage its introduction. It lists a number of technical and economic barriers to its introduction and recommends these be investigated before the government encourage the use of dynamic demand. Dynamic demand is one element of a wider government investigation into technologies that can cut greenhouse gas emissions.\n\nHowever, in 2009 it was announced that domestic refrigerators are now being sold into the UK incorporating a dynamic load control system \n\n"}
{"id": "4072264", "url": "https://en.wikipedia.org/wiki?curid=4072264", "title": "Earth Interactions", "text": "Earth Interactions\n\nEarth Interactions (EI) is a peer-reviewed scientific journal published by the American Meteorological Society, American Geophysical Union, and Association of American Geographers. \"EI\" publishes research on the interactions among the atmosphere, hydrosphere, biosphere, cryosphere, and lithosphere, including, but not limited to, research on human impacts, such as land cover change, irrigation, dams/reservoirs, urbanization, pollution, and landslides.\n\nThe journal is abstracted and indexed by Compendex, GEOBASE, GeoRef, Scopus, Current Contents, and EBSCO, and ProQuest databases.\n\n"}
{"id": "580067", "url": "https://en.wikipedia.org/wiki?curid=580067", "title": "Ecoinformatics", "text": "Ecoinformatics\n\nEcoinformatics, or ecological informatics, is the science of information (Informatics) in Ecology and Environmental science. It integrates environmental and information sciences to define entities and natural processes with language common to both humans and computers. However, this is a rapidly developing area in ecology and there are alternative perspectives on what constitutes ecoinformatics. \n\nA few definitions have been circulating, mostly centered on the creation of tools to access and analyze natural system data. However, the scope and aims of ecoinformatics are certainly broader than the development of metadata standards to be used in documenting datasets. Ecoinformatics aims to facilitate environmental research and management by developing ways to access, integrate databases of environmental information, and develop new algorithms enabling different environmental datasets to be combined to test ecological hypotheses. \n\nEcoinformatics characterize the semantics of natural system knowledge. For this reason, much of today's ecoinformatics research relates to the branch of computer science known as Knowledge representation, and active ecoinformatics projects are developing links to activities such as the Semantic Web. \n\nCurrent initiatives to effectively manage, share, and reuse ecological data are indicative of the increasing importance of fields like Ecoinformatics to develop the foundations for effectively managing ecological information. Examples of these initiatives are National Science Foundation Datanet projects, DataONE and Data Conservancy.\n\n"}
{"id": "13042135", "url": "https://en.wikipedia.org/wiki?curid=13042135", "title": "Electricity sector in Colombia", "text": "Electricity sector in Colombia\n\nThe electricity sector in Colombia is dominated by large hydropower generation (65%) and thermal generation (35%). Despite the country’s large potential in new renewable energy technologies (mainly wind, solar and biomass), this potential has been barely tapped. A 2001 law designed to promote alternative energies lacks certain key provisions to achieve this objective, such as feed-in tariffs, and has had little impact so far. Large hydropower and thermal plants dominate the current expansion plans. The construction of a transmission line with Panama, which will link Colombia with Central America, is underway.\n\nAn interesting characteristic of the Colombian electricity sector (as well as of its water sector) is a system of cross-subsidies from users living in areas considered as being relatively affluent, and from users consuming higher amounts of electricity, to those living in areas considered as being poor and to those who use less electricity.\n\nThe electricity sector has been unbundled into generation, transmission, distribution and commercialization since sector reforms carried out in 1994. About half the generation capacity is privately owned. Private participation in electricity distribution is much lower\n\nElectricity supply in Colombia relies on the National Interconnected System (SIN) and several isolated local systems in the Non-Interconnected Zones (ZNI). SIN encompasses one third of the territory, giving coverage to 96 percent of the population. The ZNI, which covers the remaining two thirds of the national territory, only serves 4 percent of the population.\n\nThirty-two large hydroelectric plants and thirty thermal power stations feed electricity into the SIN. On the other hand, the ZNI is mostly served by small diesel generators, many of which are not in good working conditions. At June 2015, installed net effective capacity was 15.5 Gigawatt (GW), with the following share by source:\n\nThe share of thermal participation in generation has increased since the mid-1990s. This has happened in response to the 1992/1993 crisis caused by El Niño-Southern Oscillation associated droughts and the high reliance of power generation on hydroelectric installations that lacked multi-year storage capacity. As a result of the new policies adopted by the country, the dominance of hydropower in the generation portfolio has been reduced from 80 percent in the early 1990s to less than 65 percent today. The expansion path involved adding 1,500 MW of new capacity, equally distributed between hydro and thermal sources, by 2011. This will entail investments of US$258 million per year.\n\nTotal electricity production in 2005 was 50.4 Terawatt-hour (TWh). Hydroelectric plants generated 81.2 percent, thermal plants 18.6 percent and the Jepírachi wind plant 0.1 percent of the total.\n\nIn 2005, total electricity consumption was 48.8 TWh, which corresponds to an average energy consumption per capita of 828 kW·h per year. Consumption per sector is divided as follows:\n\n\nDemand is growing by approximately 4 percent annually.\n\nColombia is a net power exporter. In 2005 the country exported 1.76 TWh of electricity to Ecuador (3.5% of total production). It imported only very small volumes of electricity from Venezuela and Ecuador (0.02 TWh each). According to the Ministry of Mines and Energy, exports are estimated to increase at 5 percent annually. \n\nThe Puebla Panama Plan includes a project of electric interconnection between Colombia and Panama that will allow the integration of Colombia with Central America. This project, carried out by Interconexión Eléctrica S.A. (ISA) in Colombia and Empresa de Transmisión Eléctrica S.A. (ETESA) in Panama, entails the construction of a transmission line with 300 MW capacity (3% of installed capacity) from Colombia to Panama and 200 MW capacity in the reverse way. The line is expected to become operational in 2010.\n\nIn 2005, the interconnected electricity system served 87 percent of the population, a percentage that is below the 95 percent average for Latin America and the Caribbean. In Colombia, electricity coverage is 93 percent in urban areas and 55 percent in rural areas. About 2.3 million people do not have access to electricity yet.\n\nAs in other countries, the zones outside the interconnected system pose especially challenging conditions for electrification, as well major inadequacies in service provision. This system, whose installed capacity is almost exclusively diesel-based, suffers from major diseconomies of scale as 80 percent of capacity is in plants below the 100 kW threshold.\n\nService quality in Colombia, as measured by service interruptions, is much lower than the average for Latin America and the Caribbean. In 2005, the average number of interruptions per subscriber was 185.7, far above the regional average of 13 interruptions. The duration of interruptions per subscriber was 66 hours, also far above the regional average of 14 hours.\n\nLosses in transmission and leaks are still a concern, even if the total amount has decreased in the last years. Distribution losses in 2005 were 16 percent, compared to 13.6% average in Latin America and the Caribbean (LAC).\n\nColombia has had a liberalized energy market since 1995. The sector is characterized by an unbundled generation, transmission, distribution, and commercialization framework.\n\nThe structure of the Colombian energy market is based on Laws 142 (Public Services Law) and 143 (Electricity Law) of 1994. The Ministry of Mines and Energy is the leading institution in Colombia’s energy sector. Within the Ministry, the Unit for Mining and Energy Planning (UPME) is responsible for the study of future energy requirements and supply situations, as well as for drawing up the National Energy Plan and Expansion Plan. \n\nThe Regulatory Commission for Gas and Energy (CREG) is in charge of regulating the market for the efficient supply of energy. It defines tariff structures for consumers and guarantees free network access, transmission charges, and standards for the wholesale market, guaranteeing the quality and reliability of the service and economic efficiency. Among others, CREG is responsible for providing regulations that ensure the rights of consumers, the inclusion of environmental and socially sustainable principles, improved coverage, and financial sustainability for participating entities. \n\nThe provision of public services (water, electricity, and telecommunications) to final users is supervised by the independent Superintendency for Residential Public Services, or SSPD.\n\nColombia has 66 registered electricity producers. Private companies own 60 percent of the installed generation capacity and account for 43 percent (measured in number of consumers) to 49 percent (measured in kWh sales) of energy supplied to the interconnected grid.\n\nJust three companies - the public companies Empresas Públicas de Medellín (EPM) and ISAGEN, as well as the private EMGESA - control altogether 52 percent of total generation capacity.\n\nTransmission in the National Interconnected System is carried out by seven different public companies, four of which work exclusively in transmission (ISA, EEB, TRANSELCA and DISTASA). The remaining three (EEPPM, ESSA and EPSA) are integrated companies that carry out all the activities in the electricity chain (i.e. generation, transmission and distribution). The largest company is Interconexión Eléctrica S.A. (ISA), which belongs to the government.\n\nCurrently, there are 28 pure commercializing companies; 22 distribution and commercialization ones; 8 ones that integrate generation, distribution and commercialization; and 3 fully integrated ones. The three largest players in commercialization are Unión Fenosa (with Electrocosta and Electrocaribe), Endesa (in Bogotá) and Empresas Públicas de Medellín (EPM).\n\nColombia has 28.1 MW installed capacity of renewable energy (excluding large hydro), consisting mainly of wind power. The country has significant small hydro, wind, and solar resources that remain largely unexploited. According to a study by the World Bank’s Energy Sector Management Assistance Program (ESMAP), exploitation of the country’s significant wind potential alone could cover more than the country’s current total energy needs.\n\nThe first historical landmark in the establishment of electric supply dates back from 1928, when Law 113 declared the exploitation of hydroelectric power of public interest. The system worked in a centralized manner, in which vertically integrated state companies maintained a monopoly in their corresponding regions. A public company, ISA, exchanged electricity among the different regional systems.\n\nDuring the 1980s, the sector suffered a crisis, similar to most countries in Latin America. The crisis was the result of subsidized tariffs, political influence in the state companies, and the delays and cost overruns of large generation projects.\n\nAt the beginning of the 1990s the government took steps to modernize the electricity sector, opening it to private participation. The restructuring was carried out through Laws 142 (Law of Public Services) and 143 (Electricity Law) of 1994, which defined the regulatory framework for the development of a competitive market. The new scheme, designed by the CREG, was implemented from July 1995 onwards.\n\nColombia has an ambitious reform agenda in the power sector. The country seeks to encourage foreign investment, with an emphasis on hydrocarbons and power capacity expansion; simplify modalities for small-scale energy projects; and renew interest in non-conventional renewable energy technologies with a regulatory framework to facilitate a gradual change in the energy mix.\n\nIn 2001, Law 697, which promotes the efficient and rational use of energy and alternative energies, was promulgated. This law was regulated by Decree 3683, issued in 2003. The law and the decree contemplate important aspects such as the stimulus to education and research in renewable energy sources (RES). Nevertheless, the program created under this law lacks fundamental aspects to impulse the development of RES significantly, such as a regulatory support system to encourage investment, the definition of policies to promote renewable energy, or quantitative targets for the share of renewable energy. \n\nLimitations such as the ones above present an important legal vacuum for renewable energy in Colombia. While there have been a few initiatives concerning efficient and rational use of energy (design of the Colombian program of normalization, accreditation, certification and labeling of final use of energy equipment, and promotion of carburant mixture for vehicle use and massive use of natural gas), there have been no recent initiatives related to new renewable energy technologies.\n\nThe electricity market in Colombia has regulated and non-regulated segments. The regulated market, which is directly contracted and supplied by distribution companies, applies to industrial, commercial, and residential users with power demands under 0.5MW. In this market, the tariff structure is established by the regulatory agency CREG. In the non-regulated market, consumers with power demands of 0.5 MW and above can negotiate freely and contract their supply in the wholesale market (i.e., spot and contracts markets) directly or through commercial entities, distributors, or producers.\n\nIn 2005, the average residential tariff was US$0.0979 per kWh, slightly below the LAC weighted average of US$0.115. The average industrial tariff was US$0.0975 per kWh, slightly below the LAC weighted average of US$0.107.\n\nBy law all urban areas in Colombia are classified in one of six socio-economic strata, which are used to determine the level of tariffs for electricity, water and other services. According to that system, consumers living in areas considered as poor - and consumers using low amounts of electricity - receive electricity and natural gas at subsidized tariffs. These cross-subsidies are almost entirely (approximately 98 percent) financed by consumers living in areas considered as being relatively affluent and who use more electricity. The cross-subsidies cover about 25 percent of the electricity and gas bill of low-income consumers. A special fund that covers the remaining amount not covered by consumers provided US$21.8 million in 2005. On average, 7.5 million people a month benefited from this fund. In addition, the fund provided subsidies of COP$ 17,159 million (US$7.4 million) to 1,808,061 natural gas users.\n\nSubsidies are also given to provide diesel for power production in non-grid-connected zones. While diesel in the interior of the country can cost in the order of US$0.8/gal, in remote areas it can cost in the order of US$4.5/gal because of high transport costs.\n\nThe subsidy stratification system in Colombia has proven fairly ineffective at channeling subsidies towards the poor. Although the scheme is broad in its coverage and excludes no more than 2 percent of the poor for services with broad coverage such as electricity, water and sanitation, there are also high leakage rates. Some 50-60 percent of subsidy beneficiaries are from the top half of the income distribution and, moreover, only 30-35 percent of subsidy resources are captured by the poor. Nevertheless, the performance of this subsidy scheme varies depending on the service considered, being water the sector with the poorest performance and telephony the one with the best behavior.\n\nA 2004 report by the World Bank estimated the following power sector investment needs for Colombia up to 2010:\n\nIn summary, the overall investment needs in the electricity generation, transmission and distribution sector total US$767 million per year. About 60 percent of that relates to maintenance obligations and payment of Power Purchase Agreement (PPA) guarantees, and the remaining 40 percent to new investment in generation and transmission. These investment needs are completely related to the SIN and do not take into account the needs associated with the ZNI.\n\nThere are three different funds and programs that support rural electrification in Colombia, each established at a different time with different purposes, and all administered by the Ministry of Mines and Energy. At the end of 2006, the Ministry of Mines and Energy had approved a total of US$23.3 million rural electrification funds from these three funds and programs directed to benefit 14,965 families.\n\nThe Fund for the Electrification of Non-interconnected Zones (FANZI), was established in 2000 to assist isolated regions in the zones outside the interconnected system. It contemplated both the expansion of existing networks and the establishment of stand-alone solutions.\n\nIn 2003, a special fund known as Rural Electrification Fund (FAER), of similar characteristics to the FAZNI, was established to subsidize investment in rural areas of the interconnected system. The fund was designed to collect a surcharge of US$0.40 per MWh of electricity sold to the wholesale market, which would yield approximately US$18 million per year. Projects are presented to the FAER by the local government authorities. In order to be eligible, they must form part of the local development plan and the investment plan of the corresponding distribution utility and must also pass through the national project screening and evaluation system.\nElectrification projects also receive support from the Program for Network Normalization (PRONE) that draws its resources from the National Development Plan funds.\n\nThe Institute for the Investigation and Application of Energy Solutions (IPSE) supports the Ministry of Mines and Energy in its efforts to promote rural electrification.\n\nColombia has had a liberalized energy market since 1995. The sector is characterized by an unbundled generation, transmission, distribution, and commercialization framework.\n\nWith 66 registered electricity producers, private companies own 60 percent of the installed generation capacity and account for 43 percent (measured in number of consumers) to 49 percent (measured in kWh sales) of energy supplied to the interconnected grid. Transmission is carried out by seven different public companies, while distribution and commercialization are in the hands of over 60 companies, both public and private.\n\nThe Ministry of the Environment, Housing and Territorial Development holds the environmental responsibilities in Colombia and leads the country’s commitment towards sustainable development. Within the Ministry, the Climate Change Mitigation Group addresses all the issues related with climate change.\n\nBecause of Colombia’s abundant hydroelectric potential, greenhouse gas emissions are very low per capita (1.3 te) and per unit of GDP (0.2 te).\n\nThe Latin American Energy Organization (OLADE) estimated that emissions from electricity production in 2003 were 6.5 million tons of . Currently 30 percent of emissions in Colombia come from the power sector, but these could increase if thermal generation gains a larger part of the energy mix.\n\nAs of August 2007, there are three registered Clean Development Mechanism (CDM) projects in the electricity sector in Colombia, with overall estimated emission reductions of 107,465 te per year.\n\nThe Jepírachi project, in the Uribia region, is Colombia’s first and only wind farm. This 19.5 MW project is expected to displace an estimated 430,000 t until 2019. The Jepírachi project is now in its fourth year of operation. It generated about 144 GWh and displaced about 48,500 te from February 2004 to August 2006.\n\nThe other two registered projects are the Santa Ana Hydroelectric Plant, in the Bogotá’s suburb Usaquén, with estimated emission reductions of 20,642 te per year; and the La Vuelta and La Herradura Hydroelectric Project, in the Antioquia Department, with estimated emission reductions of 69,795 te per year.\n\nThe Inter-American Development Bank has currently one energy project under implementation in Colombia, the Porce III Hydroelectric Power Plant, owned by Empresas Públicas de Medellín and approved in October 2005. This is a US$900 million project, of which the IDB is contributing US$200 million.\n\nIn addition, the IDB is supporting the Colombia-Panama electric interconnection project through US$1.5 million financing for the feasibility studies phase.\n\n\nESMAP, 2007. \"Review of Policy Framework for Increased Reliance on Renewable Energy in Colombia.\" In press \n\nMinistry of Mines and Energy & UPME, 2006. \"Plan de Expansión de Referencia: Generación, Transmisión. 2006-2020.\"\n\n"}
{"id": "9577836", "url": "https://en.wikipedia.org/wiki?curid=9577836", "title": "Energy and Utility Skills", "text": "Energy and Utility Skills\n\nEnergy & Utility Skills is an employer-led membership organisation that helps to ensure the gas, power, waste management and water industries have the skills they need - now and in the future.\n\nThrough a range of products and services they help employers attract new talent, develop their existing workforces, and assure a high level of competence across their businesses. \nAs the UK authority on professional development and employment in the energy and utilities industries, Energy & Utility Skills help their members embrace new talent and technology to meet the challenges of a competitive global market. Market intelligence is central to their approach. From projecting skills gaps to benchmarking standards, they provide the most accurate, up-to-date information on skills and employment in the energy and utilities sector. \nTheir partnerships with employers, Government bodies and educational institutions help them to support the UK's agenda, shape the future of the sector's workforce and ensure their stakeholders get the most from their investments.\n\n\nThe Chief Executive of Energy & Utility Skills is Nick Ellins. The headquarters of Energy & Utility Skills is in Solihull, West Midlands.\n\nThe National Skills Academy for Power in Solihull is part of the Energy & Utility Skills Group.\n\n\n"}
{"id": "23391668", "url": "https://en.wikipedia.org/wiki?curid=23391668", "title": "Energy in Burundi", "text": "Energy in Burundi\n\nEnergy in Burundi is a growing industry with tremendous potential.\n\nBujumbura and Gitega are the only two cities in Burundi that have municipal electricity service. Burundi's total installed capacity was 49 MW in 2001. Two dams completed since 1984 have increased the amount of power production from hydroelectric installations. In 2001, estimated production of electricity totaled 155 GWh, of which 154 GWh was from hydroelectric sources, with geothermal and thermal sources accounting for the rest. Consumption in 2001 was estimated at 170 GWh.\n\nBurundi imports all of its petroleum products from Kenya and Tanzania, and has no known reserves of petroleum or natural gas. Consumption of oil in 2001 is estimated at 3,000 barrels per day. Burundi is estimated to have no known consumption of natural gas in 2001. A subsidiary of BP has an oil exploratory concession in and around Lake Tanganyika.\n\nWood and peat account for 94% of energy consumption in Burundi. Peat offers an alternative to increasingly scarce firewood and charcoal as a domestic energy source. The government is promoting peat production and is fostering the development of renewable energy resources, such as solar electricity and biogas.\n"}
{"id": "562598", "url": "https://en.wikipedia.org/wiki?curid=562598", "title": "Fabian Gottlieb von Bellingshausen", "text": "Fabian Gottlieb von Bellingshausen\n\nFabian Gottlieb Thaddeus von Bellingshausen (, tr. ; – ), a Baltic German naval officer in the Imperial Russian Navy, cartographer and explorer, ultimately rose to the rank of admiral. He participated in the first Russian circumnavigation of the globe and subsequently became a leader of another circumnavigation expedition that discovered the continent of Antarctica.\n\nBellingshausen started his service in the Baltic Fleet, and after distinguishing himself joined the First Russian circumnavigation of the Earth in 1803-1806, serving on the merchant ship \"Nadezhda\" under the captaincy of Adam Johann von Krusenstern. After the journey he published a collection of maps of the newly explored areas and islands of the Pacific Ocean. Subsequently, he commanded several ships of the Baltic and Black Sea Fleets.\n\nAs a prominent cartographer, Bellingshausen was appointed to command the Russian circumnavigation of the globe in 1819-1821, intended to explore the Southern Ocean and to find land in the proximity of the South Pole. Mikhail Lazarev prepared the expedition and was made Bellingshausen's second-in-command and the captain of the sloop \"Mirny\", while Bellingshausen himself commanded the sloop \"Vostok\". During this expedition Bellingshausen and Lazarev became the first explorers to see the land of Antarctica on 27 January 1820 (New Style). They circumnavigated the continent twice and never lost each other from view. Thus they disproved Captain Cook's assertion that it was impossible to find land in the southern ice-fields. The expedition discovered and named Peter I Island, Zavodovski, Leskov and Visokoi Islands, the Antarctic Peninsula and Alexander Island (Alexander Coast), and made other discoveries in the tropical waters of the Pacific.\n\nMade counter admiral on his return, Bellingshausen participated in the Russo-Turkish War of 1828–1829. Promoted to vice-admiral, he again served in the Baltic Fleet in 1830s, and from 1839 he was the military governor of Kronstadt, where he died. In 1831 he published the book on his Antarctic travels, called \"Double Investigation of the Southern Polar Ocean and the Voyage Around the World\" (\"Двукратные изыскания в южнополярном океане и плавание вокруг света\"). Russians remember him as one of their greatest admirals and explorers. Multiple geographical features and locations in the Antarctic, named in honor of Bellingshausen, commemorate his role in the exploration of the southern polar region.\n\nBellingshausen was born to a noble Baltic German family in the Lahhentagge manor, Saaremaa, Governorate of Livonia, now in Salme Parish, Saare County, Estonia — then part of the Russian Empire. He enlisted as a cadet in the Imperial Russian Navy at the age of ten. After graduating from the Kronstadt naval academy at age eighteen, Bellingshausen rapidly rose to the rank of captain.\n\nA great admirer of Cook's voyages, Bellingshausen served from 1803 in the first Russian circumnavigation of the Earth. He was one of the officers of the vessel \"Nadezhda\" (\"Hope\"), commanded by Adam Johann von Krusenstern.\n\nThe mission was completed in 1806. After the journey Bellingshausen published a collection of maps of the newly explored areas and islands of the Pacific Ocean.\n\nBellingshausen's career continued with the command of various ships in the Baltic and Black Seas. From 1812 to 1816 he commanded the frigate \"\" and from 1817 to 1819 the frigate \"\", both in the Black Sea Fleet. During 1812 he met on Macquarie island Richard Siddins, the Australian captain of the ship \"Campbell Macquarie\". \n\nWhen Emperor Alexander I authorized an expedition to the south polar region in 1819, the authorities selected Bellingshausen to lead it as an experienced captain and explorer, and a prominent cartographer. The expedition was intended to explore the Southern Ocean and to find land in the proximity of the South Pole. The preparation work on the two ships, the 985-ton sloop-of-war \"Vostok\" (\"East\") and the 530-ton support vessel \"Mirny\" (\"Peaceful\") was carried out by Mikhail Lazarev, who had captained his own circumnavigation of the globe before. Bellingshausen became the captain of \"Vostok\", and Lazarev captained \"Mirny\". The journey started from Kronstadt on 4 June 1819.\n\nLeaving Portsmouth on 5 September 1819 the expedition crossed the Antarctic Circle (the first to do so since Cook) on 26 January 1820 (New Style). On 27 January the expedition discovered the Antarctic mainland approaching the Antarctic coast at a point with coordinates 69º21'28\"S 2º14'50\"W and seeing ice-fields there. The point in question lies within twenty miles of the Antarctic mainland. Bellingshausen's diary, his report to the Russian Naval Minister on 21 July 1821 and other documents, available in the Russian State Museum of the Arctic and Antarctic in Saint Petersburg, Russia, were carefully compared with the log-books of other claimants by the British polar historian A. G. E. Jones in his 1982 study \"Antarctica Observed\". Jones concluded that Bellingshausen, rather than the Royal Navy's Edward Bransfield on 30 January 1820 or the American Nathaniel Palmer on 17 November 1820, was indeed the discoverer of the sought-after Terra Australis.\n\nDuring the voyage Bellingshausen also visited Ship Cove in New Zealand, the South Shetland Islands, and discovered and named Peter I, Zavodovski, Leskov and Visokoi Islands, and a peninsula of the Antarctic mainland that he named the Alexander Coast but that has more recently borne the designation of Alexander Island.\nBellingshausen and Lazarev managed to twice circumnavigate the continent and never lost each other from view. Thus they disproved Captain Cook's assertion that it was impossible to find land in the southern ice fields. The expedition also made discoveries and observations in the tropical waters of the Pacific Ocean.\n\nReturning to Kronstadt on 4 August 1821, Bellingshausen was made counter admiral. He fought in the Russo-Turkish War of 1828–1829 and attained the rank of vice admiral in 1830. In 1831 he published the book on his Antarctic travel, called \"Double Investigation of the Southern Polar Ocean and the Voyage Around the World\" (\"Двукратные изыскания в южнополярном океане и плавание вокруг света\").\n\nHe became the military governor of Kronstadt, the port of St Petersburg, from 1839, and died there in 1852.\nFabian Gottlieb von Bellingshausen is remembered in Russia as one of its greatest admirals and explorers. In the Antarctic, multiple geographical features and locations, named in honor of Bellingshausen, remind of his role in exploration of the southern polar region.\n\nThere is a memorial stone of von Bellingshausen on the previous site (on the ruins) of Lahhentagge/Lahetaguse manor in Oesel/Saaremaa.\n\nThere is a monument to Bellingshausen in Nikolayev, Ukraine.\n\nThere is a monument to Admiral Bellingshausen in Kronstadt.\n\n\n"}
{"id": "49644467", "url": "https://en.wikipedia.org/wiki?curid=49644467", "title": "Filipstadite", "text": "Filipstadite\n\nFilipstadite is a very rare mineral of the spinel group, with the formula (Mn,Mg)(SbFe)O. It is isometric, although it was previously though to be orthorhombic. When compared to a typical spinel, both the octahedral and tetrahedral sites are split due to cation ordering. Filipstadite is chemically close to melanostibite. The mineral comes from Långban, Sweden, a manganese skarn deposit famous for many rare minerals.\n\nIn the metamorphic Fe-Mn ore bodies of the Långban-type filipstadite associates with native antimony, calcite, native copper, forsterite, hausmannite, hedyphane, ingersonite, jacobsite, phlogopite, and svabite.\n\nCations and anions in filipstadite occupy of the octahedral and of the tetrahedral holes of the spinel-type oxygen lattice, that has cubic close-packing. Tetrahedral sites are split into 5, and octahedral into 6 substitutes, due to cation ordering, which also causes the unit cell edge to be tripled. Antimony, most of magnesium and trace aluminium are located on the octahedral (M) sites, trace magnesium, zinc and silicon are on the tetrahedral (T) sites. Manganese and iron are on both M and T sites.\n"}
{"id": "8020639", "url": "https://en.wikipedia.org/wiki?curid=8020639", "title": "Global Historical Climatology Network", "text": "Global Historical Climatology Network\n\nThe Global Historical Climatology Network (GHCN) is a database of temperature, precipitation and pressure records managed by the National Climatic Data Center, Arizona State University and the Carbon Dioxide Information Analysis Center.\n\nThe aggregate data are collected from many continuously reporting fixed stations at the Earth's surface and represent the input of approximately 6000 temperature stations, 7500 precipitation stations and 2000 pressure stations.\n\nThis work has often been used as a foundation for reconstructing past global temperatures, and was used in previous versions of two of the best-known reconstructions, that prepared by the National Climatic Data Center (NCDC), and that prepared by NASA as its Goddard Institute for Space Studies (GISS) temperature set. The average temperature record is 60 years long with ~1650 records greater than 100 years and ~220 greater than 150 years (based on GHCN v2 in 2006). The earliest data included in the database were collected in 1697.\n\nThe GHCN is one of the primary reference compilations of temperature data used for climatology, and is the foundation of the GISTEMP Temperature Record. This map shows the 7,280 fixed temperature stations in the GHCN catalog color-coded by the length of the available record. Sites that are actively updated in the database (2,277) are marked as \"active\" and shown in large symbols, other sites are marked as \"historical\" and shown in small symbols. In some cases, the \"historical\" sites are still collecting data but due to reporting and data processing delays (of more than a decade in some cases) they do not contribute to current temperature estimates.\n\nAs is evident from this plot, the most densely instrumented portion of the globe is in the , while is the most sparsely instrumented land area. Parts of the Pacific and other oceans are more isolated from fixed temperature stations, but this is supplemented by volunteer observing ships that record temperature information during their normal travels. This image shows 3,832 records longer than 50 years, 1,656 records longer than 100 years, and 226 records longer than 150 years. The longest record in the collection began in Berlin in 1701 and is still collected in the present day.\n\n"}
{"id": "33511862", "url": "https://en.wikipedia.org/wiki?curid=33511862", "title": "History of geomagnetism", "text": "History of geomagnetism\n\nThe history of geomagnetism is concerned with the history of the study of Earth's magnetic field. It encompasses the history of navigation using compasses, studies of the prehistoric magnetic field (archeomagnetism and paleomagnetism), and applications to plate tectonics.\n\nMagnetism has been known since prehistory, but knowledge of the Earth's field developed slowly. The horizontal direction of the Earth's field was first measured in the fourth century BC but the vertical direction was not measured until 1544 AD and the intensity was first measured in 1791. At first, compasses were thought to point towards locations in the heavens, then towards magnetic mountains. A modern experimental approach to understanding the Earth's field began with \"de Magnete\", a book published by William Gilbert in 1600. His experiments with a magnetic model of the Earth convinced him that the Earth itself is a large magnet.\n\nKnowledge of the existence of magnetism probably dates back to the prehistoric development of iron smelting. Iron can be obtained on the Earth's surface from meteorites; the mineral lodestone is rich in the magnetic mineral magnetite and can be magnetized by a lightning strike. In his \"Natural History\", Pliny the Elder recounts a legend about a Magnes the shepherd on the island of Crete whose iron-studded boots kept sticking to the path. The earliest ideas on the nature of magnetism are attributed to Thales ( BC – BC).\n\nIn classical antiquity, little was known about the nature of magnetism. No sources mention the two poles of a magnet or its tendency to point northward. There were two main theories about the origins of magnetism. One, proposed by Empedocles of Acragas and taken up by Plato and Plutarch, invoked an invisible \"effluvium\" seeping through the pores of materials; Democritus of Abdera replaced this effluvium by atoms, but the mechanism was essentially the same. The other theory evoked the metaphysical principle of \"sympathy\" between similar objects. This was mediated by a purposeful life force that strove toward perfection. This theory can be found in the writings of Pliny the Elder and Aristotle, who claimed that Thales attributed a soul to the magnet. In China, a similar life force, or \"qi\", was believed to animate magnets, so the Chinese used early compasses for feng shui.\n\nLittle changed in the view of magnetism during the Middle Ages, and some classical ideas lingered until well after the first scientific experiments on magnetism. One belief, dating back to Pliny, was that fumes from eating garlic and onions could destroy the magnetism in a compass, rendering it useless. Even after William Gilbert disproved this in 1600, there were reports of helmsmen on British ships being flogged for eating garlic. However, this belief was far from universal. In 1558 Giambattista della Porta reported \"When I enquired of mariners whether it were so that they were forbid to eat onyones and garlick for that reason, they said they were old wives fables and things ridiculous, and that sea-men would sooner lose their lives then abstain from eating onyons and garlick.\"\n\nAt a given location, a full representation of the Earth's magnetic field requires a vector with three coordinates (see figure). These can be Cartesian (North, East and Down) or spherical (declination, inclination and intensity). In the latter system, the declination (the deviation from true north, a horizontal angle) must be measured first to establish the direction of magnetic North; then the dip (a vertical angle) can be measured relative to magnetic North. In China, the horizontal direction was measured as early as the fourth century BC, and the existence of declination first recognized in 1088. In Europe, this was not widely accepted until the middle of the fifteenth century AD. Inclination (also known as \"magnetic dip\") was first measured in 1544 AD. The intensity was not measured until 1791, after advances in the understanding of electromagnetism.\n\nThe magnetic compass existed in China back as far as the fourth century BC. It was used as much for feng shui as for navigation on land. It was not until good steel needles could be forged that compasses were used for navigation at sea; before that, they could not retain their magnetism for long. The existence of magnetic declination, the difference between magnetic north and true north, was first recognized by Shen Kuo in 1088.\n\nThe first mention of a compass in Europe was in 1190 AD by Alexander Neckam. He described it as a common navigational aid for sailors, so the compass must have been introduced to Europe some time earlier. Whether the knowledge came from China to Europe, or was invented separately, is not clear. If the knowledge was transmitted, the most likely intermediary was Arab merchants, but Arabic literature does not mention the compass until after Neckam. There is also a difference in convention: Chinese compasses point south while European compasses point north.\n\nIn 1269, Pierre de Maricourt (commonly referred to as \"Petrus Peregrinus\") wrote a letter to a friend in which he described two kinds of compass, one in which an oval lodestone floated in a bowl of water, and the first dry compass with the needle mounted on a pivot. He also was the first to write about experiments with magnetism and describe the laws of attraction. An example is the experiment where a magnet is broken into two pieces and the two pieces can attract and repel each other (in modern terms, they both have north and south poles). This letter, generally referred to as \"Epistola de Magnete\", was a landmark in the history of science.\n\nPetrus Peregrinus assumed that compasses point towards true north. While his contemporary Roger Bacon is reputed to observe that compasses deviated from true north, the idea of magnetic declination was only gradually accepted. At first it was thought that the declination must be the result of systematic error. However, by the middle of the fifteenth century, sundials in Germany were oriented using corrections for declination.\n\nA compass must be balanced to counter the tendency of the needle to dip in the direction of the Earth's field. Otherwise, it will not spin freely. Often, compasses that are balanced for one latitude do not work as well at a different latitude. This problem was first reported by Georg Hartmann, a vicar in Nuremberg, in 1544. Robert Norman was the first to recognize that this occurs because the Earth's field itself is tilted from the vertical. In his book \"The Newe Attractive\", Norman called inclination \"a newe discouered secret and subtil propertie concernyng the Declinyng of the Needle.\" He created a compass in which the needle was floated in a goblet of water, attached to a cork to make it neutrally buoyant. The needle could orient itself in any direction, so it dipped to align itself with the Earth's field. Norman also created a dip circle, a compass needle pivoted about a horizontal axis, to measure the effect.\n\nIn early attempts to understand the Earth's magnetic field, measuring it was only part of the challenge. Understanding the measurements was also difficult because the mathematical and physical concepts had not yet been developed – in particular, the concept of a vector field that associates a vector with each point in space. The Earth's field is generally represented by field lines that run from pole to pole; the field at any point is parallel to a field line but does not have to point at either pole. As late as the eighteenth century, however, a natural philosopher would believe that a magnet had to be pointing directly at something. Thus, the Earth's magnetic field had to be explained by localized sources, and as more was learned about the Earth's field, these sources became increasingly complex.\n\nAt first, in both China and Europe, the source was assumed to be in the heavens – either the celestial poles or the Pole star. These theories required that magnets point at (or very close to) true north, so they ran into difficulty when the existence of declination was accepted. Then natural philosophers began to propose earthly sources such as a rock or mountain.\n\nLegends about magnetic mountains go back to the classical era. Ptolemy recounted a legend about magnetic islands (now thought to be near Borneo) that exerted such a strong attraction on ships with nails that the ships were held in place and could not move. Even more dramatic was the Arab legend (recounted in \"One Thousand and One Nights\") that a magnetic mountain could pull all the nails out of a ship, causing the ship to fall apart and founder. The story passed to Europe and became part of several epic tales.\n\nEuropeans started to place magnetic mountains on their maps in the sixteenth century. A notable example is Gerardus Mercator, whose famous maps included a magnetic mountain or two near the North Pole. At first, he just placed a mountain in an arbitrary location; but later he attempted to measure its location based on declinations from different locations in Europe. When subsequent measurements resulted in two contradictory estimates for the mountain, he simply placed two mountains on the map.\n\n1600 was a notable year for William Gilbert. He became president of the Royal College of Physicians of London, was appointed personal physician for Queen Elizabeth I, and wrote \"De Magnete\", one of the books that mark the beginning of modern science. \"De Magnete\" is most famous for introducing (or at least popularizing) an experimental approach to science and deducing that the Earth is a great magnet.\n\nGilbert's book is divided into six chapters. The first is an introduction in which he discusses the importance of experiment and various facts about the Earth, including the insignificance of surface topography compared to the radius of the Earth. He also announces his deduction that the Earth is a great magnet. In book 2, Gilbert deals with \"coition\", or the laws of attraction. Gilbert distinguishes between magnetism and static electricity (the latter being induced by rubbing amber) and reports many experiments with both (some dating back to Peregrinus). One involves breaking a magnet in two and showing that both parts have a north and south pole. He also dismisses the idea of perpetual motion. The third book has a general description of magnetic directions along with details on how to magnetize a needle. He also introduces his \"terella\", or \"little Earth\". This is a magnetized sphere that he uses to model the magnetic properties of the Earth. In chapters 4 and 5 he goes into more detail about the two components of the direction, declination and inclination.\n\nIn the late 1590s Henry Briggs, a professor of geometry at Gresham College in London, had published a table of magnetic inclination with latitude for the earth. It agreed well with the inclinations that Gilbert measured around the circumference of his terella. Gilbert deduced that the Earth's magnetic field is equivalent to that of a uniformly magnetized sphere, magnetized parallel to the axis of rotation (in modern terms, a \"geocentric axial dipole\"). However, he was aware that declinations were not consistent with this model. Based on the declinations that were known at the time, he proposed that the continents, because of their raised topography, formed centers of attraction that made compass needles deviate. He even demonstrated this effect by gouging out some topography on his terella and measuring the effect on declinations. A Jesuit monk, Niccolò Cabeo, later took a leaf from Gilbert's book and showed that, if the topography was on the correct scale for the Earth, the differences between the highs and lows would only be about one tenth of a millimeter. Therefore, the continents could not noticeably affect the declination.\n\nThe sixth book of \"de Magnete\" was devoted to cosmology. He dismissed the prevailing Ptolemaic model of the universe, in which the planets and stars are organized in a series of concentric shells rotating about the Earth, on the grounds that the speeds involved would be absurdly large (\"there cannot be diurnal motion of infinity\"). Instead, the Earth was rotating about its own axis. In place of the concentric shells, he proposed that the heavenly bodies interacted with each other and Earth through magnetic forces. Magnetism maintained the Earths position and made it rotate, while the magnetic attraction of the Moon drove the tides. Some obscure reasoning led to the peculiar conclusion that a terella, if freely suspended, would orient itself in the same direction as the Earth and rotate daily. Both Kepler and Galileo would adopt Gilbert's idea of magnetic attraction between heavenly bodies, but Newton's law of universal gravitation would render it obsolete.\n\nIn about 1603, the Frenchman Guillaume le Nautonier (William the Navigator), Sieur de Castelfranc, published a rival theory of the Earth's field in his book \"Mecometrie de l'eymant (Measurement of longitude with a magnet)\". Le Nautonier was a mathematician, astronomer and Royal Geographer in the court of Henry IV. He disagreed with Gilbert's assumption that the Earth had to be magnetized parallel to the rotational axis, and instead produced a model in which the magnetic moment was tilted by ° – in effect, the first tilted dipole model. The last 196 pages of his book were taken up with tables of latitudes and longitudes with declination and inclination for use by mariners. If his model had been accurate, it could have been used to determine both latitude and longitude using a combination of magnetic declination and astronomical observations.\n\nLe Nautonier tried to sell his model to Henry IV, and his son to the English leader Oliver Cromwell, both without success. It was widely criticized, with Didier Dounot concluding that the work was based on \"unfounded assumptions, errors in calculation and data manipulation\". However, the geophysicist Jean-Paul Poirier examined the works of both le Nautonier and Dounot, and found that the error was in Dounot's reasoning.\n\nOne of Gilbert's conclusions was that the Earth's field could not vary in time. This was soon to be proved false by a series of measurements in London. In 1580, William Borough measured the declination and found it to be 11° NE. In 1622, Edmund Gunter found it to be 5° 56' NE. He noted the difference from Borough's result but concluded that Borough must have made a measurement error. In 1633, Henry Gellibrand measured the declination in the same location and found it to be 4° 05' NE. Because of the care with which Gunther had made his measurements, Gellibrand was confident that the changes were real. In 1635 he published \"A Discourse Mathematical on the Variation of the Magneticall Needle\" stating that the declination had changed by more than 7° in 54 years. The reality of geomagnetic secular variation was rapidly accepted in England, where Gellibrand had a high reputation, but in other countries it was met with skepticism until it was confirmed by further measurements.\n\nThe observations of Gellibrand inspired extensive efforts to determine the nature of variation - global or local, predictable or erratic. It also inspired new models for the origin of the field. Henry Bond Senior gained notoriety by successfully predicting in 1639 that the declination would be zero in London in 1657. His model, which involved a precessing dipole, was strongly criticized by a royal commission, but it continued to be published in navigational instruction manuals for decades. Dynamic models involving multiple poles were also proposed by Peter Perkins (1680) and Edmond Halley (1683, 1692), among others. In Halley's model, the Earth consisted of concentric spheres. Two magnetic poles were on a fixed outer sphere and two more were on an inner sphere that rotated westwards, giving rise to a \"westward drift\". Halley was so proud of this theory that a portrait of him at the age of eighty included a diagram of it.\n\nEarly mariners used portolan charts for navigation. These charts showed coastline with rhumb lines connecting ports. A mariner could navigate by aligning the chart with a compass and following the compass heading. Early charts had distorted coastlines because the cartographers did not know about declination, but the charts still worked because mariners were sailing in straight lines.\n\nWhile boats mainly plied seas the size of the Mediterranean, rhumb lines were sufficient for navigation. However, when they ventured into the Atlantic and Pacific oceans, it was no longer sufficient to plot a straight-line course from one destination to another. Mariners needed to determine their latitude and longitude.\n\nIn the Age of Sail, dating from the sixteenth to the mid-nineteenth century, international trade was dominated by sailing ships. More than one European government offered a generous prize to the first person who could accurately determine longitude. The British prize, the longitude prize, led to the development of the marine chronometer by John Harrison, a clockmaker from Yorkshire.\n\n\n"}
{"id": "53384839", "url": "https://en.wikipedia.org/wiki?curid=53384839", "title": "International Union for Vacuum Science, Technique and Applications", "text": "International Union for Vacuum Science, Technique and Applications\n\nThe International Union for Vacuum Science, Technique, and Applications (IUVSTA) is a union of 33 science and technology national member societies whose role is to stimulate international collaboration in the fields of vacuum science, technique and applications, and related multi-disciplinary topics.\n\nIUVSTA is a Member Scientific Associate of the International Council for Science (ICSU).\n\nFounded in 1958, IUVSTA is an interdisciplinary union which represents several thousands of physicists, chemists, materials scientists, engineers and technologists who are active in basic and applied research, development, manufacturing, sales and education. IUVSTA finances advanced scientific workshops, international schools and technical courses, worldwide.\n\nIUVSTA comprises member societies from the following countries:\nArgentina, Australia, Austria, Belgium, Brazil, Bulgaria, China, Croatia, Czech Republic, Finland, France, Germany, Hungary, India, Israel, Iran, Italy, Japan, Korea, Mexico, Netherlands, Pakistan, Philippines, Poland, Portugal, Russian Federation, Slovakia, Slovenia, Spain, Sweden, Switzerland, United Kingdom, and USA.\n\nThe main purposes of the IUVSTA are to organize and sponsor international conferences and educational activities, as well as to facilitate research and technological developments in the field of vacuum science and its applications.\n\nThe history and structure of the Union are described in two articles in scientific journals.\n\nIUVSTA has nine technical divisions:\n\n\n"}
{"id": "2670644", "url": "https://en.wikipedia.org/wiki?curid=2670644", "title": "Kappa Sculptoris", "text": "Kappa Sculptoris\n\nThe Bayer designation Kappa Sculptoris (κ Scl, κ Sculptoris) is shared by two star systems, κ¹ Sculptoris and κ² Sculptoris, in the constellation Sculptor. They are separated by 0.53° in the sky.\n\n"}
{"id": "37918559", "url": "https://en.wikipedia.org/wiki?curid=37918559", "title": "Kubrick the Dog", "text": "Kubrick the Dog\n\nKubrick the Dog is a 2011 non-fiction photography book by British director Sean Ellis. The book was released on February 28, 2011 through Schirmer Books and focuses on the life of Ellis's dog Kubrick. \"Kubrick the Dog\" features several photos of Kubrick, a Hungarian Vizsla that Ellis adopted as a puppy in 1998, in several poses and with different people such as Stella McCartney.\n\nEllis began working on the book in 2010 after Kubrick's death by canine lymphoma as a way of working through his grief over the dog's death.\n\nCritical reception for the book was mostly positive. Basler Zeitung gave an overall positive review, stating that although some of the photographs felt a little deliberate, the book was also stirring.\n"}
{"id": "58881752", "url": "https://en.wikipedia.org/wiki?curid=58881752", "title": "List of Eulophia species", "text": "List of Eulophia species\n\nThe following is a list of \"Eulophia\" species recognised by the Royal Botanic Gardens, Kew as at October 2018:\n\nAfter a molecular phylogeny published in 2014 revealed that the genus \"Eulophia\" was paraphyletic unless a clade containing \"Orthochilus\" was recognized, 34 species and one subspecies were transferred to the resurrected genus \"Orthochilus\", which included many \"Eulophia\" and all \"Pteroglossaspis\" taxa. This reduced the number of \"Eulophia\" species from 201 to 165. The authors also suggested recognizing \"Oeceoclades pulchra\" instead of \"Eulophia pulchra\". The taxa formerly recognized in \"Eulophia\" include:\n\n{{div col end}\n"}
{"id": "21548170", "url": "https://en.wikipedia.org/wiki?curid=21548170", "title": "List of Inocybe species", "text": "List of Inocybe species\n\nInocybe is a large genus of mushroom-producing fungi in the order Agaricales. The genus is widely distributed in the Northern Hemisphere. , Index Fungorum accepts 848 species in \"Inocybe\".\n\nA B C D E F G H I J K L M N O P Q R S T U V U W X Y Z\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "23803061", "url": "https://en.wikipedia.org/wiki?curid=23803061", "title": "List of Ramsar sites in Guatemala", "text": "List of Ramsar sites in Guatemala\n\nThe list of Ramsar Sites in Guatemala includes wetlands in Guatemala that are considered to be of \"international importance\" under the Ramsar Convention.\n\nFor a full list of all Ramsar Sites worldwide, see the Ramsar list of wetlands of international importance.\n"}
{"id": "1716015", "url": "https://en.wikipedia.org/wiki?curid=1716015", "title": "List of countries by carbon dioxide emissions", "text": "List of countries by carbon dioxide emissions\n\nThis is a list of sovereign states and territories by carbon dioxide emissions due to certain forms of human activity, based on the EDGAR database created by European Commission and Netherlands Environmental Assessment Agency released in 2015. The following table lists the 2015 annual emissions estimates (in thousands of tonnes) along with a list of emissions per capita (in tonnes of per year) from same source. The data only considers carbon dioxide emissions from the burning of fossil fuels and cement manufacture, but not emissions from land use, land-use change and forestry. Emissions from international shipping or bunker fuels are also not included in national figures, which can make a large difference for small countries with important ports.\nThe top 10 largest emitter countries account for 67.6% of the world total. Since 2006, China has been emitting more than any other country. Other powerful, more potent greenhouse gases, including methane, are not included in this data.\n\n\nGeneral:\n\nThe contents of this article comes from the latest figures from the millennium indicators as of 2009-07-14:\n"}
{"id": "32371522", "url": "https://en.wikipedia.org/wiki?curid=32371522", "title": "List of ecoregions in Eritrea", "text": "List of ecoregions in Eritrea\n\nThe following is a list of ecoregions in Eritrea, as identified by the Worldwide Fund for Nature (WWF).\n\n\"by major habitat type\"\n\n\n\n\n\n"}
{"id": "2148829", "url": "https://en.wikipedia.org/wiki?curid=2148829", "title": "List of mountains in Greece", "text": "List of mountains in Greece\n\nA list of mountains in Greece:\n\n"}
{"id": "20905037", "url": "https://en.wikipedia.org/wiki?curid=20905037", "title": "List of old-growth forests in Massachusetts", "text": "List of old-growth forests in Massachusetts\n\nThe following is a list of old-growth forests in the Commonwealth of Massachusetts. Old growth is defined as those forests that have not been logged (and have not been significantly disturbed by human beings) in the last 150 years. \"Virgin forests\" are those old-growth forests that show no sign of having ever been logged. \n\nA total of of old-growth forest has been identified in Massachusetts. Massachusetts' old growth occurs almost entirely within the Northeastern Highlands ecoregion. The following list identifies some of the sites and their locations:\n\n"}
{"id": "1350972", "url": "https://en.wikipedia.org/wiki?curid=1350972", "title": "List of railway electrification systems", "text": "List of railway electrification systems\n\nThis is a list of the power supply systems that are, or have been, used for tramway and railway electrification systems.\n\nNote that the voltages are nominal and vary depending on load and distance from the substation.\n\nMany modern trams and trains use on-board solid-state electronics to convert these supplies to run three-phase AC induction motors.\n\n\nVoltages are defined by two standards: BS EN 50163 and IEC 60850.\n\nThis voltage is mostly used by older tram systems worldwide but by a few new ones as well.\n\nThis voltage is used for most modern tram systems.\n\nWhile is not standardized by BS EN 50163 and IEC 60850; it is the logical equivalent of in countries where is the normal grid power frequency.\n\nAll third rail unless stated otherwise.\nUsed by most older US subways.\n\nConductor rail systems have been separated into tables based on whether they are top, side or bottom contact. All systems are third rail unless stated otherwise.\n\nAll systems are third rail unless stated otherwise.\n\nConductor rail systems have been separated into tables based on whether they are top, side or bottom contact.\n\n\n\n\n\n"}
{"id": "3983473", "url": "https://en.wikipedia.org/wiki?curid=3983473", "title": "List of rivers of Finland", "text": "List of rivers of Finland\n\nThis is a list of rivers of Finland. Listing begins with rivers flowing into the Baltic sea, from the north, that is from the Swedish border. Tributaries are listed down the page in an upstream direction.\n\nWater flows from Finland directly to the Baltic Sea, which is divided here into the Gulf of Bothnia and Gulf of Finland, and the Archipelago Sea between them. Some rivers flow to Russia, ending either to Gulf of Finland or to the White Sea, and a few to the Arctic Ocean through Russia or Norway.\n\nThere are a lot of lakes in Finland and so this listing includes also several lakes through which the rivers flow or begin from. Due to the great number of lakes especially in the Finnish Lakeland, where watercources tend to consist of chains of lakes rather than long rivers, some rivers with a large catchment area can also be quite short or there may only be a short rapid between large lakes, like for example Tammerkoski in Tampere.\n\n\n\n\nWater from these rivers flows through Lake Ladoga and Neva River to the sea.\n\n\n\nThis is a list of the rivers, exceeding 100 km, that are wholly or partly located within the borders of Finland.\n\n\n"}
{"id": "2713178", "url": "https://en.wikipedia.org/wiki?curid=2713178", "title": "Muscida", "text": "Muscida\n\nThe traditional star name Muscida has been applied to:\n\nThe name derives from the post classical Latin \"musus\", meaning \"snout, or muzzle [of the bear]\".\n"}
{"id": "21938", "url": "https://en.wikipedia.org/wiki?curid=21938", "title": "Neoproterozoic", "text": "Neoproterozoic\n\nThe Neoproterozoic Era is the unit of geologic time from .\n\nIt is the last era of the Precambrian Supereon and the Proterozoic Eon; it is subdivided into the Tonian, Cryogenian, and Ediacaran Periods. It is preceded by the Mesoproterozoic era and succeeded by the Paleozoic era.\n\nThe most severe glaciation known in the geologic record occurred during the Cryogenian, when ice sheets reached the equator and formed a possible \"Snowball Earth\".\n\nThe earliest fossils of multicellular life are found in the Ediacaran, including the Ediacarans, which were the earliest animals.\n\nAccording to Rino and co-workers, the sum of the continental crust formed in the Pan-African orogeny and the Grenville orogeny makes the Neoproterozoic the period of Earth's history that has produced most continental crust.\n\nAt the onset of the Neoproterozoic the supercontinent Rodinia, which had assembled during the late Mesoproterozoic, straddled the equator. During the Tonian, rifting commenced which broke Rodinia into a number of individual land masses.\n\nPossibly as a consequence of the low-latitude position of most continents, several large-scale glacial events occurred during the Neoproterozoic Era including the Sturtian and Marinoan glaciations of the Cryogenian Period.\n\nThese glaciations are believed to have been so severe that there were ice sheets at the equator—a state known as the \"Snowball Earth\".\n\nRussian geologists divide the Neoproterozoic of Siberia into the Baikalian from 850 to 650 Ma (loosely equivalent to the Cryogenian), which follows the Mayanian, from 1000 to 850 Ma, then the Aimchanian.\n\nThe idea of the Neoproterozoic Era was introduced in the 1960s. Nineteenth-century paleontologists set the start of multicelled life at the first appearance of hard-shelled animals called trilobites and archeocyathid sponges. This set the beginning of the Cambrian Period. In the early 20th century, paleontologists started finding fossils of multicellular animals that predated the start of the Cambrian. A complex fauna was found in South West Africa in the 1920s but was inaccurately dated. Another fauna was found in South Australia in the 1940s but was not thoroughly examined until the late 1950s. Other possible early fossils were found in Russia, England, Canada, and elsewhere (see Ediacaran biota). Some were determined to be pseudofossils, but others were revealed to be members of rather complex biotas that are still poorly understood. At least 25 regions worldwide yielded metazoan fossils older than the classical Cambrian boundary at .\n\nA few of the early animals appear possibly to be ancestors of modern animals. Most fall into ambiguous groups of frond-like organisms; discoids that might be holdfasts for stalked organisms (\"medusoids\"); mattress-like forms; small calcareous tubes; and armored animals of unknown provenance.\n\nThese were most commonly known as Vendian biota until the formal naming of the Period, and are currently known as Ediacaran Period biota. Most were soft bodied. The relationships, if any, to modern forms are obscure. Some paleontologists relate many or most of these forms to modern animals. Others acknowledge a few possible or even likely relationships but feel that most of the Ediacaran forms are representatives of unknown animal types.\n\nIn addition to Ediacaran biota, two other types of biota were discovered in China (the Doushantuo Formation and Hainan Formation).\n\nThe nomenclature for the terminal Period of the Neoproterozoic Era has been unstable. Russian and Nordic geologists referred to the last period of the Neoproterozoic as the Vendian, while Chinese geologists referred to it as the Sinian, and most Australians and North Americans used the name Ediacaran.\n\nHowever, in 2004, the International Union of Geological Sciences ratified the Ediacaran Period to be a geological age of the Neoproterozoic, ranging from to million years ago. The Ediacaran Period boundaries are the only Precambrian boundaries defined by biologic Global Boundary Stratotype Section and Points, rather than the absolute Global Standard Stratigraphic Ages.\n"}
{"id": "41185806", "url": "https://en.wikipedia.org/wiki?curid=41185806", "title": "Oil Pollution Act of 1961", "text": "Oil Pollution Act of 1961\n\nOil Pollution Act of 1961, 33 U.S.C. Chapter 20 §§ 1001-1011, established judicial definitions and coastal prohibitions for the United States maritime industry. The Act invoked the accords of the International Convention for the Prevention of the Pollution of the Sea by Oil, 1954. The international agreement provided provisions to control the discharge of fossil fuel pollutants from nautical vessels on the high seas.\n\nThe S. 2187 legislation was passed by the United States 87th Congressional session and enacted by the 35th President of the United States John F. Kennedy on August 30, 1961.\n\nThe International Convention for the Prevention of Pollution of the Sea by Oil (OILPOL) was an international convention organized by the United Kingdom in 1954. The convention was held in London, England from April 26, 1954 to May 12, 1954. The international meeting was convened to acknowledge the disposal of harmful waste which posed endangerment to the marine ecosystems.\n\nThe International Convention for the Prevention of the Pollution of the Sea by Oil, 1954 original text was penned in English and French. The 1954 international agreement was amended in 1962, 1969, and 1971.\n\nThe Act emulated the subsequent formalities of the International Convention for the Prevention of the Pollution of the Sea by Oil, 1954.\n\nThe 1961 United States statute was repealed by the enactment of Act to Prevent Pollution from Ships on October 21, 1980.\n\n"}
{"id": "13656257", "url": "https://en.wikipedia.org/wiki?curid=13656257", "title": "Pacific–North American teleconnection pattern", "text": "Pacific–North American teleconnection pattern\n\nThe Pacific–North American teleconnection pattern (PNA) is a climatological term for a large-scale weather pattern with two modes, denoted positive and negative, and which relates the atmospheric circulation pattern over the North Pacific Ocean with the one over the North American continent.\n\nThe positive phase of the PNA pattern features above-average barometric pressure heights in the vicinity of Hawaii and over the inter-mountain region of North America, and below-average heights located south of the Aleutian Islands and over the southeastern United States. The PNA pattern is associated with strong fluctuations in the strength and location of the East Asian jet stream. The positive phase is associated with an enhanced East Asian jet stream and with an eastward shift in the jet exit region toward the western United States. The negative phase is associated with a westward retraction of that jet stream toward eastern Asia, blocking activity over the high latitudes of the North Pacific, and a strong split-flow configuration over the central North Pacific.\n\nThe positive phase of the PNA pattern is associated with above-average temperatures over western Canada and the extreme western United States, and below-average temperatures across the south-central and southeastern US. The PNA tends to have little impact on surface temperature variability over North America during summer. The associated precipitation anomalies include above-average totals in the Gulf of Alaska extending into the Pacific Northwestern United States, and below-average totals over the upper Midwestern United States.\n\nThe negative PNA phase is associated with the opposite.\n\nAlthough the PNA pattern is a natural internal mode of climate variability, it is also strongly influenced by the El Niño-Southern Oscillation (ENSO) phenomenon. The positive phase of the PNA pattern tends to be associated with Pacific warm episodes (El Niño), and the negative phase tends to be associated with Pacific cold episodes (La Niña).\n\n\n"}
{"id": "40166699", "url": "https://en.wikipedia.org/wiki?curid=40166699", "title": "Petroleum industry in Kenya", "text": "Petroleum industry in Kenya\n\nThe petroleum industry in Kenya is relatively new in term of mining and exploration. Oil was first discovered in 2012 by a British firm Tullow oil. As to whether they are commercially viable is still being analysed. Currently, the oil industry is controlled by importation and refining.\n\nThe history of oil marketing in Kenya began in 1903 during colonial times. Initially, kerosene was the main import in tins but later gasoline was imported in tins and drums. Royal Dutch Shell established the first depot on the Mombasa island at Shimanzi.\n\nIn 2006, Uganda confirmed the discovery of significant quantities of oil reserves in the Albertine Graben region. By the end of 2013, oil deposits were estimated to be 3.5 billion barrels. In 2014 Uganda has revised oil reserves to 6.5 billion barrels.\n\nAccording to Deloitte in 2013, Kenya has four prospective sedimentary basins: Anza, Lamu, Mandera and the Tertiary Rift.The Lamu basin extends offshore.\n\nOil is regulated by the Energy Regulation Commission and the Ministry of Mining. Current traders include the National Oil Corporation of Kenya, Shell, Tullow Oil, KenolKobil, MOGAS, Hass, Hashi Energy, Gulf Energy, Olympic, Dalbit Petroleum.\n\n\n"}
{"id": "42325024", "url": "https://en.wikipedia.org/wiki?curid=42325024", "title": "Shtormove gas field", "text": "Shtormove gas field\n\nThe Shtormove gas field natural gas field located on the continental shelf of the Black Sea. It was discovered in 1974 and developed by Chornomornaftogaz. It started commercial production in 1983. The total proven reserves of the Shtormove gas field are around , and production is slated to be around in 2015.\n"}
{"id": "41369768", "url": "https://en.wikipedia.org/wiki?curid=41369768", "title": "Soviet Project K nuclear tests", "text": "Soviet Project K nuclear tests\n\nThe Soviet Union's K project nuclear test series was a group of 5 nuclear tests conducted in 1961-1962. These tests followed the \"1961 Soviet nuclear tests\" series and preceded the \"1962 Soviet nuclear tests\" series.\n\nThe K project nuclear testing series were all high altitude tests fired by missiles from the Kapustin Yar launch site in Russia across central Kazakhstan toward the Sary Shagan test range (see map below).\n\nTwo of the tests were 1.2 kiloton warheads tested in 1961. The remaining three tests were of 300 kiloton warheads in 1962.\n\nThe worst effects of a Soviet high altitude test were from the electromagnetic pulse of the nuclear test on 22 October 1962 (during the Cuban missile crisis). In that Operation K high altitude test, a 300 kiloton missile-warhead detonated west of Jezkazgan (also called Dzhezkazgan or Zhezqazghan) at an altitude of .\n\nThe Soviet scientists instrumented a section of telephone line in the area that they expected to be affected by the nuclear detonation in order to measure the electromagnetic pulse effects. The electromagnetic pulse (EMP) fused all of the 570-kilometer monitored overhead telephone line with measured currents of 1500 to 3400 amperes during the 22 October 1962 test. The monitored telephone line was divided into sub-lines of in length, separated by repeaters. Each sub-line was protected by fuses and by gas-filled overvoltage protectors. The EMP from the 22 October (K-3) nuclear test caused all of the fuses to blow and all of the overvoltage protectors to fire in all of the sub-lines of the telephone line. The EMP from the same test caused the destruction of the Karaganda power plant, and shut down of shallow-buried power cables between Astana (then called Aqmola) and Almaty.\n\nThe Partial Test Ban Treaty was passed the following year, ending atmospheric and exoatmospheric nuclear tests.\n\nAlthough the weapons used in the K Project were much smaller (up to 300 kilotons) than the United States Starfish Prime test of 1962, the damage caused by the resulting EMP was much greater because the K Project tests were done over a large populated land mass, and at a location where the Earth's magnetic field was greater. After the collapse of the Soviet Union, the level of this damage was communicated informally to scientists in the United States.\n\nAfter the 1991 Soviet Union collapse, there was a period of a few years of cooperation between United States and Russian scientists on the high-altitude nuclear EMP phenomenon. In addition, funding was secured to enable Russian scientists to formally report on some of the Soviet EMP results in international scientific journals.  As a result, formal scientific documentation of some of the EMP damage in Kazakhstan exists but is still sparse in the open scientific literature.\n\nThe 1998 IEEE article, however, does contain a number of details about the measurements of EMP effects on the instrumented telephone line, including details about the fuses that were used and also about the gas-filled overvoltage protectors that were used on that communications line. According to that paper, the gas-filled overvoltage protectors fired as a result of the voltages induced by the fast E1 component of the EMP, and the fuses were blown as the result of the slow E3 component of the EMP, which caused geomagnetically induced currents in all of the sub-lines.\n\nThe Aqmola (Astana) to Almaty buried power cable was also shut down by the slow E3 component of the EMP.\n\nPublished reports, including the 1998 IEEE article, have stated that there were significant problems with ceramic insulators on overhead electrical power lines during the tests of the K Project. In 2010, a technical report written for a United States government laboratory, Oak Ridge National Laboratory, stated, \"Power line insulators were damaged, resulting in a short circuit on the line and some lines detaching from the poles and falling to the ground.\"\n"}
{"id": "30771750", "url": "https://en.wikipedia.org/wiki?curid=30771750", "title": "Statkraft osmotic power prototype in Hurum", "text": "Statkraft osmotic power prototype in Hurum\n\nStatkraft osmotic power prototype is the world's first osmotic power plant, based on the energy of osmosis. The power plant is run by Statkraft. The power plant is located at Tofte in Hurum, Norway, with rooms at the factory area at Södra Cell Tofte cellulose factory. The power plant uses the osmotic gradient that occurs when fresh water and salt water meet, separated by a permeable membrane. The salt water pulls fresh water through the membrane and the pressure increases on the salt water side; this pressure increase can be used to produce electrical power with the use of a normal hydroelectric turbine/generator setup. \n\nThe plant is a prototype developed together with Sintef and began test power production on 24 November 2009. \nMette-Marit, Crown Princess of Norway officially opened the plant. This plant had been planned since the summer of 2008, with a water usage of 10 litres of fresh water and 20 litres of salt water per second. It is expected to give a power output of between 2-4 kW. With better membranes it is assumed that the power for a similar plant can be increased to about 10 kW. A commercial plant is expected to be built between 2012 and 2015.\n\nIn 2013, Statkraft announced that is discontinuing its work to develop osmotic power technology. The larger planned pilot facility, and the future commercial plant, will not be built.\n\nThis type of power generation is very reliable, consisting of only slightly more moving parts than a conventional hydroelectric power plant; in this case the addition of a pair of small pumps to move the fresh and salt water to the membrane surfaces. It is very quiet when operating and requires minimal supervision. In addition, it is expected the plant could respond very quickly as an emergency power source, using the membranes to 'store' power ready in the form of high pressure water; this water could be very quickly fed to the hydroelectric turbine to generate electricity. The expected lifetime of this plant is large; with almost no moving parts (those that do move are very simple and reliable), there will be little wear occurring. The availability of spare parts and upgrade components is also good, meaning that an installed osmotic power plant could be run for many years cost-effectively.\n\nWhilst highly reliable, simple and cheap to run/maintain, this plant is very expensive to install. The permeable membrane is currently an expensive resource, and to have any meaningful output, a very large membrane area is required. The plant described in this article could reach a power output of 4 kW in ideal conditions. By comparison, an open cycle gas turbine a fraction of the size (such as the Rolls-Royce or GE aero-derivative gas turbines) could easily produce greater than 15MW for a fraction of the installation costs, although fuel and maintenance costs would be greater. This is an increase in power output 3750 times greater, with a land usage that is much smaller. Comparing the ideal power output of this plant to the rough average household consumption of a modern home detailed in the article domestic energy consumption, it can be seen that this is a very limited technology - working back the figures, it equates that the average home requires 2 kW of power. Bear in mind however that advances in materials technology will likely greatly increase the power output per plant volume over time and thus make this a more useful form of power generation, particularly in remote locations where reliability is key and spare parts are difficult to come by (e.g. difficult-to-access coastal locations with a small stream or river nearby that can provide the fresh water required).\n\n"}
{"id": "41767727", "url": "https://en.wikipedia.org/wiki?curid=41767727", "title": "Søllerød Naturpark", "text": "Søllerød Naturpark\n\nSøllerød Naturpark is a protected area of rolling fields, meadows and small woods in Rudersdal Municipality, some 20 kilometres north of central Copenhagen, Denmark. It reaches from Søllerød Kirkeskov in the south to Høje Sandbjerg in the north. The area is state-owned and managed by the Danish Nature Agency.\n\nRygård Overdrev (\"Rygård Meadows\") takes its name after the farm Rygård which is situated in the middle of the park. The oldest of its buildings date from the 1790s. It was restored and adapted by the architect Palle Suenson who owned the estate between 1940 and 1987. The cultivated parts of Søllerød Naturpark are used for the growing of winter feed for the deer population in Jægersborg Dyrehave. The area is farmed organically and the main crop is oat.\n\nSøllerød Kirkeskov (\"Søllerød Church Forest\") is located in the southern part of the park, between the villages of Øverød and Søllerød. It once belonged to Søllerød Church and covers an area of 47 hectares. Attemosevej follows a watershed, meaning that water in Kalvemosen runs west to Indre Sø before continuing to Søllerød Lake, while water on the east side of the road is collected in Kikhanerenden and continues to the Øresud. A second wooded area, the small Rygaard Fredskov, is situated a little further to the north.\n\nHøje Sandbjerg is located just north of the nature park. It reaches a height of 85 metres, making it the second highest point in Rudersdal Municipality. A viewpoint at the top affords sweaping views of the area and the Øresund with the island of Hven to the east.\n\nThere is access from Øverødvej, Gl. Holtevej, Høje Sandbjergvej and Rude Skov. A public parking lot is available on Gl. Holtevej, south of Høje Sandbjergvej .\n"}
{"id": "29989", "url": "https://en.wikipedia.org/wiki?curid=29989", "title": "Triassic", "text": "Triassic\n\nThe Triassic () is a geologic period and system which spans 50.6 million years from the end of the Permian Period 251.9 million years ago (Mya), to the beginning of the Jurassic Period Mya. The Triassic is the first period of the Mesozoic Era. Both the start and end of the period are marked by major extinction events.\n\nThe Triassic began in the wake of the Permian–Triassic extinction event, which left the Earth's biosphere impoverished; it would take well into the middle of this period for life to recover its former diversity. Therapsids and archosaurs were the chief terrestrial vertebrates during this time. A specialized subgroup of archosaurs, called dinosaurs, first appeared in the Late Triassic but did not become dominant until the succeeding Jurassic Period.\n\nThe first true mammals, themselves a specialized subgroup of therapsids, also evolved during this period, as well as the first flying vertebrates, the pterosaurs, who, like the dinosaurs, were a specialized subgroup of archosaurs. The vast supercontinent of Pangaea existed until the mid-Triassic, after which it began to gradually rift into two separate landmasses, Laurasia to the north and Gondwana to the south.\n\nThe global climate during the Triassic was mostly hot and dry, with deserts spanning much of Pangaea's interior. However, the climate shifted and became more humid as Pangaea began to drift apart. The end of the period was marked by yet another major mass extinction, the Triassic–Jurassic extinction event, that wiped out many groups and allowed dinosaurs to assume dominance in the Jurassic.\n\nThe Triassic was named in 1834 by Friedrich von Alberti, after the three distinct rock layers (\"tri\" meaning \"three\") that are found throughout Germany and northwestern Europe—red beds, capped by marine limestone, followed by a series of terrestrial mud- and sandstones—called the \"Trias\".\n\nThe Triassic is usually separated into Early, Middle, and Late Triassic Epochs, and the corresponding rocks are referred to as Lower, Middle, or Upper Triassic. The faunal stages from the youngest to oldest are:\n\nDuring the Triassic, almost all the Earth's land mass was concentrated into a single supercontinent centered more or less on the equator and spanning from pole to pole, called Pangaea (\"all the land\"). From the east, along the equator, the Tethys sea penetrated Pangaea, causing the Paleo-Tethys Ocean to be closed.\n\nLater in the mid-Triassic a similar sea penetrated along the equator from the west. The remaining shores were surrounded by the world-ocean known as Panthalassa (\"all the sea\"). All the deep-ocean sediments laid down during the Triassic have disappeared through subduction of oceanic plates; thus, very little is known of the Triassic open ocean.\n\nThe supercontinent Pangaea was rifting during the Triassic—especially late in that period—but had not yet separated. The first nonmarine sediments in the rift that marks the initial break-up of Pangaea, which separated New Jersey from Morocco, are of Late Triassic age; in the U.S., these thick sediments comprise the Newark Group.\n\nBecause a super-continental mass has less shoreline compared to one broken up, Triassic marine deposits are globally relatively rare, despite their prominence in Western Europe, where the Triassic was first studied. In North America, for example, marine deposits are limited to a few exposures in the west. Thus Triassic stratigraphy is mostly based on organisms that lived in lagoons and hypersaline environments, such as Estheria crustaceans.\n\nAt the beginning of the Mesozoic Era, Africa was joined with Earth's other continents in Pangaea. Africa shared the supercontinent's relatively uniform fauna which was dominated by theropods, prosauropods and primitive ornithischians by the close of the Triassic period. Late Triassic fossils are found throughout Africa, but are more common in the south than north. The time boundary separating the Permian and Triassic marks the advent of an extinction event with global impact, although African strata from this time period have not been thoroughly studied.\n\nDuring the Triassic peneplains are thought to have formed in what is now Norway and southern Sweden. Remnants of this peneplain can be traced as a tilted summit accordance in the Swedish West Coast. In northern Norway Triassic peneplains may have been buried in sediments to be then re-exposed as coastal plains called strandflats. Dating of illite clay from a strandflat of Bømlo, southern Norway, have shown that landscape there became weathered in Late Triassic times ( 210 million years ago) with the landscape likely also being shaped during that time.\n\nAt Paleorrota geopark, located in Rio Grande do Sul, Brazil, the Santa Maria Formation and Caturrita Formations are exposed. In these formations, one of the earliest dinosaurs, \"Staurikosaurus\", as well as the mammal ancestors \"Brasilitherium\" and \"Brasilodon\" have been discovered.\n\nThe Triassic continental interior climate was generally hot and dry, so that typical deposits are red bed sandstones and evaporites. There is no evidence of glaciation at or near either pole; in fact, the polar regions were apparently moist and temperate, providing a climate suitable for forests and vertebrates, including reptiles. Pangaea's large size limited the moderating effect of the global ocean; its continental climate was highly seasonal, with very hot summers and cold winters. The strong contrast between the Pangea supercontinent and the global ocean triggered intense cross-equatorial monsoons.\n\nThe Triassic may have mostly been a dry period, but evidence exists that it was punctuated by several episodes of increased rainfall in tropical and subtropical latitudes of the Tethys Sea and its surrounding land. Sediments and fossils suggestive of a more humid climate are known from the Anisian to Ladinian of the Tethysian domain, and from the Carnian and Rhaetian of a larger area that includes also the Boreal domain (e.g., Svalbard Islands), the North American continent, the South China block and Argentina.\n\nThe best studied of such episodes of humid climate, and probably the most intense and widespread, was the Carnian Pluvial Event.\n\nThree categories of organisms can be distinguished in the Triassic record: survivors from the Permian–Triassic extinction event, new groups which flourished briefly, and other new groups which went on to dominate the Mesozoic Era.\n\nOn land, the surviving vascular plants included the lycophytes, the dominant cycadophytes, ginkgophyta (represented in modern times by \"Ginkgo biloba\"), ferns, horsetails and glossopterids. The spermatophytes, or seed plants, came to dominate the terrestrial flora: in the northern hemisphere, conifers, ferns and bennettitales flourished. \"Glossopteris\" (a seed fern) was the dominant southern hemisphere tree during the Early Triassic period.\n\nIn marine environments, new modern types of corals appeared in the Early Triassic, forming small patches of reefs of modest extent compared to the great reef systems of Devonian or modern times. Serpulids appeared in the Middle Triassic. Microconchids were abundant. The shelled cephalopods called ammonites recovered, diversifying from a single line that survived the Permian extinction.\n\nThe fish fauna was remarkably uniform, suggesting that very few families survived the Permian extinction. There were also many types of marine reptiles. These included the Sauropterygia, which featured pachypleurosaurus and nothosaurs (both common during the Middle Triassic, especially in the Tethys region), placodonts, and the first plesiosaurs. The first of the lizardlike Thalattosauria (askeptosaurs) and the highly successful ichthyosaurs, which appeared in Early Triassic seas soon diversified, and some eventually developed to huge size during the Late Triassic. Subequatorial saurichthyids have also been described in Early Triassic strata.\n\nGroups of terrestrial fauna, which appeared in the Triassic period or achieved a new level of evolutionary success during it include: \n\nThe Permian–Triassic extinction devastated terrestrial life. Biodiversity rebounded as the surviving species repopulated empty terrain, but these were short-lived. Diverse communities with complex food-web structures took 30 million years to reestablish.\n\nTemnospondyl amphibians were among those groups that survived the Permian–Triassic extinction; some lineages (e.g. trematosaurs) flourished briefly in the Early Triassic, while others (e.g. capitosaurs) remained successful throughout the whole period, or only came to prominence in the Late Triassic (e.g. plagiosaurs, metoposaurs). As for other amphibians, the first Lissamphibia, progenitors of first frogs, are known from the Early Triassic, but the group as a whole did not become common until the Jurassic, when the temnospondyls had become very rare. Other survivors the Chroniosuchia and Embolomeri were more closely related to amniotes than temnospondyls. Those became extinct after some million years.\n\nMost of the Reptiliomorpha, stem-amniotes that gave rise to the amniotes, disappeared in Triassic, but two water-dwelling groups survived; Embolomeri that only survived into the early part of the period, and the Chroniosuchia, which survived until the end of Triassic.\n\nArchosauromorph reptiles, especially archosaurs, progressively replaced the synapsids that had dominated the previous Permian period. The \"Cynognathus\" was the characteristic top predator in earlier Triassic (Olenekian and Anisian) on Gondwana. Both kannemeyeriid dicynodonts and gomphodont cynodonts remained important herbivores during much of the period, and ecteniniids played a role as large-sized, cursorial predators in the Late Triassic. During the Carnian (early part of the Late Triassic), some advanced cynodonts gave rise to the first mammals. At the same time the Ornithodira, which until then had been small and insignificant, evolved into pterosaurs and a variety of dinosaurs. The Crurotarsi were the other important archosaur clade, and during the Late Triassic these also reached the height of their diversity, with various groups including the phytosaurs, aetosaurs, several distinct lineages of Rauisuchia, and the first crocodylians (the Sphenosuchia). Meanwhile, the stocky herbivorous rhynchosaurs and the small to medium-sized insectivorous or piscivorous Prolacertiformes were important basal archosauromorph groups throughout most of the Triassic.\n\nAmong other reptiles, the earliest turtles, like \"Proganochelys\" and \"Proterochersis\", appeared during the Norian Age (Stage) of the Late Triassic Period. The Lepidosauromorpha, specifically the Sphenodontia, are first found in the fossil record of the earlier Carnian Age. The Procolophonidae were an important group of small lizard-like herbivores.\n\nDuring the Triassic, archosaurs displaced therapsids as the dominant amniotes. This \"Triassic Takeover\" may have contributed to the evolution of mammals by forcing the surviving therapsids and their mammaliaform successors to live as small, mainly nocturnal insectivores. Nocturnal life may have forced the mammaliaforms to develop fur and a higher metabolic rate.\n\n\nThe Monte San Giorgio lagerstätte, now in the Lake Lugano region of northern Italy and Switzerland, was in Triassic times a lagoon behind reefs with an anoxic bottom layer, so there were no scavengers and little turbulence to disturb fossilization, a situation that can be compared to the better-known Jurassic Solnhofen Limestone lagerstätte.\n\nThe remains of fish and various marine reptiles (including the common pachypleurosaur Neusticosaurus, and the bizarre long-necked archosauromorph \"Tanystropheus\"), along with some terrestrial forms like \"Ticinosuchus\" and \"Macrocnemus\", have been recovered from this locality. All these fossils date from the Anisian/Ladinian transition (about 237 million years ago).\n\nThe Triassic period ended with a mass extinction, which was particularly severe in the oceans; the conodonts disappeared, as did all the marine reptiles except ichthyosaurs and plesiosaurs. Invertebrates like brachiopods, gastropods, and molluscs were severely affected. In the oceans, 22% of marine families and possibly about half of marine genera went missing.\n\nThough the end-Triassic extinction event was not equally devastating in all terrestrial ecosystems, several important clades of crurotarsans (large archosaurian reptiles previously grouped together as the thecodonts) disappeared, as did most of the large labyrinthodont amphibians, groups of small reptiles, and some synapsids (except for the proto-mammals). Some of the early, primitive dinosaurs also became extinct, but more adaptive ones survived to evolve into the Jurassic. Surviving plants that went on to dominate the Mesozoic world included modern conifers and cycadeoids.\n\nThe cause of the Late Triassic extinction is uncertain. It was accompanied by huge volcanic eruptions that occurred as the supercontinent Pangaea began to break apart about 202 to 191 million years ago (40Ar/39Ar dates), forming the Central Atlantic Magmatic Province (CAMP), one of the largest known inland volcanic events since the planet had first cooled and stabilized. Other possible but less likely causes for the extinction events include global cooling or even a bolide impact, for which an impact crater containing Manicouagan Reservoir in Quebec, Canada, has been singled out. However, the Manicouagan impact melt has been dated to 214±1 Mya. The date of the Triassic-Jurassic boundary has also been more accurately fixed recently, at Mya. Both dates are gaining accuracy by using more accurate forms of radiometric dating, in particular the decay of uranium to lead in zircons formed at time of the impact. So, the evidence suggests the Manicouagan impact preceded the end of the Triassic by approximately 10±2 Ma. It could, therefore, not be the immediate cause of the observed mass extinction.\n\nThe number of Late Triassic extinctions is disputed. Some studies suggest that there are at least two periods of extinction towards the end of the Triassic, separated by 12 to 17 million years. But arguing against this is a recent study of North American faunas. In the Petrified Forest of northeast Arizona there is a unique sequence of late Carnian-early Norian terrestrial sediments. An analysis in 2002 found no significant change in the paleoenvironment. Phytosaurs, the most common fossils there, experienced a change-over only at the genus level, and the number of species remained the same. Some aetosaurs, the next most common tetrapods, and early dinosaurs, passed through unchanged. However, both phytosaurs and aetosaurs were among the groups of archosaur reptiles completely wiped out by the end-Triassic extinction event.\n\nIt seems likely then that there was some sort of end-Carnian extinction, when several herbivorous archosauromorph groups died out, while the large herbivorous therapsids—the kannemeyeriid dicynodonts and the traversodont cynodonts—were much reduced in the northern half of Pangaea (Laurasia).\n\nThese extinctions within the Triassic and at its end allowed the dinosaurs to expand into many niches that had become unoccupied. Dinosaurs became increasingly dominant, abundant and diverse, and remained that way for the next 150 million years. The true \"Age of Dinosaurs\" is during the following Jurassic and Cretaceous periods, rather than the Triassic.\n\n\n"}
{"id": "12461645", "url": "https://en.wikipedia.org/wiki?curid=12461645", "title": "Urmia Manifesto of the United Free Assyria", "text": "Urmia Manifesto of the United Free Assyria\n\nUrmia Manifesto of the United Free Assyria was written by Assyrian nationalist Freydun Atturaya, in his struggle for Assyrian independence during and after World War I. It was written in Syriac and completed in April 1917. Its ideology was Marxism, and it supported self regional independence for the Assyrian people in the Middle East. One of the goals of the Manifesto was to form a trade and military alliance with Russia.\n\n"}
{"id": "33931", "url": "https://en.wikipedia.org/wiki?curid=33931", "title": "Weight", "text": "Weight\n\nIn science and engineering, the weight of an object is related to the amount of force acting on the object, either due to gravity or to a reaction force that holds it in place.\n\nSome standard textbooks define weight as a vector quantity, the gravitational force acting on the object. Others define weight as a scalar quantity, the magnitude of the gravitational force. Others define it as the magnitude of the reaction force exerted on a body by mechanisms that keep it in place: the weight is the quantity that is measured by, for example, a spring scale. Thus, in a state of free fall, the weight would be zero. In this sense of weight, terrestrial objects can be weightless: ignoring air resistance, the famous apple falling from the tree, on its way to meet the ground near Isaac Newton, would be weightless.\n\nThe unit of measurement for weight is that of force, which in the International System of Units (SI) is the newton. For example, an object with a mass of one kilogram has a weight of about 9.8 newtons on the surface of the Earth, and about one-sixth as much on the Moon. Although weight and mass are scientifically distinct quantities, the terms are often confused with each other in everyday use (i.e. comparing and converting force weight in pounds to mass in kilograms and vice versa).\n\nFurther complications in elucidating the various concepts of weight have to do with the theory of relativity according to which gravity is modelled as a consequence of the curvature of spacetime. In the teaching community, a considerable debate has existed for over half a century on how to define weight for their students. The current situation is that a multiple set of concepts co-exist and find use in their various contexts.\n\nDiscussion of the concepts of heaviness (weight) and lightness (levity) date back to the ancient Greek philosophers. These were typically viewed as inherent properties of objects. Plato described weight as the natural tendency of objects to seek their kin. To Aristotle, weight and levity represented the tendency to restore the natural order of the basic elements: air, earth, fire and water. He ascribed absolute weight to earth and absolute levity to fire. Archimedes saw weight as a quality opposed to buoyancy, with the conflict between the two determining if an object sinks or floats. The first operational definition of weight was given by Euclid, who defined weight as: \"weight is the heaviness or lightness of one thing, compared to another, as measured by a balance.\" Operational balances (rather than definitions) had, however, been around much longer.\n\nAccording to Aristotle, weight was the direct cause of the falling motion of an object, the speed of the falling object was supposed to be directly proportionate to the weight of the object. As medieval scholars discovered that in practice the speed of a falling object increased with time, this prompted a change to the concept of weight to maintain this cause effect relationship. Weight was split into a \"still weight\" or \"pondus\", which remained constant, and the actual gravity or \"gravitas\", which changed as the object fell. The concept of \"gravitas\" was eventually replaced by Jean Buridan's impetus, a precursor to momentum.\n\nThe rise of the Copernican view of the world led to the resurgence of the Platonic idea that like objects attract but in the context of heavenly bodies. In the 17th century, Galileo made significant advances in the concept of weight. He proposed a way to measure the difference between the weight of a moving object and an object at rest. Ultimately, he concluded weight was proportionate to the amount of matter of an object, and not the speed of motion as supposed by the Aristotelean view of physics.\n\nThe introduction of Newton's laws of motion and the development of Newton's law of universal gravitation led to considerable further development of the concept of weight. Weight became fundamentally separate from mass. Mass was identified as a fundamental property of objects connected to their inertia, while weight became identified with the force of gravity on an object and therefore dependent on the context of the object. In particular, Newton considered weight to be relative to another object causing the gravitational pull, e.g. the weight of the Earth towards the Sun.\n\nNewton considered time and space to be absolute. This allowed him to consider concepts as true position and true velocity. Newton also recognized that weight as measured by the action of weighing was affected by environmental factors such as buoyancy. He considered this a false weight induced by imperfect measurement conditions, for which he introduced the term \"apparent weight\" as compared to the \"true weight\" defined by gravity.\n\nAlthough Newtonian physics made a clear distinction between weight and mass, the term weight continued to be commonly used when people meant mass. This led the 3rd General Conference on Weights and Measures (CGPM) of 1901 to officially declare \"The word \"weight\" denotes a quantity of the same nature as a \"force\": the weight of a body is the product of its mass and the acceleration due to gravity\", thus distinguishing it from mass for official usage.\n\nIn the 20th century, the Newtonian concepts of absolute time and space were challenged by relativity. Einstein's equivalence principle put all observers, moving or accelerating, on the same footing. This led to an ambiguity as to what exactly is meant by the force of gravity and weight. A scale in an accelerating elevator cannot be distinguished from a scale in a gravitational field. Gravitational force and weight thereby became essentially frame-dependent quantities. This prompted the abandonment of the concept as superfluous in the fundamental sciences such as physics and chemistry. Nonetheless, the concept remained important in the teaching of physics. The ambiguities introduced by relativity led, starting in the 1960s, to considerable debate in the teaching community as how to define weight for their students, choosing between a nominal definition of weight as the force due to gravity or an operational definition defined by the act of weighing.\n\nSeveral definitions exist for \"weight\", not all of which are equivalent.\n\nThe most common definition of weight found in introductory physics textbooks defines weight as the force exerted on a body by gravity. This is often expressed in the formula , where \"W\" is the weight, \"m\" the mass of the object, and \"g\" gravitational acceleration.\n\nIn 1901, the 3rd General Conference on Weights and Measures (CGPM) established this as their official definition of \"weight\":\n\nThis resolution defines weight as a vector, since force is a vector quantity. However, some textbooks also take weight to be a scalar by defining:\nThe gravitational acceleration varies from place to place. Sometimes, it is simply taken to have a standard value of , which gives the standard weight.\n\nThe force whose magnitude is equal to \"mg\" newtons is also known as the m kilogram weight (which term is abbreviated to kg-wt)\n\nIn the operational definition, the weight of an object is the force measured by the operation of weighing it, which is the force it exerts on its support. \nSince, W=downward force on the body by the centre of earth, and there is no acceleration in the body. So, there exists opposite and equal force by the support on the body. Also it is equal to the force exerted by the body on its support because action and reaction have same numerical value and opposite direction.\nThis can make a considerable difference, depending on the details; for example, an object in free fall exerts little if any force on its support, a situation that is commonly referred to as weightlessness. However, being in free fall does not affect the weight according to the gravitational definition. Therefore, the operational definition is sometimes refined by requiring that the object be at rest. However, this raises the issue of defining \"at rest\" (usually being at rest with respect to the Earth is implied by using standard gravity). In the operational definition, the weight of an object at rest on the surface of the Earth is lessened by the effect of the centrifugal force from the Earth's rotation.\n\nThe operational definition, as usually given, does not explicitly exclude the effects of buoyancy, which reduces the measured weight of an object when it is immersed in a fluid such as air or water. As a result, a floating balloon or an object floating in water might be said to have zero weight.\n\nIn the ISO International standard ISO 80000-4(2006), describing the basic physical quantities and units in mechanics as a part of the International standard ISO/IEC 80000, the definition of \"weight\" is given as:\nThe definition is dependent on the chosen frame of reference. When the chosen frame is co-moving with the object in question then this definition precisely agrees with the operational definition. If the specified frame is the surface of the Earth, the weight according to the ISO and gravitational definitions differ only by the centrifugal effects due to the rotation of the Earth.\n\nIn many real world situations the act of weighing may produce a result that differs from the ideal value provided by the definition used. This is usually referred to as the apparent weight of the object. A common example of this is the effect of buoyancy, when an object is immersed in a fluid the displacement of the fluid will cause an upward force on the object, making it appear lighter when weighed on a scale. The apparent weight may be similarly affected by levitation and mechanical suspension. When the gravitational definition of weight is used, the operational weight measured by an accelerating scale is often also referred to as the apparent weight.\n\nIn modern scientific usage, weight and mass are fundamentally different quantities: mass is an intrinsic property of matter, whereas weight is a \"force\" that results from the action of gravity on matter: it measures how strongly the force of gravity pulls on that matter. However, in most practical everyday situations the word \"weight\" is used when, strictly, \"mass\" is meant. For example, most people would say that an object \"weighs one kilogram\", even though the kilogram is a unit of mass.\n\nThe distinction between mass and weight is unimportant for many practical purposes because the strength of gravity does not vary too much on the surface of the Earth. In a uniform gravitational field, the gravitational force exerted on an object (its weight) is directly proportional to its mass. For example, object A weighs 10 times as much as object B, so therefore the mass of object A is 10 times greater than that of object B. This means that an object's mass can be measured indirectly by its weight, and so, for everyday purposes, weighing (using a weighing scale) is an entirely acceptable way of measuring mass. Similarly, a balance measures mass indirectly by comparing the weight of the measured item to that of an object(s) of known mass. Since the measured item and the comparison mass are in virtually the same location, so experiencing the same gravitational field, the effect of varying gravity does not affect the comparison or the resulting measurement.\n\nThe Earth's gravitational field is not uniform but can vary by as much as 0.5% at different locations on Earth (see Earth's gravity). These variations alter the relationship between weight and mass, and must be taken into account in high precision weight measurements that are intended to indirectly measure mass. Spring scales, which measure local weight, must be calibrated at the location at which the objects will be used to show this standard weight, to be legal for commerce.\n\nThis table shows the variation of acceleration due to gravity (and hence the variation of weight) at various locations on the Earth's surface.\n\nThe historic use of \"weight\" for \"mass\" also persists in some scientific terminology – for example, the chemical terms \"atomic weight\", \"molecular weight\", and \"formula weight\", can still be found rather than the preferred \"atomic mass\" etc.\n\nIn a different gravitational field, for example, on the surface of the Moon, an object can have a significantly different weight than on Earth. The gravity on the surface of the Moon is only about one-sixth as strong as on the surface of the Earth. A one-kilogram mass is still a one-kilogram mass (as mass is an intrinsic property of the object) but the downward force due to gravity, and therefore its weight, is only one-sixth of what the object would have on Earth. So a man of mass 180 pounds weighs only about 30 pounds-force when visiting the Moon.\n\nIn most modern scientific work, physical quantities are measured in SI units. The SI unit of weight is the same as that of force: the newton (N) – a derived unit which can also be expressed in SI base units as kg⋅m/s (kilograms times meters per second squared).\n\nIn commercial and everyday use, the term \"weight\" is usually used to mean mass, and the verb \"to weigh\" means \"to determine the mass of\" or \"to have a mass of\". Used in this sense, the proper SI unit is the kilogram (kg).\n\nIn United States customary units, the pound can be either a unit of force or a unit of mass. Related units used in some distinct, separate subsystems of units include the poundal and the slug. The poundal is defined as the force necessary to accelerate an object of one-pound \"mass\" at 1 ft/s, and is equivalent to about 1/32.2 of a pound-\"force\". The slug is defined as the amount of mass that accelerates at 1 ft/s when one pound-force is exerted on it, and is equivalent to about 32.2 pounds (mass).\n\nThe kilogram-force is a non-SI unit of force, defined as the force exerted by a one kilogram mass in standard Earth gravity (equal to 9.80665 newtons exactly). The dyne is the cgs unit of force and is not a part of SI, while weights measured in the cgs unit of mass, the gram, remain a part of SI.\n\nThe sensation of weight is caused by the force exerted by fluids in the vestibular system, a three-dimensional set of tubes in the inner ear. It is actually the sensation of g-force, regardless of whether this is due to being stationary in the presence of gravity, or, if the person is in motion, the result of any other forces acting on the body such as in the case of acceleration or deceleration of a lift, or centrifugal forces when turning sharply.\n\nWeight is commonly measured using one of two methods. A spring scale or hydraulic or pneumatic scale measures local weight, the local force of gravity on the object (strictly \"apparent\" weight force). Since the local force of gravity can vary by up to 0.5% at different locations, spring scales will measure slightly different weights for the same object (the same mass) at different locations. To standardize weights, scales are always calibrated to read the weight an object would have at a nominal standard gravity of 9.80665 m/s (approx. 32.174 ft/s). However, this calibration is done at the factory. When the scale is moved to another location on Earth, the force of gravity will be different, causing a slight error. So to be highly accurate, and legal for commerce, spring scales must be re-calibrated at the location at which they will be used.\n\nA \"balance\" on the other hand, compares the weight of an unknown object in one scale pan to the weight of standard masses in the other, using a lever mechanism – a lever-balance. The standard masses are often referred to, non-technically, as \"weights\". Since any variations in gravity will act equally on the unknown and the known weights, a lever-balance will indicate the same value at any location on Earth. Therefore, balance \"weights\" are usually calibrated and marked in mass units, so the lever-balance measures mass by comparing the Earth's attraction on the unknown object and standard masses in the scale pans. In the absence of a gravitational field, away from planetary bodies (e.g. space), a lever-balance would not work, but on the Moon, for example, it would give the same reading as on Earth. Some balances can be marked in weight units, but since the weights are calibrated at the factory for standard gravity, the balance will measure standard weight, i.e. what the object would weigh at standard gravity, not the actual local force of gravity on the object.\n\nIf the actual force of gravity on the object is needed, this can be calculated by multiplying the mass measured by the balance by the acceleration due to gravity – either standard gravity (for everyday work) or the precise local gravity (for precision work). Tables of the gravitational acceleration at different locations can be found on the web.\n\nGross weight is a term that is generally found in commerce or trade applications, and refers to the total weight of a product and its packaging. Conversely, net weight refers to the weight of the product alone, discounting the weight of its container or packaging; and tare weight is the weight of the packaging alone.\n\nThe table below shows comparative gravitational accelerations at the surface of the Sun, the Earth's moon, each of the planets in the solar system. The “surface” is taken to mean the cloud tops of the gas giants (Jupiter, Saturn, Uranus and Neptune). For the Sun, the surface is taken to mean the photosphere. The values in the table have not been de-rated for the centrifugal effect of planet rotation (and cloud-top wind speeds for the gas giants) and therefore, generally speaking, are similar to the actual gravity that would be experienced near the poles.\n\n"}
{"id": "31029323", "url": "https://en.wikipedia.org/wiki?curid=31029323", "title": "Wetland methane emissions", "text": "Wetland methane emissions\n\nContributing around 164 Tg of methane to atmospheric methane annually alone; wetlands are the biggest contributing factor of atmospheric methane in the world, wetlands remain a major area of concern with respect to climate change. Wetlands are characterized by water-logged soils and distinctive communities of plant and animal species that have evolved and adapted to the constant presence of water. This high level of water saturation as well as warm weather is the cause of the high levels of methane production.\n\nMost methanogenesis, or methane production, occurs in oxygen-poor environments. Because the microbes that live in warm, moist environments consume oxygen more rapidly than it can diffuse in from the atmosphere, wetlands are the ideal anaerobic environments for fermentation as well as methanogen activity. However, levels of methanogenesis can fluctuate as it is dependent on the availability of oxygen, temperature of the soil, and the composition of the soil; a warmer, more anaerobic environment with rich soil would allow for more efficient methanogenesis.\n\nFermentation is a process used by certain kinds of microorganisms to break down essential nutrients. In a process called acetoclastic methanogenesis, microorganisms from the classification domain archaea produce methane by fermenting acetate and H-CO into methane and carbon dioxide.\n\nHC-COOH → CH + CO\n\nDepending on the wetland and type of archaea, hydrogenotrophicc methanogenesis, another process that yields methane, can also occur. This process occurs as a result of archaea oxidizing hydrogen with carbon dioxide to yield methane and water.\n\n4H + CO → CH + 2HO\n\nMany different kinds of wetlands exist, all characterized by unique compositions of plant life and water conditions. To list a few, marshes, swamps, bogs, fens, peatlands, muskegs, prairie pothole (landform), and pocosins are all examples of different kinds of wetlands. Because each type of wetland is unique, the same characteristics used to classify each wetland can also be used to characterize the amount of methane emitted from that particular wetland. Any waterlogged environment with moderate levels of decomposition create the anaerobic conditions needed for methanogenesis, but the amount of water and decomposition will affect the magnitude of methane emissions in a specific environment. For example, lower water tables result in lower levels of methane emission because methanotrophic bacteria require oxic conditions to oxidize methane into carbon dioxide and water. Higher water tables, however, result in higher levels of methane emission because there is less habitable area for methanotrophic bacteria to live, and thus the methane can more easily diffuse into the atmosphere without being broken down.\n\nOften, the natural ecological progression of wetlands involves the development of one kind of wetland into one or several other kinds of wetlands. So over time, a wetland will naturally change the amount of methane emitted from its soil.\n\nFor example, Peatlands are wetlands that contain a large amount of peat, or partially decayed plant life. When peatlands are first developing, they often start out as fens, wetlands characterized by mineral rich soil. These flooded wetlands, with higher water tables, would naturally have higher emissions of methane. Eventually, the fens develop into bogs, acidic wetlands with accumulations of peat and lower water tables. With the lower water tables, methane emissions are more easily consumed by methanotrophic, or methane consuming, bacteria and never make it to the atmosphere. Over time, the peatlands develop and end up with accumulated pools of water, which once again increases emissions of methane.\n\nPrimary productivity fuels methane emissions both directly and indirectly. Plants not only provide much of the carbon needed for methane producing processes in wetlands, but in addition, methane can utilize three different pathways provided by primary productivity to reach the atmosphere: diffusion through the profile, plant aerenchyma, and ebullition.\n\nDiffusion through the profile refers to the movement of methane up through soil and bodies of water to reach the atmosphere. The importance of diffusion as a pathway varies per wetland based on the type of soil and vegetation. For example, in peatlands, the mass amount of dead, but not decaying, organic matter results in relatively slow diffusion of methane through the soil. Additionally, because methane can travel more quickly through soil than water, diffusion plays a much bigger role in wetlands with drier, more loosely compacted soil.\n\nPlant aerenchyma refers to the vessel-like transport tubes within the tissues of certain kinds of plants. Plants with arenchyma possess porous tissue that allows for direct travel of gases to and from the plant roots. Methane can travel directly up from the soil into the atmosphere using this transport system. The direct “shunt” created by the aerenchyma allows for methane to bypass oxidation by oxygen that is also transported by the plants to their roots.\n\nEbullition refers to the sudden release of bubbles of methane into the air. These bubbles occur as a result of methane building up over time in the soil, forming pockets of methane gas. As these pockets of trapped methane grow in size, the level of the soil will slowly rise up as well. This phenomenon continues until so much pressure builds up that the bubble “pops,” transporting the methane up through the soil so quickly that it does not have time to be consumed by the methanotrophic organisms in the soil. With this release of gas, the level of soil then falls once more.\n\nEbullition in wetlands can be recorded by delicate sensors, called piezometers, that can detect the presence of pressure pockets within the soil. Hydraulic heads are also used to detect the subtle rising and falling of the soil as a result of pressure build up and release. Using piezometers and hydraulic heads, a study was done in northern United States peatlands to determine the significance of ebullition as a source of methane. Not only was it determined that ebullition is in fact a significant source of methane emissions in northern United States peatlands, but it was also observed that there was an increase in pressure after significant rainfall, suggesting that rainfall is directly related to methane emissions in wetlands.\n\nThe magnitude of methane emission from a wetland are usually measured using eddy covariance, gradient or chamber flux techniques, and depends upon several factors, including water table, comparative ratios of methanogenic bacteria to methanotrophic bacteria, transport mechanisms, temperature, substrate type, plant life, and climate. These factors work together to effect and control methane flux in wetlands.\n\nOverall the main determinant of net flux of methane into the atmosphere is the ratio of methane produced by methanogenic bacteria that makes it to the surface relative to the amount of methane that is either consumed by methanotrophic bacteria or oxidized and lost before reaching the atmosphere. This ratio is in turn affected by the other controlling factors of methane in the environment. Additionally, pathways of methane emission affect how the methane travels into the atmosphere and thus have an equal effect on methane flux in wetlands.\n\nThe first controlling factor to consider is the level of the water table. Not only does pool and water table location determine the areas where methane production or oxidation may take place, but it also determines how quickly methane can diffuse into the air. When traveling through water, the methane molecules run into the quickly moving water molecules and thus take a longer time to reach the surface. Travel through soil, however, is much easier and results in easier diffusion into the atmosphere. This theory of movement is supported by observations made in wetlands where significant fluxes of methane occurred after a drop in the water table due to drought. If the water table is at or above the surface, then methane transport begins to take place primarily through ebullition and vascular or pressurized plant mediated transport, with high levels of emission occurring during the day from plants that use pressurized ventilation.\n\nTemperature is also an important factor to consider as the environmental temperature—and temperature of the soil in particular—affects the metabolic rate of production or consumption by bacteria. Additionally, because methane fluxes occur annually with the seasons, evidence is provided that suggests that the temperature changing coupled with water table level work together to cause and control the seasonal cycles.\n\nThe composition of soil and substrate availability change the nutrients available for methanogenic and methanotrophic bacteria, and thus directly affects the rate of methane production and consumption. For example, wetlands soils with high levels of acetate or hydrogen and carbon dioxide are conducive to methane production. Additionally, the type of plant life and amount of plant decomposition affects the nutrients available to the bacteria as well as the acidity. A constant availability of cellulose and a soil pH of about 6.0 have been determined to provide optimum conditions for methane production and consumption; however, substrate quality can be overridden by other factors. Soil pH and composition must still be compared to the effects of water table and temperature.\n\nNet ecosystem production (NEP) and climate changes are the all encompassing factors that have been shown to have a direct relationship with methane emissions from wetlands. In wetlands with high water tables, NEP has been shown to increase and decrease with methane emissions, most likely due to the fact that both NEP and methane emissions flux with substrate availability and soil composition. In wetlands with lower water tables, the movement of oxygen in and out of the soil can increase the oxidation of methane and the inhibition of methanogenesis, nulling the relationship between methane emission and NEP because methane production becomes dependent upon factors deep within the soil.\n\nA changing climate affects many factors within the ecosystem, including water table, temperature, and plant composition within the wetland—all factors that affect methane emissions. However, climate change can also affect the amount of carbon dioxide in the surrounding atmosphere, which would in turn decrease the addition of methane into the atmosphere, as shown by an 80% decrease in methane flux in areas of doubled carbon dioxide levels.\n\nHumans often drain wetlands in the name of development, housing, and agriculture. By draining wetlands, the water table is thus lowered, increasing consumption of methane by the methanotrophic bacteria in the soil. However, as a result of draining, water saturated ditches develop, which due to the warm, moist environment, end up emitting a large amount of methane. Therefore the actual effect on methane emission strongly ends up depending on several factors. If the drains are not spaced far enough apart, then saturated ditches will form, creating mini wetland environments. Additionally, if the water table is lowered significantly enough, then the wetland can actually be transformed from a source of methane into a sink that consumes methane. Finally, the actual composition of the original wetland changes how the surrounding environment is affected by the draining and human development.\n"}
