{"id": "36023761", "url": "https://en.wikipedia.org/wiki?curid=36023761", "title": "American Grown", "text": "American Grown\n\nAmerican Grown: The Story of the White House Kitchen Garden and Gardens Across America is a book by First Lady of the United States Michelle Obama published in 2012. The book promotes healthy eating and documents the White House Kitchen Garden through the seasons. The garden, planted in 2009 on the White House's South Lawn, was instrumental in the First Lady's Let's Move! initiative to end childhood obesity. Journalist Lyric Winik assisted Obama in the writing of \"American Grown\" and proceeds will be donated to the National Park Foundation.\n\n\"American Grown\" is divided into four sections (one for each season) and includes color photographs of vegetables, as well as recipes, instructions for making a compost bin, and stories about community gardens in the United States. The book delves into the history of gardens at the White House, including First Lady Eleanor Roosevelt's victory garden and Thomas Jefferson's attempts to grow a four-foot-long cucumber. Obama describes how the Kitchen Garden's Jefferson beds include a quote from the former President that is also a guiding principle for the garden: \"the failure of one thing repaired by the success of another; and instead of one harvest, a continued one throughout the year.\"\n\n\"American Grown\" blends the First Lady's personal gardening experiences with stories from the White House Kitchen Garden. Obama writes about the time in her life when she had \"no idea that tomatoes didn’t come in green plastic trays, covered by cellophane, and that they could be any color other than pale red.\" The portion of the book devoted to the White House beehive describes how an apple tree in the Children's Garden produced fruit for the first time in 25 years after the introduction of bees. Through her research for the book, Obama discovered that her maternal grandmother tended a community victory garden in Chicago.\n\nA review in \"The New York Times\" referred to Obama's book \"charming and thought-provoking\", writer Dominique Browning also stating that the garden serves as a good example to call the country's attention to the connection between food quality and health.\n\n"}
{"id": "17048535", "url": "https://en.wikipedia.org/wiki?curid=17048535", "title": "Angiostrongylus cantonensis", "text": "Angiostrongylus cantonensis\n\nAngiostrongylus cantonensis is a parasitic nematode (roundworm) that causes angiostrongyliasis, the most common cause of eosinophilic meningitis in Southeast Asia and the Pacific Basin. The nematode commonly resides in the pulmonary arteries of rats, giving it the common name rat lungworm. Snails are the primary intermediate hosts, where larvae develop until they are infectious.\n\nHumans are incidental hosts of this roundworm, and may become infected through ingestion of larvae in raw or undercooked snails or other vectors, or from contaminated water and vegetables. The larvae are then transported via the blood to the central nervous system, where they are the most common cause of eosinophilic meningitis, a serious condition that can lead to death or permanent brain and nerve damage. Angiostrongyliasis is an infection of increasing public health importance, as globalization contributes to the geographic spread of the disease.\n\nFirst described by the renowned Chinese parasitologist Hsin-Tao Chen (1904–1977) in 1935, after examining Cantonese rat specimens, the nematode \"Angiostrongylus cantonensis\" was identified in the cerebrospinal fluid of a patient with eosinophilic meningitis by Nomura and Lim in Taiwan in 1944. They noted that raw food eaten by the patient may have been contaminated by rats. In 1955, Mackerass and Sanders identified the lifecycle of the worm in rats, defining snails and slugs as the intermediate hosts and noting the path of transmission through the blood, brain, and lungs in rats.\n\n\"A. cantonensis\" is a helminth of the phylum Nematoda, order Strongylida, and superfamily Metastrongyloidea. Nematodes are roundworms characterized by a tough outer cuticle, unsegmented bodies, and a fully developed gastrointestinal tract. The order Strongylida includes hookworms and lungworms. Metastrongyloidea are characterized as 2-cm-long, slender, threadlike worms that reside in the lungs of the definitive host. \"Angiostrongylus costaricensis\" is a closely related worm that causes intestinal angiostrongyliasis in Central and South America.\n\nFollowing World War II, \"A. cantonensis\" spread throughout Southeast Asia and Western Pacific Islands, including Australia, Melanesia, Micronesia, and Polynesia. Cases were soon reported in New Caledonia, the Philippines, Rarotonga, Saipan, Sumatra, Taiwan, and Tahiti. In the 1960s, even more cases were reported from the region from locations such as Cambodia, Guam, Hawaii, Java, Thailand, Sarawak, Vietnam, and the New Hebrides (Vanuatu).\n\nIn 1961, an epidemiological study of eosinophilic meningitis in humans was conducted by Rosen, Laigret, and Bories, who hypothesized that the parasite causing these infections was carried by fish. However, Alicata noted that raw fish was consumed by large numbers of people in Hawaii without apparent consequences, and patients presenting with meningitis symptoms had a history of eating raw snails or prawns in the weeks before presenting with symptoms. This observation, along with epidemiology and autopsy of infected brains, confirmed \"A. cantonensis\" infection in humans as the cause of the majority of eosinophilic meningitis cases in Southeast Asia and the Pacific Islands.\n\nSince then, cases of \"A. cantonensis\" infestations have appeared in American Samoa, Australia, Hong Kong, Bombay, Fiji, Hawaii, Honshu, India, Kyushu, New Britain, Okinawa, Ryukyu Islands, Western Samoa, and most recently mainland China. Other sporadic occurrences of the parasite in its rat hosts have been reported in Cuba, Egypt, Louisiana, Madagascar, Nigeria, New Orleans, and Puerto Rico.\n\nIn 2013, \"A. cantonensis\" was confirmed present in Florida, USA, where its range and prevalence are expanding. In 2018, a case was found in a New Yorker, who had visited Hawaii.\n\nIn recent years, the parasite has been shown to be proliferating at an alarming rate due to modern food-consumption trends and global transportation of food products. Scientists are calling for a more thorough study of the epidemiology of \"A. cantonensis\", stricter food-safety policies, and the increase of knowledge on how to properly consume products commonly infested by the parasite, such as snails and slugs that act as intermediate hosts or those that act as paratenic hosts, such as fish, frogs, or freshwater prawns. Ingestion of food items that can be contaminated by the mucus excretions of intermediate or paratenic hosts, such as snails and slugs or by the feces of rats that act as definitive hosts, can lead to infection of \"A. cantonensis\". The most common route of infection of \"A. cantonesis\" in humans is by ingestion of either intermediate or paratenic hosts of the larvae. Unwashed fruits and vegetables, especially romaine lettuce, can be contaminated with snail and slug mucus or can result in accidental ingestion of these intermediate and paratenic hosts. These items need to be properly washed and handled to prevent accidental ingestion of \"A. cantonensis\" larvae or the larvae-containing hosts. The best mechanism of prevention of \"A. cantonesis\" outbreak is to institute an aggressive control of snail and slug population, proper cooking of intermediate and paratenic hosts such as fish, freshwater prawn, frogs, mollusks, and snails along with proper food-handling techniques. The common prevention techniques for diarrheal illness is very effective in preventing \"A. cantonensis\" infection. Not much is known about why it targets the brain in humans, but a chemically induced chemotaxis has been implicated recently. Acetylcholine has been previously reported to enhance motility of this worm via nicotinic acetylcholine receptors. Experimental assays in animal models are needed to validate a chemically induced chemotaxis by use of anticholinergic drugs to prevent cerebral infection following infections by \"A. cantonesis\".\n\nIntermediate hosts of larvae of for \"A. cantonensis\" include:\n\nDefinitive host of \"A. cantonensis\" include wild rodents, especially the brown rat (\"Rattus norvegicus\") and the black rat (\"Rattus rattus\").\n\nParatenic hosts of \"A. cantonensis\" include: predatory land flatworm \"Platydemus manokwari\" and amphibians \"Bufo asiaticus\", \"Rana catesbeiana\", \"Rhacophorus leucomystax\" and \"Rana limnocharis\".\n\nIn 2004, a captive yellow-tailed black cockatoo (\"Calyptorhynchus funereus\") and two free-living tawny frogmouths (\"Podargus strigoides\") suffering neurological symptoms were shown to have the parasite. They were the first avian hosts discovered for the organism.\n\nThe presence of parasitic worms burrowed in the neural tissue of the human central nervous system will cause obvious complications. All of the following will result in damage to the CNS:\n\n\nAlthough the clinical disease caused by \"Angiostrongylus\" invasion into the central nervous system is commonly referred to as \"eosinophilic meningitis\", the actual pathophysiology is of a meningoencephalitis with invasion not just of the meninges, or superficial lining of the brain, but also deeper brain tissue. Initial invasion through the lining of the brain, the meninges, may cause a typical inflammation of the meninges and a classic meningitis picture of headache, stiff neck, and often fever. The parasites subsequently invade deeper into the brain tissue, causing specific localizing neurologic symptoms depending on where in the brain parenchyma they migrate. Neurologic findings and symptoms wax and wane as initial damage is done by the physical in-migration of the worms and secondary damage is done by the inflammatory response to the presence of dead and dying worms. This inflammation can lead in the short term to paralysis, bladder dysfunction, visual disturbance, and coma and in the long term to permanent nerve damage, mental retardation, nerve damage, permanent brain damage, or death.\n\nEosinophilic meningitis is commonly defined by the increased number of eosinophils in the cerebrospinal fluid (CSF). In most cases, eosinophil levels rise to 10 or more eosinophils per μl in the CSF, accounting for at least 10% of the total CSF leukocyte (white blood cell) count. The chemical analysis of the CSF typically resembles the findings in \"aseptic meningitis\" with slightly elevated protein levels, normal glucose levels, and negative bacterial cultures. Presence of a significantly decreased glucose on CSF analysis is an indicator of severe meningoencephalitis and may indicate a poor medical outcome. Initial CSF analysis early in the disease process may occasionally show no increase of eosinophils, only to have classical increases in eosinophils in subsequent CSF analysis. Caution should be advised in using eosinophilic meningitis as the only criterion for diagnosing angiostrongylus infestation in someone with classic symptoms, as the disease evolves with the migration of the worms into the CNS.\n\nEosinophils are specialized white blood cells of the granulocytic cell line which contain granules in their cytoplasm. These granules contain proteins that are toxic to parasites. When these granules degranulate, or break down, chemicals are released that combat parasites such as \"A. cantonensis\". Eosinophils, which are located throughout the body, are guided to sites of inflammation by chemokines when the body is infested with parasites such as \"A. cantonensis\". Once at the site of inflammation, type 2 cytokines are released from helper T cells, which communicate with the eosinophils, signaling them to activate. Once activated, eosinophils can begin the process of degranulation, releasing their toxic proteins in the fight against the foreign parasite.\n\nAccording to a group case study, the most common symptoms in mild eosinophilic meningitis tend to be headache (with 100% of people in the study suffering from this symptom), photophobia or visual disturbance (92%), neck stiffness (83%), fatigue (83%), hyperesthesias (75%), vomiting (67%), and paresthesias (50%). Incubation period is often 3 weeks, but can be 3–36 days and even 80 days.\n\nPossible clinical signs and symptoms of mild and severe eosinophilic meningitis are:\n\nThe severity and clinical course of \"Angiostrongylus\" disease depends significantly on the ingested load of third-stage larvae, creating great variability from case to case, making clinical trials difficult to design, and effectiveness of treatments difficult to discern. Typical conservative medical management including analgesics and sedatives provide minimal relief for the headaches and hyperesthesias. Removing cerebrospinal fluid at regular 3- to 7-day intervals is the only proven method of significantly reducing intracranial pressure and can be used for symptomatic treatment of headaches. This process may be repeated until improvement is shown. There is growing evidence of moderate quality that suggests corticosteroid therapy using prednisolone or dexamethasone has beneficial effect in treating the CNS symptoms related to \"A. cantonensis\" infections. Although early research did not show treatment with antihelminthic agents (parasite-killing drugs) such as thiobendazole or albendazole effective in improving the clinical course of the illness, a number of recent studies from Thailand and China show that the combination of glucocorticoids and antihelminthics is safe and decreases the duration of headaches and the number of patients who had significant headache. Although the addition of antihelminthic agents for management of \"A. cantonensis\" infection has a theoretic risk of precipitating a neurologic crisis by releasing an overwhelming load of antigens through simultaneous death of the larvae, no study has shown this to exist in the clinical setting. Additionally, the failure to kill parasites before they attempt to migrate out of the CNS increases the risk of mechanical damage by migrating larvae. Although combination therapy using albendazole and prednisolone has no significant advantage compared to treatment using prednisolone alone in mild cases, the treatment with antihelminthics is demonstrably safe and may have significant benefit for patients with high parasite loads at risk for permanent disability or death. \nThe diagnosis of disease caused by \"A. cantonensis\" infestation is often difficult and relies heavily on the history of a likely ingestion of a commonly infested host and the presence of typical features of the disease. The presumptive diagnosis is particularly strong when eosinophilic meningoencephalitis can be confirmed. The diagnosis of eosinophilic meningitis can be arrived at through detection of elevated cranial pressure and increased numbers of eosinophils. The diagnosis of the cause of eosinophilic meningitis and the presence of \"A. cantonensis\" is remarkably more difficult. A spinal tap, or a sample of CSF, must be taken to search for \"A. cantonensis\" worms or larvae. \"A. cantonensis\" is undetectable in the CSF of more than half of the infected individuals. Current methods of detecting specific antigens associated with \"A. cantonensis\" are also unreliable. Consequently, alternative approaches to detect antigen-antibody reactions are being explored, such as immuno-PCR. A rapid dot-blot ELISA test is also available for quick, effective, and economical on-site diagnosis of \"A. cantonensis\".\n\n"}
{"id": "29009728", "url": "https://en.wikipedia.org/wiki?curid=29009728", "title": "Back-arc region", "text": "Back-arc region\n\nThe back-arc region is the area behind a volcanic arc. In island volcanic arcs it consists of back-arc basins of oceanic crust with abyssal depths, which may be separated by remnant arcs, similar to island arcs. In continental arcs the back-arc region is part of continental platform, either dry land (subaerial) or forming shallow marine basins.\n\nBack-arc deformation is a product of subduction at convergent plate tectonic boundaries. It initiates and evolves behind the volcanic arc on the overriding plate of a subduction zone. The stresses responsible for the deformation in this region of a subduction zone result from a combination of processes. The absolute motion of the upper plate as it moves towards or away from the trench strongly contributes to deformation in the back-arc region. Since the downgoing slab is partly anchored in the viscous layers of the mantle, and therefore its lateral movement is significantly slower than the surface plate, then any motion of the overriding plate will cause extensional or compressional stress in the back-arc region depending on the direction of motion. In addition, mantle convection in the upper mantle wedge caused by the downward movement of the subducted slab causes stress in the upper plate and the high heat flow that characterizes back-arcs. The pulling effect of the slab as it goes down into the mantle causes a rollback motion of the trench, which also applies stress on the back-arc region of the upper plate. However, this last process has less of an impact on deformation compared to upper plate motion.\n\nBack-arcs can form on either oceanic crust or continental crust. In the case of oceanic crust, most back-arc regions are subjected to tensional stresses and thus develop a spreading center where new oceanic crust is formed. The composition of this new crust is similar to MORB, although it contains higher amounts of water.\n\nThe back-arc deformation may be either extensional or compressional. The overriding plate will shorten when its motion is directed towards the trench, resulting in a compression of the back-arc region. This type of deformation is associated with a shallow dipping subducted slab. Inversely, an overriding plate moving away from the trench will result in extension, and a back-arc basin will form. This extensional deformation is associated with a steeply dipping slab.\n\nThe extreme cases of these two types of back-arc deformation can be found in Chile and at the Marianas arc. The shallow dipping slab subducting beneath Chile at an angle of about 10–15° causes a compressional stress on the back-arc region behind the Andes. On the other extreme, the slab going down into the mantle at the Marianas subduction zone is so steep it is nearly vertical. This is the perfect example of an oceanic back-arc basin experiencing extensional forces. The Oriente in Ecuador (the eastern part of the country covered by rainforest) is also a good example of an extensional back-arc basin, this time in a continental setting. The continental crust in this area east of the Andes has been stretched out and covered by layers of sediments.\n\n"}
{"id": "39531732", "url": "https://en.wikipedia.org/wiki?curid=39531732", "title": "Beam deflection tube", "text": "Beam deflection tube\n\nBeam deflection tubes, sometimes known as sheet beam tubes, are vacuum tubes with an electron gun, a beam intensity control grid, a screen grid, sometimes a suppressor grid, and two electrostatic deflection electrodes on opposite sides of the electron beam, that can direct the rectangular beam to either of two anodes in the same plane. They can be used as two-quadrant, single-balanced mixers or (de)modulators with very linear qualities, their mode of operation similar to one-half of a Gilbert Cell, by applying an unbalanced signal \"f\" to the control grid and a balanced signal \"f\" to the deflection electrodes, then extracting the balanced mixing products \"f\" − \"f\" and \"f\" + \"f\" from the two anodes. Similar to a pentagrid converter, the cathode and the first two grids can be made into an oscillator. Two beam deflection tubes can be combined to form a double-balanced mixer.\n\nThey need extensive shielding against external magnetic fields. The ballistic deflection transistors currently under development employ a similar principle.\n\n\nMore elaborate applications of the principle include:\n\nWith two-axis deflection:\n"}
{"id": "288216", "url": "https://en.wikipedia.org/wiki?curid=288216", "title": "Blue giant", "text": "Blue giant\n\nIn astronomy, a blue giant is a hot star with a luminosity class of III (giant) or II (bright giant). In the standard Hertzsprung–Russell diagram, these stars lie above and to the right of the main sequence.\n\nThe term applies to a variety of stars in different phases of development, all evolved stars that have moved from the main sequence but have little else in common, so blue giant simply refers to stars in a particular region of the HR diagram rather than a specific type of star. They are much rarer than red giants, because they only develop from more massive and less common stars, and because they have short lives in the blue giant stage.\n\nThe name blue giant is sometimes misapplied to other high-mass luminous stars, such as main-sequence stars, simply because they are large and hot.\n\nBlue giant is not a strictly defined term and it is applied to a wide variety of different types of stars. What they have in common is: a moderate increase in size and luminosity compared to main-sequence stars of the same mass or temperature, and are hot enough to be called blue, meaning spectral class O, B, and sometimes early A. They have temperatures from around 10,000 K upwards, zero age main sequence (ZAMS) masses greater than about twice the Sun (), and absolute magnitudes around 0 or brighter. These stars are only 5–10 times the radius of the Sun (), compared to red giants which are up to .\n\nThe coolest and least luminous stars referred to as blue giants are on the horizontal branch, intermediate-mass stars that have passed through a red giant phase and are now burning helium in their cores. Depending on mass and chemical composition these stars gradually move bluewards until they exhaust the helium in their cores and then they return redwards to the asymptotic giant branch (AGB). The RR Lyrae variable stars, usually with spectral types of A, lie across the middle of the horizontal branch. Horizontal-branch stars hotter than the RR Lyrae gap are generally considered to be blue giants, and sometimes the RR Lyrae stars themselves are called blue giants despite some of them being F class. The hottest stars, blue horizontal branch (BHB) stars, are called extreme horizontal branch (EHB) stars and can be hotter than main-sequence stars of the same luminosity. In these cases they are called blue subdwarf (sdB) stars rather than blue giants, named for their position to the left of the main sequence on the HR diagram rather than for their increased luminosity and temperature compared to when they were themselves main-sequence stars.\n\nThere are no strict upper limits for giant stars, but early O types become increasingly difficult to classify separately from main sequence and supergiant stars, have almost identical sizes and temperatures to the main-sequence stars from which they develop, and very short lifetimes. A good example is Plaskett's star, a close binary consisting of two O type giants both over , temperatures over 30,000 K, and more than 100,000 times the luminosity of the Sun (). Astronomers still differ over whether to classify at least one of the stars as a supergiant, based on subtle differences in the spectral lines.\n\nStars found in the blue giant region of the HR diagram can be in very different stages of their lives, but all are evolved stars that have largely exhausted their core hydrogen supplies.\n\nIn the simplest case, a hot luminous star begins to expand as its core hydrogen is exhausted, and first becomes a blue subgiant then a blue giant, becoming both cooler and more luminous. Intermediate-mass stars will continue to expand and cool until they become red giants. Massive stars also continue to expand as hydrogen shell burning progresses, but they do so at approximately constant luminosity and move horizontally across the HR diagram. In this way they can quickly pass through blue giant, bright blue giant, blue supergiant, and yellow supergiant classes, until they become red supergiants. The luminosity class for such stars is determined from spectral lines that are sensitive to the surface gravity of the star, with more expanded and luminous stars being given I (supergiant) classifications while somewhat less expanded and more luminous stars are given luminosity II or III. Because they are massive stars with short lives, many blue giants are found in O-B associations, that are large collections of loosely bound young stars.\n\nBHB stars are more evolved and have helium burning cores, although they still have an extensive hydrogen envelope. They also have moderate masses around so they are often much older than more massive blue giants. The BHB takes its name from the prominent horizontal grouping of stars seen on colour-magnitude diagrams for older clusters, where core helium burning stars of the same age are found at a variety of temperatures with roughly the same luminosity. These stars also evolve through the core helium burning stage at constant luminosity, first increasing in temperature then decreasing again as they move toward the AGB. However, at the blue end of the horizontal branch, it forms a \"blue tail\" of stars with lower luminosity, and occasionally a \"blue hook\" of even hotter stars.\n\nThere are other highly evolved hot stars not generally referred to as blue giants: Wolf–Rayet stars, highly luminous and distinguished by their extreme temperatures and prominent helium and nitrogen emission lines; post-AGB stars forming planetary nebulae, similar to Wolf–Rayet stars but smaller and less massive; blue stragglers, uncommon luminous blue stars observed apparently on the main sequence in clusters where main-sequence stars of their luminosity should have evolved into giants or supergiants; and the true blue supergiants, the most massive stars evolved beyond blue giants and identified by the effects of greater expansion on their spectra.\n\nA purely theoretical group of stars could be formed when red dwarfs finally exhaust their core hydrogen trillions of years into the future. These stars are convective through their depth and are expected to very slowly increase both their temperature and luminosity as they accumulate more and more helium until eventually they cannot sustain fusion and they quickly collapse to white dwarfs. Although these stars can become hotter than the Sun they will never become more luminous, so are hardly blue giants as we see them today. The name blue dwarf has been coined although that name could easily be confusing.\n"}
{"id": "14322950", "url": "https://en.wikipedia.org/wiki?curid=14322950", "title": "Centre for Energy and Environmental Markets", "text": "Centre for Energy and Environmental Markets\n\nThe Centre for Energy and Environmental Markets (CEEM) at the University of New South Wales conducts interdisciplinary research into energy and environmental markets and associated policies. CEEM brings together researchers from the areas of Business, Engineering and Social Sciences. In addition to undertaking research, CEEM hosts short courses and seminars and participates in conferences in Australia and internationally. The Centre also contributes to undergraduate teaching and to supervision of postgraduate students.\n\n"}
{"id": "2350124", "url": "https://en.wikipedia.org/wiki?curid=2350124", "title": "Claudetite", "text": "Claudetite\n\nClaudetite is an arsenic oxide mineral with chemical formula AsO. Claudetite is formed as an oxidation product of arsenic sulfides and is colorless or white. It can be associated with arsenolite (the cubic form of AsO) as well as realgar (AsS), orpiment (AsS) and native sulfur.\n\nIt was first described in 1868 for an occurrence in the San Domingo mines, Algarve, Portugal. It was first described by and named for the French chemist Frederick Claudet.\n"}
{"id": "9002170", "url": "https://en.wikipedia.org/wiki?curid=9002170", "title": "Cock throwing", "text": "Cock throwing\n\nCock throwing, also known as cock-shying or throwing at cocks, was a blood sport widely practised in England until the late 18th century. A rooster was tied to a post, and people took turns throwing coksteles (special weighted sticks) at the bird until it died. Cock throwing was traditionally associated with Shrove Tuesday; a contributor to \"The Gentleman's Magazine\" in 1737, during an anti-Gallican phase of British culture, was of the opinion that cock throwing arose from traditional enmity towards the French, for which the cock played an emblematic role.\n\nCock throwing was a popular pastime with people of all classes, especially with children, and although widespread, was less common than cockfighting. Sir Thomas More referred to his skill in casting a cokstele as a boy. If the bird had its legs broken or was lamed during the event, it was sometimes supported with sticks in order to prolong the game. The cock was also sometimes placed inside an earthenware jar to prevent it from moving. Variations on the theme included goose quailing (or squailing), when a goose was substituted, and cock thrashing or cock whipping, which involved a cock being placed in a pit where the blindfolded participants would attempt to hit it with their sticks. A Sussex variation was similar to bull-baiting with the rooster tied to a cord.\n\nIn 1660, an official pronouncement by Puritan officials in Bristol to forbid cock throwing (as well as cat and dog tossing) on Shrove Tuesday resulted in a riot by the apprentices.\n\nCock throwing's popularity slowly waned in England, as social values changed and animal welfare became a concern. William Hogarth depicted it as a barbarous activity, the first stage in a \"slippery slope\", in \"The Four Stages of Cruelty\" in 1751, and Nathan Drake credited this in part for changes in public attitudes to the sport. The Anglican divine and political economist Josiah Tucker also dismissed the sport as a \"most cruel and barbarous diversion' in his 'Earnest and Affectionate Address to the Common People of England Concerning their Usual Recreations on Shrove Tuesday' (1753), drawing attention to the suffering and 'lingering tortures' of a 'poor innocent creature'. From the middle of the 18th century, magistrates began to deal with the problem more harshly, a marker of its loss in popularity among the \"respectable\" classes, imposing fines for public order offences, and local by-laws banned the practice in many places.\n\nBy the early 19th century, the tradition was all but forgotten, lingering as isolated incidents into the 1840s.\n\n"}
{"id": "7034768", "url": "https://en.wikipedia.org/wiki?curid=7034768", "title": "Dallas Lore Sharp", "text": "Dallas Lore Sharp\n\nDallas Lore Sharp (1870–1929) was an American author and university professor, born in the Haleyville section of Commercial Township, in Cumberland County, New Jersey.\n\nHe graduated at Brown University in 1895, served as a Methodist Episcopal minister for four years, and graduated at the Boston University School of Theology in 1899. He married Grace Hastings and the couple had four sons, including Waitstill Sharp.\n\nHe was assistant librarian (1899–1902), assistant professor of English (1902–09), and thereafter professor at Boston University.\n\nAs a writer he became known through his charming magazine articles on native birds and small mammals and for his books which featured illustrations by American wildlife illustrator Robert Bruce Horsfall as well as artist Elizabeth Myers Snagg.\n\n\n"}
{"id": "13344815", "url": "https://en.wikipedia.org/wiki?curid=13344815", "title": "Electricity sector in Bolivia", "text": "Electricity sector in Bolivia\n\nThe electricity sector in Bolivia is dominated by the state-owned ENDE Corporation (Empresa Nacional de Electricidad), although the private Bolivian Power Company (\"Compañia Boliviana de Energía Eléctrica\"; COBEE) is also a major producer of electricity. ENDE had been unbundled into generation, transmission and distribution and privatized in the 1990s, but most of the sector was re-nationalized in 2010 (generation) and 2012 (transmission and distribution).\n\nThe supply is dominated by thermal generation (65%), while hydropower (35%) has a smaller share in its generation mix compared to other South American countries. (Latin America and the Caribbean, or LAC, average hydropower capacity is 51%.) In 2014, national electricity supply of 1580,35 MW comfortably exceeded the 1298,2 MW maximum demand. Like in other countries, Bolivia’s electricity sector consists of a National Interconnected System (SIN) and off-grid systems (known as the \"Aislado\").\n\nThe national government's priorities for the electricity sector include providing universal access to electricity and producing surplus energy for export. The electricity coverage in rural areas is among the lowest in Latin America and improving it represents a major challenge in the future. The government envisions a major expansion of electricity generation capacity to over 8,000MW over the decade from 2015 to 2025, primarily to export surplus generating capacity.\n\nIn Bolivia, the National Interconnected System (SIN) connects major population centers and represents 83% of the installed capacity. The SIN provides electricity to the largest cities and operates in the Departments of Cochabamba, Santa Cruz, Oruro, Potosí and Chuquisaca. Its grid extends over 1,200 miles and covers the central and southern parts of the country. The population in the northern and western parts of the country remains largely unconnected to the national grid, either served by the off-grid system (the \"Aislado\") or having no access to electricity at all. The off-grid system consists of numerous self producers and independent power plants in rural or isolated areas.\n\nTotal installed capacity in 2006 was 1.43 GW, of which 60% was thermal production, which primarily burns natural gas, and 40% hydroelectric. The contribution of other renewables is almost negligible. Total electricity production in the same year amounted to 5.29 TWh. This figure does not include electricity produced in rural areas from biomass facilities, which are unorganized, decentralized, and difficult to quantify.\n\nBolivia’s electricity export and import activities are fairly limited. Imports from Brazil amount to less than 0.01 TWh per year and have so far been devoted to supply the city of Puerto Suarez, in the Department of Santa Cruz.\n\nElectric power consumption per capita in 2006 was 588 kWh (a 19% increase since 1996). By sector, residential consumption represents 40% of the total, followed by industrial consumption with 28%.\n\nAccording to the demand projections prepared by the \"Superintendencia de Electricidad\" (SE), the generation capacity reserve will be insufficient by 2009 as it will fall below the recommended 10% if no new capacity is developed.\n\nIn 2005, total access to electricity in Bolivia was 67%, one of the lowest in Latin America. Urban access was 87%, while rural access remained as low as 30%.\n\nService quality as measured by interruptions was much better than the LAC average in 2005. In the period November 2004 – October 2005 there were a total of 141 transmission interruptions (up from 86 in the previous period), with a total duration of 4,274 minutes. 57% of the interruptions were due to weather conditions and, while 17% derived from facility operations. In 2005, the average duration of interruptions per subscriber was 5 hours (highest since 1998 although far below the 14 hours average for LAC), while the average number of interruptions per subscriber per year was 7 (highest since 1995 but also below the 13 average for LAC).\n\nDuring the 1990s and up to 2005, distribution losses have always been close to 10%, which is below the 13.6% average for the region.\n\nThe Viceministry of Electricity and Alternative Energy, within the Ministry of Petroleum and Energy, is in charge of establishing policies and designing the regulation for the electricity sector. The \"Superintendencia de Electricidad \" (SE) is responsible for applying the regulation.\n\nThe companies that belong to the National Interconnected System (SIN) must be vertically unbundled. However, the companies in the off-grid system (the \"Aislado\") are allowed to perform more than one of the activities defined in the electricity industry (i.e. generation, transmission, distribution).\n\nCurrently, there are eight generation companies in the interconnected system, all of them privately owned. The three largest companies alone represent 70% of the total generation. The largest company serving the SIN is the \" Compañia Boliviana de Energía Eléctrica\" (COBEE), which serves the region surrounding La Paz. The other two are \"Empresa Eléctrica Guarachi\" (EGSA) and \"Empresa Eléctrica Corani\" (CORANI).\n\nCurrently, there are three transmission companies which operate the high-voltage Interconnecting Trunk System (STI), the backbone of the SIN. ENDE Corporation, ISA Bolivia, which was created in 2005, and San Cristobal TESA. The state-owned ENDE incorporates \"Transportadora de Electricidad\" (TDE), which was owned by Spain’s Red Electrica de España (REE). As of 2013, its transmission network extended 2,850 km, representing 79% of the national total. ISA Bolivia, which runs 587 km, or 16%, of the transmission network in Bolivia, is a subsidiary of Interconexión Eléctrica S.A. (ISA), a corporation controlled by the government of Colombia. San Cristobal TESA has 172 km of transmission lines, or 5%. The number of companies is limited due to the existence of institutional entry barriers in this sector.\n\nIn Bolivia, the six existing distribution companies enjoy a geographic monopoly in their concession areas. The largest company is Electropaz, majority-owned by Spain’s Iberdrola; followed by \"Empresa de Luz y Fuerza Eléctrica Cochabamba\" (ELFEC), which was owned by the American PPL Global until 2007. The third place is occupied by the Rural Electrification Cooperative (CRE), which operates in the Department of Santa Cruz.\n\nIn some cases, especially in the high plateau, cooperatives and community organizations access the distribution companies’ network and sell electricity to small rural communities. Sometimes, those are organized enterprises that provide the service to middle-size towns, but in most cases, they are small organizations that serve family communities. This situation faces a legal vacuum since the consumers benefiting from these scheme, who do not consume the minimum power established legally established, cannot be considered as regulated ones. In addition, these consumers are localized outside the distribution companies’ concession areas, so they cannot receive the companies’ service. In practice, the distribution companies are reselling electricity to the mentioned organizations outside the legal framework. Accurate information on the number of organizations that operate in rural areas does not exist. However, there are approximately three in La Paz, twenty in Oruro and three in Potosi.\n\nThe departments of Beni, Pando and Tarija and the eastern region of Santa Cruz are not integrated in the SIN. As a result, there are vertically integrated operators that provide the service. The most important operators are:\n\n\nRenewable energy resources other than hydropower are barely exploited in Bolivia, and their contribution to electricity generation is insignificant. However, the potential of decentralized electricity systems (i.e. solar photovoltaics or PV, wind, etc.) for disperse populations was recognized by the government in the Rural Electrification Plan (PLABER). This component established that the service to households, schools and health facilities in areas of low population density would rely on locally available renewable energy sources.\n\nBolivia has a considerable hydro potential, its technically feasible potential being assessed at 126 TWh/yr, of which 50 TWh/yr is considered to be economically exploitable. Only a small proportion of the total potential has been harnessed so far.\nThe share of hydropower capacity in Bolivia is 40%, which is below the 51% average for the region. Installed hydroelectric capacity in 2016 was 494 MW, distributed amongst some 21 facilities. The largest plant is the 93 MW Saint Isabel, operated by Corani.\nBolivia is working with Brazil on a huge joint project to exploit the hydro-electric potential of the Rio Madeira complex in the Amazon region. Within this project are the 800 MW Cachuela Esperanza plant sited entirely in Bolivia and the Guajara-Mirim plant (3 000 MW) to be located on the border between the two countries.\n\nThe World Bank is financing a rural infrastructure project in Bolivia which, among others, plans to install 17,000 solar home systems by 2009. Another project funded by the Global Partnership on Out-based Aid which is administered by the World Bank intends to scale up the installations by an additional 7000 in the next three years. (see External Assistance below)\n\nIn the mid-2010s, ENDE began building and operating large-scale solar power plants including the following:\n\nThe National Interconnected System (SIN) was created in 1965 and continued its expansion during the ‘70s and ‘80s.\n\nUntil 1994, the vertically integrated public utility, ENDE (\"Empresa Nacional de Electricidad\") dominated the sector. In 1994, Bolivia initiated an infrastructure reform program that included the privatization of the major state owned enterprises. The Electricity Law 1994/1604 mandated the privatization of the electricity system and the unbundling of generation, transmission and distribution activities. The law aimed to increase efficiency in the sector, promote competition and encourage investment.\n\nAs a result of privatization, three generation companies were created: Corani, Guaracachi and Valle Hermoso. Each of them received a part of ENDE’s generation activities (the law limited the market share of each to 35%). Initially, these firms were granted exclusive rights, but by 1999 entry was liberalized and some other small companies entered the market. In regards to transmission, network operation was transferred from ENDE to \"Transportadora de Electricidad\", a private company, which acquired exclusive rights. Finally, several distribution firms were created after the reform. All those firms operate under tariff regulation and are subject to tariff controls. CRE (Cooperative for Rural Electrification), a pre-existing distribution cooperative, maintained its position as an independent regional monopoly. CESSA (\"Compañía Eléctrica de Sucre S.A.\") and SEPSA (\"Servicios Eléctricos Potosí S.A. \"), two pre-existing municipal distribution firms also maintained their monopolies while ELFEC (\"Empresa Luz y Fuerza Eléctrica Cochabamba\"), which was a municipal company previously to privatization, started to operate as a private firm. Lastly, the private COBEE (\"Compañía Boliviana de Energía Eléctrica\"), which operated both in generation and distribution, gave rise, after its divestiture, to Electropaz (in La Paz) and ELFEO (\"Empresa Luz y Fuerza Eléctrica Oruro\") (in Oruro).\nAll these reforms, together with the introduction of a load dispatch coordination office, shaped a wholesale electricity market that seeks to simulate competitive conditions.\n\nIn 2002, the government established Bolivia’s Rural Electrification Plan (PLABER) with the objective of contributing to the socio-economic development of rural areas through access to electricity and its efficient and productive uses. The short-term goal of the program was to make 200,000 new connections within five years (increasing electricity access in rural areas from 23% to 45%). It had been estimated that, by the end of the program, PLABER would have reached its goals by 70% of the initial objective. However, the model established by this program did not have significative effects in increasing coverage, expanding infrastructure and improving service quality in rural and isolated areas.\n\nA new Rural Electrification Decree was approved in 2005 (Supreme Decree No. 28567). This new decree aims at increasing rural access through the extension and densification of electric networks, development of renewable energy and a change in the energy mix (substitution of diesel by natural gas, biomass and other renewable energies) and an increase in distribution capacity. The Rural Electrification Decree and its associated regulatory framework encourages stakeholders in the energy sector to establish partnerships with other government agencies to implement the rural electrification plan. An agreement between the Ministry of Public Works, Services and Housing and the Ministry of Education will allow for the installation of solar PV systems in rural areas in conjunction with the literacy program, “I can” (\"Yo, sí puedo\"). Under the pilot phase, 500 solar panels are expected to be installed.\n\nIn 2006, under President Evo Morales, a new Law for Universal Access to Electricity (\"Ley de Acceso Universal\") was proposed. Under the framework of this Law, the program called ‘Electricity for a Decent Living” has been designed to improve both rural and urban electrification. The short-term goal (2006–2010) of the program is to increase rural electrification to 53% (connection of 210,000 new households) and urban electrification to 97% (connection of 460,000 new households). The medium-term goal (2010–2015) is to achieve universal access in urban areas and a 70% access in rural ones. In the final stages, rural access would have increased to 87% by 2020 and universal coverage would be reached by 2025. The Law also mandates de creation of a Common Fund for Universal Access to Public Electricity Service (FOCO) and creates a co-financing mechanism of the National Government with Prefectures, municipalities and the private sector. However, the Law has not been approved yet, although it is expected that the Law and the mechanisms it creates will be approved soon.\n\nElectricity tariffs in Bolivia are far lower than average tariffs in Latin America. The average residential tariff in 2006 was US$0.0614 per kWh (compared to US$0.115 per kWh weighted averagein LAC), while the average tariff for the industry was US$0.044 per kWh (compared to US$0.107 per kWh weighted average in LAC).\n\nElectricity prices charged by the distribution companies to their regulated clients include energy costs (including generation and transmission costs) and all the distribution costs, including a specific return on investment. However, as it was mentioned before, some distribution companies resell electricity to communal organizations or rural enterprises. The price and conditions of those transactions are not regulated, which causes an important legal void in the sector.\n\nIn March 2006, the Bolivian government approved \"Tarifa Dignidad\" (“Dignity Tariff”) by Supreme Decree 28653. This tariff grants a 25% discount in their electricity bills to those consumers whose monthly consumption is below 70 kWh in the urban areas and 30 kWh in the rural ones. This subsidy, which will be covered for four years by the electricity companies that operate in Bolivia, will benefit about 480,000 households.\n\nIn the years following the privatization of 1994, investment grew considerably due to the obligations imposed on the privatized companies. Public investment remained constant, mainly directed to rural electrification, while private investment was mainly devoted to generation.\n\nIn the period 1995-2004, total investment in transmission represented only a modest 2% (US$14 million) of the total investment in the electricity sector. Generation and distribution received 58% and 40% of total investment respectively. For the year 2004 in particular, total public investment in the electricity sector was around $US 20million which was matched by another $US20 million from private sources. Those two figures add up to less than 0.5% of Bolivia’s GDP in 2004.\n\nDistribution companies acquire investment commitments for each tariff period. For the period 2003-2005, total investment by the main distribution companies was US$39.7 million. This was divided as follows:\n\n\nThe Electricity Law 1994/1604 mandated the privatization of the electricity system and the unbundling of generation, transmission and distribution activities, which had all been in the hands of ENDE (\"Empresa Nacional de Electricidad\") the vertically integrated public utility.\n\nThree of the eight generation companies in the interconnected system represent 70% of total generation, with COBEE (\"Compañía Boliviana de Energía Eléctrica\") being the most important. Transmission in the National Interconnected System is in the hands of just two companies as entry in this sector is institutionally restricted. As for distribution, the six existing companies enjoy a geographic monopoly in their concession areas. The largest company is Electropaz, which is majority-owned by Spain's Iberdrola.\n\nIn May 2010 president Evo Morales nationalized 80% of Bolivian generation by capacity, in his Government's attempt to regain ownership of public service companies. Corani, Guarachi, and Valle Hermoso, where expropriated from their former British and French owners. In the same fashion, in May 2012 the transmission company Transmisora de Electricidad S.A., from Spanish capitals, was similarly expropriated.\n\n(*) 53% of the transmission network is operated by ISA Bolivia, a subsidiary of ISA Colombia, which is controlled by the Colombian government.\n\nThe Viceministry of Land Planning and Environment, within the Planning and Development Ministry, holds the environmental responsibilities in Bolivia.\n\nOLADE (Organización Latinoamericana de Energía) estimated that CO emissions from electricity production in 2003 were 1.73 million tons of CO, which represents 22 percent of total emissions for the energy sector.\n\nCurrently, there are just two CDM-registered projects in Bolivia, one of them in the electricity sector. That is the Rio Taquesi Hydroelectric Power Project, in the province of Sud Yungas, with an effective capacity of 89.5 megawatts and estimated emission reductions of 141,691 metric tonnes COe per annum \n\nExternal assistance to the electricity sector in Bolivia is heavily focused on rural electrification with no funding for large-scale generation, which has been fully privatized.\n\nCurrently, the World Bank is involved in two projects in the energy sector in Bolivia:\n\n\nThe Inter-American Development Bank provides technical assistance through three projects in the energy sector in Bolivia:\n\n\nIn 2005 and 2007, the \"Corporación Andina de Fomento\" (CAF) assigned US$32 million and US$15 million respectively to the increase of electricity services coverage through the construction of two transmission lines. The first one is the 115 kV Caranavi (La Paz)-Trinidad (Beni) line, which has 374 km. The second one is the 230 kV Carrasco and Santibáñez line, with 225 km.\n\nThe German Agency for International Cooperation (GIZ) and the Viceministry of Electricity and Alternative Energy recently signed an agreement to execute a Project for Network Densification in rural areas. Distribution cooperatives will also play an important role in this project, which is part of the program “Electricity to live with dignity”. To facilitate the access to electricity, the Viceministry and GIZ will provide a US$20 subsidy per household to enable their connection to the existing network.\n\nThe German development bank KfW is also financing a 5 million Euro project to improve rural access to electricity in 2005-8. The project involves the construction of six micro-hydroelectric plants and the completion of studies for another eleven.\n\n\n\n"}
{"id": "18699764", "url": "https://en.wikipedia.org/wiki?curid=18699764", "title": "Energy policy of Romania", "text": "Energy policy of Romania\n\nRomania is the 38th largest energy consumer in the world and the largest in South Eastern Europe as well as an important producer of natural gas, oil and coal in Europe.\n\nThe total energy consumption of Romania was in 2005 40.5 million toe structured as follows:\n\nRomania's natural resources in the year 2007 were structured as follows:\n\nRomania has the largest oil reserves in Central and Eastern Europe (\"except Russia\") and the second largest natural gas reserves (\"except Russia\") behind Ukraine but twice as large than Poland but it has the smallest reserves of coal in the region.\n\nRomania's oil production in 2007 was around 120,000 bbl/d while the consumption of oil was around 230,000 bbl/d.\n\nRomania had the largest oil production in the year 1976 when the total quantity extracted was close to 14.7 million tonnes. Since then the oil production increased and decreased regularly but in the last 10 years the production had a descending path.\n\nIn 2006 the total coal production of Romania was 34.1 million tonnes of which:\n\nAlmost all the coal extracted in the country is used for electricity generation, bituminous coal having a 7.2% contribution in all electricity produced in Romania and lignite having a 32.2% stake.\n\nEvolution of bituminous coal production:\n\nEvolution of lignite production:\n\nIn 2007 Romania produced a total of 12.3 billion m of natural gas the most important producers being Petrom with 6.3 billion m and Romgaz with 6 billion m.\n\nThe natural gas consumption in 2007 was 17.4 billion m with the local producers providing around 70% and imports of 5.1 billion m supplying the rest.\n\nIn 2006 Romania produced a total of 62 TWh of electricity having an installed capacity of 17,360 MW in thermal, hydro and nuclear power plants.\nThe power generation was structured as follows:\n\nRomania has an estimated total usable hydropower of 36,000 GWh per year. In 2007 the total installed capacity of hydropower plants in Romania was 6,400 MWh all of which were owned by Hidroelectrica.\n\nIn 2007, Romania produced 19.8 TWh of hydropower. Romania co-owns the Iron Gate I Hydroelectric Power Station on the Danube River located on the border between Romania and Serbia, which is one of the largest hydroelectric power plants in Europe with an installed generation capacity of 2,216 MW by 6 generating units of 175 MW each on the Serbian side and 6 generating units of 194.4 MW on the Romanian side.\n\nThe two countries also jointly operate the Iron Gate II Hydroelectric Power Station with an installed generation capacity of 537 MW by 8 generating units of 27 MW on the Serbian side and 10 generating units of 32 MW on the Romanian side.\n\nThe largest hydropower plant on the inner rivers of Romania is the Lotru-Ciunget Hydroelectric Power Station with an installed generation capacity of 510 MW but this power plant will be surpassed by the Tarniţa – Lăpuşteşti Hydroelectric Power Station which at completion in 2014 will have an installed generation capacity of 1,000 MW.\n\nRomania currently has 1,400 MW of nuclear power capacity by means of one active nuclear power plant with 2 reactors, which makes up to around 18% of the national power generation capacity of the country. This makes Romania the 23rd largest user of nuclear power in the world.\n\nThe Cernavodă Nuclear Power Plant () is the only nuclear power plant in Romania. It uses CANDU reactor technology from AECL, using heavy water produced at Drobeta-Turnu Severin as its neutron moderator and water from the Danube for cooling.\n\nThere are also plans for the construction of a second nuclear power plant in Transylvania that will either have 2 reactors of 1,200 MW each or 4 reactors of 600 MW each with an electricity generating capacity of 2,400 MW and will be constructed after 2020\n\n, Wind power in Romania has an installed capacity of 10 MW, up from the 3 MW installed capacity in 2006.\n\nRomania has a high wind power potential of around 14,000 MW and a power generating capacity of 23 TWh, but until 2007 there were no significant wind farms in operation.\n\nThe annual energy potential for renewable energy in Romania is:\n\nIn recent years Romania increased its production of biofuels in a sustained manner. The country uses rapeseed as a source of biodiesel. In 2007 Romania had a total cultivated area of 430,000 ha with Rapeseed and had a production of 750,000 tonnes.\n\nIn 2007 Romania produced 400,000 tonnes of biodiesel mainly from rapeseed and sunflower seeds.\n\nThe German company MAN Ferrostaal invested US$200 million in the construction of a biodiesel refinery in Sibiu County that produces around 142,000 tonnes of biodiesel per year.\n\nThere are other companies that are interested in investing in biodiesel in Romania like the Martifer Group in Portugal that will build a biodiesel refinery in Călăraşi County where it will invest US$120 million.\n\nWhen this refinery will be at full capacity, Romania will be in the top ten biodiesel producing countries in the world.\n"}
{"id": "2620670", "url": "https://en.wikipedia.org/wiki?curid=2620670", "title": "Fairfax Stone Historical Monument State Park", "text": "Fairfax Stone Historical Monument State Park\n\nFairfax Stone Historical Monument State Park is a West Virginia state park commemorating the Fairfax Stone, a surveyor's marker and boundary stone at the source of the North Branch of the Potomac River. The original stone was placed on October 23, 1746 to settle a boundary dispute between Thomas Fairfax, 6th Lord Fairfax of Cameron and the English Privy Council concerning the Northern Neck of Virginia. It determined the proprietorship and boundaries of a large tract of mostly unsurveyed land in the English colonies of Maryland and Virginia.\n\nFairfax Stone Historical Monument, part of a four-acre West Virginia state park, is six miles north of Thomas, West Virginia. The site is sparsely developed, lacking any buildings or restroom facilities.\n\nThe exact boundaries of the \"Northern Neck Land Grant\" (later called the \"Fairfax Grant\") had been undetermined since it was first contrived in 1649 by the then-exiled King Charles II.\nJohn Savage and his survey party had located the site of the source of the North Branch of the Potomac River (the northern boundary of the tract) in 1736, but had made no attempt to establish the western boundaries. A 1746 survey by Colonel Peter Jefferson (Thomas Jefferson's father) and Thomas Lewis resulted in both the placement of the Fairfax Stone as well as the establishment of a line of demarcation known as the \"Fairfax Line\", extending from the Stone to the south-east and ending at the source of the Rappahannock River, a distance of .\n\nThe North Branch of the Potomac River initially heads west from its source at the Fairfax Stone before curving north and then generally flowing east toward Chesapeake Bay. For this reason, the Stone is only a county corner of West Virginia counties rather than part of the state's border with Maryland, an issue that was only resolved when the Supreme Court ruled against Maryland in 1910 in \"Maryland v. West Virginia\", determining that Maryland would only go westward up the Potomac far enough to meet a point where a line north from the Fairfax Stone would cross that branch of the Potomac. Until the ruling, the boundary of Maryland was indeterminate. Three West Virginia counties—Grant, Preston and Tucker— share the boundary marked by the Fairfax Stone (West Virginia having seceded from Virginia during the American Civil War).\n\nThe original Fairfax Stone, in accordance with common surveying practices of the era, was most likely simply a natural, unmarked rock selected from among the outcroppings in the area. Legal boundary disputes between Maryland and Virginia caused the latter to relocate the stone in 1833 after the site had been lost to memory. The stone was still intact in 1859 when one Lieutenant Melcher found it again and reran the \"Fairfax Line\" on behalf of the two states. The Stone was gone by 1909, however, having been carried away by vandals. There have been six Fairfax Stones, each one replacing the last owing to weathering or vandalism. The current stone is a six-ton rock with a flat face, on which is inlaid a historical plaque detailing the stone's significance. Next to it is the 1910 stone. The present six-ton Fairfax Stone was dedicated on October 5, 1957.\n\n\"Fairfax Stone\"<br>\"This monument, at the headspring of the Potomac River, Marks one of the historic spots of America. Its name is derived from Thomas Lord Fairfax who owned all the land lying between the Potomac and Rappahannock Rivers. The first Fairfax Stone, marked \"FX\", was set in 1746 by Thomas Lewis, a surveyor employed by Lord Fairfax. This is the base point for the western dividing line between Maryland and West Virginia.\"\n\n\n"}
{"id": "15735983", "url": "https://en.wikipedia.org/wiki?curid=15735983", "title": "Fog desert", "text": "Fog desert\n\nA fog desert is a type of desert where fog drip supplies the majority of moisture needed by animal and plant life.\n\nExamples of fog deserts include the Atacama Desert of coastal Chile and Peru, the Baja California Desert of Mexico, the Namib Desert in Namibia, the Arabian Peninsula coastal fog desert,\nand Biosphere 2, an artificial closed ecosphere in Arizona.\n\nThe Atacama Desert, the driest desert in the world, is able to maintain a sufficient level of plant and animal species biodiversity because of its status as a fog desert.\n\nDrastic changes in elevation such as mountain ranges allow for maritime winds to settle in specific geographic areas, which is a common theme in fog deserts. The Andes Mountain range is the land form that divides Chile and Peru into inland and coastal regions and allows for the fog desert to form along the Pacific coast. In the Atacama Desert, plant coverage can be as high as 50% in the center fog zone to as low as no life whatsoever above the fog line. Within arid fog deserts with low precipitation levels, fog drip provides the moisture needed for agricultural development. The diverse sections within a fog desert region allow for the growth of a diverse group of plant structures such as succulents, deciduous species, and woody shrub.\n\nHumidity in foggy air is above 95%. One way for fog to form in deserts is through the interaction of hot humid air with a cooler object, such as a mountain. When warm air hits cooler objects, fog is composed. Another way for fog to be form in deserts, is if the desert is close to the coast and that coast has some cold currents running through it. When air heated in the desert land blows towards the cool water in the ocean, it gets cooler and fog is formed. The cool fog would then get blown inland by the ocean breeze. Fog is mainly formed in the early morning or after sunset.\n\nThere are certain types of plants and animals that live in fog deserts. Almost all of them depend on water contained in the fog for their survival which is evident in the way they have developed adaptations to collect water. An example would be the Stenocara beetle that lives in the Namib Desert; the beetle climbs the sand dunes when the humid wind is blowing from the ocean and transforms fog into drops of water. An example of a plant would be Welwitschia which also grows in the Namib desert and grows only two leaves through its life which can be up to 1000 years. The leaves have large pores to help it absorb water from the fog forming on them. People also living in the Namib depend on techniques to collect water from fog. A lot of technologies are being developed to help extract water from desert air Fog harvesters are undergoing improvements based on our observations of the adaptations of some of the beings in fog deserts like the Stenocara beetle. Devices being developed to extract water from air desert use Metal Organic Framework crystals to capture and hold water molecules when exposed to air flow at night. In the morning, air flow is cut and the water collected previous night is then vaporized by exposure to sunlight and then condensed into liquid water when it hits the cooler condenser.\n\n"}
{"id": "37168292", "url": "https://en.wikipedia.org/wiki?curid=37168292", "title": "Geomagnetic Field Monitoring Program of SUPARCO", "text": "Geomagnetic Field Monitoring Program of SUPARCO\n\nThe Geomagnetic Field Monitoring Program, is a scientific mission of the Space and Upper Atmosphere Research Commission of Pakistan (SUPARCO) to undertake research in geophysics, particularly geomagnetism. The objective of the mission is to provide better understanding of the Earth's magnetic field, and of associated hazard mitigation. The program monitors mathematical variation in the South Asian regional geomagnetic field. Research is conducted in specially established geomagnetic observatories in Islamabad and Karachi. Data collected provides the basis for continuous studies of Earth's magnetic field, and is made available to various national and international institutions. \n\nThe program was established in 1983 at the Sonmiani space facility. The second geomagnetic observatory was established in 2008. Suparco regularly releases a public domain bulletin of geomagnetic data to national and international users, containing research on the effects of solar flares and severe magnetic storms recorded by the observatories.\n\n"}
{"id": "11036055", "url": "https://en.wikipedia.org/wiki?curid=11036055", "title": "Grid-tied electrical system", "text": "Grid-tied electrical system\n\nA grid-tied electrical system, also called \"tied to grid\" or \"grid tie system\", is a semi-autonomous electrical generation or grid energy storage system which links to the mains to feed excess capacity back to the local mains electrical grid. When insufficient electricity is available, electricity drawn from the mains grid can make up the shortfall. Conversely when excess electricity is available, it is sent to the mains grid.\n\nWhen batteries are used for storage, the system is called battery-to-grid (B2G), which includes vehicle-to-grid (V2G).\n\nDirect Current (DC) electricity from sources such as hydro, wind or solar is passed to an inverter which is grid tied. The inverter monitors the alternating current mains supply frequency and generates electricity that is phase matched to the mains. When the grid fails to accept power during a \"black out\", most inverters can continue to provide courtesy power.\n\nA key concept of this system is the possibility of creating an electrical micro-system that is not dependent on the grid-tie to provide a high level quality of service. If the mains supply of the region is unreliable, the local generation system can be used to power important equipment.\n\nBattery-to-grid can also spare the use of fossil fuel power plants to supply energy during peak loads on the public electric grid. Regions that charge based on time of use metering may benefit by using stored battery power during prime time.\n\nLocal generation can be from an environmentally friendly source such as pico hydro, solar panels or a wind turbine. Individuals can choose to install their own system if an environmentally friendly mains provider is not available in their location.\n\nA micro generation facility can be started with a very small system such as a home wind power generation, photovoltaic (solar cells) generation, or micro combined heat and power (Micro-CHP) system.\n\n\n\n\n\nDistributed generation\nBattery (electricity)"}
{"id": "1279578", "url": "https://en.wikipedia.org/wiki?curid=1279578", "title": "Hayagriva", "text": "Hayagriva\n\nHayagriva, also spelt Hayagreeva (IAST: , literally 'Horse-neck'), is a horse-headed avatar of the Lord Vishnu in Hinduism.\n\nIn Hinduism, Lord Hayagriva is an avatar of Lord Vishnu. He is worshipped as the god of knowledge and wisdom, with a human body and a horse's head, brilliant white in color, with white garments and seated on a white lotus. Symbolically, the story represents the triumph of pure knowledge, guided by the hand of God, over the demonic forces of passion and darkness.\n\nOrigins about the worship of Hayagriva have been researched, some of the early evidences dates back to 2,000 BCE, when Indo-Aryan people worshipped the horse for its speed, strength, intelligence. Hayagriva is one of the prominent deities in Vaishnava tradition. His blessings are sought when beginning study of both sacred and secular subjects. Special worship is conducted on the day of the full moon in August (Śravaṇa-Paurṇamī) (his \"avatāra-dina\") and on Mahanavami, the ninth day of the Navaratri festival. He is also hailed as \"Hayasirsa\". Hayaśirṣa means haya=Horse, śirṣa=Head.\nIn IAST\n\"jñānānandamayaṃ devaṃ nirmalasphaṭikākṛtiṃ\"\n\"ādhāraṃ sarvavidyānāṃ hayagrīvaṃ upāsmahe\"\nIn Devanāgarī\nज्ञानानन्दमयं देवं निर्मलस्फटिकाकृतिं\nआधारं सर्वविद्यानां हयग्रीवं उपास्महे\nThis verse is originally from the Pañcarātra Agamas but is now popularly prefixed to the \"Hayagriva Stotram\" of the 13th-century poet-philosopher Vedanta Desika. It is very popular among devotees of Hayagrīva.\n\nVedanta Desika's \"dhyāna-śloka\" (meditative verse) on Hayagrīva typifies this deity's depiction in Hindu iconography:\nHe has four hands, with one in the mode of bestowing knowledge;\nanother holds books of wisdom, and the other two hold the Conch and Discus.\nHis beauty, like fresh cut crystal, is an auspicious brilliance that never decays.\nMay this Lord of speech who showers such cooling rays of grace\non me be forever manifest in my heart!\n\nIn the Mahavairocana-sutra [Sūtra of the Great Sun] translated and copied in 1796 by I-hsing it says:\n\n-- \"Hayagrīva Stotram, v.32\"\nLater on Hayagriva is referred to as the “Horse necked one”, Defender of faith”, the “Terrible executioner”, the “Excellent Horse”, and the “Aerial horse”.\n\nThis said, the Horse Avatar of Lord Vishnu is seen as pulling the sun up to the heavens every day, bringing light to darkness. Hayagriva’s consort is Marichi, the goddess of the rising sun, more accurately the sun’s light which is the life force of all things, and which is seen as the female aspect of Hayagriva. Marichi represents the essence of the power of creation of the cosmos. Whereas Hayagriva represents the other male aspect.\n\nIn several other sources he is a white horse who pulls the sun into the sky every morning. In others such as the great epic Taraka-battle where the gods are fallen on and attacked by the Danava’s [demons], Vishnu appears as a great ferocious warrior called Hayagriva when he comes to their aid. It says;\n\n“\"Hayagriva appears in his chariot, drawn by 1,000 powerful steeds, crushing the enemies of the gods beneath him\"!”\n\nThere are many other references to Hayagriva throughout the Mahabharata. It is said that Vishnu comes from battle as a conqueror in the magnificent mystic form of the great and terrible Hayagriva;\n\n“\"The great Hayagriva having been praised in this way by the different saints and ascetics, assumes a great white horses head. The verda’s [mantras] made up his shape, his body built of all the great gods; in the middle of his head was Shiva, in his heart was Brahmā [god of heaven]; the rays of sun (Marichi) were his mane, the sun and moon his eyes; the Vasus and Sadhyas were his legs, in all his bones were the gods. Agni [Ka-ten; god of fire] was his tongue, the goddess Satya his speech, while his knees were formed by the Maruts and Varuna. Having assumed this form, an awesome wonder to behold to the gods, he vanquished the asura, and cast them down, with eyes that were red with anger.\"”\n\nInvariably, Hayagriva is depicted seated, most often with his right hand either blessing the supplicant or in the \"vyākhyā mudrā\" pose of teaching. The right hand also usually holds a \"akṣa-mālā\" (rosary), indicating his identification with meditative knowledge. His left holds a book, indicating his role as a teacher. His face is always serene and peaceful, if not smiling. Unlike his Buddhist counterpart, there is no hint of a fearsome side in the Hindu description of this deity. Indeed, the two deities seem to be totally unrelated to one another.\n\nHayagriva is sometimes worshiped in a solitary pose of meditation, as in temple in Thiruvanthipuram. This form is known as Yoga-Hayagriva. However, he is most commonly worshipped along with his consort Lakshmi and is known as Lakshmi-Hayagriva. Hayagriva in this form is the presiding deity of Mysore's Parakala Mutt, a significant Sri Vaishnavism monastic institution.\n\nA legend has it that during the creation, the demons Madhu-Kaitabha stole the Vedas from Brahma, and Vishnu then took the Hayagriva form to recover them. The two bodies of Madhu and Kaitabha disintegrated into twelve pieces (two heads, two torsos, four arms and four legs). These are considered to represent the twelve seismic plates of the Earth. Yet another legend has it that during the creation, Vishnu compiled the Vedas in the Hayagrīva form.\n\nSome consider Hayagriva to be one of the Dashavataras of the Supreme Personality of Godhead. He along with Śrī Krishna, Shrī Rama and Shri Narasimha is considered to be an important avatar of the Supreme Personality of Godhead.\n\nLord Hayagriva is also amongst the deities present at the Ranganathaswamy Temple, Srirangam. Hayagreeva in Srirangam is very famous for children's education. The Sannidhi is very beautiful and all rituals are done according to Vedic principles very strictly.\n\nA great devotee named ŚrīVadirajatirtha of Udupi Śrī Kṛṣṇa Mutt used to offer cooked horse gram (Kollu) to Lord Hayagreeva. He used to recite the Hayagrīva Śloka and keep the offerings on his head. Lord Hayagrīva would come as a beautiful white horse form and would eat the horse gram. As a very staunch devotee, Vadirajatirtha would recite the following \"sloka\" -\n\nNa HayagrivAth Param Asthi MangaLam\n\nNa HayagrivAth Param Asthi Paavanam\n\nNa HayagrivAth Param Asthi Dhaivatham\n\nNa Hayagrivam Pranipathya Seedhathi!\n\nThere is no auspiciousness greater than Hayagrivan.\nNothing is more sacred than Sri Hayagrivan to destroy our accumulated sins. \nNo other God is superior to Hayagrivan. \nNo one grieves after performing Śaraṇagati at the sacred feet of Hayagrivan.\n\nHayagriva is listed as one of the ten incarnations of Vishnu in Canto 10 (skandh 10), chapter 40 of the Śrīmadbhagavatam, and Akrūra's prayer contains Hayagriva's name when he had a vision while bathing in Yamuna.\n\nThere is a story that more than 500 years ago, a devotee from the Daivajña Brahmin community was casting an idol of Lord Gaṇapati, when it shaped itself in the image of Lord Hayagriva. Sarvabhauma Sri Vadiraja Guru Swamiji had a dream about this legend which inspired him to approach that devotee and take the idol from him in reverence. He then installed it in Shri Sode Vadiraja Mutt. Since then it has been worshipped there as the originating God of the Daivajnya Brahmin community.\n\nThe members of the community who were originally Smartha Brahmins, were instructed in Dvaita philosophy and received into the fold as Mukhya Śiṣyas by the revered Sri Vadiraja Teertha. To this day, Daivajña Brahmins continue to make offerings to the Mutt and Lord Hayagriva.\n\n\nMost of the major Vishnu temples in Tamil Nadu have a separate shrine for Lord Hayagriva.\n\n\nUdupi\nSonda, Sirsi\n\n\nTelangana\n\nIn the 2015 documentary series, The Creatures of Philippine Mythology, the spread of Hinduism and the imagery of Hayagriva is tracked through South East Asia. It is speculated that Hayagriva influenced the image of the Philippine horse-headed folk spirit, the Tikbalang.\n\n\n\n\n"}
{"id": "3973438", "url": "https://en.wikipedia.org/wiki?curid=3973438", "title": "High-altitude nuclear explosion", "text": "High-altitude nuclear explosion\n\nHigh-altitude nuclear explosions are the result of nuclear weapons testing. Several such tests were performed at high altitudes by the United States and the Soviet Union between 1958 and 1962.\n\nThe strong electromagnetic pulse (EMP) that results has several components. In the first few tens of nanoseconds, about a tenth of a percent of the weapon yield appears as powerful gamma rays with energies of one to three mega-electron volts (MeV, a unit of energy). The gamma rays penetrate the atmosphere and collide with air molecules, depositing their energy to produce huge quantities of positive ions and recoil electrons (also known as Compton electrons). The impacts create MeV-energy Compton electrons that then accelerate and spiral along the Earth's magnetic field lines. The resulting transient electric fields and currents that arise generate electromagnetic emissions in the radio frequency range of 15 to 250 megahertz (MHz, or fifteen million to 250 million cycles per second). This high-altitude EMP occurs between 30 and 50 kilometers (18 and 31 miles) above the Earth's surface.\nThe potential as an anti-satellite weapon became apparent in August 1958 during \"Hardtack Teak\". The EMP observed at the Apia Observatory at Samoa was four times more powerful than any created by solar storms, while in July 1962 the \"Starfish Prime\" test, damaged electronics in Honolulu and New Zealand (approximately 1,300 kilometers away), fused 300 street lights on Oahu (Hawaii), set off about 100 burglar alarms, and caused the failure of a microwave repeating station on Kauai, which cut off the sturdy telephone system from the other Hawaiian islands. The radius for an effective satellite kill for the various Compton radiation produced by such a nuclear weapon in space was determined to be roughly 80 km. Further testing to this end was carried out, and embodied in a Department of Defense program, \"Program 437\".\nThere are problems with nuclear weapons carried over to testing and deployment scenarios, however. Because of the very large radius associated with nuclear events, it was nearly impossible to prevent indiscriminate damage to other satellites, including one's own satellites. \"Starfish Prime\" produced an artificial radiation belt in space that soon destroyed three satellites (Ariel, TRAAC, and Transit 4B all failed after traversing the radiation belt, while Cosmos V, Injun I and Telstar 1 suffered minor degradation, due to some radiation damage to solar cells, etc.). The radiation dose rate was at least 60 rads/day at four months after \"Starfish\" for a well-shielded satellite or manned capsule in a polar circular earth orbit, which caused NASA concern with regard to its manned space exploration programs.\n\nIn general, nuclear effects in space (or very high altitudes) have a qualitatively different display. While an atmospheric nuclear explosion has a characteristic mushroom-shaped cloud, high-altitude and space explosions tend to manifest a spherical 'cloud,' reminiscent of other space-based explosions until distorted by Earth's magnetic field, and the charged particles resulting from the blast can cross hemispheres to create an auroral display which has led documentary maker Peter Kuran to characterize these detonations as 'the rainbow bombs'. The visual effects of a high-altitude or space-based explosion may last longer than atmospheric tests, sometimes in excess of 30 minutes. Heat from the \"Bluegill Triple Prime\" shot, at an altitude of 50 kilometers (31 mi), was felt by personnel on the ground at Johnston Atoll, and this test caused retina burns to two personnel at ground zero who were not wearing their safety goggles.\n\nThe Soviets detonated four high-altitude tests in 1961 and three in 1962. During the Cuban Missile Crisis in October 1962, both the US and the USSR detonated several high-altitude nuclear explosions as a form of saber rattling.\n\nThe worst effects of a Soviet high-altitude test occurred on 22 October 1962, in the Soviet Project K nuclear tests (ABM System A proof tests) when a 300 kt missile-warhead detonated near Dzhezkazgan at 290-km altitude. The EMP fused 570 km of overhead telephone line with a measured current of 2,500 A, started a fire that burned down the Karaganda power plant, and shut down 1,000-km of shallow-buried power cables between Tselinograd and Alma-Ata.\n\nThe Partial Test Ban Treaty was passed the following year, ending atmospheric and exoatmospheric nuclear tests. The Outer Space Treaty of 1967 banned the stationing and use of nuclear weapons in space. The Comprehensive Nuclear-Test-Ban Treaty of 1996 prohibits all kinds of nuclear explosions; whether over- or underground, underwater or in the atmosphere.\n\n\nUS Government Films:\n"}
{"id": "19348699", "url": "https://en.wikipedia.org/wiki?curid=19348699", "title": "Historia animalium (Gessner)", "text": "Historia animalium (Gessner)\n\nHistoria animalium (\"History of the Animals\"), published at Zurich in 1551–58 and 1587, is an encyclopedic \"inventory of renaissance zoology\" by Conrad Gessner (1516–1565). Gessner was a medical doctor and professor at the Carolinum in Zürich, the precursor of the University of Zurich. The \"Historia animalium\" is the first modern zoological work that attempts to describe all the animals known, and the first bibliography of natural history writings. The five volumes of natural history of animals cover more than 4500 pages.\n\nThe \"Historia animalium\" was Gessner's magnum opus, and was the most widely read of all the Renaissance natural histories. The generously illustrated work was so popular that Gessner's abridgement, \"Thierbuch\" (\"Animal Book\"), was published in Zurich in 1563, and in England Edward Topsell translated and condensed it as a \"Historie of foure-footed beastes\" (London: William Jaggard, 1607). Gessner’s monumental work attempts to build a connection between the ancient knowledge of the animal world, its title the same as Aristotle's work on animals, and what was known at his time. He then adds his own observations, and those of his correspondents, in an attempt to formulate a comprehensive description of the natural history of animals.\n\nGessner’s \"Historia animalium\" is based on classical sources. It is compiled from ancient and medieval texts, including the inherited knowledge of ancient naturalists like Aristotle, Pliny the Elder, and Aelian. Gessner was known as \"the Swiss Pliny.\" For information he relied heavily on the \"Physiologus\". \n\nIn his larger works Gessner sought to distinguish fact from myth and popular misconceptions, and so his encyclopedic work included both extinct creatures and newly discovered animals of the East Indies, those of the far north and animals brought back from the New World. The work included extensive information on mammals, birds, fish, and reptiles. It described in detail their daily habits and movements. It also included their uses in medicine and nutrition. \n\n\"Historia animalium\" showed the animals' places in history, literature and art. Sections of each chapter detailed the animal and its attributes, in the tradition of the emblem book. Gessner's work included facts in different languages such as the names of the animals.\n\n\nThe colored woodcut illustrations were the first real attempts to represent animals in their natural environment. It is the first book to illustrate fossils. \n\nGessner acknowledges one of his main illustrators was Lucas Schan, an artist from Strasbourg. He likely used other illustrators as well as himself; the book is however famous for copying illustrations from other sources, including Durer's Rhinoceros from a well-known woodcut. Gessner's natural history was unusual for sixteenth century readers in providing illustrations.\n\nThere was extreme religious tension at the time \"Historia animalium\" came out. Under Pope Paul IV it was felt that the religious convictions of an author contaminated all his writings, and as Gessner was a Protestant, it was added to the Catholic Church's list of prohibited books.\n\n\n"}
{"id": "4859982", "url": "https://en.wikipedia.org/wiki?curid=4859982", "title": "Howz", "text": "Howz\n\nIn traditional Persian architecture, a howz () is a centrally positioned symmetrical axis pool. If in a traditional house or private courtyard, it is used for bathing, aesthetics or both. If in a sahn of a mosque, it is used for performing ablutions. A howz is usually around deep. It may be used as a \"theatre\" for people to sit on all sides of the pool while others entertain.\n\nHowz is a feature of the Persian gardens.\n\n"}
{"id": "58527266", "url": "https://en.wikipedia.org/wiki?curid=58527266", "title": "In vitro spermatogenesis", "text": "In vitro spermatogenesis\n\nIn vitro spermatogenesis is the process of creating male gametes (spermatozoa) outside of the body in a culture system. The process could be useful for fertility preservation, infertility treatment and may further develop the understanding of spermatogenesis at the cellular and molecular level.  \n\nSpermatogenesis is a highly complex process and artificially rebuilding it \"in vitro\" is challenging. These include creating a similar microenvironment to that of the testis as well as supporting endocrine and paracrine signalling, and ensuring survival of the somatic and germ cells from spermatogonial stem cells (SSCs) to mature spermatozoa.\n\nDifferent methods of culturing can be used in the process such as isolated cell cultures, fragment cultures and 3D cultures.\n\nCell cultures can include either monocultures, where one cell population is cultured, or co-culturing systems, where several cells lines (must be at least two) can be cultured together. Cells are initially isolated for culture by enzymatically digesting the testis tissue to separate out the different cells types for culture. The process of isolating cells can lead to cell damage.\n\nThe main advantage of monoculture is that the effect of different influences on one specific cell population of cells can be investigated. Co-culture allows for the interactions between cell populations to be observed and experimented on, which is seen as an advantage over the monoculture model.\n\nIsolated cell culture, specifically co-culture of testis tissue, has been a useful technique for examining the influences of specific factors such as hormones or different feeder cells on the progression of spermatogenesis \"in vitro\". For example, factors such as temperature, feeder cell influence and the role of testosterone and follicle-stimulating hormone (FSH) have all been investigated using isolated cell culture techniques.\n\nStudies have concluded that different factors can influence the culture of germ cells e.g. media, growth factors, hormones and temperature. For example, when culturing immortalized mouse germ cells at temperatures of 35, 37 and 29℃, these cells proliferate most rapidly at the highest temperature and least rapidly at the lowest but there were varying levels of differentiation. At the highest temperature no differentiation were detected, some was seen at 37℃ and some early spermatids appearing at 32℃.\n\nInvestigations of appropriate feeder cells concluded that a variety of cells could encourage development of germ cells such as Sertoli cells, Leydig cells and peritubular myoid cells but the most essential is Sertoli cells, but Leydig and peritubular myoid cells both contribute to the microenvironment that encourage stem cells to remain pluripotent and self renew in the testis.\n\nIn fragment cultures, the testis is removed and fragments of tissue are cultured in supplemental media containing different growth factors to induce spermatogenesis and form functional gametes. The development of this culture technique has taken place mainly with the use of animal models e.g. mice or rat testis tissue.\n\nThe advantage of using this method is that it maintains the natural spatial arrangement of the seminiferous tubules. However, hypoxia is a recurring problem in these cultures where the low oxygen supply hinders the development and maturation of spermatids (significantly more in adult than immature testis tissues). Other challenges with this type of culture include maintaining the structure of the seminiferous tubules which makes it more difficult for longer-term cell cultures as the tissue structures can flatten out making it hard to work with. To resolve some of these issues, 3D cultures can be used.\n\nIn 2012, mature spermatozoa capable of fertilization was isolated from \"in vitro\" culture of immature mouse testis tissue.\n\n3D cultures use sponge, models or scaffolds that resemble the elements of the extracellular matrix to achieve a more natural spatial structure of the seminiferous tubules and to better represent the tissues and the interaction between different cell types in an \"ex vivo\" experiment. Different components of the extracellular matrix such as collagen, agar and calcium alginate are commonly used to form the gel or scaffold which can provide oxygen and nutrients. To propagate 3D cultures, testicular cell cultures are imbedded into the porous sponge/scaffold and allowed to colonise the structure which can then survive for several weeks to allow spermatogonia to differentiate and mature into spermatozoa.\n\nIn addition, shaking 3D cultures during the seeding process allows for an increased oxygen supply which helps overcome the issue of hypoxia and so improves the lifespan of cells.\n\nIn contrast to monocultures, fragment/3D cultures are able to establish \"in vitro\" conditions that can somewhat resemble the testicular microenvironment to allow a more accurate study of the testicular physiology and its associations with the \"in vitro\" development of sperm cells.\n\nThe ability to recapitulate spermatogenesis \"In vitro\" provides a unique opportunity to study this biological process through oftentimes cheaper and faster method of research than \"in vivo\" work. Observation is often easier \"in vitro\", as the targeted cells are mostly isolated and immobile. Another significant advantage of \"in vitro\" research is the ease with which environmental factors can be changed and monitored. There are also techniques which are not practical or feasible \"in vivo\" which can now be explored.\n\n\"In vitro\" work is not without its own challenges. For example, one loses the natural structure provided by the \"in vivo\" tissue, and thus cell connections which could be important to the function of the tissue.\n\nWhile rodent spermatogenesis is not identical to its human counterpart, especially due to the high evolution rate of the male reproductive tract, these techniques are a solid starting point for future human applications.\n\nVarious categories of infertile men may benefit from advances in these techniques, especially those with a lack of viable gamete production. These men cannot benefit, for example, from sperm extraction techniques, and currently have little to no options for producing genetic descendants.\n\nNotably, males who have undergone chemo/radiotherapy prepubertally may benefit from \"in vitro\" spermatogenesis. These people did not have the option to cryopreserve viable sperm before their procedure, and thus the ability to generate genetically descended sperm later in life is invaluable. Possible methods that could be applied (to this and other groups) are induction of spermatogenesis in testis samples taken prepubertally, or, if these samples are not available/viable, new methods that manipulate stem cell differentiation could produce SSCs ‘from scratch’, using adult stem cell samples.\n\nAn alternative method is to graft preserved tissue back onto adult cancer survivors, however this comes with operational risks, as well as a risk of reintroducing malignant cells. Even if using this method however, \"in vitro\" spermatogenesis advances would allow for sample expansion and observation to better ensure quality and quantity of graft tissue.\n\nIn those with healthy or preserved SSCs but without a cellular environment to support them, \"in vitro\" spermatogenesis could be used following transplant of the SSCs into healthy donor tissue.\n\nAnother group that could be helped by \"in vitro\" spermatogenesis are those with any form of genetic impediment to sperm production. Those with no viable SSC development are an obvious target, but also those with varying levels of spermatogenic arrest; previously their underdeveloped germ cells have been injected into oocytes, however this has a success rate of only 3% in humans.\n\nFinally, \"in vitro\" spermatogenesis using animal or human cells can be used to assess the effects and toxicity of drugs before \"in vivo\" testing.\n"}
{"id": "29136815", "url": "https://en.wikipedia.org/wiki?curid=29136815", "title": "Institut de recherche d'Hydro-Québec", "text": "Institut de recherche d'Hydro-Québec\n\nL'Institut de recherche d'Hydro-Québec \"(Hydro-Québec Research Institute)\", known by its acronym IREQ (\"Institut de recherche en électricité du Québec\", \"Quebec Electricity Research Institute\") is a research institute established in 1967 by government-owned utility Hydro-Québec. IREQ operates from Varennes, a town on the south shore of Montreal, Quebec, Canada. IREQ operates on an annual research budget of approximately $100 million and specializes in the areas of high voltage, mechanics and thermomechanics, network simulations and calibration.\n\nIn the last 20 years, the institute has also conducted research and development work towards the electrification of ground transportation. Current projects include battery advanced materials, including work on molten salts, lithium iron phosphate and nanotitanate, improved electric drive trains and the impacts of the large scale deployment of electric vehicles on the power grid. Projects focus on technologies to increase range, improve performance in cold weather and reduce charging time.\n\n\n"}
{"id": "2127098", "url": "https://en.wikipedia.org/wiki?curid=2127098", "title": "Ionosonde", "text": "Ionosonde\n\nAn ionosonde, or chirpsounder, is a special radar for the examination of the ionosphere. The basic ionosonde technology was invented in 1925 by Gregory Breit and Merle A. Tuve and further developed in the late 1920s by a number of prominent physicists, including Edward Victor Appleton. The term \"ionosphere\" and hence, the etymology of its derivatives, was proposed by Robert Watson-Watt.\n\nAn ionosonde consists of:\n\n\nThe transmitter sweeps all or part of the HF frequency range, transmitting short pulses. These pulses are reflected at various layers of the ionosphere, at heights of 100–400 km, and their echos are received by the receiver and analyzed by the control system. The result is displayed in the form of an ionogram, a graph of reflection height (actually time between transmission and reception of pulse) versus carrier frequency.\n\nAn ionosonde is used for finding the optimum operation frequencies for broadcasts or two-way communications in the high frequency range.\n\nA chirp transmitter is a shortwave radio transmitter that sweeps the HF radio spectrum on a regular schedule. If one is monitoring a specific frequency, then a \"chirp\" is heard (in CW or SSB mode) when the signal passes through. In addition to their use in probing ionospheric properties, these transmitters are also used for over-the-horizon radar systems.\n\nAn analysis of current transmitters has been done using SDR technology. For better identification of chirp transmitters the following notation is used: <repetition rate (s)>:<chirp offset (s)>, where the repetition rate is the time between two sweeps in seconds and the chirp offset is the time of the first sweep from 0 MHz after a full hour in seconds. If the initial frequency is greater than 0 MHz, the offset time can be linearly extrapolated to 0 MHz.\n\n\n\n"}
{"id": "2990125", "url": "https://en.wikipedia.org/wiki?curid=2990125", "title": "Langhian", "text": "Langhian\n\nThe Langhian is, in the ICS geologic timescale, an age or stage in the middle Miocene epoch/series. It spans the time between 15.97 ± 0.05 Ma and 13.65 ± 0.05 Ma (million years ago) during the Middle Miocene.\n\nThe Langhian was a continuing warming period defined by Lorenzo Pareto in 1865, it was originally established in the Langhe area north of Ceva in northern Italy, hence the name. The Langhian is preceded by the Burdigalian and followed by the Serravallian stage.\n\nThe base of the Langhian is defined by the first appearance of foraminifer species \"Praeorbulina glomerosa\" and is also coeval with the top of magnetic chronozone C5Cn.1n. A GSSP for the Langhian stage was not yet established in 2009.\n\nThe top of the Langhian stage (the base of the Serravallian stage) is at the first occurrence of fossils of the nanoplankton species \"Sphenolithus heteromorphus\" and is located in magnetic chronozone C5ABr.\n\nThe Langhian is coeval with the Orleanian and Astaracian European Land Mammal Mega Zones (more precisely: with biozones MN5 and MN6, MN6 starts just below the Langhian-Serravallian boundary), with the upper Hemingfordian to mid-Barstovian North American Land Mammal Ages, with mid-Relizian to Luisian Californian regional stages (the Luisian extends barely into the early Serravallian), with the early-mid Badenian Paratethys stage of Central and eastern Europe, with the Tozawan stage in Japan (which runs barely into the early Serravallian), with the late Batesfordian through Balcombian to early Bairnsdalian Australian stages and with the mid-Cliffdenian to mid-Lillburnian New Zealand stages.\n\n\nSharks, rays, skates and relatives\n\n\n\n\n"}
{"id": "37355128", "url": "https://en.wikipedia.org/wiki?curid=37355128", "title": "List of English animal nouns", "text": "List of English animal nouns\n\nThe following is a list of English animal nouns, (the common names of kinds of animals). This list includes the common names used for the animal in general; names for the male animal and the female animal where such names exist; the name used for the young or juveniles of the animal; the common name given for the sound the animal makes, if any; the group noun where applicable; and the name of both the natural shelter and (if applicable) an artificial shelter for the animals.\n\n\n"}
{"id": "5880200", "url": "https://en.wikipedia.org/wiki?curid=5880200", "title": "List of Hahniidae species", "text": "List of Hahniidae species\n\nThis page lists all described species of the spider family Hahniidae as of Jan. 6, 2017.\n\n\"Alistra\" \n\n\"Amaloxenops\" \n\n\"Antistea\" \n\n\"Asiohahnia\" \n\n\"Austrohahnia\" \n\n\"Calymmaria\" \n\n\"Cryphoeca\" \n\n\"Cryphoecina\" \n\n\"Cybaeolus\" \n\n\"Dirksia\" \n\n\"Ethobuella\" \n\n\"Hahnia\" \n\n\"Hahniharmia\" \n\n\"Harmiella\" \n\n\"Iberina\" \n\n\"Intihuatana\" \n\n\"Kapanga\" \n\n\"Lizarba\" \n\n\"Neoantistea\" \n\n\"Neoaviola\" \n\n\"Neocryphoeca\" \n\n\"Neohahnia\" \n\n\"Pacifantistea\" \n\n\"Porioides\" \n\n\"Rinawa\" \n\n\"Scotospilus\" \n\n\"Tuberta\" \n\n\"Willisus\" \n\n"}
{"id": "6102014", "url": "https://en.wikipedia.org/wiki?curid=6102014", "title": "List of Sites of Special Scientific Interest in Orkney", "text": "List of Sites of Special Scientific Interest in Orkney\n\nThe following is a list of Sites of Special Scientific Interest in the Orkney Area of Search. For other areas, see List of SSSIs by Area of Search.\n\n\n"}
{"id": "6554860", "url": "https://en.wikipedia.org/wiki?curid=6554860", "title": "List of United States Naval reactors", "text": "List of United States Naval reactors\n\nList of United States Naval reactors is a comprehensive annotated list of all naval reactors designed, built, or used by the United States Navy.\n\nEach nuclear reactor design is given a three-character designation consisting of a letter representing the type of ship the reactor is intended for, a consecutive generation number, and a letter indicating the reactor's designer.\n\nShip types:\n\nContracted designers:\n\nNaval reactors of the United States Navy, listed alphabetically by ship type.\n\n\n\n\n\n\n"}
{"id": "17541500", "url": "https://en.wikipedia.org/wiki?curid=17541500", "title": "List of countries by energy intensity", "text": "List of countries by energy intensity\n\nThe following are lists of countries by energy intensity, or total energy consumption per unit GDP.\n\nThe following is a list of countries by energy intensity as published by the World Resources Institute for the year 2003. It is given in units of tonnes of oil equivalent per million constant year 2000 international dollars.\n\nThe following table displays the energy intensity in the world by koe/$05p (Kilogram oil equivalent per USD at constant exchange rate, price and purchasing power parities of the year 2005), by region and by country. The energy intensity are published by Enerdata and they are also available in the energy review for 2011.\n\nThe energy intensity is the ratio of primary energy consumption over gross domestic product measured in constant US $ at purchasing power parities.\n\nIn 2009, energy intensity in OECD countries remained stable at 0.15 koe/$05p, with 0.12 koe/$05p in both the European Union and Japan and 0.17 koe/$05p in the USA. It remained particularly high in CIS (0.35 koe/$05p) as well as in Africa (0.25 koe/$05p) and Middle East (0.26 koe/$05p). In Asia, energy intensity reached 0.22 koe/$05p. On the opposite, Latin America posted a relatively low ratio of 0.14 koe/$05p.\n\n"}
{"id": "24694990", "url": "https://en.wikipedia.org/wiki?curid=24694990", "title": "List of invasive species in Europe", "text": "List of invasive species in Europe\n\nThis is a list of invasive species in Europe. A species is regarded as invasive if it has become introduced to a location, area, or region where it did not previously occur naturally (i.e., is not a native species) and becomes capable of establishing a breeding population in the new location. An invasive species will be one that thrives in its new environment and negatively influences the ecology and biodiversity of that ecosystem.\n\nThe term invasive species refers to a subset of those species defined as introduced species. If a species has been introduced but remains local, and is not problematic to agriculture or to the local biodiversity, then it cannot be considered to be invasive, and does not belong on this list.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "37940561", "url": "https://en.wikipedia.org/wiki?curid=37940561", "title": "List of nature centers in Michigan", "text": "List of nature centers in Michigan\n\nThis is a list of nature centers and environmental education centers in the state of Michigan.\n\nTo use the sortable tables: click on the icons at the top of each column to sort that column in alphabetical order; click again for reverse alphabetical order.\n\n\n"}
{"id": "28334215", "url": "https://en.wikipedia.org/wiki?curid=28334215", "title": "List of rivers of Fiji", "text": "List of rivers of Fiji\n\nThis is a list of the rivers of Fiji. \nThey are listed by island in clockwise order, starting at the north end of each island. Tributaries are listed under the parent stream.\n\n\n\n\n\n\n"}
{"id": "14876384", "url": "https://en.wikipedia.org/wiki?curid=14876384", "title": "London Electrobus Company", "text": "London Electrobus Company\n\nThe London Electrobus Company, was a bus operator that ran a fleet of electric buses in London. The electrobus was the first practical battery-electric bus and a forerunner of the electric buses that are experiencing a major resurgence in the 21st century.\n\nThe company, which was first registered in April 1906, started running a service of electrobuses between London's Victoria Station and Liverpool Street on 15 July 1907. The clean and quiet electrobuses were popular with the travelling public. The company introduced a number of innovations and it was the first double-decker bus operator to experiment with a roof on the upper deck. At the peak of its success in late 1908 the company had 20 or so buses in operation and it started to run a second bus route from Victoria to Kilburn.\n\nHowever, the London Electrobus Company was beset by financial chicanery throughout its short existence. By 3 January 1910 the electrobus service had ceased and the company went into liquidation amid accusations of fraud. Eight of the electrobuses were sold to the Brighton, Hove and Preston United company. The rest of the London electrobuses were broken up for spares. The Brighton bus company was taken over by Thomas Tilling in 1916 and the last electrobus in Brighton ran in April 1917. Tilling said that a lack of spare parts had forced it to stop running electrobuses.\n\n\n"}
{"id": "994028", "url": "https://en.wikipedia.org/wiki?curid=994028", "title": "Mobile (sculpture)", "text": "Mobile (sculpture)\n\nA mobile (,) is a type of kinetic sculpture constructed to take advantage of the principle of equilibrium. It consists of a number of rods, from which weighted objects or further rods hang. The objects hanging from the rods balance each other, so that the rods remain more or less horizontal. Each rod hangs from only one string, which gives it freedom to rotate about the string. An ensemble of these balanced parts hang freely in space, by design without coming into contact with each other.\n\nMobiles are popular in the nursery, where they hang over cribs to give infants entertainment and visual stimulation. Mobiles have inspired many composers, including Morton Feldman and Earle Brown who were inspired by Alexander Calder's mobiles to create mobile-like indeterminate pieces. Frank Zappa also claimed that his compositions were modelled on Calder mobiles.\n\nThe meaning of the term \"mobile\" as applied to sculpture has evolved since it was first suggested by Marcel Duchamp in 1931 to describe the early, mechanized creations of Alexander Calder. At this point, \"mobile\" was synonymous with the term \"kinetic art\", describing sculptural works in which motion is a defining property. While motor or crank-driven moving sculptures may have initially prompted it, the word \"mobile\" later came to refer more specifically to Calder's free-moving creations. Influenced by the abstract work of Piet Mondrian, Joan Miró and Sophie Taeuber-Arp, Calder in many respects invented an art form where objects (typically brightly coloured, abstract shapes fashioned from sheet metal) are connected by wire much like a balance scale. By the sequential attachment of additional objects, the final creation consists of many balanced parts joined by lengths of wire whose individual elements are capable of moving independently or as a whole when prompted by air movement or direct contact. Thus, \"mobile\" has become a more well-defined term with its origin in the many such hanging constructs Calder produced in a prolific manner between the 1930s and his death in 1976.\n\nCalder's work is the only one defined by the term \"mobile\"; however, three other notable artists worked on a similar concept. Man Ray experimented with this idea around 1920, Armando Reverón who during the 30s made a series of movable skeletons and\nBruno Munari created his \"Useless Machines\" in 1933, made in cardboard and playful colors. \n\n\n"}
{"id": "843050", "url": "https://en.wikipedia.org/wiki?curid=843050", "title": "Net metering", "text": "Net metering\n\nNet metering (or net energy metering, NEM) allows consumers who generate some or all of their own electricity to use that electricity anytime, instead of when it is generated. This is particularly important with renewable energy sources like wind and solar, which are non-dispatchable (when not coupled to storage). Monthly net metering allows consumers to use solar power generated during the day at night, or wind from a windy day later in the month. Annual net metering rolls over a net kilowatt credit to the following month, allowing solar power that was generated in July to be used in December, or wind power from March in August.\n\nNet metering policies can vary significantly by country and by state or province: if net metering is available, if and how long banked credits can be retained, and how much the credits are worth (retail/wholesale). Most net metering laws involve monthly roll over of kWh credits, a small monthly connection fee, require monthly payment of deficits (i.e. normal electric bill), and annual settlement of any residual credit. Net metering uses a single, bi-directional meter and can measure current flowing in two directions.\nNet metering can be implemented solely as an accounting procedure, and requires no special metering, or even any prior arrangement or notification.\n\nNet metering is an enabling policy designed to foster private investment in renewable energy. However, according to the Washington Examiner, net metering forces utility companies to buy back energy from solar customers at \"artificially high rates.\" In turn, this expense comes at the expense of other poorer utility customers who cannot afford solar power.\n\nNet metering originated in the United States, where small wind turbines and solar panels were connected to the electrical grid, and consumers wanted to be able to use the electricity generated at a different time or date from when it was generated. Minnesota is commonly cited as passing the first net metering law, in 1983, and allowed anyone generating less than 40 kW to either roll over any kilowatt credit to the next month, or be paid for the excess. In 2000 this was amended to compensation \"at the average retail utility energy rate.\" This is the simplest and most general interpretation of net metering, and in addition allows small producers to sell electricity at the retail rate.\n\nUtilities in Idaho adopted net metering in 1980, and in Arizona in 1981. Massachusetts adopted net metering in 1982. By 1998, 22 states or utilities therein had adopted net metering. Two California utilities initially adopted a monthly \"net metering\" charge, which included a \"standby charge,\" until the PUC banned such charges. In 2005, all U.S. utilities were required to offer net metering \"upon request.\" Excess generation is not addressed. As of 43 U.S. states have adopted net metering, as well as utilities in 3 of the remaining states, leaving only 4 states without any established procedures for implementing net metering. However, a 2017 study showed that only 3% of U.S. utilities offer full retail compensation for net metering with the remainder offering less than retail rates, having credit expire annually, or some form of indefinite rollover.\n\nNet metering was slow to be adopted in Europe, especially in the United Kingdom, because of confusion over how to address the value added tax (VAT). Only one utility company in Great Britain offers net metering.\n\nThe United Kingdom government is reluctant to introduce the net metering principle because of complications in paying and refunding the value added tax that is payable on electricity, but pilot projects are underway in some areas.\n\nIn Canada, some provinces have net metering programs.\n\nIn the Philippines, Net Metering scheme is governed by Republic Act 9513 (Renewable Energy Act of 2008) and it's implementing rules and regulation (IRR). The implementing body is the Energy Regulatory Commission (ERC) in consultation with the National Renewable Energy Board (NREB). Unfortunately, the scheme is not a true net metering scheme but in reality a net billing scheme. As the Dept of Energy's Net Metering guidelines say, \"\n\n“Net-metering allows customers of Distribution Utilities (DUs) to install an on-site Renewable Energy (RE) facility not exceeding 100 kilowatts (kW) in capacity so they can generate electricity for their own use. Any electricity generated that is not consumed by the customer is automatically exported to the DU’s distribution system. The DU then gives a peso credit for the excess electricity received equivalent to the DU’s blended generation cost, excluding other generation adjustments, and deducts the credits earned to the customer’s electric bill.” \n\nThus Philippine consumers who generate their own electricity and sell their surplus to the utility are paid what is called the \"generation cost\" which is often less than 50% of the retail price of electricity.\n\nNet metering is controversial as it affects different interests on the grid. A report prepared by Peter Kind of Energy Infrastructure Advocates for the trade association Edison Electric Institute stated that distributed generation systems, like rooftop solar, present unique challenges to the future of electric utilities. Utilities in the United States have led a largely unsuccessful campaign to eliminate net metering\n\nRenewable advocates point out that while distributed solar and other energy efficiency measures do pose a challenge to electric utilities' existing business model, the benefits of distributed generation outweigh the costs, and those benefits are shared by all ratepayers. Grid benefits of private distributed solar investment include reduced need for centralizing power plants and reduced strain on the utility grid. They also point out that, as a cornerstone policy enabling the growth of rooftop solar, net metering creates a host of societal benefits for all ratepayers that are generally not accounted for by the utility analysis, including: public health benefits, employment and downstream economic effects, market price impacts, grid security benefits, and water savings.\n\nAn independent report conducted by the consulting firm Crossborder Energy found that the benefits of California's net metering program outweigh the costs to ratepayers. Those net benefits will amount to more than US$92 million annually upon the completion of the current net metering program.\n\nA 2012 report on the cost of net metering in the State of California, commissioned by the California Public Utilities Commission (CPUC), showed that those customers without distributed generation systems will pay US$287 in additional costs to use and maintain the grid every year by 2020. The report also showed the net cost will amount to US$1.1 billion by 2020. Notably, the same report found that solar customers do pay more on their power bills than what it costs the utility to serve them (Table 5, page 10: average 103% of their cost of service across the three major utilities in 2011).\n\nMany electric utilities state that owners of generation systems do not pay the full cost of service to use the grid, thus shifting their share of the cost onto customers without distributed generation systems. Most owners of rooftop solar or other types of distributed generation systems still rely on the grid to receive electricity from utilities at night or when their systems cannot generate sufficient power.\n\nA 2014 report funded by the Institute for Electric Innovation claims that net metering in California produces excessively large subsidies for typical residential rooftop solar PV facilities. These subsidies must then be paid for by other residential customers, most of whom are less affluent than the rooftop solar PV customers. In addition, the report points out that most of these large subsidies go to the solar leasing companies, which accounted for about 75 percent of the solar PV facilities installed in 2013. The report concludes that changes are needed in California, ranging from the adoption of retail tariffs that are more cost-reflective to replacing net metering with a separate \"Buy All - Sell All\" arrangement that requires all rooftop solar PV customers to buy all of their consumed energy under the existing retail tariffs and separately sell all of their onsite generation to their distribution utilities at the utilities' respective avoided costs.\n\nOn a nationwide basis, energy officials have debated replacement programs for net metering for several years. As of 2018, a few \"replicable models\" have emerged. Utility companies have always contended that customers with solar get their bills reduced by too much under net metering, and as a result, that shifts costs for keeping up the grid infrastructure to the rest of the non-solar customers. \"The policy has led to heated state-level debates since 2013 over whether — and how — to construct a successor to the policy,\" according to Utility Dive. The key challenge to constructing pricing and rebate schemes in a post-net metering environment is how to compensate rooftop solar customers fairly while not imposing costs on non-solar customers. Experts have said that a good \"successor tariff,\" as the post-net metering policies have been called, is one that supports the growth of distributed energy resources in a way where customers and the grid get benefits from it.\n\nThirteen states swapped successor tariffs for retail rate net metering programs in 2017. In 2018, three more states made similar changes. For example, compensation in Nevada will go down over time, but today the compensation is at the retail rate (meaning, solar customers who send energy to the grid get compensated at the same rate they pay for electricity). In Arizona, the new solar rate is ten percent below the retail rate.\n\nThe two most common successor tariffs are called net billing and buy-all-sell-all (BASA). \"Net billing pays the retail rate for customer-consumed PV generation and a below retail rate for exported generation. With BASA, the utility both charges and compensates at a below-retail rate.\"\n\nThere is considerable confusion between the terms \"net metering\" and \"feed-in tariff.\" In general there are three types of compensation for local, distributed generation:\n\nNet metering only requires one meter. A feed-in tariff requires two.\n\nTime of use (TOU) net metering employs a smart (electric) meter that is programmed to determine electricity usage any time during the day. Time-of-use allows utility rates and charges to be assessed based on when the electricity was used (i.e., day/night and seasonal rates). Typically the generation cost of electricity is highest during the daytime peak usage period, and lowest at night.\nTime of use metering is a significant issue for renewable-energy sources, since, for example, solar power systems tend to produce energy during the daytime peak-price period, and produce little or no power during the night period, when price is low. Italy has installed so many photovoltaic cells that peak prices no longer are during the day, but are instead in the evening. TOU net metering affects the apparent cost of net metering to a utility.\n\nIn market rate net metering systems the user's energy use is priced dynamically according to some function of wholesale electric prices. The users' meters are programmed remotely to calculate the value and are read remotely. Net metering applies such variable pricing to excess power produced by a qualifying system.\n\nMarket rate metering systems were implemented in California starting in 2006, and under the terms of California's net metering rules will be applicable to qualifying photovoltaic and wind systems. Under California law the payback for surplus electricity sent to the grid must be equal to the (variable, in this case) price charged at that time.\n\nNet metering enables small systems to result in zero annual net cost to the consumer provided that the consumer is able to shift demand loads to a lower price time, such as by chilling water at a low cost time for later use in air conditioning, or by charging a battery electric vehicle during off-peak times, while the electricity generated at peak demand time can be sent to the grid rather than used locally (see Vehicle-to-grid). No credit is given for annual surplus production.\n\nExcess generation is a separate issue from net metering, but it is normally dealt with in the same rules, because it can arise. If local generation offsets a portion of the demand, net metering is not used. If local generation exceeds demand some of the time, for example during the day, net metering is used. If local generation exceeds demand for the billing cycle, best practices calls for a perpetual roll over of the kilowatt credits, although some regions have considered having any kilowatt credits expire after 36 months. The normal definition of excess generation is annually, although the term is equally applicable monthly. The treatment of annual excess generation (and monthly) ranges from lost, to compensation at avoided cost, to compensation at retail rate. Left over kilowatt credits upon termination of service would ideally be paid at retail rate, from the consumer standpoint, and lost, from the utility standpoint, with avoided cost a minimum compromise. Some regions allow optional payment for excess annual generation, which allows perpetual roll over or payment, at the customers choice. Both wind and solar are inherently seasonal, and it is highly likely to use up a surplus later, unless more solar panels or a larger wind turbine have been installed than needed.\n\nNet metering systems can have energy storage integrated, to store some of the power locally (i.e. from the renewable energy source connected to the system) rather than selling everything back to the mains electricity grid. Often, the batteries used are industrial deep cycle batteries as these last for 10 to 20 years. Lead-acid batteries are often also still used, but last much less long (5 years or so). Lithium-ion batteries are sometimes also used, but too have a relatively short lifespan. Finally, nickel-iron batteries last the longest with a lifespan of up to 40 years. A 2017 study of solar panels with battery storage indicated an 8 to 14 percent extra consumption of electricity from charging and discharging batteries.\n\nIn some Australian states, the \"feed-in tariff\" is actually net metering, except that it pays monthly for net generation at a higher rate than retail, with Environment Victoria Campaigns Director Mark Wakeham calling it a \"fake feed-in tariff.\" A feed-in tariff requires a separate meter, and pays for all local generation at a preferential rate, while net metering requires only one meter. The financial differences are very substantial.\n\nIn Victoria, from 2009, householders were paid 60 cents for every excess kilowatt hour of energy fed back into the state electricity grid. This was around three times the retail price for electricity at that time. However, subsequent state governments reduced the feed-in in several updates, until in 2016 the feed-in is as low as 5 cents per kilowatt hour.\n\nIn Queensland starting in 2008, the Solar Bonus Scheme pays 44 cents for every excess kilowatt hour of energy fed back into the state electricity grid. This is around three times the current retail price for electricity. However, from 2012, the Queensland feed in tariff has been reduced to 6-10 cents per kilowatt hour depending on which electricity retailer the customer has signed up with.\n\nOntario allows net metering for up to 500 kW, however credits can only be carried for 12 consecutive months. Should a consumer establish a credit where they generate more than they consume for 8 months and use up the credits in the 10th month, then the 12-month period begins again from the date that the next credit is shown on an invoice. Any unused credits remaining at the end of 12 consecutive months of a consumer being in a credit situation are cleared at the end of that billing.\n\nAreas of British Columbia serviced by BC Hydro are allowed net metering for up to 50 kW. At each annual anniversary the customer was paid 8.16 cents per KWh, if there is a net export of power after each 12-month period, which was increased to 9.99 cents/kWh, effective June 1, 2012. Systems over 50 kW are covered under the Standing Offer Program. FortisBC which serves an area in South Central BC also allows net-metering for up to 50 kW. Customers are paid their existing retail rate for any net energy they produce. The City of New Westminster, which has its own electrical utility, also allows net metering.\n\nNew Brunswick allows net metering for installations up to 100 kW. Credits from excess generated power can be carried over until March at which time any excess credits are lost.\n\nSaskPower allows net metering for installations up to 100 kW. Credits from excess generated power can be carried over until the customer's annual anniversary date, at which time any excess credits are lost.\n\nIn Nova Scotia, in 2015, 43 residences and businesses began using solar panels for electricity. By 2017, the number was up to 133. These customers’ solar systems are net metered. The excess power produced by the solar panels is sold back from the homeowner to Nova Scotia Power at the same rate that the utility sells it to its customers. “The downside for Nova Scotia Power is that it must maintain the capacity to produce electricity even when it’s not sunny.”\n\nDenmark established net-metering for privately owned PV systems in mid-1998 for a pilot-period of four years. In 2002 the net-metering scheme was extended another four years up to end of 2006. Net-metering has proved to be a cheap, easy to administer and effective way of stimulating the deployment of PV in Denmark; however the relative short time window of the arrangement has so far prevented it from reaching its full potential. During the political negotiations in the fall of 2005 the net-metering for privately owned PV systems was made permanent.\n\nThe Netherlands has net-metering since 2004. Initially there was a limit of 3000kWh per year. Later this limit was increased to 5000kWh. The limit was removed altogether on January 1, 2014.\n\nItaly offers a support scheme, mixing net-metering and a well segmented premium FiT.\n\nSlovenia has annual net-metering since January 2016 for up to 11 kVA. In a calendar year up to 10 MVA can be installed in the country.\n\nIn 2010, Spain, net-metering has been proposed by the \"Asociación de la Industria Fotovoltaica\" (ASIF) to promote renewable electricity, without requiring additional economic support, but net-metering for privately owned systems is not yet established.\n\nSome form of net metering is now proposed by \"Électricité de France\". According to their website, energy produced by home-owners is bought at a higher price than what is charged as consumers. Hence, some recommend to sell all energy produced, and buy back all energy needed at a lower price. The price has been fixed for 20 years by the government.\n\nGuam's Consolidated Commission on Utilities in September 2018 approved a petition that would save customers who do not user solar energy over $3 million per year. Non-net metering customers (i.e. those who use conventional electricity), currently \"front a $3.4 million subsidy\" for only 1,700 customers using solar panels. The petition would decrease the net metering subsidy over a five year period.\n\nIndian states of Karnataka, and Andhra Pradesh have started implementation of net metering, and the policy has been announced by the respective state electricity boards in 2014. Feasibility study will be done by the electricity boards, and after inspection the meters will be replaced by bidirectional ones and will be installed. Applications are taken up for up to 30% of the distribution transformer capacity on a first-come, first-served basis and technical feasibility.\n\nSince September 2015 Maharashtra state (MERC) has released the Net Metering policy and consumers have started installation of Solar Rooftop Grid Tie Netmetering systems. Refer : http://www.mahadiscom.com/SolarRoofTopNetMetering.shtm MERC Policy allows up to 40% transformer capacity to be on Solar net metering.\n\nThe various DISCOMs in Maharashtra namely MSEDCL, Tata, Reliance and Torrent Power are expected to support Net Metering.\n\nAs of now MSEDCL does not use the TOD (Time Of The Day differential) charging tariffs for residential consumers and net metering. So Export and Import units considered at par for calculating Net Units and bill amount.\n\nNet metering was pioneered in the United States as a way to allow solar and wind to provide electricity whenever available and allow use of that electricity whenever it was needed, beginning with utilities in Idaho in 1980, and in Arizona in 1981. In 1983, Minnesota passed the first state net metering law. As of March 2015, 44 states and Washington, D.C. have developed mandatory net metering rules for at least some utilities. However, although the states rules are clear few utilities actually compensate at full retail rates.\n\nNet metering policies are determined by states, which have set policies varying on a number of key dimensions. The Energy Policy Act of 2005 required state electricity regulators to \"consider\" (but not necessarily implement) rules that mandate public electric utilities make available upon request net metering to their customers. Several legislative bills have been proposed to institute a federal standard limit on net metering. They range from H.R. 729, which sets a net metering cap at 2% of forecasted aggregate customer peak demand, to H.R. 1945 which has no aggregate cap, but does limit residential users to 10 kW, a low limit compared to many states, such as New Mexico, with an 80,000 kW limit, or states such as Arizona, Colorado, New Jersey, and Ohio which limit as a percentage of load.\n\nArizona, California, Colorado, Connecticut, Delaware, Maryland, Massachusetts, New Hampshire, New Jersey, New York, Ohio, Oregon, Pennsylvania, Utah, Vermont, and West Virginia are considered the most favorable states for net metering, as they are the only states to receive an \"A\" rating from Freeing the Grid in 2015.\n\nRegulators in multiple states are acting as \"referees\" in debates between utility companies and advocates of distributed resources, such as solar panel arrays. In 2016 the National Association of Regulatory Utility Commissioners (NARUC) published the \"Manual on Distributed Energy Resources Compensation\" as a way to help states decide on rate structures dealing with homes and businesses that generate their own power and send excess power back to the electric grid. The intention behind the manual is to \"provide a consistent framework for evaluating rate design decisions in the age of distributed energy resources.\" The president of NARUC, when he commissioned the manual, said his instructions to the committee writing the manual were to write a \"practical, expert and most importantly ideologically neutral guide that offers advice\" to states. A draft of the manual was released in July, which generated more than 70 public comments from stakeholder groups. Those comments were reviewed, and the final version of the manual was designed. The updated manual covers various issues that state regulators have been struggling with including net metering, the value of solar energy, and cost shifting from DER to non-DER customers. DER is being integrated into the national grid at a rapid pace, and the system of electricity generation, delivery, and utilization are constantly changing with new technology.\n\nThe Edison Electric Institute and the Solar Energy Industries Association both supported the manual. However, the main point of contention between utility companies and the solar industry is the question of whether distributed generation systems represent cost shift from those with the systems (people with solar panels) to those without them (everyone else who uses electricity).\n\nPhil Moeller of the Edison Electric Institute said, \"We want to DER [distributed energy resources] but we want to make sure the rate structure is right to minimize cost shifts.\" Moeller is a former member of the Federal Energy Regulatory Commission (FERC), a federal government regulatory agency. Sean Gallaher of the Solar Energy Industries Association said, \"There seems to be an assumption that revenue erosion from DER results in an inadequacy of cost recovery for the utility and therefore a shift of costs to non-participating customers. You can't just assume that.\"\n\nHome-based net metering in the United States \"had extremely low adoption rates\" as of 2017, with the leader, California, having a 0.77% adoption rate.\nNote: Some additional minor variations not listed in this table may apply. N/A = Not available. Lost = Excess electricity credit or credit not claimed is granted to utility. Retail rate = Final sale price of electricity. Avoided-cost = \"Wholesale\" price of electricity (cost to the utility).* = Depending on utility.** = Massachusetts distinguishes policies for different \"classes\" of systems.*** = Only available to customers of Austin Energy, CPS Energy, or Green Mountain Energy (Green Mountain Energy is not a utility but a retail electric provider; according to www.powertochoose.com).\n\nIn 2016, the Arkansas state legislature enacted Act 827, which directed the Arkansas Public Service Commission to review changes to the state's net metering system. In March 2017, regulators decided to \"grandfather existing solar customers into retail net metering rates for the next 20 years.\" Another decision made by the commission would allow the original net metering rates to stay with the home if it is sold. According to Utility Dive, \"There are relatively few solar customers in Arkansas, and advocates worry changes in the next part of the proceeding could slow the market's growth if regulators make too deep a cut to remuneration rates.\"\n\nIn Arkansas, early in 2017 state regulators voted to grandfather existing solar customers into the current retail net metering rate until 2037. The Arkansas Public Service Commission convened a working group to look at the costs and benefits of net metering; the working group was split along ideological lines and each of two subgroups submitted its own set of recommendations instead of a having a unified position. The first subgroup, composed of conservation and advanced energy groups, wanted no changes to the state's net metering rates. The second subgroup, made up of public utilities, wanted to have an embedded cost service approach that would determine the costs and benefits associated with that metering. Entergy was a part of the second subgroup and said \"the current net-metering policy that credits excess generation at the full retail rate must be changed for new net-metering customers.\"\n\nThat second group \"argued that crediting net-metering customers for costs that are not avoided 'means that the electric utility does not recover its entire cost of providing service to each net-metering customer, net of quantifiable benefits.\n\nAs of October 2018, net metering is up for debate again in California.\n\nIn late 2015, three utilities in California proposed alternative methods of compensation to solar users for the excess energy recycled back to the grid. This follows new regulatory decisions on solar net metering policies. In December 2015, a policy that would protect retail net metering for rooftop solar consumers was proposed by the California Public Utilities Commission (CPUC). When asked for their commentary, the utilities responded with lower remuneration rates for solar consumers in exchange for contributing to grid upkeep costs.\n\nIn an attempt to establish fairness and balance between solar installers and utilities, the CPUC proposed to continue with retail rate net metering with minimal changes for rooftop solar customers in California. According to the \"Los Angeles Times\", solar energy supporters rallied at a hearing at CPUC vowing to \"gut\" net metering and complicate newly proposed utility policies. California utilities were dissatisfied with the proposal. According to San Diego Gas & Electric, the proposal did not address the \"growing cost burden\" on their customers, which is estimated at $100 per month or, collectively, $160 million per year.\n\nA proposal set forth by the utilities offered to charge both residential and commercial net metering customers their \"otherwise applicable rate\" for the kWh's of electricity used from the grid. A fixed export compensation rate for exported energy over a 10-year period would also be imposed. The export rate would amount to $0.15/kWh for installations until the distributed power exceeded 7% of the utility's customer demand. Subsequently, the rate would fall to $0.13/kWh.\n\nThroughout 2017, California implemented \"Net Metering 2.0\" where the compensation to solar customers is close to retail rates. The policy also moved time of use (TOU) rates for residential customers. Additionally, the rates were updated to reflect \"conditions on California's grid.\"\n\nThe California Public Utilities Commission (CPUC) approved new TOU rates for San Diego Gas & Electric Company and moved the start of the peak period to 4 pm (four hours later than it was prior to the change). The Solar Energy Industries Association believes the new peak rates are too late in the day.\n\nIn October 2017, CPUC extended the grandfathering period for many distributed PV systems to the old rates and eliminated completion deadlines for \"qualifying PV systems.\" Grandfathered rates will last five years for residential customers and 10 years for other customers.\n\nIn early 2016, lawmakers in Florida, with encouragement from both houses, voted to put Amendment 4 on the ballot. In a state that is presently faced with various renewable energy issues, the idea of terminating personal property taxes on solar panels was widely supported. One of the sponsors of the bill backing Amendment 4, state Senator Jeff Brandes (R), said that the decision will aid in the development of renewable and solar energy statewide and lead to the creation of thousands of job opportunities.\n\nOn August 30, 2016, a proposition that will dispose of property taxes on both commercial and industrial solar panels was approved by Florida voters. The initiative to revitalize renewable energy efforts garnered overwhelming bipartisan support. According to \"Florida Politics\", on August 30, 2016, Amendment 4 accumulated almost 75 percent of the votes cast, surpassing the 60 percent minimum required. The ongoing debate regarding solar energy remains a major issue, with utility-supported policies expected to face scrutiny and resistance.\n\nThe Hawaii Public Utilities Commission eliminated retail net metering in 2015. When it did so, it replaced net metering with a \"Customer Grid Supply\" (CGS) and a \"Customer Self Supply\" (CSS) option. Since the elimination of net metering in 2015, Hawaii regulators have capped the number of solar customers who can send their excess energy back into the grid. Those customers are in the CGS program. Other customers in the CSS option use residential energy storage instead of sending their energy back into the grid.\n\nApproximately 1,400 people in Idaho are enrolled in net metering. Most of these customers use of rooftop solar systems. Idaho Power says that the current net metering system was not created to account for homeowners who installed their own solar panels, and as such, traditional power customers are \"being forced to make up any budget shortfalls.\"\n\nIn 2017 Idaho Power requested to create a new class of customers starting January 1, 2018. By doing so, Idaho Power could introduce possible rate hikes for that new class of customers, more than what they currently pay to access the state's power grid and buy electricity when their own solar panels are not producing electricity. The Idaho Public Utilities Commission said that it will hold a hearing in March 2018 before coming to a decision on Idaho Power's request.\n\nIn Indiana, solar energy makes up less than one percent of the state's energy consumption. In Indianapolis, for example, Indianapolis Power & Light has roughly 100 solar customers.\n\nIn February 2017, the state Senate approved by a vote of 39-9 Senate Bill 309 which would roll back net metering in Indiana. The bill \"would ultimately reduce the rate paid to net metering customers from the retail rate to the utility's marginal cost, plus 25%.\" People who already use solar power with net metering would be grandfathered for ten years at the current rate. The Sierra Club's \"Beyond Coal\" campaign ran a radio ad campaign to oppose the bill.\n\nIn April 2017, the state House passed their own version of the bill.\n\nAs of 2018, a new net metering law is \"limiting net metering benefits, but anyone with solar panels installed before January 1, 2018, was able to secure the credit for 30 years. After this date, the credit will be secure for 15 years.\"\n\nAs of January 2018, dozens of homeowner associations across central Indiana put up barriers to residential solar installations, according to the Indianapolis Star. \"With hundreds of Homeowners Associations across just Central Indiana, a review suggests that as many as half don't allow panels at all while the others have weak or limiting language that leaves architectural review committees to make decisions devoid of objective criteria.\"\n\nKansas City Power & Light and Westar Energy have proposed new demand fees before the Kansas Corporation Commission. If state regulators approve of those demand fees, new installations of solar panels across Kansas will likely decline to almost none. Rulings are expected in September and December 2018. The two utilities together provide about one-third of electricity to Kansas. Commercial and industrial customers of the utilities would not be affected by the demand fees.\n\nThe demand fee proposals would charge customers nine dollars per kilowatt during four summer months. Customers will be charged two dollars or three dollars per kilowatt during the remaining months of the year. Households with a smaller number of solar panels would likely see their rates go up under the proposal. The utility companies say that the demand fees are necessary because all customers must contribute to pay for the system that is capable of meeting spikes in demand. Customers who use a small amount of electricity because they produce some of their own via solar panels do not pay enough to cover their portion of transmission and distribution systems, according to the utility companies.\n\nIn early 2018, a bill was proposed in the Kentucky Legislature that could dramatically alter net metering within the state. House Bill 227 would reduce the credit that rooftop solar owners receive when they send electricity back to the grid by as much as 65 percent.\n\nHB 227 is moving across the legislature. Tyler White, the president of the Kentucky Coal Association, who supports the bill, has said that net metering is paramount to a renewable energy subsidy. The reason is because only some people (those without solar) pay to maintain the electric grid, while others (those with solar power systems) do not, despite the fact that everyone uses the grid. According to White, \"Germany has spent hundreds of billions of dollars on solar and wind, yet they provide only 3 percent of its total energy. The average German pays 3 times more for electricity than the average American.\" In Kentucky, there are more than 2.2 million utility customers; there are less than 1,000 private net metered customers, and half of them are located in Lexington and Louisville. White argues that a vote in favor of HB 227 is \"a vote to make sure the people of Eastern Kentucky are not paying more on their utility bills to fund the growth of private solar for the wealthy in Lexington and Louisville.\"\n\nBy March 2018, Kentucky legislators \"continue to wrestle over a plan to sharply reduce the amount utilities pay customers who sell excess solar power to their local utilities.\" The bill was approved by the House Natural Resources and Energy Committee but has not yet been debated on or voted on in the Kentucky House of Representatives. According to \"Daily Energy Insider\", \"A major question is whether, under the state’s current net metering law, the vast majority of ratepayers are subsidizing the cost of maintaining the regional grid for the relatively few customers with solar energy systems who are connected to it.\"\n\nIn Kentucky, homeowners who have rooftop solar sell their surplus electricity back to utility companies at the \"retail rate\" (the rate the utility charges the customer, not the rate the utility purchases electricity).\n\nA coalition of pro-solar groups filed a lawsuit in September 2018 against the Maine Public Utilities Commission. The coalition argued that regulators violated the law when they approved rules that would increase the costs of solar customers connecting to the electric grid.\n\nIn Maine, the two major issues regarding retail rates and net metering programs were how to deal with CMP reaching 1 percent of peak load net metering cap and the real value of solar. The Alliance for Solar Choice stated that it would prefer to see net metering kept intact until the policy produces solar growth.\n\nIn 2016, solar companies and major utility companies came to a legislative agreement over net metering issues. The two sides said that their deal might increase solar power in Maine \"tenfold in five years.\" In response, several national solar companies paid for lobbyists to travel to Maine and attempt to persuade state legislators to stop the deal. The legislation would replace net metering with a concept referred to as \"next metering.\" Under next metering, regulators would set the rates that utilities pay residential solar customers for the customers' excess energy. (Under normal net metering, utilities would pay the wholesale rate). The legislation includes a grandfather clause for existing solar customers.\n\nMaine is considering changing its net metering energy billing rules. On September 13, 2016, the Maine Public Utilities Commission proposed a new rule, and then held a public hearing on October 17. The proposal would consider changing net metering billing rules and is expected to be completed in early 2017.\n\nIn the Maine legislature, Assistant Majority Leader Sara Gideon (D-Freeport) introduced a new bill to increase Maine's solar industry tenfold and implement a market-based program, replacing the current net metering policy. In 2015-2016 in Maine, a collective group of environmentalists, consumer representatives, installers of solar power, and utilities proposed a bill in the state legislature suggesting the replacement of net metering with a market-based \"pay for production\" program. One provision of the bill proposes that Central Maine Power (CMP) and Emera, Maine's two main utilities, establish long-term contracts with utility developers and solar owners allowing them to purchase solar power generation. Subsequently, they would bid the generation into New England electricity markets. This arrangement could last existing net metering customers up to 12 years. A competitively set regulated price would be paid by the aggregators in order to compensate for owner costs. In turn, the utility aggregators would capitalize on the return on sales.\n\nIn March 2017, state legislators in both the Maine state House and Senate began writing legislation that would preserve retail net metering. The bills would make it temporary in the short term. In early 2017, the Maine Public Utilities Commission approved new limits that will eventually phase out net metering, beginning in 2018. The House bill would \"fully save retail net metering.\" A group called the Maine Environmental Priorities Coalition supports the legislation. The Senate bill would reinstate net metering while regulators examine \"how advanced metering can help better determine the costs and benefits of rooftop solar.\" The solar industry is supporting that bill. In February 2017, a group called the Natural Resources Council of Maine \"vowed to continue fighting new net metering restrictions.\"\n\nIn December 2017, the Maine Public Utilities Commission voted to delay implementing the state's new solar rules, which would phase down the net metering compensation for rooftop solar customers. Net metering supporters, led by the Conservation Law Foundation, filed a lawsuit to overturn the PUC's decision. The Maine Supreme Judicial Court is set to hear arguments in the case on December 13, 2017.\n\nThe Maine legislature tried to reverse the course set by the PUC by passing a bill to roll back the PUC's decision to phase down net metering. The bill passed the legislature but was vetoed by Governor Paul LePage.\n\nIn January 2018, the Massachusetts Department of Public Utilities (DPU) approved demand charges for Eversource utility's net metering customers. DPU also got rid of optional time-of-use rates for residential customers. Among renewable and clean energy advocates, demand charges are \"very controversial.\" DPU's decision has set the stage for intense debate over rate design. Eversource had argued it faced \"displaced distribution revenues\" of more than $8 million per year that should be collected from net-metered customers. The DPU agreed, saying \"the companies have demonstrated a cost shift from net metering to non-net metering customers by identifying costs directly imposed by net metering facilities on the distribution system.\"\n\nIn June 2018, the Michigan Public Service Commission decided to end net metering for new solar installations. Existing residential solar customers would have ten more years of net metering. Companies that install solar panels expect that the new policy will hurt their business. Michigan utility companies such as Consumers Energy and DTE Energy argue that other customers are subsidizing the customers [who have] solar. According to Michigan NPR, utility companies \"say paying solar customers the retail rate for power ignores the utility companies’ costs of maintaining the power lines and the power plants that provide the minimum baseload required to keep the power operating.\"\n\nThe Michigan Public Service Commission Is calculating a new rate to pay solar customers who send energy back to the grid; the commission is using measurements of inflow and outflow of electricity to calculate the rate. In other states that have ended net metering, the new rates range from 75 to 95% of the retail rate. The reason that the Michigan Public Service Commission is ending net metering is that in 2016, the state legislature called on the commission to come up with a study on the best way to measure and compensate for electricity from residential solar customers.\n\nThe plans have been met with resistance from solar advocates who worry that the new program will \"slow the rooftop industry to a crawl in the state.\" The Michigan Public Service Commission reported that the amount of installed solar grew from 361 MW at the end of 2015 to 580 MW in 2016. It projects that when the 2017 numbers are in, that number will grow to 1.2 GW. Currently, utilities pay the retail rate back to Michigan's solar customers for excess electricity that they generate and sell back to the grid.\n\nIn January 2018, officials in Ann Arbor, Michigan amended local zoning rules to prohibit ground-mounted solar panels in front yards, citing public safety.\n\nIn the spring of 2016, the city of Mt. Vernon, Missouri created a local net metering program. The local board of aldermen passed a measure on May 16, 2016 that allows for residents and businesses to apply to \"generate their own electricity while staying connected to the Mt. Vernon power grid.\" The board took up the issue after city residents asked about regulations regarding hooking up their own solar panels. The town's program would allow net metering, but consumers must pay for their own equipment including a bi-directional meter. Participants would pay for power from the city at the regular rate that any other city consumer would pay. Participants who create excess power would receive a credit on their utility bill, equal to what the city pays for the electricity at a wholesale rate from the distributor Empire.\n\nIn 2017, a bill was proposed in the state House (House Bill 340) that would give utility companies permission to increase fixed charges for rooftop solar customers by up to 75 percent. The bill also would allow the Missouri Public Service Commission to require solar customers to maintain a \"reasonable amount of liability insurance coverage or other equivalent respecting the installation and operation of the qualified electric energy generation unit.\"\n\nAccording to Utility Dive, the debate \"mirrors net metering issues taken up in other states.\"\n\nIn January 2017, the Energy and Technology Interim Committee (ETIC) in the Montana legislature passed HB 52, a bill which grandfathers net metering rates to solar customers. The bill passed with unanimous support. The bill was supported by NorthWestern Energy; however, a second bill did not have as much success. Bill HB 34 would have raised the net metering cap from 50 kW to 250 kW for government buildings.\n\nAs of February 2017, the net metering policy in Montana gives credits on energy bills for energy produced that flows back to the grid for customers with wind, solar, or hydropower systems. The credits given to customers are equivalent to the retail rate of electricity.\n\nThere are at least two bills in the state legislature that would change that rate. Senate Bill 7, sponsored by Sen. Pat Connell (R-Hamilton) would ban net metering customers from being subsidized other customers of the utility company. Senate Bill 78, sponsored by Sen. Keith Regier (R-Kalispell) would require the Montana Public Service Commission to create a separate rate class for net metering customers by January 1, 2018. Under the bill, power produced by net metering customers would be valued at the wholesale rate and net metering customers to pay a monthly service charge to help pay for the fixed costs of the public utility's operation.\n\nIn Nebraska, \"customer-owned renewable energy generation\" can be included under the state's net metering system. However, energy generated by the customer much reach a minimum of 25 kilowatts. The energy can be generated from a variety of renewable sources including solar, wind, and hydro.\n\nThe state of Nevada implemented net metering in 1997. Up until 2016, utility companies in Nevada paid the retail electricity rate to net metering consumers. Nevada's utilities pay net metering customers an average of $623 per year in southern Nevada and $471 per year in northern Nevada. (The major utility company in Nevada is NV Energy.)\n\nThe Nevada legislature passed legislation in 2015 that required the Nevada Public Utilities Commission to study the electric rate structure and come up with ways to shift costs. In December 2015, the commission updated the regulations so that utility companies would pay the wholesale rate to net metering consumers.\n\nThe group Greenpeace and Senator Harry Reid, the Democratic leader in the U.S. Senate, expressed opposition to the commission's ruling. On February 8, 2016, during a commission hearing, three individuals with guns attempted to enter the hearing. Security guards turned them back. The individuals said they would return to the next commission hearing and would be armed.\n\nOn December 22, 2016, the Nevada Public Utilities Commission unanimously decided to eliminate the previous rate structure that went into effect in 2015, which contributed to the collapse of Nevada's rooftop solar industry. The decision allows the state's solar market to be restored. The ruling determined that the rate would decrease from 11 cents per kilowatt-hour to 2.6 cents, while the monthly service fee increase from $12.75 to $38.51.\n\nIn June 2017, the legislature approved several bills intended to \"advance access to clean energy, including measures aimed at boosting the value of rooftop solar, while increasing the state's renewable energy goals.\" According to Utility DIVE, \"Clean energy advocates are hailing several pieces of legislation that will help turn around Nevada's image as being unfriendly to renewable energy.\"\n\nThe state Senate unanimously approved AB405. If signed into law, AB405 would increase restoring net metering rates paid by utility companies to rooftop solar companies; the rate utilities would have to pay to buy back energy would be close to the \"retail rate\" that customers pay utilities, instead of the wholesale rate that utility companies pay to get electricity.\n\nVote Solar, a lobbying group started in 2002 and funded by the solar industry, lobbied Governor Brian Sandoval to pass the legislation. (Vote Solar describes itself as an organization that \"has been working to make solar affordable and accessible to more Americans. We work at the state level all across the country to support the policies and programs needed to repower our grid with clean energy\").\n\nSunrun and SolarCity, companies that install rooftop solar panel, both left the state after the Nevada Public Utilities Commission 2015 decision. However, both companies will return after AB405 is signed into law.\n\nIn many states, such as New Hampshire, solar companies and utility companies are coming to the negotiation table with compromises over net metering rates. In New Hampshire, proposals put forth by both the solar companies and the utility companies in March 2017 mostly found a lot of common ground.\n\nBoth the utility companies and solar companies in New Hampshire filed proposals regarding a settlement over how customers will be compensated in the future over distributed solar systems. The proposals include compensation rate changes for rooftop solar owners, establish time-of-use rate pilot projects, and continue non-bypassable charges for solar customers. Solar companies proposed to cut the distribution credit by half in 2019; utility companies proposed to eliminate the credit completely.\n\nUnder the original policy, rooftop solar customers could net credits annually at the retail rate which is $.17 per kilowatt hour in New Hampshire. Customers could \"bank them\" and use them later. Utilities propose to eliminate this provision, instead crediting customers for the excess energy they generate, along with a transmission credit.\n\nIn March 2018 the New Hampshire Senate passed a bill that would let larger electric generators get compensated for the excess power they feed into the electric grid above what the owners use. Under the current law, generators of up to 1 megawatt are eligible for net metering. The Senate proposal would raise the cap to allow projects of up to 5 megawatts. \"While net metering legislation often focuses on homeowners’ rooftop solar installations, this bill is intended to provide an incentive for developers to install bigger systems, projects that could serve communities and large companies. It also would encompass some small-scale hydropower projects that already exist in the state.\"\n\nGovernor John Sununu vetoed Senate Bill 446, which would have expanded the state's net metering program. To accomplish that, the bill would increase the size limit for net metered projects from 1 MW to 5 MW. In August 2018, state senator Bob Giuda, one of the sponsors of the bill, pushed for the legislature to override the governor's veto.\n\nIn 2018, two candidates running for the Democratic nomination for governor, Molly Kelly and Steve Marchand, \"talk[ed] about an energy policy issue that rarely makes national headlines: net metering. It's a state program that lets electric ratepayers generate their own power, and put it back into the grid in exchange for lower energy costs.\" Kelly was the original architect of New Hampshire's net metering law, which some municipalities such as Nashua hoped would expand, under a bill that saw bipartisan support during the 2018 legislative session. Governor Chris Sununu vetoed that bill. However, the legislature may try to override the veto.\n\nIn August 2018, a hearing officer at the New Mexico Public Regulation Commission (PRC) recommended getting rid of a standby fee charged to eastern New Mexico solar customers. The utility company Southwestern Public Service Co. (SPS) had requested to increase the fee by more than 11%. It currently averages $28 per month in the eastern area of New Mexico. Some solar advocates say the fee hinders development of the residential solar market. Commissioners will vote on SPS's proposal in September 2018.\n\nIn June 2017, the North Carolina House of Representatives took action on a bill, HB589. First, the bill would try to create a process for competitive bidding among solar developers. Second, it would create a solar leasing program.\n\nThe bill passed by a vote of 108-11. As of June 8, the state Senate was expected to consider the bill the following week, but the bill, according to news reports, would face a harder time getting passed in the state Senate. Gov. Roy Cooper has said he supports the legislation.\n\nAccording to \"Utility DIVE\", \"The legislation proposes a competitive bidding process for independent solar developers that lawmakers say will help keep costs down by using market-driven solutions to develop renewable projects. The bill would also create a Green Source Rider Program to allows large utility customers to take control of their energy purchasing.\"\n\nThe federal Public Utility Regulatory Policies Act makes it mandatory for utility companies to purchase renewable energy from independent power developers. HB589 would allow utility companies to work with state regulators to propose a program that would \"procure renewable energy at a competitive rate.\" Utility companies would then be allowed to compete and bid against third-party energy developers. Duke Energy proposed a measure in 2016 that is similar to the bill (the North Carolina Utilities Commission rejected a similar measure in 2014).\n\nAdditionally, the bill would create a program for solar panel leasing. The intent is to create a competitive market to install renewable energy and encourage the installation of more rooftop and other solar energy projects. Utility companies will be allowed to propose changes to net metering rates after the completion of a cost and benefits analysis. Existing solar customers would be grandfathered under the original rates until January 2027.\n\nRecently, it has become a popular trend for utilities in several states to dramatically increase their consumers' fixed charges in order to account for the cost-shift created by the growing number of net metering customers who do not cover their share of grid upkeep costs, which, subsequently, places the burden onto non-solar panel customers. The average increase of fixed charges is estimated at 20 percent.\n\nIn Ohio, American Electric Power (AEP) proposed to more than double its fixed charges from $8.40 per month to $18.40 per month, affecting nearly 1.5 million customers in order to accommodate for the increase costs due to net metering.\n\nAEP was under investigation by the Public Utilities Commission of Ohio (PUCO) for allegedly double-charging their customers a total of $120 million to supposedly cover fuel costs for one of their power plants in Lawerenceburg, Indiana and for two other power plants operated by Ohio Valley Electric Corp. The double-charge would have affected 67 percent or 1 million customers who had opted out of an alternate supplier. In 2014, PUCO enlisted an outside firm to conduct an audit focusing on their consumers' charges. The findings concluded that the customers were indeed overpaying. At the same time, AEP was being reimbursed for their fuel costs twice in a period from 2013-2015.\n\nThe Office of the Ohio Consumer's Council, under the auditor's advice, requested AEP to disclose additional records, which was, in turn, denied by a PUCO administrative law judge, who agreed with AEP's stance that their confidentiality be protected until the audit process was resolved. Pablo Vegas, the president and chief operating officer of AEP Ohio, insisted at the time that rates are based on actual costs of system operations, solidifying the company's claim that no deceptive action had taken place.\n\nA ruling by PUCO in November 2017 reduced the amount of credit a customer could receive for excess electricity sold back into the grid. This reduction in credit applies to people whose systems generate enough electricity to offset all of their use, and still have some electricity left over. On January 10, 2018, PUCO held public oral arguments on the issue. Utilities and opponents are \"digging in against each other over proposed changes\", and the issue looks likely to head to the state Supreme Court. One of the more contentious sections limits the part of a customer's bill that can be offset by solar panel generation.\n\nIn a statewide poll of Republican or independents who also say they are conservative, the Ohio Conservative Energy Forum found in 2018 that 87 percent support net metering.\n\nIn South Carolina, residential solar can make up two percent of the energy each utility sells, according to a law passed in 2014. Solar developers and solar advocate special interests argue that the cap stifles further residential solar development in the state. In June 2018, South Carolina lawmakers declined to remove limits on solar in the state during reconciliation of the state's annual budget bill. Developers and renewable advocates criticized state legislators for cutting the proposal from the budget bill. Major utilities in South Carolina are expected to meet the net metering cap this year.\n\nThe state of Utah offers tax credit for customers of residential solar panels. In February 2017, there was a compromise in the state legislature to phase out the solar tax credit by the year 2021 by limiting how much of a tax credit each person can get. The bill, House Bill 23, was signed in March 2017 by Utah's governor Gary Herbert. In Utah, before the legislation takes effect, rooftop solar customers can claim $2,000 in tax credits. That amount will be reduced by $400 each year starting in 2018 until it is down to zero. Regarding the passage and signing of House Bill 23, the solar industry didn't fight it.\n\nTax credits currently cost the state of Utah $6 million per year. This is due to the growth in residential installations. Rooftop solar customers can currently get $2,000 in tax credits on their state income tax return. If the bill is passed into law, that amount will be reduced by $400 per year starting in 2018. The solar power industry doesn't like the bill; however, they did not contest it.\n\nThe largest threat to solar power in Utah is rate design and changes to net metering. However, in the last week of August 2017, Utah Governor Gary Herbert's office announced that stakeholders reached a compromise in Utah's net metering debate. The compromise allows Rocky Mountain Power's current solar customers, as well as those who submit their solar application through November 15, 2017, to continue receiving the \"retail rate\" credits (when their solar power systems generate excess electricity and send it back to the grid) until 2035. The compromise also implements a 3-year transition that gives export credits to rooftop solar customers. During that time, Rocky Mountain Power must study a \"new method of compensation after a value of solar study concludes.\" Over a dozen entities signed onto the agreement, including Rocky Mountain Power, Vivint Solar, Utah Clean Energy, the Utah Solar Energy Association, and the Utah Division of Public Utilities.\n\nThe plan would \"decrease the value of credits customers receive from the utility in exchange for excess power their panels generate.\" Rocky Mountain Power decided to grandfather existing net metering customers until the year 2035.\n\nUtah had a once robust rooftop solar market. However, as of 2018, the market has significantly declined after the state implemented changes to net metering.\n\nIn the second quarter of 2018, Rocky Mountain Power saw only 1,087 customers who installed distributed renewable energy systems, of which almost all of them were rooftop solar. This number represents a decline of more than half from the first quarter of 2018. It is also far less than the number of installations conducted in 2016 or 2017. In those years, over 10,000 net metering installations were performed.\n\nIn October 2017 solar panel installer SolarCity reached a settlement with the Vermont Department of Public Service over improperly filed contracts. The Vermont Public Utility Commission (VPUC) investigated SolarCity's business practices in September 2017. VPUC said SolarCity failed to file registrations with state regulators. The company will spend $200,000 \"to address net-metering contracts and registrations for about 134 customers\" under the settlement.\n\nVermont changed its net metering program. The new rules \"encourage community solar projects and help ratepayers, who subsidize the above market rates utilities are required to pay for power generated under the program.\"\n\nVirginia Governor Ralph Northam, in October 2018, announced the creation of the 2018 Virginia Energy Plan. The plan makes energy conservation recommendations in the areas of solar energy, wind energy, energy efficiency, energy storage, and electric vehicles. One of the goals laid out in the plan would expand net metering and community solar programs.\n\nNet purchase and sale is a different method of providing power to the electricity grid that does not offer the price symmetry of net metering, making this system a lot less profitable for home users of small renewable electricity systems.\n\nUnder this arrangement, two uni-directional meters are installed—one records electricity drawn from the grid, and the other records excess electricity generated and fed back into the grid. The user pays retail rate for the electricity they use, and the power provider purchases their excess generation at its avoided cost (wholesale rate). There may be a significant difference between the retail rate the user pays and the power provider's avoided cost.\n\nGermany, Spain, Ontario (Canada), some states in the USA, and other countries, on the other hand, have adopted a price schedule, or feed-in tariff (FIT), whereby customers get paid for any electricity they generate from renewable energy on their premises. The actual electricity being generated is counted on a separate meter, not just the surplus they feed back to the grid. In Germany, for the solar power generated, a feed-in tariff is being paid in order to boost solar power (figure from 2009). Germany once paid several times the retail rate for solar but has successfully reduced the rates drastically while actual installation of solar has grown exponentially at the same time due to installed cost reductions. Wind energy, in contrast, only receives around a half of the domestic retail rate, because the German system pays what each source costs (including a reasonable profit margin).\n\nSources that produce direct current, such as solar panels must be coupled with an electrical inverter to convert the output to alternating current, for use with conventional appliances. The phase of the outgoing power must be synchronized with the grid, and a mechanism must be included to disconnect the feed in the event of grid failure. This is for safety for example, workers repairing downed power lines must be protected from \"downstream\" sources, in addition to being disconnected from the main \"upstream\" distribution grid. Note: A small generator simply lacks the power to energize a loaded line. This can only happen if the line is isolated from other loads, and is extremely unlikely. Solar inverters are designed for safety while one inverter could not energize a line, a thousand might. In addition, all electrical workers should treat every line as though it was live, even when they know it should be safe.\n\nSolar Guerrilla (or the \"guerrilla solar movement\") is a term originated by \"Home Power Magazine\" and is applied to someone who connects solar panels without permission or notification and uses monthly net metering without regard for law.\n\n\n"}
{"id": "32087244", "url": "https://en.wikipedia.org/wiki?curid=32087244", "title": "OH/IR star", "text": "OH/IR star\n\nAn OH/IR star is an asymptotic giant branch (AGB) star that shows strong OH maser emission and is unusually bright at near-infrared wavelengths.\n\nIn the very late stages of AGB evolution, a star develops a \"super-wind\" with extreme mass loss. The gas in the stellar wind condenses as it cools away from the star, forming molecules such as water (HO) and silicon monoxide (SiO). This can form grains of dust, mostly silicates, which obscure the star at shorter wavelengths, leading to a strong infrared source. Hydroxyl (OH) radicals can be produced by photodissociation or collisional dissociation.\n\nHO and OH can both be pumped to produce maser emission. OH masers in particular can give rise to a powerful maser action at 1612 MHz and this is regarded as a defining feature of the OH/IR stars. Many other AGB stars such as Mira variables show weaker OH masers at other wavelengths, such as 1667MHz or 22MHz.\n"}
{"id": "11034547", "url": "https://en.wikipedia.org/wiki?curid=11034547", "title": "PS10 solar power plant", "text": "PS10 solar power plant\n\nThe PS10 Solar Power Plant (), is the world's first commercial concentrating solar power tower operating near Seville, in Andalusia, Spain. The 11 megawatt (MW) solar power tower produces electricity with 624 large movable mirrors called heliostats. It took four years to build and so far cost €35 million (US$46 million).\nPS10 produces about 23,400 megawatt-hours (MW·h) per year, for which it receives €271 (US$360) per MW·h under its power purchase agreement, equating to a revenue of €6.3 million per year.\n\nThe mirrors were delivered by Abengoa, the solar receiver was designed and built by , a Spanish engineering company; and the Solar Tower was designed and built by ALTAC, another Spanish engineering and construction company.\n\nEach of the mirrors has a surface measuring that concentrates the sun's rays to the top of a 115-meter (377 ft) high, 40-story tower where a solar receiver and a steam turbine are located. The turbine drives a generator, producing electricity.\n\nThe PS10 is located 20 km west of Seville (which receives at least nine hours of sunshine 320 days per year, with 15 hours per day in mid summer). The solar receiver at the top of the tower produces saturated steam at 275 °C. The energy conversion efficiency is approximately 17%.\n\nPS10 is the first of a set of solar power generation plants to be constructed in the same area that will total more than 300 MW by 2013. Power generation will be accomplished using a variety of technologies. The first two power plants to be brought into operation at Sanlúcar la Mayor are the PS10, and Sevilla PV, the largest low concentration system photovoltaic plant in Europe.\n\n300 MW:\nCompleted and is operating:\ntotal: 180 MW.\n\nThree more plants are planned:\nTotal 120 MW.\n\nPS20 and AZ20 are twin 20 MWe tower plants based on the same concept as PS10.\n\nThe PS10 solar power tower stores heat in tanks as superheated and pressurized water at 50 bar and 285 °C. The water evaporates and flashes back to steam, releasing energy and reducing the pressure. Storage is for 30 minutes. It is suggested that longer storage is possible, but that has not been proven in an existing power plant. However, there are many considerations for using molten salt as an energy storage medium due to the great capability of storing energy for long periods without substantial losses. Another possibility is to use a phase-change material as thermal storage where latent heat is used to store energy.\n\n\n"}
{"id": "1813406", "url": "https://en.wikipedia.org/wiki?curid=1813406", "title": "Pirwa", "text": "Pirwa\n\nPirwa (or Peruwa) is a Hittite deity whose nature is poorly understood. He/she is sometimes referred to as \"queen\", though archaeologists believe he/she was a male god.\n\nThe name may derive from the Hittite \"peruna\" or \"cliff\", and the Pirwa worship seems to have involved horses and horse-related symbols. The deity may also be a reflex of the Proto-Indo-European deity Perkwunos.\n\n"}
{"id": "35347926", "url": "https://en.wikipedia.org/wiki?curid=35347926", "title": "Qi (standard)", "text": "Qi (standard)\n\nQi (pronounced ; from the Chinese word qi) is an open interface standard that defines wireless power transfer using inductive charging over distances of up to 4 cm (1.6 inches), and is developed by the Wireless Power Consortium. The system uses a charging pad and a compatible device, which is placed on top of the pad, charging via resonant inductive coupling.\n\nMobile device manufacturers that are working with the standard include Apple, Asus, Google, HTC, Huawei, LG Electronics, Motorola Mobility, Nokia, Samsung, BlackBerry, Xiaomi, and Sony.\n\nFirst released in 2008, the Qi standard had by 2016 been incorporated into more than 140 smartphones, tablets and other devices.\n\nUnder the Qi specification, \"low power\" inductive transfers deliver power below 5 W using inductive coupling between two planar coils. These coils are typically 5 mm apart but can be up to 40 mm and possibly farther apart.\n\nRegulation of the output voltage is provided by a digital control loop where the power receiver communicates with the power transmitter and requests more or less power. Communication is unidirectional from the power receiver to the power transmitter via backscatter modulation. In backscatter modulation, the power-receiver coil is loaded, changing the current draw at the power transmitter. These current changes are monitored and demodulated into the information required for the two devices to work together.\nThe WPC published the Qi low-power specification in August 2009. The Qi specification can be downloaded freely after registration. In 2011, the Wireless Power Consortium began to extend the Qi specification to medium power. The low-power specification delivers up to 5 W (typically used to charge mobile devices), and the medium-power specification will deliver up to 120 W (typically used to power displays and laptops). In 2015, WPC demonstrated a high-power specification that will deliver up to 1 kW, allowing the powering of kitchen utensils among other high-power utilities.\n\nNokia first adopted Qi in its Lumia 920 phone in 2012, and the Google/LG Nexus 4 followed later that year. Toyota began offering a Qi charging cradle as a factory option on its 2013 Avalon Limited, with Ssangyong the second car manufacturer to offer a Qi option, also in 2013.\n\nIn 2015, a survey found that 76% of people surveyed in the United States, United Kingdom, and China were aware of wireless charging (an increase from 36% the previous year), and 20% were using it — however only 16% of those using it were using it daily. Furniture retailer IKEA introduced lamps and tables with integrated wireless chargers for sale in 2015, and the Lexus NX gained an optional Qi charging pad in the center console. An estimated 120 million wirelessly charging phones were sold that year, notably the Samsung Galaxy S6, which supported both Qi and the competing Power Matters Alliance standards. However, the existence of several competing wireless charging standards was still seen as a barrier to adoption.\n\nBy early 2017, Qi had displaced the competing standards, with no new products featuring Rezence. On September 12, 2017, Apple Inc. announced that their new smartphones, the iPhone 8, iPhone 8 Plus, and the iPhone X, will support the Qi standard. Apple also announced plans to expand the standard with a new protocol called AirPower which would have added the ability to charge multiple devices at once.\n\nAs the Qi standard gains popularity, it is expected that \"Qi Hotspots\" will begin to arise in places such as coffee shops, airports, sports arenas, etc. The Coffee Bean and Tea Leaf, a major US coffee chain, will install inductive charging stations at selected major metropolitan cities, as well as Virgin Atlantic Airways, for United Kingdom's London Heathrow Airport and New York City's John F. Kennedy International Airport.\nAutomobile manufacturers have been adding QI charging as standard or optional features since 2013.\n\nDevices that operate with the Qi standard rely on electromagnetic induction between planar coils. A Qi system consists of two types of devices – the Base Station, which is connected to a power source and provides inductive power, and Mobile Devices, which consume inductive power. The Base Station contains a power transmitter that comprises a transmitting coil that generates an oscillating magnetic field; the Mobile Device contains a power receiver holding a receiving coil. The magnetic field induces an alternating current in the receiving coil by Faraday's law of induction. Close spacing of the two coils, as well as shielding on their surfaces, ensure the inductive power transfer is efficient.\n\nBase Stations typically have a flat surface—referred to as the Interface Surface—on top of which a user can place one or more Mobile Devices. There are two methods for aligning the transmitting coil (part of the Base Station) and receiving coil (part of the Mobile Device) in order for a power transfer to happen. In the first concept—called guided positioning—a user must place the Mobile Device on a certain location of the Base Station's surface. For this purpose, the Mobile Device provides an alignment aid that is appropriate to its size, shape and function. The second concept—referred to as free positioning—does not require the user to place the Mobile Device in direct alignment with the transmitting coil. There are several ways to achieve free positioning. In one example a bundle of transmitting coils is used to generate a magnetic field at the location of the receiving coil only. Another example uses mechanical means to move a single transmitting coil underneath the receiving coil. A third option is to use a technique called \"Multiple Cooperative Flux Generators.\"Figure 1-1 illustrates the basic system configuration. As shown, a power transmitter includes two main functional units—a power conversion unit and a communications and control unit. The diagram shows the transmitting coil (array) generating the magnetic field as part of the power conversion unit. The control and communications unit regulates the transferred power to the level that the power receiver requests. The diagram also demonstrates that a Base Station may contain numerous transmitters, allowing for multiple Mobile Devices to be placed on the same Base Station and inductively charge until each of its batteries are fully charged. Finally, the system unit in the diagram comprises all other functionality of the Base Station, such as input power provisioning, control of multiple power transmitters, and user interfacing.\n\nA power receiver comprises a power pick-up unit, as well as a communications and control unit. Similar to the power conversion unit of the transmitter, Figure 1-1 illustrates the receiving coil as capturing the magnetic field of the power pick-up unit. A power pick-up unit typically contains a single receiving coil only. Moreover, a Mobile Device typically contains a single power receiver. The communications and control unit regulates the transferred power to the level that is appropriate for the subsystems (e.g., battery) connected to the output of the power receiver. These subsystems represent the main functionality of the Mobile Device.\n\nAs an example from the 2017 version 1.2.2 of the Qi specification (referenced above), the A2 reference Qi low-power transmitter has a coil of 20 turns (in two layers) in a flat coil, wound on a form with a 19mm inner diameter and a 40mm outer diameter, with a below-coil shield of soft iron at least 4mm larger in diameter which gives an inductance of 24 +-1 microhenries. This coil is placed in a series resonant circuit with a 200 nF capacitor to yield a resonant circuit with a natural resonance at ~140 kHz when coupled to the receiver coil.\n\nThis series resonant circuit is then driven by an H-bridge switching arrangement from the DC source; at full power, the voltage in the capacitor can reach 50 volts. Power control is automatic; the Qi specification requires that the actual voltage applied be controllable in steps at least as small as 50 millivolts.\n\nRather than down-regulate the charging voltage in the device, Qi chargers meeting the A2 reference use a PID (proportional-integral-derivative) controller to modulate the delivered power according to the primary cell voltage.\n\nOther Qi charge transmitters start their connections at 140 kHz, but can change frequencies to find a frequency with a better match, as the mutual inductance between transmitter and receiver coils will vary according to the standoff distance between transmitter and receiver coils, and thus the natural resonance frequency will vary. Different Qi reference designs have different coil arrangements, including oval coil and multi-coil systems as well as more complex resonance networks with multiple inductors and capacitors. These designs allow frequency-agile operation at frequencies from 105 to 205 kHz and with maximum resonant circuit voltages as high as 200 volts.\n\nThe Qi power receiver hardware reference design 1, also from version 1.2.2 of the Qi specification, starts with a rectangular coil of wire 44mm x 30mm outside size, with 14 turns of wire, and with an above-coil magnetic shield. This coil is wired into a parallel resonant circuit with a pair of capacitors (of 127 nanofarads and 1.6 nanofarads in series). The power output is taken across the 1.6 nanofarad capacitor.\n\nIn order to provide a digital communications channel back to the power transmitter, a resonance modulator consisting of a pair of 22 nanofarad capacitors and a 10kΩ resistor in a T configuration can be switched across the 1.6 nanofarad capacitor. Switching the T network across the 1.6 nanofarad capacitor causes a significant change in the resonant frequency of the coupled system that is detected by the power transmitter as a change in the delivered power.\n\nPower output to the portable device is via a full-wave bridge wired across the 1.6 nanofarad capacitor; the power is typically filtered with a 20 microfarad capacitor before delivery to the charge controller.\n\nOther Qi power receivers use alternate resonance modulators, including switching a resistor or pair of resistors across the receiver resonator capacitor, both before and after the bridge rectifier.\n\n\n"}
{"id": "26262", "url": "https://en.wikipedia.org/wiki?curid=26262", "title": "Redshift", "text": "Redshift\n\nIn physics, redshift happens when light or other electromagnetic radiation from an object undergoes an increase in wavelength.\n\n\"Redshifting\" does not mean that the light is actually red, or actually becomes red. The term \"red\" refers to the fact that in human terms, longer wavelengths are found at the red end of the visible spectrum. Whether or not the light is visible, a \"redshift\" means an increase in wavelength, equivalent to lower frequency and lower photon energy, in accordance with, respectively, the wave and quantum theories of light. A gamma ray perceived as an X-ray, or initially visible light perceived as radio waves would be typical examples of redshifting in astronomy. The opposite of a redshift is a \"blueshift\", where light experiences a shortening of wavelength, or increase in energy. Blueshifts are generally seen when a light-emitting object moves toward an observer or when electromagnetic radiation moves into a gravitational field. However, redshift is a more common term and sometimes blueshift is referred to as negative redshift.\n\nThere are three main causes of red and blue shifts in astronomy and cosmology: 1) Objects move apart or closer together in space. These shifts are an example of the Doppler effect, familiar in the change of apparent pitches of sirens and frequency of the sound waves emitted by speeding vehicles, or whenever a light source moves away from an observer. 2) Space itself expands, causing objects to become more separated (even if they have not changed their positions in space). This is known as the cosmological redshift. All sufficiently distant light sources (generally more than a few million light years away) show redshift corresponding to the rate of increase in their distance from Earth, known as Hubble's Law. 3) Finally, gravitational redshift is a relativistic effect observed due to strong gravitational fields, which distort spacetime and in effect, exert a force on light and other particles.\n\nKnowledge of redshifts and blueshifts has been applied to develop several terrestrial technologies such as Doppler radar and radar guns. Redshifts are also seen in the spectroscopic observations of astronomical objects. Its value is represented by the letter \"z.\"\n\nA special relativistic redshift formula (and its classical approximation) can be used to calculate the redshift of a nearby object when spacetime is flat. However, in many contexts, such as black holes and Big Bang cosmology, redshifts must be calculated using general relativity. Special relativistic, gravitational, and cosmological redshifts can be understood under the umbrella of frame transformation laws. There exist other physical processes that can lead to a shift in the frequency of electromagnetic radiation, including scattering and optical effects; however, the resulting changes are distinguishable from true redshift and are not generally referred to as such (see section on physical optics and radiative transfer).\n\nThe history of the subject began with the development in the 19th century of wave mechanics and the exploration of phenomena associated with the Doppler effect. The effect is named after Christian Doppler, who offered the first known physical explanation for the phenomenon in 1842. The hypothesis was tested and confirmed for sound waves by the Dutch scientist Christophorus Buys Ballot in 1845. Doppler correctly predicted that the phenomenon should apply to all waves, and in particular suggested that the varying colors of stars could be attributed to their motion with respect to the Earth. Before this was verified, however, it was found that stellar colors were primarily due to a star's temperature, not motion. Only later was Doppler vindicated by verified redshift observations.\n\nThe first Doppler redshift was described by French physicist Hippolyte Fizeau in 1848, who pointed to the shift in spectral lines seen in stars as being due to the Doppler effect. The effect is sometimes called the \"Doppler–Fizeau effect\". In 1868, British astronomer William Huggins was the first to determine the velocity of a star moving away from the Earth by this method. In 1871, optical redshift was confirmed when the phenomenon was observed in Fraunhofer lines using solar rotation, about 0.1 Å in the red.\nIn 1887, Vogel and Scheiner discovered the \"annual Doppler effect\", the yearly change in the Doppler shift of stars located near the ecliptic due to the orbital velocity of the Earth. In 1901, Aristarkh Belopolsky verified optical redshift in the laboratory using a system of rotating mirrors.\n\nThe earliest occurrence of the term \"red-shift\" in print (in this hyphenated form) appears to be by American astronomer Walter S. Adams in 1908, in which he mentions \"Two methods of investigating that nature of the nebular red-shift\". The word does not appear unhyphenated until about 1934 by Willem de Sitter, perhaps indicating that up to that point its German equivalent, \"Rotverschiebung\", was more commonly used.\n\nBeginning with observations in 1912, Vesto Slipher discovered that most spiral galaxies, then mostly thought to be spiral nebulae, had considerable redshifts. Slipher first reports on his measurement in the inaugural volume of the \"Lowell Observatory Bulletin\". Three years later, he wrote a review in the journal \"Popular Astronomy\". In it he states that \"the early discovery that the great Andromeda spiral had the quite exceptional velocity of –300 km(/s) showed the means then available, capable of investigating not only the spectra of the spirals but their velocities as well.\" Slipher reported the velocities for 15 spiral nebulae spread across the entire celestial sphere, all but three having observable \"positive\" (that is recessional) velocities. Subsequently, Edwin Hubble discovered an approximate relationship between the redshifts of such \"nebulae\" and the distances to them with the formulation of his eponymous Hubble's law. These observations corroborated Alexander Friedmann's 1922 work, in which he derived the Friedmann-Lemaître equations. They are today considered strong evidence for an expanding universe and the Big Bang theory.\n\nThe spectrum of light that comes from a single source (see idealized spectrum illustration top-right) can be measured. To determine the redshift, one searches for features in the spectrum such as absorption lines, emission lines, or other variations in light intensity. If found, these features can be compared with known features in the spectrum of various chemical compounds found in experiments where that compound is located on Earth. A very common atomic element in space is hydrogen. The spectrum of originally featureless light shone through hydrogen will show a signature spectrum specific to hydrogen that has features at regular intervals. If restricted to absorption lines it would look similar to the illustration (top right). If the same pattern of intervals is seen in an observed spectrum from a distant source but occurring at shifted wavelengths, it can be identified as hydrogen too. If the same spectral line is identified in both spectra—but at different wavelengths—then the redshift can be calculated using the table below. Determining the redshift of an object in this way requires a frequency- or wavelength-range. In order to calculate the redshift one has to know the wavelength of the emitted light in the rest frame of the source, in other words, the wavelength that would be measured by an observer located adjacent to and comoving with the source. Since in astronomical applications this measurement cannot be done directly, because that would require travelling to the distant star of interest, the method using spectral lines described here is used instead. Redshifts cannot be calculated by looking at unidentified features whose rest-frame frequency is unknown, or with a spectrum that is featureless or white noise (random fluctuations in a spectrum).\n\nRedshift (and blueshift) may be characterized by the relative difference between the observed and emitted wavelengths (or frequency) of an object. In astronomy, it is customary to refer to this change using a dimensionless quantity called . If represents wavelength and represents frequency (note, where is the speed of light), then is defined by the equations:\n\nAfter is measured, the distinction between redshift and blueshift is simply a matter of whether is positive or negative. See the formula section below for some basic interpretations that follow when either a redshift or blueshift is observed. For example, Doppler effect blueshifts () are associated with objects approaching (moving closer to) the observer with the light shifting to greater energies. Conversely, Doppler effect redshifts () are associated with objects receding (moving away) from the observer with the light shifting to lower energies. Likewise, gravitational blueshifts are associated with light emitted from a source residing within a weaker gravitational field as observed from within a stronger gravitational field, while gravitational redshifting implies the opposite conditions.\n\nIn general relativity one can derive several important special-case formulae for redshift in certain special spacetime geometries, as summarized in the following table. In all cases the magnitude of the shift (the value of ) is independent of the wavelength.\nIf a source of the light is moving away from an observer, then redshift () occurs; if the source moves towards the observer, then blueshift () occurs. This is true for all electromagnetic waves and is explained by the Doppler effect. Consequently, this type of redshift is called the \"Doppler redshift\". If the source moves away from the observer with velocity , which is much less than the speed of light (), the redshift is given by\n\nwhere is the speed of light. In the classical Doppler effect, the frequency of the source is not modified, but the recessional motion causes the illusion of a lower frequency.\n\nA more complete treatment of the Doppler redshift requires considering relativistic effects associated with motion of sources close to the speed of light. A complete derivation of the effect can be found in the article on the relativistic Doppler effect. In brief, objects moving close to the speed of light will experience deviations from the above formula due to the time dilation of special relativity which can be corrected for by introducing the Lorentz factor into the classical Doppler formula as follows (for motion solely in the line of sight):\n\nThis phenomenon was first observed in a 1938 experiment performed by Herbert E. Ives and G.R. Stilwell, called the Ives–Stilwell experiment.\n\nSince the Lorentz factor is dependent only on the magnitude of the velocity, this causes the redshift associated with the relativistic correction to be independent of the orientation of the source movement. In contrast, the classical part of the formula is dependent on the projection of the movement of the source into the line-of-sight which yields different results for different orientations. If is the angle between the direction of relative motion and the direction of emission in the observer's frame (zero angle is directly away from the observer), the full form for the relativistic Doppler effect becomes:\n\nand for motion solely in the line of sight (), this equation reduces to:\n\nFor the special case that the light is approaching at right angles () to the direction of relative motion in the observer's frame, the relativistic redshift is known as the transverse redshift, and a redshift:\n\nis measured, even though the object is not moving away from the observer. Even when the source is moving towards the observer, if there is a transverse component to the motion then there is some speed at which the dilation just cancels the expected blueshift and at higher speed the approaching source will be redshifted.\n\nIn the early part of the twentieth century, Slipher, Hubble and others made the first measurements of the redshifts and blueshifts of galaxies beyond the Milky Way. They initially interpreted these redshifts and blueshifts as due to random motions, but later Hubble discovered a rough correlation between the increasing redshifts and the increasing distance of galaxies. Theorists almost immediately realized that these observations could be explained by a mechanism for producing redshifts seen in certain cosmological solutions to Einstein's equations of general relativity. Hubble's law of the correlation between redshifts and distances is required by all such models that have a metric expansion of space. As a result, the wavelength of photons propagating through the expanding space is stretched, creating the cosmological redshift.\n\nThere is a distinction between a redshift in cosmological context as compared to that witnessed when nearby objects exhibit a local Doppler-effect redshift. Rather than cosmological redshifts being a consequence of the relative velocities that are subject to the laws of special relativity (and thus subject to the rule that no two locally separated objects can have relative velocities with respect to each other faster than the speed of light), the photons instead increase in wavelength and redshift because of a global feature of the spacetime metric through which they are traveling. One interpretation of this effect is the idea that space itself is expanding. Due to the expansion increasing as distances increase, the distance between two remote galaxies can increase at more than 3 m/s, but this does not imply that the galaxies move faster than the speed of light at their present location (which is forbidden by Lorentz covariance).\n\nThe observational consequences of this effect can be derived using the equations from general relativity that describe a homogeneous and isotropic universe.\n\nTo derive the redshift effect, use the geodesic equation for a light wave, which is\n\nwhere\n\nFor an observer observing the crest of a light wave at a position and time , the crest of the light wave was emitted at a time in the past and a distant position . Integrating over the path in both space and time that the light wave travels yields:\n\nIn general, the wavelength of light is not the same for the two positions and times considered due to the changing properties of the metric. When the wave was emitted, it had a wavelength . The next crest of the light wave was emitted at a time\n\nThe observer sees the next crest of the observed light wave with a wavelength to arrive at a time\n\nSince the subsequent crest is again emitted from and is observed at , the following equation can be written:\n\nThe right-hand side of the two integral equations above are identical which means\n\nUsing the following manipulation:\n\nwe find that:\n\nFor very small variations in time (over the period of one cycle of a light wave) the scale factor is essentially a constant ( today and previously). This yields\n\nwhich can be rewritten as\n\nUsing the definition of redshift provided above, the equation\n\nis obtained. In an expanding universe such as the one we inhabit, the scale factor is monotonically increasing as time passes, thus, is positive and distant galaxies appear redshifted.\n\nUsing a model of the expansion of the Universe, redshift can be related to the age of an observed object, the so-called \"cosmic time–redshift relation\". Denote a density ratio as :\n\nwith the critical density demarcating a universe that eventually crunches from one that simply expands. This density is about three hydrogen atoms per thousand liters of space. At large redshifts one finds:\n\nwhere is the present-day Hubble constant, and is the redshift.\n\nFor cosmological redshifts of additional Doppler redshifts and blueshifts due to the peculiar motions of the galaxies relative to one another cause a wide scatter from the standard Hubble Law. The resulting situation can be illustrated by the Expanding Rubber Sheet Universe, a common cosmological analogy used to describe the expansion of space. If two objects are represented by ball bearings and spacetime by a stretching rubber sheet, the Doppler effect is caused by rolling the balls across the sheet to create peculiar motion. The cosmological redshift occurs when the ball bearings are stuck to the sheet and the sheet is stretched.\n\nThe redshifts of galaxies include both a component related to recessional velocity from expansion of the Universe, and a component related to peculiar motion (Doppler shift). The redshift due to expansion of the Universe depends upon the recessional velocity in a fashion determined by the cosmological model chosen to describe the expansion of the Universe, which is very different from how Doppler redshift depends upon local velocity. Describing the cosmological expansion origin of redshift, cosmologist Edward Robert Harrison said, \"Light leaves a galaxy, which is stationary in its local region of space, and is eventually received by observers who are stationary in their own local region of space. Between the galaxy and the observer, light travels through vast regions of expanding space. As a result, all wavelengths of the light are stretched by the expansion of space. It is as simple as that...\" Steven Weinberg clarified, \"The increase of wavelength from emission to absorption of light does not depend on the rate of change of [here is the Robertson-Walker scale factor] at the times of emission or absorption, but on the increase of in the whole period from emission to absorption.\"\n\nPopular literature often uses the expression \"Doppler redshift\" instead of \"cosmological redshift\" to describe the redshift of galaxies dominated by the expansion of spacetime, but the cosmological redshift is not found using the relativistic Doppler equation which is instead characterized by special relativity; thus is impossible while, in contrast, is possible for cosmological redshifts because the space which separates the objects (for example, a quasar from the Earth) can expand faster than the speed of light. More mathematically, the viewpoint that \"distant galaxies are receding\" and the viewpoint that \"the space between galaxies is expanding\" are related by changing coordinate systems. Expressing this precisely requires working with the mathematics of the Friedmann-Robertson-Walker metric.\n\nIf the Universe were contracting instead of expanding, we would see distant galaxies blueshifted by an amount proportional to their distance instead of redshifted.\n\nIn the theory of general relativity, there is time dilation within a gravitational well. This is known as the gravitational redshift or \"Einstein Shift\". The theoretical derivation of this effect follows from the Schwarzschild solution of the Einstein equations which yields the following formula for redshift associated with a photon traveling in the gravitational field of an uncharged, nonrotating, spherically symmetric mass:\n\nwhere\n\nThis gravitational redshift result can be derived from the assumptions of special relativity and the equivalence principle; the full theory of general relativity is not required.\n\nThe effect is very small but measurable on Earth using the Mössbauer effect and was first observed in the Pound–Rebka experiment. However, it is significant near a black hole, and as an object approaches the event horizon the red shift becomes infinite. It is also the dominant cause of large angular-scale temperature fluctuations in the cosmic microwave background radiation (see Sachs-Wolfe effect).\n\nThe redshift observed in astronomy can be measured because the emission and absorption spectra for atoms are distinctive and well known, calibrated from spectroscopic experiments in laboratories on Earth. When the redshift of various absorption and emission lines from a single astronomical object is measured, is found to be remarkably constant. Although distant objects may be slightly blurred and lines broadened, it is by no more than can be explained by thermal or mechanical motion of the source. For these reasons and others, the consensus among astronomers is that the redshifts they observe are due to some combination of the three established forms of Doppler-like redshifts. Alternative hypotheses and explanations for redshift such as tired light are not generally considered plausible.\n\nSpectroscopy, as a measurement, is considerably more difficult than simple photometry, which measures the brightness of astronomical objects through certain filters. When photometric data is all that is available (for example, the Hubble Deep Field and the Hubble Ultra Deep Field), astronomers rely on a technique for measuring photometric redshifts. Due to the broad wavelength ranges in photometric filters and the necessary assumptions about the nature of the spectrum at the light-source, errors for these sorts of measurements can range up to , and are much less reliable than spectroscopic determinations. However, photometry does at least allow a qualitative characterization of a redshift. For example, if a Sun-like spectrum had a redshift of , it would be brightest in the infrared rather than at the yellow-green color associated with the peak of its blackbody spectrum, and the light intensity will be reduced in the filter by a factor of four, . Both the photon count rate and the photon energy are redshifted. (See K correction for more details on the photometric consequences of redshift.)\n\nIn nearby objects (within our Milky Way galaxy) observed redshifts are almost always related to the line-of-sight velocities associated with the objects being observed. Observations of such redshifts and blueshifts have enabled astronomers to measure velocities and parametrize the masses of the orbiting stars in spectroscopic binaries, a method first employed in 1868 by British astronomer William Huggins. Similarly, small redshifts and blueshifts detected in the spectroscopic measurements of individual stars are one way astronomers have been able to diagnose and measure the presence and characteristics of planetary systems around other stars and have even made very detailed differential measurements of redshifts during planetary transits to determine precise orbital parameters. Finely detailed measurements of redshifts are used in helioseismology to determine the precise movements of the photosphere of the Sun. Redshifts have also been used to make the first measurements of the rotation rates of planets, velocities of interstellar clouds, the rotation of galaxies, and the dynamics of accretion onto neutron stars and black holes which exhibit both Doppler and gravitational redshifts. Additionally, the temperatures of various emitting and absorbing objects can be obtained by measuring Doppler broadening – effectively redshifts and blueshifts over a single emission or absorption line. By measuring the broadening and shifts of the 21-centimeter hydrogen line in different directions, astronomers have been able to measure the recessional velocities of interstellar gas, which in turn reveals the rotation curve of our Milky Way. Similar measurements have been performed on other galaxies, such as Andromeda. As a diagnostic tool, redshift measurements are one of the most important spectroscopic measurements made in astronomy.\n\nThe most distant objects exhibit larger redshifts corresponding to the Hubble flow of the Universe. The largest observed redshift, corresponding to the greatest distance and furthest back in time, is that of the cosmic microwave background radiation; the numerical value of its redshift is about ( corresponds to present time), and it shows the state of the Universe about 13.8 billion years ago, and 379,000 years after the initial moments of the Big Bang.\n\nThe luminous point-like cores of quasars were the first \"high-redshift\" () objects discovered before the improvement of telescopes allowed for the discovery of other high-redshift galaxies.\n\nFor galaxies more distant than the Local Group and the nearby Virgo Cluster, but within a thousand megaparsecs or so, the redshift is approximately proportional to the galaxy's distance. This correlation was first observed by Edwin Hubble and has come to be known as Hubble's law. Vesto Slipher was the first to discover galactic redshifts, in about the year 1912, while Hubble correlated Slipher's measurements with distances he measured by other means to formulate his Law. In the widely accepted cosmological model based on general relativity, redshift is mainly a result of the expansion of space: this means that the farther away a galaxy is from us, the more the space has expanded in the time since the light left that galaxy, so the more the light has been stretched, the more redshifted the light is, and so the faster it appears to be moving away from us. Hubble's law follows in part from the Copernican principle. Because it is usually not known how luminous objects are, measuring the redshift is easier than more direct distance measurements, so redshift is sometimes in practice converted to a crude distance measurement using Hubble's law.\n\nGravitational interactions of galaxies with each other and clusters cause a significant scatter in the normal plot of the Hubble diagram. The peculiar velocities associated with galaxies superimpose a rough trace of the mass of virialized objects in the Universe. This effect leads to such phenomena as nearby galaxies (such as the Andromeda Galaxy) exhibiting blueshifts as we fall towards a common barycenter, and redshift maps of clusters showing a Fingers of God effect due to the scatter of peculiar velocities in a roughly spherical distribution. This added component gives cosmologists a chance to measure the masses of objects independent of the \"mass to light ratio\" (the ratio of a galaxy's mass in solar masses to its brightness in solar luminosities), an important tool for measuring dark matter.\n\nThe Hubble law's linear relationship between distance and redshift assumes that the rate of expansion of the Universe is constant. However, when the Universe was much younger, the expansion rate, and thus the Hubble \"constant\", was larger than it is today. For more distant galaxies, then, whose light has been travelling to us for much longer times, the approximation of constant expansion rate fails, and the Hubble law becomes a non-linear integral relationship and dependent on the history of the expansion rate since the emission of the light from the galaxy in question. Observations of the redshift-distance relationship can be used, then, to determine the expansion history of the Universe and thus the matter and energy content.\n\nWhile it was long believed that the expansion rate has been continuously decreasing since the Big Bang, recent observations of the redshift-distance relationship using Type Ia supernovae have suggested that in comparatively recent times the expansion rate of the Universe has begun to accelerate.\n\nCurrently, the objects with the highest known redshifts are galaxies and the objects producing gamma ray bursts. The most reliable redshifts are from spectroscopic data, and the highest confirmed spectroscopic redshift of a galaxy is that of GN-z11, with a redshift of , corresponding to 400 million years after the Big Bang. The previous record was held by \nUDFy-38135539 at a redshift of , corresponding to 600 million years after the Big Bang. Slightly less reliable are Lyman-break redshifts, the highest of which is the lensed galaxy A1689-zD1 at a redshift and the next highest being . The most distant observed gamma-ray burst with a spectroscopic redshift measurement was GRB 090423, which had a redshift of . The most distant known quasar, ULAS J1342+0928, is at . The highest known redshift radio galaxy (TN J0924-2201) is at a redshift and the highest known redshift molecular material is the detection of emission from the CO molecule from the quasar SDSS J1148+5251 at \n\n\"Extremely red objects\" (EROs) are astronomical sources of radiation that radiate energy in the red and near infrared part of the electromagnetic spectrum. These may be starburst galaxies that have a high redshift accompanied by reddening from intervening dust, or they could be highly redshifted elliptical galaxies with an older (and therefore redder) stellar population. Objects that are even redder than EROs are termed \"hyper extremely red objects\" (HEROs).\n\nThe cosmic microwave background has a redshift of , corresponding to an age of approximately 379,000 years after the Big Bang and a comoving distance of more than 46 billion light years. The yet-to-be-observed first light from the oldest Population III stars, not long after atoms first formed and the CMB ceased to be absorbed almost completely, may have redshifts in the range of . Other high-redshift events predicted by physics but not presently observable are the cosmic neutrino background from about two seconds after the Big Bang (and a redshift in excess of ) and the cosmic gravitational wave background emitted directly from inflation at a redshift in excess of .\n\nIn June 2015, astronomers reported evidence for Population III stars in the Cosmos Redshift 7 galaxy at . Such stars are likely to have existed in the very early universe (i.e., at high redshift), and may have started the production of chemical elements heavier than hydrogen that are needed for the later formation of planets and life as we know it.\n\nWith advent of automated telescopes and improvements in spectroscopes, a number of collaborations have been made to map the Universe in redshift space. By combining redshift with angular position data, a redshift survey maps the 3D distribution of matter within a field of the sky. These observations are used to measure properties of the large-scale structure of the Universe. The Great Wall, a vast supercluster of galaxies over 500 million light-years wide, provides a dramatic example of a large-scale structure that redshift surveys can detect.\n\nThe first redshift survey was the CfA Redshift Survey, started in 1977 with the initial data collection completed in 1982. More recently, the 2dF Galaxy Redshift Survey determined the large-scale structure of one section of the Universe, measuring redshifts for over 220,000 galaxies; data collection was completed in 2002, and the final data set was released 30 June 2003. The Sloan Digital Sky Survey (SDSS), is ongoing as of 2013 and aims to measure the redshifts of around 3 million objects. SDSS has recorded redshifts for galaxies as high as 0.8, and has been involved in the detection of quasars beyond . The DEEP2 Redshift Survey uses the Keck telescopes with the new \"DEIMOS\" spectrograph; a follow-up to the pilot program DEEP1, DEEP2 is designed to measure faint galaxies with redshifts 0.7 and above, and it is therefore planned to provide a high redshift complement to SDSS and 2dF.\n\nThe interactions and phenomena summarized in the subjects of radiative transfer and physical optics can result in shifts in the wavelength and frequency of electromagnetic radiation. In such cases, the shifts correspond to a physical energy transfer to matter or other photons rather than being by a transformation between reference frames. Such shifts can be from such physical phenomena as coherence effects or the scattering of electromagnetic radiation whether from charged elementary particles, from particulates, or from fluctuations of the index of refraction in a dielectric medium as occurs in the radio phenomenon of radio whistlers. While such phenomena are sometimes referred to as \"redshifts\" and \"blueshifts\", in astrophysics light-matter interactions that result in energy shifts in the radiation field are generally referred to as \"reddening\" rather than \"redshifting\" which, as a term, is normally reserved for the effects discussed above.\n\nIn many circumstances scattering causes radiation to redden because entropy results in the predominance of many low-energy photons over few high-energy ones (while conserving total energy). Except possibly under carefully controlled conditions, scattering does not produce the same relative change in wavelength across the whole spectrum; that is, any calculated is generally a function of wavelength. Furthermore, scattering from random media generally occurs at many angles, and is a function of the scattering angle. If multiple scattering occurs, or the scattering particles have relative motion, then there is generally distortion of spectral lines as well.\n\nIn interstellar astronomy, visible spectra can appear redder due to scattering processes in a phenomenon referred to as interstellar reddening – similarly Rayleigh scattering causes the atmospheric reddening of the Sun seen in the sunrise or sunset and causes the rest of the sky to have a blue color. This phenomenon is distinct from red\"shift\"ing because the spectroscopic lines are not shifted to other wavelengths in reddened objects and there is an additional dimming and distortion associated with the phenomenon due to photons being scattered in and out of the line-of-sight.\n\n\"For a list of scattering processes, see Scattering.\"\n\n\n\n"}
{"id": "28835012", "url": "https://en.wikipedia.org/wiki?curid=28835012", "title": "Reta Beebe", "text": "Reta Beebe\n\nReta F. Beebe (born October 10, 1936 in Baca County Colorado) is an American astronomer, author, and popularizer of astronomy. She is an expert on the planets Jupiter and Saturn, and the author of \"Jupiter: The Giant Planet\". She is a professor emeritus in the Astronomy Department at New Mexico State University and 2010 winner of the NASA Exceptional Public Service medal.\n\nBeebe spent many years helping to plan and manage NASA missions, including the Voyager program missions to the giant planets. Her specific research interest was the atmospheres of Jupiter, Saturn, Uranus, and Neptune. She designed experiments to the study and measure the clouds and winds of the giant planets. She worked interpreting the Galileo and Cassini data and used the Hubble Space Telescope to obtain additional atmospheric data on Jupiter and Saturn. She was a member of the Shoemaker/Levy team at the Space Telescope Science Institute in 1994 when the comet struck Jupiter. Formerly, she chaired the Committee for Planetary and Lunar Exploration (COMPLEX), which is the principal space committee of the United States National Research Council. More recently she was involved with organizing the data about the giant planets in NASA's Planetary Data System. She is in charge of the Atmospheres Discipline Node of that program. Her planetary data archiving skills have also been employed by the European Space Agency. She serves on the steering committee of the International Planetary Data Alliance.\n\n"}
{"id": "1866153", "url": "https://en.wikipedia.org/wiki?curid=1866153", "title": "Sascab", "text": "Sascab\n\nSascab is a naturally occurring mineral material described variously as \"decomposed limestone\", \"breccia\", and \"the lime gravel mixture the Maya used as mortar.\" It has been used as a building and paving material in Mesoamerica since antiquity. In the context of pottery the term may also apply to mixtures (with clay and water) of a more finely divided form of the same material (described as \"stone dust\").\n\nIt was used by the ancient Maya in place of (or as a partial replacement for) lime in some applications, without needing to be \"burned.\" \n\nAccording to travel writer Jeanine Kitchel, the American explorer of the Yucatan, Edward Herbert Thompson found (ca.1900) \"shallow quarries near Chichen Itza with worked veins of \"sascab\"...\"\n\n"}
{"id": "42987185", "url": "https://en.wikipedia.org/wiki?curid=42987185", "title": "Seilbomb", "text": "Seilbomb\n\nA Seilbombe (, plural Seilbomben, ), literally \"rope bomb\", was a secret German weapon developed during the Second World War designed to disable the electrical power grid of an invaded territory. Equipped with it, a German Messerschmitt Bf 110 fighter plane or an Arado 196 float plane would fly low at night over areas near enemy power plants or urban centers where power lines were located and would use it to cause local blackouts. This would, it was hoped, lead to civilian panic and the inability of local opposing forces to coordinate a defense. The piloting of planes so equipped was extremely dangerous, as it required the pilot to fly his plane almost directly at enemy power lines in the dark when visibility was already severely limited and within easy range of anti-aircraft fire.\n\nThe \"Seilbombe\" device was a box-like frame attached to the belly of plane with a roll of strong, detachable 70- to 80-foot long cables connected in series and attached to a set of pulleys, each cable having a small weight attached to one end. The fighter pilot would fly his craft low over sections of the electrical grid while trailing the cable beneath and behind it. When one cable wrapped around an electricity cable, it broke off, short-circuiting that cable. The \"Bordfunker\" (\"radio operator\") of the plane would then use a control box to reel out the next length of cable, and the Bf 110 would continue to the next section of the grid.\n\n\"Seilbomben\" saw little actual use by German forces. They were considered for the attack on Moscow, Operation Barbarossa, where assaulting the city's power plants appeared to some German commanders to be a more useful tool for subjugating the city than outright terror (the Russian populace was considered too obstinate for the latter to be effective). They were also considered but never implemented for the planned invasion of England, Operation Sea Lion.\n"}
{"id": "39242264", "url": "https://en.wikipedia.org/wiki?curid=39242264", "title": "Skifska gas field", "text": "Skifska gas field\n\nSkifska is a license block located in the Ukrainian zone on the continental shelf of the Black Sea. It was awarded in 2012 to a consortium consisting of Royal Dutch Shell, ExxonMobil, Petrom and Nadra. \n\nOriginal plans intended to start the exploration programme in 2015 on the block with estimated natural gas and condensates resources in the range of . Potential future production contingent to success during the exploration campaign could yield around .\n\nRoyal Dutch Shell stopped their negotiations over a production sharing agreement in January 2014. Due to the 2014 Crimean crisis the project was put on hold in March 2014.\n"}
{"id": "3417339", "url": "https://en.wikipedia.org/wiki?curid=3417339", "title": "Spartina alterniflora", "text": "Spartina alterniflora\n\nSpartina alterniflora, the smooth cordgrass, saltmarsh cordgrass, or salt-water cordgrass, is a perennial deciduous grass which is found in intertidal wetlands, especially estuarine salt marshes. It has been reclassified as \"Sporobolus alterniflorus\" after a taxonomic revision in 2014, but \"Spartina alterniflora\" is still in common usage. It grows tall and has smooth, hollow stems that bear leaves up to long and wide at their base, which are sharply tapered and bend down at their tips. Like its relative saltmeadow cordgrass \"S. patens\", it produces flowers and seeds on only one side of the stalk. The flowers are a yellowish-green, turning brown by the winter. It has rhizoidal roots, which, when broken off, can result in vegetative asexual growth. The roots are an important food resource for snow geese. It can grow in low marsh (frequently inundated by the tide) as well as high marsh (less frequently inundated), but it is usually restricted to low marsh because it is outcompeted by salt meadow cordgrass in the high marsh. It grows in a wide range of salinities, from about 5 psu to marine (32 psu), and has been described as the \"single most important marsh plant species in the estuary\" of Chesapeake Bay. It is described as intolerant of shade.\n\n\"S. alterniflora\" is noted for its capacity to act as an environmental engineer. It grows out into the water at the seaward edge of a salt marsh, and accumulates sediment and enables other habitat-engineering species, such as mussels, to settle. This accumulation of sediment and other substrate-building species gradually builds up the level of the land at the seaward edge, and other, higher-marsh species move onto the new land. As the marsh accretes, \"S. alterniflora\" moves still further out to form a new edge. \"S. alterniflora\" grows in tallest forms at the outermost edge of a given marsh, displaying shorter morphologies up onto the landward side of the \"Spartina\" belt.\n\n\"S. alterniflora\" is native to the Atlantic coast of the Americas from Newfoundland, Canada, south to northern Argentina, where it forms a dominant part of brackish coastal saltmarshes.\n\nThe caterpillars of Aaron's skipper (\"Poanes aaroni\") have only been found on this species to date.\n\n\"Spartina alterniflora\" can become an invasive plant, either by itself or by hybridizing with native species and interfering with the propagation of the pure native strain. The grass can hinder water circulation and drainage or block boating channels. Meadows of \"S. alterniflora\" can crowd out native species, reducing biodiversity and altering the environment; as a result of \"S. alterniflora\" growth, invertebrates that live in mud flats disappear as their habitat is overgrown, and in turn, food sources shrink for birds who feed on those invertebrates.\n\nOne example of an invasive \"Spartina alterniflora\" hybrid is that of \"Spartina anglica\". \"S. anglica\" is a fertile polyploid derived from the hybrid \"S.alterniflora\" × \"townsendii\" (\"S. alterniflora\" × \"S. maritima\"), first found when American \"S. alterniflora\" was introduced to southern England in about 1870 and came into contact with the local native \"S. maritima\". \"S. anglica\" has a variety of traits that allow it to outcompete native plants, including a high saline tolerance and the ability to perform photosynthesis at lower temperatures more productively than other similar plants. It can grow on a wider range of sediments than other species of \"Spartina\", and can survive inundation in salt water for longer periods of time. \"S. anglica\" has since spread throughout northwest Europe, and (following introduction for erosion control) eastern North America.\n\nThe world’s largest invasion of \"Spartina alterniflora\" is in China, where plants from multiple North American locations were intentionally planted starting in 1979 with the intention of providing shore protection and sediment capture. The invasion has spread to over 34,000 hectares in ten provinces and Hong Kong.\n\nIn Willapa Bay of Washington state, \"Spartina alterniflora\" was probably an accidental introduction during oyster transplants during the nineteenth century and may have dispersed from there to other parts of the state. At its peak of infestation in 2003, it covered approximately 3,000 solid hectares (more than 8,500 acres), spread across an area of 8,000 hectares (20,000 acres). As of 2016, the infestation had been reduced to less than 3 solid hectares (7 acres). \n\nIn California, four species of exotic \"Spartina\" (\"S. alterniflora\", \"S. densiflora\", \"S. patens\", and \"S. anglica\") have been introduced to the San Francisco Bay region. \"Spartina alterniflora\" is well established in San Francisco Bay, and has had the greatest impact of all the cordgrasses in San Francisco Bay. It was introduced in 1973 by the Army Corps of Engineers in an attempt to reclaim marshland, and was spread and replanted around the bay in further restoration projects. It demonstrated an ability to outcompete the native \"S. foliosa\", and to potentially eliminate it from San Francisco Bay.\n\n\"Spartina alterniflora\" has also been found to hybridize with \"S. foliosa\", producing offspring that may be an even greater threat than \"S. alterniflora\" by itself. The hybrid can physically modify the environment to the detriment of native species, and the hybrid populations have spread into creeks, bays, and more remote coastal locations. The hybrids produce enormous amounts of pollen, which swamp the stigmas of the native \"S. foliosa\" flowers to produce even larger numbers of hybrid offspring, leaving the affected native \"Spartina\" little chance to produce unhybridized offspring. The hybrids also produce much larger numbers of fertile seeds than the native \"Spartina\", and are producing a hybrid population that, left unchecked, can increase not only in population size but also in its rate of population growth. The hybrids may also be able to fertilize themselves, which the native \"Spartina\" cannot do, thus increasing the spread of the hybrid swarm even further. As of 2014, eradication efforts had reduced the infestation of \"S. alterniflora\" and hybrids in the San Francisco Bay Area by 96%, from 323 net hectares at its peak to 12 net hectares.\n\nSeveral means of control and eradication have been employed against \"Spartina alterniflora\" where it has become a pest. Hand pulling is ineffective because even small rhizome fragments that inevitably break off and get left in the soil are capable of sending up new shoots. Imazapyr, an herbicide, is approved for aquatic use and is used effectively in Washington and California to kill it. In Willapa Bay, leafhopper bugs (\"Prokelisia marginata\") were employed to kill the plants, which threaten the oyster industry there, but this method did not contain the invasion. Surveys by air, land, and sea are conducted in infested and threatened areas near San Francisco to determine \"Spartina\"'s spread.\n\n"}
{"id": "42131483", "url": "https://en.wikipedia.org/wiki?curid=42131483", "title": "Sutton tube", "text": "Sutton tube\n\nA Sutton tube, or reflex klystron, is a type of vacuum tube used to generate microwaves. It is a low-power device used primarily for two purposes; one is to provide a tuneable low-power frequency source for the local oscillators in receiver circuits, and the other, with minor modifications, as a switch that could turn on and off another microwave source. The second use, sometimes known as a soft Sutton tube or rhumbatron switch, was a key component in the development of microwave radar during World War II. Microwave switches of all designs, including these, are more generally known as T/R tubes or T/R cells.\n\nThe Sutton tube is named for one of its inventors, Robert Sutton, an expert in vacuum tube design. The original klystron designs had been developed in the late 1930s in the US, and Sutton was asked to develop a tuneable version. He developed the first models in late 1940 while working at the Admiralty Signals Establishment. Sutton tubes were widely used in a variety of forms during World War II and through the 1960s. Their role has since been taken over by solid state devices like the Gunn diode, which started to become available in the 1970s. \"Rhumbatron\" refers to the resonant cavity design that was part of many klystrons, referring to the rhumba because of the dance-like motion of the electrons.\n\nKlystrons share the basic concept that the microwave output is generated by progressively accelerating then slowing electrons in an open space surrounded by a resonant cavity. The easiest klystron designs to understand have two cavities.\n\nThe first cavity is connected to a source signal, and is designed to resonate at the desired frequency, filling its interior with an oscillating electric field. The cavity's dimensions are a function of the wavelength, most are flat cylinders the shape of a hockey puck of varying sizes. A hole is drilled through the middle, at the center of the \"puck\".\n\nA stream of electrons fired from an electron gun passes through the hole, and the varying field causes them to either accelerate or decelerate as they pass. Beyond the cavity the accelerated electrons catch up to the decelerated ones, causing the electrons to bunch up in the stream. This causes the stream to re-create the original signal's pattern in the density of the electrons. This area of the tube has to be fairly long to allow time for this process to complete.\n\nThe electrons then pass through a second cavity, similar to the first. As they pass, the bunches cause a varying electric field to be induced in the cavity, re-creating the original signal but at much higher current. A tap point on this cavity provides the amplified microwave output.\n\nThe introduction of the cavity magnetron caused a revolution in radar design, generating large amounts of power from a compact and easy-to-build device. However, it also required several additional developments before it could be used.\n\nAmong these was a suitable local oscillator about 45 MHz different than the transmitter signal, which fed the intermediate frequency section of the receiver circuits. The problem was that the magnetron's frequency drifted as it warmed and cooled, enough that some sort of tuneable microwave source was needed who's frequency could be adjusted to match. A second magnetron wouldn't work, they would not drift in sync.\n\nAs the receiver circuit requires only very little output power, the klystron, first introduced only two years earlier, was a natural choice. Sutton, a well-known expert in tube design, was asked if he could provide a version that could be tuned across the same range as the magnetron's drift. An initial model available in 1940 allowed tuning with some effort. While it worked, it was not suitable for an operational system. Sutton and Thompson continued working on the problem, and delivered a solution in October 1940. Thompson named it for Sutton, while Sutton referred to it as the Thompson Tube. The former stuck.\n\nTheir advance was to use a single resonator and clever physical arrangement to provide the same effect as two cavities. He did this by placing a second electrode at the far end of the tube, the \"reflector\" or \"repeller\", which caused the electrons to turn around and start flowing back toward the gun, similar to the Barkhausen–Kurz tube. By changing the voltage of the reflector relative to the gun, the speed of the electrons when they reached the cavity the second time could be adjusted, within limits. The frequency was a function of the velocity of the electrons, providing the tuning function.\n\nThis modification effectively folded the klystron in half, with most of the \"action\" at the center of the tube where the input and output from the single cavity were located. Furthermore, only the interior of the cavity was inside the tube, the outer surface was in the form of a metal shell wrapped around the tube. Larger changes to the frequency could be made by replacing the outer shell, and this also provided a convenient location for mounting.\n\nUnfortunately, the system needed two high-voltage power supplies, one for the initial acceleration in the gun, and a second between the gun and the reflector. And, due to the way it worked, the system was generally limited to milliwatts of power.\n\nOne of the advantages of using microwaves for radar is that the size of an antenna is based on the wavelength of the signal, and shorter wavelengths thus require much smaller antennas. This was vitally important for airborne radar systems. German aircraft, using longer wavelengths, required enormous antennas that slowed the aircraft between 25 and 50 km/h due to drag. Microwaves required antennas only a few centimetres long, and could easily fit within the aircraft nose.\n\nThis advantage was offset by the lack of a switching system to allow a single antenna to act as both a transmitter and receiver. This is not always a major problem; the Chain Home system made do with two sets of antennas, as did early airborne radars like the Mk. IV. In 1940 Bernard Lovell developed a solution for microwave radar by placing two sets of dipoles in front of a common parabolic dish and placing a disk of metal foil between them. However, this was not terribly successful, and the crystal diodes used as detectors frequently burned out as the signal bled through or around the disk. A solution using two spark gap tubes was also used, but was less than ideal.\n\nA better solution was suggested by Arthur H. Cooke of the Clarendon Laboratory, and production development was taken up by H.W.B. Skinner along with A.G. Ward and A.T. Starr at the Telecommunications Research Establishment. They took a Sutton tube and disconnected the electron gun and reflector, leaving just the cavity. This was filled with a dilute gas, initially helium or hydrogen, but eventually settling on a tiny amount of water vapour and argon.\n\nWhen the transmission signal was seen on the input, the gas would rapidly ionize (helped by a heater coil or radium). The free electrons in the plasma presented an almost perfect impedance source, blocking the signal from flowing to the output. As soon as the transmission stopped, the gas de-ionized and the impedance disappeared very rapidly. The tiny echoes caused by reflections from the target, arriving microseconds later, were far too small to cause the ionization, and allowed the signal to reach the output.\n\nThe usable soft Sutton tube arrived in March 1941, and was put into production as the CV43. It was first used as part of the AI Mk. VII radar, the first production microwave radar for aircraft. The system was widely used from then on, appearing in almost all airborne microwave radars, including the H2S radar.\n\nPost-war intelligence revealed that the Germans were baffled by the purpose of the soft Sutton tube. Several examples fell into their hands, notably in the \"Rotterdam Gerät\", an H2S that was captured in fairly complete form in February 1943. Interviews with German radar engineers after the war demonstrated that they couldn't understand the purpose of the unpowered tube.\n\nThe soft Sutton tube was used in a circuit known as a \"T/R switch\" (or many variations on that theme). Other spark tubes had been used for this purpose, in a design known as the \"Branch-Duplexer\". This consisted of two short lengths of waveguide about 1/4 of a wavelength, both of which turned on when the signal arrived. Because of the geometry of the layout, the two paths resulted in a reflection of the signal. Sutton tubes were used in a simpler design known as the \"shunt branching circuit\", which was T shaped with the transmitter and antenna located at either end of the horizontal portion of the T, and the receiver at the end of the vertical portion. By locating the Sutton tube at the right location along the waveguide to the receiver, the same effect as the branch-duplexer could be arranged.\n\n\n"}
{"id": "28685870", "url": "https://en.wikipedia.org/wiki?curid=28685870", "title": "The Big Dish (solar thermal)", "text": "The Big Dish (solar thermal)\n\nThe Big Dish is a parabolic dish concentrator developed by the Australian National University's Solar Thermal Group. \nThe initial prototype, SG3\n, was constructed on the Canberra campus of the Australian National University in 1994. A modified version of SG3 was exported to Ben-Gurion National Solar Energy Center at the Ben Gurion University in Israel. In 2006, a joint project led by the Solar Thermal Group with commercial partner Wizard Power, and funding from Australian's Government's Renewable Energy Development Initiative,\nbegan the design and construction of SG4\n. SG4 is located next to the SG3 dish, and was completed in 2009.\n\nWizard Power holds the patent rights for the SG4 Big Dish structure and is developing commercial installations which will see arrays of hundreds of dishes delivering tens to hundreds of megawatts of power. The first of these will be the Whyalla Solar Oasis which will use 300 Big Dishes to deliver a 40MWe solar thermal power plant. Construction is expected to commence in late 2013.\n\n\n"}
{"id": "3914698", "url": "https://en.wikipedia.org/wiki?curid=3914698", "title": "The Poor Man's James Bond", "text": "The Poor Man's James Bond\n\nThe Poor Man's James Bond is a five book series originally intended for the survivalist-minded, compiled by writer Kurt Saxon. They were marketed toward the survivalist movement of the 1970s and 1980s, and as a counterpoint to \"The Anarchist Cookbook\" which Saxon claimed contained inaccurate information. The first volume was an expansion of an earlier Saxon book, \"The Militant's Formulary\".\n\nAccording to Saxon, during the 1960s he sent brochures about \"The Militant's Formulary\" to \"several thousand\" police and fire chiefs. His stated purpose was that, as a result of several officers being killed by \"improvised weaponry directed at them by radicals\", police and fire departments could use the literature \"to recognize improvised bombs and such and their common components\". However, some of the recipients believed Saxon to be a radical, and sent letters stating this fact to the police in Saxon's hometown of Eureka, California. In response, the chief of the Eureka Police Department replied with letters assuring the others that Saxon was \"on their side\".\n\nMuch of the content of these books consists of reprints of old books now in the public domain. The first volume talks about how to blow up a car, make napalm and poisons such as ricin, nicotine, arsenic and cyanide. It also contained a full U.S. Army self-defense manual along with a gunsmithing workshop. This included diagrams of how to alter the firing mechanism for different firearms. These modifications were often to increase rate of fire. The second volume talks about improvised weapons that could be made legally using common household items. It also had a number of easily made and readily available booby traps and a number of other explosives recipes. The third volume, which was not widely available, is the least controversial as it mostly contained information on crime in modern society, how to beat the system, and significantly less information on \"anarchism\" or \"terrorism.\" A fourth volume was published, but it is very rare to find a copy and contains more of what the previous volumes contained. In 2002, a fifth volume was published in CD-ROM format. In each book the author states that nothing in the book should be remanufactured, procured or otherwise created without first consulting the government of the area.\n\n\n"}
{"id": "46757220", "url": "https://en.wikipedia.org/wiki?curid=46757220", "title": "The Voyage of the Odyssey", "text": "The Voyage of the Odyssey\n\nThe Voyage of the Odyssey was a 5-year program conducted by oceanographic research and education non-profit Ocean Alliance, which collected the first baseline data set on contaminants in the world’s oceans. It was launched from San Diego in March 2000, and ended five and a half years later in Boston, August 2005.\n\nIn a 1979 National Geographic magazine article Ocean Alliance founder and president Dr. Roger Payne predicted that toxic pollution would replace the harpoon as the next greatest threat to whales. Recognizing the stark lack of data on the subject, Roger set his organisation Ocean Alliance with the task of obtaining a global baseline data set on contaminants.\n\nAfter years of planning and fund-raising, the program was finally ready to launch in 2000. In the executive summary of the project, Roger stated that, ‘\"The Voyage of the Odyssey has proven irrefutably that ocean life is becoming polluted to unacceptable levels by metals and human-made contaminants\".’\n\nThe focus of the program was on Sperm whales, a cosmopolitan species found in every major ocean. As long-lived apex predators, Sperm whales represent a useful bioindicator of health in the marine ecosystem in a toxicological context, owing to the effects of three key processes: bioaccumulation, biomagnification and the generation effect. Sadly, these three processes also make Sperm whales, and other apex predators, at great risk from toxic pollution. As mammalian apex predators that nurse their young with milk, they are also relatively similar to us, and thus are seen as the ‘\"canaries in the coal mine\"’ regarding humanities relationship with the oceans.\n\nThe program also had a robust educational and outreach component. In every country they visited, Odyssey crew members met with government leaders, students, teachers and journalists-many of whom kept promoting ocean health after the Odyssey departed for its next research location. The program was also the focus of a major online diary & educational webseries through American broadcaster PBS produced by Genevieve & Chris Johnson.\n\nAside from collecting the first baseline data set on contaminants in the world’s oceans, the program was witness to a number of other successes, innovations and firsts. These include:\n\n"}
{"id": "49216819", "url": "https://en.wikipedia.org/wiki?curid=49216819", "title": "Xinhai Constructed Wetland", "text": "Xinhai Constructed Wetland\n\nThe Xinhai Constructed Wetland () is a constructed wetland in Banqiao District, New Taipei, Taiwan.\n\nIn the past half century, banks of Dahan Creek turned into garbage dumps due to the growing population and factories within the area which were not served by proper sewage treatment facilities. During heavy rain, it could easily obstruct the water flow of the creek. In 1992, the Taipei County Government Environmental Protection Bureau launched a project to relocate the old garbage dumps lining the creek. During the Typhoon Herb in July-August 1996, drifting garbage clogged the creek course causing floods. Afterwards in 2003, three sections of Xinhai Constructed Wetland were constructed. The wetland construction was finished in 2009.\n\nThe wetland is located along the Dahan Creek, featuring eight crops, which are paddy rice, wild rice stems, water spinach, foxnut, marsh calla, water caltrop, lotus and crested floatingheart.\n\nThe wetland is accessible within walking distance south of Xinzhuang Station of Taipei Metro.\n\n"}
