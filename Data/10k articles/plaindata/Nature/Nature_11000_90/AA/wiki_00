{"id": "18446241", "url": "https://en.wikipedia.org/wiki?curid=18446241", "title": "108 (artist)", "text": "108 (artist)\n\n108 (born 1978) is an Italian artist in the field of street art and contemporary art from Alessandria.\n\n108 has moved from working in traditional graffiti art to painting large and mysterious figures that invade public spaces. He was the first writer to use numbers instead of letters for his name. He started to work when he was a child on the streets of Alessandria, and used different names.\nHis work has appeared on the streets of Milan, Paris, London, Berlin, and New York City. His first works known by people are enigmatic “blob”-like yellow shapes. It is his firm intention to make visual chaos. His new works are labyrinths, dead trees, non figurative 3D objects and installations, but especially black and gloomy shapes, becoming one of the biggest and influential artists in graffiti abstractism. In the last years, he took part in a lot of international exhibitions: Nusign 2.4 in Paris, Urban Edge Show in Milano, Segundo and Tercer Asalto in Zaragoza and, in 2007 he was invited to join the project called Walls inside the Biennale di Venezia with JR and Daim. During March 2008 he was invited to join Nomadaz (a show curated by Pablo Aravena) in Los Angeles with Eltono, Dem, Microbo and other artists to represent Europe in the U.S.A. 108’s doomy black abstractions are engaging and challenging in equal measure. Whether it be within the confines of a small room in the abandoned monastery, where the large triangular constructions are most effective and have the effect of warping and playing with the room’s dimensions as the viewer attempts to back away and comprehend the pieces, or as a surreal floating void on a wall beside a busy road.\n\n\n"}
{"id": "2316269", "url": "https://en.wikipedia.org/wiki?curid=2316269", "title": "Antilles Current", "text": "Antilles Current\n\nThe Antilles Current is a highly variable surface ocean current of warm water that flows northeasterly past the island chain that separates the Caribbean Sea and the Atlantic Ocean. The current results from the flow of the Atlantic North Equatorial Current. This current completes the clockwise- cycle or convection (North Atlantic Gyre) that is located in the Atlantic Ocean. It runs north of Puerto Rico, Hispaniola and Cuba, but south to the Bahamas, facilitating maritime communication from across the Atlantic into these islands' northern coasts, and connecting to the Gulf Stream at the intersection of the Florida Strait. Because of its non-dominant pace and rich-nutrient waters, fishermen across the Caribbean Islands use it to fish. It moves almost parallel to the also rich-nutrient Caribbean Current which flows south of Puerto Rico and Cuba, and over Colombia and Venezuela.\n\n"}
{"id": "56530687", "url": "https://en.wikipedia.org/wiki?curid=56530687", "title": "Antti Aalto (ski jumper)", "text": "Antti Aalto (ski jumper)\n\nAntti Aalto (born 2 April 1995) is a Finnish ski jumper. He competed in two events at the 2018 Winter Olympics. His best ever result in a World Cup competition is 7th which he reached in Wisla 2018.\n\nAalto was 7th in the Qualification at the season opener in Wisla. He finished 9th together with the finnish team in the team competition. On the following day, Aalto was sitting in 5th place after the first round. He lost two positions in the final round, meaning he finished 7th, which was his best ever World Cup result.\n"}
{"id": "24521725", "url": "https://en.wikipedia.org/wiki?curid=24521725", "title": "Aquariids", "text": "Aquariids\n\nAquariids are several meteor showers whose radiant appears to lie in the constellation Aquarius:\n"}
{"id": "673", "url": "https://en.wikipedia.org/wiki?curid=673", "title": "Atomic number", "text": "Atomic number\n\nThe atomic number or proton number (symbol \"Z\") of a chemical element is the number of protons found in the nucleus of an atom. It is identical to the charge number of the nucleus. The atomic number uniquely identifies a chemical element. In an uncharged atom, the atomic number is also equal to the number of electrons.\n\nThe sum of the atomic number \"Z\" and the number of neutrons, \"N\", gives the mass number \"A\" of an atom. Since protons and neutrons have approximately the same mass (and the mass of the electrons is negligible for many purposes) and the mass defect of nucleon binding is always small compared to the nucleon mass, the atomic mass of any atom, when expressed in unified atomic mass units (making a quantity called the \"relative isotopic mass\"), is within 1% of the whole number \"A\".\n\nAtoms with the same atomic number \"Z\" but different neutron numbers \"N\", and hence different atomic masses, are known as isotopes. A little more than three-quarters of naturally occurring elements exist as a mixture of isotopes (see monoisotopic elements), and the average isotopic mass of an isotopic mixture for an element (called the relative atomic mass) in a defined environment on Earth, determines the element's standard atomic weight. Historically, it was these atomic weights of elements (in comparison to hydrogen) that were the quantities measurable by chemists in the 19th century.\n\nThe conventional symbol \"Z\" comes from the German word meaning \"number\", which, before the modern synthesis of ideas from chemistry and physics, merely denoted an element's numerical place in the periodic table, whose order is approximately, but not completely, consistent with the order of the elements by atomic weights. Only after 1915, with the suggestion and evidence that this \"Z\" number was also the nuclear charge and a physical characteristic of atoms, did the word (and its English equivalent \"atomic number\") come into common use in this context.\n\nLoosely speaking, the existence or construction of a periodic table of elements creates an ordering of the elements, and so they can be numbered in order.\n\nDmitri Mendeleev claimed that he arranged his first periodic tables (first published on March 6th, 1869) in order of atomic weight (\"Atomgewicht\"). However, in consideration of the elements' observed chemical properties, he changed the order slightly and placed tellurium (atomic weight 127.6) ahead of iodine (atomic weight 126.9). This placement is consistent with the modern practice of ordering the elements by proton number, \"Z\", but that number was not known or suspected at the time.\n\nA simple numbering based on periodic table position was never entirely satisfactory, however. Besides the case of iodine and tellurium, later several other pairs of elements (such as argon and potassium, cobalt and nickel) were known to have nearly identical or reversed atomic weights, thus requiring their placement in the periodic table to be determined by their chemical properties. However the gradual identification of more and more chemically similar lanthanide elements, whose atomic number was not obvious, led to inconsistency and uncertainty in the periodic numbering of elements at least from lutetium (element 71) onward (hafnium was not known at this time).\n\nIn 1911, Ernest Rutherford gave a model of the atom in which a central core held most of the atom's mass and a positive charge which, in units of the electron's charge, was to be approximately equal to half of the atom's atomic weight, expressed in numbers of hydrogen atoms. This central charge would thus be approximately half the atomic weight (though it was almost 25% different from the atomic number of gold , ), the single element from which Rutherford made his guess). Nevertheless, in spite of Rutherford's estimation that gold had a central charge of about 100 (but was element on the periodic table), a month after Rutherford's paper appeared, Antonius van den Broek first formally suggested that the central charge and number of electrons in an atom was \"exactly\" equal to its place in the periodic table (also known as element number, atomic number, and symbolized \"Z\"). This proved eventually to be the case.\n\nThe experimental position improved dramatically after research by Henry Moseley in 1913. Moseley, after discussions with Bohr who was at the same lab (and who had used Van den Broek's hypothesis in his Bohr model of the atom), decided to test Van den Broek's and Bohr's hypothesis directly, by seeing if spectral lines emitted from excited atoms fitted the Bohr theory's postulation that the frequency of the spectral lines be proportional to the square of \"Z\".\n\nTo do this, Moseley measured the wavelengths of the innermost photon transitions (K and L lines) produced by the elements from aluminum (\"Z\" = 13) to gold (\"Z\" = 79) used as a series of movable anodic targets inside an x-ray tube. The square root of the frequency of these photons increased from one target to the next in an arithmetic progression. This led to the conclusion (Moseley's law) that the atomic number does closely correspond (with an offset of one unit for K-lines, in Moseley's work) to the calculated electric charge of the nucleus, i.e. the element number \"Z\". Among other things, Moseley demonstrated that the lanthanide series (from lanthanum to lutetium inclusive) must have 15 members—no fewer and no more—which was far from obvious from the chemistry at that time.\n\nAfter Moseley's death in 1915, the atomic numbers of all known elements from hydrogen to uranium (\"Z\" = 92) were examined by his method. There were seven elements (with \"Z\" < 92) which were not found and therefore identified as still undiscovered, corresponding to atomic numbers 43, 61, 72, 75, 85, 87 and 91. From 1918 to 1947, all seven of these missing elements were discovered. By this time the first four transuranium elements had also been discovered, so that the periodic table was complete with no gaps as far as curium (\"Z\" = 96).\n\nIn 1915 the reason for nuclear charge being quantized in units of \"Z\", which were now recognized to be the same as the element number, was not understood. An old idea called Prout's hypothesis had postulated that the elements were all made of residues (or \"protyles\") of the lightest element hydrogen, which in the Bohr-Rutherford model had a single electron and a nuclear charge of one. However, as early as 1907 Rutherford and Thomas Royds had shown that alpha particles, which had a charge of +2, were the nuclei of helium atoms, which had a mass four times that of hydrogen, not two times. If Prout's hypothesis were true, something had to be neutralizing some of the charge of the hydrogen nuclei present in the nuclei of heavier atoms.\n\nIn 1917 Rutherford succeeded in generating hydrogen nuclei from a nuclear reaction between alpha particles and nitrogen gas, and believed he had proven Prout's law. He called the new heavy nuclear particles protons in 1920 (alternate names being proutons and protyles). It had been immediately apparent from the work of Moseley that the nuclei of heavy atoms have more than twice as much mass as would be expected from their being made of hydrogen nuclei, and thus there was required a hypothesis for the neutralization of the extra protons presumed present in all heavy nuclei. A helium nucleus was presumed to be composed of four protons plus two \"nuclear electrons\" (electrons bound inside the nucleus) to cancel two of the charges. At the other end of the periodic table, a nucleus of gold with a mass 197 times that of hydrogen, was thought to contain 118 nuclear electrons in the nucleus to give it a residual charge of + 79, consistent with its atomic number.\n\nAll consideration of nuclear electrons ended with James Chadwick's discovery of the neutron in 1932. An atom of gold now was seen as containing 118 neutrons rather than 118 nuclear electrons, and its positive charge now was realized to come entirely from a content of 79 protons. After 1932, therefore, an element's atomic number \"Z\" was also realized to be identical to the proton number of its nuclei.\n\nThe conventional symbol \"Z\" possibly comes from the German word (atomic number). However, prior to 1915, the word \"Zahl\" (simply \"number\") was used for an element's assigned number in the periodic table.\n\nEach element has a specific set of chemical properties as a consequence of the number of electrons present in the neutral atom, which is \"Z\" (the atomic number). The configuration of these electrons follows from the principles of quantum mechanics. The number of electrons in each element's electron shells, particularly the outermost valence shell, is the primary factor in determining its chemical bonding behavior. Hence, it is the atomic number alone that determines the chemical properties of an element; and it is for this reason that an element can be defined as consisting of \"any\" mixture of atoms with a given atomic number.\n\nThe quest for new elements is usually described using atomic numbers. As of 2010, all elements with atomic numbers 1 to 118 have been observed. Synthesis of new elements is accomplished by bombarding target atoms of heavy elements with ions, such that the sum of the atomic numbers of the target and ion elements equals the atomic number of the element being created. In general, the half-life becomes shorter as atomic number increases, though an \"island of stability\" may exist for undiscovered isotopes with certain numbers of protons and neutrons.\n\n"}
{"id": "43186663", "url": "https://en.wikipedia.org/wiki?curid=43186663", "title": "BugSat 1", "text": "BugSat 1\n\nBugSat 1 is an Argentinian micro-satellite launched in 2014. The satellite is built in flattened box shape, optimized for piggy-back launch. All instruments are powered by solar cells mounted on the spacecraft body. The satellite is running the Debian operating system.\n\nBugSat 1 was launched from Dombarovsky (air base) site 13, Russia, on 19 June 2014 by a Dnepr rocket. Although there has been no official release, amateur radio operators have succeeded in downloading status data from the satellite.\n\nThe satellite is intended primarily for technology verification in space, mostly of Earth observation telescope. Also, after the end of the primary mission phase, the satellite will serve the amateur radio community by providing a digipeater service.\n\n\n"}
{"id": "3745506", "url": "https://en.wikipedia.org/wiki?curid=3745506", "title": "Chikyū", "text": "Chikyū\n\nWhile the planned depth of the hole is significantly less than the Russian Kola Superdeep Borehole (which reached depth on land), the scientific results are expected to be much more interesting since the regions targeted by \"Chikyū\" include some of the most seismically-active regions of the world. Other deep holes have been drilled by the drill ship JOIDES Resolution during the Deep Sea Drilling Project and the Ocean Drilling Program.\n\nThe Japanese part of the IODP program is called , Japanese for \"Earth Discovery\". \"Chikyū\" is operated by the Centre for Deep Earth Research (CDEX), a subdivision of the Japan Agency for Marine-Earth Science and Technology (JAMSTEC). JAMSTEC also operates the DSV Shinkai, Earth Simulator supercomputer and other marine scientific research projects. CDEX is responsible for the services to support activities including on-board staffing, data management for core samples and logging; implements engineering site surveys; and conducts engineering developments. CDEX contracts with the Mantle Quest Japan Company for the navigation of the ship.\n\nThe \"Chikyū Hakken\" program is part of an international scientific collaborative effort with scientists from the United States, ECORD, a consortium consisting of several European countries and Canada, China, South Korea, Australia and New Zealand (ANZIC), and India.\n\nD/V \"Chikyū\" was built by the Mitsui Engineering & Shipbuilding and launched on January 18, 2002 in Nagasaki, Nagasaki. The ship was outfitted by the Mitsubishi Heavy Industries and delivered to JAMSTEC on July 29, 2005.\n\nThe hull of the ship is 210 meters long, 38 meters in width, 16.2 meters high, and has an approximate gross tonnage of about 57,000 tons. The ship has a draft of 9.2 meters and a maximum cruising speed of 12 knots. The amidships derrick is 121 m above sea level, and the top drive has a lifting capacity of 1,000 tons. Its complement of 150 crew are divided between 100 operators and 50 science personnel, with at sea crew changes handled by helicopter transfer.\n\nKey innovations include a GPS system and six adjustable computer controlled azimuth thrusters (3.8 meters in diameter) that enable precise positioning to maintain a stable platform during deep water drilling. The maximum drilling water depth for riser drilling is 2,500 meters and can support a drill string up to 10,000 meters long.\n\nThe helipad can serve very large helicopters transporting as many as 30 persons per landing.\n\nThe D/V \"Chikyū\" was built for deep-sea geological scientific research, which now includes not only research of earthquake-generating zones in the Earth's crust but also hydrothermal vents and subsea methane hydrate research.\n\nOn November 16, 2007 \"Chikyū\" began drilling the NanTroSEIZE transect as planned, reaching 1,400 meters at the site of a future deep subsea floor observatory. The first stage of four NanTroSEIZE Stages was completed in February 2008. The whole project was envisioned to be completed by 2012.\n\nThe ship was damaged by the 2011 Tōhoku earthquake and tsunami on March 11, 2011. The ship was moored 300 m off the coast of Hachinohe, Aomori, but was cut loose by the tsunami and collided with a pier of Hachinohe port. One of the six stabilizers was damaged and a 1.5 meter hole was made in the bottom. Local preliminary school children who were visiting the ship at the time of the earthquake spent one night on board and were rescued by Japan Self-Defense Forces helicopters next day. The ship was repaired at a dock in Shingū, Wakayama and returned to service in June 2011.\n\nAccording to the IODP, on 27 April 2012, \"Chikyū\" drilled to a depth of 7,740 meters (25,400 feet) below sea level, setting a new world record for deep-sea drilling. This record has since been surpassed by the ill-fated Deepwater Horizon mobile offshore drilling unit, operating on the Tiber prospect in the Mississippi Canyon Field, United States Gulf of Mexico, when it achieved a world record for total length for a vertical drilling string of 10,062 m (33,011 ft). The previous record was held by the U.S. vessel \"Glomar Challenger\", which in 1978 drilled to 7,049.5 meters (23,130 feet) below sea level in the Mariana Trench. On 6 September 2012 Scientific deep sea drilling vessel \"Chikyū\" set a new world record by drilling down and obtaining rock samples from deeper than 2,111 meters below the seafloor off the Shimokita Peninsula of Japan in the northwest Pacific Ocean.\nIn addition, the 27 April 2012 drilling set a record for the depth of water for drilling of 6960 m. That record still stands.\n\nThe D/V \"Chikyū\" is featured and plays a pivotal role in the 2006 film \"Nihon Chinbotsu\".\n\n\n"}
{"id": "1276680", "url": "https://en.wikipedia.org/wiki?curid=1276680", "title": "Chlorocardium rodiei", "text": "Chlorocardium rodiei\n\nChlorocardium rodiei (greenheart) is a species of flowering plant in the family Lauraceae. It is one of two species in the genus \"Chlorocardium\". It is native Guyana and Suriname in South America. Other common names include cogwood, demerara greenheart, ispingo moena, sipiri, bebeeru and bibiru.\n\nIt is an evergreen tree growing 15 to 30 m tall with a trunk diameter of 35 to 60 cm. The leaves are oppositely arranged and simple with smooth edges. The fruit is a drupe containing a single seed.\nThe cyclic bisbenzylisoquinoline alkaloid rodiasine was first isolated from this species. The wood is extremely hard and strong, so hard that it cannot be worked with standard tools. It is durable in marine conditions, so it is used to build docks and other structures, and it was an early choice for fly fishing rods. An estimated 15 to 28% of the original population has been harvested. The species' use as a commercial timber began in the late 18th century, but most of the harvesting has taken place since the introduction of chainsaws in 1967.\n\nThe \"Fram\" and the \"Endurance\", made famous in the polar expeditions of Amundsen and Shackleton, were the two strongest wooden ships ever constructed and were sheathed in greenheart to prevent them from being crushed by ice.\n\nGreenheart wood is often sought for construction projects in parts of the Caribbean, where wood ants are problematic in conventional pine wood construction. It was also used to build the dock gates in Liverpool, such as the Manchester dock gate.\n\n"}
{"id": "22510180", "url": "https://en.wikipedia.org/wiki?curid=22510180", "title": "Cuban moist forests", "text": "Cuban moist forests\n\nThe Cuban moist forests are a tropical moist broadleaf forest ecoregion that occupies on Cuba and Isla de la Juventud. The ecoregion receives more than of rainfall annually, and does not have a dry season. Soils are usually derived from quartz, limestone, or serpentinites. Cuban moist forests can be differentiated into lowland forests (sea level to ), sub-montane forests (), and montane forests ().\n\nLowland forests are found at elevations from sea level to and reach heights of . They consist of three tree stories. The upper story includes achiotillo (\"Alchornea latifolia\"), najesí (\"Carapa guianensis\") and acana (\"Manilkara valenzuelana\"); the middle story has tagua-tagua (\"Diospyros caribaea\"), \"Ocotea floribunda\", \"Oxandra laurifolia\", \"Talauma minor\", \"Terminalia\" spp. and \"Ficus\" spp.; and the lower story has a number of species of tree ferns, Myrtaceae and Melastomataceae. Epiphytes are abundant and varied, including the endemic \"Hymenodium crinitum\", \"Oleandra articulata\", \"Columnea tincta\", and \"Psychotria pendula\". Typical palms are \"Calyptronoma plumeriana\", \"Prestoea acuminata\" var. \"montana\" and \"Bactris cubensis\". \"Heliconia\" species, mosses, and liverworts are also important plants.\n\nSub-montane forests occur elevations of . Typical sub-montane forests consist of two tree stories and an understory; they reach a height of up to in height. Achiotillo (\"Alchornea latifolia\"), júcare amarillo (\"Buchenavia capitata\"), purío prieto (\"Guatteria blainii\"), \"Licaria jamaicensis\", roble macho (\"Tabebuia hypoleuca\") and \"Zanthoxylum elephantiasis\" grow in the upper story. Cuaba de la maestra (\"Amyris lineata\"), cuajaní (\"Prunus myrtifolia\"), \"Ditta myricoides\", \"Laplacea\" spp., \"Oxandra laurifolia\", \"Ocotea\" spp., \"Rapanea ferruginea\" and \"Podocarpus\" species can be found in the lower story. Tree ferns, Myrtaceae and Melastomataceae and Rubiaceae flourish in the understory.\n\nSub-montane forests growing in ultisols reach a height of and have two stories with trees such as \"Calophyllum utile\", \"Guatteria cubensis\", \"Magnolia cristalensis\", roble de hoja ancha (\"Tabebuia dubia\"), \"Zanthoxylum cubense\" and \"Bactris cubensis\".\n\nMontane forests are found at elevations of . These forests consist of two arboreal stories and reach a height of . The upper story is dominated by barril (\"Cyrilla racemiflora\"), marañon de la Maestra (\"Magnolia cubensis\"), \"Persea anomala\" and \"Laplacea angustifolia\". The lower story consists of \"Cleyera nimanimae\", \"Freziera grisebachii\", \"Haenianthus salicifolius\", \"Lyonia\" species, \"Torralbasia cuneata\" and enebro (\"Juniperus saxicola\"). Epiphytes, mosses, ferns, terrestrial orchids, and clubmosses are abundant.\n\nBirds of Cuba's moist forests include the Cuban tody (\"Todus multicolor\"), bee hummingbird (\"Mellisuga helenae\"), Cuban trogon (\"Priotelus temnurus\"), Cuban solitaire (\"Myadestes elisabeth\"), hook-billed kite (\"Chondrohierax uncinatus\"), red-legged honeycreeper (\"Cyanerpes cyaneus\"), Cuban parakeet (\"Aratinga euops\"), stygian owl (\"Asio stygius\") and Gundlach's hawk (\"Accipiter gundlachi\"). The rare Cuban solenodon (\"Solenodon cubanus\"), a small mammal, is endemic to eastern montane forests. Other notable mammals include the hutias, 4-5 species of small to medium-sized, climbing rodents related to the Guinea Pig.\n\n"}
{"id": "9426", "url": "https://en.wikipedia.org/wiki?curid=9426", "title": "Electromagnetic radiation", "text": "Electromagnetic radiation\n\nIn physics, electromagnetic radiation (EM radiation or EMR) refers to the waves (or their quanta, photons) of the electromagnetic field, propagating (radiating) through space, carrying electromagnetic radiant energy. It includes radio waves, microwaves, infrared, (visible) light, ultraviolet, X-rays, and gamma rays.\n\nClassically, electromagnetic radiation consists of electromagnetic waves, which are synchronized oscillations of electric and magnetic fields that propagate at the speed of light, which, in a vacuum, is commonly denoted \"c\". In homogeneous, isotropic media, the oscillations of the two fields are perpendicular to each other and perpendicular to the direction of energy and wave propagation, forming a transverse wave. The wavefront of electromagnetic waves emitted from a point source (such as a light bulb) is a sphere. The position of an electromagnetic wave within the electromagnetic spectrum can be characterized by either its frequency of oscillation or its wavelength. Electromagnetic waves of different frequency are called by different names since they have different sources and effects on matter. In order of increasing frequency and decreasing wavelength these are: radio waves, microwaves, infrared radiation, visible light, ultraviolet radiation, X-rays and gamma rays.\n\nElectromagnetic waves are emitted by electrically charged particles undergoing acceleration, and these waves can subsequently interact with other charged particles, exerting force on them. EM waves carry energy, momentum and angular momentum away from their source particle and can impart those quantities to matter with which they interact. Electromagnetic radiation is associated with those EM waves that are free to propagate themselves (\"radiate\") without the continuing influence of the moving charges that produced them, because they have achieved sufficient distance from those charges. Thus, EMR is sometimes referred to as the far field. In this language, the near field refers to EM fields near the charges and current that directly produced them specifically, electromagnetic induction and electrostatic induction phenomena.\n\nIn quantum mechanics, an alternate way of viewing EMR is that it consists of photons, uncharged elementary particles with zero rest mass which are the quanta of the electromagnetic force, responsible for all electromagnetic interactions. Quantum electrodynamics is the theory of how EMR interacts with matter on an atomic level. Quantum effects provide additional sources of EMR, such as the transition of electrons to lower energy levels in an atom and black-body radiation. The energy of an individual photon is quantized and is greater for photons of higher frequency. This relationship is given by Planck's equation \"E\" = \"hν\", where \"E\" is the energy per photon, \"ν\" is the frequency of the photon, and \"h\" is Planck's constant. A single gamma ray photon, for example, might carry ~100,000 times the energy of a single photon of visible light.\n\nThe effects of EMR upon chemical compounds and biological organisms depend both upon the radiation's power and its frequency. EMR of visible or lower frequencies (i.e., visible light, infrared, microwaves, and radio waves) is called \"non-ionizing radiation\", because its photons do not individually have enough energy to ionize atoms or molecules or break chemical bonds. The effects of these radiations on chemical systems and living tissue are caused primarily by heating effects from the combined energy transfer of many photons. In contrast, high frequency ultraviolet, X-rays and gamma rays are called \"ionizing radiation\", since individual photons of such high frequency have enough energy to ionize molecules or break chemical bonds. These radiations have the ability to cause chemical reactions and damage living cells beyond that resulting from simple heating, and can be a health hazard.\n\nJames Clerk Maxwell derived a wave form of the electric and magnetic equations, thus uncovering the wave-like nature of electric and magnetic fields and their symmetry. Because the speed of EM waves predicted by the wave equation coincided with the measured speed of light, Maxwell concluded that light itself is an EM wave. Maxwell’s equations were confirmed by Heinrich Hertz through experiments with radio waves.\n\nAccording to Maxwell's equations, a spatially varying electric field is always associated with a magnetic field that changes over time. Likewise, a spatially varying magnetic field is associated with specific changes over time in the electric field. In an electromagnetic wave, the changes in the electric field are always accompanied by a wave in the magnetic field in one direction, and vice versa. This relationship between the two occurs without either type field causing the other; rather, they occur together in the same way that time and space changes occur together and are interlinked in special relativity. In fact, magnetic fields can be viewed as electric field in other frame of reference(s)' and electric fields which can be viewed also as magnetic fields in other frame of reference(s), but they have equal significance as physics is same in all frame of reference(s). so the close relationship between space and time changes here is more than an analogy. Together, these fields form a propagating electromagnetic wave, which moves out into space and need never again affect the source. The distant EM field formed in this way by the acceleration of a charge carries energy with it that \"radiates\" away through space, hence the term.\n\nMaxwell's equations established that some charges and currents (\"sources\") produce a local type of electromagnetic field near them that does \"not\" have the behaviour of EMR. Currents directly produce a magnetic field, but it is of a magnetic dipole type that dies out with distance from the current. In a similar manner, moving charges pushed apart in a conductor by a changing electrical potential (such as in an antenna) produce an electric dipole type electrical field, but this also declines with distance. These fields make up the near-field near the EMR source. Neither of these behaviours are responsible for EM radiation. Instead, they cause electromagnetic field behaviour that only efficiently transfers power to a receiver very close to the source, such as the magnetic induction inside a transformer, or the feedback behaviour that happens close to the coil of a metal detector. Typically, near-fields have a powerful effect on their own sources, causing an increased “load” (decreased electrical reactance) in the source or transmitter, whenever energy is withdrawn from the EM field by a receiver. Otherwise, these fields do not “propagate” freely out into space, carrying their energy away without distance-limit, but rather oscillate, returning their energy to the transmitter if it is not received by a receiver.\n\nBy contrast, the EM far-field is composed of \"radiation\" that is free of the transmitter in the sense that (unlike the case in an electrical transformer) the transmitter requires the same power to send these changes in the fields out, whether the signal is immediately picked up or not. This distant part of the electromagnetic field \"is\" \"electromagnetic radiation\" (also called the far-field). The far-fields propagate (radiate) without allowing the transmitter to affect them. This causes them to be independent in the sense that their existence and their energy, after they have left the transmitter, is completely independent of both transmitter and receiver. Due to conservation of energy, the amount of power passing through any spherical surface drawn around the source is the same. Because such a surface has an area proportional to the square of its distance from the source, the power density of EM radiation always decreases with the inverse square of distance from the source; this is called the inverse-square law. This is in contrast to dipole parts of the EM field close to the source (the near-field), which varies in power according to an inverse cube power law, and thus does \"not\" transport a conserved amount of energy over distances, but instead fades with distance, with its energy (as noted) rapidly returning to the transmitter or absorbed by a nearby receiver (such as a transformer secondary coil).\n\nThe far-field (EMR) depends on a different mechanism for its production than the near-field, and upon different terms in Maxwell’s equations. Whereas the magnetic part of the near-field is due to currents in the source, the magnetic field in EMR is due only to the local change in the electric field. In a similar way, while the electric field in the near-field is due directly to the charges and charge-separation in the source, the electric field in EMR is due to a change in the local magnetic field. Both processes for producing electric and magnetic EMR fields have a different dependence on distance than do near-field dipole electric and magnetic fields. That is why the EMR type of EM field becomes dominant in power “far” from sources. The term “far from sources” refers to how far from the source (moving at the speed of light) any portion of the outward-moving EM field is located, by the time that source currents are changed by the varying source potential, and the source has therefore begun to generate an outwardly moving EM field of a different phase.\n\nA more compact view of EMR is that the far-field that composes EMR is generally that part of the EM field that has traveled sufficient distance from the source, that it has become completely disconnected from any feedback to the charges and currents that were originally responsible for it. Now independent of the source charges, the EM field, as it moves farther away, is dependent only upon the accelerations of the charges that produced it. It no longer has a strong connection to the direct fields of the charges, or to the velocity of the charges (currents).\n\nIn the Liénard–Wiechert potential formulation of the electric and magnetic fields due to motion of a single particle (according to Maxwell's equations), the terms associated with acceleration of the particle are those that are responsible for the part of the field that is regarded as electromagnetic radiation. By contrast, the term associated with the changing static electric field of the particle and the magnetic term that results from the particle's uniform velocity, are both associated with the electromagnetic near-field, and do not comprise EM radiation.\n\nElectrodynamics is the physics of electromagnetic radiation, and electromagnetism is the physical phenomenon associated with the theory of electrodynamics. Electric and magnetic fields obey the properties of superposition. Thus, a field due to any particular particle or time-varying electric or magnetic field contributes to the fields present in the same space due to other causes. Further, as they are vector fields, all magnetic and electric field vectors add together according to vector addition. For example, in optics two or more coherent lightwaves may interact and by constructive or destructive interference yield a resultant irradiance deviating from the sum of the component irradiances of the individual lightwaves.\n\nSince light is an oscillation it is not affected by traveling through static electric or magnetic fields in a linear medium such as a vacuum. However, in nonlinear media, such as some crystals, interactions can occur between light and static electric and magnetic fields — these interactions include the Faraday effect and the Kerr effect.\n\nIn refraction, a wave crossing from one medium to another of different density alters its speed and direction upon entering the new medium. The ratio of the refractive indices of the media determines the degree of refraction, and is summarized by Snell's law. Light of composite wavelengths (natural sunlight) disperses into a visible spectrum passing through a prism, because of the wavelength-dependent refractive index of the prism material (dispersion); that is, each component wave within the composite light is bent a different amount.\n\nEM radiation exhibits both wave properties and particle properties at the same time (see wave-particle duality). Both wave and particle characteristics have been confirmed in many experiments. Wave characteristics are more apparent when EM radiation is measured over relatively large timescales and over large distances while particle characteristics are more evident when measuring small timescales and distances. For example, when electromagnetic radiation is absorbed by matter, particle-like properties will be more obvious when the average number of photons in the cube of the relevant wavelength is much smaller than 1. It is not too difficult to experimentally observe non-uniform deposition of energy when light is absorbed, however this alone is not evidence of \"particulate\" behavior. Rather, it reflects the quantum nature of \"matter\". Demonstrating that the light itself is quantized, not merely its interaction with matter, is a more subtle affair.\n\nSome experiments display both the wave and particle natures of electromagnetic waves, such as the self-interference of a single photon. When a single photon is sent through an interferometer, it passes through both paths, interfering with itself, as waves do, yet is detected by a photomultiplier or other sensitive detector only once.\n\nA quantum theory of the interaction between electromagnetic radiation and matter such as electrons is described by the theory of quantum electrodynamics.\n\nElectromagnetic waves can be polarized, reflected, refracted, diffracted or interfere with each other.\n\n In homogeneous, isotropic media, electromagnetic radiation is a transverse wave, meaning that its oscillations are perpendicular to the direction of energy transfer and travel. The electric and magnetic parts of the field stand in a fixed ratio of strengths in order to satisfy the two Maxwell equations that specify how one is produced from the other. In dissipation less (lossless) media, these E and B fields are also in phase, with both reaching maxima and minima at the same points in space (see illustrations). A common misconception is that the E and B fields in electromagnetic radiation are out of phase because a change in one produces the other, and this would produce a phase difference between them as sinusoidal functions (as indeed happens in electromagnetic induction, and in the near-field close to antennas). However, in the far-field EM radiation which is described by the two source-free Maxwell curl operator equations, a more correct description is that a time-change in one type of field is proportional to a space-change in the other. These derivatives require that the E and B fields in EMR are in-phase (see math section below).\n\nAn important aspect of light's nature is its frequency. The frequency of a wave is its rate of oscillation and is measured in hertz, the SI unit of frequency, where one hertz is equal to one oscillation per second. Light usually has multiple frequencies that sum to form the resultant wave. Different frequencies undergo different angles of refraction, a phenomenon known as dispersion.\n\nA wave consists of successive troughs and crests, and the distance between two adjacent crests or troughs is called the wavelength. Waves of the electromagnetic spectrum vary in size, from very long radio waves the size of buildings to very short gamma rays smaller than atom nuclei. Frequency is inversely proportional to wavelength, according to the equation:\n\nwhere \"v\" is the speed of the wave (\"c\" in a vacuum, or less in other media), \"f\" is the frequency and λ is the wavelength. As waves cross boundaries between different media, their speeds change but their frequencies remain constant.\n\nElectromagnetic waves in free space must be solutions of Maxwell's electromagnetic wave equation. Two main classes of solutions are known, namely plane waves and spherical waves. The plane waves may be viewed as the limiting case of spherical waves at a very large (ideally infinite) distance from the source. Both types of waves can have a waveform which is an arbitrary time function (so long as it is sufficiently differentiable to conform to the wave equation). As with any time function, this can be decomposed by means of Fourier analysis into its frequency spectrum, or individual sinusoidal components, each of which contains a single frequency, amplitude and phase. Such a component wave is said to be \"monochromatic\". A monochromatic electromagnetic wave can be characterized by its frequency or wavelength, its peak amplitude, its phase relative to some reference phase, its direction of propagation and its polarization.\n\nInterference is the superposition of two or more waves resulting in a new wave pattern. If the fields have components in the same direction, they constructively interfere, while opposite directions cause destructive interference. An example of interference caused by EMR is electromagnetic interference (EMI) or as it is more commonly known as, radio-frequency interference (RFI). Additionally, multiple polarization signals can be combined (i.e. interfered) to form new states of polarization, which is known as parallel polarization state generation.\n\nThe energy in electromagnetic waves is sometimes called radiant energy.\n\nAn anomaly arose in the late 19th century involving a contradiction between the wave theory of light and measurements of the electromagnetic spectra that were being emitted by thermal radiators known as black bodies. Physicists struggled with this problem, which later became known as the ultraviolet catastrophe, unsuccessfully for many years. In 1900, Max Planck developed a new theory of black-body radiation that explained the observed spectrum. Planck's theory was based on the idea that black bodies emit light (and other electromagnetic radiation) only as discrete bundles or packets of energy. These packets were called quanta. Later, Albert Einstein proposed that light quanta be regarded as real particles. Later the particle of light was given the name photon, to correspond with other particles being described around this time, such as the electron and proton. A photon has an energy, \"E\", proportional to its frequency, \"f\", by\n\nwhere \"h\" is Planck's constant, formula_3 is the wavelength and \"c\" is the speed of light. This is sometimes known as the Planck–Einstein equation. In quantum theory (see first quantization) the energy of the photons is thus directly proportional to the frequency of the EMR wave.\n\nLikewise, the momentum \"p\" of a photon is also proportional to its frequency and inversely proportional to its wavelength:\n\nThe source of Einstein's proposal that light was composed of particles (or could act as particles in some circumstances) was an experimental anomaly not explained by the wave theory: the photoelectric effect, in which light striking a metal surface ejected electrons from the surface, causing an electric current to flow across an applied voltage. Experimental measurements demonstrated that the energy of individual ejected electrons was proportional to the \"frequency\", rather than the \"intensity\", of the light. Furthermore, below a certain minimum frequency, which depended on the particular metal, no current would flow regardless of the intensity. These observations appeared to contradict the wave theory, and for years physicists tried in vain to find an explanation. In 1905, Einstein explained this puzzle by resurrecting the particle theory of light to explain the observed effect. Because of the preponderance of evidence in favor of the wave theory, however, Einstein's ideas were met initially with great skepticism among established physicists. Eventually Einstein's explanation was accepted as new particle-like behavior of light was observed, such as the Compton effect.\n\nAs a photon is absorbed by an atom, it excites the atom, elevating an electron to a higher energy level (one that is on average farther from the nucleus). When an electron in an excited molecule or atom descends to a lower energy level, it emits a photon of light at a frequency corresponding to the energy difference. Since the energy levels of electrons in atoms are discrete, each element and each molecule emits and absorbs its own characteristic frequencies. Immediate photon emission is called fluorescence, a type of photoluminescence. An example is visible light emitted from fluorescent paints, in response to ultraviolet (blacklight). Many other fluorescent emissions are known in spectral bands other than visible light. Delayed emission is called phosphorescence.\n\nThe modern theory that explains the nature of light includes the notion of wave–particle duality. More generally, the theory states that everything has both a particle nature and a wave nature, and various experiments can be done to bring out one or the other. The particle nature is more easily discerned using an object with a large mass. A bold proposition by Louis de Broglie in 1924 led the scientific community to realize that electrons also exhibited wave–particle duality.\n\nTogether, wave and particle effects fully explain the emission and absorption spectra of EM radiation. The matter-composition of the medium through which the light travels determines the nature of the absorption and emission spectrum. These bands correspond to the allowed energy levels in the atoms. Dark bands in the absorption spectrum are due to the atoms in an intervening medium between source and observer. The atoms absorb certain frequencies of the light between emitter and detector/eye, then emit them in all directions. A dark band appears to the detector, due to the radiation scattered out of the beam. For instance, dark bands in the light emitted by a distant star are due to the atoms in the star's atmosphere. A similar phenomenon occurs for emission, which is seen when an emitting gas glows due to excitation of the atoms from any mechanism, including heat. As electrons descend to lower energy levels, a spectrum is emitted that represents the jumps between the energy levels of the electrons, but lines are seen because again emission happens only at particular energies after excitation. An example is the emission spectrum of nebulae. Rapidly moving electrons are most sharply accelerated when they encounter a region of force, so they are responsible for producing much of the highest frequency electromagnetic radiation observed in nature.\n\nThese phenomena can aid various chemical determinations for the composition of gases lit from behind (absorption spectra) and for glowing gases (emission spectra). Spectroscopy (for example) determines what chemical elements comprise a particular star. Spectroscopy is also used in the determination of the distance of a star, using the red shift.\n\nWhen any wire (or other conducting object such as an antenna) conducts alternating current, electromagnetic radiation is propagated at the same frequency as the current. In many such situations it is possible to identify an electrical dipole moment that arises from separation of charges due to the exciting electrical potential, and this dipole moment oscillates in time, as the charges move back and forth. This oscillation at a given frequency gives rise to changing electric and magnetic fields, which then set the electromagnetic radiation in motion.\n\nAt the quantum level, electromagnetic radiation is produced when the wavepacket of a charged particle oscillates or otherwise accelerates. Charged particles in a stationary state do not move, but a superposition of such states may result in a transition state that has an electric dipole moment that oscillates in time. This oscillating dipole moment is responsible for the phenomenon of radiative transition between quantum states of a charged particle. Such states occur (for example) in atoms when photons are radiated as the atom shifts from one stationary state to another.\n\nAs a wave, light is characterized by a velocity (the speed of light), wavelength, and frequency. As particles, light is a stream of photons. Each has an energy related to the frequency of the wave given by Planck's relation \"E = hf\", where \"E\" is the energy of the photon, \"h\" = 6.626 × 10 J·s is Planck's constant, and \"f\" is the frequency of the wave.\n\nOne rule is obeyed regardless of circumstances: EM radiation in a vacuum travels at the speed of light, \"relative to the observer\", regardless of the observer's velocity. (This observation led to Einstein's development of the theory of special relativity.)\n\nIn a medium (other than vacuum), velocity factor or refractive index are considered, depending on frequency and application. Both of these are ratios of the speed in a medium to speed in a vacuum.\n\nBy the late nineteenth century, various experimental anomalies could not be explained by the simple wave theory. One of these anomalies involved a controversy over the speed of light. The speed of light and other EMR predicted by Maxwell's equations did not appear unless the equations were modified in a way first suggested by FitzGerald and Lorentz (see history of special relativity), or else otherwise that speed would depend on the speed of observer relative to the \"medium\" (called luminiferous aether) which supposedly \"carried\" the electromagnetic wave (in a manner analogous to the way air carries sound waves). Experiments failed to find any observer effect. In 1905, Einstein proposed that space and time appeared to be velocity-changeable entities for light propagation and all other processes and laws. These changes accounted for the constancy of the speed of light and all electromagnetic radiation, from the viewpoints of all observers—even those in relative motion.\n\nElectromagnetic radiation of wavelengths other than those of visible light were discovered in the early 19th century. The discovery of infrared radiation is ascribed to astronomer William Herschel, who published his results in 1800 before the Royal Society of London. Herschel used a glass prism to refract light from the Sun and detected invisible rays that caused heating beyond the red part of the spectrum, through an increase in the temperature recorded with a thermometer. These \"calorific rays\" were later termed infrared.\n\nIn 1801, German physicist Johann Wilhelm Ritter discovered ultraviolet in an experiment similar to Hershel's, using sunlight and a glass prism. Ritter noted that invisible rays near the violet edge of a solar spectrum dispersed by a triangular prism darkened silver chloride preparations more quickly than did the nearby violet light. Ritter's experiments were an early precursor to what would become photography. Ritter noted that the ultraviolet rays (which at first were called \"chemical rays\") were capable of causing chemical reactions.\nIn 1862-4 James Clerk Maxwell developed equations for the electromagnetic field which suggested that waves in the field would travel with a speed that was very close to the known speed of light. Maxwell therefore suggested that visible light (as well as invisible infrared and ultraviolet rays by inference) all consisted of propagating disturbances (or radiation) in the electromagnetic field. Radio waves were first produced deliberately by Heinrich Hertz in 1887, using electrical circuits calculated to produce oscillations at a much lower frequency than that of visible light, following recipes for producing oscillating charges and currents suggested by Maxwell's equations. Hertz also developed ways to detect these waves, and produced and characterized what were later termed radio waves and microwaves.\n\nWilhelm Röntgen discovered and named X-rays. After experimenting with high voltages applied to an evacuated tube on 8 November 1895, he noticed a fluorescence on a nearby plate of coated glass. In one month, he discovered X-rays' main properties.\n\nThe last portion of the EM spectrum to be discovered was associated with radioactivity. Henri Becquerel found that uranium salts caused fogging of an unexposed photographic plate through a covering paper in a manner similar to X-rays, and Marie Curie discovered that only certain elements gave off these rays of energy, soon discovering the intense radiation of radium. The radiation from pitchblende was differentiated into alpha rays (alpha particles) and beta rays (beta particles) by Ernest Rutherford through simple experimentation in 1899, but these proved to be charged particulate types of radiation. However, in 1900 the French scientist Paul Villard discovered a third neutrally charged and especially penetrating type of radiation from radium, and after he described it, Rutherford realized it must be yet a third type of radiation, which in 1903 Rutherford named gamma rays. In 1910 British physicist William Henry Bragg demonstrated that gamma rays are electromagnetic radiation, not particles, and in 1914 Rutherford and Edward Andrade measured their wavelengths, finding that they were similar to X-rays but with shorter wavelengths and higher frequency, although a 'cross-over' between X and gamma rays makes it possible to have X-rays with a higher energy (and hence shorter wavelength) than gamma rays and vice versa. The origin of the ray differentiates them, gamma rays tend to be a natural phenomena originating from the unstable nucleus of an atom and X-rays are electrically generated (and hence man-made) unless they are as a result of bremsstrahlung X-radiation caused by the interaction of fast moving particles (such as beta particles) colliding with certain materials, usually of higher atomic numbers.\n\nEM radiation (the designation 'radiation' excludes static electric and magnetic and near fields) is classified by wavelength into radio, microwave, infrared, visible, ultraviolet, X-rays and gamma rays. Arbitrary electromagnetic waves can be expressed by Fourier analysis in terms of sinusoidal monochromatic waves, which in turn can each be classified into these regions of the EMR spectrum.\n\nFor certain classes of EM waves, the waveform is most usefully treated as \"random\", and then spectral analysis must be done by slightly different mathematical techniques appropriate to random or stochastic processes. In such cases, the individual frequency components are represented in terms of their \"power\" content, and the phase information is not preserved. Such a representation is called the power spectral density of the random process. Random electromagnetic radiation requiring this kind of analysis is, for example, encountered in the interior of stars, and in certain other very wideband forms of radiation such as the Zero point wave field of the electromagnetic vacuum.\n\nThe behavior of EM radiation depends on its frequency. Lower frequencies have longer wavelengths, and higher frequencies have shorter wavelengths, and are associated with photons of higher energy. There is no fundamental limit known to these wavelengths or energies, at either end of the spectrum, although photons with energies near the Planck energy or exceeding it (far too high to have ever been observed) will require new physical theories to describe.\n\nSoundwaves are not electromagnetic radiation. At the lower end of the electromagnetic spectrum, about 20 Hz to about 20 kHz, are frequencies that might be considered in the audio range. However, electromagnetic waves cannot be directly perceived by human ears. Sound waves are instead the oscillating compression of molecules. To be heard, electromagnetic radiation must be converted to pressure waves of the fluid in which the ear is located (whether the fluid is air, water or something else).\n\nWhen EM radiation interacts with matter, its behavior changes qualitatively as its frequency changes.\n\nAt radio and microwave frequencies, EMR interacts with matter largely as a bulk collection of charges which are spread out over large numbers of affected atoms. In electrical conductors, such induced bulk movement of charges (electric currents) results in absorption of the EMR, or else separations of charges that cause generation of new EMR (effective reflection of the EMR). An example is absorption or emission of radio waves by antennas, or absorption of microwaves by water or other molecules with an electric dipole moment, as for example inside a microwave oven. These interactions produce either electric currents or heat, or both.\n\nLike radio and microwave, infrared also is reflected by metals (as is most EMR into the ultraviolet). However, unlike lower-frequency radio and microwave radiation, Infrared EMR commonly interacts with dipoles present in single molecules, which change as atoms vibrate at the ends of a single chemical bond. It is consequently absorbed by a wide range of substances, causing them to increase in temperature as the vibrations dissipate as heat. The same process, run in reverse, causes bulk substances to radiate in the infrared spontaneously (see thermal radiation section below).\n\nAs frequency increases into the visible range, photons have enough energy to change the bond structure of some individual molecules. It is not a coincidence that this happens in the \"visible range,\" as the mechanism of vision involves the change in bonding of a single molecule (retinal) which absorbs light in the rhodopsin in the retina of the human eye. Photosynthesis becomes possible in this range as well, for similar reasons, as a single molecule of chlorophyll is excited by a single photon. Animals that detect infrared make use of small packets of water that change temperature, in an essentially thermal process that involves many photons (see infrared sensing in snakes). For this reason, infrared, microwaves and radio waves are thought to damage molecules and biological tissue only by bulk heating, not excitation from single photons of the radiation.\n\nVisible light is able to affect a few molecules with single photons, but usually not in a permanent or damaging way, in the absence of power high enough to increase temperature to damaging levels. However, in plant tissues that conduct photosynthesis, carotenoids act to quench electronically excited chlorophyll produced by visible light in a process called non-photochemical quenching, in order to prevent reactions that would otherwise interfere with photosynthesis at high light levels. Limited evidence indicate that some reactive oxygen species are created by visible light in skin, and that these may have some role in photoaging, in the same manner as ultraviolet A.\n\nAs frequency increases into the ultraviolet, photons now carry enough energy (about three electron volts or more) to excite certain doubly bonded molecules into permanent chemical rearrangement. In DNA, this causes lasting damage. DNA is also indirectly damaged by reactive oxygen species produced by ultraviolet A (UVA), which has energy too low to damage DNA directly. This is why ultraviolet at all wavelengths can damage DNA, and is capable of causing cancer, and (for UVB) skin burns (sunburn) that are far worse than would be produced by simple heating (temperature increase) effects. This property of causing molecular damage that is out of proportion to heating effects, is characteristic of all EMR with frequencies at the visible light range and above. These properties of high-frequency EMR are due to quantum effects that permanently damage materials and tissues at the molecular level.\n\nAt the higher end of the ultraviolet range, the energy of photons becomes large enough to impart enough energy to electrons to cause them to be liberated from the atom, in a process called photoionisation. The energy required for this is always larger than about 10 electron volts (eV) corresponding with wavelengths smaller than 124 nm (some sources suggest a more realistic cutoff of 33 eV, which is the energy required to ionize water). This high end of the ultraviolet spectrum with energies in the approximate ionization range, is sometimes called \"extreme UV.\" Ionizing UV is strongly filtered by the Earth's atmosphere).\n\nElectromagnetic radiation composed of photons that carry minimum-ionization energy, or more, (which includes the entire spectrum with shorter wavelengths), is therefore termed ionizing radiation. (Many other kinds of ionizing radiation are made of non-EM particles). Electromagnetic-type ionizing radiation extends from the extreme ultraviolet to all higher frequencies and shorter wavelengths, which means that all X-rays and gamma rays qualify. These are capable of the most severe types of molecular damage, which can happen in biology to any type of biomolecule, including mutation and cancer, and often at great depths below the skin, since the higher end of the X-ray spectrum, and all of the gamma ray spectrum, penetrate matter.\n\nMost UV and X-rays are blocked by absorption first from molecular nitrogen, and then (for wavelengths in the upper UV) from the electronic excitation of dioxygen and finally ozone at the mid-range of UV. Only 30% of the Sun's ultraviolet light reaches the ground, and almost all of this is well transmitted.\n\nVisible light is well transmitted in air, as it is not energetic enough to excite nitrogen, oxygen, or ozone, but too energetic to excite molecular vibrational frequencies of water vapor.\n\nAbsorption bands in the infrared are due to modes of vibrational excitation in water vapor. However, at energies too low to excite water vapor, the atmosphere becomes transparent again, allowing free transmission of most microwave and radio waves.\n\nFinally, at radio wavelengths longer than 10 meters or so (about 30 MHz), the air in the lower atmosphere remains transparent to radio, but plasma in certain layers of the ionosphere begins to interact with radio waves (see skywave). This property allows some longer wavelengths (100 meters or 3 MHz) to be reflected and results in shortwave radio beyond line-of-sight. However, certain ionospheric effects begin to block incoming radiowaves from space, when their frequency is less than about 10 MHz (wavelength longer than about 30 meters).\n\nRadio waves have the least amount of energy and the lowest frequency. When radio waves impinge upon a conductor, they couple to the conductor, travel along it and induce an electric current on the conductor surface by moving the electrons of the conducting material in correlated bunches of charge. Such effects can cover macroscopic distances in conductors (such as radio antennas), since the wavelength of radiowaves is long.\n\nMicrowaves are a form of electromagnetic radiation with wavelengths ranging from as long as one meter to as short as one millimeter; with frequencies between 300 MHz (0.3 GHz) and 300 GHz.\n\nNatural sources produce EM radiation across the spectrum. EM radiation with a wavelength between approximately 400 nm and 700 nm is directly detected by the human eye and perceived as visible light. Other wavelengths, especially nearby infrared (longer than 700 nm) and ultraviolet (shorter than 400 nm) are also sometimes referred to as light.\n\nThe basic structure of matter involves charged particles bound together. When electromagnetic radiation impinges on matter, it causes the charged particles to oscillate and gain energy. The ultimate fate of this energy depends on the context. It could be immediately re-radiated and appear as scattered, reflected, or transmitted radiation. It may get dissipated into other microscopic motions within the matter, coming to thermal equilibrium and manifesting itself as thermal energy, or even kinetic energy, in the material. With a few exceptions related to high-energy photons (such as fluorescence, harmonic generation, photochemical reactions, the photovoltaic effect for ionizing radiations at far ultraviolet, X-ray and gamma radiation), absorbed electromagnetic radiation simply deposits its energy by heating the material. This happens for infrared, microwave and radio wave radiation. Intense radio waves can thermally burn living tissue and can cook food. In addition to infrared lasers, sufficiently intense visible and ultraviolet lasers can easily set paper afire.\n\nIonizing radiation creates high-speed electrons in a material and breaks chemical bonds, but after these electrons collide many times with other atoms eventually most of the energy becomes thermal energy all in a tiny fraction of a second. This process makes ionizing radiation far more dangerous per unit of energy than non-ionizing radiation. This caveat also applies to UV, even though almost all of it is not ionizing, because UV can damage molecules due to electronic excitation, which is far greater per unit energy than heating effects.\n\nInfrared radiation in the spectral distribution of a black body is usually considered a form of heat, since it has an equivalent temperature and is associated with an entropy change per unit of thermal energy. However, \"heat\" is a technical term in physics and thermodynamics and is often confused with thermal energy. Any type of electromagnetic energy can be transformed into thermal energy in interaction with matter. Thus, \"any\" electromagnetic radiation can \"heat\" (in the sense of increase the thermal energy temperature of) a material, when it is absorbed.\n\nThe inverse or time-reversed process of absorption is thermal radiation. Much of the thermal energy in matter consists of random motion of charged particles, and this energy can be radiated away from the matter. The resulting radiation may subsequently be absorbed by another piece of matter, with the deposited energy heating the material.\n\nThe electromagnetic radiation in an opaque cavity at thermal equilibrium is effectively a form of thermal energy, having maximum radiation entropy.\n\nBioelectromagnetics is the study of the interactions and effects of EM radiation on living organisms. The effects of electromagnetic radiation upon living cells, including those in humans, depends upon the radiation's power and frequency. For low-frequency radiation (radio waves to visible light) the best-understood effects are those due to radiation power alone, acting through heating when radiation is absorbed. For these thermal effects, frequency is important only as it affects penetration into the organism (for example, microwaves penetrate better than infrared). It is widely accepted that low frequency fields that are too weak to cause significant heating could not possibly have any biological effect.\n\nDespite the commonly accepted results, some research has been conducted to show that weaker \"non-thermal\" electromagnetic fields, (including weak ELF magnetic fields, although the latter does not strictly qualify as EM radiation), and modulated RF and microwave fields have biological effects. Fundamental mechanisms of the interaction between biological material and electromagnetic fields at non-thermal levels are not fully understood.\n\nThe World Health Organization has classified radio frequency electromagnetic radiation as Group 2B - possibly carcinogenic. This group contains possible carcinogens such as lead, DDT, and styrene. For example, epidemiological studies looking for a relationship between cell phone use and brain cancer development, have been largely inconclusive, save to demonstrate that the effect, if it exists, cannot be a large one.\n\nAt higher frequencies (visible and beyond), the effects of individual photons begin to become important, as these now have enough energy individually to directly or indirectly damage biological molecules. All UV frequences have been classed as Group 1 carcinogens by the World Health Organization. Ultraviolet radiation from sun exposure is the primary cause of skin cancer.\n\nThus, at UV frequencies and higher (and probably somewhat also in the visible range), electromagnetic radiation does more damage to biological systems than simple heating predicts. This is most obvious in the \"far\" (or \"extreme\") ultraviolet. UV, with X-ray and gamma radiation, are referred to as ionizing radiation due to the ability of photons of this radiation to produce ions and free radicals in materials (including living tissue). Since such radiation can severely damage life at energy levels that produce little heating, it is considered far more dangerous (in terms of damage-produced per unit of energy, or power) than the rest of the electromagnetic spectrum.\n\nThe heat ray is an application of EMR that makes use of microwave frequencies to create an unpleasant heating effect in the upper layer of the skin. A publicly known heat ray weapon called the Active Denial System was developed by the US military as an experimental weapon to deny the enemy access to an area. A death ray is a weapon that delivers heat ray electromagnetic energy at levels that injure human tissue. The inventor of the death ray, Harry Grindell Matthews, claims to have lost sight in his left eye while developing his death ray weapon based on a primitive microwave magnetron from the 1920s (note that a typical microwave oven induces a tissue damaging cooking effect inside the oven at about 2 kV/m).\n\nElectromagnetic waves are predicted by the classical laws of electricity and magnetism, known as Maxwell's equations. There are nontrivial solutions of the homogeneous Maxwell's equations (without charges or currents), describing \"waves\" of changing electric and magnetic fields. Beginning with Maxwell's equations in free space:\n\nBesides the trivial solution\nuseful solutions can be derived with the following vector identity, valid for all vectors formula_16 in some vector field:\n\nTaking the curl of the second Maxwell equation () yields:\n\nEvaluating the left hand side of () with the above identity and simplifying using (), yields:\n\nEvaluating the right hand side of () by exchanging the sequence of derivations and inserting the fourth yields:\n\nCombining () and () again, gives a vector-valued differential equation for the electric field, solving the homogeneous Maxwell equations:\n\nTaking the curl of the fourth Maxwell equation () results in a similar differential equation for a magnetic field solving the homogeneous Maxwell equations:\n\nBoth differential equations have the form of the general wave equation for waves propagating with speed formula_18 where formula_19 is a function of time and location, which gives the amplitude of the wave at some time at a certain location:\n\nfor a generic wave traveling in the formula_24 direction.\n\nThis form will satisfy the wave equation.\n\nThe first of Maxwell's equations implies that the electric field is orthogonal to the direction the wave propagates.\n\nThe second of Maxwell's equations yields the magnetic field. The remaining equations will be satisfied by this choice of formula_29.\n\nThe electric and magnetic field waves in the far-field travel at the speed of light. They have a special restricted orientation and proportional magnitudes, formula_30, which can be seen immediately from the Poynting vector. The electric field, magnetic field, and direction of wave propagation are all orthogonal, and the wave propagates in the same direction as formula_31. Also, E and B far-fields in free space, which as wave solutions depend primarily on these two Maxwell equations, are in-phase with each other. This is guaranteed since the generic wave solution is first order in both space and time, and the curl operator on one side of these equations results in first-order spatial derivatives of the wave solution, while the time-derivative on the other side of the equations, which gives the other field, is first-order in time, resulting in the same phase shift for both fields in each mathematical operation.\n\nFrom the viewpoint of an electromagnetic wave traveling forward, the electric field might be oscillating up and down, while the magnetic field oscillates right and left. This picture can be rotated with the electric field oscillating right and left and the magnetic field oscillating down and up. This is a different solution that is traveling in the same direction. This arbitrariness in the orientation with respect to propagation direction is known as polarization. On a quantum level, it is described as photon polarization. The direction of the polarization is defined as the direction of the electric field.\n\nMore general forms of the second-order wave equations given above are available, allowing for both non-vacuum propagation media and sources. Many competing derivations exist, all with varying levels of approximation and intended applications. One very general example is a form of the electric field equation, which was factorized into a pair of explicitly directional wave equations, and then efficiently reduced into a single uni-directional wave equation by means of a simple slow-evolution approximation.\n\n\n"}
{"id": "19085457", "url": "https://en.wikipedia.org/wiki?curid=19085457", "title": "Energy Markets Emergency Act of 2008", "text": "Energy Markets Emergency Act of 2008\n\nThe Energy Markets Emergency Act of 2008 (H.R.6377) was a bill in the 110th Congress that \"directs the Commodity Futures Trading Commission to use its authority to deal with issues causing major market disturbances.\" More specifically, the legislation directed the Commodity Futures Trading Commission to utilize all its authority, including its emergency powers, to curb immediately excessive speculation, price distortion, sudden or unreasonable fluctuations or unwarranted changes in prices, or other unlawful activity that is allegedly causing major market disturbances that prevent the market from accurately reflecting the forces of supply and demand for commodities. \n\nThe bill was passed in the House. But in the Senate, sixty \"yes\" votes were required for the July 25, 2008 cloture motion to be successful, but the measure received only 50 \"yes\" votes with 43 lawmakers opposed. Senate Democrats said the legislation was needed to give the government new powers to curb alleged speculators, whom some lawmakers accuse of being behind the run-up in crude oil and gasoline prices. However, Senate Republicans strongly opposed the bill, arguing the legislation should be modified to also boost U.S. oil production by allowing more offshore drilling and developing vast oil shale fields in the West.\n\n"}
{"id": "16392407", "url": "https://en.wikipedia.org/wiki?curid=16392407", "title": "Energy Victory", "text": "Energy Victory\n\nEnergy Victory: Winning the War on Terror by Breaking Free of Oil is a 2007 book by Robert Zubrin. Zubrin's central argument is that the decisive front in the War on Terror is America's struggle for energy independence. He outlines the manner in which radical Islam has been financed by oil revenues, the technological feasibility of ethanol-fueled vehicles as well as the economic and agricultural imperatives for ethanol production, and the environmental implications of his plan.\n\nZubrin contends that OPEC nations, particularly Saudi Arabia, have used their enormous oil wealth to fund Islamic extremism; in effect, the US is financing both sides of the War on Terror. They have been able to do this through colluding to keep oil prices high. Due to its dependence on their oil, the United States (and the rest of the world) is powerless to do anything about this.\n\nThe key to winning the war on terror, therefore, is to create a substitute for oil. Zubrin argues that a mandate that all new cars sold in the United States be flex-fueled (FFV, for Flex-Fuel Vehicle, able to run on gasoline, ethanol or methanol, or any combination thereof) would very quickly make such vehicles the world standard, as occurred in the early 1980s with the introduction of catalytic converters. As a result, consumers would demand ethanol- and methanol-blended fuels due to their price competitiveness with gasoline, which would in turn prompt gas stations to instal biofuel pumps. Under such a situation, competition would drive oil prices down. Zubrin argues that biofuels should be subsidized in order to keep their price advantage over gasoline, as it is the only way to cripple OPEC.\n\nSome have argued that a switch to electric cars would be more beneficial. While this may be a longer-term solution, a switch to biofuel can be achieved in a few years (as in the case of Brazil). Additionally, existing cars (including hybrids) can be retrofitted with flex-fuel capability for \"between $100 and $500\".\n\nA switch to biofuel would have the additional benefit that it is potentially a carbon-neutral fuel.\n\nEthanol is produced primarily via the fermentation of corn or sugar cane (or indeed any other glucose-rich crop). Methanol can be produced from any plant matter. As both of these products can easily be produced in developing countries, Zubrin contends that the resultant expanding market for farm produce would be greatly beneficial for third-world farmers. There would be no need for western nations to subsidize their own farmers, as third-world produce could be absorbed into the larger market without causing a price-crash that would bankrupt western farmers.\n\nAnne Korin, of The Institute for the Analysis of Global Security, has developed this concept further, adding to Zubrin's mandate the necessity to eliminate ethanol and sugar import tariffs in the United States for it to succeed.\n\nGal Luft, writing for the Institute for the Analysis of Global Security, called \"Energy Victory\" \"one of the best books written on our oil dependence problem\".\n\nZubrin presented the arguments from \"Energy Victory\" at a series of \"go green\" lectures sponsored by the Advanced Planning and Partnership Office and hosted by NASA, in January 2008.\n\n\n"}
{"id": "23031575", "url": "https://en.wikipedia.org/wiki?curid=23031575", "title": "Environmental issues in Malaysia", "text": "Environmental issues in Malaysia\n\nEnvironmental issues in Malaysia\n\nThe IUCN Red List gives 2890 native endangered species.\n\n\nDeforestation at the following locations are threatening flora and fauna:\n\nDeforestation is taking place for a variety of reasons, including:\n\n\nCoastal reclamation is damaging mangroves and turtle nesting sights\n\n\n"}
{"id": "5801968", "url": "https://en.wikipedia.org/wiki?curid=5801968", "title": "Field mill", "text": "Field mill\n\nA field mill is a specialized instrument used for measuring the strength of electrical fields in the atmosphere near thunderstorm clouds. They are used in the launch criteria for rockets bound for orbit, as well as the now-retired Space Shuttle, to avoid lightning strikes. They are also used in outdoor laboratories for lightning protection equipment to determine favorable experiment conditions.\n\nThe \"mill\" is a typical rotating shutter design in the instrument. It is usually deployed airborne and flown through anvil head clouds to make measurements.\n\nAt Kennedy Space Center (KSC) in Florida, 31 field mills are deployed around KSC and the nearby Cape Canaveral Air Force Station. Data from the field mills help forecasters determine when electric charge aloft might trigger lightning during a launch.\n\n"}
{"id": "630247", "url": "https://en.wikipedia.org/wiki?curid=630247", "title": "Fourteener", "text": "Fourteener\n\nIn the mountaineering parlance of the Western United States, a fourteener is a mountain peak with an elevation of at least . There are 96 fourteeners in the United States, all west of the Mississippi River. Colorado has the most (53) of any single state; Alaska is in second place with 29. Many peak baggers try to climb all fourteeners in the contiguous United States, one particular state, or another region.\n\nThe summit of a mountain or hill may be measured in three principal ways:\n\nNot all summits over 14,000 feet qualify as fourteeners. Summits which qualify are those considered by mountaineers to be independent. Objective standards for independence include topographic prominence and isolation (distance from a higher summit), or a combination of the two. However, fourteener lists do not always consistently use such objective rules. \n\nA rule commonly used by mountaineers in the contiguous United States is that a peak must have at least of prominence to qualify. By this rule, Colorado has 53 fourteeners, California has 12, and Washington has two.\n\nAccording to the Mountaineering Club of Alaska, it is standard in Alaska to use a prominence rule rather than a rule. By this rule, Alaska has at least 21 peaks over and its 12 highest peaks exceed .\n\nThe following table lists the 96 mountain peaks of the United States with at least of topographic elevation and at least of topographic prominence. Of these 96 fourteeners, 53 rise in Colorado, 29 in Alaska, 12 in California, and two in Washington. The 22 highest fourteeners all rise in Alaska.\n\nThe table above uses a minimum topographic prominence criterion of and includes 97 peaks. The number of peaks included depends upon the minimum topographic prominence criterion. A criterion of includes 90 peaks, includes 77 peaks, includes 63 peaks, and includes 46 peaks.\n\nThe following U.S. summits have 14,000 feet of elevation, but have less than 300 feet of topographic prominence:\n\n\n"}
{"id": "47725587", "url": "https://en.wikipedia.org/wiki?curid=47725587", "title": "Frank and John Craighead", "text": "Frank and John Craighead\n\nFrank Cooper Craighead Jr. (August 14, 1916 – October 21, 2001) and John Johnson Craighead (August 14, 1916 – September 18, 2016), twin brothers, were American conservationists, naturalists, and researchers who made important contributions to the study of falconry and grizzly bear biology. Born in Washington, D.C. where both graduated from Western High School in 1935, the brothers began collecting and identifying animals and plants they found alongside the Potomac, and soon expanded their interests to birds and hawks, going west in 1934 to begin studying falconry. After the war, during which they were employed as survival trainers, they each married and resumed their work in falconry. During the 1950s they expanded their work to other animals, including many species living in and around Yellowstone, and eventually separated.\n\nIn 1959 their careers merged again, this time to begin a 12-year study of grizzly bears in Yellowstone, as the animals were considered threatened by increased human activity; however a 1971 disagreement with the National Park Service ended their Yellowstone studies; however it continued elsewhere in Montana, including the Scapegoat Wilderness. After 1976 their work was mostly confined to field guides and educating the public about environmentalism; however, field ecology continued until Frank's death in 2001 from Parkinson's disease.\n\nIn 1998, the National Audubon Society named the brothers among the top 100 conservationists of the 20th century.\n\nFrank Cooper Craighead and John Johnson Craighead were born in Washington, D.C. on August 14, 1916. Their father, Frank Craighead Sr., was an environmentalist and founder of the modern environmentalist Craighead family. Their sister, Jean Craighead George, was an author of books with nature and environmental themes for children and young adults. The twin brothers, almost identical to one another, spent much of their time collecting animals and plants along the banks of the Potomac while out of school, but their breakthrough with wildlife came in 1927, when they raised a baby owl at their home. Their interest in hawks and owls grew, and by the early 1930s they regularly visited hawk and owl nests all along the Potomac. Eventually, after high school, they moved to Pennsylvania and attended college there, graduating with science degrees in 1939.\n\nAt age 20, the brothers wrote their first article for National Geographic Society, published in the July 1937 issue, \"Adventures with Birds of Prey.\" They would write a total of 14 articles for the magazine between 1937 and 1976. During the war, R. S. Dharmakumarsinhji, an Indian prince living in Bhavnagar, impressed by the Craigheads' 1937 and 1950 articles, invited them to visit India, where they learned about Indian ways of life, but returned home in 1942, as they missed home and their studies of falconry. They also became deeply opposed to killing animals after participating in Indian hunts during their stay. After coming home again, they continued survival training until 1950, during that year they also received their Ph.Ds in 1949 from the University of Michigan. During this time they practiced much wildlife research in Wyoming and Montana, writing \"Cloud Gardens in the Tetons\" in 1948 and \"Wildlife Adventuring in Jackson Hole\" in 1956.\n\nIn 1950, Frank and John were survival consultants to the Strategic Air Command, and in 1951 they organized survival training schools for the Air Force at Mountain Home and McCall, Idaho. From 1953 to 1955 Frank conducted classified defense research. His log home in Moose wasn't winterized, so the family lived in various places around the Jackson Hole valley, including stays on the Murie Ranch, the old Budge house in Wilson, and a house in Jackson. Frank and John went their separate ways in the early 1950s, when John accepted a permanent position with the University of Montana and Frank decided to work outside of Academia. From 1955 to 1957 he managed the Desert Game Range outside Las Vegas, Nevada, for the USFWS. This was the era of nuclear testing and Frank had great concerns about the effects of radiation, but his efforts to measure and document radiation levels on the refuge were not encouraged by the federal government.\n\nDuring 1959, Frank and John's careers merged again. At the request of Yellowstone National Park, they began a 12-year study of grizzly bears. Frank would drive from Pennsylvania, arriving in Yellowstone early in the spring and staying until late in the fall when the bears denned. Esther, Frank's wife waited until the kids were out of school and then drove to Moose for the summer. In late August she would load up the station wagon and drive back to Boiling Springs. By 1966 the long cross-country drives had become too much. Frank added indoor plumbing to his cabin on Antelope Flats, and he and Esther moved to Moose, Wyoming permanently. The brothers became famous in radio tracking and studying the grizzlies and black bears, by satellite, pioneered tranquilization, and studied the negative effects of grizzlies wandering outside the park boundaries. The Craigheads tagged 30 grizzlies in their first year, 37 in their second, and eventually, over 600 bears were transmitted and studied. They were often treed or chased by bears, but no injuries occurred. They went through the tragedy of seeing a bear die after being tagged in 1963, and the fact that many bears died at age 5 or 6 after human encounters persuaded the Craigheads to ask park officials to enforce animal rules more strictly.\n\nThat sadly ended in 1971 when the Park Service planned to erase human effect on the park by closing the artificial food supplies (dumps) that the grizzlies depended on, which resulted in more aggressive bears being killed after many fatal maulings in the 1970s. The Craigheads were barred from doing any more work in the park by 1971 for speaking against this; however, they continued to do bear research in Montana until the 1980s.\n\nThe brothers, especially Frank, were deeply concerned about preserving the West's rivers, and after educating the public about how vital rivers were for water, recreation and fishing they created the Environmental Research Institute, which paved the way for clean water protection and President Johnson signing the National Wild and Scenic Rivers bill of 1965. The Craigheads ended their active research after Frank's log cabin in Moose burned down in 1978.\n\nFrank married Esther Craighead in 1945. Meanwhile, John had married Margaret Smith, a mountain climber and daughter of a Grand Teton National Park ranger. Frank and Esther, and John and Margaret built identical log cabins on their property in Moose, and began families. While Frank was completing his various field studies during the late 1940s and early 1950s, he and Esther had three children - Lance, Charlie, and Jana - all born in Jackson at the old log cabin.\n\nFrank's health deteriorated due to Parkinson's disease he had been diagnosed with in 1987, during his second marriage and seven years after Esther died, and he died in 2001 at the age of 85. The Craighead institute has offices in both Bozeman and Moose and is run by Frank's son Lance.\n\nJohn Craighead lived in Missoula, Montana. He turned 100 in August 2016.\nHe died in South Missoula on September 18, 2016 at the age of 100.\n\n\n\n"}
{"id": "57747791", "url": "https://en.wikipedia.org/wiki?curid=57747791", "title": "Great Trees of London", "text": "Great Trees of London\n\nGreat Trees of London was a list created by Trees for Cities after the Great Storm of 1987, when the general public were asked to suggest suitable trees. 41 were chosen, with a further 20 added in 2008.\nTime Out published a book 'The Great Trees of London' listing all 61 trees in 2010.\n\n\n\n"}
{"id": "36172555", "url": "https://en.wikipedia.org/wiki?curid=36172555", "title": "Growth of wind power in the United States", "text": "Growth of wind power in the United States\n\nWind power has been growing in the United States since the 1970s.\n\nThe growth of wind power in the United States has been enhanced by a production tax credit (PTC), which pays 2.3¢/kWh for the first 10 years. Development has been off and on again due to the expiration and late renewal of the PTC. It expired at the end of 2013, only to be restored in December 2014. In 2015 it was renewed but is phased out over a five-year period. One of the goals set by the Department of Energy is to increase the number of states which have over 1,000 MW of wind power to 15 by 2018. By the end of 2015 there were 17. Another goal was to increase the number of states with at least 100 MW to 30 by 2010. As of 2011, there are 29. The 2015 vision for wind calls for 10% generation by wind by 2020 (113 GW), 30% by 2030 (224 GW), and 35% by 2050 (404 GW). It calls for 40 states to each have at least 1 GW, and wind power in all 50 states. Larger wind turbines has expanded the commercial viability of wind to all 50 states. Analysts expect 25 GW more between 2016-18.\n\nSource:\n\nSource:\n\nThe United States uses approximately 4 million GWh/year.\n"}
{"id": "24367869", "url": "https://en.wikipedia.org/wiki?curid=24367869", "title": "IKF World Korfball Ranking", "text": "IKF World Korfball Ranking\n\nThe IKF World Korfball Ranking is the ranking for national korfball teams, done by the International Korfball Federation.\n\n"}
{"id": "29956501", "url": "https://en.wikipedia.org/wiki?curid=29956501", "title": "James Grigor", "text": "James Grigor\n\nJames Grigor (1811?–1848), was a botanist.\n\nGrigor was the author of the ‘Eastern Arboretum, or Register of Remarkable Trees, Seats, Gardens, &c., in the County of Norfolk,’ London 18[40-]41, with fifty etched plates, issued in fifteen numbers. In the preface (dated Norwich, 1 Sept. 1841) he states that he had devoted ‘twenty years to practical botanical pursuits,’ and his work was highly praised by J. C. Loudon.\n\nHe wrote a ‘Report on Trimingham and Runton Plantations in the county of Norfolk, belonging to Sir Edward North Buxton, Baronet,’ published in the ‘Transactions’ of the Highland Agricultural Society of Scotland, x. (new ser.) 557-74, for which he earned a gold medal, and where he is described as ‘Nurseryman and Land Improver, Norwich.’ He died at Norwich, 22 April 1848, ‘about thirty-seven years old.’\n\n"}
{"id": "4243419", "url": "https://en.wikipedia.org/wiki?curid=4243419", "title": "John Livingston (naturalist)", "text": "John Livingston (naturalist)\n\nJohn Allen Livingston (November 10, 1923 – January 17, 2006) was a Canadian naturalist, broadcaster, author, and teacher. He was most known as the voice-over of the \"Hinterland Who's Who\" series of television zoological shorts in the 1960s.\n\nBorn in Hamilton, Ontario, he enlisted in the Royal Canadian Navy at the beginning of World War II and earned a degree in English literature in 1943 while serving on active service.\n\nLivingston was the author of several books, including \"The Fallacy of Wildlife Conservation\" (1981) and the Governor General's Award-winning \"Rogue Primate\" (1994).\n\nIn an interview with Thomas G. Philpott, he once said that a person is lucky to have one or two truly original ideas in his or her entire life and that the thinking that led to \"Rogue Primate\" was his.\n\n\n"}
{"id": "446712", "url": "https://en.wikipedia.org/wiki?curid=446712", "title": "Josephson effect", "text": "Josephson effect\n\nThe Josephson effect is the phenomenon of supercurrent, a current that flows indefinitely long without any voltage applied, across a device known as a Josephson junction (JJ), which consists of two or more superconductors coupled by a weak link. The weak link can consist of a thin insulating barrier (known as a superconductor–insulator–superconductor junction, or S-I-S), a short section of non-superconducting metal (S-N-S), or a physical constriction that weakens the superconductivity at the point of contact (S-s-S).\n\nThe Josephson effect is an example of a macroscopic quantum phenomenon. It is named after the British physicist Brian David Josephson, who predicted in 1962 the mathematical relationships for the current and voltage across the weak link. The DC Josephson effect had been seen in experiments prior to 1962, but had been attributed to \"super-shorts\" or breaches in the insulating barrier leading to the direct conduction of electrons between the superconductors. The first paper to claim the discovery of Josephson's effect, and to make the requisite experimental checks, was that of Philip Anderson and John Rowell. These authors were awarded patents on the effects that were never enforced, but never challenged.\n\nBefore Josephson's prediction, it was only known that normal (i.e. non-superconducting) electrons can flow through an insulating barrier, by means of quantum tunneling. Josephson was the first to predict the tunneling of superconducting Cooper pairs. For this work, Josephson received the Nobel Prize in Physics in 1973. Josephson junctions have important applications in quantum-mechanical circuits, such as SQUIDs, superconducting qubits, and RSFQ digital electronics. The NIST standard for one volt is achieved by an array of 20,208 Josephson junctions in series.\n\nTypes of Josephson junction include the pi Josephson junction, varphi Josephson junction, long Josephson junction, and Superconducting tunnel junction. A \"Dayem bridge\" is a thin-film variant of the Josephson junction in which the weak link consists of a superconducting wire with dimensions on the scale of a few micrometres or less. The Josephson junction count of a device is used as a benchmark for its complexity. The Josephson effect has found wide usage, for example in the following areas:\n\n\nThe basic equations governing the dynamics of the Josephson effect are\n\nwhere formula_4 and formula_5 are the voltage across and the current through the Josephson junction, formula_6 is the \"phase difference\" across the junction (i.e., the difference in phase factor, or equivalently, argument, between the Ginzburg–Landau complex order parameter of the two superconductors composing the junction), and formula_7 is a constant, the \"critical current\" of the junction. The critical current is an important phenomenological parameter of the device that can be affected by temperature as well as by an applied magnetic field. The physical constant formula_8 is the magnetic flux quantum formula_9, the inverse of which is the Josephson constant.\nThe three main effects predicted by Josephson follow from these relations:\n\nThe DC Josephson effect is a direct current crossing the insulator in the absence of any external electromagnetic field, owing to tunneling. This DC Josephson current is proportional to the sine of the phase difference across the insulator, and may take values between formula_10 and formula_7.\n\nWith a fixed voltage formula_12 across the junction, the phase will vary linearly with time and the current will be an AC current with amplitude formula_7 and frequency formula_14. The complete expression for the current drive formula_15 becomes:\n\nThis means a Josephson junction can act as a perfect voltage-to-frequency converter.\n\nIf the phase takes the form formula_17, the voltage and current will be\n\nThe DC components will then be\n\nHence, for distinct AC voltages, the junction may carry a DC current and the junction acts like a perfect frequency-to-voltage converter.\n\nThe Josephson phase is the difference of the phases of the quantum mechanical wave function in two superconducting electrodes forming a Josephson junction.\n\nIf the macroscopic wave functions formula_20 and formula_21 in superconductors 1 and 2 are given by\n\nthen the Josephson phase is defined by\n\nThe Josephson energy is the potential energy accumulated in a Josephson junction when a supercurrent flows through it. One can think of a Josephson junction as a non-linear inductance which accumulates (magnetic field) energy when a current passes through it. In contrast to real inductance, no magnetic field is created by a supercurrent in a Josephson junction — the accumulated energy is the Josephson energy.\n\nFor the simplest case the current-phase relation (CPR) is given by the first Josephson relation:\n\nwhere formula_25, is the supercurrent flowing through the junction, formula_7, is the critical current, and formula_27, is the Josephson phase.\nImagine that initially at time formula_28 the junction was in the ground state formula_29 and finally at time formula_30 the junction has the phase formula_27. The work done on the junction (so the junction energy is increased by)\n\nHere formula_33 sets the characteristic scale of the Josephson energy, and formula_34 sets its dependence on the phase formula_27. The energy formula_36 accumulated inside the junction depends only on the current state of the junction, but not on history or velocities, i.e. it is a potential energy. Note, that formula_36 has a minimum equal to zero for the ground state formula_38, formula_39 is any integer.\n\nImagine that the Josephson phase across the junction is formula_40, and the supercurrent flowing through the junction is\n\nImagine that we add little extra current (direct or alternating) formula_46 through the junction, and want to see how it reacts. The phase across the junction changes to become formula_47. One can write:\n\nAssuming that formula_49 is small, we make a Taylor expansion in the right hand side to arrive at\n\nThe voltage across the junction (we use the 2nd Josephson relation) is\n\nIf we compare this expression with the expression for voltage across the conventional inductance\n\nwe can define the so-called Josephson inductance\nOne can see that this inductance is not constant, but depends on the phase formula_40 across the junction. The typical value is given by formula_55 and is determined only by the critical current formula_7. Note that, according to definition, the Josephson inductance can even become infinite or negative, if formula_57.\n\nOne can also calculate the change in Josephson energy\n\nMaking Taylor expansion for small formula_49, we get\n\nIf we now compare this with the expression for increase of the inductance energy formula_61, we again get the same expression for formula_62.\n\nNote, that although Josephson junction behaves like an inductance, there is no associated magnetic field. The corresponding energy is hidden inside the junction. The Josephson Inductance is also known as a Kinetic Inductance – the behaviour is derived from the kinetic energy of the charge carriers, not energy in a magnetic field.\n\nAs an alternative to the above approach to finding the Josephson Inductance, the equation for voltage across an inductor can be used (given by formula_63). By finding the derivative of the current with respect to time, and rearranging in the form of the inductance equation, inductance can be found.\n\nFirstly, using the chain rule\n\nand from the Josephson junction equations\n\nCombining these three equations gives\n\nand by rearranging to find in the form of formula_63\n\nThe Josephson penetration depth characterizes the typical length on which an externally applied magnetic field penetrates into the long Josephson junction. Josephson penetration depth is usually denoted as formula_70 and is given by the following expression (in SI):\n\nwhere formula_9 is the magnetic flux quantum, formula_73 is the critical current density (A/m²), and formula_74 characterizes the inductance of the superconducting electrodes\n\nwhere formula_76 is the thickness of the Josephson barrier (usually insulator), formula_77 and formula_78 are the thicknesses of superconducting electrodes, and formula_79 and formula_80 are their London penetration depths.\n"}
{"id": "3988315", "url": "https://en.wikipedia.org/wiki?curid=3988315", "title": "Karoo Ice Age", "text": "Karoo Ice Age\n\nThe Karoo Ice Age, more commonly referred to as the Late Paleozoic Ice Age, from 360–260 million years ago (Mya) was the second major ice age of the Phanerozoic Eon. It is named after the tillite (Dwyka Group) found in the Karoo Basin of South Africa, where evidence for this ice age was first clearly identified in the 19th century.\n\nThe tectonic assembly of the continents of Euramerica (later with the Uralian orogeny, into Laurasia) and Gondwana into Pangaea, in the Hercynian-Alleghany Orogeny, made a major continental land mass within the Antarctic region, and the closure of the Rheic Ocean and Iapetus Ocean saw disruption of warm-water currents in the Panthalassa Ocean and Paleotethys Sea, which led to progressive cooling of summers, and the snowfields accumulating in winters, causing mountainous alpine glaciers to grow, and then spread out of highland areas, making continental glaciers which spread to cover much of Gondwana.\n\nAt least two major periods of glaciation have been discovered:\n\nAccording to Eyles and Young, \"Renewed Late Devonian glaciation is well documented in three large intracratonic basins in Brazil (Solimoes, Amazonas and Paranaiba basins) and in Bolivia. By the Early Carboniferous (c. 350 Ma) glacial strata were beginning to accumulate in sub-andean basins of Bolivia, Argentina and Paraguay. By the mid-Carboniferous glaciation had spread to Antarctica, Australia, southern Africa, the Indian Subcontinent, Asia and the Arabian Peninsula. During the Late Carboniferous glacial accumulation (c. 300 Ma) a very large area of Gondwana land mass was experiencing glacial conditions. The thickest glacial deposits of Permo-Carboniferous age are the Dwyka Formation (1000 m thick) in the Karoo Basin in southern Africa, the Itarare Group of the Parana Basin, Brazil (1400 m) and the Carnarvon Basin in eastern Australia. The Permo-Carboniferous glaciations are significant because of the marked glacio-eustatic changes in sea level that resulted and which are recorded in non-glacial basins. Late Paleozoic glaciation of Gondwana could be explained by the migration of the supercontinent across the South Pole.\"\n\nIn northern Ethiopia glacial landforms like striations, rôche moutonnées and chatter marks can be found buried beneath Late Carboniferous-Early Permian glacial deposits.\n\nThe evolution of land plants with the onset of the Devonian Period, began a long-term increase in planetary oxygen levels. Large tree ferns, growing to 20 m high, were secondarily dominant to the large arborescent lycopods (30–40 m high) of the Carboniferous coal forests that flourished in equatorial swamps stretching from Appalachia to Poland, and later on the flanks of the Urals. Oxygen levels reached up to 35%, and global carbon dioxide got below the 300 parts per million level, which is today associated with glacial periods. This reduction in the greenhouse effect was coupled with lignin and cellulose (as tree trunks and other vegetation debris) accumulating and being buried in the great Carboniferous Coal Measures. The reduction of carbon dioxide levels in the atmosphere would be enough to begin the process of changing polar climates, leading to cooler summers which could not melt the previous winter's snow accumulations. The growth in snowfields to 6 m deep would create sufficient pressure to convert the lower levels to ice.\n\nEarth's increased planetary albedo produced by the expanding ice sheets would lead to positive feedback loops, spreading the ice sheets still further, until the process hit limit. Falling global temperatures would eventually limit plant growth, and the rising levels of oxygen would increase the frequency of fire-storms because damp plant matter could burn. Both these effects return carbon dioxide to the atmosphere, reversing the \"snowball\" effect and forcing greenhouse warming, with CO levels rising to 300 ppm in the following Permian period. Over a longer period the evolution of termites, whose stomachs provided an anoxic environment for methanogenic lignin-digesting bacteria, prevented further burial of carbon, returning carbon to the air as the greenhouse gas methane.\n\nOnce these factors brought a halt and a small reversal in the spread of ice sheets, the lower planetary albedo resulting from the fall in size of the glaciated areas would have been enough for warmer summers and winters and thus limit the depth of snowfields in areas from which the glaciers expanded. Rising sea levels produced by global warming drowned the large areas of flatland where previously anoxic swamps assisted in burial and removal of carbon (as coal). With a smaller area for deposition of carbon, more carbon dioxide was returned to the atmosphere, further warming the planet. By 250 Mya, planet Earth had returned to a percentage of oxygen similar to that found today.\n\nThe rising levels of oxygen in the Karoo Ice Age had major effects upon evolution of plants and animals. Higher oxygen concentration (and accompanying higher atmospheric pressure) enabled energetic metabolic processes which encouraged evolution of large land-dwelling vertebrates and flight, with the dragonfly-like \"Meganeura\", an aerial predator, with a wingspan of 60 to 75 cm.\n\nThe herbivorous stocky-bodied and armoured millipede-like \"Arthropleura\" was 1.8 m long, and the semiterrestrial Hibbertopterid eurypterids were perhaps as large, and some scorpions reached 50 or 70 cm.\n\nThe rising levels of oxygen also led to the evolution of greater fire resistance in vegetation and ultimately to the evolution of flowering plants.\n\nIn addition, the Karoo Ice Age has unique sedimentary sequences called cyclothems. These were produced by the repeated alterations of marine and nonmarine environments.\n\n"}
{"id": "2309752", "url": "https://en.wikipedia.org/wiki?curid=2309752", "title": "Komatiite", "text": "Komatiite\n\nKomatiite () is a type of ultramafic mantle-derived volcanic rock. Komatiites have low silicon, potassium and aluminium, and high to extremely high magnesium content. Komatiite was named for its type locality along the Komati River in South Africa.\n\nTrue komatiites are very rare and essentially restricted to rocks of Archaean age, with few Proterozoic or Phanerozoic komatiites known (although high-magnesian lamprophyres are known from the Mesozoic). This restriction in age is thought to be due to cooling of the mantle, which may have been up to 500 °C hotter during the early to middle Archaean (3.8 to 2.8 billion years ago). The early Earth had much higher heat production, due to the residual heat from planetary accretion, as well as the greater abundance of radioactive elements.\n\nGeographically, komatiites are restricted in distribution to the Archaean shield areas. Komatiites occur with other ultramafic and high-magnesian mafic volcanic rocks in Archaean greenstone belts. The youngest komatiites are from the island of Gorgona on the Caribbean oceanic plateau off the Pacific coast of Colombia.\n\nMagmas of komatiitic compositions have a very high melting point, with calculated eruption temperatures in excess of 1600 °C. Basaltic lavas normally have eruption temperatures of about 1100 to 1250 °C. The higher melting temperatures required to produce komatiite have been attributed to the presumed higher geothermal gradients in the Archaean Earth.\n\nKomatiitic lava was extremely fluid when it erupted (possessing the viscosity close to that of water but with the density of rock). Compared to the basaltic lava of the Hawaiian plume basalts at ~1200 °C, which flows the way treacle or honey does, the komatiitic lava would have flowed swiftly across the surface, leaving extremely thin lava flows (down to 10 mm thick). The major komatiitic sequences preserved in Archaean rocks are thus considered to be lava tubes, ponds of lava etc., where the komatiitic lava accumulated.\n\nKomatiite chemistry is different from that of basaltic and other common mantle-produced magmas, because of differences in degrees of partial melting. Komatiites are considered to have been formed by high degrees of partial melting, usually greater than 50%, and hence have high MgO with low KO and other incompatible elements. \n\nThere are two geochemical classes of komatiite; aluminium undepleted komatiite (AUDK) (also known as Group I komatiites) and aluminium depleted komatiite (ADK) (also known as Group II komatiites), defined by their AlO/TiO ratios. These two classes of komatiite are often assumed to represent a real petrological source difference between the two types related to depth of melt generation. Al-depleted komatiites have been modeled by melting experiments as being produced by high degrees of partial melting at high pressure where garnet in the source is not melted, whereas Al-undepleted komatiites are produced by high degrees of partial melts at lesser depth. However, recent studies of fluid inclusions in chrome spinels from the cumulate zones of komatiite flows have shown that a single komatiite flow can be derived from the mixing of parental magmas with a range of AlO/TiO ratios, calling into question this interpretation of the formations of the different komatiite groups. Komatiites probably form in extremely hot mantle plumes.\n\nBoninite magmatism is similar to komatiite magmatism but is produced by fluid-fluxed melting above a subduction zone. Boninites with 10–18% MgO tend to have higher large-ion lithophile elements (LILE: Ba, Rb, Sr) than komatiites.\n\nThe pristine volcanic mineralogy of komatiites is composed of forsteritic olivine (Fo90 and upwards), calcic and often chromian pyroxene, anorthite (An85 and upwards) and chromite.\n\nA considerable population of komatiite examples show a cumulate texture and morphology. The usual cumulate mineralogy is highly magnesium rich forsterite olivine, though chromian pyroxene cumulates are also possible (though rarer).\n\nVolcanic rocks rich in magnesium may be produced by accumulation of olivine phenocrysts in basalt melts of normal chemistry: an example is picrite. Part of the evidence that komatiites are not magnesium-rich simply because of cumulate olivine is textural: some contain spinifex texture, a texture attributable to rapid crystallization of the olivine in a thermal gradient in the upper part of a lava flow. \"Spinifex\" texture is named after the common name for the Australian grass \"Triodia\", which grows in clumps with similar shapes.\n\nAnother line of evidence is that the MgO content of olivines formed in komatiites is toward the nearly pure MgO forsterite composition, which can only be achieved in bulk by crystallisation of olivine from a highly magnesian melt.\n\nThe often rarely preserved flow top breccia and pillow margin zones in some komatiite flows are essentially volcanic glass, quenched in contact with overlying water or air. Because they are rapidly cooled, they represent the liquid composition of the komatiites, and thus record an anhydrous MgO content of up to 32% MgO. Some of the highest magnesian komatiites with clear textural preservation are those of the Barberton belt in South Africa, where liquids with up to 34% MgO can be inferred using bulk rock and olivine compositions.\n\nThe mineralogy of a komatiite varies systematically through the typical stratigraphic section of a komatiite flow and reflects magmatic processes which komatiites are susceptible to during their eruption and cooling. The typical mineralogical variation is from a flow base composed of olivine cumulate, to a spinifex textured zone composed of bladed olivine and ideally a pyroxene spinifex zone and olivine-rich chill zone on the upper eruptive rind of the flow unit. \n\nPrimary (magmatic) mineral species also encountered in komatiites include olivine, the pyroxenes augite, pigeonite and bronzite, plagioclase, chromite, ilmenite and rarely pargasitic amphibole. Secondary (metamorphic) minerals include serpentine, chlorite, amphibole, sodic plagioclase, quartz, iron oxides and rarely phlogopite, baddeleyite, and pyrope or hydrogrossular garnet.\n\nAll known komatiites have been metamorphosed, therefore should technically be termed 'metakomatiite' though the prefix meta is inevitably assumed. Many komatiites are highly altered and serpentinized or carbonated from metamorphism and metasomatism. This results in significant changes to the mineralogy and the texture.\n\nThe metamorphic mineralogy of ultramafic rocks, particularly komatiites, is only partially controlled by composition. The character of the connate fluids which are present during low temperature metamorphism whether prograde or retrograde control the metamorphic assemblage of a metakomatiite (\"hereafter the prefix meta- is assumed\").\n\nThe factor controlling the mineral assemblage is the partial pressure of carbon dioxide within the metamorphic fluid, called the XCO. If XCO is above 0.5, the metamorphic reactions favor formation of talc, magnesite (magnesium carbonate), and tremolite amphibole. These are classed as talc-carbonation reactions. Below XCO of 0.5, metamorphic reactions in the presence of water favor production of serpentinite.\n\nThere are thus two main classes of metamorphic komatiite; carbonated and hydrated. Carbonated komatiites and peridotites form a series of rocks dominated by the minerals chlorite, talc, magnesite or dolomite and tremolite. Hydrated metamorphic rock assemblages are dominated by the minerals chlorite, serpentine-antigorite, brucite. Traces of talc, tremolite and dolomite may be present, as it is very rare that no carbon dioxide is present in metamorphic fluids. At higher metamorphic grades, anthophyllite, enstatite, olivine and diopside dominate as the rock mass dehydrates.\n\nKomatiite tends to fractionate from high-magnesium compositions in the flow bases where olivine cumulates dominate, to lower magnesium compositions higher up in the flow. Thus, the current metamorphic mineralogy of a komatiite will reflect the chemistry, which in turn represents an inference as to its volcanological facies and stratigraphic position.\n\nTypical metamorphic mineralogy is tremolite-chlorite, or talc-chlorite mineralogy in the upper spinifex zones. The more magnesian-rich olivine-rich flow base facies tend to be free from tremolite and chlorite mineralogy and are dominated by either serpentine-brucite +/- anthophyllite if hydrated, or talc-magnesite if carbonated. The upper flow facies tend to be dominated by talc, chlorite, tremolite, and other magnesian amphiboles (anthophyllite, cummingtonite, gedrite, etc.).\n\nFor example, the typical flow facies (see below) may have the following mineralogy;\nKomatiite can be classified according to the following geochemical criteria;\n\nThe above geochemical classification must be the essentially unaltered magma chemistry and not the result of crystal accumulation (as in peridotite). Through a typical komatiite flow sequence the chemistry of the rock will change according to the internal fractionation which occurs during eruption. This tends to lower MgO, Cr, Ni, and increase Al, KO, Na, CaO and SiO toward the top of the flow.\n\nRocks rich in MgO, KO, Ba, Cs, and Rb may be lamprophyres, kimberlites or other rare ultramafic, potassic or ultrapotassic rocks.\n\nKomatiites often show pillow lava structure, autobrecciated upper margins consistent with underwater eruption forming a rigid upper skin to the lava flows. Proximal volcanic facies are thinner and interleaved with sulfidic sediments, black shales, cherts and tholeiitic basalts. Komatiites were produced from a relatively wet mantle. Evidence of this is from their association with felsics, occurrences of komatiitic tuffs, Niobium anomalies and by S- and HO-borne rich mineralizations.\n\nA common and distinctive texture is known as \"spinifex texture\" and consists of long acicular phenocrysts of olivine (or pseudomorphs of alteration minerals after olivine) or pyroxene which give the rock a bladed appearance especially on a weathered surface. Spinifex texture is the result of rapid crystallization of highly magnesian liquid in the thermal gradient at the margin of the flow or sill..\n\n\"Harrisite texture\", first described from the locality of Harris Bay, Rùm, Scotland, is formed by nucleation of crystals on the floor of the lava flow chamber. Harrisites are known to form megacrystal aggregates of pyroxene and olivine up to 1 metre in length.\n\nKomatiite volcano morphology is interpreted to have the general form and structure of a shield volcano, typical of most large basalt edifices, as the magmatic event which forms komatiites erupts less magnesian materials.\n\nHowever, the initial flux of the most magnesian magmas is interpreted to form a channelised flow facie, which is envisioned as a fissure vent releasing highly fluid komatiitic lava onto the surface. This then flows outwards from the vent fissure, concentrating into topographical lows, and forming channel environments composed of high MgO olivine adcumulate flanked by a 'sheeted flow facies' aprons of lower MgO olivine and pyroxene thin-flow spinifex sheets.\n\nThe typical komatiite lava flow has six stratigraphically related elements;\nIndividual flow units may not be entirely preserved, as subsequent flow units may thermally erode the A zone spinifex flows. In the distal thin flow facies,\nB zones are poorly developed to absent, as not enough through-flowing liquid existed to grow the adcumulate.\n\nThe channel and sheeted flows are then covered by high-magnesian basalts and tholeiitic basalts as the volcanic event evolves to less magnesian compositions. The subsequent magmatism, being higher silica melts, tends to form a more typical shield volcano architecture.\n\nKomatiite magma is extremely dense and unlikely to reach the surface, being more likely to pool lower within the crust. Modern (post-2004) interpretations of some of the larger olivine adcumulate bodies in the Yilgarn craton have revealed that the majority of komatiite olivine adcumulate occurrences are likely to be subvolcanic to intrusive in nature.\n\nThis is recognised at the Mt Keith nickel deposit where wall-rock intrusive textures and xenoliths of felsic country rocks have been recognised within the low-strain contacts. The previous interpretations of these large komatiite bodies was that they were \"super channels\" or reactivated channels, which grew to over 500 m in stratigraphic thickness during prolonged volcanism.\n\nThese intrusions are considered to be channelised sills, formed by injection of komatiitic magma into the stratigraphy, and inflation of the magma chamber. Economic nickel-mineralised olivine adcumulate bodies may represent a form of sill-like conduit, where magma pools in a staging chamber before erupting onto the surface.\n\nThe economic importance of komatiite was first widely recognised in the early 1960s with the discovery of massive nickel sulfide mineralisation at Kambalda, Western Australia. Komatiite-hosted nickel-copper sulfide mineralisation today accounts for about 14% of the world's nickel production, mostly from Australia, Canada and South Africa.\n\nKomatiites are associated with nickel and gold deposits in Australia, Canada, South Africa and most recently in the Guiana shield of South America.\n\n\n\n"}
{"id": "2670699", "url": "https://en.wikipedia.org/wiki?curid=2670699", "title": "Lambda Sculptoris", "text": "Lambda Sculptoris\n\nThe Bayer designation Lambda Sculptoris (λ Scl, λ Sculptoris) is shared by two star systems, λ¹ Sculptoris and λ² Sculptoris, in the constellation Sculptor. They are separated by 0.29° on the sky.\n\n"}
{"id": "708218", "url": "https://en.wikipedia.org/wiki?curid=708218", "title": "Late Jurassic", "text": "Late Jurassic\n\nThe Late Jurassic is the third epoch of the Jurassic period, and it spans the geologic time from 163.5 ± 1.0 to 145.0 ± 0.8 million years ago (Ma), which is preserved in Upper Jurassic strata. In European lithostratigraphy, the name \"malm\" indicates rocks of Late Jurassic age. In the past, this name was also used to indicate the unit of geological time, but this usage is now discouraged to make a clear distinction between lithostratigraphic and geochronologic/chronostratigraphic units.\n\nThe Late Jurassic is divided into three ages, which correspond with the three (faunal) stages of Upper Jurassic rock:\n\nDuring the Late Jurassic epoch, Pangaea broke up into two supercontinents, Laurasia to the north, and Gondwana to the south. The result of this break-up was the spawning of the Atlantic Ocean. However, at this time, the Atlantic Ocean was relatively narrow.\n\nThis epoch is well known for many famous types of dinosaurs, such as the sauropods, the theropods, the thyreophorans, and the ornithopods. Other animals, such as crocodiles and the first birds, appeared in the Jurassic. Listed here are only a few of the many Jurassic animals:\n\n"}
{"id": "42912852", "url": "https://en.wikipedia.org/wiki?curid=42912852", "title": "List of Caloplaca species", "text": "List of Caloplaca species\n\nA B C D E F G H I J K L M N O P Q R S T U V U W X Y Z\n\nThis is an incomplete list of species in the lichen genus \"Caloplaca\". A 2008 estimate suggests there are about 510 species.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "56357508", "url": "https://en.wikipedia.org/wiki?curid=56357508", "title": "List of Eragrostis species", "text": "List of Eragrostis species\n\n, the World Checklist of Selected Plant Families accepted the following species of \"Eragrostis\".\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "39608150", "url": "https://en.wikipedia.org/wiki?curid=39608150", "title": "List of European countries by coal production", "text": "List of European countries by coal production\n\nThe following is a list of European countries by coal production in 2014, based mostly on the Statistical Review of World Energy published in 2015 by British Petroleum, ranking nations with coal production larger than 0.05 percent of world production. Amounts are expressed in tonnes of oil equivalent.\n"}
{"id": "3257031", "url": "https://en.wikipedia.org/wiki?curid=3257031", "title": "List of Michigan rivers named Black River", "text": "List of Michigan rivers named Black River\n\nBlack River may refer to any of seven streams in the U.S. state of Michigan:\n\n\n"}
{"id": "9013721", "url": "https://en.wikipedia.org/wiki?curid=9013721", "title": "List of cassava diseases", "text": "List of cassava diseases\n\nThis article is a list of diseases of cassava (\"Manihot esculenta\").\n"}
{"id": "54724218", "url": "https://en.wikipedia.org/wiki?curid=54724218", "title": "List of ecoregions in Argentina", "text": "List of ecoregions in Argentina\n\nTemperate broadleaf and mixed forests\n\nTemperate grasslands, savannas, and shrublands\n\nTropical and subtropical dry broadleaf forests\n\nMontane grasslands and shrublands\n\nTropical and subtropical grasslands, savannas, and shrublands\n\nTropical and subtropical moist broadleaf forests\n\nFlooded grasslands and savannas\n"}
{"id": "11752", "url": "https://en.wikipedia.org/wiki?curid=11752", "title": "List of freshwater aquarium invertebrate species", "text": "List of freshwater aquarium invertebrate species\n\nThis is a list of invertebrates, animals without a backbone, that are commonly kept in freshwater aquaria by hobby aquarists. Numerous shrimp species of various kinds, crayfish, a number of freshwater snail species, and at least one freshwater clam species are found in freshwater aquaria.\n\n\n\n\n\n\n\n"}
{"id": "10077307", "url": "https://en.wikipedia.org/wiki?curid=10077307", "title": "List of fungal orders", "text": "List of fungal orders\n\nThis article lists the orders of the Fungi. and \"The Mycota: A Comprehensive Treatise on Fungi as Experimental Systems for Basic and Applied Research\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "51134805", "url": "https://en.wikipedia.org/wiki?curid=51134805", "title": "List of least concern plants", "text": "List of least concern plants\n\nAs of September 2016, the International Union for Conservation of Nature (IUCN) lists 6645 least concern plant species. 30% of all evaluated plant species are listed as least concern. \nThe IUCN also lists 131 subspecies and 118 varieties as least concern. No subpopulations of plants have been evaluated by the IUCN.\n\nThis is a complete list of least concern plant species, subspecies and varieties evaluated by the IUCN.\nThere are 14 bryophyte species assessed as least concern.\nThere are 113 species, three subspecies, and one variety of pteridophyte assessed as least concern.\nThere are 78 species, three subspecies, and one variety in the class Polypodiopsida assessed as least concern.\n\nSpecies\n\nSubspecies\n\nVarieties\n\n\n\nThere are 419 species, 34 subspecies, and 66 varieties of gymnosperm assessed as least concern.\n\nSpecies\n\nSubspecies\n\nSpecies\n\nSubspecies\n\nVarieties\nThere are 76 species in the class Gnetopsida assessed as least concern.\nThere are 4104 species, 86 subspecies, and 44 varieties of dicotyledon assessed as least concern.\nThere are 25 species and four subspecies in Campanulales assessed as least concern.\n\nSpecies\n\nSubspecies\n\nThere are 129 species and three subspecies in Theales assessed as least concern.\n\nSpecies\n\nSubspecies\n\n\nSpecies\n\nVarieties\n\nSpecies\n\nSubspecies\n\nSpecies\n\nVarieties\n\nSpecies\n\nSubspecies\n\nSpecies\n\nSubspecies\n\nSpecies\n\nSubspecies\n\nVarieties\nThere are 37 species and one subspecies in the order Laurales assessed as least concern.\n\nSpecies\n\nSubspecies\n\nThere are 43 species and two subspecies in Ebenales assessed as least concern.\n\nSpecies\n\nSubspecies\n\n\nThere are 22 species, one subspecies, and one variety in the order Celastrales assessed as least concern.\n\nSpecies\n\nSubspecies\nVarieties\n\nThere are 149 species in the order Myrtales assessed as least concern.\n\nThere are 83 species, three subspecies, and four varieties in the order Sapindales assessed as least concern.\n\nSpecies\n\nVarieties\n\nSpecies\n\nVarieties\n\nSpecies\n\nSubspecies\nVarieties\n\nSpecies\n\nSubspecies\n\nSpecies\n\nSubspecies\n\n\nSpecies\n\nSubspecies\nVarieties\n\nThere are 118 species, 19 subspecies, and six varieties in the order Magnoliales assessed as least concern.\n\nSpecies\n\nSubspecies\nVarieties\n\nSpecies\n\nSubspecies\n\nSpecies\n\nSubspecies\n\nVarieties\nThere are 45 species in Capparales assessed as least concern.\n\n\n\nThere are 74 species and two subspecies in the order Apiales assessed as least concern.\n\nSpecies\n\nSubspecies\n\nThere are 79 species, two subspecies, and one variety in the order Gentianales assessed as least concern.\n\nSpecies\n\nSubspecies\nVarieties\n\nThere are 89 species, one subspecies, and four varieties in the order Rosales assessed as least concern.\n\nSpecies\n\nVarieties\n\nSpecies\n\nSubspecies\n\nThere are 37 species and two varieties in Primulales assessed as least concern.\n\nSpecies\n\nVarieties\n\nSpecies\n\nVarieties\n\nThere are 38 species, three subspecies, and two varieties in Urticales assessed as least concern.\n\nSpecies\n\nVarieties\n\n\n\nSpecies\n\nSubspecies\n\nVarieties\n\nSpecies\n\nVarieties\n\nThere are 48 species, one subspecies, and one variety in the order Solanales assessed as least concern.\n\nSpecies\n\nSubspecies\n\nSpecies\n\nVarieties\n\n\nThere are 277 species and 11 subspecies in the order Scrophulariales assessed as least concern.\n\nSpecies\n\nSubspecies\n\n\nSpecies\n\nSubspecies\n\nThere are 134 species, five subspecies, and two varieties in the order Lamiales assessed as least concern.\n\nSpecies\n\nSubspecies\n\nSpecies\n\nSubspecies\n\nSpecies\n\nSubspecies\n\nVarieties\n\nThere are 35 species and two subspecies in Nepenthales assessed as least concern.\n\nSpecies\n\nSubspecies\n\nThere are 51 species and one subspecies in the order Ranunculales assessed as least concern.\n\n\n\nSpecies\n\nSubspecies\n\nThere are 16 species and two varieties in the order Ericales assessed as least concern.\n\nSpecies\n\nVarieties\n\nSpecies\n\nSubspecies\n\nVarieties\nThere are 928 species and two subspecies in the order Caryophyllales assessed as least concern.\n\nSpecies\n\nSubspecies\n\n\nSpecies\n\nSubspecies\n\nThere are 143 species, five subspecies, and one variety in the order Fagales assessed as least concern.\n\nSpecies\n\nSubspecies\n\nVarieties\n\nSpecies\n\nVarieties\n\nThere are 1983 species, eight subspecies, and seven varieties of monocotyledon assessed as least concern.\n\nSpecies\n\nVarieties\n\nThere are 261 species in Orchidales assessed as least concern.\n\nSpecies\n\nVarieties\n\nThere are 225 species and five subspecies in the order Liliales assessed as least concern.\n\nSpecies\n\nSubspecies\n\nSpecies\n\nSubspecies\n\nSpecies\n\nSubspecies\n\nSpecies\n\nSubspecies\n\nThere are 103 species and one subspecies in Arales assessed as least concern.\n\nSpecies\n\nSubspecies\n\n\nThere are 67 species and two varieties in the order Zingiberales assessed as least concern.\n\nSpecies\n\nVarieties\n\nSpecies\n\nVarieties\n\nThere are 39 species in the order Commelinales assessed as least concern.\nThere are 825 species and two varieties in Cyperales assessed as least concern.\n\nSpecies\n\nVarieties\n\nThere are 27 species in the order Alismatales assessed as least concern.\n\n\nThere are 96 species in Najadales assessed as least concern.\n\nSpecies\n\nSubspecies\n\n\n"}
{"id": "58694823", "url": "https://en.wikipedia.org/wiki?curid=58694823", "title": "List of mountains of the British Isles by height", "text": "List of mountains of the British Isles by height\n\nThis is a list of mountains in Britain and Ireland by height and by prominence. Height and prominence are the most important metrics for the classifications of mountains by the UIAA; with isolation a distant third criterion. The list is sourced from the \"Database of British and Irish Hills\" (\"DoBIH\") for peaks that meet the consensus height threshold for a mountain, namely ; the list also rules out peaks with a prominence below , and thus, the list is therefore precisely a list of the 2,754 Simms in the British Isles (as at October 2018). Many classifications of mountains in the British Isles consider a prominence between as being a \"top\", and not a mountain; however, using the prominence threshold gives the broadest possible list of mountains. For a ranking of mountains with a higher prominence threshold use:\n\n\n, 6,414 people had climbed all 282 Scottish Munros, over 270 people had climbed all 1,557 Marilyns of Great Britain, while only 2 people had completed the 2,529 Simms of Great Britain, Ken Whyte and Iain Thow, and no one had yet completed all 2,754 Simms of the British Isles.\n\nDespite using the lower threshold for prominence of , the UIAA threshold for an \"independent\" peak, one Scottish Munro is missing, namely Maoile Lunndaidh whose official prominence changed to in 2014; , the list of 2,754 British Isles Simms contained:\n\nThis list was downloaded from the DoBIH in October 2018, and excludes all Britain and Ireland peaks with a prominence below . Note that topological prominence, is complex to measure and requires a survey of the entire contours of a peak, rather than a single point of height. These tables are therefore subject to being revised over time, and should not be amended or updated unless the entire DoBIH data is re-downloaded again. The default table ranking is by height, so where the table is sorted by for example Region, the table will list the mountains within each Region by order of height.\n\nThe DoBIH uses the following codes for the various classifications of mountains and hills in the British Isles, which many of the above peaks also fall into:\n<br>\n\nsuffixes:<br>\n= twin\n\n\n"}
{"id": "2385136", "url": "https://en.wikipedia.org/wiki?curid=2385136", "title": "Louise de Kiriline Lawrence", "text": "Louise de Kiriline Lawrence\n\nLouise de Kiriline Lawrence (née Flach; January 30, 1894 – April 27, 1992) was an internationally renowned naturalist, author and nurse. She was the most prolific contributor to the National Audubon Society magazine \"Audubon\".\n\nLouise de Kiriline Lawrence was born on January 30, 1894, in Sweden.\n\nDe Kiriline Lawrence trained as a nurse and was employed by the Danish Red Cross during World War I. She met a Russian officer, Lt. Greb de Kiriline, in Denmark, and married him in 1918. He returned to Russia to fight in the Russian Civil War, and she followed him there. Greb de Kiriline disappeared in Siberia where, unknown to his wife, he was shot. She worked as a nurse in Russia for several years while she searched for him. In 1927 de Kiriline Lawrence emigrated to Canada and continued to work as a nurse. Stationed in rural northern Ontario, she became well known as the nurse to the Dionne quintuplets during the first year of their lives.\n\nShe retired from nursing in 1935, and lived in a cabin in Northern Ontario. She met Leonard Lawrence, a carpenter, and married him in 1939. By this time she had begun a new career as an ornithologist and nature writer. She is recognized for her study of the red-eyed vireo, which identified the songbird as capable of producing 22,197 distinct calls in a single day. De Kiriline Lawrence carried out the majority of her scientific work on her property, located outside of North Bay, Ontario.\n\nShe was an Elective Member of the American Ornithologists' Union and received an Honorary LL.D. from Laurentian University in 1970.\n\n\n\n\n"}
{"id": "30139095", "url": "https://en.wikipedia.org/wiki?curid=30139095", "title": "Mann Eddy", "text": "Mann Eddy\n\nThe Mann Eddy is a very small feature of ocean currents in the Atlantic. It is a persistent clockwise circulation in the middle of the North Atlantic ocean.\n"}
{"id": "18706053", "url": "https://en.wikipedia.org/wiki?curid=18706053", "title": "Mountains of Kong", "text": "Mountains of Kong\n\nThe Mountains of Kong are a non-existent mountain range charted on maps of Africa from 1798 through to the late 1880s. The mountains were once thought to begin in West Africa near the highland source of the Niger River close to Tembakounda in Guinea, then continue east to the also fictitious Central African Mountains of the Moon, thought to be where the White Nile had its source.\n\nIn 1798 a map resulting from the explorations of the area by Mungo Park showed this west to east mountain range for the first time. It was produced by the English cartographer James Rennell who showed the Niger evaporating inland at Wangara.\n\nThe German map maker Johann Reinecke included the mountains in his map of 1804 as the \"Geburg Kong\". In 1805 the London engraver John Cary showed them for the first time linking to the Moon Mountains.\n\nVarious nineteenth century explorers of West Africa included the range on maps they produced or directed the production of after exploring the area. These include French explorer René Caillié, who explored the highlands of Guinea at Fouta Djallon, near the source of the Niger; the Cornish explorer Richard Lemon Lander and his younger brother John; and Scottish explorer Hugh Clapperton who also sought the course of the Niger River in its upper reaches.\nThe historians Thomas Basset and Phillip Porter have identified forty maps showing the mountains published between 1798 and 1892.\nDespite the failure of other later explorers to locate the range, it continued to appear on maps until late in the nineteenth century.\n\nCartographers stopped including the mountains on maps after French explorer Louis Gustave Binger established that the mountains were fictitious in his 1887–89 expedition to chart the Niger River from its mouth in the Gulf of Guinea and through Côte d'Ivoire.\n\nIn 1928 Bartholemew's Oxford Advanced Atlas still contained them in its index locating them at 8° 40' N, 5° 0' W. Even later they appeared erroneously in Goode's World Atlas of 1995.\n\nThe town of Kong, Ivory Coast dates to the 12th century and bears the name which it gave to the mountains. This rose in power during the 17th century to form the Kong Empire which eventually passed under French colonial rule.\n\nA range of hills called the Kong Hills are part of the band of high ground separating the inner plains of West Africa from the coast regions. In general the summits of the hills are below 610m (2000 ft.) and not more than 215m (700 ft.) above the level of the country.\n"}
{"id": "26078437", "url": "https://en.wikipedia.org/wiki?curid=26078437", "title": "MultiSpeak", "text": "MultiSpeak\n\nMultiSpeak is a specification / standard that defines standardized interfaces among software applications commonly used by electric utilities. It defines details of data that need to be exchanged between software applications in order to support different processes commonly applied at utilities. MultiSpeak effort is funded by National Rural Electric Cooperative Association (NRECA).\n\nThe National Institute of Standards and Technology (NIST) has developed a Smart Grid Conceptual Reference Model as part of its Smart Grid Standards Framework and Roadmap. NIST has identified 42 standards to support this vision. MultiSpeak was chosen by NIST as a key standard in the Operations area of the NIST Conceptual Model.\n\nThe MultiSpeak Specification is the most widely applied de facto integration standard in North America pertaining to distribution utilities. It is currently in use in daily operations of more than 600 electric cooperatives, investor-owned utilities, municipals, and public power districts in at least 15 different countries. Over 80 software vendors active in the utility industry have joined the MultiSpeak initiative and contribute their experience to refining the standard.\n\nMultiSpeak standard makes use of three components.\n\n\nIn June 2008 MultiSpeak and WG14 announced an initiative to establish two sets of standards that will lead towards harmonization of their respective specifications. After completion of the same, will provide a mapping between MultiSpeak Version 4.0, IEC 61970 Version 13 and IEC 61968 Version 10. Two sets of standard work planned for the same are listed below\n\n"}
{"id": "49044859", "url": "https://en.wikipedia.org/wiki?curid=49044859", "title": "Ore-pine", "text": "Ore-pine\n\nOre-pine (; ; ; ) is a cured pinewood used extensively in the Middle Ages in the construction of Scandinavian stave churches. Ore-pine is the heartwood of prepared old-growth mountain pines; the trees had their branches removed and were left to stand, the tree resins bleeding upward and out through the cut branches and thus making the heartwood more resinous. The resultant ore-pine is much more resistant to rot and decay, as evidenced by stave churches surviving from the 12th and 13th centuries.\n"}
{"id": "24106690", "url": "https://en.wikipedia.org/wiki?curid=24106690", "title": "Pridoli epoch", "text": "Pridoli epoch\n\nIn the geologic timescale, the Pridoli epoch of the Silurian period of the Paleozoic era of the Phanerozoic eon is comprehended between 423 ± 1.5 and 419.2 ± 2.8 mya (million years ago), approximately. The Pridoli epoch succeeds the Ludfordian age and precedes the Lochkovian age of the Devonian. It is named after one locality at the \"Homolka a Přídolí\" nature reserve near the Prague suburb Slivenec in the Czech Republic. Přídolí is the old name of a cadastral field area.\n"}
{"id": "81755", "url": "https://en.wikipedia.org/wiki?curid=81755", "title": "Psamathe (Nereid)", "text": "Psamathe (Nereid)\n\nPsamathe (Ancient Greek: Ψάμαθη, from ψάμαθος \"sand of the sea-shore\") was a Nereid in Greek mythology, i.e., one of the fifty daughters of Nereus and Doris. The goddess of sand beaches, Psamathe was the wife of Proteus and the mother of Phocus by Aeacus.\n\nSome translations of Ovid have the name as Psamanthe.\n\nPsamathe has a moon named after her.\n"}
{"id": "50390340", "url": "https://en.wikipedia.org/wiki?curid=50390340", "title": "Residence time", "text": "Residence time\n\nFor material flowing through a volume, the residence time is a measure of how much time the matter spends in it. Examples include fluids in a chemical reactor, specific elements in a geochemical reservoir, water in a catchment, bacteria in a culture vessel and drugs in human body. A molecule or small parcel of fluid has a single residence time, but more complex systems have a residence time distribution (RTD).\n\nThere are at least three time constants that are used to represent a residence time distribution. The \"turn-over time\" or \"flushing time\" is the ratio of the material in the volume to the rate at which it passes through; the \"mean age\" is the mean length of time the material in the reservoir has spent there; and the \"mean transit time\" is the mean length of time the material spends in the reservoir.\n\nApplications of residence times or residence time distributions can be found in a wide variety of disciplines including environmental science, engineering, chemistry, and hydrology.\n\nThe concept of residence time originated in models of chemical reactors. The first such model was an \"axial dispersion model\" by Irving Langmuir in 1908. This received little attention for 45 years; other models were developed such as the plug flow reactor model and the continuous stirred-tank reactor, and the concept of a \"washout function\" (representing the response to a sudden change in the input) was introduced. Then, in 1953, Peter Danckwerts resurrected the axial dispersion model and formulated the modern concept of residence time.\n\nBasic residence time theory treats a system with an input and an output, both of which have flow only in one direction. The system is homogeneous and the substance that is flowing through is conserved (neither created nor destroyed). A small particle entering the system will eventually leave, and the time spent there is its residence time. In a particularly simple model of flow, plug flow, particles that enter at the same time continue to move at the same rate and leave together. In this case, there is only one residence time. Generally, though, their rates vary and there is a distribution of exit times. One measure of this is the \"washout function\" formula_1, the fraction of particles leaving the system after having been there for a time formula_2 or greater. Its complement, formula_3, is the \"cumulative distribution function\". The \"differential distribution\", also known as the \"residence time distribution\" or \"exit age distribution\", is given by\nThis has the properties of a probability distribution: it is always nonnegative and\n\nOne can also define a density function based on the flux (mass per unit time) out of the system. The \"transit time\" function is the fraction of particles leaving the system that have been in it for up to a given time. It is the integral of a distribution formula_6. If, in a steady state, the mass in the system is formula_7 and the outgoing flux is formula_8, the distributions are related by\nAs an illustration, for a human population to be in a steady state, the deaths per year of people older than formula_2 years (the left hand side of the equation) must be balanced by the number of people per year reaching age formula_2 (the right hand side).\n\n\"Residence time\" can be a synonym for more than one constant used to represent the distribution.\n\nSome statistical properties of the residence time distribution are frequently used. The mean residence time, or \"mean age\", is given by the first moment of the residence time distribution:\nand the variance is given by\nor by the dimensionless form formula_14.\n\nThe \"mean transit time\" is the first moment of the transit time distribution:\n\nThe \"turnover time\", also known as the \"flushing time\", is simply the ratio of mass to flux:\nWhen applied to liquids, it is also known as the \"hydraulic retention time\" (\"HRT\"), \"hydraulic residence time\" or \"hydraulic detention time\".\n\nIt can be shown that, in a steady state, the mean transit time and flushing time are equal (formula_17).\n\nThe relationship between formula_18 or formula_19 and formula_20 is determined by the type of distribution:\n\nIn an ideal plug flow reactor the fluid elements leave in the same order they arrived, not mixing with those in front and behind. Therefore, fluid entering at time formula_2 will exit at time formula_27, where formula_20 is the residence time. The fraction leaving is a step function, going from 0 to 1 at time formula_20. The distribution function is therefore a Dirac delta function at formula_20.\nThe mean is formula_20 and the variance is zero.\n\nThe RTD of a real reactor deviates from that of an ideal reactor, depending on the hydrodynamics within the vessel. A non-zero variance indicates that there is some dispersion along the path of the fluid, which may be attributed to turbulence, a non-uniform velocity profile, or diffusion. If the mean of the formula_23 curve arrives earlier than the expected time formula_20 it indicates that there is stagnant fluid within the vessel. If the RTD curve shows more than one main peak it may indicate channeling, parallel paths to the exit, or strong internal circulation.\n\nIn an ideal continuous stirred-tank reactor (CSTR), the flow at the inlet is completely and instantly mixed into the bulk of the reactor. The reactor and the outlet fluid have identical, homogeneous compositions at all times. The residence time distribution is exponential:\nThe mean is formula_20 and the variance is 1. A notable difference from the plug flow reactor is that material introduced into the system will never completely leave it.\n\nIn reality, it is impossible to obtain such rapid mixing, especially on industrial scales where reactor vessels may range between 1 and thousands of cubic meters, and hence the RTD of a real reactor will deviate from the ideal exponential decay. For example, there will be some finite delay before formula_23 reaches its maximum value and the length of the delay will reflect the rate of mass transfer within the reactor. Just as was noted for a plug-flow reactor, an early mean will indicate some stagnant fluid within the vessel, while the presence of multiple peaks could indicate channeling, parallel paths to the exit, or strong internal circulation. Short-circuiting fluid within the reactor would appear in an RTD curve as a small pulse of concentrated tracer that reaches the outlet shortly after injection.\n\nIn a laminar flow reactor, the fluid flows through a long tube or parallel plate reactor and the flow is in layers parallel to the walls of the tube. The velocity of the flow is a parabolic function of radius. In the absence of molecular diffusion, the RTD is\nand\nThe variance is infinite. In a real system, diffusion will eventually mix the layers so that the tail of the RTD becomes exponential and the variance finite; but laminar flow reactors can have variance greater than 1, the maximum for CTSD reactors.\n\nResidence time distributions are measured by introducing a non-reactive tracer into the system at the inlet. Its input concentration is changed according to a known function and the output concentration measured. The tracer should not modify the physical characteristics of the fluid (equal density, equal viscosity) or the hydrodynamic conditions and it should be easily detectable.\nIn general, the change in tracer concentration will either be a \"pulse\" or a \"step\". Other functions are possible, but they require more calculations to deconvolute the RTD curve.\n\nThis method required the introduction of a very small volume of concentrated tracer at the inlet of the reactor, such that it approaches the Dirac delta function. Although an infinitely short injection cannot be produced, it can be made much smaller than the mean residence time of the vessel. If a mass of tracer, formula_40, is introduced into a vessel of volume formula_41 and an expected residence \ntime of formula_20, the resulting curve of formula_43 can be transformed into a dimensionless residence time distribution curve by the following relation:\n\nThe concentration of tracer in a step experiment at the reactor inlet changes abruptly from 0 to formula_45. The concentration of tracer at the outlet is measured and normalized to the concentration formula_45 to obtain the non-dimensional curve formula_47 which goes from 0 to 1:\n\nThe step- and pulse-responses of a reactor are related by the following:\n\nA step experiment is often easier to perform than a pulse experiment, but it tends to smooth over some of the details that a pulse response could show. It is easy to numerically integrate an experimental pulse response to obtain a very high-quality estimate of the step response, but the reverse is not the case because any noise in the concentration measurement will be amplified by numeric differentiation.\n\nIn chemical reactors, the goal is to make components react with a high yield. In a homogeneous, first-order reaction, the probability that an atom or molecule will react depends only on its residence time:\nfor a rate constant formula_51. Given a RTD, the average probability is equal to the ratio of the concentration formula_52 of the component before and after:\n\nIf the reaction is more complicated, then the output is not uniquely determined by the RTD. It also depends on the degree of \"micromixing\", the mixing between molecules that entered at different times. If there is no mixing, the system is said to be \"completely segregated\", and the output can be given in the form\nFor given RTD, there is an upper limit on the amount of mixing that can occur, called the \"maximum mixedness\", and this determines the achievable yield. A continuous stirred-tank reactor can be anywhere in the spectrum between completely segregated and perfect mixing.\n\nHydraulic residence time (HRT) is an important factor in the transport of environmental toxins or other chemicals through groundwater. The amount of time that a pollutant spends traveling through a delineated subsurface space is related to the saturation and the hydraulic conductivity of the soil or rock. Porosity is another significant contributing factor to the mobility of water through the ground (e.g. toward the water table). The intersection between pore density and size determines the degree or magnitude of the flow rate through the media. This idea can be illustrated by a comparison of the ways water moves through clay versus gravel. The retention time through a specified vertical distance in clay will be longer than through the same distance in gravel, even though they are both characterized as high porosity materials. This is because the pore sizes are much larger in gravel media than in clay, and so there is less hydrostatic tension working against the subsurface pressure gradient and gravity.\n\nGroundwater flow is important parameter for consideration in the design of waste rock basins for mining operations. Waste rock is heterogeneous material with particles varying from boulders to clay-sized particles, and it contains sulfidic pollutants which must be controlled such that they do not compromise the quality of the water table and also so the runoff does not create environmental problems in the surrounding areas. Aquitards are clay zones that can have such a degree of impermeability that they partially or completely retard water flow. These clay lenses can slow or stop seepage into the water table, although if an aquitard is fractured and contaminated then it can become a long-term source of groundwater contamination due to its low permeability and high HRT.\n\nPrimary treatment for wastewater or drinking water includes settling in a sedimentation chamber to remove as much of the solid matter as possible before applying additional treatments. The amount removed is controlled by the hydraulic residence time (HRT). When water flows through a volume at a slower rate, less energy is available to keep solid particles entrained in the stream and there is more time for them to settle to the bottom. Typical HRTs for sedimentation basins are around two hours, although some groups recommend longer times to remove micropollutants such as pharmaceuticals and hormones.\n\nDisinfection is the last step in the tertiary treatment of wastewater or drinking water. The types of pathogens that occur in untreated water include those that are easily killed like bacteria and viruses, and those that are more robust such as protozoa and cysts. The disinfection chamber must have a long enough HRT to kill or deactivate all of them.\n\nAtoms and molecules of gas or liquid can be trapped on a solid surface in a process called adsorption. This is an exothermic process involving a release of heat, and heating the surface increases the probability that an atom will escape within a given time. At a given temperature formula_55, the residence time of an adsorbed atom is given by \nwhere formula_57 is the gas constant, formula_58 is an activation energy, and formula_59 is a prefactor that is correlated with the vibration times of the surface atoms (generally of the order of formula_60 seconds).\n\nIn vacuum technology, the residence time of gases on the surfaces of a vacuum chamber can determine the pressure due to outgassing. If the chamber can be heated, the above equation shows that the gases can be \"baked out\"; but if not, then surfaces with a low residence time are needed to achieve ultra-high vacuums.\n\nIn environmental terms, the residence time definition is adapted to fit with ground water, the atmosphere, glaciers, lakes, streams, and oceans. More specifically it is the time during which water remains within an aquifer, lake, river, or other water body before continuing around the hydrological cycle. The time involved may vary from days for shallow gravel aquifers to millions of years for deep aquifers with very low values for hydraulic conductivity. Residence times of water in rivers are a few days, while in large lakes residence time ranges up to several decades. Residence times of continental ice sheets is hundreds of thousands of years, of small glaciers a few decades.\n\nGround water residence time applications are useful for determining the amount of time it will take for a pollutant to reach and contaminate a ground water drinking water source and at what concentration it will arrive. This can also work to the opposite effect to determine how long until a ground water source becomes uncontaminated via inflow, outflow, and volume. The residence time of lakes and streams is important as well to determine the concentration of pollutants in a lake and how this may affect the local population and marine life.\n\nHydrology, the study of water, discusses the water budget in terms of residence time. The amount of time that water spends in each different stage of life (glacier, atmosphere, ocean, lake, stream, river), is used to show the relation of all of the water on the earth and how it relates in its different forms.\n\nA large class of drugs are enzyme inhibitors that bind to enzymes in the body and inhibit their activity. In this case it is the drug-target residence time (the length of time the drug stays bound to the target) that is of interest. Drugs with long residence times are desirable because they remain effective for longer and therefore can be used in lower doses. This residence time is determined by the kinetics of the interaction and is proportional to the half life of the chemical dissociation. One way to measure the residence time is in a \"preincubation-dilution\" experiment where a target enzyme is incubated with the inhibitor, allowed to approach equilibrium, then rapidly diluted. The amount of product is measured and compared to a control in which no inhibitor is added.\n\nResidence time can also refer to the amount of time that a drug spends in the part of the body where it needs to be absorbed. The longer the residence time, the more of it can be absorbed. If the drug is delivered in an oral form and destined for the upper intestines, it usually moves with food and its residence time is roughly that of the food. This generally allows 3 to 8 hours for absorption. If the drug is delivered through a mucous membrane in the mouth, the residence time is short because saliva washes it away. Strategies to increase this residence time include bioadhesive polymers, gums, lozenges and dry powders.\n\nBeyond fluid dynamics and chemistry, the definition(s) of residence time can be applied to any flow network, where the flows of generic \"resources\" is modeled (e.g.: people, cars, money, products). Most notably, the over-mentioned definition of residence time is extended to stationary random processes by averaging on time (fluid limit), obtaining the so-called Little's Law, which is a prominent relation in queueing theory and supply chain management. In the context of queueing theory, the residence time is addressed as \"waiting time\", while in the context of supply chain management it is most often addressed as \"lead time\".\n\nIn size-exclusion chromatography, the residence time of a molecule is related to its volume, which is roughly proportional to its molecular weight. Residence times also affect the performance of continuous fermentors.\n\nBiofuel cells utilize the metabolic processes of anodophiles (electronegative bacteria) to convert chemical energy from organic matter into electricity. A biofuel cell mechanism consists of an anode and a cathode that are separated by an internal proton exchange membrane (PEM) and connected in an external circuit with an external load. Anodophiles grow on the anode and consume biodegradable organic molecules to produce electrons, protons, and carbon dioxide gas, and as the electrons travel though the circuit they feed the external load. The HRT for this application is the rate at which the feed molecules are passed through the anodic chamber. This can be quantified by dividing the volume of the anodic chamber by the rate at which the feed solution is passed into the chamber. The hydraulic residence time (HRT) affects the substrate loading rate of the microorganisms that the anodophiles consume, which affects the electrical output. Longer HRTs reduce substrate loading in the anodic chamber which can lead to reduced anodophile population and performance when there is a deficiency of nutrients. Shorter HRTs support the development of non-exoelectrogenous bacteria which can reduce the Coulombic efficiency electrochemical performance of the fuel cell if the anodophiles must compete for resources or if they do not have ample time to effectively degrade nutrients.\n\n\n"}
{"id": "24206158", "url": "https://en.wikipedia.org/wiki?curid=24206158", "title": "Second Nature (book)", "text": "Second Nature (book)\n\nSecond Nature: A Gardener's Education was Michael Pollan's first book. It is a collection of essays about gardening arranged by seasons.\n\nIt is listed in the American Horticultural Society's 75 Great American Garden Books.\n\nIn the book, Pollan describes the relationship between the wild and gardens, nature vs. cultivation, and nature vs. chemicals. He discusses the difficulty of raising roses, which have become so specialized that can no longer survive in the wild.\n\n"}
{"id": "3538621", "url": "https://en.wikipedia.org/wiki?curid=3538621", "title": "Snake worship", "text": "Snake worship\n\nSnake worship is devotion to serpent deities. The tradition is present in several ancient cultures, particularly in religion and mythology, where snakes were seen as entities of strength and renewal.\n\nIn Africa the chief centre of serpent worship was Dahomey, but the cult of the python seems to have been of exotic origin, dating back to the first quarter of the 17th century. By the conquest of Whydah the Dahomeyans were brought in contact with a people of serpent worshippers, and ended by adopting from them the beliefs which they at first despised. At Whydah, the chief centre, there is a serpent temple, tenanted by some fifty snakes. Every python of the \"danh-gbi\" kind must be treated with respect, and death is the penalty for killing one, even by accident. \"Danh-gbi\" has numerous wives, who until 1857 took part in a public procession from which the profane crowd was excluded; a python was carried round the town in a hammock, perhaps as a ceremony for the expulsion of evils. The rainbow-god of the Ashanti was also conceived to have the form of a snake. His messenger was said to be a small variety of boa, but only certain individuals, not the whole species, were sacred. In many parts of Africa the serpent is looked upon as the incarnation of deceased relatives. Among the Amazulu, as among the Betsileo of Madagascar, certain species are assigned as the abode of certain classes. The Maasai, on the other hand, regard each species as the habitat of a particular family of the tribe.\n\nEva Meyerowitz wrote of an earthenware pot that was stored at the Museum of Achimota College in Gold Coast. The base of the neck of this pot is surrounded by the rainbow snake (Meyerowitz 1940, p. 48). The legend of this creature explains that the rainbow snake only emerged from its home when it was thirsty. Keeping its tail on the ground the snake would raise its head to the sky looking for the rain god. As it drank great quantities of water, the snake would spill some which would fall to the earth as rain (Meyerowitz 1940, p. 48).\n\nThere are four other snakes on the sides of this pot: Danh – gbi, the life giving snake, Li, for protection, Liwui, which was associated with Wu, god of the sea, and Fa, the messenger of the gods (Meyerowitz 1940, p. 48). The first three snakes Danh – gbi, Li, Liwui were all worshipped at Whydah, Dahomey where the serpent cult originated (Meyerowitz 1940, p. 48). For the Dahomeans, the spirit of the serpent was one to be feared as he was unforgiving (Nida & Smalley 1959, p. 17). They believed that the serpent spirit could manifest itself in any long, winding objects such as plant roots and animal nerves. They also believed it could manifest itself as the umbilical cord, making it a symbol of fertility and life (Nida & Smalley 1959, p. 17).\n\nMami Wata is a water spirit or class of spirits associated with fertility and healing, usually depicted as a woman holding a large snake or with the lower body of a serpent or fish. She is worshipped in West, Central, and Southern Africa and the African diaspora.\n\nIn Haitian Vodou, the creator loa Damballa is represented as a serpent, and his wife Ayida-Weddo is called the \"rainbow serpent.\" In West African mythology, Ayida-Weddo is believed to hold up the sky. Simbi are a type of serpentine loa in Haitian Vodou. They are associated with water and sometimes are believed to act as psychopomps serving Papa Legba.\n\nAncient Egyptians worshipped snakes, especially the cobra. The cobra was not only associated with the sun god Ra, but also many other deities such as Wadjet, Renenutet, Nehebkau, and Meretseger. Serpents could also be evil and harmful such as the case of Apep and Set. They were also referenced in the Book of the Dead, in which Spell 39 was made to help repel an evil snake in the underworld. \"Get back! Crawl away! Get away from me, you snake! Go, be drowned in the Lake of the Abyss, at the place where your father commanded that the slaying of you should be carried out.\" \n\nWadjet was the patron goddess of Upper Egypt, and was represented as a cobra with spread hood, or a cobra-headed woman. She later became one of the protective emblems on the pharaoh's crown once Upper and Lower Egypt were united. She was said to 'spit fire' at the pharaoh's enemies, and the enemies of Ra. Sometimes referred to as one of the eyes of Ra, she was often associated with the lioness goddess Sekhmet, who also bore that role.\n\nIndigenous peoples of the Americas such as the Hopi give reverence to the rattlesnake as grandfather and king of snakes who is able to give fair winds or cause tempest. Among the Hopi of Arizona, snake-handling figures largely in a dance to celebrate the union of Snake Youth (a Sky spirit) and Snake Girl (an Underworld spirit). The rattlesnake was worshipped in the Natchez temple of the sun. The Mound Builders evidently reverenced the serpent, as the Serpent Mound demonstrates, though we are unable to unravel the particular associations.\n\nThe Maya deity Kukulkan and the Aztec Quetzalcoatl (both meaning \"feathered serpent\") figured prominently in their respective cultures of origin. Kukulkan (Q'uq'umatz in K'iche' Maya) is associated with Vision Serpent iconography in Maya art. Kukulkan was an official state deity of Itza in the northern Yucatan. In many Mesoamerican cultures, the serpent was regarded as a portal between two worlds. \n\nThe worship of Quetzalcoatl dates back to as early as the 1st century BC at Teotihuacan. In the Postclassic period (AD 900-1519), the cult was centered at Cholula. Quetzalcoatl was associated with wind, the dawn, the planet Venus as the morning star, and was a tutelary patron of arts, crafts, merchants, and the priesthood.\n\nSerpents figure prominently in the art of the pre-Incan Chavín culture, as can be seen at the type-site of Chavín de Huántar in Peru. In Chile the Mapuche mythology featured a serpent figure in stories about a deluge. Lake Guatavita in Colombia also maintains a Cacique legend of a \"Serpent God\" living in the waters, which the tribe worshipped by placing gold and silver jewelry into the lake.\n\nSerpents, or nāgas, play a particularly important role in Cambodian mythology. A well-known story explains the emergence of the Khmer people from the union of Indian and indigenous elements, the latter being represented as nāgas. According to the story, an Indian brahmana named Kaundinya came to Cambodia, which at the time was under the dominion of the naga king. The naga princess Soma sallied forth to fight against the invader but was defeated. Presented with the option of marrying the victorious Kaundinya, Soma readily agreed to do so, and together they ruled the land. The Khmer people are their descendants.\n\nSnakes, nagas, had high status in Hindu mythology. \"\" (Sanskrit:नाग) is the Sanskrit and Pāli word for a deity or class of entity or being, taking the form of a very large snake, found in Hinduism and Buddhism. The use of the term \"nāga\" is often ambiguous, as the word may also refer, in similar contexts, to one of several human tribes known as or nicknamed \"Nāgas\"; to elephants; and to ordinary snakes, particularly the King Cobra and the Indian Cobra, the latter of which is still called \"nāg\" in Hindi and other languages of India. A female nāga is a \"nāgī\". The Snake primarily represents rebirth, death and mortality, due to its casting of its skin and being symbolically \"reborn\". Over a large part of India there are carved representations of cobras or nagas or stones as substitutes. To these human food and flowers are offered and lights are burned before the shrines. Among some Indians, a cobra which is accidentally killed is burned like a human being; no one would kill one intentionally. The serpent-god's image is carried in an annual procession by a celibate priestess.\n\nAt one time there were many prevalent different renditions of the serpent cult located in India. In Northern India, a masculine version of the serpent named Nagaraja and known as the “king of the serpents” was worshipped. Instead of the “king of the serpents,” actual live snakes were worshipped in Southern India (Bhattacharyya 1965, p. 1). The Manasa-cult in Bengal, India, however, was dedicated to the anthropomorphic serpent goddess, Manasa (Bhattacharyya 1965, p. 1).\n\nNāgas form an important part of Hindu mythology. They play prominent roles in various legends:\n\nShiva is depicted wearing a snake around his neck.\n\nNag panchami is an important Hindu festival associated with snake worship which takes place of the fifth day of Shravana (July-August). Snake idols are offered gifts of milk and incense to help the worshipper to gain knowledge, wealth, and fame.\n\nDifferent districts of Bengal celebrate the serpent in various ways. In the districts of East Mymensing, West Sylhet, and North Tippera, serpent-worship rituals were very similar, however (Bhattacharyya 1965, p. 5). On the very last day of the Bengali month Shravana, all of these districts celebrate serpent-worship each year (Bhattacharyya 1965, p. 5). Regardless of their class and station, every family during this time created a clay model of the serpent-deity – usually the serpent-goddess with two snakes spreading their hoods on her shoulders. The people worshipped this model at their homes and sacrificed a goat or a pigeon for the deity’s honor (Bhattacharyya 1965, p. 5). Before the clay goddess was submerged in water at the end of the festival, the clay snakes were taken from her shoulders. The people believed that the earth these snakes were made from cured illnesses, especially children’s diseases (Bhattacharyya 1965, p. 6).\n\nThese districts also worshipped an object known as a Karandi (Bhattacharyya 1965, p. 6). Resembling a small house made of cork, the Karandi is decorated with images of snakes, the snake goddess, and snake legends on its walls and roof (Bhattacharyya 1965, p. 6). The blood of sacrificed animals was sprinkled on the Karandi and it also was submerged in the river at the end of the festival (Bhattacharyya 1965, p. 6).\n\nEight dragon kings who assembled at the gathering where Shakyamuni preached the Lotus Sutra, as described in the sutra. Kumarajiva's translation of the Lotus Sutra refers to them by their Sanskrit names: Nanda, Upananda, Sagara, Vasuki, Takshaka, Anavatapta, Manasvin, and Utpalaka. According to the \"Introduction\" (first) chapter of the Lotus Sutra, each attends the gathering accompanied by several hundreds of thousands of followers.\n\nIn Korean mythology, Eobshin, the wealth goddess, appears as an eared, black snake. Chilseongshin (the Jeju Island equivalent to Eobshin) and her seven daughters are all snakes. These goddesses are deities of orchards, courts, and protect the home. According to the \"Jeju Pungtorok\", \"The people fear snakes. They worship it as a god...When they see a snake, they call it a great god, and do not kill it or chase it away.\" The reason for snakes symbolizing worth was because they ate rats and other pests.\nMatsura Sayohime (松浦佐用姫) was a legendary heroine in Japanese Buddhist mythology. As recounted, she was born to Lord Kyōgoku after he and his wife prayed to the Bodhisattva Kannon. After her father's death, Sayohime was too poor to sponsor a memorial service for him; to raise funds, she sold herself to a man named Gonga no Tayu, who (unbeknownst to Sayohime) intended to sacrifice her to the snake deity of his village in place of his own daughter. When presented to the snake, Sayohime read from the Lotus sutra, enabling the deity to achieve enlightenment and shed its monstrous form. The deity then returned Sayohime to the care of her mother.\n\nIn Australia, various Aboriginal mythologies tell of a huge python, known by a variety of names but universally referred to as the Rainbow Serpent, that was said to have created the landscape, embodied the spirit of fresh water, and punished lawbreakers. The Aborigines in southwest Australia called the serpent the Waugyl, while the Warramunga of the east coast worshipped the mythical Wollunqua.\n\nSerpent worship was well known in ancient Europe. The Roman genius loci took the form of a serpent. \n\nIn Italy, the Marsian goddess Angitia, whose name derives from the word for \"serpent,\" was associated with witches, snakes, and snake-charmers. Angitia is believed to have also been a goddess of healing. Her worship was centered in the Central Apennine region.\n\nA snake was kept and fed with milk during rites dedicated to Potrimpus, a Prussian god. On the Iberian Peninsula there is evidence that before the introduction of Christianity, and perhaps more strongly before Roman invasions, serpent worship was a standout feature of local religions (see Sugaar). To this day there are numerous traces in European popular belief, especially in Germany, of respect for the snake, possibly a survival of ancestor worship: The \"house snake\" cares for the cows and the children, and its appearance is an omen of death; and the lives of a pair of house snakes are often held to be bound with that of the master and the mistress. Tradition states that one of the Gnostic sects known as the Ophites caused a tame serpent to coil around the sacramental bread, and worshipped it as the representative of the Savior.\nIn Lanuvium (32 km from Rome) a big snake was venerated as a god and they offered human sacrifice to it. See Plutarch, Parallela Minora XIV, 309a and Sextus Propertius Elegies IV, 8. \n\nSerpents figured prominently in archaic Greek myths. According to some sources, Ophion (\"serpent\", a.k.a. Ophioneus), ruled the world with Eurynome before the two of them were cast down by Kronos and Rhea. The oracles of the ancient Greeks were said to have been the continuation of the tradition begun with the worship of the Egyptian cobra goddess, Wadjet. We learn from Herodotus of a great serpent which defended the citadel of Athens. \nThe Minoan Snake Goddess brandished a serpent in either hand, perhaps evoking her role as source of wisdom, rather than her role as Mistress of the Animals (\"Potnia Theron\"), with a leopard under each arm. It is not by accident that later the infant Herakles, a liminal hero on the threshold between the old ways and the new Olympian world, also brandished the two serpents that \"threatened\" him in his cradle. Although the Classical Greeks were clear that these snakes represented a threat, the snake-brandishing gesture of Herakles is the same as that of the Cretan goddess.\n\nTyphon, the enemy of the Olympian gods, is described as a vast grisly monster with a hundred heads and a hundred serpents issuing from his thighs, who was conquered and cast into Tartarus by Zeus, or confined beneath volcanic regions, where he is the cause of eruptions. Typhon is thus the chthonic figuration of volcanic forces. Amongst his children by Echidna are Cerberus (a monstrous three-headed dog with a snake for a tail and a serpentine mane), the serpent-tailed Chimaera, the serpent-like water beast Hydra, and the hundred-headed serpentine dragon Ladon. Both the Lernaean Hydra and Ladon were slain by Herakles.\n\nPython, an enemy of Apollo, was always represented in vase-paintings and by sculptors as a serpent. Apollo slew Python and made her former home, Delphi, his own oracle. The Pythia took her title from the name Python.\n\nAmphisbaena, a Greek word, from amphis, meaning \"both ways\", and bainein, meaning \"to go\", also called the \"Mother of Ants\", is a mythological, ant-eating serpent with a head at each end. According to Greek mythology, the mythological amphisbaena was spawned from the blood that dripped from Medusa the Gorgon's head as Perseus flew over the Libyan Desert with her head in his hand.\n\nMedusa and the other Gorgons were vicious female monsters with sharp fangs and hair of living, venomous snakes whose origins predate the written myths of Greece and who were the protectors of the most ancient ritual secrets. The Gorgons wore a belt of two intertwined serpents in the same configuration of the caduceus. The Gorgon was placed at the highest point and central of the relief on the Parthenon.\n\nAsclepius, the son of Apollo and Koronis, learned the secrets of keeping death at bay after observing one serpent bringing another (which Asclepius himself had fatally wounded) healing herbs. To prevent the entire human race from becoming immortal under Asclepius's care, Zeus killed him with a bolt of lightning. Asclepius' death at the hands of Zeus illustrates man's inability to challenge the natural order that separates mortal men from the gods. In honor of Asclepius, snakes were often used in healing rituals. Non-poisonous Aesculapian snakes were left to crawl on the floor in dormitories where the sick and injured slept. The author of the \"Bibliotheca\" claimed that Athena gave Asclepius a vial of blood from the Gorgons. Gorgon blood had magical properties: if taken from the left side of the Gorgon, it was a fatal poison; from the right side, the blood was capable of bringing the dead back to life. However Euripides wrote in his tragedy Ion that the Athenian queen Creusa had inherited this vial from her ancestor Erichthonios, who was a snake himself. In this version the blood of Medusa had the healing power while the lethal poison originated from Medusa's serpents. Zeus placed Asclepius in the sky as the constellation Ophiucus, \"the Serpent-Bearer\". The modern symbol of medicine is the rod of Asclepius, a snake twining around a staff, while the symbol of pharmacy is the bowl of Hygieia, a snake twining around a cup or bowl. Hygieia was a daughter of Asclepius.\n\nLaocoön was allegedly a priest of Poseidon (or of Apollo, by some accounts) at Troy; he was famous for warning the Trojans in vain against accepting the Trojan Horse from the Greeks, and for his subsequent divine execution. Poseidon (some say Athena), who was supporting the Greeks, subsequently sent sea-serpents to strangle Laocoön and his two sons, Antiphantes and Thymbraeus. Another tradition states that Apollo sent the serpents for an unrelated offense, and only unlucky timing caused the Trojans to misinterpret them as punishment for striking the Horse.\n\nOlympias, the mother of Alexander the Great and a princess of the primitive land of Epirus, had the reputation of a snake-handler, and it was in serpent form that Zeus was said to have fathered Alexander upon her; tame snakes were still to be found at Macedonian Pella in the 2nd century AD (Lucian, \"Alexander the false prophet\") and at Ostia a bas-relief shows paired coiled serpents flanking a dressed altar, symbols or embodiments of the Lares of the household, worthy of veneration (Veyne 1987 illus p 211).\n\nAeetes, the king of Colchis and father of the sorceress Medea, possessed the Golden Fleece. He guarded it with a massive serpent that never slept. Medea, who had fallen in love with Jason of the Argonauts, enchanted it to sleep so Jason could seize the Fleece.\n\nImbolc was traditionally a time of weather divination, and the old tradition of watching to see if serpents or badgers came from their winter dens may be a forerunner of the North American Groundhog Day. Ax\nAmong other things, the Celtic goddess Brigid was said to be associated with serpents. Her festival day, Imbolc is traditionally a time for weather prognostication based on watching to see if serpents or badgers came from their winter dens. A Scottish Gaelic proverb about the day is:\n\nAncient Mesopotamians and Semites believed that snakes were immortal because they could infinitely shed their skin and appear forever youthful, appearing in a fresh guise every time. The Sumerians worshipped a serpent god named Ningishzida. Before the arrival of the Israelites, snake cults were well established in Canaan in the Bronze Age, for archaeologists have uncovered serpent cult objects in Bronze Age strata at several pre-Israelite cities in Canaan: two at Megiddo, one at Gezer, one in the \"sanctum sanctorum\" of the Area H temple at Hazor, and two at Shechem.\n\nIn the surrounding region, serpent cult objects figured in other cultures. A late Bronze Age Hittite shrine in northern Syria contained a bronze statue of a god holding a serpent in one hand and a staff in the other. In sixth-century Babylon a pair of bronze serpents flanked each of the four doorways of the temple of Esagila. At the Babylonian New Year's festival, the priest was to commission from a woodworker, a metalworker, and a goldsmith two images, one of which \"shall hold in its left hand a snake of cedar, raising its right [hand] to the god Nabu\". At the tell of Tepe Gawra, at least seventeen Early Bronze Age Assyrian bronze serpents were recovered.\nSignificant finds of pottery, bronze-ware and even gold depictions of snakes have been made throughout the United Arab Emirates (UAE). The Bronze Age and Iron Age metallurgical centre of Saruq Al Hadid has yielded probably the richest trove of such objects, although finds have been made bearing snake symbols in bronze age sites at Rumailah, Bithnah and Masafi. Most of the depictions of snakes are similar, with a consistent dotted decoration applied to them.\n\nAlthough the widespread depiction of snakes in sites across the UAE is thought by archaeologists to have a religious purpose, this remains conjecture.\n\nContemporary Christian culture identifies the snake as a symbol of evil and of the devil himself. Snake handling is a religious ritual in a small number of Christian churches in the U.S., usually characterized as rural and Pentecostal, particularly the Church of God with Signs Following. Practitioners believe it dates to antiquity and quote the Bible to support the practice, using references such as (Mark 16:18) and (Luke 10:19).\n\nSee Ophites, Naassenes\n\n\n\n"}
{"id": "50972791", "url": "https://en.wikipedia.org/wiki?curid=50972791", "title": "Souain corporals affair", "text": "Souain corporals affair\n\nThe Souain corporals affair was an incident where four corporals in the French Army were shot by firing squad as an example to the rest of their companies during the First World War. The executions, which occurred in the vicinity of Souain on 17 March 1915, are considered to be the most egregious and most publicized military injustice during World War I in France. The events inspired the 1935 anti-war novel \"Paths of Glory\" by Humphrey Cobb, later adapted for film by Stanley Kubrick.\n\nIn March 1915, units of the French Army holding a section of the Western Front through Champagne had seen no tangible results despite two months of fighting. After two recent unsuccessful attacks, the 21st Company of the 336th Infantry Regiment (part of the 60th Infantry Division) was ordered by Général de division Géraud Réveilhac to retake positions captured by the Germans north of the village of Souain in the Marne. A bayonet assault would begin at 5 am on March 10 against a stretch of enemy trenches that was heavily defended by machine guns and barbed wire. Several unsuccessful attacks had already left this part of No Man's Land strewn with French dead.\nHowever, on the morning of the planned assault, a preceding artillery barrage dropped shells on the French trenches instead of the German lines. This also ploughed up the ground over which the assault troops were ordered to cross. When the first wave of troops started \"going over the top\" most became casualties of the undamaged enemy machine guns. The remaining soldiers of 21st Company, who were both exhausted after days of front line duty (in 1915, French Army troop rotation was much slower than later in the war) and demoralized by failure, refused to leave their trenches. On hearing that the troops were refusing to attack, General Réveilhac ordered his divisional artillery to bombard their positions to force them out of their trenches. But the Divisions’s commanding artillery officer, Colonel Raoul Berube, refused to obey the command without a written order. Réveilhac did not issue one.\n\nWith the failure of the assault he had ordered, General Réveilhac demanded that action be taken against the soldiers of the 21st company. Its company commander, Captain Equilbey, was ordered to produce a list of names that included six corporals and 18 enlisted men chosen from the two youngest members in every squad.\n\nOn March 15, General Réveilhac announced that all 24 men would be court-martialed as an example to the others.\n\nThe trial was based on the interpretation of the French Army's Code of Military Regulations that was implemented on September 6, 1914. It stated:\n\ndiscipline is the main strength of armies, it is important that a superior receives a subordinate’s entire obedience and submission at all times.\n\nThe code also established war-time tribunals that employed a panel of three judges who would decide the case. No appeal of the decision were possible. Sentences were carried out very quickly, usually the day after the judgement. These types of courts martial were abolished on April 24, 1916.\n\nOn March 16, 1915, the tribunal was convened under the auspices of the 60th Infantry Division. General Réveilhac opened the case by stating it was about his troops' \"refusal to leap out of the trenches.\" But Corporal Théophile Maupas, one of the defendants, refuted the claim saying \"anyone there had the choice of being killed by the shells from our side or by the fire of the German machine gunners\". Nevertheless, the verdict of the tribunal was to sentence all 24 defendants to death.\n\nHowever, the 18 enlisted men received a stay of execution on the grounds that they were arbitrarily chosen from the ranks and two corporals, named Gosselin and Lorin, were shown clemency because they had not heard the order to attack. Only four corporals, three from Manche and the fourth from Brittany, did not have their death sentences commuted:\n\n\nIn early afternoon, the next day, the four men were executed by firing squad in front of the 336th Infantry Regiment. Two hours after the executions, the French High Command commuted their death sentences to forced labor.\n\nIn April 1915, Blanche Maupas, the widow of Théophile Maupas, contacted the League of Human Rights (\"La Ligue des droits de l'Homme \") about the execution of her husband. They then began a two-decade long fight to have her husband's and other men's convictions annulled. On April 11, 1920 France's Ministry of Justice refused to review the case. On March 26, 1922 and April 21, 1926, a folder concerning the Souain corporals was rejected by France's Court of Cassation. Despite these setbacks, Blanche Maupas created the \"Maupas Committee\" (\"Comité Maupas\") which became the \" Comité national pour la réhabilitation des victimes de guerre\" (National Committee for the rehabilitation of war victims) in 1928.\n\nEulalie Lechat, the sister of Corporal Lucien Lechat, also established a committee in 1923 with the help of the League of Human Rights. Her brother was re-interred in a cemetery at Le Ferré, Brittany on October 16, 1924. Together Maupas and Lechat hosted and organized meetings throughout France. They also got stories in the regional and national press. Many supporters were from the associations for war veterans. Eventually protests were held in front of the Chamber of Deputies asking for posthumous pardons for the Corporals of Souain.\n\nEventually on March 3, 1934, almost 19 years after they were shot, a judge at the \"Cour spéciale de justice\" (Special Court of Justice) agreed to exonerate the four corporals. He concluded that the order had been \"impracticable\" and the \"sacrifice\" exceeded \"the limits of human strength\". It was therefore that with \"some doubt on the willingness [...] to commit disobedience for which they were convicted, they can not be held criminally responsible \". The Special courts, which - for parity - had benches made up of both judges and veterans representatives, had been established in 1932 to re-examine the decisions and sentences made by the French military's wartime tribunals.\n\nFollowing the decision, the families of the executed men received a symbolic franc in respect of damages. However, the main outcome was that the four men had been exonerated. Their families could also claim their pension rights.\n\nIn 1925, a monument was erected in memory of the Corporals of Souain inside Sartilly cemetery where Théophile Maupas was re-interred in 1923 . \n\nStreets in Villeurbanne and Bréhal are named in honour of Maupas. \n\nA school in Sartilly has been named Théophile Maupas since 1998. \n\nOn December 1, 2007, a monument to the Corporals of Souain was unveiled in Suippes in Marne.\n\n"}
{"id": "16593531", "url": "https://en.wikipedia.org/wiki?curid=16593531", "title": "Terceira Rift", "text": "Terceira Rift\n\nThe Terceira Rift is a geological rift located amidst the Azores islands in the Atlantic Ocean. It runs between the Azores Triple Junction to the west and the Azores–Gibraltar Transform Fault to the southeast. It separates the Eurasian Plate to the north from the African Plate to the south. The Terceira Rift is named for Terceira Island through which it passes.\n\nThe Terceira Rift is 550 km long, and represents the world’s slowest spreading center, with plate divergence of 2-4 mm/year. It developed from a transform fault and now operates as a hyper-slow spreading center, as recognized by the relative movement between the African and Eurasian plates. There is a strong resemblance between the Terceira Rift and other ultra- or super- slow spreading ridges, such as the Gakkel Rift and the Southwest Indian ridges. In particular, high obliquity values of 40°-65°, and a magmatic segmentation wavelength of 100 km, are similar to other very slow rifts. Rift valleys present along the rift are 1000-2200 m deep, and 30-60 km wide, similar to the Mid-Atlantic Ridge median valley. However, the amplitude of the Terceira Rift along-strike topography is 2000-4000 m, which is much larger than expected for ultraslow spreading ridges. This irregularity is thought to be due to the association of the rift with the Azores hotspot, in combination with slow spreading rates. Slow spreading results in a strong and thick axial elastic plate, and slower volcanic extrusions from the rift zone, resulting in high topography when merged with plume-related volcanism. The large volume of volcanism associated with the hotspot is mainly controlled by regional extension between Africa and Eurasia, as indicated by strikes of extension fractures. Furthermore, the majority of inter-plate deformation is focused on the Terceira Rift, although a clear deformation pattern has not yet been recognized.\n\nThe structures present along the Terceira Rift have directions likely associated with pre-rift geometry, as they do not correspond to the current direction of spreading motion, which is approximately N70°. Evidence of pre-rift structures include ancient transform directions of N110°-N125°, reactivating as transtensional fault zones, and N-S directions from former middle-oceanic rift faults, reactivating as left-lateral fault zones. It is important to note other hotspot associated islands do not display these features, and thus, the reactivated structures are most likely a result of the complex tectonic setting associated with the Azores triple junction instability. The above observations may represent earlier stages of development between the rifting of Eurasia and Africa. The first stage began in 25 Ma and lasted until 8 Ma, corresponding to the initial rifting of the oceanic plateau. At 8 Ma, extension began along a transform fault, until approximately 3 Ma, when the Terceira Rift axis was initiated.\n\nThe Azores Plateau began to form around 10 Ma, and is characterized by isotopic and elemental variations indicative of large heterogeneities in the mantle beneath it. There is also evidence of only one magmatic source, and no interaction with other volcanic systems. Additionally, the presence of thick crust and complex volcano-tectonic fabric implies the plateau developed through the migration of the Terceira Rift towards the NE, causing a constant position over a fixed hotspot. As previously mentioned, the Azores hotspot is thought to be the primary source for the excess magmatism along the Terceira Rift, resulting in the anomalously high relief of along-strike topography, and is considered to be sampling a relatively undegassed, primitive reservoir. Lines of evidence for the plume-shaped structure in this area include mantle seismic anomalies, bathymetry and gravity anomalies, and plume noble gas signatures associated with the hotspot. However, there is also a possibility the increased magmatic activity on the Terceira Rift is a partial result of the involvement with the rift-rift-rift triple junction between the Eurasian, African, and North American plates.\n"}
{"id": "53195493", "url": "https://en.wikipedia.org/wiki?curid=53195493", "title": "Venezuelan Llanos", "text": "Venezuelan Llanos\n\nThe Venezuelan Llanos (Spanish: \"Llanos Venezolanos\") also simply known as Los Llanos (English: \"the Plains\") in Venezuela, is a large central depression very flat in a vast natural region of approximately 243,774 km of extension, equivalent to 26.6% of the total continental territory of the country.\n\nIt is the largest sedimentary basin of Venezuela of Quaternary origin; since the large volumes of sediments, which are fundamentally alluvial, were deposited during the last two million years of the geological history of the planet. Consequently, the sedimentary fill and its modeling in plain is very recent.\n\nIt extends between the Guiana Shield, to the south; the Venezuelan Coastal Range to the north; and the Cordillera de Mérida to the west. It presents two natural exits to the sea; the Unare Depression puts it in contact with the Caribbean Sea in the central-eastern part, and on the east it has access to the Atlantic Ocean, without interruption of continuity, through the Orinoco Delta.\n\nAlthough it is the most uniform relief region of the country, the detailed study of the same makes it possible to distinguish three large subregions, with their own morphological and topographical characteristics, that affect the possibilities of use and exploitation by human groups that occupy it. This subregions are:\n\nIt is surrounded by orography that date of different ages: to the south the Guiana Shield, of Precambrian origin; and to the north and northwest the Venezuelan Coastal Range and Venezuelan Andes, whose genesis occur in the course of a long period ranging from the Cretaceous to the Pliocene. These emerged areas discharge most of their current waters to this plain, therefore, filled and acquired its present form as a result of the deposition processes caused by the large sedimentary masses that these water currents carry. That is why its origin is in that long process of filling that is fulfilled in the Tertiary; this process of landfilling is still maintained today.\n\nThis region suffered a marine transgression, where this primitive sea was a prolongation of the Atlantic Ocean that, like a wide channel, penetrated in Venezuelan territory; so it was until the Upper Tertiary, and to a great extent until the Quaternary when there was a marine regression.\n\nAt the bottom of this primitive interior sea giant quantities of organic matter were accumulated, formed by animal and plant remains. The process of decomposition of this matter for thousands and thousands of years came to form the immense oil wealth and coal deposits of this region. In it are located the petroleum basins of Barinas-Apure and the Oriental , which includes the Orinoco Belt, also the coal-basins of Guárico and Anzoátegui.\n\nAlthough it does not present a completely uniform topography, the Llanos integrate the Venezuelan region of greater uniformity in its relief and the one that has greater extension of flat and low lands of the country. It has an almost imperceptible slope that is generally inferior to one meter per kilometer (70 cm per km). Los Llanos, therefore, are a broad plain, very flat or slightly undulating, that descends gently from north to south and from east to west, that is, from the mountainous alignments of the Cordillera de Mérida and the Sierra del Interior of the Caribbean Mountain System to the Orinoco river, to the south and the Atlantic Ocean to the east.\n\n\n"}
