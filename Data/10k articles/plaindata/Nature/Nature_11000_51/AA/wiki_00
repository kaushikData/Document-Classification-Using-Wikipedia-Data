{"id": "22508780", "url": "https://en.wikipedia.org/wiki?curid=22508780", "title": "Alternative natural materials", "text": "Alternative natural materials\n\nAlternative natural materials are natural materials like rock or adobe that are not as commonly in use as materials such as wood or iron. Alternative natural materials have many practical uses in areas such as sustainable architecture and engineering. The main purpose of using such materials is to minimize the negative effects that our built environment can have on the planet while increasing the efficiency and adaptability of the structures.\n\nAlternative natural materials have existed for quite some time but often in very basic forms or only as ingredients to a particular material in the past. For example, earth used as a building material for walls of houses has existed for thousands of years. Much more recently, in the 1920s, the United States government promoted rammed earth as a fireproof construction method for building farmhouses. Another more common example is adobe. Adobe homes are prominent in the southwestern U.S. and several Spanish-speaking countries.\n\nStraw bale construction is a more modern concept, but there even exists evidence that straw was used to make homes in African prairies as far back as the Paleolithic times.\nAlternative natural materials, specifically their applications, have only recently made their way into more common use. The ideas of being both green and sustainable in response to global warming and climate change shifted more of a focus onto the materials and methods used to build our cityscape and homes. As environmentally conscious decisions became commonplace, the use of alternative natural materials instead of typical natural materials or man-made materials that rely heavily on natural resources became prominent.\n\nRock is a great alternative to conventional materials which contain chemicals that may be harmful to people, pets or the environment. Rocks have two great characteristics: good thermal mass and thermal insulation. These characteristics make stone a great idea because the temperature in the house stays rather constant thus requiring less air conditioning and other cooling systems. Types of rocks that can be employed are reject stone (pieces of stone that are not able to be used for another task), limestone, and flagstone.\nStraw bales can be used as a basis for walls instead of drywall. Straw provides excellent insulation and fire resistance in a traditional post-and-beam structure, where a wood frame supports the house. These straw walls are about 75% more energy efficient than standard drywalls and because no oxygen can get through the walls, fire cannot spread and there is no chance of combustion.\nIn Asian countries, bamboo is being used for structures like bridges and homes. Bamboo is surprisingly strong and rather flexible and grows incredibly fast, making it a rather abundant material. Although it can be difficult to join corners together, bamboo is immensely strong and makes up for the hardships that can be encountered while building it.\n\nCordwood is a combination of small remnants of firewood and other lumber that usually go to waste. These small blocks of wood can easily be put together to make a structure that, like stone, has great insulation as well as thermal mass. Cordwood provides the rustic look of log cabins without the use of tons of lumber. You can build an entire building with just cordwood or use stones to fill in the walls.\n\nRammed earth is a very abundant material that can be used in place of concrete and brick. Soil is packed tightly into wall molds where it is rammed together and hardened to form a durable wall packing made of nothing more than dirt, stones, and sticks. Rammed Earth also provides great thermal mass, which means great energy savings. In addition, it is very weatherproof and durable enough that it was used in the Great Wall of China.\n\nEarth sheltering is a unique building technique in which buildings are completely constructed on at least one side by some form of Earth whether it be a grass roof, clay walls, or both. This unique system usually includes plenty of windows because of the difficulty involved with using too much electricity in such a house. This adds to the energy efficiency of the house by reducing lighting costs.\n\nPapercrete is an interesting and very new material that is a good substitute for concrete. Papercrete is shredded paper, sand, and cement mixed together that forms a very durable brick-like material. Buildings utilizing papercrete are very well-insulated as well as being termite- and fire-resistant. Papercrete is very cheap as it usually only costs about $0.35 per square foot.\n\nAdobe is an age-old technique that is cheap, easy to obtain, and ideal for hot environments. A mixture of sand, clay, and water is poured into a mold and left in the sun to dry. When dried, it is exceptionally strong and heat-resistant. Adobe doesn’t let much heat through to the inside of the structure, thus providing excellent insulation during the summer to reduce energy costs. Although this clay mixture provides excellent insulation from heat, it is not very waterproof and can be dangerous in earth-quake prone areas due to its tendency to crack easily.\n\nSawdust is a good material to combine with clay or cement mixtures and use for walls. These walls turn out surprisingly sturdy and effectively recycle any trees that may need to be excavated from the building area. Depending what type of sawdust used (hardwood is best) the wood chips in the walls absorb moisture and help prevent cracking during freeze/thaw cycles. Sawdust may be combined with water and frozen to produce a material commonly known as pykrete, which is strong, and less prone to melting than regular ice.\n\nAlthough this is a newer technology there are some buildings that have already employed these materials, as well as other tactics, to make themselves green.\n\n"}
{"id": "346133", "url": "https://en.wikipedia.org/wiki?curid=346133", "title": "Annihilation", "text": "Annihilation\n\nIn particle physics, annihilation is the process that occurs when a subatomic particle collides with its respective antiparticle to produce other particles, such as an electron colliding with a positron to produce two photons. The total energy and momentum of the initial pair are conserved in the process and distributed among a set of other particles in the final state. Antiparticles have exactly opposite additive quantum numbers from particles, so the sums of all quantum numbers of such an original pair are zero. Hence, any set of particles may be produced whose total quantum numbers are also zero as long as conservation of energy and conservation of momentum are obeyed.\n\nDuring a low-energy annihilation, photon production is favored, since these particles have no mass. However, high-energy particle colliders produce annihilations where a wide variety of exotic heavy particles are created.\n\nThe word annihilation takes use informally for the interaction of two particles that are not mutual antiparticles - not charge conjugate. Some quantum numbers may then not sum to zero in the initial state, but conserve with the same totals in the final state. An example is the \"annihilation\" of a high-energy electron antineutrino with an electron to produce a .\n\nIf the annihilating particles are composite, such as mesons or baryons, then several different particles are typically produced in the final state.\n\nIf the initial two particles are elementary (not composite), then they may combine to produce only a single elementary boson, such as a photon (), gluon (), , or a Higgs boson (). If the total energy in the center-of-momentum frame is equal to the rest mass of a real boson (which is impossible for a massless boson such as the ), then that created particle will continue to exist until it decays according to its lifetime. Otherwise, the process is understood as the initial creation of a boson that is virtual, which immediately converts into a real particle+antiparticle pair. This is called an s-channel process. An example is the annihilation of an electron with a positron to produce a virtual photon, which converts into a muon and anti-muon. If the energy is large enough, a could replace the photon. \n\nWhen a low-energy electron annihilates a low-energy positron (antielectron), they can only produce two gamma ray photons, since the electron and positron do not carry enough mass-energy to produce heavier particles, and the creation of only one photon is forbidden by momentum conservation—a single photon would carry nonzero momentum in any frame, including the center-of-momentum frame where the total momentum vanishes. Both the annihilating electron and positron particles have a rest energy of about 0.511 million electron volts (MeV). If their kinetic energies are relatively negligible, this total rest energy appears as the photon energy of the gamma rays produced. Each of the gamma rays then has an energy of about 0.511 MeV. Momentum and energy are both conserved, with 1.022 MeV of gamma rays (accounting for the rest energy of the particles) moving in opposite directions (accounting for the total zero momentum of the system). \n\nIf one or both charged particles carry a larger amount of kinetic energy, various other particles can be produced. Furthermore, the annihilation (or decay) of an electron-positron pair into a \"single\" photon can occur in the presence of a third charged particle to which the excess momentum can be transferred by a virtual photon from the electron or positron. The inverse process, pair production by a single real photon, is also possible in the electromagnetic field of a third particle.\n\nWhen a proton encounters its antiparticle (and more generally, if any species of baryon encounters the corresponding antibaryon), the reaction is not as simple as electron-positron annihilation. Unlike an electron, a proton is a composite particle consisting of three \"valence quarks\" and an indeterminate number of \"sea quarks\" bound by gluons. Thus, when a proton encounters an antiproton, one of its quarks, usually a constituent valence quark, may annihilate with an antiquark (which more rarely could be a sea quark) to produce a gluon, after which the gluon together with the remaining quarks, antiquarks, and gluons will undergo a complex process of rearrangement (called hadronization or fragmentation) into a number of mesons, (mostly pions and kaons), which will share the total energy and momentum. The newly created mesons are unstable, and unless they encounter and interact with some other material, they will decay in a series of reactions that ultimately produce only gamma rays, electrons, positrons, and neutrinos. This type of reaction will occur between any baryon (particle consisting of three quarks) and any antibaryon consisting of three antiquarks, one of which corresponds to a quark in the baryon. (This reaction is unlikely if at least one among the baryon and anti-baryon is exotic enough that they share no constituent quark flavors.) Antiprotons can and do annihilate with neutrons, and likewise antineutrons can annihilate with protons, as discussed below.\n\nReactions in which proton-antiproton annihilation produces as many as nine mesons have been observed, while production of thirteen mesons is theoretically possible. The generated mesons leave the site of the annihilation at moderate fractions of the speed of light, and decay with whatever lifetime is appropriate for their type of meson.\n\nSimilar reactions will occur when an antinucleon annihilates within a more complex atomic nucleus, save that the resulting mesons, being strongly interacting, have a significant probability of being absorbed by one of the remaining \"spectator\" nucleons rather than escaping. Since the absorbed energy can be as much as ~2 GeV, it can in principle exceed the binding energy of even the heaviest nuclei. Thus, when an antiproton annihilates inside a heavy nucleus such as uranium or plutonium, partial or complete disruption of the nucleus can occur, releasing large numbers of fast neutrons. Such reactions open the possibility for triggering a significant number of secondary fission reactions in a subcritical mass, and may potentially be useful for spacecraft propulsion.\n\nIn collisions of two nucleons at very high energies, sea quarks and gluons tend to dominate the interaction rate, so neither nucleon need be an anti-particle for annihilation of a quark pair or \"fusion\" of two gluons to occur. Examples of such processes contribute to the production of the long-sought Higgs boson. The Higgs is directly produced very weakly by annihilation of light (valence) quarks, but heavy or sea or produced quarks are available. In 2012, the CERN laboratory in Geneva announced the discovery of the Higgs in the debris from proton-proton collisions at the Large Hadron Collider (LHC). The strongest Higgs yield is from fusion of two gluons (via annihilation of a heavy quark pair), while two (anti)quarks produce more easily identified events through radiation of a Higgs by a produced virtual vector boson or annihilation of two such vector bosons.\n\n"}
{"id": "315203", "url": "https://en.wikipedia.org/wiki?curid=315203", "title": "Bionomics", "text": "Bionomics\n\nIn ecology, bionomics (Greek: bio = life; nomos = law) is the comprehensive study of an organism and its relation to its environment. As translated from the French word \"Bionomie\", its first use in English was in the period of 1885-1890. Another way of expressing this word is the term currently referred to as \"ecology\". An example of studies of this type is Richard B. Selander's Bionomics, Systematics and Phylogeny of Lytta, a Genus of Blister Beetles (Coleoptera, Meloidae), Illinois Biological Monographs: number 28, 1960. \nAccording to some scholars, who still adhere to bionomics, it transforms many principles of traditional ecology, recognizing that Life on Earth is hierarchically organized in complex systems, acting as living entities well farther populations and communities.\n\nWhen related to the territory Ignegnoli talks about Landscape Bionomics, defining Landscape as the \"level of biological organization integrating complex systems of plants, animals and humans in a living Entity recognizable in a territory as characterized by suitable emerging properties in a determined spatial configuration\". (Ingegnoli, 2011, 2015; Ingegnoli, Bocchi, Giglio, 2017)\n\n"}
{"id": "66668", "url": "https://en.wikipedia.org/wiki?curid=66668", "title": "Bullhead shark", "text": "Bullhead shark\n\nThe bullhead sharks are a small order (Heterodontiformes) of basal modern sharks (Neoselachii). The nine living species are placed in a single genus, Heterodontus, in the family Heterodontidae. All are relatively small, with the largest species reaching just in maximum length. They are bottom feeders in tropical and subtropical waters.\n\nThe Heterodontiforms appear in the fossil record in the Early Jurassic, well before any of the other Galeomorphii, a group that includes all modern sharks except the dogfish and its relatives. However, they have never been common, and their origin probably lies even further back.\n\nThe bullhead sharks are morphologically rather distinctive. The mouth is located entirely anterior to the orbits. Labial cartilages are found in the most anterior part of the mouth. Nasoral grooves are present, connecting the external nares to the mouth. The nasal capsules are trumpet-shaped and well-separated from orbits. Circumnarial skin folds are present, but the rostral process of the neurocranium (braincase) is absent, although a precerebral fossa is present. Finally, the braincase bears a supraorbital crest.\n\nThe eyes lack a nictitating membrane. A spiracle is present, but small. The dorsal ends of the fourth and fifth branchial arches are attached, but not fused into a \"pickaxe\" as in lamniform sharks. Heterodontiforms have two dorsal fins, with fin spines, as well as an anal fin. The dorsal and anal fins also contain basal cartilages, not just fin rays.\n\nBullhead Sharks have distinctive small spikes on the front of their dorsal fins. These are rumoured to be poisonous, but no further scientific tests have been done to prove this rumor true or false.\n\nNine living species of bullhead shark are described, with another potential undescribed species in Baja California:\n\n\n"}
{"id": "51030981", "url": "https://en.wikipedia.org/wiki?curid=51030981", "title": "Calcium triplet", "text": "Calcium triplet\n\nThe infrared Ca II triplet, commonly known as the Calcium triplet, is a triplet of three ionised calcium spectral lines at the wavelength of 8498 Å, 8542 Å and 8662 Å. The triplet has a strong emission, and is most prominently observed in the absorption of spectral type G, K and M stars.\n\n"}
{"id": "38213645", "url": "https://en.wikipedia.org/wiki?curid=38213645", "title": "Center for Earthquake Studies", "text": "Center for Earthquake Studies\n\nThe Center for Earthquake Studies (CES), is an academic earth science, earthquake studies and mathematical national research institute, located in Islamabad, Pakistan. The institute is headquartered in the campus area of the National Centre for Physics (NCP) and conducts mathematical research in earth sciences, with the close coordination with the NCP.\n\nThe institute was founded by the Government of Pakistan after taking the scientific advice from the science adviser dr. Ishfaq Ahmad. It was established in response to the deadliest and devastating earthquake in Pakistan, on October 8, 2005. Initially established as the Earthquake Studies Department at the National Center for Physics, it gained its independence shortly after its establishment. The CES undertakes research scientific studies in the development of expertise in anomalous geophysical phenomenon prior to the seismic activities. The CES primarily produces its research outcomes by using the computer simulation and mathematical modelling to interpret the seismic activities and the earthquake predictions.\n\nThe CES's campus also includes the various ATROPATENA stations network, and supports its research and development with the close collaboration with the Global Network for the Forecasting of Earthquakes. Its first and founding director was dr. Ahsan Mubarak who is still designated as the CES's senior scientist. As of current, dr. Muhammad Qaisar is the CES's current administrator.\n\n\n"}
{"id": "86258", "url": "https://en.wikipedia.org/wiki?curid=86258", "title": "Coventina", "text": "Coventina\n\nCoventina was a Romano-British goddess of wells and springs. She is known from multiple inscriptions at one site in Northumberland county of England, an area surrounding a wellspring near Carrawburgh on Hadrian's Wall. It is possible that other inscriptions, two from Hispania and one from Narbonensis, refer to Coventina, but this is disputed.\n\nDedications to Coventina and votive deposits were found in a walled area which had been built to contain the outflow from a spring now called \"Coventina's Well\". The well and the walled area surrounding it are nearby the site variously referred to as \"Procolita\", \"Brocolitia\", or \"Brocolita\", once a Roman fort and settlement on Hadrian's Wall, now known as Carrawburgh (The name \"Procolita\" is found in the 5th century document Notitia Dignitatum, and \"Brocolita\" in the 7th century Ravenna Cosmography). The remains of a Roman Mithraeum and Nymphaeum are also found near the site.\n\nThe well itself was a spring in a rectangular basin 2.6m x 2.4m in the centre of a walled enclosure 11.6m x 12.2m within a wall 0.9m thick. The contents of the well included 13487 coins from Mark Anthony to Gratian, a relief of three water nymphs, the head of a male statue, two dedication slabs to the goddess Coventina, ten altars to Coventina and Minerva, two clay incense burners, and a wide range of votive objects.\n\nThe site near Coventina's Well was excavated by British archaeologist, John Clayton, in 1876. The date of the wall at Coventina's Well is uncertain, but some have theorized that it was built sometime after the completion of the Roman fort (dated between the years 128 and 133). Since Hadrian's Wall does not deviate to avoid the well, this may suggest that the boundary wall around the well was built some time after in order to control the flow of water in a marshy area.\n\nEvidence from coin hoards and stones which covered them and those also blocking the well suggest a fairly abrupt end around 388, perhaps due to events linked to anti-Pagan edicts of Theodosius I.\n\nExcavation of the site revealed several inscribed altars, some with depictions of Coventina in typical Roman nymph form - reclining, partially clothed and associated with water. On one, Coventina is either depicted in triple form or with two attendants.\n\nAt least ten inscriptions to Coventina are recorded from Carrawburgh. Several stone altars contained dedications to Coventina, as did two pottery incense burners.\n\nAn example of an inscription from the site reads:\n\n“To the Goddess Coventina, Titus D[unclear] Cosconianus, Prefectus of the First Cohort of Batavians, freely and deservedly (dedicated this stone).”\n\nThree altars dedicated to Mithras were placed there by the Prefects of the military garrison.\n\n\n\n"}
{"id": "1827882", "url": "https://en.wikipedia.org/wiki?curid=1827882", "title": "Cowan Lake State Park", "text": "Cowan Lake State Park\n\nCowan Lake State Park is a Ohio state park in Clinton County, Ohio, in the United States. It is operated by the Ohio Department of Natural Resources. Cowan Lake State Park is open for year-round recreation and is known for a variety of birds that attract birdwatching enthuiasts to the park in southwestern Ohio.\n\nWhat is now Cowan Lake State Park was once home to the Miami and Shawnee Indians. The Battle of Fallen Timbers in 1794 resulted in the opening of the land to American settlement. The first settler in the area was William Smalley. Smalley had previously been held captive by the Lenape. His language skills and knowledge of the land left him ideally suited to be among the pioneers in settling what became Clinton County, Ohio.\n\nCowan Creek, named for pioneer surveyor John Cowan, was dammed in 1950. The creation of Cowan Lake led to the establishment of Cowan Lake State Park in 1968. Its surface area is .\n\nCowan Lake State Park lies atop limestone that is laden with fossils. The fossilized plants and animals attract attention from both professional and amateur paleontologists. Much of what is now Ohio was covered by massive inland seas and lakes such as Lake Maumee. As the Appalachian Mountains were formed, the limestone that was once the sea floor was pushed to the surface.\n\nCowan Lake is surrounded by a deciduous forest of beech and maple trees. The combination of lake and forest makes the park a destination for birders, who can observe mallards, American wigeons, mergansers, great blue and green-backed herons, killdeer, eastern bluebirds, catbirds, and many other species of birds. The most common mammals at the park are groundhogs, skunks, raccoons, and opossums.\n\nOther plant life found at the park include bloodroot, wild ginger, and American lotus. The lotus is a water lily that thrives in the shallow waters of Cowan Lake. The leaves of the plant are quite large with yellow flowers.\n\nCowan Lake State Park is open for year-round recreation. There are six picnic areas and two pavilions. Cowan Lake State Park features six hiking trails and one trail that is open to mountain bikes.\n\nThere are 27 cottages that are heated and have air-conditioning. The cottages are fully furnished and linens are provided. There are 237 electric camp-sites and 17 sites without electricity. A centrally located bathhouse has showers, flush toilets and a sanitary dump station for RVs.\n\nCowan Lake is open for boating, swimming and fishing. Motorboats have a 10 horsepower limit. A marina on the southern shore of the lake provides rental boats, canoes and motors, with four launch ramps, fuel and dock rentals. Common game fish in Cowan Lake are muskellunge, crappie, largemouth bass and bluegill. There is a handicapped accessible fishing pier at the Pine Tree picnic area. A public swimming beach is on the southern shore of the lake. The 1,000 foot beach is located near the campers bathhouse and also has a snack bar.\n\n"}
{"id": "17800165", "url": "https://en.wikipedia.org/wiki?curid=17800165", "title": "Dendronized polymer", "text": "Dendronized polymer\n\nDendronized polymers (or dendronised polymers) are linear polymers to every repeat unit of which dendrons are attached. Dendrons are regularly branched, tree-like fragments and for larger ones the polymer backbone is wrapped to give sausage-like, cylindrical molecular objects. Figure 1 shows a cartoon representation with the backbone in red and the dendrons like cake slices in green. It also provides a concrete chemical structure showing a polymethylmethacrylate (PMMA) backbone, the methyl group of which is replaced by a dendron of the third generation (three consecutive branching points).\n\nFigure 1. Cartoon representation (left) and a concrete example of a third generation dendronized polymer (right). The peripheral amine groups are modified by a substituent X which often is a protection group. Upon deprotection and modification substantial property changes can be achieved. The subscript n denotes the number of repeat units.\n\nDendronized polymers can contain several thousands of dendrons in one macromolecule and have a stretched out, anisotropic structure. In this regard they differ from the more or less spherically shaped dendrimers, where a few dendrons are attached to a small, dot-like core resulting in an isotropic structure. Depending on dendron generation, the polymers differ in thickness as the atomic force microscopy image shows (Figure 2). Neutral and charged dendronized polymers are highly soluble in organic solvents and in water, respectively. This is due to their low tendency to entangle. Dendronized polymers have been synthesized with, e.g., polymethylmethacrylate, polystyrene, polyacetylene, polyphenylene, polythiophene, polyfluorene, poly(phenylene vinylene), poly(phenylene acetylene), polysiloxane, polyoxanorbornene, poly(ethylene imine) (PEI) backbones. Molar masses up to 200 Mio g/mol have been obtained. Dendronized polymers have been investigated for/as bulk structure control, responsivity to external stimuli, single molecule chemistry, templates for nanoparticle formation, catalysis, electro-optical devices, and bio-related applications. Particularly attractive is the use of water-soluble dendronized polymers for the immobilization of enzymes on solid surfaces (inside glass tubes or microfluidic devices) and for the preparation of dendronized polymer-enzyme conjugates.\n\nThe two main approaches into this class of polymers are the macromonomer route and the attach-to route. In the former, a monomer which already carries the dendron of final size is polymerized. In the latter the dendrons are constructed generation by generation directly on an already existing polymer. Figure 4 illustrates the difference for a simple case. The macromonomer route results in shorter chains for higher generations and the attach-to route is prone to lead to structure imperfections as an enormous number of chemical reactions have to be performed for each macromolecule.\n\nThe name “Dendronized Polymer” which meanwhile is internationally accepted was coined by Schlüter in 1998. The first report on such a macromolecule which at that time was called “Rod-shaped Dendrimer” goes back to a patent by Tomalia in 1987 and was followed by Percec’s first mentioning in the open literature of a polymer with “tapered side chains” in 1992. In 1994 the potential of these polymers as cylindrical nanoobjects was recognized. Many groups worldwide contributed to this field. They can be found in review articles.\n\n"}
{"id": "26065172", "url": "https://en.wikipedia.org/wiki?curid=26065172", "title": "Dinaric Mountains mixed forests", "text": "Dinaric Mountains mixed forests\n\nThe Dinaric Mountains mixed forests form a terrestrial ecoregion of the Temperate Broadleaf and Mixed Forests Biome in Southeastern Europe, according to both the WWF and Digital Map of European Ecological Regions by the European Environment Agency. It also is in the Palearctic ecozone.\n\nThe Dinaric Mountains mixed forests compose the montane forest region of the Dinaric Alps. This mountain range stretches along the eastern coast of the Adriatic Sea and occupy 58,200 km² (22,500 mi²) in Slovenia, Croatia, Montenegro, Bosnia and Herzegovina, Serbia, northern Albania, and northeastern Italy.\n\nThe climate of the ecoregion is wet and extremely humid. Precipitation ranges are generally above 1500 mm and ranges between 2000 and 3500 mm are common. Maximums measured in the hinterland of the Bay of Kotor surpass annual averages beyond 4500 mm (Crkvice), the highest precipitation ranges in Europe and one of the highest of the Northern Hemisphere. It is part of Köppen's temperate type with humid summers (Cfs) and mountain snow climates (Dfs). Due to snow abundance and avalanching, alpine biotopes are common for all high mountains. Several small glaciers still persist in the Durmitor and Prokletije ranges, the Dinaric Alps' highest massifs.\n\nWith high precipitation ranges and an overabundance of limestones, karst relief is dominating.\n\nDue to the wide altitudinal range of this ecoregion the highest elevations (above 1,500 m) are covered with conifer forests, with a mixed broadleaf vegetation and occurring lower. \n\nThe conifer zone is dominated by the Norway spruce, silver fir, and European black pine with the admixture of the European beech. The dominant species of the lower zones include various deciduous oaks (\"Quercus frainetto\", \"Q. pubescens\", \"Q. cerris\") and the oriental hornbeam (\"Carpinus orientalis\"). The South European flowering ash (\"Fraxinus ornus\") marks the submediterranean zone at region below 600–1000 m (from N to S). \n\nPhytogeographically, the ecoregion is shared between the Adriatic and East Mediterranean provinces of the Mediterranean Region, within the Holarctic Kingdom (according to Armen Takhtajan's delineation).\n"}
{"id": "6786225", "url": "https://en.wikipedia.org/wiki?curid=6786225", "title": "Distance decay", "text": "Distance decay\n\nDistance decay is a geographical term which describes the effect of distance on cultural or spatial interactions. The distance decay effect states that the interaction between two locales declines as the distance between them increases. Once the distance is outside of the two locales' activity space, their interactions begin to decrease.\n\nWith the advent of faster travel, distance has less effect than it did in the past, except where places previously connected by now-abandoned railways, for example, have fallen off the beaten path. Advances in communications technology, such as telegraphs, telephones, broadcasting, and internet, have further decreased the effects of distance.\n\nDistance decay is graphically represented by a curving line that swoops concavely downward as distance along the x-axis increases. Distance decay can be mathematically represented as an Inverse-square law by the expression\n\nformula_1 \nor\nformula_2,\n\nwhere I is interaction and d is distance. It can take other forms such as negative exponential, i.e.\n\nformula_3\n\nDistance decay is evident in town/city centres. It can refer to various things which decline with greater distance from the center of the Central Business District (CBD):\n\nDistance decay weighs into the decision to migrate, leading many migrants to move less far.\n\nRelated terms include \"friction of distance\", which describes the force that creates distance decay and Waldo R. Tobler's \"First law of geography\", an informal statement that \"All things are related, but near things are more related than far things.\" \n\"Loss of Strength Gradient\" holds that the amount of a nation's military power that could be brought to bear in any part of the world depends on geographic distance.\n\n\n"}
{"id": "52947494", "url": "https://en.wikipedia.org/wiki?curid=52947494", "title": "Dr. Neil Trivett Global Atmosphere Watch Observatory", "text": "Dr. Neil Trivett Global Atmosphere Watch Observatory\n\nThe Dr. Neil Trivett Global Atmosphere Watch Observatory is an atmospheric baseline station operated by Environment and Climate Change Canada located at Alert, Nunavut, on the north-eastern tip of Ellesmere Island, about 800 km south of the geographic north pole.\n\nThe observatory is the northernmost of 31 Global Stations in an international network coordinated by the World Meteorological Organization (WMO) under its Global Atmosphere Watch (GAW) program to study the long-term effects of pollution on the atmospheric environment. Among these 31 stations, Alert is one of three greenhouse gas \"intercomparison supersites\", along with Mauna Loa in Hawaii and Cape Grim in Australia, which, due to their locations far from industrial activity, provide the international scientific community with a baseline record of atmospheric chemistry.\n\nThe observatory is located on a plateau about 6 km south of Canadian Forces Station (CFS) Alert, which is itself located on the shore of the Lincoln Sea, 15 kilometres from the mouth of the Nares Strait. The region is characterized by recent glacial activity, with still extant glaciers visible among the peaks of the United States Range approximately 40 kilometres to the west. The landscape immediately surrounding the observatory is undulating, marked by cliffs and crevasses and a number of small rivers which can become impassable during freshet.\n\nTo the south, the Winchester Hills are the dominant visible feature. A number of small freshwater lakes provide CFS Alert (and by extension, the observatory) with drinking water.\n\nDue to its high latitude, the observatory experiences 24-hour daylight from the beginning of April to early September, and the sun remains below the horizon from mid-October to late February. The intermediate periods are marked by a slight diurnal cycle. The dark season is responsible for much of the unique atmospheric chemistry that occurs during polar sunrise. The lack of sunlight to act as a catalyst causes a buildup of pollution from industrial areas down south, and the polar vortex amplifies this effect by containing pollution within the high Arctic. During sunrise, this pollution is responsible for a phenomenon known as Arctic haze.\n\nAlert's climate is exceptionally dry, seeing very little annual precipitation. (It is, in fact, considered a desert.) What rain does fall generally comes in the form of mist or drizzle. Heavy snowfalls are typically confined to the period from September through November, but can occur again after polar sunrise. Fog is common due to proximity to the ocean, particularly in the summer and fall. When temperatures drop below freezing in September and October, the damp air also causes accumulations of hoar frost.\n\nThough conditions in Alert are cold, with only two months of the year seeing average temperatures above the freezing point (snow is possible in any month of the year), they are not as cold as other locations further south, such as Eureka, because proximity to the Arctic Ocean has as a moderating effect. It is more accurate to characterize conditions in Alert as consistently cold, rather than extremely cold.\n\nPrevailing winds at the observatory are from the southwest, which usually bring clear skies and warmer temperatures. North winds off the ocean are typically accompanied by fog and sudden drops in temperature. (North winds are also unwelcome at the observatory, as they bring exhaust from the station's diesel generators.) Conditions are typically calm through the dark winter months, once the polar vortex has set up for the season. Three out of five days have wind speeds below two metres per second at this time of year. (However, when windy conditions arise in the winter, they tend to be extreme. Storms can last for days at a time.) These conditions continue through polar sunrise, but reverse dramatically with the onset of freshet, which brings high, gusting winds.\n\nThe establishment of the Joint Arctic Weather Station (JAWS) on April 9, 1950 marked the beginning of continuous meteorological measurements in Alert. The weather station frequently provided support for scientific research in Alert, including the collection of weekly flask samples for the measurement of carbon dioxide, the first of which was taken on July 8, 1975.\n\nIncreasing interest in studies of phenomena such as Arctic haze led to a conference in 1977 in Lillestrøm, Norway with participation from scientists of seven different countries. Following this conference, the Arctic Air-Sampling Network (AASN) was created to share data between scientific organizations in each member nation. To fulfill Canadian obligations to this program, the Canadian Arctic Aerosol Sampling Network (CAASN) was created, eventually consisting of three stations: Mould Bay (April 1979), Igloolik (November 1979), and Alert (July 1980). In 1984, the program was condensed and refocused. The sampling programs in Igloolik and Mould Bay were discontinued, and the program in Alert became part of the core mandate of the newly reformed Canadian Arctic Aerosol Chemistry Program (CAACP).\n\nThe gradually increasing amount of experimental research being done in Alert made the construction of a permanent observatory a viable option. In 1985, the Canadian Baseline Program was founded, and on August 29 the following year the Alert Background Air Pollution Monitoring Network (BAPMoN) Observatory was officially opened. In 1989, the WMO's BAPMoN program was merged with the Global Ozone Observing System (GOOS) to form the Global Atmosphere Watch Program.\n\nIn 1992, the original observatory building was expanded to roughly three times its size, including the addition of a 10 m tall walk-up tower.\n\nFollowing the death of Dr. Neil Trivett in 2002 (an Environment Canada researcher who was largely responsible for the construction of the observatory), it was officially renamed in July 2006 as the Dr. Neil Trivett Global Atmosphere Watch Observatory.\n\nIn addition to the ongoing flask sample programs, the observatory maintains a core group of continuous measurement programs which include monitoring of aerosols, mercury, greenhouse gases, ozone (both ground level and stratospheric), broadband solar irradiance and albedo, permafrost temperatures, persistent organic pollutants (POPs), and ground level meteorological conditions.\n\nThe observatory also supports experimental testing of new monitoring equipment, and short-term, intensive research programs, e.g., NETCARE (Network on Climate and Aerosols: Addressing Key Uncertainties in Remote Canadian Environments).\n\nMuch of the research and data collection at the observatory is collaborative, including long-standing partnerships with, e.g., NOAA, the University of Heidelberg, the Scripps Institution of Oceanography, CSIRO, Natural Resources Canada, and Health Canada.\n\nWhile it had been known since 1988 that ground level ozone underwent periodic rapid depletions during March–June each year, it was not discovered until 1995 that atmospheric mercury behaved in a nearly identical manner. (The first continuous monitoring instruments for atmospheric mercury were installed at the observatory that year.) Until that time, it was generally agreed that elemental mercury persisted in the atmosphere for 6–12 months, which allowed it to reach remote locations such as the Arctic, far from emission sources.\n\nIn the springtime, elemental mercury undergoes photochemically initiated oxidation reactions and converts to a more reactive and less stable form of mercury in the atmosphere. This was a means by which mercury could be removed from the atmosphere and deposit to the ground that was not previously known. These events were later termed atmospheric mercury depletion events (AMDEs), and the underlying chemistry that connects them with the simultaneous ozone depletions has been and continues to be thoroughly researched.\n\n"}
{"id": "10197275", "url": "https://en.wikipedia.org/wiki?curid=10197275", "title": "Dry matter", "text": "Dry matter\n\nThe dry matter or dry weight is a measurement of the mass of something when completely dried.\n\nThe dry matter of plant and animal material consists of all its constituents excluding water. The dry matter of food includes carbohydrates, fats, proteins, vitamins, minerals, and antioxidants (e.g., thiocyanate, anthocyanin, and quercetin). Carbohydrates, fats, and proteins, which provide the energy in foods (measured in kilocalories or kilojoules), make up ninety percent of the dry weight of a diet. Water content in foods varies widely. A large number of foods are more than half water by weight, including boiled oatmeal (84.5%), cooked macaroni (78.4%), boiled eggs (73.2%), boiled rice (72.5%), white meat chicken (70.3%) and sirloin steak (61.9%). Fruits and vegetables are 70 to 95% water. Most meats are on average about 70% water. Breads are approximately 36% water. Some foods have a water content of less than 5%, e.g., peanut butter, crackers, and chocolate cake.\nWater content of dairy products is quite variable. Butter is 15% water. Cow's milk ranges between 88-86% water. Swiss cheese is 37% percent water. The water content of milk and dairy products varies with the percentage of butterfat so that whole milk has the lowest percentage of water and skimmed milk has the highest.\n\nIn the sugar industry the dry matter content is an important parameter to control the crystallization process and is often measured on-line by means of microwave density meters.\n\nThe nutrient or mineral content of foods, animal feeds or plant tissues are often expressed on a dry matter basis, i.e. as a proportion of the total dry matter in the material. For example a 138-gram apple contains 84% water (116 g water and 22 g dry matter per apple). The potassium content is 0.72% on a dry matter basis, i.e. 0.72% of the dry matter is potassium. The apple, therefore, contains 158 mg potassium (0.72/100 X 22 g). Dried apple contains the same concentration of potassium on a dry matter basis (0.72%), but is only 32% water (68% dry matter). So 138 g of dried apple contains 93.8 g dry matter and 675 mg potassium (0.72/100 x 93.8 g).\n\nWhen formulating a diet or mixed animal feed, nutrient or mineral concentrations are generally given on a dry matter basis; it is therefore important to consider the moisture content of each constituent when calculating total quantities of the different nutrients supplied.\n\nCheese contains both dry matter and water. The dry matter in cheese contains proteins, butterfat, minerals, and lactose (milk sugar), although little lactose survives fermentation when the cheese is made. A cheese's fat content is expressed as the percentage of fat in the cheese's dry matter (abbreviated FDM or FiDM), which excludes the cheese's water content. For example, if a cheese is 50% water (and, therefore, 50% dry matter) and has 25% fat, its fat content would be 50% fat in dry matter.\n\nDry matter can refer to the dry portion of animal feed. A substance in the feed, such as a nutrient or toxin, can be referred to on a dry matter basis (abbreviated DMB) to show its level in the feed (e.g., ppm). Considering nutrient levels in different feeds on a dry matter basis (rather than an as-is basis) makes a comparison easier because feeds contain different percentages of water. This also allows a comparison between the level of a given nutrient in dry matter and the level needed in an animal’s diet. Dry matter intake (DMI) refers to feed intake excluding its water content. The percentage of water is frequently determined by heating the feed on a paper plate in a microwave oven or using the Koster Tester to dry the feed. Ascertaining DMI can be useful for low-energy feeds with a high percentage of water in order to ensure adequate energy intake. Animals eating these kinds of feeds have been shown to consume less dry matter and food energy. A problem called dry matter loss can result from heat generation, as caused by microbial respiration. It decreases the content of nonstructural carbohydrate, protein, and food energy.\n\n"}
{"id": "480107", "url": "https://en.wikipedia.org/wiki?curid=480107", "title": "Ecological genetics", "text": "Ecological genetics\n\nEcological genetics is the study of genetics in natural populations.\n\nThis contrasts with classical genetics, which works mostly on crosses between laboratory strains, and DNA sequence analysis, which studies genes at the molecular level.\n\nResearch in this field is on traits of ecological significance—that is, traits related to fitness, which affect an organism's survival and reproduction. Examples might be: flowering time, drought tolerance, polymorphism, mimicry, avoidance of attacks by predators.\n\nResearch usually involves a mixture of field and laboratory studies. Samples of natural populations may be taken back to the laboratory for their genetic variation to be analysed. Changes in the populations at different times and places will be noted, and the pattern of mortality in these populations will be studied. Research is often done on insects and other organisms that have short generation times.\n\nAlthough work on natural populations had been done previously, it is acknowledged that the field was founded by the English biologist E.B. Ford (1901–1988) in the early 20th century. Ford was taught genetics at Oxford University by Julian Huxley, and started research on the genetics of natural populations in 1924. Ford also had a long working relationship with R.A. Fisher. By the time Ford had developed his formal definition of genetic polymorphism, Fisher had got accustomed to high natural selection values in nature. This was one of the main outcomes of research on natural populations. Ford's magnum opus was \"Ecological genetics\", which ran to four editions and was widely influential.\n\nOther notable ecological geneticists would include Theodosius Dobzhansky who worked on chromosome polymorphism in fruit flies. As a young researcher in Russia, Dobzhansky had been influenced by Sergei Chetverikov, who also deserves to be remembered as a founder of genetics in the field, though his significance was not appreciated until much later. Dobzhansky and colleagues carried out studies on natural populations of \"Drosophila\" species in western USA and Mexico over many years.\n\nPhilip Sheppard, Cyril Clarke, Bernard Kettlewell and A.J. Cain were all strongly influenced by Ford; their careers date from the post World War II era. Collectively, their work on lepidoptera, and on human blood groups, established the field, and threw light on selection in natural populations where its role had been once doubted.\n\nWork of this kind needs long-term funding, as well as grounding in both ecology and genetics. These are both difficult requirements. Research projects can last longer than a researcher's career; for instance, research into mimicry started 150 years ago, and is still going strongly. Funding of this type of research is still rather erratic, but at least the value of working with natural populations in the field cannot now be doubted.\n\n\n"}
{"id": "2199757", "url": "https://en.wikipedia.org/wiki?curid=2199757", "title": "Ectasian", "text": "Ectasian\n\nThe Ectasian Period (from Greek ἔκτασις \"(éktasis)\", meaning \"extension\") is the second geologic period in the Mesoproterozoic Era and lasted from Mya ago to Mya (million years ago). Instead of being based on stratigraphy, these dates are defined chronometrically.\n\nGeologically the name refers to the continued expansion of platform covers during this period.\n\nThis period is interesting for the first evidence of sexual reproduction. The 1.2 billion years old Hunting Formation on Somerset Island, Canada, dates from the end of the Ectasian. It contains the microfossils of the multicellular filaments of \"Bangiomorpha pubescens\" (type of red algae), the first taxonomically resolved eukaryote. This was the first organism that exhibited sexual reproduction, which is an essential feature for complex multicellularity. Complex multicellularity is different from \"simple\" multicellularity, such as colonies of organisms living together. True multicellular organisms contain cells that are specialized for different functions. This is, in fact, an essential feature of sexual reproduction as well, since the male and female gametes are specialized cells. Organisms that reproduce sexually must solve the problem of generating an entire organism from just the germ cells.\n\nSexual reproduction and the ability of gametes to develop into an organism are the necessary antecedents to true multicellularity. In fact, we tend to think of sexual reproduction and true multicellularity as occurring at the same time, and true multicellularity is often taken as a marker for sexual reproduction.\n\n\n"}
{"id": "11944078", "url": "https://en.wikipedia.org/wiki?curid=11944078", "title": "Efficient energy use", "text": "Efficient energy use\n\nEfficient energy use, sometimes simply called energy efficiency, is the goal to reduce the amount of energy required to provide products and services. For example, insulating a home allows a building to use less heating and cooling energy to achieve and maintain a comfortable temperature. Installing LED lighting, fluorescent lighting, or natural skylight windows reduces the amount of energy required to attain the same level of illumination compared to using traditional incandescent light bulbs. Improvements in energy efficiency are generally achieved by adopting a more efficient technology or production process or by application of commonly accepted methods to reduce energy losses.\n\nThere are many motivations to improve energy efficiency. Reducing energy use reduces energy costs and may result in a financial cost saving to consumers if the energy savings offset any additional costs of implementing an energy-efficient technology. Reducing energy use is also seen as a solution to the problem of reducing greenhouse gas emissions. According to the International Energy Agency, improved energy efficiency in buildings, industrial processes and transportation could reduce the world's energy needs in 2050 by one third, and help control global emissions of greenhouse gases. Another important solution is to remove government-led energy subsidies that promote high energy consumption and inefficient energy use in more than half of the countries in the world.\n\nEnergy efficiency and renewable energy are said to be the \"twin pillars\" of sustainable energy policy and are high priorities in the sustainable energy hierarchy. In many countries energy efficiency is also seen to have a national security benefit because it can be used to reduce the level of energy imports from foreign countries and may slow down the rate of energy at which domestic energy resources are depleted.\n\nEnergy efficiency has proved to be a cost-effective strategy for building economies without necessarily increasing energy consumption. For example, the state of California began implementing energy-efficiency measures in the mid-1970s, including building code and appliance standards with strict efficiency requirements. During the following years, California's energy consumption has remained approximately flat on a per capita basis while national US consumption doubled. As part of its strategy, California implemented a \"loading order\" for new energy resources that puts energy efficiency first, renewable electricity supplies second, and new fossil-fired power plants last. States such as Connecticut and New York have created quasi-public Green Banks to help residential and commercial building-owners finance energy efficiency upgrades that reduce emissions and cut consumers' energy costs.\n\nLovin's Rocky Mountain Institute points out that in industrial settings, \"there are abundant opportunities to save 70% to 90% of the energy and cost for lighting, fan, and pump systems; 50% for electric motors; and 60% in areas such as heating, cooling, office equipment, and appliances.\" In general, up to 75% of the electricity used in the US today could be saved with efficiency measures that cost less than the electricity itself, the same holds true for home settings. The US Department of Energy has stated that there is potential for energy saving in the magnitude of 90 Billion kWh by increasing home energy efficiency.\n\nOther studies have emphasized this. A report published in 2006 by the McKinsey Global Institute, asserted that \"there are sufficient economically viable opportunities for energy-productivity improvements that could keep global energy-demand growth at less than 1 percent per annum\"—less than half of the 2.2 percent average growth anticipated through 2020 in a business-as-usual scenario. Energy productivity, which measures the output and quality of goods and services per unit of energy input, can come from either reducing the amount of energy required to produce something, or from increasing the quantity or quality of goods and services from the same amount of energy.\n\nThe Vienna Climate Change Talks 2007 Report, under the auspices of the United Nations Framework Convention on Climate Change (UNFCCC), clearly shows \"that energy efficiency can achieve real emission reductions at low cost.\"\n\nInternational standards ISO17743 and ISO17742 provide a documented methodology for calculating and reporting on energy savings and energy efficiency for countries and cities.\n\nModern appliances, such as, freezers, ovens, stoves, dishwashers, and clothes washers and dryers, use significantly less energy than older appliances. Installing a clothesline will significantly reduce one's energy consumption as their dryer will be used less. Current energy-efficient refrigerators, for example, use 40 percent less energy than conventional models did in 2001. Following this, if all households in Europe changed their more than ten-year-old appliances into new ones, 20 billion kWh of electricity would be saved annually, hence reducing CO emissions by almost 18 billion kg. In the US, the corresponding figures would be 17 billion kWh of electricity and CO. According to a 2009 study from McKinsey & Company the replacement of old appliances is one of the most efficient global measures to reduce emissions of greenhouse gases. Modern power management systems also reduce energy usage by idle appliances by turning them off or putting them into a low-energy mode after a certain time. Many countries identify energy-efficient appliances using energy input labeling.\n\nThe impact of energy efficiency on peak demand depends on when the appliance is used. For example, an air conditioner uses more energy during the afternoon when it is hot. Therefore, an energy-efficient air conditioner will have a larger impact on peak demand than off-peak demand. An energy-efficient dishwasher, on the other hand, uses more energy during the late evening when people do their dishes. This appliance may have little to no impact on peak demand.\n\nBuildings are an important field for energy efficiency improvements around the world because of their role as a major energy consumer. However, the question of energy use in buildings is not straightforward as the indoor conditions that can be achieved with energy use vary a lot. The measures that keep buildings comfortable, lighting, heating, cooling and ventilation, all consume energy. Typically the level of energy efficiency in a building is measured by dividing energy consumed with the floor area of the building which is referred to as specific energy consumption (SEC) or energy use intensity (EUI):\n\nHowever, the issue is more complex as building materials have embodied energy in them. On the other hand, energy can be recovered from the materials when the building is dismantled by reusing materials or burning them for energy. Moreover, when the building is used, the indoor conditions can vary resulting in higher and lower quality indoor environments. Finally, overall efficiency is affected by the use of the building: is the building occupied most of the time and are spaces efficiently used — or is the building largely empty? It has even been suggested that for a more complete accounting of energy efficiency, SEC should be amended to include these factors:\n\nThus a balanced approach to energy efficiency in buildings should be more comprehensive than simply trying to minimize energy consumed. Issues such as quality of indoor environment and efficiency of space use should be factored in. Thus the measures used to improve energy efficiency can take many different forms. Often they include passive measures that inherently reduce the need to use energy, such as better insulation. Many serve various functions improving the indoor conditions as well as reducing energy use, such as increased use of natural light.\n\nA building's location and surroundings play a key role in regulating its temperature and illumination. For example, trees, landscaping, and hills can provide shade and block wind. In cooler climates, designing northern hemisphere buildings with south facing windows and southern hemisphere buildings with north facing windows increases the amount of sun (ultimately heat energy) entering the building, minimizing energy use, by maximizing passive solar heating. Tight building design, including energy-efficient windows, well-sealed doors, and additional thermal insulation of walls, basement slabs, and foundations can reduce heat loss by 25 to 50 percent.\n\nDark roofs may become up to 39 °C (70 °F) hotter than the most reflective white surfaces. They transmit some of this additional heat inside the building. US Studies have shown that lightly colored roofs use 40 percent less energy for cooling than buildings with darker roofs. White roof systems save more energy in sunnier climates. Advanced electronic heating and cooling systems can moderate energy consumption and improve the comfort of people in the building.\n\nProper placement of windows and skylights as well as the use of architectural features that reflect light into a building can reduce the need for artificial lighting. Increased use of natural and task lighting has been shown by one study to increase productivity in schools and offices. Compact fluorescent lamps use two-thirds less energy and may last 6 to 10 times longer than incandescent light bulbs. Newer fluorescent lights produce a natural light, and in most applications they are cost effective, despite their higher initial cost, with payback periods as low as a few months. LED lamps use only about 10% of the energy an incandescent lamp requires.\n\nEffective energy-efficient building design can include the use of low cost Passive Infra Reds (PIRs) to switch-off lighting when areas are unnoccupied such as toilets, corridors or even office areas out-of-hours. In addition, lux levels can be monitored using daylight sensors linked to the building's lighting scheme to switch on/off or dim the lighting to pre-defined levels to take into account the natural light and thus reduce consumption. Building Management Systems (BMS) link all of this together in one centralised computer to control the whole building's lighting and power requirements.\n\nIn an analysis that integrates a residential bottom-up simulation with an economic multi-sector model, it has been shown that variable heat gains caused by insulation and air-conditioning efficiency can have load-shifting effects that are not uniform on the electricity load. The study also highlighted the impact of higher household efficiency on the power generation capacity choices that are made by the power sector.\n\nThe choice of which space heating or cooling technology to use in buildings can have a significant impact on energy use and efficiency. For example, replacing an older 50% efficient natural gas furnace with a new 95% efficient one will dramatically reduce energy use, carbon emissions, and winter natural gas bills. Ground source heat pumps can be even more energy-efficient and cost-effective. These systems use pumps and compressors to move refrigerant fluid around a thermodynamic cycle in order to \"pump\" heat against its natural flow from hot to cold, for the purpose of transferring heat into a building from the large thermal reservoir contained within the nearby ground. The end result is that heat pumps typically use four times less electrical energy to deliver an equivalent amount of heat than a direct electrical heater does. Another advantage of a ground source heat pump is that it can be reversed in summertime and operate to cool the air by transferring heat from the building to the ground. The disadvantage of ground source heat pumps is their high initial capital cost, but this is typically recouped within five to ten years as a result of lower energy use.\n\nSmart meters are slowly being adopted by the commercial sector to highlight to staff and for internal monitoring purposes the building's energy usage in a dynamic presentable format. The use of Power Quality Analysers can be introduced into an existing building to assess usage, harmonic distortion, peaks, swells and interruptions amongst others to ultimately make the building more energy-efficient. Often such meters communicate by using wireless sensor networks.\n\nGreen Building XML (gbXML) is an emerging schema, a subset of the Building Information Modeling efforts, focused on green building design and operation. gbXML is used as input in several energy simulation engines. But with the development of modern computer technology, a large number of building performance simulation tools are available on the market. When choosing which simulation tool to use in a project, the user must consider the tool's accuracy and reliability, considering the building information they have at hand, which will serve as input for the tool. Yezioro, Dong and Leite developed an artificial intelligence approach towards assessing building performance simulation results and found that more detailed simulation tools have the best simulation performance in terms of heating and cooling electricity consumption within 3% of mean absolute error.\n\nLeadership in Energy and Environmental Design (LEED) is a rating system organized by the US Green Building Council (USGBC) to promote environmental responsibility in building design. They currently offer four levels of certification for existing buildings (LEED-EBOM) and new construction (LEED-NC) based on a building's compliance with the following criteria: Sustainable sites, water efficiency, energy and atmosphere, materials and resources, indoor environmental quality, and innovation in design. In 2013, USGBC developed the LEED Dynamic Plaque, a tool to track building performance against LEED metrics and a potential path to recertification. The following year, the council collaborated with Honeywell to pull data on energy and water use, as well as indoor air quality from a BAS to automatically update the plaque, providing a near-real-time view of performance. The USGBC office in Washington, D.C. is one of the first buildings to feature the live-updating LEED Dynamic Plaque.\n\nA deep energy retrofit is a whole-building analysis and construction process that uses to achieve much larger energy savings than conventional energy retrofits. Deep energy retrofits can be applied to both residential and non-residential (“commercial”) buildings. A deep energy retrofit typically results in energy savings of 30 percent or more, perhaps spread over several years, and may significantly improve the building value. The Empire State Building has undergone a deep energy retrofit process that was completed in 2013. The project team, consisting of representatives from Johnson Controls, Rocky Mountain Institute, Clinton Climate Initiative, and Jones Lang LaSalle will have achieved an annual energy use reduction of 38% and $4.4 million. For example, the 6,500 windows were remanufactured onsite into superwindows which block heat but pass light. Air conditioning operating costs on hot days were reduced and this saved $17 million of the project's capital cost immediately, partly funding other retrofitting. Receiving a gold Leadership in Energy and Environmental Design (LEED) rating in September 2011, the Empire State Building is the tallest LEED certified building in the United States.\nThe Indianapolis City-County Building recently underwent a deep energy retrofit process, which has achieved an annual energy reduction of 46% and $750,000 annual energy saving.\n\nEnergy retrofits, including deep, and other types undertaken in residential, commercial or industrial locations are generally supported through various forms of financing or incentives. Incentives include pre-packaged rebates where the buyer/user may not even be aware that the item being used has been rebated or \"bought down\". \"Upstream\" or \"Midstream\" buy downs are common for efficient lighting products. Other rebates are more explicit and transparent to the end user through the use of formal applications. In addition to rebates, which may be offered through government or utility programs, governments sometimes offer tax incentives for energy efficiency projects. Some entities offer rebate and payment guidance and facilitation services that enable energy end use customers tap into rebate and incentive programs.\n\nTo evaluate the economic soundness of energy efficiency investments in buildings, cost-effectiveness analysis or CEA can be used. A CEA calculation will produce the value of energy saved, sometimes called \"negawatts\", in $/kWh. The energy in such a calculation is virtual in the sense that it was never consumed but rather saved due to some energy efficiency investment being made. Thus CEA allows comparing the price of negawatts with price of energy such as electricity from the grid or the cheapest renewable alternative. The benefit of the CEA approach in energy systems is that it avoids the need to guess future energy prices for the purposes of the calculation, thus removing the major source of uncertainty in the appraisal of energy efficiency investments.\n\nEnergy efficiency targets for 2020 and 2030.\n\nThe first EU-wide target was set in 1998. Member states agreed to improve energy efficiency by 1 percent a year over twelve years. In addition, legislation about products, industry, transport and buildings has contributed to a general energy efficiency framework. More effort is needed to address heating and cooling: there is more heat wasted during electricity production in Europe than is required to heat all buildings in the continent. All in all, EU energy efficiency legislation is estimated to deliver savings worth the equivalent of up to 326 million tons of oil per year by 2020.\n\nThe EU set itself a 20% energy savings target by 2020 compared to 1990 levels, but member states decide individually how energy savings will be achieved. At an EU summit in October 2014, EU countries agreed on a new energy efficiency target of 27% or greater by 2030. One mechanism used to achieve the target of 27% is the 'Suppliers Obligations & White Certificates'. The ongoing debate around the 2016 Clean Energy Package also puts an emphasis on energy efficiency, but the goal will probably remain around 30% greater efficiency compared to 1990 levels. Some have argued that this will not be enough for the EU to meet its Paris Agreement goals of reducing greenhouse gas emissions by 40% compared to 1990 levels.\n\nThe Australian national government is actively leading the country in efforts to increase their energy efficiency, mainly through the government's Department of Industry and Science. In July 2009, the Council of Australian Governments, which represents the individual states and territories of Australia, agreed to a National Strategy on Energy Efficiency (NSEE).\n\nThis is a ten-year plan accelerate the implementation of a nationwide adoption of energy-efficient practices and a preparation for the country's transformation into a low carbon future. There are several different areas of energy use addressed within the NSEE. But, the chapter devoted to the approach on energy efficiency that is to be adopted on a national level stresses four points in achieving stated levels of energy efficiency. They are:\n\nThe overriding agreement that governs this strategy is the National Partnership Agreement on Energy Efficiency.\n\nThis document also explains the role of both the commonwealth and the individual states and territories in the NSEE, as well provides for the creation of benchmarks and measurement devices which will transparently show the nation's progress in relation to the stated goals, and addresses the need for funding of the strategy in order to enable it to move forward.\n\nIn August 2017, the Government of Canada released Build Smart - Canada's Buildings Strategy, as a key driver of the Pan Canadian Framework on Clean Growth and Climate Change, Canada's national climate strategy.\n\nThe Build Smart strategy seeks to dramatically increase the energy-efficiency performance of existing and new Canadian buildings, and establishes five goals to that end:\n\nThe strategy details a range of activities the Government of Canada will pursue, and investments it will make, in support of the goals. As of early 2018, only one of Canada's 10 provinces and three territories, British Columbia, has developed a policy in support of federal government's goal to reach net zero energy ready ambitions: the BC Energy Step Code.\n\nLocal British Columbia governments may use the BC Energy Step Code, if they wish, to incentivize or require a level of energy efficiency in new construction that goes above and beyond the requirements of the base building code. The regulation and standard is designed as a technical roadmap to help the province reach its target that all new buildings will attain a net zero energy ready level of performance by 2032.\n\nEnergy efficiency is central to energy policy in Germany.\nAs of late 2015, national policy includes the following efficiency and consumption targets (with actual values for 2014):\n\nRecent progress toward improved efficiency has been steady aside from the financial crisis of 2007–08.\nSome however believe energy efficiency is still under-recognised in terms of its contribution to Germany's energy transformation (or \"Energiewende\").\n\nEfforts to reduce final energy consumption in transport sector have not been successful, with a growth of 1.7% between 2005–2014. This growth is due to both road passenger and road freight transport. Both sectors increased their overall distance travelled to record the highest figures ever for Germany. Rebound effects played a significant role, both between improved vehicle efficiency and the distance travelled, and between improved vehicle efficiency and an increase in vehicle weights and engine power.\n\nOn 3 December 2014, the German federal government released its National Action Plan on Energy Efficiency (NAPE).\nThe areas covered are the energy efficiency of buildings, energy conservation for companies, consumer energy efficiency, and transport energy efficiency. The policy contains both immediate and forward-looking measures. The central short-term measures of NAPE include the introduction of competitive tendering for energy efficiency, the raising of funding for building renovation, the introduction of tax incentives for efficiency measures in the building sector, and the setting up energy efficiency networks together with business and industry. German industry is expected to make a sizeable contribution.\n\nOn 12 August 2016, the German government released a green paper on energy efficiency for public consultation (in German). It outlines the potential challenges and actions needed to reduce energy consumption in Germany over the coming decades. At the document's launch, economics and energy minister Sigmar Gabriel said \"we do not need to produce, store, transmit and pay for the energy that we save\". The green paper prioritizes the efficient use of energy as the \"first\" response and also outlines opportunities for sector coupling, including using renewable power for heating and transport. Other proposals include a flexible energy tax which rises as petrol prices fall, thereby incentivizing fuel conservation despite low oil prices.\n\nIn May 2016 Poland adopted a new Act on Energy Efficiency, to enter into force on 1October 2016.\n\nA 2011 Energy Modeling Forum study covering the United States examines how energy efficiency opportunities will shape future fuel and electricity demand over the next several decades. The US economy is already set to lower its energy and carbon intensity, but explicit policies will be necessary to meet climate goals. These policies include: a carbon tax, mandated standards for more efficient appliances, buildings and vehicles, and subsidies or reductions in the upfront costs of new more energy-efficient equipment.\n\nIndustries use a large amount of energy to power a diverse range of manufacturing and resource extraction processes. Many industrial processes require large amounts of heat and mechanical power, most of which is delivered as natural gas, petroleum fuels, and electricity. In addition some industries generate fuel from waste products that can be used to provide additional energy.\n\nBecause industrial processes are so diverse it is impossible to describe the multitude of possible opportunities for energy efficiency in industry. Many depend on the specific technologies and processes in use at each industrial facility. There are, however, a number of processes and energy services that are widely used in many industries.\n\nVarious industries generate steam and electricity for subsequent use within their facilities. When electricity is generated, the heat that is produced as a by-product can be captured and used for process steam, heating or other industrial purposes. Conventional electricity generation is about 30% efficient, whereas combined heat and power (also called co-generation) converts up to 90 percent of the fuel into usable energy.\n\nAdvanced boilers and furnaces can operate at higher temperatures while burning less fuel. These technologies are more efficient and produce fewer pollutants.\n\nOver 45 percent of the fuel used by US manufacturers is burnt to make steam. The typical industrial facility can reduce this energy usage 20 percent (according to the US Department of Energy) by insulating steam and condensate return lines, stopping steam leakage, and maintaining steam traps.\n\nElectric motors usually run at a constant speed, but a variable speed drive allows the motor's energy output to match the required load. This achieves energy savings ranging from 3 to 60 percent, depending on how the motor is used. Motor coils made of superconducting materials can also reduce energy losses. Motors may also benefit from voltage optimisation. \n\nIndustry uses a large number of pumps and compressors of all shapes and sizes and in a wide variety of applications. The efficiency of pumps and compressors depends on many factors but often improvements can be made by implementing better process control and better maintenance practices. Compressors are commonly used to provide compressed air which is used for sand blasting, painting, and other power tools. According to the US Department of Energy, optimizing compressed air systems by installing variable speed drives, along with preventive maintenance to detect and fix air leaks, can improve energy efficiency 20 to 50 percent.\n\nThe estimated energy efficiency for an automobile is 280 Passenger-Mile/10 Btu. There are several ways to enhance a vehicle's energy efficiency. Using improved aerodynamics to minimize drag can increase vehicle fuel efficiency. Reducing vehicle weight can also improve fuel economy, which is why composite materials are widely used in car bodies.\n\nMore advanced tires, with decreased tire to road friction and rolling resistance, can save gasoline. Fuel economy can be improved by up to 3.3% by keeping tires inflated to the correct pressure. Replacing a clogged air filter can improve a cars fuel consumption by as much as 10 percent on older vehicles. On newer vehicles (1980s and up) with fuel-injected, computer-controlled engines, a clogged air filter has no effect on mpg but replacing it may improve acceleration by 6-11 percent. Aerodynamics also aid in efficiency of a vehicle. The design of a car impacts the amount of gas needed to move it through air. Aerodynamics involves the air around the car, which can affect the efficiency of the energy expended. \n\nTurbochargers can increase fuel efficiency by allowing a smaller displacement engine. The 'Engine of the year 2011' is a Fiat 500 engine equipped with an MHI turbocharger. \"Compared with a 1.2-liter 8v engine, the new 85 HP turbo has 23% more power and a 30% better performance index. The performance of the two-cylinder is not only equivalent to a 1.4-liter 16v engine, but fuel consumption is 30% lower.\"\n\nEnergy-efficient vehicles may reach twice the fuel efficiency of the average automobile. Cutting-edge designs, such as the diesel Mercedes-Benz Bionic concept vehicle have achieved a fuel efficiency as high as , four times the current conventional automotive average.\n\nThe mainstream trend in automotive efficiency is the rise of electric vehicles (all@electric or hybrid electric). Hybrids, like the Toyota Prius, use regenerative braking to recapture energy that would dissipate in normal cars; the effect is especially pronounced in city driving. Plug-in hybrids also have increased battery capacity, which makes it possible to drive for limited distances without burning any gasoline; in this case, energy efficiency is dictated by whatever process (such as coal-burning, hydroelectric, or renewable source) created the power. Plug-ins can typically drive for around purely on electricity without recharging; if the battery runs low, a gas engine kicks in allowing for extended range. Finally, all-electric cars are also growing in popularity; the Tesla Model S sedan is the only high-performance all-electric car currently on the market.\n\nCities around the globe light up millions of streets with 300 million lights. Some cities are seeking to reduce street light power consumption by dimming lights during off-peak hours or switching to LED lamps. It is not clear whether the high luminous efficiency of LEDs will lead to real reductions in energy, as cities may end up installing extra lamps or lighting areas more brightly than in the past.\n\nThere are several ways to reduce energy usage in air transportation, from modifications to the planes themselves, to how air traffic is managed. As in cars, turbochargers are an effective way to reduce energy consumption; however, instead of allowing for the use of a smaller-displacement engine, turbochargers in jet turbines operate by compressing the thinner air at higher altitudes. This allows the engine to operate as if it were at sea-level pressures while taking advantage of the reduced drag on the aircraft at higher altitudes.\n\nAir traffic management systems are another way to increase the efficiency of not just the aircraft but the airline industry as a whole. New technology allows for superior automation of takeoff, landing, and collision avoidance, as well as within airports, from simple things like HVAC and lighting to more complex tasks such as security and scanning.\n\nAlternative fuels, known as non-conventional or advanced fuels, are any materials or substances that can be used as fuels, other than conventional fuels. Some well known alternative fuels include biodiesel, bioalcohol (methanol, ethanol, butanol), chemically stored electricity (batteries and fuel cells), hydrogen, non-fossil methane, non-fossil natural gas, vegetable oil, and other biomass sources.\n\nEnergy conservation is broader than energy efficiency in including active efforts to decrease energy consumption, for example through behaviour change, in addition to using energy more efficiently. Examples of conservation without efficiency improvements are heating a room less in winter, using the car less, air-drying your clothes instead of using the dryer, or enabling energy saving modes on a computer. As with other definitions, the boundary between efficient energy use and energy conservation can be fuzzy, but both are important in environmental and economic terms. This is especially the case when actions are directed at the saving of fossil fuels. Energy conservation is a challenge requiring policy programmes, technological development and behavior change to go hand in hand. Many energy intermediary organisations, for example governmental or non-governmental organisations on local, regional, or national level, are working on often publicly funded programmes or projects to meet this challenge. Psychologists have also engaged with the issue of energy conservation and have provided guidelines for realizing behavior change to reduce energy consumption while taking technological and policy considerations into account.\n\nThe National Renewable Energy Laboratory maintains a comprehensive list of apps useful for energy efficiency.\n\nCommercial property managers that plan and manage energy efficiency projects generally use a software platform to perform energy audits and to collaborate with contractors to understand their full range of options. The Department of Energy (DOE) Software Directory describes EnergyActio software, a cloud based platform designed for this purpose.\n\nEnergy efficiency and renewable energy are said to be the “twin pillars” of a sustainable energy policy. Both strategies must be developed concurrently in order to stabilize and reduce carbon dioxide emissions. Efficient energy use is essential to slowing the energy demand growth so that rising clean energy supplies can make deep cuts in fossil fuel use. If energy use grows too rapidly, renewable energy development will chase a receding target. Likewise, unless clean energy supplies come online rapidly, slowing demand growth will only begin to reduce total carbon emissions; a reduction in the carbon content of energy sources is also needed. A sustainable energy economy thus requires major commitments to both efficiency and renewables.\n\nCompanies such as Lieef (www.Lieef.com) have started reporting ESG metrics on behalf of companies, and investment funds, in an effort to increase transparency in the space which to date has increased in importance, but has not found a unified measurement tool. In addition, the majority of companies that report sustainability do so on a 'net' basis, and do not reflect their carbon emissions, and separate those emissions from their activities that offset those emissions, such as buying renewable credits and green power..\n\nIf the demand for energy services remains constant, improving energy efficiency will reduce energy consumption and carbon emissions. However, many efficiency improvements do not reduce energy consumption by the amount predicted by simple engineering models. This is because they make energy services cheaper, and so consumption of those services increases. For example, since fuel efficient vehicles make travel cheaper, consumers may choose to drive farther, thereby offsetting some of the potential energy savings. Similarly, an extensive historical analysis of technological efficiency improvements has conclusively shown that energy efficiency improvements were almost always outpaced by economic growth, resulting in a net increase in resource use and associated pollution. These are examples of the direct rebound effect.\n\nEstimates of the size of the rebound effect range from roughly 5% to 40%. The rebound effect is likely to be less than 30% at the household level and may be closer to 10% for transport. A rebound effect of 30% implies that improvements in energy efficiency should achieve 70% of the reduction in energy consumption projected using engineering models. The rebound effect may be particularly large for lighting, because in contrast to tasks like transport there is effectively no upper limit on how much light could be considered useful. In fact, it appears that lighting has accounted for about 0.7% of GDP across many societies and hundreds of years, implying a rebound effect of 100%.\n\nInternational\n\nChina\n\nAustralia\n\nEuropean Union\nIceland\nIndia\n\nIndonesia\n\nJapan\n\nLebanon\n\nUnited Kingdom\n\nUnited States\n"}
{"id": "10388235", "url": "https://en.wikipedia.org/wiki?curid=10388235", "title": "Euboea (mythology)", "text": "Euboea (mythology)\n\nEuboea (/juːˈbiːə/; Ancient Greek: Ευβοια) was the name of several women in Greek mythology.\n\n"}
{"id": "40408388", "url": "https://en.wikipedia.org/wiki?curid=40408388", "title": "Fast radio burst", "text": "Fast radio burst\n\nIn radio astronomy, a fast radio burst (FRB) is a high-energy astrophysical phenomenon of unknown origin manifested as a transient radio pulse lasting a few milliseconds on average. The first FRB was discovered by Duncan Lorimer and his student David Narkevic in 2007 when they were looking through archival pulsar survey data, and it is therefore commonly referred to as Lorimer Burst. Many FRBs have since been found, including a repeating FRB. Although the exact origin and cause is uncertain, they are almost definitely extragalactic.\n\nWhen the FRBs are polarized, it indicates that they are emitted from a source contained within an extremely powerful magnetic field. The origin of the FRBs has yet to be determined; proposals for its origin range from a rapidly rotating neutron star and a black hole, to extraterrestrial intelligence.\n\nThe localization and characterization of the one known repeating source, FRB 121102, has revolutionized the understanding of the source class. FRB 121102 is identified with a galaxy at a distance of approximately 3 billion light years, well outside the Milky Way Galaxy, and embedded in an extreme environment.\n\nFast radio bursts are named by the date the signal was recorded, as \"FRB YYMMDD\". The first fast radio burst to be described, the Lorimer Burst FRB 010724, was identified in 2007 in archived data recorded by the Parkes Observatory on 24 July 2001. Since then, most known FRBs have been found in previously recorded data. On 19 January 2015, astronomers at Australia's national science agency (CSIRO) reported that a fast radio burst had been observed for the first time live, by the Parkes Observatory.\n\nFast radio bursts are bright, unresolved (pointsource-like), broadband (spanning a large range of radio frequencies), millisecond flashes found in parts of the sky outside the Milky Way. Unlike many radio sources the signal from a burst is detected in a short period of time with enough strength to stand out from the noise floor. The burst usually appears as a single spike of energy without any change in its strength over time. The bursts last for a period of several milliseconds (thousandths of a second). The bursts come from all over the sky, and are not concentrated on the plane of the Milky Way. Known FRB locations are biased by the parts of the sky that the observatories can image.\n\nThe component frequencies of each burst are delayed by different amounts of time depending on the wavelength. This delay is described by a value referred to as a dispersion measure (DM). This results in a received signal that sweeps rapidly down in frequency, as longer wavelengths are delayed more.\n\nFast radio bursts have pulse dispersion measurements which are much larger (> 100 pc cm) than expected for a source inside the Milky Way; and consistent with propagation through an ionized plasma, and furthermore their distribution is isotropic (not especially coming from the galactic plane), thus they are conjectured to be of extragalactic origin.\n\nThe interferometer UTMOST has also put a lower limit of 10,000 kilometers for the distance to the FRBs it has detected, supporting the case for an astronomical origin (because signal sources on Earth are ruled out as being closer than this limit). This limit can be determined from the fact that closer sources would have a curved wave front that could be detected by the multiple antennas of the interferometer.\n\nThe first FRB, the Lorimer Burst FRB 010724, was discovered in 2007 when Duncan Lorimer assigned his student David Narkevic to look through archival data taken in 2001 by the Parkes radio dish in Australia.\nAnalysis of the survey data found a 30-jansky dispersed burst which occurred on 24 July 2001, less than 5 milliseconds in duration, located 3° from the Small Magellanic Cloud. The reported burst properties argue against a physical association with the Milky Way galaxy or the Small Magellanic Cloud. The burst became known as the Lorimer Burst. The discoverers argue that current models for the free electron content in the universe imply that the burst is less than 1 gigaparsec distant. The fact that no further bursts were seen in 90 hours of additional observations implies that it was a singular event such as a supernova or merger of relativistic objects. It is suggested that hundreds of similar events could occur every day and, if detected, could serve as cosmological probes.\n\nIn 2010 there was a report of 16 similar pulses: clearly of terrestrial origin; detected by the Parkes radio telescope; and given the name perytons. In 2015 perytons were shown to be generated when microwave oven doors were suddenly opened during a heating cycle, with emission generated by the magnetron.\n\nIn 2015, FRB 110523 was discovered in archival data from the Green Bank Telescope. It was the first FRB for which linear polarization was detected (allowing a measurement of Faraday rotation). Measurement of the signal's dispersion delay suggested that this burst is of extragalactic origin, possibly up to 6 billion light years away.\n\nVictoria Kaspi of McGill University estimated that as many as 10,000 fast radio bursts may occur per day over the entire sky.\n\nAn observation in 2012 of a fast radio burst (FRB 121102) in the direction of Auriga in the northern hemisphere using the Arecibo radio telescope confirmed the extragalactic origin of fast radio pulses by an effect known as plasma dispersion.\n\nIn November 2015, astronomer Paul Scholz at McGill University in Canada, found ten non-periodically repeated fast radio pulses in archival data gathered in May and June 2015 by the Arecibo radio telescope. The ten bursts have dispersion measures and sky positions consistent with the original burst FRB 121102, detected in 2012. Like the 2012 burst, the 10 bursts have a plasma dispersion measure that is three times larger than possible for a source in the Milky Way Galaxy. \nThe team thinks that this finding rules out self-destructive, cataclysmic events that could only occur once, such as the explosion of a black hole or the collision between two neutron stars. According to the scientists, the data support an origin in a young rotating neutron star (pulsar), or in a highly magnetized neutron star (magnetar), or from highly magnetized pulsars travelling through asteroid belts, or from an intermittent Roche lobe overflow in a neutron star-white dwarf binary.\n\nOn 16 December 2016 six new FRBs were reported in the same direction (one having been received on 13 Nov 2015, four on 19 Nov 2015, and one on 8 Dec 2015). This is the only known instance in which these signals have been found twice in the same location in space. FRB 121102 is located at a minimum distance of about 1150 AU away from Earth leaving out possibility of human-made source, and is almost certainly extragalactic in nature.\n\nAs of January 2017, FRB 121102 is thought to be co-located in a dwarf galaxy about three billion light-years from Earth with a low-luminosity active galactic nucleus, or a previously unknown type of extragalactic source, or a young neutron star energising a supernova remnant.\n\nOn 26 August 2017, astronomers using data from the Green Bank Telescope detected 15 additional repeating FRBs coming from FRB 121102 at 5 to 8 GHz. The researchers also noted that FRB 121102 is presently in a \"heightened activity state, and follow-on observations are encouraged, particularly at higher radio frequencies\". The waves are highly polarized, meaning \"twisting\" transverse waves, that could only have formed when passing through hot plasma with an extremely strong magnetic field. FRB 121102's radio bursts are about 500 times more twisted (polarized) than those from any other FRB to date. Since it is a repeating FRB source, it suggests that it does not come from some one-time cataclysmic event, so a hypothesis proposed in January 2018 proposes that these particular repeating bursts may come from a dense stellar core called a neutron star near an extremely powerful magnetic field, such as one near a massive black hole, or one embedded in a nebula.\n\nIn April 2018, it was reported that FRB 121102 consisted of 21 bursts spanning one hour. In September 2018, an additional 72 bursts spanning five hours had been detected using a convolutional neural network.\n\nIn 2013, four bursts were identified that supported the likelihood of extragalactic sources.\n\nIn 2014, FRB 140514 was caught 'live' in 2014 and was found to be 21% (±7%) circularly polarised.\n\nFast radio bursts discovered up until 2015 had dispersion measures that were close to multiples of 187.5 pc cm. However subsequent observations do not fit this pattern.\n\nOn 18 April 2015, FRB 150418 was detected by the Parkes observatory and within hours, several telescopes including the Australia Telescope Compact Array caught an \"afterglow\" of the flash, which took six days to fade. The Subaru telescope was used to find what was thought to be the host galaxy and determine its redshift and the implied distance to the burst.\n\nHowever, the origin of the burst was soon disputed, and by April 2016 it was established that the emission instead originates from an active galactic nucleus that is powered by a supermassive black hole with dual jets blasting outward from the black hole. It was also noted that what was thought to be an \"afterglow\", did not fade away as would be expected, meaning that what was observed was unassociated with the actual fast radio burst.\n\nThe upgraded Molonglo Observatory Synthesis Telescope (UTMOST), near Canberra (Australia), reported finding 3 more FRBs. A 180-day 3 part survey in 2015 and 2016 found 3 FRBs at 843 MHz. Each FRB located with a narrow elliptical 'beam'; the relatively narrow band 828–858 MHz gives a less precise dispersion measure (DM).\n\nA short survey using part of ASKAP found one FRB in 3.4 days. FRB170107 was bright with a fluence of 58±6 Jy ms.\n\nThe upcoming and unusual Canadian radio telescope called CHIME will also be used to detect \"hundreds\" of fast radio bursts as its secondary objective.\n\nAccording to Anastasia Fialkov and Abraham Loeb, FRB's could be occurring as often as once per second. Earlier research could not identify the occurrence of FRB's to this degree.\n\nThree FRBs were reported in March 2018 by Parkes Observatory in Australia. One (FRB 180309) had the highest signal to noise ratio yet seen of 411.\n\nFRB 180725A was reported (by CHIME) as the first detection of a FRB under 700 MHz – as low as 580 MHz.\n\nIn October 2018, astronomers reported 19 more new non-repeating FRB bursts detected by the Australian Square Kilometre Array Pathfinder (ASKAP). These included three with dispersion measure (DM) smaller than seen before.\n\nBecause of the isolated nature of the observed phenomenon, the nature of the source remains speculative. , there is no generally accepted explanation. The source is estimated to be no larger than a few hundred kilometers in size because of causality (the bursts last for only a few milliseconds). If the bursts come from cosmological distances, their sources must be very energetic, equivalent to the amount released by the Sun in 80 years.\n\nOne possible explanation would be a collision between very dense objects like merging black holes or neutron stars. It has been suggested that there is a connection to gamma-ray bursts. Some have speculated that these signals might be artificial in origin, that they may be signs of extraterrestrial intelligence.\n\nIn 2007, just after the publication of the e-print with the first discovery, it was proposed that fast radio bursts could be related to hyperflares of magnetars. In 2015 three studies supported the magnetar hypothesis.\n\nEspecially energetic supernova could be the source of these bursts. Blitzars were proposed in 2013 as an explanation.\nIn 2014 it was suggested that following dark matter-induced collapse of pulsars, the resulting expulsion of the pulsar magnetospheres could be the source of fast radio bursts. In 2015 it was suggested that FRBs are caused by explosive decays of axion miniclusters. Another exotic possible source are cosmic strings that produced these burst as they interacted with the plasma that permeated the early Universe. In 2016 the collapse of the magnetospheres of Kerr–Newman black holes were proposed to explain the origin of the FRBs' \"afterglow\" and the weak gamma-ray transient 0.4 s after GW 150914. It has also been proposed that if fast radio bursts originate in black hole explosions, FRBs would be the first detection of quantum gravity effects. In early 2017, it was proposed that the strong magnetic field near a supermassive black hole could destabilize the current sheets within a pulsar's magnetosphere, releasing trapped energy to power the FRBs.\n\nRepeated bursts of FRB 121102 have initiated multiple origin hypotheses. A coherent emission phenomenon known as superradiance, which involves large-scale entangled quantum mechanical states possibly arising in environments such as active galactic nuclei, has been proposed to explain these and other associated observations with FRBs (e.g. high event rate, variable intensity profiles).\n\nFRBs are also cataloged at frbcat.\n"}
{"id": "1245462", "url": "https://en.wikipedia.org/wiki?curid=1245462", "title": "Friends' Ambulance Unit", "text": "Friends' Ambulance Unit\n\nThe Friends' Ambulance Unit (FAU) was a volunteer ambulance service, founded by individual members of the British Religious Society of Friends (Quakers), in line with their Peace Testimony. The FAU operated from 1914–1919, 1939–1946 and 1946–1959 in 25 different countries around the world. It was independent of the Quakers' organisation and chiefly staffed by registered conscientious objectors.\n\nThe Unit was founded as The First Anglo-Belgian Ambulance Unit at the start of World War I in 1914 and later renamed the Friends' Ambulance Unit. Members were trained at Jordans, a hamlet in Buckinghamshire, that was a centre for Quakerism. Altogether it sent over a thousand men to France and Belgium, where they worked on ambulance convoys and ambulance trains with the French and British armies. The FAU came under the jurisdiction of the British Red Cross Society. It was dissolved in 1919.\n\nIt was refounded by a committee of former members at the start of World War II in September 1939 with the establishment of a training camp at Manor Farm, Bristol Road, Northfield, Birmingham. More than 1,300 members were trained and went on to serve as ambulance drivers and medical orderlies in London during the Blitz, as well as overseas in Finland, Norway and Sweden (1940), the Middle East (1940–1943), Greece (1941, 1944–1946), China and Syria (1941–1946), India and Ethiopia (1942–1945), Italy (1943–1946), France, Belgium, Netherlands, Yugoslavia and Germany (1944–1946) and Austria (1945–1946).\n\nThe Sino-Japanese War had led to deteriorating conditions in China and in 1941 agreement was reached for the FAU to deploy 40 volunteers to deliver medical aid (dubbed the \"China Convoy\"). At first, their job was to secure the delivery of supplies via the \"Burma Road\", the sole remaining route. When Burma fell to the Japanese in May 1942, the FAU volunteers escaped to India and China. They regrouped and took on the distribution of medical supplies delivered by \"The Hump\", the air transport route to Kunming. It is estimated that 80% of medical supplies to China were distributed by the FAU.\n\nThe FAU's role expanded and they provided a range medical treatments, preventative measures and training of Chinese medical personnel. This expanded further into the reconstruction of medical facilities, notably the hospital at Tengchong in 1944, and into agricultural improvements and training.\n\nThe activities in China were international, employing personnel, men and women, from Britain (the largest national group), China, United States, Canada, New Zealand and elsewhere. Around 200 foreigners took part, eight died and others had their health permanently damaged. About half of the recruits were Quakers but all had a commitment to pacifism and wished to deliver practical help.\n\nResponsibility for the relief work in China was passed to the American Friends Service Committee in 1946.\n\nTwo 12-man sections with eight vehicles, FAU Relief Sections Nos 1 and 2, landed at Arromanches, Normandy on 6 September 1944 from a tank landing craft. Attached to the British Army's civilian affairs branch, the FAU sections provided relief to civilians in Normandy. No 2 FAU was then posted to a newly liberated refugee camp at Leopoldsburg, Belgium, managing reception, registration, disinfection, catering, dormitories and departures.\n\nIn November 1944, in response to a request from 21st Army Group, a further five more sections were established and arrived in Europe at the end of 1944. One new member was Gerald Gardiner, who subsequently became Lord Chancellor in Harold Wilson's Labour Party government of 1964–1970.\n\nAfter a period in Nijmegen, assisting local civilian medical organisations during Operation Market Garden, No 2 FAU cared for a colony of the mentally ill near Cleves in Germany which grew to a population of 25,000. By April, the main work had become the accommodation and care of displaced persons until they could return home. No 2 FAU was heavily involved with the care and support of inmates at the newly liberated Stalag X-B prisoner-of-war camp near Sandbostel, between Bremen and Hamburg in northern Germany in May 1945.\n\nThe FAU was wound up in 1946 and replaced by the Friends Ambulance Unit Post-War Service, which continued until 1959.\n\nThe work of the Friends' Ambulance Unit was referred to in the 1947 award of the Nobel Peace Prize to Quakers worldwide and accepted by the Friends Service Council and the American Friends Service Committee.\n\nThe original trainees in the 1939 training camp issued a statement expressing their purpose:\n\nWe purpose to train ourselves as an efficient Unit to undertake ambulance and relief work in areas under both civilian and military control, and so, by working as a pacifist and civilian body where the need is greatest, to demonstrate the efficacy of co-operating to build up a new world rather than fighting to destroy the old.\n\nWhile respecting the views of those pacifists who feel they cannot join an organization such as our own, we feel concerned among the bitterness and conflicting ideologies of the present situation to build up a record of goodwill and positive service, hoping that this will help to keep uppermost in men's minds those values which are so often forgotten in war and immediately afterwards.\n\n\nMuch archival material has survived and has been deposited at Friends House Library, Euston Road, London. The Library has produced Guides to the material:\n\n\n\n"}
{"id": "46229393", "url": "https://en.wikipedia.org/wiki?curid=46229393", "title": "George Calef", "text": "George Calef\n\nGeorge Calef is a Canadian wildlife biologist, author and photographer, who won the Governor General's Award for English-language non-fiction at the 1981 Governor General's Awards for his nature book \"Caribou and the Barren-Lands\".\n\nBorn in Los Angeles, California and educated at the University of British Columbia, Calef worked as a wildlife biology researcher in the Canadian Arctic, studying caribou for the governments of Canada and the Northwest Territories. In 1977, he testified before the Mackenzie Valley Pipeline Inquiry on the potential impact of the pipeline development on caribou habitat.\n\nIn the 1990s, he was a co-owner of Oldsquaw Lodge, a wilderness lodge off the Canol Road near the Northwest Territories-Yukon border.\n"}
{"id": "1251508", "url": "https://en.wikipedia.org/wiki?curid=1251508", "title": "Geothermal heating", "text": "Geothermal heating\n\nGeothermal heating is the direct use of geothermal energy for some heating applications. Humans have taken advantage of geothermal heat this way since the Paleolithic era. Approximately seventy countries made direct use of a total of 270 PJ of geothermal heating in 2004. As of 2007, 28 GW of geothermal heating capacity is installed around the world, satisfying 0.07% of global primary energy consumption. Thermal efficiency is high since no energy conversion is needed, but capacity factors tend to be low (around 20%) since the heat is mostly needed in the winter.\n\nGeothermal energy originates from the heat retained within the Earth since the original formation of the planet, from radioactive decay of minerals, and from solar energy absorbed at the surface. Most high temperature geothermal heat is harvested in regions close to tectonic plate boundaries where volcanic activity rises close to the surface of the Earth. In these areas, ground and groundwater can be found with temperatures higher than the target temperature of the application. However, even cold ground contains heat, below the undisturbed ground temperature is consistently at the Mean Annual Air Temperature and it may be extracted with a heat pump.\n\nThere are a wide variety of applications for cheap geothermal heat. In 2004 more than half of direct geothermal heat was used for space heating, and a third was used for spas. The remainder was used for a variety of industrial processes, desalination, domestic hot water, and agricultural applications. The cities of Reykjavík and Akureyri pipe hot water from geothermal plants under roads and pavements to melt snow. Geothermal desalination has been demonstrated.\n\nGeothermal systems tend to benefit from economies of scale, so space heating power is often distributed to multiple buildings, sometimes whole communities. This technique, long practiced throughout the world in locations such as Reykjavík, Iceland, Boise, Idaho, and Klamath Falls, Oregon is known as district heating.\n\nSome parts of the world, including substantial portions of the western USA, are underlain by relatively shallow geothermal resources. Similar conditions exist in Iceland, parts of Japan, and other geothermal hot spots around the world. In these areas, water or steam may be captured from natural hot springs and piped directly into radiators or heat exchangers. Alternatively, the heat may come from waste heat supplied by co-generation from a geothermal electrical plant or from deep wells into hot aquifers. Direct geothermal heating is far more efficient than geothermal electricity generation and has less demanding temperature requirements, so it is viable over a large geographical range. If the shallow ground is hot but dry, air or water may be circulated through earth tubes or downhole heat exchangers which act as heat exchangers with the ground.\n\nSteam under pressure from deep geothermal resources is also used to generate electricity from geothermal power. The Iceland Deep Drilling Project struck a pocket of magma at 2,100m. A cemented steelcase was constructed in the hole with a perforation at the bottom close to the magma. The high temperatures and pressure of the magma steam were used to generate 36MW of electricity, making IDDP-1 the world’s first magma-enhanced geothermal system.\n\nIn areas where the shallow ground is too cold to provide comfort directly, it is still warmer than the winter air. The thermal inertia of the shallow ground retains solar energy accumulated in the summertime, and seasonal variations in ground temperature disappear completely below 10m of depth. That heat can be extracted with a geothermal heat pump more efficiently than it can be generated by conventional furnaces. Geothermal heat pumps are economically viable essentially anywhere in the world.\n\nIn theory, geothermal energy (usually cooling) can also be extracted from existing infrastructure, such as municipal water pipes.\n\nIn regions without any high temperature geothermal resources, a ground-source heat pump (GSHP) can provide space heating and space cooling. Like a refrigerator or air conditioner, these systems use a heat pump to force the transfer of heat from the ground to the building. Heat can be extracted from any source, no matter how cold, but a warmer source allows higher efficiency. A ground-source heat pump uses the shallow ground or ground water (typically starting at ) as a source of heat, thus taking advantage of its seasonally moderate temperatures. In contrast, an air-source heat pump draws heat from the air (colder outside air) and thus requires more energy.\n\nGround source heat pumps (GSHPs) are \"not\" geothermal, i.e. there is no geyser providing heat to be captured. A GSHP merely accesses stored solar heat energy in the soil or rock. GSHPs circulate a carrier fluid (usually a mixture of water and small amounts of antifreeze) through closed pipe loops buried in the ground. Single-home systems can be \"vertical loop field\" systems with bore holes deep or, if adequate land is available for extensive trenches, a \"horizontal loop field\" is installed approximately six feet subsurface. As the fluid circulates underground it absorbs heat from the ground and, on its return, the warmed fluid passes through the heat pump which uses electricity to extract heat from the fluid. The re-chilled fluid is sent back into the ground thus continuing the cycle. The heat extracted and that generated by the heat pump appliance as a byproduct is used to heat the house. The addition of the ground heating loop in the energy equation means that significantly more heat can be transferred to a building than if electricity alone had been used directly for heating.\n\nSwitching the direction of heat flow, the same system can be used to circulate the cooled water through the house for cooling in the summer months. The heat is exhausted to the relatively cooler ground (or groundwater) rather than delivering it to the hot outside air as an air conditioner does. As a result, the heat is pumped across a larger temperature difference and this leads to higher efficiency and lower energy use.\n\nThis technology makes ground source heating economically viable in any geographical location. In 2004, an estimated million ground-source heat pumps with a total capacity of 15 GW extracted 88 PJ of heat energy for space heating. Global ground-source heat pump capacity is growing by 10% annually.\n\nHot springs have been used for bathing at least since Paleolithic times. The oldest known spa is a stone pool on China's Mount Li built in the Qin dynasty in the 3rd century BC, at the same site where the Huaqing Chi palace was later built. Geothermal energy supplied channeled district heating for baths and houses in Pompeii around 0 AD. In the first century AD, Romans conquered Aquae Sulis in England and used the hot springs there to feed public baths and underfloor heating. The admission fees for these baths probably represents the first commercial use of geothermal power. A 1,000-year-old hot tub has been located in Iceland, where it was built by one of the island's original settlers. The world's oldest working geothermal district heating system in Chaudes-Aigues, France, has been operating since the 14th century. The earliest industrial exploitation began in 1827 with the use of geyser steam to extract boric acid from volcanic mud in Larderello, Italy.\n\nIn 1892, America's first district heating system in Boise, Idaho was powered directly by geothermal energy, and was soon copied in Klamath Falls, Oregon in 1900. A deep geothermal well was used to heat greenhouses in Boise in 1926, and geysers were used to heat greenhouses in Iceland and Tuscany at about the same time. Charlie Lieb developed the first downhole heat exchanger in 1930 to heat his house. Steam and hot water from the geysers began to be used to heat homes in Iceland in 1943.\n\nBy this time, Lord Kelvin had already invented the heat pump in 1852, and Heinrich Zoelly had patented the idea of using it to draw heat from the ground in 1912. But it was not until the late 1940s that the geothermal heat pump was successfully implemented. The earliest one was probably Robert C. Webber's home-made 2.2 kW direct-exchange system, but sources disagree as to the exact timeline of his invention. J. Donald Kroeker designed the first commercial geothermal heat pump to heat the Commonwealth Building (Portland, Oregon) and demonstrated it in 1946. Professor Carl Nielsen of Ohio State University built the first residential open loop version in his home in 1948. The technology became popular in Sweden as a result of the 1973 oil crisis, and has been growing slowly in worldwide acceptance since then. The 1979 development of polybutylene pipe greatly augmented the heat pump’s economic viability. As of 2004, there are over a million geothermal heat pumps installed worldwide providing 12 GW of thermal capacity. Each year, about 80,000 units are installed in the USA and 27,000 in Sweden.\n\nGeothermal energy is a type of renewable energy that encourages conservation of natural resources. According to the U.S. Environmental Protection Agency, geo-exchange systems save homeowners 30–70 percent in heating costs, and 20–50 percent in cooling costs, compared to conventional systems. Geo-exchange systems also save money because they require much less maintenance. In addition to being highly reliable they are built to last for decades.\n\nSome utilities, such as Kansas City Power and Light, offer special, lower winter rates for geothermal customers, offering even more savings.\n\nIn geothermal heating projects the underground is penetrated by trenches or drillholes. As with all underground work, projects may cause problems if the geology of the area is poorly understood.\n\nIn the spring of 2007 an exploratory geothermal drilling operation was conducted to provide geothermal heat to the town hall of Staufen im Breisgau. After initially sinking a few millimeters, a process called subsidence, the city center has started to rise gradually causing considerable damage to buildings in the city center, affecting numerous historic houses including the town hall. It is hypothesized that the drilling perforated an anhydrite layer bringing high-pressure groundwater to come into contact with the anhydrite, which then began to expand. Currently no end to the rising process is in sight. Data from the TerraSAR-X radar satellite before and after the changes confirmed the localised nature of the situation.\n\nA geochemical process called anhydrite swelling has been confirmed as the cause of these uplifts. This is a transformation of the mineral anhydrite (anhydrous calcium sulphate) into gypsum (hydrous calcium sulphate). A pre-condition for this transformation is that the anhydrite is in contact with water, which is then stored in its crystalline structure.\n\n\n"}
{"id": "5012276", "url": "https://en.wikipedia.org/wiki?curid=5012276", "title": "Guadalquivir Marshes", "text": "Guadalquivir Marshes\n\nThe Guadalquivir Marshes (in or simply \"Las Marismas\") are a natural region of marshy lowlands on the lower Guadalquivir River. \n\nThe \"Las Marismas\" zone forms a large part of the province of Huelva, province of Seville and province of Cádiz in Andalucia, Spain. The area includes parts of the municipalities of Isla Mayor, Los Palacios y Villafranca, La Puebla del Río, Utrera, Las Cabezas de San Juan and Lebrija.\n\nRice farming in this area produces 40% of Spain's national crop. Some areas are protected for wildlife and habitat, including dunes.\n\nApproximately two millennia ago the wetlands comprised a large lagoon and estuary, known as Lacus Ligustinus in Latin, leading to the Guadalquivir River mouth with some sand bars to the South. Over time the lake silted up, gradually transforming into marshland. This silt has formed into a barrier of dunes extending approximately along the coast, known as \"Arenas Gordas\" (English: \"the fat sands\").\n\nThe marquessate de las Marismas del Guadalquivir is a Spanish noble title named after the area; this noble title was created by Royal Decree of King Ferdinand VII in 1829 for Alejandro María Aguado y Ramírez de Estenoz. \n\nIn the early 21st century, the Guadalquivir wetland region's main economic activity is agriculture, specialising in the cultivation of rice. An area of about 400 km² is devoted to rice farming. It has an annual output of about 310,000 metric tonnes, equating to approximately 40% of Spain's rice production.\n\nThe wetland zone acts as a buffer zone between the human settlements of the Guadalquivir region and Doñana National Park, a protected area of marshland, streams, and sand dunes. The National Park was established in 1969 as a nature reserve when the World Wildlife Fund joined forces with the Spanish Government to purchase a substantial part of the local wetlands in order to preserve them.\n\nThe Las Marismas area is home to a large number of wildlife species. Commonly seen terrestrial animals include deer, lynx, and wild boars. Notable avian species include vultures, flamingos, herons, spoonbills, and ducks. The rare Spanish imperial eagle lives here, whose population is considered vulnerable by the International Union for Conservation of Nature.\n\n\n"}
{"id": "23271819", "url": "https://en.wikipedia.org/wiki?curid=23271819", "title": "Helmholtz–Kohlrausch effect", "text": "Helmholtz–Kohlrausch effect\n\nThe Helmholtz–Kohlrausch effect (after Hermann von Helmholtz and Rudolf Kohlrausch) is a perceptual phenomenon wherein the intense saturation of spectral hue is perceived as part of the color's luminance. This brightness increase by saturation, which grows stronger as saturation increases, might better be called chromatic luminance, since \"white\" or achromatic luminance is the standard of comparison. It appears in both self-luminous and surface colors, although it is most pronounced in spectral lights.\nEven when they have the same luminance, colored lights seem brighter to human observers than white light does. The way humans perceive the brightness of the lights will be different for everyone. When the colors are more saturated, our eyes interpret it as the color's luminance and chroma. This makes us believe that the colors are actually brighter. An exception to this is when the human observer is red-green colorblind, they cannot distinguish the differences between the lightness of the colors. Certain colors do not have significant effect, however, any hue of colored lights still seem brighter than white light that has the same luminance. Two colors that do not have as great of an Helmholtz–Kohlrausch effect as the others are green and yellow.\n\nThe Helmholtz–Kohlrausch effect is affected by the viewing environment. This includes the surroundings of the object and the lighting that the object is being viewed under. The Helmholtz–Kohlrausch effect works best in darker environments where there are not any other outside factors influencing the colors. For example, this is why theaters are all dark environments.\n\nAn example of this lightness factor would be if there were different colors on a grey background that all are of the same lightness. Obviously the colors look different because they are different colors not just grey, but if the image were converted all to grey scale, all of the colors would match the grey background because they all have the same lightness.\n\nBrightness is affected most by what is surrounding the object. In other words, the object can look lighter or darker depending on what is around it. In addition, the brightness can also appear different depending on the color of the object. For example, an object that is more saturated will look brighter than the same object that is less saturated even when they have the same luminance.\n\nThe difference between brightness and lightness is that the brightness is the intensity of the object independent of the light source. Lightness is the brightness of the object in respect to the light reflecting on it. This is important because the Helmholtz–Kohlrausch effect is a measure of the ratio between the two.\n\nSimilar to the Munsell Color System, Helmholtz designed a coordinate system. He used the principals of wavelength and purity (chroma) of the color for each hue to describe the location of when high saturation indicates a small amount of white.\n\nThe percentage of purity for each wavelength can be determined by the equation below:\n\nWhere %P is the percent of purity, S is the point being assessed, N is the position of the white point, and DW the dominant wavelength.\n\nIt is essential for lighting users to be aware of the Helmholtz–Kohlrausch effect when working in theaters or in other venues where lighting is often used. In order to get the greatest effect to illuminate their stage or theater, the lighting users need to understand that color has an effect on brightness. For example, one color may appear brighter than another but really they have the same brightness. On stage, lighting users have the ability to make the colored lights appear much brighter than the white light by adding gels. This occurs even though gels can only absorb some of the light. When lighting a stage, the lighting users tend to choose reds, pinks, and blues because they are highly saturated colors and are really very dim. However, we perceive them as being brighter than the other colors because they are most affected by the Helmholtz–Kohlrausch effect. We perceive that the color white does not look any brighter to us than individual colors. LED lights are a good example of this.\n\nThe Helmholtz–Kohlrausch effect influences the use of LED lights in different technological practices. Aviation is one field that relies upon the results of the Helmholtz–Kohlrausch effect. A comparison of runway LED lamps and filtered and unfiltered incandescent lights all at the same luminance shows that in order to accomplish the same brightness, the white reference incandescent lamp needs to have twice the luminance of the red LED lamp, therefore suggesting that the LED lights do appear to have a greater brightness than the traditional incandescent lights. One condition that affects this theory is the presence of fog.\n\nAnother field that uses this is the automotive industry. LEDs in the dashboard and instrument lighting are designed for use in mesopic luminance. In studies, it has been found that red LEDs appear brighter than green LEDs, which means that a driver would be able to see red light before the green lights when driving at night.\n\n\n\n"}
{"id": "55564133", "url": "https://en.wikipedia.org/wiki?curid=55564133", "title": "Historical eruptions in Tenerife", "text": "Historical eruptions in Tenerife\n\nRelation of the eruptions of which one has historical record in the Island of Tenerife:\n\n"}
{"id": "24692736", "url": "https://en.wikipedia.org/wiki?curid=24692736", "title": "Intermediate composition", "text": "Intermediate composition\n\nIn igneous petrology an intermediate composition refers to the chemical composition of a rock that has 52-63 wt% SiO being an intermediate between felsic and mafic compositions. Typical intermediate rocks include andesite, dacite and trachyandesite among volcanic rocks and diorite and granodiorite among plutonic rocks.\n"}
{"id": "5259128", "url": "https://en.wikipedia.org/wiki?curid=5259128", "title": "International Association for Energy Economics", "text": "International Association for Energy Economics\n\nInternational Association for Energy Economics (IAEE) is an international non-profit society of professionals interested in energy economics. IAEE was founded in 1977, in the period of the energy crisis, when it became obvious that lack of knowledge on energy economics is one of the problems when dealing with the short- and long-term issues of energy supply and demand. IAEE is incorporated under United States laws and has headquarters in Cleveland. \n\nThe IAEE operates through a 17 member Council of elected and appointed members. Council and officer members serve in a voluntary position.\n\nThe IAEE publishes \"The Energy Journal\", the policy oriented journal \"Economics of Energy & Environmental Policy\", and the IAEE Forum newsletter.\n\nIAEE has over 4,500 members worldwide (in over 100 countries). There are over 25 national chapters, in countries where membership exceeds 25 individual members. Some of the regularly active national chapters of the IAEE are: USAEE - United States; GEE - Germany; BIEE - Great Britain; AEE - France; AIEE - Italy.\n\nIn the middle to late 1970s, in the post-Watergate and post-oil embargo era in the U.S., energy controversies had become very heated and lacked much economic content. Jim Plummer had just moved from the federal government to Occidental Petroleum. He observed that there were talented energy economists in private industry, in academia, and in government, but these energy economists did not have many opportunities to meet or exchange ideas in a neutral environment free of suspicions.\n\nPlummer approached the American Economics Association and asked them to provide a room for an organizing session at its meetings in January 1977 in New York City. The only room they had available seated 125 people. Plummer did not want to have a small group huddled in a corner of the large room, so he quickly promoted the meeting to many energy economists. To his surprise, there was standing room only in the room at the start of the meeting.\n\nThe initial dues level was $10 and the checks poured in to the first Treasurer, Richard Richels. Membership was soon about 300 and growing rapidly. In 1978, Plummer asked Mike Telson to put together the first annual meeting in June 1979 in Washington, D.C. Telson, as the senior Capitol Hill staff member for the House Energy Committee was a natural choice because his job was to know who was knowledgeable on particular energy issues.\n\nThe number of people with organizing responsibilities grew rapidly, and included Bill Hughes, Joy Dunkerley, Bill Johnson, Sam Schurr, Denny Ellerman, Joan Greenwood, Fred Abel, Richard Gordon, Milt Russell, Bill Hogan, Walter Meade, Les Deman, and Dennis O'Brien. The 1979 Annual Meeting in Washington, D.C. featured James Schlesinger, Secretary of Energy, as the keynote speaker.\n\nThe first Vice President for Publications was Professor Ed Erickson of North Carolina State University. From the beginning, the Council wanted the organization to either have its own journal or affiliate with an established journal. Erickson solicited proposals from existing journals, as well as universities that might wish to host a new journal. Professor Helmut Frank at the University of Arizona convinced his university to subsidize a new journal. That proposal was accepted. There was much debate about potential names for the new journal. Ed Erickson lobbied forcefully for a broad name, and his proposal for The Energy Journal was accepted. If Ed Erickson should be considered the \"founder\" of The Energy Journal in the sense of spearheading its formation, then Helmut Frank, its founding editor, has to be considered the soul of the The Energy Journal. He threw himself into that role with so much enthusiasm and integrity that he has continued to be an inspiration to all those who have ever since been involved with the journal.\n\nSam Schurr was the first elected. President of the organization, Jim Plummer the second, and Professor Morris Adelman of M.I.T. the third. Sam Schurr of Resources for the Future had a career and publications that straddled the public sector, academia, and industry. His prestige and support were instrumental in establishing the credibility of the organization in its early days.\n\nThe organization was initially named the Association of Energy Economists, but quickly adopted the broader name of the International Association of Energy Economists. Finally, in recognition that its membership included many engineers, physicists, geologists and others that were not formally economists, the name was further broadened to be the International Association for Energy Economics. The international reputation of Professor Morris Adelman helped the organization achieve a reputation outside the U.S. Among the Canadians that actively participated early on were Campbell Watkins of the University of Calgary and Leonard Waverman of the University of Toronto.\n\nWishing to \"jump the Atlantic,\" Jim Plummer used his business trips to London to cultivate British energy economists. The most prominent British energy economists to respond were Jane Carter, the Assistant Secretary for Conservation of the U.K. Energy Department and Paul Tempest of the Bank of England. They were quickly followed by Robert Deam, Tony Scanlan, Richard Eden, David Newbery, Robert Mabro and many others. The IAEE had already held two international conferences in Toronto and Calgary, but its international conference at Churchill College, in Cambridge, in the summer of 1980, really established its international credentials. Paul Tempest was the chairman of that conference.\n\nAs the IAEE spread internationally, beyond its initial U.S./Canadian/U.K. roots, there were many individuals that provided support and guidance, including Kenichi Matsui, Alirio Parra, Mariano Gurfinkel, Fereidun Fesharaki, and R.K. Pachauri.\n\nToday, the IAEE has members in over 100 nations and 27 organized affiliates. It holds multiple conferences each year in cities around the world. The IAEE prides itself as being a place where younger energy economists can quickly learn about the literature and people sources in every subfield of energy economics and also obtain mentoring from energy economists with decades of experience.\n\nThe International Association for Energy Economics publishes three publications throughout the year: \n\n\nThe IAEE conferences address critical issues of vital concern and importance to governments and industries and provide a forum where policy issues are presented, considered and discussed at both formal sessions and informal social functions.\n\nIAEE typically holds five Conferences each year.The main annual conference for IAEE is the \"IAEE International Conference\" that is organized at diverse locations around the world. From year 1996 on these conferences have taken place (or will take place) in the following cities:\n\n\nOther annual IAEE conferences are the North American Conference European Conference.\n\nThe Association’s Immediate Past President annually chairs the Awards committee that selects the award recipients.\n\n\n"}
{"id": "23923443", "url": "https://en.wikipedia.org/wiki?curid=23923443", "title": "Kystagerparken", "text": "Kystagerparken\n\nKystagerparken is a park located in Hvidovre, Denmark, it borders Kalveboderne, the waters between Zealand and Amager. The nearest station is Åmarken.\n\nKystagerparken is just one of several connected parks and green spaces in and around Hvidovre and link to areas such as Strandengen, Mågeparken, Vestvolden, Rebæk Søpark, Valbyparken and Vigerslevparken to name some.\n\nThe artificial hill of 'Bjerget' (English: \"The Mountain\") in the park, was constructed from excess dirt from the excavations at Kongens Bryghus in Frederiksberg in 1957. Kystagerparken has a status as a wildlife reserve and became an EU protected bird sanctuary in 1994. The grass is allowed to grow freely and the many wild flowers, seeds and plants provides habitat for insects, shrew, skylark, foxes and the common kestrel.\n\nKystagerparken invites to a number of activities throughout the year:\n\nA Solar system model on a scale of 1:1,000,000,000 stretches through the park to Mågeparken - another park nearby. The planet path reaches from the Sun on the hill of 'Bjerget', to the outermost planet Pluto 5.9 km away. In 2000, when the planet path was constructed as a public elementary school project, Pluto was still considered a planet. Each planet is cast in bronze and placed in the correct distance relative to each other.\n\nIn autumn, when stronger winds blow across the country, the hill of 'Bjerget' is considered an excellent place for flying kites. During snowfall in winter, it facilitates activities like sledding and on Easter Day in the spring, the Cultural Council of Hvidovre arrange \"egg rolling\" for kids here; an old Danish Easter tradition.\n\n"}
{"id": "10829022", "url": "https://en.wikipedia.org/wiki?curid=10829022", "title": "Liljequist parhelion", "text": "Liljequist parhelion\n\nA Liljequist parhelion is a rare halo, an optical phenomenon in the form of a brightened spot on the parhelic circle approximately 150–160° from the sun; i.e., between the position of the 120° parhelion and the anthelion.\n\nWhile the sun touches the horizon, a Liljequist parhelion is located approximately 160° from the sun and is about 10° long. As the sun rises up to 30° the phenomenon gradually moves towards 150°, and as the sun reaches over 30° the optical effect vanishes. The parhelia are caused by light rays passing through oriented plate crystals. Like the 120° parhelia, the Liljequist parhelia display a white-bluish colour. This colour is, however, associated with the parhelic circle itself, not the ice crystals causing the Liljequist parhelia.\nThe phenomenon was first observed by Gösta Hjalmar Liljequist in 1951 at Maudheim, Antarctica during the Norwegian–British–Swedish Antarctic Expedition in 1949–1952. It was then simulated by Dr. Eberhard Tränkle (1937–1997) and Robert Greenler in 1987 and theoretically explained by Walter Tape in 1994.\n\nA theoretical and experimental investigation of the Liljequist parhelion caused by perfect hexagonal plate crystals showed that the azimuthal position of maximum intensity occurs at\n\n<math>\\theta_"}
{"id": "25680909", "url": "https://en.wikipedia.org/wiki?curid=25680909", "title": "List of Aiphanes species", "text": "List of Aiphanes species\n\nThis is a list of Aiphanes species. \"Aiphanes\" is a genus of spiny palms which is native to tropical regions of South America, Central America, and the Caribbean. \n\nNames in green are currently accepted species, while those in red are not.\n"}
{"id": "25086186", "url": "https://en.wikipedia.org/wiki?curid=25086186", "title": "List of Callistemon cultivars", "text": "List of Callistemon cultivars\n\nThis is a list of cultivars of the plant genus \"Callistemon\".\n\n\n"}
{"id": "47190731", "url": "https://en.wikipedia.org/wiki?curid=47190731", "title": "List of Ramsar sites in the Philippines", "text": "List of Ramsar sites in the Philippines\n\nThe Ramsar Convention (formally, the \"Convention on Wetlands of International Importance, especially as Waterfowl Habitat\") is an international treaty for the conservation and sustainable utilization of wetlands, recognizing the fundamental ecological functions of wetlands and their economic, cultural, scientific, and recreational value. It is named after the city of Ramsar in Iran, where the Convention was signed in 1971.\n\nThe Philippines enforced the Ramsar Convention in its whole territory on November 8, 1994. Since then, Ramsar has designated 7 Ramsar sites in the country. Two Ramsar sites in the Philippines have been declared by UNESCO as world heritage sites, namely Puerto Princesa Subterranean River National Park, and Tubbataha Reefs Natural Park.\n\nA national inventory of wetland resources, based on the best scientific information available, is mandated by the Convention since 1990 through Recom 4.6. A Framework for Wetland Inventory was also established by the Convention in 2002 to aid member states in establishing their own national inventory for wetland sites. The inventory makes it possible to prioritize appropriate wetland sites for designation on the Ramsar List, similar to the tentative list of UNESCO, where UNESCO sites must always come from the tentative list. There is currently no publicly disclosed wetland inventory in the Philippines, but it is assumed that such inventory exists as the Philippines, through the Department of Environment and Natural Resources, has already nominated at least seven Philippine wetlands since 1994.\n\nThe current representation of the Philippines in Ramsar wetland nominations is inactive compared with the Ramsar wetland nominations of its peers. By comparison, the Philippines has 7 sites, while Mexico has more than 140, the archipelagic country of Japan has more than 50, the African nation of Algeria has more than 50, and the small country of South Korea has 22. The Philippines, an archipelagic country, theoretically possesses more wetlands than South Korea and North Korea combined. This has prompted various scholars to push the government to participate more in the nominations of Philippine wetlands in the Ramsar Convention. By rule, the Philippines may nominate multiple sites as Ramsar sites like what Mexico did in 2008, where at least 45 of its nominated sites were specifically declared as new Ramsar sites.\n"}
{"id": "1261243", "url": "https://en.wikipedia.org/wiki?curid=1261243", "title": "List of U.S. state grasses", "text": "List of U.S. state grasses\n\nThe following is a list of official U.S. state grasses.\n\n"}
{"id": "51446502", "url": "https://en.wikipedia.org/wiki?curid=51446502", "title": "List of cities by average temperature", "text": "List of cities by average temperature\n\nThis is a list of cities by average temperature (monthly and yearly). The temperatures listed are averages of the daily highs and lows. Thus, the actual daytime temperature in a given month will be higher than the temperature listed here, depending on how large the difference between daily highs and lows is.\n\n"}
{"id": "56112504", "url": "https://en.wikipedia.org/wiki?curid=56112504", "title": "List of genera of Scarabaeidae", "text": "List of genera of Scarabaeidae\n\nThis is a list of genera within the beetle family Scarabaeidae.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "11464147", "url": "https://en.wikipedia.org/wiki?curid=11464147", "title": "List of ice storms", "text": "List of ice storms\n\nThis is a list of ice storms that are described in Wikipedia articles.\n\n\n\n\n\n\nAlabama 1982\n\n\n\n\n"}
{"id": "46877839", "url": "https://en.wikipedia.org/wiki?curid=46877839", "title": "List of mountains in Finland", "text": "List of mountains in Finland\n\n"}
{"id": "41960512", "url": "https://en.wikipedia.org/wiki?curid=41960512", "title": "List of unsolved problems in geoscience", "text": "List of unsolved problems in geoscience\n\nThis list provides references to currently unsolved problems in geoscience.\n\n\n\n"}
{"id": "55116359", "url": "https://en.wikipedia.org/wiki?curid=55116359", "title": "List of wettest tropical cyclones", "text": "List of wettest tropical cyclones\n\nThis is a list of the wettest tropical cyclones, listing all tropical cyclones known to have dropped at least of precipitation on a single location. Data is most complete for Australia, Cuba, Dominican Republic, Japan, Hong Kong, Mexico, Yap, Chuuk, and the United States, with fragmentary data available for other countries. The French region of Réunion holds several world records for tropical cyclone and worldwide rainfall due to rough topography of the island and its proximity to the Indian Ocean.\n\n"}
{"id": "15959530", "url": "https://en.wikipedia.org/wiki?curid=15959530", "title": "Litang Horse Festival", "text": "Litang Horse Festival\n\nThe Litang Horse Festival is a summer horse festival held in Litang County, Sichuan province, China. Khampas from all over the Tibetan Plateau come to trade, celebrate and ride. Khampas are nomads that are Tibetan and are usually herders. The festival is held generally the first week in August, and it usually runs from August 1 to August 7. During the festival horsemanship and horse races are held upon the Tibetan Ponies. These small and fast horses are raced and shown to see who owns the best horse. The horse festival is significant because it helps to establish socio-economic hierarchy in Khampas who participate. A lot of honor and prestige is placed on who owns the best horse. A very large tourism business has been built up on adventure trips and tours provided by companies who cater to individuals who are interested in horses and horsemanship. These companies travel around Tibet taking groups of tourists throughout the different villages hosting horse festivals. This benefits the nomads' economy as well as the rest of China's economy.\n\n\n"}
{"id": "1067710", "url": "https://en.wikipedia.org/wiki?curid=1067710", "title": "Mahishasura", "text": "Mahishasura\n\nMahishasura is both reviled as well as worshipped by sections of Hindus. For some, he was a buffalo demon in Hindu mythology whereas for many others he is a God. He is known among some sections of Hindus for his deception and as someone who pursued his evil ways by shape shifting into different forms. He was ultimately killed by Durga getting named Mahishasuramardini. It is an important symbolic legend in Hindu mythology, particularly Shaktism. The legendary battle of Mahishasura as evil and Durga as good is narrated in many parts of South Asian and Southeast Asian Hindu temples, monuments and texts such as the \"Devi Mahatmya\". The story is also told in the Sikh text \"Chandi di Var\", also called \"Var Durga di\", which many in Sikh tradition believe was included in the \"Dasam Granth\" by Guru Gobind Singh.\n\nMany Dalit and Adivasi communities in Indian Subcontinent worship and believe in the power of Mahishasura. It is also believed by them that Mahishasura was an indigenous king who was killed during the invasion by the upper caste Hindus. Those who mourn Mahishasura's death believe he was portrayed as a 'demon king' by the Brahmins and upper caste Hindus who tops the charts of Hindu caste system hierarchy.\n\nMahishasura is a Sanskrit word composed of \"mahisha\" meaning buffalo and \"asura\" meaning demon, or \"buffalo demon\". As an Asura, Mahishasura waged war against the Devas, as the Devas and Asuras' were perpetually in conflict. Mahishasura had gained the boon that no man could kill him. In the battles between the gods and the demons, the Devas led by Indra were defeated by Mahishasura. Dejected by their defeat, the Devas assemble in the mountains where their combined divine energies coalesce into goddess Durga. The new born Durga led a battle against Mahishasura, riding a lion, and killed him. Thereafter she is named Mahishasuramardini, meaning \"the killer of Mahishasura\".\n\nMahishasura's legend is told in a major text of the Shaktism tradition known as the \"Devi Mahatmya\". He is described as an evil being who can change his outer form, but never his demonic goals. According to Christopher Fuller, Mahishasura symbolically represents forces of ignorance and chaos hidden by outer appearances. The symbolism is carried in Hindu arts found in South Asia and southeast Asia (Javanese artwork, for example), where Durga is shown as serene, calm, collected and graceful symbol of good as she pierces the heart and kills the scared, overwhelmed and outwitted Mahishasura.\n\nDurga slaying Mahishasura is a prominent theme which was sculpted in various caves and temples across India. Some of the prominent representations are seen at the Mahishasuramardini caves in Mahabalipram, the Ellora caves, in the entrance of Rani ki vav Hoysaleswara Temple in Halebidu and many more temples across India.\n\nThe worship of Durga during Durga puja in West Bengal is represented in \"pandal\" which depict Durga killing Mahishasura.\n\nThe popular legend is that Mysuru gets its name from Mahishasuramardini, a manifestation of Goddess Durga. The Buffalo Demon Mahishasura, states the regional tradition, had terrified the local population. Goddess Durga killed the Mahishasura, an event that is annually celebrated at Navratri and Mysore Dasara.\n\nThe temple of the city’s guardian deity, Chamunda, has a giant statue of Mahishasura on the hill facing the city. The earliest mention of Mysore in recorded history may be traced to 245 B.C., i.e., to the period of Ashoka when on the conclusion of the third Buddhist convocation, a team was dispatched to \"Mahisha mandala\".\n\n\n\n\nmysore maharaja 1929"}
{"id": "55695", "url": "https://en.wikipedia.org/wiki?curid=55695", "title": "Mirage", "text": "Mirage\n\nA mirage is a naturally occurring optical phenomenon in which light rays bend to produce a displaced image of distant objects or the sky. The word comes to English via the French \"mirage\", from the Latin \"mirari\", meaning \"to look at, to wonder at\". This is the same root as for \"mirror\" and \"to admire\".\n\nMirages can be categorized as \"inferior\" (meaning lower), \"superior\" (meaning higher) and \"Fata Morgana\", one kind of superior mirage consisting of a series of unusually elaborate, vertically stacked images, which form one rapidly changing mirage.\n\nIn contrast to a hallucination, a mirage is a real optical phenomenon that can be captured on camera, since light rays are actually refracted to form the false image at the observer's location. What the image appears to represent, however, is determined by the interpretive faculties of the human mind. For example, inferior images on land are very easily mistaken for the reflections from a small body of water.\n\nFor exhausted travelers in the desert, an inferior mirage may appear to be a lake of water in the distance. An inferior mirage is called \"inferior\" because the mirage is located under the real object. The real object in an inferior mirage is the (blue) sky or any distant (therefore bluish) object in that same direction. The mirage causes the observer to see a bright and bluish patch on the ground in the distance which is also called oasis mirage.\n\nLight rays coming from a particular distant object all travel through nearly the same air layers and all are bent over about the same amount. Therefore, rays coming from the top of the object will arrive lower than those from the bottom. The image usually is upside down, enhancing the illusion that the sky image seen in the distance is really a water or oil puddle acting as a mirror.\n\nInferior images are not stable. Hot air rises, and cooler air (being more dense) descends, so the layers will mix, giving rise to turbulence. The image will be distorted accordingly. It may be vibrating; it may be vertically extended (towering) or horizontally extended (stooping). If there are several temperature layers, several mirages may mix, perhaps causing double images. In any case, mirages are usually not larger than about half a degree high (same apparent size as the sun and moon) and from objects only a few kilometers away.\n\nHeat haze, also called heat shimmer, refers to the inferior mirage experienced when viewing objects through a layer of heated air; for example, viewing objects across hot asphalt or through the exhaust gases produced by jet engines. When appearing on roads due to the hot asphalt, it is often referred to as a highway mirage.\n\nConvection causes the temperature of the air to vary, and the variation between the hot air at the surface of the road and the denser cool air above it creates a gradient in the refractive index of the air. This produces a blurred shimmering effect, which affects the ability to resolve objects, the effect being increased when the image is magnified through a telescope or telephoto lens.\n\nLight from the sky at a shallow angle to the road is refracted by the index gradient, making it appear as if the sky is reflected by the road's surface. The mind interprets this as a pool of water on the road, since water also reflects the sky. The illusion fades as one gets closer.\n\nOn tarmac roads it may look as if water, or even oil, has been spilled. These kinds of inferior mirages are often called \"desert mirages\" or \"highway mirages\". Both sand and tarmac can become very hot when exposed to the sun, easily being more than 10 °C hotter than the air one meter above, enough to create conditions suitable for the formation of the mirage.\n\nHeat haze is not related to the atmospheric phenomenon of haze.\nA superior mirage occurs when the air below the line of sight is colder than the air above it. This unusual arrangement is called a temperature inversion, since warm air above cold air is the opposite of the normal temperature gradient of the atmosphere. Passing through the temperature inversion, the light rays are bent down, and so the image appears above the true object, hence the name \"superior\". Superior mirages are in general less common than inferior mirages, but, when they do occur, they tend to be more stable, as cold air has no tendency to move up and warm air has no tendency to move down.\n\nSuperior mirages are quite common in polar regions, especially over large sheets of ice that have a uniform low temperature. Superior mirages also occur at more moderate latitudes, although in those cases they are weaker and tend to be less smooth and stable. For example, a distant shoreline may appear to \"tower\" and look higher (and, thus, perhaps closer) than it really is. Because of the turbulence, there appear to be dancing spikes and towers. This type of mirage is also called the Fata Morgana or \"hafgerdingar\" in the Icelandic language.\n\nA superior mirage can be right-side up or upside down, depending on the distance of the true object and the temperature gradient. Often the image appears as a distorted mixture of up and down parts.\n\nSuperior mirages can have a striking effect due to the Earth's curvature. Were the Earth flat, light rays that bend down would soon hit the ground and only nearby objects would be affected. Since Earth is round, if their downward bending curve is about the same as the curvature of the Earth, light rays can travel large distances, perhaps from beyond the horizon. This was observed and documented for the first time in 1596, when a ship under the command of Willem Barentsz in search of the Northeast passage became stuck in the ice at Novaya Zemlya. The crew was forced to endure the polar winter there. They saw their midwinter night come to an end with the rise of a distorted Sun about two weeks earlier than expected. It was not until the 20th century that science could explain the reason: the real Sun had still been below the horizon, but its light rays followed the curvature of the Earth. This effect is often called a Novaya Zemlya mirage. For every the light rays can travel parallel to the Earth's surface, the Sun will appear 1° higher on the horizon. The inversion layer must have just the right temperature gradient over the whole distance to make this possible.\n\nIn the same way, ships that are in reality so far away that they should not be visible above the geometric horizon may appear on the horizon or even above the horizon as superior mirages. This may explain some stories about flying ships or coastal cities in the sky, as described by some polar explorers. These are examples of so-called Arctic mirages, or \"hillingar\" in Icelandic.\n\nIf the vertical temperature gradient is +12.9 °C per 100 meters (where the positive sign means temperature gets hotter as one goes higher) then horizontal light rays will just follow the curvature of the Earth, and the horizon will appear flat. If the gradient is less (as it almost always is) the rays are not bent enough and get lost in space, which is the normal situation of a spherical, convex \"horizon\".\n\nIn some situations, distant objects can get elevated or lowered, stretched or shortened with no mirage involved.\n\nA Fata Morgana, the name of which comes from the Italian translation of Morgan le Fay, the fairy shapeshifting half-sister of King Arthur, is a very complex superior mirage. It appears with alternations of compressed and stretched areas, erect images, and inverted images. A Fata Morgana is also a fast-changing mirage.\n\nFata Morgana mirages are most common in polar regions, especially over large sheets of ice with a uniform low temperature, but they can be observed almost anywhere. In polar regions, a Fata Morgana may be observed on cold days; in desert areas and over oceans and lakes, a Fata Morgana may be observed on hot days. For a Fata Morgana, temperature inversion has to be strong enough that light rays' curvatures within the inversion are stronger than the curvature of the Earth.\n\nThe rays will bend and create arcs. An observer needs to be within an atmospheric duct to be able to see a Fata Morgana.\nFata Morgana mirages may be observed from any altitude within the Earth's atmosphere, including from mountaintops or airplanes. \n\nA Fata Morgana can go from superior to inferior mirage and back within a few seconds, depending on the constantly changing conditions of the atmosphere. Sixteen frames of the mirage of the Farallon Islands, which cannot be seen from sea level at all under normal conditions because they are located below the horizon, were photographed on the same day. The first fourteen frames have elements of a Fata Morgana display—alternations of compressed and stretched zones. The last two frames were photographed a few hours later around sunset. The air was cooler while the ocean was probably a little bit warmer, which made temperature inversion lower. The mirage was still present, but it was not as complex as it had been a few hours before sunset, and it corresponded no longer to a Fata Morgana but rather to a superior mirage display.\n\nDistortions of image and bending of light can produce spectacular effects. In his book \"Pursuit: The Chase and Sinking of the \"Bismarck\"\", the author Ludovic Kennedy describes an incident that allegedly took place below the Denmark Strait during 1941, following the sinking of the \"Hood\". The \"Bismarck\", while pursued by the British cruisers \"Norfolk\" and \"Suffolk\", passed out of sight into a sea mist. Within a matter of seconds, the ship re-appeared steaming toward the British ships at high speed. In alarm the cruisers separated, anticipating an imminent attack, and observers from both ships watched in astonishment as the German battleship fluttered, grew indistinct and faded away. Radar watch during these events indicated that the \"Bismarck\" had in fact made no changes of course.\n\nThe conditions for producing a mirage can take place at night. Under most conditions, these are not observed. However, under some circumstances lights from moving vehicles, aircraft, ships, buildings, etc. can be observed at night, even though, as with a daytime mirage, they would not be observable.\n\nThis includes the mirage of astronomical objects.\n\nA mirage of an astronomical object is a naturally occurring optical phenomenon, in which light rays are bent to produce distorted or multiple images of an astronomical object. The mirages might be observed for such astronomical objects as the Sun, the Moon, the planets, bright stars, and very bright comets. The most commonly observed are sunset and sunrise mirages.\n\n\n"}
{"id": "20020609", "url": "https://en.wikipedia.org/wiki?curid=20020609", "title": "Mirage of astronomical objects", "text": "Mirage of astronomical objects\n\nA mirage of an astronomical object is an atmospheric optical phenomenon, in which light rays are bent to produce distorted or multiple images of an astronomical object. The mirages might be observed for such celestial objects as the Sun, the Moon, the planets, bright stars, and very bright comets. The most commonly observed of these are sunset and sunrise mirages.\n\nMirages are distinguished from other phenomena caused by atmospheric refraction. One of the most prominent features of mirages is that a mirage might only produce images vertically, not sideways, while a simple refraction might distort and bend the images in any way.\n\nThe distortion in both images displayed in this section was caused by refraction, but while the image on the left, which is a mirage, demonstrates only vertical distortion, the image on the right demonstrates distortion in all the ways possible. It is easier to see the vertical direction of the mirage not even at the mirage of the Sun itself, but rather at the mirage of a sunspot. As a matter of fact, it is at least a three-image mirage of a sunspot, and all these images show a clear vertical direction.\n\nInferior mirage of astronomical objects is the most common mirage. Inferior mirage occurs when the surface of the Earth or the oceans produces a layer of hot air of lower density, just at the surface. There are two images, the inverted one and the erect one, in inferior mirage.They both are displaced from the geometric direction to the actual object. While the erect image is setting, the inverted image appears to be rising from the surface.\n\nThe shapes of inferior mirage sunsets and sunrises stay the same for all inferior mirage sunsets and sunrises. One well-known shape, the Etruscan vase, was named by Jules Verne.\nAs the sunset progresses the shape of Etruscan vase slowly changes; the stem of the vase gets shorter until the real and the miraged Suns create a new shape – Greek letter omega Ω. The inferior mirage got its name because the inverted image appears\nbelow the erect one.\n\nHere's how Jules Verne describes an inferior mirage sunset.\nOn very rare occasions the mirages of astronomical objects other than the Sun and the Moon might be observed. An apparent magnitude of an astronomical object should be low enough (that is, bright enough) in order to see it as not only a real object, but also a miraged one.\n\nA mock mirage of astronomical objects is much more complex than an inferior mirage. While an inferior mirage of astronomical objects can produce only two images, a mock mirage can produce multiple miraged images. The shapes of the miraged object are changing constantly and unpredictably. In order for a mock mirage to appear, the cooler air needs to be trapped below the inversion. Several inversion layers produce multiple pancake-like shapes.\n\nIt is possible that one of the earliest mock mirages of the setting Sun was mentioned in the Book of Joshua.\n\nJoshua launched a surprise attack on the Amorites following a night march, causing the Amorites to panic and flee as far as Beth-horon, but they did not find a safe haven there. ...\"they were more who died with the hailstones than they whom the children of Israel slew with the sword.\" \"Hailstones\" are a rare event in the deserts and are a good precondition for creating a mock/superior mirage of the setting sun. Inferior mirage is the most common mirage in the deserts. When Israelites went from a hot desert to a hail-covered desert to fight the Amorites, the inversion layers might have created a mock mirage of the setting sun. To the Israelites, the sun appeared to stand still. A poem is quoted from the Book of Jasher, which states that the sun stood still at Gibeon, and the moon in the valley of Ajalon, in order that Joshua could complete the battle.\n\nDue to a normal atmospheric refraction, sunrise occurs shortly before the Sun crosses above the horizon. Light from the Sun is bent, or refracted, as it enters earth's atmosphere. This effect causes the apparent sunrise to be earlier than the actual sunrise. Similarly, apparent sunset occurs slightly later than actual sunset.\n\nIn ordinary atmospheric conditions, the setting or rising Sun appears to be about half a degree above its geometric position. But sometimes, very unusual atmospheric circumstances can make it to be visible when it is really between two and five degrees below the horizon. This is called the Novaya Zemlya effect, because it was first observed in Novaya Zemlya, where the Sun was seen when, according to astronomical calculations, it should have been two degrees below the horizon.\n\nHowever, it should be noted that due to changes in air pressure, relative humidity, and other quantities, the exact effects of atmospheric refraction on sunrise and sunset time cannot be predicted. Also note that this possible error increases with higher (closer to the poles) latitudes.\nNovaya Zemlya is a polar region in Russia. The Novaya Zemlya effect is a mirage caused by high refraction of sunlight between atmospheric thermoclines. The Novaya Zemlya effect will give the impression that the sun is rising earlier than it actually should or the sun is setting later than it actually should.\nFridtjof Nansen wrote\nIt is possible to observe the Novaya Zemlya effect in any place, where the temperature variations are great enough to produce a high refraction.\n\nGreen flash is a rare optical phenomenon that occurs during or shortly after set and during or before rise of a bright astronomical object, when a green spot is visible for a short period of time above a mirage of an astronomical object or its set/rise point.\nGreen flashes are enhanced by atmospheric inversions which increase the density gradient in the atmosphere, and therefore increase refraction. In other words, to see a green flash a mirage should be present.\n\nJules Verne described a green flash. \n\nMany tend to believe that seeing a green flash brings good luck.\n\nGreen flashes might be observed from any place with a low horizon. Deserts, oceans and ice shelves are probably the best places to observe mirages and therefore green flashes. It is easier not to miss a green flash during sunset than during sunrise. It is especially true regarding an inferior mirage green flash, when the timing should be just right to capture it at sunrise.\nFrom the above observation it is clear that the author observed an inferior mirage green flash, when the much warmer surface of the desert with the help of the rising Sun was fighting the cool morning air, producing in the process a green flash – one of nature's great spectacles.\n\nA green flash might be also seen with a rising or setting Moon.<br>\nIn the right conditions it is common to observe multiple green flashes during one mock mirage sunset.<br>Some claim they saw a green flash from Venus. This may be true, but it might be that a color of the setting or the rising planet is mistaken for a real green flash that is a by-product of a mirage.\n\nAs an astronomical object sets or rises, the light it emits travels through the atmosphere, which works as a prism separating the light into different colors. The color of the upper limb of an astronomical object could go from blue to green to violet depending on the decrease in concentration of pollutants as they spread throughout an increasing volume of atmosphere. The lower limb of an astronomical object is always red.<br>The green rim is very thin, and is difficult or impossible to see with the naked eye. In usual conditions a green rim of an astronomical object gets fainter, when an astronomical object is very low above the horizon because of atmospheric reddening, but sometimes the conditions are right to see a green rim just above the horizon. The following quote describes probably the longest green rim, which sometimes could have been green flash, observation. It was seen on and off for 35 long minutes by members of the Richard Evelyn Byrd party from the Little America exploration base.\n\nOften the green rim of the setting Sun will change to a green flash and then back again to a green rim during the same sunset. The image below might be a good illustration of what members of the Richard Evelyn Byrd party from the Little America exploration base might have seen.\nHowever, to see a green rim and green flash on and off for 35 minutes, there must have been some degree of mirage present.\n\nA green rim is present in every sunset, but it is too thin to be seen with a naked eye.The best time to observe the green rim is about 10 minutes before sunset time. However the solar disc is too bright at that time to use magnification such as binoculars or telescopes to look directly at the Sun. Of course a telescope or binoculars image can be projected on a sheet of paper for viewing. When the Sun gets closer to the horizon, the green rim gets fainter because of atmospheric reddening.\nAlthough a green rim is present in every sunset, and a green flash is rare because it requires a mirage to be present, but despite this it is actually more common for people to have seen a green flash rather than a green rim.\nThe composite image on the left is made out of five frames of different sunsets. None of the images is a mirage. Frames # 1 and # 2 could fool even a very experienced observer. They do look like a mock mirage of the setting Sun, but they are not. Frames #3 and # 4 are clearly not a mirage. Frame # 5 is not a mirage and not even a sunspot, it is a spider with the Sun as a background. The strange shapes of the Sun in this composite is due to vog and clouds.\nNumerous atmospheric effects, such as vog, clouds, smoke, smog and others could generate a mirage like appearance of an astronomical object. Lens flares and ghost images also might be responsible for a false mirage or a false green flash.\n\n\n"}
{"id": "22126", "url": "https://en.wikipedia.org/wiki?curid=22126", "title": "Northern Hemisphere", "text": "Northern Hemisphere\n\nThe Northern Hemisphere is the half of Earth that is north of the Equator. For other planets in the Solar System, north is defined as being in the same celestial hemisphere relative to the invariable plane of the solar system as Earth's North Pole.\nOwing to the Earth's axial tilt, winter in the Northern Hemisphere lasts from the December solstice (typically December 21 UTC) to the March equinox (typically March 20 UTC), while summer lasts from the June solstice through to the September equinox (typically September 23 UTC). The dates vary each year due to the difference between the calendar year and the astronomical year.\n\nIts surface is 60.7% water, compared with 80.9% water in the case of the Southern Hemisphere, and it contains 67.3% of Earth's land.\n\nThe Arctic is the region north of the Arctic Circle. Its climate is characterized by cold winters and cool summers. Precipitation mostly comes in the form of snow. The Arctic experiences some days in summer when the Sun never sets, and some days during the winter when it never rises. The duration of these phases varies from one day for locations right on the Arctic Circle to several months near the North Pole, which is the middle of the Northern Hemisphere.\n\nBetween the Arctic Circle and the Tropic of Cancer lies the Northern temperate zone. The changes in these regions between summer and winter are generally mild, rather than extreme hot or cold. However, a temperate climate can have very unpredictable weather.\n\nTropical regions (between the Tropic of Cancer and the Equator) are generally hot all year round and tend to experience a rainy season during the summer months, and a dry season during the winter months.\n\nIn the Northern Hemisphere, objects moving across or above the surface of the Earth tend to turn to the right because of the coriolis effect. As a result, large-scale horizontal flows of air or water tend to form clockwise-turning gyres. These are best seen in ocean circulation patterns in the North Atlantic and North Pacific oceans.\n\nFor the same reason, flows of air down toward the northern surface of the Earth tend to spread across the surface in a clockwise pattern. Thus, clockwise air circulation is characteristic of high pressure weather cells in the Northern Hemisphere. Conversely, air rising from the northern surface of the Earth (creating a region of low pressure) tends to draw air toward it in a counter-clockwise pattern. Hurricanes and tropical storms (massive low-pressure systems) spin counter-clockwise in the Northern Hemisphere.\n\nThe shadow of a sundial moves clockwise in the Northern Hemisphere (opposite of the Southern Hemisphere). During the day, the Sun tends to rise to its maximum at a southerly position except between the Tropic of Cancer and the Equator, where the sun can be seen to the north, directly overhead, or to the south at noon dependent on the time of year.\n\nWhen viewed from the Northern Hemisphere, the Moon appears inverted compared to a view from the Southern Hemisphere. The North Pole faces away from the galactic center of the Milky Way. This results in the Milky Way being sparser and dimmer in the Northern Hemisphere compared to the Southern Hemisphere, making the Northern Hemisphere more suitable for deep-space observation, as it is not \"blinded\" by the Milky Way.\n\nThe Northern Hemisphere is home to approximately 6.57 billion people which is around 90% of the earth's total human population of 7.3 billion people.\n\n\n"}
{"id": "26530670", "url": "https://en.wikipedia.org/wiki?curid=26530670", "title": "Ocean privatization", "text": "Ocean privatization\n\nOcean privatization is the sale of the oceans to private individuals or companies. Libertarians have long proposed such privatization.\n\n\"The Market for Liberty\", published in 1970, stated that \"as companies drilling for off-shore oil have proved, there is no reason why a piece of land cannot be owned and used simply because it is covered by water.\" In proposing privatization, Walter Block stated that \"there are vast areas of human existence where private property rights play no role at all: oceans, seas, rivers and other bodies of water. But why should we expect that there would be any better results from such 'water socialism' than we have experienced from socialism on land?\" Murray Rothbard's \"For a New Liberty\" likewise notes, \"Anyone can capture fish in the ocean, or extract its resources, but only on the run, only as hunters and gatherers. No one can farm the ocean, no one can engage in aquaculture. In this way we are deprived of the use of the immense fish and mineral resources of the seas...Even now there is a simple but effective technique that could be used for increasing fish productivity: parts of the ocean could be fenced off electronically, and through this readily available electronic fencing, fish could be segregated by size. By preventing big fish from eating smaller fish, the production of fish could be increased enormously.\" Mary Ruwart's \"Healing Our World\" states, \"Owners would also be more likely to invest in artificial reefs to bolster the fish population. Whalers could operate only with the permission of the owners, much as hunters must request permission to stalk deer on privately owned land. Ocean owners profit most by making sure that the valuable species in their region are not hunted to extinction.\" Much as ownership of land also, under the common law, included ownership of all the airspace above him upward indefinitely unto the heavens and downward into the center of the earth, and causing polluted air to enter another person's airspace would constitute aggression, causing polluted water to enter waters owned by another person would constitute aggression that would be legally treated as such, under an anarcho-capitalist system.\n\nIn 2009 George Mason University professor Peter T. Leeson has suggested that ocean privatization could help combat Somalian piracy. Ršgnvaldur Hannesson's book \"The Privatization of the Oceans\" argues that privatization will help alleviate the tragedy of the commons. Donald R. Leal's \"Let's Homestead the Oceans\" argues that regulation has failed and that individual transferable quotas, which guarantee each fisher a specific share of the total allowable catch before the season begins, should be established, or preferably, fullfledged property rights either to fishing areas or to the fish themselves should be created. The article argues that the more secure the property rights, the healthier fish populations and fishing communities will be. Some questions that arise in reference to water privatization are what areas are to be privatized exactly; how it can be ensured that the general public interest is not corrupted by private corporations acting alone or with government collusion to destroy these once-public resources; what would be a fair price to privatize oceanic tracts; how to hand over title to private firms to manage water resources; and how to manage any future transfers from one private institution to another.\n\n"}
{"id": "31443350", "url": "https://en.wikipedia.org/wiki?curid=31443350", "title": "Optical depth (astrophysics)", "text": "Optical depth (astrophysics)\n\nOptical depth in astrophysics refers to a specific level of transparency. Optical depth and actual depth, formula_1 and formula_2 respectively, can vary widely depending on the absorptivity of the astrophysical environment. Indeed, formula_1 is able to show the relationship between these two quantities and can lead to a greater understanding of the structure inside a star.\n\nOptical depth is a measure of the extinction coefficient or absorptivity up to a specific 'depth' of a star's makeup.\n\nThe assumption here is that either the extinction coefficient formula_5 or the column number density formula_6 is known. These can generally be calculated from other equations if a fair amount of information is known about the chemical makeup of the star. From the definition, it is also clear that large optical depths correspond to higher rate of obscuration. Optical depth can therefore be thought of as the opacity of a medium. \n\nThe extinction coefficient formula_5 can be calculated using the transfer equation. In most astrophysical problems, this is exceptionally difficult to solve since solving the corresponding equations requires the incident radiation as well as the radiation leaving the star. These values are usually theoretical.\n\nIn some cases the Beer-Lambert Law can be useful in finding formula_5.\n\nwhere formula_10 is the refractive index, and formula_11 is the wavelength of the incident light before being absorbed or scattered. It is important to note that the Beer-Lambert Law is only appropriate when the absorption occurs at a specific wavelength, formula_11. For a gray atmosphere, for instance, it is most appropriate to use the Eddington Approximation.\n\nTherefore, formula_1 is simply a constant that depends on the physical distance from the outside of a star. To find formula_1 at a particular depth formula_15, the above equation may be used with formula_5 and integration from formula_17 to formula_18.\n\nSince it is difficult to define where the photosphere of a star ends and the chromosphere begins, astrophysicists usually rely on the Eddington Approximation to derive the formal definition of formula_19\n\nDevised by Sir Arthur Eddington the approximation takes into account the fact that formula_20 produces a \"gray\" absorption in the atmosphere of a star, that is, it is independent of any specific wavelength and absorbs along the entire electromagnetic spectrum. In that case,\n\nwhere formula_22 is the effective temperature at that depth and formula_1 is the optical depth.\n\nThis illustrates not only that the observable temperature and actual temperature at a certain physical depth of a star vary, but that the optical depth plays a crucial role in understanding the stellar structure. It also serves to demonstrate that the depth of the photosphere of a star is highly dependent upon the absorptivity of its environment. The photosphere extends down to a point where formula_1 is about 2/3, which corresponds to a state where a photon would experience, in general, less than 1 scattering before leaving the star.\n\nThe above equation can be rewritten in terms of formula_5 in the following way:\n\nWhich is useful, for example, when formula_1 is not known but formula_5 is.\n"}
{"id": "38644973", "url": "https://en.wikipedia.org/wiki?curid=38644973", "title": "Protogaea", "text": "Protogaea\n\nProtogaea is a work by Gottfried Leibniz on geology and natural history. Unpublished in his lifetime, it was conceived as a preface to his incomplete history of the House of Brunswick.\n\n\"Protogaea\" is a history of the Earth written in conjectural terms; it was composed by Leibniz in the period 1691 to 1693. The text was first published in full in 1749, shortly after Benoît de Maillet's more far-reaching ideas on the origin of the Earth, circulated in manuscript, had been printed.\n\n\"Protogaea\" built on, and criticised, the natural philosophy of René Descartes, as expressed in his \"Principia Philosophiae\". Leibniz in the work adopted the Cartesian theory of the Earth as a sun crusted over with sunspots. He relied on the authority of Agostino Scilla writing about fossils to discredit speculations of Athanasius Kircher and Johann Joachim Becher; he had met Scilla in Rome a few years earlier. He took up suggestions of Nicolaus Steno that argued for the forms of fossils being prior to their inclusion in rocks, for stratification, and for the gradual solidification of the Earth.\n\n"}
{"id": "2576912", "url": "https://en.wikipedia.org/wiki?curid=2576912", "title": "Rupununi savannah", "text": "Rupununi savannah\n\nThe Rupununi Savannah is a savanna plain in Guyana, in the Upper Takutu-Upper Essequibo region. It is an ecoregion of the Tropical and subtropical grasslands, savannas, and shrublands Biome.\n\nThe Rupununi Savannah is located between the Rupununi River and the border with Brazil and Venezuela. The Rupununi forms the southwestern wilderness territory of Guyana, a Caribbean country situated on the Northeastern littoral of South America. The savannah is dissected by the Kanuku Mountains. The Rupununi Savannah encompasses 5000 square miles of virtually untouched grasslands, swamplands, rain-forested mountains. The region usually floods in the wet season (May to August). Early European explorers believed that the Rupununi floodplains were the legendary Lake Parime. The area is inhabited by some 15,000 Amerindians.\n\nThe savannah is divided north from south, by the Kanuku Mountains, Guyana’s most biologically diverse region. According to Conservation International, the \"area supports a large percentage of Guyana’s biodiversity\", including 250 species of bird life, 18 of which are native \"only to the lowland forests of the Guianas.\" The savannah is teeming with wildlife, including a large variety of bird species. The savannah is also home to the jaguar as well as the Harpy Eagle, the world’s most powerful bird of prey, an extremely rare and endangered species which once ranged the forests of South America and is found in the Rupununi/Kanuku mountain range.\n\nThe Rupununi is the home of the Wapishana, Macushi, Wai-Wai and Patamona peoples. A recent survey recorded a population of 14,689 Amerindians. The Wapishana live mainly in the south savannah, the Macushi in the north. Some 200 Wai-Wai live in near isolation in the remote southeastern region bordering Brazil virtually untouched by modern life.\n\nThe major occupations or industries in the Rupununi Savannah are cattle ranching for beef, Balatá bleeding to extract latex; farming groundnuts, maize (corn), cassava, and vegetables; fishing and hunting; and craft work such as the manufacture of hammocks, leather articles, Nibbi furniture and beadwork).\n\nThere are Amerindian villages dotted throughout the Rupununi Savannah, as well as many ranches worked by \"vaqueros\" (cowboys), some of whom are descendants of 19th century Scottish settlers. The main town is Lethem, located beside the Takutu River, on the border with Brazil. Owing to the savanna's remoteness from the rest of the country most trade is conducted with Brazil and most people speak Portuguese.\n\nIn 1969 some ranchers started what has been referred to as the Rupununi Uprising. The revolt was quelled within a few days.\n\nThe Rupununi region caters towards ecotourists. It is designated a \"protected area\" by the government of Guyana, housing some 80% of the mammals and 60% of the bird life found in Guyana’s tropical forests and savannahs. Several Lodges welcome guests, for example Dadanawa Ranch or Karanabo ranch.\n\nThe Rupununi is accessible by small aircraft and helicopter flights regularly available from Guyana’s capital Georgetown on the Atlantic coast. In the dry season it is accessible by an unpaved \"all-weather\" road using trucks or 4x4 vehicles. It takes about 48 hours of tough driving. Heavy flooding makes this drive unpredictable and dangerous in the rainy season during the months of April to June.\n"}
{"id": "33363052", "url": "https://en.wikipedia.org/wiki?curid=33363052", "title": "Tauhara Power Station", "text": "Tauhara Power Station\n\nThe Tauhara Power Station is a geothermal power station north of Taupo in New Zealand. Stage 2 of the project is being developed by Contact Energy and Tauhara Moana Trust.\n\nStage 1 of the Tauhara project is operational as the Te Huka Power Station. This is a 24MW binary plant supplied with geothermal steam from the Tauhara field.\n\nThe application for resource consents for the 250MW power station was submitted in February 2010. The Minister for the Environment determined that this project was one of national significance, and referred it to an independent Board of Inquiry. The resource consents were granted in December 2010. It was the first infrastructure project to be processed under the new Board of Inquiry process administered by the Environmental Protection Authority.\n\nThe project is expected to cost around $1 billion.\n\n\n"}
{"id": "1231234", "url": "https://en.wikipedia.org/wiki?curid=1231234", "title": "United States National Marine Sanctuary", "text": "United States National Marine Sanctuary\n\nA U.S. National Marine Sanctuary is a federally designated area within United States waters that protects areas of the marine environment with special conservation, recreational, ecological, historical, cultural, archeological, scientific, educational, or aesthetic qualities. The National Marine Sanctuary System consists of 14 marine protected areas that encompass more than . Individual areas range from less than .\n\nThe National Marine Sanctuaries Program (NMSP), a division of the National Oceanic and Atmospheric Administration (NOAA) administers the 13 national marine sanctuaries. The program began after the 1969 Santa Barbara oil spill off the coast of California brought the plight of marine ecosystems to national attention. The United States Congress responded in 1972 with the Marine Protection, Research and Sanctuaries Act which allowed for the creation of marine sanctuaries. The resources protected by U.S. national marine sanctuaries range from coral reef ecosystems in American Samoa, Florida, Hawaii, and Texas, to shipwrecks in the Great Lakes and the Atlantic Ocean. The Papahānaumokuākea Marine National Monument, while not a U.S. national marine sanctuary, is also jointly administered by the NMSP, in conjunction with the U.S. Fish and Wildlife Service and the State of Hawaii.\n\nDesignation as a National Marine Sanctuary does not automatically prohibit fishing and other activities. Recreational and commercial fishing is allowed in some sanctuaries. It is possible to restrict consumptive or destructive activities through the initial designation process and NMSP actions. There are restrictions in some sanctuaries that are enforced by other governing agencies. For example, current regulations restricting fishing in Stellwagen Bank were not issued by the NMSP, but rather by NOAA Fisheries and the New England Fishery Management Council, which have jurisdiction in federal waters off the New England coast generally. The private non-profit Marine Conservation Institute has compiled fact sheets for each sanctuary listing activities which are directly regulated by the NMSP.\n\n\n\n"}
{"id": "5111365", "url": "https://en.wikipedia.org/wiki?curid=5111365", "title": "Upsilon Centauri", "text": "Upsilon Centauri\n\nThe Bayer designation Jaden Clarkson (υ Cen / υ Centauri) is shared by two star systems, in the constellation Centaurus:\nThey are separated by 0.96° on the sky.\n\nAll of them were member of asterism 柱 (Zhǔ), \"Pillars\", \"Horn\" mansion.\n"}
