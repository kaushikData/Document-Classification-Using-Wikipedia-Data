{"id": "56531205", "url": "https://en.wikipedia.org/wiki?curid=56531205", "title": "2018 CL", "text": "2018 CL\n\nThe Zwicky Transient Facility is a wide-field sky survey using a new camera attached to the Samuel Oschin Telescope at the Palomar Observatory. The Zwicky Transient Facility is designed to detect transient objects that rapidly change in brightness, for example supernova, gamma ray bursts, and collision between two neutron stars, and moving objects like comets and asteroids. The new camera is made up of 16 CCDs of 6000×6000 pixels each, enabling each exposure to cover an area of 47 square degrees. The Zwicky Transient Facility is designed to image the entire northern sky in three nights and scan the plane of the Milky Way twice each night to a limiting magnitude of 20.5. First light was recorded of an area in the constellation Orion on November 1, 2017.\n\n"}
{"id": "30508447", "url": "https://en.wikipedia.org/wiki?curid=30508447", "title": "ARkStorm", "text": "ARkStorm\n\nAn ARkStorm (for Atmospheric River 1,000 Storm) is a hypothetical but scientifically realistic \"megastorm\" scenario developed and published by the United States Geological Survey, Multi Hazards Demonstration Project (MHDP). It describes an extreme storm that might impact much of California causing up to $725 billion in losses (most caused by flooding), and affect a quarter of California's homes. The event would be similar to exceptionally intense California storms which occurred between December 1861 and January 1862. The storm would be a 1-in-1000-year event. The name \"ARkStorm\" means \"Atmospheric River (AR) 1,000 (k).\"\n\nThe hypothetical ARkStorm, if it occurred, would have the following effects: \n\n\n"}
{"id": "2853", "url": "https://en.wikipedia.org/wiki?curid=2853", "title": "Aberdeen Bestiary", "text": "Aberdeen Bestiary\n\nThe Aberdeen Bestiary (Aberdeen University Library, Univ Lib. MS 24) is a 12th-century English illuminated manuscript bestiary that was first listed in 1542 in the inventory of the Old Royal Library at the Palace of Westminster.\n\nInformation about its origins and patron are circumstantial. It probably comes from the 12th century and was owned by a wealthy ecclesiastical patron of the north or south province. The \"Aberdeen Bestiary\" is related to other bestiaries of the Middle Ages such as the \"Ashmole Bestiary\".\n\n\n\nAfter folio 9 verso some leaves are missing which should have contained antelope (\"Antalops\"), unicorn (\"Unicornis\"), lynx (\"Lynx\"), griffin (\"Gryps\") and part of elephant (\"Elephans\").\n\n\nAfter folio 15 verso some leaves are missing which should have contained crocodile (\"Crocodilus\"), manticore (\"Mantichora\") and part of parandrus (\"Parandrus\").\n\n\n\nAfter folio 21 verso two leaves are missing which should have contained ox (\"Bos\"), camel (\"Camelus\"), dromedary (\"Dromedarius\"), ass (\"Asinus\"), onager (\"Onager\") and part of horse (\"Equus\").\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "24435552", "url": "https://en.wikipedia.org/wiki?curid=24435552", "title": "Aerotoxic syndrome", "text": "Aerotoxic syndrome\n\nAerotoxic syndrome is a phrase coined by Chris Winder and Jean-Christophe Balouet in 2000, to describe their claims of short- and long-term ill-health effects caused by breathing airliner cabin air which was alleged to have been contaminated to toxic levels (exceeding known, parts per million, safe levels) with atomized engine oils or other chemicals. An assessment by the UK's House of Lords Science and Technology Committee found that claims of health effects were unsubstantiated.\nAn update in 2008 found no significant new evidence. this syndrome is not recognized in medicine.\n\nModern jetliners have an environmental control system (ECS) that manages the flow of cabin air. Outside air enters the engines and is compressed in the forward section, prior to the combustion section, ensuring no combustion products can enter the cabin. A portion of that compressed bleed air is used to pressurize the cabin. The ECS then recirculates some of that cabin air through HEPA filters, while the rest is directed to outflow valves, ensuring there is a constant supply of fresh, clean air coming into the cabin pressurization system at all times.\n\nIt is possible for contaminants to enter the cabin through the air-supply system and through other means. Substances used in the maintenance and treatment of aircraft, including aviation engine oil, hydraulic fluid, cleaning compounds and de-icing fluids, can contaminate the ECS. Ground and flight crews, as well as passengers themselves can be sources of contaminants such as pesticides, bioeffluents, viruses, bacteria, allergens, and fungal spores.\n\nPossible sources of poor-quality cabin air include exposures related to normal operations of the aircraft:\n\nJet engines require synthetic oils for lubrication. These oils contain ingredients such as tricresyl phosphate (TCP or TOCP), an organophosphate, which can be toxic to humans in quantities much larger than are found in aviation engine oil [citation needed].\n\nEngine bearing seals are installed to ensure that critical engine bearings are continuously lubricated, and to prevent engine oil from leaking into the compressed air stream. If a bearing seal fails and begins to leak, depending on the location of the seal, some amount of engine oil may be released into the compressed air stream. Oil leaks may be detected by an odour akin to hot frying-pan fume, or, in more serious cases, by smoke in the cabin. This is known in the industry as a fume event.\n\nA year-long Australian Senate investigation in 2000 received evidence of some \"successful applications for workers’ compensation\" for illness which the applicants attributed to fumes on the BAe 146. Approximately 20 crew members described oil fumes leaking into the aircraft cabin. That investigative committee concluded \"the issue of fume contaminants should also be considered a safety issue with regard to the ability of cabin crew to properly supervise the evacuation of an aircraft and the ability of passengers to take part in an evacuation\".\n\nOn 5 November 2000, both the captain and first officer of a Jersey European Airways BAe 146 became unwell while landing at Birmingham International Airport.\nBoth became nauseous, and the captain experienced double vision and had difficulty judging height, but managed to land the aircraft safely. Both pilots were taken to a hospital but no cause for their illness was found.\nThe incident investigation report concluded that \"There is circumstantial evidence to suggest that the flight crew on G–JEAK were affected by contamination of the air supply, as a result of oil leakage from the auxiliary power unit (APU) cooling fan seal into the APU air stream, and into the ECS system ducting. This contamination allowed fumes to develop, a proportion of which entered the cabin and cockpit air supply.\"\n\nThe report noted that both captain and first officer had visited the forward toilet before the onset of their symptoms. Four years before the G-JEAK incident, another operator reported overuse of a disinfectant (formaldehyde) for the toilets and to clean the galley floor and that inhalation of the fumes from that chemical, would produce similar symptoms reported by both the captain and first officer of G-JEAK. \"The CAA notified UK Operators at that time (CAA ref. 10A/380/15, dated 2 August 1996) of this potential hazard, as the misuse of this agent was apparently widespread.\"\n\nIn 1986, the United States Congress commissioned a report by the National Research Council (NRC) into cabin air quality. The report recommended a ban on smoking on aircraft in order to improve air quality. In 1988, the FAA banned smoking on domestic flights of less than two hours, and in 2000 extended the ban to all domestic and international flights.\n\nResearch commissioned by the UK government's Department for Transport (DfT) and published in 2000 found no link to long term health. The UK Parliament's Select Committee on Science and Technology concluded in its response to the many complaints received \"from a number of witnesses, particularly the Organophosphate Information Network, BALPA, and the International Association of Flight Attendants, expressing concerns about the risk of tricresyl phosphate (TCP or TOCP) poisoning for cabin occupants, particularly for crew who might be subjected to repeated exposure in some aircraft types, as a result of oil leaking into the cabin air supply.\"\n\nIn 2009 the UK House of Commons Library service to Members of Parliament summarized the research into a \"relationship between the [engine oil chemical] leaks and these health symptoms\" as inconclusive, citing \"problems with identifying the exact chemical that might be entering the air supply and therefore identifying what impact it may have on health\" and \"reports of problems with fumes and/or health symptoms not being reported correctly\".\n\nAccording to a 2008 report by Michael Bagshaw, Aviation Medicine Director at King's College London, there have been no peer-reviewed recorded cases of neurological harm in humans following TCP exposure. He pointed to an unpublished report from the Medical Toxicology Unit at Guy's Hospital in 2001 which looked at all exposures dating back to 1943 that showed that all documented exposures were to high concentrations greatly in excess of the amount present in jet oil.\n\nIn his 2013 paper, \"Cabin Air Quality: A review of current aviation medical understanding,\" Bagshaw noted further: \"A German study in 2013 of 332 crew members who had reported fume/odour during their last flight, failed to detect metabolites of TCP in urine samples. The authors concluded that health complaints could not be linked to TCP exposure in cabin air...A syndrome is a symptom complex, consistent and common to a given condition. Sufferers of the ‘aerotoxic syndrome’ describe a wide range of inconsistent symptoms and signs with much individual variability.\"\n\nThe evidence was independently reviewed by the Aerospace Medical Association, the US National Academy of Sciences and the Australian Civil Aviation Safety Authority (CASA) Expert Panel. All concluded there is insufficient consistency to establish a medical syndrome, and the ‘aerotoxic syndrome’ is not recognised in aviation medicine.\"\n\nThe 'nocebo effect' was among the conclusions published in a 2013 COT (Committee on Toxicity) position paper: \"The acute illness which has occurred in relation to perceived episodes of contamination might reflect a toxic effect of one or more chemicals, but it could also have occurred through nocebo effects. There is strong scientific evidence that nocebo effects can lead to (sometimes severely disabling) illness from environmental exposures that are perceived as hazardous.\"\n\nIn a 2006 article in \"Aviation Today\", Simon Bennett found that media coverage of contaminated cabin air has been sensationalized, with distortions of facts. He cited headlines such as \"You are being gassed when you travel by air,\" and \"Death in the Air\" and a sub-title of \"Every day, planes flying in and out of London City Airport are slowly killing us.\" Bennett noted that the article with the latter subtitle stated in its body that the Department of the Environment, Transport and the Regions (DETR) found that oil seal failures occur only once in every 22,000 flights.\n\nThe \"Sunday Sun\" in an article entitled \"Flight Fumes Warning\", cited the industry pressure group AOPIS in saying that passengers jetting off to their holidays were unknowingly exposed to deadly chemicals, and that brain damage could result if they breathed the toxic fumes. The Sun also cited the UK Civil Aviation Authority finding that leakage into aircraft cabins is a very rare event occurring only if there is a fault with an aircraft.\n\nWhen the results of a clinical audit of the \"cognitive functioning of aircrew exposed to contaminated air\" were submitted by Sarah Mackenzie Ross to the UK government's Committee on Toxicity of Chemicals in Food, Consumer Products and the Environment (COT), some media used it to write articles that were sensational and misleading. \"Dagbladet.no\", wrote that the Ross report \"... adds weight to the hypothesis that compounds resembling nerve gas in cabin and flight deck air have caused irreparable neurological damage to aircrew\", though the report itself stated that:\n\n\"[T]he evidence available to us in this audit does not enable us to draw firm conclusions regarding a causal link with exposure to contaminated air.\" Additionally,\n\nIn 2015, made a documentary on aerotoxic syndrome called \"Unfiltered Breathed In.\"\n\n\n\n"}
{"id": "1942366", "url": "https://en.wikipedia.org/wiki?curid=1942366", "title": "Air quality index", "text": "Air quality index\n\nAn air quality index (AQI) is a number used by government agencies to communicate to the public how polluted the air currently is or how polluted it is forecast to become. As the AQI increases, an increasingly large percentage of the population is likely to experience increasingly severe adverse health effects. Different countries have their own air quality indices, corresponding to different national air quality standards. Some of these are the Air Quality Health Index (Canada), the Air Pollution Index (Malaysia), and the Pollutant Standards Index (Singapore).\n\nComputation of the AQI requires an air pollutant concentration over a specified averaging period, obtained from an air monitor or model. Taken together, concentration and time represent the dose of the air pollutant. Health effects corresponding to a given dose are established by epidemiological research. Air pollutants vary in potency, and the function used to convert from air pollutant concentration to AQI varies by pollutant. Its air quality index values are typically grouped into ranges. Each range is assigned a descriptor, a color code, and a standardized public health advisory.\n\nThe AQI can increase due to an increase of air emissions (for example, during rush hour traffic or when there is an upwind forest fire) or from a lack of dilution of air pollutants. Stagnant air, often caused by an anticyclone, temperature inversion, or low wind speeds lets air pollution remain in a local area, leading to high concentrations of pollutants, chemical reactions between air contaminants and hazy conditions. \n\nOn a day when the AQI is predicted to be elevated due to fine particle pollution, an agency or public health organization might:\n\nDuring a period of very poor air quality, such as an air pollution episode, when the AQI indicates that acute exposure may cause significant harm to the public health, agencies may invoke emergency plans that allow them to order major emitters (such as coal burning industries) to curtail emissions until the hazardous conditions abate.\n\nMost air contaminants do not have an associated AQI. Many countries monitor ground-level ozone, particulates, sulfur dioxide, carbon monoxide and nitrogen dioxide, and calculate air quality indices for these pollutants.\n\nThe definition of the AQI in a particular nation reflects the discourse surrounding the development of national air quality standards in that nation. A website allowing government agencies anywhere in the world to submit their real-time air monitoring data for display using a common definition of the air quality index has recently become available.\n\nAir quality in Canada has been reported for many years with provincial Air Quality Indices (AQIs). Significantly, AQI values reflect air quality management objectives, which are based on the lowest achievable emissions rate, and not exclusively concern for human health. The Air Quality Health Index or (AQHI) is a scale designed to help understand the impact of air quality on health. \nIt is a health protection tool used to make decisions to reduce short-term exposure to air pollution by adjusting activity levels during increased levels of air pollution. The Air Quality Health Index also provides advice on how to improve air quality by proposing behavioural change to reduce the environmental footprint. This index pays particular attention to people who are sensitive to air pollution. It provides them with advice on how to protect their health during air quality levels associated with low, moderate, high and very high health risks.\n\nThe Air Quality Health Index provides a number from 1 to 10+ to indicate the level of health risk associated with local air quality. On occasion, when the amount of air pollution is abnormally high, the number may exceed 10. The AQHI provides a local air quality current value as well as a local air quality maximums forecast for today, tonight, and tomorrow, and provides associated health advice.\n\nOn December 30, 2013 Hong Kong replaced the Air Pollution Index with a new index called the \"Air Quality Health Index\". This index, reported by the Environmental Protection Department, is measured on a scale of 1 to 10+ and considers four air pollutants: ozone; nitrogen dioxide; sulphur dioxide and particulate matter (including PM10 and PM2.5). For any given hour the AQHI is calculated from the sum of the percentage excess risk of daily hospital admissions attributable to the 3-hour moving average concentrations of these four pollutants. The AQHIs are grouped into five AQHI health risk categories with health advice provided:\n\nEach of the health risk categories has advice with it. At the \"low\" and \"moderate\" levels the public are advised that they can continue normal activities. For the \"high\" category, children, the elderly and people with heart or respiratory illnesses are advising to reduce outdoor physical exertion. Above this (\"very high\" or \"serious\") the general public are also advised to reduce or avoid outdoor physical exertion.\n\nChina's Ministry of Environmental Protection (MEP) is responsible for measuring the level of air pollution in China. As of January 1, 2013, MEP monitors daily pollution level in 163 of its major cities. The AQI level is based on the level of six atmospheric pollutants, namely sulfur dioxide (SO), nitrogen dioxide (NO), suspended particulates smaller than 10 μm in aerodynamic diameter (PM), suspended particulates smaller than 2.5 μm in aerodynamic diameter (PM)，carbon monoxide (CO), and ozone (O) measured at the monitoring stations throughout each city.\n\nAQI MechanicsAn individual score (Individual Air Quality Index, IAQI) is assigned to each pollutant and the final AQI is \"the highest\" of these six scores. The final AQI value can be calculated either per hour or per 24 hours. The concentrations of pollutants can be measured quite differently. If the AQI value is calculated hourly, then SO, NO, CO concentrations are measured as average per 24h, O concentration is measured as average per hour and the moving average per 8h, PM and PM concentrations are measured as average per hour and per 24h. If the AQI value is calculated per 24h, then SO, NO, CO, PM and PM concentrations are measured as average per 24h, while O concentration is measured as the maximum 1h average and the maximum 24h moving average. The IAQI of each pollutant is calculated according to a formula published by the MEP.\n\nThe score for each pollutant is non-linear, as is the final AQI score. Thus an AQI of 300 does not mean twice the pollution of AQI at 150, nor does it mean the air is twice as harmful. The concentration of a pollutant when its IAQI is 100 does not equal twice its concentration when its IAQI is 50, nor does it mean the pollutant is twice as harmful. While an AQI of 50 from day 1 to 182 and AQI of 100 from day 183 to 365 does provide an annual average of 75, it does \"not\" mean the pollution is acceptable even if the benchmark of 100 is deemed safe. Because the benchmark is a 24-hour target, and the annual average must match the annual target, it is entirely possible to have safe air every day of the year but still fail the annual pollution benchmark.\n\nAQI and Health Implications (HJ 633—2012)\nThe National Air Quality Index (AQI) was launched in New Delhi on September 17, 2014 under the Swachh Bharat Abhiyan.\n\nThe Central Pollution Control Board along with State Pollution Control Boards has been operating National Air Monitoring Program (NAMP) covering 240 cities of the country having more than 342 monitoring stations. An Expert Group comprising medical professionals, air quality experts, academia, advocacy groups, and SPCBs was constituted and a technical study was awarded to IIT Kanpur. IIT Kanpur and the Expert Group recommended an AQI scheme in 2014. While the earlier measuring index was limited to three indicators, the new index measures eight parameters. The continuous monitoring systems that provide data on near real-time basis are installed in New Delhi, Mumbai, Pune and Ahmedabad.\n\nThere are six AQI categories, namely Good, Satisfactory, Moderately polluted, Poor, Very Poor, and Severe. The proposed AQI will consider eight pollutants (PM, PM, NO, SO, CO, O, NH, and Pb) for which short-term (up to 24-hourly averaging period) National Ambient Air Quality Standards are prescribed. Based on the measured ambient concentrations, corresponding standards and likely health impact, a sub-index is calculated for each of these pollutants. The worst sub-index reflects overall AQI. Likely health impacts for different AQI categories and pollutants have also been suggested, with primary inputs from the medical experts in the group. The AQI values and corresponding ambient concentrations (health breakpoints) as well as associated likely health impacts for the identified eight pollutants are as follows:\n\nThe air quality in Mexico City is reported in IMECAs. The IMECA is calculated using the measurements of average times of the chemicals ozone (O), sulphur dioxide (SO), nitrogen dioxide (NO), carbon monoxide (CO), particles smaller than 2.5 micrometers (PM), and particles smaller than 10 micrometers (PM).\n\nSingapore uses the Pollutant Standards Index to report on its air quality, with details of the calculation similar but not identical to that used in Malaysia and Hong Kong\nThe PSI chart below is grouped by index values and descriptors, according to the National Environment Agency.\n\nThe Ministry of Environment of South Korea uses the Comprehensive Air-quality Index (CAI) to describe the ambient air quality based on the health risks of air pollution. The index aims to help the public easily understand the air quality and protect people's health. The CAI is on a scale from 0 to 500, which is divided into six categories. The higher the CAI value, the greater the level of air pollution.\nOf values of the five air pollutants, the highest is the CAI value. The index also has associated health effects and a colour representation of the categories as shown below.\n\nThe N Seoul Tower on Namsan Mountain in central Seoul, South Korea, is illuminated in blue, from sunset to 23:00 and 22:00 in winter, on days where the air quality in Seoul is 45 or less. During the spring of 2012, the Tower was lit up for 52 days, which is four days more than in 2011.\n\nThe most commonly used air quality index in the UK is the \"Daily Air Quality Index\" recommended by the Committee on Medical Effects of Air Pollutants (COMEAP). This index has ten points, which are further grouped into 4 bands: low, moderate, high and very high. Each of the bands comes with advice for at-risk groups and the general population.\n\nThe index is based on the concentrations of 5 pollutants. The index is calculated from the concentrations of the following pollutants: Ozone, Nitrogen Dioxide, Sulphur Dioxide, PM2.5 (particles with an aerodynamic diameter less than 2.5 μm) and PM10. The breakpoints between index values are defined for each pollutant separately and the overall index is defined as the maximum value of the index. Different averaging periods are used for different pollutants.\n\nThe \"Common Air Quality Index\" (CAQI) is an air quality index used in Europe since 2006. In November 2017, the European Environment Agency announced the \"European Air Quality Index\" (EAQI) and started encouraging its use on websites and for other ways of informing the public about air quality.\n\n, the EU-supported project \"CiteairII\" argued that the CAQI had been evaluated on a \"large set\" of data, and described the CAQI's motivation and definition. \"CiteairII\" stated that having an air quality index that would be easy to present to the general public was a major motivation, leaving aside the more complex question of a health-based index, which would require, for example, effects of combined levels of different pollutants. The main aim of the CAQI was to have an index that would encourage wide comparison across the EU, without replacing local indices. \"CiteairII\" stated that the \"main goal of the CAQI is not to warn people for possible adverse health effects of poor air quality but to attract their attention to urban air pollution and its main source (traffic) and help them decrease their exposure.\" \n\nThe CAQI is a number on a scale from 1 to 100, where a low value means good air quality and a high value means bad air quality. The index is defined in both hourly and daily versions, and separately near roads (a \"roadside\" or \"traffic\" index) or away from roads (a \"background\" index). , the CAQI had two mandatory components for the roadside index, NO and PM, and three mandatory components for the background index, NO, PM and O. It also included optional pollutants PM, CO and SO. A \"sub-index\" is calculated for each of the mandatory (and optional if available) components. The CAQI is defined as the sub-index that represents the worst quality among those components.\n\nSome of the key pollutant densities in μg/m for the hourly background index, the corresponding sub-indices, and five CAQI ranges and verbal descriptions are as follows.\nFrequently updated CAQI values and maps are shown on the \"www.airqualitynow.eu\" and other websites. A separate \"Year Average Common Air Quality Index\" (YACAQI) is also defined, in which different pollutant sub-indices are separately normalised to a value typically near unity. For example, the yearly averages of NO, PM and PM are divided by 40 μg/m^3, 40 μg/m^3 and 20 μg/m^3, respectively. The overall background or traffic YACAQI for a city is the arithmetic mean of a defined subset of these sub-indices.\n\nThe United States Environmental Protection Agency (EPA) has developed an Air Quality Index that is used to report air quality. This AQI is divided into six categories indicating increasing levels of health concern. An AQI value over 300 represents hazardous air quality and below 50 the air quality is good.\nThe AQI is based on the five \"criteria\" pollutants regulated under the Clean Air Act: ground-level ozone, particulate matter, carbon monoxide, sulfur dioxide, and nitrogen dioxide. The EPA has established National Ambient Air Quality Standards (NAAQS) for each of these pollutants in order to protect public health. An AQI value of 100 generally corresponds to the level of the NAAQS for the pollutant. The Clean Air Act (USA) (1990) requires EPA to review its National Ambient Air Quality Standards every five years to reflect evolving health effects information. The Air Quality Index is adjusted periodically to reflect these changes.\n\nThe air quality index is a piecewise linear function of the pollutant concentration.\nAt the boundary between AQI categories, there is a discontinuous jump of one AQI unit.\nTo convert from concentration to AQI this equation is used:\n\nformula_1\n\nwhere:\n\nEPA's table of breakpoints is:\nSuppose a monitor records a 24-hour average fine particle (PM) concentration of 12.0 micrograms per cubic meter. The equation above results in an AQI of:\ncorresponding to air quality in the \"Good\" range. To convert an air pollutant concentration to an AQI, EPA has developed a calculator.\n\nIf multiple pollutants are measured at a monitoring site, then the largest or \"dominant\" AQI value is reported for the location. The ozone AQI between 100 and 300 is computed by selecting the larger of the AQI calculated with a 1-hour ozone value and the AQI computed with the 8-hour ozone value.\n\n8-hour ozone averages do not define AQI values greater than 300; AQI values of 301 or greater are calculated with 1-hour ozone concentrations. 1-hour SO values do not define higher AQI values greater than 200. AQI values of 201 or greater are calculated with 24-hour SO concentrations.\n\nReal time monitoring data from continuous monitors are typically available as 1-hour averages. However, computation of the AQI for some pollutants requires averaging over multiple hours of data. (For example, calculation of the ozone AQI requires computation of an 8-hour average and computation of the PM or PM AQI requires a 24-hour average.) To accurately reflect the current air quality, the multi-hour average used for the AQI computation should be centered on the current time, but as concentrations of future hours are unknown and are difficult to estimate accurately, EPA uses surrogate concentrations to estimate these multi-hour averages. For reporting the PM, PM and ozone air quality indices, this surrogate concentration is called the NowCast. The Nowcast is a particular type of weighted average that provides more weight to the most recent air quality data when air pollution levels are changing. There is a free email subscription service for New York inhabitants - AirNYC. Subscribers get notification about AQI values changes for selected location (eg home address), based on air quality conditions.\n\nReal time monitoring data and forecasts of air quality that are color-coded in terms of the air quality index are available from EPA's AirNow web site. Historical air monitoring data including AQI charts and maps are available at EPA's AirData website. Detailed map about current AQI level and its two day forecast is available from Aerostate web site.\n\nThe AQI made its debut in 1968, when the National Air Pollution Control Administration undertook an initiative to develop an air quality index and to apply the methodology to Metropolitan Statistical Areas. The impetus was to draw public attention to the issue of air pollution and indirectly push responsible local public officials to take action to control sources of pollution and enhance air quality within their jurisdictions.\n\nJack Fensterstock, the head of the National Inventory of Air Pollution Emissions and Control Branch, was tasked to lead the development of the methodology and to compile the air quality and emissions data necessary to test and calibrate resultant indices.\n\nThe initial iteration of the air quality index used standardized ambient pollutant concentrations to yield individual pollutant indices. These indices were then weighted and summed to form a single total air quality index. The overall methodology could use concentrations that are taken from ambient monitoring data or are predicted by means of a diffusion model. The concentrations were then converted into a standard statistical distribution with a preset mean and standard deviation. The resultant individual pollutant indices are assumed to be equally weighted, although values other than unity can be used. Likewise, the index can incorporate any number of pollutants although it was only used to combine SOx, CO, and TSP because of a lack of available data for other pollutants.\n\nWhile the methodology was designed to be robust, the practical application for all metropolitan areas proved to be inconsistent due to the paucity of ambient air quality monitoring data, lack of agreement on weighting factors, and non-uniformity of air quality standards across geographical and political boundaries. Despite these issues, the publication of lists ranking metropolitan areas achieved the public policy objectives and led to the future development of improved indices and their routine application.\n\n\nSome of the following websites display actively updated air quality index maps; others are archived versions of inactive websites:\n"}
{"id": "42963616", "url": "https://en.wikipedia.org/wiki?curid=42963616", "title": "Alkali sink", "text": "Alkali sink\n\nAn alkali sink is a salty basin land form. The term may also refer to a North American desert vegetation type (biome) characteristic of that landform. Rainwater drains to the basin and collects in areas where it cannot penetrate the soil due to a layer of clay or caliche, producing a pond or lake. When the water evaporates, it leaves behind increasing amounts of salts in the soil. Plants that tolerate the extreme salt concentrations are known as halophytes. It is generally below the saltbrush scrub vegetation type, which is typified by less salt tolerant species than alkali sink types.\n"}
{"id": "28464149", "url": "https://en.wikipedia.org/wiki?curid=28464149", "title": "Argia (mythology)", "text": "Argia (mythology)\n\nArgia , Argea , or Argeia (Ancient Greek: Ἀργεία) is a name borne by four minor characters in Greek mythology. These are:\n\n"}
{"id": "55667801", "url": "https://en.wikipedia.org/wiki?curid=55667801", "title": "Association for the Protection of Sea-Birds", "text": "Association for the Protection of Sea-Birds\n\nThe Association for the Protection of Sea-Birds was formed in the late 1860s by The Rev. Henry Frederick Barnes-Lawrence the Incumbent (ecclesiastical) at Bridlington Priory. to stop the practice of shooting sea birds for sport, a practice which was legislated for in 1869.\nAs well as promoting the society in Yorkshire Barnes-Frederick also set up a branch in Ryde where he had been a Curate.\n"}
{"id": "33602881", "url": "https://en.wikipedia.org/wiki?curid=33602881", "title": "Bumpy torus", "text": "Bumpy torus\n\nThe bumpy torus is a class of magnetic fusion energy devices that consist of a series of magnetic mirrors connected end-to-end to form a closed torus. Such an arrangement is not stable on its own, and most bumpy torus designs use secondary fields or relativistic electrons to create a stable field inside the reactor. The main disadvantage of magnetic mirror confinement, that of excessive plasma leakage, is circumvented by the arrangement of multiple mirrors end-to-end in a ring. It is described as \"bumpy\" because the fuel ions comprising the plasma tend to concentrate inside the mirrors at greater density than the leakage currents between mirror cells.\n\nBumpy torus designs were an area of active research in the 1960s and 70s, notably with the ELMO (ELectro Magnetic Orbit) Bumpy Torus at the Oak Ridge National Laboratory. One in particular has been described: \"Imagine a series of magnetic mirror machines placed end to end and twisted into a torus. An ion or electron that leaks out of one mirror cavity finds itself in another mirror cell. This constitutes a bumpy torus.\" These demonstrated problems and most research on the concept has ended.\n"}
{"id": "27721398", "url": "https://en.wikipedia.org/wiki?curid=27721398", "title": "Carl Burger", "text": "Carl Burger\n\nCarl V. Burger (June 18, 1888 – December 30, 1967) was an American \"artist and writer of children’s books about animals and natural history.\" He is known for his children's and youth literature illustrations of \"The Incredible Journey\" by Sheila Burnford and the Newbery Medal honor novels \"Old Yeller\" by Fred Gipson and \"Little Rascal\" by Sterling North.\n\nBurger was born in Maryville, Tennessee, to a banker, Joseph Burger and Elizabeth (Knox) Burger. He attended Maryville College and Stanford University prior to transferring to Cornell University, where he took his Bachelor of Architecture in 1912. At Cornell, Burger was art editor for the \"Cornell Era\" and the \"Cornellian\". His mentor was the noted naturalist Louis Agassiz Fuertes. Burger later studied at the School of Museum of Fine Arts in Boston for three years.\n\nPrior to the First World War, he worked for the \"Boston Post\", drawing sports and political cartoons, as well as illustrations for the Sunday edition. He married Margaret Rothery on September 18, 1920. The Burgers had one child, Knox Burger. Carl completed his national service in the United States Armed Forces between 1917 and 1920, rising to the rank of Captain, United States Army. During this period he organized and directed the American Expeditionary Forces School of Painting in Beaune, France. The illustrations in \"The History of the Inter-Allied Games\", published in Paris by the United States Army, were drawn under Burger’s direction.\n\nCarl Burger commenced his artistic career as an Art Director for N.W. Ayer and Sons, a New York-based advertising firm with offices in Philadelphia. He also worked for Edwin Bird Wilson, Inc., a financial advertising firm with offices in New York City and Chicago and Barton, Durstine & Osborn. During World War Two, he was the art director for the American Red Cross in Washington, D.C. Burger wrote and illustrated his own books as well as illustrating many books and magazines. These included \"All About Fish\", published by Random House in 1965. Burger also contributed \"All About Dogs\" and \"All About Elephants\". He also painted large murals for the Bronx Zoo and the New York Aquarium.\n\nBurger was a member of the American Museum of Natural History, the New York Zoological Society, the Cornell Club of New York, the Phi Kappa Psi Fraternity, and through that organization, the Irving Literary Society. He lived in Pleasantville, New York, at his death in 1967.\n"}
{"id": "17122264", "url": "https://en.wikipedia.org/wiki?curid=17122264", "title": "Deep water source cooling", "text": "Deep water source cooling\n\nDeep water source cooling (DWSC) or deep water air cooling is a form of air cooling for process and comfort space cooling which uses a large body of naturally cold water as a heat sink. It uses water at 4 to 10 degrees Celsius drawn from deep areas within lakes, oceans, aquifers or rivers, which is pumped through the one side of a heat exchanger. On the other side of the heat exchanger, cooled water is produced.\n\nWater is most dense at at standard atmospheric pressure. Thus as water cools below 3.98 °C it decreases in density and will rise. As the temperature climbs above 3.98 °C, water density also decreases and causes the water to rise, which is why lakes are warmer on the surface during the summer. The combination of these two effects means that the bottom of most deep bodies of water located well away from the equatorial regions is at a constant 3.98 °C.\n\nAir conditioners are heat pumps. During the summer, when outside air temperatures are higher than the temperature inside a building, air conditioners use electricity to transfer heat from the cooler interior of the building to the warmer exterior ambient. This process uses electrical energy.\n\nUnlike residential air conditioners, most modern commercial air conditioning systems do not transfer heat directly into the exterior air. The thermodynamic efficiency of the overall system can be improved by utilizing evaporative cooling, where the temperature of the cooling water is lowered close to the wet-bulb temperature by evaporation in a cooling tower. This cooled water then acts \nas the heat sink for the heat pump.\n\nDeep lake water cooling uses cold water pumped from the bottom of a lake as a heat sink for climate control systems. Because heat pump efficiency improves as the heat sink gets colder, deep lake water cooling can reduce the electrical demands of large cooling systems where it is available. It is similar in concept to modern geothermal sinks, but generally simpler to construct given a suitable water source.\n\nDeep lake water cooling allows higher thermodynamic efficiency by using cold deep lake water, which is colder than the ambient wet bulb temperature. The higher efficiency results in less electricity used. For many buildings, the lake water is sufficiently cold that the refrigeration portion of the air conditioning systems can be shut down during some environmental conditions and the building interior heat can be transferred directly to the lake water heat sink. This is referred to as \"free cooling\", but is not actually free, since pumps and fans run to circulate the lake water and building air.\n\nOne added attraction of deep lake water cooling is that it saves energy during peak load times, such as summer afternoons, when a sizable amount of the total electrical grid load is air conditioning.\n\nDeep water source cooling is very energy efficient, requiring only 1/10 of the average energy required by conventional cooler systems. Consequently, its running costs can also be expected to be much lower.\n\nThe energy source is very local and fully renewable, provided that the water and heat rejected into the environment (often the same lake or a nearby river) does not disturb the natural cycles. It does not use any ozone depleting refrigerant.\n\nDepending on the needs and on the water temperature, couple heating and cooling can be considered. For example, heat could first be extracted from the water (making it colder); and, secondly, that same water could cycle to a refrigerating unit to be used for even more effective cold production.\n\nDeep water source cooling requires a large and deep water quantity in the surroundings. To obtain water in the 3 to 6 °C (37 to 43 °F) range, a depth of to is generally required, depending on the local conditions.\n\nThe set-up of a system is expensive and labour-intensive. The system also requires a great amount of source material for its construction and placement.\n\nCornell University's Lake Source Cooling System uses Cayuga Lake as a heat sink to operate the central chilled water system for its campus and to also provide cooling to the Ithaca City School District. The system has operated since the summer of 2000 and was built at a cost of $55–60 million. It cools a 14,500 ton (51 megawatt) load.\n\nSince August 2004, a deep lake water cooling system has been operated by the Enwave Energy Corporation in Toronto, Ontario.\nIt draws water from Lake Ontario through tubes extending into the lake, reaching to a depth of . The deep lake water cooling system is part of an integrated district cooling system that covers Toronto's financial district, and has a cooling power of 59,000 tons (207 MW). The system currently has enough capacity to cool of office space.\n\nThe cold water drawn from Lake Ontario's deep layer in the Enwave system is not returned directly to the lake once it has been run through the heat exchange system. The Enwave system only uses water that is destined to meet the city's domestic water needs. Therefore, the Enwave system does not pollute the lake with a plume of waste heat.\n\nAlso known as Ocean Water Cooling. The InterContinental Resort and Thalasso-Spa on the island of Bora Bora uses a seawater air conditioning (SWAC) system to air-condition its buildings. The system accomplishes this by passing cold seawater through a heat exchanger where it cools freshwater in a closed loop system. This cool freshwater is then pumped to buildings and is used for cooling directly (no conversion to electricity takes place). Similar systems are also in place in The Excelsior hotel and The Hong Kong and Shanghai Banking Corporation main building in Hong Kong, and at the Natural Energy Laboratory of Hawaii Authority. The InterContinental Resort is the largest seawater air conditioning system to date, though there are several other, larger systems being planned. Honolulu Seawater Air Conditioning is a project intending to use seawater air conditioning to deliver renewable cooling to commercial and residential properties in the downtown Honolulu area. It is majority owned by eBay founder Pierre Omidyar's Ulupono Initiative.\n\n\n\n"}
{"id": "18830020", "url": "https://en.wikipedia.org/wiki?curid=18830020", "title": "Economic conductivity", "text": "Economic conductivity\n\nEconomic Conductivity (2008) is a sophisticated financial analysis of an oil or natural gas reservoir's ability to provide maximum financial payout through optimized productive capacity.\n\nOil and natural gas are typically found hundreds or thousands of feet underground in the small pores of rock formations. The conductivity or permeability of a reservoir is one of the most critical factors in getting oil and gas to the surface. Reservoirs are often treated by hydraulic fracturing or other activities to stimulate greater conductivity.\n\nFor petroleum exploration and production (E&P) companies (ranging from multi-national supermajors to small independents) as well as service companies which drill oil and natural gas wells, a single well represents an investment of millions of dollars. While producers have an understandable incentive to minimize their costs, a growing number are choosing to look beyond initial costs, instead designing completion and stimulation programs that optimize production for cost-effective output, saving money in the long run.\n\nEconomic Conductivity analysis can result in optimized conductivity under realistic conditions to provide the best return on investment.\n\nThe petroleum industry traditionally used simple formulae and basic models to try to predict a well’s production capacity. Economic Conductivity analysis draws upon significant advances in technology and detailed case studies, enabling petroleum engineers to factor in complex variables and downhole conditions such as closure stress, non-Darcy Flow, multiphase flow, fluid velocity, and cyclic stress to determine the realistic conductivity of the reservoir. \n\nThe costs of hydraulic fracturing and other stimulation activities can then be assessed according to the corresponding increases in production, allowing producers to achieve the most cost-efficient production of oil and gas.\n\nEconomic Conductivity is measured in millidarcy-feet per dollar (mD-ft/$).\n\nEconomic Conductivity is a trademark owned by CARBO Ceramics Inc., a manufacturer of ceramic proppants for use in the hydraulic fracturing of natural gas and oil wells. In 2008, CARBO Ceramics filed U.S. trademark application no. 77/445,017 seeking federal registration of the Economic Conductivity trademark. The application is currently pending before the U.S. Patent and Trademark Office.\n\n\n"}
{"id": "13729760", "url": "https://en.wikipedia.org/wiki?curid=13729760", "title": "Environmental pricing reform", "text": "Environmental pricing reform\n\nEnvironmental pricing reform (EPR) is the process of adjusting market prices to include environmental costs and benefits.\n\nAn externality (a type of market failure) exists where a market price omits environmental costs and/or benefits. In such a situation, rational (self-interested) economic decisions can lead to environmental harm, as well as to economic distortions and inefficiencies.\n\nEnvironmental pricing reform can be economy-wide, or more focused (e.g. specific to a sector (such as electric power generation or mining) or a particular environmental issue (such as climate change). A \"market based instruments\" or \"economic instrument for environmental protection\" is an individual instance of Environmental Pricing Reform. Examples include green tax-shifting (ecotaxation), tradeable pollution permits, or the creation of markets for ecological services.\n\nA similar term, \"ecological fiscal reform\" differs in more narrowly dealing with fiscal (i.e. tax) policies as opposed to using non-fiscal regulations to achieve the government's environmental goals.\n\n\n"}
{"id": "1816232", "url": "https://en.wikipedia.org/wiki?curid=1816232", "title": "Etugen Eke", "text": "Etugen Eke\n\nEtügen Eke (\"Mother Earth\", also transliterated variously as Itügen or Etügen Ekhe) is a Mongolian ( — Etügen ekh) and Turkic earth goddess. She was believed to be perpetually virginal. In Mongolian language, the word \"etugen\" associates with woman and daughter of Kayra. Also her name may originated from Ötüken, the holy mountain of the earth and fertility goddess of the ancient Turks. Medieval sources sometimes pair Etugen with a male counterpart named Natigai or Nachigai (Natikai, Natıkay), although this is probably a mistake based on a mispronunciation of Etugen. In Mongol mythology Etugen is often represented as a young woman riding a grey bull.\n\nEtugen existed in the middle of the Universe. The Turkish people depicted Etugen as a voluptuous, beautiful woman, who was patroness of the Homeland and nature. All living beings were subordinate to her. Therefore, the Turkish people viewed Etugen as the second highest deity, after Kök-Tengri (Gök Tanrı). The dominant role in determining the fate of people and nations belonged to Tengri, but natural forces yielded to Etugen. Sometimes on Tengri's command, Etugen punished people for their sins. But she was generally considered a benevolent Goddess. To appease the goddess Etugen, sacrifices were made every spring in preparation for the cattle-breeding season and before planting crops. Sacrifices were also conducted in the autumn, after the completion of the harvest. During the times of the Khaganates, sacrifices to Etugen had a nationwide character. They were conducted near rivers and on the banks of lakes. A reddish horse was sacrificed with appeals for the fertility of cattle and crops, and for general well being.\n\n\n\n"}
{"id": "7007708", "url": "https://en.wikipedia.org/wiki?curid=7007708", "title": "Foundation for Ecological Security", "text": "Foundation for Ecological Security\n\nThe Foundation for Ecological Security (FES) is a registered non-profit organisation based in Anand, Gujarat, India working towards the ecological restoration and conservation of land and water resources in ecologically fragile, degraded and marginalised regions of the country, through concentrated and collective efforts of village communities.\n\nFES has been involved in assisting the restoration, management and governance of Common Property Land Resources since 1986. The organisation uses a holistic approach to resource management by “intertwining principles of nature conservation and local self-governance in order to accelerate ecological restoration, as well as improve the living conditions of the poor.”\n\nMost of FES’ efforts are concentrated in the dryland regions of the country; however the landscapes worked on are as diverse as scrub lands, tidal mudflats, dense forests, ravines, grasslands, farm fields and water bodies.\n\nRegistered under the Societies Registration Act XXI 1860, the Foundation for Ecological Security was set up in 2001 to strengthen the “massive and critical task of ecological restoration” and improve the governance of natural resources in India.\n\nAccording to their website the mission statement of the organization is, “As ‘ecological security’ is the foundation of sustainable and equitable development, the Foundation for Ecological Security (FES) is committed to strengthening, reviving or restoring, where necessary, the process of ecological succession and the conservation of land, forest and water resources in the country.”\n\nGiven its presence at various levels of governance – from villages and districts to the state and national level – FES is well poised to voice local concerns on regional, national and global platforms.\n\nFES works towards centre-staging ecological agenda alongside priorities of economic growth, reorienting progress from the standpoint of conservation and social justice, and presenting local visions and voices at both local and global levels. Working from a systems perspective, efforts are aimed at three fundamental dimensions (and their interfaces) of rural life: ecological restoration, local governance, and livelihoods.\n\nFES uses a holistic approach to resource management that includes securing legal rights for rural communities, strengthening village institutions, restoration of the degraded landscapes and improving the long-term sustainability of natural resources.\n\nMost contemporary initiatives on livelihood promotion do not take into account the threshold limits of ecosystems and instead push for an exploitative trend that is obviously untenable in the long run. FES, however, strives to highlight the threshold limits of the given agro-ecological system so as to aid communities in determining consumption levels within the ecological capacity of the area, by highlighting natural resource-based livelihood options that are ecologically sound and economically rewarding.\n\nFES’ work with land, water and people is governed by a deep understanding of interrelationships – inter-relationships between natural and human systems, between different ecosystems within a landscape, and between different elements within an ecosystem, as also the inter-linkages between Commons, livestock and agriculture.\n\nThe organisation’s efforts focus on strengthening systemic drivers (such as soil, moisture, nutrients, pollinators and biodiversity) and the natural inter-linkages between various elements of the farming system, while parallelly aiding village communities in strengthening community institutions for local self-governance and in this way, adding to the resilience of both rural landscapes and people’s endeavours.\n\nFES works through local self-governance institutions to promote the judicious management of natural resources, partnering with village communities committed to restoring ecosystems and landscapes, and crafting suitable institutional spaces which safeguard the interests of the poor.\n\nIn various parts of the country, alongside community institutions, FES joins hands with civil society, academia, local elected representatives and government functionaries to promote informed stewardship and concerted action towards restoring ecological health. FES also leverages funds available under Rural Employment Guarantee Programmes and channels their effective utilisation to restore degraded landscapes and revitalise local self-governance institutions.\n\nFES has considerable experience in building capacities of representatives of village institutions, Panchayats, government and non-government officials, who can steer development processes at the village level in areas of local governance and stewardship of natural resources. In 2011, FES initiated Prakriti Karyashala or rural colleges to assist communities and local self-governance institutions in shaping and translating their visions of local development and filling the gaps in knowledge, leadership and skills. The Karyashala offers programmes to Village Forest Committees along forest fringes in Rajasthan and Orissa, to village institutions integrating MGNREGA and managing Commons in Rajasthan and Andhra Pradesh, and on watershed associations planning to monitor hydrological changes and craft usage regulations in Andhra Pradesh.\n\nFES is one of the largest organizations focused on giving India’s rural poor rights to common land (“the Commons”). Commons in India have been continuously projected as 'wastelands' and diverted for alternate uses such as biofuel cultivation, corporate contract farming and industrial zones. While more than 90% of India’s population depends on the Commons for their livelihood, few have formal rights to these resources. To challenge the growing threats that Commons face from their reallocation, over exploitation and encroachment, FES launched the ‘Commons Initiative’ in 2009. The Initiative aims to build strategic collaborations and bring together practitioners, decision-makers and scholars for a long-term campaign that would influence policy and programmatic action on Commons in India. In this regard, FES works towards representing landless communities, organizing long-term leasing arrangements and securing tenure with State governments.\n\nFES undertakes studies to help locate its work in the larger context and designs pursuits that are both grounded and technically rigorous, while providing a sound basis for influencing policy. The studies are designed to engage local communities in search of suitable solutions and build on their knowledge for informed community-level action for ecological restoration, natural resource management and institution building. A comprehensive framework has also been developed to study ecological, social and economic issues in representative locations and to monitor changes over a period of time in order to upgrade the effectiveness of our work at the village and landscape level. \nFES has a well developed Geographic Information System (GIS) and Remote Sensing Facility that supports these studies with spatial information, representation and analysis. In 2010, FES launched Indian Biodiversity Information System (IBIS), a web-based modular and searchable portal to provide reliable species-level information on a single user-friendly platform. Catering to a wide range of stakeholder groups, ranging from amateur wildlife enthusiasts to serious researchers, conservationists and educationists, IBIS aims at becoming a crucial tool for achieving conservation goals in the subcontinent.\n\nAs of March, 2013 FES works with 28 districts in 7 states of the country engaging with 5323 village institutions and protecting around 4,71,521 hectares of revenue wastelands, forest lands and Panchayat grazing lands creating a difference in the lives of 28,95,024 individuals. The central theme of work done by FES revolves around intertwining principles of nature conservation and strengthening village institutions so as to directly improve the living conditions of the poor and the marginalised. FES support Panchayats and their subcommittees, Village Forest Committees, Gramya Jungle Committees, Water Users Associations and Watershed Committees. Regardless of the form of the institution, FES strives for a future where local village communities determine and move towards desirable land-use based on principles of conservation and social justice.\n\nFES collaborates with several practitioner and academic bodies engaged in ecological restoration, community institutions and rural livelihoods.\nFES partners with the Dakshin Foundation to publish Common Voices and Current Conservation. With Kalpavriksh, FES brings out the Protected Area Update and Forest Case Update.\n\nIn collaboration with Collective Action and Property Rights (CAPRi), FES seeks to advance common interests on collective action and property rights of communities through developing effective advocacy, communication, and training materials.\n\nFES collaborates with different universities: Washington University, St. Louis, to study subjects related to systems dynamics, energy conservation, coupled human and natural systems; Clemson University, USA, on hydrological studies; University of Michigan, Ann Arbor, and University of Illinois, Urbana-Champaign, and Indiana University, Bloomington, on forest resource institutions and climate change.\n\nFES anchors the Rainfed Livestock Network (RLN), a consortium of NGOs which works to highlight issues related to livestock rearers in rainfed areas of India. FES is also a member of the ‘Future of Conservation’ consortium, and Revitalization of Rainfed Agricultural Network.\n\nFES is currently a member of the World Conservation Union (IUCN), International Land Coalition (ILC), International Association for the Study on the Commons (IASC), International Society of Ecological Economics (ISEE) and its Indian chapter, the Indigenous and Community Conserved Areas (ICCA) Consortium, and the UN Economic and Social Council (UNECOSOC).\n\nFES has been supported by many funding partners over the years, including Arghyam, Concern Worldwide, The Duleep Matthai Nature Conservation Trust, Fondation Ensemble, Ford Foundation, Department of Rural Development (Government of Andhra Pradesh), Department of Rural Development (Government of Gujarat), Department of Rural Development (Government of Andhra Rajasthan), Grow-Trees, Hilton Foundation, Irrigation and Command Area Development (I&CAD) Department (Government of Andhra Pradesh), ITC – Sunehra Kal Initiative, National Bank for Agriculture and Rural Development (NABARD), Omidyar Network, Royal Bank of Scotland Foundation, Rufford Small Grant Programme, Sir Dorabji Tata Trust and the Allied Trusts (SDTT), Jamsetji Tata Trust (JTT), Sir Ratan Tata Trust (SRTT), Water and Sanitation Management Organisation (WASMO), UNDP-GEF Small Grants Programme (SGP), and GIZ.\n\nFES was a recipient of the Skoll Award for Social Entrepreneurship in 2015, and Jagdeesh Rao was honored during the awards ceremony as part of the Skoll World Forum in Oxford, England on 16 April 2015.\n\nIn 2013, FES was awarded the Times of India Social Impact Award 2012 in the Environment category. It was jointly shared with Dhan Foundation.\n\nOn the World Day to Combat Desertification (17 June) the Foundation for Ecological Security was awarded the United Nations Convention to Combat Desertification (UNCCD) instituted Land for Life Award 2013 for its work on assisting village communities in sustainable management of common lands in India.\n\nThe Foundation for Ecological Security (FES) is also the recipient of the prestigious Elinor Ostrom International Award on Collective Governance of the Commons for the year 2013, for outstanding contribution to the practice of Commons governance.\n\n\n"}
{"id": "4598878", "url": "https://en.wikipedia.org/wiki?curid=4598878", "title": "Guinean Forests of West Africa", "text": "Guinean Forests of West Africa\n\nThe Guinean forests of West Africa is a biodiversity hotspot designated by Conservation International, which includes the belt of tropical moist broadleaf forests along the coast of West Africa, running from Sierra Leone and Guinea in the west to the Sanaga River of Cameroon in the east. The Dahomey Gap, a region of savanna and dry forest in Togo and Benin, divides the Guinean forests into the Upper Guinean forests and Lower Guinean forests.\n\nThe Upper Guinean forests extend from Sierra Leone and Guinea in the west through Liberia, Côte d'Ivoire, and Ghana to Togo in the east. The Lower Guinean forests extend east from Benin through Nigeria and Cameroon. The Lower Guinean forests also extend south past the Sanaga River, the southern boundary of the hotspot, into southern Cameroon, Equatorial Guinea, Gabon, Republic of the Congo, Cabinda, and Democratic Republic of the Congo.\n\nThe World Wide Fund for Nature divides the Upper and Lower Guinean forests into a number of distinct ecoregions:\n\nUpper Guinean forests\n\nLower Guinean forests\n\n"}
{"id": "10977572", "url": "https://en.wikipedia.org/wiki?curid=10977572", "title": "Haradh gas plant", "text": "Haradh gas plant\n\nThe Haradh Gas Plant is one of the major gas plants in Saudi Arabia. It is located near Haradh village, 300 km southwest of Dhahran. The plant has a capacity of producing 1.6 BSCFD of natural gas and 170,000 BBL/day of condensate (oil). The plant processes only non-associated gas. The plant is considered to be a mid-size, when compared to other sister plants in the region. However, the amount of oil processed is considered to be relatively large. \n\nThe plant started operating in April 2003.\n"}
{"id": "240778", "url": "https://en.wikipedia.org/wiki?curid=240778", "title": "Heat lightning", "text": "Heat lightning\n\nHeat lightning, sometimes known as silent lightning, summer lightning, or dry lightning (mainly used in the American south; not to be confused with dry thunderstorms, which are also often called dry lightning), is a misnomer used for the faint flashes of lightning on the horizon or other clouds from distant thunderstorms that do not appear to have accompanying sounds of thunder.\n\nThe actual phenomenon commonly called heat lightning is simply cloud-to-ground lightning that occurs very far away, with thunder that dissipates before it reaches the observer. At night, it is possible to see the flashes of lightning from very far distances, up to 100 miles (160 kilometres), but the sound does not carry that far. In Florida, this type of lightning is often seen over the water at night, the remnants of storms that formed during the day along a sea breeze front coming in from the opposite coast.\n\nHeat lightning is not to be confused with electrically-induced luminosity actually generated at mesospheric altitudes above thunderstorm systems (and likewise visible at exceedingly great ranges), a phenomenon known as \"sprites\".\n\nThe movement of sound in the atmosphere depends on the properties of the air, such as temperature and density. Because temperature and density change with height, the sound of thunder is refracted through the troposphere. This refraction results in spaces through which the thunder does not propagate. The sound of thunder often reflects off the Earth's surface. The rumbling sound is partly due to these reflections. This reflection and refraction leaves voids where thunder cannot be heard.\n\nThe Earth's curvature also contributes to distant observers not hearing the thunderclap. Thunder is more likely to be bounced off the Earth's surface before it reaches an observer far from the strike, and only the right refraction and reflection of the sound off of the atmosphere will give it range it needs to be heard far away. The reflection and refraction in the troposphere determines who hears the strike and who doesn't. Usually the troposphere will reflect the light, and leave out the sound - in these cases some fraction of the light emanating from distant thunderstorms (whose distant clouds may be so low to the horizon as to be essentially invisible) is scattered by the upper atmosphere and thus visible to remote observers.\n\nUnder optimum conditions, the most intense thunderstorms can be seen at up to 100 miles (161 km) over flat terrain or water when the clouds are illuminated by large lightning discharges. However, an upper limit of 30–50 miles (48–80 km) is more common due to topography, trees on the horizon, low to mid level clouds, and the fact that local visibilities are generally no more than 25 miles (40 km). The height of the anvil (the large, plume-like top of a thunderhead) also contributes—45,000 feet (13,715 m) is very common in the mid latitudes for warm season thunderstorms, but the anvil height can range from 35,000 feet (10,665 m) to a current record of 78,000 feet (23,770 m).\n\n"}
{"id": "23256215", "url": "https://en.wikipedia.org/wiki?curid=23256215", "title": "Henryk Jaskuła", "text": "Henryk Jaskuła\n\nHenryk Jaskuła (born 22 October 1923 in Radziszów) is a yachtsman, sailing captain, and electrician. He was the first Pole to perform a single-handed non-stop circumnavigation of the globe. He achieved it on the yacht \"Dar Przemyśla\".\n\nJaskuła became the third man to circumnavigate the globe non-stop and single-handed on 20 May 1980, the day he returned to Gdynia.\n\n\n"}
{"id": "24734441", "url": "https://en.wikipedia.org/wiki?curid=24734441", "title": "Hubert Whittell", "text": "Hubert Whittell\n\nHubert Massey Whittell OBE (24 March 1883 – 7 February 1954) was a British army officer, and later an Australian farmer and ornithologist who compiled a history and bibliography of ornithology in Australia from its origins until the mid-20th century.\n\nWhittell was born at Stratford in Essex, England, now part of Greater London. His father, an engineer and naval architect was the Bombay representative of Lloyd's of London, and Whittell grew up in both India and England, as well as attending school for a year in Germany in 1894. In 1899 he began studying medicine at Edinburgh University. In 1899, Whittell donated to Edinburgh Museum a specimen of an Atlantic puffin (\"Fratercula arctica\") taken in County Mayo, Ireland. His address was then given as 53, Merchiston Crescent, Edinburgh. In August 1901, he made an ornithological expedition to Orkney and collected a specimen of a red-necked phalarope (\"Phalaropus hyperboreus\") from Stromness. However, in 1903 he gave up his medical studies to pursue a career in the British Army.\n\nIn September 1904, after service with the 2nd Battalion the Royal Guernsey Light Infantry (Channel Islands Militia), Whittell passed a competitive examination and in December was gazetted a Second Lieutenant in the 1st Battalion the Royal Sussex Regiment, with which he was posted to India in 1905. In November 1907 he was promoted lieutenant, and transferred to the Indian Army, joining the 56th Punjabi Rifles, being promoted to captain in 1913. He studied Urdu, Pushtu and Persian, collected old Indian coins, went big game hunting and published papers on local history. He married Sydney Margaret O'Hara Hodgkins in 1911 in Bombay. Following the outbreak of World War I he served in France, Belgium and Egypt for the duration of the war, after which he returned to India to serve in the Third Anglo-Afghan War in 1919. He was twice Mentioned in Despatches and was promoted to major. In June 1921, while serving with the Supplies and Transport Corps of the Indian Army, he was appointed an Officer of the Order of the British Empire (Military Division). In 1926 he retired from the Indian Army and emigrated to Australia.\n\nWhittell settled with his family at Bridgetown, Western Australia, 270 km south of Perth, where he bought an orchard and a dairy farm which he managed successfully, becoming a prosperous member of the local farming community. By 1929 he had revived a lifelong interest in birds and was ornithologically active, joining the Royal Australasian Ornithologists Union (RAOU) that year. He started building up a bird egg and skin collection, as well as establishing an ornithological library. He also made several collecting trips in the south-west and south-east of Western Australia.\n\nIn 1939 Whittell became Convener of the RAOU's Checklist Committee, serving also as President of the Union 1941–1943. A keen conservationist, from 1946 he served on the State Fauna Protection Advisory Committee. In 1943 he began a collaboration with Dominic Serventy on a regional handbook, the \"Birds of Western Australia\", the first edition of which was published in 1948. From the mid-1930s he had been working on a comprehensive history and bibliography of Australian ornithology, \"The Literature of Australian Birds\", a monumental 900-page volume published in 1954 not long after his death.\n\nWhittell was described by his friend and collaborator Dom Serventy as follows:\n”Physically he was a slight man, of restless disposition, and his physiognomy, with the keen face and the alert prominent blue-grey eyes, reminded one strikingly of some of the profile portraits of Frederick the Great. Personally he was of the highest integrity and he despised any adventitious aid to advancement, such as joining the 'right' social organisations or currying favour with persons supposed to have influence. He found it difficult to suffer fools gladly and this, combined with a somewhat choleric temperament at times, alienated some of his acquaintances. Others were never quite at their ease in his presence, never knowing whether to take his quips, uttered in a clipped English accent, as real reproaches or humorous chaffing. However, when acquaintance passed to friendship his ripe observations from an intelligent and well-stocked mind, on people, places and things, combined with his essential good nature and fair dealing, made him an agreeable and congenial companion.”\nWhittell was awarded Life Memberships of the St John Ambulance Association, the Returned Sailors' and Soldiers' Imperial League of Australia, and the Western Australian Naturalists' Club. He died in 1954 following long and painful complications from surgery carried out more than a year previously in December 1952. He was survived by his wife, a daughter, two sons and three grandchildren.\n\nAs well as numerous papers in \"The Emu\" and other journals, notes and popular articles, book-length works authored or coauthored by Whittell include:\n\n"}
{"id": "26242611", "url": "https://en.wikipedia.org/wiki?curid=26242611", "title": "Imperial College Faculty of Natural Sciences", "text": "Imperial College Faculty of Natural Sciences\n\nThe Faculty of Natural Sciences is one of the three main faculties of Imperial College London in London, England. It was formed in 2001 from the former Royal College of Science, a constituent college of Imperial College which dated back to 1848, and the faculty largely consists of the original departments of the college. Undergraduate teaching occurs for all departments at the South Kensington campus, with research being split between South Kensington and the new innovation campus at White City.\n\nStudents who study at the departments of the faulty are represented by the Royal College of Science Union, a constituent union of the college union which caters specifically to students on natural science courses. Graduates who obtain an undergraduate degree, either BSc or MSci, from the faculty are awarded the Associateship of the Royal College of Science (ARCS) as an additional degree.\n\nThe origins of the faculty lie in the Royal College of Chemistry, which, after being founded in 1845, moved to a new site in South Kensington in the early 1870s. Incorporated into the Normal School of Science, the college was later renamed the Royal College of Science in 1890, and in 1907 became a constituent college of the newly formed Imperial College of Science and Technology.\n\nIn 2001, Imperial College was restructured to form four new faculties, including the faculties of Physical Sciences and Life Sciences, which took over the role of the Royal College of Science. These faculties were later re-merged over the course of 2005-2006 to form the Faculty of Natural Sciences, which comprises the same departments as the original Royal College of Science.\n\nThe faculty includes five academic departments:\n\n\nImperial College Faculty of Natural Sciences website\n"}
{"id": "15444827", "url": "https://en.wikipedia.org/wiki?curid=15444827", "title": "Indian National Centre for Ocean Information Services", "text": "Indian National Centre for Ocean Information Services\n\nIndian National Center for Ocean Information Services (INCOIS) is an autonomous organization of the Government of India, under the Ministry of Earth Sciences, located in Pragathi Nagar, Hyderabad. ESSO-INCOIS was established as an autonomous body in 1999 under the Ministry of Earth Sciences (MoES) and is a unit of the Earth System Science Organization (ESSO). ESSO- INCOIS is mandated to provide the best possible ocean information and advisory services to society, industry, government agencies and the scientific community through sustained ocean observations and constant improvements through systematic and focussed research.\n\nDuring 90s, the Ministry of Earth Sciences, formerly Department of Ocean Development (DOD), had initiated a project titled \"PFZ Mission\" and it was handed over to National Remote Sensing Centre (NRSC), Hyderabad, AP. The project slowly blossomed into a full-fledged one, Due to the sincere work done by Dr. A. Narendra Nath. As a result of this, the project was separated from NRSC and a new organization was instituted to look after it. The new organization so formed was named as an Indian National Centre for Ocean Information Services (INCOIS) and was placed under reputed scientist Dr A.Narendra Nath, Who was also the founder director for INCOIS. Since its Inception in February 1998. Dr. Narendra Nath is the person who initiated the PFZ Mission.. The newly formed entity had potential fishing zone (PFZ) advisories as its major project. Apart from PFZ services, the other services such as Indian Early Tsunami Warning, Ocean State Forecast, Ocean Modeling, Data and Web Services Management were also initiated and their products are being delivered to various stakeholders in the country on a daily basis. In recognition of these services, INCOIS was identified as one of the key International Oceanic Organizations. INCOIS renders its services through its web portal and various other devices that are installed at different places in the country.\n\nThis is the first advisory service started by INCOIS. The backbone of this service is the real-time data for ocean color and SST provided by the OCEANSAT and NOAA respectively. This service was started because there was a need to identify the potential fishing zones to help the fishermen to get better catch while they were at the sea. This service was started by the Ministry of Earth Sciences with the help of the Department of Space and several institutions under the Ministry of Agriculture. These organizations are collaborating with the State Governments of the beneficiary states to offer these services to the end users.\n\nThis service makes use of parameters such as sea surface temperature and chlorophyll content provided by NOAA-AVHRR and Oceancolor satellites. Features such as oceanic fronts, Meandering Patterns, Eddies, Rings, Up Welling areas etc. are identified sites for fish accumulation. These features can easily be identified from Sea Surface Temperature and Chlorophyll data. The availability of Chlorophyll from OCEANSAT and MOdDIS has further enriched these advisories in the recent years. Hence, PFZ advisories have helped the fishing community to locate the fishing zones with accuracy.\n\nAnother feature of PFZ service is the generation of species-specific advisory to enable the fishermen folk to distinguish between the exploited and under-exploited species in the potential fishing zones. This enables them to have sustainable fishery management by targeting only the under-exploited species in the fishing zones. This approach enables them to avoid fishing the over-exploited species over and over again. One such species-specific advisory is Tuna Fisheries Forecasting System that enables the fishing community to adequately prepare for the Tuna catch. Being a highly migratory fish, it inhabits a wide range of ecosystems and wide area. Hence the fishing of tuna is a costly affair and it requires resource-specific orientation such as long lining by the fishing boats and trawlers. With the help of the parameters such as Oceanic Fronts, water clarity and sea surface temperature we can trace the distribution of tuna species.\n\nIn the aftermath of Sumatra earthquake in 2004 and the killer tsunami it set off, the Government of India wanted to set up an early warning centre for tsunami and other storm surges in the Indian Ocean region. Accordingly, on 15 October 2007, a centre housing the Indian Tsunami Early Warning System (TEWS) was established in INCOIS by the Ministry of Earth Sciences, the nodal ministry, with the collaboration of the Department of Science and Technology (DST), Department of Space (DOS), and Council for Scientific and Industrial Research (CSIR). At the time of its establishment, the centre had the mandate to provide important tsunami advisories to the people living in the coastal areas of the country. For this purpose, the center was equipped with state-of-the-art infrastructure and well trained manpower.\nBy 2012, the centre has started to give a round-the-clock alert and advisory services to all Indian Ocean Rim Countries (IOR).\n\nAt present, the tsunami warning centre receives data from 17 seismic stations of the Indian Meteorological Department (IMD), 10 stations of the Wadia Institute of Himalayan Geology (WIHG) and more than 300 international stations. In addition, it receives data from 17 sea-level tide gauges at intervals of five minutes. These tide gauges have been positioned at Aerial Bay, Chennai, Ennore, Garden Reach, Haldia, Kandla, Karwar, Krishnapatnam, Marmagao, Machilipatnam, Nagapattinam, Paradeep, Port Blair, Vadinar, Visakhapatnam etc. Apart from sea-level tide sensors, the wave-rider buoys have also been installed at various locations. This network of tide gauges and buoys helps the center to validate the arrivals of tsunami waves in the Indian Ocean region. To further supplement the cause, a network of seismographs has also been installed at appropriate positions to forecast the occurrence of tsunami-producing earthquakes. It has also installed three bottom pressure recorders in different regions to supplement its needs.\n\nUsing the data obtained from a network of equipments, models have been run to provide maps depicting inundation scenarios for the entire coastal belt. Based on these scenarios, the potential risk zones are identified to generate advisories for various stakeholders in the region. When there is a disaster, the advisories are generated in intervals telling the location of the storm surges or tsunamis to help the local government authorities to evacuate the people from the regions that are likely to be affected.\n\nBeing a peninsula, India is covered from three sides with water body. Hence there is a need for knowing the state of ocean in advance for carrying out various commercial activities. Prior information about the state of the ocean helps us to plan and execute our activities in the marine environment safely. Moreover, the ocean has an effect on the local climate as well. Thinking about all these factors, a new service was rolled out to forecast the ocean state, which is capable of predicting the surface and sub-surface features of the Indian Ocean in advance. It is called the Indian Ocean Forecasting System (INDOFOS). The forecast is also passed to the stakeholders through Village Information Centers, All India Radio, FM Radio, Digital Display Boards, NGO Websites, and TV channels in the regional languages.\n\nAt present, OSF gives forecasts of wave height and direction, sea surface temperature, sea surface currents, mixed layer depth, and depth of 20 degree isotherm. These forecasts are generated by state-of-the art numerical models. These models are customized to simulate and predict the Indian Ocean features realistically. Different models used for forecasts are WAVEWATCH III, WAM, Mike and Regional Ocean Modeling Systems (ROMS). In the forecast mode, these models are forced with atmospheric products developed by various Meteorological agencies to generate different forecasts. Apart from these services, the OSF center also offers value-added services for the benefit of its end users.\n\nThe generated forecasts fall under four categories. They are global, regional, location-specific and coastal forecasts. They differ in spatial and temporal resolutions extend of validations. For coastal forecast, the models are set up with the concept of \"coarse grid\" with coarse resolution in open ocean region whereas very fine resolution is used for location-specific forecasts. These models are tested for their accuracy and reliability by comparing their output with the data from satellite and in-situ measurements. The validation is mainly done during the extreme conditions and monsoon seasons.\n\nThe main activity of this group in INCOIS is to measure and monitor the surface temperature and salinity of the upper 2000 meters of the ocean. For this purpose, there is a global array of 3000 free-drifting, profiling floats that relay the measurement data to agencies, which in turn make available these data publicly within hours of reception. This will help them to continuously monitor the climate state of the ocean. This system was named as Argo, which emphasizes the strong complementary relationship between the free-drifting floats and Jason altimeter mission.\n\nBasically, Argo builds on the existing upper-ocean thermal networks, extending their spatial and temporal coverage, depth range and accuracy, and improving them through the addition of salinity and velocity measurements. The Argo system helps to study quantitatively the evolving state of the upper ocean and the patterns of its climate variability, including heat, fresh water storage and transport. Apart from these,it also enhances the value of altimeter by sufficiently improving the coverage and resolution of sea surface height variability interpretation and initializes the ocean and coupled forecast models for data assimilation and dynamic model testing.\n\n"}
{"id": "590995", "url": "https://en.wikipedia.org/wiki?curid=590995", "title": "Intermodulation", "text": "Intermodulation\n\nIntermodulation (IM) or intermodulation distortion (IMD) is the amplitude modulation of signals containing two or more different frequencies, caused by nonlinearities or time variance in a system. The intermodulation between frequency components will form additional components at frequencies that are not just at harmonic frequencies (integer multiples) of either, like harmonic distortion, but also at the sum and difference frequencies of the original frequencies and at sums and differences of multiples of those frequencies.\n\nIntermodulation is caused by non-linear behaviour of the signal processing (physical equipment or even algorithms) being used. The theoretical outcome of these non-linearities can be calculated by generating a Volterra series of the characteristic, or more approximately by a Taylor series.\n\nPractically all audio equipment has some non-linearity, so it will exhibit some amount of IMD, which however may be low enough to be imperceptible by humans. Due to the characteristics of the human auditory system, the same percentage of IMD is perceived as more bothersome when compared to the same amount of harmonic distortion.\n\nIntermodulation is also usually undesirable in radio, as it creates unwanted spurious emissions, often in the form of sidebands. For radio transmissions this increases the occupied bandwidth, leading to adjacent channel interference, which can reduce audio clarity or increase spectrum usage.\n\nIMD is only distinct from harmonic distortion in that the stimulus signal is different. The same nonlinear system will produce both total harmonic distortion (with a solitary sine wave input) and IMD (with more complex tones). In music, for instance, IMD is intentionally applied to electric guitars using overdriven amplifiers or effects pedals to produce new tones at \"sub\"harmonics of the tones being played on the instrument. See Power chord#Analysis.\n\nIMD is also distinct from intentional modulation (such as a frequency mixer in superheterodyne receivers) where signals to be modulated are presented to an intentional nonlinear element (multiplied). See non-linear mixers such as mixer diodes and even single-transistor oscillator-mixer circuits. However, while the intermodulation products of the received signal with the local oscillator signal are intended, superheterodyne mixers can, at the same time, also produce unwanted intermodulation effects from strong signals near in frequency to the desired signal that fall within the passband of the receiver.\n\nA linear system cannot produce intermodulation. If the input of a linear time-invariant system is a signal of a single frequency, then the output is a signal of the same frequency; only the amplitude and phase can differ from the input signal.\n\nNon-linear systems generate harmonics in response to sinusoidal input, meaning that if the input of a non-linear system is a signal of a single frequency, formula_1 then the output is a signal which includes a number of integer multiples of the input frequency signal; (i.e. some of formula_2).\n\nIntermodulation occurs when the input to a non-linear system is composed of two or more frequencies. Consider an input signal that contains three frequency components atformula_3, formula_4, and formula_5; which may be expressed as\n\nwhere the formula_7 and formula_8 are the amplitudes and phases of the three components, respectively.\n\nWe obtain our output signal, formula_9, by passing our input through a non-linear function formula_10:\n\nformula_9 will contain the three frequencies of the input signal, formula_3, formula_4, and formula_5 (which are known as the \"fundamental\" frequencies), as well as a number of linear combinations of the fundamental frequencies, each of the form\n\nwhere formula_17, formula_18, and formula_19 are arbitrary integers which can assume positive or negative values. These are the intermodulation products (or IMPs).\n\nIn general, each of these frequency components will have a different amplitude and phase, which depends on the specific non-linear function being used, and also on the amplitudes and phases of the original input components.\n\nMore generally, given an input signal containing an arbitrary number formula_20 of frequency components formula_21, the output signal will contain a number of frequency components, each of which may be described by\n\nwhere the coefficients formula_23 are arbitrary integer values.\n\nThe \"order\" formula_24 of a given intermodulation product is the sum of the absolute values of the coefficients,\n\nFor example, in our original example above, third-order intermodulation products (IMPs) occur where formula_26:\n\n\nIn many radio and audio applications, odd-order IMPs are of most interest, as they fall within the vicinity of the original frequency components, and may therefore interfere with the desired behaviour.\n\nAs explained in a previous section, intermodulation can only occur in non-linear systems. Non-linear systems are generally composed of \"active\" components, meaning that the components must be biased with an external power source which is not the input signal (i.e. the active components must be \"turned on\").\n\nPassive intermodulation (PIM), however, occurs in passive devices (which may include cables, antennas etc.) that are subjected to two or more high power tones. The PIM product is the result of the two (or more) high power tones mixing at device nonlinearities such as junctions of dissimilar metals or metal-oxide junctions, such as loose corroded connectors. The higher the signal amplitudes, the more pronounced the effect of the nonlinearities, and the more prominent the intermodulation that occurs — even though upon initial inspection, the system would appear to be linear and unable to generate intermodulation.\n\nIt is also possible for a single broadband carrier to generate PIM if it passes through a PIM generating surface or defect. These distortions would show up as side lobes in a telecommunication signal and interfere with adjacent channels and impede reception.\n\nPIM can be severe problem in modern communication systems. Paths that share both high power transmit and the receive signal are most susceptible to this kind of interference. Once PIM interference finds its way to receive path, it can not be filtered or separated.\n\nFerromagnetic materials are the most common materials to avoid and include ferrites, nickel, (including nickel plating) and steels (including some stainless steels). These materials exhibit hysteresis when exposed to reversing magnetic fields, resulting in PIM generation.\n\nPIM can also be generated in components with manufacturing or workmanship defects, such as cold or cracked solder joints or poorly made mechanical contacts. If these defects are exposed to high RF currents, PIM can be generated. As a result, RF equipment manufacturers perform factory PIM tests on components, to eliminate PIM caused by these design and manufacturing defects.\n\nPIM can also be inherent in the design of a high power RF component where RF current is forced to narrow channels or restricted.\n\nIn the field, PIM can be caused by components that were damaged in transit to the cell site, installation workmanship issues and by external PIM sources. Some of these include:\n\nIEC 62037 is the international standard for PIM testing and gives specific details as to PIM measurement setups. The standard specifies the use of two +43 dBm (20W) tones for the test signals for PIM testing. This power level has been used by RF equipment manufacturers for more than a decade to establish PASS / FAIL specifications for RF components.\n\nSlew-induced distortion (SID) can produce intermodulation distortion (IMD) when the first signal is slewing (changing voltage) at the limit of the amplifier's power bandwidth product. This induces an effective reduction in gain, partially amplitude-modulating the second signal. If SID only occurs for a portion of the signal, it is called \"transient\" intermodulation distortion.\n\nIntermodulation distortion in audio is usually specified as the root mean square (RMS) value of the various sum-and-difference signals as a percentage of the original signal's RMS voltage, although it may be specified in terms of individual component strengths, in decibels, as is common with RF work. Audio IMD standard tests include SMPTE standard RP120-1994 where two signals (at 60 Hz and 7 kHz, with 4:1 amplitude ratios) are used for the test; many other standards (such as DIN, CCIF) use other frequencies and amplitude ratios. Opinion varies over the ideal ratio of test frequencies (e.g. 3:4, or almost — but not exactly — 3:1 for example).\n\nAfter feeding the equipment under test with low distortion input sinewaves, the output distortion can be measured by using an electronic filter to remove the original frequencies, or spectral analysis may be made using Fourier Transformations in software or a dedicated spectrum analyser, or when determining intermodulation effects in communications equipment, may be made using the receiver under test itself.\n\nIn radio applications, intermodulation may be measured as adjacent channel power ratio. Hard to test are intermodulation signals in the GHz-range generated from passive devices (PIM: passive intermodulation). Manufacturers of these scalar PIM-instruments are Summitek and Rosenberger. The newest developments are PIM-instruments to measure also the distance to the PIM-source. Anritsu offers a radar-based solution with low accuracy and Heuermann offers a frequency converting vector network analyzer solution with high accuracy.\n\n"}
{"id": "825959", "url": "https://en.wikipedia.org/wiki?curid=825959", "title": "International Table Tennis Federation", "text": "International Table Tennis Federation\n\nThe International Table Tennis Federation (ITTF) is the governing body for all national table tennis associations. The role of the ITTF includes overseeing rules and regulations and seeking technological improvement for the sport of table tennis. The ITTF is responsible for the organization of numerous international competitions, including the World Table Tennis Championships that has continued since 1926.\n\nThe ITTF was founded in 1926 by William Henry Lawes of Wymondham, the nine founding members being Austria, Czechoslovakia, Denmark, England, Germany, Hungary, India, Sweden and Wales. The first international tournament was held in January 1926 in Berlin while the first World Table Tennis Championships was held in December 1926 in London.\n\nToward the end of 2000, the ITTF instituted several rules changes aimed at making table tennis more viable as a televised spectator sport. The older 38 mm balls were officially replaced by 40 mm balls. This increased the ball's air resistance and effectively slowed down the game.\n\nOn 29 February 2008, the ITTF announced several rules changes after an ITTF Executive Meeting in Guangzhou, Guangdong, China with regards to a player's eligibility to play for a new association. The new ruling is to encourage associations to develop their own players.\n\nThe headquarters of the ITTF is in Lausanne, Switzerland. The previous president of the ITTF was Adham Sharara from Canada; the current president since 2014 is from Germany.\n\nThe ITTF recognises six continental federations. Each continental federation has a president as its top official and owns its constitution. The following are recognised federations:\n\nThere are currently 226 member associations within the ITTF.\n\nAll member associations of the ITTF attend annual general meeting (AGM). Agendas on changes of the constitution, laws of table tennis, applications for membership etc. are discussed and finalised through votes. Also, the president of ITTF, 8 executive vice-presidents, and 32 or less continental representatives are elected at an AGM, serving for a four-year term. The president, executive vice-presidents, and the chairman of the athletes' commission compose executive committee.\n\nThe executive committee, continental representatives and presidents of the six continental federations or their appointees compose the board of directors (Board). The Board manages the work of the ITTF between AGMs. Several committees, commissions, working groups or panels work under the constitution of ITTF or under the Board.\n\nUnlike the organisations for more popular sports, the ITTF tends to recognise teams from generally unrecognised governing bodies for disputed territory. For example, it currently recognises the Table Tennis Federation of Kosovo even though Kosovo is excluded from most other sports. It recognised the People's Republic of China in 1953 and allowed some basic diplomacy\nwhich lead to an opening for U.S. President Richard Nixon, called \"Ping Pong Diplomacy\", in the early 1970s.\n\nFor ITTF World Title events, a player is eligible to play for his association by registering with the ITTF. If the player chooses to play for a new association, he shall register with the ITTF, through the new association.\n\n\nThe table tennis point system was reduced from a 21 to an 11-point scoring system in 2001. A game shall be won by the player or pair first scoring 11 points unless both players or pairs score 10 points, when the game shall be won by the first player or pair subsequently gaining a lead of 2 points. This was intended to make games more fast-paced and exciting. The ITTF also changed the rules on service to prevent a player from hiding the ball during service, in order to increase the average length of rallies and to reduce the server's advantage. Today, the game changes from time to time mainly to improve on the excitement for television viewers.\n\nIn 2007, ITTF's board of directors in Zagreb decided to implement the VOC-free glue rule at Junior events, starting from 1 January 2008, as a transitional period before the full implementation of the VOC ban on 1 September 2008.\n\nAs of 1 January 2009, all speed glue was to have been banned.\n\nConventions: MT/WT: Men's/Women's Teams; MS/WS: Men's/Women's Singles; MD/WD: Men's/Women's Doubles; XD: Mixed Doubles\nThe ITTF maintains an official World Ranking list based on players' results in tournaments throughout the year.\n\nThe tables below show the current ITTF World Ranking for men and women:\n\n\n"}
{"id": "416808", "url": "https://en.wikipedia.org/wiki?curid=416808", "title": "James Dwight Dana", "text": "James Dwight Dana\n\nJames Dwight Dana FRS FRSE (February 12, 1813 – April 14, 1895) was an American geologist, mineralogist, volcanologist, and zoologist. He made pioneering studies of mountain-building, volcanic activity, and the origin and structure of continents and oceans around the world.\n\nDana was born February 12, 1813, in Utica, New York. His father was merchant James Dana (1780–1860) and his mother was Harriet Dwight (1792–1870). Through his mother he was related to the Dwight New England family of missionaries and educators including uncle Harrison Gray Otis Dwight and first cousin Henry Otis Dwight.\nHe showed an early interest in science, which had been fostered by Fay Edgerton, a teacher in the Utica high school, and in 1830 he entered Yale College in order to study under Benjamin Silliman the elder. \n\nGraduating in 1833, for the next two years he was teacher of mathematics to midshipmen in the Navy, and sailed to the Mediterranean while engaged in his duties. In 1836 and 1837 he was assistant to Professor Silliman in the chemical laboratory at Yale, and then, for four years, acted as mineralogist and geologist of the United States Exploring Expedition, commanded by Captain Charles Wilkes, in the Pacific Ocean. His labors in preparing the reports of his explorations occupied parts of thirteen years after his return to America in 1842. His notebooks from the four years of travel contained fifty sketches, maps, and diagrams, including views of both Mount Shasta and Castle Crags. Dana's sketch of Mount Shasta was engraved in 1849 for publication in the \"American Journal of Science and Arts\" (which Silliman had founded in 1818), along with a lengthy article based on Dana's 1841 geological notes. In the article he described in scientific terms the rocks, minerals, and geology of the Shasta region. As far as is known, his sketch of Mount Shasta became the second view of the mountain ever published.\n\nIn 1844 he again became a resident of New Haven, and married Professor Silliman's daughter, Henrietta Frances Silliman. In 1850, he was appointed as Silliman's successor, as Silliman Professor of Natural History and Geology in Yale College, a position which he held until 1892. In 1846 he became joint editor, and during the later years of his life was chief editor, of the \"American Journal of Science and Arts\", to which he was a constant contributor, principally of articles on geology and mineralogy.\n\nThe 1849 publication of his geology of Mount Shasta was undoubtedly a response to the California gold rush publicity. Dana was the pre-eminent U.S. geologist of his time, and he also was one of the few trained observers anywhere who had first-hand knowledge of the northern California terrain. He had previously written that there was a likelihood that gold was to be found all along the route between the Umpqua River in Oregon and the Sacramento Valley. He was probably deluged with inquiries about the Shasta region, and was forced to publish in more detail some advice to the would-be gold miners.\n\nDana was responsible for developing much of the early knowledge on Hawaiian volcanism. In 1880 and 1881 he led the first geological study of the volcanics of Hawaii island. Dana theorized that the volcanic chain consisted of two volcanic strands, dubbed the \"Loa\" and \"Kea\" trends. The Kea trend included Kīlauea, Mauna Kea, Kohala, Haleakala, and West Maui. The Loa trend includes Lōihi, Mauna Loa, Hualālai, Kahoolawe, Lānai, and West Molokai.\n\nFollowing another expedition by fellow geologist C. E. Dutton in 1884, Dana returned to the island once again and in 1890 he published a manuscript on the island that was the most detailed of its day, and would be the definitive source upon the island's volcanics for decades.\n\nDana died on April 14, 1895.\n\nDana was married to Henrietta Silliman in 1844.\n\nTheir son, Edward Salisbury Dana (1849–1935), was also a distinguished mineralogist.\n\nDana's best known books were his \n\nThe \"Manual of Mineralogy\" by J. D. Dana became a standard college text, and has been continuously revised and updated by a succession of editors including W. E. Ford (13th-14th eds., 1912–1929), Cornelius S. Hurlbut (15th-21st eds., 1941–1999), and beginning with the 22nd by Cornelis Klein. The 23rd edition is now in print under the title \"Manual of Mineral Science (Manual of Mineralogy)\" (2007), revised by Cornelis Klein and Barbara Dutrow.\n\nDana's \"System of Mineralogy\" has also been revised, the 6th edition (1892) being edited by his son Edward Salisbury Dana. A 7th edition was published in 1944, and the 8th edition was published in 1997 under the title \"Dana's New Mineralogy\", edited by R. V. Gaines et al.\n\nBetween 1856 and 1857, Dana published a number of manuscripts in an effort to reconcile scientific findings with the Bible. Among these, he wrote \"Science and the Bible: A Review of \"The Six Days of Creation\" of Prof. Tayler Lewis\" (1856), and \"Creation, Or, The Biblical Cosmogony in the Light of Modern Science\" (1885).\n\nDana was awarded the Copley Medal by the Royal Society in 1877, the Wollaston Medal by the Geological Society of London in 1874 and the Clarke Medal by the Royal Society of New South Wales in 1882. Dana was president of the Geological Society of America in 1890.\n\n\n\n\n\n"}
{"id": "53089666", "url": "https://en.wikipedia.org/wiki?curid=53089666", "title": "Kappa Lupi", "text": "Kappa Lupi\n\nThe Bayer designation κ Lupi (Kappa Lupi) is shared by two star systems in the constellation Lupus:\n"}
{"id": "47515234", "url": "https://en.wikipedia.org/wiki?curid=47515234", "title": "Keratophyre", "text": "Keratophyre\n\nKeratophyre is a volcanic rock of intermediate composition. Although similar to trachyte, keratophyre's plagioclase component is richer in sodium than the plagioclase found in trachyte. Keratophyre forms lava flows and subvolcanic intrusions (dykes and sills). Keratophyre occurs, for example, at Hüttenrode in the Harz Mountains of Germany and in the Berwyn Hills of Wales. Keratophyre tuff of Early Devonian age occurs in Sauerland (Germany).\n\nThe term quartz keratophyre has traditionally been used in the Nordic countries to describe a metamorphosed, felsic extrusive rock, corresponding to rhyolite, dacite, or rhyodacite according to IUGS terminology.\n"}
{"id": "25520630", "url": "https://en.wikipedia.org/wiki?curid=25520630", "title": "List of CIGS companies", "text": "List of CIGS companies\n\nThis list of notable companies manufacturing copper indium gallium selenide solar cells (CIGS) includes a number of companies, some of which have significantly reduced or completely closed down production:\n\n\nFormer companies\n"}
{"id": "20793623", "url": "https://en.wikipedia.org/wiki?curid=20793623", "title": "List of Indian state animals", "text": "List of Indian state animals\n\nIndia, officially the Republic of India is a country in South Asia. It is made up of 29 states and 7 union territories. All Indian states have their own government and theUnion territories come under the jurisdiction of the Central Government. As most of the other countries India too has a national emblem—the Lion Capital of Sarnath.\n\nApart from India's national emblem, each of its States and Union Territories have their own state seals and symbols which include state animals, birds, trees, flowers etc. A list of state animals of India is given below. See Symbols of Indian states and territories for a complete list of all State characters and seals. \n\n\n"}
{"id": "9012992", "url": "https://en.wikipedia.org/wiki?curid=9012992", "title": "List of almond diseases", "text": "List of almond diseases\n\nThis article is a list of diseases of almonds (\"Prunus dulcis\").\n\n"}
{"id": "17002786", "url": "https://en.wikipedia.org/wiki?curid=17002786", "title": "List of earthquakes in the United States", "text": "List of earthquakes in the United States\n\nThe following is a list of notable earthquakes and/or tsunamis which had their epicenter in areas that are now part of the United States with the latter affecting areas of the United States. Those in \"italics\" were not part of the United States when the event occurred.\n\nEarthquake swarms which affected the United States:\n\nEarthquakes which affected the United States but whose epicenters were outside the United States borders:\n\nEarthquakes which did not affect the United States directly, but caused tsunamis which did:\n\n\n"}
{"id": "39012708", "url": "https://en.wikipedia.org/wiki?curid=39012708", "title": "List of large-scale temperature reconstructions of the last 2,000 years", "text": "List of large-scale temperature reconstructions of the last 2,000 years\n\nThis list of large scale temperature reconstructions of the last 2,000 years includes climate reconstructions which have contributed significantly to the modern consensus on the temperature record of the past 2,000 years.\n\nThe instrumental temperature record only covers the last 150 years at a hemispheric or global scale, and reconstructions of earlier periods are based on climate proxies. In an early attempt to show that climate had changed, Hubert Lamb's 1965 paper generalised from temperature records of central England together with historical, botanical and archeological evidence to produce a qualitative estimate of temperatures in the north Atlantic region. Subsequent quantitative reconstructions used statistical techniques with various climate proxies to produce larger scale reconstructions. Tree ring proxies can give an annual resolution of extratropical regions of the northern hemisphere, and can be statistically combined with other sparser proxies to produce multiproxy hemispherical or global reconstructions.\n\nQuantitative reconstructions have consistently shown earlier temperatures below the temperature levels reached in the late 20th century. This pattern as seen in was dubbed the hockey stick graph, and as of 2010 this broad conclusion was supported by more than two dozen reconstructions, using various statistical methods and combinations of proxy records, with variations in how flat the pre-20th century \"shaft\" appears.\n\n\nThe IPCC Third Assessment Report (TAR WG1) of 2001 cited the following reconstructions supporting its conclusion that the 1990s was likely to have been the warmest Northern Hemisphere decade for 1,000 years:\n\n highlighted six recent reconstructions, one of which was not cited in AR4:\n\nThe IPCC Fourth Assessment Report (AR4 WG1) of 2007 cited the following reconstructions in support of its conclusion that the 20th century was likely to have been the warmest in the Northern Hemisphere for at least 1,300 years:\n\nThe IPCC Fifth Assessment Report (AR5 WG1) of 2013 cited the following reconstructions in support of its conclusion that for average annual Northern Hemisphere temperatures, \"the period 1983–2012 was \"very likely\" the warmest 30-year period of the last 800 years (\"high confidence\") and \"likely\" the warmest 30-year period of the last 1400 years (\"medium confidence\")\":\n\n\n1965\n1979\n1989\n1990 \n1993 \n1994\n1995\n1996\n1997\n1998\n1999\n2000\n2001\n\n2002\n2003\n2004\n2005\n2006\n2007\n2008\n2009\n2010\n2011\n2012\n2013\n"}
{"id": "9019208", "url": "https://en.wikipedia.org/wiki?curid=9019208", "title": "List of lettuce diseases", "text": "List of lettuce diseases\n\nThis article is a list of diseases of lettuce (\"Lactuca sativa\").\n\n"}
{"id": "356910", "url": "https://en.wikipedia.org/wiki?curid=356910", "title": "List of mountains on Io", "text": "List of mountains on Io\n\nMore than 135 mountains have been identified on the surface of Jupiter's moon Io. Despite the extensive active volcanism taking place on Io, most mountains on Io are formed through tectonic processes. These structures average 6 km (4 mi) in height and reach a maximum of 17.5 ± 1.5 km (10.9 ± 1 mi) at South Boösaule Montes. Mountains often appear as large (the average mountain is 157 km (98 mi) long), isolated structures with no apparent global tectonic patterns outlined, in contrast to the situation on Earth. To support the tremendous topography observed at these mountains requires rock compositions consisting mostly of silicate, as opposed to sulfur.\n\nMountains on Io (generally, structures rising above the surrounding plains) have a variety of morphologies. Plateaus are most common. These structures resemble large, flat-topped mesas with rugged surfaces. Other mountains appear to be tilted crustal blocks, with a shallow slope from the formerly flat surface and a steep slope consisting of formerly sub-surface materials uplifted by compressive stresses. Both types of mountains often have steep scarps along one or more margins. Only a handful of mountains on Io appear to have a volcanic origin. These mountains resemble small shield volcanoes, with steep slopes (6–7°) near a small, central caldera and shallow slopes along their margins. These volcanic mountains are often smaller than the average mountain on Io, averaging only 1 to 2 km (0.6 to 1.2 mi) in height and 40 to 60 km (25 to 37 mi) wide. Other shield volcanoes with much shallower slopes are inferred from the morphology of several of Io's volcanoes, where thin flows radiate out from a central patera, such as at Ra Patera.\n\nSome of Io's mountains have received official names from the International Astronomical Union. The names are a combination of a name of a person or place derived from the Greek mythological story of Io, Dante's \"Inferno\", or from the name of a nearby feature on Io surface and an approved descriptive term. The descriptive terms, or categories, used for these mountains depends on their morphology, which is a reflection of the mountain's age, geologic origin (volcanic or tectonic), and mass wasting processes. Mountains consisting of massifs, ridges, or isolated peaks use the descriptive term, \"mons\" or the plural \"montes\", the Latin term for mountain. These features are named after prominent locations from the Greek mythological travels of Io or places mentioned in Dante's \"Inferno\". Plateaus are normally given the descriptive term \"mensa\" (pl. \"mensae\"), the Latin term for mesa, though some mountains with plateau morphology use \"mons\". Ionian mensae are named after mythological figures associated with the Io myth, characters from Dante's \"Inferno\". Like mountains, these features can also be named after nearby volcanoes. Some units of layered plains have names using the descriptive term \"planum\" (pl. \"plana\"). However other more mountainous structures, such as Danube Planum, use the term. Partly as a result of the inconsistent use of this term, \"planum\" has not been used since the Voyager era. Ionian plana are named after locations associated with the Io myth. Rare cases of volcanic mountains, such as the shield volcano Tsũi Goab Tholus, use the term \"tholus\" (plural: \"tholi\"). Ionian tholi are named after people associated with the Io myth or nearby features on Io's surface.\n\nSee also the list of volcanic features on Io and the list of regions on Io.\n\nThe following table lists those positive topographic structures (mountains, plateaus, shield volcanoes, and layered plains) that have been given names by the International Astronomical Union. Coordinates and Length come from the USGS website that hosts that nomenclature list. Height information from Paul Schenk's 2001 paper, \"The mountains of Io: Global and geological perspectives from Voyager and Galileo\". When the name refers to multiple mountains, the tallest peak from Schenk \"et al.\" 2001 is listed. Those whose heights come from another sources are noted and sourced in the table. Height ranges result from uncertainties due to different methods used to determine the height of the mountain.\n\n\n"}
{"id": "3157454", "url": "https://en.wikipedia.org/wiki?curid=3157454", "title": "List of plants by common name", "text": "List of plants by common name\n\nThis is a list of plants organized by their common names. However, the common names of plants often vary from region to region, which is why most plant encyclopedias refer to plants using their scientific names, in other words using binomials or \"Latin\" names.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLists of plants\n\n"}
{"id": "53231495", "url": "https://en.wikipedia.org/wiki?curid=53231495", "title": "List of species and habitats of principal importance in England", "text": "List of species and habitats of principal importance in England\n\nThe UK countries of England, Wales, Scotland and Northern Ireland are obliged by their individual laws to maintain lists of species and habitats of principal importance for biodiversity conservation. Public bodies, including local authorities now have a legal duty to have regard to conserving biodiversity in the exercise of their normal functions. In England, this obligation derives from the Natural Environment and Rural Communities (NERC) Act 2006.\n\nThe species that have been designated to be of \"principal importance for the purpose of conserving biodiversity\" are those that are most threatened, in greatest decline, or where the UK holds a significant proportion of the world's total population. They mainly derive from lists originally drawn up for the UK Biodiversity Action Plan (UK BAP). Similarly the list of habitats of principal importance in England also derive from the UK Biodiversity Action Plan. Both lists were reviewed in 2007, and the total number of UK BAP habitats increased from 45 to 65, and the number of UK BAP species increased from under 600 to 1,150.\n\nFrom these, the formal list just for England (and laid out below) now contains 56 of those 65 habitats, and 943 of the original 1,150 UK species.\n\nSection 40 of The Natural Environment and Rural Communities (NERC) Act 2006 places a legal obligation on public bodies in England to have regard to particular living organisms and types of habitat which are of the greatest conservation importance whilst carrying out their functions, whilst also having a general regard for protecting all biodiversity. Section 41 of that same Act of Parliament requires the Secretary of State to maintain and publish statutory lists of these features - a function carried out on his/her behalf by Defra and Natural England. The lists given here are sometimes known as the 'Section 41 lists', or 'priority habitats' and 'priority species' lists. They replace an earlier list which was required under Section 74 of the CRoW Act of 2000, and which was published by Defra two years later, though their contents were at that time identical to the UK BAP priority habitats and species lists.\n\nAwareness of the presence of any principal species or principal habitat identified on these lists is of great importance within the local authority planning process when land is considered for development. Along with legally protected species, statutory and non-statutory sites, knowledge of their presence is required if the impact of future development is to be avoided or mitigated. By fully considering all these features in the decision-making process, the planning authority will have demonstrated that it has discharged its duties to conserve biodiversity.\n\nThe latest update to the list of Section 41 habitats of principal importance (priority habitats) was published by Natural England in August 2010.\n\nThe list shows the broad habitat group, followed by name of the habitat of principal importance (as used by original source).\n\nThe latest update to the Section 41 list of species of principal importance for the conservation of biodiversity in England (priority species) was published by Natural England in May 2014. It now includes 943 species.\n\n\n"}
{"id": "27885903", "url": "https://en.wikipedia.org/wiki?curid=27885903", "title": "List of volcanic settlements", "text": "List of volcanic settlements\n\nSettlements built on volcanoes of recent origin or somewhere on their volcanic complexes or volcanic fields. These volcanoes may be active, dormant or otherwise may not be entirely extinct. Such settlements are subject to potential future volcanic hazards, including but not limited to volcanic eruption, hydrothermal activity or volcanic gases in their vicinity. This category does not include settlements merely \"threatened\" by volcanic activity but which are located at a distance away, and not located on the volcanic structure itself. It does include settlements which have volcano evacuation routes/procedures or warning systems.\n\n\n"}
{"id": "38244316", "url": "https://en.wikipedia.org/wiki?curid=38244316", "title": "Maximum demand indicator", "text": "Maximum demand indicator\n\nA Maximum Demand Indicator (MDI) is an instrument which measures the maximum amount of electrical energy required by a specific consumer during a given period of time.\n\nMDI instruments are designed in such a way that they record the base load requirement of electrical energy. They can also measure the peak load but are unable to record sudden short circuit or High motor Starting Currents.\nIts main construction parts are:\nMaximum demand indicator is often available as a built in feature of three phase energy meters, included in a single case.\n\nMaximum Demand is calculated by\n\nMaximum Demand(KW)=\n"}
{"id": "859234", "url": "https://en.wikipedia.org/wiki?curid=859234", "title": "Mechanical energy", "text": "Mechanical energy\n\nIn physical sciences, mechanical energy is the sum of potential energy and kinetic energy. It is the energy associated with the motion and position of an object. The principle of conservation of mechanical energy states that in an isolated system that is only subject to conservative forces, the mechanical energy is constant. If an object is moved in the opposite direction of a conservative net force, the potential energy will increase and if the speed (not the velocity) of the object is changed, the kinetic energy of the object is changed as well. In all real systems, however, nonconservative forces, like frictional forces, will be present, but often they are of negligible values and the mechanical energy's being constant can therefore be a useful approximation. In elastic collisions, the mechanical energy is conserved but in inelastic collisions, some mechanical energy is converted into heat. The equivalence between lost mechanical energy (dissipation) and an increase in temperature was discovered by James Prescott Joule.\n\nMany devices are used to convert mechanical energy to or from other forms of energy, e.g. an electric motor converts electrical energy to mechanical energy, an electric generator converts mechanical energy into electrical energy and a steam engine converts heat energy to mechanical energy.\n\nEnergy is a scalar quantity and the mechanical energy of a system is the sum of the potential energy which is measured by the position of the parts of the system. The kinetic energy which is also called the energy of motion:\n\nThe potential energy, \"U\", depends on the position of an object subjected to a conservative force. It is defined as the object's ability to do work and is increased as the object is moved in the opposite direction of the direction of the force. If \"F\" represents the conservative force and \"x\" the position, the potential energy of the force between the two positions \"x\" and \"x\" is defined as the negative integral of \"F\" from \"x\" to \"x\":\n\nThe kinetic energy, \"K\", depends on the speed of an object and is the ability of a moving object to do work on other objects when it collides with them. It is defined as one half the product of the object's mass with the square of its speed, and the total kinetic energy of a system of objects is the sum of the kinetic energies of the respective objects:\n\nThe principle of conservation of mechanical energy states that if a body or system is subjected only to conservative forces, the mechanical energy of that body or system remains constant. The difference between a conservative and a non-conservative force is that when a conservative force moves an object from one point to another, the work done by the conservative force is independent of the path. On the contrary, when a non-conservative force acts upon an object, the work done by the non-conservative force is dependent of the path.\n\nAccording to the principle of conservation of mechanical energy, the mechanical energy of an isolated system remains constant in time, as long as the system is free of friction and other non-conservative forces. In any real situation, frictional forces and other non-conservative forces are present, but in many cases their effects on the system are so small that the principle of conservation of mechanical energy can be used as a fair approximation. Though energy cannot be created or destroyed in an isolated system, it can be converted to another form of energy.\n\nIn a mechanical system like a swinging pendulum subjected to the conservative gravitational force where frictional forces like air drag and friction at the pivot are negligible, energy passes back and forth between kinetic and potential energy but never leaves the system. The pendulum reaches greatest kinetic energy and least potential energy when in the vertical position, because it will have the greatest speed and be nearest the Earth at this point. On the other hand, it will have its least kinetic energy and greatest potential energy at the extreme positions of its swing, because it has zero speed and is farthest from Earth at these points. However, when taking the frictional forces into account, the system loses mechanical energy with each swing because of the work done by the pendulum to oppose these non-conservative forces.\n\nThat the loss of mechanical energy in a system always resulted in an increase of the system's temperature has been known for a long time, but it was the amateur physicist James Prescott Joule who first experimentally demonstrated how a certain amount of work done against friction resulted in a definite quantity of heat which should be conceived as the random motions of the particles that comprise matter. This equivalence between mechanical energy and heat is especially important when considering colliding objects. In an elastic collision, mechanical energy is conserved – the sum of the mechanical energies of the colliding objects is the same before and after the collision. After an inelastic collision, however, the mechanical energy of the system will have changed. Usually, the mechanical energy before the collision is greater than the mechanical energy after the collision. In inelastic collisions, some of the mechanical energy of the colliding objects is transformed into kinetic energy of the constituent particles. This increase in kinetic energy of the constituent particles is perceived as an increase in temperature. The collision can be described by saying some of the mechanical energy of the colliding objects has been converted into an equal amount of heat. Thus, the total energy of the system remains unchanged though the mechanical energy of the system has reduced.\n\nA satellite of mass formula_4 at a distance formula_5 from the centre of Earth possesses both kinetic energy, formula_6, (by virtue of its motion) and gravitational potential energy, formula_7, (by virtue of its position within the Earth’s gravitational field; Earth's mass is formula_8).\nHence, mechanical energy formula_9 of a satellite is given by\n\nIf the satellite is in circular orbit, the energy conservation equation can be further simplified into\n\nsince in circular motion, Newton's 2nd Law of motion can be taken to be\n\nToday, many technological devices convert mechanical energy into other forms of energy or vice versa. These devices can be placed in these categories:\n\nThe classification of energy into different types often follows the boundaries of the fields of study in the natural sciences.\n\n\nNotes\nCitations\nBibliography\n"}
{"id": "20549371", "url": "https://en.wikipedia.org/wiki?curid=20549371", "title": "Miles per gallon gasoline equivalent", "text": "Miles per gallon gasoline equivalent\n\nMiles per gallon gasoline equivalent (MPGe or MPG) is a measure of the average distance traveled per unit of energy consumed. MPGe is used by the United States Environmental Protection Agency (EPA) to compare energy consumption of alternative fuel vehicles, plug-in electric vehicles and other advanced technology vehicles with the energy consumption of conventional internal combustion vehicles rated in miles per US gallon.\n\nMPGe does not necessarily represent an equivalency in the operating costs between alternative fuel vehicles and the MPG rating of internal combustion engine vehicles due to the wide variation in costs for the fuel sources regionally since the EPA assumes prices that represents the national averages. Miles per gallon equivalent cost for alternate fuel can be calculated with a simple conversion to the conventional MPG. See conversion to MPG by cost below.\n\nThe MPGe metric was introduced in November 2010 by EPA in the Monroney sticker of the Nissan Leaf electric car and the Chevrolet Volt plug-in hybrid. The ratings are based on EPA's formula, in which of electricity is equivalent to one (US) gallon of gasoline, and the energy consumption of each vehicle during simulating varying driving conditions. All new cars and light-duty trucks sold in the U.S. are required to have this label showing the EPA's estimate of fuel economy of the vehicle.\n\nIn a joint ruling issued in May 2011 the National Highway Traffic Safety Administration (NHTSA) and EPA established the new requirements for a fuel economy and environment label that is mandatory for all new passenger cars and trucks starting with model year 2013. This ruling uses miles per gallon gasoline equivalent for all fuel and advanced technology vehicles available in the U.S. market including plug-in hybrids, electric vehicles, flexible-fuel vehicles, hydrogen fuel cell vehicle, natural gas vehicles, diesel-powered vehicles, and gasoline-powered vehicles. In addition to being displayed on new vehicles, fuel economy ratings are used by the U.S. Department of Energy (DOE) to publish the annual Fuel Economy Guide; the U.S. Department of Transportation (DOT) to administer the Corporate Average Fuel Economy (CAFE) program; and the Internal Revenue Service (IRS) to collect gas guzzler taxes.\n\nFuel economy estimates for window stickers and CAFE standard compliance are different. The EPA MPGe rating shown in the Monroney label is based on the consumption of the on-board energy content stored in the fuel tank or in the vehicle's battery, or any other energy source, and only represents the tank-to-wheel energy consumption. CAFE estimates are based on a well-to-wheel basis and in the case of liquid fuels and electric drive vehicles also account for the energy consumed upstream to produce the fuel or electricity and deliver it to the vehicle. Fuel economy for CAFE purposes include an incentive adjustment for alternative fuel vehicles and plug-in electric vehicles which results in higher MPGe than those estimated for window stickers.\n\nThe Alternative Motor Fuels Act (AMFA) enacted in 1988 provides Corporate Average Fuel Economy (CAFE) incentives for the manufacture of vehicles that use ethanol, methanol, or natural gas fuels, either powered exclusively on these alternative fuels or in conjunction with gasoline or diesel fuel, such flexible-fuel vehicles. In order to provide incentives for the widespread use of these fuels and to promote the production of alternative fuel vehicles, AMFA allows manufacturers producing alternative fuel vehicles to gain CAFE credits by manufacturing these vehicles, which allows them to raise their overall fleet fuel economy levels to comply with the CAFE standards until the established cap level.\n\nBeginning in 1993, manufacturers of qualified alternative fuel vehicles can benefit for their CAFE estimation, by computing the weighted average of the fuel economy of the produced alternative fuel vehicles by dividing the alcohol fuel economy by a factor of 0.15. As an example, a dedicated alternative fuel vehicle that would achieve 15 mpg fuel economy while operating on alcohol would have a CAFE calculated as follows:\n\nFE = (1/0.15)(15) = 100 miles per gallon\n\nFor alternative dual-fuel vehicles, an assumption is made that the vehicles would operate 50% of the time on the alternative fuel and 50% of the time on conventional fuel, resulting in a fuel economy that is based on a harmonic average of alternative fuel and conventional fuel. For example, for an alternative dual-fuel model that achieves 15 miles per gallon operating on an alcohol fuel and 25 mpg on the conventional fuel, the resulting CAFE would be:\n\nFE = 1 / [(0.5/25) + (0.5/100)] = 40 miles per gallon\n\nCalculation of fuel economy for natural gas vehicles is similar. For the purposes of this calculation, the fuel economy is equal to the weighted average of the fuel economy while operating on natural gas and while operating on either gasoline or diesel fuel. AMFA specifies the energy equivalency of 100 cubic feet of natural gas to be equal to 0.823 gallons of gasoline, with the gallon equivalency of natural gas to be considered to have a fuel content, similar to that for alcohol fuels, equal to 0.15 gallons of fuel. For example, under this conversion and gallon equivalency, a dedicated natural gas vehicle that achieves 25 miles per 100 cubic feet of natural gas would have a CAFE value as follows:\n\nFE = (25/100) x (100/0.823)(1/0.15) = 203 miles per gallon\n\nThe Energy Policy Act of 1992 expanded the definition of alternative fuel to include liquefied petroleum gas, hydrogen, liquid fuels derived from coal and biological materials, electricity and any other fuel that the Secretary of Transportation determines to be substantially non-petroleum based and has environmental and energy security benefits. Beginning in 1993, manufacturers of these other alternative fuel automobiles that meet the qualifying requirements can also benefit for special treatment in the calculation of their CAFE.\n\nIn 1994 the U.S. National Institute of Standards and Technology (NIST) introduced gasoline gallon equivalent (GGE) as a metric for fuel economy for natural gas vehicles. NIST defined a gasoline gallon equivalent (GGE) as 5.660 pounds of natural gas, and gasoline liter equivalent (GLE) as 0.678 kilograms of natural gas.\n\nEnergy efficiency for selected electric cars leased in California between 1996–2003:\nDuring the late 1990s and early 2000s several electric cars were produced in limited quantities as a result of the California Air Resources Board (CARB) mandate for more fuel-efficient zero-emissions vehicles. Popular models available in California included the General Motors EV1 and the Toyota RAV4 EV. The US DoE and EPA rating for on board energy efficiency for these electric vehicles was expressed as kilowatt-hour/mile (KWh/mi), the most commonly known metric in science and engineering for measuring energy consumption, and used as the billing unit for energy delivered to consumers by electric utilities.\n\nIn order to address the Corporate Average Fuel Economy (CAFE) regulations mandated by the US Congress in 1975, the U.S. Department of Energy established in July 2000 a methodology for calculating the petroleum-equivalent fuel economy of electric vehicles on a well-to-wheel basis. The methodology considers the upstream efficiency of the processes involved in the two fuel cycles, including efficiency factors for petroleum refining and distribution, as well as the national average efficiency for electricity generation and transmission.\nThe formula also includes a fuel efficiency incentive factor of 1/0.15 to benefit electric vehicles. This reward factor is intended provide an incentive for vehicle manufactures to produce and sell electric vehicles, as a higher equivalent fuel economy for EVs improves the carmaker overall fleet fuel economy levels in complying with the CAFE standards, and Congress anticipated that such an incentive would help accelerate the commercialization of electric vehicles. The incentive factor chosen by DoE for EVs is the same 1/0.15 factor already applied in the regulatory treatment of other types of alternative fuel vehicles. When all factors are considered in DoE's formula, the energy efficiency or equivalent fuel economy of electric vehicles increases from 33,705 Wh/gallon (plug-to-wheel) to 82,049 Wh/gallon (well-to-wheel).\n\nIn April 2007, as part of Draft Competition Guidelines released at the New York Auto Show, MPGe was announced as the main merit metric for the Progressive Insurance Automotive X Prize, a competition developed by the X Prize Foundation for super-efficient vehicles that can achieve at least 100 MPGe. In February 2009, \"Consumer Reports\" announced that, as part of a partnership with the X Prize Foundation, they planned to report MPGe as one of several measures that will help consumers understand and compare vehicle efficiency for alternative fuel vehicles.\n\nAs required by the 2007 Energy Independence and Security Act (EISA), with the introduction of advanced-technology vehicles in the U.S. new information should be incorporated in the Monroney label of new cars and light-duty trucks sold in the country, such as ratings on fuel economy, greenhouse gas emissions, and other air pollutants. The U.S. Environmental Protection Agency and the National Highway Traffic Safety Administration (NHTSA) have conducted a series of studies to determine the best way to redesign this label to provide consumers with simple energy and environmental comparisons across all vehicles types, including battery electric vehicles (BEV), plug-in hybrid electric vehicles (PHEV), and conventional internal combustion engine vehicles powered by gasoline and diesel, in order to help consumers choose more efficient and environmentally friendly vehicles. These changes were proposed to be introduced in new vehicles beginning with model year 2012.\n\nThe EPA rating for on board energy efficiency for electric vehicles before 2010 was expressed as kilowatt-hour per 100 miles (kW-h/100 mi). The window sticker of the 2009 Mini E showed an energy consumption of 33 kW-h/100 mi for city driving and 36 kW-h/100 mi on the highway, technically equivalent to 102 mpg city and 94 mpg highway. The 2009 Tesla Roadster was rated 32 kW-h/100 mi in city and 33 kW-h/100 mi on the highway, equivalent to 105 mpg city and 102 mpg highway.\n\nAs part of the research and redesign process, EPA conducted focus groups where participants were presented with several options to express the consumption of electricity for plug-in electric vehicles. The research showed that participants did not understand the concept of a kilowatt hour as a measure of electric energy use despite the use of this unit in their monthly electric bills. Instead, participants favored a miles per gallon equivalent, MPGe, as the metric to compare with the familiar miles per gallon used for gasoline vehicles. The research also concluded that the kW-h per 100 miles metric was more confusing to focus group participants compared to a miles per kW-h. Based on these results, EPA decided to use the following fuel economy and fuel consumption metrics on the redesigned labels: MPG (city and highway, and combined); MPGe (city and highway, and combined); Gallons per 100 miles; kW-h per 100 miles.\n\nThe proposed design and final content for two options of the new sticker label that would be introduced in 2013 model year cars and trucks were consulted for 60 days with the public in 2010, and both include miles per gallon equivalent and kW-h per 100 miles as the fuel economy metrics for plug-in cars, but in one option MPGe and annual electricity cost are the two most prominent metrics. In November 2010, EPA introduced MPGe as comparison metric on its new sticker for fuel economy for the Nissan Leaf and the Chevrolet Volt.\n\nIn May 2011, the National Highway Traffic Safety Administration (NHTSA) and EPA issued a joint final rule establishing new requirements for a fuel economy and environment label that is mandatory for all new passenger cars and trucks starting with model year 2013. The ruling includes new labels for alternative fuel and alternative propulsion vehicles available in the US market, such as plug-in hybrids, electric vehicles, flexible-fuel vehicles, hydrogen fuel cell vehicle, and natural gas vehicles. The common fuel economy metric adopted to allow the comparison of alternative fuel and advanced technology vehicles with conventional internal combustion engine vehicles is miles per gallon of gasoline equivalent (MPGe). A gallon of gasoline equivalent means the number of kilowatt-hours of electricity, cubic feet of compressed natural gas (CNG), or kilograms of hydrogen that is equal to the energy in a gallon of gasoline.\n\nThe new labels also show for the first time an estimate of how much fuel or electricity it takes to drive , introducing to U.S. consumers with fuel consumption per distance traveled, the metric commonly used in many other countries. EPA explained that the objective is to avoid the traditional miles per gallon metric that can be potentially misleading when consumers compare fuel economy improvements, and known as the \"MPG illusion.\"\n\nAs mentioned above, confusion and misinterpretation is common in the public between the two types of \"fuel efficiency\". Fuel economy measures how far a vehicle will go per amount of fuel (units of MPGe). Fuel consumption is the reciprocal of fuel economy, and measures the fuel used to drive a fixed distance (units of Gal/100 miles or kW-h/100 miles). The unit of Gal/100 miles is accurately described as fuel consumption in some EPA brochures, but this unit appears in the fuel economy section of the Monroney label (which doesn't use the term fuel consumption).\nThe miles per gallon gasoline equivalent is based on the energy content of gasoline. The energy obtainable from burning one US gallon of gasoline is 115,000 BTU, 33.70 kWh, or 121.3 MJ.\n\nTo convert the mile per gallon rating into other units of distance per unit energy used, the mile per gallon value can be multiplied by one of the following factors to obtain other units:\n\nMPGe is determined by converting the vehicle consumption per unit distance, as determined through computer modeling or completion of an actual driving cycle, from its native units into a gasoline energy equivalent. Examples of native units include W·h for electric vehicles, kg-H2 for hydrogen vehicles, gallons for biodiesel or liquefied natural gas vehicles, cubic feet for compressed natural gas vehicles, and pounds for propane or Liquefied petroleum gas vehicles. Special cases for specific alternative fuels are discussed below, but a general formula for MPGe is:\n\nformula_1\n\nFor EPA, this considers the tank-to-wheel for liquids and wall-to-wheel energy consumption for electricity, i.e. it measures the energy for which the owner usually pays. For EVs the energy cost includes the conversions from AC to charge the battery. The EPA MPGe ratings displayed in window stickers do not account for the energy consumption upstream, which includes the energy or fuel required to generate the electricity or to extract and produce the liquid fuel; the energy losses due to power transmission; or the energy consumed for the transportation of the fuel from the well to the station.\n\nBasic values for the energy content of various fuels are given by the defaults used in the Department of Energy GREET (Greenhouse gases, Regulated Emissions, and Energy used in Transportation) model, as follows:\n\nNote: 1 kWh is equivalent to 3,412 BTU\n\nThe energy content of a particular fuel can vary somewhat given its specific chemistry and production method. For example, in the new efficiency ratings that have been developed by the United States Environmental Protection Agency (EPA) for battery electric vehicles (BEVs) and plug-in hybrid electric vehicles (PHEVs) – see below – the energy content of a gallon of gasoline is assumed to be 114,989.12 BTU or 33.7 kWh.\n\nThe miles per gallon equivalent cost of an alternative fuel vehicle can be calculated by a simple formula to directly compare the MPG operating costs (rather than the energy consumption of MPGe) with traditional vehicles since the cost of resources varies substantially from region to region. For reference, the complete equation is:\n\n<math>MPG = Mi/G = \n"}
{"id": "3798805", "url": "https://en.wikipedia.org/wiki?curid=3798805", "title": "Molecular tagging velocimetry", "text": "Molecular tagging velocimetry\n\nMolecular tagging velocimetry (MTV) is a specific form of flow velocimetry, a technique for determining the velocity of currents in fluids such as air and water. In its simplest form, a single \"write\" laser beam is shot once through the sample space. Along its path an optically induced chemical process is initiated, resulting in the creation of a new chemical species or in changing the internal energy state of an existing one, so that the molecules struck by the laser beam can be distinguished from the rest of the fluid. Such molecules are said to be \"tagged.\"\n\nThis line of tagged molecules is now transported by the fluid flow. To obtain velocity information, images at two instances in time are obtained and analyzed (often by correlation of the image intensities) to determine the displacement. If the flow is three-dimensional or turbulent the line will not only be displaced, it will also be deformed.\n\nThere are three optical ways via which these tagged molecules can be visualized: fluorescence, phosphorescence and laser-induced fluorescence (LIF). In all three cases molecules relax to a lower state and their excess energy is released as photons. In fluorescence this energy decay occurs rapidly (within formula_1 s to formula_2 s at atmospheric pressure), thus making \"direct\" fluorescence impractical for tagging. In phosphorescence the decay is slower, because the transition is quantum-mechanically forbidden.\n\nIn some \"writing\" schemes, the tagged molecule ends up in an excited state. If the molecule relaxes through phosphorescence, lasting long enough to see line displacement, this can be used to track the written line and no additional visualisation step is needed. If during tagging the molecule did not reach a phosphorescing state, or relaxed before the molecule was \"read\", a second step is needed. The tagged molecule is then excited using a second laser beam, employing a wavelength such that it specifically excites the tagged molecule. The molecule will fluoresce and this fluorescence is captured by means of a camera. This manner of visualisation is called laser induced fluorescence (LIF).\n\nOptical techniques are frequently used in modern fluid velocimetry but most are opto-mechanical in nature. Opto-mechanical techniques do not rely on photonics alone for flow measurements but require macro-size seeding. The best known and often used examples are particle image velocimetry (PIV) and laser Doppler velocimetry (LDV). Within the field of all-optical techniques we can distinguish analogous techniques but using molecular tracers. In Doppler schemes, light quasi-elastically scatters off molecules and the velocity of the molecules convey a Doppler shift to the frequency of the scattered light. In molecular tagging techniques, like in PIV, velocimetry is based on visualizing the tracer displacements.\n\nMTV techniques have proven to allow measurements of velocities in inhospitable environments, like jet engines, flames, high pressure vessels, where it is difficult for techniques like Pitot, hot-wire velocimetry and PIV to work. The field of MTV is fairly young; the first demonstration of implementation emerged within the 1980s and the number of schemes developed and investigated for use in air is still fairly small. These schemes differ in the molecule that is created, whether seeding the flow with foreign molecules is necessary and what wavelength of light are being used. \n\nThe most thorough fluid mechanics studies have been performed using the RELIEF scheme and the APART scheme. Both techniques can be used in ambient air without the need of additional seeding.\nIn RELIEF, excited oxygen is used as tracer. The method takes advantage of quantum mechanical properties that prohibit relaxation of the molecule, so that the excited oxygen has a relatively long lifetime.\n\nAPART is based on the \"photosynthesis\" of nitric oxide. Since NO is a stable molecule, patterns written with it can, in principle, be followed almost indefinitely.\n\nAnother well-developed and widely documented technique that yields extremely high accuracy is hydroxyl tagging velocimetry (HTV). It is based on photo-dissociation of water vapor followed by visualisation of the resulting OH radical using LIF. HTV has been successfully demonstrated in many test conditions ranging from room air temperature flows to Mach 2 flows within a cavity.\n\n\n"}
{"id": "11999250", "url": "https://en.wikipedia.org/wiki?curid=11999250", "title": "Museum of Natural History, Belgrade", "text": "Museum of Natural History, Belgrade\n\nNatural History Museum (Serbian: Природњачки музеј / \"Prirodnjački muzej\") is one of the oldest specialized national institutions in Serbia. In fact, it is the only museum of this type in Serbia.\nBy the richness and diversity of the exhibited species, as well as the results achieved in the domain of museology and science, this museum is one of the most important in the South-Eastern Europe. It was officially founded in 1895, registered as The Natural History Museum of Serbian Land at that time (Јестаственички музеј српске земље.\nPrior to the foundation of the Museum, in the first half of the 19th century, in the former Principality of Serbia, there was a large number of natural history collections, predominantly kept in the Natural History Cabinet of the Great Lyceum (Great School). The museum was initially located at an Endowment house of Stevče Mihajlovic on the Vracar (central municipality of Belgrade).\n\nJosif Pancic is considered to be the founder of the Natural History Museum who, accompanied by the group of his associates and students, systematically collected, studied and extracted specimens from nature to be exhibited. He was the first president of the Serbian Royal Academy and the Great Lyceum professor. Thoroughly investigating the flora of Serbia during his era, he has discovered and described about 50, even nowadays scientifically valid, plant species (Picea omorica, Ramonda sebica, Eryngium serbicum, Centaurea derventana and others).\n\nThe first Museum exhibition was held in 1904 in Belgrade, in the presence of King Peter I and his officials, with the first guest presentation of the naturalist items abroad being held in the same year, an important contribution to the World Exhibition in Paris. The first manager of the Museum was the academician Petar Pavlovic, a geologist and a lecturer at the Great Lyceum.\n\nNatural history collections, from as early as 1939 up until the present day, have been preserved in a temporary space in the former building of the very First Female Gymnasium in Njegoseva Street, 51, Belgrade. Museum collections house documents regarding the natural history not only of Serbia, but also the neighboring Balkan regions, showing their development, from ancient times till today. Examples of plant species and animals are kept in the museum, flora and fauna which no longer can be found on the terrains of Serbia because, due to human influence, they have disappeared or migrated to other areas. The collections contain several thousand holotypes and unique specimens of minerals, rocks, plant and zoological species. \n\nBack in 1972 the Natural History Museum was associated with the Museum of Forestry and Hunting, with its collections of hunting trophies and hunting weapons.\n\nThe Museum is divided into four sectors: Geology Sector (dealing with mineralogy, petrology and paleonthology), Biology Sector (dealing with botany, zoology and micology), Education and Public Relations Sector and the Joint Service Sector. \n\nThe museum won the award Mihailo Valtrovic for the Museum of the Year 2012, granted by the Museum Society of Serbia.\n\nOn 15 February 2016, by the decree of the President of the Republic of Serbia, the Museum has been awarded the Candlemas medal (Sretenjski orden) of the second degree for special merits in relation to the science, culture and museology, on occasion of 120 years of its foundation. The Museum celebrated a jubilee within the solemn conference held in the Serbian Academy of Science and Arts (SANU – Srpska akademija nauka i umetnosti) in December 2015.\n\nThe Museum of Natural History has been declared a cultural institution of national importance. From its foundation up to the present day the Museum doesn't have its own building nor appropriate showroom.\n\nThe temporary space is occupied by 120 collections, which represent the natural and cultural heritage of these areas. These collections comprise about two million specimens from Serbia, the Balkan Peninsula, but also other parts of the world. Considering the number of items, a few following collections stand out: mineralogical, petrological and collections of fossil and recent molluscs, insects, birds, mammals and General Herbarium of the Balkan Peninsula. All the above mentioned collections have enormous scientific and museological value. Collections are organized in two divisions, as follows:\n\nThe library makes the part of the Natural History Museum and was officially established in 1903. The first great contribution to the Library consisted of a comprehensive professional library of the prof. Ph.D. Lazar Dokic, which is considered to be the basic and initial Library fund. The library today encompasses 25245 titles, 1236 domestic and foreign periodicals (79823 numbers), 1032 geographical and geological maps and 226 manuscripts. The library fund consists of a large number of antique and rare books, first editions, as well as sets of numerous respectable scientific magazines worldwide. Albeit being a rather specialized library, it is available to the public.\n\nPublishing activity of the Museum commenced in 1903. Initially, the former Natural History Museum of the Serbian Land has published concise articles on fauna, flora and gea. The first papers were printed in periodicals such as The Hunter, The Teacher and The Educational Gazette. The Natural History Museum has its own magazine, which first issue have been published in 1948, as the Gazette of the Natural History Museum of Serbian Land (beginning with 1958 as The Gazette of the Natural History Museum, and from 2008 on as The Bulletin of the Natural History Museum in Belgrade). Since 2007, the museum initiated edition: Yearbook of Natural History Museum. The yearbook was designed to disclose data on all activities and events in a calendar year: news, information on curator`s results, collections arrangements, training and workshops, published papers, participation at conferences, media appearances, the work of the library, educational work and the like activities.\n\nThe Museum has no permanent exhibition and various exhibition activities, educational workshops, promotions and conferences are all held in the Gallery at Kalemegdan. Educational service of the Museum collaborates with numerous schools and nurseries of Serbia, adjusting its activities to children of all ages.\n\n"}
{"id": "48658120", "url": "https://en.wikipedia.org/wiki?curid=48658120", "title": "Pact of the catacombs", "text": "Pact of the catacombs\n\nThe Pact of the Catacombs is an agreement signed by 42 bishops of the Catholic Church at a meeting following Mass in the Catacombs of Domitilla near Rome on the evening of 16 November 1965, three weeks before the close of the Second Vatican Council. They pledged to live like the poorest of their parishioners and adopt a lifestyle free of attachment to ordinary possessions. The Pact said they would \"try to live according to the ordinary manner of our people in all that concerns housing, food, means of transport... We renounce forever the appearance and the substance of wealth, especially in clothing ... and symbols made of precious metals.\" More than 500 bishops added their signatures in the next few months. The catacombs were chosen for their association with early Christian martyrs in the centuries when the Church was without worldly power and existed in its simplest form.\n\nLaying the theological foundation for the Pact, Cardinal Giacomo Lercaro, Archbishop of Bologna, in December 1962 addressed the Council at length on the centrality of poverty. He held, according to one summary of his views, that \"the question of the church of the poor ... should be the general and synthesizing subject of the whole Council.\" Hélder Câmara, Archbishop of Olinda e Recife, Brazil, was the moving force behind the Pact itself. Others included the Brazilians Bishop Antônio Batista Fragoso of Crateús and Bishop Jose Maria Pires of Araçuaí; Bishop Manuel Larraín Errázuriz of Talca, Chile; Bishop Marcos Gregorio McGrath of Santiago de Veraguas, Panama; and Bishop Leonidas Proaño of Riobamba, Ecuador. Bishop Charles-Marie Himmer (1902-1994) of Tournai, Belgium, presided at the Mass. The only North American among the first to sign was Bishop Gerard-Marie Coderre of Saint-Jean-de-Quebec.\n\nLuigi Bettazzi, who was Auxiliary Bishop of Bologna under Lercaro when he signed and who became the last survivor of the original signors, said a few bishops created the document and then plans for a signing ceremony \"spread by word of mouth\". He felt the document was forgotten because Pope Paul VI, given the Cold War environment of his papacy, preferred not to be associated with its implicit criticism of capitalism. He has also cited the impact of the upheavals of 1968, which \"frightened everyone and everything closed down\". Thus it failed to put poverty at the center of the Church's mission, except in Latin America where it became associated with liberation theology. The document itself has been lost, but as the fiftieth anniversary of its signing approached the Pact gained increasing notice due to the efforts of theologians and historians, especially in Germany, to draw attention to its significance. The Pontifical Urban University held a conference on its legacy in November 2015. According to Bettazzi, \"The Pact of the Catacombs today is ... Pope Francis\". Cardinal Walter Kasper, who mentioned the pact in his book \"Mercy\" (2014), has said of Pope Francis that \"His program is to a high degree what the Catacomb Pact was\". Francis met with Bettazzi in September 2017 before addressing priests, religious, seminarians and deacons in Bologna, where he began his speech with words reminiscent of the Pact: \"It is a consolation to be with those who carry on the apostolate of the Church; religious men seeking to bear witness against worldliness.\"\n\n"}
{"id": "2221504", "url": "https://en.wikipedia.org/wiki?curid=2221504", "title": "Steve Gantlett", "text": "Steve Gantlett\n\nStephen J. M. Gantlett is a British birder. He lives with his wife Sue in Cley-next-the-Sea, Norfolk. A qualified optician, SJMG retired from this profession in 1982 to become a full-time ornithologist. \n\nHe was the editor of Birding World magazine until it ceased production in January 2014. He also co-runs Birdline, a telephone information service containing news of rare birds, with Richard Millington. \n\nSteve Gantlett is a twitcher. He is one of a small number of birders who have seen over 500 species in Britain. \n\nHe is a former member of the British Birds Rarities Committee. \n\nGantlett's specialism as an ornithologist is the advancement of bird identification. Among the areas of knowledge in which Gantlett has played a key role are the identification of orange-billed terns. He also found the UK's first Rock Sparrow at Cley on 14 June 1981 (with Richard Millington), still the only record in the UK. He has published papers on this subject in Birding World and in British Birds.\n"}
{"id": "23984205", "url": "https://en.wikipedia.org/wiki?curid=23984205", "title": "Strangeness production", "text": "Strangeness production\n\nStrangeness production is a signature and a diagnostic tool of quark–gluon plasma (or QGP) formation and properties. Unlike up and down quarks, from which everyday matter is made, strange quarks are formed in pair-production processes in collisions between constituents of the plasma. The dominant mechanism of production involves gluons only present when matter has become a quark–gluon plasma. When quark–gluon plasma disassembles into hadrons in a breakup process, the high availability of strange antiquarks helps to produce antimatter containing multiple strange quarks, which is otherwise rarely made. Similar considerations are at present made for the heavier charm flavor, which is made at the beginning of the collision process in the first interactions and is only abundant in the high-energy environments of CERN's Large Hadron Collider.\n\nThe majority of ordinary matter in the universe is found in atomic nuclei, which are made of neutrons and protons. These neutrons and protons are made up of smaller particles called quarks. For every type of matter particle there is a corresponding antiparticle with the same mass and the opposite charge. It is hypothesized that during the first few instants of the universe, it was composed of almost equal amounts of matter and antimatter, and thus contained nearly equal number of quarks and antiquarks. Once the universe expanded and cooled to a critical temperature of approximately , quarks combined into normal matter and antimatter. Antimatter annihilated with matter up to the small initial asymmetry of about one part in five billion, leaving the matter around us. Free and separate individual quarks and antiquarks have never been observed in experiments—quarks and antiquarks are always found in groups of three (baryons), or bound in quark–antiquark pairs (mesons).\n\nFree quarks probably existed in the extreme conditions of the very early universe until about 30 microseconds after the Big Bang, in a very hot gas of free quarks, antiquarks and gluons. This gas is called a \"quark–gluon plasma\" (QGP), since the quark-interaction charge (color charge) is mobile and quarks and gluons move around. This is possible because at a high temperature the early universe is in a different vacuum state, in which normal matter cannot exist but quarks and gluons can, they are deconfined (able to exist independently as separate unbound particles). In order to recreate this deconfined phase of matter in the laboratory it is necessary to exceed a minimum temperature, or its equivalent, a minimum energy density. Scientists achieve this using particle collisions at extremely high speeds, where the energy released in the collision can raise the subatomic particles' energies to an exceedingly high level, sufficient for them to briefly form a tiny amount of quark-gluon plasma which can be studied for a brief fraction of a second before it cools again. In this way, it is possible to study conditions akin to those in the early Universe at the age of 10–40 microseconds. Discovery of this new QGP state of matter has been announced both at CERN and at Brookhaven National Laboratory (BNL). At this time comprehensive experimental evidence about its properties is being assembled.\n\nThe process of the formation of quark–gluon plasma lasts little longer than the time the light takes to pass through the volume occupied by the atomic nucleus used to produce the ultra-high pressure and temperature in the highly-energetic collision. After this brief time the hot drop of quark plasma evaporates in a process called hadronization. The extremely limited duration of the plasma makes it challenging (difficult) to study free quarks in quark–gluon plasma.\n\nThe diagnosis and the study of the properties of quark–gluon plasma can be undertaken using quarks not present in matter seen around us. The experimental and theoretical work relies on the idea of strangeness enhancement. This was the first observable of quark–gluon plasma proposed in 1980 by Johann Rafelski and Rolf Hagedorn. Unlike the up and down quarks, strange quarks are not brought into the reaction by the colliding nuclei. Therefore, any strange quarks or antiquarks observed in experiments have been \"freshly\" made from the kinetic energy of colliding nuclei. Conveniently, the mass of strange quarks and antiquarks is equivalent to the temperature or energy at which protons, neutrons and other hadrons dissolve into quarks. This means that the abundance of strange quarks is sensitive to the conditions, structure and dynamics of the deconfined matter phase, and if their number is large it can be assumed that deconfinement conditions were reached.\n\nOne cannot assume that under all conditions the yield of strange quarks is in thermal equilibrium. In general, the quark-flavor composition of the plasma varies during its ultra short lifetime as new flavors of quarks such as strangeness are cooked up inside. The up and down quarks from which normal matter is made are easily produced as quark-antiquark pairs in the hot fireball because they have small masses. On the other hand the next lightest quark flavor, strange quarks, will reach its high quark–gluon plasma thermal abundance only on the most violent collisions generating high temperatures and that at the end of the cooking process.\n\nThis is only possible due to a new process, the gluon fusion, as shown by Rafelski and Müller in 1981. The top section of the figure shows gluon fusion in form of Feynman diagrams: gluons are the wavy lines; strange quarks are the solid lines; time runs from left to right. The bottom section is the process where the heavier quark pair arises from the lighter pair of quarks shown as dashed lines. The gluon fusion process occurs almost ten times faster than the quark based strangeness process, and allows achievement of the high thermal yield where the quark based process would fail to do so during the duration of the \"micro-bang\". The gluon collisions here are occurring within the thermal matter phase and are thus different from the high energy processes that can ensue in the early stages of the collisions when the nuclei crash into each other. The heavier, charm and bottom quarks are produced there dominantly. The study in relativistic nuclear (heavy ion) collisions of charmed and soon also bottom hadronic particle production beside strangeness will provide complementary and important confirmation of the mechanisms of formation, evolution and hadronization of quark gluon plasma.\n\nThese newly cooked strange quarks find their way into a multitude of different final particles that emerge as the hot quark–gluon plasma fireball breaks up, see the scheme of different processes in figure. Given the ready supply of antiquarks in the \"fireball\", one also finds a multitude of antimatter particles containing more than one strange quark. On the other hand, in a system involving a cascade of nucleon-nucleon collisions, multi-strange antimatter are produced less frequently considering that several relatively improbable events must occur in the same collision process. For this reason one expects that the yield of multi-strange antimatter particles produced in the presence of quark matter is enhanced compared to conventional series of reactions.\n\nStrange quarks also bind with the heavier charm and bottom quarks which also like to bind with each other. Thus in presence of a large number of these quarks quite unusually abundant exotic particles can be produced, some of these have never been observed before. This should be the case in the forthcoming exploration at the new Large Hadron Collider at CERN of the particles that have both charm and strange quarks, and even bottom quarks as components.\n\nStrange quarks are naturally radioactive and decay by weak interactions into lighter quarks on a timescale that is extremely long compared with the nuclear-collision times. This makes it relatively easy to detect strange particles through the tracks left by their decay products. Consider as example the decay of a negatively charged Xi baryon (green in figure, dss), into a negative pion (d) and a neutral Lambda baryon (uds). Subsequently, the Lambda decays into a proton and another negative pion. In general this is the signature of the decay of a Xi. Although the negative Omega baryon (sss) has a similar final state decay topology, it can be clearly distinguished from the Xi because its decay products are different.\n\nMeasurement of abundant formation of Xi (uss/dss), Omega (sss) and especially their antiparticles is an important cornerstone of the claim that quark–gluon plasma has been formed. This abundant formation is often presented in comparison with the scaled expectation from normal proton-proton collisions; however, such a comparison is not a necessary step in view of the large absolute yields which defy conventional model expectations. The overall yield of strangeness is also larger than expected if the new form of matter has been achieved. However, considering that the light quarks are also produced in gluon fusion processes, one expects increased production of all hadrons. The study of the relative yields of strange and non strange particles provides information about the competition of these processes and thus the reaction mechanism of particle production.\n\nThe first strangeness signature of a possible quark-gluon formation was presented in May 1990 by the CERN-NA35 experimental collaboration at the Quark Matter meeting in Menton, France These results on Antilambda formation in S-S reaction shown in the figure indicate a significant enhancement of the production of this antimatter particle comprising one antistrange quark as well as antiup and antidown quarks. All three constituents of the Lambda particle are newly produced in the reaction. The expected no-quark–gluon plasma production yield is shown at the bottom of the figure. These results are presented as function of the variable called rapidity which characterizes the speed of the source. The peak of emission indicates that the additionally formed antimatter particles do not originate from the colliding nuclei themselves, but from a source that moves at a speed corresponding to one-half of the rapidity of the incident nucleus that is a common center of momentum frame of reference source formed when both nuclei collide, that is the hot quark–gluon plasma fireball.\n\nThe work of Koch, Muller, Rafelski predicts that in a quark–gluon plasma hadronization process the enhancement for each particle species increases with the strangeness content of the particle. The enhancements for particles carrying one, two and three strange or antistrange quarks were measured and this effect was demonstrated by the CERN WA97 experiment in time for the CERN announcement in 2000 of a possible quark–gluon plasma formation in its experiments. These results were elaborated by the successor collaboration NA57 as shown in figure. The gradual rise of the enhancement as a function of the variable representing the amount of nuclear matter participating in the collisions, and thus as function of the geometric centrality of nuclear collision strongly favors the quark–gluon plasma source over normal matter reactions.\n\nA very similar enhancement was obtained by the STAR experiment at the RHIC. Here results obtained when two colliding systems at 100 A GeV in each beam are considered: in red the heavier Gold-Gold collisions and in blue the smaller Copper-Copper collisions. The energy at RHIC is 11 times greater in the CM frame of reference compared to the earlier CERN work. The important result is that enhancement observed by STAR also increases with the number of participating nucleons. We further note that for the most peripheral events at the smallest number of participants, copper and gold systems show at the same number of participants the same enhancement as could be expected.\n\nAnother remarkable feature of these results comparing CERN and STAR is that the enhancement is of similar magnitude for the vastly different energy available in the reaction. This near energy independence of the enhancement also agrees with the quark–gluon plasma approach regarding the mechanism of production of these particles and confirms that a quark–gluon plasma is created over a wide range of collision energies, very probably once a minimal energy threshold is exceeded.\n\nOne of most interesting questions is if there is a threshold in reaction energy and/or volume size which needs to be exceeded in order to form a domain in which quarks can move freely. It is natural to expect that if such a threshold exists the particle yields/ratios we have shown above should indicate that. One of the most accessible signatures would be the relative Kaon yield ratio. A possible structure has been predicted, and indeed, an unexpected structure is seen in the ratio of particles comprising the positive kaon K (comprising anti s-quarks and up-quark) and positive pion particles, seen in the figure (solid symbols). The rise and fall (square symbols) of the ratio has been reported by the CERN NA49. The reason the negative kaon particles do not show this \"horn\" feature is that the s-quarks prefer to hadronize bound in the Lambda particle, where the counterpart structure is observed. The first exploratory data point from BNL-RHIC-STAR (red star) in figure agrees with the CERN data.\n\nIn view of these results the objective of ongoing NA61/SHINE experiment at CERN SPS and the proposed low energy run at BNL RHIC where in particular the STAR detector can search for the onset of production of quark–gluon plasma as function of energy in the domain where the horn maximum is seen, in order to improve the understanding of these results, and to record the behavior of other related quark–gluon plasma observables.\n\nAt the much higher energy of the Large Hadron Collider (LHC) the production of strangeness in quark gluon plasma saturates leading to quark-level chemical equilibrium yield. As the fireball of matter expands and breaks apart, this, in turn, provides a very high abundance of strange hadrons. Among these, heavy mesons composed of strange quarks and a heavy quark, such as bottom () or charm (), are of particular interest. The coincident high yield of charm and strangeness present at LHC will lead to copious production of . Other heavy flavor particles, some which have not even been discovered at this time are also likely to appear. In that way, the Strangeness has turned today more generally into the Quark Flavor signature of quark–gluon plasma.\n\nThe strangeness production and its diagnostic potential as signature of quark–gluon plasma has been discussed for nearly 30 years. The work in this field today focuses on the theoretical interpretation of the overall particle production data and the derivation of the resulting properties of the bulk of quark–gluon plasma at the time of breakup. The global description of all produced particles can be attempted based on the picture of hadronizing hot drop of quark–gluon plasma or, alternatively, on the picture of confined and equilibrated hadron matter. In both cases one describes the data within the statistical thermal production model, but considerable differences in detail differentiate the nature of the source of these particles. The experimental groups working in the field also like to develop their own analysis models and the outside observer sees many different analysis results. For this reason the presentation of the experimental results was made above without a comparison of data to model, so that the results following the pattern predicted can speak for themselves. There are as many as 10 different particles species which follow the pattern predicted for the QGP as function of reaction energy, reaction centrality, and strangeness content. At yet higher LHC energies saturation of strangeness yield and binding to heavy flavor open new experimental opportunities.\n\n"}
{"id": "50561501", "url": "https://en.wikipedia.org/wiki?curid=50561501", "title": "Transparent wood composites", "text": "Transparent wood composites\n\nTransparent wood composites are novel wood materials which have up to 90% transparency and higher mechanical properties than wood itself, made for the first time in 1992.\n\nWhen these materials are commercially available, a significant benefit is expected due to their inherent biodegradable properties since it is wood. These materials are significantly more biodegradable than glass and plastics.. Transparent wood it is also shatterproof. On the other hand, concerns may be relevant due to the use of non-biodegradable plastics for long lasting purpose, such as in building.\n\nA research group led by Professor Lars Berglund from Swedish KTH University along with a University of Maryland research group led by Professor Liangbing Hu have developed a method to remove the color and some chemicals from small blocks of wood, followed by adding polymers, such as Poly(methyl methacrylate) and epoxy, at the cellular level, thereby rendering them transparent.\n\nAs soon as released in between 2015 and 2016, see-through wood had a large press reaction, with articles in ScienceDaily, Wired, the Wall Street Journal, the New York Times, to name a few.\n\nActually those research groups rediscovered a work from Siegfried Fink, a German Researcher, from as early as 1992: with a process very similar to Berglund's and Hu's, the German Researcher turned wood transparent to reveal specific cavities of the wood structure for analytical purpose.\n\nTransparent wood was made based on the idea of removing light-absorbing components (mainly lignin) followed by infiltration of a polymer with a refractive index matching the wood substrate .\n\nOne example is a three step process:\n\nThe first step consists of immersing the 4 or 5 inch block of wood in a solution of water, sodium hydroxide, and sodium sulfite at boiling temperature for two hours. This enables the lignin in the cell walls to be leached out. \"Lignins are particularly important in the formation of cell walls, especially in wood and bark, because they lend rigidity and do not rot easily\".\n\nThe second step of oxidation with hydrogen peroxide completes the leaching of the lignin.\n\nThe third step consists of immersing the material in epoxy and putting it under alternating vacuum and atmospheric pressure; this fills the wood's natural but now disused nutrient and hydrating microscopic channels. The epoxy-filled microscopic channels create a material that has transparent refractive properties.\n\nThe new material is rated stronger than plastic, but is only in the laboratory and experimental stage, and not yet ready for commercial use.\n\nThe length of the process is determined by the size and species of the wood.\n\nTransparent wood could transform architecture because we could make transparent wood structures, or load-bearing windows, and much more. All the future buildings would be much more energy efficient because as it is known that wood is much better isolation material than glass.\n\nTransparent wood could also be solution for making much more durable smartphones and other electronic devices that won't break as easily when dropped on the ground. \n\n"}
{"id": "263970", "url": "https://en.wikipedia.org/wiki?curid=263970", "title": "Volcanic cone", "text": "Volcanic cone\n\nVolcanic cones are among the simplest volcanic landforms. They are built by ejecta from a volcanic vent, piling up around the vent in the shape of a cone with a central crater. Volcanic cones are of different types, depending upon the nature and size of the fragments ejected during the eruption. Types of volcanic cones include stratocones, spatter cones, tuff cones, and cinder cones.\n\nStratocones are large cone-shaped volcanoes made up of lava flows, explosively erupted pyroclastic rocks, and igneous intrusives that are typically centered around a cylindrical vent. Unlike shield volcanoes, they are characterized by a steep profile and periodic, often alternating, explosive eruptions and effusive eruptions. Some have collapsed craters called calderas. The central core of a stratocone is commonly dominated by a central core of intrusive rocks that range from around to over several kilometers in diameter. This central core is surrounded by multiple generations of lava flows, many of which are brecciated, and a wide range of pyroclastic rocks and reworked volcanic debris. The typical stratocone is an andesitic to dacitic volcano that is associated with subduction zones. They are also known as either \"stratified volcano,\" \"composite cone,\" \"bedded volcano,\" \"cone of mixed type\" or \"Vesuvian-type volcano.\"\n\nA spatter cone is a low, steep-sided hill or mound that consists of welded lava fragments, called \"spatter,\" which has formed around a lava fountain issuing from a central vent. Typically, spatter cones are about high. In case of a linear fissure, lava fountaining will create broad embankments of spatter, called \"spatter ramparts,\" along both sides of the fissure. Spatter cones are more circular and cone shaped, while spatter ramparts are linear wall-like features.\n\nSpatter cones and spatter ramparts are typically formed by lava fountaining associated with mafic, highly fluid lavas, such as those erupted in the Hawaiian Islands. As blobs of molten lava, \"spatter,\" are erupted into the air by a lava fountain, they can lack the time needed to cool completely before hitting the ground. Consequently, the spatter are not fully solid, like taffy, as they land and they bind to the underlying spatter as both often slowly ooze down the side of the cone. As a result, the spatter builds up a cone that is composed of spatter either agglutinated or welded to each other.\n\nA tuff cone, sometimes called an ash cone, is a small monogenetic volcanic cone produced by phreatic (hydrovolcanic) explosions directly associated with magma brought to the surface through a conduit from a deep-seated magma reservoir. They are characterized by high rims that have a maximum relief of above the crater floor and steep slopes that are greater than 25 degrees. They typically have a rim to rim diameter of . A tuff cone consists typically of thick-bedded pyroclastic flow and surge deposits created by eruption-fed density currents and bomb-scoria beds derived from fallout from its eruption column. The tuffs composing a tuff cone have commonly been altered, palagonitized, by either its interaction with groundwater or when it was deposited warm and wet. The pyroclastic deposits of tuff cones differ from the pyroclastic deposits of spatter cones by their lack or paucity of lava spatter, smaller grain-size, and excellent bedding. Typically, but not always, tuff cones lack associated lava flows.\n\nA tuff ring is a related type of small monogenetic volcano that is also produced by phreatic (hydrovolcanic) explosions directly associated with magma brought to the surface through a conduit from a deep-seated magma reservoir. . They are characterized by rims that have a low, broad topographic profiles and gentle topographic slopes that are 25 degrees or less. The maximum thickness of the pyroclastic debris comprising the rim of a typical tuff ring is generally thin, less than to thick. The pyroclastic materials that comprise their rim consist primarily of relatively fresh and unaltered, distinctly and thin-bedded volcanic surge and air fall deposits. Their rims also can contain variable amounts of local country rock (bedrock) blasted out of their crater. In contrast to tuff cones, the crater of a tuff ring generally has been excavated below the existing ground surface. As a result, water commonly fills a tuff ring's crater to form a lake once eruptions cease.\n\nBoth tuff cones and their associated tuff rings were created by explosive eruptions from a vent where the magma is interacting with either groundwater or a shallow body of water as found within a lake or sea. The interaction between the magma, expanding steam, and volcanic gases resulted in the production and ejection of fine-grained pyroclastic debris called \"ash\" with the consistency of flour. The volcanic ash comprising a tuff cone accumulated either as fallout from eruption columns, from low-density volcanic surges and pyroclastic flows, or combination of these. Tuff cones are typically associated with volcanic eruptions within shallow bodies of water and tuff rings are associated with eruptions within either water saturated sediments and bedrock or permafrost\n\nNext to spatter (scoria) cones, tuff cones and their associated tuff rings are among the most common types of volcanoes on Earth. An example of a tuff cone is Diamond Head at Waikīkī in Hawaii. Clusters of pitted cones observed in the Nephentes/Amenthes region of Mars at the southern margin of the ancient Utopia impact basin are currently interpreted as being tuff cones and rings.\n\nCinder cones, also known as scoria cones and less commonly scoria mounds, are small, steep-sided volcanic cones built of loose pyroclastic fragments, such as either volcanic clinkers, cinders, volcanic ash, or scoria. They consist of loose pyroclastic debris formed by explosive eruptions or lava fountains from a single, typically cylindrical, vent. As the gas-charged lava is blown violently into the air, it breaks into small fragments that solidify and fall as either cinders, clinkers, or scoria around the vent to form a cone that often is beautifully symmetrical; with slopes between 30 and 40°; and a nearly circular ground plan. Most cinder cones have a bowl-shaped crater at the summit. The basal diameters of cinder cones average about and range from . The diameter of their craters ranges between . Cinder cones rarely rise more than or so above their surroundings.\n\nCinder cones most commonly occur as isolated cones in large basaltic volcanic fields. They also occur in nested clusters in association with complex tuff ring and maar complexes. Finally, they are also common as parasitic and monogenetic cones on complex shield and stratovolcanoes. Globally, cinder cones are the most typical volcanic landform found within continental intraplate volcanic fields and also occur in some subduction zone settings as well. Parícutin, the Mexican cinder cone which was born in a cornfield on February 20, 1943, and Sunset Crater in Northern Arizona in the US Southwest are classic examples of cinder cones, as are ancient volcanic cones found in New Mexico's Petroglyph National Monument. Cone-shaped hills observed in satellite imagery of the calderas and volcanic cones of Ulysses Patera, Ulysses Colles and Hydraotes Chaos are argued to be cinder cones.\n\nCinder cones typically only erupt once like Paricutin. As a result, they are considered to be monogenetic volcanoes and most of them form monogenetic volcanic fields. Cinder cones are typically active for very brief periods of time before becoming inactive. Their eruptions range in duration from a few days to a few years. Of observed cinder cone eruptions, 50% have lasted for less than 30 days, and 95% stopped within one year. In case of Paricutin in Mexico, its eruption lasted for nine years from 1943 to 1952. Rarely do they erupt either two, three, or more times. Later eruptions typically produce new cones within a volcanic field at separation distances of a few kilometers and separate by periods of 100 to 1,000 years. Within a volcanic field, eruptions can occur over a period of a million years. Once eruptions cease, being unconsolidated, cinder cones tend to erode rapidly unless further eruptions occur.\n\nRootless cones, also called pseudocraters, are volcanic cones that are not directly associated with a conduit that brought magma to the surface from a deep-seated magma reservoir. Generally, three types of rootless cones, littoral cones, explosion craters, and hornitos are recognized. Littoral cones and explosion craters are the result of mild explosions that were generated locally by the interaction of either hot lava or pyroclastic flows with water. Littoral cones typically form on the surface of a basaltic lava flow where it has entered into a body of water, usually a sea or ocean. Explosion craters form where either hot lava or pyroclastic flows have covered either marshy ground or water-saturated ground of some sort. Hornitos are rootless cones that are composed welded lava fragments and were formed on the surface of basaltic lava flows by the escape of gas and clots of molten lava through cracks or other openings in the crust of a lava flow.\n"}
{"id": "34268", "url": "https://en.wikipedia.org/wiki?curid=34268", "title": "Yggdrasil", "text": "Yggdrasil\n\nYggdrasil (; from Old Norse , pronounced ) is an immense mythical tree that connects the nine worlds in Norse cosmology.\n\nYggdrasil is attested in the \"Poetic Edda\", compiled in the 13th century from earlier traditional sources, and the \"Prose Edda\", written in the 13th century by Snorri Sturluson. In both sources, Yggdrasil is an immense ash tree that is center to the cosmos and considered very holy. The gods go to Yggdrasil daily to assemble at their things, traditional governing assemblies. The branches of Yggdrasil extend far into the heavens, and the tree is supported by three roots that extend far away into other locations; one to the well Urðarbrunnr in the heavens, one to the spring Hvergelmir, and another to the well Mímisbrunnr. Creatures live within Yggdrasil, including the dragon Níðhöggr, an unnamed eagle, and the stags Dáinn, Dvalinn, Duneyrr and Duraþrór.\n\nConflicting scholarly theories have been proposed about the etymology of the name \"\", the possibility that the tree is of another species than ash, its connection to the many sacred trees and groves in Germanic paganism and mythology, and the fate of Yggdrasil during the events of Ragnarök.\n\nThe generally accepted meaning of Old Norse ' is \"Odin's horse\", meaning \"gallows\". This interpretation comes about because ' means \"horse\" and ' is one of Odin's many names. The \"Poetic Edda\" poem ' describes how Odin sacrificed himself by hanging from a tree, making this tree Odin's gallows. This tree may have been Yggdrasil. Gallows can be called \"the horse of the hanged\" and therefore Odin's gallows may have developed into the expression \"Odin's horse\", which then became the name of the tree.\n\nNevertheless, scholarly opinions regarding the precise meaning of the name ' vary, particularly on the issue of whether ' is the name of the tree itself or if only the full term ' (where Old Norse ' means \"ash tree\") refers specifically to the tree. According to this interpretation, \"\" would mean the world tree upon which \"the horse [Odin's horse] of the highest god [Odin] is bound\". Both of these etymologies rely on a presumed but unattested \"*Yggsdrasill\".\n\nA third interpretation, presented by F. Detter, is that the name ' refers to the word ' (\"terror\"), yet not in reference to the Odinic name, and so ' would then mean \"tree of terror, gallows\". F. R. Schröder has proposed a fourth etymology according to which ' means \"yew pillar\", deriving ' from \"*igwja\" (meaning \"yew-tree\"), and ' from \"*dher-\" (meaning \"support\").\n\nIn the \"Poetic Edda\", the tree is mentioned in the three poems \"Völuspá\", \"Hávamál\" and \"Grímnismál\".\n\nIn the second stanza of the \"Poetic Edda\" poem \"Völuspá\", the völva (a shamanic seeress) reciting the poem to the god Odin says that she remembers far back to \"early times\", being raised by jötnar, recalls nine worlds and \"nine wood-ogresses\" (), and when Yggdrasil was a seed (\"glorious tree of good measure, under the ground\"). In stanza 19, the völva says:\n\nIn stanza 20, the völva says that from the lake under the tree come three \"maidens deep in knowledge\" named Urðr, Verðandi, and Skuld. The maidens \"incised the slip of wood,\" \"laid down laws\" and \"chose lives\" for the children of mankind and the destinies (\"\") of men. In stanza 27, the völva details that she is aware that \"Heimdallr's hearing is couched beneath the bright-nurtured holy tree.\" In stanza 45, Yggdrasil receives a final mention in the poem. The völva describes, as a part of the onset of Ragnarök, that Heimdallr blows Gjallarhorn, that Odin speaks with Mímir's head, and then:\n\nIn stanza 138 of the poem \"Hávamál\", Odin describes how he once sacrificed himself to himself by hanging on a tree. The stanza reads:\n\nIn the stanza that follows, Odin describes how he had no food nor drink there, that he peered downward, and that \"I took up the runes, screaming I took them, then I fell back from there.\" While Yggdrasil is not mentioned by name in the poem and other trees exist in Norse mythology, the tree is near universally accepted as Yggdrasil, and if the tree is Yggdrasil, then the name \"Yggdrasil\" directly relates to this story.\n\nIn the poem \"Grímnismál\", Odin (disguised as \"Grímnir\") provides the young Agnar with cosmological lore. Yggdrasil is first mentioned in the poem in stanza 29, where Odin says that, because the \"bridge of the Æsir burns\" and the \"sacred waters boil,\" Thor must wade through the rivers Körmt and Örmt and two rivers named Kerlaugar to go \"sit as judge at the ash of Yggdrasill.\" In the stanza that follows, a list of names of horses are given that the Æsir ride to \"sit as judges\" at Yggdrasil.\n\nIn stanza 31, Odin says that the ash Yggdrasil has three roots that grow in three directions. He details that beneath the first lives Hel, under the second live frost jötnar, and beneath the third lives mankind. Stanza 32 details that a squirrel named Ratatoskr must run across Yggdrasil and bring \"the eagle's word\" from above to Níðhöggr below. Stanza 33 describes that four harts named Dáinn, Dvalinn, Duneyrr and Duraþrór consume \"the highest boughs\" of Yggdrasil.\n\nIn stanza 34, Odin says that more serpents lie beneath Yggdrasil \"than any fool can imagine\" and lists them as Góinn and Móinn (possibly meaning Old Norse \"land animal\"), which he describes as sons of Grafvitnir (Old Norse, possibly \"ditch wolf\"), Grábakr (Old Norse \"Greyback\"), Grafvölluðr (Old Norse, possibly \"the one digging under the plain\" or possibly amended as \"the one ruling in the ditch\"), Ófnir (Old Norse \"the winding one, the twisting one\"), and Sváfnir (Old Norse, possibly \"the one who puts to sleep = death\"), who Odin adds that he thinks will forever gnaw on the tree's branches.\n\nIn stanza 35, Odin says that Yggdrasil \"suffers agony more than men know\", as a hart bites it from above, it decays on its sides, and Níðhöggr bites it from beneath. In stanza 44, Odin provides a list of things that are what he refers to as the \"noblest\" of their kind. Within the list, Odin mentions Yggdrasil first, and states that it is the \"noblest of trees\".\n\nYggdrasil is mentioned in two books in the \"Prose Edda\"; \"Gylfaginning\" and \"Skáldskaparmál\". In \"Gylfaginning\", Yggdrasil is introduced in chapter 15. In chapter 15, Gangleri (described as king Gylfi in disguise) asks where is the chief or holiest place of the gods. High replies \"It is the ash Yggdrasil. There the gods must hold their courts each day\". Gangleri asks what there is to tell about Yggdrasil. Just-As-High says that Yggdrasil is the biggest and best of all trees, that its branches extend out over all of the world and reach out over the sky. Three of the roots of the tree support it, and these three roots also extend extremely far: one \"is among the Æsir, the second among the frost jötnar, and the third over Niflheim. The root over Niflheim is gnawed at by the wyrm Níðhöggr, and beneath this root is the spring Hvergelmir. Beneath the root that reaches the frost jötnar is the well Mímisbrunnr, \"which has wisdom and intelligence contained in it, and the master of the well is called Mimir\". Just-As-High provides details regarding Mímisbrunnr and then describes that the third root of the well \"extends to heaven\" and that beneath the root is the \"very holy\" well Urðarbrunnr. At Urðarbrunnr the gods hold their court, and every day the Æsir ride to Urðarbrunnr up over the bridge Bifröst. Later in the chapter, a stanza from \"Grímnismál\" mentioning Yggdrasil is quoted in support.\n\nIn chapter 16, Gangleri asks \"what other particularly notable things are there to tell about the ash?\" High says there is quite a lot to tell about. High continues that an eagle sits on the branches of Yggdrasil and that it has much knowledge. Between the eyes of the eagle sits a hawk called Veðrfölnir. A squirrel called Ratatoskr scurries up and down the ash Yggdrasil carrying \"malicious messages\" between the eagle and Níðhöggr. Four stags named Dáinn, Dvalinn, Duneyrr, and Duraþrór run between the branches of Yggdrasil and consume its foliage. In the spring Hvergelmir are so many snakes along with Níðhöggr \"that no tongue can enumerate them\". Two stanzas from \"Grímnismál\" are then cited in support. High continues that the norns that live by the holy well Urðarbrunnr each day take water from the well and mud from around it and pour it over Yggdrasil so that the branches of the ash do not rot away or decay. High provides more information about Urðarbrunnr, cites a stanza from \"Völuspá\" in support, and adds that dew falls from Yggdrasil to the earth, explaining that \"this is what people call honeydew, and from it bees feed\".\n\nIn chapter 41, the stanza from \"Grímnismál\" is quoted that mentions that Yggdrasil is the foremost of trees. In chapter 54, as part of the events of Ragnarök, High describes that Odin will ride to the well Mímisbrunnr and consult Mímir on behalf of himself and his people. After this, \"the ash Yggdrasil will shake and nothing will be unafraid in heaven or on earth\", and then the Æsir and Einherjar will don their war gear and advance to the field of Vígríðr. Further into the chapter, the stanza in \"Völuspá\" that details this sequence is cited.\n\nIn the \"Prose Edda\" book \"Skáldskaparmál\", Yggdrasil receives a single mention, though not by name. In chapter 64, names for kings and dukes are given. \"Illustrious one\" is provided as an example, appearing in a Christianity-influenced work by the skald Hallvarðr Háreksblesi: \"There is not under the pole of the earth [Yggdrasil] an illustrious one closer to the lord of monks [God] than you.\"\n\nHilda Ellis Davidson comments that the existence of nine worlds around Yggdrasil is mentioned more than once in Old Norse sources, but the identity of the worlds is never stated outright, though it can be deduced from various sources. Davidson comments that \"no doubt the identity of the nine varied from time to time as the emphasis changed or new imagery arrived\". Davidson says that it is unclear where the nine worlds are located in relation to the tree; they could either exist one above the other or perhaps be grouped around the tree, but there are references to worlds existing beneath the tree, while the gods are pictured as in the sky, a rainbow bridge (Bifröst) connecting the tree with other worlds. Davidson opines that \"those who have tried to produce a convincing diagram of the Scandinavian cosmos from what we are told in the sources have only added to the confusion\".\n\nDavidson notes parallels between Yggdrasil and shamanic lore in northern Eurasia:\nThe conception of the tree rising through a number of worlds is found in northern Eurasia and forms part of the shamanic lore shared by many peoples of this region. This seems to be a very ancient conception, perhaps based on the Pole Star, the centre of the heavens, and the image of the central tree in Scandinavia may have been influenced by it... Among Siberian shamans, a central tree may be used as a ladder to ascend the heavens.\n\nDavidson says that the notion of an eagle atop a tree and the world serpent coiled around the roots of the tree has parallels in other cosmologies from Asia. She goes on to say that Norse cosmology may have been influenced by these Asiatic cosmologies from a northern location. Davidson adds, on the other hand, that it is attested that the Germanic peoples worshiped their deities in open forest clearings and that a sky god was particularly connected with the oak tree, and therefore \"a central tree was a natural symbol for them also\".\n\nConnections have been proposed between the wood Hoddmímis holt (Old Norse \"Hoard-Mímir's\" holt) and the tree Mímameiðr (\"Mímir's tree\"), generally thought to refer to the world tree Yggdrasil, and the spring Mímisbrunnr. John Lindow concurs that \"Mímameiðr\" may be another name for Yggdrasil and that if the Hoard-Mímir of the name \"Hoddmímis holt\" is the same figure as Mímir (associated with the spring named after him, Mímisbrunnr), then the Mímir's holt—Yggdrasil—and Mímir's spring may be within the same proximity.\n\nCarolyne Larrington notes that it is nowhere expressly stated what will happen to Yggdrasil during the events of Ragnarök. Larrington points to a connection between the primordial figure of Mímir and Yggdrasil in the poem \"Völuspá\", and theorizes that \"it is possible that Hoddmimir is another name for Mimir, and that the two survivors hide in Yggdrasill.\"\n\nRudolf Simek theorizes that the survival of Líf and Lífþrasir through Ragnarök by hiding in Hoddmímis holt is \"a case of reduplication of the anthropogeny, understandable from the cyclic nature of the Eddic eschatology.\" Simek says that Hoddmímis holt \"should not be understood literally as a wood or even a forest in which the two keep themselves hidden, but rather as an alternative name for the world-tree Yggdrasill. Thus, the creation of mankind from tree trunks (Askr, Embla) is repeated after the Ragnarǫk as well.\" Simek says that in Germanic regions, the concept of mankind originating from trees is ancient. Simek additionally points out legendary parallels in a Bavarian legend of a shepherd who lives inside a tree, whose descendants repopulate the land after life there has been wiped out by plague (citing a retelling by F. R. Schröder). In addition, Simek points to an Old Norse parallel in the figure of Örvar-Oddr, \"who is rejuvenated after living as a tree-man (\"Ǫrvar-Odds saga\" 24–27)\".\n\nContinuing as late as the 19th century, warden trees were venerated in areas of Germany and Scandinavia, considered to be guardians and bringers of luck, and offerings were sometimes made to them. A massive birch tree standing atop a burial mound and located beside a farm in western Norway is recorded as having had ale poured over its roots during festivals. The tree was felled in 1874.\n\nDavidson comments that \"the position of the tree in the centre as a source of luck and protection for gods and men is confirmed\" by these rituals to Warden Trees. Davidson notes that the gods are described as meeting beneath Yggdrasil to hold their things, and that the pillars venerated by the Germanic peoples, such as the pillar Irminsul, were also symbolic of the center of the world. Davidson details that it would be difficult to ascertain whether a tree or pillar came first, and that this likely depends on if the holy location was in a thickly wooded area or not. Davidson notes that there is no mention of a sacred tree at Þingvellir in Iceland yet that Adam of Bremen describes a huge tree standing next to the Temple at Uppsala in Sweden, which Adam describes as remaining green throughout summer and winter, and that no one knew what type of tree it was. Davidson comments that while it is uncertain that Adam's informant actually witnessed that tree is unknown, but that the existence of sacred trees in pre-Christian Germanic Europe is further evidenced by records of their destruction by early Christian missionaries, such as Thor's Oak by Saint Boniface.\n\nKen Dowden comments that behind Irminsul, Thor's Oak in Geismar, and the sacred tree at Uppsala \"looms a mythic prototype, an Yggdrasil, the world-ash of the Norsemen\".\n\nModern works of art depicting Yggdrasil include \"Die Nornen\" (painting, 1888) by K. Ehrenberg; \"Yggdrasil\" (fresco, 1933) by Axel Revold, located in the University of Oslo library auditorium in Oslo, Norway; \"Hjortene beiter i løvet på Yggdrasil asken\" (wood relief carving, 1938) on the Oslo City Hall by Dagfin Werenskjold; and the bronze relief on the doors of the Swedish Museum of National Antiquities (around 1950) by B. Marklund in Stockholm, Sweden. Poems mentioning Yggdrasil include \"Vårdträdet\" by Viktor Rydberg and \"Yggdrasill\" by J. Linke.\n\n"}
{"id": "9797507", "url": "https://en.wikipedia.org/wiki?curid=9797507", "title": "Đavolja Varoš", "text": "Đavolja Varoš\n\nĐavolja varoš (, meaning \"Devil's Town\") is a peculiar rock formation, located in south Serbia on the Radan Mountain on the territory of the village of Đake in the municipality Kuršumlija. \n\nĐavolja Varoš features 202 exotic formations described as earth pyramids or \"towers\", as the locals refer to them. They are tall and wide at the base. These formations were created by strong erosion of the soil that was scene of intense volcanic activity millions of years ago. Most of the towers have \"caps\" or \"heads\" of andesite, which protect them from further erosion. Volatile volcanic history left marks in the multicolored rocks in the towers hinterlands. However, Đavolja Varoš in its modern form is a relatively new feature. As the inhabitants of the surrounding region were cutting down the forests, they enabled for the precipitation to erode the rocks. The area beneath the towers is called The Hell gully (\"Paklena jaruga\") and the surrounding terrain is a location of the mine shafts from the medieval Nemanjić Serbia.\n\nA natural spring is located beneath the formations and has a high mineral concentration. There are two springs: Đavolja voda (Devil’s Water), with extremely acidic water (pH 1.5) and high mineral concentration (15 g/l of water), and Crveno vrelo (Red Well). The unusually pungent spring waters were examined for the first time in 1905 by Aleksandar Zega, founder of the Serbian Chemical Society.\n\nThe formations were scientifically examined and described in 1955 by Tomislav Rakićević. Since 1959, Đavolja Varoš has been protected by the state and a 1995 decision of the Serbian Government declared it a major natural monument subject to category one protection. It is visited by 50,000 tourists yearly.\n\nĐavolja Varoš was a nominee in the New Seven Wonders of Nature campaign.\n\n\n"}
