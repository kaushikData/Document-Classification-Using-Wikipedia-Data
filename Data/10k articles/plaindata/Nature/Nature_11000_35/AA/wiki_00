{"id": "1126135", "url": "https://en.wikipedia.org/wiki?curid=1126135", "title": "Alfred Wegener Institute for Polar and Marine Research", "text": "Alfred Wegener Institute for Polar and Marine Research\n\nThe Alfred Wegener Institute, Helmholtz Centre for Polar and Marine Research (German: \"Alfred-Wegener-Institut, Helmholtz-Zentrum für Polar- und Meeresforschung\") is located in Bremerhaven, Germany, and a member of the Helmholtz Association of German Research Centres. It conducts research in the Arctic, in the Antarctic and in the high and mid latitude oceans. Additional research topics are: North Sea research, marine biological monitoring and technical marine developments. The institute was founded in 1980 and is named after meteorologist, climatologist and geologist Alfred Wegener.\n\nThe institute has three major departments: \n\nThe institute is distributed over several sites within North Germany and the Otto Schmidt Laboratory for Polar and Marine Research (OSL) at the Arctic and Antarctic Research Institute (AARI) in Saint Petersburg as Russian-German cooperation in the field of Arctic research, named after the polar explorer Otto Schmidt.\n\nThe headquarters was founded by Gotthilf Hempel. Nowadays, the AWI has several buildings within the city of Bremerhaven.\n\nThe Forschungstelle Potsdam is situated on the Telegrafenberg next to Potsdam. It belongs to AWI since 1992. The research focuses on the atmospheric physics and atmospheric chemistry of the atmosphere on the one hand and periglacial research on the other hand.\n\nThe Wadden Sea Station Sylt is located on the North German island Sylt.\nIt was founded in 1924 as an oyster laboratory to study the decline of oyster stocks and in order to study how they could be cultivated. In 1937, the name changed from \"oyster laboratory\" to \"Wadden Sea station\". The station grew, and in 1949 the station was shifted from the northernmost edge of the island to the current location, next to the harbor of List. In 1998 the station became part of AWI. Nowadays, there are about 30 scientists and technicians. Two guest houses allow to perform workshops and video conferences are possible with the AWI headquarters.\n\nThe research focuses on coast ecology and coast geology. In the 1930 there have been oyster reefs below the mussel banks at the water level. Below these, there have been sabellaria reefs which have been destroyed by fishery. Nowadays there are only the mussel banks left.\n\nThe Biologische Anstalt Helgoland is situated at on the island Heligoland (German: \"Helgoland\"). The station exists since 1892. Scientists study the ecology of the North Sea in this research station.\n\nSince 1962, at Heligoland roadstead , phytoplankton and water samples are taken every weekday morning, \nthe turbidity is measured (e.g. using a Secchi disk) and other parameters are recorded. The North Sea warmed by 1.65 °C since the start of the time series.\n\nThe institute maintains several research stations around the Arctic Ocean and on the Antarctic continent.\n\nThe Neumayer-Station III located at , about 5 km away from the previous station, \"Neumayer II\" which is now abandoned and covered by a thick ice cover. The new station (Webcam) is a futuristic-looking combined platform above the snow surface offering space for research, operations and living since 2009. The station stands on 16 hydraulic posts which are used to adjust the building to the growing snow cover. Below the station s and other equipment are stored an underground garage.\nIn summer, the station can host up to 40 people. The station contains several laboratories, has a weather balloon launching facility and a hospital with telemedical equipment.\n\nIn cooperation with the Instituto Antártico Argentino (IAA), in 1994 the AWI opened a research station on King George Island at . The station is named after Eduard Dallmann, a German whaler, trader and Polar explorer living next to Bremen.\n\nThe Koldewey Station at is named after the German polar explorer Carl Koldewey and part of the French-German Arctic Research \"AWIPEV\" Arctic in Ny-Ålesund on Svalbard.\n\nThe Kohnen Station was established in 2001 as logistical base for ice core drilling in Dronning Maud Land, Antarctica at \n\nThe Samoylov Station at lays within the Lena Delta close to the Laptev Sea. The station was set up as a logistic base for permafrost studies by the \"Lena Delta Reserve\" (LDR) and the AWI.\n\nAltogether there are six ships that belong to AWI.\n\nThe AWI flagship is Germany's research icebreaker RV \"Polarstern\". The ship was commissioned in 1982. The double-hulled icebreaker is operational up to temperatures as low as −50 °C (−58 °F). Polarstern can break through sea ice of 1.5 m thickness at a speed of 5 knots, thicker ice must be broken by ramming. A possible but not probable follow-up flagship might be the research icebreaker \"Aurora Borealis\".\n\nThe vessel RV \"Heincke\" is a multifunctional and low-noise ship for research in ice-free waters, named after German zoologist and ichthyologist Friedrich Heincke . With a length of 54.6 m, a width of 12.5 m and a draft 4.16 m, the ship is categorized as \"medium research vessel\" within the German research fleet. \nThe ship was put into operation in 1990, its building costs have been around 16 Millionen Euro.\nOn the vessel, up to 12 scientists and 8 crew members can work for up to 30 passage days. This corresponds to an operating range of roughly 7500 nautical miles. The shipowner is Briese Schiffahrts GmbH & Co. KG from Leer, a city in East Frisia.\n\nThe research cutter RV \"Uthörn\" is named after the small island Uthörn next to Sylt in the North Sea. The vessel is regularly on research tours in German Bight, but is also used to supply the AWI branch \"Biologische Anstalt Helgoland\" mentioned above. Two scientists and 4 crew members can live and work on board for up to 180 days, but the vessel mainly used for day trips. Another operation purpose are short term cruises of a few hours for up to 25 students to demonstrate oceanographic and biological sampling methods.\n\nBeing commissioned in 1982 RV \"Uthörn\" replaced a vessel with the same name which was built in 1947 and had a length of 24 m. The current vessel is powered by two V12\nfour-stroke Diesel engines manufactured by the company MWM GmbH from Mannheim. Each engine delivers up to 231 kW to a controllable-pitch propeller; the maximal speed is around 10 kn.\nOn the working deck, there is a dry lab and a laboratory for wet work like sorting fish. The ship is equipped with standard sampling devices: You may find on board a demersal trawl, a Van Veen Grab Sampler, Niskin bottles, and even deprecated reversing thermometers for teaching purposes.\n\nThe research catamaran \"Mya\" is specially designed for research in the intertidal zone, it can fall dry at low tide. The main research area is the Wadden Sea. Last but not least, there are two small motor boats, \"Aade\" and \"Diker\" for sampling and diving operations around Heligoland.\n\nThe Alfred Wegener Institute operated five airplanes under the name of \"Polar\", those being:\n\nAWI currently uses two Basler BT-67. These planes are 20 m long, 5.2 m high and have a wingspan of 29 m. The empty weight is 7680 kg, with ski landing gear it weighs 8340 kg.\nThe minimal cruising speed is 156 km/h, the maximum is SFA. Without payload, the flying range is around 3900 km. The planes are maintained by the company Kenn Borek Air located in Calgary, Alberta, Canada.\n\nThe plane hull was built in 1942 but completely refurbished after the AWI acquired the plane in 2007. Since then it \"has supplied a large volume of valuable data\" said Prof. , the former director of the AWI.\n\nThis plane with the call sign C-FHGF was acquired by AWI in 2011. The BMBF, the German Federal Minister of Education and Research funded the purchase and equipping of the plane with a total of 9.78 million euros.\n\n\nOn 24 February 1985, the \"Polar 3\", a research airplane of the institute of the type Dornier 228, was shot down by guerrillas of the Polisario Front over West Sahara. All three crew members died. \"Polar 3\", together with unharmed \"Polar 2\", was on its way back from Antarctica and had taken off in Dakar, Senegal, to reach Arrecife, Canary Islands.\n\n\n"}
{"id": "15580858", "url": "https://en.wikipedia.org/wiki?curid=15580858", "title": "Andrewsite", "text": "Andrewsite\n\nAndrewsite is a now discredited mineral originally reported at the Wheal Phoenix mine, near Liskeard in Cornwall. It was named for Thomas Andrews FRS, the English chemist.\n\nIt has been shown to be a mixture of hentschelite and rockbridgeite, with minor chalcosiderite.\n\n"}
{"id": "26366005", "url": "https://en.wikipedia.org/wiki?curid=26366005", "title": "Biosafety cabinet", "text": "Biosafety cabinet\n\nA biosafety cabinet (BSC)—also called a biological safety cabinet or microbiological safety cabinet—is an enclosed, ventilated laboratory workspace for safely working with materials contaminated with (or potentially contaminated with) pathogens requiring a defined biosafety level. Several different types of BSC exist, differentiated by the degree of biocontainment required. BSCs first became commercially available in 1950.\n\nThe primary purpose of a BSC is to serve as a means to protect the laboratory worker and the surrounding environment from pathogens. All exhaust air is HEPA-filtered as it exits the biosafety cabinet, removing harmful bacteria and viruses. This is in contrast to a laminar flow clean bench, which blows unfiltered exhaust air towards the user and is not safe for work with pathogenic agents. Neither are most BSCs safe for use as fume hoods. Likewise, a fume hood fails to provide the environmental protection that HEPA filtration in a BSC would provide. However, most classes of BSCs have a secondary purpose to maintain the sterility of materials inside (the \"product\").\n\nThe U.S. Centers for Disease Control and Prevention (CDC) classifies BSCs into three classes. These classes and the types of BSCs within them are distinguished in two ways: the level of personnel and environmental protection provided and the level of product protection provided.\n\nClass I cabinets provide personnel and environmental protection but no product protection. In fact, the inward flow of air can contribute to contamination of samples.<ref name=\"DePalmaafASFDASFDQWREASDFASFDASF,KZXV XZV ZXV/AFA;'A;LWE0IRWEQ'JK2009\"></ref> Inward airflow is maintained at a minimum velocity of 75 ft/min(0.38 m/s). These BSCs are commonly used to enclose specific equipment (\"e.g.\" centrifuges) or procedures (\"e.g.\" aerating cultures) that potentially generate aerosols. BSCs of this class are either ducted (connected to the building exhaust system) or unducted (recirculating filtered exhaust back into the laboratory).\n\nClass II cabinets provide both kinds of protection (of the samples and of the environment) since makeup air is also HEPA-filtered. There are five types: Type A1 (formerly A), Type A2 (formerly A/B3), Type B1, Type B2 and Type C1. Each type's requirements are defined by NSF International Standard 49, which in 2002 reclassified A/B3 cabinets (classified under the latter type if connected to an exhaust duct) as Type A2, and added the Type C1 in the 2016 standard. About 90% of all biosafety cabinets installed are Type A2 cabinets.\n\nPrinciples of operation use motor driven blowers (fans) mounted in the cabinet to draw directional mass airflow around a user and into the air grille - protecting the operator. The air is then drawn underneath the work surface and back up to the top of the cabinet where it passes through the HEPA filters. A column of HEPA filtered, sterile air is also blown downward, over products and processes to prevent contamination. Air is also exhausted through a HEPA filter, and depending on the Type of Class II BSC, the air is either recirculated back into the laboratory or pulled by an exhaust fan, through ductwork where it is expelled from the building. \n\nThe Type A1 cabinet, formerly known as Type A, has a minimum inflow velocity of 75 ft/min. The downflow air, considered contaminated, splits just above the work surface (the BSCs smoke split) and mixes with the inflow. This air is drawn, through ductwork, up the back of the cabinet where it is then blown into a positive pressure, contaminated plenum. Here, the air is either recirculated, through a HEPA filter, back down over the work zone, or exhausted out of the cabinet (also through a HEPA filter). Sizing of HEPA filters and an internal damper are used to balance these air volumes. This type is not safe for work with hazardous chemicals even when exhausted with a \"thimble\" or canopy to avoid disturbing internal air flow.\n\nThe Type A2 cabinet, formerly designated A/B3, has a minimum inflow velocity of 100 ft/min. A negative air pressure plenum surrounds all contaminated positive pressure plenums. In other respects, the specifications are identical to those of a Type A1 cabinet.\n\nType B1 and B2 cabinets have a minimum inflow velocity of 100 ft/min, and these cabinets must be hard-ducted to an exhaust system rather than exhausted through a thimble connection. Their exhaust systems must also be dedicated (one BSC per duct run, per blower). In contrast to the type A1 and A2 cabinets, Type B BSCs use single pass airflow (air that does not mix and recirculate) in order to also control hazardous chemical vapors. Type B1 cabinets split the airflow so that the air behind the smoke-split is directed to the exhaust system, while air between the operator and the smoke-split mixes with inflow air and is recirculated as downflow. Since exhaust air is drawn from the rear grille, the CDC advises that work with hazardous chemistry be conducted in the rear of the cabinet. This is complicated, since the smoke split (demarking the \"rear of the cabinet\") is an invisible line that extends the width of the cabinet (approximately 10-14 inches from the front grille) and drifts as the internal HEPA filters load with particulate.\n\nThe Type B2 cabinet (also known as a Total Exhaust BSC) is expensive to operate because no air is recirculated within. Therefore, this type is mainly found in such applications as toxicology laboratories, where the ability to safely use hazardous chemistry is important. Additionally, there is the risk that contaminated air would flow into the laboratory if the exhaust system for a Type B1 or B2 cabinet were to fail. To mitigate this risk, cabinets of these types generally monitor the exhaust flow, shutting off the supply blower and sounding an alarm if the exhaust flow is insufficient.\n\nThe Type C1 BSC was borne out of necessity to control infectious material, chemical hazards, reduce operating costs and add flexibility in modern laboratories. The Type C1 moves air by mixing inflow air with the air in the columns of downflow air marked for recirculation. Air above a clearly delineated section of the work surface is drawn by a second internal fan where it is exhausted through a HEPA filter. The C1 differs from a Type A in that it can use this single pass airflow, and when installed in a ducted operating mode, can protect from hazardous chemistry, like the Type Bs. The C1 also differs from the Type B BSCs in several ways; (1) it does not require a hard connected, dedicated exhaust system and blower to operate, (2) pending a risk assessment, the BSC can run for an extended duration to increase operator protection during a remote exhaust system failure, and (3) Type C1 BSCs can run without being connected to an exhaust system at all. \n\nClass II cabinets are the commonly used cabinets in clinical and research laboratories.\n\nThe Class III cabinet, generally only installed in maximum containment laboratories, is specifically designed for work with BSL-4 pathogenic agents, providing maximum protection. The enclosure is gas-tight, and all materials enter and leave through a dunk tank or double-door autoclave. Gloves attached to the front prevent direct contact with hazardous materials (Class III cabinets are sometimes called glove box). These custom-built cabinets often attach into a line, and the lab equipment installed inside is usually custom-built as well.\n\nBiosafety cabinets are used on a daily basis for hours. Besides protection of user and sample material, the human design factors (ergonomics) of the work become more and more important. This includes reduction of the noise level (for a more convenient working atmosphere), a height adjustable stand or stool (for optimized sitting position), panorama side windows (more light within cabinet), 10°angled front sash (enables better sitting position) as well as strong light sources (better view within cabinet) to improve the working conditions.\n\nThe CDC does not recommend the installation of UV lamps in BSCs. The American Biological Safety Association supports this position, citing the safety risk to personnel, shallow penetration, reduced effectiveness in high relative humidity, and the frequent need to clean and replace the bulb. UV lamps should not be used as a primary source of surface decontamination within a BSC. However, these assertions have been formally disputed in at least one peer-reviewed article which points out that: \n\nCabinets need to be maintained on a regular schedule. During this certification check, the airflow and the filter capacities are verified. The filters have a limited lifespan - determined by the air quality within the laboratory space and the amount of particles and aerosols generated inside the BSC' work zone. As these filters load, the internal fan is required to do more work to push/pull the same volume of air through them. Newer cabinets measure the air flow constantly and self compensate fan performance to ensure constant volumes of air moving through the filters and the cabinet. If the flow drops below desired performance an audio and visual alarm will alert the operator. Changing the filter should be limited to trained persons as the filter is potentially contaminated. This can be done either after the cabinet has been decontaminated using a gaseous procedure (using Formaldehyde, Chlorine Dioxide or Vaporized Hydrogen Peroxide) or a \"bag-in/bag-out\" procedure.\n\nWhen an UV light is used, this lamp should be checked and changed as well. UV lights decrease in power over time, resulting in diminished disinfection of the working area.\n\nAs with work on open bench tops, work performed within a BSC must be performed carefully and safely. To avoid contamination and the risk of personnel exposure, the CDC advises investigators to follow best practices to reduce and control splatter and aerosol generation, such as keeping clean materials at least from aerosol-generating activities and arranging the work flow \"from clean to contaminated\". In particular, open flames, not necessary within the clean environment of a Class II or III BSC, cause disruption of the airflow inside. Once work inside a BSC has been completed, it is necessary to decontaminate the surfaces of the BSC as with other lab equipment and materials.\n\nWhen a BSC is serviced or relocated, including replacement of HEPA filters, it must be gas decontaminated. Gas decontamination involves filling the BSC with a poisonous gas, most commonly formaldehyde gas.\n\n"}
{"id": "2496566", "url": "https://en.wikipedia.org/wiki?curid=2496566", "title": "Botany Bay", "text": "Botany Bay\n\nBotany Bay, an open oceanic embayment, is located in Sydney, New South Wales, Australia, south of the Sydney central business district. Its source is the confluence of the Georges River at Taren Point and the Cooks River at Kyeemagh, which flows to the east before meeting its mouth at the Tasman Sea, midpoint between La Perouse and Kurnell.\n\nThe total catchment area of the bay is approximately . Despite its relative shallowness, the bay serves as greater metropolitan Sydney's main cargo seaport, located at Port Botany, with facilities managed by Sydney Ports Corporation. Two runways of Sydney Airport extend into the bay. Botany Bay National Park is located on the northern and southern headlands of the bay. The area surrounding the bay is generally managed by Roads and Maritime Services.\n\nThe land adjacent to Botany Bay was settled for many thousands of years by the Tharawal and Eora Aboriginal peoples and their associated clans. On 29 April 1770, Botany Bay was the site of James Cook's first landing of HMS \"Endeavour\" on the land mass of Australia, after his extensive navigation of New Zealand. Later the British planned Botany Bay as the site for a penal colony. Out of these plans came the first European habitation of Australia at Sydney Cove. Although the penal settlement was almost immediately shifted to Sydney Cove, for some time in Britain transportation to \"Botany Bay\" was a metonym for transportation to any of the Australian penal settlements.\n\nArchaeological evidence from the shores of Botany Bay has yielded evidence of an Aboriginal settlement dating back 5,000 years. The Aboriginal people of Sydney were known as the Eora with sub-groups derived from the languages they spoke. The people living between the Cooks River and while on the northern shore it was the Kameygal clan. An artefact collected on Cook's first voyage in Botany Bay is the bark shield left behind by a member of a local Aboriginal tribe. This very rare object is now in the British Museum's collection and was the subject of a programme in the BBC radio series A History of the World in 100 Objects.\n\nLieutenant James Cook first landed at Kurnell, on the southern banks of Botany Bay, on Sunday 29 April 1770, when navigating his way up the east coast of Australia on his ship, HMS \"Endeavour\". Cook's landing marked the beginning of Britain's interest in Australia and in the eventual colonisation of this new \"southern continent\". Initially the name \"Stingrays Harbour\" was used by Cook and other journal keepers on his expedition, for the stingrays they caught. That name was also recorded on an Admiralty chart. Cook's log for 6 May 1770 records \"The great quantity of these sort of fish found in this place occasioned my giving it the name of Stingrays Harbour\". However, in the journal prepared later from his log, Cook wrote instead: (sic) \"The great quantity of plants Mr. Banks and Dr. Solander found in this place occasioned my giving it the name of Botanist Botany Bay\".\n\nEighteen years later, in 1788, Governor Arthur Phillip sailed the armed tender HMS \"Supply\" into the bay on 18 January. Two days later the remaining ships of the First Fleet arrived to found the planned penal colony. However, the land was quickly ruled unsuitable for settlement as there was insufficient fresh water; Phillip also believed the swampy foreshores would render any colony unhealthy. Phillip decided instead to move to the excellent natural harbour of Port Jackson to the north.\n\nOn the morning of 24 January the French exploratory expedition of Jean-François de Galaup, comte de Lapérouse was seen outside Botany Bay. On 26 January, the \"Supply\" left the bay to move up to Port Jackson and anchor in Sydney Cove. On the afternoon of 26 January, the remaining ships of First Fleet arrived at Sydney Cove.\n\nIn 1789, Captain John Hunter surveyed Botany Bay after returning from the Cape of Good Hope, trading for grain. The good supply of fresh water in the area led to the expansion of its population in the 19th century.\n\nSydney Airport, Australia's busiest airport, sits on northwestern side of Botany Bay. Land was reclaimed from the bay to extend its first north-south runway and build a second one parallel to it.\n\nThe first container terminal at Port Botany, to the east of the airport, was completed during the 1970s, and is the largest container terminal in Sydney. A second container terminal was completed during the 1980s and bulk liquid storage facilities are located on the northern and southern edge of the bay. A third container terminal was completed in 2011. \n\nThe land around the headlands of the bay is protected by the National Parks and Wildlife Service as Kamay Botany Bay National Park. On the northern side of the mouth of the bay is the historic site of La Perouse, and to the south is Kurnell. Despite its relative isolation, the southern shore of the bay is dominated by an unusual mixture of pristine national park and heavy industrial use that includes Kurnell Desalination Plant, the Caltex Oil Refinery, sewer treatment, and historical sand mining facilities. On the southern side of the bay, a section of water has been fenced off under the authority of the National Parks and Wildlife Service at Towra Point for environmental conservation purposes.\n\nThe western shores of the bay feature many popular swimming beaches including Lady Robinsons Beach and is highly urbanised.\n\nBotany Bay has a diverse marine population, and the area around its entrance is a popular area for scuba diving. In recent times, the Botany Bay Watch Project has begun with volunteers assisting to monitor and protect the Bay Catchment and its unique marine life.\n\nThe world's largest population of weedy sea dragon ever surveyed is found at the 'Steps' dive site, on the Kurnell side of the Botany Bay National Park. Weedy sea dragons are just one of hundreds of territorial marine creatures found within Botany Bay. The eastern blue grouper is the state fish of New South Wales; and are commonly found following divers along the shore line of Botany Bay.\n\n\n\n"}
{"id": "861121", "url": "https://en.wikipedia.org/wiki?curid=861121", "title": "Christian name", "text": "Christian name\n\nA Christian name, sometimes referred to as a baptismal name, is a religious personal name historically given on the occasion of a Christian baptism, though now most often assigned by parents at birth. In English-speaking cultures, a person's Christian name is commonly their first name and is typically the name by which they are primarily known.\n\nTraditionally, a Christian name was given on the occasion of Christian baptism, with the ubiquity of infant baptism in medieval Christendom. In Elizabethan England, as suggested by Camden, the term \"Christian name\" was not necessarily related to baptism, used merely in the sense of \"given name\": Christian names were imposed for the distinction of persons, surnames for the difference of families.\n\nIn more modern times, the terms have been used interchangeably with given name, first name and forename in traditionally Christian countries, and are still common in day-to-day use.\n\nStrictly speaking, the Christian name is not merely the forename distinctive of the individual member of a family, but the name given to the person (generally a child) at his or her christening or baptism. In pre-Reformation England, the laity was taught to administer baptism in case of necessity with the words: \"I christen thee in the name of the Father\" etc. To \"christen\" in this context is therefore to \"baptise\", and \"Christian name\" means \"baptismal name\".\n\nIn view of the Hebrew practice of giving a name to the male child at the time of his circumcision on the eighth day after birth (Luke 1:59), it has been maintained that the custom of conferring a name upon the newly baptised was of Apostolic origin. For instance, the apostle of the Gentiles was called Saul before his conversion and Paul afterwards. But modern scholars have rejected this contention, since the baptism of St. Paul is recorded in Acts 9:18, but the name Paul does not occur before Acts 13:9 while Saul is found several times in the interval. There is no more reason to connect the name Paul with the Apostle's baptism than there is to account in the same way for the giving of the name Cephas or Peter, which is due to another cause. In the inscriptions of the catacombs and in early Christian literature, the names of Christians in the first three centuries did not distinctively differ from the names of the pagans around them. A reference to the Epistles of St. Paul indicates that the names of pre-Christian gods and goddesses were used by his converts after their conversion as before. Hermes occurs in Romans 16:14, with a number of other purely pagan names, Epaphroditus in Phil. 4:18, Phoebe, the deaconess, in Romans 16:1.\n\nSimilar names are found in the Christian inscriptions of the earlier period and in the signatories appended to such councils as Nicaea or Ancyra, or again in the lists of martyrs. At a later date the names are of a most miscellaneous character. The following classification is one that has been worked out by J. Bass Mullinger founded on Martigny.\n\nThis category may be divided as follows:\n\n\nThese include the following:\n\n\nThough the recurrence of such names as Agnes, Balbina, Cornelius, Felicitas, Irenaeus, Justinus, etc. may be due to veneration for the martyrs who first used these names, the names of the New Testament are rarely found while those of the Old Testament are more common. Susanna, Daniel, Moyses, Tobias, occur frequently, but towards the end of the 4th century the name of the Blessed Lady becomes as familiar as those of the Apostles. Paulus may be an intentional reference to St. Paul, and Johannes, Andreas, and Petrus with derivatives such as Petronia, Petrius, Petronilla, etc. may also refer to the Apostles. The name of Mary occurs occasionally in the catacomb inscriptions towards the end of the 4th century, for example, in the form \"LIVIA MARIA IN PACE\", and there is a martyr Maria assigned to the date AD 256.\n\nIn the Acts of St. Balsamus, who died AD 331, there is an early example of the connection between baptism and the giving of a name. \"By my paternal name\", this martyr is said to have declared, \"I am called Balsamus, but by the spiritual name which I received in baptism, I am known as Peter.\" The assumption of a new name was fairly common amongst Christians. Eusebius the historian took the name Pamphili from Pamphilus, the martyr whom he especially venerated. Earlier still St. Cyprian chose to be called Cyprianus Caecilius out of gratitude to the Caecilius to whom he owed his conversion. St. Dionysius of Alexandria (c. 260) declared, \"I am of opinion that there were many of the same name as the Apostle John, who on account of their love for him, and because they admired and emulated him, and desired to be loved by the Lord as he was, took to themselves the same name, just as many of the children of the faithful are called Paul or Peter.\"\n\nThe assumption of any such new name would take place formally at baptism, in which the catechumen, then probably as now, had to be addressed by some distinctive appellation, and the imposition of a new name at baptism had become general. Every child had necessarily to receive some name or other, and when baptism followed soon after birth this allowed public recognition of the choice made.\n\nIn the thirtieth of the supposed \"Arabian Canons of Nicaea\": \"Of giving only names of Christians in baptism\"; but the sermons of St. John Chrysostom assume in many different places that the conferring of a name, presumably at baptism, ought to be regulated by some idea of Christian edification, and he implies that such had been the practice of earlier generations. For example, he says: \"When it comes to giving the infant a name, caring not to call it after the saints, as the ancients at first did, people light lamps and give them names and so name the child after the one which continues burning the longest, from thence conjecturing that he will live a long time\" (Hom. in Cor., xii, 13).\n\nSimilarly he commends the practice of the parents of Antioch in calling their children after the martyr Meletius (P.G. 50, 515) and urges his hearers not to give their children the first name that occurs, nor to seek to gratify fathers or grandfathers or other family connections by giving their names, but rather to choose the names of holy men conspicuous for virtue and for their courage before God (P.G. 53, 179). There are other historic examples of such a change of name in adult converts.\n\nSocrates (Hist. Eccl., VII, xxi) wrote of Athenais who married the Emperor Theodosius the Younger, and who previously to marriage was baptized (AD 421) receiving the name Eudoxia.\n\nBede wrote that King Caedwalla went to Rome and was baptized by the Pope Sergius who gave him the name of Peter. Dying soon afterwards he was buried in Rome and his epitaph beginning \"Hic depositus est Caedwalla qui est Petrus\" was pointed out (Bede, \"Hist. Eccl.\", V, vii).\n\nLater Guthrum the Danish leader in England after his long contest with King Alfred was eventually defeated, and consenting to accept Christianity was baptized in 878, taking the name Æthelstan.\n\nVarious Fathers and spiritual writers and synodal decrees have exhorted Christians to give no names to their children in baptism but those of canonized saints or of the angels of God, but at no point in the history of the Church were these injunctions strictly attended to.\n\nThey were not observed during the early or the later Middle Ages. In extensive lists of medieval names, such as those found in the indexes of legal proceedings which have been edited in modern times, while ordinary names without religious associations, such as William, Robert, Roger, Geoffrey, Hugh, etc. are common (William about the year 1200 was by far the most common Christian name in England), there are also a number of exceptional names which have apparently no religious associations at all. These include Ademar, Ailma, Ailward, Albreza, Alditha, Almaury, Ascelina, Avice, Aystorius (these come from the lists of those cured at the shrine of St. Thomas of Canterbury). A rubric in the official \"Rituale Romanum\" mandates that the priest ought to see that names of deities or of godless pagans are not given in baptism (\"curet ne obscoena, fabulosa aut ridicula vel inanium deorum vel impiorum ethnicorum hominum nomina imponantur\").\n\nA pronouncement from Bourges (1666) addressing parents and godparents urges: \"Let them give to boys the names of male saints and to girls those of women saints as right order requires, and let them avoid the names of festivals like Easter (\"Pâques\"), Christmas (\"Noël\"), All Saints (\"Toussaint\") and others that are sometimes chosen.\" Despite such injunctions \"Toussaint\" has become a common French Christian name and \"Noël\" has also found popularity abroad. The addition of Marie, especially in the form Jean-Marie, for girls, and of Joseph for boys is common in present-day France.\n\nIn Spain and Italy Marian festivals have also created names for girls: \"Concepción\", of which the diminutive is \"Concha\", as well as \"Asunción, Encarnación, Mercedes, Dolores\" etc. in Spanish, and in Italian \"Assunta, Annunziata, Concetta\", etc.\nThe name Mary has not always been a favourite for girls. In England in the 12th century, Mary as a Christian name was rare. The name George, often given in recognition of the Saint George the patron saint of England, was not common in the 13th and 14th centuries, though it grew in popularity after the Protestant Reformation.\n\nIn the registers of Oxford University from 1560 to 1621, the more common names used by the students in order of popularity were: John, 3826; Thomas, 2777; William, 2546; Richard, 1691; Robert, 1222; Edward, 957; Henry, 908; George, 647; Francis, 447; James, 424; Nicholas, 326; Edmund, 298. In Italy and Spain it has always been common practice to call a child after the saint upon whose feast he is born.\n\nThe practice of adopting a new name was not limited to baptism. Many medieval examples show that any notable change of condition, especially in the spiritual order, was often accompanied by the reception of a new name. In the 8th century, the two Englishmen Winfrith and Willibald going on different occasions to Rome received from the Pope, along with a new commission to preach, the names respectively of Boniface and Clement. Emma of Normandy when she married King Ethelred in 1002 took the name Ælfgifu; while the reception of a new, monastic name upon entering a religious order remains almost universal.\n\nAt confirmation, in which the interposition of a godfather emphasizes the resemblance with baptism, it has been customary to take a new name, but usually, use is made of it is infrequent. In the case of Henry III, King of France, godson of the English Edward VI had been christened Edouard Alexandre in 1551, the same French prince at confirmation received the name of Henri, and subsequently reigned under this name.\n\nIn England after the Reformation, the practice of adopting a new name at confirmation was still used, as Sir Edward Coke wrote that a man might validly buy land by his \"confirmation name\", and he recalled the case of a Sir Francis Gawdye, late Chief Justice of the Common Pleas, whose name of baptism was Thomas and his name of confirmation Francis.\n\n"}
{"id": "50431941", "url": "https://en.wikipedia.org/wiki?curid=50431941", "title": "Cognitive ecology", "text": "Cognitive ecology\n\nCognitive ecology is the study of cognitive phenomena within social and natural contexts. It is an integrative perspective drawing from aspects of ecological psychology, cognitive science, evolutionary ecology and anthropology. Notions of domain-specific modules in the brain and the cognitive biases they create are central to understanding the enacted nature of cognition within a cognitive ecological framework. This means that cognitive mechanisms not only shape the characteristics of thought, but they dictate the success of culturally transmitted ideas. Because culturally transmitted concepts can often inform ecological decision-making behaviors, group-level trends in cognition (i.e., culturally salient concepts) are hypothesized to address ecologically relevant challenges.\n\nCognitive ecology explores the interactive relationship between organism-environment interactions and its impact on cognitive phenomena. Human cognition in this framework is multimodal and viewed similarly to enactivist perspectives on cognitive processing. For cultural concepts, this emphasizes cognitive distribution across an ecosystem, which is predicated on models of the extended mind thesis.\n\nWhile the multi-faceted nature of cognitive ecology is a consequence of its interdisciplinary history, it primarily derives from early work in ecological psychology. Paradigm shifts from behaviorist orientations of psychology to cognition, or the \"cognitive revolution\", gave rise to the ecological psychology approach, which distanced itself from mainstream cognitivist views by breaking down the common mind-environment dichotomy of psychological theory.\n\nOne particularly influential progenitor of this work was ecological psychologist James Gibson, whose legacy is marked by his ideas on ecological and social affordances. These are the opportunistic features of environmental objects that can be exploited for human use, and are therefore particularly perceptible (e.g., a knob affords twisting, an agreeable social cue affords a warm reaction). Gibson argued further that organisms cannot be disentangled from their environments, and that their cognitive constraints were consequences of a limited set of environmental invariants which shaped them over evolutionary time. An illustrative example for Gibson is the human capacity for three-dimensional visual perception, which he argues is a cognitive concept resulting from the way that people interact with their environment.\n\nAnother foreshadowed element of cognitive ecological theory comes from ecological anthropologist Gregory Bateson, who considered the notion of informational feedback loops between mind and environment, particularly their role in generating meaning and awareness of one's surroundings. In an essay, he speculates on how an observer might best delineate the \"self\" of a blind man. In his treatment, he questions whether one may arbitrarily choose to carve out the man's informational processing loop at his brain or his hands or his walking stick without offering an incomplete view of his cognitive process. This discussion of concept remains influential in modern cognitive ecological considerations of the densely interconnected elements of ecology that play relevant roles in cognition.\n\nAn enactive perspective of cognition is fundamental to a cognitive ecological view. Rather than a passive interpretation of internally represented information, cognition is considered to be an active process involving the transformation of information into meaningful relationships between the organism and its environment. For humans then, a perceived environment is only constructed insofar as cognitive constraints will allow. In other words, they \"enact a world\" by building perspectives out of ecological information, using their evolved cognitive equipment.\n\nCognitive ecology borrows ideas from views of extended cognition, as articulated by Chalmers and Clark (1998). They argue that humans cognitively utilize elements of their environment to aid the cognitive process and further entangle the mind-environment relationship as a result. They illustrate their claim with a hypothetical example of two people who achieve the same navigational success through a museum by different means; a person with Alzheimer's may use a notebook with written directions, while another may use her memory. The primary difference between the two people is that the former outsourced his memory to readily available external representations of information about the museum, whereas the latter relied on internal representations. A variant of this concept they also consider is socially extended cognition, which is a similar outsourcing of cognitive representations into other peoples' minds. These ideas elaborate a cognitive interpretation of broader anthropological notions maintaining that humans are a species deeply entangled in social and material elements of culture.\n\nDistributed cognition is an important model of the extended mind thesis for cognitive ecological theory put forth by Edwin Hutchins. This conceptualizes human groups as active networks with cognitive properties of their own, much like neural networks themselves yield emergent cognitive properties. For a social group, cognitive properties are disseminated into an individual's surrounding network. The cognitive properties of a group, Hutchins notes, is completely distinct from a given those of an individual. Distributed cognition is fundamentally contingent on and emergent from trending ideas among a collection of brains and artefacts.\n\nThis is conceptually similar to models of collective cognition in other social animal groups, which use agent based models to understanding insect swarming, fish schooling, bird flocking and baboon pack behaviors. Collective cognition in social animal groups is adaptive because the group can amplify its overall responsiveness to ecological cues. Likewise, the computational power of a human group can be more effective than that of even its best individuals. This idea is echoed by anthropologists noting the collective intentionality of cultural institutions.\n\nExisting models of cultural learning dynamics seem to articulate the mechanisms by which information is acquired by and distributed within groups. In particular, cultural evolution theorists assert that individual learning is required for tracking environmental dynamics, but this information is retained in culture by social learning. For Hutchins, this theoretical similarity is not a coincidence. After describing distributed cognitive networks and their relationships with ecological dynamics as \"cognitive ecosystems\", he defines culture as a \"shorthand way of referring to a complex cognitive ecosystem.\"\n\nReligious behaviors typically exist in the form of ritual and correspond to religious god concepts. These behaviors are phenotypic outcomes of god concepts that are ultimately subject to natural selection. Cognitive ecologists who study religion predict that god concepts across cultures can be linked to coordination solutions for local socioecological challenges, such as large-scale cooperation, intragroup cohesion and commitment, and resource management. For example, an omniscient and morally punitive \"Big God\" may be adaptive for large-scale populations by motivating prosocial behavior, whereas gods associated with small-scale societies are often concerned about the stability of local resources.\n\nSocial contracts and their associated fairness norms are thought by many economists to be contingent on means of production. A hunter-gatherer society, for instance, may operate at an equilibrium where each person contributes to the best of his or her ability and receives according to need. But if this society were to shift toward larger-scale agricultural practices, this equilibrium would be destabilized by increases in free riding and general temptations to profit by defecting. This has been supported empirically in cross-cultural studies using experimental economic game data, which showed a wide range of variance in fairness expectations between populations based on culturally-specific exchange concepts.\n\nThis shift in fairness expectations has also been implicated in archaeological data. In particular, the relaxed sharing norms hypothesized to be built upon periods of successful maize exploitation in the pre-Hispanic Pueblo Southwest seemed to be eroded by decreases in agricultural success. In other words, when crops began to fail and supply became low, cultural exchange norms became more stringent, kin-based and based on reciprocity.\n"}
{"id": "44343957", "url": "https://en.wikipedia.org/wiki?curid=44343957", "title": "Dennis L. Hartmann", "text": "Dennis L. Hartmann\n\nDennis L Hartmann is an American atmospheric scientist at the University of Washington. He has done research on ozone depletion and climate change.\n\nIn 2016 he was elected to the National Academy of Sciences.\nHe is a Revelle Medal committee member.\n"}
{"id": "55539946", "url": "https://en.wikipedia.org/wiki?curid=55539946", "title": "Dunajské trstiny", "text": "Dunajské trstiny\n\nDunajské trstiny is a nature reserve in the Slovak municipalities of Veľké Kosihy and Klížska Nemá in the Komárno District. The nature reserve covers an area of 104.1016 ha of the Danube floodplain area. It has a protection level of 4 under the Slovak nature protection system.\n\nThe area protects continuous reed communities. It presents a possibility to study the final stage of the silting process, houses wetland and hydrophile plant species and presents nesting places for wetland and reed birds.\n"}
{"id": "51308712", "url": "https://en.wikipedia.org/wiki?curid=51308712", "title": "Effort Sharing Regulation", "text": "Effort Sharing Regulation\n\nThe Effort Sharing Regulation is a policy framework, part of the European Union climate and energy package.\n\nIt sets binding national greenhouse gas targets for each of the 28 Member States of the European Union, collectively amounting to a 30% cut in emissions by 2030 (from a 2005 baseline).\n\nIts predecessor, the Effort Sharing Decision, covered the years up to 2020. which collectively amount to a 10% cut in emissions between 2005-2020. These cuts come from areas including transport, buildings and heat, and agriculture.\n\nIt is the companion policy to the European Union Emissions Trading System, which covers emission cuts in power and industry.\n\nAs the starting level of the reduction is actually too high, in the end a reduction of only 25.5 % will be achieved, not the desired and declared 30 %. \n"}
{"id": "20966671", "url": "https://en.wikipedia.org/wiki?curid=20966671", "title": "Electric energy markets by country", "text": "Electric energy markets by country\n\nThis article includes a chart representing production, consumption, exports and imports of electricity by country. \n\nBelow the numbers there is specified which position a country holds by the corresponding parameter. Dependent territories, not fully recognized countries and supranational entities are not ranked. By default countries are ranked by their total electricity production.\n\nAll data is taken from CIA World Factbook. Note that data related to one parameter may be more up to date than data related to some other.\n\n\nGeneral:\n"}
{"id": "23435761", "url": "https://en.wikipedia.org/wiki?curid=23435761", "title": "Energy in Djibouti", "text": "Energy in Djibouti\n\nDjibouti had no proven reserves of oil or natural gas, or refining capacity, Djibouti has no known reserves of coal. The country's is supplied primarily by thermal plants (about 120 MW) and some imported hydro energy from Ethiopia. However, the supplemental supply of power from Ethiopia does not always satisfy Djibouti’s demand for power. Based on 2013 data, Djibouti’s national electrification rate reached 50%, (14% in rural areas, 61% in urban areas).\n\nThe peak annual demand in 2014 was about 90 MW but is expected that it will grow to about 300 MW by around 2020. Electricity supply services are provided through the vertically integrated utility Electricité de Djibouti (EDD). A small amount of additional energy is generated by a solar plant (300 kW capacity). Djibouti has wind and geothermal generation potential and is actively studying these options. \n\nAll petroleum products are imported. In 2002, imports of refined petroleum products totaled , with consumption placed at . There were no natural gas imports for that year. The port in Djibouti's capital city is an important oil shipment and storage site.\n"}
{"id": "907123", "url": "https://en.wikipedia.org/wiki?curid=907123", "title": "Flagstone", "text": "Flagstone\n\nFlagstone (flag) is a generic flat stone, usually used for paving slabs or walkways, patios, fences and roofing. It may be used for memorials, headstones, facades and other construction. The name derives from Middle English \"flagge\" meaning turf, perhaps from Old Norse \"flaga\" meaning slab or chip.\nFlagstone is a sedimentary rock that is split into layers along bedding planes. Flagstone is usually a form of a sandstone composed of feldspar and quartz and is arenaceous in grain size (0.16 mm – 2 mm in diameter). The material that binds flagstone is usually composed of silica, calcite, or iron oxide. The rock color usually comes from these cementing materials. Typical flagstone colors are red, blue, and buff, though exotic colors exist.\n\nFlagstone is quarried in places with bedded sedimentary rocks with fissile bedding planes. Examples include Arizona flagstone and Pennsylvania Bluestone.\nAround the thirteenth century, the ceilings, walls and floors in European architecture became more ornate. Anglo-Saxons in particular used flagstones as flooring materials in the interior rooms of castles and other structures. \nLindisfarne Castle in England and Muchalls Castle (14th century) in Scotland are among many examples of buildings with surviving flagstone floors.\n\nFlagstone shingles are a traditional roofing material, a type of roof shingle commonly used in the Alps, where they are laid dry, often held in place with pegs or hooks. In the Aosta Valley, Italy, stone shingles are mandatory to cover buildings in historical areas.\n\n"}
{"id": "432138", "url": "https://en.wikipedia.org/wiki?curid=432138", "title": "Growing season", "text": "Growing season\n\nThe growing season is the part of the year during which local weather conditions (i.e. rainfall and temperature) permit normal plant growth. While each plant or crop has a specific growing season that depends on its genetic adaptation, growing seasons can generally be grouped into macro-environmental classes.\n\nGeographic conditions have major impacts on the growing season for any given area. The elevation, or the height above sea level, and temperature of a region are two of the main factors that affect the growing season. Generally speaking, the distance a location is from the equator can be a strong indicator as to what the growing season will look like, however in a high elevation area, regardless of proximity to the equator, a shorter growing season will generally be experienced.\nProximity to the ocean also can create less extreme conditions, especially in terms of temperature, which has the potential to extend the growing season further in either direction.\nIn hotter climates, particularly in deserts, despite the geographic barrier of limited water sources, people have been able to extend their growing season in these regions by way of diverting water from other areas and using it in their agriculture. The ability to use these irrigation methods, despite geographic challenges, has made it possible to enjoy almost a year-round growing season.\n\nIn agriculture, season extension is anything that allows a crop to be cultivated beyond its normal outdoor growing season. Examples include greenhouses, polytunnels, row cover, and cloches.\n\nIn the United States and Canada, the growing season usually refers to the time between two dates: the last frost in the spring and the first hard frost in the fall. Specifically, it is defined as the period of time between the average last date at which the overnight low temperature drops below in the spring and the average date at which the overnight low first drops down below in the fall. These average last and first frost dates have reportedly been occurring earlier and later, respectively, at a steady rate, as observed over the last 30 years. As a result, the overall observed length of the growing season in the United States has increased by about two weeks in the last 30 years.\n\nIn the cooler areas of North America, specifically the northern regions of the United States and Canada, the growing season is observed to be between April or May and goes through October. A longer season starts as early as February or March along the lower East Coast from northern Florida to South Carolina, along the Gulf coast in south Texas and southern Louisiana, and along the West Coast from coastal Oregon southward and can continue all the way through November or December. The longest growing season is found in central and south Florida where many tropical fruits are grown year round. These rough timetables vary significantly for areas that are at higher elevations or close to the ocean.\n\nBecause several crops grown in the United States require a long period of growth, growing season extension practices are commonly used as well. These include various row-covering techniques, such as using cold frames and garden fabric over crops. Greenhouses are also a common practice to extend the season, particularly in elevated regions that only enjoy 90-day growing seasons.\n\nIn much of Europe, the growing season is defined as the average number of days a year with a 24-hour average temperature of at least 5 °C (6 °C is sometimes used). This is typically from April until October or November, although this varies considerably with latitude and altitude. The growing season is almost year-round in most of Portugal and Spain, and may be only from June to September in northern Finland and the higher Alps. Proximity to the Gulf Stream and other maritime mediations of temperature extremes can extend the season.\n\nIn the United Kingdom, the growing season is defined as starting when the temperature on five consecutive days exceeds 5 °C, and ends after five consecutive days of temperatures below 5 °C. The 1961 to 1990 average season length was .\n\nIn some warm climates, such as the subtropical savanna and Sonoran Deserts or in the drier Mediterranean climates, the growing season is limited by the availability of water, with little growth in the dry season. Unlike in cooler climates where snow or soil freezing is a generally insurmountable obstacle to plant growth, it is often possible to greatly extend the growing season in hot climates by irrigation using water from cooler and/or wetter regions. This can in fact go so far as to allow year-round growth in areas that without irrigation could only support xerophytic plants.\nAlso in these tropical regions; the growing season can be interrupted by periods of heavy rainfall, called the rainy season. For example, in Colombia, where coffee is grown and can be harvested year-round, they don’t see a rainy season. However, in Indonesia, another large coffee-producing area, they experience this rainy season and the growth of the coffee beans is interrupted.\n\n"}
{"id": "21044813", "url": "https://en.wikipedia.org/wiki?curid=21044813", "title": "High Alps", "text": "High Alps\n\nThe High Alps are those parts of the Alps unsuitable for habitation or seasonal transhumance. This includes all regions higher than 3,000 m above sea level, as well as most regions between 2,500 m and 3,000 m (Juf at 2,126 m is the highest permanently inhabited village in the Alps. Alpine pastures are typically below 2,400 m but may exceptionally be located as high as 2,800 m.).\n\nThe High Alps have tundra or ice cap climate rather than the Alpine climate eponymous of the Alpine region at 1,800-2,500 m, above the tree-line but still amenable to transhumance economy. \n\nExploration of the High Alps began in the 18th century, with Horace-Bénédict de Saussure. The first ascent of the highest peak of the alps, Mont Blanc, dates to 1786.\n\nAll important mountain passes in Switzerland are below 2,500 m (with Nufenen Pass as high as 2,478 m) but there are a few minor foot passes above 3,000 m: Schöllijoch at 3,343 m, Theodul Pass at 3,301 m, Zwischbergen Pass at 3,268 m, and others. Of historical interest is the Schnidejoch at 2,756 m which appears to have served as a pass since prehistoric times. In other Alpine countries there are higher road passes, such as the Col de l'Iseran (the highest paved road in Europe) in France and the Stelvio Pass in Italy.\n\nThe alpine line of perpetual snow is not fixed. The occurrence of favorable meteorological conditions during several successive seasons can increase the extent of the snowfields and lower the limit of seemingly permanent snow, while the opposite may cause the limit to rise higher on the flanks of the mountains. In some parts of the Alps the limit is about 2,400 m elevation, while in others it cannot be placed much below 2,900 m. As very little snow remains on rocks angled more than 60°, this is soon removed by the wind, some steep masses of rock remain bare even near the summits of the highest peaks, but as almost every spot offering the least hold for vegetation is covered with snow, few flowering plants are seen above 3,350 m.\n\nThe climate of the glacial region has often been compared to that of the polar regions, but they are very different. Here, intense solar radiation by day, which raises the surface when dry to a temperature approaching 27°C, alternates with severe frost by night. There, the Sun, which never sets is only able to send feeble rays that maintain a low temperature, rarely rising more than a few degrees above the freezing point. Hence the upper region of the Alps sustains a far more varied and brilliant vegetation.\n\n"}
{"id": "21504334", "url": "https://en.wikipedia.org/wiki?curid=21504334", "title": "Home energy monitor", "text": "Home energy monitor\n\nA home energy monitor provides feedback on electrical energy use. Devices may also display cost of energy used, and estimates of greenhouse gas emissions. Various studies have shown a reduction in home energy use of 4-15% through use of home energy display.\n\nElectricity use may be measured with an inductive clamp placed around the electric main, via the electric meter (either through an optical port, or by sensing the meters actions), by communicating with a smart meter, or by direct connection to the electrical system. The display portion may be remote from the measurement, communicating with the sensor using a cable, network, power line communications, or using radio. Online displays are also available which allow the user to use an internet connected display to show near real-time consumption.\n\nA means to reduce household energy consumption is to provide real-time feedback to homeowners so they can change their energy use. In 2010, UK based Current Cost announced a partnership with Google PowerMeter, a former online tool that connected to Current Cost devices, enabling users to receive real-time energy information on their customised Google homepage, wherever they were. Real-time data on how much energy is being consumed in the home was sent directly to the Google PowerMeter. The free software tool then visualised the information for users to view on their own iGoogle homepage, a personal web portal which enabled individuals to create and access a wide range of customisable information, web feeds and Google Gadgets.\nNote Google Power Meter is now defunct.\n\nA study using the PowerCost Monitor deployed in 500 Ontario homes by \"Hydro One\" showed an average 6.5% drop in total electricity use when compared with a similarly sized control group. \"Hydro One\" subsequently offered free power monitors to 30,000 customers based on the success of the pilot. Blue Line Innovations also indicates 100,000 units in the market today.\n\nAnother study carried out in the city of Sabadell (Spain) in 2009 using the efergy e2 in 29 households during a six-month period came to 11.8% on a weekly comparison between the first and last weeks of the campaign. On a monthly basis, the savings were 14.3%. Expected annual CO2 savings for all households is estimated to be 4.1 tonnes; projected CO2 emissions savings for 2020 are 180.6 tonnes.\n\nIn January 2009 the government of the state of Queensland, Australia began offering wireless energy monitors as part of its ClimateSmart Home Service program. By August 2009, almost 100,000 homes had signed up for the service, by August 2010 that number had risen to 200,000 homes. By the end of the program more than 335,000 households across Queensland had received the service with the Elite energy monitoring device supplied by Efergy Technologies.\n\nIn mid-2013 the government of the state of Victoria, Australia enabled Zigbee-based In-Home Displays to be connected to Victorian Smart Meter \n\n\n"}
{"id": "14946", "url": "https://en.wikipedia.org/wiki?curid=14946", "title": "Ice", "text": "Ice\n\nIce is water frozen into a solid state. Depending on the presence of impurities such as particles of soil or bubbles of air, it can appear transparent or a more or less opaque bluish-white color.\n\nIn the Solar System, ice is abundant and occurs naturally from as close to the Sun as Mercury to as far away as the Oort cloud objects. Beyond the Solar System, it occurs as interstellar ice. It is abundant on Earth's surfaceparticularly in the polar regions and above the snow lineand, as a common form of precipitation and deposition, plays a key role in Earth's water cycle and climate. It falls as snowflakes and hail or occurs as frost, icicles or ice spikes.\n\nIce molecules can exhibit eighteen or more different phases (packing geometries) that depend on temperature and pressure. When water is cooled rapidly (quenching), up to three different types of amorphous ice can form depending on the history of its pressure and temperature. When cooled slowly correlated proton tunneling occurs below giving rise to macroscopic quantum phenomena. Virtually all the ice on Earth's surface and in its atmosphere is of a hexagonal crystalline structure denoted as ice I (spoken as \"ice one h\") with minute traces of cubic ice denoted as ice I. The most common phase transition to ice I occurs when liquid water is cooled below (, ) at standard atmospheric pressure. It may also be deposited directly by water vapor, as happens in the formation of frost. The transition from ice to water is melting and from ice directly to water vapor is sublimation.\n\nIce is used in a variety of ways, including cooling, winter sports and ice sculpture.\n\nAs a naturally occurring crystalline inorganic solid with an ordered structure, ice fits the properties of a mineral. It possesses a regular crystalline structure based on the molecule of water, which consists of a single oxygen atom covalently bonded to two hydrogen atoms, or H–O–H. However, many of the physical properties of water and ice are controlled by the formation of hydrogen bonds between adjacent oxygen and hydrogen atoms; while it is a weak bond, it is nonetheless critical in controlling the structure of both water and ice.\n\nAn unusual property of ice frozen at atmospheric pressure is that the solid is approximately 8.3% less dense than liquid water (which is equivalent to volumetric expansion of 9%). The density of ice is 0.9167–0.9168 g/cm at 0 °C and standard atmospheric pressure (101,325 Pa), whereas water has a density of 0.9998–0.999863 g/cm at the same temperature and pressure. Liquid water is densest, essentially 1.00 g/cm, at 4 °C and becomes less dense as the water molecules begin to form the hexagonal crystals of ice as the freezing point is reached. This is due to hydrogen bonding dominating the intermolecular forces, which results in a packing of molecules less compact in the solid. Density of ice increases slightly with decreasing temperature and has a value of 0.9340 g/cm at −180 °C (93 K).\n\nWhen water freezes, it increases in volume (about 9% for fresh water). The effect of expansion during freezing can be dramatic, and ice expansion is a basic cause of freeze-thaw weathering of rock in nature and damage to building foundations and roadways from frost heaving. It is also a common cause of the flooding of houses when water pipes burst due to the pressure of expanding water when it freezes.\n\nThe result of this process is that ice (in its most common form) floats on liquid water, which is an important feature in Earth's biosphere. It has been argued that without this property, natural bodies of water would freeze, in some cases permanently, from the bottom up, resulting in a loss of bottom-dependent animal and plant life in fresh and sea water. Sufficiently thin ice sheets allow light to pass through while protecting the underside from short-term weather extremes such as wind chill. This creates a sheltered environment for bacterial and algal colonies. When sea water freezes, the ice is riddled with brine-filled channels which sustain sympagic organisms such as bacteria, algae, copepods and annelids, which in turn provide food for animals such as krill and specialised fish like the bald notothen, fed upon in turn by larger animals such as emperor penguins and minke whales.\n\nWhen ice melts, it absorbs as much energy as it would take to heat an equivalent mass of water by 80 °C. During the melting process, the temperature remains constant at 0 °C. While melting, any energy added breaks the hydrogen bonds between ice (water) molecules. Energy becomes available to increase the thermal energy (temperature) only after enough hydrogen bonds are broken that the ice can be considered liquid water. The amount of energy consumed in breaking hydrogen bonds in the transition from ice to water is known as the \"heat of fusion\".\n\nAs with water, ice absorbs light at the red end of the spectrum preferentially as the result of an overtone of an oxygen–hydrogen (O–H) bond stretch. Compared with water, this absorption is shifted toward slightly lower energies. Thus, ice appears blue, with a slightly greener tint than liquid water. Since absorption is cumulative, the color effect intensifies with increasing thickness or if internal reflections cause the light to take a longer path through the ice.\n\nOther colors can appear in the presence of light absorbing impurities, where the impurity is dictating the color rather than the ice itself. For instance, icebergs containing impurities (e.g., sediments, algae, air bubbles) can appear brown, grey or green.\n\nIce may be any one of the 18 known solid crystalline phases of water, or in an amorphous solid state at various densities.\n\nMost liquids under increased pressure freeze at \"higher\" temperatures because the pressure helps to hold the molecules together. However, the strong hydrogen bonds in water make it different: For some pressures higher than , water freezes at a temperature \"below\" 0 °C, as shown in the phase diagram below. The melting of ice under high pressures is thought to contribute to the movement of glaciers.\n\nIce, water, and water vapour can coexist at the triple point, which is exactly 273.16 K (0.01 °C) at a pressure of 611.657 Pa. The kelvin is in fact defined as of the difference between this triple point and absolute zero. Unlike most other solids, ice is difficult to superheat. In an experiment, ice at −3 °C was superheated to about 17 °C for about 250 picoseconds.\n\nSubjected to higher pressures and varying temperatures, ice can form in 18 separate known crystalline phases. With care, at least 15 of these phases (one of the known exceptions being ice X) can be recovered at ambient pressure and low temperature in metastable form. The types are differentiated by their crystalline structure, proton ordering, and density. There are also two metastable phases of ice under pressure, both fully hydrogen-disordered; these are IV and XII. Ice XII was discovered in 1996. In 2006, XIII and XIV were discovered. Ices XI, XIII, and XIV are hydrogen-ordered forms of ices I, V, and XII respectively. In 2009, ice XV was found at extremely high pressures and −143 °C. At even higher pressures, ice is predicted to become a metal; this has been variously estimated to occur at 1.55 TPa or 5.62 TPa.\n\nAs well as crystalline forms, solid water can exist in amorphous states as amorphous ice (ASW) of varying densities. Water in the interstellar medium is dominated by amorphous ice, making it likely the most common form of water in the universe. Low-density ASW (LDA), also known as hyperquenched glassy water, may be responsible for noctilucent clouds on Earth and is usually formed by deposition of water vapor in cold or vacuum conditions. High-density ASW (HDA) is formed by compression of ordinary ice I or LDA at GPa pressures. Very-high-density ASW (VHDA) is HDA slightly warmed to 160K under 1–2 GPa pressures.\n\nIn outer space, hexagonal crystalline ice (the predominant form found on Earth) is extremely rare. Amorphous ice is more common; however, hexagonal crystalline ice can be formed by volcanic action.\n\nThe low coefficient of friction (\"slipperiness\") of ice has been attributed to the pressure of an object coming into contact with the ice, melting a thin layer of the ice and allowing the object to glide across the surface. For example, the blade of an ice skate, upon exerting pressure on the ice, would melt a thin layer, providing lubrication between the ice and the blade. This explanation, called \"pressure melting\", originated in the 19th century. It, however, did not account for skating on ice temperatures lower than , which is often skated upon.\n\nA second theory describing the coefficient of friction of ice suggested that ice molecules at the interface cannot properly bond with the molecules of the mass of ice beneath (and thus are free to move like molecules of liquid water). These molecules remain in a semi-liquid state, providing lubrication regardless of pressure against the ice exerted by any object. However, the significance of this hypothesis is disputed by experiments showing a high coefficient of friction for ice using atomic force microscopy.\n\nA third theory is \"friction heating\", which suggests that friction of the material is the cause of the ice layer melting. However, this theory does not sufficiently explain why ice is slippery when standing still even at below-zero temperatures.\n\nA comprehensive theory of ice friction takes into account all the above-mentioned friction mechanisms. This model allows quantitative estimation of the friction coefficient of ice against various materials as a function of temperature and sliding speed. In typical conditions related to winter sports and tires of a vehicle on ice, melting of a thin ice layer due to the frictional heating is the primary reason for the slipperiness.\n\nThe term that collectively describes all of the parts of the Earth's surface where water is in frozen form is the \"cryosphere.\" Ice is an important component of the global climate, particularly in regard to the water cycle. Glaciers and snowpacks are an important storage mechanism for fresh water; over time, they may sublimate or melt. Snowmelt is an important source of seasonal fresh water. The World Meteorological Organization defines several kinds of ice depending on origin, size, shape, influence and so on. Clathrate hydrates are forms of ice that contain gas molecules trapped within its crystal lattice.\n\nIce that is found at sea may be in the form of drift ice floating in the water, fast ice fixed to a shoreline or anchor ice if attached to the sea bottom. Ice which calves (breaks off) from an ice shelf or glacier may become an iceberg. Sea ice can be forced together by currents and winds to form pressure ridges up to tall. Navigation through areas of sea ice occurs in openings called \"polynyas\" or \"leads\" or requires the use of a special ship called an \"icebreaker\".\n\nIce on land ranges from the largest type called an \"ice sheet\" to smaller ice caps and ice fields to glaciers and ice streams to the snow line and snow fields.\n\nAufeis is layered ice that forms in Arctic and subarctic stream valleys. Ice, frozen in the stream bed, blocks normal groundwater discharge, and causes the local water table to rise, resulting in water discharge on top of the frozen layer. This water then freezes, causing the water table to rise further and repeat the cycle. The result is a stratified ice deposit, often several meters thick.\n\nFreezing rain is a type of winter storm called an ice storm where rain falls and then freezes producing a glaze of ice. Ice can also form icicles, similar to stalactites in appearance, or stalagmite-like forms as water drips and re-freezes.\n\nThe term \"ice dam\" has three meanings (others discussed below). On structures, an ice dam is the buildup of ice on a sloped roof which stops melt water from draining properly and can cause damage from water leaks in buildings.\n\nIce which forms on moving water tends to be less uniform and stable than ice which forms on calm water. Ice jams (sometimes called \"ice dams\"), when broken chunks of ice pile up, are the greatest ice hazard on rivers. Ice jams can cause flooding, damage structures in or near the river, and damage vessels on the river. Ice jams can cause some hydropower industrial facilities to completely shut down. An ice dam is a blockage from the movement of a glacier which may produce a proglacial lake. Heavy ice flows in rivers can also damage vessels and require the use of an icebreaker to keep navigation possible.\n\nIce discs are circular formations of ice surrounded by water in a river.\n\nPancake ice is a formation of ice generally created in areas with less calm conditions.\n\nIce forms on calm water from the shores, a thin layer spreading across the surface, and then downward. Ice on lakes is generally four types: Primary, secondary, superimposed and agglomerate. Primary ice forms first. Secondary ice forms below the primary ice in a direction parallel to the direction of the heat flow. Superimposed ice forms on top of the ice surface from rain or water which seeps up through cracks in the ice which often settles when loaded with snow.\n\nShelf ice occurs when floating pieces of ice are driven by the wind piling up on the windward shore.\n\nCandle ice is a form of rotten ice that develops in columns perpendicular to the surface of a lake.\n\nRime is a type of ice formed on cold objects when drops of water crystallize on them. This can be observed in foggy weather, when the temperature drops during the night. Soft rime contains a high proportion of trapped air, making it appear white rather than transparent, and giving it a density about one quarter of that of pure ice. Hard rime is comparatively dense.\n\nIce pellets are a form of precipitation consisting of small, translucent balls of ice. This form of precipitation is also referred to as \"sleet\" by the United States National Weather Service. (In Commonwealth English \"sleet\" refers to a mixture of rain and snow). Ice pellets are usually smaller than hailstones. They often bounce when they hit the ground, and generally do not freeze into a solid mass unless mixed with freezing rain. The METAR code for ice pellets is \"PL\".\n\nIce pellets form when a layer of above-freezing air is located between above the ground, with sub-freezing air both above and below it. This causes the partial or complete melting of any snowflakes falling through the warm layer. As they fall back into the sub-freezing layer closer to the surface, they re-freeze into ice pellets. However, if the sub-freezing layer beneath the warm layer is too small, the precipitation will not have time to re-freeze, and freezing rain will be the result at the surface. A temperature profile showing a warm layer above the ground is most likely to be found in advance of a warm front during the cold season, but can occasionally be found behind a passing cold front.\n\nLike other precipitation, hail forms in storm clouds when supercooled water droplets freeze on contact with condensation nuclei, such as dust or dirt. The storm's updraft blows the hailstones to the upper part of the cloud. The updraft dissipates and the hailstones fall down, back into the updraft, and are lifted up again. Hail has a diameter of or more. Within METAR code, GR is used to indicate larger hail, of a diameter of at least and GS for smaller. Stones just larger than golf ball-sized are one of the most frequently reported hail sizes. Hailstones can grow to and weigh more than . In large hailstones, latent heat released by further freezing may melt the outer shell of the hailstone. The hailstone then may undergo 'wet growth', where the liquid outer shell collects other smaller hailstones. The hailstone gains an ice layer and grows increasingly larger with each ascent. Once a hailstone becomes too heavy to be supported by the storm's updraft, it falls from the cloud.\n\nHail forms in strong thunderstorm clouds, particularly those with intense updrafts, high liquid water content, great vertical extent, large water droplets, and where a good portion of the cloud layer is below freezing . Hail-producing clouds are often identifiable by their green coloration. The growth rate is maximized at about , and becomes vanishingly small much below as supercooled water droplets become rare. For this reason, hail is most common within continental interiors of the mid-latitudes, as hail formation is considerably more likely when the freezing level is below the altitude of . Entrainment of dry air into strong thunderstorms over continents can increase the frequency of hail by promoting evaporational cooling which lowers the freezing level of thunderstorm clouds giving hail a larger volume to grow in. Accordingly, hail is actually less common in the tropics despite a much higher frequency of thunderstorms than in the mid-latitudes because the atmosphere over the tropics tends to be warmer over a much greater depth. Hail in the tropics occurs mainly at higher elevations.\n\nSnow crystals form when tiny supercooled cloud droplets (about 10 μm in diameter) freeze. These droplets are able to remain liquid at temperatures lower than , because to freeze, a few molecules in the droplet need to get together by chance to form an arrangement similar to that in an ice lattice; then the droplet freezes around this \"nucleus.\" Experiments show that this \"homogeneous\" nucleation of cloud droplets only occurs at temperatures lower than . In warmer clouds an aerosol particle or \"ice nucleus\" must be present in (or in contact with) the droplet to act as a nucleus. Our understanding of what particles make efficient ice nuclei is poor – what we do know is they are very rare compared to that cloud condensation nuclei on which liquid droplets form. Clays, desert dust and biological particles may be effective, although to what extent is unclear. Artificial nuclei are used in cloud seeding. The droplet then grows by condensation of water vapor onto the ice surfaces.\n\nSo-called \"diamond dust\", also known as ice needles or ice crystals, forms at temperatures approaching due to air with slightly higher moisture from aloft mixing with colder, surface-based air. The METAR identifier for diamond dust within international hourly weather reports is \"IC\".\n\nAblation of ice refers to both its melting and its dissolution.\n\nIn fresh ambient melting describes a phase transition from solid to liquid.\n\nTo melt ice means breaking the hydrogen bonds between the water molecules. The ordering of the molecules in the solid breaks down to a less ordered state and the solid melts to become a liquid. This is achieved by increasing the internal energy of the ice beyond the melting point. When ice melts it absorbs as much energy as would be required to heat an equivalent amount of water by 80 °C. While melting, the temperature of the ice surface remains constant at 0 °C. The velocity of the melting process depends on the efficiency of the energy exchange process. An ice surface in fresh water melts solely by free convection with a velocity that depends linearly on the water temperature, \"T\", when \"T\" is less than 3.98 °C, and superlinearly when \"T\" is equal to or greater than 3.98 °C, with the rate being proportional to (T − 3.98 °C), with \"α\" =  for \"T\" much greater than 8 °C, and α =  for inbetween temperatures \"T\".\n\nIn salty ambient conditions, dissolution rather than melting often causes the ablation of ice. For example, the temperature of the Arctic Ocean is generally below the melting point of ablating sea ice. The phase transition from solid to liquid is achieved by mixing salt and water molecules, similar to the dissolution of sugar in water, even though the water temperature is far below the melting point of the sugar. Hence dissolution is rate limited by salt transport whereas melting can occur at much higher rates that are characteristic for heat transport.\n\nHumans have used ice for cooling and food preservation for centuries, relying on harvesting natural ice in various forms and then transitioning to the mechanical production of the material. Ice also presents a challenge to transportation in various forms and a setting for winter sports.\n\nIce has long been valued as a means of cooling. In 400 BC Iran, Persian engineers had already mastered the technique of storing ice in the middle of summer in the desert. The ice was brought in during the winters from nearby mountains in bulk amounts, and stored in specially designed, naturally cooled \"refrigerators\", called yakhchal (meaning \"ice storage\"). This was a large underground space (up to 5000 m) that had thick walls (at least two meters at the base) made of a special mortar called \"sarooj\", composed of sand, clay, egg whites, lime, goat hair, and ash in specific proportions, and which was known to be resistant to heat transfer. This mixture was thought to be completely water impenetrable. The space often had access to a qanat, and often contained a system of windcatchers which could easily bring temperatures inside the space down to frigid levels on summer days. The ice was used to chill treats for royalty.\n\nThere were thriving industries in 16th/17th century England whereby low-lying areas along the Thames Estuary were flooded during the winter, and ice harvested in carts and stored inter-seasonally in insulated wooden houses as a provision to an icehouse often located in large country houses, and widely used to keep fish fresh when caught in distant waters. This was allegedly copied by an Englishman who had seen the same activity in China. Ice was imported into England from Norway on a considerable scale as early as 1823.\n\nIn the United States, the first cargo of ice was sent from New York City to Charleston, South Carolina in 1799, and by the first half of the 19th century, ice harvesting had become big business. Frederic Tudor, who became known as the \"Ice King\", worked on developing better insulation products for the long distance shipment of ice, especially to the tropics; this became known as the ice trade.\n\nTrieste sent ice to Egypt, Corfu, and Zante; Switzerland sent it to France; and Germany sometimes was supplied from Bavarian lakes. The Hungarian Parliament building used ice harvested in the winter from Lake Balaton for air conditioning.\n\nIce houses were used to store ice formed in the winter, to make ice available all year long, and early refrigerators were known as iceboxes, because they had a block of ice in them. In many cities, it was not unusual to have a regular ice delivery service during the summer. The advent of artificial refrigeration technology has since made delivery of ice obsolete.\n\nIce is still harvested for ice and snow sculpture events. For example, a swing saw is used to get ice for the Harbin International Ice and Snow Sculpture Festival each year from the frozen surface of the Songhua River.\n\nIce is now produced on an industrial scale, for uses including food storage and processing, chemical manufacturing, concrete mixing and curing, and consumer or packaged ice. Most commercial icemakers produce three basic types of fragmentary ice: flake, tubular and plate, using a variety of techniques. Large batch ice makers can produce up to 75 tons of ice per day. In 2002, there were 426 commercial ice-making companies in the United States, with a combined value of shipments of $595,487,000. Home refrigerators can also make ice with a built in icemaker, which will typically make ice cubes or crushed ice. Stand-alone icemaker units that make ice cubes are often called ice machines.\n\nIce can present challenges to safe transportation on land, sea and in the air.\n\nIce forming on roads is a dangerous winter hazard. Black ice is very difficult to see, because it lacks the expected frosty surface. Whenever there is freezing rain or snow which occurs at a temperature near the melting point, it is common for ice to build up on the windows of vehicles. Driving safely requires the removal of the ice build-up. Ice scrapers are tools designed to break the ice free and clear the windows, though removing the ice can be a long and laborious process.\n\nFar enough below the freezing point, a thin layer of ice crystals can form on the inside surface of windows. This usually happens when a vehicle has been left alone after being driven for a while, but can happen while driving, if the outside temperature is low enough. Moisture from the driver's breath is the source of water for the crystals. It is troublesome to remove this form of ice, so people often open their windows slightly when the vehicle is parked in order to let the moisture dissipate, and it is now common for cars to have rear-window defrosters to solve the problem. A similar problem can happen in homes, which is one reason why many colder regions require double-pane windows for insulation.\n\nWhen the outdoor temperature stays below freezing for extended periods, very thick layers of ice can form on lakes and other bodies of water, although places with flowing water require much colder temperatures. The ice can become thick enough to drive onto with automobiles and trucks. Doing this safely requires a thickness of at least 30 cm (one foot).\n\nFor ships, ice presents two distinct hazards. Spray and freezing rain can produce an ice build-up on the superstructure of a vessel sufficient to make it unstable, and to require it to be hacked off or melted with steam hoses. And icebergs – large masses of ice floating in water (typically created when glaciers reach the sea) – can be dangerous if struck by a ship when underway. Icebergs have been responsible for the sinking of many ships, the most famous being the \"Titanic\". For harbors near the poles, being ice-free is an important advantage. Ideally, all year long. Examples are Murmansk (Russia), Petsamo (Russia, formerly Finland) and Vardø (Norway). Harbors which are not ice-free are opened up using icebreakers.\n\nFor aircraft, ice can cause a number of dangers. As an aircraft climbs, it passes through air layers of different temperature and humidity, some of which may be conducive to ice formation. If ice forms on the wings or control surfaces, this may adversely affect the flying qualities of the aircraft. During the first non-stop flight across the Atlantic, the British aviators Captain John Alcock and Lieutenant Arthur Whitten Brown encountered such icing conditions – Brown left the cockpit and climbed onto the wing several times to remove ice which was covering the engine air intakes of the Vickers Vimy aircraft they were flying.\n\nOne vulnerability effected by icing that is associated with reciprocating internal combustion engines is the carburetor. As air is sucked through the carburetor into the engine, the local air pressure is lowered, which causes adiabatic cooling. Thus, in humid near-freezing conditions, the carburetor will be colder, and tend to ice up. This will block the supply of air to the engine, and cause it to fail. For this reason, aircraft reciprocating engines with carburetors are provided with carburetor air intake heaters. The increasing use of fuel injection—which does not require carburetors—has made \"carb icing\" less of an issue for reciprocating engines.\n\nJet engines do not experience carb icing, but recent evidence indicates that they can be slowed, stopped, or damaged by internal icing in certain types of atmospheric conditions much more easily than previously believed. In most cases, the engines can be quickly restarted and flights are not endangered, but research continues to determine the exact conditions which produce this type of icing, and find the best methods to prevent, or reverse it, in flight.\n\nIce also plays a central role in winter recreation and in many sports such as ice skating, tour skating, ice hockey, bandy, ice fishing, ice climbing, curling, broomball and sled racing on bobsled, luge and skeleton. Many of the different sports played on ice get international attention every four years during the Winter Olympic Games.\n\nA sort of sailboat on blades gives rise to ice yachting. Another sport is ice racing, where drivers must speed on lake ice, while also controlling the skid of their vehicle (similar in some ways to dirt track racing). The sport has even been modified for ice rinks.\n\n\n\nThe solid phases of several other volatile substances are also referred to as \"ices\"; generally a volatile is classed as an ice if its melting point lies above or around 100 K. The best known example is dry ice, the solid form of carbon dioxide.\n\nA \"magnetic analogue\" of ice is also realized in some insulating magnetic materials in which the magnetic moments mimic the position of protons in water ice and obey energetic constraints similar to the Bernal-Fowler ice rules arising from the geometrical frustration of the proton configuration in water ice. These materials are called spin ice.\n\n"}
{"id": "2061993", "url": "https://en.wikipedia.org/wiki?curid=2061993", "title": "Impurity", "text": "Impurity\n\nImpurities are chemical substances inside a confined amount of liquid, gas, or solid, which differ from the chemical composition of the material or compound.\n\nImpurities are either naturally occurring or added during synthesis of a chemical or commercial product. During production, impurities may be purposely, accidentally, inevitably, or incidentally added into the substance.\n\nThe levels of impurities in a material are generally defined in relative terms. Standards have been established by various organizations that attempt to define the permitted levels of various impurities in a manufactured product. Strictly speaking, then a material's level of purity can only be stated as being more or less pure than some other material.\n\nImpurities can be destructive when they obstruct the working nature of the material. Examples include ash and debris in metals and leaf pieces in blank white papers. The removal of impurities is usually done chemically. For example, in the manufacturing of iron, calcium carbonate is added to the blast furnace to remove silicon dioxide from the iron ore. Zone refining is an economically important method for the purification of semiconductors.\n\nHowever, some kinds of impurities can be removed by physical means. A mixture of water and salt can be separated by distillation, with water as the distillate and salt as the solid residue. Impurities are usually physically removed from liquids and gases. Removal of sand particles from metal ore is one example with solids.\n\nNo matter what method is used, it is usually impossible to separate an impurity completely from a material. The reason that it is impossible to remove impurities completely is of thermodynamic nature and is predicted by the second law of thermodynamics. Removing impurities completely means reducing the entropy of the system to zero. This would require an infinite amount of work and energy as predicted by the second law of thermodynamics. What technicians can do is to increase the purity of a material to as near 100% as possible or economically feasible.\n\nWhen an impure liquid is cooled to its melting point the liquid, undergoing a phase transition, crystallizes around the impurities and becomes a crystalline solid. If there are no impurities then the liquid is said to be pure and can be supercooled below its melting point without becoming a solid. This occurs because the liquid has nothing to condense around so the solid cannot form a natural crystalline solid. The solid is eventually formed when dynamic arrest or glass transition occurs, but it forms into an amorphous solid – a glass, instead, as there is no long-range order in the structure.\n\nImpurities play an important role in the nucleation of other phase transitions. For example, the presence of foreign elements may have important effects on the mechanical and magnetic properties of metal alloys. Iron atoms in copper cause the renowned Kondo effect where the conduction electron spins form a magnetic bound state with the impurity atom. Magnetic impurities in superconductors can serve as generation sites for vortex defects. Point defects can nucleate reversed domains in ferromagnets and dramatically affect their coercivity. In general impurities are able to serve as initiation points for phase transitions because the energetic cost of creating a finite-size domain of a new phase is lower at a point defect. In order for the nucleus of a new phase to be stable, it must reach a critical size. This threshold size is often lower at an impurity site.\n\n\n"}
{"id": "1943983", "url": "https://en.wikipedia.org/wiki?curid=1943983", "title": "International Commission for the Conservation of Atlantic Tunas", "text": "International Commission for the Conservation of Atlantic Tunas\n\nThe International Commission for the Conservation of Atlantic Tunas (ICCAT) is an intergovernmental organization responsible for the management and conservation of tuna and tuna-like species in the Atlantic Ocean and adjacent seas. The organization was established in 1969, at a conference in Rio de Janeiro, Brazil, and operates in English, French and Spanish. The organisation has been strongly criticised by scientists for its repeated failure to conserve the sustainability of the tuna fishery by consistently supporting over-fishing – an internal review branded ICCAT's policies on the eastern Atlantic bluefin tuna fishery a \"travesty of fisheries management\", and an \"international disgrace\". Conservationists often refer to ICCAT as \"The International Conspiracy to Catch All Tuna\".\n\nHowever, in recent years the organization seems to be turning around. For the most iconic species within its management, the Eastern Bluefin Tuna, a very strict recovery plan was adopted. It is too early to judge its final outcome, but initial indications are encouraging. In general, ICCAT contracting parties seem to have agreed to steer the organization into a direction of relying on sound science, insisting on compliance and following a good governance model.\n\nTuna and tuna-like fishes are highly migratory, and stocks cross numerous international boundaries. ICCAT is involved in management of 30 species, including the Atlantic bluefin (\"Thunnus thynnus thynnus\"), yellowfin (\"T. albacares\"), albacore (\"T. alalunga\") and bigeye tuna (\"T. obesus\"); from the billfishes, swordfish (\"Xiphias gladius\"), white marlin (\"Tetrapturus albidus\"), blue marlin (\"Makaira nigricans\"), sailfish (\"Istiophorus albicans\"); mackerels such as spotted Spanish mackerel (\"Scomberomorus maculatus\") and king mackerel (\"S. cavalla\"); and, small tunas like skipjack tuna (\"Katsuwonus pelamis\").\n\nScientists participating in ICCAT carry out studies on biometry, fisheries ecology, and oceanography, focusing on the effects of fishing on tuna stock abundance. They also collect and analyse fisheries statistics which are relative to conditions the management of resources. ICCAT is also involved in work on data for other fish species that are caught during tuna fishing (\"bycatch\" - principally sharks) in the Atlantic and surrounding area, and which are not investigated by another international fishery organization.\n\nBased on scientific and other information, such as fishery statistics and stock assessments provided by members, each year the Commission decides on conservation and management measures aimed at maintaining target stocks at levels that permit the maximum sustainable catch for food and other purposes.\n\nICCAT is widely criticised by environmental bodies for having short-term policies that favour fisherman over the long-term conservation of the species. \n\nIn November 2008, ICCAT ignored the advice of their scientists that quotas should not exceed 15,000 tonnes per year – which had been determined as the maximum sustainable yield – and instead set quotas at 22,000 tonnes. An independent review of ICCAT, commissioned by the organisation themselves, concluded that their policies on the eastern Atlantic bluefin tuna fishery are a \"travesty of fisheries management\", and an \"international disgrace\". Dr Sergi Tudela, head of WWF Mediterranean's fisheries programme, said \"Today's outcome is a recipe for economic as well as biological bankruptcy with the European Union squarely to blame. ICCAT's string of successive failures leaves us little option now but to seek effective remedies through trade measures and extending the boycott of retailers, restaurants, chefs and consumers\".\n\nIn November 2009, ICCAT's scientific advisors announced that a total ban on international trade in Atlantic tuna was justified, based on the decline in the tuna fishery population to less than 15% of its original size. However, later in the same month, ICCAT recommended catch quotas of 13,500 tonnes per year. This was met by sharp criticisms from environmental organisations, and prompted calls for alternative methods to regulate Atlantic tuna fisheries, such as protection under CITES. Susan Lieberman, Director of International Policy for the Pew Environment Group said Since its inception, the International Commission for the Conservation of Atlantic Tunas has been driven by short-term commercial fishing interests, not the conservation ethic implied by its name...ICCAT's actions and inactions highlight the need to take these issues to CITES—the Convention on International Trade in Endangered Species. The ICCAT fisheries managers have shown scant interest in the long-term preservation of the key resources they are supposed to manage. It is now time to turn to other bodies to seek the needed protections that ICCAT has failed to provide.\nThe US National Oceanic and Atmospheric Administration (NOAA) released a statement with strongly worded criticism, saying that the new agreement was \"a marked improvement over the current rules, but it is insufficient to guarantee the long-term viability of either the fish or the fishery\".\n\nIn recent years ICCAT adopted a draconic recovery plan for Eastern Bluefin Tuna, which led to the reduction of the total allowable catches from 27,500 in 2007 to 13,400 tons in 2014.\n\nApart from reduced total allowable catches, the recovery plan also introduced strict monitoring, reporting and control measures Over the years the plan seemed to produce results and recently earlier critics have welcome the plan and the action undertaken by ICCAT.\n\nIn November 2012 Susan Lieberman, international policy director of Pew Environment Group stated that \"It is encouraging that ICCAT listened to the recommendations of its own scientists and agreed to keep catch limits for bluefin tuna within their advice. This decision will give this depleted species a fighting chance to continue on the path to recovery after decades of overfishing and mismanagement\".\nIn November 2013 Dr Sergi Tudela, Head of Fisheries at WWF Mediterranean said: “WWF congratulates ICCAT member countries for sticking to science again this year regarding bluefin tuna quotas in the East Atlantic and Mediterranean. This is a good sign for the credibility of ICCAT. However, failure to address countries’ failure to comply with rules remains an issue of grave concern”.\n"}
{"id": "9787184", "url": "https://en.wikipedia.org/wiki?curid=9787184", "title": "International Journal of Speleology", "text": "International Journal of Speleology\n\nThe International Journal of Speleology is since 1978 the official peer-reviewed scientific journal of the \"Union Internationale de Spéléologie\". Since 1981 it has been published by the Società Speleologica Italiana.\n\nThe \"International Journal of Speleology\" is divided into four sections: Botany-Microbiology, Zoology, Geology-Geomorphology, and Abstract-News. The first issue was edited by G. Claus (US), subsequently two other editors were added: R. Husson (France) and G Nicholas (USA). The first two volumes were published by the J. Kramer Verlag (Germany), followed by Swets & Zeitlinger N.V. (The Netherlands).\n\nIn 1972 R. Husson became editor in chief and nine volumes were issued in the period 1964 to 1977 until the cost of printing requested a radical change. In 1978 the journal became the official journal of the Union Internationale de Spéléologie and it was published in Italy where the cost of printing was lower.\n\nIn 1981 V. Sbordoni (Italy) became editor in chief. In the meantime the Società Speleologica Italiana acquired the property of the journal which was printed with a contribution of the Italian Ministry for Culture and Environment, the Consiglio Nazionale delle Ricerche, and the University of Trieste.\n\nIn the second half of the 1990s the Museo di Speleologia \"V. Rivera\" also contributed to the financial support of the journal. The issues of the journal were alternatively dedicated to biospeleology and physical speleology until 1998 when the journal was split into two different series: \"A\" for Biospeleology (which ceased publication in 2003) and \"B\" for Physical Speleology.\n\nIn addition to the regular issues of the journal, in 1996 a \"Manual for Karst water analysis\" by W.E. Krawczyk was printed.\n\n\n"}
{"id": "1197312", "url": "https://en.wikipedia.org/wiki?curid=1197312", "title": "Lake Manasarovar", "text": "Lake Manasarovar\n\nLake Manasarovar (; Chinese: 玛旁雍錯 , 瑪旁雍錯 also called Mapam Yumtso, is a high altitude freshwater lake fed by the Kailash Glaciers near Mount Kailash in the Tibet. The lake is revered a sacred place in four religions: Hinduism, Bön, Buddhism and Jainism.\n\nThe Sanskrit word \"\"Manasarovar\" (मानसरोवर) is a combination of two Sanskrit words; \"Mánas\" (मानस) meaning \"mind (in its widest sense as applied to all the mental powers), intellect, intelligence, understanding, perception, sense, conscience\" while \"sarovara\" (सरोवर) means \"a lake or large pond\"\".\n\nLake Manasarovar lies at above mean sea level, a relatively high elevation for a large freshwater lake on the mostly saline lake-studded Tibetan Plateau.\n\nLake Manasa sarovar is relatively round in shape with the circumference of . Its depth reaches a maximum depth of and its surface area is . It is connected to nearby Lake Rakshastal by the natural Ganga Chhu channel. Lake Manasarovar is near the source of the Sutlej, which is the easternmost large tributary of the Sindhu. Nearby are the sources of the Brahmaputra River, the Indus River, and the Ghaghara, an important tributary of the Ganges.\n\nLake Manasarovar overflows into Lake Rakshastal which is a salt-water endorheic lake. These lakes used to be part of the Sutlej basin and were separated due to tectonic activity.\n\nAccording to Hinduism, the lake was first created in the mind of the Lord Brahma after which it manifested on Earth. In Hinduism, Lake Manasarovar is a personification of purity, and one who drinks water from the lake will go to the abode of Shiva after death. He is believed to be cleansed of all his sins committed over even a hundred lifetimes.\n\nLike Mount Kailash, Lake Manasarovar is a place of pilgrimage, attracting religious people from India, Nepal, Tibet and neighboring countries. Bathing in Manasarovar and drinking its water is believed by Hindus to cleanse all sins. Pilgrimage tours are organized regularly, especially from India, the most famous of which is the yearly \"Kailash Manas Sarovar Yatra\". Pilgrims come to take ceremonial baths in the waters of the lake.\n\nLake Manasarovar has long been viewed by the pilgrims as being nearby to the sources of four great rivers of Asia, namely the Brahmaputra, Ghaghara, Sindhu and Sutlej, thus it is an axial point which has been thronged to by pilgrims for thousands of years. The region was closed to pilgrims from the outside following the Battle of Chamdo; no foreigners were allowed between 1951 and 1980. After the 1980s it has again become a part of the Indian pilgrim trail.\n\nAccording to the Hinduism, the lake was first created in the mind of Brahma after which it manifested on Earth. Hence it is called \"Manasa sarovaram\", which is a combination of the Sanskrit words for \"mind\" and \"lake\". The lake is also supposed to be the summer abode of the hamsa. Considered to be sacred, the hamsa is an important element in the symbology of the subcontinent, representing wisdom and beauty.\n\nAccording to Hindu theology, there are five sacred lakes; collectively called \"Panch-Sarovar\"; Mansarovar, Bindu Sarovar, Narayan Sarovar, Pampa Sarovar and Pushkar Sarovar. They are also mentioned in Shrimad Bhagavata Purana.\n\nThe People belong to this region is called Manasarovariya.Most of Them follow Hindu Religion are belong to Koli tribe called Manasarovariya Patel or Mandhata Patel claimed that their tribe belong to ancient King Mandhata of suryavansha of Ikshvaku dynasty and There is mountain named after his name called Gurla Mandhata is the highest peak of the Nalakankar Himal for glorify his achievement.\n\nThe Bon religion is also associated with the holy place of Zhang Zhung Meri sacred deity. When Tonpa Shenrab, the founder of the Bon religion, visited Tibet for the first time – from Tagzig Wolmo Lungring – he washed in the lake.\n\nBuddhists associate the lake with the legendary lake Anavatapta (Sanskrit; Pali \"Anotatta\") where Maya is believed to have conceived Buddha. The lake has a few monasteries on its shores, the most notable of which is the ancient Chiu Monastery built on a steep hill, looking as if it has been carved right out of the rock.\n\nThe lake is very popular in Buddhist literature and associated with many teachings and stories. Buddha, it is reported, stayed and meditated near this lake on several occasions. Lake Manasarovar is also the subject of the meditative Tibetan tradition, \"The Jewel of Tibet\". A modern narration and description of the meditation was made popular by Robert Thurman.\n\nIn Jainism, Lake Manasarovar is associated with the first Tirthankara, Rishabha. As per Jain scriptures, the first Tirthankar, Bhagwan Rushabhdev, had attained nirvana on the Ashtapad Mountain. The son of Bhagwan Rishabhdev, Chakravati Bharat, had built a palace adorned with gems on the Ashtapad Mountain located in the serene Himalayas.There are many stories related to Ashtapad Maha Tirth like Kumar and Sagar's sons, Tapas Kher Parna, Ravan and Mandodri Bhakti, among many others.\n"}
{"id": "7652313", "url": "https://en.wikipedia.org/wiki?curid=7652313", "title": "Leaf angle distribution", "text": "Leaf angle distribution\n\nThe Leaf Angle Distribution (or LAD) of a plant canopy refers to the mathematical description of the angular orientation of the leaves in the vegetation. Specifically, if each leaf is conceptually represented by a small flat plate, its orientation can be described with the zenith and the azimuth angles of the surface normal to that plate. If the leaf has a complex structure and is not flat, it may be necessary to approximate the actual leaf by a set of small plates, in which case there may be a number of leaf normals and associated angles. The LAD describes the statistical distribution of these angles.\n\nDifferent plant canopies exhibit different LADs: For instance, grasses and willows have their leaves largely hanging vertically (such plants are said to have an erectophile LAD), while oaks tend to maintain their leaves more or less horizontally (these species are known as having a planophile LAD). In some tree species, leaves near the top of the canopy follow an erectophile LAD while those at the bottom of the canopy are more planophile. This may be interpreted as a strategy by that plant species to maximize exposure to light, an important constraint to growth and development. Yet other species (notably sunflower) are capable of reorienting their leaves throughout the day to optimize exposure to the Sun: this is known as heliotropism.\n\nThe LAD of a plant canopy has a significant impact on the reflectance, transmittance and absorption of solar light in the vegetation layer, and thus also on its growth and development. LAD can also serve as a quantitative index to monitor the state of the plants, as wilting usually results in more erectophile LADs. Models of radiation transfer need to take this distribution into account to predict, for instance, the albedo or the productivity of the canopy.\n\nAccurately measuring the statistical properties of leaf angle distributions is not a trivial matter, especially for small leaves. Clinometers can be used but may be rather bulky or inconvenient. Most leaves are quite light and tend to move with the slightest breeze or air turbulence. Nevertheless, when these environmental conditions are suitable or can be controlled, it is possible to acquire data on leaf orientation.\n\nThis may be done, for instance, with a Spatial Coordinate Apparatus, which is an articulated mechanical device capable or recording the position in three-dimensional space of three separate points forming a small triangle. The orientation of the triangle is computed from these coordinates.\n\nYet another approach is to use a laser scanner: this instrument can record the angular and distance coordinates of the intersection of a laser beam with objects within its range. The LAD of canopy leaves can be derived from such measurements.\n\nExtensive data sets of LAD have been recorded, especially during intensive field campaigns, such as the First ISLSCP Field Experiment (FIFE) or the SAFARI 2000 field campaign. See the external links below for getting access to these data.\n\nIn general, LAD are modelled with one- or two-parameters functions including ellipsoidal, rotated-ellipsoidal and Beta functions. Comparison between different LAD functions with in-situ measurements show that, two-parameter functions (especially Beta function) may perform better than one-parameter functions.\n\n\nNon accessible as of July 3, 2018:\nhttp://www.daac.ornl.gov/FIFE/Datasets/Vegetation/Leaf_Angle_Data.html\nNon accessible as of July 3, 2018:\nhttp://www.daac.ornl.gov/S2K/guides/kt_canopy_structure.html\n"}
{"id": "2811275", "url": "https://en.wikipedia.org/wiki?curid=2811275", "title": "List of California hurricanes", "text": "List of California hurricanes\n\n<onlyinclude>A California hurricane is a tropical cyclone that affects the state of California. Usually, only the remnants of tropical cyclones affect California. Since 1900, only two tropical storms have hit California, one by direct landfall from offshore, another after making landfall in Mexico.</onlyinclude>\n\nSince 1850, only seven tropical cyclones have brought gale-force winds to the Southwestern United States. They are: The 1858 San Diego hurricane that was reconstructed as just missing landfall in 1858, the 1939 Long Beach tropical storm that made landfall near San Pedro in 1939, the remnants of Tropical Storm Jennifer-Katherine in 1963, the remnants of Hurricane Emily in 1965, the remnants of Hurricane Joanne in 1972, the remnants of Hurricane Kathleen in 1976, and Hurricane Nora in 1997, after it was downgraded to a tropical storm.\n\nIn most cases, rainfall is the only effect that these cyclones have on California. Sometimes, the rainfall is severe enough to cause flooding and damage. For example, floods from Hurricane Kathleen devastated Ocotillo, California and killed several people.\n\nThere are two reasons why tropical cyclones rarely strike California at tropical storm intensity or higher: sea surface temperatures, and the usual upper level steering winds in the eastern Pacific, with sea surface temperatures being more important.\n\nTropical cyclones usually require very warm water to depth, generally above 26.5 °C (80 °F) extending to a depth of 50 meters (160 ft). However, the waters off California are cold even in summer. They rarely rise above 24 °C (75 °F) in near-shore southern California, and usually remain below 17 °C (63 °F) along most of the rest of the coast and outer coastal waters, although El Niño events may warm the waters somewhat. This is due primarily to the extensive upwelling of colder sub-surface waters caused by the prevailing northwesterly winds acting through the Ekman Effect. The winds drive surface water to the right of the wind flow, that is offshore, which draws water up from below to replace it. The upwelling further cools the already cool California Current which runs north to south along coastal California and even much of coastal Baja California. This is the same mechanism which produces coastal California's characteristic fog.\n\nThe second reason is the general path of tropical cyclones in the eastern Pacific. They generally move north-westward or westward due to steering by the prevailing upper level winds, which takes them far out to sea and away from land.\n\nThese factors make eastern Pacific landfalls improbable north of about central Baja California. In those instances when upper level steering winds do allow a more northerly path, much cooler sea surface temperatures quickly weaken tropical cyclones that approach California, although torrential rainfall can still occur. For example, the September 24-hour rainfall record for Los Angeles is held by the 1939 Long Beach Tropical Storm, as of January 2007.\n\nHurricanes that affect California are mainly the remnants of hurricanes or tropical storms. In the twentieth century, only four eastern Pacific tropical cyclones have brought tropical storm-force winds to the Continental United States: the 1939 Long Beach Tropical Storm, Tropical Storm Joanne in 1972, Tropical Storm Kathleen in 1976, and Tropical Storm Nora in 1997.\n\n<onlyinclude>\n\n\n\n\n\n\n\n\n\n\nMost tropical cyclones impacting California do so in the month of September. September 1939 was \"unprecedented\" in having four tropical cyclones impact the state.\n\nThe following is a list of all known tropical cyclone-related deaths in California.\n\nWhile rare, tropical cyclones do affect California, occasionally very seriously as far as rainfall is concerned.\n\nA modern repeat of the 1858 storm is estimated to cause damages of hundreds of millions of dollars. A repeat of the 1939 tropical storm would cause around 200 million dollars in damage. The most serious damage would be due to rains rather than winds or storm surge, although distant hurricanes may still create heavy surf, possibly injuring or killing people. Nora caused millions in damage and skirted extreme southeastern California.\n\nWhen Hurricane Linda was forecast to make landfall, statements about its possible impact were issued by the Oxnard, California office of the National Weather Service. They stressed the uncertainty of a forecast that far in the future.\n\nWhen Nora was threatening, \"unprecedented coordination\" was required between the NHC and several other agencies. The coordination was \"smooth and effective\". However, no inland tropical storm warnings were issued for any area in the United States as Nora was approaching from the south.\n\nThere are seven Pacific Coast breakpoints in the United States. They are, from north to south, Point Piedras Blancas, Point Sal, Point Conception, Point Mugu, the mouth of the San Gabriel River, San Mateo Point, and the mouth of the Tijuana River (although places outside this area can be selected if conditions warrant). Should there be the threat of landfall, warnings or watches would be issued for those sections of the coast. It is highly unlikely that any tropical cyclone will threaten areas farther north, due to the stronger influence of the California Current.\n\n\n"}
{"id": "39047438", "url": "https://en.wikipedia.org/wiki?curid=39047438", "title": "List of Ramsar sites in the United States", "text": "List of Ramsar sites in the United States\n\nThis list of Ramsar sites in the United States includes wetlands that are considered to be of international importance under the Ramsar Convention. The United States currently has 38 sites designated as \"Wetlands of International Importance\" with a surface area of . For a full list of all Ramsar sites worldwide, see List of Ramsar wetlands of international importance.\n\n"}
{"id": "56836662", "url": "https://en.wikipedia.org/wiki?curid=56836662", "title": "List of Solar System objects most distant from the Sun in 2018", "text": "List of Solar System objects most distant from the Sun in 2018\n\nThis is a listing of minor planets orbiting the Sun by their approximate distance from the Sun in March 2018 (as opposed to those with the greatest calculated aphelion in their orbit). In March 2018 several new Trans-Neptunian objects (TNOs) with a current heliocentric distance greater than 50 AU were announced by Scott S. Sheppard, Chad Trujillo, and David J. Tholen.\n\nTrans-Neptunian objects are typically announced publicly months or years after their discovery, so as to make sure the orbit is correct before announcing it. Due to their greater distance from the Sun and slow movement across the sky, trans-Neptunian objects with observation arcs less than several years often have poorly constrained orbits. Particularly distant asteroids discovered in 2018 may only be announced in 2020 or later.\n\n"}
{"id": "25512546", "url": "https://en.wikipedia.org/wiki?curid=25512546", "title": "List of Superfund sites in West Virginia", "text": "List of Superfund sites in West Virginia\n\nThis is a list of Superfund sites in West Virginia designated under the Comprehensive Environmental Response, Compensation, and Liability Act (CERCLA) environmental law. The CERCLA federal law of 1980 authorized the United States Environmental Protection Agency (EPA) to create a list of polluted locations requiring a long-term response to clean up hazardous material contamination. These locations are known as Superfund sites, and are placed on the National Priorities List (NPL). \n\nThe NPL guides the Environmental Protection Agency in \"determining which sites warrant further investigation\" for environmental remediation. As of March 26, 2010, there were 9 Superfund sites on the National Priorities List in West Virginia. No additional sites are currently proposed for entry on the list. Two sites have been cleaned up and removed from the list.\n\n\n"}
{"id": "9013143", "url": "https://en.wikipedia.org/wiki?curid=9013143", "title": "List of azalea diseases", "text": "List of azalea diseases\n\nThis article is a list of diseases of azalea (\"Rhododendron\" spp.).\n\n"}
{"id": "15576818", "url": "https://en.wikipedia.org/wiki?curid=15576818", "title": "List of ecoregions in Kenya", "text": "List of ecoregions in Kenya\n\nThe following is a list of ecoregions in Kenya, as identified by the Worldwide Fund for Nature (WWF).\n\n\"by major habitat type\"\n\n\n\n\n\n\n\n\"by bioregion\"\n\n\n\n\n\n\n"}
{"id": "15566291", "url": "https://en.wikipedia.org/wiki?curid=15566291", "title": "List of ecoregions in South Africa", "text": "List of ecoregions in South Africa\n\nThe following is a list of ecoregions in South Africa, as identified by the Worldwide Fund for Nature (WWF).\n\nby major habitat type\n\n\n\n\n\n\n\n\nby bioregion\n\n\n\n\n"}
{"id": "15676779", "url": "https://en.wikipedia.org/wiki?curid=15676779", "title": "List of environmental economics journals", "text": "List of environmental economics journals\n\nThis is a list of articles about scholarly journals in ecological, resource and environmental economics.\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "3617219", "url": "https://en.wikipedia.org/wiki?curid=3617219", "title": "List of national parks of Italy", "text": "List of national parks of Italy\n\nThe Italian national parks cover about five per cent of the country's land. The parks are managed by the Ministry of the Environment based in Rome ().\n\n\n"}
{"id": "40265525", "url": "https://en.wikipedia.org/wiki?curid=40265525", "title": "Lituanica SAT-1", "text": "Lituanica SAT-1\n\nLituanicaSAT-1 is one of the two first Lithuanian satellites. It was launched along with the second Cygnus spacecraft and 28 Flock-1 CubeSats aboard an Antares 120 carrier rocket flying from Pad 0B at the Mid-Atlantic Regional Spaceport on Wallops Island to the International Space Station. The launch was scheduled to occur in December 2013, but later was rescheduled to 9 January 2014. The satellite is broadcasting greetings of Lithuanian president, Mrs. Dalia Grybauskaitė. The satellite was deployed from the International Space Station via the NanoRacks CubeSat Deployer on February 28, 2014. All LituanicaSAT-1 subsystems have been turned on, tested and proved to be working properly. The mission is considered a complete success by its team of engineers. The mission ended upon the reentry and disintegration of the satellite on July 28, 2014.\n\nThe satellite conforms to standard 1U size cubesat form factor as to the latest cubesat design specifications. The satellite does not have any active systems except the antenna deployment mechanism that is engaged 30 minutes after deployment sequence. Both attitude and thermal control sub-systems are implemented passively for simplicity and safety. The total mass of the body including the equipment within it is 1,090 g.\n\nLituanicaSAT-1 uses passive magnetic attitude control system consisting of permanent magnets that create a control torque and soft magnets that provide dampening torque using hysteresis effect. Following attitude sensors are implemented for attitude determination:\n\n\nThere are two on board computers in LituanicaSAT-1 due to redundancy requirements: the flight computer based on ARM Cortex-M4F microcontroller and secondary (back-up) computer based on an Atmel ATMega2560 microcontroller. The flight computer is the central control unit of the satellite responsible for maintaining the normal operating mode of the satellite, monitoring and control of energy resources, control of attitude determination sub-system and performance of telecommands received from the satellite ground station in Vilnius University, Lithuania. Secondary flight computer is based on Arduino. It ensures limited but safe functionality of the satellite in case of the main CPU failure and will also take and record the first pictures made from space, as well as control the radio beacon of the satellite.\n\nThe main payload is amateur radio FM mode V/U voice repeater. It operates on 145.950 MHz uplink (PL 67 Hz CTCSS) and 435.180 MHz downlink. The FM repeater subsystem identifies itself with callsign LY5N. The first filter of repeaters receiver is 15 kHz wide, second is 12 kHz. The transmit filter is set to +/- 5 kHz, and bandwidth to 10 kHz, but this width depends highly on the incoming signal width, for example if the uplink signal is 15 kHz wide, it will be cut down with 12 kHz filter. The repeater payload was engineered and developed by Žilvinas Atkočiūnas and Žilvinas Batisa. Due to temperature changes, the downlink frequency may be shifted down by 5 kHz.\n\nThe power supply sub-system includes a GomSpace Nanopower P31u power board with a lithium-ion battery and solar cells.\n\nComm subsystem consists of AX.25 transceiver and corresponding antennas. He-100 COTS transceiver is used for establishing and maintaining radio communication with the ground station. The key technical specifications of the radio transceiver are as follows:\n\n\nThere are 4 monopole antennas on LS-1: three UHF antennas and one VHF antenna. Each antenna is made of approx. 0.2 mm thick and 5 mm wide spring steel measurement tape. In deployed configuration, all UHF antennas are pointed towards the Z+ body axis direction and VHF antenna is pointed toward –Z body axis.\n\nThe satellite is commanded from Vilnius University amateur radio station, LY1BWB. Ground control software is written in Erlang and has a web interface, served by Yaws. The source code of the software is partly available for public use. Communication is done using custom protocol, which is built on top of reduced set of AX.25 and handles full-duplex data transfer, when required. Recent versions of ground control software support direct upload of binary telemetry files, which were collected and sent to ground control by amateur radio operators worldwide.\n\nOn June 8, 2014, AMSAT-NA OSCAR number administrator Bill Tynan, W3XO, stated that \"LituanicaSAT-1 has met all of the requirements for an OSCAR number\" and assigned the designation of LO-78 (LituanicaSAT-OSCAR 78) to the satellite. The team of engineers subsequently announced that it will try to keep the onboard FM repeater operative for the rest of the mission.\n\n"}
{"id": "5336226", "url": "https://en.wikipedia.org/wiki?curid=5336226", "title": "Lower Mainland Ecoregion", "text": "Lower Mainland Ecoregion\n\nThe Lower Mainland Ecoregion is the biogeoclimatic region that surrounds Vancouver, British Columbia, comprising the eastern edge of the Georgia Depression and extending from Powell River, British Columbia on the Sunshine Coast to Hope at the eastern end of the Fraser Valley. It thus corresponds, for the most part, with the popular usage of the term \"Lower Mainland.\" The Lower Mainland Ecoregion is a part of the Pacific Maritime Ecozone.\n\nThe ecoregion is bounded by the Coast and Cascade Mountains and traversed by the Fraser River. It has a unique climate, flora and fauna, geology and land use. The following description is adapted from Environment Canada's \"Ecological Framework of Canada.\"\n\nThe ecoregion extends west from the Skagit Range of the Cascade Mountains at Chilliwack to the Fraser river delta at Richmond and north to include a portion of the Georgia Lowland along the Sunshine Coast.\n\nThe mean annual temperature in the Lower Mainland is 9°C with a summer mean of 15 °C and a winter mean of 3.5 °C. Annual precipitation ranges from an annual mean of 850 mm in the west end to 2000 mm in the eastern end of the Fraser Valley and at higher elevations. Maximum precipitation occurs as rain in winter. Less than ten percent falls as snow at sea level but the amount of snowfall increases significantly with elevation.\n\nForests of Coast Douglas-fir, with an understory of salal, Oregon-grape and moss, are typical of the mature native vegetation found throughout the ecoregion. Mixed stands of Coast Douglas-fir and Western Hemlock are common, with some dogwood and arbutus occurring on drier sites. Red alder is a common pioneer species where sites have been disturbed. Wet sites support Coast Douglas-fir, Western Hemlock, and Western Redcedar. Wildlife includes black-tailed deer, coyote, raccoon, shorebirds, and waterfowl.\n\nThe ecoregion is underlain by unconsolidated glaciofluvial deposits, silty alluvium, silty and clayey marine sediments and glacial till. Bedrock outcrops of Mesozoic and Palaeozoic origin form rolling hills up to about 310 metres above sea level. The Fraser River dominates this lowland. Gleysols, Mesisols, and Humisols are the dominant wetland soils in the region, while Eutric and Dystric Brunisols and some Podzols have developed on sandy to loamy outwash and glacial till in the uplands.\n\nThe region includes a mix of urban settlement and agricultural land, comprising both the largest population centre and some of the most fertile farmland in British Columbia. Intensive agriculture occurs on the rich bottomland of the Fraser Valley where it competes with urban development. Forestry takes place on the slopes of the mountains. Coastal salt marshes provide important wildlife habitat in the Fraser delta and Boundary Bay. There are about 870 square kilometres of productive farmland in the ecoregion. \n\nThere is rapid urban and suburban growth in the Vancouver area and communities in the Fraser Valley and Sunshine Coast. The main population centre in this ecoregion is Greater Vancouver. Other centres include North Vancouver (city and district), Chilliwack, Abbotsford, and Mission. \n\nAgricultural land area remained relatively stable between 1971 and 2006, declining by less than 3%. While the number of cattle declined by 12% during this period, poultry inventories increased significantly, rising by 129%. The region accounted for 13% of Canada’s poultry production in 2006. The ecoregion's population increased 102% between 1971 and 2006 as compared to Canada's population growth of 47%. With 473 persons per square kilometre in 2006, the ecoregion was Canada’s most densely populated. The population of the ecoregion in 2006 was approximately 2.4 million people, which represents 7.6% of Canada’s population.\n\n"}
{"id": "16749537", "url": "https://en.wikipedia.org/wiki?curid=16749537", "title": "Macquarie Fault Zone", "text": "Macquarie Fault Zone\n\nThe Macquarie Fault Zone is a major right lateral-moving transform fault along the seafloor of the south Pacific Ocean which runs from New Zealand southwestward to the Macquarie Triple Junction. It is also the tectonic plate boundary between the Indo-Australian Plate to the northwest and the Pacific Plate to the southeast.\n\nThe Macquarie Fault Zone includes a component of convergence which increases as it approaches the South Island of New Zealand. Many researchers conclude that the fault zone here is an incipient subduction zone, with oblique motion corresponding to the transition from lateral (strike-slip) motion. In the area known as the Puysegur Trench, the Indo-Australian Plate appears to be starting to sink beneath the Pacific Plate, the reverse of what is occurring off of New Zealand's North Island (see Kermadec-Tonga Subduction Zone).\n\nA major geographic feature which runs along the Macquarie Fault Zone is known as the Macquarie Ridge. This ridge represents both the different relative heights of the abutting plates as well as the component of compression between the plates. The namesake Macquarie Island, named after Lachlan Macquarie lies atop a segment of the Macquarie Ridge.\n\nThe Macquarie Fault Zone merges into the Alpine Fault which cuts across the continental crust of New Zealand's South Island.\n\n"}
{"id": "20479", "url": "https://en.wikipedia.org/wiki?curid=20479", "title": "Magnetosphere", "text": "Magnetosphere\n\nA magnetosphere is a region of space surrounding an astronomical object in which charged particles are manipulated or affected by that object's magnetic field. It is created by a planet having an active interior dynamo.\n\nIn the space environment close to a planetary body, the magnetic field resembles a magnetic dipole. Further out, field lines can be significantly distorted by the flow of electrically conducting plasma, as emitted from the Sun or a nearby star. e.g. the solar wind. Planets having active magnetospheres, like the Earth, are capable of mitigating or blocking the effects of solar radiation or cosmic radiation, that also protects all living organisms from potentially detrimental and dangerous consequences. This is studied under the specialized scientific subjects of plasma physics, space physics and aeronomy.\n\nStudy of Earth's magnetosphere began in 1600, when William Gilbert discovered that the magnetic field on the surface of Earth resembled that on a terrella, a small, magnetized sphere. In the 1940s, Walter M. Elsasser proposed the model of dynamo theory, which attributes Earth's magnetic field to the motion of Earth's iron outer core. Through the use of magnetometers, scientists were able to study the variations in Earth's magnetic field as functions of both time and latitude and longitude. \n\nBeginning in the late 1940s, rockets were used to study cosmic rays. In 1958, Explorer 1, the first of the Explorer series of space missions, was launched to study the intensity of cosmic rays above the atmosphere and measure the fluctuations in this activity. This mission observed the existence of the Van Allen radiation belt (located in the inner region of Earth's magnetosphere), with the follow up Explorer 3 later that year definitively proving its existence. Also during 1958, Eugene Parker proposed the idea of the solar wind, with the term 'magnetosphere' being proposed by Thomas Gold in 1959 to explain how the solar wind interacted with the Earth's magnetic field. The later mission of Explorer 12 in 1961 led by the Cahill and Amazeen observation in 1963 of a sudden decrease in magnetic field strength near the noon-time meridian, later was named the magnetopause. By 1983, the International Cometary Explorer observed the magnetotail, or the distant magnetic field.\n\nMagnetospheres are dependent on several variables: the type of astronomical object, the nature of sources of plasma and momentum, the period of the object's spin, the nature of the axis about which the object spins, the axis of the magnetic dipole, and the magnitude and direction of the flow of solar wind.\n\nThe planetary distance where the magnetosphere can withstand the solar wind pressure is called the Chapman–Ferraro distance. This is usefully modeled by the formula wherein formula_1 represents the radius of the planet, formula_2 represents the magnetic field on the surface of the planet at the equator, and formula_3 represents the velocity of the solar wind:\n\nA magnetosphere is classified as \"intrinsic\" when formula_5, or when the primary opposition to the flow of solar wind is the magnetic field of the object. Mercury, Earth, Jupiter, Ganymede, Saturn, Uranus, and Neptune, for example, exhibit intrinsic magnetospheres. A magnetosphere is classified as \"induced\" when formula_6, or when the solar wind is not opposed by the object's magnetic field. In this case, the solar wind interacts with the atmosphere or ionosphere of the planet (or surface of the planet, if the planet has no atmosphere). Venus has an induced magnetic field, which means that because Venus appears to have no internal dynamo effect, the only magnetic field present is that formed by the solar wind's wrapping around the physical obstacle of Venus (see also Venus' induced magnetosphere). When formula_7, the planet itself and its magnetic field both contribute. It is possible that Mars is of this type.\n\nThe bow shock forms the outermost layer of the magnetosphere; the boundary between the magnetosphere and the ambient medium. For stars, this is usually the boundary between the stellar wind and interstellar medium; for planets, the speed of the solar wind there decreases as it approaches the magnetopause.\n\nThe magnetosheath is the region of the magnetosphere between the bow shock and the magnetopause. It is formed mainly from shocked solar wind, though it contains a small amount of plasma from the magnetosphere. It is an area exhibiting high particle energy flux, where the direction and magnitude of the magnetic field varies erratically. This is caused by the collection of solar wind gas that has effectively undergone thermalization. It acts as a cushion that transmits the pressure from the flow of the solar wind and the barrier of the magnetic field from the object.\n\nThe magnetopause is the area of the magnetosphere wherein the pressure from the planetary magnetic field is balanced with the pressure from the solar wind. It is the convergence of the shocked solar wind from the magnetosheath with the magnetic field of the object and plasma from the magnetosphere. Because both sides of this convergence contain magnetized plasma, the interactions between them are complex. The structure of the magnetopause depends upon the Mach number and beta of the plasma, as well as the magnetic field. The magnetopause changes size and shape as the pressure from the solar wind fluctuates.\n\nOpposite the compressed magnetic field is the magnetotail, where the magnetosphere extends far beyond the astronomical object. It contains two lobes, referred to as the northern and southern tail lobes. Magnetic field lines in the northern tail lobe point towards the object while those in the southern tail lobe point away. The tail lobes are almost empty, with few charged particles opposing the flow of the solar wind. The two lobes are separated by a plasma sheet, an area where the magnetic field is weaker, and the density of charged particles is higher.\n\nOver Earth's equator, the magnetic field lines become almost horizontal, then return to reconnect at high latitudes. However, at high altitudes, the magnetic field is significantly distorted by the solar wind and its solar magnetic field. On the dayside of Earth, the magnetic field is significantly compressed by the solar wind to a distance of approximately . Earth's bow shock is about thick and located about from Earth. The magnetopause exists at a distance of several hundred kilometers above Earth's surface. Earth's magnetopause has been compared to a sieve because it allows solar wind particles to enter. Kelvin–Helmholtz instabilities occur when large swirls of plasma travel along the edge of the magnetosphere at a different velocity from the magnetosphere, causing the plasma to slip past. This results in magnetic reconnection, and as the magnetic field lines break and reconnect, solar wind particles are able to enter the magnetosphere. On Earth's nightside, the magnetic field extends in the magnetotail, which lengthwise exceeds . Earth's magnetotail is the primary source of the polar aurora. Also, NASA scientists have suggested that Earth's magnetotail might cause \"dust storms\" on the Moon by creating a potential difference between the day side and the night side.\n\nThe magnetosphere of Jupiter is the largest planetary magnetosphere in the Solar System, extending up to on the dayside and almost to the orbit of Saturn on the nightside. Jupiter's magnetosphere is stronger than Earth's by an order of magnitude, and its magnetic moment is approximately 18,000 times larger.\nPluto, on the other hand, has no magnetic field.\n\n"}
{"id": "52701130", "url": "https://en.wikipedia.org/wiki?curid=52701130", "title": "Methanogens in digestive tract of ruminants", "text": "Methanogens in digestive tract of ruminants\n\nMethanogens are a group of microorganisms that can produce methane as a byproduct of their metabolism.They hold an important place in the digestive system of ruminants. The digestive tract of ruminants contain four major parts, they are abomasum, rumen, omasum and reticulum.The food with saliva is first passed to the rumen for breaking them into smaller particles and then it moves to the reticulum where the food is broken into further smaller particles and the indigestable particles are sent back for rechewing and then to rumen.  The majority of the anaerobic microbes assisting the cellulose breakdown occupy the rumen. They initiate the fermentation process.The animal observes the fatty acids, vitamins and nutrient content on passing the partially digested food from rumen to omasum which, decreases the pH level and thus initiates the release of enzymes for further break down the food which is later passed to the abomasumthat absorbs remaining nutrients before excretion.This process takes about 9–12 hours.\n\nSome of the microbes in ruminant digestive system are:\n\n"}
{"id": "19712", "url": "https://en.wikipedia.org/wiki?curid=19712", "title": "Methanol", "text": "Methanol\n\nMethanol, also known as methyl alcohol among others, is a chemical with the formula CHOH (a methyl group linked to a hydroxyl group, often abbreviated MeOH). Methanol acquired the name wood alcohol because it was once produced chiefly by the destructive distillation of wood. Today, methanol is mainly produced industrially by hydrogenation of carbon monoxide.\n\nMethanol is the simplest alcohol, consisting of a methyl group linked to a hydroxyl group. It is a light, volatile, colorless, flammable liquid with a distinctive odor similar to that of ethanol (drinking alcohol). Methanol is however far more toxic than ethanol. At room temperature, it is a polar liquid. With more than 20 million tons produced annually, it is used as a precursor to other commodity chemicals, including formaldehyde, acetic acid, methyl tert-butyl ether, as well as a host of more specialized chemicals.\nSmall amounts of methanol are found in normal, healthy human individuals, concluded by one study which found a mean of 4.5 ppm in the exhaled breath of subjects. The mean endogenous methanol in humans of 0.45 g/d may be metabolized from pectin found in fruit; one kilogram of apple produces up to 1.4 g methanol.\n\nMethanol is produced naturally in the anaerobic metabolism of many varieties of bacteria and is commonly present in small amounts in the environment. As a result, the atmosphere contains a small amount of methanol vapor. Atmospheric methanol is oxidized by air in sunlight to carbon dioxide and water over the course of days.\n\nMethanol is also found in abundant quantities in star-forming regions of space and is used in astronomy as a marker for such regions. It is detected through its spectral emission lines.\n\nIn 2006, astronomers using the MERLIN array of radio telescopes at Jodrell Bank Observatory discovered a large cloud of methanol in space, 288 million miles (463 million km) across. In 2016, astronomers detected methyl alcohol in a planet-forming disc around the young star TW Hydrae using ALMA radio telescope.\n\nMethanol has low acute toxicity in humans but is dangerous because, together with ethanol, it is occasionally ingested in large volumes. As little as of pure methanol can cause permanent blindness by destruction of the optic nerve. is potentially fatal. The median lethal dose is (i.e. 1–2 mL/kg body weight of pure methanol). The reference dose for methanol is 2 mg/kg in a day. Toxic effects begin hours after ingestion, and antidotes can often prevent permanent damage. Because of its similarities in both appearance and odor to ethanol (the alcohol in beverages), it is difficult to differentiate between the two (such is also the case with denatured alcohol, adulterated liquors or very low quality alcoholic beverages). However, cases exist of methanol resistance, such as that of Mike Malloy who was the victim of a failed murder attempt by methanol in the early 1930s.\n\nMethanol is toxic by two mechanisms. First, methanol can be fatal due to its CNS depressant properties in the same manner as ethanol poisoning. Second, in a process of toxication, it is metabolized to formic acid (which is present as the formate ion) via formaldehyde in a process initiated by the enzyme alcohol dehydrogenase in the liver. Methanol is converted to formaldehyde via alcohol dehydrogenase (ADH) and formaldehyde is converted to formic acid (formate) via aldehyde dehydrogenase (ALDH). The conversion to formate via ALDH proceeds completely, with no detectable formaldehyde remaining. Formate is toxic because it inhibits mitochondrial cytochrome c oxidase, causing hypoxia at the cellular level, and metabolic acidosis, among a variety of other metabolic disturbances.\nOutbreaks of methanol poisoning have occurred due to contamination of drinking alcohol. This is more common in the developing world. In 2013 more than 1700 cases occurred in the United States. Those affected are often adult men. Outcomes may be good with early treatment. Toxicity to methanol was described as early as 1856.\n\nBecause of its toxic properties, methanol is frequently used as a denaturant additive for ethanol manufactured for industrial uses. This addition of methanol exempts industrial ethanol (commonly known as \"denatured alcohol\" or \"methylated spirit\") from liquor excise taxation in the US and some other countries.\n\nMethanol is primarily converted to formaldehyde, which is widely used in many areas, especially polymers. The conversion entails oxidation:\nAcetic acid can be produced from methanol.\nMethanol and isobutene are combined to give methyl tert-butyl ether (MTBE). MTBE is a major octane booster in gasoline.\n\nCondensation of methanol to produce hydrocarbons and even aromatic systems is the basis of several technologies related to gas to liquids. These include methanol-to-hydrocarbons (MTH), methanol to gasoline (MTG), and methanol to olefins (MTO), and methanol to propylene (MTP). These conversions are catalyzed by zeolites as heterogeneous catalysts. The MTG process was once commercialized at Motunui in New Zealand.\n\nThe European Fuel Quality Directive allows up to 3% methanol with an equal amount of cosolvent to be blended with gasoline sold in Europe. China uses more than one billion gallons of methanol per year as a transportation fuel in low level blends for conventional vehicles and high level blends in vehicles designed for methanol fuels.\n\nMethanol is the precursor to most simple methylamines, methyl halides, methyl ethers. Methyl esters are produced from methanol, including the transesterification of fats and production of biodiesel via transesterification.\n\nMethanol is a promising energy carrier because, as a liquid, it is easier to store than hydrogen and natural gas. Its energy density is however low reflecting the fact that it represents partially combusted methane. Its energy density is 15.6 MJ/L, whereas ethanol's is 24 and gasoline's is 33 MJ/L.\n\nFurther advantages for methanol is its ready biodegradability and low toxicity. It does not persist in either aerobic (oxygen-present) or anaerobic (oxygen-absent) environments. The half-life for methanol in groundwater is just one to seven days, while many common gasoline components have half-lives in the hundreds of days (such as benzene at 10–730 days). Since methanol is miscible with water and biodegradable, it is unlikely to accumulate in groundwater, surface water, air or soil.\n\nMethanol is occasionally used to fuel internal combustion engines. It burns forming carbon dioxide and water:\nOne problem with high concentrations of methanol in fuel is that alcohols corrode some metals, particularly aluminium. Methanol fuel has been proposed for ground transportation. The chief advantage of a methanol economy is that it could be adapted to gasoline internal combustion engines with minimum modification to the engines and to the infrastructure that delivers and stores liquid fuel. Its energy density is however only half that of gasoline, meaning that twice the volume of methanol would be required.\n\nMethanol is a traditional denaturant for ethanol, the product being known as \"denatured alcohol\" or \"methylated spirit\". This was commonly used during the Prohibition to discourage consumption of bootlegged liquor, and ended up causing several deaths.\n\nMethanol is used as a solvent and as an antifreeze in pipelines and windshield washer fluid. Methanol was used as an automobile coolant antifreeze in the early 1900s. As of May 2018, methanol was banned in the EU for use in windscreen washing or defrosting due to its risk of human consumption.\n\nIn some wastewater treatment plants, a small amount of methanol is added to wastewater to provide a carbon food source for the denitrifying bacteria, which convert nitrates to nitrogen gas and reduce the nitrification of sensitive aquifers.\n\nMethanol is used as a destaining agent in polyacrylamide gel electrophoresis.\n\nDirect-methanol fuel cells are unique in their low temperature, atmospheric pressure operation, allowing them to be miniaturized to an unprecedented degree. This, combined with the relatively easy and safe storage and handling of methanol, may open the possibility of fuel cell-powered consumer electronics, such as laptop computers and mobile phones.\n\nMethanol is also a widely used fuel in camping and boating stoves. Methanol burns well in an unpressurized burner, so alcohol stoves are often very simple, sometimes little more than a cup to hold fuel. This lack of complexity makes them a favorite of hikers who spend extended time in the wilderness. Similarly, the alcohol can be gelled to reduce risk of leaking or spilling, as with the brand \"Sterno\".\n\nMethanol is mixed with water and injected into high performance diesel and gasoline engines for an increase of power and a decrease in intake air temperature in a process known as water methanol injection.\n\nCarbon monoxide and hydrogen react over a catalyst to produce methanol. Today, the most widely used catalyst is a mixture of copper and zinc oxides, supported on alumina, as first used by ICI in 1966. At 5–10 MPa (50–100 atm) and , the reaction is characterized by high selectivity (>99.8%):\n\nThe production of synthesis gas from methane produces three moles of hydrogen for every mole of carbon monoxide, whereas the synthesis consumes only two moles of hydrogen gas per mole of carbon monoxide. One way of dealing with the excess hydrogen is to inject carbon dioxide into the methanol synthesis reactor, where it, too, reacts to form methanol according to the equation:\n\nIn terms of mechanism, the process occurs via initial conversion of CO into CO, which is then hydrogenated:\nwhere the HO byproduct is recycled via the water-gas shift reaction\nThis gives an overall reaction, which is the same as listed above.\n\nThe catalytic conversion of methane to methanol is effected by enzymes including methane monooxygenases. These enzymes are mixed-function oxygenases, i.e. oxygenation is coupled with production of water:\nBoth Fe- and Cu-dependent enzymes have been characterized. Intense but largely fruitless efforts have been undertaken to emulate this reactivity. Methanol is more easily oxidized than is the feedstock methane, so the reactions tend not to be selective. Some strategies exist to circumvent this problem. Examples include Shilov systems and Fe- and Cu containing zeolites. These systems do not necessarily mimick the mechanisms employed by metalloenzymes, but draw some inspiration from them. Active sites can vary substantially from those known in the enzymes. For example, a dinuclear active site is proposed in the sMMO enzyme, whereas a mononuclear iron (alpha-Oxygen) is proposed in the Fe-zeolite.\n\nMethanol is available commercially in various purity grades. Commercial methanol is generally classified according to ASTM purity grades A and AA. Methanol for chemical use normally corresponds to Grade AA. In addition to water, typical impurities include acetone and ethanol (which are very difficult to separate by distillation). UV-vis spectroscopy is a convenient method for detecting aromatic impurities. Water content can be determined by the Karl-Fischer titration.\n\nIn their embalming process, the ancient Egyptians used a mixture of substances, including methanol, which they obtained from the pyrolysis of wood. Pure methanol, however, was first isolated in 1661 by Robert Boyle, when he produced it via the distillation of buxus (boxwood). It later became known as \"pyroxylic spirit\". In 1834, the French chemists Jean-Baptiste Dumas and Eugene Peligot determined its elemental composition.\n\nThey also introduced the word \"methylène\" to organic chemistry, forming it from Greek \"methy\" = \"alcoholic liquid\" + \"hȳlē\" = \"woodland, forest\", with a Greek language error: \"xylon\" = \"wood as a material\" would have been more suitable. \"Methylène\" designated a \"radical\" that was about 14% hydrogen by weight and contained one carbon atom. This would be CH, but at the time carbon was thought to have an atomic weight only six times that of hydrogen, so they gave the formula as CH. They then called wood alcohol (l'esprit de bois) \"bihydrate de méthylène\" (bihydrate because they thought the formula was CHO = (CH)(HO)). The term \"methyl\" was derived in about 1840 by back-formation from \"methylene\", and was then applied to describe \"methyl alcohol\". This was shortened to \"methanol\" in 1892 by the International Conference on Chemical Nomenclature. The suffix -yl used in organic chemistry to form names of carbon groups, was extracted from the word \"methyl\".\n\nIn 1923, the German chemists Alwin Mittasch and Mathias Pier, working for Badische-Anilin & Soda-Fabrik (BASF), developed a means to convert synthesis gas (a mixture of carbon monoxide, carbon dioxide, and hydrogen) into methanol. US patent 1,569,775 () was applied for on 4 Sep 1924 and issued on 12 January 1926; the process used a chromium and manganese oxide catalyst with extremely vigorous conditions: pressures ranging from 50 to 220 atm, and temperatures up to 450 °C. Modern methanol production has been made more efficient through use of catalysts (commonly copper) capable of operating at lower pressures. The modern low pressure methanol (LPM) process was developed by ICI in the late 1960s with the technology now owned by Johnson Matthey, which is a leading licensor of methanol technology.\n\nDuring World War II, methanol was used as a fuel in several German military rocket designs, under the name M-Stoff, and in a roughly 50/50 mixture with hydrazine, known as C-Stoff.\n\nThe use of methanol as a motor fuel received attention during the oil crises of the 1970s. By the mid-1990s, over 20,000 methanol \"flexible fuel vehicles\" capable of operating on methanol or gasoline were introduced in the U.S. In addition, low levels of methanol were blended in gasoline fuels sold in Europe during much of the 1980s and early-1990s. Automakers stopped building methanol FFVs by the late-1990s, switching their attention to ethanol-fueled vehicles. While the methanol FFV program was a technical success, rising methanol pricing in the mid- to late-1990s during a period of slumping gasoline pump prices diminished interest in methanol fuels.\n\nIn the early 1970s, a process was developed by Mobil for producing gasoline fuel from methanol.\n\nBetween the 1960s and 1980s methanol emerged as a precursor to the feedstock chemicals acetic acid and acetic anhydride. These processes include the Monsanto acetic acid synthesis, Cativa process, and Tennessee Eastman acetic anhydride process.\n\n\n\n"}
{"id": "20842544", "url": "https://en.wikipedia.org/wiki?curid=20842544", "title": "Official Women's Squash World Ranking", "text": "Official Women's Squash World Ranking\n\nThe Official Women's Squash World Ranking is the official world ranking for women's squash. The ranking is to rate the performance level of female professional squash player. It is also a merit-based method used for determining entry and seeding in women's squash tournaments. The rankings are produced monthly. The current world number one is Nour El Sherbini of Egypt, who replaced Laura Massaro in May 2016.\n\nPlayers competing in PSA tournaments earn ranking points according to how far they get in the draw. The points available depend on the prize money and the draw size. The monthly rankings (issued on the 1st of the month) are used in selecting entries to tournaments and in determining the seeds.\n\nThe total number of points a player earns in a year (52 weeks) is divided by the number of tournaments played (a minimum of eight are required) to give a ranking average. Where a player has played more than 8 tournaments the best scores may be selected (i.e., the lowest are not included).\n\nFor example, a player who has competed in 13 events will have selected her best 10 scores, which will be accumulated and divided by 10.\n\nPlayers competing in PSA World Tour Events earn ranking points according to the prize money, classification of the event, and the final position in the draw the player reaches.\n\nThe total number of points a player accumulates in any year (52-week period) is divided by the number of tournaments played (minimum 8 tournaments in 52 weeks) to give an average score:\n\nNote: The monthly ranking for the women's squash (world ranking) is taken directly from the Professional Squash Association (PSA) official website.\n\nThe following is a list of players who have achieved the world number one position since April 1983 (active players in green):\n\n\"Last update: April, 2017\"\n\nnajmi\n\nNote: \n1) Natalie Grinham changed her nationality in 2008.\n\n\n\n"}
{"id": "20895158", "url": "https://en.wikipedia.org/wiki?curid=20895158", "title": "Olszewski tube", "text": "Olszewski tube\n\nAn Olszewski tube is a pipe designed to bring oxygen-poor water from the bottom of a lake to the top. This tube was first proposed by a Polish limnologist named Olszewski in 1961 and helps combat the negative effects of eutrophication, high nutrient content, in lakes. The basic concept behind the Olszewski tube is the reduction of nutrient concentration and destratification; the more specific goal is hypolimnetic withdrawal.\n\nWhen nutrients build up in a lake, eutrophication occurs, and this generally occurs in the top layer of a lake. The nutrients come both naturally and artificially and usually contain phosphates. The artificial nutrients can come from sewage and fertilizers, from agricultural runoff. Phosphorus from the phosphates causes algae to grow rapidly and spread throughout the top layer of the lake. Algal blooms have negative effects on both the aesthetics and the ecology of the lake. Aesthetically, the lake is not pleasing because it is covered with algae. Ecologically, eutrophication causes organisms in the lake to die because the algae deplete the dissolved oxygen in the lake.\n\nAt the most simple level, the Olszewski tube is a pipe that spans from the bottom, hypolimnetic layer of the lake to the outlet. The outlet part of the pipe is installed under lake level in order for the device to act as a siphon. Once warm water flows in the lake at the surface, it forces the cold anoxic water of the hypolimnetic layer through and up the tube. This oxygen-poor water is then brought to the top of the lake where the eutrophication occurs. This eventually helps the lake as a whole because the bottom of the lake will have more dissolved oxygen and the top of the lake will have less eutrophication.\n\nThe first implementation of the Olszewski tube was attempted at Lake Kortowo in Poland and this led to oligotrophication, reduction of nutrient cycling. This tube has shown the most promise in a 3.9 meter deep eutrophic lake in Switzerland because the phosphorus and nitrogen levels in the summer drastically decreased, oxygen levels increased, and the amount of cyanobacteria decreased from 152 grams per square meter to 41 grams per square meter. It has also been reported by a scientist named Bjork that there have been successes with the Olszewski tube in European lakes. Other limnologists like Pechlaner and Gachter have reported successes in small lakes where the total phosphorus decreased, transparency of water increased, and less algae was present.\n\nSome complications that can arise with the use of an Olszewski tube include disruption of the thermocline and excessive water loss. The thermocline separates the upper layer of water that is mixed temperatures with the deeper, cooler water. If the thermocline is disrupted, it could alter the ecology of the lake, potentially making it uninhabitable. Another complication is that the installation must be a long-term process. Short-term uses of Olszewski tubes have largely failed because it takes some time for the anoxic condition of the hypolimnetic layer to increase in dissolved oxygen. Also, it must be a slow process in order to avoid disrupting the thermocline in a lake. If the Olszewski tube is operated slowly enough, the rate of water going in and going out will be fairly constant causing the thermocline to stay intact.\n\nOne advantage to hypolimnetic withdrawal is that it is relatively inexpensive to install an Olszewski tube or any similar device. Along with low initial cost, it also has a relatively low annual maintenance cost. The following are four systems installed in the United States (2002), their area in hectares, the rate of flow in cube-meters per minute, and their initial installation costs in US dollars:\n41 ha\n3.4 m/min\n$420,000\n287 ha\n6.3 m/min\n$62,000\n151 ha\n9.1 m/min\n$310,000\n412 ha\n5.3 m/min\n$282,000\n\nAside from using an Olszewski tube and hypolimnetic withdrawal, there are other techniques implemented to achieve the same goals as an Olszewski tube. These include increasing dissolved oxygen, reducing nutrient concentration, and lessening the amount of algae and unwanted biomass in lakes.\n"}
{"id": "9301973", "url": "https://en.wikipedia.org/wiki?curid=9301973", "title": "Ozone monitoring instrument", "text": "Ozone monitoring instrument\n\nThe ozone monitoring instrument (OMI) is a visual and ultraviolet spectrometer aboard the NASA Aura spacecraft. OMI can distinguish between aerosol types, such as smoke, dust, and sulfates, and can measure cloud pressure and coverage, which provide data to derive tropospheric ozone. OMI follows in the heritage of TOMS, SBUV, GOME, SCIAMACHY, and GOMOS. It is a wide-field-imaging spectrometer with a 114° across-track viewing angle range that provides a 2600 km wide swath, enabling measurements with a daily global coverage. OMI is continuing the TOMS record for total ozone and other atmospheric parameters related to ozone chemistry and climate.\n\nThe OMI project is a cooperation between the Netherlands Agency for Aerospace Programmes (NIVR), the Finnish Meteorological Institute (FMI) and the National Aeronautics and Space Agency (NASA).\n\nThe OMI project was carried out under the direction of the NIVR and financed by the Ministries of Economic Affairs, Transport and Public Works and the Ministry of Education and Science. The instrument was built by Dutch Space in co-operation with Netherlands Organisation for Applied Scientific Research Science and Industry and Netherlands Institute for Space Research. The Finnish industry supplied the electronics. The scientific part of the OMI project is managed by KNMI (principal investigator Prof. Dr. P. F. Levelt), in close co-operation with NASA and the Finnish Meteorological Institute.\n\n"}
{"id": "949357", "url": "https://en.wikipedia.org/wiki?curid=949357", "title": "Polynya", "text": "Polynya\n\nA polynya is an area of open water surrounded by sea ice. It is now used as geographical term for an area of unfrozen sea within the ice pack. It is a loanword from (polynya) , which refers to a natural ice hole, and was adopted in the 19th century by polar explorers to describe navigable portions of the sea. In past decades, for example, some polynyas, such as the Weddell Polynya, have lasted over multiple winters (1974–1976).\n\nPolynyas are formed through two main processes:\n\n\nLatent heat polynyas are regions of high ice production and therefore are possible sites of dense water production in both polar regions. The high ice production rates within these polynyas leads to a large amount of brine rejection into the surface waters. This salty water then sinks. It is an open question as to whether the polynyas of the Arctic can produce enough dense water to form a major portion of the dense water required to drive the thermohaline circulation.\n\nSome polynyas, such as the North Water Polynya between Canada and Greenland occur seasonally at the same time and place each year. Because animals can adapt their life strategies to this regularity, these types of polynyas are of special ecological research significance. In winter, marine mammals such as walruses, narwhals and belugas that do not migrate south, remain there. In spring, the thin or absent ice cover allows light in, through the surface layer as soon as the winter night ends, which triggers the early blooming of microalgae that are at the basis of the marine food chain. So, polynyas are suspected to be places where intense and early production of the planktonic herbivores ensure the transfer of solar energy (food chain) fixed by planktonic microalgae to Arctic cod, seals, whales, and polar bears. Polar bears are known to be able to swim as far as 65 kilometres across open waters of a polynya.\n\nThe presence of polynyas in McMurdo Sound in the Antarctic provides an ice-free area where penguins can feed, thus they are important for the survival of the Cape Royds penguin colony.\n\nAntarctic Bottom Water is the dense water with high salinity that exists in the abyssal layer of the Southern Ocean and it plays a major role in the global overturning circulation. Coastal Polynyas (Latent heat polynyas) are a source of AABW as brine rejection during the formation of sea ice at these polynyas increase the salinity of the sea water which then sinks down to the ocean bottom as AABW. Antarctic polynyas form when ice masses diverge from the coast and move away in the direction of the wind; creating an exposed area of sea water which subsequently freezes over, with brine rejection, to form another mass of ice.\n\nWhen submarines of the U.S. Navy made expeditions to the North Pole in the 1950s and 60s, there was a significant concern about surfacing through the thick pack ice of the Arctic Ocean. In 1962, both the USS \"Skate\" and USS \"Seadragon\" surfaced within the same, large polynya near the North Pole, for the first polar rendezvous of the U.S. Atlantic Fleet and the U.S. Pacific Fleet.\n\n\n"}
{"id": "49250446", "url": "https://en.wikipedia.org/wiki?curid=49250446", "title": "Reactive carbonyl species", "text": "Reactive carbonyl species\n\nReactive carbonyl species (RCS) are molecules with highly reactive carbonyl groups, and often known for their damaging effects on proteins, nucleic acids, and lipids. They are often generated as metabolic products or derived from the environment. Several RCS have been found to activate caspase-like proteases in plants. Categories of reactive species include α,β-unsaturated carbonyl compounds and dialdehydes.\n\nExamples \n"}
{"id": "11301067", "url": "https://en.wikipedia.org/wiki?curid=11301067", "title": "Seven Wonders of Canada", "text": "Seven Wonders of Canada\n\nThe Seven Wonders of Canada was a 2007 competition sponsored by CBC Television's \"\" and CBC Radio One's \"Sounds Like Canada\". They sought to determine Canada's \"seven wonders\" by receiving nominations from viewers, and then from on-line voting of the short list. After the vote, a panel of judges, Ra McGuire, Roy MacGregor and Roberta L. Jamieson, picked the winners based on geographic and poetic criteria. Their seven picks were revealed on \"\" on June 7, 2007. The Seven Wonders as chosen by Canada were the Sleeping Giant, Niagara Falls, the Bay of Fundy, Nahanni National Park Reserve, the Northern Lights, the Rockies, and the Cabot Trail.\n\nFull voting results\n\n"}
{"id": "4766792", "url": "https://en.wikipedia.org/wiki?curid=4766792", "title": "Taxonomy of Drosera", "text": "Taxonomy of Drosera\n\nThe genus \"Drosera\" was divided in 1994 by Seine & Barthlott into three subgenera and 11 sections on the basis of morphological characteristics.\n\nDiscovery and description of new species has been occurring since the 10th century, and as recently as the 1940s barely more than 80 species were known. In recent years, Australian Allen Lowrie has done extensive work in the genus, particularly in describing numerous new species from Australia. His classification of the genus was replaced by Jan Schlauer's work in 1996, although the correct classification is still disputed.\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "303469", "url": "https://en.wikipedia.org/wiki?curid=303469", "title": "Teak", "text": "Teak\n\nTeak (\"Tectona grandis\") is a tropical hardwood tree species placed in the flowering plant family Lamiaceae. \"Tectona grandis\" is a large, deciduous tree that occurs in mixed hardwood forests. It has small, fragrant white flowers and large papery leaves that are often hairy on the lower surface. It is sometimes known as the \"Burmese teak\". Teak wood has a leather-like smell when it is freshly milled. It is particularly valued for its durability and water resistance, and is used for boat building, exterior construction, veneer, furniture, carving, turnings, and other small wood projects. \"Tectona grandis\" is native to south and southeast Asia, mainly India, Sri Lanka, Indonesia, Malaysia, Thailand, Myanmar and Bangladesh but is naturalised and cultivated in many countries in Africa and the Caribbean. Myanmar's teak forests account for nearly half of the world's naturally occurring teak.\nMolecular studies show that there are two centres of genetic origin of teak; one in India and the other in Myanmar and Laos. \"CP teak\" (\"Central Province\" teak) is a description of teak from the central states of India. \"Nagpur teak\" is another regional Indian teak. It produces small, white flowers arranged in dense clusters (panicles) at the end of the branches. Flowers contain both types of reproductive organs (perfect flowers).\n\nThe word teak comes from Tamil \"tekku\" (தேக்கு), Malayalam \"thekku\" (തേക്ക്), Kannada \"tega\" (ತೇಗ) via the Portuguese \"teca\". The plant is known in Sinhala as Thekka (තේක්ක) among the Sri Lankan people and \"Segun\" (সেগুন) in Bangladesh and West Bengal.\n\nTeak is a large, long, deciduous tree up to tall with gray to grayish brown branches. These are mostly known for their finest quality wood. Leaves are ovate-elliptic to ovate, long by wide, and are held on robust petioles which are long. Leaf margins are entire.\n\nFragrant white flowers are borne on long by wide panicles from June to August. The corolla tube is 2.5–3 mm long with 2 mm wide obtuse lobes. \"Tectona grandis\" sets fruit from September to December; fruits are globose and 1.2-1.8 cm in diameter. Flowers are weakly protandrous in that the anthers precede the stigma in maturity and pollen is shed within a few hours of the flower opening. The flowers are primarily entomophilous (insect-pollinated), but can occasionally be anemophilous (wind-pollinated). A 1996 study found that in its native range in Thailand, the major pollinator were species in the bee genus \"Ceratina\".\n\n\n\"Tectona grandis\" was first formally described by Carl Linnaeus the Younger in his 1782 work \"Supplementum Plantarum\". In 1975, Harold Norman Moldenke published new descriptions of four forms of this species in the journal \"Phytologia\". Moldenke described each form as varying slightly from the type specimen: \"T. grandis\" f. \"canescens\" is distinguished from the type material by being densely canescent, or covered in hairs, on the underside of the leaf, \"T. grandis\" f. \"pilosula\" is distinct from the type material in the varying morphology of the leaf veins, \"T. grandis\" f. \"punctata\" is only hairy on the larger veins on the underside of the leaf, and \"T. grandis\" f. \"tomentella\" is noted for its dense yellowish tomentose hairs on the lower surface of the leaf.\n\n\"Tectona grandis\" is one of three species in the genus \"Tectona\". The other two species, \"T. hamiltoniana\" and \"T. philippinensis\", are endemics with relatively small native distributions in Myanmar and the Philippines, respectively. \"Tectona grandis\" is native to India, Sri Lanka, Indonesia, Myanmar, northern Thailand, and northwestern Laos.\n\n\"Tectona grandis\" is found in a variety of habitats and climatic conditions from arid areas with only 500 mm of rain per year to very moist forests with up to 5,000 mm of rain per year. Typically, though, the annual rainfall in areas where teak grows averages 1,250-1,650 mm with a 3-5 month dry season.\n\nTeak's natural oils make it useful in exposed locations, and make the timber termite and pest resistant. Teak is durable even when not treated with oil or varnish. Timber cut from old teak trees was once believed to be more durable and harder than plantation grown teak. Studies have shown that plantation teak performs on par with old-growth teak in erosion rate, dimensional stability, warping, and surface checking, but is more susceptible to color change from UV exposure.\n\nThe vast majority of commercially harvested teak is grown on teak plantations found in Indonesia and controlled by Perum Perhutani (a state owned forest enterprise) that manages the country's forests. The primary use of teak harvested in Indonesia is in the production of outdoor teak furniture for export. Nilambur in Kerala, India, is also a major producer of teak of fine quality, holds the world's oldest teak plantation.\n\nTeak consumption raises a number of environmental concerns, such as the disappearance of rare old-growth teak. However, its popularity has led to growth in sustainable plantation teak production throughout the seasonally dry tropics in forestry plantations. The Forest Stewardship Council offers certification of sustainably grown and harvested teak products. Propagation of teak via tissue culture for plantation purposes is commercially viable.\n\nTeak plantations were widely established in Equatorial Africa during the Colonial era. These timber resources, as well as the oil reserves, are at the heart of the current (2014) South Sudanese conflict.\n\nMuch of the world's teak is exported by Indonesia and Myanmar. There is also a rapidly growing plantation grown market in Central America (Costa Rica) and South America. With a depletion of remaining natural hectares of teak forests, a growth in plantations in Latin America is expected to rise.\n\n\"Hyblaea puera\", commonly known as the teak defoliator, is a moth native to southeast Asia. It is a teak pest whose caterpillar feeds on teak and other species of trees common in the region of southeast Asia.\n\nTeak's high oil content, high tensile strength and tight grain make it particularly suitable where weather resistance is desired. It is used in the manufacture of outdoor furniture and boat decks. It is also used for cutting boards, indoor flooring, countertops and as a veneer for indoor furnishings. Although easily worked, it can cause severe blunting on edged tools because of the presence of silica in the wood. Over time teak can weather to a silvery-grey finish, especially when exposed to sunlight.\n\nTeak is used extensively in India to make doors and window frames, furniture, and columns and beams in old type houses. It is resistant to termite attacks and damage caused by other insects. Mature teak fetches a very good price. It is grown extensively by forest departments of different states in forest areas.\n\nLeaves of the teak wood tree are used in making Pellakai gatti (jackfruit dumpling), where batter is poured into a teak leaf and is steamed. This type of usage is found in the coastal district of Udupi in the Tulunadu region in South India. The leaves are also used in gudeg, a dish of young jackfruit made in Central Java, Indonesia, and give the dish its dark brown color.\n\nTeak is used as a food plant by the larvae of moths of the genus \"Endoclita\" including \"E. aroura\", \"E. chalybeatus\", \"E. damor\", \"E. gmelina\", \"E. malabaricus\", \"E. sericeus\" and \"E. signifer\" and other Lepidoptera including Turnip Moth.\n\nTeak has been used as a boatbuilding material for over 2000 years (it was found in an archaeological dig in Berenice Panchrysos, a port on the Indian Roman trade). In addition to relatively high strength, teak is also highly resistant to rot, fungi and mildew. In addition, teak has a relatively low shrinkage ratio, which makes it excellent for applications where it undergoes periodic changes in moisture. Teak has the unusual properties of being both an excellent structural timber for framing, planking, etc., while at the same time being easily worked, unlike some other similar woods such as purpleheart, and finished to a high degree. For this reason, it is also prized for the trim work on boat interiors. Due to the oily nature of the wood, care must be taken to properly prepare the wood before gluing.\n\nWhen used on boats, teak is also very flexible in the finishes that may be applied. One option is to use no finish at all, in which case the wood will naturally weather to a pleasing silver-grey. The wood may also be oiled with a finishing agent such as linseed or tung oil. This results in a pleasant, somewhat dull finish. Finally, teak may also be varnished for a deep, lustrous glow.\n\nTeak is also used extensively in boat decks, as it is extremely durable and requires very little maintenance. The teak tends to wear in to the softer 'summer' growth bands first, forming a natural 'non-slip' surface. Any sanding is therefore only damaging. Use of modern cleaning compounds, oils or preservatives will shorten the life of the teak, as it contains natural teak-oil a very small distance below the white surface. Wooden boat experts will only wash the teak with salt water, and re-caulk when needed. This cleans the deck, and prevents it from drying out and the wood shrinking. The salt helps it absorb and retain moisture, and prevents any mildew and algal growth. Over-maintenance, such as cleaning teak with harsh chemicals, can shorten its usable lifespan as decking.\n\nDue to the increasing cost of teak, various alternatives have been employed. These include purpleheart, iroko, and angelique.\n\nTeak is propagated mainly from seeds. Germination of the seeds involves pretreatment to remove dormancy arising from the thick pericarp. Pretreatment involves alternate wetting and drying of the seed. The seeds are soaked in water for 12 hours and then spread to dry in the sun for 12 hours. This is repeated for 10–14 days and then the seeds are sown in shallow germination beds of coarse peat covered by sand. The seeds then germinate after 15 to 30 days.\n\nClonal propagation of teak has been successfully done through grafting, rooted stem cuttings and micro propagation. While bud grafting on to seedling root stock has been the method used for establishing clonal seed orchards that enables assemblage of clones of the superior trees to encourage crossing, rooted stem cuttings and micro propagated plants are being increasingly used around the world for raising clonal plantations.\n\nMinistry of Environmental Conservation and Forestry (Myanmar) found out two world biggest living teak on 28 August 2017 in Homalin Township, Sagaing Region, Myanmar. The biggest one, named Homemalynn 1, is in girth and tall. The second biggest one, named Homemalynn 2, is in girth.\n\nPreviously, the world's biggest recorded teak tree was located within the Parambikulam Wildlife Sanctuary in the Palakkad District of Kerala in India, named Kannimara. The tree is approximately tall.\n\nIn 2017, a tree was discovered in the Ottakallan area of Thundathil range of Malayattoor Forest Division in Kerala with a girth of and height of . A teak tree in Kappayam, Edamalayar, Kerala which used to considered as the biggest, has a diameter of only 7.23 meters.\n\nThe International Teak Information Network (Teaknet) supported by the Food and Agriculture Organization (FAO) Regional office for Asia-Pacific, Bangkok, currently has its offices at the Kerala Forest Research Institute, Peechi, Thrissur, Kerala State in India. Teaknet is an international network of institutions and individuals interested in teak. Teaknet addresses the interests of all the categories of stakeholders related to teak, whether they are growers, traders, researchers or other groups with a profound interest or concerned with teak. From time to time, the organisation formulates action plans focusing on the short term and long term needs of the global teak sector. The TEAKNET website provides information to all those concerned with research, conservation, growing, management and utilisation of teak.\n\n\n"}
{"id": "4662630", "url": "https://en.wikipedia.org/wiki?curid=4662630", "title": "Tundra orbit", "text": "Tundra orbit\n\nA tundra orbit (Russian: Тундра) is a highly elliptical geosynchronous orbit with a high inclination (usually near 63.4°) and an orbital period of one sidereal day. A satellite placed in this orbit spends most of its time over a chosen area of the Earth, a phenomenon known as apogee dwell. The ground track of a satellite in a Tundra orbit is a closed figure-eight with a smaller loop over either the northern or southern hemisphere. Tundra orbits have moderate eccentricity, typically between 0.2 and 0.3. These orbits are conceptually similar to Molniya orbits, which have the same inclination but half the period.\n\nUntil 2016, Sirius Satellite Radio, now part of Sirius XM Holdings, operated a constellation of three satellites in Tundra orbits for satellite radio. The RAAN and mean anomaly of each satellite were offset by 120° so that when one satellite moved out of position, another had passed perigee and was ready to take over. The three satellites were launched in 2000 and moved into circular disposal orbits in 2016; Sirius XM now broadcasts only from geostationary satellites.\n\nTwo spacecraft in Tundra orbits are able to provide continuous coverage over an area.\n\nTundra and Molniya orbits are used to provide high-latitude users with higher elevation angles than a geostationary orbit. These orbits remain over their desired high-latitude regions for long periods of time due to their slow movement at apogee. Neither the Tundra nor Molniya orbit is geostationary because that is possible only over the equator, so both orbits are elliptical to reduce the time that the satellite is away from its service area. An argument of perigee of 270° places apogee at the northernmost point of the orbit. An argument of perigee of 90° would likewise serve the high southern latitudes. An argument of perigee of 0° or 180° would cause the satellite to dwell over the equator, but there would be little point to this as this could be better done with a conventional geostationary orbit.\n\nThe Tundra and Molniya orbits use a inclination to null the secular perturbation of the argument of perigee caused by the Earth's equatorial bulge. With any inclination other than 63.4° or its supplement, 116.6°, the argument of perigee would change steadily over time, and apogee would occur either before or after the highest latitude is reached.\n"}
{"id": "35887216", "url": "https://en.wikipedia.org/wiki?curid=35887216", "title": "Upsilon Hydrae", "text": "Upsilon Hydrae\n\nThe Bayer designation υ Hydrae (Latinised to Upsilon Hydrae; abbreviated υ Hya, Ups Hya) is shared by two stars in the constellation of Hydra:\n"}
{"id": "20602805", "url": "https://en.wikipedia.org/wiki?curid=20602805", "title": "Western Front Association", "text": "Western Front Association\n\nThe Western Front Association (WFA) was inaugurated on 11 November 1980, in order to further interest in The Great War of 1914-1918. The WFA aims to perpetuate the memory, courage and comradeship of all those who fought on all sides and who served their countries during The Great War. The Western Front Association does not seek to justify or glorify war. It is not a re-enactment society, nor is it commercially motivated. It is entirely non-political. The object of the Association is to educate the public in the history of The Great War with particular reference to the Western Front.\n\nThe WFA was established by military historian John Giles, who enlisted the help of John Terraine, who had co-written the landmark television series The Great War, which was first broadcast in 1964. Giles was driven to form The Western Front Association as a result of the creation of such groups as 'The Gallipoli Association' which had been established in 1969, and - in the early 1970s - the 'Waterloo Association' which had been set up to save the old battlefield. It was in this context, over the following years, that John Giles developed the idea for an association on the First World War with its emphasis on the Western Front. It would be, John Giles was clear, \"The Western Front\" and not, in his words about \"Salonica or Naval battles\" - the definitive article 'The' was also stipulated. And thus in November 1980 The Western Front Association was established.\n\nSince its foundation the WFA has grown over the years to in excess of 6,000 members worldwide. There are around 60 branches in the UK, Europe, USA, Canada, Australia and New Zealand.\n\nThe WFA is a UK registered charity, numbered 298365.\n\nThe website of the Western Front Association contains articles on The Great War. One of the sections of the website is a \"For Schools\" section which has educational resources for teachers and students.\n\nWith nearly sixty branches worldwide, including fifty in the UK and Ireland, the association has a representation in most counties in the UK and in most of 'main combatant' countries which took part in the Great War. Membership of the WFA is not a requisite to attend branch meetings, which are open to the public. There are no formal charges for attending these meetings, but a donation on the door is usually requested, this is typically between £3 and £5. Most branches hold an event at least once a month, so there are between 500 and 600 meetings up and down the UK every year in which an aspect of the Great War is discussed. The number of branches being formed is increasing.\n\nThe WFA publishes its journal, \"Stand To!\" three times a year. This contains articles on the subject of The Great War. There is also a house magazine for members, the \"Bulletin\", which is also published three times each year. An electronic newsletter is also available to which members and non-members can subscribe.\n\nThe Western Front Association holds conferences in various venues with typically four or five speakers addressing a range of subjects. These conferences are often filmed and these videos have been made freely available to the public via the Association's web site and via YouTube.\n\nThe WFA organises each year the Remembrance ceremony held at the Cenotaph in Whitehall, London on 11 November (except when the 11th falls on Remembrance Sunday). The 2014 ceremony was attended by the Prime Minister, David Cameron.\n\nThe Western Front Association owned a small but historically important part of the \"old front line\" on the Somme, the Butte de Warlencourt. This land was bought by The Western Front Association in 1990 in order to ensure its future preservation.\n\nA memorial detailing the fighting that took place in the area was dedicated in a ceremony on the Butte on 30 June 1990. The ceremony involved the then President of the WFA, well known historian and author John Terraine.\n\nIn October 2018, the Western Front Association announced its sale \"at fair market value\" to Bob Paterson who was chairman of the WFA between 2014-2016.\n\nThe Medal Index Cards were saved from destruction in 2005. Medal Index Cards (MICs) are the original method of recording medal entitlement for soldiers who served in the Great War.Each soldier who served in an active theatre of operations was awarded a medal.\n\nThe Medal Index Cards were stored at the Ministry of Defence record centre in Hayes until 2005. Due to the need to make space, the MoD sold the Hayes site for redevelopment; the MoD (which owned the cards) proposed the cards would be destroyed. No museum or archive was prepared to take them on, so the Western Front Association came forward and agreed to save these records.\n\nSince obtaining these cards, the WFA has been storing them. Many WFA members have requested copies of these, often placing the card in a frame alongside the medals.\n\nIn November 2012, the WFA announced that it has secured over six million Pension Index Cards and Ledgers from the UK's Ministry of Defence, which would otherwise have been destroyed. These are an extremely valuable primary source for family and military historians as they provide information on men (and some women) who served in the Great War and who subsequently applied for a pension. This significantly adds to the data that is available, particularly for those individuals who survived the Great War.\n\nThe Imperial War Museum's sound archive holds over 33,000 recordings relating to conflict since 1914. This consists of the largest oral history collection of its type in the world, with contributions from both service personnel and non-combatants as well as significant holdings of speeches, sound effects, broadcasts, poetry and music. The Western Front Association funded digitisation of the majority of IWM's First World War sound recordings, thereby widening public access to this important historical resource.\n\nThe WFA organises tours of the Battlefields for its members three times a year. Many of the WFA's branches organise similar tours at a local level.\n\nThe address of the association is:\nThe Western Front AssociationBM Box 1914LondonWC1N 3XXTelephone: +44 (0)207 118 1914\n\nPatron:\nProf. Hew Strachan FRSE, FRHistS\n\nPresident:\n\nProf. Peter Simkins MBE, FRHistS\n\nVice Presidents:\n\nProf. John Bourne BA, PhD, FRHistSProf. Gary Sheffield BA MA PhD Lt. Col. (ret'd) Christopher Pugsley DPhil, FRHistSGeneral (ret'd) the Lord Richard Dannatt GCB CBE MC DLDr. Roger Lee PhD jssc Lt. Col. (ret'd) Graham Parker OBEAndré CoilliotThe Burgomaster of YpresThe Mayor of Albert\n\nPast Patrons:\n\nSir John Glubb KCB, CMG, DSO, OBE, MC John Terraine FRHist.S Colonel Terry Cave, C.B.E.\n\nPast Presidents:\n\nJohn Terraine FRHist.SCorrelli Barnett C.B.E., D.Sc., MA, F.R.S.I, F.R. Hist..S., F.R.S.A.\n\nPast Vice Presidents: The Earl Kitchener TD, DL Tony Noyes CEng MICE John TolandHRH The Prince Albrecht, Duke of BavariaGeneral Sir Anthony Farrar-Hockley, GBE, KCB, DSO, MCThe Earl Haig, OBE, KStJ, DLCol Terry Cave CBEHon. Leonard G. Shurtleff\n\n"}
{"id": "379647", "url": "https://en.wikipedia.org/wiki?curid=379647", "title": "Yellowstone Caldera", "text": "Yellowstone Caldera\n\nThe Yellowstone Caldera is a volcanic caldera and supervolcano in Yellowstone National Park in the Western United States, sometimes referred to as the Yellowstone Supervolcano. The caldera and most of the park are located in the northwest corner of Wyoming. The major features of the caldera measure about 34 by 45 miles (55 by 72 km). \n\nThe caldera formed during the last of three supereruptions over the past 2.1 million years: the Huckleberry Ridge eruption 2.1 million years ago (which created the Island Park Caldera and the Huckleberry Ridge Tuff); the Mesa Falls eruption 1.3 million years ago (which created the Henry's Fork Caldera and the Mesa Falls Tuff); and the Lava Creek eruption approximately 630,000 years ago (which created the Yellowstone Caldera and the Lava Creek Tuff).\n\nVolcanism at Yellowstone is relatively recent, with calderas that were created during large eruptions that took place 2.1 million, 1.3 million, and 630,000 years ago. The calderas lie over a hotspot where light and hot magma (molten rock) from the mantle rises toward the surface. While the Yellowstone hotspot is now under the Yellowstone Plateau, it did help to create the eastern Snake River Plain (to the west of Yellowstone) through a series of huge volcanic eruptions. The hotspot appears to move across terrain in the east-northeast direction, but in fact the hotspot is much deeper than terrain and remains stationary while the North American Plate moves west-southwest over it. \n\nOver the past 18 million years or so, this hotspot has generated a succession of violent eruptions and less violent floods of basaltic lava. Together these eruptions have helped create the eastern part of the Snake River Plain from a once-mountainous region. At least a dozen of these eruptions were so massive that they are classified as supereruptions. Volcanic eruptions sometimes empty their stores of magma so swiftly that the overlying land collapses into the emptied magma chamber, forming a geographic depression called a caldera.\n\nThe oldest identified caldera remnant straddles the border near McDermitt, Nevada–Oregon, although there are volcaniclastic piles and arcuate faults that define caldera complexes more than in diameter in the Carmacks Group of southwest-central Yukon, Canada, which are interpreted to have been formed 70 million years ago by the Yellowstone hotspot. Progressively younger caldera remnants, most grouped in several overlapping volcanic fields, extend from the Nevada–Oregon border through the eastern Snake River Plain and terminate in the Yellowstone Plateau. One such caldera, the Bruneau-Jarbidge caldera in southern Idaho, was formed between 10 and 12 million years ago, and the event dropped ash to a depth of one foot (30 cm) away in northeastern Nebraska and killed large herds of rhinoceros, camel, and other animals at Ashfall Fossil Beds State Historical Park. The United States Geological Survey (\"USGS\") estimates there are one or two major caldera-forming eruptions and 100 or so lava extruding eruptions per million years, and \"several to many\" steam eruptions per century.\n\nThe loosely defined term \"supervolcano\" has been used to describe volcanic fields that produce exceptionally large volcanic eruptions. Thus defined, the Yellowstone Supervolcano is the volcanic field which produced the latest three supereruptions from the Yellowstone hotspot; it also produced one additional smaller eruption, thereby creating the West Thumb of Yellowstone Lake 174,000 years ago. The three supereruptions occurred 2.1 million, 1.3 million, and approximately 630,000 years ago, forming the Island Park Caldera, the Henry's Fork Caldera, and Yellowstone calderas, respectively. The Island Park Caldera supereruption (2.1 million years ago), which produced the Huckleberry Ridge Tuff, was the largest, and produced 2,500 times as much ash as the 1980 Mount St. Helens eruption. The next biggest supereruption formed the Yellowstone Caldera (~ 630,000 years ago) and produced the Lava Creek Tuff. The Henry's Fork Caldera (1.2 million years ago) produced the smaller Mesa Falls Tuff, but is the only caldera from the Snake River Plain-Yellowstone hotspot that is plainly visible today.\n\nNon-explosive eruptions of lava and less-violent explosive eruptions have occurred in and near the Yellowstone caldera since the last supereruption. The most recent lava flow occurred about 70,000 years ago, while a violent eruption excavated the West Thumb of Lake Yellowstone around 150,000 years ago. Smaller steam explosions occur as well: an explosion 13,800 years ago left a diameter crater at Mary Bay on the edge of Yellowstone Lake (located in the center of the caldera). Currently, volcanic activity is exhibited via numerous geothermal vents scattered throughout the region, including the famous Old Faithful Geyser, plus recorded ground-swelling indicating ongoing inflation of the underlying magma chamber.\n\nThe volcanic eruptions, as well as the continuing geothermal activity, are a result of a great cove of magma located below the caldera's surface. The magma in this cove contains gases that are kept dissolved by the immense pressure under which the magma is contained. If the pressure is released to a sufficient degree by some geological shift, then some of the gases bubble out and cause the magma to expand. This can cause a chain reaction. If the expansion results in further relief of pressure, for example, by blowing crust material off the top of the chamber, the result is a very large gas explosion.\n\nAccording to analysis of earthquake data in 2013, the magma chamber is long and wide. It also has underground volume, of which 6–8% is filled with molten rock. This is about 2.5 times bigger than scientists had previously imagined it to be; however, scientists believe that the proportion of molten rock in the chamber is much too low to allow another supereruption.\n\nThe source of the Yellowstone hotspot is controversial. Some geoscientists hypothesize that the Yellowstone hotspot is the effect of an interaction between local conditions in the lithosphere and upper mantle convection. Others suggest an origin in the deep mantle (mantle plume). Part of the controversy is the relatively sudden appearance of the hotspot in the geologic record. Additionally, the Columbia Basalt flows appeared at the same approximate time in the same place, causing speculation about their common origin. As the Yellowstone hotspot traveled to the east and north, the Columbia disturbance moved northward and eventually subsided.\n\nAn alternate theory to the mantle plume model was proposed in 2018. It is suggested that the volcanism may be caused by upwellings from the lower mantle resulting from water-rich fragments of the Farallon plate descending from the Cascadia subduction region, sheared off at a subducted spreading rift.\n\nVolcanic and tectonic actions in the region cause between 1,000 and 2,000 measurable earthquakes annually. Most are relatively minor, measuring a magnitude of 3 or weaker. Occasionally, numerous earthquakes are detected in a relatively short period of time, an event known as an earthquake swarm. In 1985, more than 3,000 earthquakes were measured over a period of several months. More than 70 smaller swarms were detected between 1983 and 2008. The USGS states these swarms are likely caused by slips on pre-existing faults rather than by movements of magma or hydrothermal fluids.\n\nIn December 2008, continuing into January 2009, more than 500 quakes were detected under the northwest end of Yellowstone Lake over a seven-day span, with the largest registering a magnitude of 3.9. Another swarm started in January 2010, after the Haiti earthquake and before the Chile earthquake. With 1,620 small earthquakes between January 17, 2010, and February 1, 2010, this swarm was the second-largest ever recorded in the Yellowstone Caldera. The largest of these shocks was a magnitude 3.8 that occurred on January 21, 2010. This swarm reached the background levels by February 21. On March 30, 2014, at 6:34 AM MST, a magnitude 4.8 earthquake struck Yellowstone, the largest recorded there since February 1980. In February 2018, more than 300 earthquakes occurred, with the largest being a magnitude 2.9.\n\nThe last full-scale eruption of the Yellowstone Supervolcano, the Lava Creek eruption which happened approximately 640,000 years ago, ejected approximately of rock, dust and volcanic ash into the sky.\n\nGeologists are closely monitoring the rise and fall of the Yellowstone Plateau, which has been rising as quickly as per year, as an indication of changes in magma chamber pressure.\n\nThe upward movement of the Yellowstone caldera floor between 2004 and 2008—almost each year—was more than three times greater than ever observed since such measurements began in 1923. From 2004 to 2008, the land surface within the caldera moved upward as much as at the White Lake GPS station. By the end of 2009, the uplift had slowed significantly and appeared to have stopped. In January 2010, the USGS stated that \"uplift of the Yellowstone Caldera has slowed significantly\" and that uplift continues but at a slower pace. The U.S. Geological Survey, University of Utah and National Park Service scientists with the Yellowstone Volcano Observatory maintain that they \"see no evidence that another such cataclysmic eruption will occur at Yellowstone in the foreseeable future. Recurrence intervals of these events are neither regular nor predictable.\" This conclusion was reiterated in December 2013 in the aftermath of the publication of a study by University of Utah scientists finding that the \"size of the magma body beneath Yellowstone is significantly larger than had been thought\". The Yellowstone Volcano Observatory issued a statement on its website stating,\n\nAlthough fascinating, the new findings do not imply increased geologic hazards at Yellowstone, and certainly do not increase the chances of a 'supereruption' in the near future. Contrary to some media reports, Yellowstone is not 'overdue' for a supereruption.\n\nOther media reports were more hyperbolic in their coverage.\n\nA study published in \"GSA Today\", the monthly news and science magazine of the Geological Society of America, identified three fault zones on which future eruptions are most likely to be centered. Two of those areas are associated with lava flows aged 174,000–70,000 years, and the third is a focus of present-day seismicity.\n\nIn 2017, NASA conducted a study to determine the feasibility of preventing the volcano from erupting. The results suggested that cooling the magma chamber by 35 percent would be enough to forestall such an incident. NASA proposed introducing water at high pressure 10 kilometers underground. The circulating water would release heat at the surface, possibly in a way that could be used as a power source. If enacted, the plan would cost about $3.46 billion. Nevertheless, according to Brian Wilson of the Jet Propulsion Laboratory, a completed project might trigger, instead of prevent, an eruption.\n\nStudies and analysis may indicate that the greater hazard comes from hydrothermal activity which occurs independently of volcanic activity. Over 20 large craters have been produced in the past 14,000 years, resulting in such features as Mary Bay, Turbid Lake, and Indian Pond which was created in an eruption about 1300 BC.\n\nIn a 2003 report, USGS researchers proposed that an earthquake may have displaced more than (576,000,000 US gallons) of water in Yellowstone Lake, creating colossal waves that unsealed a capped geothermal system and led to the hydrothermal explosion that formed Mary Bay.\n\nFurther research shows that very distant earthquakes reach and have effects upon the activities at Yellowstone, such as the 1992 7.3 magnitude Landers earthquake in California’s Mojave Desert that triggered a swarm of quakes from more than away, and the 2002 7.9 magnitude Denali fault earthquake away in Alaska that altered the activity of many geysers and hot springs for several months afterward.\n\nIn 2016, the United States Geological Survey announced plans to map the subterranean systems responsible for feeding the area's hydrothermal activity. According to the researchers, these maps could help predict when another supereruption occurs.\n\n\n\n"}
