{"id": "33151905", "url": "https://en.wikipedia.org/wiki?curid=33151905", "title": "(7888) 1993 UC", "text": "(7888) 1993 UC\n\n(7888) 1993 UC is a near-Earth minor planet in the Apollo group. It was discovered by Robert H. McNaught at the Siding Spring Observatory in Coonabarabran, New South Wales, Australia, on October 20, 1993. The asteroid has an observation arc of 23 years and has a well determined orbit. Its estimated size is 2.3 to 5.2 km.\n\nOn March 20, 2013, the asteroid passed 49 lunar distances or from Earth at a relative velocity of . The approach posed no threat to Earth. (7888) 1993 UC is not classified as a potentially hazardous asteroid (PHA) because its Earth MOID (Minimum Orbit Intersection Distance) is only 0.089 AU, and only objects with an Earth MOID less than 0.05 AU are considered PHAs.\n\nIt was discovered to be a binary asteroid by Arecibo Observatory in March 2013.\n\nOn April 29, 2146, the asteroid will pass from asteroid 4 Vesta.\n\n"}
{"id": "920549", "url": "https://en.wikipedia.org/wiki?curid=920549", "title": "2004 FU162", "text": "2004 FU162\n\nIt has only been observed once, on 31 March 2004, by the Lincoln Near-Earth Asteroid Research (LINEAR) team at Lincoln Laboratory's Experimental Test Site in Socorro, New Mexico, and remains a lost asteroid as of 2017. The estimated 4 to 6 meter sized body has made one of the closest known approaches to Earth.\n\nOn 31 March 2004, at 15:35 UTC, it passed within approximately 1 Earth radius () or 6,400 kilometers of the surface of the Earth (or 2.02  from its center). By comparison, geostationary satellites orbit at 5.6 and GPS satellites orbit at 3.17 from the center of the Earth.\n\nIt was only observed four times in the space of 44 minutes and could not be followed up. Nevertheless, \"the orbit is quite determinate and, given the exceptional nature of this close approach, the object is now receiving a designation\". No precovery images have been found.\n\nOn 26 March 2010, it may have come within 0.0825 AU (12.3 million km) of Earth, but with an uncertainty parameter of 9, the orbit is poorly determined.\n\nAnother, larger near-Earth asteroid, 2004 FH passed just two weeks prior to .\n\n"}
{"id": "13487138", "url": "https://en.wikipedia.org/wiki?curid=13487138", "title": "Arctic realm", "text": "Arctic realm\n\nThe Arctic realm is one of the planet's twelve marine realms, as designated by the WWF and Nature Conservancy. It includes the coastal regions and continental shelves of the Arctic Ocean and adjacent seas, including the Arctic Archipelago, Hudson Bay, and the Labrador Sea of northern Canada, the seas surrounding Greenland, the northern and eastern coasts of Iceland, and the eastern Bering Sea.\n\nThe Arctic realm transitions to the Temperate Northern Atlantic realm in the Atlantic Basin, and the Temperate Northern Pacific realm in the Pacific Basin.\n\nThe Arctic realm is further subdivided into 19 marine ecoregions:\n\n\n\n"}
{"id": "37049753", "url": "https://en.wikipedia.org/wiki?curid=37049753", "title": "Barents Sea Opening", "text": "Barents Sea Opening\n\nThe Barents Sea Opening (BSO) is an oceanographic term for the Western Barents Sea, the sea area between Bear Island in the south of Svalbard and the northern extremity of Norway through which a water mass of Atlantic origin flows into the Arctic Ocean. The inflow of relative warm water into the Arctic Ocean occurs not only through the Barents Sea Opening, but also through Fram Strait which is much deeper. The internal energy entering the colder waters has an influence on the atmosphere and the sea ice above and therefore possibly on the global climate.\n\nThe Norwegian Polar Institute performs about six\nhydrographic survey per year across the Barents Sea Opening from to since 1977. \nA set of oceanographic current meters captures the seasonal cycle of the inflow since 1997. Roughly every 30 nm two instruments are deployed, one at 50 m depth, and another 15 m above the sea floor.\n"}
{"id": "29673894", "url": "https://en.wikipedia.org/wiki?curid=29673894", "title": "Cave of Chinguaro", "text": "Cave of Chinguaro\n\nCave-Shrine of Chinguaro is a Roman Catholic church and cave located in Güímar on Tenerife (Canary Islands, Spain). It was the traditional palace of the Guanche King of the Menceyato de Güímar, Acaimo.\n\nIn this cave, the ancient Guanches worshiped the Virgin of Candelaria (Patron Saint of the Canary Islands) as the goddess Chaxiraxi of their traditional faith. This deity was worshiped in the Canary Islands until the Castillian conquest of the archipelago. The icon was later identified with the Virgin Mary and was moved by the Guanches themselves to the Cave of Achbinico in Candelaria. This cave was the first shrine devoted to the Virgin of Candelaria, and the first aboriginal Guanche shrine to contain a Christian idol in the Canary Islands. However, the Guanches at the time still generally adhered to their traditional religion.\n\nThe cave is also a place of great archaeological importance.\n\n\n"}
{"id": "1616580", "url": "https://en.wikipedia.org/wiki?curid=1616580", "title": "Chilbolton Observatory", "text": "Chilbolton Observatory\n\nThe Chilbolton Observatory is a facility for atmospheric and radio research located on the edge of the village of Chilbolton near Stockbridge in Hampshire, England. The facilities are run by the STFC Radio Communications Research Unit of the Rutherford Appleton Laboratory and form part of the Science and Technology Facilities Council.\n\nThe Chilbolton Observatory operates many pieces of research equipment associated with radar propagation and meteorology. , these include :\n\nThe observatory also hosts the UK's LOFAR station.\n\n\nConstruction of Chilbolton Observatory started in 1963. It was built partially on the site of RAF Chilbolton, which was decommissioned in 1946. Several sites around the south-east of England were considered for the construction. The site at Chilbolton, on the edge of Salisbury Plain, was chosen in part because of excellent visibility of the horizon and its relative remoteness from major roads whose cars could cause interference.\n\nThe facility was opened in April 1967. Within several months of being commissioned the azimuth bearing of the antenna suffered a catastrophic failure. GEC were contracted to repair the bearing and devised a system to replace the failed part while leaving the 400 tonne dish ostensibly in-place.\n\nOriginally, the antenna was engaged in K band radio astronomy, but now operates as a S and L band radar.\n\n"}
{"id": "30507352", "url": "https://en.wikipedia.org/wiki?curid=30507352", "title": "Closure (atmospheric science)", "text": "Closure (atmospheric science)\n\nA closure experiment in atmospheric science is a combination of different measurement techniques to describe the current state of the investigated system as fully as possible, and to find inaccuracies in one or some of the methods involved. The comparison of different types of measurement often involves model calculations, which may also be tested in this process.\n\nA common example for closure experiments are aerosol studies. Aerosols can be studied from space, from aircraft, and from the surface. The different properties of aerosols (chemical composition, particle size, particle number concentration, optical absorptivity, thermal emissivity, index of refraction, pattern of light scattering, etc.) require very different kinds of measurement techniques. Different instruments have been developed to perform each technique, and researchers usually specialize in one or a few of them. Due to the logistic efforts required to bring different groups of researchers with different measurement techniques together, such experiments stand out among routine experiments that are more focused on one or a few techniques and have fewer participants.\n\nAn aerosol closure experiment might look like this:\n\nAdditionally, measurements can be performed within the plume or outside, by means of remote sensing, from the surface, from aircraft, and from satellites in space. Different viewing geometries again introduce complexity, which can be assessed in closure experiments.\n\nAlgorithms exist, for example, to predict the optical properties from a given particle shape, size distribution, and index of refraction. Closure is achieved if the predictions of such an algorithm agree (within the limits of measurement uncertainty) with the optical properties that have been derived from the independent radiation measurements; if the lidar information about the vertical distribution of the aerosol sums up to the total optical depth determined by the sun photometer; etc.\n\nQuinn, P. K., and D. J. Coffman (1998), Local closure during the First Aerosol Characterization Experiment (ACE 1): Aerosol mass concentration and scattering and backscattering coefficients, J. Geophys. Res., 103(D13), 16,575–16,596, .\n\nhttp://hdl.handle.net/10068/250054\n"}
{"id": "5813824", "url": "https://en.wikipedia.org/wiki?curid=5813824", "title": "Complex volcano", "text": "Complex volcano\n\nA complex volcano, also called a compound volcano, is mixed landform consisting of related volcanic centers and their associated lava flows and pyroclastic rock. They may form due to changes in eruptive habit or in the location of the principal vent area on a particular volcano. Stratovolcanoes can also form a large caldera that gets filled in by a lava dome, or else multiple small cinder cones, lava domes and craters may develop on the caldera's rim.\n\nAlthough a comparatively unusual type of volcano, they are widespread in the world and in geologic history. Metamorphosed ash flow tuffs are widespread in the Precambrian rocks of northern New Mexico, which indicates that caldera complexes have been important for much of Earth's history. Yellowstone National Park is on three partly covered caldera complexes. The Long Valley Caldera in eastern California is also a complex volcano; the San Juan Mountains in southwestern Colorado are formed on a group of Neogene-age caldera complexes, and most of the Mesozoic and Cenozoic rocks of Nevada, Idaho, and eastern California are also caldera complexes and their erupted ash flow tuffs. The Bennett Lake Caldera in British Columbia and the Yukon Territory is another example of a Cenozoic (Eocene) caldera complex.\n\n\n"}
{"id": "28425269", "url": "https://en.wikipedia.org/wiki?curid=28425269", "title": "Cook Inlet taiga", "text": "Cook Inlet taiga\n\nThe Cook Inlet taiga ecoregion, in the Taiga and Boreal forests Biome, is located in Alaska.\n\nThis ecoregion is located around the upper Cook Inlet on the south-central coast of Alaska, sheltered by mountains on all sides. This coast has a gentle landscape and a relatively mild climate for Alaska with 380–680 mm of rainfall per year.\n\nThe plant life here is varied for Alaska, a mixture of conifers and other trees, shrubs and herbs, with the dominant trees being black spruce \"(Picea mariana)\", white spruce \"(Picea glauca)\", the large Sitka spruce \"(Picea sitchensis)\", quaking aspen \"(Populus tremuloides)\", balsam poplar \"(Populus balsamifera)\", black cottonwood \"(Populus trichocarpa)\" and paper birch \"(Betula papyrifera)\". The spruce forest is regularly renewed following severe damage by spruce bark beetle infestations.\n\nThis area is rich in wildlife including grey wolves, bears and Canada lynx. The Kenai River is home to five species of Pacific salmon including the largest chinook salmon in the world. Birds include large numbers of bald eagles and wintering snow geese from Wrangell Island who gather in the mouth of the Kenai River before their spring migration.\n\nThe Kenai River, Anchorage, Palmer and Wasilla areas are the most populated part of Alaska and a base for logging and the oil and gas industries on the Kenai Peninsula. There is also some clearance for farming in Palmer and Point MacKenzie. Nonetheless natural habitats remain very well preserved although wildlife of the Kenai Peninsula has become isolated from that of the Matanuska-Susitna Valley and the west side of Cook Inlet, and therefore from the rest of Alaska. Protected areas include Kenai National Wildlife Refuge, Chugach State Park, Nancy Lake State Recreation Area and the Susitna Flats.\n"}
{"id": "1558144", "url": "https://en.wikipedia.org/wiki?curid=1558144", "title": "Diamond willow", "text": "Diamond willow\n\nDiamond willow is a type of tree with wood that is deformed into diamond-shaped segments with alternating colors. This is most likely the result of attack by a fungus (\"Valsa sordida\" and possibly others), which causes cankers to form in the wood in response to the infection. Diamond willow is prized by wood carvers and furniture makers for its strong contrasting colors (red and white) and its sculptural irregularity of shape. \n\nThere are at least six different species that have been identified as being susceptible to diamonding, including \"Salix bebbiana\", the most common diamond willow, plus \"S. pseudomonticola\", \"S. arbusculoides\", \"S. discolor\", \"S. scouleriana\", and \"S. alaxensis\". \n\nThe diamonding is usually found with a branch at its center or is found in the Y of a tree. Diamonding in willow does not seem to be specific to an area that willows grow in, and where one bunch of willow will have diamonds, the next clump of willows may have none at all. Although diamond willow is often thought of as being a northern phenomenon, of the boreal forest, there is mention of diamond willow growing as far south as Missouri.\n\nThe tree grows diamond-shaped cankers in response to the fungus. The cankers seem to result from the tree growing away from the site of attack. This usually happens at the crotch of a branch on a larger branch or main stem. If the branch is relatively small it seems to die very quickly. If the branch is larger, it may continue to grow and the diamond is formed on the branch and the stem. By growing away from the fungus, new layers of growth occur further and further away from the site of the fungal attack. Thus the affected area gets larger and deeper. If the tree has been affected in several places close together, then the diamonds run into each other. This can result in pronounced ridges if some sapwood continues to survive, or it may strangle the small ridge of sapwood, which then dies. \n\nThe shape of the diamonds seems to vary from one clump of willow to the next, although there may be some general tendencies within a single species. Some stems will form long narrow diamonds; others will be short and wide. Usually all the diamonds on the stems in one clump will have similar growth patterns. If the new layers of sapwood do not move back very much each year, then the diamonds will be deep bowl- or cleft-shaped. These stems will be able to survive longer than those whose diamonds are flat \n\nThe bark that is left overtop of the diamond changes quite markedly from the bark over the living sapwood. Depending on the species of willow, the living bark is usually smoother and slightly lighter in color. The bark over the diamond usually becomes rougher and somewhat darker. It also becomes tougher and adheres much more to the underlying wood. The sapwood is white to cream in color—again depending on the species, but also on the location. The heartwood is reddish brown. This color tends to darken with exposure to light over a number of years.\n\nIf one stem in a clump of willow is affected, then all of them are likely to be. However, the neighboring clump may be completely without diamonds. Quaking Aspen (\"Populus tremuloides\") has been known to bear depressions that resemble diamonding.\n\n"}
{"id": "49305325", "url": "https://en.wikipedia.org/wiki?curid=49305325", "title": "Dušni Brav", "text": "Dušni Brav\n\nDušni Brav (; \"Soul Ram\" (also referred to as \"Dušno\")) ; refers to a practice of Christianized animal sacrifices amongst the Serbian Orthodox. It involves the ritualistic slaughter of a lamb for a funeral feast (\"Daća\").\n\nIt is believed that the sacrifice of the \"Dušni Brav\" is meant to pacify and appease the departed's soul.\n\nThe practice involves the blood sacrifice of a sheep (душни брав, \"dušni brav\") to the soul of the deceased. The animal is slaughtered in the deceased's backyard or property, on the day of their funeral or on the 40-day and/or year anniversary (годишњица, \"godišnjica\") of their death. The sheep must be the same gender as the deceased, and about the same age. No other animal but a white sheep may be sacrificed.\n\nBefore the sacrifice, a white cloth is put on top of the sacrificial sheep and a candle is lit. After the sacrifice, the sheep is roasted and prepared, and the white cloth is placed upon it again and it is served with Koliba at the Daća (funeral feast). Before eating the Dušni Brav it is proper etiquette to cross oneself.\n\n\n"}
{"id": "50438604", "url": "https://en.wikipedia.org/wiki?curid=50438604", "title": "E-st@r-II", "text": "E-st@r-II\n\ne-st@r-II (Educational Satellite @ Polytechnic University of Turin 2) is a miniaturized satellite designed and built by Polytechnic University of Turin, as part of the \"Fly Your Satellite\" program of the European Space Agency.\n\nIt is a CubeSat satellite, placed in orbit by Arianespace with Soyuz Booster, VS14 flight, on April 25, 2016 at 21:02 GMT (23:02 CEST) from Europe's spaceport in Kourou, French Guiana . The main payload of the launch was Sentinel-1B from program Copernicus and Microscope designed by the CNES, the French space agency.\n\nIn addition to e-st@ r-II there was other two CubeSats 1U (AAUSAT 4, .htm OUFTI 1) they were packed aboard the Soyuz. These small satellites, each measuring just 10 × 10 × 11 cm in height, have been developed by teams of university students through ESA programs.\n\nE-st@ r-II is the successor of e-st@r, the first Italian CubeSat and the first Cubesat of the Polytechnic University of Turin.\n\nMore than 30 students have been working on this project, with the unique opportunity of practical experience of space applications.\n\nE-st@r-II is a 1U CubeSat developed for demonstrating the autonomous active attitude control capabilities based on magnetic actuation: in fact, the payload is an Active Attitude Determination and Control System. The commissioning phase foresees that the payload is deactivated leaving the satellite in its free tumbling motion, without any attitude stabilization. The A-ADCS starts its work when commanded from GCS, controlling the angular velocities and the attitude of the satellite.\n\nThe primary scientific objectives of e-st@r-II mission are:\n\nE-st@r-II is a follow-on of e-st@r-I, the first Italian CubeSat and the first satellite of Politecnico di Torino to be inserted on orbit.\nMore than 30 students worked on this project, using a unique opportunity of hands-on experience on space applications.\nIt will demonstrate the ability of determining (with gyros and magnetometers) and actively controlling its attitude.\nThe bus functionalities, as a basis for other CubeSats, will be demonstrated: it is a step towards future missions and applications.\n\nOnce deployed into orbit, e-st@r-II will begin transmitting signals to Earth approximately 30 minutes after activation. Signals can be picked up by anyone with common amateur radio equipment. \nTo celebrate the launch of this second CubeSat, e-st@r-II, the world amateur radio community are invited in a contest to listen out for satellite.\n\nCurrently the team that developed the satellite, the CubeSatTeam of Politecnico di Torino, is actively working on development of the Next model: 3-STAR, a 3U CubeSat.\n\n\n"}
{"id": "3666280", "url": "https://en.wikipedia.org/wiki?curid=3666280", "title": "Eflatun Pınar", "text": "Eflatun Pınar\n\nEflatun Pınar (, \"Plato's Spring\") is the name given to a spring, which rises up from the ground, and the stone-built pool monument built at the time of the Hittite Empire. The spring lies inside the Lake Beyşehir National Park, west of Konya, and drains into Lake Beyşehir in central Anatolia at ancient Pisidia region. During the Late Bronze Age, a sacred pool monument was built here in trachyte ashlar masonry dedicated to the sacred spring cult of ancient Hittites. The monument was interpreted as a shrine to Plato during the medieval (Seljuk) period.\n\nEflatun Pınar's location near the lake shore corresponds to an almost exact level with other important ruins on the opposite shore, those of Kubadabad Palace, which were built by Seljuks.\n\nEflatun Pınar was briefly examined by the University of Oxford archaeologist Dr. Lucia Nixon in her paper on Çatalhöyük, and she makes use of F.W.Hasluck's early-20th century work. The site remains largely unexplored to date.\n\n\n"}
{"id": "23398912", "url": "https://en.wikipedia.org/wiki?curid=23398912", "title": "Energy in Chad", "text": "Energy in Chad\n\nThe most important measure in the energy balance of Chad is the total consumption of 200.00 million kWh of electric energy per year. Per capita this is an average of 13 kWh. Chad can provide itself completely with self-produced energy. The total production of all electric energy producing facilities is 215 m kWh, also 108% of own requirements. The rest of the self-produced energy is either exported into other countries or unused. Along with pure consumptions the production, imports and exports play an important role. Other energy sources such as natural gas or crude oil are also used.\n\nChad’s ability to achieve increased energy access and poverty reduction is constrained by significant challenges in the power sector. It currently only has about 125 MW of installed generation capacity to serve a population of 14.5 million people. As a result, Chad’s government is working to expand its electricity supply and encourage investment in the energy sector to stimulate the economy. Chad is endowed with the tenth-largest oil reserves in Africa, as well as wind and solar resource potential. \n"}
{"id": "31925401", "url": "https://en.wikipedia.org/wiki?curid=31925401", "title": "Enlil and Ninlil", "text": "Enlil and Ninlil\n\nEnlil and Ninlil or the Myth of Enlil and Ninlil or Enlil and Ninlil: The begetting of Nanna is a Sumerian creation myth, written on clay tablets in the mid to late 3rd millennium BC.\n\nThe first lines of the myth were discovered on the University of Pennsylvania Museum of Archaeology and Anthropology, catalogue of the Babylonian section (CBS), tablet number 9205 from their excavations at the temple library at Nippur. This was translated by George Aaron Barton in 1918 and first published as \"Sumerian religious texts\" in \"Miscellaneous Babylonian Inscriptions\", number seven, entitled \"A Myth of Enlil and Ninlil\". The tablet is by by at its thickest point. Barton noted that Theophilus G. Pinches had published part of an equivalent Akkadian version of the same story in 1911, noting \"The two texts in general agree closely, though there are minor variations here and there.\"\n\nAnother tablet from the same collection, number 13853 was used by Edward Chiera to restore part of the second column of Barton's tablet in \"Sumerian Epics and Myths\", number 77. Samuel Noah Kramer included CBS tablets 8176, 8315, 10309, 10322, 10412, 13853, 29.13.574 and 29.15.611. He also included translations from tablets in the Nippur collection of the Museum of the Ancient Orient in Istanbul, catalogue number 2707. Another tablet used as cuneiform source for the myth is held by the British Museum, BM 38600, details of which were published in 1919. Other tablets and versions were used to bring the myth to its present form with the latest composite text by Miguel Civil produced in 1989 with latest translations by Willem Römer in 1993 and Joachim Krecher in 1996.\n\nThe story opens with a description of the city of Nippur, its walls, river, canals and well, portrayed as the home of the gods and, according to Kramer \"that seems to be conceived as having existed before the creation of man.\" A.R. George suggests \"According to a well-known tradition, represented by the myth of Enlil and Ninlil, time was when Nippur was a city inhabited by gods not men, and this would suggest that it had existed from the very beginning.\" He discusses Nippur as the \"first city\" (uru-sag, 'city-head(top)') of Sumer. This conception of Nippur is echoed by Joan Goodnick Westenholz, describing the setting as \"civitas dei\", existing before the \"axis mundi\".\n\nThe story continues by introducing the goddess Nun-bar-ce-gunu warning her daughter Ninlil about the likelihood of romantic advances from Enlil if she strays too near the river. Ninlil resists Enlil's first approach after which he entreats his minister Nuska to take him across the river, on the other side the couple meet and float downstream, either bathing or in a boat, then lie on the bank together, kiss, and conceive Suen-Acimbabbar, the moon god. The story then cuts to Enlil walking in the Ekur, where the other gods arrest him for his relationship with Ninlil and exile him from the city for being ritually impure.\n\nThere follows three similar episodes as Enlil leaves the city, speaking to as the keeper of the city gate (\"keeper of the holy barrier\" or \"man of the pure lock\"), the man who guards Id-kura; the Sumerian river of the underworld (similar to the river Styx in Greek Mythology) and lastly SI.LU.IGI, the underworld ferryman (similar to Charon). Each time Enlil tells these characters \"When your lady Ninlil comes, if she asks after me, don't you tell her where I am!\". Ninlil follows him asking each \"When did your lord Enlil go by?\" To this, Enlil (in disguise) tells her \"My lord has not talked with me at all, O loveliest one. Enlil has not talked with me at all, O loveliest one\" upon which Ninlil offers to have sex with him and each time they conceive another god. Two of the offspring are gods of the underworld, Nergal-Meclamta-ea and Ninazu. The third god, Enbilulu is called the \"inspector of canals\", however Jeremy Black has linked this god to management of irrigation. The myth ends with praise for the fertility of Enlil and Ninlil.\n\nJeremy Black discusses the problems of serial pregnancy and multiple births along with the complex psychology of the myth. He also notes that there are no moral overtones about Enlil being ritually impure. Ewa Wasilewska noted about the location of the tale that \"Black and Green suggest the Sumerians located their underworld in the east mountains where the entrance to Kur was believed to exist. He (Enlil) was thus the 'King of the Foreign Lands/Mountains,' where the underworld to which he was banished and from which he returned, was located.\" Robert Payne has suggested that the initial scene of the courtship takes place on the bank of a canal instead of a river.\n\nHerman Behrens has suggested a ritual context for the myth where dramatic passages were acted out on a voyage between the Ekur and the sanctuary in Nippur. Jerrold Cooper has argued for a more sociological interpretation, explaining about the creation of gods who seem to perform as substitutes for Enlil, he suggests the purpose of the work is \"to tell the origins of four gods\" and that it \"explains why one (Suen) is shining in the heavens, while the other three dwell in the Netherworld\". Cooper also argues that the text uses local geographical placenames in regard to the netherworld.\n\nFrom the analysis of Thorkild Jacobsen, Dale Launderville has suggested the myth provides evidence that Sumerian society prohibited premarital sex in a discussion entitled \"Channeling the Sex Drive Toward the Creation of Community\". He discusses the attributes of the gods \"(1) the moon god was regarded as rejuvenating living things; (2) Nergal was associated occasionally with agricultural growth but more often with plague, pestilence, famine and sudden death; (3) Ninazu and (4) Enbilulu were forces that ensured successful agriculture.\" He concludes that the narrative exonerates Enlil and Ninlil indicating nature to have its way even where societal conventions try to contain sexual desire.\n\n\n\n"}
{"id": "2848333", "url": "https://en.wikipedia.org/wiki?curid=2848333", "title": "Epistles (Horace)", "text": "Epistles (Horace)\n\nThe Epistles (or Letters) of Horace were published in two books, in 20 BCE and 14 BCE, respectively.\n\n\nAs one commentator has put it: \"Horace's \"Epistles\" may be said to be a continuation of his Satires in the form of letters... But few of the epistles are [actually] letters except in form...\" They do indeed contain an excellent specimen of a letter of introduction (I.9); a piece of playful banter (I.14); pieces of friendly correspondence (I.3, I.4 and I.5); while the last, \"Epistle\" I.20, is inscribed 'To His Book,\" and forms a sort of epilogue to the \"Epistles\" he had already written. However, as a rule, the Epistles \"are compositions like those which Pope, following the manner of Horace, has made familiar to us as Moral Essays.\"\n\nThe \"Epistles\" were published about four years after the first three books of \"Odes,\" and were introduced by a special address to his patron Maecenas, as his \"Odes,\" \"Epodes,\" and \"Satires\" had been. The form of composition may have been suggested by some of the satires of Lucilius, which were composed as letters to his personal friends... \"From the \"Epistles\"... we gather that [Horace] had gradually adopted a more retired and meditative life, and had become fonder of the country and of study, and that while owing allegiance to no school or sect of philosophy, he was framing for himself a scheme of life, was endeavoring to conform to it, and was bent on inculcating it in others.\"\n\n\"In both his \"Satires\" and \"Epistles\", Horace shows himself a genuine moralist, a subtle observer and true painter of life, and an admirable writer.\" But in spirit the \"Epistles\" are more philosophic, more ethical and meditative. Like the \"Odes\" they exhibit the twofold aspects of Horace's philosophy, that of temperate Epicureanism and that of more serious and elevated conviction.\n\nBook 1 contains 20 Epistles.\n\n\nBook 2 consists of 3 \"epistles.\" However, the third epistle – the \"Ars Poetica\" – is usually treated as a separate composition. (For further discussion, see the Wikipedia article on the \"Ars Poetica\").\n\n\n\n\n"}
{"id": "2938915", "url": "https://en.wikipedia.org/wiki?curid=2938915", "title": "Functional group (ecology)", "text": "Functional group (ecology)\n\nA functional group is merely a set of species, or collection of organisms, that share alike characteristics within a community. Ideally, the lifeforms would perform equivalent tasks based on domain forces, rather than a common ancestor or evolutionary relationship. This could potentially lead to analogous structures that overrule the possibility of homology. More specifically, these beings produce resembling effects to external factors of an inhabiting system. Due to the fact that a majority of these creatures share an ecological niche, it is practical to assume they require similar structures in order to achieve the greatest amount of fitness. This refers to such as the ability to successfully reproduce to create offspring, and furthermore sustain life by avoiding alike predators and sharing meals.\n\nRather than the idea of this concept based upon a set of theories, functional groups are directly observed and determined by research specialists. It is important that this information is witnessed first-hand in order to state as usable evidence. Behavior and overall contribution to others are common key points to look for. Individuals use the corresponding perceived traits to further link genetic profiles to one another. Although, the life-forms themselves are different, variables based upon overall function and performance are interchangeable. These groups share an indistinguishable part within their energy flow, providing a key position within food chains and relationships within environment(s).\n\nWhat is an ecosystem and why is that important? An ecosystem is the biological organization that defines and expands on various environment factors- abiotic and biotic, that relate to simultaneous interaction. Whether it be a producer or relative consumer, each and every piece of life maintains a critical position in the ongoing survival rates of its own surroundings. As it pertains, a functional groups shares a very specific role within any given ecosystem and the process of cycling vitality.\n\nThere are generally two types of functional groups that range between flora and specific animal populations. Groups that relate to vegetation science, or flora, are known as plant functional types. Also referred to as PFT for short, those of such often share identical photosynthetic processes and require comparable nutrients. As an example, plants that undergo photosynthesis share an identical purpose in producing chemical energy for others. In contrast, those within the animal science range are called guilds, typically sharing feeding types. This could be easily simplified when viewing trophic levels. Examples include primary consumers, secondary consumers, tertiary consumers, and quaternary consumers.\n\nFunctional diversity is often referred to as the \"value and the range of those species and organismal traits that influence ecosystem functioning”. Traits of an organism that make it unique, for example, way it moves, gathers resources, reproduces, or the time of year it is active add to the overall diversity of an entire ecosystem, and therefore enhance the overall function, or productivity, of that ecosystem. Functional diversity increases the overall productivity of an ecosystem by allowing for an increase in niche occupation. Species have evolved to be more diverse through each epoch of time, with plants and insects having some of the most diverse families discovered thus far. The unique traits of an organism can allow a new niche to be occupied, allow for better defense against predators, and potentially lead to specialization. Organismal level functional diversity, which adds to the overall functional diversity of an ecosystem, is important for conservation efforts, especially in systems used for human consumption. Functional diversity can be difficult to measure accurately, but when done correctly, it provides useful insight to the overall function and stability of an ecosystem.\n\nFunctional redundancy refers to the phenomenon that species in the same ecosystem fill similar roles, which results in a sort of \"insurance\" in the ecosystem. Redundant species can easily do the job of a similar species from the same functional niche. This is possible because similar species have adapted to fill the same niche overtime. Functional redundancy varies across ecosystems and can vary from year to year depending on multiple factors including habitat availability, overall species diversity, competition among species for resources, and anthropogenic influence. This variation can lead to a fluctuation in overall ecosystem production. It is not always known how many species occupy a functional niche, and how much, if any, redundancy is occurring in each niche in an ecosystem. It is hypothesized that each important functional niche is filled by multiple species. Similar to functional diversity, there is no one clear method for calculating functional redundancy accurately, which can be problematic. One method is to account for the number of species occupying a functional niche, as well as the abundance of each species. This can indicate how many total individuals in an ecosystem are performing one function.\n\nStudies relating to functional diversity and redundancy occur in a large proportion of conservation and ecological research. As the human population increases, the need for ecosystem function subsequently increases. In addition, habitat destruction and modification continue to increase, and suitable habitat for many species continues to decrease, this research becomes more important. As the human population continues to expand, and urbanization is on the rise, native and natural landscapes are disappearing, being replaced with modified and managed land for human consumption. Alterations to landscapes are often accompanied with negative side effects including fragmentation, species losses, and nutrient runoff, which can effect the stability of an ecosystem, productivity of an ecosystem, and the functional diversity and functional redundancy by decreasing species diversity.\n\nIt has been shown that intense land use affects both the species diversity, and functional overlap, leaving the ecosystem and organisms in it vulnerable. Specifically, bee species, which we rely on for pollination services, have both lower functional diversity and species diversity in managed landscapes when compared to natural habitats, indicating that anthropogenic change can be detrimental for organismal functional diversity, and therefore overall ecosystem functional diversity. Additional research demonstrated that the functional redundancy of herbaceous insects in streams varies due to stream velocity, demonstrating that environmental factors can alter functional overlap. When conservation efforts begin, it is still up for debate whether preserving specific species, or functional traits is a more beneficial approach for the preservation of ecosystem function. Higher species, diversity can lead to an increase in overall ecosystem productivity, but does not necessarily insure the security of functional overlap. In ecosystems with high redundancy, losing a species (which lowers overall functional diversity) will not always lower overall ecosystem function due to high functional overlap, and thus in this instance it is most important to conserve a group, rather than an individual. In ecosystems with dominant species, which contribute to a majority of the biomass output, it may be more beneficial to conserve this single species, rather than a functional group.\n\nUnderstanding functional diversity and redundancy, and the roles each play in conservation efforts is often hard to accomplish because the tools with which we measure diversity and redundancy cannot be used interchangeably. Due to this, recent empirical work most often analyzes the effects of either functional diversity or functional redundancy, but not both. This does not create a complete picture of the factors influencing ecosystem production. In ecosystems with similar and diverse vegetation, functional diversity is more important for overall ecosystem stability and productivity. Yet, in contrast, functional diversity of native bee species in highly managed landscapes provided evidence for higher functional redundancy leading to higher fruit production, something humans rely heavily on for food consumption. A recent paper has stated that until a more accurate measuring technique is universally used, it is too early to determine which species, or functional groups, are most vulnerable and susceptible to extinction. Overall, understanding how extinction affects ecosystems, and which traits are most vulnerable can protect ecosystems as a whole.\n"}
{"id": "99750", "url": "https://en.wikipedia.org/wiki?curid=99750", "title": "Gandharva", "text": "Gandharva\n\nGandharva is a name used for distinct heavenly beings in Hinduism and Buddhism; it is also a term for skilled singers in Indian classical music.\n\nIn Hinduism, the gandharvas (Sanskrit: गन्धर्व, \"gandharva\", Assamese: গন্ধৰ্ব্ব \"gandharbba\", Bengali: গন্ধর্ব \"gandharba\", Kannada: ಗಂಧರ್ವ, Tamil: கந்தர்வர், Telugu: గంధర్వ \"Gandharvudu\", Malayalam: ഗന്ധർവൻ) are male nature spirits, husbands of the Apsaras. Some are part animal, usually a bird or horse. They have superb musical skills. They guarded the Soma and made beautiful music for the gods in their palaces. Gandharvas are frequently depicted as singers in the court of Gods.\n\nGandharvas act as messengers between the gods and humans. In Hindu law, a gandharva marriage is one contracted by mutual consent and without formal rituals.\n\nGandharvas are mentioned extensively in the epic Mahabharata as associated with the devas (as dancers and singers) and with the yakshas, as formidable warriors. They are mentioned as spread across various territories.\n\nVarious parentage is given for the gandharvas. They are called the creatures of Prajapati, of Brahma, of Kasyapa, of the Munis, of Arishta, or of Vāc.\n\nA gandharva (Sanskrit) or gandhabba (Pāli) (Japanese: 乾闥婆 \"Kendatsuba\") is one of the lowest-ranking devas in Buddhist cosmology. They are classed among the Cāturmahārājikakāyika devas, and are subject to the Great King , Guardian of the East. Beings are reborn among the gandharvas as a consequence of having practiced the most basic form of ethics (Janavasabha-sutta, DN.18). It was considered embarrassing for a monk to be born in no better birth than that of a gandharva.\n\nGandharvas can fly through the air, and are known for their skill as musicians. They are connected with trees and flowers, and are described as dwelling in the scents of bark, sap, and blossom. They are among the beings of the wilderness that might disturb a monk meditating alone.\n\nThe terms gandharva and are sometimes used for the same person; in these cases is the more general term, including a variety of lower deities.\n\nIn the \"Mahātanhasankhaya Sutta\" in the \"Majjhima Nikāya\", Gautama Buddha explains to his bhikkhus that an embryo develops when three conditions are met: the woman must be in the correct point of her menstrual cycle, the woman and man must have sexual intercourse, and a gandhabba must be present. According to the commentary to this sutta, in this instance the word \"gandhabba\" doesn't mean a celestial deva, but a being enabled to be born by its kamma.\n\nAmong the notable gandharvas are mentioned (in DN.20 and DN.32) Panāda, Opamañña, , Cittasena, Rājā. Janesabha is probably the same as Janavasabha, a rebirth of King Bimbisāra of Magadha. Mātali the Gandharva is the charioteer for Śakra.\n\nTimbarū (Tumburu) was a chieftain of the gandharvas. There is a romantic story told about the love between his daughter Bhaddā Suriyavacchasā (Sanskrit: Bhadrā Sūryavarcasā) and another gandharva, Pañcasikha (Sanskrit: Pañcaśikha). Pañcasikha fell in love with Suriyavacchasā when he saw her dancing before Śakra, but she was then in love with Sikhandī (or Sikhaddi), son of Mātali the charioteer. Pañcasikha then went to Timbarū's home and played a melody on his lute of \"beluva\"-wood, on which he had great skill, and sang a love-song in which he interwove themes about the Buddha and his arhats.\n\nLater, Śakra prevailed upon Pañcasikha to intercede with the Buddha so that Śakra might have an audience with him. As a reward for Pañcasikha's services, Śakra was able to get Suriyavacchasā, already pleased with Pañcasikha's display of skill and devotion, to agree to marry Pañcasikha.\n\nPañcasikha also acts as a messenger for the Four Heavenly Kings, conveying news from them to Mātali, the latter representing Śakra and the devas.\n\n\"Gandharva\" or \"gandhabba\" is also used in a completely different sense, referring to a being (or, strictly speaking, part of the causal continuum of consciousness) in a liminal state between death and rebirth.\n\nThere are many singers known as gandharvas for their mastery of Indian classical music. All of them, at one time or another, were theater actors who performed in various musicals. Their style of music is known as \"Kula Sangeet\" in Marathi, literally \"hereditary music\". They are regarded as masters of Indian classical music by the vast majority of the general population, predominantly in the state of Maharashtra.\n\n"}
{"id": "3379451", "url": "https://en.wikipedia.org/wiki?curid=3379451", "title": "Journey to the Centre of the Earth (album)", "text": "Journey to the Centre of the Earth (album)\n\nJourney to the Centre of the Earth is the third album by English keyboardist Rick Wakeman, released on 3 May 1974 by A&M Records. It is a live recording of the second of his two concerts at the Royal Festival Hall on 18 January 1974, the premiere of his 40-minute orchestral rock piece based on Jules Verne's science fiction novel of the same name. It tells the story of Professor Lidenbrook, his nephew Axel, and their guide Hans, who follow a passage to the Earth's centre originally discovered by Arne Saknussemm, an Icelandic alchemist. Wakeman performs with the London Symphony Orchestra, the English Chamber Choir, and a group of hand-picked musicians for his rock band, which later became the English Rock Ensemble. Actor David Hemmings narrates the story.\n\n\"Journey to the Centre of the Earth\" was overall well received by music critics. It reached No. 1 on the UK Albums Chart, the first album from A&M to do so, and peaked at No. 3 on the \"Billboard\" 200 in the United States. It was certified gold by the Recording Industry Association of America in October 1974 for selling 500,000 copies. The album earned Wakeman a nomination for an Ivor Novello Award and a Grammy Award for Best Pop Instrumental Performance. In 1999, Wakeman released a sequel album \"Return to the Centre of the Earth\". After the original score was presumed lost, Wakeman was reunited with it in 2009 and re-recorded the album three years later with 18 minutes of music previously cut due to time constraints.\n\nBy mid-1973, Wakeman had been with the progressive rock band Yes for almost two years, and he had released his debut solo effort, \"The Six Wives of Henry VIII\", to critical acclaim. For his next release Wakeman wished to make an album that told a story with its music, something that he had been inspired to do so since his father took him to see a performance of the symphonic fairy tale \"Peter and the Wolf\" by Sergei Prokofiev which features a narrator telling the story and an orchestra illustrating the action. Wakeman had wanted to do an orchestral rock piece based on the 1864 science fiction novel \"Journey to the Centre of the Earth\" by Jules Verne as early as November 1971, but he put the project on hold until he had finished recording \"The Six Wives of Henry VIII\" in October 1972, and had accumulated some money and had written some music for it.\n\nThe project developed in December 1972 when Wakeman took part in the orchestral concerts of The Who's rock opera \"Tommy\" at the Rainbow Theatre in London, which featured the London Symphony Orchestra, the English Chamber Choir, conductor David Measham, and musical arranger Wil Malone. Wakeman told his idea for \"Journey to the Centre of the Earth\" to the show's producer Lou Reizner, who put him in contact with Measham to further discuss plans. Wakeman then produced a demo tape that contained a rough outline of the overall structure of the music using a Minimoog synthesiser, Mellotron, Rhodes piano and clavinet and presented it to Measham, indicating where the orchestral parts were to be placed. After Measham agreed to be involved, Wakeman met with his manager Brian Lane to pitch the idea of performing it with an orchestra, choir, and a rock band. As the cost of recording the album in a studio was too high, Wakeman's label A&M Records agreed to have the work recorded live in concert. To help finance the project, Wakeman sold several of his cars and \"mortgaged himself up to the hilt\" to cover the estimated £40,000 in costs.\n\nAfter the album received the green-light from A&M Records, Wakeman worked on the music \"on and off\" through 1973 and had assistance with the orchestral and choir arrangements with Malone and Danny Beckerman; the latter first met Wakeman during a Yes tour of Australia. A typical session had Malone devising chords and melody lines while Beckerman wrote the parts out on a score, which took several hours. It was Malone's first attempt at writing for a symphony orchestra' he had not received classical training. The original score lasted 55 minutes but it was reduced to 40 so it could fit the time constraints of an LP. Malone called the project a challenge and \"completely different\" to what he had been involved with previously. A&M Records had wanted Wakeman to select a group of known musicians to play in his rock band, but he opposed to the idea as he intended for the public to like the album for its music rather than the performers. Wakeman chose a group that he used to play with at the Valiant Trooper, a pub in Holmer Green in Buckinghamshire. \"I'd played with them for fun quite a bit on Sunday evenings...I was playing keyboards with the lads when I thought, they could play \"Journey\" for me. I'm sure they could do the concert and do it well\". He picked vocalists Ashley Holt of Warhorse and Gary Pickford-Hopkins from Wild Turkey, drummer Barney James, also of Warhorse, bassist Roger Newell, and guitarist Mike Egan, who had also played on \"The Six Wives of Henry VIII\". The first bassist picked was Dave Wintour, also a performer on \"Six Wives\". Actor and singer Richard Harris was the first choice to narrate the story but he was unavailable, so Wakeman picked actor David Hemmings.\n\nThe 40-minute piece is in four distinct sections: \"The Journey\", \"Recollection\", \"The Battle\", and \"The Forest\". Wakeman wrote all the lyrics and narration. Wakeman was not confident with his lyric writing, and admitted that his first set of lyrics were \"really bad\" which prompted a rewrite. His band nicknamed him \"Longfellow\". \"The Forest\" includes an excerpt of \"In the Hall of the Mountain King\" by Edvard Grieg.\n\nGerman Professor Lidenbrook discovers an old parchment that detailed a journey to the centre of Earth undertaken by Arne Saknussemm, an Icelandic alchemist. The parchment, when decoded into Latin and translated by Lidenbrook's nephew Axel, reveals an entrance to the route in the extinct volcano of Snæfellsjökull in Iceland. The pair embark on their journey with their guide Hans.\n\nUpon entering the volcano they pass a lava gallery and find themselves in an intersection of two paths. Lidenbrook chooses the eastern tunnel, but after three days it had taken the trio to a dead end. They returned with just one day's supply of water, reaching the intersection weak and tired. After sleep, they continued their journey and Hans hears flowing water behind a wall of rock and attacks it with a pick axe, revealing a stream of boiling water they named the Hansbach.\n\nThe three temporarily separate, and a lone Axel becomes increasingly frightened. Thinking of those who he had left at home, he cries and runs through a tunnel blindly. He almost gives up, but suddenly hears Lidenbrook's voice in the distance and calculates he is just four miles apart and sets off to reunite. At one point the ground beneath Axel collapses and he finds himself with Lidenbrook and Hans in a giant mushroom forest nearby cliffs and sea.\n\nThe trio build a raft and set sail for a port they named after Axel's fiancee, Port Grauben. Five days into their sail, they witness a battle between an \"Ichthyosaurus\" and \"Plesiosaurus\". The \"Ichthyosaurus\" wins, and the travellers are hit with a four-day storm and take shelter by some overhanging rocks. The storm had caused them to travel only some miles north of Port Grauben, so they set on land to track Saknussem's original route once more. They cross a plain of bones and into a forest inhabited by giant Mastodons led by a 12-ft high Proteus, a mythological human. Stunned, the three flee the forest for the Lidenbrook Sea and enter a dark tunnel that plunged deep into rock which they blast through with dynamite. The explosion causes an earthquake, and they become trapped in an active volcano shaft which projects them to the surface of the Earth by Mount Etna in Sicily.\n\nThe concerts and album were first announced in October 1973, and organised during a break when Yes were touring \"Tales from Topographic Oceans\". Two sell-out performances were held at 6 and 8 p.m. on Friday 18 January 1974 at the Royal Festival Hall in London, attended by 3,000 people each. Rehearsals took place at Farmyard Studios in Little Chalfont, owned by Trevor Morais and on the day of the shows, began at 9 a.m. Each performance lasted for one hour and forty minutes. Performing with Wakeman and his band were the London Symphony Orchestra and the English Chamber Choir conducted by Measham. A projection screen was placed above the stage, initially to display stock footage of mountains and caves, but permission was granted from 20th Century Fox to show excerpts from the 1959 adventure film of the same name to accompany the music. An initial plan was to have the concerts filmed for a prospective home video release when it was \"commercially viable\", but it did not come into fruition.\n\nThe shows were introduced with an excerpt of Symphony No. 1 by Sergei Rachmaninoff. The first half was taken up by \"Catherine Parr\", \"Catherine Howard\", and \"Anne Boleyn\" from \"The Six Wives of Henry VIII\", with comical pieces \"A Road to Ruin\" and a comical rendition of \"Twelfth Street Rag\" with bajos, minstrel dancers, and accompanying footage of Laurel and Hardy and various silent films. Wakeman thought the segment would be a disaster as the film arrived at the venue shortly before the first performance and resorted to improvising on the piano without knowing what the footage was. Wakeman wanted the first half to be \"musical and entertaining\" and a way of easing \"Journey\" to the audience, which made up the second half of the program. The encore was The Pearl and Dean Piano Concerto, a humorous piece based on various television and film music. News reporter Chris Welch attended the shows and noted: \"Several members of the choir could be seen jiving during the more rhythmic moments, and when Rick played some beautiful classical piano, approving nods could be detected from the massed ranks of the orchestra\". A party was held after the second show which Wakeman did not attend due to exhaustion. According to Welch, \"He was driven home – asleep\".\n\nWakeman had hoped to record both concerts and select the best performance of the two, but the London Symphony Orchestra requested double pay if this went ahead. He then took \"the frightening decision of only recording the second performance and hoping there weren't too many mistakes\". The performance was recorded using Ronnie Lane's Mobile Studio which housed a 16-track studio fitted in an Airstream trailer. The first half of the second show was recorded, initially as a test to see if the equipment worked correctly. It remained unreleased until 2002 as part of Wakeman's limited edition box set \"Treasure Chest\".\n\nThe recordings were produced by Wakeman, and mixed by him and engineer Paul Tregurtha at Morgan Studios in London from 21 to 29 January 1974. They encountered a number of problems during this time. Wakeman said: \"Someone in the street had accidentally kicked out the vocal mike cable just before we started recording. So we boosted up the vocals that were picked up on the other mikes\". A snare drum and its microphone broke during the performance, and Hemmings re-recorded some narration in the studio after a tape change occurred during of one of his passages. There were four bars of \"complete shambles\" between the orchestra and the band, so an identical passage that occurred later in the performance was inserted.\n\nThe original plan was for A&M Records to produce the album quickly for a February 1974 release, but the additional time required to fix the recordings and a shortage of vinyl at the time caused the label to push the release to early April. This sparked concern from management for potential bootleg recordings of the concert to be sold to the public. A&M reported that a later release would \"tie in more conveniently with Wakeman's plans\" as he had resumed touring with Yes during this time. Wakeman heard cuts of the album during the subsequent Yes tour, rejecting several of them. \"I just didn't like the sound, and it was worth doing it properly for the sake of a few extra days\". Another factor in the delay was a paper shortage as the original album design consisted of a gatefold sleeve with an 8-page booklet, but the designer refused to reduce the package to a standard sleeve.\n\nUpon its arrival at A&M Records, the finished album was poorly received among management; they refused to sell it. However, as Wakeman was under contract with A&M in the United States, a cassette was sent to co-founder Jerry Moss in California, who subsequently agreed to release the record. According to Wakeman, the album received 50,000 advanced orders.\n\nReleased on 3 May 1974, \"Journey to the Centre of the Earth\" topped the UK Albums Chart for one week, the first album from A&M to do so. It peaked at No. 3 on the US \"Billboard\" 200 chart for two weeks in July during a stay of 27 weeks. The album became a multimillion-dollar seller in six weeks. Wakeman received an Ivor Novello Award for the album, and it earned him a Grammy Award nomination for Best Pop Instrumental Performance. The record was certified gold by the Recording Industry Association of America in September 1974, and a year later in Brazil. It was subsequently released in the four-channel Quadradisc CD-4 format. The album has sold 14 million copies worldwide.\n\nIn 1999, marking the album's 25th anniversary, Wakeman released a sequel titled \"Return to the Centre of the Earth\". The story follows a group of adventurers who attempt to follow the previous expedition to the Earth's centre as discovered by Saknussemm.\n\nIn 2002, Wakeman released the 8-CD compilation box set \"Treasure Chest\" which contained the previously unreleased first half of the second concert at the Royal Festival Hall. The recording was presumed to have been wiped, but a rough mix was accidentally discovered on a poorly conditioned and mislabelled tape initially used as a guide for the mastering, and was digitally remastered. The CD also contained Hemmings record narration in five dialects during a recording session when he and Wakeman had been drinking while the album was being mixed.\n\nIn May 2016, a 3 CD+DVD Super Deluxe Edition box set was released containing a new remaster of the original album, live performances from 1974 and 1993, and a DVD-Audio with a Quad surround sound mix and Mobile Fidelity Sound Lab mix.\n\nThe album received some negative reaction upon its release, with music critics having described the record as a \"classical pastiche...genuinely appalling\" and \"brutal synthesiser overkill\". \"Journey\" however, was well received by others. A journalist for \"The Sunday Times\" missed the Royal Festival Hall concert, but thought on record the music \"comes over magnificently ... a striking work which only occasionally lapses into pretentiousness\". Music journalist Chris Welch of \"Melody Maker\" thought the album was \"entertaining, fresh and disalarmingly unpretentious ... This could be a score for a Hollywood musical – tuneful, but with epic overtones\". Welch noted Wakeman's \"familiarity of the story\" and his \"close observance to detail engenders a warmth to the work, which made it a resounding success as a concert performance\". In a retrospective review, Mike DeGange of Allmusic called the album \"one of progressive rock's crowning achievements\" and noted \"interesting conglomerations of orchestral and synthesized music\".\n\nWakeman presumed the original conductor's score to the piece, which had been kept in storage, was lost following the collapse of his management company MAM Records in the early 1980s. He recalled that no one had knowledge of its location thereafter, but wished not to rewrite it as he thought it would not live up to the quality of the original. In 2009 however, a box arrived at his house which stayed in his garage for about five months before Wakeman looked through it and found the original score at the bottom which by then had suffered from water damage. The score was digitised and pieced together in the course of a year with assistance from conductor and arranger Guy Protheroe, resulting in a new studio re-recording of the album with the 18 minutes of music that had been previously cut. It was recorded from July to September 2012 with the Orion Orchestra, the English Chamber Choir, and members of his rock band, the English Rock Ensemble. As Hemmings died in 2003, the narration is voiced by actor Peter Egan. Released on 20 November 2012, the new album is packaged with a one-off magazine published by \"Classic Rock\" and a replica of the 1974 Royal Festival Hall concert program and 132-page booklet.\n\nCredits adapted from the album's 1974 liner notes.\n\nMusicians\n\nProduction\n\nCitations\nSources\n"}
{"id": "17327", "url": "https://en.wikipedia.org/wiki?curid=17327", "title": "Kinetic energy", "text": "Kinetic energy\n\nIn physics, the kinetic energy of an object is the energy that it possesses due to its motion.\nIt is defined as the work needed to accelerate a body of a given mass from rest to its stated velocity. Having gained this energy during its acceleration, the body maintains this kinetic energy unless its speed changes. The same amount of work is done by the body when decelerating from its current speed to a state of rest.\n\nIn classical mechanics, the kinetic energy of a non-rotating object of mass \"m\" traveling at a speed \"v\" is . In relativistic mechanics, this is a good approximation only when \"v\" is much less than the speed of light.\n\nThe standard unit of kinetic energy is the joule.\n\nThe adjective \"kinetic\" has its roots in the Greek word κίνησις \"kinesis\", meaning \"motion\". The dichotomy between kinetic energy and potential energy can be traced back to Aristotle's concepts of actuality and potentiality.\n\nThe principle in classical mechanics that \"E ∝ mv\" was first developed by Gottfried Leibniz and Johann Bernoulli, who described kinetic energy as the \"living force\", \"vis viva\". Willem 's Gravesande of the Netherlands provided experimental evidence of this relationship. By dropping weights from different heights into a block of clay, Willem 's Gravesande determined that their penetration depth was proportional to the square of their impact speed. Émilie du Châtelet recognized the implications of the experiment and published an explanation.\n\nThe terms \"kinetic energy\" and \"work\" in their present scientific meanings date back to the mid-19th century. Early understandings of these ideas can be attributed to Gaspard-Gustave Coriolis, who in 1829 published the paper titled \"Du Calcul de l'Effet des Machines\" outlining the mathematics of kinetic energy. William Thomson, later Lord Kelvin, is given the credit for coining the term \"kinetic energy\" c. 1849–51.\n\nEnergy occurs in many forms, including chemical energy, thermal energy, electromagnetic radiation, gravitational energy, electric energy, elastic energy, nuclear energy, and rest energy. These can be categorized in two main classes: potential energy and kinetic energy. Kinetic energy is the movement energy of an object. Kinetic energy can be transferred between objects and transformed into other kinds of energy.\n\nKinetic energy may be best understood by examples that demonstrate how it is transformed to and from other forms of energy. For example, a cyclist uses chemical energy provided by food to accelerate a bicycle to a chosen speed. On a level surface, this speed can be maintained without further work, except to overcome air resistance and friction. The chemical energy has been converted into kinetic energy, the energy of motion, but the process is not completely efficient and produces heat within the cyclist.\n\nThe kinetic energy in the moving cyclist and the bicycle can be converted to other forms. For example, the cyclist could encounter a hill just high enough to coast up, so that the bicycle comes to a complete halt at the top. The kinetic energy has now largely been converted to gravitational potential energy that can be released by freewheeling down the other side of the hill. Since the bicycle lost some of its energy to friction, it never regains all of its speed without additional pedaling. The energy is not destroyed; it has only been converted to another form by friction. Alternatively, the cyclist could connect a dynamo to one of the wheels and generate some electrical energy on the descent. The bicycle would be traveling slower at the bottom of the hill than without the generator because some of the energy has been diverted into electrical energy. Another possibility would be for the cyclist to apply the brakes, in which case the kinetic energy would be dissipated through friction as heat.\n\nLike any physical quantity that is a function of velocity, the kinetic energy of an object depends on the relationship between the object and the observer's frame of reference. Thus, the kinetic energy of an object is not invariant.\n\nSpacecraft use chemical energy to launch and gain considerable kinetic energy to reach orbital velocity. In an entirely circular orbit, this kinetic energy remains constant because there is almost no friction in near-earth space. However, it becomes apparent at re-entry when some of the kinetic energy is converted to heat. If the orbit is elliptical or hyperbolic, then throughout the orbit kinetic and potential energy are exchanged; kinetic energy is greatest and potential energy lowest at closest approach to the earth or other massive body, while potential energy is greatest and kinetic energy the lowest at maximum distance. Without loss or gain, however, the sum of the kinetic and potential energy remains constant.\n\nKinetic energy can be passed from one object to another. In the game of billiards, the player imposes kinetic energy on the cue ball by striking it with the cue stick. If the cue ball collides with another ball, it slows down dramatically, and the ball it hit accelerates its speed as the kinetic energy is passed on to it. Collisions in billiards are effectively elastic collisions, in which kinetic energy is preserved. In inelastic collisions, kinetic energy is dissipated in various forms of energy, such as heat, sound, binding energy (breaking bound structures).\n\nFlywheels have been developed as a method of energy storage. This illustrates that kinetic energy is also stored in rotational motion.\n\nSeveral mathematical descriptions of kinetic energy exist that describe it in the appropriate physical situation. For objects and processes in common human experience, the formula ½mv² given by Newtonian (classical) mechanics is suitable. However, if the speed of the object is comparable to the speed of light, relativistic effects become significant and the relativistic formula is used. If the object is on the atomic or sub-atomic scale, quantum mechanical effects are significant, and a quantum mechanical model must be employed.\n\nIn classical mechanics, the kinetic energy of a \"point object\" (an object so small that its mass can be assumed to exist at one point), or a non-rotating rigid body depends on the mass of the body as well as its speed. The kinetic energy is equal to 1/2 the product of the mass and the square of the speed. In formula form:\n\nwhere formula_2 is the mass and formula_3 is the speed (or the velocity) of the body. In SI units, mass is measured in kilograms, speed in metres per second, and the resulting kinetic energy is in joules.\n\nFor example, one would calculate the kinetic energy of an 80 kg mass (about 180 lbs) traveling at 18 metres per second (about 40 mph, or 65 km/h) as\n\nWhen a person throws a ball, the person does work on it to give it speed as it leaves the hand. The moving ball can then hit something and push it, doing work on what it hits. The kinetic energy of a moving object is equal to the work required to bring it from rest to that speed, or the work the object can do while being brought to rest: net force × displacement = kinetic energy, i.e.,\n\nSince the kinetic energy increases with the square of the speed, an object doubling its speed has four times as much kinetic energy. For example, a car traveling twice as fast as another requires four times as much distance to stop, assuming a constant braking force. As a consequence of this quadrupling, it takes four times the work to double the speed.\n\nThe kinetic energy of an object is related to its momentum by the equation:\n\nwhere:\n\nFor the \"translational kinetic energy,\" that is the kinetic energy associated with rectilinear motion, of a rigid body with constant mass formula_8, whose center of mass is moving in a straight line with speed formula_10, as seen above is equal to\n\nwhere:\n\nThe kinetic energy of any entity depends on the reference frame in which it is measured. However the total energy of an isolated system, i.e. one in which energy can neither enter nor leave, does not change over time in the reference frame in which it is measured. Thus, the chemical energy converted to kinetic energy by a rocket engine is divided differently between the rocket ship and its exhaust stream depending upon the chosen reference frame. This is called the Oberth effect. But the total energy of the system, including kinetic energy, fuel chemical energy, heat, etc., is conserved over time, regardless of the choice of reference frame. Different observers moving with different reference frames would however disagree on the value of this conserved energy.\n\nThe kinetic energy of such systems depends on the choice of reference frame: the reference frame that gives the minimum value of that energy is the center of momentum frame, i.e. the reference frame in which the total momentum of the system is zero. This minimum kinetic energy contributes to the invariant mass of the system as a whole.\n\nThe work done in accelerating a particle with mass \"m\" during the infinitesimal time interval \"dt\" is given by the dot product of \"force\" F and the infinitesimal \"displacement \"dx\"\nwhere we have assumed the relationship p = \"m\" v and the validity of Newton's Second Law. (However, also see the special relativistic derivation below.)\n\nApplying the product rule we see that:\n\nTherefore, (assuming constant mass so that \"dm\"=0), we have,\n\nSince this is a total differential (that is, it only depends on the final state, not how the particle got there), we can integrate it and call the result kinetic energy. Assuming the object was at rest at time 0, we integrate from time 0 to time t because the work done by the force to bring the object from rest to velocity \"v\" is equal to the work necessary to do the reverse:\n\nThis equation states that the kinetic energy (\"E\") is equal to the integral of the dot product of the velocity (v) of a body and the infinitesimal change of the body's momentum (p). It is assumed that the body starts with no kinetic energy when it is at rest (motionless).\n\nIf a rigid body Q is rotating about any line through the center of mass then it has \"rotational kinetic energy\" (formula_18) which is simply the sum of the kinetic energies of its moving parts, and is thus given by:\n\nwhere:\n\n(In this equation the moment of inertia must be taken about an axis through the center of mass and the rotation measured by ω must be around that axis; more general equations exist for systems where the object is subject to wobble due to its eccentric shape).\n\nA system of bodies may have internal kinetic energy due to the relative motion of the bodies in the system. For example, in the Solar System the planets and planetoids are orbiting the Sun. In a tank of gas, the molecules are moving in all directions. The kinetic energy of the system is the sum of the kinetic energies of the bodies it contains.\n\nA macroscopic body that is stationary (i.e. a reference frame has been chosen to correspond to the body's center of momentum) may have various kinds of internal energy at the molecular or atomic level, which may be regarded as kinetic energy, due to molecular translation, rotation, and vibration, electron translation and spin, and nuclear spin. These all contribute to the body's mass, as provided by the special theory of relativity. When discussing movements of a macroscopic body, the kinetic energy referred to is usually that of the macroscopic movement only. However all internal energies of all types contribute to body's mass, inertia, and total energy.\n\nThe speed, and thus the kinetic energy of a single object is frame-dependent (relative): it can take any non-negative value, by choosing a suitable inertial frame of reference. For example, a bullet passing an observer has kinetic energy in the reference frame of this observer. The same bullet is stationary to an observer moving with the same velocity as the bullet, and so has zero kinetic energy. By contrast, the total kinetic energy of a system of objects cannot be reduced to zero by a suitable choice of the inertial reference frame, unless all the objects have the same velocity. In any other case, the total kinetic energy has a non-zero minimum, as no inertial reference frame can be chosen in which all the objects are stationary. This minimum kinetic energy contributes to the system's invariant mass, which is independent of the reference frame.\n\nThe total kinetic energy of a system depends on the inertial frame of reference: it is the sum of the total kinetic energy in a center of momentum frame and the kinetic energy the total mass would have if it were concentrated in the center of mass.\n\nThis may be simply shown: let formula_22 be the relative velocity of the center of mass frame \"i\" in the frame \"k\".\nSince formula_23,\n\nHowever, let formula_25 the kinetic energy in the center of mass frame, formula_26 would be simply the total momentum that is by definition zero in the center of mass frame, and let the total mass: formula_27. Substituting, we get:\n\nThus the kinetic energy of a system is lowest to center of momentum reference frames, i.e., frames of reference in which the center of mass is stationary (either the center of mass frame or any other center of momentum frame). In any different frame of reference, there is additional kinetic energy corresponding to the total mass moving at the speed of the center of mass. The kinetic energy of the system in the center of momentum frame is a quantity that is invariant (all observers see it to be the same).\n\nIt sometimes is convenient to split the total kinetic energy of a body into the sum of the body's center-of-mass translational kinetic energy and the energy of rotation around the center of mass (rotational energy):\n\nwhere:\n\nThus the kinetic energy of a tennis ball in flight is the kinetic energy due to its rotation, plus the kinetic energy due to its translation.\n\nIf a body's speed is a significant fraction of the speed of light, it is necessary to use relativistic mechanics to calculate its kinetic energy. In special relativity theory, the expression for linear momentum is modified.\n\nWith \"m\" being an object's rest mass, v and \"v\" its velocity and speed, and \"c\" the speed of light in vacuum, we use the expression for linear momentum formula_30, where formula_31.\n\nIntegrating by parts yields\nSince formula_33,\nformula_35 is a constant of integration for the indefinite integral.\nSimplifying the expression we obtain\nformula_35 is found by observing that when formula_38 and formula_39, giving\nresulting in the formula\n\nThis formula shows that the work expended accelerating an object from rest approaches infinity as the velocity approaches the speed of light. Thus it is impossible to accelerate an object across this boundary.\n\nThe mathematical by-product of this calculation is the mass-energy equivalence formula—the body at rest must have energy content\n\nAt a low speed (formula_3«formula_44), the relativistic kinetic energy is approximated well by the classical kinetic energy. This is done by binomial approximation or by taking the first two terms of the Taylor expansion for the reciprocal square root:\n\nSo, the total energy formula_46 can be partitioned into the rest mass energy plus the Newtonian kinetic energy at low speeds.\n\nWhen objects move at a speed much slower than light (e.g. in everyday phenomena on Earth), the first two terms of the series predominate. The next term in the Taylor series approximation\n\nis small for low speeds. For example, for a speed of the correction to the Newtonian kinetic energy is 0.0417 J/kg (on a Newtonian kinetic energy of 50 MJ/kg) and for a speed of 100 km/s it is 417 J/kg (on a Newtonian kinetic energy of 5 GJ/kg).\n\nThe relativistic relation between kinetic energy and momentum is given by\n\nThis can also be expanded as a Taylor series, the first term of which is the simple expression from Newtonian mechanics:\nThis suggests that the formulae for energy and momentum are not special and axiomatic, but concepts emerging from the equivalence of mass and energy and the principles of relativity.\n\nUsing the convention that\n\nwhere the four-velocity of a particle is\n\nand formula_52 is the proper time of the particle, there is also an expression for the kinetic energy of the particle in general relativity.\n\nIf the particle has momentum\n\nas it passes by an observer with four-velocity \"u\", then the expression for total energy of the particle as observed (measured in a local inertial frame) is\n\nand the kinetic energy can be expressed as the total energy minus the rest energy:\n\nConsider the case of a metric that is diagonal and spatially isotropic (\"g\",\"g\",\"g\",\"g\"). Since\n\nwhere \"v\" is the ordinary velocity measured w.r.t. the coordinate system, we get\n\nSolving for \"u\" gives\n\nThus for a stationary observer (\"v\"= 0)\n\nand thus the kinetic energy takes the form\n\nFactoring out the rest energy gives:\n\nThis expression reduces to the special relativistic case for the flat-space metric where\n\nIn the Newtonian approximation to general relativity\n\nwhere Φ is the Newtonian gravitational potential. This means clocks run slower and measuring rods are shorter near massive bodies.\n\nIn quantum mechanics, observables like kinetic energy are represented as operators. For one particle of mass \"m\", the kinetic energy operator appears as a term in the Hamiltonian and is defined in terms of the more fundamental momentum operator formula_66. The kinetic energy operator in the non-relativistic case can be written as\n\nNotice that this can be obtained by replacing formula_68 by formula_66 in the classical expression for kinetic energy in terms of momentum,\n\nIn the Schrödinger picture, formula_66 takes the form formula_72 where the derivative is taken with respect to position coordinates and hence\n\nThe expectation value of the electron kinetic energy, formula_74, for a system of \"N\" electrons described by the wavefunction formula_75 is a sum of 1-electron operator expectation values:\nwhere formula_77 is the mass of the electron and formula_78 is the Laplacian operator acting upon the coordinates of the \"i\" electron and the summation runs over all electrons.\n\nThe density functional formalism of quantum mechanics requires knowledge of the electron density \"only\", i.e., it formally does not require knowledge of the wavefunction. Given an electron density formula_79, the exact N-electron kinetic energy functional is unknown; however, for the specific case of a 1-electron system, the kinetic energy can be written as\nwhere formula_81 is known as the von Weizsäcker kinetic energy functional.\n\n\n"}
{"id": "27931595", "url": "https://en.wikipedia.org/wiki?curid=27931595", "title": "Kramers' opacity law", "text": "Kramers' opacity law\n\nKramers' opacity law describes the opacity of a medium in terms of the ambient density and temperature, assuming that the opacity is dominated by bound-free absorption (the absorption of light during ionization of a bound electron) or free-free absorption (the absorption of light when scattering a free ion, also called bremsstrahlung). It is often used to model radiative transfer, particularly in stellar atmospheres. The relation is named after the Dutch physicist Hendrik Kramers, who first derived the form in 1923.\n\nThe general functional form of the opacity law is\n\nformula_1\n\nwhere formula_2 is the resulting average opacity, formula_3 is the density and formula_4 the temperature of the medium. Often the overall opacity is inferred from observations, and this form of the relation describes how changes in the density or temperature will affect the opacity.\n\nThe specific forms for bound-free and free-free are\n\nBound-free:\nformula_5\n\nFree-free:\nformula_6\n\nElectron-scattering:\nformula_7\n\nHere, formula_8 and formula_9 are the Gaunt factors (quantum mechanical correction terms) associated with bound-free and free-free transitions respectively. The formula_10 is an additional correction factor, typically having a value between 1 and 100. The opacity depends on the number density of electrons and ions in the medium, described by the fractional abundance (by mass) of elements heavier than hydrogen formula_11, and the fractional abundance (by mass) of hydrogen formula_12.\n\n"}
{"id": "2969194", "url": "https://en.wikipedia.org/wiki?curid=2969194", "title": "List of Lepidoptera that feed on Calluna", "text": "List of Lepidoptera that feed on Calluna\n\nHeather (Calluna vulgaris) is used as a food plant by the larvae of a number of Lepidoptera species, including:\n\n\n"}
{"id": "14578196", "url": "https://en.wikipedia.org/wiki?curid=14578196", "title": "List of Superfund sites in Pennsylvania", "text": "List of Superfund sites in Pennsylvania\n\nThis is a list of Superfund sites in Pennsylvania designated under the Comprehensive Environmental Response, Compensation, and Liability Act (CERCLA) environmental law. The CERCLA federal law of 1980 authorized the United States Environmental Protection Agency (EPA) to create a list of polluted locations requiring a long-term response to clean up hazardous material contaminations. These locations are known as Superfund sites, and are placed on the National Priorities List (NPL). \n\nThe NPL guides the EPA in \"determining which sites warrant further investigation\" for environmental remediation. As of November 29, 2010, there were 95 Superfund sites on the National Priorities List in Pennsylvania. Two additional sites are currently proposed for entry on the list. Twenty-eight sites have been cleaned up and removed from the list.\n\n\n"}
{"id": "19561913", "url": "https://en.wikipedia.org/wiki?curid=19561913", "title": "List of Ultras of the Himalayas", "text": "List of Ultras of the Himalayas\n\nThis is a list of all the Ultra prominent peaks (with topographic prominence greater than 1,500 metres) in the Himalayas. Listed separately, to the west and north-west are the Karakoram and Hindu Kush Ultras, and while to the north-east and east are the ultras of Tibet.\n\n9 of the 10 Himalayan 8,000m peaks are ultras (the exception is Lhotse), and there are a further 28 peaks over 7000m.\n\nThe Kali Gandaki River is normally considered to mark the divide between the Western and Eastern Himalayas.\n\n"}
{"id": "16657872", "url": "https://en.wikipedia.org/wiki?curid=16657872", "title": "List of alluvial sites in Switzerland", "text": "List of alluvial sites in Switzerland\n\nThe List of alluvial sites in Switzerland, listing the Swiss heritage floodplains. It is based on the \"Federal Inventory of Alluvial sites of National Importance\", part of a 1992 Ordinance of the Swiss Federal Council implementing the Federal Law on the Protection of Nature and Cultural Heritage.\n\n\n\n[[Category:Switzerland geography-related lists]\n[[Category:Nature-related lists]]\n[[Category:Protected areas of Switzerland]]\n[[Category:European drainage basins of the Atlantic Ocean|Switzerland]]"}
{"id": "40940425", "url": "https://en.wikipedia.org/wiki?curid=40940425", "title": "List of longest rivers of Mexico", "text": "List of longest rivers of Mexico\n\nAmong the longest rivers of Mexico are 26 streams of at least . In the case of rivers such as the Colorado, the length listed in the table below is solely that of the main stem. In the case of the Grijalva and Usumacinta, it is the combined lengths of two river systems that share a delta. In the case of the Nazas and Aguanaval, it is the combined lengths of separate rivers that flow into the same closed basin.\n\nThree rivers in this list cross international boundaries or form them. The Colorado and the Rio Grande (Río Bravo del Norte or Río Bravo) begin in the United States and flow into Mexico, while the Usumacinta begins in Guatemala and flows into Mexico. \n\nThe primary source for the length, watershed, and surface runoff data in the table below is the 10th edition of \"Statistics on Water in Mexico\", published by the National Water Commission in Mexico (CONAGUA); exceptions are as noted. U.S. states and departments of Guatemala appear in italics in the \"States\" column.\n\n\n\n"}
{"id": "36891624", "url": "https://en.wikipedia.org/wiki?curid=36891624", "title": "List of nature centers in Florida", "text": "List of nature centers in Florida\n\nThere are nature centers and environmental education centers throughout the state of Florida. To use the sortable tables: click on the icons at the top of each column to sort that column in alphabetical order; click again for reverse alphabetical order.\n\n\n"}
{"id": "9152771", "url": "https://en.wikipedia.org/wiki?curid=9152771", "title": "List of pines by region", "text": "List of pines by region\n\nThis is a list of pine species by geographical distribution. For a taxonomy of the genus, see Pinus classification.\n\n\n\n\n\n\n"}
{"id": "9238941", "url": "https://en.wikipedia.org/wiki?curid=9238941", "title": "List of rivers of Ethiopia", "text": "List of rivers of Ethiopia\n\nThis is a list of streams and rivers in Ethiopia, arranged geographically by drainage basin. There is an alphabetic list at the end of this article. \n\n\n\n\n\n\nAbay River - Adabay River - Akaki River - Akobo River - Ala River - Alero River - Angereb River - Ataba River - Ataye River - Atbarah River - Awash River - Awetu River - Ayesha River\n\nBalagas River - Baro River - Bashilo River - Beles River - Bilate River - Birbir River - Blue Nile - Borkana River\n\nCheleleka River\n\nDabus River - Dawa River - Dechatu River - Dembi River - Denchya River - Dhidheesa River - Dinder River - Doha River - Dukem River - Durkham River\n\nErer River\n\nFafen River\n\nGaletti River - Ganale Dorya River - Gebba River - Gebele River - Germama River - Gestro River - Gidabo River - Gibe River - Gilgel Gibe River - Gilo River - Gojeb River - Golima River - Gololcha River - Greater Angereb River - Guder River - Gumara River\n\nHanger River - Hawadi River\n\nJamma River - Jerer River - Jikawo River - Jubba River\n\nKabenna River - Karsa River - Katar River - Keleta River - Kibish River - Kulfo River\n\nLagabora River - Lesser Abay - Lesser Angereb - Logiya River\n\nMago River - Magech River - Mareb River - Meki River - Mena River - Mille River - Modjo River - Mofar River - Muger River - Mui River\n\nNeri River \n\nOmo River\n\nPibor River\n\nQechene River\n\nRahad River - Reb River - Robe River\n\nSagan River - Shebelle River - Shinfa River - Sor River\n\nTekezé River\nTella River \nUsno River - Ubbi Ubbi River\n\nWabe River - Walaqa River - Wajja River - Wanchet River - Wari River - Weito River - Weyib River - Welmel River\n\nYabus River\n\nZarima River\n\n"}
{"id": "50749661", "url": "https://en.wikipedia.org/wiki?curid=50749661", "title": "List of species named simplex", "text": "List of species named simplex\n\nThe species name simplex (Latin for \"simple\") occurs frequently in binomial names throughout the taxonomy of life. Examples include:\n\n\n\n\n\n\n\n\n\n\"Moths of the family Noctuidae:\"\n\n\"Other lepidopterans:\"\n\n\n\n"}
{"id": "3207063", "url": "https://en.wikipedia.org/wiki?curid=3207063", "title": "Mashu", "text": "Mashu\n\nMashu, as described in the Epic of Gilgamesh of Mesopotamian mythology, is a great cedar mountain through which the hero-king Gilgamesh passes via a tunnel on his journey to Dilmun after leaving the Cedar Forest, a forest of ten thousand leagues span. Siduri, the Alewife, lived on the shore, associated with \"the Waters of Death\" that Gilgamesh had to cross to reach Utnapishtim, the far-away.\n\nThe corresponding location in reality has been the topic of speculation, as no confirming evidence has been found. Jeffrey H. Tigay suggests that in the Sumerian version, through its association with the sun god Utu, \"(t)he Cedar Mountain is implicitly located in the east, whereas in the Akkadian versions, Gilgamesh's destination (is) removed from the east\" and \"explicitly located in the north west, in or near Lebanon\".\n\n\nJennifer Westwood: \"Gilgamesh & Other Babylonian Tales\", 1968, Coward-McCann, New York\n"}
{"id": "47172131", "url": "https://en.wikipedia.org/wiki?curid=47172131", "title": "National Kitchens", "text": "National Kitchens\n\nNational Kitchens were restaurants established in a British Government initiative during the First World War to feed people cheaply and economically, at a time when food supplies were scarce because of the German U-boat campaign.\n\nBefore the outbreak of war in 1914, the United Kingdom relied on imported food to feed the population; as much as 60 percent of food stocks had come from abroad. In wartime, the increased costs of shipping together with a complete lack of any government controls led to a rapid rise in the price of food, especially meat and bread. In addition, the Imperial German Navy had launched an unrestricted submarine blockade; in April 1917, a record 550,000 tons of shipping had been sunk. During the first years of the war, voluntary organisations began to open \"communal kitchens\" in various parts of the country. However, their public image was dreadful and only the poorest people made use of them. One government official stated: \"It was thought that Public Kitchens were to be inflicted on the poor as some kind of punishment for a crime unstated\". The newly created Ministry of Food adopted the idea of community kitchens but realised that they would have to be much better presented.\n\nAccording to historian Bryce Evans, the government was highly sensitive to possible criticism of the National Kitchens as being soup kitchens. The Ministry of Food Control stated that the National Kitchens \"must not resemble a soup kitchen for the poorest section of society\" and should instead be places for \"ordinary people in ordinary circumstances\". A meal of soup, meat and vegetables was available for as little as sixpence, equivalent to roughly £1 today. However, in some kitchens there was nowhere to sit and in others patrons had to bring their own mugs and plates. In order to further distance them from charitable canteens, the kitchens were run in a businesslike manner: in at least one kitchen it was possible to \"buy your Sunday dinner on Saturday\"; the ability to show the means to pay for a meal in advance, and to make reservations as at a restaurant, would contribute to the image that the Kitchens were for \"ordinary people.\"\n\nThe first National Kitchen was opened by Queen Mary in Westminster Bridge Road, London, on 21 May 1917. By late 1918 there were 363 National Kitchens. The kitchens were partly funded by the state and could typically feed up to 2,000 people per day. They were mainly staffed by volunteers, particularly well-to-do women who were anxious to \"do their bit\" for the war effort; serving in the kitchens became known as \"canteening\".\n\nA typical menu comprised:\n\n\n\nEvans, Bryce (2017) The British 'National Kitchen' of the First World War. Journal of War & Culture Studies, 10. ISSN 1752-6272\n"}
{"id": "28610468", "url": "https://en.wikipedia.org/wiki?curid=28610468", "title": "North Central Rockies forest", "text": "North Central Rockies forest\n\nThe North Central Rockies forests is a temperate coniferous forest ecoregion of Canada and the United States. This region gets more rain on average than the South Central Rockies forests and is notable for containing the only inland populations of many species from the Pacific coast.\n\nThis ecoregion is located in the Rocky Mountains regions of southeastern British Columbia, southwestern Alberta, northwestern Montana and northern Idaho. The climate here is varied. Areas west of the Continental Divide experience greater precipitation and the moderating effects of the Pacific Ocean, while areas east of the Divide experience a drier, more continental climate. In the Canadian portion of the ecoregion, mean annual temperatures range from in the east to west, summer mean temperatures range from to , and average winter temperatures range from to . Valleys experience warm, wet summers and mildly cold, snowy winters, while subalpine zones experience cool, wet summers with the possibility of frosts, and very cold, snowy winters. Precipitation is moderate to high, with valleys usually receiving between and , and high elevations receiving well over .\n\nThis ecoregion is predominately coniferous forest. Lower elevation forests are dominated by western hemlock (\"Tsuga heterophylla\"), western red cedar (\"Thuja plicata\") and subalpine fir (\"Abies lasiocarpa\"), with smaller populations of lodgepole pine (\"Pinus contorta\"), douglas-fir (\"Pseudotsuga menziesii\"), western white pine (\"Pinus monticola\") and western larch (\"Larix occidentalis\"). Subalpine zones are dominated by Engelmann spruce (\"Picea engelmanni\"), subalpine fir (\"Abies lasiocarpa\"), and, in areas affected by fire, lodgepole pine (\"Pinus contorta\"). This ecoregion also contains meadows, foothill grasslands, riverside woodlands, and tree line/alpine zone communities.\n\nMammals of the North Central Rockies forests include the gray wolf (\"Canis lupus\"), grizzly bear (\"Ursus arctos horriblus\"), wolverine (\"Gulo gulo\"), woodland caribou (\"Rangifer tarandus caribou\"), black bear (\"Ursus americanus cinnamomum\"), mountain goat (\"Oreamnos americanus\"), mule deer (\"Odocoileus hemonius\"), white-tailed deer (\"Odocoileus virginianus\"), Rocky Mountain elk (\"Cervus canadensis nelson\"), moose (\"Alces alces\"), coyote (\"Canis latrans\"), cougar (\"Puma concolor\"), bobcat (\"Lynx rufus\"), fisher (\"Martes pennanti\"), and American marten (\"Martes americana\").\n\nThough large portions of this ecoregion are intact and protected, its conservation status is listed as \"vulnerable\". The main threats to this ecoregion's integrity are resource extraction and development, increasing human activity, logging, mining, livestock grazing and the introduction of exotic species.\nProtected areas in this ecoregion include Glacier National Park in northwestern Montana, Yoho and Kootenay National Parks in southeastern British Columbia, Waterton Lakes National Park in far southwestern Alberta and the Selway-Bitterroot Wilderness in northeastern Idaho.\n\n"}
{"id": "25111839", "url": "https://en.wikipedia.org/wiki?curid=25111839", "title": "Overexploitation", "text": "Overexploitation\n\nOverexploitation, also called overharvesting, refers to harvesting a renewable resource to the point of diminishing returns. Continued overexploitation can lead to the destruction of the resource. The term applies to natural resources such as: wild medicinal plants, grazing pastures, game animals, fish stocks, forests, and water aquifers.\n\nIn ecology, overexploitation describes one of the five main activities threatening global biodiversity. Ecologists use the term to describe populations that are harvested at a rate that is unsustainable, given their natural rates of mortality and capacities for reproduction. This can result in extinction at the population level and even extinction of whole species. In conservation biology the term is usually used in the context of human economic activity that involves the taking of biological resources, or organisms, in larger numbers than their populations can withstand. The term is also used and defined somewhat differently in fisheries, hydrology and natural resource management.\n\nOverexploitation can lead to resource destruction, including extinctions. However it is also possible for overexploitation to be sustainable, as discussed below in the section on fisheries. In the context of fishing, the term overfishing can be used instead of overexploitation, as can overgrazing in stock management, overlogging in forest management, overdrafting in aquifer management, and endangered species in species monitoring. Overexploitation is not an activity limited to humans. Introduced predators and herbivores, for example, can overexploit native flora and fauna.\n\nConcern about overexploitation is relatively recent, though overexploitation itself is not a new phenomenon. It has been observed for millennia. For example, ceremonial cloaks worn by the Hawaiian kings were made from the mamo bird; a single cloak used the feathers of 70,000 birds of this now-extinct species. The dodo, a flightless bird from Mauritius, is another well-known example of overexploitation. As with many island species, it was naive about certain predators, allowing humans to approach and kill it with ease.\n\nFrom the earliest of times, hunting has been an important human activity as a means of survival. There is a whole history of overexploitation in the form of overhunting. The overkill hypothesis (Quaternary extinction events) explains why the megafaunal extinctions occurred within a relatively short period of time. This can be traced with human migration. The most convincing evidence of this theory is that 80% of the North American large mammal species disappeared within 1000 years of the arrival of humans on the western hemisphere continents. The fastest ever recorded extinction of megafauna occurred in New Zealand, where by 1500 AD, just 200 years after settling the islands, ten species of the giant moa birds were hunted to extinction by the Māori. A second wave of extinctions occurred later with European settlement.\n\nIn more recent times, overexploitation has resulted in the gradual emergence of the concepts of sustainability and sustainable development, which has built on other concepts, such as sustainable yield, eco-development and deep ecology.\n\nOverexploitation doesn't necessarily lead to the destruction of the resource, nor is it necessarily unsustainable. However, depleting the numbers or amount of the resource can change its quality. For example, footstool palm is a wild palm tree found in Southeast Asia. Its leaves are used for thatching and food wrapping, and overharvesting has resulted in its leaf size becoming smaller.\n\nThe tragedy of the commons refers to a dilemma described in an article by that name written by Garrett Hardin and first published in the journal \"Science\" in 1968.\n\nCentral to Hardin's essay is an example which is a useful parable for understanding how overexploitation can occur. This example was first sketched in an 1833 pamphlet by William Forster Lloyd, as a hypothetical and simplified situation based on medieval land tenure in Europe, of herders sharing a common on which they are each entitled to let their cows graze. In Hardin's example, it is in each herder's interest to put each succeeding cow he acquires onto the land, even if the carrying capacity of the common is exceeded and it is temporarily or permanently damaged for all as a result. The herder receives all of the benefits from an additional cow, while the damage to the common is shared by the entire group. If all herders make this individually rational economic decision, the common will be overexploited or even destroyed to the detriment of all. However, since all herders reach the same rational conclusion, overexploitation in the form of overgrazing occurs, with immediate losses, and the pasture may be degraded to the point where it gives very little return.\n\"Therein is the tragedy. Each man is locked into a system that compels him to increase his herd without limit—in a world that is limited. Ruin is the destination toward which all men rush, each pursuing his own interest in a society that believes in the freedom of the commons.\" (Hardin, 1968)\nIn the course of his essay, Hardin develops the theme, drawing in many examples of latter day commons, such as national parks, the atmosphere, oceans, rivers and fish stocks. The example of fish stocks had led some to call this the \"tragedy of the fishers\". A major theme running through the essay is the growth of human populations, with the Earth's finite resources being the general common.\n\nThe tragedy of the commons has intellectual roots tracing back to Aristotle, who noted that \"what is common to the greatest number has the least care bestowed upon it\", as well as to Hobbes and his \"Leviathan\". The opposite situation to a tragedy of the commons is sometimes referred to as a tragedy of the anticommons: a situation in which rational individuals, acting separately, collectively waste a given resource by underutilizing it.\n\nThe tragedy of the commons can be avoided if it is appropriately regulated. Hardin's use of \"commons\" has frequently been misunderstood, leading Hardin to later remark that he should have titled his work \"The tragedy of the unregulated commons\".\n\nIn wild fisheries, overexploitation or overfishing occurs when a fish stock has been fished down \"below the size that, on average, would support the long-term maximum sustainable yield of the fishery\". However, overexploitation can be sustainable.\n\nWhen a fishery starts harvesting fish from a previously unexploited stock, the biomass of the fish stock will decrease, since harvesting means fish are being removed. For sustainability, the rate at which the fish replenish biomass through reproduction must balance the rate at which the fish are being harvested. If the harvest rate is increased, then the stock biomass will further decrease. At a certain point, the maximum harvest yield that can be sustained will be reached, and further attempts to increase the harvest rate will result in the collapse of the fishery. This point is called the maximum sustainable yield, and in practice, usually occurs when the fishery has been fished down to about 30% of the biomass it had before harvesting started.\n\nIt is possible to fish the stock down further to, say, 15% of the pre-harvest biomass, and then adjust the harvest rate so the biomass remains at that level. In this case, the fishery is sustainable, but is now overexploited, because the stock has been run down to the point where the sustainable yield is less than it could be.\n\nFish stocks are said to \"collapse\" if their biomass declines by more than 95 percent of their maximum historical biomass. Atlantic cod stocks were severely overexploited in the 1970s and 1980s, leading to their abrupt collapse in 1992. Even though fishing has ceased, the cod stocks have failed to recover. The absence of cod as the apex predator in many areas has led to trophic cascades.\n\nAbout 25% of world fisheries are now overexploited to the point where their current biomass is less than the level that maximizes their sustainable yield. These depleted fisheries can often recover if fishing pressure is reduced until the stock biomass returns to the optimal biomass. At this point, harvesting can be resumed near the maximum sustainable yield.\n\nThe tragedy of the commons can be avoided within the context of fisheries if fishing effort and practices are regulated appropriately by fisheries management. One effective approach may be assigning some measure of ownership in the form of individual transferable quotas (ITQs) to fishermen. In 2008, a large scale study of fisheries that used ITQs, and ones that didn't, provided strong evidence that ITQs help prevent collapses and restore fisheries that appear to be in decline.\n\nWater resources, such as lakes and aquifers, are usually renewable resources which naturally recharge (the term fossil water is sometimes used to describe aquifers which don't recharge). Overexploitation occurs if a water resource, such as the Ogallala Aquifer, is mined or extracted at a rate that exceeds the recharge rate, that is, at a rate that exceeds the practical sustained yield. Recharge usually comes from area streams, rivers and lakes. An aquifer which has been overexploited is said to be overdrafted or depleted. Forests enhance the recharge of aquifers in some locales, although generally forests are a major source of aquifer depletion. Depleted aquifers can become polluted with contaminants such as nitrates, or permanently damaged through subsidence or through saline intrusion from the ocean.\n\nThis turns much of the world's underground water and lakes into finite resources with peak usage debates similar to oil. These debates usually centre around agriculture and suburban water usage but generation of electricity from nuclear energy or coal and tar sands mining is also water resource intensive. A modified Hubbert curve applies to any resource that can be harvested faster than it can be replaced. Though Hubbert's original analysis did not apply to renewable resources, their overexploitation can result in a Hubbert-like peak. This has led to the concept of peak water.\n\nForests are overexploited when they are logged at a rate faster than reforestation takes place. Reforestation competes with other land uses such as food production, livestock grazing, and living space for further economic growth. Historically utilization of forest products, including timber and fuel wood, have played a key role in human societies, comparable to the roles of water and cultivable land. Today, developed countries continue to utilize timber for building houses, and wood pulp for paper. In developing countries almost three billion people rely on wood for heating and cooking. Short-term economic gains made by conversion of forest to agriculture, or overexploitation of wood products, typically leads to loss of long-term income and long term biological productivity. West Africa, Madagascar, Southeast Asia and many other regions have experienced lower revenue because of overexploitation and the consequent declining timber harvests.\n\nOverexploitation is one of the main threats to global biodiversity. Other threats include pollution, introduced and invasive species, habitat fragmentation, habitat destruction, uncontrolled hybridization, global warming, ocean acidification and the driver behind many of these, human overpopulation.\n\nOne of the key health issues associated with biodiversity is drug discovery and the availability of medicinal resources. A significant proportion of drugs are natural products derived, directly or indirectly, from biological sources. Marine ecosystems are of particular interest in this regard. However unregulated and inappropriate bioprospecting could potentially lead to overexploitation, ecosystem degradation and loss of biodiversity.\n\nOverexploitation threatens one-third of endangered vertebrates, as well as other groups. Excluding edible fish, the illegal trade in wildlife is valued at $10 billion per year. Industries responsible for this include the trade in bushmeat, the trade in Chinese medicine, and the fur trade. The Convention for International Trade in Endangered Species of Wild Fauna and Flora, or CITES was set up in order to control and regulate the trade in endangered animals. It currently protects, to a varying degree, some 33,000 species of animals and plants. It is estimated that a quarter of the endangered vertebrates in the United States of America and half of the endangered mammals is attributed to overexploitation.\n\nAll living organisms require resources to survive. Overexploitation of these resources for protracted periods can deplete natural stocks to the point where they are unable to recover within a short time frame. Humans have always harvested food and other resources they have needed to survive. Human populations, historically, were small, and methods of collection limited to small quantities. With an exponential increase in human population, expanding markets and increasing demand, combined with improved access and techniques for capture, are causing the exploitation of many species beyond sustainable levels. In practical terms, if continued, it reduces valuable resources to such low levels that their exploitation is no longer sustainable and can lead to the extinction of a species, in addition to having dramatic, unforeseen effects, on the ecosystem. Overexploitation often occurs rapidly as markets open, utilising previously untapped resources, or locally used species.\n\nToday, overexploitation and misuse of natural resources is an ever-present threat for species richness. This is more prevalent when looking at island ecology and the species that inhabit them, as islands can be viewed as the world in miniature. Island endemic populations are more prone to extinction from overexploitation, as they often exist at low densities with reduced reproductive rates. A good example of this are island snails, such as the Hawaiian \"Achatinella\" and the French Polynesian \"Partula\". Achatinelline snails have 15 species listed as extinct and 24 critically endangered while 60 species of partulidae are considered extinct with 14 listed as critically endangered. The WCMC have attributed over-collecting and very low lifetime fecundity for the extreme vulnerability exhibited among these species.\n\nAs another example, when the humble hedgehog was introduced to the Scottish island of Uist, the population greatly expanded and took to consuming and overexploiting shorebird eggs, with drastic consequences for their breeding success. Twelve species of avifauna are affected, with some species numbers being reduced by 39%.\n\nWhere there is substantial human migration, civil unrest, or war, controls may no longer exist. With civil unrest, for example in the Congo and Rwanda, firearms have become common and the breakdown of food distribution networks in such countries leaves the resources of the natural environment vulnerable. Animals are even killed as target practice, or simply to spite the government. Populations of large primates, such as gorillas and chimpanzees, ungulates and other mammals, may be reduced by 80% or more by hunting, and certain species may be eliminated altogether. This decline has been called the bushmeat crisis.\n\nOverall, 50 bird species that have become extinct since 1500 (approximately 40% of the total) have been subject to overexploitation, including:\n\n\nOther species affected by overexploitation include: \n\n\nOverexploitation of species can result in knock-on or cascade effects. This can particularly apply if, through overexploitation, a habitat loses its apex predator. Because of the loss of the top predator, a dramatic increase in their prey species can occur. In turn, the unchecked prey can then overexploit their own food resources until population numbers dwindle, possibly to the point of extinction.\n\nA classic example of cascade effects occurred with sea otters. Starting before the 17th century and not phased out until 1911, sea otters were hunted aggressively for their exceptionally warm and valuable pelts, which could fetch up to $2500 US. This caused cascade effects through the kelp forest ecosystems along the Pacific Coast of North America.\n\nOne of the sea otters’ primary food sources is the sea urchin. When hunters caused sea otter populations to decline, an ecological release of sea urchin populations occurred. The sea urchins then overexploited their main food source, kelp, creating urchin barrens, areas of seabed denuded of kelp, but carpeted with urchins. No longer having food to eat, the sea urchin became locally extinct as well. Also, since kelp forest ecosystems are homes to many other species, the loss of the kelp caused other cascade effects of secondary extinctions.\n\nIn 1911, when only one small group of 32 sea otters survived in a remote cove, an international treaty was signed to prevent further exploitation of the sea otters. Under heavy protection, the otters multiplied and repopulated the depleted areas, which slowly recovered. More recently, with declining numbers of fish stocks, again due to overexploitation, killer whales have experienced a food shortage and have been observed feeding on sea otters, again reducing their numbers.\n\n"}
{"id": "22718", "url": "https://en.wikipedia.org/wiki?curid=22718", "title": "Ozone", "text": "Ozone\n\nOzone , or trioxygen, is an inorganic molecule with the chemical formula . It is a pale blue gas with a distinctively pungent smell. It is an allotrope of oxygen that is much less stable than the diatomic allotrope , breaking down in the lower atmosphere to or dioxygen. Ozone is formed from dioxygen by the action of ultraviolet light (UV) and electrical discharges within the Earth's atmosphere. It is present in very low concentrations throughout the latter, with its highest concentration high in the ozone layer of the stratosphere, which absorbs most of the Sun's ultraviolet (UV) radiation.\n\nOzone's odour is reminiscent of chlorine, and detectable by many people at concentrations of as little as in air. Ozone's O structure was determined in 1865. The molecule was later proven to have a bent structure and to be diamagnetic. In standard conditions, ozone is a pale blue gas that condenses at progressively cryogenic temperatures to a dark blue liquid and finally a violet-black solid. Ozone's instability with regard to more common dioxygen is such that both concentrated gas and liquid ozone may decompose explosively at elevated temperatures or fast warming to the boiling point.\nIt is therefore used commercially only in low concentrations.\n\nOzone is a powerful oxidant (far more so than dioxygen) and has many industrial and consumer applications related to oxidation. This same high oxidising potential, however, causes ozone to damage mucous and respiratory tissues in animals, and also tissues in plants, above concentrations of about . While this makes ozone a potent respiratory hazard and pollutant near ground level, a higher concentration in the ozone layer (from two to eight ppm) is beneficial, preventing damaging UV light from reaching the Earth's surface.\n\nThe trivial name \"ozone\" is the most commonly used and preferred IUPAC name. The systematic names \"2λ-trioxidiene\" and \"catena-trioxygen\", valid IUPAC names, are constructed according to the substitutive and additive nomenclatures, respectively. The name \"ozone\" derives from \"ozein\" (ὄζειν), the Greek verb for smell, referring to ozone's distinctive smell.\n\nIn appropriate contexts, ozone can be viewed as trioxidane with two hydrogen atoms removed, and as such, \"trioxidanylidene\" may be used as a context-specific systematic name, according to substitutive nomenclature. By default, these names pay no regard to the radicality of the ozone molecule. In even more specific context, this can also name the non-radical singlet ground state, whereas the diradical state is named \"trioxidanediyl\".\n\n\"Trioxidanediyl\" (or \"ozonide\") is used, non-systematically, to refer to the substituent group (-OOO-). Care should be taken to avoid confusing the name of the group for the context-specific name for ozone given above.\n\nIn 1785, the Dutch chemist Martinus van Marum was conducting experiments involving electrical sparking above water when he noticed an unusual smell, which he attributed to the electrical reactions, failing to realize that he had in fact created ozone. \n\nA half century later, Christian Friedrich Schönbein noticed the same pungent odour and recognized it as the smell often following a bolt of lightning. In 1839, he succeeded in isolating the gaseous chemical and named it \"ozone\", from the Greek word \"\" () meaning \"to smell\". \nFor this reason, Schönbein is generally credited with the discovery of ozone. The formula for ozone, O, was not determined until 1865 by Jacques-Louis Soret and confirmed by Schönbein in 1867.\n\nFor much of the second half of the nineteenth century and well into the twentieth, ozone was considered a healthy component of the environment by naturalists and health-seekers. Beaumont, California had as its official slogan \"Beaumont: Zone of Ozone\", as evidenced on postcards and Chamber of Commerce letterhead. Naturalists working outdoors often considered the higher elevations beneficial because of their ozone content. \"There is quite a different atmosphere [at higher elevation] with enough ozone to sustain the necessary energy [to work]\", wrote naturalist Henry Henshaw, working in Hawaii. Seaside air was considered to be healthy because of its believed ozone content; but the smell giving rise to this belief is in fact that of halogenated seaweed metabolites.\n\nMuch of ozone's appeal seems to have resulted from its \"fresh\" smell, which evoked associations with purifying properties. Scientists, however, noted its harmful effects. In 1873 James Dewar and John Gray McKendrick documented that frogs grew sluggish, birds gasped for breath, and rabbits’ blood showed decreased levels of oxygen after exposure to \"ozonized air\", which \"exercised a destructive action\". Schönbein himself reported that chest pains, irritation of the mucous membranes and difficulty breathing occurred as a result of inhaling ozone, and small mammals died. In 1911, Leonard Hill and Martin Flack stated in the \"Proceedings of the Royal Society B\" that ozone's healthful effects \"have, by mere iteration, become part and parcel of common belief; and yet exact physiological evidence in favour of its good effects has been hitherto almost entirely wanting... The only thoroughly well-ascertained knowledge concerning the physiological effect of ozone, so far attained, is that it causes irritation and œdema of the lungs, and death if inhaled in relatively strong concentration for any time.\"\n\nDuring World War I, ozone was tested at Queen Alexandra's Military Hospital in London as a possible disinfectant for wounds. The gas was applied directly to wounds for as long as 15 minutes. This resulted in damage to both bacterial cells and human tissue. Other sanitizing techniques, such as irrigation with antiseptics, were found preferable.\n\nOzone is colourless or pale blue gas (blue when liquefied), slightly soluble in water and much more soluble in inert non-polar solvents such as carbon tetrachloride or fluorocarbons, in which it forms a blue solution. At , it condenses to form a dark blue liquid. It is dangerous to allow this liquid to warm to its boiling point, because both concentrated gaseous ozone and liquid ozone can detonate. At temperatures below , it forms a violet-black solid.\n\nMost people can detect about 0.01 μmol/mol of ozone in air where it has a very specific sharp odour somewhat resembling chlorine bleach. Exposure of 0.1 to 1 μmol/mol produces headaches, burning eyes and irritation to the respiratory passages.\nEven low concentrations of ozone in air are very destructive to organic materials such as latex, plastics and animal lung tissue.\n\nOzone is diamagnetic, with all its electrons paired. In contrast, O is paramagnetic, containing two unpaired electrons.\n\nAccording to experimental evidence from microwave spectroscopy, ozone is a bent molecule, with C symmetry (similar to the water molecule). The O – O distances are . The O – O – O angle is 116.78°. The central atom is \"sp\"² hybridized with one lone pair. Ozone is a polar molecule with a dipole moment of 0.53 D. The molecule can be represented as a resonance hybrid with two contributing structures, each with a single bond on one side and double bond on the other. The arrangement possesses an overall bond order of 1.5 for both sides.\n\nOzone is among the most powerful oxidizing agents known, far stronger than O. It is also unstable at high concentrations, decaying into ordinary oxygen. Its half-life varies with atmospheric conditions such as temperature, humidity, and air movement. In a sealed chamber with a fan that moves the gas, ozone has a half-life of approximately one day at room temperature. Some unverified claims assert that ozone can have a half life of as short as thirty minutes under atmospheric conditions.\n\nThis reaction proceeds more rapidly with increasing temperature. Deflagration of ozone can be triggered by a spark and can occur in ozone concentrations of 10 wt% or higher.\n\nOzone can also be produced from oxygen at the anode of an electrochemical cell. This reaction can create smaller quantities of ozone for research purposes.\n\n(g) + 2H + 2e ←→ (g) + E°= 2.075V \n\nThis can be observed as an unwanted reaction in a Hoffman gas apparatus during the electrolysis of water when the voltage is set above the necessary voltage.\n\nOzone will oxidize most metals (except gold, platinum, and iridium) to oxides of the metals in their highest oxidation state. For example:\n\nOzone also oxidizes nitric oxide to nitrogen dioxide:\nThis reaction is accompanied by chemiluminescence. The can be further oxidized:\nThe formed can react with to form .\n\nSolid nitronium perchlorate can be made from NO, ClO, and gases:\n\nOzone does not react with ammonium salts, but it oxidizes ammonia to ammonium nitrate:\n\nOzone reacts with carbon to form carbon dioxide, even at room temperature:\n\nOzone oxidises sulfides to sulfates. For example, lead(II) sulfide is oxidised to lead(II) sulfate:\n\nSulfuric acid can be produced from ozone, water and either elemental sulfur or sulfur dioxide:\n\nIn the gas phase, ozone reacts with hydrogen sulfide to form sulfur dioxide:\n\nIn an aqueous solution, however, two competing simultaneous reactions occur, one to produce elemental sulfur, and one to produce sulfuric acid:\n\nAlkenes can be oxidatively cleaved by ozone, in a process called ozonolysis, giving alcohols, aldehydes, ketones, and carboxylic acids, depending on the second step of the workup.\n\nOzone can also cleave alkynes to form a acid anhydride or diketone product. If the reaction is performed in the presence of water, the anhydride hydrolyzes to give two carboxylic acids.\n\nUsually ozonolysis is carried out in a solution of dichloromethane, at a temperature of −78C. After a sequence of cleavage and rearrangement, an organic ozonide is formed. With reductive workup (e.g. zinc in acetic acid or dimethyl sulfide), ketones and aldehydes will be formed, with oxidative workup (e.g. aqueous or alcoholic hydrogen peroxide), carboxylic acids will be formed.\n\nAll three atoms of ozone may also react, as in the reaction of tin(II) chloride with hydrochloric acid and ozone:\nIodine perchlorate can be made by treating iodine dissolved in cold anhydrous perchloric acid with ozone:\n\nOzone can be used for combustion reactions and combustible gases; ozone provides higher temperatures than burning in dioxygen (O). The following is a reaction for the combustion of carbon subnitride which can also cause higher temperatures:\n\nOzone can react at cryogenic temperatures. At , atomic hydrogen reacts with liquid ozone to form a hydrogen superoxide radical, which dimerizes:\n\nReduction of ozone gives the ozonide anion, O. Derivatives of this anion are explosive and must be stored at cryogenic temperatures. Ozonides for all the alkali metals are known. KO, RbO, and CsO can be prepared from their respective superoxides:\nAlthough KO can be formed as above, it can also be formed from potassium hydroxide and ozone:\nNaO and LiO must be prepared by action of CsO in liquid NH on an ion exchange resin containing Na or Li ions:\nA solution of calcium in ammonia reacts with ozone to give to ammonium ozonide and not calcium ozonide:\n\nOzone can be used to remove iron and manganese from water, forming a precipitate which can be filtered:\nOzone will also oxidize dissolved hydrogen sulfide in water to sulfurous acid:\n\nThese three reactions are central in the use of ozone based well water treatment.\n\nOzone will also detoxify cyanides by converting them to cyanates.\n\nOzone will also completely decompose urea:\n\nOzone is a bent triatomic molecule with three vibrational modes: the symmetric stretch (1103.157 cm), bend (701.42 cm) and antisymmetric stretch (1042.096 cm). The symmetric stretch and bend are weak absorbers, but the antisymmetric stretch is strong and responsible for ozone being an important minor greenhouse gas. This IR band is also used to detect ambient and atmospheric ozone although UV based measurements are more common.\n\nThe electronic spectrum of ozone is quite complex. An overview can be seen at the MPI Mainz UV/VIS Spectral Atlas of Gaseous Molecules of Atmospheric Interest.\n\nAll of the bands are dissociative, meaning that the molecule falls apart to after absorbing a photon. The most important absorption is the Hartley band, extending from slightly above 300 nm down to slightly above 200 nm. It is this band that is responsible for absorbing UV C in the stratosphere.\n\nOn the high wavelength side, the Hartley band transitions to the so-called Huggins band, which falls off rapidly until disappearing by ~360 nm. Above 400 nm, extending well out into the NIR, are the Chappius and Wulf bands. There, unstructured absorption bands are useful for detecting high ambient concentrations of ozone, but are so weak that they do not have much practical effect.\n\nThere are additional absorption bands in the far UV, which increase slowly from 200 nm down to reaching a maximum at ~120 nm.\n\nThe standard way to express total ozone levels (the amount of ozone in a given vertical column) in the atmosphere is by using Dobson units. Point measurements are reported as mole fractions in nmol/mol (parts per billion, ppb) or as concentrations in μg/m. The study of ozone concentration in the atmosphere started in the 1920s.\n\nThe highest levels of ozone in the atmosphere are in the stratosphere, in a region also known as the ozone layer between about 10 km and 50 km above the surface (or between about 6 and 31 miles). However, even in this \"layer\", the ozone concentrations are only two to eight parts per million, so most of the oxygen there is dioxygen, O, at about 210,000 parts per million by volume.\n\nOzone in the stratosphere is mostly produced from short-wave ultraviolet rays between 240 and 160 nm. Oxygen starts to absorb weakly at 240 nm in the Herzberg bands, but most of the oxygen is dissociated by absorption in the strong Schumann–Runge bands between 200 and 160 nm where ozone does not absorb. While shorter wavelength light, extending to even the X-Ray limit, is energetic enough to dissociate molecular oxygen, there is relatively little of it, and, the strong solar emission at Lyman-alpha, 121 nm, falls at a point where molecular oxygen absorption is a minimum.\n\nThe process of ozone creation and destruction is called the Chapman cycle and starts with the photolysis of molecular oxygen\n\nfollowed by reaction of the oxygen atom with another molecule of oxygen to form ozone.\n\nwhere \"M\" denotes the third body that carries off the excess energy of the reaction. The ozone molecule can then absorb a UV-C photon and dissociate\n\nThe excess kinetic energy heats the stratosphere when the O atoms and the molecular oxygen fly apart and collide with other molecules. This conversion of UV light into kinetic energy warms the stratosphere. The oxygen atoms produced in the photolysis of ozone then react back with other oxygen molecule as in the previous step to form more ozone. In the clear atmosphere, with only nitrogen and oxygen, ozone can react with the atomic oxygen to form two molecules of O\n\nAn estimate of the rate of this termination step to the cycling of atomic oxygen back to ozone can be found simply by taking the ratios of the concentration of O to O. The termination reaction is catalysed by the presence of certain free radicals, of which the most important are hydroxyl (OH), nitric oxide (NO) and atomic chlorine (Cl) and bromine (Br). In the second half of the 20th Century the amount of ozone in the stratosphere was discovered to be declining, mostly because of increasing concentrations of chlorofluorocarbons (CFC) and similar chlorinated and brominated organic molecules. The concern over the health effects of the decline led to the 1987 Montreal Protocol, the ban on the production of many ozone depleting chemicals and in the first and second decade of the 21st Century the beginning of the recovery of stratospheric ozone concentrations.\n\nOzone in the ozone layer filters out sunlight wavelengths from about 200 nm UV rays to 315 nm, with ozone peak absorption at about 250 nm. This ozone UV absorption is important to life, since it extends the absorption of UV by ordinary oxygen and nitrogen in air (which absorb all wavelengths < 200 nm) through the lower UV-C (200–280 nm) and the entire UV-B band (280–315 nm). The small unabsorbed part that remains of UV-B after passage through ozone causes sunburn in humans, and direct DNA damage in living tissues in both plants and animals. Ozone's effect on mid-range UV-B rays is illustrated by its effect on UV-B at 290 nm, which has a radiation intensity 350 million times as powerful at the top of the atmosphere as at the surface. Nevertheless, enough of UV-B radiation at similar frequency reaches the ground to cause some sunburn, and these same wavelengths are also among those responsible for the production of vitamin D in humans.\n\nThe ozone layer has little effect on the longer UV wavelengths called UV-A (315–400 nm), but this radiation does not cause sunburn or direct DNA damage, and while it probably does cause long-term skin damage in certain humans, it is not as dangerous to plants and to the health of surface-dwelling organisms on Earth in general (see ultraviolet for more information on near ultraviolet).\n\nLow level ozone (or tropospheric ozone) is an atmospheric pollutant. It is not emitted directly by car engines or by industrial operations, but formed by the reaction of sunlight on air containing hydrocarbons and nitrogen oxides that react to form ozone directly at the source of the pollution or many kilometers down wind.\n\nOzone reacts directly with some hydrocarbons such as aldehydes and thus begins their removal from the air, but the products are themselves key components of smog. Ozone photolysis by UV light leads to production of the hydroxyl radical HO• and this plays a part in the removal of hydrocarbons from the air, but is also the first step in the creation of components of smog such as peroxyacyl nitrates, which can be powerful eye irritants. The atmospheric lifetime of tropospheric ozone is about 22 days; its main removal mechanisms are being deposited to the ground, the above-mentioned reaction giving HO•, and by reactions with OH and the peroxy radical HO•.\n\nThere is evidence of significant reduction in agricultural yields because of increased ground-level ozone and pollution which interferes with photosynthesis and stunts overall growth of some plant species. The United States Environmental Protection Agency is proposing a secondary regulation to reduce crop damage, in addition to the primary regulation designed for the protection of human health.\n\nCertain examples of cities with elevated ozone readings are Houston, Texas, and Mexico City, Mexico. Houston has a reading of around 41 nmol/mol, while Mexico City is far more hazardous, with a reading of about 125 nmol/mol.\n\nOzone gas attacks any polymer possessing olefinic or double bonds within its chain structure, such as natural rubber, nitrile rubber, and styrene-butadiene rubber. Products made using these polymers are especially susceptible to attack, which causes cracks to grow longer and deeper with time, the rate of crack growth depending on the load carried by the rubber component and the concentration of ozone in the atmosphere. Such materials can be protected by adding antiozonants, such as waxes, which bond to the surface to create a protective film or blend with the material and provide long term protection. Ozone cracking used to be a serious problem in car tires, for example, but it is not an issue with modern tires. On the other hand, many critical products, like gaskets and O-rings, may be attacked by ozone produced within compressed air systems. Fuel lines made of reinforced rubber are also susceptible to attack, especially within the engine compartment, where some ozone is produced by electrical components. Storing rubber products in close proximity to a DC electric motor can accelerate ozone cracking. The commutator of the motor generates sparks which in turn produce ozone.\n\nAlthough ozone was present at ground level before the Industrial Revolution, peak concentrations are now far higher than the pre-industrial levels, and even background concentrations well away from sources of pollution are substantially higher. Ozone acts as a greenhouse gas, absorbing some of the infrared energy emitted by the earth. Quantifying the greenhouse gas potency of ozone is difficult because it is not present in uniform concentrations across the globe. However, the most widely accepted scientific assessments relating to climate change (e.g. the Intergovernmental Panel on Climate Change Third Assessment Report) suggest that the radiative forcing of tropospheric ozone is about 25% that of carbon dioxide.\n\nThe annual global warming potential of tropospheric ozone is between 918–1022 tons carbon dioxide equivalent/tons tropospheric ozone. This means on a per-molecule basis, ozone in the troposphere has a radiative forcing effect roughly 1,000 times as strong as carbon dioxide. However, tropospheric ozone is a short-lived greenhouse gas, which decays in the atmosphere much more quickly than carbon dioxide. This means that over a 20-year span, the global warming potential of tropospheric ozone is much less, roughly 62 to 69 tons carbon dioxide equivalent / ton tropospheric ozone.\n\nBecause of its short-lived nature, tropospheric ozone does not have strong global effects, but has very strong radiative forcing effects on regional scales. In fact, there are regions of the world where tropospheric ozone has a radiative forcing up to 150% of carbon dioxide.\n\nOzone precursors are a group of pollutants, predominantly those emitted during the combustion of fossil fuels. Ground-level ozone pollution (tropospheric ozone) is created near the Earth's surface by the action of daylight UV rays on these precursors. The ozone at ground level is primarily from fossil fuel precursors, but methane is a natural precursor, and the very low natural background level of ozone at ground level is considered safe. This section examines the health impacts of fossil fuel burning, which raises ground level ozone far above background levels.\n\nThere is a great deal of evidence to show that ground-level ozone can harm lung function and irritate the respiratory system. Exposure to ozone (and the pollutants that produce it) is linked to premature death, asthma, bronchitis, heart attack, and other cardiopulmonary problems.\n\nLong-term exposure to ozone has been shown to increase risk of death from respiratory illness. A study of 450,000 people living in United States cities saw a significant correlation between ozone levels and respiratory illness over the 18-year follow-up period. The study revealed that people living in cities with high ozone levels, such as Houston or Los Angeles, had an over 30% increased risk of dying from lung disease.\n\nAir quality guidelines such as those from the World Health Organization, the United States Environmental Protection Agency (EPA) and the European Union are based on detailed studies designed to identify the levels that can cause measurable ill health effects.\n\nAccording to scientists with the US EPA, susceptible people can be adversely affected by ozone levels as low as 40 nmol/mol. In the EU, the current target value for ozone concentrations is 120 µg/m which is about 60 nmol/mol. This target applies to all member states in accordance with Directive 2008/50/EC. Ozone concentration is measured as a maximum daily mean of 8 hour averages and the target should not be exceeded on more than 25 calendar days per year, starting from January 2010. Whilst the directive requires in the future a strict compliance with 120 µg/m limit (i.e. mean ozone concentration not to be exceeded on any day of the year), there is no date set for this requirement and this is treated as a long-term objective.\n\nIn the USA, the Clean Air Act directs the EPA to set National Ambient Air Quality Standards for several pollutants, including ground-level ozone, and counties out of compliance with these standards are required to take steps to reduce their levels. In May 2008, under a court order, the EPA lowered its ozone standard from 80 nmol/mol to 75 nmol/mol. The move proved controversial, since the Agency's own scientists and advisory board had recommended lowering the standard to 60 nmol/mol. Many public health and environmental groups also supported the 60 nmol/mol standard, and the World Health Organization recommends 51 nmol/mol.\n\nOn January 7, 2010, the U.S. Environmental Protection Agency (EPA) announced proposed revisions to the National Ambient Air Quality Standard (NAAQS) for the pollutant ozone, the principal component of smog:\n\nOn October 26, 2015, the EPA published a final rule with an effective date of December 28, 2015 that revised the 8-hour primary NAAQS from 0.075 ppm to 0.070 ppm.\n\nThe EPA has developed an Air Quality Index (AQI) to help explain air pollution levels to the general public. Under the current standards, eight-hour average ozone mole fractions of 85 to 104 nmol/mol are described as \"unhealthy for sensitive groups\", 105 nmol/mol to 124 nmol/mol as \"unhealthy\", and 125 nmol/mol to 404 nmol/mol as \"very unhealthy\".\n\nOzone can also be present in indoor air pollution, partly as a result of electronic equipment such as photocopiers. A connection has also been known to exist between the increased pollen, fungal spores, and ozone caused by thunderstorms and hospital admissions of asthma sufferers.\n\nIn the Victorian era, one British folk myth held that the smell of the sea was caused by ozone. In fact, the characteristic \"smell of the sea\" is caused by dimethyl sulfide, a chemical generated by phytoplankton. Victorian British folk considered the resulting smell \"bracing\".\n\nOzone production rises during heat waves, because plants absorb less ozone. It is estimated that curtailed ozone absorption by plants was responsible for the loss of 460 lives in the UK in the hot summer of 2006. A similar investigation to assess the joint effects of ozone and heat during the European heat waves in 2003, concluded that these appear to be additive.\n\nOzone, along with reactive forms of oxygen such as superoxide, singlet oxygen, hydrogen peroxide, and hypochlorite ions, is produced by white blood cells and other biological systems (such as the roots of marigolds) as a means of destroying foreign bodies. Ozone reacts directly with organic double bonds. Also, when ozone breaks down to dioxygen it gives rise to oxygen free radicals, which are highly reactive and capable of damaging many organic molecules. Moreover, it is believed that the powerful oxidizing properties of ozone may be a contributing factor of inflammation. The cause-and-effect relationship of how the ozone is created in the body and what it does is still under consideration and still subject to various interpretations, since other body chemical processes can trigger some of the same reactions. A team headed by Paul Wentworth Jr. of the Department of Chemistry at the Scripps Research Institute has shown evidence linking the antibody-catalyzed water-oxidation pathway of the human immune response to the production of ozone. In this system, ozone is produced by antibody-catalyzed production of trioxidane from water and neutrophil-produced singlet oxygen.\n\nWhen inhaled, ozone reacts with compounds lining the lungs to form specific, cholesterol-derived metabolites that are thought to facilitate the build-up and pathogenesis of atherosclerotic plaques (a form of heart disease). These metabolites have been confirmed as naturally occurring in human atherosclerotic arteries and are categorized into a class of secosterols termed \"atheronals\", generated by ozonolysis of cholesterol's double bond to form a 5,6 secosterol as well as a secondary condensation product via aldolization.\n\nOzone has been implicated to have an adverse effect on plant growth: \"... ozone reduced total chlorophylls, carotenoid and carbohydrate concentration, and increased 1-aminocyclopropane-1-carboxylic acid (ACC) content and ethylene production. In treated plants, the ascorbate leaf pool was decreased, while lipid peroxidation and solute leakage were significantly higher than in ozone-free controls. The data indicated that ozone triggered protective mechanisms against oxidative stress in citrus.\"\n\nBecause of the strongly oxidizing properties of ozone, ozone is a primary irritant, affecting especially the eyes and respiratory systems and can be hazardous at even low concentrations. The Canadian Centre for Occupation Safety and Health reports that: \"Even very low concentrations of ozone can be harmful to the upper respiratory tract and the lungs. The severity of injury depends on both by the concentration of ozone and the duration of exposure. Severe and permanent lung injury or death could result from even a very short-term exposure to relatively low concentrations.\" To protect workers potentially exposed to ozone, U.S. Occupational Safety and Health Administration has established a permissible exposure limit (PEL) of 0.1 μmol/mol (29 CFR 1910.1000 table Z-1), calculated as an 8-hour time weighted average. Higher concentrations are especially hazardous and NIOSH has established an Immediately Dangerous to Life and Health Limit (IDLH) of 5 μmol/mol. Work environments where ozone is used or where it is likely to be produced should have adequate ventilation and it is prudent to have a monitor for ozone that will alarm if the concentration exceeds the OSHA PEL. Continuous monitors for ozone are available from several suppliers.\n\nElevated ozone exposure can occur on passenger aircraft, with levels depending on altitude and atmospheric turbulence. United States Federal Aviation Authority regulations set a limit of 250 nmol/mol with a maximum four-hour average of 100 nmol/mol. Some planes are equipped with ozone converters in the ventilation system to reduce passenger exposure.\n\nOzone generators are used to produce ozone for cleaning air or removing smoke odours in unoccupied rooms. These ozone generators can produce over 3 g of ozone per hour. Ozone often forms in nature under conditions where O will not react. Ozone used in industry is measured in μmol/mol (ppm, parts per million), nmol/mol (ppb, parts per billion), μg/m, mg/h (milligrams per hour) or weight percent. The regime of applied concentrations ranges from 1% to 5% (in air) and from 6% to 14% (in oxygen) for older generation methods. New electrolytic methods can achieve up 20% to 30% dissolved ozone concentrations in output water.\n\nTemperature and humidity play a large role in how much ozone is being produced using traditional generation methods (such as corona discharge and ultraviolet light). Old generation methods will produce less than 50% of nominal capacity if operated with humid ambient air, as opposed to very dry air. New generators, using electrolytic methods, can achieve higher purity and dissolution through using water molecules as the source of ozone production.\n\nThis is the most common type of ozone generator for most industrial and personal uses. While variations of the \"hot spark\" coronal discharge method of ozone production exist, including medical grade and industrial grade ozone generators, these units usually work by means of a corona discharge tube. They are typically cost-effective and do not require an oxygen source other than the ambient air to produce ozone concentrations of 3–6%. Fluctuations in ambient air, due to weather or other environmental conditions, cause variability in ozone production. However, they also produce nitrogen oxides as a by-product. Use of an air dryer can reduce or eliminate nitric acid formation by removing water vapor and increase ozone production. Use of an oxygen concentrator can further increase the ozone production and further reduce the risk of nitric acid formation by removing not only the water vapor, but also the bulk of the nitrogen.\n\nUV ozone generators, or vacuum-ultraviolet (VUV) ozone generators, employ a light source that generates a narrow-band ultraviolet light, a subset of that produced by the Sun. The Sun's UV sustains the ozone layer in the stratosphere of Earth.\n\nUV ozone generators use ambient air for ozone production, no air prep systems are used (air dryer or oxygen concentrator), therefore these generators tend to be less expensive. However UV ozone generators usually produce ozone with a concentration of about 0.5% or lower which limits the potential ozone production rate. Another disadvantage of this method is that it requires the ambient air (oxygen) to be exposed to the UV source for a longer amount of time, and any gas that is not exposed to the UV source will not be treated. This makes UV generators impractical for use in situations that deal with rapidly moving air or water streams (in-duct air sterilization, for example). Production of ozone is one of the potential dangers of ultraviolet germicidal irradiation. VUV ozone generators are used in swimming pool and spa applications ranging to millions of gallons of water. VUV ozone generators, unlike corona discharge generators, do not produce harmful nitrogen by-products and also unlike corona discharge systems, VUV ozone generators work extremely well in humid air environments. There is also not normally a need for expensive off-gas mechanisms, and no need for air driers or oxygen concentrators which require extra costs and maintenance.\n\nIn the cold plasma method, pure oxygen gas is exposed to a plasma created by dielectric barrier discharge. The diatomic oxygen is split into single atoms, which then recombine in triplets to form ozone.\n\nCold plasma machines utilize pure oxygen as the input source and produce a maximum concentration of about 5% ozone. They produce far greater quantities of ozone in a given space of time compared to ultraviolet production. However, because cold plasma ozone generators are very expensive, they are found less frequently than the previous two types.\n\nThe discharges manifest as filamentary transfer of electrons (micro discharges) in a gap between two electrodes. In order to evenly distribute the micro discharges, a dielectric insulator must be used to separate the metallic electrodes and to prevent arcing.\n\nSome cold plasma units also have the capability of producing short-lived allotropes of oxygen which include O, O, O, O, etc. These species are even more reactive than ordinary .\n\nElectrolytic ozone generation (EOG) splits water molecules into H, O, and O.\nIn most EOG methods, the hydrogen gas will be removed to leave oxygen and ozone as the only reaction products. Therefore, EOG can achieve higher dissolution in water without other competing gases found in corona discharge method, such as nitrogen gases present in ambient air. This method of generation can achieve concentrations of 20–30% and is independent of air quality because water is used as the source material. Production of ozone electrolytically is typically unfavorable because of the high overpotential required to produce ozone as compared to oxygen. This is why ozone is not produced during typical water electrolysis. However, it is possible to increase the overpotential of oxygen by careful catalyst selection such that ozone is preferentially produced under electrolysis. Catalysts typically chosen for this approach are lead dioxide or boron-doped diamond.\n\nOzone cannot be stored and transported like other industrial gases (because it quickly decays into diatomic oxygen) and must therefore be produced on site. Available ozone generators vary in the arrangement and design of the high-voltage electrodes. At production capacities higher than 20 kg per hour, a gas/water tube heat-exchanger may be utilized as ground electrode and assembled with tubular high-voltage electrodes on the gas-side. The regime of typical gas pressures is around absolute in oxygen and absolute in air. Several megawatts of electrical power may be installed in large facilities, applied as one phase AC current at 50 to 8000 Hz and peak voltages between 3,000 and 20,000 volts. Applied voltage is usually inversely related to the applied frequency.\n\nThe dominating parameter influencing ozone generation efficiency is the gas temperature, which is controlled by cooling water temperature and/or gas velocity. The cooler the water, the better the ozone synthesis. The lower the gas velocity, the higher the concentration (but the lower the net ozone produced). At typical industrial conditions, almost 90% of the effective power is dissipated as heat and needs to be removed by a sufficient cooling water flow.\n\nBecause of the high reactivity of ozone, only a few materials may be used like stainless steel (quality 316L), titanium, aluminium (as long as no moisture is present), glass, polytetrafluorethylene, or polyvinylidene fluoride. Viton may be used with the restriction of constant mechanical forces and absence of humidity (humidity limitations apply depending on the formulation). Hypalon may be used with the restriction that no water come in contact with it, except for normal atmospheric levels. Embrittlement or shrinkage is the common mode of failure of elastomers with exposure to ozone. Ozone cracking is the common mode of failure of elastomer seals like O-rings.\n\nSilicone rubbers are usually adequate for use as gaskets in ozone concentrations below 1 wt%, such as in equipment for accelerated aging of rubber samples.\n\nOzone may be formed from by electrical discharges and by action of high energy electromagnetic radiation. Unsuppressed arcing in electrical contacts, motor brushes, or mechanical switches breaks down the chemical bonds of the atmospheric oxygen surrounding the contacts [ → 2O]. Free radicals of oxygen in and around the arc recombine to create ozone []. Certain electrical equipment generate significant levels of ozone. This is especially true of devices using high voltages, such as ionic air purifiers, laser printers, photocopiers, tasers and arc welders. Electric motors using brushes can generate ozone from repeated sparking inside the unit. Large motors that use brushes, such as those used by elevators or hydraulic pumps, will generate more ozone than smaller motors.\n\nOzone is similarly formed in the Catatumbo lightning storms phenomenon on the Catatumbo River in Venezuela, though ozone's instability makes it dubious that it has any effect on the ozonosphere.\nIt is the world's largest single natural generator of ozone, lending calls for it to be designated a UNESCO World Heritage Site.\n\nIn the laboratory, ozone can be produced by electrolysis using a 9 volt battery, a pencil graphite rod cathode, a platinum wire anode and a 3 molar sulfuric acid electrolyte. The half cell reactions taking place are:\n\nIn the net reaction, three equivalents of water are converted into one equivalent of ozone and three equivalents of hydrogen. Oxygen formation is a competing reaction.\n\nIt can also be generated by a high voltage arc. In its simplest form, high voltage AC, such as the output of a Neon-sign transformer is connected to two metal rods with the ends placed sufficiently close to each other to allow an arc. The resulting arc will convert atmospheric oxygen to ozone.\n\nIt is often desirable to contain the ozone. This can be done with an apparatus consisting of two concentric glass tubes sealed together at the top with gas ports at the top and bottom of the outer tube. The inner core should have a length of metal foil inserted into it connected to one side of the power source. The other side of the power source should be connected to another piece of foil wrapped around the outer tube. A source of dry is applied to the bottom port. When high voltage is applied to the foil leads, electricity will discharge between the dry dioxygen in the middle and form and which will flow out the top port. The reaction can be summarized as follows:\n\nThe largest use of ozone is in the preparation of pharmaceuticals, synthetic lubricants, and many other commercially useful organic compounds, where it is used to sever carbon-carbon bonds. It can also be used for bleaching substances and for killing microorganisms in air and water sources. Many municipal drinking water systems kill bacteria with ozone instead of the more common chlorine. Ozone has a very high oxidation potential. Ozone does not form organochlorine compounds, nor does it remain in the water after treatment. Ozone can form the suspected carcinogen bromate in source water with high bromide concentrations. The U.S. Safe Drinking Water Act mandates that these systems introduce an amount of chlorine to maintain a minimum of 0.2 μmol/mol residual free chlorine in the pipes, based on results of regular testing. Where electrical power is abundant, ozone is a cost-effective method of treating water, since it is produced on demand and does not require transportation and storage of hazardous chemicals. Once it has decayed, it leaves no taste or odour in drinking water.\n\nAlthough low levels of ozone have been advertised to be of some disinfectant use in residential homes, the concentration of ozone in dry air required to have a rapid, substantial effect on airborne pathogens exceeds safe levels recommended by the U.S. Occupational Safety and Health Administration and Environmental Protection Agency. Humidity control can vastly improve both the killing power of the ozone and the rate at which it decays back to oxygen (more humidity allows more effectiveness). Spore forms of most pathogens are very tolerant of atmospheric ozone in concentrations at which asthma patients start to have issues.\n\nIndustrially, ozone is used to:\n\nOzone is a reagent in many organic reactions in the laboratory and in industry. Ozonolysis is the cleavage of an alkene to carbonyl compounds.\n\nMany hospitals around the world use large ozone generators to decontaminate operating rooms between surgeries. The rooms are cleaned and then sealed airtight before being filled with ozone which effectively kills or neutralizes all remaining bacteria.\n\nOzone is used as an alternative to chlorine or chlorine dioxide in the bleaching of wood pulp. It is often used in conjunction with oxygen and hydrogen peroxide to eliminate the need for chlorine-containing compounds in the manufacture of high-quality, white paper.\n\nOzone can be used to detoxify cyanide wastes (for example from gold and silver mining) by oxidising cyanide to cyanate and eventually to carbon dioxide.\n\nDevices generating high levels of ozone, some of which use ionization, are used to sanitize and deodorize uninhabited buildings, rooms, ductwork, woodsheds, boats and other vehicles.\n\nIn the U.S., air purifiers emitting low levels of ozone have been sold. This kind of air purifier is sometimes claimed to imitate nature's way of purifying the air without filters and to sanitize both it and household surfaces. The United States Environmental Protection Agency (EPA) has declared that there is \"evidence to show that at concentrations that do not exceed public health standards, ozone is not effective at removing many odor-causing chemicals\" or \"viruses, bacteria, mold, or other biological pollutants\". Furthermore, its report states that \"results of some controlled studies show that concentrations of ozone considerably higher than these [human safety] standards are possible even when a user follows the manufacturer’s operating instructions\".\n\nOzonated water is used to launder clothes and to sanitize food, drinking water, and surfaces in the home. According to the U.S. Food and Drug Administration (FDA), it is \"amending the food additive regulations to provide for the safe use of ozone in gaseous and aqueous phases as an antimicrobial agent on food, including meat and poultry.\" Studies at California Polytechnic University demonstrated that 0.3 μmol/mol levels of ozone dissolved in filtered tapwater can produce a reduction of more than 99.99% in such food-borne microorganisms as salmonella, \"E. coli\" 0157:H7 and \"Campylobacter\". This quantity is 20,000 times the WHO-recommended limits stated above.\nOzone can be used to remove pesticide residues from fruits and vegetables.\n\nOzone is used in homes and hot tubs to kill bacteria in the water and to reduce the amount of chlorine or bromine required by reactivating them to their free state. Since ozone does not remain in the water long enough, ozone by itself is ineffective at preventing cross-contamination among bathers and must be used in conjunction with halogens. Gaseous ozone created by ultraviolet light or by corona discharge is injected into the water.\n\nOzone is also widely used in treatment of water in aquariums and fish ponds. Its use can minimize bacterial growth, control parasites, eliminate transmission of some diseases, and reduce or eliminate \"yellowing\" of the water. Ozone must not come in contact with fish's gill structures. Natural salt water (with life forms) provides enough \"instantaneous demand\" that controlled amounts of ozone activate bromide ion to hypobromous acid, and the ozone entirely decays in a few seconds to minutes. If oxygen fed ozone is used, the water will be higher in dissolved oxygen, fish's gill structures will atrophy and they will become dependent on higher dissolved oxygen levels.\n\nOzonation – a process of infusing water with ozone – can be used in aquaculture to facilitate organic breakdown. Ozone is also added to recirculating systems to reduce nitrite levels through conversion into nitrate. If nitrite levels in the water are high, nitrites will also accumulate in the blood and tissues of fish, where it interferes with oxygen transport (it causes oxidation of the heme-group of haemoglobin from ferrous () to ferric (), making haemoglobin unable to bind ). Despite these apparent positive effects, ozone use in recirculation systems has been linked to reducing the level of bioavailable iodine in salt water systems, resulting in iodine deficiency symptoms such as goitre and decreased growth in Senegalese sole (\"Solea senegalensis\") larvae.\n\nOzonate seawater is used for surface disinfection of haddock and Atlantic halibut eggs against nodavirus. Nodavirus is a lethal and vertically transmitted virus which causes severe mortality in fish. Haddock eggs should not be treated with high ozone level as eggs so treated did not hatch and died after 3–4 days.\n\nOzone application on freshly cut pineapple and banana shows increase in flavonoids and total phenol contents when exposure is up to 20 minutes. Decrease in ascorbic acid (one form of vitamin C) content is observed but the positive effect on total phenol content and flavonoids can overcome the negative effect. Tomatoes upon treatment with ozone shows an increase in β-carotene, lutein and lycopene. However, ozone application on strawberries in pre-harvest period shows decrease in ascorbic acid content.\n\nOzone facilitates the extraction of some heavy metals from soil using EDTA. EDTA forms strong, water-soluble coordination compounds with some heavy metals (Pb, Zn) thereby making it possible to dissolve them out from contaminated soil. If contaminated soil is pre-treated with ozone, the extraction efficacy of Pb, Am and Pu increases by 11.0–28.9%, 43.5% and 50.7% respectively.\n\nVarious therapeutic uses for ozone have been proposed, but are not supported by high quality evidence and generally considered alternative medicine.\n\n\n\n"}
{"id": "1424864", "url": "https://en.wikipedia.org/wiki?curid=1424864", "title": "Plate electrode", "text": "Plate electrode\n\nA plate, usually called anode in Britain, is a type of electrode that forms part of a vacuum tube. It is usually made of sheet metal, connected to a wire which passes through the glass envelope of the tube to a terminal in the base of the tube, where it is connected to the external circuit. The plate is given a positive potential, and its function is to attract and capture the electrons emitted by the cathode. Although it is sometimes a flat plate, it is more often in the shape of a cylinder or flat open-ended box surrounding the other electrodes.\n\nThe plate must dissipate heat created when the electrons hit it with a high velocity after being accelerated by the voltage between the plate and cathode. Most of the waste power used in a vacuum tube is dissipated as heat by the plate. In low power tubes it is usually given a black coating, and often has \"fins\" to help it radiate heat. In power vacuum tubes used in radio transmitters, it is often made of a refractory metal like molybdenum. and is part of a large heat sink that projects through the glass or ceramic tube envelope and is cooled by radiation cooling, forced air or water.\n\nA problem in early vacuum tubes was \"secondary emission\"; electrons striking the plate could knock other electrons out of the metal surface. In some tubes such as tetrodes these \"secondary electrons\" could be absorbed by other electrods such as grids in the tube, resulting in a current out of the plate. This current could cause the plate circuit to have negative resistance, which could cause unwanted parasitic oscillations. To prevent this most plates in modern tubes are given a chemical coating which reduces secondary emission.\n\n\n\n"}
{"id": "226187", "url": "https://en.wikipedia.org/wiki?curid=226187", "title": "Prandtl–Glauert singularity", "text": "Prandtl–Glauert singularity\n\nThe Prandtl–Glauert singularity is the prediction by the Prandtl–Glauert transformation that infinite pressures would be experienced by an aircraft as it approaches the speed of sound. Because it is invalid to apply the transformation at these speeds, the predicted singularity does not emerge. This is related to the early-20th-century misconception of the impenetrability of the sound barrier.\n\nThe Prandtl–Glauert transformation assumes linearity (i.e. a small change will have a small effect that is proportional to its size). This assumption becomes inaccurate at high Mach numbers and is entirely invalid in places where the flow reaches supersonic speeds, since sonic shock waves are instantaneous (and thus manifestly non-linear) changes in the flow. Indeed, one assumption in the Prandtl–Glauert transformation is approximately constant Mach number throughout the flow, and the increasing slope in the transformation indicates that very small changes will have a very strong effect at higher Mach numbers, thus violating the assumption, which breaks down entirely at the speed of sound.\n\nThis means that the singularity featured by the transformation near the sonic speed (\"M=1\") is not within the area of validity. The aerodynamic forces are calculated to approach infinity at the so-called \"Prandtl–Glauert singularity\"; in reality, the aerodynamic and thermodynamic perturbations do get amplified strongly near the sonic speed, but they remain finite and a singularity does not occur. The Prandtl–Glauert transformation is a linearized approximation of compressible, inviscid potential flow. As the flow approaches sonic speed, the nonlinear phenomena dominate within the flow, which this transformation completely ignores for the sake of simplicity.\n\nThe Prandtl–Glauert transformation is found by linearizing the potential equations associated with compressible, inviscid flow. For two-dimensional flow, the linearized pressures in such a flow are equal to those found from incompressible flow theory multiplied by a correction factor. This correction factor is given below:\n\nwhere\n\nThis formula is known as \"Prandtl's rule\", and works well up to low-transonic Mach numbers (M < ~0.7). However, note the limit:\n\nformula_2\n\nThis obviously nonphysical result (of an infinite pressure) is known as the Prandtl–Glauert singularity.\n\nThe reason for the condensation cloud that is being observed is that humid air is entering a low-pressure region, which also reduces local density and temperature sufficiently to cause condensation. The vapour vanishes as soon as the pressure increases again to ambient levels.\nIn the case of objects at transonic speeds, the pressure increase happens at the shock wave location.\nCondensation in free flows does not require supersonic flow. Given a humidity high enough, it can happen in purely subsonic flow over a wing or in the cores of wing tip and other vortices. This can often be observed on humid days on aircraft approaching an airport.\n\n"}
{"id": "25142797", "url": "https://en.wikipedia.org/wiki?curid=25142797", "title": "Red Data Book of Ukraine", "text": "Red Data Book of Ukraine\n\nThe Red Data Book of Ukraine is a list of endangered animals and plants that live in Ukraine.\n\n"}
{"id": "55519469", "url": "https://en.wikipedia.org/wiki?curid=55519469", "title": "Ricciocarpos", "text": "Ricciocarpos\n\nRicciocarpos natans is the only species in the genus Ricciocarpos, a genus of liverworts in the family Ricciaceae. It was formerly listed in 1759 as a species of \"Riccia\" by Linnaeus, but then assigned to a new genus of its own in 1829 by August Carl Joseph Corda.\n\nDespite having many common features with the genus \"Riccia\", its most obvious difference from that genus are the long sword-shaped purple scales that hang from the under surface of floating plants. The genus has occasionally appeared in the literature under the spelling Ricciocarpus, but the spelling with an o is the original and accepted spelling. The specific epithet \"natans\" comes from the Latin word for \"swimming\", because plants typically float freely in ponds or quiet waters.\n\nPlants of \"R. natans\" have two very different forms, depending on the conditions under which the plant grows. One form develops in plants that grow on land (terrestrial), and another form develops when plants grow floating in the water (aquatic). The terrestrial form develops into rosettes 25–35 millimetres across, of short and narrow branches having almost parallel sides. The more usual form is aquatic, and develops as broader, heart-shaped thallus with fewer branchings and long slender purple scales hanging from the underside. The two forms are so physically different from each other that they were originally thought to be different species.\n\n\"Ricciocarpos\" is distributed globally, being found almost everywhere except the polar regions though it is rare in parts of the tropics. It may form extensive floating colonies in quiet waters, and grows readily in laboratory cultures. Although fertile plants are not unknown, mature plants bearing spore capsules are rarely found. It is therefore assumed that \"Ricciocarpos\" spreads primarily through vegetative reproduction as the plants break apart. It has been suggested that the aquatic forms remain sterile and that sexual reproduction is largely limited to terrestrial forms, but other sources maintain that terrestrial forms are normally sterile as well.\n"}
{"id": "17005658", "url": "https://en.wikipedia.org/wiki?curid=17005658", "title": "Subatlantic", "text": "Subatlantic\n\nThe Subatlantic is the current climatic age of the Holocene epoch. It started at about 2,500 years BP and is still ongoing. Its average temperatures were slightly lower than during the preceding Subboreal and Atlantic. During its course the temperature underwent several oscillations which had a strong influence on fauna and flora and thus indirectly on the evolution of human civilizations. With intensifying industrialisation, human society started to stress the natural climatic cycles with increased greenhouse gas emissions.\n\nThe term \"subatlantic\" was first introduced in 1889 by Rutger Sernander to differentiate it from Axel Blytt's \"atlantic\". It follows upon the previous \"subboreal\". According to Franz Firbas (1949) and Litt \"et al.\" (2001) it consists of the pollen zones IX and X. This corresponds in the scheme of Fritz Theodor Overbeck to the pollen zones XI and XII.\n\nIn climate stratigraphy the subatlantic is usually subdivided into an older subatlantic and a younger subatlantic. The older subatlantic corresponds to pollen zone IX (or XI in an alternate nomenclature made of more zones) characterized in central and northern Europe by beech or oak-beech forests, the younger subatlantic to pollen zone X (or XII in the alternate nomenclature made of more zones).\n\nIn eastern Germany Dietrich Franke subdivides the subatlantic into four stages (from young to old):\n\n\nThe beginning of the subatlantic is usually defined as 2,400 calendar years BP or 450 BC. Yet this lower limit is by no means rigid. Some authors prefer to define the start of the subatlantic as 2,500 radiocarbon years which represents roughly 625 BC. Occasionally the onset of the subatlantic has even been pushed back as far as 1200 BC.\n\nAccording to Franz Firbas the changeover from the subboreal (pollen zone VIII) to the older subatlantic (pollen zone IX) is characterized by the recession of hazel and lime and the simultaneous spreading of hornbeam due to anthropogenic influences. This recession was not synchronous. It occurred in the western reaches of the Lower Oder valley between 930 and 830 BC, whereas in southwestern Poland this event had taken place already between 1170 and 1160 BC.\n\nThe beginning of the younger subatlantic at 1250 AD coincides with the medieval population increase and distinguishes itself by an increase in pine and in indicator plants for human settlements. In Silesia this event can be dated between 1050 and 1270 AD. If one equates the onset of the younger subatlantic with the first maximum of beech occurrence it shifts back to Carolingian times around 700 AD.\n\nThe summer temperatures of the subatlantic are generally somewhat cooler (by up to 1.0 °C) than during the preceding subboreal, the yearly average temperatures reduced by 0.7 °C. At the same time the winter precipitations augmented by up to 50%. Overall the climate during the subatlantic therefore tends to cooler and wetter conditions. The lower limit of the glaciers in Scandinavia descended during the subatlantic by 100 to 200 meters.\n\nThe beginning of the subatlantic opened at the middle of the first millennium BC with the so-called Roman Warm Period which lasted to the beginning of the 4th century. This corresponds broadly to classical antiquity. The optimum is marked by a temperature spike centered around 2,500 BP. As a consequence in Europe the winter temperatures were raised by 0.6 °C during this period, yet on average were still by 0.3 °C lower than during the subboreal. Ice cores from Greenland also demonstrate a distinct temperature rise after the younger subboreal. The cooling that followed coincides with the Migration Period. It was not very pronounced and of short duration – an average temperature drop of 0.2 °C and a winter temperature drop of 0.4 °C center around 350 AD (or 1,600 years BP). This climatic deterioration with the establishment of drier and cooler conditions might have forced the Huns to move west thus in turn triggering the migrations of the Germanic tribes. At about the same time the Byzantine Empire reached its first acme and Christianity established itself in Europe as the leading monotheistic religion.\n\nAfter this relatively short cool interlude the climate ameliorated again and reached between 800 and 1200 almost the values of the Roman Warm Period (used temperature proxies are sediments in the North Atlantic). This warming happened during the High Middle Ages wherefore this event is known as \"Medieval Global Warming\" or the Medieval Warm Period. This warmer climate peaked around 850 AD and 1050 AD, and raised the tree line in Scandinavia and in Russia by 100 to 140 meters; it enabled the Vikings to settle in Iceland and Greenland. During this period the Crusades took place and the Byzantine Empire was eventually pushed back by the rise of the Ottoman Empire.\n\nThe end of the Medieval Warm Period coincides with the early 14th century reaching a temperature minimum around 1350, and by the Crisis of the Late Middle Ages. Many settlements were abandoned and left deserted. As a consequence, the population in Central Europe drastically receded by as much as 50 percent.\n\nAfter a short warming pulse around 1500, the Little Ice Age followed, lasting from c. 1550 until 1860. The Northern Hemisphere snow line descended by 100 to 200 meters. Human history during this time includes the Renaissance and the Age of Enlightenment, and also major rebellious events like the Thirty Years War and the French Revolution. The beginning of the Industrial Revolution also dates back to this period, while Southeast Asia experienced the Dark ages of Cambodia.\n\nFrom 1860 onwards, the temperatures started to rise again and initiated the modern climatic optimum. This warming was severely amplified by anthropogenic influences (i.e. increasing industrialisation, greenhouse gas emissions and global warming). The modern warming shows a distinct temperature rise from the 1970s onwards. According to NASA, this is not expected to change within the 21st century.\n\nIce core analyses from Greenland and Antarctica show a very similar evolution in greenhouse gases. After a temporary minimum during the preceding subboreal and atlantic the concentrations of carbon monoxide, nitrous oxide and methane slowly started to rise during the Subatlantic. Since 1800 onwards this rise has dramatically accelerated paralleling roughly the concomittant temperature rise. For example, the CO-concentration increased from 280 ppm to a recent value of nearly 400 ppm, methane from 700 to 1800 ppb and NO from 265 to 320 ppb. A comparable rise had already taken place at the changeover to the Holocene, but this process then took nearly 5,000 years. This sudden release of greenhouse gases into the atmosphere by human society represents an unprecedented experiment with unpredictable consequences for Earth's climate. Within the same context the release of juvenile water tied up in fossil fuels like coal, lignite, gas and petrol is generally overlooked.\n\nDuring the 2,500 year duration of the subatlantic global sea level kept on rising by about 1 meter. This corresponds to a rather low rate of 0.4 millimeters per year. Yet at the end of the 19th century a drastic change can be witnessed with a rate increase to 1.8 mm per year in the period 1880 to 2000. In the last twenty years alone satellite measurements document a rise of 50 millimeters which corresponds to a sixfold increase on the pre-industrial rate and a new rise of 2.5 millimeters per year.\n\nToday's sea level was already reached during the oldest subatlantic by the third Litorina transgression. The sea level rise had amounted to 1 meter, since then it oscillated around the zero mark. The transgression established during the \"postlitorine phase\" the Limnea Sea, which is characterized by lower salinity compared with the preceding Littorina Sea due to an isostatic shallowing of the Danish sea straits (Great Belt, Little Belt and Öresund). As a consequence the sea snail \"Littorina littorea\" was gradually replaced by the freshwater snail \"Limnaea ovata\".\n\nDuring the middle subatlantic about 1,300 years ago another rather weak sea level rise took place. Yet the salinity kept falling and therefore new freshwater species were able to immigrate. During the younger and youngest subatlantic about 400 years ago the Limnea Sea was replaced by the Mya Sea as distinguished by the immigration of the clam \"Mya arenaria\" which eventually gave way to the recent Baltic Sea.\n\nIn the North Sea area, which had experienced a slight sea level fall and sea level stagnation during the subboreal, the renewed transgressive pulses of the Dunkerque transgression during the older subatlantic achieved the recent level.\n\nThe wet and cool older subatlantic (pollen zone IX a) is characterized in central Europe by an oak forest intruded more and more by beech (mixed oak forests with lime and elm or mixed oak forests with ash and beech). Humid terrains were generally occupied by alder and ash. The mixed oak forests lasted until the middle subatlantic (pollen zone IX b), which also had a wet but somewhat milder climate. Interspersed within the middle subatlantic are peaks in the occurrence of European beech and European hornbeam (mixed oak forests with beech or mixed oak forests with elm, hornbeam and beech).\n\nDuring the younger subatlantic (pollen zone X a), whose wet and temperate climate resembled already today's conditions, a mixed or an almost pure beech forest established itself. Anthropogenic influences (i. e. agricultural land uses, grazing and forestry) that date back to the bronze age started to become dominant. The actual youngest subatlantic (pollen zone X b) with its wet and temperate climate shows a distinct precipitation gradient with decreasing rainfall from west to east. Natural and indigenous forest communities were severely diminished and more and more replaced by artificially managed forest communities.\n\nIn northwestern Germany mixed oak forests take up 40% amongst the total tree pollen during the older subatlantic and are therefore dominant. Afterwards their count starts fluctuating and they are definitely receding during the younger subatlantic. The percentage of elms and limes as members of the mixed oak forests yet stayed constant. Alders receded from 30 to 10%. Pine trees were also receding but peaked during the youngest subatlantic due to forestry. Hazel (15%), birch (5%) and willow (< 1%) roughly kept their numbers. Significant was the spreading of beech (from 5 to 45%) and hornbeam (from 1 to 15%). According to H. M. Müller the spreading of beech was caused by an increase in humidity since 550 BC and later favoured by a decrease in human settlements during the migrations.\n\nHerbs like cornflower, atriplex, sorrel and plantago also show a pronounced rise from 15 to 65% amongst the total pollen. Cereals were also on the increase – they augmented from 5 to 30% and clearly document an expanding agriculture during the younger subatlantic.\n\nIn northern Germany (Ostholstein) the vegetational evolution was very similar. Remarkable here is the rapid rise of non-tree pollen from 30 to more than 80% (including an increase in cereals from 2 to over 20%) during the younger subatlantic. Amongst the tree pollen the mixed oak forest was able to keep its share of 30%. Alders were also retreating from 40 to 25%. Let alone small fluctuations birch, beech and hornbeam overall conserved their share (hornbeam showed a distinct peak at the beginning younger subatlantic). Pine trees were also augmenting during the youngest subatlantic.\n\nSeveral distinct events could be recognized (from young to old):\n\n\nFaunal diversity has severely suffered since the middle of the 19th century by forced industrialisation and the concomitant pollution of the environment. This trend has reached alarming proportions since 1975. According to the Living Planet Index vertebrates have so far suffered a loss of 40% of their species. Freshwater taxa have even been more severely affected — they have lost up to 50%, mainly due to biotope loss and water pollution. According to NASA, agriculture, fisheries and ecosystems will be increasingly compromised in the Northeastern United States. In the Southeastern United States, increasing wildfires, insect outbreaks and tree diseases are causing widespread tree die-off.\n\n"}
{"id": "3178732", "url": "https://en.wikipedia.org/wiki?curid=3178732", "title": "Surface condenser", "text": "Surface condenser\n\nA surface condenser is a commonly used term for a water-cooled shell and tube heat exchanger installed on the exhaust steam from a steam turbine in thermal power stations. These condensers are heat exchangers which convert steam from its gaseous to its liquid state at a pressure below atmospheric pressure. Where cooling water is in short supply, an air-cooled condenser is often used. An air-cooled condenser is however, significantly more expensive and cannot achieve as low a steam turbine exhaust pressure (and temperature) as a water-cooled surface condenser.\n\nSurface condensers are also used in applications and industries other than the condensing of steam turbine exhaust in power plants.\n\nIn thermal power plants, the purpose of a surface condenser is to condense the exhaust steam from a steam turbine to obtain maximum efficiency, and also to convert the turbine exhaust steam into pure water (referred to as steam condensate) so that it may be reused in the steam generator or boiler as boiler feed water.\n\nThe steam turbine itself is a device to convert the heat in steam to mechanical power. The difference between the heat of steam per unit mass at the inlet to the turbine and the heat of steam per unit mass at the outlet from the turbine represents the heat which is converted to mechanical power. Therefore, the more the conversion of heat per pound or kilogram of steam to mechanical power in the turbine, the better is its efficiency. By condensing the exhaust steam of a turbine at a pressure below atmospheric pressure, the steam pressure drop between the inlet and exhaust of the turbine is increased, which increases the amount of heat available for conversion to mechanical power. Most of the heat liberated due to condensation of the exhaust steam is carried away by the cooling medium (water or air) used by the surface condenser.\n\nThe adjacent diagram depicts a typical water-cooled surface condenser as used in power stations to condense the exhaust steam from a steam turbine driving an electrical generator as well in other applications. There are many fabrication design variations depending on the manufacturer, the size of the steam turbine, and other site-specific conditions.\n\nThe shell is the condenser's outermost body and contains the heat exchanger tubes. The shell is fabricated from carbon steel plates and is stiffened as needed to provide rigidity for the shell. When required by the selected design, intermediate plates are installed to serve as baffle plates that provide the desired flow path of the condensing steam. The plates also provide support that help prevent sagging of long tube lengths.\n\nAt the bottom of the shell, where the condensate collects, an outlet is installed. In some designs, a sump (often referred to as the hotwell) is provided. Condensate is pumped from the outlet or the hotwell for reuse as boiler feedwater.\n\nFor most water-cooled surface condensers, the shell is under [partial] vacuum during normal operating conditions.\n\nFor water-cooled surface condensers, the shell's internal vacuum is most commonly supplied by and maintained by an external steam jet ejector system. Such an ejector system uses steam as the motive fluid to remove any non-condensible gases that may be present in the surface condenser. \nThe Venturi effect, which is a particular case of Bernoulli's principle, applies to the operation of steam jet ejectors.\n\nMotor driven mechanical vacuum pumps, such as the liquid ring type, are also popular for this service.\n\nAt each end of the shell, a sheet of sufficient thickness usually made of stainless steel is provided, with holes for the tubes to be inserted and rolled. The inlet end of each tube is also bellmouthed for streamlined entry of water. This is to avoid eddies at the inlet of each tube giving rise to erosion, and to reduce flow friction. Some makers also recommend plastic inserts at the entry of tubes to avoid eddies eroding the inlet end. In smaller units some manufacturers use ferrules to seal the tube ends instead of rolling. To take care of length wise expansion of tubes some designs have expansion joint between the shell and the tube sheet allowing the latter to move longitudinally. In smaller units some sag is given to the tubes to take care of tube expansion with both end water boxes fixed rigidly to the shell.\n\nGenerally the tubes are made of stainless steel, copper alloys such as brass or bronze, cupro nickel, or titanium depending on several selection criteria. The use of copper bearing alloys such as brass or cupro nickel is rare in new plants, due to environmental concerns of toxic copper alloys. Also depending on the steam cycle water treatment for the boiler, it may be desirable to avoid tube materials containing copper. Titanium condenser tubes are usually the best technical choice, however the use of titanium condenser tubes has been virtually eliminated by the sharp increases in the costs for this material. The tube lengths range to about 85 ft (26 m) for modern power plants, depending on the size of the condenser. The size chosen is based on transportability from the manufacturers’ site and ease of erection at the installation site. The outer diameter of condenser tubes typically ranges from 3/4 inch to 1-1/4 inch, based on condenser cooling water friction considerations and overall condenser size.\n\nThe tube sheet at each end with tube ends rolled, for each end of the condenser is closed by a fabricated box cover known as a waterbox, with flanged connection to the tube sheet or condenser shell. The waterbox is usually provided with man holes on hinged covers to allow inspection and cleaning.\n\nThese waterboxes on inlet side will also have flanged connections for cooling water inlet butterfly valves, small vent pipe with hand valve for air venting at higher level, and hand operated drain valve at bottom to drain the waterbox for maintenance. Similarly on the outlet waterbox the cooling water connection will have large flanges, butterfly valves, vent connection also at higher level and drain connections at lower level. Similarly thermometer pockets are located at inlet and outlet pipes for local measurements of cooling water temperature.\n\nIn smaller units, some manufacturers make the condenser shell as well as waterboxes of cast iron.\n\nOn the cooling water side of the condenser:\n\nThe tubes, the tube sheets and the water boxes may be made up of materials having different compositions and are always in contact with circulating water. This water, depending on its chemical composition, will act as an electrolyte between the metallic composition of tubes and water boxes. This will give rise to electrolytic corrosion which will start from more anodic materials first.\n\nSea water based condensers, in particular when sea water has added chemical pollutants, have the worst corrosion characteristics. River water with pollutants are also undesirable for condenser cooling water.\n\nThe corrosive effect of sea or river water has to be tolerated and remedial methods have to be adopted. One method is the use of sodium hypochlorite, or chlorine, to ensure there is no marine growth on the pipes or the tubes. This practice must be strictly regulated to make sure the circulating water returning to the sea or river source is not affected.\n\nOn the steam (shell) side of the condenser:\n\nThe concentration of undissolved gases is high over air zone tubes. Therefore, these tubes are exposed to higher corrosion rates. Some times these tubes are affected by stress corrosion cracking, if original stress is not fully relieved during manufacture. To overcome these effects of corrosion some manufacturers provide higher corrosive resistant tubes in this area.\n\nAs the tube ends get corroded there is the possibility of cooling water leakage to the steam side contaminating the condensed steam or condensate, which is harmful to steam generators. The other parts of water boxes may also get affected in the long run requiring repairs or replacements involving long duration shut-downs.\n\nCathodic protection is typically employed to overcome this problem. Sacrificial anodes of zinc (being cheapest) plates are mounted at suitable places inside the water boxes. These zinc plates will get corroded first being in the lowest range of anodes. Hence these zinc anodes require periodic inspection and replacement. This involves comparatively less down time. The water boxes made of steel plates are also protected inside by epoxy paint.\n\nAs one might expect, with millions of gallons of circulating water flowing through the condenser tubing from seawater or fresh water, anything that is contained within the water flowing through the tubes can ultimately end up on either the condenser tubesheet (discussed previously) or within the tubing itself. Tube-side fouling for surface condensers falls into five main categories; particulate fouling like silt and sediment, biofouling like slime and biofilms, scaling and crystallization such as calcium carbonate, macrofouling which can include anything from zebra mussels that can grow on the tubesheet, to wood or other debris that blocks the tubing, and finally, corrosion products (discussed previously).\n\nDepending on the extent of the fouling, the impact can be quite severe on the condenser's ability to condense the exhaust steam coming from the turbine. As fouling builds up within the tubing, an insulating effect is created and the heat-transfer characteristics of the tubes are diminished, often requiring the turbine to be slowed to a point where the condenser can handle the exhaust steam produced. Typically, this can be quite costly to power plants in the form of reduced output, increase fuel consumption and increased CO emissions. This \"derating\" of the turbine to accommodate the condenser's fouled or blocked tubing is an indication that the plant needs to clean the tubing in order to return to the turbine's nameplate capacity. A variety of methods for cleaning are available, including online and offline options, depending on the plant's site-specific conditions.\n\n\nNational and international test codes are used to standardize the procedures and definitions used in testing large condensers. In the U.S., ASME publishes several performance test codes on condensers and heat exchangers. These include ASME PTC 12.2-2010, Steam Surface Condensers,and PTC 30.1-2007, Air cooled Steam Condensers.\n"}
{"id": "44655902", "url": "https://en.wikipedia.org/wiki?curid=44655902", "title": "Tasman Front", "text": "Tasman Front\n\nThe Tasman Front is a relatively warm water east-flowing surface current and thermal boundary that separates the Coral Sea to the north and the Tasman Sea to the south. The name was proposed by Denham and Crook in 1976, to describe a thermal front that extends from Australia and New Zealand between the Coral Sea and Tasman Sea. Originating in the edge of the East Australian Current (EAC), the Tasman Front meanders eastward between longitudes 152° E and 164° E and latitudes 31° S and 37° S, then reattaches to the coastline at New Zealand, forming the East Auckland Current.\n\nTopography plays a dominant role in establishing the Tasman Front. Data on the Tasman Front shows that the path of the front is influenced in part by the forcing of the flow over the major ridge systems. Meanders observed in the Tasman Front can be driven by meridional flows along ridges such as those observed at the New Caledonia Trough (166° E) and the Norfolk Ridge (167° E). Abyssal currents also drive meanders associated with the Lord Howe Rise (161° E) and Dampier Ridge (159° E).\n\nThere have been a number of observational and modeling studies on this front in addition to a number of paleo-oceanographic studies of marine sediments. Contrarily, there have been few biological observational studies, but those have been conducted resulted in relating the physical features of the front to properties of fish communities. Likewise, there are even fewer studies relating biogeochemical properties to physical processes of the Tasman Front.\n\n"}
{"id": "17520820", "url": "https://en.wikipedia.org/wiki?curid=17520820", "title": "The Messiah's Donkey", "text": "The Messiah's Donkey\n\nIn Jewish tradition, the Messiah's Donkey (Hebrew: חמורו של משיח) refers to the donkey upon which the Messiah will arrive to redeem the world at the end of days. In Modern Hebrew the phrase \"the Messiah's donkey\" is used to refer to someone who does the 'dirty work' on behalf of someone else.\n\nThe origin of the belief can be found in : \"... your king is coming to you; righteous and having salvation is he, humble and mounted on a donkey, on a colt, the foal of a donkey.\" The 'king' referred to in this verse is interpreted by Chazal as referring to the Messiah.\n\nIn the discussion regarding this verse in the Babylonian Talmud (Sanhedrin 98a) a story is told of the Persian king Shevor, who asks \nSamuel one of the Amoraim: \"Why doesn't your Messiah come riding on a horse? If he lacks one, I'll be glad to provide him with one of my fast horses!\" In response to the ridicule of the king, Samuel answers: \"Do you have a horse that has a hundred shades of color?\" [As the donkey of the messiah will be such].\n\nIn the New Testament () it is told that as Jesus approached the Mount of Olives, he sent two of his disciples to a nearby village to fetch him a donkey. Upon their return, Jesus then rode the donkey into Jerusalem where he was met by cheering crowds. According to the Christian religious tradition, this was the fulfillment of the prophecy of Zechariah 9:9.\n\nAccording to some Muslim historians and scholars the prophecy of the Messiah's Donkey was fulfilled when Umar ibn al-Khattab travelling alone with one donkey and one servant. When he arrived in Jerusalem, he was greeted by Sophronius, who undoubtedly must have been amazed that the caliph of the Muslims, one of the most powerful people in world arriving on a donkey which fulfilled the prophecy in Zechariah 9:9: \"... your king is coming to the arrival.\"\n\nIn Israel, the phrase \"the Messiah's Donkey\" can also refer to the controversial political-religious doctrine ascribed to the teachings of Avraham Yitzhak Kook which claims that secular Jews, which represent the material world, are an instrument in the hands of God whose purpose it was to establish the State of Israel and begin the process of redemption, but upon its establishment they would be required to step aside and allow the Religious-Haredi public to govern the state. According to this analogy, the secular Jewish public are the \"donkey\", while the Religious-Haredi public who would take their place represent a collective quasi-Messianic body. A book called \"The Messiah's Donkey\", which focuses on this issue, was published in 1998 by Seffi Rachlevsky and caused widespread controversy among the Jewish-Israeli public; according to Hassidic teaching the donkey is a symbol of the fact that the Messiah and Messianic age will not oppose the material world, but rather harness it for sacred purposes. Thus, the act of riding upon the donkey is a symbol of the sovereignty of the Messiah over the material world (represented by the donkey).\n"}
{"id": "37373101", "url": "https://en.wikipedia.org/wiki?curid=37373101", "title": "The Quiet Garden Trust", "text": "The Quiet Garden Trust\n\nThe Quiet Garden Trust is a non-profit organisation which encourages the provision of Quiet Gardens where people can set aside time for contemplation, prayer and renewal. The opening of the gardens is controlled by the respective owners, and the Trust plays a co-ordinating and resourcing role. There are currently about three hundred Quiet Gardens in eighteen countries, and the idea has grown to encompass quiet spaces in churches, schools, hospitals and prisons. The gardens are open to people of all faiths, for stillness and reflection.\n\nThe Quiet Garden Movement started in 1992, founded by Revd Philip Roderick, then the Director of the Chiltern Christian Training Programme in the Diocese of Oxford. The first Quiet Garden was a domestic garden in Stoke Poges, Buckinghamshire, belonging to Geoffrey Cooper, who was The Daily Telegraph’s aviation correspondent.\n\nRoderick is an Anglican priest, a percussionist, an educator and a writer who worked as a trainer in theology and spirituality for both laity and clergy. Originally from South Wales, he was brought up a Methodist, and read Philosophy and English at Swansea University. Many threads are woven into his spiritual experience, including meditation, the monastic tradition and contact with other religions. He was ordained and became a university chaplain, and it was after visiting Christian communities in India and America during a sabbatical in 1992 that he realised the significance in the life of Jesus of withdrawing to a quiet place to spend time in solitude. It was this that led him to found the Quiet Garden Movement later that year.\n\nThe Quiet Garden Trust was registered as a charity by the Charity Commission for England and Wales on 21 June 1994.\n\nThe Quiet Garden Movement has spread over the past twenty years to encompass about three hundred gardens in many parts of the world, not only in European countries such as the United Kingdom, Ireland, France, Belgium, Finland, Austria, Switzerland and Cyprus, but also in Australia, New Zealand, South Africa, Botswana, Kenya, Uganda, Brazil, Haiti, Canada and the USA.\n\nThe gardens vary widely in size, location and character. Most cater for a broad public, but some were created to serve a particular community, such as the Quiet Garden on the edge of the Nsambya Hospital in Kampala, Uganda for AIDS patients and carers. Polesworth Abbey in Warwickshire has created a sensory garden with fragrant herbal plants, designed in consultation with both sighted and partially sighted people, and a Quiet Prison Garden in HM Prison Bedford (now closed) benefited both prisoners and staff.\n\nOthers are simply private gardens which people have decided to share. For example, Michael and Janet Chapman frequently open their three-acre garden just outside London. “We are lucky enough to have this lovely garden. To hog it all to ourselves would be like keeping an Old Master tucked away in the basement,” says Michael.\n\nThe Quiet Garden Movement transcends normal doctrinal barriers and denominational divisions. Although Roderick is an Anglican clergyman, the concept has been embraced by a diverse range of faiths.\n\nA Quaker group in Beverley, Yorkshire celebrated its fiftieth anniversary by creating a Quiet Garden next to their Friends meeting house. The first Orthodox Quiet Garden was set up in the grounds of the Orthodox Church in Sifton, Manitoba, Canada. Other fellowships that have established Quiet Gardens include Alcester Baptist Church, Immanuel Evangelical Lutheran Church in San Jose, California, and a Roman Catholic church in Stainforth, South Yorkshire that reclaimed derelict land with the help of prisoners from a nearby prison. There is also a Quiet Garden at the English Benedictine Monastery of Worth Abbey in Crawley, West Sussex.\n\nIn the words of Philip Roderick, \"Openness to God in unexpected places is what it is all about. God is all over the place, we only have to open our ears and our eyes.\"\n\nThe patrons of the Trust are:\n"}
{"id": "1716420", "url": "https://en.wikipedia.org/wiki?curid=1716420", "title": "Toxic capacity", "text": "Toxic capacity\n\nToxic capacity can mean the toxicity of a substance, possibly in relation to a specific organism and toxic capacity can mean the capacity of an organism, organic system or ecosystem to contain a toxic substance or a selection of toxic substances (a compound) without showing signs of poisoning or dying.\n\nGenerally people with less mass have a lower toxic capacity than people with larger mass. In particular, children (who have lower mass compared to an adult) are more vulnerable to toxic effects of compounds. The compounds do not have to be poisons but could be medications as well, which is why children's dosages are almost always less than those of an adult, and the overdose danger higher for children.\n\n\n"}
{"id": "18966340", "url": "https://en.wikipedia.org/wiki?curid=18966340", "title": "Turnoff point", "text": "Turnoff point\n\nThe turnoff point for a star refers to the point on the Hertzsprung-Russell diagram where it leaves the main sequence after the exhaustion of its main fuel. It is often referred to as the main sequence turnoff.\n\nBy plotting the turnoff point of the stars in star clusters, one can estimate the cluster's age.\n\nRed dwarfs are stars of 0.08-0.4 solar masses and are also referred to as class M stars. Red dwarfs have sufficient hydrogen mass to sustain hydrogen fusion to helium via the proton-proton chain reaction, but do not have sufficient mass to create the temperatures and pressures necessary to fuse helium to carbon, nitrogen or oxygen (see CNO cycle). However, all their hydrogen is available for fusion, and the low temperatures and pressures mean the lifetimes of these stars on the main sequence from zero point to turn off point is measured in trillions of years. For example, the lifespan of a star of 0.1 solar masses is 6 trillion years. This lifespan greatly exceeds the current age of the universe, therefore all red dwarfs are main sequence stars. Even though extremely long lived, those stars will eventually run out of fuel. Once all of the available hydrogen has been fused stellar nucleosynthesis stops and the remaining heated helium slowly cools by radiation. Gravity will contract the star from lack of expansive pressure from fusion until electron degeneracy pressure compensates. The cooling star is now off the main sequence and is known as a helium white dwarf.\n"}
{"id": "55770940", "url": "https://en.wikipedia.org/wiki?curid=55770940", "title": "Uholka-Shyrokyi Luh primeval beech forest", "text": "Uholka-Shyrokyi Luh primeval beech forest\n\nThe protected forest area, Uholka-Shyrokyi Luh, is located in the Trancarpathian region of Ukraine and belongs to the Carpathian Biosphere Reserve. It is the largest primeval beech forest worldwide with an area of 8800 ha. Since 1920, some parts of this forest have been protected. In 1992, Uholka-Shyrokyi Luh primeval beech forest was designated as UNESCO World Heritage Site. Later, several other primeval and old-growth beech forests in Europe were included (in 2007 and 2011) in the UNESCO World Heritage Site Ancient and Primeval Beech Forests of the Carpathians and Other Regions of Europe as well.<br>\nAs a primeval beech forest, Uholka-Shyrokyi Luh is an important research hotspot. In 2001, a 10 ha (200 x 500 m) research sample plot was installed in Mala Uholka as part of a Swiss-Ukrainian research project, and in 2010 a full statistical inventory of the primeval forest was carried out. The next inventory is planned in 2019 to detect any changes.\n\nThe forest inventory took place on a forest area of 10 300 ha, 8800 ha of which are primeval forests. The rest is considered to be natural forest. During the fieldwork, 314 sample plots, each 500 m in area, were surveyed. Within each plot, all trees with a diameter at breast height (DBH) of 6 cm and more were measured. 97% of all the 6779 trees measured were European beech (\"Fagus sylvatica\" L.). The oldest beech trees were nearly 500 years. The largest tree was a wych elm (\"Ulmus glabra\" Huds.) with a DBH of 150 cm, whereas the largest beech tree had a DBH of 140 cm. 10 beech trees per hectare had a DBH of at least 80 cm. The number of living trees consisted of 435 trees per hectare with a basal area of 36.6 m/ha and volume of 582 m/ha. In addition, the volume of standing and lying deadwood was 163 m/ha. The vertical structure of this forest consisted of mainly three layers. Most gaps in the canopy cover were not larger than the crown of a canopy tree. \n\nTwo ecological hiking trails have been created for visitors to the primeval beech forest of Uholka-Shyrokyi Luh:\n\n"}
{"id": "8154694", "url": "https://en.wikipedia.org/wiki?curid=8154694", "title": "WD-11", "text": "WD-11\n\nThe WD-11 vacuum tube, a triode, was introduced by the Westinghouse Electric corporation in 1922 for their Aeriola RF model radio and found use in other contemporary regenerative receivers (used as a detector-amplifier) including the Regenoflex and Radiola series. \n\nThe WD11 and \"RCA-11\" (and later simply named \"11\" by RCA and Philips/Miniwatt) have the following characteristics:\n\nThe WD-11's design was somewhat ill thought out, when the filament burns out it has a tendency to contact the plate, feeding high voltages back through the heater circuitry. It was replaced just a year later by higher performance tubes which were less likely to encounter this problem, Westinghouse Electric's WD-12 and General Electric's UX-199. No radios using the WD-11 tube were designed after 1924, RCA ceased production and issued a service bulletin describing how to retrofit existing sets to use the newer UX-199 triodes.\n\nBecause of its rarity it has become one of the most valuable vacuum tubes in the world. New-old-stock units have sold for as much as US$180 and used tubes for over $100, more than the original price of the radios that use them. Collectors rarely, if ever use these tubes for fear of burning them out.\n\nSets that use the costly WD-11 and UX-199 tubes can be modified to use the 1A5/GT octal power pentode (Which cost around $2.50) by wiring a 5.1 ohm resistor between the pins of the filament and fabricating an octal-to-four pin adaptor. The pin for the 1A5's suppressor is left unconnected and the screen connected to the plate.\n\nThe type 12 (also known as RCA-12) is electrically identical to the type 11, but with a more common UX4 base.\n\n\nHere is a link on how to modify other tubes to use in place of a wd11:\n"}
{"id": "8521120", "url": "https://en.wikipedia.org/wiki?curid=8521120", "title": "Winter solstice", "text": "Winter solstice\n\nThe winter solstice (or hibernal solstice), also known as midwinter, is an astronomical phenomenon marking the day with the shortest period of daylight and the longest night of the year. It occurs when one of the Earth's poles has its maximum tilt away from the Sun. It happens twice yearly, once in each hemisphere. In the Northern Hemisphere this is the December solstice and in the Southern Hemisphere this is the June solstice.\n\nThe axial tilt of Earth and gyroscopic effects of its daily rotation mean that the two opposite points in the sky to which the Earth's axis of rotation points (axial precession) change very slowly (at the current rate it would take just under 26,000 years to make a complete circle). As the Earth follows its orbit around the Sun, the polar hemisphere that faced away from the Sun, experiencing winter, will, in half a year, face towards the Sun and experience summer. This is because the two hemispheres face opposite directions along Earth's axis, and so as one polar hemisphere experiences winter, the other experiences summer.\n\nMore evident from high latitudes, a hemisphere's winter solstice occurs on the day with the shortest period of daylight and longest night of the year, when the sun's daily maximum elevation in the sky is at its lowest. Although the winter solstice itself lasts only a moment in time, the term sometimes refers to the day on which it occurs. Other names are \"midwinter\", the \"extreme of winter\" (Dongzhi), or the \"shortest day\". In some cultures it is seen as the middle of winter, while in others it is seen as the beginning of winter. In meteorology, winter in the Northern Hemisphere spans the entire period of December through February. The seasonal significance of the winter solstice is in the reversal of the gradual lengthening of nights and shortening hours of daylight during the day. The earliest sunset and latest sunrise dates differ from winter solstice, however, and these depend on latitude, due to the variation in the solar day throughout the year caused by the Earth's elliptical orbit (see earliest and latest sunrise and sunset).\n\nWorldwide, interpretation of the event has varied across cultures, but many have held a recognition of rebirth, involving holidays, festivals, gatherings, rituals or other celebrations around that time.\n\nThe solstice may have been a special moment of the annual cycle for some cultures even during neolithic times. Astronomical events were often used to guide activities such as the mating of animals, the sowing of crops and the monitoring of winter reserves of food. Many cultural mythologies and traditions are derived from this. This is attested by physical remains in the layouts of late Neolithic and Bronze Age archaeological sites, such as Stonehenge in England and Newgrange in Ireland. The primary axes of both of these monuments seem to have been carefully aligned on a sight-line pointing to the winter solstice sunrise (Newgrange) and the winter solstice sunset (Stonehenge). It is significant that at Stonehenge the Great Trilithon was oriented outwards from the middle of the monument, i.e. its smooth flat face was turned towards the midwinter Sun. The winter solstice was immensely important because the people were economically dependent on monitoring the progress of the seasons. Starvation was common during the first months of the winter, January to April (northern hemisphere) or July to October (southern hemisphere), also known as \"the famine months\". In temperate climates, the midwinter festival was the last feast celebration, before deep winter began. Most cattle were slaughtered so they would not have to be fed during the winter, so it was almost the only time of year when a plentiful supply of fresh meat was available. The majority of wine and beer made during the year was finally fermented and ready for drinking at this time. The concentration of the observances were not always on the day commencing at midnight or at dawn, but at the beginning of the pagan day, which in many cultures fell on the previous eve.\n\nBecause the event was seen as the reversal of the Sun's ebbing presence in the sky, concepts of the birth or rebirth of sun gods have been common and, in cultures which used cyclic calendars based on the winter solstice, the \"year as reborn\" was celebrated with reference to life-death-rebirth deities or \"new beginnings\" such as Hogmanay's \"redding\", a New Year cleaning tradition. Also \"reversal\" is yet another frequent theme, as in Saturnalia's slave and master reversals.\n\nIranian people celebrate the night of the Northern Hemisphere's winter solstice as, \"Yalda night\", which is known to be the \"longest and darkest night of the year\". In this night all the family gather together, usually at the house of the oldest, and celebrate it by eating, drinking and reading poems (esp. Hafez). Nuts, pomegranates and watermelons are particularly served during this festival.\n\nThe pagan Scandinavian and Germanic people of northern Europe celebrated a twelve-day \"midwinter\" (winter solstice) holiday called Yule (also called Jul, Julblot, jólablót, midvinterblot, julofferfest). Many modern Christmas traditions, such as the Christmas tree, the Christmas wreath, the Yule log, and others, are direct descendents of Yule customs. Scandinavians still call Yule \"Jul\". In English, the word \"Yule\" is often used in combination with the season \"yuletide\" a usage first recorded in 900. It is believed that the celebration of this day was a worship of these peculiar days, interpreted as the reawakening of nature. The Yule (Jul) particular god was Jólner, which is one of Odin's many names. \n\nThe concept of Yule occurs in a tribute poem to Harold Hårfager from about AD 900, where someone said \"drinking Jul\". Julblot is the most solemn sacrifice feast. At the \"julblotet\", sacrifices were given to the gods to earn blessing on the forthcoming germinating crops. Julblotet was eventually integrated into the Christian Christmas. As a remainder from this Viking era, the Midsummer is still important in Scandinavia, and hence vividly celebrated.\n\nSol Invictus (\"The Unconquered Sun\") was originally a Syrian god who was later adopted as the chief god of the Roman Empire under Emperor Aurelian. His holiday is traditionally celebrated on December 25, as are several gods associated with the winter solstice in many pagan traditions. It have been speculated to be the reason behind Christmas' proximity to the solstice.\n\nIn East Asia, the winter solstice has been celebrated as one of the Twenty-four Solar Terms, called \"Dongzhi\" in Chinese. In Japan, in order not to catch cold in the winter, there is a custom to soak oneself in the yuzu hot bath ( = Yuzuyu). \n\nAlthough the instant of the solstice can be calculated, direct observation of the solstice by amateurs is impossible because the sun moves too slowly or appears to stand still (the meaning of \"solstice\"). However, by use of astronomical data tracking, the precise timing of its occurrence is now public knowledge. One cannot directly detect the precise instant of the solstice (by definition, one cannot observe that an object has stopped moving until one later observes that it has not moved further from the preceding spot, or that it has moved in the opposite direction). Further, to be precise to a single day, one must be able to observe a change in azimuth or elevation less than or equal to about 1/60 of the angular diameter of the sun. Observing that it occurred within a two-day period is easier, requiring an observation precision of only about 1/16 of the angular diameter of the sun. Thus, many observations are of the day of the solstice rather than the instant. This is often done by observing the sunrise and sunset or using an astronomically aligned instrument that allows a ray of light to be cast on a certain point around that time.\n\n\n\nLength of day increases from the equator towards the South Pole in the Southern Hemisphere in December (around the summer solstice there), but decreases towards the North Pole in the Northern Hemisphere at the time of the northern winter solstice.\n\n\n"}
{"id": "4462942", "url": "https://en.wikipedia.org/wiki?curid=4462942", "title": "Worlds in Collision", "text": "Worlds in Collision\n\nWorlds in Collision is a book written by Immanuel Velikovsky and first published April 3, 1950. The book postulated that around the 15th century BC, Venus was ejected from Jupiter as a comet or comet-like object, and passed near Earth (an actual collision is not mentioned). The object changed Earth's orbit and axis, causing innumerable catastrophes that were mentioned in early mythologies and religions around the world. Many of the book's claims are completely rejected by the established scientific community as they are not supported by any available evidence.\n\nThe book was first published on April 3, 1950, by Macmillan Publishers. Macmillan's interest in publishing it was encouraged by the knowledge that Velikovsky had obtained a promise from Gordon Atwater, Director of the Hayden Planetarium, for a sky show based on the book when it was published. The book, Velikovsky's most criticized and controversial, was an instant \"New York Times\" bestseller, topping the charts for eleven weeks while being in the top ten for twenty-seven straight weeks. Despite this popularity, overwhelming rejection of its thesis by the scientific community forced Macmillan to stop publishing it and to transfer the book to Doubleday within two months.\n\nIn the book's preface, Velikovsky summarized his arguments:\n\nThe book proposed that around the 15th century BCE, Venus was ejected from Jupiter as a comet or comet-like object, passed near Earth (an actual collision is not mentioned). The object changed Earth's orbit and axial inclination, causing innumerable catastrophes which were mentioned in early mythologies and religions around the world. Fifty-two years later, it passed close by again, stopping the Earth's rotation for a while and causing more catastrophes. Then, in the 8th and 7th centuries BCE, Mars (itself displaced by Venus) made close approaches to the Earth; this incident caused a new round of disturbances and disasters. After that, the current \"celestial order\" was established. The courses of the planets stabilized over the centuries and Venus gradually became a \"normal\" planet.\n\nThese events lead to several key statements:\n\n\nVelikovsky suggested some additional ideas that he said derived from these claims, including:\nVelikovsky arrived at these proposals using a methodology which would today be called comparative mythology - he looked for concordances in myths and written history of unconnected cultures across the world, following a literal reading of their accounts of the exploits of planetary deities. In this book, he argues on the basis of ancient cosmological myths from places as disparate as India and China, Greece and Rome, Assyria and Sumer. For example, ancient Greek mythology asserts that the goddess Athena sprang from the head of Zeus. Velikovsky identifies Zeus (whose Roman counterpart was the god Jupiter) with the planet Jupiter and Athena (the Roman Minerva) with the planet Venus. This myth, along with others from ancient Egypt, Israel, Mexico, etc. are used to support the claim that \"Venus was expelled as a comet and then changed to a planet after contact with a number of members of our solar system\" (Velikovsky 1972:182).\n\nThe plausibility of the theory was summarily rejected by the physics community, as the cosmic chain of events proposed by Velikovsky contradicts basic laws of physics.\n\nVelikovsky's ideas had been known to astronomers for years before the publication of the book, partially by his writing to astronomer Harlow Shapley of Harvard, partially through his 1946 pamphlet \"Cosmos Without Gravitation\", and partially by a preview of his work in an article in the August 11, 1946, edition of the \"New York Herald Tribune\". An article about the upcoming book was published by \"Harper's Magazine\" in January 1950, which was followed by an article in \"Newsweek\" (Bauer 1984:3-4) and \"Reader's Digest\" in March 1950.\n\nShapley, along with others such as astronomer Cecilia Payne-Gaposchkin (also at Harvard), instigated a campaign against the book before its publication. Initially, they were highly critical of a publisher as reputable as Macmillan publishing such a pseudoscientific book, even as a trade book, and then their disapproval was re-invigorated when Macmillan included it among other trade books of possible interest to professors listed under the category \"Science\" in the back of a textbook catalog mailed to college professors. Within two months of the book's initial release, the publishing of the book was transferred to Doubleday, which has no textbook division.\n\nThe fundamental criticism against this book from the astronomy community was that its celestial mechanics were irreconcilable with Newtonian celestial mechanics, requiring planetary orbits which could not be made to conform to the laws of conservation of energy and conservation of angular momentum (Bauer 1984:70). Velikovsky conceded that the behavior of the planets in his theories is not consistent with Newton's laws of motion and universal gravitation. He proposed that electromagnetic forces could be the cause of the movement of the planets, although such forces between astronomical bodies are essentially zero.\n\nVelikovsky tried to protect himself from criticism of his celestial mechanics by removing the original Appendix on the subject from \"Worlds in Collision\", hoping that the merit of his ideas would be evaluated on the basis of his comparative mythology and use of literary sources alone. However this strategy did not protect him: the appendix was an expanded version of the \"Cosmos Without Gravitation\" monograph, which he had already distributed to Shapley and others in the late 1940s—and they had regarded the physics within it as egregiously in error.\n\nCarl Sagan wrote that the high surface temperature of Venus was known prior to Velikovsky, and that Velikovsky misunderstood the mechanism for this heat. Velikovsky believed that Venus was heated by its close encounter with the Earth and Mars. He also did not understand the greenhouse effect on Venus, which had earlier been elucidated by astronomer Rupert Wildt. Ultimately, Venus is hot due to its proximity to the sun; it does not emit more heat than it receives from the sun, and any heat produced by its celestial movements would have long dissipated. Sagan concludes: \"(1) the temperature in question was never specified [by Velikovsky]; (2) the mechanism proposed for providing this temperature is grossly inadequate; (3) the surface of the planet does not cool off with time as advertised; and (4) the idea of a high surface temperature on Venus was published in the dominant astronomical journal of its time and with an essentially correct argument ten years before the publication of \"Worlds in Collision\"\" (p. 118).\n\nCarl Sagan also noted that \"Velikovsky's idea that the clouds of Venus are composed of hydrocarbons or carbohydrates is neither original nor correct.\" Sagan notes that the presence of hydrocarbon gases (such as petroleum gases) on Venus was earlier suggested, and abandoned, again by Rupert Wildt, whose work is not credited by Velikovsky. Also, the 1962 Mariner 2 probe was erroneously reported in the popular press to have discovered hydrocarbons on Venus. These errors were subsequently corrected, and Sagan later concluded that \"[n]either Mariner 2 nor any subsequent investigation of the Venus atmosphere has found evidence for hydrocarbons or carbohydrates\" (p. 113).\n\nRegarding Jupiter's radio emissions, Sagan noted that \"all objects give off radio waves if they are at temperatures above absolute zero. The essential characteristics of the Jovian radio emission — that it is nonthermal, polarized, intermittent radiation, connected with the vast belts of charged particles which surround Jupiter, trapped by its strong magnetic field — are nowhere predicted by Velikovsky. Further, his 'prediction' is clearly not linked in its essentials to the fundamental Velikovskian theses. Merely guessing something right does not necessarily demonstrate prior knowledge or a correct theory.\" Sagan concluded that \"there is not one case where [Velikovsky's] ideas are simultaneously original and consistent with simple physical theory and observation.\"\n\nHe also noted that it was Athena and not Venus who was born from the head of Zeus – two utterly different goddesses. Athena was never identified with a planet.\n\nTim Callahan, religion editor of \"Skeptic\", presses the case further in claiming that the composition of the atmosphere of Venus is a complete disproof of \"Worlds in Collision\". \"...Velikovsky's hypothesis stands or falls on Venus having a reducing atmosphere made up mainly of hydrocarbons. In fact, the atmosphere of Venus is made up mainly of carbon dioxide—carbon in its \"oxidized\" form—along with clouds of sulfuric acid. Therefore, it couldn't have carried such an atmosphere with it out of Jupiter and it couldn't be the source of hydrocarbons to react with oxygen in our atmosphere to produce carbohydrates. Velikovsky's hypothesis is falsified by the carbon dioxide atmosphere of Venus.\"\n\nAstronomer Dr Philip Plait has pointed out that Velikovsky's hypothesis is also falsified by the presence of the Moon with its nearly circular orbit for which the length of the month has not changed sensibly in the 5,800 years the Jews have used their lunar calendar. \"If Venus were to get so close to the Earth that it could actually exchange atmospheric contents [i.e., closer than from the surface of the Earth],\" as Velikovsky claimed, \". . . the Moon would have literally been flung into interplanetary space. At the very least its orbit would have been profoundly changed, made tremendously elliptical... Had Venus done any of the things Velikovsky claimed, the Moon's orbit would have changed.\"\n\nBy 1974, the controversy surrounding Velikovsky's work had reached the point where the American Association for the Advancement of Science felt obliged to address the situation, as they had previously done in relation to UFOs, and devoted a scientific meeting to Velikovsky. The meeting featured, among others, Velikovsky himself and Carl Sagan. Sagan gave a critique of Velikovsky's ideas and attacked most of the assumptions made in \"Worlds in Collision\". His criticism is published in \"Scientists Confront Velikovsky\" (Ithaca, New York, 1977) edited by Donald Goldsmith and presented in a revised and corrected version in his book \"\" and is much longer than that given in the talk. Sagan's arguments were aimed at a popular audience and he did not remain to debate Velikovsky in person, facts that some have taken to undermine Sagan's analysis. Sagan rebutted these charges, and further critiqued Velikovsky's ideas in his PBS television series \"\". In \"Cosmos\", Sagan also criticizes the scientific community for their attitude toward Velikovsky, stating that while science is a process in which all ideas are subject to a process of extensive scrutiny before any idea can be accepted as fact, the attempt by some scientists to suppress outright Velikovsky's ideas was \"the worst aspect of the Velikovsky affair.\"\n\nLater in November 1974 at the Biennial Meeting of the Philosophy of Science Association held at the University of Notre Dame, Michael W. Friedlander, professor of physics at Washington University in St. Louis, confronted Velikovsky in the symposium \"Velikovsky and the Politics of Science\" with examples of his \"substandard scholarship\" involving the \"distortion of the published scientific literature in quotations that he used to support his theses\". For example, contrary to Velikovsky, R.A. Lyttleton did not write \"the terrestrial planets, Venus included, \"must\" [emphasis added] have originated from the giant planets…\" Rather, Lyttleton wrote \"…it is even possible…\" As Friedlander recounts, \"When I gave each example, [Velikovsky's] response was 'Where did I write that?'; when I showed a photo copy of the quoted pages, he simply switched to a different topic.\"\nA thorough examination of the original material cited in Velikovsky's publications, and a severe criticism of its use, was published by Bob Forrest. Earlier in 1974, James Fitton published a brief critique of Velikovsky's interpretation of myth, drawing on the section \"The World Ages\" and the later interpretation of the Trojan war, that was ignored by Velikovsky and his defenders whose indictment began: \"In at least three important ways Velikovsky's use of mythology is unsound. The first of these is his proclivity to treat all myths as having independent value; the second is the tendency to treat only such material as is consistent with his thesis; and the third is his very unsystematic method.\" A short analysis of the position of arguments in the late 20th century was given by Velikovsky's ex-associate C. Leroy Ellenberger, a former senior editor of \"Kronos\" (a journal to promote Velikovsky's ideas) (Bauer 1995:11), in his essay. Almost ten years later, Ellenberger criticized some Velikovskian and neo-Velikovskian qua \"Saturnist\" ideas in an invited essay.\n\nThe storm of controversy that was created by Velikovsky's works, especially \"Worlds in Collision\", may have helped revive the Catastrophist movements in the last half of the 20th century; it is also held by some working in the field that progress has actually been retarded by the negative aspects of the so-called Velikovsky Affair. The assessment of Velikovsky's work by tree-ring expert Mike Baillie is instructive: \"But fundamentally, Velikovsky did not understand anything about comets … As if to comfort his readers, at one point he says that no planet at present has a course which poses a danger to this planet: '…only a few asteroids—mere rocks, a few kilometres in diameter—have orbits which cross the path of the earth.' … He did not know about the hazard posed by relatively small objects, and, just in case there is any doubt about his mistake, he repeats the notion by noting that a possibility exists of some future collision between planets, 'not a mere encounter between a planet and an asteroid'. This failure to recognize the power of comets and asteroids means that it is reasonable to go back to Velikovsky and delete all the physically impossible text about Venus and Mars passing close to the earth.\"\n\nMore recently, the absence of supporting material in ice core studies (such as the Greenland Dye-3 and Vostok cores), bristlecone pine tree ring data, Swedish clay varves, and many hundreds of cores taken from ocean and lake sediments from all over the world has ruled out any basis for the proposition of a global catastrophe of the proposed dimension within the Late Holocene age. Also, the fossils, geological deposits, and landforms in \"Earth in Upheaval\", which Velikovsky regards as corroborating the hypothesis presented in \"Worlds in Collision\" have been, since their publication, explained in terms of mundane noncatastrophic geologic processes. So far, the only piece of the geologic evidence which has shown to have a catastrophic origin is a \"raised beach\" containing coral-bearing conglomerates found at an elevation of 1,200 feet above sea level within the Hawaiian Islands. The sediments, which were misidentified as a \"raise beach\", are now attributed to megatsunamis generated by massive landslides created by the periodic collapse of the sides of the islands. In addition, these conglomerates, as many of the items cited as evidence for his ideas in \"Earth in Upheaval\" are far too old to be used as valid evidence supporting the hypothesis presented in \"Worlds in Collision\".\n\nIt is referenced in the 1978 version of Invasion of the Body Snatchers.\n\n\n\n"}
