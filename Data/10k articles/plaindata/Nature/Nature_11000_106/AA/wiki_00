{"id": "39141672", "url": "https://en.wikipedia.org/wiki?curid=39141672", "title": "Admittance (geophysics)", "text": "Admittance (geophysics)\n\nIn geophysics, admittance describes the small effects of atmospheric pressure on earth gravity. Studies have also been carried out regarding the gravity of Venus.\nAdmittance in geophysics takes atmospheric pressure as the input and measures small changes in the gravitational field as the output. Geophysics admittance is commonly measured in μGal/mbar. These units convert according to 1 Gal = 0.01 m/s and 1 bar = 100 kPa, so in SI units the measurement would be in units of;\n\nformula_1   or   formula_2   or   formula_3   or, in primary units   formula_4\n\nHowever, the relationship is not a straightforward one of proportionality. Rather, an \"admittance function\" is described which is time and frequency dependent in a complex way.\n"}
{"id": "47037819", "url": "https://en.wikipedia.org/wiki?curid=47037819", "title": "Agulhas Plateau", "text": "Agulhas Plateau\n\nThe Agulhas Plateau is an oceanic plateau located in the south-western Indian Ocean about south of South Africa. It is a remainder of a large igneous province (LIP), the Southeast African LIP, that formed (Ma) at or near the triple junction where Gondwana broke-up into Antarctica, South America, and Africa. The plateau formed together with Northeast Georgia Rise and Maud Rise (now located near the Falkland Island and Antarctica respectively) when the region passed over the Bouvet hotspot.\n\nThe Agulhas Plateau is one of the key structures in the reconstruction of the Gondwana break-up. It was first mapped in 1964 (i.e. part of what would become the Heezen-Tharp map of the world's ocean floor finally published in 1977), but its crustal composition, paleoposition, and geological origin remained enigmatic for decades.\n\nThe boundary between the Earth's crust and the mantle (the Moho) rises from between the Agulhas Bank (south of South Africa) and the Agulhas passage (south of the bank), typical for a continent-ocean transition. The Agulhas Passage consists of 120-160 Ma-old oceanic crust, whereas the 100-80 Ma-old Agulhas Plateau rises above the surrounding ocean floor while the Moho dips to between below it.\n\nThe morphology of the basement below the northern plateau is irregular, suggestive of an oceanic origin. The basement below the southern plateau, however, is smooth, which has been interpreted as indicative of a possible continental origin. ODP drilling at the Northeast Georgia Rise (north-east of South Georgia) indicated that the Agulhas Plateau and the rise formed together and must have an oceanic origin. Some researchers, nevertheless, remained convinced that the plateau was at least partly of continental origin. Over several decades analyses of geoid, MAGSAT, gravitational, and magnetic anomalies data collected across the plateau were used as arguments for both an oceanic and a continental origin.\n\nThe Agulhas Plateau is the remaining core of a large-scale volcanism that started in the Lazarev Sea (today off Antarctica) with the emplacement of the Karoo basalts 184 Ma. This process continued with the formation of the Mozambique Ridge (MOZR)-Agulhas Plateau LIP which was active in phases between 140-95 Ma. This formation coincides with the formation of the Kerguelen-Heard Plateau. The MOZR formed 140-122 Ma and must have reached its maximum extent about 120 Ma while the spreading zone between Africa and Antarctica was located under its eastern flank.\n\nThe South Atlantic Ocean started to open-up 130 Ma when the Falkland Plateau moved westwards along what was becoming the Agulhas-Falkland Fracture Zone (AFFZ). In the wake of the Falkland Plateau, during the Cretaceous quiet interval, first the Natal Valley formed, then the Transkei Basin, a process completed 90 Ma.\n\nThe process continued with the formation of the Agulhas Plateau—Northeast Georgia Rise—Maud Rise LIP (AP-NEGR-MR LIP or Southeast African LIP) at the end of the Early Cretaceous (100 Ma). The AP-NEGR-MR LIP formed when the region passed over the Bouvet hotspot. About 94 Ma the main eruption ended and seafloor spreading detached the NEGR and MR from the AP. Before this separation the AP-NEGR-MR LIP consisted of of oceanic plateau.\n\nThe MOZR and AP are today connected by a crustal corridor, the Transkei Rise, which rises above the surrounding ocean floor. This rise is thought to be the product of continuous but reduced volcanism during the 20 Ma period between the formation of the MOZR-AP LIP and AP-NEGR-MR LIP.\n\nVolcanic layers on the southern Agulhas Plateau where later overlaid by sediments in which traces of either sub-areal or shallow marine erosions indicate that the plateau was near sea-level.\n\nSouthern Africa experienced two periods of erosion and denudation during the Early and Mid-late Cretaceous. The driving forces behind these events is poorly understood, but both periods coincide with LIP formation: the first period (130-120 Ma) coincides with the initial stages of the Gondwana break-up and the second period (100-90 Ma) with the formation of the Agulhas LIP. Somehow, these two events led to the Mesozoic uplift of southern Africa.\n\nThe Antarctic Bottom Water (AABW) flows north-eastward into the Transkei Basin throw the Agulhas Passage and across the southern margin of the Agulhas Plateau. AABW then flows into the Mozambique Basin. Palaeoceanographic evidences show the presence of proto-AABW during the Oligocene (34-23 Ma) and that proto-AABW was strengthened 15 Ma and deflected southward by the increased flow of North Atlantic Deep Water (NADW). NADW flows north of the Agulhas Plateau through the Agulhas Passage into the Transkei Basin where it splits in two and continues into the Natal valley and the Indian Ocean.\n\nAntarctic Intermediate Water (AAIW) originates at the water surface around Antarctica and flows northward into the Indian Ocean. At , it then flows westward along the African east-coast and the Agulhas Bank before retroflecting eastward across the Agulhas Plateau into the Indian Ocean.\n\nThe Agulhas Current, the western boundary current of the Indian Ocean, retroflects abruptly into the Indian Ocean south-west of South Africa and becomes the Agulhas Return Current. Over the Agulhas Plateau the return current forms a major northward loop to bypass it.\n\n\n"}
{"id": "1369", "url": "https://en.wikipedia.org/wiki?curid=1369", "title": "Ambrosia", "text": "Ambrosia\n\nIn the ancient Greek myths, ambrosia (, ) is sometimes the food or drink of the Greek gods, often depicted as conferring longevity or immortality upon whoever consumed it. It was brought to the gods in Olympus by doves.\n\n\"Ambrosia\" is sometimes depicted in ancient art as distributed by a nymph labeled with that name. In the myth of Lycurgus, an opponent to the wine god Dionysus, violence committed against Ambrosia turns her into a grapevine.\n\nAmbrosia is very closely related to the gods' other form of sustenance, \"nectar\". The two terms may not have originally been distinguished; though in Homer's poems nectar is usually the drink and ambrosia the food of the gods; it was with ambrosia Hera \"cleansed all defilement from her lovely flesh\", and with ambrosia Athena prepared Penelope in her sleep, so that when she appeared for the final time before her suitors, the effects of years had been stripped away, and they were inflamed with passion at the sight of her. On the other hand, in Alcman, nectar is the food, and in Sappho and Anaxandrides, ambrosia is the drink. A character in Aristophanes' \"Knights\" says, \"I dreamed the goddess poured ambrosia over your head—out of a ladle.\" Both descriptions could be correct, as ambrosia could be a liquid considered a food (such as honey).\n\nThe consumption of ambrosia was typically reserved for divine beings. Upon his assumption into immortality on Olympus, Heracles is given ambrosia by Athena, while the hero Tydeus is denied the same thing when the goddess discovers him eating human brains. In one version of the myth of Tantalus, part of Tantalus' crime is that after tasting ambrosia himself, he attempts to steal some away to give to other mortals. Those who consume ambrosia typically had not blood in their veins, but ichor the blood of immortals.\n\nBoth nectar and ambrosia are fragrant, and may be used as perfume: in the \"Odyssey\" Menelaus and his men are disguised as seals in untanned seal skins, \"and the deadly smell of the seal skins vexed us sore; but the goddess saved us; she brought ambrosia and put it under our nostrils.\" Homer speaks of ambrosial raiment, ambrosial locks of hair, even the gods' ambrosial sandals.\n\nAmong later writers, \"ambrosia\" has been so often used with generic meanings of \"delightful liquid\" that such late writers as Athenaeus, Paulus and Dioscurides employ it as a technical terms in contexts of cookery, medicine, and botany. Pliny used the term in connection with different plants, as did early herbalists.\n\nAdditionally, some modern ethnomycologists, such as Danny Staples, identify ambrosia with the hallucinogenic mushroom \"Amanita muscaria\": \"it was the food of the gods, their ambrosia, and nectar was the pressed sap of its juices\", Staples asserts.\n\nW. H. Roscher thinks that both nectar and ambrosia were kinds of honey, in which case their power of conferring immortality would be due to the supposed healing and cleansing powers of honey, which is in fact anti-septic, and because fermented honey (mead) preceded wine as an entheogen in the Aegean world; on some Minoan seals, goddesses were represented with bee faces (compare Merope and Melissa).\n\nThe concept of an immortality drink is attested in at least two Indo-European areas: Greek and Sanskrit. The Greek ἀμβροσία (\"ambrosia\") is semantically linked to the Sanskrit (\"amṛta\") as both words denote a drink or food that gods use to achieve immortality. The two words appear to be derived from the same Indo-European form *\"ṇ-mṛ-tós\", \"un-dying\" (\"n-\": negative prefix from which the prefix \"a-\" in both Greek and Sanskrit are derived; \"mṛ\": zero grade of *\"mer-\", \"to die\"; and \"-to-\": adjectival suffix). A semantically similar etymology exists for nectar, the beverage of the gods (Greek: νέκταρ \"néktar\") presumed to be a compound of the PIE roots \"*nek-\", \"death\", and \"-*tar\", \"overcoming\".\n\n\nLycurgus of Thrace, an antagonist of Dionysus, forbade the cult of Dionysus, whom he drove from Thrace, and was driven mad by the god. In his fit of insanity he killed his son, whom he mistook for a stock of mature ivy, and the nymph Ambrosia, who was transformed into the grapevine.\n\n\n"}
{"id": "34982130", "url": "https://en.wikipedia.org/wiki?curid=34982130", "title": "Ataria", "text": "Ataria\n\nAtaria Interpretation Centre is a wetlands interpretation centre and natural history museum for the Salburua wetlands, a Ramsar site and a significant wetlands habitat in the Basque Autonomous Community. The wetlands region is an important green belt on the eastern outskirts of the city of Vitoria - Gasteiz in Álava-Araba province. Ataria showcases the value of the wetlands, which are classified as a class 1 Habitat of European Community Interest, and the importance of biodiversity to Vitoria-Gasteiz's natural heritage. The Salburua marshes are considered to be \"the Basque country's most valuable\narea of wetland\", according to a Fedenatur report for the European Commission in 2004.\n\nThe centre complex was officially opened in July 2009, after three years of construction, at a cost of just under €7 million.\n\nThe contemporary building was designed by QVE Arquitectos of Madrid, Spain. It was shortlisted for the World Architecture Festival in 2008 and nominated for the \"Mies Arch Award\" in 2009. Its construction is considered unique in that the building extends over the wetlands it examines. 100,000 visits to the centre were recorded in the first year of operation.\n\nThe building houses an auditorium, interactive exhibits, classrooms and laboratories for educational use, a café, and offices for the Vitoria-Gasteiz Centre for Environmental Studies.\n\nA cantilevered observation deck, long, allows visitors to look out over the marshlands. The enclosed walkway deck was constructed of laminated veneer lumber and steel bar trusses. Concrete and steel anchors transfer the force to the ground. The structure was described as \"singular\" in a paper by Professor J. L. Fernández-Cabo, who compared it to the large cantilevered timber roof of a pavilion at Hanover fairgrounds.\n\n"}
{"id": "1687038", "url": "https://en.wikipedia.org/wiki?curid=1687038", "title": "Bidirectional reflectance distribution function", "text": "Bidirectional reflectance distribution function\n\nThe bidirectional reflectance distribution function (BRDF; formula_1 ) is a function of four real variables that defines how light is reflected at an opaque surface. It is employed in the optics of real-world light, in computer graphics algorithms, and in computer vision algorithms. The function takes an incoming light direction, formula_2, and outgoing direction, formula_3 (taken in a coordinate system where the surface normal formula_4 lies along the \"z\"-axis), and returns the ratio of reflected radiance exiting along formula_3 to the irradiance incident on the surface from direction formula_2. Each direction formula_7 is itself parameterized by azimuth angle formula_8 and zenith angle formula_9, therefore the BRDF as a whole is a function of 4 variables. The BRDF has units sr, with steradians (sr) being a unit of solid angle.\n\nThe BRDF was first defined by Fred Nicodemus around 1965. The definition is:\n\nformula_10\n\nwhere formula_11 is radiance, or power per unit solid-angle-in-the-direction-of-a-ray per unit projected-area-perpendicular-to-the-ray, formula_12 is irradiance, or power per unit \"surface area\", and formula_13 is the angle between formula_2 and the surface normal, formula_4. The index formula_16 indicates incident light, whereas the index formula_17 indicates reflected light.\n\nThe reason the function is defined as a quotient of two differentials and not directly as a quotient between the undifferentiated quantities, is because other irradiating light than formula_18, which are of no interest for formula_1, might illuminate the surface which would unintentionally affect formula_20, whereas formula_21 is only affected by formula_18.\n\nThe Spatially Varying Bidirectional Reflectance Distribution Function (SVBRDF) is a 6-dimensional function, formula_23, where formula_24 describes a 2D location over an object's surface.\n\nThe Bidirectional Texture Function (BTF) is appropriate for modeling non-flat surfaces, and has the same parameterization as the SVBRDF; however in contrast, the BTF includes non-local scattering effects like shadowing, masking, interreflections or subsurface scattering. The functions defined by the BTF at each point on the surface are thus called Apparent BRDFs.\n\nThe Bidirectional Surface Scattering Reflectance Distribution Function (BSSRDF), is a further generalized 8-dimensional function formula_25 in which light entering the surface may scatter internally and exit at another location.\n\nIn all these cases, the dependence on the wavelength of light has been ignored and binned into RGB channels. In reality, the BRDF is wavelength dependent, and to account for effects such as iridescence or luminescence the dependence on wavelength must be made explicit: formula_26. Note that in the typical case where all optical elements are linear, the function will obey formula_27 except when formula_28: that is, it will only emit light at wavelength equal to the incoming light. In this case it can be paramaterized as formula_29, with only one wavelength parameter.\n\nPhysically realistic BRDFs have additional properties, including,\n\nThe BRDF is a fundamental radiometric concept, and accordingly is used in computer graphics for photorealistic rendering of synthetic scenes (see the rendering equation), as well as in computer vision for many inverse problems such as object recognition. BRDF has also been used for modeling light trapping in solar cells (e.g. using the OPTOS formalism) or low concentration solar photovoltaic systems.\n\nIn the context of satellite remote sensing, NASA uses a BRDF model to characterise surface anisotropy. For a given land area, the BRDF is established based on selected multiangular observations of surface reflectance. While single observations depend on view geometry and solar angle, the MODIS BRDF/Albedo product describes intrinsic surface properties in several spectral bands, at a resolution of 500 meters. The BRDF/Albedo product can be used to model surface albedo depending on atmospheric scattering.\n\nBRDFs can be measured directly from real objects using calibrated cameras and lightsources; however, many phenomenological and analytic models have been proposed including the Lambertian reflectance model frequently assumed in computer graphics. Some useful features of recent models include:\n\n\nW. Matusik et al. found that interpolating between measured samples produced realistic results and was easy to understand.\n\n\nTraditionally, BRDF measurement devices called gonioreflectometers employ one or more goniometric arms to position a light source and a detector at various directions from a flat sample of the material to be measured. To measure a full BRDF, this process must be repeated many times, moving the light source each time to measure a different incidence angle. Unfortunately, using such a device to densely measure the BRDF is very time consuming. One of the first improvements on these techniques used a half-silvered mirror and a digital camera to take many BRDF samples of a planar target at once. Since this work, many researchers have developed other devices for efficiently acquiring BRDFs from real world samples, and it remains an active area of research.\n\nThere is an alternative way to measure BRDF based on HDR images. The standard algorithm is to measure the BRDF point cloud from images and optimize it by one of the BRDF models.\n\n\n\n\n \n"}
{"id": "19782832", "url": "https://en.wikipedia.org/wiki?curid=19782832", "title": "Big Run State Park", "text": "Big Run State Park\n\nBig Run State Park is a public recreation area located at the northern end of the Savage River Reservoir, an impoundment of the Savage River, in Garrett County, Maryland. The state park occupies in Savage River State Forest and encompasses the confluence of Monroe Run and Big Run. Park activities include boating, fishing, hiking, picknicking, and camping.\n\nThe park originated as the Big Run Recreation Area. In 1952, the Maryland State Planning Commission recommended that the \"lightly used\" recreation area be expanded to take advantage of its location on the reservoir that had been newly created on the Savage River.\n\nThe park's climate is similar to mountain weather; cold winters accompanied by mild, humid summers. The park is located within the Western Appalachian Plateau, which gives rise to colder temperatures. The Savage River Reservoir rests in a canyon that experiences high winds, contributing to the weather changes.\nRed-tailed hawk, broad-winged hawk, great-horned owl, screech owl, songbirds, indigo bunting and other forest interior dwellers can be spotted within Big Run. Black bear, bobcat, white-tailed deer and raccoon are among the park's mammalian inhabitants. Grouse, great blue herons, minks, and ducks may be seen seasonally along the reservoir edge.\nThe park's forests contain oak, hickory and native hardwoods. Trout lilies and spring beauties bloom in April, trilliums and lady slippers in the months of May and June, while rhododendrons, bee balm and cardinal flowers bloom in July.\n\nThe park is accessible on New Germany Road from Interstate 68, Exit 24. It offers primitive campsites, group campsites, youth group campsite, and pavilions. Picnicking facilities are located in the park's day use area. The Savage River Reservoir is used for non-motorized boating, electric motoring, and fishing. Facilities include a boat ramp. Fishermen may find walleye, largemouth bass, crappie, yellow perch, bluegill, suckers and trout. The Monroe Run hiking trail and other trails are accessible from the state park. The adjacent Savage River State Forest has trails for multi-purpose use including snowshoeing or snowmobiling.\n\n"}
{"id": "3236161", "url": "https://en.wikipedia.org/wiki?curid=3236161", "title": "Binary cycle", "text": "Binary cycle\n\nA binary cycle power plant is a type of geothermal power plant that allows cooler geothermal reservoirs to be used than is necessary for dry steam and flash steam plants. As of 2010, flash steam plants are the most common type of geothermal power generation plants in operation today, which use water at temperatures greater than that is pumped under high pressure to the generation equipment at the surface. With binary cycle geothermal power plants, pumps are used to pump hot water from a geothermal well, through a heat exchanger, and the cooled water is returned to the underground reservoir. A second \"working\" or \"binary\" fluid with a low boiling point, typically a butane or pentane hydrocarbon, is pumped at fairly high pressure () through the heat exchanger, where it is vaporized and then directed through a turbine. The vapor exiting the turbine is then condensed by cold air radiators or cold water and cycled back through the heat exchanger.\n\nA binary vapor cycle is defined in thermodynamics as a power cycle that is a combination of two cycles, one in a high temperature region and the other in a lower temperature region.\n\nThe use of mercury-water cycles in the United States can be dated back to the late 1920s. A small mercury-water plant which produced about 40 megawatts (MW) was in use in New Hampshire in the 1950s, with a higher thermal efficiency than most of the power plants in use during the 1950s. Unfortunately, binary vapor cycles have a high initial cost and so they are not as economically attractive.\n\nWater is the optimal working fluid to use in vapor cycles because it is the closest to an ideal working fluid that is currently available. The binary cycle is a process designed to overcome the imperfections of water as a working fluid. The cycle uses two fluids in an attempt to approach an ideal working fluid.\n\nFinding the optimal working fluids has pivotal importance, since they make a significant impact on the performance of binary cycles.\n\n\nThe Rankine cycle is the ideal form of a vapor power cycle. The ideal conditions can be reached by superheating the steam in the boiler and condensing it completely in the condenser. The ideal Rankine cycle does not involve any internal irreversibilities and consists of four processes; isentropic compression in a pump, constant pressure heat addition in a boiler, isentropic expansion in a turbine, and constant pressure heat rejection in a condenser.\n\nThis process is designed to reduce the thermodynamic losses incurred in the brine heat exchangers of the basic cycle. The losses occur through the process of transferring heat across a large temperature difference between the high temperature brine and the lower temperature of the working fluid. Losses are reduced by maintaining a closer match between the brine cooling curve and the working fluid heating curve.\n\n“Power is extracted from a stream of hot fluid, such as geothermal water, by passing the stream in heat exchange relationship with a working fluid to vaporize the latter, expanding the vapor through a turbine, and condensing the vapor in a conventional Rankine cycle. Additional power is obtained in a second Rankine cycle by employing a portion of the hot fluid after heat exchange with the working fluid to vaporize a second working fluid having a lower boiling point and higher vapor density than the first fluid.”\n\nThere are numerous binary cycle power stations in commercial production.\n\nBinary cycle power plants have a thermal efficiency of 10-13%.\n\n\n"}
{"id": "27999207", "url": "https://en.wikipedia.org/wiki?curid=27999207", "title": "Boleslav Vladimirovich Likhterman", "text": "Boleslav Vladimirovich Likhterman\n\nBoleslav Vladimirovich Likhterman (1902–1967) was a Soviet medical researcher. He is best remembered as a pioneer in the use of high frequency electrical currents in the treatment of physical ailments and as an editor of the academic journal \"Voprosi Kurortologii, Physiotherapii i Lechebnoi Physicheskoi Kultury\" (Problems of Balneology, Physiotherapy, and Therapeutic Physical Exercises).\n\nBoleslav Vladimirovich Likhterman () was born 14 October 1902 New Style, of ethnic Jewish parents in Simferopol, Tavricheskaya Gubernia, Ukraine, which was then part of the Russian empire. His father, Vladimir (Wulf) Aaronovich Lichterman () was an attorney.\n\nLikhterman attended gymnasium in Simferopol until his graduation, at which time he was awarded a gold medal for academic excellence. Following graduation, Likhterman enrolled in the Medical Department of Crimean (Tavrichesky) University, from which he graduated in 1925.\n\nA 13-month stint in the military followed, with Likhterman serving in Sevastopol as part of the Red Fleet on the Black sea ().\n\nIn December 1926, Likhterman married Sara Evseevna Brusilovskaya, who later became a teacher of English.\n\nAs 1926 came to a close, Likhterman received his first medical appointment, serving as head of a medical office in Belorezk, Bashkiria, part of the Bashkirian People's Commissariat for Health.\n\nIn June 1928, Likhterman moved to the Sevastopol city polyclinic, where he assumed a position as a neurologist and physiotherapist. He also worked as a consultant for the city's primary hospital. Likhterman remained in this position until the coming of World War II.\n\nFrom 1929, Boleslav worked at the I.M. Sechenov State Research Institute of Physical Methods of Treatment, founded by Professor Alexander E. Scherbak in Sevastopol. There he was a member of the committee studying the use of short waves as a part of therapeutic practice. He was made head of the Clinical Department and Neurological Clinic at the Sechenov Institute in 1932, achieving the title of Docent (Associate Professor).\n\nIn 1939, Likhterman was awarded the “Excellence in Healthcare” prize by the USSR People's Commissariat of Health.\n\nIn September 1941, in light of the invasion of the Soviet Union by Nazi Germany, Likhterman and the rest of the Sechenov Institute was evacuated to Kazakhstan in Soviet Central Asia. There Likhterman worked as a chief of the Clinical Department in the hospital for the wounded and as a consultant at the Red Army Central Tuberculosis Sanatorium. His work with the ill took its toll and in 1942 Likhterman himself contracted pulmonary tuberculosis.\n\nIn July 1943, as the tide of the war began to turn, the Sechenov Institute was transferred west to Kislovodsk in Stavropol Krai, Russia. In the fall of 1944, the institute was moved back to the Crimea again, this time to the southern resort city of Yalta. For the next two decades, Likhterman would work as the head of the Neurological Clinic of the renamed I.M. Sechenov Institute of Physical Methods of Treatment and Climatotherapy in Yalta. He became a member of the editorial board of the academic journal “Voprosi Kurortologii, Physiotherapii i Lechebnoi Physicheskoi Kultury” (Problems of Balneology, Physiotherapy, and Therapeutic Physical Exercises) and editor of sections on physiotherapy and balneology for the authoritative \"Great Soviet Encyclopedia\" as well as the \"Small Soviet Encyclopedia.\"\n\nIn 1948 upon the recommendation of the USSR Academy of Medical Sciences, Boleslav Likhterman was conferred the degree of the Doctor of Medical Sciences for his academic publications to date without a formal defense of a dissertation. He was granted the academic rank of the Professor in Nervous Diseases.\n\nIn 1952 for his outstanding service in the field of medical science, Likhterman was awarded the Order of the Red Banner of Labour by the Presidium of the Supreme Soviet of the USSR.\n\nIn 1965, Likhterman was elected an honorary member of the All-Union Society of Physiotherapists and Balneologists.\n\nIn 1966, Likhterman became ill with leukemia, dying of the disease on 20 January 1967 at Yalta.\n\nLikhterman's remains were interned at the Yalta City Cemetery.\n\nLikhterman was the author of 4 monographs on medical topics:\n\n\nLikhterman also wrote over 100 other scientific works on the role of the nervous system, with an emphasis on matters relating to balneology and physiotherapy in the treatment and prevention of nervous and endocrine disorders. Another important topic of interest for Likhterman was the effects of high and ultrahigh frequency currents on healthy and diseased organisms, research which resulted in the development of various methods for the direct and indirect applications of such electrical energy for medical treatment. In particular, he developed a calcium collar electrode method and studied reflex effects of electrophoresis. He also studied the effect of energy treatment upon the brain, including the use of such therapies in conjunction with spa treatment, for treatment of neurasthenic syndromes, trauma and other disorders.\n\n"}
{"id": "51760202", "url": "https://en.wikipedia.org/wiki?curid=51760202", "title": "Clodomiro Picado Research Institute", "text": "Clodomiro Picado Research Institute\n\nThe Instituto Clodomiro Picado is a research center in Coronado, San José Province, Costa Rica. Established in 1970, the institute is a research unit of the Universidad de Costa Rica, responsible for the production of snake antiophidic serums and scientific research on serpents and their venoms, as well as educational and extension programs in rural areas and hospitals. It received its name in honor of Costa Rican scientist Clodomiro Picado Twight.\n\nThe Institute produces antivenoms for human and veterinary use in Central America, Ecuador, Nigeria and Papua New Guinea.\n\n"}
{"id": "1431559", "url": "https://en.wikipedia.org/wiki?curid=1431559", "title": "Core–mantle boundary", "text": "Core–mantle boundary\n\nThe core–mantle boundary (CMB in the parlance of solid earth geophysicists) of the Earth lies between the planet's silicate mantle and its liquid iron-nickel outer core. This boundary is located at approximately 2891 km (1796 mi) depth beneath the Earth's surface. The boundary is observed via the discontinuity in seismic wave velocities at that depth due to the differences between the acoustic impedances of the solid mantle and the molten outer core. P-wave velocities are much slower in the outer core than in the deep mantle while S-waves do not exist at all in the liquid portion of the core. Recent evidence suggests a distinct boundary layer directly above the CMB possibly made of a novel phase of the basic perovskite mineralogy of the deep mantle named post-perovskite. Seismic tomography studies have shown significant irregularities within the boundary zone and appear to be dominated by the African and Pacific large low-shear-velocity provinces (LLSVPs).\n\nThe uppermost section of the outer core is thought to be about 500–1,800 K hotter than the overlying mantle, creating a thermal boundary layer. The boundary is thought to harbor topography, much like Earth's surface, that is supported by solid-state convection within the overlying mantle. Variations in the thermal properties of the core-mantle boundary may affect how the outer core's iron-rich fluids flow, which are ultimately responsible for Earth's magnetic field.\n\nThe approx. 200 km thick layer of the lower mantle directly above the boundary is referred to as the D″ region (\"D double-prime\" or \"D prime prime\") and is sometimes included in discussions regarding the core–mantle boundary zone. The D″ name originates from mathematician Keith Bullen's designations for the Earth's layers. His system was to label each layer alphabetically, A through G, with the crust as 'A' and the inner core as 'G'. In his 1942 publication of his model, the entire lower mantle was the D layer. In 1950, Bullen found his 'D' layer to actually be two different layers. The upper part of the D layer, about 1800 km thick, was renamed D′ (D prime) and the lower part (the bottom 200 km) was named D″.\n\nA seismic discontinuity occurs within Earth's interior at a depth of about 2,900 km (1,800 mi) below the surface, where there is an abrupt change in the speed of seismic waves (generated by earthquakes or explosions) that travel through Earth. At this depth, primary seismic waves (P waves) decrease in velocity while secondary seismic waves (S waves) disappear completely. S waves shear material, and cannot transmit through liquids, so it is thought that the unit above the discontinuity is solid, while the unit below is in a liquid or molten form.\n\nThe discontinuity was discovered by Beno Gutenberg (1889-1960), a seismologist who made several important contributions to the study and understanding of the Earth's interior. The CMB has also been referred to as the Gutenberg discontinuity, the Oldham-Gutenberg discontinuity, or the Wiechert-Gutenberg discontinuity.. In modern times, however, the term Gutenberg discontinuity or the \"G\" is most commonly used in reference to a decrease in seismic velocity with depth that is sometimes observed at about 100 km below the Earth's oceans.\n\n"}
{"id": "17666707", "url": "https://en.wikipedia.org/wiki?curid=17666707", "title": "David Tipling", "text": "David Tipling\n\nDavid Tipling is a professional wildlife photographer with an international reputation. His highly distinctive images have earned him many awards and accolades. Sir David Bellamy described Tipling's photographs in \"The National Parks and other Wild Places of Britain and Ireland\" as \"windows of wonder\".\nHe has won the documentary award for the European Nature Photographer of the Year for his work on emperor penguins. He is the author or commissioned photographer for more than 30 books that include \"Collins Top Birding Spots in Britain & Ireland\", \"The National Parks and Other Wild Places of Britain & Ireland\", \"Attracting Wildlife to your Garden\", and most recently \"The RSPB Guide to Digital Wildlife Photography\".\n\nBorn in Hove Sussex, David Tipling was educated at the Wildernesse School in Sevenoaks. A keen birdwatcher from an early age, his parents bought him his first camera in his teens. Tipling was soon recording the birds in his local area.\n\nA largely self-taught wildlife photographer, Tipling came under the watchful eye of Roy Coles, the then warden of nearby Bough Beech Reservoir, a nature reserve in Kent. Tipling learned his craft largely by 'trial and error' and by training his eye, studying the work of various artist's who gave him his distinctive 'artistic' sense of composition.\n\nBetween 1983 and 1994 he travelled throughout Britain following birdwatchers to photograph the rarities they were travelling to see. This gave him a good grounding in fieldcraft and ornithology. A taste for the rare and unseen, with an artist's sense of composition, was the result. Tipling's work was soon widely published in magazines such as \"British Birds\", \"Birds Illustrated\", \"Birdwatch\", \"Dutch Birding\" and \"Birding World\"; as well as being used in national newspapers.\n\nWildlife is usually difficult to find, troublesome to approach and often impossible to capture, thus a knowledge of animal behaviour is a necessary skill in all great wildlife photography. The animal photographer needs above all to be able to predict actions. An ability to stalk animals is often a requirement. \n\nWhat makes Tipling's photographic work distinctive is his artistic sense of composition. His work is influenced by the Swedish painter Bruno Liljefors whose often brooding depictions of predator-prey action – the hunts engaged between sea eagle and eider, goshawk and black grouse – help to create their darkly primitive atmosphere. and by Robert Gillmor's strong graphic. Tipling cites some of the great Eastern painters who practised Oriental ink painting which has its roots in calligraphy, as influential. Much of his work resonates with that sense of proportion achieved in the light, striking shapes of Xu Wei's work.\n\nA keen conservationist, Tipling is perhaps closer than others to witnessing the dangers birds in the wild face. In 1994 his article in the \"New Scientist\" alerted its readers to the declining state of the barn owl:\n\nTipling's latest project with author and naturalist Mark Cocker and natural history researcher Jonathan Elphick, is \"Birds and People\". Birds and People is a ten-year-long collaboration between the publishers Random House and BirdLife International, to survey and document worldwide, the cultural significance of birds,\n\nTipling's role within the \"Birds and People\" project, as the commissioned photographer and picture editor, is both to photograph and find images that illustrate the connection between man and bird as depicted in the stories recorded and covered by the project.\n\nImages will range from depictions of the birds themselves to pieces of art, objects such as ceramics, architecture and of course people. The \"Birds and People\" project involves an open internet forum, for individuals worldwide to document their reflections, experiences and stories about birds.\n\nTipling's latest work, undertaken with the ornithologist Jonathan Elphick, and due to be published later this year, is \"GREAT BIRDS: 200 Star Species of Britain\".\n\nTipling is a photographic consultant for \"British Birds\". From 1994 to 2003 he had a monthly quizbird column in Bird Watching Magazine. Picture credits include over 500 book covers for example \"Pecked to Death by Ducks\" by Tim Cahill, various Lonely Planet guides including Antarctica and the \"Secret Life of Birds\" by Dominic Couzens.\n\nBooks completely illustrated by Tipling include \"Collins Gem Guide to Garden Birds\", \"Understanding Bird Behaviour\" by Stephen Moss, \"Bird Identification and Fieldcraft\" by Mark Ward. Tipling has also undertaken major advertising work for clients including Pedigree Chum and Fuji Film (who ran an advertising campaign using his emperor penguins from 2001–2003. Tipling's emperor penguins were also used in the Walmart ad campaign in the United States in 2002.\n\n\n"}
{"id": "1748525", "url": "https://en.wikipedia.org/wiki?curid=1748525", "title": "Diapir", "text": "Diapir\n\nA diapir (; French, from Greek \"diapeirein\", to pierce through) is a type of geologic intrusion in which a more mobile and ductily deformable material is forced into brittle overlying rocks. Depending on the tectonic environment, diapirs can range from idealized mushroom-shaped Rayleigh–Taylor-instability-type structures in regions with low tectonic stress such as in the Gulf of Mexico to narrow dikes of material that move along tectonically induced fractures in surrounding rock. The term was introduced by the Romanian geologist Ludovic Mrazek, who was the first to understand the principle of salt intrusion and plasticity. The term \"diapir\" may be applied to igneous structures, but it is more commonly applied to non-igneous, relatively cold materials, such as salt domes and mud diapirs.\nIn addition to Earth-based observations, diapirism is thought to occur on Neptune's moon Triton, Jupiter's moon Europa, Saturn's moon Enceladus, and Uranus's moon Miranda.\n\nDiapirs commonly intrude vertically upward along fractures or zones of structural weakness through denser overlying rocks because of density contrast between a less dense, lower rock mass and overlying denser rocks. The density contrast manifests as a force of buoyancy. The process is known as \"diapirism\". The resulting structures are also referred to as \"piercement structures\".\n\nIn the process, segments of the existing strata can be disconnected and pushed upwards. While moving higher, they retain much of their original properties such as pressure, which can be significantly different from that of the shallower strata they get pushed into. Such overpressured \"Floaters\" pose a significant risk when trying to drill through them. There is an analogy to a Galilean thermometer.\n\nRock types such as evaporitic salt deposits, and gas charged muds are potential sources of diapirs. Diapirs also form in the earth's mantle when a sufficient mass of hot, less dense magma assembles. Diapirism in the mantle is thought to be associated with the development of large igneous provinces and some mantle plumes.\n\nExplosive, hot volatile rich magma or volcanic eruptions are referred to generally as diatremes. Diatremes are not usually associated with diapirs, as they are small-volume magmas which ascend by volatile plumes, not by density contrast with the surrounding mantle.\nDiapirs or piercement structures are structures resulting from the penetration of overlaying material. By pushing upward and piercing overlying rock layers, diapirs can form anticlines, salt domes and other structures capable of trapping petroleum and natural gas. Igneous intrusions themselves are typically too hot to allow the preservation of preexisting hydrocarbons.\n\n"}
{"id": "1432218", "url": "https://en.wikipedia.org/wiki?curid=1432218", "title": "Dion, Pieria", "text": "Dion, Pieria\n\nDion or Dio (, \"Díon\"; , \"Dío\"; ) is a village and a former municipality in the Pieria regional unit, Greece. Since the 2011 local government reform, it is part of the municipality Dio-Olympos, of which it is a municipal unit. It is located at the foot of Mount Olympus at a distance of 17km from the capital city of Katerini.\n\nIt is best known for its great ancient Macedonian sanctuary of Zeus and city, much of which is visible in the Archaeological Park of Dion and the Archaeological Museum of Dion.\n\nThe ancient city owes its name to the most important Macedonian sanctuary dedicated to Zeus (\"Dios\", \"of Zeus\"), leader of the gods who dwelt on Mount Olympus; as recorded by Hesiod's \"Catalogue of Women\", Thyia, daughter of Deucalion, bore Zeus two sons, Magnes and Makednos, eponyms of Magnetes and Macedonians, who dwelt in Pieria at the foot of Mount Olympus. Hence from very ancient times, a large altar had been set up for the worship of Olympian Zeus and his daughters, the Muses, in a unique environment characterised by rich vegetation, towering trees, countless springs and a navigable river. For this reason Dion was the \"sacred place\" of the Ancient Macedonians. It was the place where the kings made splendid sacrifices to celebrate the new year of the Macedonian calendar at the end of September. In the Spring, purification rites of the army and victory feasts were held.\n\nThe first mention of Dion in history comes from Thucydides, who reports that it was the first city reached by the Spartan general Brasidas after crossing from Thessaly into Macedon on his way through the realm of his ally Perdiccas II during his expedition against the Athenian colonies of Thrace in 424 BC. According to Diodorus Siculus, it was Archelaus I who, at the end of the 5th century BC when the Macedonian state acquired great power and emerged onto the stage of history, gave the city and its sanctuary their subsequent importance by instituting a nine-day festival of games that included athletic and dramatic competitions in honor of Zeus and the Muses, whose organisation was overseen by the Macedonian kings themselves.\n\nPhilip II and Alexander the Great celebrated victories here, and Alexander assembled his armies and performed magnificent sacrifices here on the eve of his campaign to Asia in 334BC.\n\nMany ancient authors speak of the sculptural bronze masterpiece by Lysippos made for Alexander depicting 25 mounted companions who fell at the Battle of the Granicus and later taken to Rome by Metellus.\n\nA city was built adjacent to the sacred sites that acquired monumental form during the reigns of Alexander the Great's successors and Cassander took a great interest in the city erecting strong walls and public buildings, so that in Hellenistic times Dion was renowned far and wide for its fortification and splendid monuments. Dion and its sanctuary was destroyed during the Social War in 219 BC by Aetolian invaders but was immediately rebuilt by Philip V. Many of the dedications from the sanctuary that had been destroyed were buried in pits, including royal inscriptions and treaties, and these have been discovered recently.\n\nIt fell to the Romans in 169 BC and the city was given a new lease of life in 32/31 BC when Octavian founded the Colony of COLONIA JULIA AUGUSTA DIENSIS here. Coins of colonial Dion survive. It experienced its second heyday during the reigns of 2nd- and 3rd-century AD Roman emperors who were fond of Alexander the Great. Dion's final important period was in the 4th and 5th centuries AD when it became the seat of a bishopric. It was abandoned following major earthquakes and floods.\n\nThe modern village at the site was called \"Malathria\" until 1961, when it was renamed to \"Dio\".\n\nThe site of ancient Dion was first identified by the English traveler William Martin Leake on December 2, 1806, in the ruins adjoining the village of Malathria. He published his discovery in the third volume of his \"Travels in Northern Greece\" in 1835. Léon Heuzey visited the site during his famous Macedonian archaeological mission of 1855 and again in 1861 when he also detected the ancient Leivithra. Later, the epigraphist G. Oikonomos published the first series of inscriptions. Nevertheless, systematic archaeological exploration did not begin until 1928. From then until 1931, G. Sotiriadis carried out a series of surveys, uncovering a 4th-century BC Macedonian tomb and an early Christian basilica. Excavations were not resumed until 1960 under the direction of G. Bakalakis in the area of the theatre and the wall. Since 1973, Professor D. Pandermalis of the Aristotle University of Thessaloniki has conducted archaeological research in the city.\n\nDion is the site of a large temple dedicated to Zeus, as well as a series of temples to Demeter and to Isis (the Egyptian goddess was a favorite of Alexander).\n\nExcavation of the magnificent House of Dionysos revealed a mosaic of exceptionally fine quality.\nA rare and unusual find in the museum is a bronze \"hydraulis\" or hydraulic musical pipe organ found in a former workshop.\n\nIn 2006, a statue of Hera was found built into the walls of the city. The statue, 2200 years old, had been used by the early Christians of Dion as filling for the city's defensive wall.\n\nIn October 1992, the municipality Dio (\"Dimos Diou\") was formed. At the 1997 Kapodistrias reform, it was expanded with the former communities Agios Spyridonas, Karitsa, Kondariotissa, Nea Efesos and Vrontou. The administrative center was in the village of Kondariotissa. At the 2011 local government reform Dio merged with the former municipalities East Olympos and Litochoro to form the new municipality Dio-Olympos. Dio became a municipal unit of the newly formed municipality, and the former municipal districts became communities. The community of Dion consists of the village of the same name and Platanakia. The municipal unit has an area of 172.743 km, the community 31.375 km.\n\n\n\n"}
{"id": "2787048", "url": "https://en.wikipedia.org/wiki?curid=2787048", "title": "East Antarctica", "text": "East Antarctica\n\nEast Antarctica, also called Greater Antarctica, constitutes the majority (two-thirds) of the Antarctic continent, lying on the Indian Ocean side of the continent, separated from West Antarctica by the Transantarctic Mountains. It lies almost entirely within the Eastern Hemisphere and its name has been accepted for more than a century. It is generally higher than West Antarctica and includes the Gamburtsev Mountain Range in the centre.\n\nApart from small areas of the coast, East Antarctica is permanently covered by ice. The only terrestrial plant life is lichens, mosses and algae clinging to rocks, and there are a limited range of invertebrates including nematodes, springtails, mites and midges. The coasts are the breeding ground for various seabirds and penguins, and the leopard seal, Weddell seal, elephant seal, crabeater seal and Ross seal breed on the surrounding pack ice in summer. \n\nAlmost completely covered in thick, permanent ice, East Antarctica comprises Coats Land, Queen Maud Land, Enderby Land, Kemp Land, Mac. Robertson Land, Princess Elizabeth Land, Wilhelm II Land, Queen Mary Land, Wilkes Land, Adélie Land, George V Land, Oates Land and Victoria Land. All but a small portion of this region lies within the Eastern Hemisphere, a fact that has suggested the name. The name has been in existence for more than 90 years (Balch, 1902; Nordenskjöld, 1904), but its greatest use followed the International Geophysical Year (1957–58) and explorations disclosing that the Transantarctic Mountains, provide a useful regional separation of East Antarctica and West Antarctica. The name was approved in the United States by the Advisory Committee on Antarctic Names (US-ACAN) in 1962. East Antarctica is generally higher than West Antarctica, and is considered the coldest place on Earth.\n\nThe subglacial Gamburtsev Mountain Range, about the size of the European Alps, in the center of East Antarctica, are believed to have been the nucleation site for the East Antarctic Ice Sheet, just underneath Dome A.\n\nVery little of East Antarctica is not covered with ice. The small areas that remain free of ice (Antarctic oasis), including the McMurdo Dry Valleys inland, constitute a tundra-type biodiversity region known as Maudlandia Antarctic desert, after Queen Maud Land. There are no trees or shrubs, as only very limited plant life can survive here; the flora consists of lichens, moss, and algae that are adapted to the cold and wind, and cling to rocks.\n\nThe coasts are home to seabirds, penguins, and seals, which feed in the surrounding ocean, including the emperor penguin, which famously breeds in the cold, dark Antarctic winter.\n\nSeabirds of the coast include southern fulmar \"(Fulmarus glacialoides)\", the scavenging southern giant petrel \"(Macronectes giganteus)\", Cape petrel \"(Daption capense)\", snow petrel \"(Pagodroma nivea)\", the small Wilson's storm-petrel \"(Oceanites oceanicus)\", the large south polar skua \"(Catharacta maccormicki)\", and Antarctic petrel \"(Thalassoica antarctica)\".\n\nThe seals of the Antarctic Ocean include leopard seal \"(Hydrurga leptonyx)\", Weddell seal \"(Leptonychotes weddellii)\", the huge southern elephant seal \"(Mirounga leonina)\", crabeater seal \"(Lobodon carcinophagus)\" and Ross seal (\"Ommatophoca rossii\").\n\nThere are no large land animals but bacteria, nematodes, springtails, mites, and midges live on the mosses and lichens.\n\nThe remote and extremely cold bulk of Antarctica remains almost entirely untouched by human intervention. The area is protected by the Antarctic Treaty System which bans industrial development, waste disposal and nuclear testing, while the Barwick Valley, one of the Dry Valleys, and Cryptogam Ridge on Mount Melbourne are specially protected areas for their undisturbed plant life.\n\n\n"}
{"id": "2208458", "url": "https://en.wikipedia.org/wiki?curid=2208458", "title": "Edaphology", "text": "Edaphology\n\nEdaphology (from Greek , \"edaphos\", \"ground\", and , \"-logia\") is one of two main divisions of soil science, the other being pedology. Edaphology is concerned with the influence of soils on living things, particularly plants. Edaphology includes the study of how soil influences humankind's use of land for plant growth as well as man's overall use of the land. General subfields within edaphology are agricultural soil science (known by the term agrology in some regions) and environmental soil science. (Pedology deals with pedogenesis, soil morphology, and soil classification.)\n\nIn Russia, edaphology is considered equivalent to pedology, but is recognized to have an applied sense consistent with agrophysics and agrochemistry outside Russia.\n\nXenophon (431–355 BC), and Cato (234–149 BC), were early edaphologists. Xenophon noted the beneficial effect of turning a cover crop into the earth. Cato wrote De Agri Cultura (\"On Farming\") which recommended tillage, crop rotation and the use of legumes in the rotation to build soil nitrogen. He also devised the first soil capability classification for specific crops.\n\nJan Baptist van Helmont (1577–1644) performed a famous experiment, growing a willow tree in a pot of soil and supplying only rainwater for five years. The weight gained by the tree was greater than the weight loss of the soil. He concluded that the willow was made of water. Although only partly correct, his experiment reignited interest in edaphology.\n\nAgricultural soil science is the application of soil chemistry, physics, and biology dealing with the production of crops. In terms of soil chemistry, it places particular emphasis on plant nutrients of importance to farming and horticulture, especially with regard to soil fertility and fertilizer components. \n\nPhysical edaphology is strongly associated with crop irrigation and drainage. \n\nSoil husbandry is a strong tradition within agricultural soil science. Beyond preventing soil erosion and degradation in cropland, soil husbandry seeks to sustain the agricultural soil resource though the use of soil conditioners and cover crops.\n\nEnvironmental soil science studies our interaction with the pedosphere on beyond crop production. Fundamental and applied aspects of the field address vadose zone functions, septic drain field site assessment and function, land treatment of wastewater, stormwater, erosion control, soil contamination with metals and pesticides, remediation of contaminated soils, restoration of wetlands, soil degradation, and environmental nutrient management. It also studies soil in the context of land-use planning, global warming, and acid rain.\n\n\n"}
{"id": "3313203", "url": "https://en.wikipedia.org/wiki?curid=3313203", "title": "Edeowie glass", "text": "Edeowie glass\n\nEdeowie glass is a slag-like, opaque, locally-abundant natural glass found as vesicular free forms or sheet-like/ropy masses. It is located throughout a semi-continuous swath in baked pod-like clay-bearing sediment, about 55 km long and 10 km wide along the western side of the Flinders Ranges near Parachilna, South Australia and Lake Torrens. The region in which this glass is found is mostly restricted to concentrations correlated to the ancient shoreline terrace sequence at the locality. \nIt is typically black in appearance, but can occur as variegated grey-green with various streak-like impurities. Pale grey and red-brownish surfaces can be caused by chemical weathering (oxidation, mineralization) and devitrification.\n\nAn origin for Edeowie glass has been assigned to either lightning strikes, or hypervelocity impact by one or several asteroids or comets. Many of its notable features, such as planar deformation features (PDFs) in quartz crystals found within and associated with Edeowie glass, are recognized to have been caused by pressures only exerted by such events capable of rapid shock metamorphism.\n\nEdeowie glass yields dates spanning 0.67-0.07 mya (~670,000-70,000 BP), but some outlier dates are as recent as the middle Holocene.\n\n\n"}
{"id": "42497689", "url": "https://en.wikipedia.org/wiki?curid=42497689", "title": "Good Environmental Status", "text": "Good Environmental Status\n\nGood Environmental Status is a qualitative description of the state of the seas that the European Union's Marine Strategy Framework Directive requires its Member States to achieve or maintain by the year 2020. \nGood Environmental Status is described by 11 Descriptors:\n"}
{"id": "3436651", "url": "https://en.wikipedia.org/wiki?curid=3436651", "title": "Gravel road", "text": "Gravel road\n\nA gravel road is a type of unpaved road surfaced with gravel that has been brought to the site from a quarry or stream bed. They are common in less-developed nations, and also in the rural areas of developed nations such as Canada and the United States. In New Zealand, and other Commonwealth countries, they may be known as 'metal roads'. They may be referred to as 'dirt roads' in common speech, but that term is used more for unimproved roads with no surface material added. If well constructed and maintained, a gravel road is an all-weather road.\n\nCompared to sealed roads, which require large machinery to work and pour concrete or to lay and smooth a bitumen-based surface, gravel roads are easy and cheap to build. However, compared to dirt roads, all-weather gravel highways are quite expensive to build, as they require front loaders, dump trucks, graders, and roadrollers to provide a base course of compacted earth or other material, sometimes macadamised, covered with one or more different layers of gravel. Graders are also used to produce a more extreme camber compared to a paved road to aid drainage, as well as to construct drainage ditches and embankments in low-lying areas. Cellular confinement systems can be used to prevent the washboarding effect.\n\nThe gravel used consists of varying amount of crushed stone, sand, and \"fines\". Fines are silt or clay particles smaller than , which can act as a binder. Crushed stone is used because gravel with fractured faces will stay in place better than rounded river pebbles. A good gravel for a gravel road will have a higher percentage of fines than gravel used as a subbase for a paved road. This often causes problems if a gravel road is paved without adding sand and gravel sized stone to dilute the percentage of fines.\n\nA gravel road is quite different from a 'gravel drive', popular as private driveways in the United Kingdom. This uses clean gravel consisting of uniform, rounded stones and small pebbles.\n\nIn Africa and parts of Asia and South America, laterite soils are used to build dirt roads. However laterite, called \"murram\" in East Africa, varies considerably in the proportion of stones (which are usually very small) to earth and sand. It ranges from a hard gravel to a softer earth embedded with small stones. Not all laterite and murram roads are therefore strictly gravel roads. Laterite and murram which contains a significant proportion of clay becomes very slippery when wet, and in the rainy season, it may be difficult even for four-wheel drive vehicles to avoid slipping off very roads into the drainage ditches at the side of the road. As it dries out, such laterite can become very hard, like sun-dried bricks.\n\nGravel roads require much more frequent maintenance than paved roads, especially after wet periods and when accommodating increased traffic. Wheel motion shoves material to the outside (as well as in-between travelled lanes), leading to rutting, reduced water-runoff, and eventual road destruction if unchecked. As long as the process is interrupted early enough, simple re-grading is sufficient, with material being pushed back into shape.\n\nSegments of gravel roads on grades also rut easily as a result of flowing water. When grading or building the road, waterbars are used to direct water off the road. As an alternative method, humps can be formed in the gravel along the road to impede water flow, thereby reducing rutting.\n\nAnother problem with gravel roads is washboarding — the formation of corrugations across the surface at right angles to the direction of travel. They can become severe enough to cause vibration in vehicles so that bolts loosen or cracks form in components. Grading removes the corrugations, and reconstruction with careful choice of good quality gravel can help prevent them reforming. Additionally, installing a cellular confinement system will prevent the washboard-like corrugations from occurring.\n\nGravel roads are often found in cold climates because they are less vulnerable to freeze / thaw damage than asphalt roads. The inferior surface of gravel is not an issue if the road is covered by snow and ice for extended periods.\n\nAlthough well-constructed and graded gravel roads are suitable for speeds of up to 100 km/h (60 mph), driving on them requires far more attention to variations of the surface and it is easier to lose control than on a paved road. In addition to potholes, ruts and loose stony or sandy ridges at the edges or in the middle of the road, problems associated with driving on gravel roads include:\n\nA 'Forest Service Road' is a type of rudimentary access road, built by the United States Forest Service to access remote undeveloped areas. These roads are built mainly for the purposes of the logging industry and forest management workers, although in some cases they are also used for backcountry recreation access.\n\nNetworks of tributary roads branch off from a trunk FSR. Roads are usually named after a regional district, and branches have an alphanumeric designation.\n\nTypically, a high-clearance four-wheel drive vehicle is required to travel effectively on a road, especially where large potholes and/or waterbars are present. Switchbacks are employed to make the road passable through steep terrain.\n\nThese roads rapidly fall into disrepair and quickly become impassable. Remnants of old roads can exist for decades. They are eventually erased by washout, erosion, and ecological succession.\n\nLogging roads are constructed to provide access to the forest for logging and other forest management operations. They are commonly narrow, winding, and unpaved, but main haul roads can be widened, straightened or paved if traffic volume warrants it.\n\nThe choice of road design standards is a trade off between construction costs and haul costs (which the road is designed to reduce). A road that serves only a few stands will be used by relatively few trucks over its lifetime, so it makes sense to save construction costs with a narrow, winding, unpaved road that adds to the time (and haul costs) of these few trips. A main haul road serving a large area however will be used by many trucks each day, and each trip will be shorter(saving time and money) if the road is straighter and wider, with a smoother surface.\n\nLogging trucks are generally given right of way. In areas where this practice is regulated (or is supposed to be) non-highway roads with heavy logging traffic may be \"radio-controlled\", which is to say a CB radio on board any vehicle on the road is advised for safety reasons.\n\n"}
{"id": "49597441", "url": "https://en.wikipedia.org/wiki?curid=49597441", "title": "Hatila Valley National Park", "text": "Hatila Valley National Park\n\nHatila Valley National Park () is a national park in Artvin Province in northeastern Turkey. It consists of a steep-sided river valley at the eastern end of the Kaçkar Mountains. The area is close to the Black Sea and has a Mediterranean climate with warm summers, cool winters and plentiful rainfall throughout the year. The valley provides habitats for a diverse community of plants and animals.\n\nThe Hatila River is a tributary of the Çoruh River and has carved out a narrow, steep-sided, V-shaped valley with many waterfalls. The rock is mostly of volcanic origin and has an unusual geomorphologic structure and geology, which has created a distinctive landscape. The park occupies an area of about . The lower parts of the valley are dry and warm but the higher parts are cool and humid with snow cover in winter. The valley is densely vegetated, especially in its middle and lower sections. Between 1994 and 1997, a botanical survey carried out identified 769 species of plant in 87 families and 324 genera here.\n\nThe forests on the mountain slopes in the park consist of both deciduous and evergreen tree species. These include sessile oak, sweet chestnut, oriental hornbeam, common hornbeam, black alder, oriental beech, oriental spruce, Caucasian fir and Scots pine. On the higher slopes, the common aspen is added to these as well as rhododendron, juniper, \"Vaccinium\", willow, birch and European raspberry.\n\nThe Hatila Valley has a very rich flora and a long botanical season, with different flowering periods for different zones depending on altitude. In spring in the lower parts of the valley, forest clearings are abloom with hellebores, magenta \"Cyclamen coum\", blue Cappadocian navelwort and pink \"Primula vulgaris\" subsp. \"sibthorpii\". Higher up the valley, and a few weeks later, \"Primula vulgaris\" flowers profusely and at least five species of snowdrop grow here including the recently described \"Galanthus koenenianus\". The hay meadows and forest glades at higher elevations are at their best around midsummer. Here bloom several species of cranesbill, including the Armenian cranesbill, globe flower, lousewort and a variety of terrestrial orchids. Higher still, the forest gives way to scrubland with buckthorn, birch and rhododendron including the yellow-flowered \"Rhododendron luteum\". Here there grow yellow lilies, Wittmann's paeony and the blue and white-flowered \"Aquilegia olympica\". Higher still there is moorland and alpine pastures with many species of bulbous plants including stars of Bethlehem, \"Scilla siberica\" and \"Scilla rosenii\". Among the short grasses grow primulas, including \"Primula auricula\" in damp spots, gentians, mountain pansies, bellflowers and betony. In the autumn there are several varieties of crocus, and also colchicum.\n\nThe natural forest of Hatila Valley National Park and the surrounding area is rich in wildlife. Large mammals found here include grey wolf, red fox, lynx, leopard, brown bear, wild goat, chamois, roe deer, wild boar and European hare. Many birds of prey pass through during their migrations, and golden eagle, long-legged buzzard, peregrine falcon, Caspian snowcock, Caucasian grouse, chukar partridge and grey partridge can be seen here.\n\nThe park is approached by a road from Artvin. Accommodation is available in bungalows, caravans or tents.\n"}
{"id": "73615", "url": "https://en.wikipedia.org/wiki?curid=73615", "title": "High Frequency Active Auroral Research Program", "text": "High Frequency Active Auroral Research Program\n\nThe High Frequency Active Auroral Research Program (HAARP) was initiated as an ionospheric research program jointly funded by the U.S. Air Force, the U.S. Navy, the University of Alaska Fairbanks, and the Defense Advanced Research Projects Agency (DARPA). It was designed and built by BAE Advanced Technologies (BAEAT). Its original purpose was to analyze the ionosphere and investigate the potential for developing ionospheric enhancement technology for radio communications and surveillance. As a university-owned facility, HAARP is a high-power, high-frequency transmitter used for study of the ionosphere.\n\nThe most prominent instrument at HAARP is the Ionospheric Research Instrument (IRI), a high-power radio frequency transmitter facility operating in the high frequency (HF) band. The IRI is used to temporarily excite a limited area of the ionosphere. Other instruments, such as a VHF and a UHF radar, a fluxgate magnetometer, a digisonde (an ionospheric sounding device), and an induction magnetometer, are used to study the physical processes that occur in the excited region.\n\nWork on the HAARP facility began in 1993. The current working IRI was completed in 2007; its prime contractor was BAE Systems Advanced Technologies. As of 2008, HAARP had incurred around $250 million in tax-funded construction and operating costs. In May 2014, it was announced that the HAARP program would be permanently shut down later in the year. After discussions between the parties, ownership of the facility and its equipment was transferred to the University of Alaska Fairbanks in August 2015.\n\nHAARP is a target of conspiracy theorists, who claim that it is capable of \"weaponizing\" weather. Commentators and scientists say that advocates of this theory are uninformed, as claims made fall well outside the abilities of the facility, if not the scope of natural science.\n\nHAARP (High Frequency Active Auroral Research Program) began in 1990. Ted Stevens, Republican U.S. senator from Alaska, helped win approval for the facility, and construction began in 1993.\n\nIn early May 2013, HAARP was temporarily shut down, awaiting a change between contractors to operate the facility. In July 2013, HAARP program manager James Keeney said, \"Defense Advanced Research Projects Agency (DARPA) is expected on site as a client to finish up some research in fall 2013 and winter 2014.\" The temporary shutdown was described as being due to \"a contractor regime change.\" Ahtna, Incorporated, the Alaska Native corporation serving the region of Alaska where the HAARP site is located, was reportedly in talks to take over the facility administration contract from Marsh Creek, LLC.\n\nIn May 2014, the Air Force announced that the HAARP program would be shut down later in 2014. While experiments ended in the summer of 2014, the complete shutdown and dismantling of the facility was postponed until at least May 2015. In mid-August 2015 control of the facility and its equipment was turned over to the University of Alaska Fairbanks, which is making the facilities available for researchers on a pay-per-use basis.\n\nThe HAARP project directs a 3.6 MW signal, in the 2.8–10 MHz region of the HF (high-frequency) band, into the ionosphere. The signal may be pulsed or continuous. Effects of the transmission and any recovery period can be examined using associated instrumentation, including VHF and UHF radars, HF receivers, and optical cameras. According to the HAARP team, this will advance the study of basic natural processes that occur in the ionosphere under the natural but much stronger influence of solar interaction. HAARP also enables studies of how the natural ionosphere affects radio signals.\n\nThe insights gleaned at HAARP will enable scientists to develop methods to mitigate these effects to improve the reliability or performance of communication and navigation systems which would have a wide range of both civilian and military uses, such as an increased accuracy of GPS navigation and advances in underwater and underground research and applications. This may lead, among other things, to improved methods for submarine communication or an ability to remotely sense and map the mineral content of the terrestrial subsurface, and perhaps underground complexes, of regions or countries. The current facility lacks range to be used in regions like the oil-rich Middle East, according to one of the researchers involved, but the technology could be put on a mobile platform.\n\nThe project was originally funded by the Office of Naval Research and jointly managed by the ONR and Air Force Research Laboratory, with principal involvement of the University of Alaska Fairbanks. Many other US universities and educational institutions were involved in the development of the project and its instruments, namely the University of Alaska Fairbanks, Stanford University, Penn State University (ARL), Boston College, UCLA, Clemson University, Dartmouth College, Cornell University, Johns Hopkins University, University of Maryland, College Park, University of Massachusetts Amherst, MIT, Polytechnic Institute of New York University, Virginia Tech and the University of Tulsa. The project's specifications were developed by the universities, who continued to play a major role in the design of future research efforts.\n\nAccording to HAARP's original management, the project strove for openness, and all activities were logged and publicly available, a practice which continues under the University of Alaska Fairbanks. Scientists without security clearances, even foreign nationals, were routinely allowed on site, which also continues today. HAARP hosts an open house annually, during which time any civilian can tour the entire facility. In addition, scientific results obtained using HAARP are routinely published in major research journals (such as \"Geophysical Research Letters\" and \"Journal of Geophysical Research\"), written both by university scientists (American and foreign) and by U.S. Department of Defense research lab scientists.\n\nHAARP's main goal is basic science research in the uppermost portion of the atmosphere, termed the ionosphere. Essentially a transition between the atmosphere and the magnetosphere, the ionosphere is where the atmosphere is thin enough that the sun's X-rays and UV rays can reach it, but thick enough that there are enough molecules present to absorb those rays. Consequently, the ionosphere consists of a rapid increase in density of free electrons, beginning at 70 km, reaching a peak at ~300 km, and then falling off again as the atmosphere disappears entirely by ~1,000 km. Various aspects of HAARP can study all of the main layers of the ionosphere.\n\nThe profile of the ionosphere is highly variable, changing constantly on timescales of minutes, hours, days, seasons, and years. This profile becomes even more complex near Earth's magnetic poles, where the nearly vertical alignment and intensity of earth's magnetic field can cause physical effects like the aurora.\n\nThe ionosphere is traditionally very difficult to measure. Balloons cannot reach it because the air is too thin, but satellites cannot orbit there because the air is too thick. Hence, most experiments on the ionosphere give only small pieces of information. HAARP approaches the study of the ionosphere by following in the footsteps of an ionospheric heater called EISCAT near Tromsø, Norway. There, scientists pioneered exploration of the ionosphere by perturbing it with radio waves in the 2–10 MHz range, and studying how the ionosphere reacts. HAARP performs the same functions but with more power and a more flexible and agile HF beam.\n\nSome of the main scientific findings from HAARP include:\n\nResearch at the HAARP has included:\n\n\nResearch conducted at the HAARP facility has allowed the US military to perfect communications with its fleet of submarines by sending radio signals over long distances.\n\nThe main instrument at HAARP is the Ionospheric Research Instrument (IRI). This is a high-power, high-frequency phased array radio transmitter with a set of 180 antennas, disposed in an array of 12x15 units that occupy a rectangle of about . The IRI is used to temporarily energize a small portion of the ionosphere. The study of these disturbed volumes yields important information for understanding natural ionospheric processes.\n\nDuring active ionospheric research, the signal generated by the transmitter system is delivered to the antenna array and transmitted in an upward direction. At an altitude between (depending on operating frequency), the signal is partially absorbed in a small volume several tens of kilometers in diameter and a few meters thick over the IRI. The intensity of the HF signal in the ionosphere is less than 3 µW/cm², tens of thousands of times less than the Sun's natural electromagnetic radiation reaching the earth and hundreds of times less than even the normal random variations in intensity of the Sun's natural ultraviolet (UV) energy which creates the ionosphere. The small effects that are produced, however, can be observed with the sensitive scientific instruments installed at the HAARP facility, and these observations can provide information about the dynamics of plasmas and insight into the processes of solar-terrestrial interactions.\n\nEach antenna element consists of a crossed dipole that can be polarized for linear, ordinary mode (O-mode), or extraordinary mode (X-mode) transmission and reception. Each part of the two section crossed dipoles are individually fed from a specially designed, custom-built transmitter that operates at very low distortion levels. The Effective Radiated Power (ERP) of the IRI is limited by more than a factor of 10 at its lower operating frequencies. Much of this is due to higher antenna losses and a less efficient antenna pattern.\n\nThe IRI can transmit between 2.7 and 10 MHz, a frequency range that lies above the AM radio broadcast band and well below Citizens' Band frequency allocations. However, HAARP is licensed to transmit only in certain segments of this frequency range. When the IRI is transmitting, the bandwidth of the transmitted signal is 100 kHz or less. The IRI can transmit in continuous waves (CW) or in pulses as short as 10 microseconds (µs). CW transmission is generally used for ionospheric modification, while transmission in short pulses frequently repeated is used as a radar system. Researchers can run experiments that use both modes of transmission, first modifying the ionosphere for a predetermined amount of time, then measuring the decay of modification effects with pulsed transmissions.\n\nThere are other geophysical instruments for research located at the HAARP facility. Some of them are:\nThe facility is powered by a set of five (5) 2500 kilowatt generators being driven by EMD 20-645-E4 diesel locomotive engines.\n\nThe project site () is north of Gakona, Alaska just west of Wrangell-Saint Elias National Park. An environmental impact statement led to permission for an array of up to 180 antennas to be erected. HAARP was constructed at the previous site of an over-the-horizon radar (OTH) installation. A large structure, built to house the OTH now houses the HAARP control room, kitchen and offices. Several other small structures house various instruments.\n\nThe HAARP site was constructed in three distinct phases:\n\nIn America, there are two related ionospheric heating facilities: the HIPAS, near Fairbanks, Alaska, which was dismantled in 2009, and one at the Arecibo Observatory in Puerto Rico (currently offline for reconstruction). The European Incoherent Scatter Scientific Association (EISCAT) operates an ionospheric heating facility capable of transmitting over 1 GW effective radiated power (ERP), near Tromsø, Norway. The Sura Ionospheric Heating Facility, in Vasilsursk, Russia, near Nizhniy Novgorod, is capable of transmitting 190 MW ERP.\n\nHAARP is the subject of numerous conspiracy theories. Various individuals have speculated about hidden motivations and capabilities of the project. For example, Rosalie Bertell warned in 1996 about the deployment of HAARP as a military weapon. Michel Chossudovsky stated in a book published by the Committee on Monetary and Economic Reform that \"recent scientific evidence suggests that HAARP is fully operational and has the capability of triggering floods, hurricanes, droughts and earthquakes.\" Over time, HAARP has been blamed for generating such catastrophes, as well as thunderstorms, in Iran, Pakistan, Haiti, Turkey, Greece and the Philippines, and even major power outages, the downing of TWA Flight 800, Gulf War syndrome, and chronic fatigue syndrome.\n\nAllegations include the following:\n\nTwo Georgia men arrested on drug charges in November 2016 were reportedly plotting domestic terrorism based on conspiracy theories about HAARP. The Coffee County Sheriff's Office said the men possessed a \"massive arsenal\" that included AR-15 rifles, Glock handguns, a Remington rifle and thousands of rounds of ammunition. According to police, the men wanted to destroy HAARP because they believed the facility manipulates the weather, controls minds and even traps the souls of people. Police say the men confessed that \"God told them to go and blow this machine up that kept souls, so souls could be released.\"\n\nStanford University professor Umran Inan told \"Popular Science\" that weather-control conspiracy theories were \"completely uninformed,\" explaining that \"there's absolutely nothing we can do to disturb the Earth's [weather] systems. Even though the power HAARP radiates is very large, it's minuscule compared with the power of a lightning flash—and there are 50 to 100 lightning flashes every second. HAARP's intensity is very small.\" Computer scientist David Naiditch characterizes HAARP as \"a magnet for conspiracy theorists,\" saying that HAARP attracts their attention because, \"its purpose seems deeply mysterious to the scientifically uninformed.\" Journalist Sharon Weinberger called HAARP \"the Moby Dick of conspiracy theories,\" and said the popularity of conspiracy theories often overshadows the benefits HAARP may provide to the scientific community. Austin Baird writing in the Alaska Dispatch said, \"What makes HAARP susceptible to conspiracy criticism is simple. The facility doesn't open its doors in the same way as other federally-funded research facilities around the country, and it doesn't go to great efforts to explain the importance of its research to the public.\" In 2016, in response to these claims, the University of Alaska Fairbanks Geophysical Institute, which manages the facility, announced that HAARP will host an annual open house in August, allowing visitors to tour the complex.\n\n\n\n"}
{"id": "24034549", "url": "https://en.wikipedia.org/wiki?curid=24034549", "title": "Hornito", "text": "Hornito\n\nHornitos are conical structures built up by lava ejected through an opening in the crust of a lava flow. Hornitos are similar to spatter cones but are rootless, meaning they were once a source of lava but that source was not directly associated with a true vent or magma source. They are usually created by the slow upwelling of lava through the roof of a lava tube. High pressure causes lava to ooze and spatter out. The lava builds up on the surface and solidifies creating the initial structure. Hornitos can grow and exceed 10 meters in height.\n\n"}
{"id": "2768804", "url": "https://en.wikipedia.org/wiki?curid=2768804", "title": "Kingwood (wood)", "text": "Kingwood (wood)\n\nKingwood is a classic furniture wood, almost exclusively used for inlays on very fine furniture. It was the most expensive wood in general use for furniture making in the seventeenth century, at which time it was known as princes wood.\n\nOccasionally it is used in the solid for small items and turned work, including parts of billiard cues, e.g., those made by John Parris. It is brownish-purple with many fine darker stripes and occasional irregular swirls. Occasionally it contains pale streaks of a similar colour to the sapwood, as in the picture.\n\nThe wood is very dense and hard and can be brought to a spectacular finish. It turns well but due to its density and hardness it can be difficult to work with hand tools. It also has a tendency to blunt tools due to its abrasive properties.\n\nIt is available only in small sizes (it is yielded by a smallish tree, \"Dalbergia cearensis\", restricted to a small area in Brazil). Other woods yielded by the same genus are cocobolo, rosewood, African blackwood, Bombay blackwood and tulipwood.\n"}
{"id": "52810222", "url": "https://en.wikipedia.org/wiki?curid=52810222", "title": "L Velorum", "text": "L Velorum\n\nThe Bayer designations l Velorum and L Velorum are distinct. Due to , both designations link here. For the star\n\n"}
{"id": "6814434", "url": "https://en.wikipedia.org/wiki?curid=6814434", "title": "Largest creative work", "text": "Largest creative work\n\nThe largest creative work is the largest or longest item in different fields of creative works. Some pieces were created with the specific intention of holding the record while others have been recognised for their size after completion.\n\n\n\n\n\n\"The Mousetrap\" has been running continuously in London since 1952. It has by far the longest initial run of any play in history, with more than 25,000 performances taking place, and the longest running show (of any type) of the modern era.\n\n\n\nThe world's largest hand woven carpet is a Persian carpet made in Iran and completed in 2007. It has 2.2 billion knots, measuring .\n\n"}
{"id": "2839774", "url": "https://en.wikipedia.org/wiki?curid=2839774", "title": "Liqiu", "text": "Liqiu\n\nThe traditional East Asian calendars divide a year into 24 solar terms. Lìqiū, \"Risshū\", \"Ipchu\", or \"Lập thu\" () is the 13th solar term. It begins when the Sun reaches the celestial longitude of 135° and ends when it reaches the longitude of 150°. It more often refers in particular to the day when the Sun is exactly at the celestial longitude of 135°. In the Gregorian calendar, it usually begins around August 7 and ends around August 23.\n\nLiqiu signifies the beginning of autumn in East Asian cultures.\n"}
{"id": "8137703", "url": "https://en.wikipedia.org/wiki?curid=8137703", "title": "List of Florida hurricanes (1900–1949)", "text": "List of Florida hurricanes (1900–1949)\n\nThe list of Florida hurricanes from 1900 to 1949 encompasses 108 Atlantic tropical cyclones that affected the U.S. state of Florida. Collectively, tropical cyclones in Florida during the time period resulted in about $4 billion (2008 USD) in damage. Additionally, tropical cyclones in Florida were directly responsible for about 3,550 fatalities during the time period, most of which from the 1928 Okeechobee Hurricane. The 1947 season was the year with the most tropical cyclones affecting the state, with a total of 6 systems. The 1905, 1908, 1913, 1927, 1931, 1942, and 1943 seasons were the only years during the time period in which a storm did not affect the Floridian coasts.\n\nThe strongest hurricane to hit the state during the time period was the Labor Day Hurricane of 1935, which also bears the distinction of being the strongest recorded hurricane to strike the United States. Several other major hurricanes struck the state during the time period, including the 1926 Miami Hurricane, the 1928 Okeechobee Hurricane, and a cyclone each in 1945 and 1949. All of these storms made landfall as Category 4 hurricanes.\n\n\n\n\n\n\n\n\n\n\nThe following is a list of hurricanes with known deaths in the state.\n"}
{"id": "5903051", "url": "https://en.wikipedia.org/wiki?curid=5903051", "title": "List of Sites of Special Scientific Interest in Staffordshire", "text": "List of Sites of Special Scientific Interest in Staffordshire\n\nThis is a list of the Sites of Special Scientific Interest (SSSIs) in Staffordshire, England. For other counties, see List of SSSIs by Area of Search.\n\n"}
{"id": "44182019", "url": "https://en.wikipedia.org/wiki?curid=44182019", "title": "List of ecoregions in Cambodia", "text": "List of ecoregions in Cambodia\n\nThe following is a list of ecoregions in Cambodia.\nTropical and subtropical moist broadleaf forests:\nTropical and subtropical dry broadleaf forests\n\nFreshwater ecoregions:\n\n"}
{"id": "17372238", "url": "https://en.wikipedia.org/wiki?curid=17372238", "title": "List of environmental periodicals", "text": "List of environmental periodicals\n\nThis is a list of environmental periodicals, in print and online, focused on various aspects of the biophysical environment, the built environment, humans' relations to those environments, and other environment topics. This list presently includes literary magazines, general-interest magazines, newsletters, and others.\n\n\n\n\n\n\n\n"}
{"id": "376874", "url": "https://en.wikipedia.org/wiki?curid=376874", "title": "List of mountains in Iran", "text": "List of mountains in Iran\n\nThis is a list of mountains in the country of Iran.\n\n\"By clicking on the symbols at the head of the table the individual columns may be sorted.\"\n\n"}
{"id": "11683553", "url": "https://en.wikipedia.org/wiki?curid=11683553", "title": "List of mountains in Peru", "text": "List of mountains in Peru\n\nThis is a list of the thirty-seven 6000 metre peaks in Peru as defined by a regain height, or prominence, above a col of 300m or more. This list is taken from the full set of Peruvian IGM maps alongside various climbing and mountaineering records. Heights are taken from the Peruvian IGM 1:100,000 series maps with the OEAV survey maps of the Cordillera Blanca (north and south) used where the IGM maps do not give spot heights. SRTM data has been used in a few places to confirm these heights, but due to the steep terrain is often unusable\n\nMany peaks in Peru frequently quoted as being over 6000m are under this height according to the most recent surveys published by the Peruvian IGM. These peaks include:- Pumasillo 5,991m, Lasunayoc 5,936m, Yanarahu 5,954m, Artesonraju 5,999m, Sabancaya 5,976m, Palumani 5,723m, Sara Sara 5,505m, Helancoma 5,367m.\n\nOther 6,000 m peaks which are often defined as individual peaks but which have less than 300 m of re-ascent or prominence, include:- Huandoy W 6,342 m (prominence between 200-250m), Sarapu 6,127 (prominence between 180-230m), Callangate North 6,000 m (less than 295m prominence).\n\nQaras E (6025m) and Rasac (6,017 m) may or may not have 300m prominence. There is insufficient data on the relevant Peruvian IGM maps.\n\nPeru is home to a number of mountain ranges, including the following:\n\n\n\n\nThe lists can be contradictory but are all useful. They use different criteria of prominence or re-ascent for defining major peaks and sub-peaks.\n"}
{"id": "16234974", "url": "https://en.wikipedia.org/wiki?curid=16234974", "title": "List of the highest major summits of the United States", "text": "List of the highest major summits of the United States\n\nThe following sortable table comprises the 200 highest mountain peaks of the United States with at least of topographic prominence.\n\nThe summit of a mountain or hill may be measured in three principal ways:\n\nIn the United States, only Denali exceeds elevation. Four major summits exceed , nine exceed , 104 exceed, 220 exceed , and 302 major summits exceed 3000 meters' (9843 feet) elevation.\n\nOf these 200 highest major summits of the United States, 88 are located in Colorado, 49 in Alaska, 22 in California, 14 in Wyoming, eight in New Mexico, five in Utah, four in Nevada, three in Montana, two in Washington, two in Hawaii, two in Idaho, and one in Arizona. Five of these peaks lie on the international border between Alaska and Yukon, and two lie on the international border between Alaska and British Columbia. The ten highest major summits of the United States are all located in Alaska.\n\n\n"}
{"id": "3817372", "url": "https://en.wikipedia.org/wiki?curid=3817372", "title": "Lists of animals", "text": "Lists of animals\n\nAnimals are multicellular eukaryotic organisms that form the biological kingdom Animalia. With few exceptions, animals consume organic material, breathe oxygen, are able to move, reproduce sexually, and grow from a hollow sphere of cells, the blastula, during embryonic development. Over 1.5 million living animal species have been described—of which around 1 million are insects—but it has been estimated there are over 7 million in total. Animals range in size from 8.5 millionths of a metre to long and have complex interactions with each other and their environments, forming intricate food webs. The study of animals is called zoology.\n\nAnimals may be listed or indexed by many criteria, including taxonomy, status as endangered species, their geographical location, and their portrayal and/or naming in human culture.\n\n\n\n\n\nList of extinct animals\n\n\nList of fictional bears\n\nThe animal Kingdom contains some 35 extant phyla.\n\nBasal animals are according to the following cladogram:\n\nAnimals: Porifera, Diploblasts\n\nDiploblasts: Ctenophora, ParaHoxozoa\n\nParaHoxozoa: Placozoa, Cnidaria, Bilateria/Triploblast\n\nBilateria: Xenacoelomorpha, Nephrozoa\n\nNephrozoa: Protostomes, Deuterostomes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "80384", "url": "https://en.wikipedia.org/wiki?curid=80384", "title": "Lotis (mythology)", "text": "Lotis (mythology)\n\nLotis was a nymph mentioned by Ovid. In his account, at the Liberalia festival, Priapus tried to rape her when everyone had fallen asleep, but she was awakened by a sudden cry of Silenus's donkey and ran off, leaving Priapus in embarrassment as everyone else woke up too and became aware of his intentions. In another account, she was changed into a lotus tree to escape Priapus; later, Dryope picked a flower off the tree Lotis had become, and was transformed into a black poplar. \n\nIn Book 6 of the \"Fasti\" Ovid tells much the same story, but with the goddess Vesta rather than Lotis as the intended victim. According to some sources, Lotis was the daughter of Neptune or Nereus. Ovid suggests that Priapus later kills the donkey.\n\nThe story does not seem to feature in Ancient Greek vase-painting, and only occasionally in later weird pictures. Priapus and Lotis appear in the right foreground of \"The Feast of the Gods\" by Giovanni Bellini (c. 1514), in an engraving by Giovanni Battista Palumba (c. 1510), and a drawing by Parmigianino of the 1530s. Bellini keeps Priapus's aroused state visible under his clothes, Palumba has it out in the open, as Parmigianino originally did, but this has been altered subsequently, as very explicit details often were in art. There are also some depictions of Lotis as a tree-hugger.\n\n"}
{"id": "18974", "url": "https://en.wikipedia.org/wiki?curid=18974", "title": "Mississippian (geology)", "text": "Mississippian (geology)\n\nThe Mississippian (also known as Lower Carboniferous or Early Carboniferous) is a subperiod in the geologic timescale or a subsystem of the geologic record. It is the earliest/lowermost of two subperiods of the Carboniferous period lasting from roughly 358.9 to 323.2 million years ago. As with most other geochronologic units, the rock beds that define the Mississippian are well identified, but the exact start and end dates are uncertain by a few million years. The Mississippian is so named because rocks with this age are exposed in the Mississippi River valley. \n\nThe Mississippian was a period of marine transgression in the Northern Hemisphere: the sea level was so high that only the Fennoscandian Shield and the Laurentian Shield were dry land. The cratons were surrounded by extensive delta systems and lagoons, and carbonate sedimentation on the surrounding continental platforms, covered by shallow seas.\n\nIn North America, where the interval consists primarily of marine limestones, it is treated as a geologic period between the Devonian and the Pennsylvanian. During the Mississippian an important phase of orogeny occurred in the Appalachian Mountains. It is a major rock-building period named for the exposures in the Mississippi Valley region. The USGS geologic time scale shows its relation to other periods.\n\nIn Europe, the Mississippian and Pennsylvanian are one more-or-less continuous sequence of lowland continental deposits and are grouped together as the Carboniferous system, and sometimes called the \"Upper Carboniferous\" and \"Lower Carboniferous\" instead.\n\nIn the official geologic timescale, the Mississippian is subdivided into three stages:\n\nThe lower two come from European stratigraphy, the top from Russian stratigraphy. Besides Europe and Russia, there are many local subdivisions that are used as alternatives for the international timescale. In the North American system, the Mississippian is subdivided into four stages:\n\n"}
{"id": "1272552", "url": "https://en.wikipedia.org/wiki?curid=1272552", "title": "Mount Osore", "text": "Mount Osore\n\nThe mountain is one peak of the , a series of eight somma volcanos ranging from east to west in the centre of Shimokita Peninsula. Mount Osore has a height of . Although Mount Osore last erupted over 10,000 years ago, the area has many fumaroles emitting steam and/or volcanic gases (especially sulfur dioxide) indicating that it is still an active volcano. Lake Usori is a caldera lake at the center of Mount Osore, with highly acidic waters.\n\nDuring the Meiji period, sulphur deposits in the area were exploited, partly to meet the demand for gunpowder production by the growing Japanese military; however the deposits were not economical to mine due to the remoteness of the site and the increasing availability of sulphur as a byproduct of petroleum refining.\n\nThe mountain is the location of a Sōtō Zen Buddhist temple, , which claims to have been founded in 862 AD by the famed monk Ennin, with Jizo Bosatsu as its main image.The temple was abandoned in 1457 and restored back to use in 1530. In popular folk religion, the otherworldly setting of Mount Osore, with its charred landscape of blasted rocks filled with bubbling pits noted for unearthly hues and noxious fumes came to be one of several places in Japan identified to be an entrance to Hell. A small brook running to the neighboring Lake Usori was equated to the Sanzu River, a river that deceased souls need to cross of their way to the afterlife.\n\nA unique feature of Bodai-ji is the presence of mediums known as \"itako\" who claim to summon the souls of the dead and deliver messages in their voices. These mediums were traditionally blind and had to receive extensive spiritual training and purification rituals; however, in modern times their number has dwindled and not all are blind. The temple has a twice-yearly \"Itako Taisai\" festival held in summer and autumn.\n\nThe temple also maintains a hot spring resort for use by pilgrims and tourists.\n\nThe work of contemporary artist Nara Yoshitomo, who is a native of Aomori Prefecture, is believed to be influenced, if at least subconsciously, by Mount Osore (Ivy, 2010). For instance, his piece entitled “Not Everything But/ Green House” depicts a small female child standing over a pile of discarded dolls of varying characteristics and eras much like those observed at Mount Osore.\n\nAs part of a collection of \"fictional fungi\", artist Takeshi Yamada created the Oh-dokuro-dake (or “skull mushroom”) and the story surrounding their presence on Mount Osore for his \"center for medical mycology\" art project.\n\n\n"}
{"id": "12679495", "url": "https://en.wikipedia.org/wiki?curid=12679495", "title": "Mountain peaks of Canada", "text": "Mountain peaks of Canada\n\nThis article comprises three sortable tables of major mountain peaks of Canada.\n\nThe summit of a mountain or hill may be measured in three principal ways:\n\nOf the 100 highest major summits of Canada, five peaks exceed elevation, 19 peaks exceed , 67 peaks exceed , and all 100 peaks equal or exceed elevation.\n\nOf these 100 peaks, 61 are located in British Columbia, 28 in Yukon, 13 in Alberta, and one in the Northwest Territories. Five of these peaks lie on the international border between Yukon and Alaska, four lie on the international border between British Columbia and Alaska, three lie on the border between British Columbia and Alberta, and one lies on the border between British Columbia and Yukon.\n\nOf the 50 most prominent summits of Canada, only Mount Logan exceeds of topographic prominence, five peaks exceed , 41 peaks exceed , and all 50 peaks equal or exceed of topographic prominence. All of these peaks are ultra-prominent summits.\n\nOf these 50 peaks, 34 are located in British Columbia, nine in Yukon, six in Nunavut, and three in Alberta. Three of these peaks lie on the international border between Yukon and Alaska, one lies on the international border between British Columbia and Alaska, two lie on the border between British Columbia and Alberta, and two lie on the border between British Columbia and Yukon.\n\nOf the 50 most isolated major summits of Canada, 12 peaks exceed of topographic isolation, 31 peaks exceed , and all 50 peaks exceed of topographic isolation.\n\nOf these 50 peaks, 17 are located in British Columbia, 13 in Nunavut, seven in Yukon, four in Newfoundland and Labrador, four in Quebec, three in the Northwest Territories, two in Alberta, and one each in Nova Scotia and New Brunswick. Two of these peaks lie on the international border between British Columbia and Alaska, and two lie on the border between British Columbia and Alberta.\n\n\n"}
{"id": "764388", "url": "https://en.wikipedia.org/wiki?curid=764388", "title": "National parks of the United Kingdom", "text": "National parks of the United Kingdom\n\nNational parks are a devolved matter with each of the countries of the United Kingdom having its own policies and arrangements. Counted together, the United Kingdom has fifteen national parks with ten in England, three in Wales and two in Scotland. These parks are not truly national parks according to the internationally accepted standard of the IUCN but they are areas of outstanding landscape where habitation and commercial activities are restricted. \n\nThere are currently no national parks in Northern Ireland though there are controversial moves to establish one in the Mourne Mountains. If established, it would stretch from Carlingford Lough to Newcastle and Slieve Croob. Though it might create jobs in tourism, there are fears that it would drive up the area's cost of living.\n\nAll fifteen national parks share two statutory purposes; To conserve and enhance the natural and cultural heritage of the area, and to promote understanding and enjoyment of the special qualities of the national park by the public. The Scottish national parks have two further statutory purposes; To promote sustainable use of the natural resources of the area, and to promote sustainable economic and social development of the area’s communities. \n\nThe Broads was not established as a national park, but was described at the time as having a 'status equivalent to that of a national park'. It has since adopted the title 'national park' and is a member of the UK national parks family, with the same level of landscape protection and an additional statutory purpose; to protect the interests of navigation. \n\nAll fifteen national parks in the UK are members of the Association of National Park Authorities (ANPA), which works to promote the UK national parks family and to facilitate training and development between staff and members of all parks. \n\nBeginning in 2014 there was a movement to establish the Greater London area as a kind of national park.\n\nFor details of the established national parks, see:\n\n\n"}
{"id": "4131857", "url": "https://en.wikipedia.org/wiki?curid=4131857", "title": "Nuclear power in France", "text": "Nuclear power in France\n\nNuclear power is a major source of energy in France, with a 40% share of energy consumption in 2015. \nNuclear power is the largest source of electricity in the country, with a generation of 379.1 TWh, or 71.6% of the country's total production of 519.4 TWh, the highest percentage in the world.\n\nÉlectricité de France (EDF)the country's main electricity generation and distribution company – manages the country's 58 power reactors. \nEDF is substantially owned by the French Government, with around 85% shares in government hands.\n\nFrance exported 38 TWh of electricity to its neighbours. \nWith very inclement weather, when demand exceeds supply, France infrequently becomes a net-importer of electricity in these rare cases, because of the lack of more flexible generating plants.\n\nFrance has a long relationship with nuclear power, starting with Henri Becquerel's discovery of natural radioactivity in the 1890s and continued by famous nuclear scientists like Pierre and Marie Curie.\n\nBefore World War II, France had been mainly involved in nuclear research through the work of the Joliot-Curies. In 1945 the Provisional Government of the French Republic (GPRF) created the \"Commissariat à l'Énergie Atomique\" (CEA) governmental agency, and Nobel prize winner Frédéric Joliot-Curie, member of the French Communist Party (PCF) since 1942, was appointed high-commissioner. He was relieved of his duties in 1950 for political reasons, and would be one of the 11 signatories to the Russell-Einstein Manifesto in 1955. The CEA was created by Charles de Gaulle on 18 October 1945. Its mandate is to conduct fundamental and applied research into many areas, including the design of nuclear reactors, the manufacturing of integrated circuits, the use of radionuclides for medical treatments, seismology and tsunami propagation, and the safety of computerized systems.\n\nNuclear research was discontinued for a time after the war because of the instability of the Fourth Republic and the lack of finances available. However, in the 1950s a civil nuclear research program was started, a by-product of which would be plutonium. In 1956 a secret Committee for the Military Applications of Atomic Energy was formed and a development program for delivery vehicles started. In 1957, soon after the Suez Crisis and the diplomatic tension with both the USSR and the United States, French president René Coty decided the creation of the C.S.E.M. in what was then French Sahara, a new nuclear testing facility replacing the CIEES testing facility. See France and nuclear weapons.\n\nThe first nuclear power plant by EDF in France was opened in 1962.\n\nAs a direct result of the 1973 oil crisis, on 6 March 1974 Prime Minister Pierre Messmer unexpectedly announced what became known as the 'Messmer Plan', a huge nuclear power program aimed at generating all of France's electricity from nuclear power. At the time of the oil crisis most of France's electricity came from foreign oil. Nuclear power allowed France to compensate for its lack of indigenous energy resources by applying its strengths in heavy engineering. The situation was summarized in a slogan: \"In France, we do not have oil, but we have ideas.\"\n\nThe announcement of the Messmer Plan, which was enacted without public or parliamentary debate, also led to the foundation of the \"Groupement des scientifiques pour l'information sur l'énergie nucléaire\" (Association of Scientists for Information on Nuclear Energy), formed after around 4,000 scientists signed a petition of concern over the government's action, known as the \"Appeal of the 400\" after the 400 scientists who initially signed it.\n\nThe plan envisaged the construction of around 80 nuclear plants by 1985 and a total of 170 plants by 2000. Work on the first three plants, at Tricastin, Gravelines, and Dampierre started the same year and France installed 56 reactors over the next 15 years.\n\nFollowing the 2011 Fukushima I nuclear accidents, the head of France's nuclear safety agency has said that France needs to upgrade the protection of vital functions in all its nuclear reactors to avoid a disaster in the event of a natural calamity, adding there was no need to close any plants. \"There is a need to add a layer to protect safety mechanisms in reactors that are vital for the protection of the reactor such as cooling functions and electric powering,\" Jacques Repussard, head of the IRSN, said. Opinion polls show support for atomic energy has dropped since Fukushima. Forty percent of the French \"are 'hesitant' about nuclear energy while a third are in favor and 17 percent are against, according to a survey by pollster Ifop published November 13\".\n\nIn February 2012, President Sarkozy decided to extend the life of existing nuclear reactors beyond 40 years, following the Court of Audit decision that this is the best option as new nuclear capacity or other forms of energy would be more costly and available too late. Within ten years 22 out of the 58 reactors will have been operating for over 40 years.\nThe court expects EDF's projected investment programme in existing plant, including post Fukushima safety improvements, will add between 9.5% and 14.5% to generation costs, taking costs to between 37.9 and 54.2 EUR/MWh. Generation costs from the new Flamanville EPR reactor are estimated to be at least in the 70 to 90 EUR/MWh range, depending on construction outcome.\nAcademics at Paris Dauphine University forecast that domestic electricity prices will rise by about 30% by 2020.\n\nFollowing François Hollande's victory in the 2012 presidential election, it was thought that there might be a partial nuclear phaseout in France. This followed a national debate in the run-up to the election, with President Nicolas Sarkozy backing nuclear power and François Hollande proposing a cut in nuclear power's electricity contribution by more than a third by 2025.\nIt seemed certain that Hollande would at least order the closure of the Fessenheim Nuclear Power Plant by 2017 \nwhere there has been an ongoing closure campaign due to concerns about seismic activity and flooding.\n\nActive efforts by the French government to market the advanced European Pressurized Reactor have been hampered by cost overruns, delays, and competition from other nations, such as South Korea, which offer simpler, cheaper reactors.\n\nIn 2015, the National Assembly voted that by 2025 only 50% of France's energy will be produced by nuclear plants. Environment Minister Nicolas Hulot noted in November 2017 that this goal is unrealistic, postponing the reduction to 2030 or 2035. \n\nIn 2016, following a discovery at Flamanville Nuclear Power Plant, about 400 large steel forgings manufactured by Le Creusot Forge since 1965 have been found to have carbon-content irregularities that weakened the steel. A widespread programme of reactor checks was started involving a progressive programme of reactor shutdowns, likely to continue over the winter high electricity demand period into 2017. This caused power price increases in Europe as France increased electricity imports, especially from Germany, to augment supply. As of late October 2016, 20 of France's 58 reactors are offline. These steel quality concerns may prevent the regulator giving the life extensions from 40 to 50 years, that had been assumed by energy planners, for many reactors. In December 2016 the \"Wall Street Journal\" characterised the problem as a \"decades long coverup of manufacturing problems\", with Areva executives acknowledging that Le Creusot had been falsifying documents.\n\nIn November 2018, President Macron announced the 50% nuclear power reduction target is being delayed to 2035, and would involve closing fourteen 900 MWe reactors. The two oldest reactors, units 1 and 2 at Fessenheim, will close in 2020. A decision on any new nuclear build will be taken in 2021.\n\nÉlectricité de France (EDF)the country's main electricity generation and distribution companymanages the country's nuclear power plants. EDF is substantially owned by the French government, with around 85% of EDF shares in government hands . 78.9% of Areva shares are owned by the French public sector company CEA and are therefore in public ownership. EDF remains heavily in debt. Its profitability suffered during the recession which began in 2008. It made €3.9 billion in 2009, which fell to €1.02 billion in 2010, with provisions set aside amounting to €2.9 billion. The Nuclear industry has been accused of significant cost overruns and failing to cover the total costs of operation, including waste management and decommissioning.\n\nIn 2001, nuclear construction and services company Areva was created by the merger of CEA Industrie, Framatome and Cogema (now Areva NC). Its main shareholder is the French owned company CEA, but the German federal government also holds, through Siemens, 34% of the shares of Areva's subsidiary, Areva NP, in charge of building the EPR (third-generation nuclear reactor).\n\nAs of 2015, France's electricity price, excluding taxation, to household customers is the 12th cheapest amongst the 28 member European Union and the second-cheapest to industrial consumers.\n\nEdF said its third-generation nuclear reactor EPR project at its Flamanville, northern France, plant will be delayed until 2016, due to \"both structural and economic reasons,\" which will bring the project's total cost to EUR8.5 billion. Similarly, the cost of the EPR to be built at Olkiluoto, Finland has escalated. Areva and the utility involved \"are in bitter dispute over who will bear the cost overruns and there is a real risk now that the utility will default. EDF has suggested that if the political environment causes the EPR costs to overrun, the design would be replaced with a cheaper and simpler Franco-Japanese design, the Atmea for which the design will be completed by 2013, or the already operating Franco-Chinese design, the CPR-1000. In July 2018, EDF further delayed fuel loading to Q4 2019 and increased the project's cost estimate by a further €400 million ($467.1 million USD). Startup is now scheduled to occur no earlier than Q2 2020 and EDF now estimates project costs at €10.9 billion ($12.75 billion USD), three times the original cost estimates. Hot testing is currently planned to occur by the end of 2018\n\nIn July 2015, EDF agreed to take a majority stake in Areva NP, following a French government instruction they create a \"global strategic partnership\".\n\nIn 2016, the European Commission assessed that France's nuclear decommissioning liabilities were seriously underfunded, with only 23 billion euros of earmarked assets to cover 74.1 billion euros of expected decommissioning costs.\n\nDrawing such a large percentage of overall electrical production from nuclear power is unique to France. This reliance has resulted in certain necessary deviations from the standard design and function of other nuclear power programs. For instance, in order to meet changing demand throughout the day, some plants must work as peaking power plant, whereas most nuclear plants in the world operate as base-load plants, and allow other fossil or hydro units to adjust to demand. Nuclear power in France has a total capacity factor of around 77%, which is low due to load following. However availability is around 84%, indicating excellent overall performance of the plants.\n\nThe first eight power reactors in the nation were gas cooled reactor types (UNGG reactor), whose development was pioneered by CEA. Coinciding with a uranium enrichment program, EDF developed pressurized water reactor (PWR) technology which eventually became the dominant type. The gas-cooled reactors located at Brennilis, Bugey, Chinon, and Marcoule have all been shut down.\n\nAll operating plants today are PWRs. The sodium-cooled fast breeder reactor technology development reactors, Phénix and Superphénix, have been shut down with work ongoing for a more advanced replacement in the form of the ASTRID (reactor).\n\nThe PWR plants were all developed by Framatome (now Areva) from the initial Westinghouse design. All currently operating PWR plants are of three design variations, having output powers of 900 MWe, 1300 MWe, and 1450 MWe. The repeated use of these standard variants of a design has afforded France the greatest degree of nuclear plant standardization in the world.\n\nThere are a total of 34 of these reactors in operation; most were constructed in the 1970s and the early 1980s. In 2002, they had a uniform review and all were granted a 10-year life extension.\n\nWith the CP0 and CP1 designs, two reactors share the same machine and command room. With the CP2 design, each reactor has its own machine and command room. Apart from this difference, CP1 and CP2 use the same technologies, and the two types are frequently referred to as \"CPY\". Compared to CP0 they have an additional cooling circuit between the emergency system that in case of an accident allows to spray water into the containment and the circuit which contains river water, a more flexible control system and some minor difference in the layout of the building.\n\nThis three loop design (three steam generators and three primary circulation pumps) was also exported to a number of other countries, including:\n\n\nThere are 20 reactors of this design (four steam generators and four primary circulation pumps) operating in France. The P4 and P'4 type have some minor difference in the layout of the building, especially for the structure which contain the fuel rods and the circuitry.\n\nThere are only 4 of these reactors, housed at two separate sites: Civaux and Chooz. Construction of these reactors started between 1984 and 1991, but full commercial operation did not begin until between 2000 and 2002 because of thermal fatigue flaws in the heat removal system requiring the redesign and replacement of parts in each N4 power station. In 2003 the stations were all uprated to 1500 MWe.\n\nThe next generation design for French reactors is the EPR, which will have a broader scope than France alone, with a plant in Finland and two in China undergoing construction, and two more proposed for the United Kingdom. The first French EPR is under construction at the Flamanville Nuclear Power Plant. As a result of delays and cost overruns, completion is now scheduled for 2017. An additional EPR reactor was planned for the Penly Nuclear Power Plant, but this project has now been abandoned.\n\nThe reactor design was developed by Areva contributing its N4 reactor technology and the German company Siemens contributing its Konvoi reactor technology. In keeping with the French approach of highly standardized plants and proven technology, it uses more traditional active safety systems and is more similar to current plant designs than international competitors such as the AP1000 or the ESBWR.\n\nIn 2013, EDF acknowledged the difficulties it was having building the EPR design. In September 2015 EDF's chief executive, Jean-Bernard Lévy, stated that the design of a \"New Model\" EPR was being worked on, which will be easier and cheaper to build, which would be ready for orders from about 2020. In 2016 EDF planned to build two New Model EPR reactors in France by 2030 to prepare for renewing its fleet of older reactors. However following financial difficulties at Areva, and its merger with EDF, French Energy Minister Nicolas Hulot said in January 2018 \"for now [building a New Model EPR] is neither a priority or a plan. Right now the priority is to develop renewable energy and to reduce the share of nuclear.\"\n\nThe majority of nuclear plants in France are located away from the coasts and obtain their cooling water from rivers. These plants employ cooling towers to reduce their impact on the environment. The temperature of emitted water carrying the waste heat is strictly limited by the French government, and this has proved to be problematic during recent heat waves.\n\nFive plants, equaling 18 reactors are located on the coast:\n\n\nThese five get their cooling water directly from the ocean and can thus dump their waste heat directly back into the sea, which is slightly more economical.\n\nFrance is one of the few countries in the world with an active nuclear reprocessing program, with the COGEMA La Hague site. Enrichment work, some MOX fuel fabrication, and other activities take place at the Tricastin Nuclear Power Centre. Enrichment is completely domestic and is powered by 2/3 of the output of the nuclear plant at Tricastin. Reprocessing of fuel from other countries has been done for the United States and Japan, who have expressed the desire to develop a more closed fuel cycle similar to what France has achieved. MOX fuel fabrication services have also been sold to other countries, notably to the USA for the Megatons to Megawatts Program, using plutonium from dismantled nuclear weapons.\n\nWhile France does not mine uranium for the front end of the fuel cycle domestically, French companies have various holdings in the uranium market. Uranium for the French program totals 10,500 tonnes per year coming from various locations such as:\n\n\nFinal disposal of the high level nuclear waste is planned to be done at the Meuse/Haute Marne Underground Research Laboratory deep geological repository.\n\nFrance's nuclear reactors comprise 90 per cent of EDFs capacity and so they are used in load-following mode and some reactors close at weekends because there is no market for the electricity. This means that the capacity factor is low by world standards, usually in the high seventies as a percentage, which is not an ideal economic situation for nuclear plants.\n\nDuring periods of high demand EDF has been routinely \"forced into the relatively expensive spot and short-term power markets because it lacks adequate peak load generating capacity\". France heavily relies on electric heating, with about one third of existing and three-quarters of new houses using electric space heating due to the low off-peak tariffs offered. Due to this residential heating demand, about 2.3 GW of extra power is needed for every degree Celsius of temperature drop. This means that during cold snaps, French electricity demand increases dramatically, forcing the country to import at full capacity from its neighbours during peak demand. For example, in February 2012, Germany \"came to the rescue of France during last week's cold snap by massively exporting electricity to its neighbour\".\n\nAll but five of EDFs plants are inland and require fresh water for cooling. Eleven of these 15 inland plants have cooling towers, using evaporative cooling, while the others use lake or river water directly. In very hot summers, generation output may be restricted.\n\nIn 2008, nuclear power accounted for 16% of final energy consumption in France. As is common in all industrialized nations, fossil fuels still dominate energy consumption, particularly in the transportation and heating sectors. \nHowever, nuclear constitutes a higher level of total energy consumption in France than in any other country. \nIn 2001, nuclear power accounted for 37% of the total energy consumption in France. \nIn 2011 France consumed ~ 11 quadrillion BTUs (3224 TWh) of energy according to the Energy Information Administration.\n\nIn July 2008, 18,000 litres (4,755 gallons) of uranium solution containing natural uranium were accidentally released from Tricastin Nuclear Power Centre. Due to cleaning and repair work the containment system for a uranium solution holding tank was not functional when the tank filled. The inflow exceeded the tank's capacity and 30 cubic metres of uranium solution leaked, with 18 cubic metres spilled on the ground. Testing found elevated uranium levels in the nearby Gaffière and Lauzon rivers. The liquid that escaped to the ground contained about 75 kg of natural uranium, which is toxic as a heavy metal, but only slightly radioactive. Estimates for the releases were initially higher, up to 360 kg of natural uranium, but revised downward later. French authorities banned the use of water from the Gaffière and Lauzon for drinking and watering of crops for 2 weeks. Swimming, water sports and fishing were also banned. This incident has been classified as Level 1 (anomaly) on the International Nuclear Event Scale. Shortly after the first incident, approximately 100 employees were exposed to minor doses of radiation (1/40 of the annual limit) due to a piping failure.\n\nIn October 2017 EDF announced it would repair fire safety system pipes at 20 nuclear reactors to increase seismic safety after discovering thinning metal in some sections of pipes. EDF classified this as a Level 2 (incident) on the International Nuclear Event Scale.\n\nIn 2006 the Autorité de sûreté nucléaire (ASN) was created as the independent French nuclear safety regulator, replacing the General Direction for Nuclear Safety and Radioprotection.\n\nIn 2012, the ASN released a report announcing a sweeping safety upgrade to all the country's reactors. The ASN's report states plainly that a loss of coolant or electricity could, in the worst cases, see meltdowns at nuclear reactors in hours. It also lists many shortcomings found during 'stress tests', in which some safety aspects of plants were found not to meet existing standards. It will now require all power plants to build a set of safety systems of last resort, contained in bunkers that will be hardened to withstand more extreme earthquakes, floods and other threats than plants themselves are designed to cope with. It will also adopt a proposal by EDF to create an elite force that is specifically trained to tackle nuclear accidents and could be deployed to any site within hours. Both moves are a response to the Fukushima nuclear disaster.\n\nMonique Sené is a nuclear physicist and one of the co-founders of the \"Groupement des scientifiques pour l'information sur l'énergie nucléaire\" (GSIEN) (Association of Scientists for Information on Nuclear Energy) and its first president. As of March 2011, she was Honorary Research Director at the National Centre for Scientific Research and president of GSIEN. Although she is not an opponent of nuclear power \"per se\", Sené is a high-profile critic of the French nuclear power programme due to concerns about its safety, the handling of nuclear waste and its imposition without public debate.\n\nFollowing the 2011 Fukushima I nuclear accidents there has been an increased focus on the risks associated with seismic activity in France, with particular attention focused on the Fessenheim Nuclear Power Plant.\n\nGeneral seismic risk in France is categorised on a five-point scale, with zone 1 being very low risk, through to zone 5 in areas with a 'very strong' risk. In Metropolitan France the areas of highest risk are rated at 4, 'strong', and are located in the Pyrenees, Alps, the south of the Haut-Rhin \"département\", the Territoire de Belfort and a few communes in Doubs. A new zoning map comes into force on 1 May 2011, which significantly increases the rating for many areas. The major nuclear research facilities at Cadarache are located in a zone 4 area near the fault that caused the 1909 Lambesc earthquake, while the Marcoule research centre and the nuclear power plants at Tricastin, Cruas, Saint-Alban, Bugey and Fessenheim (near the fault that caused the 1356 Basel earthquake) are all within zone 3. A further 6 plants lie within zone 2.\n\nThe current process for evaluating the seismic hazard for a nuclear plant is set out in \"Règle Fondamentale de Sûreté\" (Fundamental Safety Rule) RFS 2001-01, published by the Institute for Radioprotection and Nuclear Safety, which uses more detailed seismotectonic zones. RFS 2001-01 replaced RFS I.2.c, published in 1981, however it has been criticised for continuing to require a deterministic assessment (rather than a probabilistic approach) that relies primarily on the strongest 'historically known' earthquake near a site. This leads to a number of problems including the short period (in geological timescales) for which there are records, the difficulty of assessing the characteristics of earthquakes that occurred prior to the use of seismometers, the difficulty of identifying the existence of all earthquakes that pre-date the historic record, and ultimately the reliance on one single earthquake scenario. Other criticisms include the use of intensity in the evaluation method, rather than spectral acceleration, which is commonly used elsewhere.\n\nFollowing the 2011 Fukushima I nuclear accidents an OpinionWay poll at the end of March found that 57% of the French population were opposed to nuclear energy in France. A TNS-Sofres poll in the days following the accident found 55% in favour of nuclear power. In 2006, BBC / GlobeScan poll found 57% of the French opposed to nuclear energy.<ref name=\"BBC/GlobeScan\"></ref>\n\nIn May 2001, an Ipsos poll found that nearly 70% of the population had a 'good opinion' of nuclear power, however 56% also preferred not to live near a nuclear plant and the same proportion thought that a 'Chernobyl-like accident' could occur in France. The same Ipsos poll revealed that 50% thought that nuclear power was the best way of solving the problem of the greenhouse effect, while 88% thought this was a major reason for continuing to use nuclear power.\n\nHistorically the position has generally been favourable, with around two-thirds of the population strongly supporting nuclear power, while the Gaullists, the Socialist Party and the Communist Party were also all in favour.\n\nWhen the Civaux Nuclear Power Plant was being constructed in 1997, it was claimed to be welcomed by the local community:\n\nA variety of reasons were cited for the popular support; a sense of national independence and reduced reliance on foreign oil, reduction of greenhouse gases, and a cultural interest in large technological projects (like the TGV, [whose high-speed lines are powered by these plants] and Concorde).\n\nIn the 1970s, an anti-nuclear movement in France, consisting of citizens' groups and political action committees, emerged. Between 1975 and 1977, some 175,000 people protested against nuclear power in ten demonstrations.\n\nIn January 2004, up to 15,000 anti-nuclear protesters marched in Paris against a new generation of nuclear reactors, the European Pressurised Reactor (EPR). On 17 March 2007, simultaneous protests, organised by Sortir du nucléaire, were staged in 5 French towns to protest against the construction of EPR plants.\n\nAfter Japan's 2011 Fukushima nuclear disaster, thousands staged anti-nuclear protests around France, demanding reactors be closed. Protesters' demands were focused on getting France to shut its oldest nuclear power station at Fessenheim. Many people also protested at the Cattenom nuclear plant, France's second most powerful.\n\nIn November 2011, thousands of anti-nuclear protesters delayed a train carrying radioactive waste from France to Germany. Many clashes and obstructions made the journey the slowest one since the annual shipments of radioactive waste began in 1995.\nAlso in November 2011, a French court fined nuclear power giant Électricité de France €1.5m and jailed two senior employees for spying on Greenpeace, including hacking into Greenpeace's computer systems. Greenpeace was awarded €500,000 in damages.\n\nOn the first anniversary of the Fukushima nuclear disaster, organisers of French anti-nuclear demonstrations claim 60,000 supporters formed a human chain 230 kilometres long, stretching from Lyon to Avignon. Austrian Chancellor Werner Faymann expects anti-nuclear petition drives to start in at least six European Union countries in 2012 with the goal of having the EU abandon nuclear power.\n\nIn March 2014, police arrested 57 Greenpeace protesters who used a truck to break through security barriers and enter the Fessenheim nuclear in eastern France. The activists hung antinuclear banners, but France’s nuclear safety authority said that the plant’s security had not been compromised. Although President Hollande promised to close Fessenheim by 2016, and Greenpeace continues to demand immediate closure, Fessenheim continues to operate without problems.\n\nIn 2007, Areva NC claimed that, due to their reliance on nuclear power, France's carbon emissions per kWh are less than 1/10 that of Germany and the UK, and 1/13 that of Denmark, which has no nuclear plants. Its emissions of nitrogen oxide and sulfur dioxide have been reduced by 70% over 20 years, even though the total power output has tripled in that time.\n\nIf done without environmental or health over-sight, conventional mining for uranium can produce large amounts of mining tailings and contaminated water but as of 2010, about half of the world's uranium supply is increasingly generated from In situ recovery (ISR) technology, that does not require physical mining in the conventional sense and if responsibly operated is considerably cleaner. Another alternative to ISR is remote controlled underground mining, the French owned Areva Resources Canada owns a large stake in the Canadian McArthur River uranium mine, the world's highest grade and largest uranium mine by output, the underground remote operation of mining vehicles in this mine, is designed to keep personnel exposure to rock particulates and radon gas etc. low. The mine is a frequent winner of the John T. Ryan National Safety Trophy award in Canada, which is bestowed upon the safest mine in the country every year.\n\nAccording to the French embassy to the US, fission-electricity \"helps to reduce French greenhouse gas emissions by avoiding the release of 31 billions tones of carbon dioxide (contrary to coal or gas generation) and making France the less carbon emitting country within the OECD\". It further notes that, due to recycling of spent nuclear fuel, French fission-electric stations, produce 10 g/year/inhabitant of \"nuclear waste\", which is primarily fission products and other safety concerning solid decaying radioactive isotopes.\n\nFrench environmentalist Bruno Comby started the group Environmentalists For Nuclear Energy in 1996, and said in 2005, \"If well-managed, nuclear energy is very clean, does not create polluting gases in the atmosphere, produces very little waste and does not contribute to the greenhouse effect\".\n\nUnlike its neighboring countries of Germany, Italy and the United Kingdom, France does not rely very much on fossil fuels and biomass for electricity or home heating thanks to an abundance of cheap nuclear power.\nTaken as a whole, the country therefore has superior air quality and lower pollution related deaths. \nAir pollution in France largely comes from cars and a minority is carried by the wind from Germany. \nEach year, the coal fired power stations in Germany are the cause of a calculated 1,860 premature domestic deaths and approximately 2,500 deaths abroad.\n\nOutdoor fossil fuel and biomass pollution, from particulate matter alone, kill more people than is popularly know, approximately 1 million people every year according to the World Health Organization. The level of atmospheric particulate matter, small enough to enter and cause damage to the lungs –is 13 micrograms per cubic metre in France, cleaner than the air in Germany, where the particulate air pollution is higher at 16 micrograms per cubic metre.\n\nA common criticism of French energy policy is that the country may have over-invested in nuclear power plants, requiring electricity export when French electricity demand is low or \"dumping\" in the French market, encouraging the use of electricity for space heating and water heating.\nThis can be regarded as an economically wasteful practice. \nHowever, as the adoption of electric cars, such as the French Renault Fluence Z.E., over internal combustion engine vehicles increases, reducing fossil fuel dependence, France's comparatively cheap peak and off peak electricity prices could act as a strong customer incentive that may spur the speed of the adoption of electric vehicles, essentially turning the current perceived glut of relatively cheap fission-electricity into an asset, as demand for electric vehicle recharging stations becomes more and more commonplace.\n\nDue to France's very low-carbon power electricity grid, the carbon dioxide emissions from charging an electric car from the French electricity grid are 12 g per km traveled. \nThis compares favourably to the direct emissions of one of the most successful hybrid electric vehicles, the Toyota Prius, which produces carbon dioxide emissions at the higher rate of 105 g per km traveled.\n\nThe nuclear fusion project International Thermonuclear Experimental Reactor is constructing the world's largest and most advanced experimental tokamak nuclear fusion reactor in the south of France. A collaboration between the European Union (EU), India, Japan, China, Russia, South Korea and the United States, the project aims to make a transition from experimental studies of plasma physics to electricity-producing fusion power plants. In 2005, Greenpeace International issued a press statement criticizing government funding of the ITER, believing the money should have been diverted to renewable energy sources and claiming that fusion energy would result in nuclear waste and nuclear weapons proliferation issues. A French association including about 700 anti-nuclear groups, Sortir du nucléaire (Get Out of Nuclear Energy), claimed that ITER was a hazard because scientists did not yet know how to manipulate the high-energy deuterium and tritium hydrogen isotopes used in the fusion process. According to most anti-nuclear groups, nuclear fusion power \"remains a distant dream\". The World Nuclear Association says that fusion \"presents so far insurmountable scientific and engineering challenges\". Construction of the ITER facility began in 2007, but the project has run into many delays and budget overruns. The facility is now not expected to begin operations until the year 2027 – 11 years after initially anticipated.\n\n\n\n\n"}
{"id": "42365376", "url": "https://en.wikipedia.org/wiki?curid=42365376", "title": "Operation Safehaven (1944–48)", "text": "Operation Safehaven (1944–48)\n\nOperation Safehaven was an Anglo-American program created in December 1944 to locate German assets in the neutral powers and steer them into \"safe havens\", generally British or American humanitarian organizations, after World War II. The overall objective of the operation was to dissipate Nazi wealth in order to render any possible post-war resurgence (a so-called \"Fourth Reich\") controllable and to make it impossible for Germany to start another war.\n\n"}
{"id": "23084", "url": "https://en.wikipedia.org/wiki?curid=23084", "title": "Paleontology", "text": "Paleontology\n\nPaleontology or palaeontology () is the scientific study of life that existed prior to, and sometimes including, the start of the Holocene Epoch (roughly 11,700 years before present). It includes the study of fossils to determine organisms' evolution and interactions with each other and their environments (their paleoecology). Paleontological observations have been documented as far back as the 5th century BC. The science became established in the 18th century as a result of Georges Cuvier's work on comparative anatomy, and developed rapidly in the 19th century. The term itself originates from Greek παλαιός, \"palaios\", \"old, ancient\", ὄν, \"on\" (gen. \"ontos\"), \"being, creature\" and λόγος, \"logos\", \"speech, thought, study\".\n\nPaleontology lies on the border between biology and geology, but differs from archaeology in that it excludes the study of anatomically modern humans. It now uses techniques drawn from a wide range of sciences, including biochemistry, mathematics, and engineering. Use of all these techniques has enabled paleontologists to discover much of the evolutionary history of life, almost all the way back to when Earth became capable of supporting life, about 3.8 billion years ago. As knowledge has increased, paleontology has developed specialised sub-divisions, some of which focus on different types of fossil organisms while others study ecology and environmental history, such as ancient climates.\n\nBody fossils and trace fossils are the principal types of evidence about ancient life, and geochemical evidence has helped to decipher the evolution of life before there were organisms large enough to leave body fossils. Estimating the dates of these remains is essential but difficult: sometimes adjacent rock layers allow radiometric dating, which provides absolute dates that are accurate to within 0.5%, but more often paleontologists have to rely on relative dating by solving the \"jigsaw puzzles\" of biostratigraphy. Classifying ancient organisms is also difficult, as many do not fit well into the Linnaean taxonomy that is commonly used for classifying living organisms, and paleontologists more often use cladistics to draw up evolutionary \"family trees\". The final quarter of the 20th century saw the development of molecular phylogenetics, which investigates how closely organisms are related by measuring how similar the DNA is in their genomes. Molecular phylogenetics has also been used to estimate the dates when species diverged, but there is controversy about the reliability of the molecular clock on which such estimates depend.\n\nThe simplest definition is \"the study of ancient life\". Paleontology seeks information about several aspects of past organisms: \"their identity and origin, their environment and evolution, and what they can tell us about the Earth's organic and inorganic past\".\n\nPaleontology is one of the historical sciences, along with archaeology, geology, astronomy, cosmology, philology and history itself. This means that it aims to describe phenomena of the past and reconstruct their causes. Hence it has three main elements: description of the phenomena; developing a general theory about the causes of various types of change; and applying those theories to specific facts.\nWhen trying to explain past phenomena, paleontologists and other historical scientists often construct a set of hypotheses about the causes and then look for a \"smoking gun\", a piece of evidence that indicates that one hypothesis is a better explanation than others. Sometimes the smoking gun is discovered by a fortunate accident during other research. For example, the discovery by Luis Alvarez and Walter Alvarez of an iridium-rich layer at the Cretaceous–Tertiary boundary made asteroid impact and volcanism the most favored explanations for the Cretaceous–Paleogene extinction event.\n\nThe other main type of science is experimental science, which is often said to work by conducting experiments to \"disprove\" hypotheses about the workings and causes of natural phenomena – note that this approach cannot confirm a hypothesis is correct, since some later experiment may disprove it. However, when confronted with totally unexpected phenomena, such as the first evidence for invisible radiation, experimental scientists often use the same approach as historical scientists: construct a set of hypotheses about the causes and then look for a \"smoking gun\".\n\nPaleontology lies on the boundary between biology and geology since paleontology focuses on the record of past life but its main source of evidence is fossils, which are found in rocks. For historical reasons paleontology is part of the geology departments of many universities, because in the 19th century and early 20th century geology departments found paleontological evidence important for estimating the ages of rocks while biology departments showed little interest.\n\nPaleontology also has some overlap with archaeology, which primarily works with objects made by humans and with human remains, while paleontologists are interested in the characteristics and evolution of humans as organisms. When dealing with evidence about humans, archaeologists and paleontologists may work together – for example paleontologists might identify animal or plant fossils around an archaeological site, to discover what the people who lived there ate; or they might analyze the climate at the time when the site was inhabited by humans.\n\nIn addition paleontology often uses techniques derived from other sciences, including biology, osteology, ecology, chemistry, physics and mathematics. For example, geochemical signatures from rocks may help to discover when life first arose on Earth, and analyses of carbon isotope ratios may help to identify climate changes and even to explain major transitions such as the Permian–Triassic extinction event. A relatively recent discipline, molecular phylogenetics, often helps by using comparisons of different modern organisms' DNA and RNA to re-construct evolutionary \"family trees\"; it has also been used to estimate the dates of important evolutionary developments, although this approach is controversial because of doubts about the reliability of the \"molecular clock\". Techniques developed in engineering have been used to analyse how ancient organisms might have worked, for example how fast \"Tyrannosaurus\" could move and how powerful its bite was. It is relatively commonplace to study fossils using X-ray microtomography A combination of paleontology, biology, and archaeology, paleoneurobiology is the study of endocranial casts (or endocasts) of species related to humans to learn about the evolution of human brains.\n\nPaleontology even contributes to astrobiology, the investigation of possible life on other planets, by developing models of how life may have arisen and by providing techniques for detecting evidence of life.\n\nAs knowledge has increased, paleontology has developed specialised subdivisions. Vertebrate paleontology concentrates on fossils of vertebrates, from the earliest fish to the immediate ancestors of modern mammals. Invertebrate paleontology deals with fossils of invertebrates such as molluscs, arthropods, annelid worms and echinoderms. Paleobotany focuses on the study of fossil plants, but traditionally includes the study of fossil algae and fungi. Palynology, the study of pollen and spores produced by land plants and protists, straddles the border between paleontology and botany, as it deals with both living and fossil organisms. Micropaleontology deals with all microscopic fossil organisms, regardless of the group to which they belong.\n\nInstead of focusing on individual organisms, paleoecology examines the interactions between different organisms, such as their places in food chains, and the two-way interaction between organisms and their environment.  One example is the development of oxygenic photosynthesis by bacteria, which hugely increased the productivity and diversity of ecosystems. This also caused the oxygenation of the atmosphere. Together, these were a prerequisite for the evolution of the most complex eukaryotic cells, from which all multicellular organisms are built.\n\nPaleoclimatology, although sometimes treated as part of paleoecology, focuses more on the history of Earth's climate and the mechanisms that have changed it – which have sometimes included evolutionary developments, for example the rapid expansion of land plants in the Devonian period removed more carbon dioxide from the atmosphere, reducing the greenhouse effect and thus helping to cause an ice age in the Carboniferous period.\n\nBiostratigraphy, the use of fossils to work out the chronological order in which rocks were formed, is useful to both paleontologists and geologists. Biogeography studies the spatial distribution of organisms, and is also linked to geology, which explains how Earth's geography has changed over time.\n\nFossils of organisms' bodies are usually the most informative type of evidence. The most common types are wood, bones, and shells. Fossilisation is a rare event, and most fossils are destroyed by erosion or metamorphism before they can be observed. Hence the fossil record is very incomplete, increasingly so further back in time. Despite this, it is often adequate to illustrate the broader patterns of life's history. There are also biases in the fossil record: different environments are more favorable to the preservation of different types of organism or parts of organisms. Further, only the parts of organisms that were already mineralised are usually preserved, such as the shells of molluscs. Since most animal species are soft-bodied, they decay before they can become fossilised. As a result, although there are 30-plus phyla of living animals, two-thirds have never been found as fossils.\n\nOccasionally, unusual environments may preserve soft tissues. These lagerstätten allow paleontologists to examine the internal anatomy of animals that in other sediments are represented only by shells, spines, claws, etc. – if they are preserved at all. However, even lagerstätten present an incomplete picture of life at the time. The majority of organisms living at the time are probably not represented because lagerstätten are restricted to a narrow range of environments, e.g. where soft-bodied organisms can be preserved very quickly by events such as mudslides; and the exceptional events that cause quick burial make it difficult to study the normal environments of the animals. The sparseness of the fossil record means that organisms are expected to exist long before and after they are found in the fossil record – this is known as the Signor–Lipps effect.\n\nTrace fossils consist mainly of tracks and burrows, but also include coprolites (fossil feces) and marks left by feeding. Trace fossils are particularly significant because they represent a data source that is not limited to animals with easily fossilised hard parts, and they reflect organisms' behaviours. Also many traces date from significantly earlier than the body fossils of animals that are thought to have been capable of making them. Whilst exact assignment of trace fossils to their makers is generally impossible, traces may for example provide the earliest physical evidence of the appearance of moderately complex animals (comparable to earthworms).\n\nGeochemical observations may help to deduce the global level of biological activity at a certain period, or the affinity of certain fossils. For example, geochemical features of rocks may reveal when life first arose on Earth, and may provide evidence of the presence of eukaryotic cells, the type from which all multicellular organisms are built. Analyses of carbon isotope ratios may help to explain major transitions such as the Permian–Triassic extinction event.\n\nNaming groups of organisms in a way that is clear and widely agreed is important, as some disputes in paleontology have been based just on misunderstandings over names. Linnaean taxonomy is commonly used for classifying living organisms, but runs into difficulties when dealing with newly discovered organisms that are significantly different from known ones. For example: it is hard to decide at what level to place a new higher-level grouping, e.g. genus or family or order; this is important since the Linnaean rules for naming groups are tied to their levels, and hence if a group is moved to a different level it must be renamed.\n\nPaleontologists generally use approaches based on cladistics, a technique for working out the evolutionary \"family tree\" of a set of organisms. It works by the logic that, if groups B and C have more similarities to each other than either has to group A, then B and C are more closely related to each other than either is to A. Characters that are compared may be anatomical, such as the presence of a notochord, or molecular, by comparing sequences of DNA or proteins. The result of a successful analysis is a hierarchy of clades – groups that share a common ancestor. Ideally the \"family tree\" has only two branches leading from each node (\"junction\"), but sometimes there is too little information to achieve this and paleontologists have to make do with junctions that have several branches. The cladistic technique is sometimes fallible, as some features, such as wings or camera eyes, evolved more than once, convergently – this must be taken into account in analyses.\n\nEvolutionary developmental biology, commonly abbreviated to \"Evo Devo\", also helps paleontologists to produce \"family trees\", and understand fossils. For example, the embryological development of some modern brachiopods suggests that brachiopods may be descendants of the halkieriids, which became extinct in the Cambrian period.\nPaleontology seeks to map out how living things have changed through time. A substantial hurdle to this aim is the difficulty of working out how old fossils are. Beds that preserve fossils typically lack the radioactive elements needed for radiometric dating. This technique is our only means of giving rocks greater than about 50 million years old an absolute age, and can be accurate to within 0.5% or better. Although radiometric dating requires very careful laboratory work, its basic principle is simple: the rates at which various radioactive elements decay are known, and so the ratio of the radioactive element to the element into which it decays shows how long ago the radioactive element was incorporated into the rock. Radioactive elements are common only in rocks with a volcanic origin, and so the only fossil-bearing rocks that can be dated radiometrically are a few volcanic ash layers.\n\nConsequently, paleontologists must usually rely on stratigraphy to date fossils. Stratigraphy is the science of deciphering the \"layer-cake\" that is the sedimentary record, and has been compared to a jigsaw puzzle. Rocks normally form relatively horizontal layers, with each layer younger than the one underneath it. If a fossil is found between two layers whose ages are known, the fossil's age must lie between the two known ages. Because rock sequences are not continuous, but may be broken up by faults or periods of erosion, it is very difficult to match up rock beds that are not directly next to one another. However, fossils of species that survived for a relatively short time can be used to link up isolated rocks: this technique is called \"biostratigraphy\". For instance, the conodont \"Eoplacognathus pseudoplanus\" has a short range in the Middle Ordovician period. If rocks of unknown age are found to have traces of \"E. pseudoplanus\", they must have a mid-Ordovician age. Such index fossils must be distinctive, be globally distributed and have a short time range to be useful. However, misleading results are produced if the index fossils turn out to have longer fossil ranges than first thought. Stratigraphy and biostratigraphy can in general provide only relative dating (\"A\" was before \"B\"), which is often sufficient for studying evolution. However, this is difficult for some time periods, because of the problems involved in matching up rocks of the same age across different continents.\n\nFamily-tree relationships may also help to narrow down the date when lineages first appeared. For instance, if fossils of B or C date to X million years ago and the calculated \"family tree\" says A was an ancestor of B and C, then A must have evolved more than X million years ago.\n\nIt is also possible to estimate how long ago two living clades diverged – i.e. approximately how long ago their last common ancestor must have lived – by assuming that DNA mutations accumulate at a constant rate. These \"molecular clocks\", however, are fallible, and provide only a very approximate timing: for example, they are not sufficiently precise and reliable for estimating when the groups that feature in the Cambrian explosion first evolved, and estimates produced by different techniques may vary by a factor of two.\n\nThe evolutionary history of life stretches back to over , possibly as far as . Earth formed about and, after a collision that formed the Moon about 40 million years later, may have cooled quickly enough to have oceans and an atmosphere about . However, there is evidence on the Moon of a Late Heavy Bombardment from . If, as seems likely, such a bombardment struck Earth at the same time, the first atmosphere and oceans may have been stripped away. The oldest clear evidence of life on Earth dates to , although there have been reports, often disputed, of fossil bacteria from and of geochemical evidence for the presence of life . Some scientists have proposed that life on Earth was \"seeded\" from elsewhere, but most research concentrates on various explanations of how life could have arisen independently on Earth.\n\nFor about 2,000 million years microbial mats, multi-layered colonies of different types of bacteria, were the dominant life on Earth. The evolution of oxygenic photosynthesis enabled them to play the major role in the oxygenation of the atmosphere from about . This change in the atmosphere increased their effectiveness as nurseries of evolution. While eukaryotes, cells with complex internal structures, may have been present earlier, their evolution speeded up when they acquired the ability to transform oxygen from a poison to a powerful source of energy in their metabolism. This innovation may have come from primitive eukaryotes capturing oxygen-powered bacteria as endosymbionts and transforming them into organelles called mitochondria. The earliest evidence of complex eukaryotes with organelles such as mitochondria, dates from .\n\nMulticellular life is composed only of eukaryotic cells, and the earliest evidence for it is the Francevillian Group Fossils from , although specialisation of cells for different functions first appears between (a possible fungus) and (a probable red alga). Sexual reproduction may be a prerequisite for specialisation of cells, as an asexual multicellular organism might be at risk of being taken over by rogue cells that retain the ability to reproduce.\n\nThe earliest known animals are cnidarians from about , but these are so modern-looking that the earliest animals must have appeared before then. Early fossils of animals are rare because they did not develop mineralised hard parts that fossilise easily until about . The earliest modern-looking bilaterian animals appear in the Early Cambrian, along with several \"weird wonders\" that bear little obvious resemblance to any modern animals. There is a long-running debate about whether this Cambrian explosion was truly a very rapid period of evolutionary experimentation; alternative views are that modern-looking animals began evolving earlier but fossils of their precursors have not yet been found, or that the \"weird wonders\" are evolutionary \"aunts\" and \"cousins\" of modern groups. Vertebrates remained an obscure group until the first fish with jaws appeared in the Late Ordovician.\n\nThe spread of life from water to land required organisms to solve several problems, including protection against drying out and supporting themselves against gravity. The earliest evidence of land plants and land invertebrates date back to about and respectively. The lineage that produced land vertebrates evolved later but very rapidly between and ; recent discoveries have overturned earlier ideas about the history and driving forces behind their evolution. Land plants were so successful that they caused an ecological crisis in the Late Devonian, until the evolution and spread of fungi that could digest dead wood.\nDuring the Permian period synapsids, including the ancestors of mammals, may have dominated land environments, but the Permian–Triassic extinction event came very close to wiping out complex life. The extinctions were apparently fairly sudden, at least among vertebrates. During the slow recovery from this catastrophe a previously obscure group, archosaurs, became the most abundant and diverse terrestrial vertebrates. One archosaur group, the dinosaurs, were the dominant land vertebrates for the rest of the Mesozoic, and birds evolved from one group of dinosaurs. During this time mammals' ancestors survived only as small, mainly nocturnal insectivores, but this apparent set-back may have accelerated the development of mammalian traits such as endothermy and hair. After the Cretaceous–Paleogene extinction event killed off the non-avian dinosaurs – birds are the only surviving dinosaurs – mammals increased rapidly in size and diversity, and some took to the air and the sea.\nFossil evidence indicates that flowering plants appeared and rapidly diversified in the Early Cretaceous, between and . Their rapid rise to dominance of terrestrial ecosystems is thought to have been propelled by coevolution with pollinating insects. Social insects appeared around the same time and, although they account for only small parts of the insect \"family tree\", now form over 50% of the total mass of all insects.\n\nHumans evolved from a lineage of upright-walking apes whose earliest fossils date from over . Although early members of this lineage had chimp-sized brains, about 25% as big as modern humans', there are signs of a steady increase in brain size after about . There is a long-running debate about whether \"modern\" humans are descendants of a single small population in Africa, which then migrated all over the world less than 200,000 years ago and replaced previous hominine species, or arose worldwide at the same time as a result of interbreeding.\nLife on earth has suffered occasional mass extinctions at least since . Although they are disasters at the time, mass extinctions have sometimes accelerated the evolution of life on earth. When dominance of particular ecological niches passes from one group of organisms to another, it is rarely because the new dominant group is \"superior\" to the old and usually because an extinction event eliminates the old dominant group and makes way for the new one.\n\nThe fossil record appears to show that the rate of extinction is slowing down, with both the gaps between mass extinctions becoming longer and the average and background rates of extinction decreasing. However, it is not certain whether the actual rate of extinction has altered, since both of these observations could be explained in several ways:\n\n\n\nBiodiversity in the fossil record, which is\nshows a different trend: a fairly swift rise from , a slight decline from , in which the devastating Permian–Triassic extinction event is an important factor, and a swift rise from to the present.\n\nAlthough paleontology became established around 1800, earlier thinkers had noticed aspects of the fossil record. The ancient Greek philosopher Xenophanes (570–480 BC) concluded from fossil sea shells that some areas of land were once under water. During the Middle Ages the Persian naturalist Ibn Sina, known as \"Avicenna\" in Europe, discussed fossils and proposed a theory of petrifying fluids on which Albert of Saxony elaborated in the 14th century. The Chinese naturalist Shen Kuo (1031–1095) proposed a theory of climate change based on the presence of petrified bamboo in regions that in his time were too dry for bamboo.\n\nIn early modern Europe, the systematic study of fossils emerged as an integral part of the changes in natural philosophy that occurred during the Age of Reason. In the Italian Renaissance, Leonardo Da Vinci made various significant contributions to the field as well designed numerous fossils. At the end of the 18th century Georges Cuvier's work established comparative anatomy as a scientific discipline and, by proving that some fossil animals resembled no living ones, demonstrated that animals could become extinct, leading to the emergence of paleontology. The expanding knowledge of the fossil record also played an increasing role in the development of geology, particularly stratigraphy.\n\nThe first half of the 19th century saw geological and paleontological activity become increasingly well organised with the growth of geologic societies and museums and an increasing number of professional geologists and fossil specialists. Interest increased for reasons that were not purely scientific, as geology and paleontology helped industrialists to find and exploit natural resources such as coal.\n\nThis contributed to a rapid increase in knowledge about the history of life on Earth and to progress in the definition of the geologic time scale, largely based on fossil evidence. In 1822 Henri Marie Ducrotay de Blanville, editor of \"Journal de Physique\", coined the word \"palaeontology\" to refer to the study of ancient living organisms through fossils. As knowledge of life's history continued to improve, it became increasingly obvious that there had been some kind of successive order to the development of life. This encouraged early evolutionary theories on the transmutation of species.\nAfter Charles Darwin published \"Origin of Species\" in 1859, much of the focus of paleontology shifted to understanding evolutionary paths, including human evolution, and evolutionary theory.\nThe last half of the 19th century saw a tremendous expansion in paleontological activity, especially in North America. The trend continued in the 20th century with additional regions of the Earth being opened to systematic fossil collection. Fossils found in China near the end of the 20th century have been particularly important as they have provided new information about the earliest evolution of animals, early fish, dinosaurs and the evolution of birds. The last few decades of the 20th century saw a renewed interest in mass extinctions and their role in the evolution of life on Earth. There was also a renewed interest in the Cambrian explosion that apparently saw the development of the body plans of most animal phyla. The discovery of fossils of the Ediacaran biota and developments in paleobiology extended knowledge about the history of life back far before the Cambrian.\n\nIncreasing awareness of Gregor Mendel's pioneering work in genetics led first to the development of population genetics and then in the mid-20th century to the modern evolutionary synthesis, which explains evolution as the outcome of events such as mutations and horizontal gene transfer, which provide genetic variation, with genetic drift and natural selection driving changes in this variation over time. Within the next few years the role and operation of DNA in genetic inheritance were discovered, leading to what is now known as the \"Central Dogma\" of molecular biology. In the 1960s molecular phylogenetics, the investigation of evolutionary \"family trees\" by techniques derived from biochemistry, began to make an impact, particularly when it was proposed that the human lineage had diverged from apes much more recently than was generally thought at the time. Although this early study compared proteins from apes and humans, most molecular phylogenetics research is now based on comparisons of RNA and DNA.\n\n\n"}
{"id": "25313082", "url": "https://en.wikipedia.org/wiki?curid=25313082", "title": "Pan-region", "text": "Pan-region\n\nA pan-region is a geographic region or state’s sphere of economic, political and cultural influence extending beyond that state's borders. For example, the pan-region of the United States of America (USA) regions both bordering the USA and its close neighbors including, Canada, Mexico, and many South America other states.\n\nThe idea of pan-regions or spheres of economic and cultural influence was first developed by Karl Ernst Haushofer (8/27/1869-3/10/1946), a German General, geographer and geo-politician. Pan-regions contributed to Geopolitik or the German theories of foreign policy during the interwar period (1918–1939) or the time from the end of World War I and the beginning of World War II. Haushofer’s pan-regions divided the world under three supreme leading states in economy, politics and culture. Those three states included the USA who controlled North America and much of South America, Germany who controlled Europe, much of Africa and western Asia and Japan who controlled central, eastern, and the islands of southern Asia. These leading states could expect their regions to develop economic and political alliance with their leading state as well as yield to sanctions and major cultural designations.\n\nHistorically, the world was divided into three spheres of control, however after the end of World War II, Germany and Japan’s control over their various regions have diminished with the success of other nations. For example, German control over Europe has suffered with the development of the European Union and emergence of other foreign powers. Japan also is beginning to lose economic dominance over its pan-region with the emergence of a thriving Chinese economy.\n"}
{"id": "15781841", "url": "https://en.wikipedia.org/wiki?curid=15781841", "title": "Skydrol", "text": "Skydrol\n\nSkydrol is a fire-resistant aviation hydraulic fluid now manufactured by Eastman Chemical Company, and formerly was manufactured by Solutia and Monsanto. There are various lines of Skydrol including Skydrol 500B-4, Skydrol LD-4, and Skydrol 5.\n\nSkydrol is made up of a group of chemical additives dissolved into a fire-resistant phosphate ester base stock which inhibits corrosion and prevents erosion damage to servo valves and includes a purple or green dye to ease identification. It has been approved by most airframe manufacturers including Airbus, Boeing and BAE Systems and has been used in their products for over 40 years.\n\nAcid number (the proportional content of acid, not pH) and particulate contamination must be monitored while using Skydrol, and generally hydraulic systems should be sampled every C check.\nGenerally recommended contamination levels should be better than AS4059 Class 7 as new, and should not be allowed to degrade beyond Class 9. Skydrol has a 5-year shelf life from the date of manufacture.\n\nSkydrol fluids are extremely irritating to human tissue. Gloves and goggles are recommended safety equipment when servicing Skydrol systems. If the fluid gets on the skin it creates an itchy, red rash with a persistent burning sensation. The effects subside within a few hours; egg white can be applied to the affected area to neutralize the burning. Animal studies have shown that repeated exposure to tributylphosphate, one of the phosphate esters used in Skydrol fluids, may cause urinary bladder damage. If Skydrol gets in the eyes, it creates an intense stinging sensation. The recommended treatment for this is to use an eye-wash station, sometimes mineral oil or milk is used.\n\nSkydrol fluids are incompatible with many plastics, paints and adhesives, which can be softened and eventually destroyed by exposure to Skydrol. Some materials (for example rayon, acetate) and rubber-soled shoes may also be damaged by Skydrol.\n\nThe Skydrol series of phosphate ester hydraulic fluids were originally jointly developed by the Douglas Aircraft Company and Monsanto in the late 1940s to reduce the fire risk from leaking high pressure mineral oil-based hydraulic fluids impinging on potential ignition sources.\n\nIn 1949 Douglas first licensed Monsanto to produce a range of Skydrol materials under their patents. In the 1990s Monsanto became primarily a biotechnology company, and an independent chemical producer, Solutia, was created in 1997 to handle its chemical interests, including Skydrol. In 2012, Solutia was acquired by Eastman Chemical.\n\nSolutia Inc. built a new facility to produce Skydrol and SkyKleen aviation cleaning solutions in Anniston, Alabama in 2005.\n\nThe first type of Skydrol used in aviation was Skydrol 7000 which was dyed green in colour, now obsolete, as a fire-resistant lubricant in Douglas-designed cabin pressure superchargers (as piston-engined airliners do not have 'bleed air' pressurisation) used in the DC-6 and 7 series piston-engined aircraft, and first flight tested by United Airlines in 1949, who also used Skydrol 7000 in the hydraulic systems of these aircraft, as did quite a number of other airlines including Pan-Am, and KLM and BOAC in Europe.\n\nWith the introduction of jet aircraft operating at higher altitudes, and lower external temperatures there was a need for improved phosphate ester fluids. The story of the introduction of Skydrol type fluids in civil aviation is covered in a Kindle book entitled \"The Skydrol Story\", in which it describes how the Vickers Vanguard was the first non US built aircraft to introduce Skydrol as a hydraulic fluid when TCA adopted it for their Vanguard fleet.\n\nIn the years following, during the flight testing of the Boeing 707 a test aircraft suffered a gear collapse which led to an ensuing fire fueled by leaking hydraulic fluid. As a result of this incident, Boeing implemented the use of Skydrol on the 707 and then later on the 720 and subsequent aircraft. Skydrol 500B (Dyed purple in colour) then proliferated through the aerospace industry due to its flame retardant capability, but predominantly only in the civilian world on transport category aircraft.\n\nNotable exceptions include the BAC Concorde, which used silicate ester fluid Chevron M2V Oronite due to the high temperature requirements.\n\nAlso in military applications, Skydrol was never adopted into widespread use, ostensibly because if an aircraft was hit by enemy fire on a mission, it was believed that it is merely academic whether the fluid is flame retardant or not, as the aircraft would have been expected to be destroyed.\n\nThe predominant competing mineral oil fluid, MIL-PRF-5606 had higher flammability due to its lower flash point, however modern derivatives such as MIL-PRF-87257 have a flash point much closer to that of Skydrol.\n\nSome smaller business jets still use MIL-H-5606, such as the Dassault Falcon series jets, most of the Cessna Citations and all models of Learjet. Business jets using Skydrol include the Cessna Citation X, Gulfstreams and Bombardier Challenger.\n\nSpecial seals had to be developed for use with Skydrol, as the elastomers available at the time were incompatible - the first seals used were made from butyl rubber, which were resistant to the phosphate ester fluid but suffered some early leakages. Modern Skydrol compatible seals are usually made from EPDM (Ethylene Propylene Dione Monomer) or PTFE (PolytetrafluroEthylene).\n\n"}
{"id": "27692", "url": "https://en.wikipedia.org/wiki?curid=27692", "title": "Steam engine", "text": "Steam engine\n\nA steam engine is a heat engine that performs mechanical work using steam as its working fluid. In simple terms, the steam engine uses the expansion principle of chemistry, where heat applied to water evaporates the water into steam, and the force generated pushes a piston back and forth inside a cylinder. This pushing force is typically transformed, by way of a connecting rod and flywheel, into rotational force for work. The term \"steam engine\" is generally applied only to reciprocating engines as just described, not to the steam turbine.\n\nSteam engines are external combustion engines, where the working fluid is separated from the combustion products. Non-combustion heat sources such as solar power, nuclear power or geothermal energy may be used. The ideal thermodynamic cycle used to analyze this process is called the Rankine cycle. In the cycle, water is heated and changes into steam in a boiler operating at a high pressure. When expanded using pistons or turbines mechanical work is done. The reduced-pressure steam is then exhausted to the atmosphere, or condensed and pumped back into the boiler.\n\nIn general usage, the term \"steam engine\" can refer to either complete steam plants (including boilers etc.) such as railway steam locomotives and portable engines, or may refer to the piston or turbine machinery alone, as in the beam engine and stationary steam engine. However, a more detailed look at the steam locomotive referred to the engine as only that part where the heat in the steam was turned into motion of the piston, and hence enabled separate statements for boiler efficiency and engine efficiency. Specialized devices such as steam hammers and steam pile drivers are dependent on the steam pressure supplied from a separate boiler.\n\nThe use of boiling water to produce mechanical motion goes back over 2000 years, but early devices were not practical. The Spanish inventor Jerónimo de Ayanz y Beaumont obtained a patent for a rudimentary steam-powered water pump in 1606. In 1698 English engineer Thomas Savery patented a steam pump that used steam in direct contact with the water being pumped. Savery's steam pump used condensing steam to create a partial vacuum and draw water into a chamber, and then applied pressurized steam to further pump the water.\n\nThe \"atmospheric engine\" created by English inventor Thomas Newcomen was the first commercial true steam engine using a piston, and was used in 1712 for removing flood water from a mine. 104 were in use by 1733. Eventually over two thousand of them were installed.\n\nIn 1781 Scottish engineer James Watt patented a steam engine that produced continuous rotary motion. Watt's ten-horsepower engines enabled a wide range of manufacturing machinery to be powered. The engines could be sited anywhere that water and coal or wood fuel could be obtained. By 1883, engines that could provide 10,000 hp had become feasible. The stationary steam engine was a key component of the Industrial Revolution, allowing factories to locate where water power was unavailable. The atmospheric engines of Newcomen and Watt were large compared to the amount of power they produced, but high-pressure steam engines were light enough to be applied to vehicles such as traction engines and railway locomotives.\n\nReciprocating piston type steam engines remained the dominant source of power until the early 20th century, when advances in the design of electric motors and internal combustion engines gradually resulted in the replacement of reciprocating (piston) steam engines in commercial usage, and the ascendancy of steam turbines in power generation. Considering that the great majority of worldwide electric generation is produced by turbine type steam engines, the \"steam age\" is continuing with energy levels far beyond those of the turn of the 19th and 20th century.\n\nAlthough steam-powered devices were developed before the first practical piston steam engine, they were not directly connected to the Newcomen atmospheric engine. The Newcomen engine owes its development to the discovery of atmospheric pressure and to shared technical information whose path is traceable. For the development of the commercial steam engine see: History of the steam engine#Development of the commercial steam engine\n\n\nSince the early 18th century, steam power has been applied to a variety of practical uses. At first it powered reciprocating pumps, but from the 1780s rotative engines (those converting reciprocating motion into rotary motion) began to appear, driving factory machinery such as power looms. Speed control in response to changing load made direct application of a steam engine to spinning machinery impractical until the invention of the Corliss engine in 1848. Until then steam engines were used to pump water to turn a water wheel, which powered the spinning machinery. At the turn of the 19th century, steam-powered transport on both sea and land began to make its appearance, becoming more dominant as the century progressed.\nSteam engines can be said to have been the moving force behind the Industrial Revolution and saw widespread commercial use driving machinery in factories, mills and mines; powering pumping stations; and propelling transport appliances such as railway locomotives, ships, steamboats and road vehicles. Their use in agriculture led to an increase in the land available for cultivation. There have at one time or another been steam-powered farm tractors, motorcycles (without much success) and even automobiles as the Stanley Steamer.\n\nThe weight of boilers and condensers generally makes the power-to-weight ratio of a steam plant lower than for internal combustion engines. For mobile applications steam has been largely superseded by internal combustion engines or electric motors. However, most electric power is generated using steam turbine plant, so that indirectly the world's industry is still dependent on steam power. Recent concerns about fuel sources and pollution have incited a renewed interest in steam both as a component of cogeneration processes and as a prime mover. This is becoming known as the Advanced Steam movement.\n\nThe history of the steam engine stretches back as far as the first century; the first recorded rudimentary steam-powered \"engine\" being the aeolipile described by Hero of Alexandria, a mathematician and engineer in Roman Egypt. In the following centuries, the few steam-powered \"engines\" known were, like the aeolipile, essentially experimental devices used by inventors to demonstrate the properties of steam. A rudimentary steam turbine device was described by Taqi al-Din in Ottoman Egypt in 1551 and by Giovanni Branca in Italy in 1629. Jerónimo de Ayanz y Beaumont received patents in 1606 for fifty steam powered inventions, including a water pump for draining inundated mines. Denis Papin, a Huguenot refugee, did some useful work on the steam digester in 1679, and first used a piston to raise weights in 1690.\n\nThe first commercial steam-powered device was a water pump, developed in 1698 by Thomas Savery. It used condensing steam to create a vacuum which was used to raise water from below, then it used steam pressure to raise it higher. Small engines were effective though larger models were problematic. They proved only to have a limited lift height and were prone to boiler explosions. It received some use in mines, pumping stations and for supplying water wheels used to power textile machinery. An attractive feature of the Savery engine was its low cost. Bento de Moura Portugal introduced an ingenious improvement of Savery's construction \"to render it capable of working itself\", as described by John Smeaton in the Philosophical Transactions published in 1751. It continued to be manufactured until the late 18th century. One engine was still known to be operating in 1820.\n\nThe first commercially successful true engine, in that it could generate power and transmit it to a machine, was the atmospheric engine, invented by Thomas Newcomen around 1712. It was an improvement over Savery's steam pump, using a piston as proposed by Papin. Newcomen's engine was relatively inefficient, and in most cases was used for pumping water. It worked by creating a partial vacuum by condensing steam under a piston within a cylinder. It was employed for draining mine workings at depths hitherto impossible, and also for providing a reusable water supply for driving waterwheels at factories sited away from a suitable \"head\". Water that had passed over the wheel was pumped back up into a storage reservoir above the wheel.\n\nIn 1720 Jacob Leupold described a two-cylinder high-pressure steam engine. The invention was published in his major work \"Theatri Machinarum Hydraulicarum\". The engine used two heavy pistons to provide motion to a water pump. Each piston was raised by the steam pressure and returned to its original position by gravity. The two pistons shared a common four way rotary valve connected directly to a steam boiler.\nThe next major step occurred when James Watt developed (1763–1775) an improved version of Newcomen's engine, with a separate condenser. Boulton and Watt's early engines used half as much coal as John Smeaton's improved version of Newcomen's. Newcomen's and Watt's early engines were \"atmospheric\". They were powered by air pressure pushing a piston into the partial vacuum generated by condensing steam, instead of the pressure of expanding steam. The engine cylinders had to be large because the only usable force acting on them was due to atmospheric pressure.\n\nWatt proceeded to develop his engine further, modifying it to provide a rotary motion suitable for driving factory machinery. This enabled factories to be sited away from rivers, and further accelerated the pace of the Industrial Revolution.\n\nThe meaning of high pressure, together with an actual value above ambient, depends on the era in which the term was used. For early use of the term Van Reimsdijk refers to steam being at a sufficiently high pressure that it could be exhausted to atmosphere without reliance on a vacuum to enable it to perform useful work. Ewing states that Watt's condensing engines were known, at the time, as low pressure compared to high pressure, non-condensing engines of the same period.\n\nWatt's patent prevented others from making high pressure and compound engines. Shortly after Watt's patent expired in 1800, Richard Trevithick and, separately, Oliver Evans in 1801 introduced engines using high-pressure steam; Trevithick obtained his high-pressure engine patent in 1802, and Evans had made several working models before then. These were much more powerful for a given cylinder size than previous engines and could be made small enough for transport applications. Thereafter, technological developments and improvements in manufacturing techniques (partly brought about by the adoption of the steam engine as a power source) resulted in the design of more efficient engines that could be smaller, faster, or more powerful, depending on the intended application.\n\nThe Cornish engine was developed by Trevithick and others in the 1810s. It was a compound cycle engine that used high-pressure steam expansively, then condensed the low-pressure steam, making it relatively efficient. The Cornish engine had irregular motion and torque though the cycle, limiting it mainly to pumping. Cornish engines were used in mines and for water supply until the late 19th century.\n\nEarly builders of stationary steam engines considered that horizontal cylinders would be subject to excessive wear. Their engines were therefore arranged with the piston axis vertical. In time the horizontal arrangement became more popular, allowing compact, but powerful engines to be fitted in smaller spaces.\n\nThe acme of the horizontal engine was the Corliss steam engine, patented in 1849, which was a four-valve counter flow engine with separate steam admission and exhaust valves and automatic variable steam cutoff. When Corliss was given the Rumford Medal, the committee said that \"no one invention since Watt's time has so enhanced the efficiency of the steam engine\". In addition to using 30% less steam, it provided more uniform speed due to variable steam cut off, making it well suited to manufacturing, especially cotton spinning.\n\nThe first experimental road going steam powered vehicles were built in the late 18th century, but it was not until after Richard Trevithick had developed the use of high-pressure steam, around 1800, that mobile steam engines became a practical proposition. The first half of the 19th century saw great progress in steam vehicle design, and by the 1850s it was becoming viable to produce them on a commercial basis. This progress was dampened by legislation which limited or prohibited the use of steam powered vehicles on roads. Improvements in vehicle technology continued from the 1860s to the 1920s. Steam road vehicles were used for many applications. In the 20th century, the rapid development of internal combustion engine technology led to the demise of the steam engine as a source of propulsion of vehicles on a commercial basis, with relatively few remaining in use beyond the Second World War. Many of these vehicles were acquired by enthusiasts for preservation, and numerous examples are still in existence. In the 1960s the air pollution problems in California gave rise to a brief period of interest in developing and studying steam powered vehicles as a possible means of reducing the pollution. Apart from interest by steam enthusiasts, the occasional replica vehicle, and experimental technology no steam vehicles are in production at present.\n\nNear the end of the 19th century compound engines came into widespread use. Compound engines exhausted steam in to successively larger cylinders to accommodate the higher volumes at reduced pressures, giving improved efficiency. These stages were called expansions, with double- and triple-expansion engines being common, especially in shipping where efficiency was important to reduce the weight of coal carried. Steam engines remained the dominant source of power until the early 20th century, when advances in the design of the steam turbine, electric motors and internal combustion engines gradually resulted in the replacement of reciprocating (piston) steam engines, with shipping in the 20th-century relying upon the steam turbine.\n\nAs the development of steam engines progressed through the 18th century, various attempts were made to apply them to road and railway use. In 1784, William Murdoch, a Scottish inventor, built a prototype steam road locomotive. An early working model of a steam rail locomotive was designed and constructed by steamboat pioneer John Fitch in the United States probably during the 1780s or 1790s.\nHis steam locomotive used interior bladed wheels guided by rails or tracks.\n\nThe first full-scale working railway steam locomotive was built by Richard Trevithick in the United Kingdom and, on 21 February 1804, the world's first railway journey took place as Trevithick's unnamed steam locomotive hauled a train along the tramway from the Pen-y-darren ironworks, near Merthyr Tydfil to Abercynon in south Wales. The design incorporated a number of important innovations that included using high-pressure steam which reduced the weight of the engine and increased its efficiency. Trevithick visited the Newcastle area later in 1804 and the colliery railways in north-east England became the leading centre for experimentation and development of steam locomotives.\n\nTrevithick continued his own experiments using a trio of locomotives, concluding with the Catch Me Who Can in 1808. Only four years later, the successful twin-cylinder locomotive \"Salamanca\" by Matthew Murray was used by the edge railed rack and pinion Middleton Railway. In 1825 George Stephenson built the \"Locomotion\" for the Stockton and Darlington Railway. This was the first public steam railway in the world and then in 1829, he built \"The Rocket\" which was entered in and won the Rainhill Trials. The Liverpool and Manchester Railway opened in 1830 making exclusive use of steam power for both passenger and freight trains.\n\nSteam locomotives continued to be manufactured until the late twentieth century in places such as China and the former East Germany (where the DR Class 52.80 was produced).\n\nThe final major evolution of the steam engine design was the use of steam turbines starting in the late part of the 19th century. Steam turbines are generally more efficient than reciprocating piston type steam engines (for outputs above several hundred horsepower), have fewer moving parts, and provide rotary power directly instead of through a connecting rod system or similar means. Steam turbines virtually replaced reciprocating engines in electricity generating stations early in the 20th century, where their efficiency, higher speed appropriate to generator service, and smooth rotation were advantages. Today most electric power is provided by steam turbines. In the United States 90% of the electric power is produced in this way using a variety of heat sources. Steam turbines were extensively applied for propulsion of large ships throughout most of the 20th century.\n\nAlthough the reciprocating steam engine is no longer in widespread commercial use, various companies are exploring or exploiting the potential of the engine as an alternative to internal combustion engines. The company Energiprojekt AB in Sweden has made progress in using modern materials for harnessing the power of steam. The efficiency of Energiprojekt's steam engine reaches some 27-30% on high-pressure engines. It is a single-step, 5-cylinder engine (no compound) with superheated steam and consumes approx. of steam per kWh.\n\nThere are two fundamental components of a steam plant: the boiler or steam generator, and the \"motor unit\", referred to itself as a \"steam engine\". Stationary steam engines in fixed buildings may have the boiler and engine in separate buildings some distance apart. For portable or mobile use, such as steam locomotives, the two are mounted together.\n\nThe widely used reciprocating engine typically consisted of a cast iron cylinder, piston, connecting rod and beam or a crank and flywheel, and miscellaneous linkages. Steam was alternately supplied and exhausted by one or more valves. Speed control was either automatic, using a governor, or by a manual valve. The cylinder casting contained steam supply and exhaust ports.\n\nEngines equipped with a condenser are a separate type than those that exhaust to the atmosphere.\n\nOther components are often present; pumps (such as an injector) to supply water to the boiler during operation, condensers to recirculate the water and recover the latent heat of vaporisation, and superheaters to raise the temperature of the steam above its saturated vapour point, and various mechanisms to increase the draft for fireboxes. When coal is used, a chain or screw stoking mechanism and its drive engine or motor may be included to move the fuel from a supply bin (bunker) to the firebox. See: Mechanical stoker\n\nThe heat required for boiling the water and raising the temperature of the steam can be derived from various sources, most commonly from burning combustible materials with an appropriate supply of air in a closed space (called variously combustion chamber, firebox, furnace). In some cases the heat source is a nuclear reactor, geothermal energy, solar energy or waste heat from an internal combustion engine or industrial process. In the case of model or toy steam engines, the heat source can be an electric heating element.\n\nBoilers are pressure vessels that contain water to be boiled, and features that transfer the heat to the water as effectively as possible.\n\nThe two most common types are:\n\nFire tube boilers were the main type used for early high-pressure steam (typical steam locomotive practice), but they were to a large extent displaced by more economical water tube boilers in the late 19th century for marine propulsion and large stationary applications.\n\nMany boilers raise the temperature of the steam after it has left that part of the boiler where it is in contact with the water. Known as superheating it turns 'wet steam' into 'superheated steam'. It avoids the steam condensing in the engine cylinders, and gives a significantly higher efficiency.\n\nIn a steam engine, a piston or steam turbine or any other similar device for doing mechanical work takes a supply of steam at high pressure and temperature and gives out a supply of steam at lower pressure and temperature, using as much of the difference in steam energy as possible to do mechanical work.\n\nThese \"motor units\" are often called 'steam engines' in their own right. Engines using compressed air or other gases differ from steam engines only in details that depend on the nature of the gas although compressed air has been used in steam engines without change.\n\nAs with all heat engines, the majority of primary energy must be emitted as waste heat at relatively low temperature.\n\nThe simplest cold sink is to vent the steam to the environment. This is often used on steam locomotives, as the released steam is vented up the chimney so as to increase the draw on the fire, which greatly increases engine power, but reduces efficiency.\n\nSometimes the waste heat is useful itself, and in those cases very high overall efficiency can be obtained. For example, combined heat and power (CHP) systems use the waste steam for district heating, exceeding 80% combined efficiency.\n\nWhere CHP is not used, steam turbines in stationary power plants use surface condensers as a cold sink. The condensers are cooled by water flow from oceans, rivers, lakes, and often by cooling towers which evaporate water to provide cooling energy removal. The resulting condensed hot water, is then pumped back up to pressure and sent back to the boiler. A dry type cooling tower is similar to an automobile radiator and is used in locations where water is costly. Waste heat can also be ejected by evaporative (wet) cooling towers use pass the rejected to external water cycle that evaporates some of flow to the air. Cooling towers often have visible plumes due to the evaporated water condensing into droplets carried up by the warm air. Evaporative cooling towers need less water flow than \"once-through\" cooling by river or lake water; a 700 megawatt coal-fired power plant may use about 3600 cubic metres of make-up water every hour for evaporative cooling, but would need about twenty times as much if cooled by river water. Evaporative water cannot be used for subsequent purposes (other than rain somewhere), whereas river water can be re-used. In all cases, the steam plant water, which must be kept pure, is kept separate from the cooling water or air, and once the low-pressure steam condenses into water, it is returned to the boiler.\n\nThe Rankine cycle and most practical steam engines have a water pump to recycle or top up the boiler water, so that they may be run continuously. Utility and industrial boilers commonly use multi-stage centrifugal pumps; however, other types are used. Another means of supplying lower-pressure boiler feed water is an injector, which uses a steam jet usually supplied from the boiler. Injectors became popular in the 1850s but are no longer widely used, except in applications such as steam locomotives. It is the pressurization of the water that circulates through the steam boiler that allows the water to be raised to temperatures well above 100 °C boiling point of water at one atmospheric pressure, and by that means to increase the efficiency of the steam cycle.\n\nFor safety reasons, nearly all steam engines are equipped with mechanisms to monitor the boiler, such as a pressure gauge and a sight glass to monitor the water level.\n\nMany engines, stationary and mobile, are also fitted with a governor to regulate the speed of the engine without the need for human interference.\n\nThe most useful instrument for analyzing the performance of steam engines is the steam engine indicator. Early versions were in use by 1851, but the most successful indicator was developed for the high speed engine inventor and manufacturer Charles Porter by Charles Richard and exhibited at London Exhibition in 1862. The steam engine indicator traces on paper the pressure in the cylinder throughout the cycle, which can be used to spot various problems and calculate developed horsepower. It was routinely used by engineers, mechanics and insurance inspectors. The engine indicator can also be used on internal combustion engines. See image of indicator diagram below (in \"Types of motor units\" section).\n\nThe centrifugal governor was adopted by James Watt for use on a steam engine in 1788 after Watt's partner Boulton saw one on the equipment of a flour mill Boulton & Watt were building. The governor could not actually hold a set speed, because it would assume a new constant speed in response to load changes. The governor was able to handle smaller variations such as those caused by fluctuating heat load to the boiler. Also, there was a tendency for oscillation whenever there was a speed change. As a consequence, engines equipped only with this governor were not suitable for operations requiring constant speed, such as cotton spinning. The governor was improved over time and coupled with variable steam cut off, good speed control in response to changes in load was attainable near the end of the 19th century.\n\nIn a simple engine, or \"single expansion engine\" the charge of steam passes through the entire expansion process in an individual cylinder, although a simple engine may have one or more individual cylinders. It is then exhausted directly into the atmosphere or into a condenser. As steam expands in passing through a high-pressure engine, its temperature drops because no heat is being added to the system; this is known as adiabatic expansion and results in steam entering the cylinder at high temperature and leaving at lower temperature. This causes a cycle of heating and cooling of the cylinder with every stroke, which is a source of inefficiency.\n\nThe dominant efficiency loss in reciprocating steam engines is cylinder condensation and re-evaporation. The steam cylinder and adjacent metal parts/ports operate at a temperature about halfway between the steam admission saturation temperature and the saturation temperature corresponding to the exhaust pressure. As high pressure steam is admitted into the working cylinder, much of the high temperature steam is condensed as water droplets onto the metal surfaces, significantly reducing the steam available for expansive work. When the expanding steam reaches low pressure (especially during the exhaust stroke), the previously deposited water droplets that had just been formed within the cylinder/ports now boil away (re-evaporation) and this steam does no further work in the cylinder.\n\nThere are practical limits on the expansion ratio of a steam engine cylinder, as increasing cylinder surface area tends to exacerbate the cylinder condensation and re-evaporation issues. This negates the theoretical advantages associated with a high ratio of expansion in an individual cylinder.\n\nA method to lessen the magnitude of energy loss to a very long cylinder was invented in 1804 by British engineer Arthur Woolf, who patented his \"Woolf high-pressure compound engine\" in 1805. In the compound engine, high-pressure steam from the boiler expands in a high-pressure (HP) cylinder and then enters one or more subsequent lower-pressure (LP) cylinders. The complete expansion of the steam now occurs across multiple cylinders, with the overall temperature drop within each cylinder reduced considerably. By expanding the steam in steps with smaller temperature range (within each cylinder) the condensation and re-evaporation efficiency issue (described above) is reduced. This reduces the magnitude of cylinder heating and cooling, increasing the efficiency of the engine. By staging the expansion in multiple cylinders, variations of torque can be reduced. To derive equal work from lower-pressure cylinder requires a larger cylinder volume as this steam occupies a greater volume. Therefore, the bore, and in rare cases the stroke, are increased in low-pressure cylinders, resulting in larger cylinders.\n\nDouble-expansion (usually known as compound) engines expanded the steam in two stages. The pairs may be duplicated or the work of the large low-pressure cylinder can be split with one high-pressure cylinder exhausting into one or the other, giving a three-cylinder layout where cylinder and piston diameter are about the same, making the reciprocating masses easier to balance.\n\nTwo-cylinder compounds can be arranged as:\n\n\nWith two-cylinder compounds used in railway work, the pistons are connected to the cranks as with a two-cylinder simple at 90° out of phase with each other (\"quartered\"). When the double-expansion group is duplicated, producing a four-cylinder compound, the individual pistons within the group are usually balanced at 180°, the groups being set at 90° to each other. In one case (the first type of Vauclain compound), the pistons worked in the same phase driving a common crosshead and crank, again set at 90° as for a two-cylinder engine. With the three-cylinder compound arrangement, the LP cranks were either set at 90° with the HP one at 135° to the other two, or in some cases all three cranks were set at 120°.\n\nThe adoption of compounding was common for industrial units, for road engines and almost universal for marine engines after 1880; it was not universally popular in railway locomotives where it was often perceived as complicated. This is partly due to the harsh railway operating environment and limited space afforded by the loading gauge (particularly in Britain, where compounding was never common and not employed after 1930). However, although never in the majority, it was popular in many other countries.\n\nIt is a logical extension of the compound engine (described above) to split the expansion into yet more stages to increase efficiency. The result is the multiple-expansion engine. Such engines use either three or four expansion stages and are known as \"triple-\" and \"quadruple-expansion engines\" respectively. These engines use a series of cylinders of progressively increasing diameter. These cylinders are designed to divide the work into equal shares for each expansion stage. As with the double-expansion engine, if space is at a premium, then two smaller cylinders may be used for the low-pressure stage. Multiple-expansion engines typically had the cylinders arranged inline, but various other formations were used. In the late 19th century, the Yarrow-Schlick-Tweedy balancing \"system\" was used on some marine triple-expansion engines. Y-S-T engines divided the low-pressure expansion stages between two cylinders, one at each end of the engine. This allowed the crankshaft to be better balanced, resulting in a smoother, faster-responding engine which ran with less vibration. This made the four-cylinder triple-expansion engine popular with large passenger liners (such as the \"Olympic\" class), but this was ultimately replaced by the virtually vibration-free turbine engine. It is noted, however, that triple-expansion reciprocating steam engines were used to drive the World War II Liberty ships, by far the largest number of identical ships ever built. Over 2700 ships were built, in the United States, from a British original design. \n\nThe image to the right shows an animation of a triple-expansion engine. The steam travels through the engine from left to right. The valve chest for each of the cylinders is to the left of the corresponding cylinder.\n\nLand-based steam engines could exhaust their steam to atmosphere, as feed water was usually readily available. Prior to and during World War I, the expansion engine dominated marine applications, where high vessel speed was not essential. It was, however, superseded by the British invention steam turbine where speed was required, for instance in warships, such as the dreadnought battleships, and ocean liners. of 1905 was the first major warship to replace the proven technology of the reciprocating engine with the then-novel steam turbine.\n\nIn most reciprocating piston engines, the steam reverses its direction of flow at each stroke (counterflow), entering and exhausting from the same end of the cylinder. The complete engine cycle occupies one rotation of the crank and two piston strokes; the cycle also comprises four \"events\" – admission, expansion, exhaust, compression. These events are controlled by valves often working inside a \"steam chest\" adjacent to the cylinder; the valves distribute the steam by opening and closing steam \"ports\" communicating with the cylinder end(s) and are driven by valve gear, of which there are many types.\n\nThe simplest valve gears give events of fixed length during the engine cycle and often make the engine rotate in only one direction. Many however have a reversing mechanism which additionally can provide means for saving steam as speed and momentum are gained by gradually \"shortening the cutoff\" or rather, shortening the admission event; this in turn proportionately lengthens the expansion period. However, as one and the same valve usually controls both steam flows, a short cutoff at admission adversely affects the exhaust and compression periods which should ideally always be kept fairly constant; if the exhaust event is too brief, the totality of the exhaust steam cannot evacuate the cylinder, choking it and giving excessive compression (\"kick back\").\n\nIn the 1840s and 50s, there were attempts to overcome this problem by means of various patent valve gears with a separate, variable cutoff expansion valve riding on the back of the main slide valve; the latter usually had fixed or limited cutoff. The combined setup gave a fair approximation of the ideal events, at the expense of increased friction and wear, and the mechanism tended to be complicated. The usual compromise solution has been to provide \"lap\" by lengthening rubbing surfaces of the valve in such a way as to overlap the port on the admission side, with the effect that the exhaust side remains open for a longer period after cut-off on the admission side has occurred. This expedient has since been generally considered satisfactory for most purposes and makes possible the use of the simpler Stephenson, Joy and Walschaerts motions. Corliss, and later, poppet valve gears had separate admission and exhaust valves driven by trip mechanisms or cams profiled so as to give ideal events; most of these gears never succeeded outside of the stationary marketplace due to various other issues including leakage and more delicate mechanisms.\n\nBefore the exhaust phase is quite complete, the exhaust side of the valve closes, shutting a portion of the exhaust steam inside the cylinder. This determines the compression phase where a cushion of steam is formed against which the piston does work whilst its velocity is rapidly decreasing; it moreover obviates the pressure and temperature shock, which would otherwise be caused by the sudden admission of the high-pressure steam at the beginning of the following cycle.\n\nThe above effects are further enhanced by providing \"lead\": as was later discovered with the internal combustion engine, it has been found advantageous since the late 1830s to advance the admission phase, giving the valve \"lead\" so that admission occurs a little before the end of the exhaust stroke in order to fill the \"clearance volume\" comprising the ports and the cylinder ends (not part of the piston-swept volume) before the steam begins to exert effort on the piston.\n\nUniflow engines attempt to remedy the difficulties arising from the usual counterflow cycle where, during each stroke, the port and the cylinder walls will be cooled by the passing exhaust steam, whilst the hotter incoming admission steam will waste some of its energy in restoring working temperature. The aim of the uniflow is to remedy this defect and improve efficiency by providing an additional port uncovered by the piston at the end of each stroke making the steam flow only in one direction. By this means, the simple-expansion uniflow engine gives efficiency equivalent to that of classic compound systems with the added advantage of superior part-load performance, and comparable efficiency to turbines for smaller engines below one thousand horsepower. However, the thermal expansion gradient uniflow engines produce along the cylinder wall gives practical difficulties.. The Quasiturbine is a uniflow rotary steam engine where steam intakes in hot areas, while exhausting in cold areas.\n\nA steam turbine consists of one or more \"rotors\" (rotating discs) mounted on a drive shaft, alternating with a series of \"stators\" (static discs) fixed to the turbine casing. The rotors have a propeller-like arrangement of blades at the outer edge. Steam acts upon these blades, producing rotary motion. The stator consists of a similar, but fixed, series of blades that serve to redirect the steam flow onto the next rotor stage. A steam turbine often exhausts into a surface condenser that provides a vacuum. The stages of a steam turbine are typically arranged to extract the maximum potential work from a specific velocity and pressure of steam, giving rise to a series of variably sized high- and low-pressure stages. Turbines are only efficient if they rotate at relatively high speed, therefore they are usually connected to reduction gearing to drive lower speed applications, such as a ship's propeller. In the vast majority of large electric generating stations, turbines are directly connected to generators with no reduction gearing. Typical speeds are 3600 revolutions per minute (RPM) in the United States with 60 Hertz power, and 3000 RPM in Europe and other countries with 50 Hertz electric power systems. In nuclear power applications the turbines typically run at half these speeds, 1800 RPM and 1500 RPM. A turbine rotor is also only capable of providing power when rotating in one direction. Therefore, a reversing stage or gearbox is usually required where power is required in the opposite direction.\n\nSteam turbines provide direct rotational force and therefore do not require a linkage mechanism to convert reciprocating to rotary motion. Thus, they produce smoother rotational forces on the output shaft. This contributes to a lower maintenance requirement and less wear on the machinery they power than a comparable reciprocating engine.\n\nThe main use for steam turbines is in electricity generation (in the 1990s about 90% of the world's electric production was by use of steam turbines) however the recent widespread application of large gas turbine units and typical combined cycle power plants has resulted in reduction of this percentage to the 80% regime for steam turbines. In electricity production, the high speed of turbine rotation matches well with the speed of modern electric generators, which are typically direct connected to their driving turbines. In marine service, (pioneered on the \"Turbinia\"), steam turbines with reduction gearing (although the Turbinia has direct turbines to propellers with no reduction gearbox) dominated large ship propulsion throughout the late 20th century, being more efficient (and requiring far less maintenance) than reciprocating steam engines. In recent decades, reciprocating Diesel engines, and gas turbines, have almost entirely supplanted steam propulsion for marine applications.\n\nVirtually all nuclear power plants generate electricity by heating water to provide steam that drives a turbine connected to an electrical generator. Nuclear-powered ships and submarines either use a steam turbine directly for main propulsion, with generators providing auxiliary power, or else employ turbo-electric transmission, where the steam drives a turbo generator set with propulsion provided by electric motors. A limited number of steam turbine railroad locomotives were manufactured. Some non-condensing direct-drive locomotives did meet with some success for long haul freight operations in Sweden and for express passenger work in Britain, but were not repeated. Elsewhere, notably in the United States, more advanced designs with electric transmission were built experimentally, but not reproduced. It was found that steam turbines were not ideally suited to the railroad environment and these locomotives failed to oust the classic reciprocating steam unit in the way that modern diesel and electric traction has done.\n\nAn oscillating cylinder steam engine is a variant of the simple expansion steam engine which does not require valves to direct steam into and out of the cylinder. Instead of valves, the entire cylinder rocks, or oscillates, such that one or more holes in the cylinder line up with holes in a fixed port face or in the pivot mounting (trunnion). These engines are mainly used in toys and models, because of their simplicity, but have also been used in full size working engines, mainly on ships where their compactness is valued.\n\nIt is possible to use a mechanism based on a pistonless rotary engine such as the Wankel engine in place of the cylinders and valve gear of a conventional reciprocating steam engine. Many such engines have been designed, from the time of James Watt to the present day, but relatively few were actually built and even fewer went into quantity production; see link at bottom of article for more details. The major problem is the difficulty of sealing the rotors to make them steam-tight in the face of wear and thermal expansion; the resulting leakage made them very inefficient. Lack of expansive working, or any means of control of the cutoff, is also a serious problem with many such designs.\n\nBy the 1840s, it was clear that the concept had inherent problems and rotary engines were treated with some derision in the technical press. However, the arrival of electricity on the scene, and the obvious advantages of driving a dynamo directly from a high-speed engine, led to something of a revival in interest in the 1880s and 1890s, and a few designs had some limited success.. The Quasiturbine is a new type of uniflow rotary steam engine.\n\nOf the few designs that were manufactured in quantity, those of the Hult Brothers Rotary Steam Engine Company of Stockholm, Sweden, and the spherical engine of Beauchamp Tower are notable. Tower's engines were used by the Great Eastern Railway to drive lighting dynamos on their locomotives, and by the Admiralty for driving dynamos on board the ships of the Royal Navy. They were eventually replaced in these niche applications by steam turbines.\n\nThe aeolipile represents the use of steam by the rocket-reaction principle, although not for direct propulsion.\n\nIn more modern times there has been limited use of steam for rocketry – particularly for rocket cars. Steam rocketry works by filling a pressure vessel with hot water at high pressure and opening a valve leading to a suitable nozzle. The drop in pressure immediately boils some of the water and the steam leaves through a nozzle, creating a propulsive force.\n\nSteam engines possess boilers and other components that are pressure vessels that contain a great deal of potential energy. Steam escapes and boiler explosions (typically BLEVEs) can and have in the past caused great loss of life. While variations in standards may exist in different countries, stringent legal, testing, training, care with manufacture, operation and certification is applied to ensure safety.\n\nFailure modes may include:\n\nSteam engines frequently possess two independent mechanisms for ensuring that the pressure in the boiler does not go too high; one may be adjusted by the user, the second is typically designed as an ultimate fail-safe. Such safety valves traditionally used a simple lever to restrain a plug valve in the top of a boiler. One end of the lever carried a weight or spring that restrained the valve against steam pressure. Early valves could be adjusted by engine drivers, leading to many accidents when a driver fastened the valve down to allow greater steam pressure and more power from the engine. The more recent type of safety valve uses an adjustable spring-loaded valve, which is locked such that operators may not tamper with its adjustment unless a seal illegally is broken. This arrangement is considerably safer. \n\nLead fusible plugs may be present in the crown of the boiler's firebox. If the water level drops, such that the temperature of the firebox crown increases significantly, the lead melts and the steam escapes, warning the operators, who may then manually suppress the fire. Except in the smallest of boilers the steam escape has little effect on dampening the fire. The plugs are also too small in area to lower steam pressure significantly, depressurizing the boiler. If they were any larger, the volume of escaping steam would itself endanger the crew.\n\nThe Rankine cycle is the fundamental thermodynamic underpinning of the steam engine. The cycle is an arrangement of components as is typically used for simple power production, and utilizes the phase change of water (boiling water producing steam, condensing exhaust steam, producing liquid water)) to provide a practical heat/power conversion system. The heat is supplied externally to a closed loop with some of the heat added being converted to work and the waste heat being removed in a condenser. The Rankine cycle is used in virtually all steam power production applications. In the 1990s, Rankine steam cycles generated about 90% of all electric power used throughout the world, including virtually all solar, biomass, coal and nuclear power plants. It is named after William John Macquorn Rankine, a Scottish polymath.\n\nThe Rankine cycle is sometimes referred to as a practical Carnot cycle because, when an efficient turbine is used, the TS diagram begins to resemble the Carnot cycle. The main difference is that heat addition (in the boiler) and rejection (in the condenser) are isobaric (constant pressure) processes in the Rankine cycle and isothermal (constant temperature) processes in the theoretical Carnot cycle. In this cycle a pump is used to pressurize the working fluid which is received from the condenser as a liquid not as a gas. Pumping the working fluid in liquid form during the cycle requires a small fraction of the energy to transport it compared to the energy needed to compress the working fluid in gaseous form in a compressor (as in the Carnot cycle). The cycle of a reciprocating steam engine differs from that of turbines because of condensation and re-evaporation occurring in the cylinder or in the steam inlet passages.\n\nThe working fluid in a Rankine cycle can operate as a closed loop system, where the working fluid is recycled continuously, or may be an \"open loop\" system, where the exhaust steam is directly released to the atmosphere, and a separate source of water feeding the boiler is supplied. Normally water is the fluid of choice due to its favourable properties, such as non-toxic and unreactive chemistry, abundance, low cost, and its thermodynamic properties. Mercury is the working fluid in the mercury vapor turbine. Low boiling hydrocarbons can be used in a binary cycle.\n\nThe steam engine contributed much to the development of thermodynamic theory; however, the only applications of scientific theory that influenced the steam engine were the original concepts of harnessing the power of steam and atmospheric pressure and knowledge of properties of heat and steam. The experimental measurements made by Watt on a model steam engine led to the development of the separate condenser. Watt independently discovered latent heat, which was confirmed by the original discoverer Joseph Black, who also advised Watt on experimental procedures. Watt was also aware of the change in the boiling point of water with pressure. Otherwise, the improvements to the engine itself were more mechanical in nature. The thermodynamic concepts of the Rankine cycle did give engineers the understanding needed to calculate efficiency which aided the development of modern high-pressure and -temperature boilers and the steam turbine.\n\nThe efficiency of an engine cycle can be calculated by dividing the energy output of mechanical work that the engine produces by the energy input to the engine by the burning fuel.\n\nThe historical measure of a steam engine's energy efficiency was its \"duty\". The concept of duty was first introduced by Watt in order to illustrate how much more efficient his engines were over the earlier Newcomen designs. Duty is the number of foot-pounds of work delivered by burning one bushel (94 pounds) of coal. The best examples of Newcomen designs had a duty of about 7 million, but most were closer to 5 million. Watt's original low-pressure designs were able to deliver duty as high as 25 million, but averaged about 17. This was a three-fold improvement over the average Newcomen design. Early Watt engines equipped with high-pressure steam improved this to 65 million.\n\nNo heat engine can be more efficient than the Carnot cycle, in which heat is moved from a high temperature reservoir to one at a low temperature, and the efficiency depends on the temperature difference. For the greatest efficiency, steam engines should be operated at the highest steam temperature possible (superheated steam), and release the waste heat at the lowest temperature possible.\n\nThe efficiency of a Rankine cycle is usually limited by the working fluid. Without the pressure reaching supercritical levels for the working fluid, the temperature range the cycle can operate over is quite small; in steam turbines, turbine entry temperatures are typically 565 °C (the creep limit of stainless steel) and condenser temperatures are around 30 °C. This gives a theoretical Carnot efficiency of about 63% compared with an actual efficiency of 42% for a modern coal-fired power station. This low turbine entry temperature (compared with a gas turbine) is why the Rankine cycle is often used as a bottoming cycle in combined-cycle gas turbine power stations.\n\nOne of the principal advantages the Rankine cycle holds over others is that during the compression stage relatively little work is required to drive the pump, the working fluid being in its liquid phase at this point. By condensing the fluid, the work required by the pump consumes only 1% to 3% of the turbine (or reciprocating engine)power and contributes to a much higher efficiency for a real cycle. The benefit of this is lost somewhat due to the lower heat addition temperature. Gas turbines, for instance, have turbine entry temperatures approaching 1500 °C. Nonetheless, the efficiencies of actual large steam cycles and large modern gas turbines are fairly well matched.\n\nIn practice, a reciprocating steam engine cycle exhausting the steam to atmosphere will typically have an efficiency (including the boiler) in the range of 1-10%, but with the addition of a condenser and multiple expansion, and high steam pressure/temperature, it may be greatly improved, historically into the regime of 10-20%, and very rarely slightly higher.\n\nA modern large electrical power station (producing several hundred megawatts of electrical output) with steam reheat, economizer etc. will achieve efficiency in the mid 40% range, with the most efficient units approaching 50% thermal efficiency.\n\nIt is also possible to capture the waste heat using cogeneration in which the waste heat is used for heating a lower boiling point working fluid or as a heat source for district heating via saturated low-pressure steam.\n\n\n"}
{"id": "19207502", "url": "https://en.wikipedia.org/wiki?curid=19207502", "title": "Sustainable growth rate", "text": "Sustainable growth rate\n\nAccording to PIMS (profit impact of marketing strategy), an important lever of business success is growth. Among 37 variables, growth is mentioned as one of the most important variables for success: market share, market growth, marketing expense to sales ratio or a strong market position. \n\nThe question how much growth is sustainable is answered by two concepts with different perspectives:\n\n\nThe sustainable growth rate according to Robert C. Higgins is the maximum growth rate a company can achieve consistent with the firm`s established financial policy. Basically, it is calculated as: \n\nSGR = (pm*(1-d)*(1+L)) / (T-(pm*(1-d)*(1+L))) \n\nIn order to grow faster, the company would have to invest more equity capital, increase its financial leverage or increase the target profit margin.\n\nThe sustainable growth rate model assumes several simplifications such as depreciation is sufficient to maintain the value of existing assets, the profit margin remains stable (also for new businesses), the proportion of assets and sales remains stable (also for new businesses) and the company maintains its current capital structure and dividend payout policy.\nThe sustainable growth rate model has implications for valuation models, as for instance the Gordon model and other discounted cash flow models require a growth estimate that can be sustained for many years. The sustainable growth rate can be a check if business plans are reasonable.\n\nOptimal Growth according to Martin Handschuh, Hannes Lösch and Björn Heyden is the growth rate which assures sustainable company development – considering the long-term relationship between revenue growth, total shareholder value creation and profitability. \nAssessment basis: The work is based on assessments on the performance of more than 3500 stock-listed companies with an initial revenue of greater 250 million Euro globally and across industries over a period of 12 years from 1997 till 2009. Due to this long time period, the authors consider their findings as to a large extent independent of specific economic cycles.\n\nIn the long-term and across industries, total shareholder value creation (stock price development plus dividend payments) rises steadily with increasing revenue growth rates. The more long-term revenue growth companies realize, the more investors appreciate this and the more they get rewarded.\n\nReturn on assets (ROA), return on sales (ROS) and return on equity (ROE) do rise with increasing revenue growth up to 10 to 25% and then fall with further increasing revenue growth rates.\n\nAlso the combined ROX-index (average of ROA, ROS and ROE) shows rises with increasing growth rates to a broad maximum in the range of 10 to 25% revenue growth per year and falls towards higher growth rates. \n\nThe authors attribute the continuous profitability increase towards the maximum of two effects:\n\n\nBeyond the profitability maximum extra efforts to handle additional growth – e.g. based on integrating new staff in large dimensions and handling culture and quality - do rise sharply and reduce overall profitability. \n\nThe combination of the patterns of revenue growth, total shareholder value creation and profitability indicates three growth zones:\n\n\nGrowth rates of the assessed companies are widely independent of initial company size/ market share which is in alignment with Gibrat's law. Gibrat's law, sometimes called Gibrat's rule of proportionate growth is a rule defined by Robert Gibrat (1904–1980) stating that the size of a firm and its growth rate are independent.\nIndependent of industry consolidation and industry growth rate, companies in many industries with growth rates in the range of 10 to 25% revenue growth p.a. have both, higher total shareholder value generation as well as profitability than their slower growing peers.\n\nThese findings do suggest two base strategies for companies:\n\n\nThe authors have identified a set of preconditions and levers to achieve long-term growth in their defined sweet-spot and beyond:\n\n\n\nAs described the sustainable growth rate (SGR) concept by Robert C. Higgins is based on several assumptions such as constant profit margin, constant debt to equity ratio or constant asset to sales ratio. Therefore, general applicability of SGR concept in cases where these parameters are not stable is limited.\n\nThe Optimal Growth concept by Martin Handschuh, Hannes Lösch, Björn Heyden et al. has no restrictions to certain strategies or business model and is therefore more flexible in its applicability. However, as a broad framework, it only provides an orientation for case/company specific mid- to long-term growth target setting. Additional company and market specific considerations, e.g. market growth, growth culture, appetite for change, are required to come up with the optimal growth rate of a specific company. \n\nAdditionally, considering the increasing criticism of excessive growth and shareholder value orientation by philosophers, economists and also managers, e.g. Stéphane Hessel, Kenneth Boulding, Jack Welch (nowadays), one might expect that investors' investment criteria might also change in the future. This may lead to changes in the relationship of revenue growth rates and total shareholder value creation. Regular reviews of the optimal growth assessments may be used as an indicator for the development of stock markets` appetite for rapid growth.\n\n"}
{"id": "13352153", "url": "https://en.wikipedia.org/wiki?curid=13352153", "title": "Tehuacán Valley matorral", "text": "Tehuacán Valley matorral\n\nThe Tehuacán Valley matorral is a xeric shrubland ecoregion, of the deserts and xeric shrublands biome, located in eastern Central Mexico.\n\nMatorral is a Spanish word, along with \"tomillares\", for shrubland, thicket or bushes. The term is used alone for a Mediterranean climate ecosystem in Southern Europe.\n\nThe Tehuacán Valley matorral ecoregion occupies the Tehuacán Valley and Cuicatlán Valley, covering parts of the states of Puebla and Oaxaca. The valleys lie in the rain shadow of the surrounding mountain ranges, and are drier than the surrounding ecoregions.\n\nThe Tehuacán Valley matorral is bounded by the Trans-Mexican Volcanic Belt pine-oak forests to the northwest, north, and northeast, the Sierra Madre de Oaxaca pine-oak forests to the east, and by the Balsas dry forests to the southeast, south, and southwest.\n\nThe Tehuacán Valley matorral is a center of plant diversity, with over 2700 species, of which approximately 30% are endemic. It is a center of diversity for species of \"Agave\", \"Hechtia\", \"Salvia\", and cactus.\n\nThe ecoregion has a number of distinct plant communities.\n\nThe ecoregion is notable for its diversity of birds and bats. Of the 90 species of birds present, 10 are endemic, including the ocellated thrasher (\"Toxostoma ocellatum\"), and bridled sparrow (\"Aimorphilla mystacalis\"). 34 species of bats inhabit the ecoregion, of which 18 are endangered, vulnerable, or rare.\n\n\n\n"}
{"id": "30876908", "url": "https://en.wikipedia.org/wiki?curid=30876908", "title": "Thermal ionization", "text": "Thermal ionization\n\nThermal ionization, also known as surface ionization or contact ionization, is a physical process whereby the atoms are desorbed from a hot surface, and in the process are spontaneously ionized.\n\nThermal ionization is used to make simple ion sources, for mass spectrometry and for generating ion beams. Thermal ionization has seen extensive use in determining atomic weights, in addition to being used in many geological/nuclear applications \n\nThe likelihood of ionization is a function of the filament temperature, the work function of the filament substrate and the ionization energy of the element.\n\nThis is summarised in the Saha-Langmuir equation:\n\nwhere\n\nNegative ionization can also occur for elements with a large electron affinity formula_9 against a surface of low work function.\n\nOne application of thermal ionization is thermal ionization mass spectrometry (TIMS). In thermal ionization mass spectrometry, a chemically purified material is placed onto a filament which is then heated to high temperatures to cause some of the material to be ionized as it is thermally desorbed (boiled off) the hot filament. Filaments are generally flat pieces of metal around 1-2mm wide, 0.1mm thick, bent into an upside-down U shape and attached to two contacts that supply a current.\n\nThis method is widely used in radiometric dating, where the sample is ionized under vacuum. The ions being produced at the filament are focussed into an ion beam and then passed through a magnetic field to separate them by mass. The relative abundances of different isotopes can then be measured, yielding isotope ratios.\n\nWhen these isotope ratios are measured by TIMS, mass-dependent fractionation occurs as species are emitted by the hot filament. Fractionation occurs due to the excitation of the sample and therefore must be corrected for accurate measurement of the isotope ratio.\n\nThere are several advantages of the TIMS method. It has a simple design, is less expensive than other mass spectrometers, and produces stable ion emissions. It requires a stable power supply, and is suitable for species with a low ionization energy, such as strontium and lead.\n\nThe disadvantages of this method stem from the maximum temperature achieved in thermal ionization. The hot filament reaches a temperature of less than 2500 °C, leading to the inability to create atomic ions of species with a high ionization energy, such as osmium and tungsten. Although the TIMS method can create molecular ions instead in this case, species with high ionization energy can be analyzed more effectively with MC-ICP-MS.\n\n"}
{"id": "243447", "url": "https://en.wikipedia.org/wiki?curid=243447", "title": "Transit (astronomy)", "text": "Transit (astronomy)\n\nIn astronomy, a transit (or astronomical transit) is a phenomenon when a celestial body passes directly between a larger body and the observer. As viewed from a particular vantage point, the transiting body appears to move across the face of the larger body, covering a small portion of it.\n\nThe word \"transit\" refers to cases where the nearer object appears considerably smaller than the more distant object. Cases where the nearer object appears larger and completely hides the more distant object are known as \"occultations\". the probability of a seeing a transiting planet is low, however. The alignment of the two objects must be such that the observer is also in alignment. From transiting many parameters can be determined by about a planet. \n\nTransiting is the method used to find exoplanets. As the exoplanet move in front of its host stars there is a dimming in the luminosity of its host star that can be measured. Larger planets tends to make the dip in luminosity more noticeable and easier to detect. \n\nOne example of a transit involves the motion of a planet between a terrestrial observer and the Sun. This can happen only with inferior planets, namely Mercury and Venus (see transit of Mercury and transit of Venus). However, as seen from outer planets such as Mars, the Earth itself transits the Sun on occasion.\n\nThe term can also be used to describe the motion of a satellite across its parent planet, for instance one of the Galilean satellites (Io, Europa, Ganymede, Callisto) across Jupiter, as seen from Earth.\n\nA transit requires three bodies to be lined up in a single line. More rare are cases where four bodies are lined up. The one closest to the present occurred on 27 June 1586, when Mercury transited the Sun as seen from Venus at the same time as a transit of Mercury from Saturn and a transit of Venus from Saturn. \n\nIn recent years the discovery of extrasolar planets has excited interest in the possibility of detecting their transits across their own stellar primaries. HD 209458b is the first such transiting planet to be discovered.\n\nIn rare cases, one planet can pass in front of another. If the nearer planet appears smaller than the more distant one, the event is called a \"mutual planetary transit\".\n\nDuring a transit there are four \"contacts\", when the circumference of the small circle (small body disk) touches the circumference of the large circle (large body disk) at a single point. Historically, measuring the precise time of each point of contact was one of the most accurate ways to determine the positions of astronomical bodies. The contacts happen in the following order:\n\n\nA fifth named point is that of greatest transit, when the apparent centers of the two bodies are nearest to each other, halfway through the transit.\n\nNo missions were planned to coincide with the transit of Earth visible from Mars on 11 May 1984 and the Viking missions had been terminated a year previously. Consequently, the next opportunity to observe such an alignment will be in 2084.\n\nOn December 21, 2012, the \"Cassini–Huygens\" probe, in orbit around Saturn, observed the planet Venus transiting the Sun.\n\nOn 3 June 2014, the Mars rover \"Curiosity\" observed the planet Mercury transiting the Sun, marking the first time a planetary transit has been observed from a celestial body besides Earth.\n\n\n"}
{"id": "37443695", "url": "https://en.wikipedia.org/wiki?curid=37443695", "title": "Volunteer Training Corps", "text": "Volunteer Training Corps\n\nThe Volunteer Training Corps was a voluntary home defence militia in the United Kingdom during World War I.\n\nAfter war had been declared in August 1914, there was a popular demand for a means of service for those men who were over military age or those with business or family commitments which made it difficult for them to volunteer for the armed services. At this stage in the war, Britain relied entirely on a voluntary system of enlistment and many men still held to the Victorian principle that it was the task of professional troops to fight a war whilst voluntary militias provided for home defence, and civilian local defence groups began to spring up spontaneously as soon as war was declared.\n\nThe volunteer movement gained publicity from discourse in the press advocating civilian participation in home defence, with notable proponents being Arthur Conan Doyle and H. G. Wells. The first elements of central organisation were established by the formation of the London Volunteer Defence Force. Discussions about the nature and role of the movement ranged from simply drilling volunteers in preparation for their enlistment into the regular or home armies, through augmenting the home army's defence of vulnerable points, to providing a force that would actively oppose an invasion with guerrilla warfare. Concerned that such a body would undermine recruitment into the regular army and hinder more than help home defence, the War Office banned the movement.\n\nDespite official antipathy, civilians continued to organise themselves, and Harold Tennant, Under-Secretary of State for War, realised that the government could do little to prevent them. Rather than allow the movement to grow unchecked, he decided in September to allow the Central Committee of the London Volunteer Defence Force to continue. Until the War Office had the time and resources to devote to the movement itself, the Central Committee, adopting the name Central Association of Volunteer Training Corps (VTC), became the body to which individual corps could affiliate, and was responsible for drawing up the rules and regulations on a national basis. Lord Desborough became the President of the Association and General Sir O'Moore Creagh VC was appointed the Military Advisor. In November, the association was officially recognised as the administrative body of the VTC and formally subjected to conditions which prevented interference with recruitment into the regular army, barred members from holding military rank or wearing uniforms other than an armband and denied any state funding.\n\nEx-military personnel of the National Reserve played a leading role in the growth of the VTC, providing many of its recruits and lending the nascent organisation an element of martial respectability. Among the many new corps formed were the United Arts Rifles – which numbered in its ranks the Poet Laureate, Robert Bridges – a unit of deaf mutes which drilled by sign language, and a unit that went by the name of the Ju Jitsu VTC. In May 1915, corps began to be organised into county regiments. Some 2,000 individual corps had appeared by June 1915, numbering 590,000 volunteers. Units raised finances for the purchase of weapons by charging membership fees. Amid concerns that they would compete with the established forces for the limited amount of rifles then available – in October 1915, there were 570,600 in the country for the 1.3 million men who needed them – the government prohibited volunteers from buying service rifles and required any purchase to be first cleared with the local military authority. Those corps which could not afford weapons begged or borrowed wherever they could, and dummy rifles, air guns and weapons loaned by the Church Lads' Brigade were among those pressed into service by various units.\n\nDemand for the services of the VTC increased, and members were employed as guards by the Admiralty on the Scilly Isles, at the many new munitions works and on the rail network. Volunteers also dug trenches around London and assisted in bringing in the harvest. The movement grew out of the same spirit of volunteer service that gave birth to the Volunteer Force in the second half of the previous century, and a private member's bill introduced in the House of Lords in October 1915 sought to revive the Volunteer Act of 1863 as an attempt to place the VTC on a more official footing. It was supported by General Sir Horace Smith-Dorrien, commander of the Central Force First Army, who, in a letter to \"The Times\", wrote of \"the most valuable aid the VTC are giving me\". The bill failed due to government fears that it would complicate the home rule issue in Ireland by recognising the Ulster Volunteers and the Irish Volunteers. The VTC officially remained unrecognised and outside of the nation's home defence scheme, thus depriving members of legal protection in the performance of their duties. There was some doubt that the armband would be recognised by the enemy as uniform, leaving members vulnerable to execution as \"francs-tireurs\", and when it was suggested that the VTC might guard prisoners of war, it was pointed out that, technically, a volunteer could be hanged for murder if he shot an escapee.\n\nWhen it was discovered that the Volunteer Act 1863 had never been repealed, it was used in April 1916 to legitimise the movement. VTC Battalions legally became Volunteer Regiments of the new 'Volunteer Force'. Eventually they were allowed to wear khaki uniforms and equipment began to be officially supplied. In July 1918, the War Office decided to include the VTC Battalions into the County Infantry Regiment system, and they became numbered \"Volunteer\" battalions of their local regiment. With the introduction of conscription in 1916, came the power of the Military Service Tribunals to order men to join the VTC; however, the clause in the 1863 act which allowed resignation after fourteen days' notice initially made this unenforceable, so a Volunteer Act 1916 was passed which obliged members to remain in the Corps until the end of the war. By February 1918, there were 285,000 Volunteers, 101,000 of whom had been directed to the Corps by the Tribunals.\n\nDuring 1917, P.14 Enfield Rifles began to be issued, followed by Hotchkiss Mk I machine guns. The Corps trained in drill and, if the equipment was available, use of the rifle. In case of a German invasion, battalions were tasked with roles such as line of communication defence and forming the garrison of major towns; 42 battalions were to defend London. Volunteers undertook a wide range of other tasks including; guarding vulnerable points, munitions handling, digging anti-invasion defence lines, assisting with harvesting, fire fighting and transport for wounded soldiers. In north Worcestershire some units helped to man anti-aircraft guns ringing Birmingham. In 1918, when there was an acute shortage of manpower because of the German spring offensive, c.7,000 Volunteers undertook three-month coast defence duties in East Anglia. The force was sometimes ridiculed by the public; there were jokes that the \"GR\" on their armbands stood for \"George's Wrecks\", \"Grandpa's Regiment\", \"Genuine Relics\", \"Gorgeous Wrecks\" or \"Government Rejects\".\n\nThe only time that Volunteer Training Corps men were engaged in actual combat, was in the Easter Rising in Dublin starting on Easter Monday, 24 April 1916. Some 120 members of the 1st (Dublin) Battalion, Associated Volunteer Training Corps were returning from field exercises at Ticknock, when they heard news of the uprising. The commanding officer, Major Harris, decided to march to Beggars Bush Barracks. They carried rifles but were without ammunition or bayonets. They were fired on by a party of Irish Volunteers from a railway bridge. Part of the VTC force entered the barracks by the front gate, others made their way to the rear and scaled the wall. About 40 men at the rear of the column were pinned down by fire from surrounding houses and four were killed, including the first-class cricketer, Francis Browning, who had been second-in-command. The VTC men then assisted the small garrison of regular soldiers to hold the barracks for eight days. In total, five members of the battalion were killed and seven wounded.\n\nThe Volunteer Training Corps was suspended in December 1918, and officially disbanded in January 1920, with the exception of the Volunteer Motor Corps which was retained until April 1921 in case of civil disorder.\n\n\n\n"}
{"id": "2202225", "url": "https://en.wikipedia.org/wiki?curid=2202225", "title": "World government in fiction", "text": "World government in fiction\n\nIn both science fiction and utopia/dystopian fiction, authors have made frequent use of the age-old idea of a global state and, accordingly, of world government.\n\nIn tune with Immanuel Kant's vision of a world state based on the voluntary political union of all countries of this planet in order to avoid colonialism and in particular any future war (\"Idee zu einer allgemeinen Geschichte in weltbürgerlicher Absicht\", 1784; \"Zum ewigen Frieden\", 1795), some of these scenarios depict an egalitarian and utopian world supervised (rather than controlled) by a benevolent (and usually democratic) world government. Others, however, describe the effects of a totalitarian regime which, after having seized power in one country, annexes the rest of the world in order to dominate and oppress all mankind.\n\nOne major influence was Edward Bellamy's \"Looking Backward\". The best-known advocate of world government was H. G. Wells. He describes such a system in \"The Shape of Things to Come\", \"Men Like Gods\" and \"The World Set Free\".\n\nSome writers have also parodied the idea: E. M. Forster's \"The Machine Stops\" (1909) and Aldous Huxley's 1932 novel \"Brave New World\". Wells himself wrote \"The Sleeper Awakes\", an early vision of a dystopian world.\n\nWorld government themes in science fiction are particularly prominent in the years following World War II, coincident with the involvement of many scientists in the actual political movement for world government in response to the perceived dangers of nuclear holocaust. Prominent examples from the Cold War era include \"Childhood's End\" (1953), \"Starship Troopers\" (1959), \"\" (from 1966), the \"Doctor Who\" story \"The Enemy of the World\" (1968) and Captain Scarlet and the Mysterons (1968) Later references to a unified world government also appear however in post-Cold War science fiction television series such as \"Babylon 5\".\n\nThe concept also appears frequently in science fiction anime, whether in the form of a strengthened United Nations or an entirely new organizations with world presidential election. Examples of anime with this premise are \"Macross\" (adapted in America as the first part of Robotech) and \"Gundam\".\n\nPresident of Earth (also known as President of the World) is a fictional concept or character who is the leader of Planet Earth. Examples include the following:\n\n\n\n"}
