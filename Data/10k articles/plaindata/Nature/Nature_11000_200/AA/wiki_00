{"id": "38757924", "url": "https://en.wikipedia.org/wiki?curid=38757924", "title": "1490 Ch'ing-yang event", "text": "1490 Ch'ing-yang event\n\nThe Ch'ing-yang event of 1490 (also Ch'ing-yang, Chi-ing-yang or Chíing-yang meteor shower) is a presumed meteor shower or air burst in Qìngyáng (Ch'ing-Yang, ) in March or April 1490. The area was in the district of Shaanxi (陕西, now part of Gansu 甘肃 Province). If a meteor shower did occur, it may have been the result of the disintegration of an asteroid during an atmospheric entry air burst.\n\nA large number of deaths were recorded in historical Chinese accounts of the meteor shower, but have not been confirmed by researchers in the modern era. In the same year, Asian astronomers coincidentally discovered comet C/1490 Y1, a possible progenitor of the Quadrantid meteor showers.\n\nAt least three surviving Chinese historical records describe a shower during which \"stones fell like rain\", killing more than 10,000 people. At least one report of the event is found in the official History of the Ming Dynasty, and other journal records which describe the event are also generally considered reliable. But the official Ming Dynasty history omits the number of casualties, which has been frequently either doubted or discounted by present-day researchers.\n\nDue to the paucity of detailed information and the lack of surviving meteorites or other physical evidence, researchers have also been unable to definitively state the exact nature of the dramatic event, even examining the possible occurrence of severe hail. However Kevin Yau \"et al.\" of NASA's Jet Propulsion Laboratory did note several similarities of the Ch'ing-yang meteor fall to the Tunguska event, which would have destroyed a highly populous district.\n\nOne surviving account records:\nOne source of Chinese astronomical information of celestial events, the \"Zhongguo gudai tianxiang jilu zongji\" (Complete collection of records of celestial phenomena in ancient China) records ten works that discuss the March–April 1490 event, including the official History of the Ming Dynasty. Additionally there are records of it in local gazettes and histories of the region. The \"History of Ming\" work (the 明史, or Míng Shǐ) states only that there was a rain of uncountable stones of various sizes. The large objects were as big \"as a goose egg, and the small ones were the size of the fruit of an aquatic plant\". The date given was the third lunar month of 1490, which translates as March 21 to April 19, 1490.\n\nAlthough the Ming Dynasty history did not record the number of deaths or injuries, other sources do. One semi-official document recorded that a provincial Shaanxi official reported there had been a rain of stones that weighed as much as 4–5 jin (市斤), down to 2–3 jin (modern jins being 605 grams). The number of deaths in this account was in the several tens of thousands. The semi-official report and two others date the event to the second lunar month with one source dating the event as April 4, 1490, part of the 3rd lunar month.\n\nIn 2007 astronomers determined that the annual January Quadrantid meteor shower may have originated with the disintegration of Comet C/1490 Y1, approximately a century after it was first identified in 1490 by Chinese, Japanese, and Korean astronomers. A connection with asteroid (196256) 2003 EH1 has also been suggested. The large difference between the timing of the January meteor showers and the 1490 Ch'ing-yang event—which occurred in March or April 1490 CE—makes a relationship between the comet and the Ch'ing-yang event appear unlikely.\n\n"}
{"id": "2653438", "url": "https://en.wikipedia.org/wiki?curid=2653438", "title": "A Practical Handbook of British Beetles", "text": "A Practical Handbook of British Beetles\n\nA Practical Handbook of British Beetles is a two-volume work on the British beetle fauna, by Norman H. Joy, first published by H. F. & G. Witherby in January 1932. \n\nVolume one (xxviii + 622 pages) consists of the text (largely a set of identification keys, with brief status notes for each species). Volume two (194 pages) contains 2040 line-drawings of whole beetles and features referred to in the keys (390 of these were taken from Spry and Shuckard's 1840 publication \"The British Coleoptera Delineated\" but the remainder were drawn by Joy). \n\nThe book covers a fauna of about 3560 different species and has an emphasis on species identification, being \"essentially a manual of identification for the use of collectors.\"\n\nA reduced-size reprint was produced by E. W. Classey in 1976, and again in 1997, while Pisces Conservation released an electronic version in 2009, solving a longstanding problem of availability.\n\nOne of the main points of attraction for Joy's book was its reasonable price. According to a 1932 review in Nature, William Weekes Fowler's standard work \"The Coleoptera of the British Islands\" (1887–1891, 1913) was \"beyond the means of most students and collectors of insects\", while \"A Practical Handbook\" offered a much more affordable option.\n\n\"A Practical Handbook of British Beetles\" instantly became one of the important works on British coleoptera, already being referred to as a classic by the 1956 Handbooks for the Identification of British Insects volume on beetles, which uses it as one of the major works to be cross-referenced.\n\nDespite its age, it has remained the standard work on the identification of British beetles into the 21st century, although the British Entomological and Natural History Society produced a companion volume, \"New British Beetles - species not in Joy's practical handbook\" by Peter J. Hodge and Richard A. Jones in 1995. British coleopterists refer to the book colloquially simply as \"Joy\".\n"}
{"id": "24090052", "url": "https://en.wikipedia.org/wiki?curid=24090052", "title": "Ali Dinar", "text": "Ali Dinar\n\nAli Dinar () (1856 – November 6, 1916) was a Sultan of the Sultanate of Darfur and ruler from the Keira dynasty. \nIn 1898, with the decline of the Mahdists, He managed to regain Darfurs independence.\n\nA rebellion led by him in 1915—in the context of his giving his support to the Ottoman Empire during the First World War—provoked the British to launch a punitive expedition, the Anglo-Egyptian Darfur Expedition, in which he was killed, After which his Sultanate was incorporated into the Anglo-Egyptian Condominium.\n\n"}
{"id": "7827602", "url": "https://en.wikipedia.org/wiki?curid=7827602", "title": "Bendigo State Park", "text": "Bendigo State Park\n\nBendigo State Park is a Pennsylvania state park in Jones Township, Elk County, Pennsylvania in the United States.\nThe park is in a valley on the East Branch Clarion River. of the park are developed. The other are undeveloped woodlands of beech, birch, cherry and maple.\n\nBendigo State Park is named for an Irish tavern keeper, who lived in England, named William Thompson. Thompson was also a prizefighter of some renown. Boxing for money was illegal at the time in England and Thompson was arrested 28 times for breaking the law. Legend holds that Thompson fled to America, to avoid prosecution, when an opponent died in the ring due to injuries sustained in the fight. He adopted the name Bendigo, a corruption of the Old Testament name Abed-nego and moved to the frontier of northwestern Pennsylvania. The newly named, Bendigo, began serving as a Methodist evangelist while also working on the railroads. He was a very tall and strong man and soon became a favorite of his Italian co-workers. No records exist to support the claim that Thompson ever left the British Isles let alone lived in the area. The lumber industry was beginning to boom in northwestern Pennsylvania as Bendigo was making his rounds as an evangelist and a mill town in Elk County was named Bendigo in his honor. The lumber era was not to last forever. The forests were largely chopped down by the early 20th century and many of the mill towns, like Bendigo, were abandoned.\n\nThe borough of Johnsonburg acquired the land that is currently Bendigo State Park in the 1920s and established a community park. The Works Progress Administration of U.S. President Franklin D. Roosevelt made many improvements to the park during the Great Depression. Flooding on the East Branch of the Clarion River destroyed most of the works of the WPA in the 1940s, but a stone wall and dam on the river are still standing. Ownership of the park was transferred to the Commonwealth of Pennsylvania in 1949 and over the course of ten years improvements were made. Bendigo State Park was formally opened to the public on August 15, 1959. The park is at an elevation of .\n\nBendigo State Park attracts families interested in using the swimming and picnicking facilities. The pool is opened daily from 11:00 am until 7:00 pm beginning Memorial Day weekend and ending Labor Day weekend. There are over 150 picnic tables in several picnic areas. The picnic areas have charcoal grills, drinking water, horseshoe pits and restrooms.\n\nThe following state parks are within of Bendigo State Park:\n"}
{"id": "50253290", "url": "https://en.wikipedia.org/wiki?curid=50253290", "title": "Binary mass function", "text": "Binary mass function\n\nIn astronomy, the binary mass function or simply mass function is a function that constrains the mass of the unseen component (typically a star or exoplanet) in a single-lined spectroscopic binary star or in a planetary system. It can be calculated from observable quantities only, namely the orbital period of the binary system, and the peak radial velocity of the observed star. The velocity of one binary component and the orbital period provide (limited) information on the separation and gravitational force between the two components, and hence on the masses of the components.\n\nThe binary mass function follows from Kepler's third law when the radial velocity of one (observed) binary component is introduced.\nKepler's third law describes the motion of two bodies orbiting a common center of mass. It relates the orbital period (the time it takes to complete one full orbit) with the distance between the two bodies (the orbital separation), and the sum of their masses. For a given orbital separation, a higher total system mass implies higher orbital velocities. On the other hand, for a given system mass, a longer orbital period implies a larger separation and lower orbital velocities.\n\nBecause the orbital period and orbital velocities in the binary system are related to the masses of the binary components, measuring these parameters provides some information about the masses of one or both components. But because the true orbital velocity cannot be determined generally, this information is limited.\n\nRadial velocity is the velocity component of orbital velocity in the line of sight of the observer. Unlike true orbital velocity, radial velocity can be determined from Doppler spectroscopy of spectral lines in the light of a star, or from variations in the arrival times of pulses from a radio pulsar. A binary system is called a single-lined spectroscopic binary if the radial motion of only one of the two binary components can be measured. In this case, a lower limit on the mass of the \"other\" (unseen) component can be determined.\n\nThe true mass and true orbital velocity cannot be determined from the radial velocity because the orbital inclination is generally unknown. (The inclination is the orientation of the orbit from the point of view of the observer, and relates true and radial velocity.) This causes a degeneracy between mass and inclination. For example, if the measured radial velocity is low, this can mean that the true orbital velocity is low (implying low mass objects) and the inclination high (the orbit is seen edge-on), or that the true velocity is high (implying high mass objects) but the inclination low (the orbit is seen face-on).\n\nThe peak radial velocity formula_1 is the semi-amplitude of the radial velocity curve, as shown in the figure. The orbital period formula_2 is found from the periodicity in the radial velocity curve. These are the two observable quantities needed to calculate the binary mass function.\n\nThe observed object of which the radial velocity can be measured is taken to be object 1 in this article, its unseen companion is object 2.\n\nLet formula_3 and formula_4 be the stellar masses, with formula_5 the total mass of the binary system, formula_6 and formula_7 the orbital velocities, and formula_8 and formula_9 the distances of the objects to the center of mass. formula_10 is the semi-major axis (orbital separation) of the binary system.\n\nWe start out with Kepler's third law, with formula_11 the orbital frequency and formula_12 the gravitational constant,\n\nformula_13\n\nUsing the definition of the center of mass location, formula_14, we can write\n\nformula_15\n\nInserting this expression for formula_16 into Kepler's third law, we find\n\nformula_17\n\nwhich can be rewritten to\n\nformula_18\n\nThe peak radial velocity of object 1, formula_1, depends on the orbital inclination formula_20 (an inclination of 0° corresponds to an orbit seen face-on, an inclination of 90° corresponds to an orbit seen edge-on). For a circular orbit (orbital eccentricity = 0) it is given by\n\nformula_21\n\nAfter substituting formula_8 we obtain\n\nformula_23\n\nThe binary mass function formula_24 (with unit of mass) is\n\nformula_25\n\nFor an estimated or assumed mass formula_3 of the observed object 1, a minimum mass formula_27 can be determined for the unseen object 2 by assuming formula_28. The true mass formula_4 depends on the orbital inclination. The inclination is typically not known, but to some extent it can be determined from observed eclipses, be constrained from the non-observation of eclipses, or be modelled using ellipsoidal variations (the non-spherical shape of a star in binary system leads to variations in brightness over the course of an orbit that depend on the system's inclination).\n\nIn the case of formula_30 (for example, when the unseen object is an exoplanet), the mass function simplifies to\n\nformula_31\n\nIn the other extreme, when formula_32 (for example, when the unseen object is a high-mass black hole), the mass function becomes\n\nformula_33\n\nand since formula_34 for formula_35, the mass function gives a lower limit on the mass of the unseen object 2.\n\nIn general, for any formula_20 or formula_3,\n\nformula_38\n\nIn an orbit with eccentricity formula_39, the mass function is given by\n\nformula_40\n\nIf the accretor in an X-ray binary has a minimum mass that significantly exceeds the Tolman–Oppenheimer–Volkoff limit (the maximum possible mass for a neutron star), it is expected to be a black hole. This is the case in Cygnus X-1, for example, where the radial velocity of the companion star has been measured.\n\nAn exoplanet causes its host star to move in a small orbit around the center of mass of the star-planet system. This 'wobble' can be observed if the radial velocity of the star is sufficiently high. This is the radial velocity method of detecting exoplanets. Using the mass function and the radial velocity of the host star, the minimum mass of an exoplanet can be determined. Applying this method on Proxima Centauri, the closest star to the solar system, led to the discovery of Proxima Centauri b, a terrestrial planet with a minimum mass of .\n\nPulsar planets are planets orbiting pulsars, and several have been discovered using pulsar timing. The radial velocity variations of the pulsar follow from the varying intervals between the arrival times of the pulses. The first exoplanets were discovered this way in 1992 around the millisecond pulsar PSR 1257+12. Another example is PSR J1719-1438, a millisecond pulsar whose companion, PSR J1719-1438 b, has a minimum mass approximate equal to the mass of Jupiter, according to the mass function.\n"}
{"id": "27107782", "url": "https://en.wikipedia.org/wiki?curid=27107782", "title": "BlueNext", "text": "BlueNext\n\nBlueNext was a European environmental trading exchange, considered the largest CO permit spot market, with headquarters in Paris, France. On October 26, 2012, BlueNext announced that it would close permanently its spot and derivatives trading operations as of December 5, 2012.\n\nBlueNext was founded in December 2007 when NYSE Euronext and Caisse des Dépôts purchased the carbon market from PowerNext. \nNYSE Euronext holds a 60 percent majority stake in BlueNext and Caisse des Dépôts owns the other 40 percent. PowerNext continues to operate its electricity market separately.\n\nMembers of BlueNext are offered spot trading of carbon (CO) emission rights and derivative products (futures) on European Union Allowances (EUAs) and Certified Emission Reductions (CERs). LCH.Clearnet SA provides clearing services for BlueNext futures EUA and BlueNext futures CER.\n\nAs an expansion to their existing products and services, BlueNext began offering auction services, and as a result of strategic partnership with Markit issued their first index, the Markit BlueNext EUA Spot Index.\n\nNYSE Euronext and APX announced on September 7, 2010 plans for a join venture, NYSE Blue, that will focus exclusively on environmental and sustainable energy markets, and expansion of these services in North America and Asia. NYSE Euronext will contribute its ownership in BlueNext in return for a majority interest in the joint venture, while APX will contribute its business (operational, regulatory infrastructure and services for the environmental and sustainable energy markets) in return for a minority interest in the venture.\n\nChina is the world’s leading source of greenhouse gases and is identified as the source of more than 80 percent of carbon credits traded globally. In June 2009, BlueNext and the China Beijing Environmental Exchange (CBEEX) signed an agreement to set up an international carbon-trading related information platform that would jointly publish and promote Clean Development Mechanism (CDM) projects based in China. This information is to be sourced from the China Beijing Environmental Exchange.\n\nThe second joint project between BlueNext and CBEEX is the Panda Standard – the first voluntary standard designed specifically for China and the Chinese carbon marketplace, with an initial primary focus on agriculture and forestry.\n\nBlueNext announced its intention to extend its presence into the United States. The company is currently going through a period of study and research to design products for this North American market.\n\n\n\n"}
{"id": "20215551", "url": "https://en.wikipedia.org/wiki?curid=20215551", "title": "Chinese national carbon trading scheme", "text": "Chinese national carbon trading scheme\n\nThe Chinese national carbon trading scheme is a cap and trade system for carbon dioxide emissions set to be implemented by the end of 2017. This emission trading scheme (ETS) creates a carbon market where emitters can buy and sell emission credits. From this scheme, China can limit emissions, but allow economic freedom for emitters to reduce emissions or purchase emission allowances from other emitters. China is currently the largest emitter of greenhouse gases and many major Chinese cities have severe air pollution. With this plan, China will soon be the largest market in carbon trading. The scheme will limit emissions from six of China’s top carbon dioxide emitting industries, including coal-fired power plants. China was able to gain experience in drafting and implementation of an ETS plan from the United Nations Framework Convention on Climate Change (UNFCCC), where China was part of the Clean Development Mechanism (CDM). From this experience with carbon markets, and lengthy discussions with the next largest carbon market, the European Union (EU), as well as analysis of small scale pilot markets in major Chinese cities and provinces, China’s national ETS will be the largest of its kind and will help China achieve its Intended Nationally Determined Contribution (INDC) from the Paris Agreement in 2016.\n\nChina promised in the Conference of Parties to reduce their carbon intensity per unit of GDP by 60-65% by 2030. To achieve this, they decided to use market-based mechanisms. They developed the Clean Development Mechanism, which consists in a “bottom up” architecture. China has learned from the European Union, whose carbon trading market is currently twice as big, along with California in the United States, to implement mechanisms such as cap and trade. The idea is to create an international market through exchanges where allowances are traded and carbon emissions get monitored and reported.\n\nFor now, the nation has implemented seven pilot carbon markets in various zones that thrive on production of cement, electricity, heat, petroleum and oil extraction. These zones are: Beijing, Chongqing, Guangdong, Hubei, Shanghai, Shenzhen and Tianjin, which represent 25% of China’s total GDP. These stationary activities are known for being the most pollutant and emitter of GHG. Since the pilot plan started, it is estimated that 40.24 millions of metric tons of carbon dioxide has been traded. In July 2017, the official launch of the plan will occur, inviting more Chinese cities and industries to collaborate on the cap and trade.\n\nThese pilot zones proved the cap and trade efficiency. Cap refers to a permitted amount of emissions, if the industry exceed said limitation, it would require an allowance. Allowances can be traded, auctioned or even given away for free, depending on the case. Through cap and trade, it is believed that both competitiveness and possibility on carbon leakage would be reduced. Each cap and allowance was assigned to the cities according to their purpose, production rates, or ability to pass along the costs of carbon along the consumer chain. The caps of greenhouse gas emissions vary from 30-350 metric tons of carbon dioxide equivalent per year when the price for carbon varies from $1.4-$13.00 US dollars per ton of carbon dioxide. There also are 2 types of allowances: new entry vs. governmental. The new entry allowances are for those who are in need for growth and are freely distributed, whereas the governmental allowance is a fixed, stable fraction that must be sold or auctioned.\n\nThere are also conditions that each zone must uphold, mainly regarding monitoring, reporting and verification. Each zone has their own mechanism of doing so, but they all face the same kind of penalties if failing to do so. These penalties include: a reduction on free allowances, a threat on publicizing said status in order to create social pressure, a 2 year restricted access to special funds for energy research and if an excess of emission were to take place, the zone’s government or company would have to pay 3 times the original allowance price.\n\nIn order for China to achieve their aforementioned goals, they need to put front to certain challenges. China will have to overlook that there will no be any overlap with already existing policies from prevention, reduction and consumption of pollutants. The country will also have to strictly overlook and enforce the said regime, and make sure that there is ultimate transparency, especially on monitoring and reporting advances all the way throughout. There will also need to be special attention on carbon leakage and on price volatility. Ever since the pilot cities began this project, the price on carbon and caps have been fluctuating. The government will also need to make sure that there is an efficient trade and exchange of allowances in spot. In general, there will also need to be an existent plan on reduction of greenhouse gas emissions to achieve their Paris agreement.\n\nOn the other hand, policy makers face the struggle on allowance allocations. For the free allowance, they need to think about who to give them to. For the auctioned ones, they need to think about the type of auction that is most convenient, and the combined ones, all of the above.\n\nPrior to the conception and design of China’s national carbon trading scheme, carbon emission trading (CET) had never been executed in China. With no CET experience to draw from, in late 2011 the National Development and Reform Commission (NDRC) approved two provinces and five cities of varying degrees of economic development as pilots. In the Notice on Launching Pilots for Emissions Trading System (ETS), the NDRC approved Beijing, Tianjin, Shanghai, Chongqing, Hubei, Guangdong, and Shenzhen as ETS pilots. Shenzhen was the first pilot to launch, on June 18, 2013, and was soon followed by the other designated pilots, which all completed their first compliance period by June 2015. All of the pilots with compliance data had compliance rates of over 96%, with Shanghai having the highest compliance rate at 100% and Tianjin having the lowest compliance rate at 96.5%. In order to aid the design of implementation details in China’s national carbon trading scheme, each of the pilots were given the freedom to decide values for trading scheme parameters such as allowance allocation, coverage of sectors, and punishment mechanisms. They also vary in their approach to transactions, issues with price uncertainty, and managing risk. To assess the success of one pilot’s trading scheme versus another, market performance was considered.\n\nThe pilots' approach to allowance allocation were largely based on historical emissions for most sectors except the power sector, which was calculated an allowance based on benchmarks and production. Guangdong was the only pilot to implement auctioning allowances for its power sector. Additionally, all pilots except Hubei allowed allowance rollover into the next compliance period. To standardize monitoring, reporting, and verification of carbon data, the NDRC issued regulations on monitoring and reporting guidelines. Enterprises were required to monitor and report their emissions, which was compared to a report from a third-party verification agency. Discrepancies in the reports above a threshold would require a re-verification. For most pilots, this threshold was set at a difference of 10% or 100 thousand tons. Funding required for verification was supplied by the local government rather than the enterprise, in order to reduce the compliance burden.\n\nFive of the pilot programs allowed individuals to participate in carbon trading while two only allowed enterprises to do so. Transaction formats varied slightly but were all in spot markets with no carbon futures. In all pilots, enterprises needed to pay the cost of trading, which was a two-way charging scheme. In order to ensure stability in the carbon market, each pilot set a price limit based on the closing price of carbon in the previous compliance period, as well as limits on maximum allowance holdings for enterprises. Each pilot implemented varying degrees of fines for faking carbon data or withholding data. Shenzhen was the only pilot to implement a variable fine, setting it as three times the market clearing price times the excess emissions. Other pilot programs charged a flat fee. All pilots deducted the excess emissions from the next period’s allowance for the enterprise in question.\n\nFrom the pilot program’s inception to May 2015, 20.27 MtCO2e had been traded for a total value of 720 million CNY. The carbon price for Shenzhen and Guangdong were the greatest, ranging from 60 to 80 CNY. The price fluctuated more in Shenzhen and Tianjin when compared to other pilots, especially near the compliance period deadline and near the beginning of new periods. The transitive behavior of the carbon market is a result more so of trading entities' understanding of policy and the timing of carbon data acquisition rather than market demand. From the pilot program data, China should improve on the designs of the pilot programs in order to achieve a stable and stimulated national carbon market.\n\nThe scheme set the initial carbon allowances to 3–5 billion tones per year. Comparing this to the EU-ETS scheme, it is almost twice as much as the EU allowance. By the time of July 2016, EU-ETS is the world's largest carbon trading system, with a carbon market of two billion tonnes per year. the National Development and Reform Commission (NDRC) announced that eight sectors would be included in this market, these eight sectors are petrochemicals, chemicals, building materials, steel, ferrous metals, paper-making, power-generation and aviation. The companies that participate in this market are compulsory to use more designated amount of energy. The current number is 10,000 tonnes of standard coal equivalent of energy per year. According to seven pilot cities (7 trial carbon market in China) average price, the launch price would be set to be around 5 dollars per ton, which would generate a revenue of $0.17 to $1.16 billion in the first trading year. The expected carbon trading volume would increase to 7–58 billion dollars per year after 2020 since more market would be introduced to the carbon trading system.\n\nThe first auction for vintage 2016 allowances happened in Guangdong on September 21. This was held by the China Emission Exchange of Guangdong. The settle price of $1.48 per ton was set while the floor price is $1.40 for 500,000 tonnes. In the future, China’s carbon trading allowance would expect to be increase to 3–5 billion tonnes of CO2, which would bring a revenue of around 60–400 CNY.\n\n\n"}
{"id": "44560534", "url": "https://en.wikipedia.org/wiki?curid=44560534", "title": "Cipetir, Sukabumi", "text": "Cipetir, Sukabumi\n\nCipetir (pre-EYD spelling and Dutch: Tjipetir) is a village in Sukabumi, West Java, Indonesia. As of 2010, it has 5,716 inhabitants.\n\nThe name comes from Sundanese, and literally means \"Thunder River\", from \"ci\" (\"river\") and \"petir\" (\"thunder\").\n\nCipetir was the site of a gutta-percha plantation in the 19th and early 20th century.\n\nMarine debris consisting of blocks of gutta-percha, which is highly resistant to water corrosion, with the word \"TJIPETIR\" on them has been found on beaches throughout Europe. They are believed to be coming from one or more sunken ships from the early 1900s. The wreck of the \"Miyazaki Maru\", sank in 1917, has been put forward as potential source of the blocks.\n\n"}
{"id": "14822794", "url": "https://en.wikipedia.org/wiki?curid=14822794", "title": "Clipstone Colliery", "text": "Clipstone Colliery\n\nClipstone Colliery was a coal mine situated near the village of the same name on the edge of an area of Nottinghamshire known as “The Dukeries” because of the number of stately homes in the area. The colliery was owned by the Bolsover Colliery Company and passed to the National Coal Board in 1947. The headstocks and powerhouse are grade II listed buildings.\n\nThe colliery was sunk to exploit the Barnsley seam or “Tophard”, as it is known locally. In the 1950s the shafts were deepened to over 1000 yards (920 m) to exploit other seams.\n\nThe colliery was closed by British Coal, as the National Coal Board had become, in 1993 and reopened by RJB Mining (now UK Coal) in April 1994, the licence to dig for coal being limited to the Yard seam which is located at a depth of 957 yards (870 m). The colliery was finally closed in April 2003.\n"}
{"id": "1202852", "url": "https://en.wikipedia.org/wiki?curid=1202852", "title": "Constructed wetland", "text": "Constructed wetland\n\nA constructed wetland (CW) is an artificial wetland to treat municipal or industrial wastewater, greywater or stormwater runoff. It may also be designed for land reclamation after mining, or as a mitigation step for natural areas lost to land development.\n\nConstructed wetlands are engineered systems that use natural functions vegetation, soil, and organisms to treat wastewater. Depending on the type of wastewater the design of the constructed wetland has to be adjusted accordingly. Constructed wetlands have been used to treat both centralized and on-site wastewater. Primary treatment is recommended when there is a large amount of suspended solids or soluble organic matter (measured as BOD and COD).\n\nSimilarly to natural wetlands, constructed wetlands also act as a biofilter and/or can remove a range of pollutants (such as organic matter, nutrients, pathogens, heavy metals) from the water. Constructed wetlands are a sanitation technology that have not been designed specifically for pathogen removal, but instead, have been designed to remove other water quality constituents such as suspended solids, organic matter and nutrients (nitrogen and phosphorus). All types of pathogens (i.e., bacteria, viruses, protozoan and helminths) are expected to be removed to some extent in a constructed wetland. Subsurface wetland provide greater pathogen removal than surface wetlands.\n\nThere are two main types of constructed wetlands: subsurface flow and surface flow constructed wetlands. The planted vegetation plays an important role in contaminant removal. The filter bed, consisting usually of sand and gravel, has an equally important role to play. Some constructed wetlands may also serve as a habitat for native and migratory wildlife, although that is not their main purpose. Subsurface flow constructed wetlands are designed to have either horizontal flow or vertical flow of water through the gravel and sand bed. Vertical flow systems have a smaller space requirement than horizontal flow systems.\n\nMany terms are used to denote constructed wetlands, such as reed beds, soil infiltration beds, treatment wetlands, engineered wetlands, man-made or artificial wetlands. A biofilter has some similarities with a constructed wetland, but is usually without plants.\n\nThe term of constructed wetlands can also be used to describe restored and recultivated land that was destroyed in the past through draining and converting into farmland, or mining.\n\nA constructed wetland is an engineered sequence of water bodies designed to filter and treat waterborne pollutants found in sewage, industrial effluent or storm water runoff. Constructed wetlands are used for wastewater treatment or for greywater treatment. They can be used after a septic tank for primary treatment (or other types of systems) in order to separate the solids from the liquid effluent. Some constructed wetland designs however do not use upfront primary treatment.\n\nVegetation in a wetland provides a substrate (roots, stems, and leaves) upon which microorganisms can grow as they break down organic materials. This community of microorganisms is known as the periphyton. The periphyton and natural chemical processes are responsible for approximately 90 percent of pollutant removal and waste breakdown. The plants remove about seven to ten percent of pollutants, and act as a carbon source for the microbes when they decay. Different species of aquatic plants have different rates of heavy metal uptake, a consideration for plant selection in a constructed wetland used for water treatment. Constructed wetlands are of two basic types: subsurface flow and surface flow wetlands.\n\nConstructed wetlands are one example of nature-based solutions and of phytoremediation.\n\nMany regulatory agencies list treatment wetlands as one of their recommended \"best management practices\" for controlling urban runoff.\n\nPhysical, chemical, and biological processes combine in wetlands to remove contaminants from wastewater. An understanding of these processes is fundamental not only to designing wetland systems but to understanding the fate of chemicals once they enter the wetland. Theoretically, wastewater treatment within a constructed wetland occurs as it passes through the wetland medium and the plant rhizosphere. A thin film around each root hair is aerobic due to the leakage of oxygen from the rhizomes, roots, and rootlets. Aerobic and anaerobic micro-organisms facilitate decomposition of organic matter. Microbial nitrification and subsequent denitrification releases nitrogen as gas to the atmosphere. Phosphorus is coprecipitated with iron, aluminium, and calcium compounds located in the root-bed medium. Suspended solids filter out as they settle in the water column in surface flow wetlands or are physically filtered out by the medium within subsurface flow wetlands. Harmful bacteria and viruses are reduced by filtration and adsorption by biofilms on the gravel or sand media in subsurface flow and vertical flow systems.\n\nThe dominant forms of nitrogen in wetlands that are of importance to wastewater treatment include organic nitrogen, ammonia, ammonium, nitrate and nitrite. Total nitrogen refers to all nitrogen species. Wastewater nitrogen removal is important because of ammonia’s toxicity to fish if discharged into watercourses. Excessive nitrates in drinking water is thought to cause methemoglobinemia in infants, which decreases the blood's oxygen transport ability. Moreover, excess input of N from point and non-point sources to surface water promotes eutrophication in rivers, lakes, estuaries, and coastal oceans which causes several problems in aquatic ecosystems e.g. toxic algal blooms, oxygen depletion in water, fish mortality, loss of aquatic biodiversity.\n\nAmmonia removal occurs in constructed wetlands – if they are designed to achieve biological nutrient removal – in a similar ways as in sewage treatment plants, except that no external, energy-intensive addition of air (oxygen) is needed. It is a two-step process, consisting of nitrification followed by denitrification. The nitrogen cycle is completed as follows: ammonia in the wastewater is converted to ammonium ions; the aerobic bacterium \"Nitrosomonas\" sp. oxidizes ammonium to nitrite; the bacterium \"Nitrobacter\" sp. then converts nitrite to nitrate. Under anaerobic conditions, nitrate is reduced to relatively harmless nitrogen gas that enters the atmosphere.\n\nNitrification is the biological conversion of organic and inorganic nitrogenous compounds from a reduced state to a more oxidized state, based on the action of two different bacteria types. Nitrification is strictly an aerobic process in which the end product is nitrate (). The process of nitrification oxidizes ammonium (from the wastewater) to nitrite (), and then nitrite is oxidized to nitrate ().\n\nDenitrification is the biochemical reduction of oxidized nitrogen anions, nitrate and nitrite to produce the gaseous products nitric oxide (NO), nitrous oxide () and nitrogen gas (), with concomitant oxidation of organic matter. The end product, , and to a lesser extent the intermediary by product, , are gases that re-enter the atmosphere.\n\nConstructed wetlands have been used to remove ammonia and other nitrogenous compounds from contaminated mine water, including cyanide and nitrate.\n\nPhosphorus occurs naturally in both organic and inorganic forms. The analytical measure of biologically available orthophosphates is referred to as soluble reactive phosphorus (SR-P). Dissolved organic phosphorus and insoluble forms of organic and inorganic phosphorus are generally not biologically available until transformed into soluble inorganic forms.\n\nIn freshwater aquatic ecosystems phosphorus is typically the major limiting nutrient. Under undisturbed natural conditions, phosphorus is in short supply. The natural scarcity of phosphorus is demonstrated by the explosive growth of algae in water receiving heavy discharges of phosphorus-rich wastes. Because phosphorus does not have an atmospheric component, unlike nitrogen, the phosphorus cycle can be characterized as closed. The removal and storage of phosphorus from wastewater can only occur within the constructed wetland itself. Phosphorus may be sequestered within a wetland system by:\n\nAquatic vegetation may play an important role in phosphorus removal and, if harvested, extend the life of a system by postponing phosphorus saturation of the sediments. Plants create a unique environment at the biofilm's attachment surface. Certain plants transport oxygen which is released at the biofilm/root interface, adding oxygen to the wetland system. Plants also increase soil or other root-bed medium hydraulic conductivity. As roots and rhizomes grow they are thought to disturb and loosen the medium, increasing its porosity, which may allow more effective fluid movement in the rhizosphere. When roots decay they leave behind ports and channels known as macropores which are effective in channeling water through the soil.\n\nConstructed wetlands have been used extensively for the removal of dissolved metals and metalloids. Although these contaminants are prevalent in mine drainage, they are also found in stormwater, landfill leachate and other sources (e.g., leachate or FDG washwater at coal-fired power plants), for which treatment wetlands have been constructed for mines.\n\nConstructed wetlands can also be used for treatment of acid mine drainage from coal mines.\n\nConstructed wetlands are a sanitation technology that have not typically been designed for pathogen removal, but instead, have been designed to remove other water quality constituents such as suspended solids, organic matter (BOD/COD) and nutrients (nitrogen and phosphorus).\n\nAll types of pathogens are expected to be removed in a constructed wetland; however, greater pathogen removal is expected to occur in a subsurface wetland. In a free water surface flow wetland one can expect 1 to 2 log10 reduction of pathogens; however, bacteria and virus removal may be less than 1 log10 reduction in systems that are heavily planted with vegetation. This is because constructed wetlands typically include vegetation which assists in removing other pollutants such as nitrogen and phosphorus. Therefore, the importance of sunlight exposure in removing viruses and bacteria is minimized in these systems.\n\nRemoval in a properly designed and operated free water surface flow wetland is reported to be less than 1 to 2 log10 for bacteria, less than 1 to 2 log10 for viruses, 1 to 2 log10 for protozoa:, and 1 to 2 log10 for helminths. In subsurface flow wetlands, the expected removal of pathogens is reported to be 1 to 3 log10 for bacteria, 1 to 2 log10 for viruses, 2 log10 for protozoa, and 2 log10 for helminths.\n\nThe log10 removal efficiencies reported here can also be understood in terms of the common way of reporting removal efficiencies as percentages: 1 log10 removal is equivalent to a removal efficiency of 90%; 2 log10 = 99%; 3 log10 = 99.9%; 4 log10 = 99.99% and so on.\n\nThe main three broad types of constructed wetlands include:\n\nThe former types are placed in a basin with a substrate to provide a surface area upon which large amounts of waste degrading biofilms form, while the latter relies on a flooded treatment basin upon which aquatic plants are held in flotation till they develop a thick mat of roots and rhizomes upon which biofilms form. In most cases, the bottom is lined with either a polymer geomembrane, concrete or clay (when there is appropriate clay type) in order to protect the water table and surrounding grounds. The substrate can be either gravel—generally limestone or pumice/volcanic rock, depending on local availability, sand or a mixture of various sizes of media (for vertical flow constructed wetlands).\n\nIn subsurface flow constructed wetlands the flow of wastewater occurs between the roots of the plants and there is no water surfacing (it is kept below gravel). As a result, the system is more efficient, does not attract mosquitoes, is less odorous and less sensitive to winter conditions. Also, less area is needed to purify water. A downside to the system are the intakes, which can clog or bioclog easily, although some larger sized gravel will often solve this problem.\n\nSubsurface flow wetlands can be further classified as horizontal flow or vertical flow constructed wetlands. In the vertical flow constructed wetland, the effluent moves vertically from the planted layer down through the substrate and out (requiring air pumps to aerate the bed). In the horizontal flow constructed wetland the effluent moves horizontally via gravity, parallel to the surface, with no surface water thus avoiding mosquito breeding. Vertical flow constructed wetlands are considered to be more efficient with less area required compared to horizontal flow constructed wetlands. However, they need to be interval-loaded and their design requires more know-how while horizontal flow constructed wetlands can receive wastewater continuously and are easier to build.\n\nDue to the increased efficiency a vertical flow subsurface constructed wetland requires only about of space per person equivalent, down to 1.5 square metres in hot climates.\n\nThe \"French System\" combines primary and secondary treatment of raw wastewater. The effluent passes various filter beds whose grain size is getting progressively smaller (from gravel to sand).\n\nSubsurface flow wetlands can treat a variety of different wastewaters, such as household wastewater, agricultural, paper mill wastewater, mining runoff, tannery or meat processing wastes, storm water.\n\nThe quality of the effluent is determined by the design and should be customized for the intended reuse application (like irrigation or toilet flushing) or the disposal method.\n\nDepending on the type of constructed wetlands, the wastewater passes through a gravel and more rarely sand medium on which plants are rooted. A gravel medium (generally limestone or volcanic rock lavastone) can be used as well (the use of lavastone will allow for a surface reduction of about 20% over limestone) is mainly deployed in horizontal flow systems though it does not work as efficiently as sand (but sand will clog more readily).\n\nConstructed subsurface flow wetlands are meant as secondary treatment systems which means that the effluent needs to first pass a primary treatment which effectively removes solids. Such a primary treatment can consist of sand and grit removal, grease trap, compost filter, septic tank, Imhoff tank, anaerobic baffled reactor or upflow anaerobic sludge blanket (UASB) reactor. The following treatment is based on different biological and physical processes like filtration, adsorption or nitrification. Most important is the biological filtration through a biofilm of aerobic or facultative bacteria. Coarse sand in the filter bed provides a surfaces for microbial growth and supports the adsorption and filtration processes. For those microorganisms the oxygen supply needs to be sufficient.\n\nEspecially in warm and dry climates the effects of evapotranspiration and precipitation are significant. In cases of water loss, a vertical flow constructed wetland is preferable to a horizontal because of an unsaturated upper layer and a shorter retention time, although vertical flow systems are more dependent on an external energy source. Evapotranspiration (as is rainfall) is taken into account in designing a horizontal flow system.\n\nThe effluent can have a yellowish or brownish colour if domestic wastewater or blackwater is treated. Treated greywater usually does not tend to have a colour. Concerning pathogen levels, treated greywater meets the standards of pathogen levels for safe discharge to surface water. Treated domestic wastewater might need a tertiary treatment, depending on the intended reuse application.\n\nPlantings of reedbeds are popular in European constructed subsurface flow wetlands, although at least twenty other plant species are usable. Many fast growing timer plants can be used, as well for example as Musa spp., Juncus spp., cattails (\"Typha\" spp.) and sedges.\n\nOverloading peaks should not cause performance problems while continuous overloading lead to a loss of treatment capacity through too much suspended solids, sludge or fats.\n\nSubsurface flow wetlands require the following maintenance tasks: regular checking of the pretreatment process, of pumps when they are used, of influent loads and distribution on the filter bed.\n\nSubsurface wetlands are less hospitable to mosquitoes compared to surface flow wetlands, as there is no water exposed to the surface. Mosquitos can be a problem in surface flow constructed wetlands. Subsurface flow systems have the advantage of requiring less land area for water treatment than surface flow. However, surface flow wetlands can be more suitable for wildlife habitat.\n\nFor urban applications the area requirement of a subsurface flow constructed wetland might be a limiting factor compared to conventional municipal wastewater treatment plants. High rate aerobic treatment processes like activated sludge plants, trickling filters, rotating discs, submerged aerated filters or membrane bioreactor plants require less space. The advantage of subsurface flow constructed wetlands compared to those technologies is their operational robustness which is particularly important in developing countries. The fact that constructed wetlands do not produce secondary sludge (sewage sludge) is another advantage as there is no need for sewage sludge treatment. However, primary sludge from primary settling tanks does get produced and needs to be removed and treated.\n\nThe costs of subsurface flow constructed wetlands mainly depend on the costs of sand with which the bed has to be filled. Another factor is the cost of land.\n\nSurface flow wetlands, also known as free water surface constructed wetlands, can be used for tertiary treatment or polishing of effluent from wastewater treatment plants. They are also suitable to treat stormwater drainage.\n\nSurface flow constructed wetlands always have horizontal flow of wastewater across the roots of the plants, rather than vertical flow. They require a relatively large area to purify water compared to subsurface flow constructed wetlands and may have increased smell and lower performance in winter.\n\nSurface flow wetlands have a similar appearance to ponds for wastewater treatment (such as \"waste stabilization ponds\") but are in the technical literature not classified as ponds.\n\nPathogens are destroyed by natural decay, predation from higher organisms, sedimentation and UV irradiation since the water is exposed to direct sunlight. The soil layer below the water is anaerobic but the roots of the plants release oxygen around them, this allows complex biological and chemical reactions.\n\nSurface flow wetlands can be supported by a wide variety of soil types including bay mud and other silty clays.\n\nPlants such as Water Hyacinth (\"Eichhornia crassipes\") and \"Pontederia\" spp. are used worldwide (although Typha and Phragmites are highly invasive).\n\nHowever, surface flow constructed wetlands may encourage mosquito breeding. They may also have high algae production that lowers the effluent quality and due to open water surface mosquitos and odours, it is more difficult to integrate them in an urban neighbourhood.\n\nA combination of different types of constructed wetlands is possible to use the specific advantages of each system.\n\nAn integrated constructed wetland (ICW) is an unlined free surface flow constructed wetland with emergent vegetated areas and local soil material. Its objectives is not only to treat wastewater from farmyards and other wastewater sources, but also to integrate the wetland infrastructure into the landscape and enhancing its biological diversity.\n\nIntegrated constructed wetland facilitates may be more robust treatment systems compared to other constructed wetlands. This is due to the greater biological complexity and generally relatively larger land area use and associated longer hydraulic residence time of integrated constructed wetland compared to conventional constructed wetlands.\n\nIntegreated constructed wetlands are used in Ireland, the UK and the United States since about 2007. Farm constructed wetlands, which are a subtype of integrated constructed wetlands, are promoted by the Scottish Environment Protection Agency and the Northern Ireland Environment Agency since 2008.\n\nTyphas and Phragmites are the main species used in constructed wetland due to their effectiveness, even though they can be invasive outside their native range.\n\nIn North America, cattails (\"Typha latifolia\") are common in constructed wetlands because of their widespread abundance, ability to grow at different water depths, ease of transport and transplantation, and broad tolerance of water composition (including pH, salinity, dissolved oxygen and contaminant concentrations). Elsewhere, Common Reed (\"Phragmites australis\") are common (both in blackwater treatment but also in greywater treatment systems to purify wastewater).\n\nPlants are usually indigenous in that location for ecological reasons and optimum workings.\n\nLocally grown non-predatory fish can be added to surface flow constructed wetlands to eliminate or reduce pests, such as mosquitos. \n\nStormwater wetlands provide habitat for amphibians but the pollutants they accumulate can affect the survival of larval stages, potentially making them function as \"ecological traps\".\n\nSince constructed wetlands are self-sustaining their lifetime costs are significantly lower than those of conventional treatment systems. Often their capital costs are also lower compared to conventional treatment systems. They do take up significant space, and are therefore not preferred where real estate costs are high.\n\nSubsurface flow constructed wetlands with sand filter bed have their origin in Europe and are now used all over the world. Subsurface flow constructed wetlands with a gravel bed are mainly found in North Africa, South Africa, Asia, Australia and New Zealand.\n\nThe total number of constructed wetlands in Austria is 5,450 (in 2015). Due to legal requirements (nitrification), only vertical flow constructed wetlands are implemented in Austria as they achieve better nitrification performance than horizontal flow constructed wetlands. Only about 100 of these constructed wetlands have a design size of 50 population equivalents or more. The remaining 5,350 treatment plants are smaller than that.\n\nThe Arcata Marsh in Arcata, California is a sewage treatment and wildlife protection marsh.\n\nThe Urrbrae Wetland in Australia was constructed for urban flood control and environmental education.\n\nAt the Ranger Uranium Mine, in Australia, ammonia is removed in \"enhanced\" natural wetlands (rather than fully engineered constructed wetlands), along with manganese, uranium and other metals.\n\n\n\n"}
{"id": "52973918", "url": "https://en.wikipedia.org/wiki?curid=52973918", "title": "Czech Crater", "text": "Czech Crater\n\nCzech Crater or Bohemian Crater is a working hypothesis that considers the Bohemian Massif as an approximately 2 billion year old impact crater. This is contrary to mainstream geological ideas that describe the Massif as the result of continental collision. Individual independent units collided more than 300 million years ago thanks to their drifting. This is explained by a widely accepted theory of plate tectonics.\n\nA first comparison of the craters on the Moon and the Czech circular structure was made by Galileo Galilei in 1610. Boston University astronomers Farouk El-Baz and Michael D. Papagiannis informed about the structure in 1980's as well. So did also Petr Rajlich in 1992. He informed about the discovery of an impact breccia. Rajlich, Benes and Cobbold also proposed a tectonic experiment related to the hypothesis in 1996. They demonstrated the origin of roughly circular ridges around the Bohemian Massif as a result of simple lithospheric wrenching.\n\nThere exist many results of radiometric dating from the Bohemian Massif like zircon, muscovite or monazite ages. The age is often around 340 million years (a presumed variscan orogeny). The rocks could only be affected by a late variscan hydrothermal event and the ages of zircons or monazites do not indicate the real ages of them according to some papers.\n\nThere was found a large conical depression in the Moho discontinuity 2-D model just beneath the probable center of the crater according to seismic data.\n\nMacroscopic white lamellae exist inside quartz and other minerals in the Bohemian Massif and even at another places in whole of the world like wavefronts generated by a meteorite impact according to the Rajlich's Hypothesis. The hypothetical wavefronts are composed of many microcavities. Their origin is seen in a physical phenomenon of ultrasonic cavitation, which is well known from the technical practice.\n"}
{"id": "26192986", "url": "https://en.wikipedia.org/wiki?curid=26192986", "title": "Dapeng LNG terminal", "text": "Dapeng LNG terminal\n\nThe Dapeng LNG is China's first Liquefied Natural Gas (LNG) terminal. Guangdong Dapeng LNG Company ltd. was established in February 2004.\n\n\n"}
{"id": "11991929", "url": "https://en.wikipedia.org/wiki?curid=11991929", "title": "Depth in a well", "text": "Depth in a well\n\nIn the oil and gas industry, depth in a well is the measurement, for any point in that well, of the distance between a reference point or elevation, and that point. It is the most common method of reference for locations in the well, and therefore, in oil industry speech, \"depth\" also refers to the location itself.\n\nBy extension, depth can refer to locations below, or distances from, a reference point or elevation, even when there is no well. In that sense, depth is a concept related to elevation, albeit in the opposite direction. Depth in a well is not necessarily measured vertically or along a straight line.\n\nBecause wells are not always drilled vertically, there may be two \"depths\" for every given point in a wellbore: the measured depth (MD) measured along the path of the borehole, and the true vertical depth (TVD), the absolute vertical distance between the datum and the point in the wellbore. In perfectly vertical wells, the TVD equals the MD; otherwise, the TVD is less than the MD measured from the same datum. Common datums used are ground level (GL), drilling rig floor (DF), rotary table (RT), kelly bushing (KB) and mean sea level (MSL).\n\nAlthough it is an intuitive concept, depth in a well is the source of much confusion because it is frequently not specified correctly. Absolute depth should always be specified with three components: \nand none of these 3 components should ever be left implicit. \n\"Example: the top of a reservoir may be found at 1,500 mMDRT in a particular well (1,500 m measured depth below the rotary table), which may be equal to 1,492 mTVDMSL (1,492  m true-vertical-depth below mean sea level) after correction for deviations from vertical.\"\n\n \"Example: RT = -10 mMDLAT\"\n\nAny combination of unit, path, and reference can be used, as long as they result in fully specified, unambiguous depths. A well may reach to many kilometers.\n\nSpecification of an absolute depth: in Figure 1 above, point P1 might be at 3207 mMDRT and 2370 mTVDMSL, while point P2 might be at 2530 mMDRT and 2502 mTVDLAT. \nSpecification of a differential depth or a thickness: in Figure 2 above, the thickness of the reservoir penetrated by the well might be 57 mMD or 42 mTVD, even though the reservoir true stratigraphic thickness in that area (or isopach) might be only 10 m, and its true vertical thickness (isochore), 14 m.\n\n"}
{"id": "2920270", "url": "https://en.wikipedia.org/wiki?curid=2920270", "title": "Earth-Three", "text": "Earth-Three\n\nEarth-Three is a fictional alternate universe set in the . It is the Earth of an alternate reality in the DC Multiverse. It first appeared in \"Justice League of America\" #29 (1964).\n\nEarth-Three was introduced by Gardner Fox and Mike Sekowsky in a 1964 issue of \"Justice League of America\". Earth-Three's history is depicted as a mirror image to that of the Earth we know. On Earth-Three, Christopher Columbus was American and discovered Europe; England (a colony of America) won freedom in a reversed form of the Revolutionary War (with George Washington surrendering his sword to Charles Cornwallis) in 1774; President John Wilkes Booth was assassinated by actor Abraham Lincoln. Crucially, Earth-Three was home to an analogue to the Justice League, the Crime Syndicate of America.\n\nThe Crime Syndicate would recur as powerful enemies of the Justice League until DC's 1985 company-wide crossover, \"Crisis on Infinite Earths\". \"Crisis\" revealed that Lex Luthor, here called Alexander Luthor, is the only superhero on an Earth otherwise occupied entirely by villains, most of whom are reversed analogues of heroes on other DC Earths. Earth-Three is destroyed by waves of antimatter in the opening scenes of the series. The sole survivor is the son of Alexander and Lois Lane Luthor, Alexander Luthor, Jr. At the conclusion of the series, all other worlds in the Multiverse were merged.\n\nDC used \"Crisis on Infinite Earths\" to simplify its complex continuity and multiverse into a single narrative set on a single universe, not counting the antimatter universe which was integral to the story of how the Green Lantern villain Sinestro acquired his powers. Editorial mandate initially meant stories featuring the Crime Syndicate were entirely unavailable to writers, but DC later attempted to reintroduce the Crime Syndicate without the setting of Earth-Three in 1992's \"Justice League Quarterly\" #8, which featured a group of aliens from the planet Qward (the antimatter counterpart of Oa) who functioned as \"more powerful\" Justice League analogues.\n\nThis first attempt at bringing back the Crime Syndicate did not stick, and the principle concept behind Earth-Three would be revisited in Grant Morrison's \"\". Morrison recast much of Earth-Three's history as that of the Antimatter Universe's own version of Earth, which is home to the Crime Syndicate of America. He makes notable departures to this formula however, by presenting this world as the product of an alternate history and by reimagining various Crime Syndicate members (for example, by recasting as Batman's brother Thomas Wayne, and by recasting Ultraman not as the alien Kal-El but a human astronaut who acquires Kryptonian abilities). At the end of \"JLA: Earth-2\", Amerika had launched a nuclear strike on London, against Britain's independence movement.\n\nIn \"Superman/Batman Annual\" #1, three members of this Crime Syndicate of Amerika—Ultraman, Owlman, and Superwoman—appear on the main DC Earth, along with an unnamed antimatter doppelganger of Deathstroke (whose behavior, including humorous breaking of the fourth wall, and powers are the same as Marvel Comics' Deadpool) hired to protect Bruce Wayne. The story supposedly takes place as the first time Superman and Batman figure out each other's identities, and matches Batman, Superman, and Deathstroke against their respective antimatter selves. The story is told by Mr. Mxyzptlk and may therefore be completely untrue.\n\nIn the final issue of the 52-issue weekly series \"52\" in 2007, a new Multiverse is revealed, originally consisting of 52 parallel realities. Among the parallel realities shown is one designated \"Earth-3\". As a result of Mister Mind \"eating\" aspects of this reality, it takes on visual aspects of the pre-Crisis Earth-Three. The Earth-3 concept was not heavily explored after this, but does figure in a couple of issues of \"52\"s follow-up weekly series, \"Countdown to Final Crisis\" (2007–8). The name of the new Earth-3 team is revealed to be the \"Crime Society of America\". The Crime Society are considered to be evil versions of the heroes of Earth-2, acting as a new Golden Age counterpart to the Antimatter Earth. A hero known as the Jokester operates in this universe, as later do the Riddler, Three-Face (Evelyn Dent), and Duela Dent. In \"Countdown\" #31 the version of Zatanna (Annataz Arataz) from this world was used by Superman-Prime to keep Mister Mxyzptlk in check. Based on comments by Grant Morrison, this alternate universe is not the pre-Crisis Earth-Three. Despite the return of the DC Multiverse and the creation of a new Earth-3, the Antimatter Earth still exists in Qward, acting as an inverted microcosm of New Earth. The pre-established Crime Syndicate of Amerika from the Antimatter Universe were then featured heavily in \"Trinity\", DC's third year-long weekly series.\n\nDC again rebooted its continuity in 2011 as part of The New 52. In 2013, the \"Trinity War\" crossover event reintroduces Earth-3. It is mentioned as the home of true evil and of the Crime Syndicate and that it was destroyed by an unknown entity. The Crime Syndicate is largely modeled after Morrison's, with the introduction of new characters, and by re-envisioning Ultraman once again as an alien with an origin story which more closely parallels Superman's. In the closing scenes of \"Trinity War\", Ultraman, Superwoman, Owlman, Johnny Quick, Power Ring, Deathstorm, Alfred Pennyworth and Atomica reveal themselves to the Justice League of New Earth. Sea King also inhabited Earth-3, but quickly died after passing through the gateway to New Earth. An Earth-3 version Martian Manhunter is also revealed to exist. Alexander Luthor, who can become Mazahs, is also from Earth-3, and an enemy of the Crime Syndicate. The official site of DC Comics describes Earth-Three as a world where the values of \"good\" and \"evil\" are reversed, with \"evil\" being the way of the world.\n\nNotes:\n\n\n"}
{"id": "1800265", "url": "https://en.wikipedia.org/wiki?curid=1800265", "title": "Emergy", "text": "Emergy\n\nEmergy is the amount of energy that was consumed in direct and indirect transformations to make a product or service. Emergy is a measure of quality differences between different forms of energy. Emergy is an expression of all the energy used in the work processes that generate a product or service in units of one type of energy. Emergy is measured in units of \"emjoule\"s, a unit referring to the available energy consumed in transformations. Emergy accounts for different forms of energy and resources (e.g. sunlight, water, fossil fuels, minerals, etc.) Each form is generated by transformation processes in nature and each has a different ability to support work in natural and in human systems. The recognition of these quality differences is a key concept.\n\nThe theoretical and conceptual basis for the emergy methodology is grounded in thermodynamics, general system theory and systems ecology. Evolution of the theory by Howard T. Odum over the first thirty years is reviewed in \"Environmental Accounting\" and in the volume edited by C. A. S. Hall titled \"Maximum Power\".\n\nBeginning in the 1950s, Odum analyzed energy flow in ecosystems (\"e.g.\" Silver Springs, Florida; Enewetak atoll in the south Pacific; Galveston Bay, Texas and Puerto Rican rainforests, amongst others) where energies in various forms at various scales were observed. His analysis of energy flow in ecosystems, and the differences in the potential energy of sunlight, fresh water currents, wind and ocean currents led him to make the suggestion that when two or more different energy sources drive a system, they cannot be added without first converting them to a common measure that accounts for their differences in energy quality. This led him to introduce the concept of \"energy of one kind\" as a common denominator with the name \"energy cost\". He then expanded the analysis to model food production in the 1960s, and in the 1970s to fossil fuels.\n\nOdum's first formal statement of what would later be termed emergy was in 1973:\nEnergy is measured by calories, btu's, kilowatthours, and other intraconvertable units, but energy has a scale of quality which is not indicated by these measures. The ability to do work for man depends on the energy quality and quantity and this is measurable by the amount of energy of a lower quality grade required to develop the higher grade. The scale of energy goes from dilute sunlight up to plant matter, to coal, from coal to oil, to electricity and up to the high quality efforts of computer and human information processing.\n\nIn 1975, he introduced a table of \"Energy Quality Factors\", kilocalories of sunlight energy required to make a kilocalorie of a higher quality energy, the first mention of the energy hierarchy principle which states that \"energy quality is measured by the energy used in the transformations\" from one type of energy to the next.\n\nThese energy quality factors, were placed on a fossil-fuel basis and called \"Fossil Fuel Work Equivalents\" (FFWE), and the quality of energies were measured based on a fossil fuel standard with rough equivalents of 1 kilocalorie of fossil fuel equal to 2000 kilocalories of sunlight. \"Energy quality ratios\" were computed by evaluating the quantity of energy in a transformation process to make a new form and were then used to convert different forms of energy to a common form, in this case fossil fuel equivalents. FFWE's were replaced with coal equivalents (CE) and by 1977, the system of evaluating quality was placed on a solar basis and termed solar equivalents (SE).\n\nThe term \"embodied energy\" was used for a time in the early 1980s to refer to energy quality differences in terms of their costs of generation, and a ratio called a \"quality factor\" for the calories (or joules) of one kind of energy required to make those of another. However, since the term embodied energy was used by other groups who were evaluating the fossil fuel energy required to generate products and were not including all energies or using the concept to imply quality, embodied energy was dropped in favor of \"embodied solar calories\", and the quality factors became known as \"transformation ratios\".\n\nUse of the term \"embodied energy\" for this concept was modified in 1986 when David Scienceman, a visiting scholar at the University of Florida from Australia, suggested the term \"emergy\" and \"emjoule\" or \"emcalorie\" as the unit of measure to distinguish emergy units from units of available energy. The term transformation ratio was shortened to transformity in about the same time. It is important to note that throughout this twenty years the baseline or the basis for evaluating forms of energy and resources shifted from organic matter, to fossil fuels and finally to solar energy.\n\nAfter 1986, the emergy methodology continued to develop as the community of scientists expanded and as new applied research into combined systems of humans and nature presented new conceptual and theoretical questions. The maturing of the emergy methodology resulted in more rigorous definitions of terms and nomenclature and refinement of the methods of calculating transformities. The International Society for the Advancement of Emergy Research and a biennial International Conference at the University of Florida support this research.\n\nEmergy— amount of energy of one form that is used in transformations directly and indirectly to make a product or service. The unit of emergy is the emjoule or emergy joule. Using emergy, sunlight, fuel, electricity, and human service can be put on a common basis by expressing each of them in the emjoules of solar energy that is required to produce them. If solar emergy is the baseline, then the results are solar emjoules (abbreviated seJ). Although other baselines have been used, such as coal emjoules or electrical emjoules, in most cases emergy data are given in solar emjoules.\n\nUnit Emergy Values (UEVs) — the emergy required to generate one unit of output. Types of UEVs:\n\nEmergy accounting converts the thermodynamic basis of all forms of energy, resources and human services into equivalents of a single form of energy, usually solar. To evaluate a system, a system diagram organizes the evaluation and account for energy inputs and outflows. A table of the flows of resources, labor and energy is constructed from the diagram and all flows are evaluated. The final step involves interpreting the results.\n\nIn some cases, an evaluation is done to determine the fit of a development proposal within its environment. It also allows comparison of alternatives. Another purpose is to seek the best use of resources to maximize economic vitality.\n\nSystem diagrams show the inputs that are evaluated and summed to obtain the emergy of a flow. A diagram of a city and its regional support area is shown in Figure 1.\n\nA table (see example below) of resource flows, labor and energy is constructed from the diagram. Raw data on inflows that cross the boundary are converted into emergy units, and then summed to obtain total emergy supporting the system. Energy flows per unit time (usually per year) are presented in the table as separate line items.\n\nAll tables are followed by footnotes that show citations for data and calculations.\n\nThe table allows a unit emergy value to be calculated. The final, output row (row “O” in the example table above) is evaluated first in units of energy or mass. Then the input emergy is summed and the unit emergy value is calculated by dividing the emergy by the units of the output.\n\nFigure 2 shows non-renewable environmental contributions (N) as an emergy storage of materials, renewable environmental inputs (R), and inputs from the economy as purchased (F) goods and services. Purchased inputs are needed for the process to take place and include human service and purchased non-renewable energy and material brought in from elsewhere (fuels, minerals, electricity, machinery, fertilizer, etc.). Several ratios, or indices are given in Figure 2 that assess the global performance of a process.\n\nOther ratios are useful depending on the type and scale of the system under evaluation.\n\nThe recognition of the relevance of energy to the growth and dynamics of complex systems has resulted in increased emphasis on environmental evaluation methods that can account for and interpret the effects of matter and energy flows at all scales in systems of humanity and nature. The following table lists some general areas in which the emergy methodology has been employed.\n\nThe concept of emergy has been controversial within academe including ecology, thermodynamics and economy. Emergy theory has been criticized for allegedly offering an energy theory of value to replace other theories of value. The stated goal of emergy evaluations is to provide an \"ecocentric\" valuation of systems, processes. Thus it does not purport to replace economic values but to provide additional information, from a different point of view.\n\nThe idea that a calorie of sunlight is not equivalent to a calorie of fossil fuel or electricity strikes many as absurd, based on the 1st Law definition of energy units as measures of heat (i.e. Joule's mechanical equivalent of heat). Others have rejected the concept as impractical since from their perspective it is impossible to objectively quantify the amount of sunlight that is required to produce a quantity of oil. In combining systems of humanity and nature and evaluating environmental input to economies, mainstream economists criticize the emergy methodology for disregarding market values.\n\n"}
{"id": "28815488", "url": "https://en.wikipedia.org/wiki?curid=28815488", "title": "Experimental Lakes Area", "text": "Experimental Lakes Area\n\nIISD Experimental Lakes Area (IISD-ELA, known as ELA before 2014) is an internationally unique research station encompassing 58 formerly pristine freshwater lakes in Kenora District Ontario, Canada. Previously run by Fisheries and Oceans Canada, after being de-funded by the Canadian Federal Government, the facility is now managed and operated by the International Institute for Sustainable Development (IISD) and has a mandate to investigate the aquatic effects of a wide variety of stresses on lakes and their catchments. IISD-ELA uses the whole ecosystem approach and makes long-term, whole-lake investigations of freshwater focusing on eutrophication.\n\nIn an article published in AAAS's well-known scientific journal \"Science,\" Eric Stokstad described ELA's \"extreme science\" as the manipulation of whole lake ecosystem with ELA researchers collecting long-term records for climatology, hydrology, and limnology that address key issues in water management. The site has influenced public policy in water management in Canada, the USA, and around the world.\n\nMinister of State for Science and Technology, Gary Goodyear, argued that \"our government has been working hard to ensure that the Experimental Lakes Area facility is transferred to a non-governmental operator better suited to conducting the type of world-class research that can be undertaken at this facility\" and that \"[t]he federal government has been leading negotiations in order to secure an operator with an international track record.\" On April 1, 2014, the International Institute for Sustainable Development announced that it had signed three agreements to ensure that it will be the long-term operator of the research facility and that the facility would henceforth be called IISD Experimental Lakes Area. Since taking over the facility, IISD has expanded the function of the site to include educational and outreach opportunities and a broader research portfolio.\n\nIn 1968, the Province of Ontario and the Government of Canada set aside an area in a sparsely inhabited region of central Canada, southeast of Kenora, Ontario, which is relatively unaffected by external human influences and industrial activities, for experimental studies of the causes and control of eutrophication and other types of water pollution. It included 46 small, deep, pristine lakes and their catchment areas in the Precambrian Shield.\n\nThe ELA project originated as a Canadian governmental response to the International Joint Commission (IJC)'s recommendation (1965) to Canada and the United States for additional support for studies on transboundary pollution in the lower Great Lakes.\n\nIn the 1960s, there was a widespread concern about the consequences of eutrophication but there was a lack of solid scientific evidence. Dr. W. E. Johnson of the Freshwater Institute of Winnipeg (Manitoba, Canada) convinced the Canadian government that unimpeachable evidence could be obtained by experimental pollution of pristine lakes through controlled overfertilization of specified elements. The Experimental Lakes Area was established in 1968 by the Fisheries Research Board of Canada. Dr. John Reubec Vallentyne and Dr. W. E. Johnson of the Freshwater Institute created the Experimental Lakes Area (ELA). While Vallentyne was Scientific Leader of the Eutrophication Section from 1966 to 1972, he attracted a stellar staff of scientists from around the world in the late 1960s and early 1970s. He recruited a junior scientist, David W. Schindler. Schindler, who would become one of the world's leading limnologists, would direct ELA projects from 1968 to 1989.\n\nIn 1969, the fertilization experiment began with Lake 227, and in 1973, the double-basin eutrophication experiment on Lake 226 began, in which a section of the Lake 226S was overfertilized with carbon and nitrogen and the other section 226N with carbon and nitrogen as well as phosphorus. The iconic image of the green eutrophied section 226N has been described as the most important in the history of limnology. It convinced the public and policy-makers that phosphorus levels needed to be controlled.\n\n\"Work at the ELA has produced important evidence on the effects of acid rain and led to the discovery that phosphates from household detergents cause algal blooms. It has elucidated the impacts on fish of mercury and shown how wetland flooding for hydroelectricity leads to increased production of greenhouse gases.\"\n\nIn the years leading up to the 2001 Gerhard Herzberg Gold Medal for Science and Engineering award, Schindler's research \"demonstrated the cumulative impacts on boreal lake life of global warming, acidification and ozone depletion. Using long-term reference data collected at the ELA, he has shown that climate warming and drought have severe and previously unrecognized effects on the physics, chemistry, and biology of lakes.\"\n\nELA has produced 745 peer-reviewed scientific articles, 126 graduate theses, 102 book chapters and synthesis papers, 185 data reports, and several books. ELA scientists have been the recipients of numerous prestigious international water awards, including the Stockholm Water Prize, the Tyler Prize for Environmental Achievement and the Gerhard Herzberg Gold Medal for Science and Engineering.\n\nAccording to John Shearer, who worked at the ELA as Senior Biologist and Operations Manager, from 1969 until his retirement 2007, 47 PhD candidates completed dissertations and 80 master’s students completed theses, using research they participated in at the ELA.\n\nHundreds of peer-reviewed articles have been based on ELA research in journals such as \"Environmental Toxicology and Chemistry\", \"Canadian Journal of Earth Sciences\", \"Atmospheric Chemistry and Physics (ACP)\", \"Transactions of the American Fisheries Society\", \"Hydrological Processes\", \"Limnology and Oceanography\", \"Environmental Science and Technology\", including at least 184 in the \"Canadian Journal of Fisheries and Aquatic Sciences\" and 30 in \"Biogeochemistry\".\n\nThe Bill C-38 Jobs, Growth and Long-term Prosperity Act(informally referred to as Bill C-38) is an Act of the Parliament of Canada, an omnibus bill passed as a 2012 Budget Implementation Act in June 2012.\n\nBill C-38 amended the Fisheries Act and closed the Experimental Lakes Area. Bill C-38 was given Royal Assent on June 29, 2012.\n\nIn 2012, the Department of Fisheries and Oceans announced that it planned to discontinue supporting the site at the end of the financial year, March 31, 2013, at a cost of $50 million. The site would be either decommissioned or handed to a third-party operator.\n\nSenator Jim Cowan at the 1st Session, 41st Parliament (June 21, 2012) expressed his concerns. \"There are a number of proposed changes to the Fisheries Act that are causing deep concern among Canadians. The bill amends the act to limit fish protection to the support of \"commercial, recreational and Aboriginal fisheries.\" Protection of fish habitat is relegated to a vastly lower priority — something that caused those four former fisheries ministers, in their words, \"especial alarm.\" Cowan also expressed dismay at the closure of Experimental Lakes Area.\n\nAccording to Elizabeth May,\n\nIn March 2013, with no advance notification to scientists whose personal belongings remained at the site or to the IISD, Department of Fisheries and Oceans began dismantling cabins that had been used by the Experimental Lakes Project scientists. Scientist Roberto Quinlan of the Society of Canadian Limnologists said that this move \"brings into serious doubt the government’s sincerity to actually transfer the facility over to another operator.\"\n\nThe decision to abruptly defund the ELA was widely condemned by the Canadian and international scientific community. The scientific journal \"Nature\" in 2012, described the decision as \"disturbing\", and said that it \"is hard to believe that finance is the true reason\" for the closure. An open letter from five prominent scientific organizations, the Association for the Sciences of Limnology and Oceanography, the Ecological Society of America, the International Society of Limnology, the Society of Canadian Limnologists, and the Society for Freshwater Sciences, expressed concern over the impact that a closure would have \"on the strong and creative science that has been, and continues to be, conducted by Canadian freshwater researchers.\" An organization of Canadian citizens and scientists, the Coalition to Save ELA has been formed to pressure the Canadian government to reverse the decision to close the Experimental Lakes Area.\n\nThe planned closure of the centre was the subject of an article 21 May 2012 in \"Nature\" journal.\n\nOn 25 May 2012 The North American Lake Management Society (NALMS), representing nearly 1,000 members-researchers, scientists, administrators, and citizens-wrote a letter of concern the imminent closure of ELA, arguing that NALMS' work \"depends on findings from the ELA.\" NALMS asked the federal government to reconsider. \"The Experimental Lakes Area (ELA) is a rare resource not only in Canada but throughout the world, as a dedicated research facility for ecosystem-scale experimental investigations and long-term monitoring of ecosystem processes. Operating for more than 40 years, it continues to study physical, chemical and biological processes and interactions operating on an ecosystem spatial scale and a multi-year time frame. These have led to extremely important discoveries. As an example, the world’s fertilizer industry now recognizes the importance of phosphorus in lakes and reservoirs, 40 years after its importance was demonstrated at ELA. Regulatory actions have been supported by ELA research, and now there is action by the industry as a result of research and activities at ELA over several decades. The experience gained at ELA by many scientists has resulted in the dissemination of environmental expertise and problem solving throughout the world, improving human conditions, protecting the environment, and saving millions of dollars for citizens and government agencies. Furthermore, we consider the work now in progress at ELA very important to the future of lake and reservoir management.\"\n\nMany Canadian MPs called on the Harper Government to reverse its decision on the forced closure of the Experimental Lakes Project. NDP MP Philip Toone argued that the \"internationally recognized program with huge spin-offs for Canada will cost more to close and move than the $2 million that the government hopes to save.\"\n\nThe federal government led negotiations with the Ontario government, the Manitoba government and the International Institute for Sustainable Development (IISD), headquartered in Winnipeg, to keep the area operational in 2013 and ensure longer-term operations. IISD is funded by the UN, governments including the Canadian government, international organizations and philanthropic foundations. It also gets money from universities and private-sector companies including TransCanada Energy, Enbridge and Manitoba Hydro. On April 1, 2014, IISD announced that three agreements have been signed involving IISD, the Government of Ontario and the Government of Canada that together ensure the long-term operation of the Experimental Lakes Area (ELA) research facility.\n\nOntario Premier Kathleen Wynne has committed support to keep the ELA \"operating in the long term after the federal Conservative government walks away from the world-class freshwater research station at the end of August 2013.\"\n\nDr. John Rudd, former DFO Research Scientist (1977-2002), ELA Chief Scientist (1998-2002), and winner of DFO’s most prestigious award, the Deputy Minister’s Prix d’Excellence (2002) argued that \"IISD is not qualified on a scientific basis to run ELA... ELA is a unique facility and its scientific research needs to be directed by scientists who know how to do these experiments, but unfortunately, almost all present and recently retired scientists have been cut out of the transfer process.\"\n\nElizabeth May, who served on the board of the International Institute for Sustainable Development for nine years prior to entering politics, argues that the \"IISD, a think tank, is not necessarily the right organization to take on this mandate. However, keeping the ELA open and functioning, and in a public and transparent context, was paramount.\"\n\nIn a January 2014 episode of CBC's The Fifth Estate, investigative reporter Linden MacIntyre examined the climate for scientists in Canada. He included a section on the closure of the ELA, and interviewed Dr. Schindler who continued to work on his area of concern for many years, the state of hydrology in the oil sands region.\n\nSince IISD took over the site in 2014, it has been called IISD Experimental Lakes Area (IISD-ELA). IISD-ELA has continued to carry out its whole-lake experimentation, but has also expanded its research portfolio to include research on nanosilver, microplastics, climate change and more. It also places a greater emphasis on educational and outreach opportunities and on garnering media coverage for its work.\n\n\n"}
{"id": "23045168", "url": "https://en.wikipedia.org/wiki?curid=23045168", "title": "Geoscience e-Journals", "text": "Geoscience e-Journals\n\nGeoscience e-Journals is an academic portal that includes 54 open access electronic journals, all dealing with geosciences. Most journals grouped under this aegis give a free and full access to their contents (except for one journal that has an embargo period). Some are electronic only, others are also available in paper-printed versions or in CD or DVD versions.\n\nGeoscience e-Journals was originally a web ring launched on March 27, 2003 by Bruno Granier; it was converted into a portal on April 15, 2006. This facility was made possible through joint efforts of RedIRIS and the Université de Bretagne Occidentale.\n\nThe list of journals at October 2009 includes:\n"}
{"id": "18238240", "url": "https://en.wikipedia.org/wiki?curid=18238240", "title": "Glossary of botanical terms", "text": "Glossary of botanical terms\n\nThis glossary of botanical terms is a list of terms relevant to botany and plants in general. Terms of plant morphology are included here as well as at the related Glossary of plant morphology and Glossary of leaf morphology. See also List of Latin and Greek words commonly used in systematic names. You can help by adding illustrations that assist an understanding of the terms.\n\n \n\n \n\n\n\n\n\n\n"}
{"id": "14073", "url": "https://en.wikipedia.org/wiki?curid=14073", "title": "Hydropower", "text": "Hydropower\n\nHydropower or water power (from , \"water\") is power derived from the energy of falling water or fast running water, which may be harnessed for useful purposes. Since ancient times, hydropower from many kinds of watermills has been used as a renewable energy source for irrigation and the operation of various mechanical devices, such as gristmills, sawmills, textile mills, trip hammers, dock cranes, domestic lifts, and ore mills. A trompe, which produces compressed air from falling water, is sometimes used to power other machinery at a distance.\n\nIn the late 19th century, hydropower became a source for generating electricity. Cragside in Northumberland was the first house powered by hydroelectricity in 1878 and the first commercial hydroelectric power plant was built at Niagara Falls in 1879. In 1881, street lamps in the city of Niagara Falls were powered by hydropower.\n\nSince the early 20th century, the term has been used almost exclusively in conjunction with the modern development of hydroelectric power. International institutions such as the World Bank view hydropower as a means for economic development without adding substantial amounts of carbon to the atmosphere,\nbut dams can have significant negative social and environmental impacts.\n\nIn India, water wheels and watermills were built, possibly as early as the 4th century BC, although records of that era are spotty at best.\n\nIn the Roman Empire, water-powered mills produced flour from grain, and were also used for sawing timber and stone; in China, watermills were widely used since the Han dynasty. In China and the rest of the Far East, hydraulically operated \"pot wheel\" pumps raised water into crop or irrigation canals.\n\nThe power of a wave of water released from a tank was used for extraction of metal ores in a method known as hushing. The method was first used at the Dolaucothi Gold Mines in Wales from 75 AD onwards, but had been developed in Spain at such mines as Las Médulas. Hushing was also widely used in Britain in the Medieval and later periods to extract lead and tin ores. It later evolved into hydraulic mining when used during the California Gold Rush.\n\nIn the Middle Ages, Islamic mechanical engineer Al-Jazari described designs for 50 devices, many of them water powered, in his book, \"The Book of Knowledge of Ingenious Mechanical Devices\", including clocks, a device to serve wine, and five devices to lift water from rivers or pools, though three are animal-powered and one can be powered by animal or water. These include an endless belt with jugs attached, a cow-powered shadoof, and a reciprocating device with hinged valves.\n\nIn 1753, French engineer Bernard Forest de Bélidor published \"Architecture Hydraulique\" which described vertical- and horizontal-axis hydraulic machines. By the late nineteenth century, the electric generator was developed by a team led by project managers and prominent pioneers of renewable energy Jacob S. Gibbs and Brinsley Coleberd and could now be coupled with hydraulics. The growing demand for the Industrial Revolution would drive development as well.\n\nAt the beginning of the Industrial Revolution in Britain, water was the main source of power for new inventions such as Richard Arkwright's water frame. Although the use of water power gave way to steam power in many of the larger mills and factories, it was still used during the 18th and 19th centuries for many smaller operations, such as driving the bellows in small blast furnaces (e.g. the Dyfi Furnace) and gristmills, such as those built at Saint Anthony Falls, which uses the 50-foot (15 m) drop in the Mississippi River.\n\nIn the 1830s, at the early peak in the US canal-building, hydropower provided the energy to transport barge traffic up and down steep hills using inclined plane railroads. As railroads overtook canals for transportation, canal systems were modified and developed into hydropower systems; the history of Lowell, Massachusetts is a classic example of commercial development and industrialization, built upon the availability of water power.\n\nTechnological advances had moved the open water wheel into an enclosed turbine or water motor. In 1848 James B. Francis, while working as head engineer of Lowell's Locks and Canals company, improved on these designs to create a turbine with 90% efficiency. He applied scientific principles and testing methods to the problem of turbine design. His mathematical and graphical calculation methods allowed the confident design of high-efficiency turbines to exactly match a site's specific flow conditions. The Francis reaction turbine is still in wide use today. In the 1870s, deriving from uses in the California mining industry, Lester Allan Pelton developed the high efficiency Pelton wheel impulse turbine, which utilized hydropower from the high head streams characteristic of the mountainous California interior.\n\nHydraulic power networks used pipes to carry pressurized water and transmit mechanical power from the source to end users. The power source was normally a head of water, which could also be assisted by a pump. These were extensive in Victorian cities in the United Kingdom. A hydraulic power network was also developed in Geneva, Switzerland. The world-famous Jet d'Eau was originally designed as the over-pressure relief valve for the network.\n\nWhere there is a plentiful head of water it can be made to generate compressed air directly without moving parts. In these designs, a falling column of water is purposely mixed with air bubbles generated through turbulence or a venturi pressure reducer at the high-level intake. This is allowed to fall down a shaft into a subterranean, high-roofed chamber where the now-compressed air separates from the water and becomes trapped. The height of the falling water column maintains compression of the air in the top of the chamber, while an outlet, submerged below the water level in the chamber allows water to flow back to the surface at a lower level than the intake. A separate outlet in the roof of the chamber supplies the compressed air. A facility on this principle was built on the Montreal River at Ragged Shutes near Cobalt, Ontario in 1910 and supplied 5,000 horsepower to nearby mines.\n\nHydropower is used primarily to generate electricity. Broad categories include:\nA hydropower resource can be evaluated by its available power. Power is a function of the hydraulic head and rate of fluid flow. The head is the energy per unit weight (or unit mass) of water. The static head is proportional to the difference in height through which the water falls. Dynamic head is related to the velocity of moving water. Each unit of water can do an amount of work equal to its weight times the head.\n\nThe power available from falling water can be calculated from the flow rate and density of water, the height of fall, and the local acceleration due to gravity.\nIn SI units, the power is:\n\nformula_1\n\nwhere\n\nTo illustrate, power is calculated for a turbine that is 85% efficient, with water at 1000 kg/cubic metre (62.5 pounds/cubic foot) and a flow rate of 80 cubic-meters/second (2800 cubic-feet/second), gravity of 9.81 metres per second squared and with a net head of 145 m (480 ft).\n\nIn SI units:\n\nIn English units, the density is given in pounds per cubic foot so acceleration due to gravity is inherent in the unit of weight. A conversion factor is required to change from foot lbs/second to kilowatts:\n\nOperators of hydroelectric stations will compare the total electrical energy produced with the theoretical potential energy of the water passing through the turbine to calculate efficiency. Procedures and definitions for calculation of efficiency are given in test codes such as ASME PTC 18 and IEC 60041. Field testing of turbines is used to validate the manufacturer's guaranteed efficiency. Detailed calculation of the efficiency of a hydropower turbine will account for the head lost due to flow friction in the power canal or penstock, rise in tail water level due to flow, the location of the station and effect of varying gravity, the temperature and barometric pressure of the air, the density of the water at ambient temperature, and the altitudes above sea level of the forebay and tailbay. For precise calculations, errors due to rounding and the number of significant digits of constants must be considered.\n\nSome hydropower systems such as water wheels can draw power from the flow of a body of water without necessarily changing its height. In this case, the available power is the kinetic energy of the flowing water. Over-shot water wheels can efficiently capture both types of energy.\nThe water flow in a stream can vary widely from season to season. Development of a hydropower site requires analysis of flow records, sometimes spanning decades, to assess the reliable annual energy supply. Dams and reservoirs provide a more dependable source of power by smoothing seasonal changes in water flow. However reservoirs have significant environmental impact, as does alteration of naturally occurring stream flow. The design of dams must also account for the worst-case, \"probable maximum flood\" that can be expected at the site; a spillway is often included to bypass flood flows around the dam. A computer model of the hydraulic basin and rainfall and snowfall records are used to predict the maximum flood.\n\nAs with other forms of economic activity, hydropower projects can have both a positive and a negative environmental and social impact, because the construction of a dam and power plant, along with the impounding of a reservoir, creates certain social and physical changes.These environmental and social impact are not limited to but include the removal of communities near the dam, as well as the hindrance of fish migration. Since water is being taken away, this also affects local communities access and amount of drinking water. There is also evidence that dams affect the soil, which then alters the ability of vegetation to grow.\nHydropower projects can also have indirect consequences, contributing to global warming: reservoirs accumulate plant material, which then decomposes, emitting methane in uneven bursts.\n\nThere are several tools to assess the impact of hydropower projects:\n\n\n\n\n\n\n\n"}
{"id": "47291523", "url": "https://en.wikipedia.org/wiki?curid=47291523", "title": "Ingression coast", "text": "Ingression coast\n\nAn ingression coast or depressed coast is a generally level coastline that is shaped by the penetration of the sea as a result of crustal movements or a rise in the sea level.\n\nSuch coasts are characterised by a subaerially formed relief that has previously experienced little deformation by littoral (tidal) processes, because the sea level, which had fallen by more than 100 metres during the last glacial period, did not reach its current level until about 6,000 years ago.\n\nDepending on the geomorphological shaping of the flooded landform – e. g. glacially or fluvially formed relief – various types of ingression coast emerge, such as rias, skerry and fjard coasts as well as förde and bodden coasts.\n\n"}
{"id": "9001558", "url": "https://en.wikipedia.org/wiki?curid=9001558", "title": "Islamic garden", "text": "Islamic garden\n\nTraditionally, an Islamic garden is a cool place of rest and reflection, and a reminder of paradise. The Qur'an has many references to gardens, and the garden is used as an earthly analogue for the life in paradise which is promised to believers:\n\nThere are surviving formal Islamic gardens in a wide zone extending from Spain and Morocco in the west to India in the east. Famous Islamic gardens include those of the Taj Mahal in India and the Generalife and Alhambra in Spain.\n\nThe general theme of a traditional Islamic garden is water and shade, not surprisingly since Islam came from and generally spread in a hot and arid climate. Unlike English gardens, which are often designed for walking, Islamic gardens are intended for rest and contemplation. For this reason, Islamic gardens usually include places for sitting.\n\nFairchild Ruggles refers to the universal nature of gardening, and the basic human needs it fulfills; the needs to cultivate, to master the wild landscape, and to bring order to it. The spiritual aspects of gardening, according to this view, were a later development. She further points out the classic formal garden, known as the Charbagh (or Chahar Bagh), is but one form which exists in the Islamic civilization; a civilization which has traditionally included peoples of many faiths and cultures.\n\nClifford A. Wright, an author on Mediterranean cuisine, describes different garden types for different purposes:\n\nAfter the Arab invasions of the 7th century CE, the traditional design of the Persian garden was used in the Islamic garden. Persian gardens after that time were traditionally enclosed by walls and were designed to represent paradise; the Persian word for enclosed space is 'pairi-daeza.' In the Charbagh, or paradise garden, four water canals typically carry water into a central pool or fountain, interpreted as the four rivers in paradise, filled with milk, honey, wine and water. Hellenistic influences are also apparent; the Western use of straight lines in the plan is blended with Sassanid ornamental plantations and fountains.\n\nMany of the gardens of Islamic civilization are lost to us today. While most others may retain their forms, the original plantings have been replaced with modern ones. The garden is a transient form of architectural art dependent upon the climate, and the resources available to those who care for it.\n\nEvliya Çelebi's 17th century CE Seyahatname (travel book) contains descriptions of Paradise Gardens around the towns of Berat and Elbasan, Albania. According to Dr. Robert Elsie, an expert on Albanian culture, very few traces of the refined oriental culture of the Ottoman era remain here today.\n\nÇelebi describes the town of Berat:\n\nÇelebi describes the town of Elbasan:\n\nDar al-Bahr, the Lake Palace, is situated on the southern end of Beni Hammad Fort, a ruined fortified city which has remained uninhabited for 800 years. Artifacts recovered from the site attest to a high degree of civilization. During its time, it was remarked upon by visitors for the nautical spectacles enacted in its large pool. Surrounding the pool and the palace were terraces, courtyards and gardens. Little is known of the details of these gardens, other than the lion motifs carved in their stone fountains. Beni Hammad Fort is a UNESCO World Heritage Site, noted as an \"authentic picture of a fortified Muslim city.\"\n\nThe name Shalimar is thought to mean, among other things, \"abode of bliss\" or \"light of the moon\". There were originally three gardens with the name Shalimar: one in Lahore, Pakistan, another in Jammu Kashmir, India and finally one, located in Delhi, which has completely disappeared.\n\nShalimar Gardens, Lahore, was built by the governor of Lahore, with funds supplied by the Mughal emperor Shah Jahan, beginning in 1641 CE. The water is supplied by a canal dug from the nearby Ravi river. Built in the Mughal style, it is surrounded by high walls with towers in the corners. The inner face of the walls have traces of frescoes done in floral patterns. The canal passes through the gardens, which are constructed on three separate terraces at different elevations. The garden terraces are laid out in the traditional \"paradise\" motif of four channels converging on a central fountain, and cover a total of forty acres. \n\nThe garden was a common feature of homes in Arab Spain. Andalusian designs emphasized privacy and coolness, with rooms opening onto a roofed, open corridor. Next to this corridor, one would typically find a verdant patio garden complete with central fountain.\n\nAl-Azhar Park is a modern landmark in Cairo, Egypt. It is laid out along a central series of terraced, formal Islamic gardens. Multicolored Mamluk stonework, fountains and Islamic geometric patterns are the predominating stylistic theme of the park. It is listed as one of the world's sixty great public spaces by the Project for Public Spaces (PPS).\n\n\n\n"}
{"id": "988796", "url": "https://en.wikipedia.org/wiki?curid=988796", "title": "Jevons paradox", "text": "Jevons paradox\n\nIn economics, the Jevons paradox (; sometimes Jevons effect) occurs when technological progress or government policy increases the efficiency with which a resource is used (reducing the amount necessary for any one use), but the rate of consumption of that resource rises due to increasing demand. The Jevons paradox is perhaps the most widely known paradox in environmental economics. However, governments and environmentalists generally assume that efficiency gains will lower resource consumption, ignoring the possibility of the paradox arising.\n\nIn 1865, the English economist William Stanley Jevons observed that technological improvements that increased the efficiency of coal-use led to the increased consumption of coal in a wide range of industries. He argued that, contrary to common intuition, technological progress could not be relied upon to reduce fuel consumption.\n\nThe issue has been re-examined by modern economists studying consumption rebound effects from improved energy efficiency. In addition to reducing the amount needed for a given use, improved efficiency also lowers the relative cost of using a resource, which increases the quantity demanded. This counteracts (to some extent) the reduction in use from improved efficiency. Additionally, improved efficiency increases real incomes and accelerates economic growth, further increasing the demand for resources. The Jevons paradox occurs when the effect from increased demand predominates, and improved efficiency increases the speed at which resources are used.\n\nConsiderable debate exists about the size of the rebound in energy efficiency and the relevance of the Jevons paradox to energy conservation. Some dismiss the paradox, while others worry that it may be self-defeating to pursue sustainability by increasing energy efficiency. Some environmental economists have proposed that efficiency gains be coupled with conservation policies that keep the cost of use the same (or higher) to avoid the Jevons paradox. Conservation policies that increase cost of use (such as cap and trade or green taxes) can be used to control the rebound effect.\n\nThe Jevons paradox was first described by the English economist William Stanley Jevons in his 1865 book \"The Coal Question\". Jevons observed that England's consumption of coal soared after James Watt introduced the Watt steam engine, which greatly improved the efficiency of the coal-fired steam engine from Thomas Newcomen's earlier design. Watt's innovations made coal a more cost-effective power source, leading to the increased use of the steam engine in a wide range of industries. This in turn increased total coal consumption, even as the amount of coal required for any particular application fell. Jevons argued that improvements in fuel efficiency tend to increase (rather than decrease) fuel use, writing: \"It is a confusion of ideas to suppose that the economical use of fuel is equivalent to diminished consumption. The very contrary is the truth.\"\n\nAt that time, many in Britain worried that coal reserves were rapidly dwindling, but some experts opined that improving technology would reduce coal consumption. Jevons argued that this view was incorrect, as further increases in efficiency would tend to increase the use of coal. Hence, improving technology would tend to increase the rate at which England's coal deposits were being depleted, and could not be relied upon to solve the problem.\n\nAlthough Jevons originally focused on the issue of coal, the concept has since been extended to the use of any resource, including, for example, water usage and interpersonal contact. It is perhaps the most widely known paradox in environmental economics.\n\nEconomists have observed that consumers tend to travel more when their cars are more fuel efficient, causing a 'rebound' in the demand for fuel. An increase in the efficiency with which a resource (e.g. fuel) is used, causes a decrease in the cost of using that resource when measured in terms of what it can achieve (e.g. travel). Generally speaking, a decrease in the cost (or price) of a good or service will increase the quantity demanded (the law of demand). With a lower cost for travel, consumers will travel more, increasing the demand for fuel. This increase in demand is known as the rebound effect, and it may or may not be large enough to offset the original drop in demand from the increased efficiency. The Jevons paradox occurs when the rebound effect is greater than 100%, exceeding the original efficiency gains.\n\nThe size of the direct rebound effect is dependent on the price elasticity of demand for the good. In a perfectly competitive market where fuel is the sole input used, if the price of fuel remains constant but efficiency is doubled, the effective price of travel would be halved (twice as much travel can be purchased). If in response, the amount of travel purchased more than doubles (i.e. demand is price elastic), then fuel consumption would increase, and the Jevons paradox would occur. If demand is price inelastic, the amount of travel purchased would less than double, and fuel consumption would decrease. However, goods and services generally use more than one type of input (e.g. fuel, labour, machinery), and other factors besides input cost may also affect price. These factors tend to reduce the rebound effect, making the Jevons paradox less likely to occur.\n\nIn the 1980s, economists Daniel Khazzoom and Leonard Brookes revisited the Jevons paradox for the case of society's energy use. Brookes, then chief economist at the UK Atomic Energy Authority, argued that attempts to reduce energy consumption by increasing energy efficiency would simply raise demand for energy in the economy as a whole. Khazzoom focused on the narrower point that the potential for rebound was ignored in mandatory performance standards for domestic appliances being set by the California Energy Commission.\n\nIn 1992, the economist Harry Saunders dubbed the hypothesis that improvements in energy efficiency work to increase (rather than decrease) energy consumption the \"Khazzoom–Brookes postulate\", and argued that the hypothesis is broadly supported by neoclassical growth theory (the mainstream economic theory of capital accumulation, technological progress and long-run economic growth). Saunders showed that the Khazzoom–Brookes postulate occurs in the neoclassical growth model under a wide range of assumptions.\n\nAccording to Saunders, increased energy efficiency tends to increase energy consumption by two means. First, increased energy efficiency makes the use of energy relatively cheaper, thus encouraging increased use (the direct rebound effect). Second, increased energy efficiency increases real incomes and leads to increased economic growth, which pulls up energy use for the whole economy. At the microeconomic level (looking at an individual market), even with the rebound effect, improvements in energy efficiency usually result in reduced energy consumption. That is, the rebound effect is usually less than 100%. However, at the macroeconomic level, more efficient (and hence comparatively cheaper) energy leads to faster economic growth, which increases energy use throughout the economy. Saunders argued that, taking into account both microeconomic and macroeconomic effects, technological progress that improves energy efficiency will tend to increase overall energy use.\n\nThere is considerable debate about whether the Khazzoom-Brookes Postulate is correct, and of the relevance of the Jevons paradox to energy conservation policy. Most governments, environmentalists and NGOs pursue policies that improve efficiency, holding that these policies will lower resource consumption and reduce environmental problems. Others, including many environmental economists, doubt this 'efficiency strategy' towards sustainability, and worry that efficiency gains may in fact lead to higher production and consumption. They hold that for resource use to fall, efficiency gains should be coupled with other policies that limit resource use. However, other environmental economists point out that, while the Jevons Paradox may occur in some situations, the empirical evidence for its widespread applicability is limited. \n\nJevons warned that fuel efficiency gains tend to increase fuel use. However, this does not imply that improved fuel efficiency is worthless if the Jevons paradox occurs; higher fuel efficiency enables greater production and a higher material quality of life. For example, a more efficient steam engine allowed the cheaper transport of goods and people that contributed to the Industrial Revolution. Nonetheless, if the Khazzoom–Brookes postulate is correct, increased fuel efficiency, by itself, will not reduce the rate of depletion of fossil fuels.\n\nThe Jevons paradox is sometimes used to argue that energy conservation efforts are futile, for example, that more efficient use of oil will lead to increased demand, and will not slow the arrival or the effects of peak oil. This argument is usually presented as a reason not to enact environmental policies or pursue fuel efficiency (e.g. if cars are more efficient, it will simply lead to more driving). Several points have been raised against this argument. First, in the context of a mature market such as for oil in developed countries, the direct rebound effect is usually small, and so increased fuel efficiency usually reduces resource use, other conditions remaining constant. Second, even if increased efficiency does not reduce the total amount of fuel used, there remain other benefits associated with improved efficiency. For example, increased fuel efficiency may mitigate the price increases, shortages and disruptions in the global economy associated with peak oil. Third, environmental economists have pointed out that fuel use will unambiguously decrease if increased efficiency is coupled with an intervention (e.g. a fuel tax) that keeps the cost of fuel use the same or higher.\n\nThe Jevons paradox indicates that increased efficiency by itself may not reduce fuel use, and that sustainable energy policy must rely on other types of government interventions. As the imposition of conservation standards or other government interventions that increase cost of use do not display the Jevon paradox, they can be used to control the rebound effect. To ensure that efficiency-enhancing technological improvements reduce fuel use, efficiency gains can be paired with government intervention that reduces demand (e.g. green taxes, cap and trade, or higher emissions standards). The ecological economists Mathis Wackernagel and William Rees have suggested that any cost savings from efficiency gains be \"taxed away or otherwise removed from further economic circulation. Preferably they should be captured for reinvestment in natural capital rehabilitation.\" By mitigating the economic effects of government interventions designed to promote ecologically sustainable activities, efficiency-improving technological progress may make the imposition of these interventions more palatable, and more likely to be implemented.\n\n\nNotes\nFurther reading\n"}
{"id": "36951734", "url": "https://en.wikipedia.org/wiki?curid=36951734", "title": "Kaigas", "text": "Kaigas\n\nThe Kaigas glaciation was a hypothesized snowball earth event in the Neoproterozoic Era, preceding the Sturtian glaciation. Its occurrence was inferred based on the interpretation of Kaigas Formation conglomerates in the stratigraphy overlying the Kalahari Craton as correlative with pre-Sturtian Numees formation glacial diamictites. However, the Kaigas formation was later determined to be non-glacial, and a Sturtian age was assigned to the Numees diamictites. Thus, there is no longer any evidence for a Neoproterozoic glaciation prior to the Sturtian snowball earth event.\n"}
{"id": "31375734", "url": "https://en.wikipedia.org/wiki?curid=31375734", "title": "Köyceğiz-Dalyan Special Environmental Protection Area", "text": "Köyceğiz-Dalyan Special Environmental Protection Area\n\nThe Köyceğiz-Dalyan Special Environmental Protection Area is a protected natural reserve in the Turkish province of Muğla. In June 1988 it was determined and declared the first protected area of its kind (\"Özel Çevre Koruma Bölgesi\") of Turkey. In 1990 the original SPA area was extended westwards. \nUp to now, there are fourteen natural reserves with this status, of which Pamukkale is probably the best-known. All these areas are under the supervision of the ÖÇKK, the Turkish Environmental Protection Agency for Special Areas.\n\nThe area got its special status as a result of Prince Philip´s request to the Turkish Prime Minister Turgut Özal for a moratorium on the construction of a hotel complex at İztuzu Beach, while awaiting an environmental impact assessment. At the time Prince Philip was President of the WWF, which had been approached by environmentalists such as June Haimoff, Günther Peter, David Bellamy, Lily Venizelos, Nergis Yazgan and Keith Corbett to help stop the construction of a hotel complex at the beach. İztuzu Beach was one of the main nesting areas for the endangered loggerhead turtle (\"Caretta caretta\"), and the environmentalists were trying to preserve the turtle’s habitat. In September 1987 the construction project was suspended for an environmental impact assessment, and in 1988 the Turkish government decided upon a construction prohibition because of the area’s special significance. That was not only because of the natural importance of the beach and its significance as a turtle habitat, but also because of the cultural and historical significance and the geological importance of the Dalyan-Köyceğiz hinterland.\nBecause of its protected status, the area offers good and ample opportunities for ecotourism and recreation. For one, the Köyceğiz-Dalyan SEPA boasts the most fantastic panoramic vistas.\n\nFrom 1990 onwards the protected status of the area was elaborated in a number of sustainable environmental projects, in order to:\n\nThe SEPA covers an area of 461 km and includes the Köyceğiz, Toparlar, Beyobaşı and Dalyan districts. There are 17 settlements of which Köyceğiz on the north bank of the lake, and the tourist town of Dalyan on the river emerging from the lake are the most important ones.\n\nThe area consists of various terrestrial structures around the Köyceğiz subsidence lake. \nKöyceğiz Lake is one of Turkey’s largest coastal lakes, which was formed in Antiquity as a result of the formation of a long stretch of dunes (İztuzu Beach). The lake is fed by the waters of the Namnam river in the northwest and the Yuvarlakçay in the east. The latter is reported to be an otter habitat. In the north and west some mountain streams flow into the lake. \nIt is the biotope variety in such a relatively small area that makes the SEPA so special. There are fresh water lakes, rivers and brooks, a vast brackish water zone in the estuary, the sea and its sandy beach. Apart from agricultural land for growing citrus fruit, sesame and cotton, there are wetlands, extensive liquidambar forests, pine forests and maquis.\nAround the lake there are numerous small wetlands with a rich bird population. Among the birds are the penduline tit, the Eurasian reed warbler, the great reed warbler and black-crowned night heron. The wetlands often border marshy Turkish sweetgum forests. The sweetgum forests are often intersected by ditches and frequently have an undergrowth of horse tail.\nThe Turkish or Oriental sweetgum is an endemic species that thrives in boggy areas and is often seen in combination with plane trees. The species has traditionally been important for the extraction of its sap, which is processed to balsam storax or Turkish sweetgum oil. The sap, which is collected by carefully stripping part of the bark, is used for medical purposes and for the production of perfume. For this reason, the forests used to be of major economical importance. However, there are fewer and fewer master oil makers who can pass on their trade. \nUnfortunately, the liquidambar forest area has been reduced by illegal tree felling to create new or vaster agricultural fields. Representative sweetgum forests can still be found on the Yuvarlakçay near the village of Kavakarası, on the bank north of Sultaniye, northwest of Hamitköy, at Köyceğiz and at Tepearası. At the south side of Lake Köyceğiz there is a NW-SE fault, along which several sulphurous thermal springs are located. \nThe only spa resort, at Sultaniye, was restored in the 90’s and has been a popular tourist attraction ever since. The water temperature amounts to 40 °C. Apart from the thermal spring, it also has a mud bath. There is another mud bath somewhat downriver at a former river branch. This is a location where the Nile turtle (\"Trionyx triunguis\") can be spotted. It is a turtle with a soft shell, thence also called African softshell turtle, which is about 1 metre in size. It is a protected, thermophile species. Consequently, it is often seen near the thermal springs along the fault line. The species occurs near to the sea, in lagoons, lakes, rivers and canals, both in fresh and brackish waters. The Nile turtle is mainly aquatic and will only leave the water for short periods to rest in the sun. In the Köyceğiz-Dalyan SEPA, oviposition takes place at the brackish waterside of İztuzu Beach. The Dalyan-Köyceğiz basin is in the utmost northwest of the species’ range.Northeast and southeast of Köyceğiz Lake there are lowlands, whereas other parts are surrounded by hills. The highest among these are the Ölemez (937 m) in the southwest, and Bozburun Hill (556 m) in the south. The hills at the periphery of the area are mainly covered with Turkish pine trees (\"Pinus brutia\"). \nThe Turkish pine is endemic for the eastern Mediterranean and Turkey is the main part of its range. It usually grows at lower altitudes, from sea level to 600 metres height. The tree reaches a height of 20–35 metres, with a maximum trunk diameter of approx. 1 metre. On the river opposite Dalyan are the rock tombs of Caunos, the capital of ancient Caria. From the topmost rows of its amphitheatre and its Acropolis one has a wonderful view of the river, Alagöl (Çandır bay) and the reeds at the estuary. Apart from its cultural and historical significance, the historic city is interesting for its abundance of reptiles, birds and insects. Many of the altogether 33 reptile species of the Special Environmental Protected Area can be found at Caunos. Among its most conspicuous species are the hardim or starred agama, the European glass lizard, the common chameleon, and the black whip snake. Moreover, there are numerous tortoises and terrapins at the site. Spectacular birds at and around Caunos are the rock nuthatch, the common kestrel, the blue rock-thrush and the European roller. The Dalyan estuary is a labyrinth of canals through the reeds. During a boat trip through the reeds one can see all sorts of birds, such as kingfishers, the little egret, the great egret and the cormorant. At the west side of the delta is Alagöl, the bay of Çandır, which is heated by a number of thermal springs. This may be the reason why the bay is the mating scene for the loggerhead turtle in the second half of April. \nOviposition takes place at İztuzu Beach in May and June. With an average number of nests of approx. 200 per annum (57-330 range over the years of recording), İztuzu Beach is one of the main breeding areas of the species in the Mediterranean. For this reason there are strict regulations to counter any disturbance\n\nSince May 2009 there has been a sea turtle centre at the beach, run by the Biology Department of Pamukkale University. The staff and volunteers patrol the beach and keep records during the breeding season. Injured turtles found at the beach or at the estuary are taken to the centre for treatment and rehabilitation. \nThe beach is also accessible by road. The route leads along the wetlands of Sülüngür Lake, which is shut off for boat traffic at the river side by a \"dalyan\" or fishing weir. \nOn Bozburun Tepesi above İztuzu Beach lies a radar station, which offers a wonderful panoramic vista of the Köyceğiz-Dalyan SEPA.\n\n"}
{"id": "36952641", "url": "https://en.wikipedia.org/wiki?curid=36952641", "title": "List of bioluminescent fungus species", "text": "List of bioluminescent fungus species\n\nFound largely in temperate and tropical climates, currently there are known more than 75 species of bioluminescent fungi, all of which are members of the order Agaricales (Basidiomycota) with one exceptional ascomycete belonging to the order Xylariales. All known bioluminescent Agaricales are mushroom-forming, white-spored agarics that belong to four distinct evolutionary lineages. The Omphalotus lineage (comprising the genera Omphalotus and \"Neonothopanus\") contains 12 species, the \"Armillaria\" lineage has 10 known species, while the Mycenoid lineage (\"Mycena\", \"Panellus\", \"Prunulus\", \"Roridomyces\") has more than 50 species. The recently discovered Lucentipes lineage contains two species, \"Mycena lucentipes\" and \"Gerronema viridilucens\", which belong to a family that has not yet been formally named. \"Armillaria mellea\" is the most widely distributed of the luminescent fungi, found across Asia, Europe, North America, and South Africa.\n\nBioluminescent fungi emit a greenish light at a wavelength of 520–530 nm. The light emission is continuous and occurs only in living cells. No correlation of fungal bioluminescence with cell structure has been found. Bioluminescence may occur in both mycelia and fruit bodies, as in \"Panellus stipticus\" and \"Omphalotus olearius\", or only in mycelia and young rhizomorphs, as in \"Armillaria mellea\". In \"Roridomyces roridus\" luminescence occurs only in the spores, while in \"Collybia tuberosa\", it is only in the sclerotia.\n\nAlthough the biochemistry of fungal bioluminescence has not fully been characterized, the preparation of bioluminescent, cell-free extracts has allowed researchers to characterize the \"in vitro\" requirements of fungal bioluminescence. Experimental data suggest that a two-stage mechanism is required. In the first, a light-emitting substance (called \"luciferin\") is reduced by a soluble reductase enzyme at the expense of NAD(P)H. In the second stage, reduced luciferin is oxidized by an insoluble luciferase that releases the energy in the form of bluish-green light. Conditions that affect the growth of fungi, such as pH, light and temperature, have been found to influence bioluminescence, suggesting a link between metabolic activity and fungal bioluminescence.\n\nAll bioluminescent fungi share the same enzymatic mechanism, suggesting that there is a bioluminescent pathway that arose early in the evolution of the mushroom-forming Agaricales. All known luminescent species are white rot fungi capable of breaking down lignin, found in abundance in wood. Bioluminescence is an oxygen-dependent metabolic process because it provides antioxidant protection against the potentially damaging effects of reactive oxygen species produced during wood decay. The physiological and ecological function of fungal bioluminescence has not been established with certainty. It has been suggested that in the dark beneath closed tropical forest canopies, bioluminescent fruit bodies may be at an advantage by attracting grazing animals (including insects and other arthropods) that could help disperse their spores. Conversely, where mycelium (and vegetative structures like rhizomorphs and sclerotia) are the bioluminescent tissues, the argument has been made that light emission could deter grazing.\n\nThe following list of bioluminescent mushrooms is based on a 2008 literature survey by Dennis Desjardin and colleagues, in addition to accounts of several new species published since then.\n\n\n"}
{"id": "32371350", "url": "https://en.wikipedia.org/wiki?curid=32371350", "title": "List of ecoregions in Somalia", "text": "List of ecoregions in Somalia\n\nThe following is a list of ecoregions in Somalia, as identified by the Worldwide Fund for Nature (WWF).\n\n\"by major habitat type\"\n\n\n\n\n\n"}
{"id": "395218", "url": "https://en.wikipedia.org/wiki?curid=395218", "title": "List of introduced species", "text": "List of introduced species\n\nA complete list of introduced species for even quite small areas of the world would be dauntingly long. Humans have introduced more different species to new environments than any single document can hope to record. This list is generally for established species with truly wild populations— not kept domestically—that have been seen numerous times, and have breeding populations. While most introduced species can cause a negative impact to new environments they reach, some can have a positive impact, just for conservation purpose.\n\n\n\n\n\n\n\n\n\n\nAround 15% of Australia's flora is made up of introduced species. The following is a non-inclusive list of some of the more significant plant species. \n\n\nFurther information can be found at the GB Non-native Species Secretariat, which has a free tool kit of resources on non-native species, including a photo gallery, ID sheets, risk assessments, projects database, case studies and resources for local action groups.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<section begin=Hawaii_mammals/>*Wild pig\n\nPrimary source for this list is Robert L. Pyle and Peter Pyle, \"The Birds of the Hawaiian Islands\" unless otherwise stated\n\n\n<section begin=Hawaii_reptiles/>\n<section end=Hawaii_reptiles/>\n\n<section begin=Hawaii_amphibians/>Primary source for this list is Nonindigenous Aquatic Species Database unless otherwise stated.\n\nPrimary source for this list is Nonindigenous Aquatic Species Database unless otherwise stated.\n\n<section begin=Hawaii_arthropods/>* \"Adoretus sinicus\" (Chinese rose beetle) \n\n\n\n\n\n\n\n\n\n\n\nUp to 26,000 plants have been introduced into New Zealand. This list is a few of the more common and more invasive species.\n\n\n\n\n\nPrimary source for this list is Nonindigenous Aquatic Species Database unless otherwise stated.\n\nPrimary source for this list is Nonindigenous Aquatic Species Database unless otherwise stated.\n\nPrimary source for this list is Nonindigenous Aquatic Species Database unless otherwise stated.\n\nMarine\nFreshwater\nTerrestrial\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is a non-exhaustive list of some of the more significant plant species\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "42200287", "url": "https://en.wikipedia.org/wiki?curid=42200287", "title": "List of invasive plant species in California", "text": "List of invasive plant species in California\n\nA list of invasive plant species in California. \n\nNumerous plants have been introduced to the California Floristic Province and within the state's borders. Many of them have become invasive species and/or noxious weeds.\nThe following are some of these species:\n\n\n"}
{"id": "4162239", "url": "https://en.wikipedia.org/wiki?curid=4162239", "title": "List of mountains in Norway by height", "text": "List of mountains in Norway by height\n\nThere are 291 peaks in Norway with elevations over sea level of or more and that have a topographic prominence of more than 10 meters. The following list includes those 186 that have a topographic prominence of 50 meters or more. The Topographic isolation refers to the shortest \"horizontal\" distance one would have to travel to find a higher summit.\n\nMost of these peaks are in the municipalities of Lom, Skjåk, Luster, and Vågå, connected to the mountain chain that reaches its prominence with Jotunheimen. There are also several peaks in Dovrefjell, Rondane, Dovre, Lesja, and Folldal that also reach above 2000 meters. All the peaks are to be found in 14 topographical maps () published by the Norwegian government cartography office, of which 21 peaks are in \"1518 II Galdhøpiggen\", 18 in \"1618 III Glittertinden\", and 13 in \"1617 IV Gjende\". The northernmost is in the Dovre area, meaning there are no 2000 m peaks in northern Norway, even though there are some almost 2000 m there, and some above 2000 m in Sweden near the border.\n\nMost of the difficult summits were ascended in the late 19th and early 20th century by a combination of Norwegian explorers, local guides (in particular the Sulheim family), and British adventurers. Five women - Margaret S. Green, Theresa Bertheau, Antonette Kamstrup, Anne Aukrust, and Rønnaug Garmo - were the first to ascend some of these peaks. Aukrust and Garmo were from farms in Lom, where many of the peaks are located. The peak most recently climbed for the first time was Veobrehesten, first ascended in 1949.\n\nThe listing originates from www.nfo2000m.no.\n\n\n"}
{"id": "5898579", "url": "https://en.wikipedia.org/wiki?curid=5898579", "title": "List of national parks of Montenegro", "text": "List of national parks of Montenegro\n\nMontenegro has five national parks which cover approximately 10 percent of the country's territory. The parks are managed by the National Parks of Montenegro government agency (Montenegrin: \"Nacionalni parkovi Crne Gore\").\n"}
{"id": "13364923", "url": "https://en.wikipedia.org/wiki?curid=13364923", "title": "List of spider species of Madagascar", "text": "List of spider species of Madagascar\n\nThis is a list of spider species that occur on Madagascar. Unless otherwise noted, they are endemic (they occur nowhere else). Some cosmopolitan or pantropical species that occur also in Madagascar may be missing.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "4540036", "url": "https://en.wikipedia.org/wiki?curid=4540036", "title": "List of tornadoes striking downtown areas of large cities", "text": "List of tornadoes striking downtown areas of large cities\n\nThis article is a list of tornadoes that have impacted the central business district (downtown or city centre) of a large city (that is, one having at least 50,000 people, not counting suburbs or outlying communities, at the time of the storm).\n\nIt is a common myth that tornadoes do not strike downtown areas. The odds are much lower due to the small areas covered, but paths can go anywhere, including over downtown areas. St. Louis, Missouri has taken a direct hit four times in less than a century. Many of the tornadoes listed were extremely destructive or caused numerous casualties, and the occurrence of a catastrophic event somewhere is inevitable.\n\nThis list is not exhaustive (listing every single tornado that has struck a downtown area or central business district of any city), as it may never be known if a tornado struck a downtown area, or if it was just a microburst (powerful downward and outward gush of wind, which cause damage from straight-line winds), particularly for older events or from areas with limited information. Downbursts often accompany intense tornadoes, extending damage across a wider area than the tornado path. When a tornado strikes a city, it is occasionally very difficult to determine whether it was a tornadic event at all or if the affected area was indeed the \"downtown\", \"city centre\", or \"central business district\" consisting of very high population density and mid to high-rises, as opposed to other heavily urbanized/built-up parts of the city or suburbs. It is sometimes also difficult to determine tornadoes that strike urban cores before 1950, when tornado records (particularly in the US) started to be consistently logged with detail. Before this, lack of details on information from the events, as well as that most cities were far smaller in area and population complicate the record.\n\nFor the list of cities that are not listed here for certain reasons, see below.\n\nNote: The F-Scale was superseded by the EF scale in the U.S. on February 1, 2007, and in Canada on April 18, 2013\nFor tornadoes and cities in: United States, Canada, Mexico, Bahamas, Cuba, Central America, and The Caribbean\n\nSouth America has no default tornado strength measurement system, so the storms here will be listed using the Fujita Scale.\n\nNote: The UK uses the TORRO scale.\n\nAfrica has no default tornado strength measurement system, so the storms here will be listed using the Fujita Scale.\n\nMost of Asia has no default tornado strength measurement system (though Japan has been known to use the Fujita Scale in the past), so the storms here will be listed using the Fujita Scale.\n\nThe 1985 Barrie Tornado that struck Barrie, Ontario would be listed here, had it struck today. It is not listed, however, since the town's population was under 50,000 at the time (May 31, 1985). The tornado that hit Grand Valley, Ontario is not listed for similar reasons. Similarly, the 1999 Oklahoma tornado outbreak that spawned the deadly F5 tornado which struck Oklahoma City and some of its suburbs, including Moore and Midwest City, is not listed because it did not reach the downtown core of Oklahoma City.\n\nSimilarly, the downtown areas of two then-small towns (now large cities) in North Carolina were struck during the 1884 Enigma outbreak: Concord and Cary. Downtown Concord was struck a second time by a tornado in May 1936.\n\nThe 1987 Edmonton Tornado is likewise not listed because it struck industrial parks, trailer parks, and suburban areas, and was far away from Edmonton's downtown core. The \"Oak Lawn tornado\" of April 21, 1967 which killed 33 people, mostly those in rush hour traffic at a busy intersection, and moved across southern Chicago onto Lake Michigan is not included because it missed the downtown core. Most recently, the 2008 Memphis tornado on February 5, 2008 also missed the downtown area (by a significant distance).\n\nThe 1998 tornado that hit Spencer, South Dakota, killing 6 people, the 2000 tornado that hit a campground, killing 12 people, the 2007 tornado that hit Greensburg, Kansas, killing 11 people, the Elie, Manitoba tornado that hit Elie, Manitoba in 2007 with no fatalities, and the 2009 tornado that hit Durham, Ontario, killing one person, are not listed because none of these tornadoes hit a metropolitan area that had a population higher than 50,000. The Southern Ontario tornado outbreak of 2005 that hit Toronto was not confirmed and is therefore not listed.\n\nAnother notable absence is the July 7, 1915 storm that struck Cincinnati, killing 38 people. This was determined to be most likely a windstorm causing downbursts or even a series of microbursts (with much of the damage coming from the straight-line winds), and not a tornado.\n\nEuropean tornadoes that are listed before 1950 are for cities that had at least 50,000 people in them at the time. Tornadoes dating back to 1054 are confirmed, due to extensive record-keeping for many weather events and other until-then unexplained weather occurrences.\n\nOn September 8, 2010, Tropical Storm Hermine went over the state of Texas and produced a few tornadoes in the Dallas–Fort Worth metroplex. One notable tornado was located in an industrialized area west of downtown Dallas before it lifted up over Interstate 35E just south of Dallas Love Field. However the EF-2 tornado was only about 3–4 miles away and did not strike the immediate downtown Dallas area.\n\n\n"}
{"id": "27232111", "url": "https://en.wikipedia.org/wiki?curid=27232111", "title": "Longest recorded sniper kills", "text": "Longest recorded sniper kills\n\nReports regarding the longest recorded sniper kills that contain information regarding the shooting distance and the identity of the sniper have been presented to the general public since 1967. Snipers in modern warfare have had a substantial history following the development of long distance weaponry. As weapons, ammunition, and aids to determine ballistic solutions improved, so too did the distance from which a kill could be targeted. In mid-2017 it was reported that an unnamed Canadian special forces operator, based in Iraq, had set a new record of , beating the record previously held by British Corporal Craig Harrison at 2,475 m (2,707 yd).\n\nAlthough technology such as electronics have improved, optical equipment such as rangefinders and ballistic calculators have eliminated manual mathematical calculations to determine elevation and windage, the fundamentals of accurate and precise long-range shooting remain largely the same since the early history of shooting, and the skill and training of the shooter and the shooter's spotter where applicable are the primary factors. Accuracy and precision of ammunition and firearms are also still reliant primarily on human factors and attention to detail in the complex process of producing maximum performance.\n\nThe modern method of long-distance sniping (shots over ) requires intense training and practice. A sniper must have the ability to accurately estimate the various factors that influence a bullet's trajectory and point of impact, such as the shooter's distance from the target, wind direction, wind speed, air density, elevation, and even the Coriolis effect due to the rotation of the Earth. Mistakes in estimation compound over distance and can cause a shot to only injure, or to miss completely. Any given combination of firearm and ammunition will have an associated value, known as the circular error probable (CEP), defined as the radius of a circle whose boundary is expected to contain the impact points of half of the rounds fired.\n\nIf the shooter wishes to improve accuracy and precision, wishes to increase range or wishes to do all of these things, the accuracy of \"estimates\" of external factors must improve accordingly. At extreme ranges, extremely accurate \"estimates\" are required and even with the most accurate estimates, hitting the target becomes subject to uncontrollable factors. For example, a rifle capable of firing a 1/2 MOA (approximately 1/2\" center to center of the two holes furthest apart) 5-round group (often referred to as \"grouping\") at 100 yards will fire a theoretical 12.5\" group at 2,500 yards. Unless the group is centered perfectly on the target at 100 yards, the 2,500-yard group will be centered 25 times the off-center error at 100 yards. This example ignores all other factors and assumes \"perfect\" no-wind shooting conditions and identical muzzle velocities and ballistic performance for each shot.\n\nUSMC Gunnery Sergeant Carlos Hathcock's confirmed kill in the Vietnam War was primarily due to the enemy soldier stopping his bicycle on the spot Hathcock had fired at while sighting in his Browning M2 heavy machine-gun.\n\nDevices such as laser rangefinders, handheld meteorological measuring equipment, handheld computers, and ballistic-prediction software can contribute to increased accuracy (i.e. reduced CEP), although they rely on proper use and training to realize any advantages. In addition, as instruments of measure, they are subject to accuracy errors and malfunction. Handheld meteorological instruments only measure conditions at the location they are used. Wind direction and speed can vary dramatically along the path of the bullet.\n\nThe science of long-range sniping came to fruition in the Vietnam War. Carlos Hathcock held the record from 1967 to 2002 at . He recorded 93 official kills. After returning to the U.S., Hathcock helped to establish the Marine Corps Scout Sniper School at Quantico, Virginia.\n\nIn addition to his success as a USMC scout-sniper during multiple deployments to Vietnam, Gunnery Sergeant Hathcock competed in multiple USMC shooting teams. Hathcock also won the 1966 Wimbledon Cup, which is earned by the winner of the U.S. 1,000-yard high-powered rifle National Championship. Even after being severely burned during an attack on an Amtrac on which he was riding and his efforts to rescue other soldiers, and after being diagnosed with multiple sclerosis, Hathcock continued to serve, shoot and instruct. In Vietnam, Hathcock also completed missions involving a \"through the scope\" shot which killed an enemy sniper specifically hunting him, and a multiple-day solo stalk and kill of an enemy general.\n\nHathcock's record stood until Canadian sniper Arron Perry of the Princess Patricia's Canadian Light Infantry exceeded it with a shot of . Perry held the title for only a few days, as another man in his unit (Corporal Rob Furlong) beat Perry's distance with a shot in March 2002. Perry and Furlong were part of a six-man sniper team during 2002's Operation Anaconda, part of the War in Afghanistan.\n\nCorporal Furlong's record was bested by a British soldier, Corporal of Horse Craig Harrison, of the Blues and Royals, Household Cavalry, who recorded two shots (confirmed by GPS) in November 2009, also during the War in Afghanistan, in which he hit two Taliban insurgents consecutively. Harrison killed the two Taliban machine gunners with shots that took the 8.59 mm rounds almost five seconds to hit their targets, which were beyond the L115A3 sniper rifle’s recommended range. A third shot took out the insurgents' machine gun. The rifle used was made by Accuracy International.\n\nIn June 2017, an unnamed sniper from Canada's Tier 1 special forces unit, Joint Task Force 2, surpassed the 2009 record by over , with a shot in the \nIraqi Civil War. As with the previous two Canadian records, a McMillan Tac-50 with Hornady A-MAX .50 (.50 BMG) ammunition was used.\n\nThis list is not exhaustive, as such data is generally not tracked nor managed under any official procedure. For example, the Canadian Army 2002 sniper team that saw two soldiers (Arron Perry/2,310 m and Rob Furlong/2,430 m) set consecutive new records, also made a number of kills at that are not counted here. The list also shows that, in some cases, an armed force command may choose to withhold the name of the actual sniper for security reasons. The United Nations Security Forces, such as in the Balkans, also had one U.S. sniper (name withheld) attributed with a shot.\n\nWhile not on the list due to the range being less than the minimum distance used to compile it, Hathcock's second-longest confirmed kill was using a \"standard\" USMC sniper rifle chambered in .30-06 Springfield. At the time of Hathcock's service, snipers had essentially been eliminated from the USMC, and its sniper rifles were a hodgepodge mix of commercial Remington 700 and Winchester Model 70 rifles chambered for multiple cartridges. The major challenge for Hathcock and other scout-snipers was improving the performance and reliability of their rifles and ammunition.\n\n\n\n"}
{"id": "2571737", "url": "https://en.wikipedia.org/wiki?curid=2571737", "title": "Maria Makiling", "text": "Maria Makiling\n\nMaria Makiling, in Philippine mythology, is a \"diwata\" (anito) or \"lambana\" (fairy) associated with Mount Makiling in Laguna, Philippines. She is the most widely known \"diwata\" in Philippine mythology, and was venerated in pre-colonial Philippines as a goddess known as Dayang Masalanta or Dian Masalanta who was invoked to stop deluge, storms and earthquakes.\n\nMaria Makiling is the guardian spirit of the mountain, responsible for protecting its bounty and thus, is also a benefactor for the townspeople who depend on the mountain's resources. In addition to being a guardian of the mountain, some legends also identify Laguna de Bay — and the fish caught from it — as part of her domain. She was sent by Bathala to aid the people of the area in their everyday life.\n\nIt is often said that Mount Makiling resembles the profile of a woman, said to be Maria herself. This phenomenon is described as true from several different perspectives, so there is no single location associated with this claim. The mountain's various peaks are said to be Maria's face and two breasts, respectively, and her hair cascades downwards a gentle slope away from her body.\n\nMaria Makiling is a prominent example of the mountain-goddesses motiff in Philippine mythology, other prominent examples being Maria Sinukuan of Pampanga's Mount Arayat and Maria Cacao on Cebu's Mount Lantoy.\n\nLegends do not clarify whether this spirit was named after the mountain or the mountain was named after her. The evolution of the name, however, presents some clues.\n\nThe name \"Mariang Makiling\" is the Spanish-Tagalog contraction of \"Maria ng Makiling\" (Maria of Makiling). The term is a hispanized evolution of an alternate name for the Diwata, \"Dayang Makiling\" - \"dayang\" being an Austronesian word meaning \"princess\" or \"noble lady\". Prior to the conversion of the natives to Christianity, Maria Makiling was already known as \"Makiling\", an anito sent by Bathala in Mount Makiling to aid mankind in their daily tasks. The 'Maria' was added by the Spanish in a bid to 'rebrand' her as Catholic.\n\nProfessor Grace Odal of the University of the Philippines believes there is a significant link between Maria Makiling and the mythical woman (\"Ba'i\") for whom the town of Bay and the lake of Laguna de Bay are named. When the lady of the lake also became associated with the nearby mountain, the common description of her became that of the 'lady of the mountain'.\n\nAs for the word \"Makiling\", it has been noted that the mountain rises from Laguna de Bay \"\"to a rugged top and breaks into irregular hills southward, thus 'leaning' or 'uneven.'\" The Tagalog word for 'leaning' or 'uneven' is \"Makiling\".\" This corresponds with the common belief that the profile of the mountain resembles that of a reclining woman, from certain angles.\n\nA less often mentioned possible origin for the name of the mountain is that the name describes the mountain as having plenty of the bamboo variety known as \"kawayang kiling\" (Bambusa vulgaris Schrad.). By this etymology, the mountain would have been named after the bamboo, and the lady named after the mountain.\n\nDescriptions of Maria Makiling are fairly consistent. She is a breathtakingly beautiful young woman who never ages. Lanuza describes her as having \"light olive skin, long shining black hair, and twinkling eyes.\"\n\nIt is said that the abundance and serenity of the enchanted mountain complements Maria's own persona.\n\nShe is also closely associated with the white mist that often surrounds the mountain. While in just a few stories either her skin or hair is white, in most tales, it is her radiant clothing which makes people who have seen her think that perhaps they just saw a wisp of cloud through the trees and mistook it for Maria.\n\nUnlike Maria Sinukuan and Maria Cacao who live in caves in their respective mountains, Makiling is often described as living in a humble hut.\n\nIn some stories, this hut is situated in the village, amongst the people, where Maria Makiling lived before she fled to the mountains after having been offended for some reason.\n\nIn other stories, the hut is up in the mountain, and can only be found if one is allowed by Maria to find it.\n\nBecause stories about Maria Makiling were part of oral tradition long before they were documented, there are numerous versions of the Maria Makiling legend. Some of these are not stories per se, but superstitions.\n\nOne superstition is that every so often, men would disappear into the forests of the mountain. It is said that Makiling has fallen in love with that particular man, and has taken him to her house to be her husband, there to spend his days in matrimonial bliss.\nAnother superstition says that one can go into the forests and pick and eat any fruits one might like, but never carry any of them home. In doing so, one runs the risk of angering Maria Makiling. One would get lost, and be beset by insect stings and thorn pricks. The only solution is to throw away the fruit, and then to reverse one's clothing as evidence to Maria that one is no longer carrying any of her fruit.\n\nPerhaps the most common \"full\" story is that of Maria turning ginger into gold to help one villager or the other. In these stories, Maria is said to live in a place known to the villagers, and interacts with them regularly. The villager in question is often either a mother seeking a cure for her ill child, or a husband seeking a cure for his wife.\n\nThe wise Maria recognizes the symptoms as signs not of disease, but of hunger brought about by extreme poverty. She gives the villager some ginger, which, by the time the villager gets home, has magically turned to gold\n\nIn versions where the villager is going home to his wife, he unwisely throws some of the ginger away because it had become too heavy to carry.\n\nIn some versions, the villagers love her all the more for her act of kindness.\n\nIn most, however, greedy villagers break into Maria's garden to see if her other plants were really gold. Distressed by the villager's greed, Maria runs away up the mountain, her pristine white clothing soon becoming indistinguishable from the white clouds that play amongst the trees on the upper parts of the mountains.\n\nIn many other stories, Makiling is characterized as a spurned lover.\n\nIn one story, she fell in love with a hunter who had wandered into her kingdom. Soon the two became lovers, with the hunter coming up the mountain every day. They promised to love each other forever. When Maria discovered that he had met, fell in love with, and married a mortal woman, she was deeply hurt. Realizing that she could not trust townspeople because she was so different from them, and that they were just using her, she became angry and refused to give fruits to the trees, let animals and birds roam the forests for hunters to catch, and let fish abound in the lake. People seldom saw her, and those times when she could be seen were often only during pale moonlit nights.\n\nIn another version of the story, told by the Philippines' National Hero, Jose Rizal, Maria falls in love with a farmer, whom she then watches over. This leads the townspeople say he is endowed with a charm, or mutya, as it is called, that protected him from harm. The young man was good at heart and simple in spirit, but also quiet and secretive. In particular, he would not say much of his frequent visits into the wood of Maria Makiling. But then war came to the land, and army officers came recruiting unmarried young men. The man entered an arranged marriage so that he could stay safely in the village. A few days before his marriage, he visits Maria one last time. \"I hope that you were devoted to me,\" she said sadly, \"but you need an earthly love, and you do not have enough faith in me besides. I could have protected you and your family.\" After saying this, she disappeared. Maria Makiling was never seen by the peasants again, nor was her humble hut ever rediscovered.\n\nMichelle Lanuza tells another version of the story, set during the later part of the Spanish occupation:\n\nMaria was sought for and wooed by many suitors, three of whom were the Captain Lara, a Spanish soldier; Joselito, a Spanish mestizo studying in Manila; and Juan who was but a common farmer. Despite his lowly status, Juan was chosen by Maria Makiling.\n\nSpurned, Joselito and Captain Lara conspired to frame Juan for setting fire to the cuartel of the Spanish. Juan was shot as the enemy of the Spaniards. Before he died, he cried Maria's name out loud.\n\nThe \"diwata\" quickly came down from her mountain while Captain Lara and Joselito fled to Manila in fear of Maria's wrath. When she learned what happened, she cursed the two, along with all other men who cannot accept failure in love.\n\nSoon, the curse took effect. Joselito suddenly contracted an incurable illness. The revolutionary Filipinos killed Captain Lara.\n\n\"From then on,\" Lanuza concludes, \"Maria never let herself be seen by the people again. Every time somebody gets lost on the mountain, they remember the curse of the \"diwata\". Yet they also remember the great love of Maria Makiling.\"\n\nMount Makiling still abounds with superstitions and stories concerning Makiling. When people get lost on the mountain, the disappearances are still attributed to the diwata or to spirits who follow her.\n\nA common story is that of a group of hikers who leave their camp dirty with human waste and empty cans and bottles. Searching for water, the hikers find themselves befuddled, coming back to the same place over and over again. Only after they cleaned their camp did they manage to find water.\n\nIn the University of the Philippines Los Baños, a university that sits on the foot of Mount Makiling, students still tell stories of a woman in white who is sighted walking down the long uphill road heading to the Upper (College of Forestry) Campus. Sometimes, the woman appears to be trying to hitch a ride down the mountain. Invariably, the observers are said to be frightened and just ignore the woman, believing her to be Maria Makiling.\n\nThe unusual weather patterns on the mountain area are also often attributed to Maria Makiling. Often this means sudden rains whenever particularly noisy events are held in the areas near the mountain. Locals say that the diwata does not approve of the event. Acclaimed stage and screen actor and director Behn Cervantes relates a reverse version of this legend, during the launching program for the UP Alumni Association's Maria Makiling Foundation, an advocacy group formed for the protection and conservation of Mount Makiling:\n\nMaria Makiling is a common theme among Filipino artists, ranging from painters and sculptors to graphic novelists.\n\n\n\n"}
{"id": "445131", "url": "https://en.wikipedia.org/wiki?curid=445131", "title": "Microburst", "text": "Microburst\n\nA microburst is an intense small-scale downdraft produced by a thunderstorm or rain shower. There are two types of microbursts: wet microbursts and dry microbursts. They go through three stages in their cycle, the downburst, outburst, and cushion stages also called \"Suriano's Stroke\". A microburst can be particularly dangerous to aircraft, especially during landing, due to the wind shear caused by its gust front. Several fatal and historic crashes have been attributed to the phenomenon over the past several decades, and flight crew training goes to great lengths on how to properly recover from a microburst/wind shear event.\n\nA microburst often has high winds that can knock over fully grown trees. They usually last for seconds to minutes.\n\nThe term was defined by mesoscale meteorology expert Ted Fujita as affecting an area in diameter or less, distinguishing them as a type of downburst and apart from common wind shear which can encompass greater areas. Fujita also coined the term macroburst for downbursts larger than .\n\nA distinction can be made between a wet microburst which consists of precipitation and a dry microburst which typically consists of virga. They generally are formed by precipitation-cooled air rushing to the surface, but they perhaps also could be powered by strong winds aloft being deflected toward the surface by dynamical processes in a thunderstorm (see rear flank downdraft).\n\nWhen rain falls below the cloud base or is mixed with dry air, it begins to evaporate and this evaporation process cools the air. The cool air descends and accelerates as it approaches the ground. When the cool air approaches the ground, it spreads out in all directions. High winds spread out in this type of pattern showing little or no curvature are known as straight-line winds.\n\nDry microbursts produced by high based thunderstorms that generate little to no surface rainfall, occur in environments characterized by a thermodynamic profile exhibiting an inverted-V at thermal and moisture profile, as viewed on a Skew-T log-P thermodynamic diagram. Wakimoto (1985) developed a conceptual model (over the High Plains of the United States) of a dry microburst environment that comprised three important variables: mid-level moisture, a deep and dry adiabatic lapse rate in the sub-cloud layer, and low surface relative humidity.\n\nWet microbursts are downbursts accompanied by significant precipitation at the surface. These downbursts rely more on the drag of precipitation for downward acceleration of parcels as well as the negative buoyancy which tend to drive \"dry\" microbursts. As a result, higher mixing ratios are necessary for these downbursts to form (hence the name \"wet\" microbursts). Melting of ice, particularly hail, appears to play an important role in downburst formation (Wakimoto and Bringi, 1988), especially in the lowest above ground level (Proctor, 1989). These factors, among others, make forecasting wet microbursts difficult.\n\nThe evolution of microbursts is broken down into three stages: the contact stage, the outburst stage, and the cushion stage.\n\nStart by using the vertical momentum equation:\n\nformula_1\n\nBy decomposing the variables into a basic state and a perturbation, defining the basic states, and using the ideal gas law (formula_2), then the equation can be written in the form\n\nformula_3\n\nwhere B is buoyancy. The virtual temperature correction usually is rather small and to a good approximation; it can be ignored when computing buoyancy. Finally, the effects of precipitation loading on the vertical motion are parametrized by including a term that decreases buoyancy as the liquid water mixing ratio (formula_4) increases, leading to the final form of the parcel's momentum equation:\n\nformula_5\n\nThe first term is the effect of perturbation pressure gradients on vertical motion. In some storms this term has a large effect on updrafts (Rotunno and Klemp, 1982) but there is not much reason to believe it has much of an impact on downdrafts (at least to a first approximation) and therefore will be ignored.\n\nThe second term is the effect of buoyancy on vertical motion. Clearly, in the case of microbursts, one expects to find that B is negative meaning the parcel is cooler than its environment. This cooling typically takes place as a result of phase changes (evaporation, melting, and sublimation). Precipitation particles that are small, but are in great quantity, promote a maximum contribution to cooling and, hence, to creation of negative buoyancy. The major contribution to this process is from evaporation.\n\nThe last term is the effect of water loading. Whereas evaporation is promoted by large numbers of small droplets, it only requires a few large drops to contribute substantially to the downward acceleration of air parcels. This term is associated with storms having high precipitation rates. Comparing the effects of water loading to those associated with buoyancy, if a parcel has a liquid water mixing ratio of 1.0 g kg, this is roughly equivalent to about 0.3 K of negative buoyancy; the latter is a large (but not extreme) value. Therefore, in general terms, negative buoyancy is typically the major contributor to downdrafts.\n\nUsing pure \"parcel theory\" results in a prediction of the maximum downdraft of\n\nformula_6\n\nwhere NAPE is the negative available potential energy,\n\nformula_7\n\nand where LFS denotes the level of free sink for a descending parcel and SFC denotes the surface. This means that the maximum downward motion is associated with the integrated negative buoyancy. Even a relatively modest negative buoyancy can result in a substantial downdraft if it is maintained over a relatively large depth. A downward speed of results from the relatively modest NAPE value of 312.5 m s. To a first approximation, the maximum gust is roughly equal to the maximum downdraft speed.\n\nThe scale and suddenness of a microburst makes it a notorious danger to aircraft, particularly those at low altitude which are taking off or landing. The following are some fatal crashes and/or aircraft incidents that have been attributed to microbursts in the vicinity of airports:\n\nA microburst often causes aircraft to crash when they are attempting to land (the above-mentioned BOAC and Pan Am flights are notable exceptions). The microburst is an extremely powerful gust of air that, once hitting the ground, spreads in all directions. As the aircraft is coming in to land, the pilots try to slow the plane to an appropriate speed. When the microburst hits, the pilots will see a large spike in their airspeed, caused by the force of the headwind created by the microburst. A pilot inexperienced with microbursts would try to decrease the speed. The plane would then travel through the microburst, and fly into the tailwind, causing a sudden decrease in the amount of air flowing across the wings. The decrease in airflow over the wings of the aircraft causes a drop in the amount of lift produced. This decrease in lift combined with a strong downward flow of air can cause the thrust required to remain at altitude to exceed what is available, thus causing the aircraft to stall. If the plane is at a low altitude shortly after takeoff or during landing, it will not have sufficient altitude to recover.\n\nThe strongest microburst recorded thus far occurred at Andrews Field, Maryland on August 1st 1983, with wind speeds reaching 240.5 km/h (149.5 mi/h).\n\n\n\nNotes\nBibliography\n\n"}
{"id": "53534406", "url": "https://en.wikipedia.org/wiki?curid=53534406", "title": "Ministry of Electricity and Energy (Myanmar)", "text": "Ministry of Electricity and Energy (Myanmar)\n\nMinistry of Electricity and Energy (; abbreviated as MOEE) is the ministry of Myanmar composed by two ministries, Electrical Power (MOEP) and Energy (MOE) by President Htin Kyaw.\n\nPlanning \n Implementation \nTransmission and System Control \n"}
{"id": "25807626", "url": "https://en.wikipedia.org/wiki?curid=25807626", "title": "Mount Potalaka", "text": "Mount Potalaka\n\nMount Potalaka (, Japanese: 補陀洛 \"Fudaraku-san\"), which means \"Brilliance\", is the mythical dwelling of the Buddhist bodhisattva Avalokiteśvara, said to exist in the seas south of India.\n\nThe mountain is first mentioned in the final chapter of the \"Avataṃsaka Sūtra\", the \"Gaṇḍavyūha Sūtra\", where the chapter's protagonist journeys to seek the advice of Avalokiteśvara.\n\nThe Japanese scholar Shu Hikosaka, on the basis of his study of Buddhist scriptures, ancient Tamil literary sources, as well as field survey, proposes the hypothesis that, the ancient mount Potalaka, the residence of Avalokiteśvara described in the \"Gaṇḍavyūha Sūtra\" and Xuanzang’s \"Great Tang Records on the Western Regions\", is the real mountain Potikai or Potiyil situated at Ambasamudram in Tirunelveli district, Tamil Nadu. Shu also says that mount Potiyil / Potalaka has been a sacred place for the people of South India from time immemorial.\n\nWith the spread of Buddhism in the region beginning at the time of the great king Aśoka in the third century BCE, it became a holy place also for Buddhists who gradually became dominant as a number of their hermits settled there. The local people, though, mainly remained followers of Hinduism. The mixed Hindu-Buddhist cult culminated in the formation of the figure of Avalokiteśvara.\n\nLater Japanese Buddhists, such as the Japanese Yogacara monk Jōkei, espoused aspiring rebirth on Mount Potalaka as an easier way to attain progress on the Buddhist path than the more well-known pure land of Amitābha.\n\nPlaces named after Mount Potalaka include:\n"}
{"id": "20512223", "url": "https://en.wikipedia.org/wiki?curid=20512223", "title": "Ocean reanalysis", "text": "Ocean reanalysis\n\nOcean reanalysis is a method of combining historical ocean observations with a general ocean model (typically a computational model) driven by historical estimates of surface winds, heat, and freshwater, by way of a data assimilation algorithm to reconstruct historical changes in the state of the ocean.\n\nHistorical observations are sparse and insufficient for understanding the history of the ocean and its circulation. By utilizing data assimilation techniques in combination with advanced computational models of the global ocean, researchers are able to interpolate the historical observations to all points in the ocean. This process has an analog in the construction of atmospheric reanalysis and is closely related to ocean state estimation.\n\nA number of efforts have been initiated in recent years to apply data assimilation to estimate the physical state of the ocean, including temperature, salinity, currents, and sea level, in recent years. There are three alternative state estimation approaches. The first approach is used by the ‘no-model’ analyses, for which temperature or salinity observations update a first guess provided by climatological monthly estimates.\n\nThe second approach is that of the sequential data assimilation analyses, which move forward in time from a previous analysis using a numerical simulation of the evolving temperature and other variables produced by an ocean general circulation model. The simulation provides the first guess of the state of the ocean at the next analysis time, while corrections are made to this first guess based on observations of variables such as temperature, salinity, or sea level.\n\nThe third approach is 4D-Var, which in the implementation described uses the initial conditions and surface forcing as control variables to be modified in order to be consistent with the observations as well as a numerical representation of the equations of motion through iterative solution of a giant optimization problem.\n\nISHII and LEVITUS begin with a first guess of the climatological monthly upper-ocean temperature based on climatologies produced by the NOAA National Oceanographic Data Center. The innovations are mapped onto the analysis levels. ISHII uses and alternative 3DVAR approach to do an objective mapping with a smaller decorrelation scale in midlatitudes (300 km) that elongates in the zonal direction by a factor of 3 at equatorial latitudes. LEVITUS begins similarly to ISHII, but uses the technique of Cressman and Barnes with a homogeneous scale of 555 km to objectively map the temperature innovation onto a uniform grid.\n\nThe sequential approaches can be further divided into those using Optimal Interpolation and its more sophisticated cousin the Kalman Filter, and those using 3D-Var. Among those mentioned above, INGV and SODA use versions of Optimal Interpolation. CERFACS, GODAS, and GFDL all use 3DVar. \"To date we are unaware of any attempt to use Kalman Filter for multi-decadal ocean reanalyses.\" The 4-Dimensional Local Ensemble Transform Kalman Filter (4D-LETKF) has been applied to the Geophysical Fluid Dynamics Laboratory's (GFDL) Modular Ocean Model (MOM2) for a 7-year ocean reanalysis from January 1997 – 2004.\n\nOne innovative attempt by GECCO has been made to apply 4D-Var to the decadal ocean estimation problem. This approach faces daunting computational challenges, but provides some interesting benefits including satisfying some conservation laws and the construction of the ocean model adjoint.\n\n\n"}
{"id": "3584826", "url": "https://en.wikipedia.org/wiki?curid=3584826", "title": "Orthotropic material", "text": "Orthotropic material\n\nIn material science and solid mechanics, orthotropic materials have material properties that differ along three mutually-orthogonal twofold axes of rotational symmetry. They are a subset of anisotropic materials, because their properties change when measured from different directions.\n\nA familiar example of an orthotropic material is wood. In wood, one can define three mutually perpendicular directions at each point in which the properties are different. These are the axial direction (along the grain), the radial direction, and the circumferential direction. Because the preferred coordinate system is cylindrical-polar, this type of orthotropy is also called polar orthotropy. Mechanical properties, such as strength and stiffness, measured axially (along the grain) are typically better than those measured in the radial and circumferential directions (across the grain). These directional differences in strength can be quantified with Hankinson's equation.\n\nAnother example of an orthotropic material is sheet metal formed by squeezing thick sections of metal between heavy rollers. This flattens and stretches its grain structure. As a result, the material becomes anisotropic—its properties differ between the direction it was rolled in and each of the two transverse directions.\n\nIf orthotropic properties vary between points inside an object, it possesses both orthotropy and inhomogeneity. This suggests that orthotropy is the property of a point within an object rather than for the object as a whole (unless the object is homogeneous). The associated planes of symmetry are also defined for a small region around a point and do not necessarily have to be identical to the planes of symmetry of the whole object.\n\nOrthotropic materials are a subset of anisotropic materials; their properties depend on the direction in which they are measured. Orthotropic materials have three planes/axes of symmetry. An isotropic material, in contrast, has the same properties in every direction. It can be proved that a material having two planes of symmetry must have a third one. Isotropic materials have an infinite number of planes of symmetry.\n\nTransversely isotropic materials are special orthotropic materials that have one axis of symmetry (any other pair of axes that are perpendicular to the main one and orthogonal among themselves are also axes of symmetry). One common example of transversely isotropic material with one axis of symmetry is a polymer reinforced by parallel glass or graphite fibers. The strength and stiffness of such a composite material will usually be greater in a direction parallel to the fibers than in the transverse direction, and the thickness direction usually has properties similar to the transverse direction. Another example would be a biological membrane, in which the properties in the plane of the membrane will be different from those in the perpendicular direction. Orthotropic material properties have been shown to provide a more accurate representation of bone's elastic symmetry and can also give information about the three-dimensional directionality of bone's tissue-level material properties.\n\nIt is important to keep in mind that a material which is anisotropic on one length scale may be isotropic on another (usually larger) length scale. For instance, most metals are polycrystalline with very small grains. Each of the individual grains may be anisotropic, but if the material as a whole comprises many randomly oriented grains, then its measured mechanical properties will be an average of the properties over all possible orientations of the individual grains.\n\nMaterial behavior is represented in physical theories by constitutive relations. A large class of physical behaviors can be represented by linear material models that take the form of a second-order tensor. The material tensor provides a relation between two vectors and can be written as\nwhere formula_2 are two vectors representing physical quantities and formula_3 is the second-order material tensor. If we express the above equation in terms of components with respect to an orthonormal coordinate system, we can write\nSummation over repeated indices has been assumed in the above relation. In matrix form we have\nExamples of physical problems that fit the above template are listed in the table below.\nThe material matrix formula_6 has a symmetry with respect to a given orthogonal transformation (formula_7) if it does not change when subjected to that transformation. \nFor invariance of the material properties under such a transformation we require\nHence the condition for material symmetry is (using the definition of an orthogonal transformation)\nOrthogonal transformations can be represented in Cartesian coordinates by a formula_10 matrix formula_11 given by\nTherefore the symmetry condition can be written in matrix form as\n\nAn orthotropic material has three orthogonal symmetry planes. If we choose an orthonormal coordinate system such that the axes coincide with the normals to the three symmetry planes, the transformation matrices are\nIt can be shown that if the matrix formula_6 for a material is invariant under reflection about two orthogonal planes then it is also invariant under reflection about the third orthogonal plane.\n\nConsider the reflection formula_16 about the formula_17 plane. Then we have\nThe above relation implies that formula_19. Next consider a reflection formula_20 about the formula_21 plane. We then have\nThat implies that formula_23. Therefore the material properties of an orthotropic material are described by the matrix\n\nIn linear elasticity, the relation between stress and strain depend on the type of material under consideration. This relation is known as Hooke's law. For anisotropic materials Hooke's law can be written as\nwhere formula_26 is the stress tensor, formula_27 is the strain tensor, and formula_28 is the elastic stiffness tensor. If the tensors in the above expression are described in terms of components with respect to an orthonormal coordinate system we can write\nwhere summation has been assumed over repeated indices. Since the stress and strain tensors are symmetric, and since the stress-strain relation in linear elasticity can be derived from a strain energy density function, the following symmetries hold for linear elastic materials\nBecause of the above symmetries, the stress-strain relation for linear elastic materials can be expressed in matrix form as\nAn alternative representation in Voigt notation is\nor \nThe stiffness matrix formula_34 in the above relation satisfies point symmetry.\n\nThe stiffness matrix formula_34 satisfies a given symmetry condition if it does not change when subjected to the corresponding orthogonal transformation. The orthogonal transformation may represent symmetry with respect to a point, an axis, or a plane. Orthogonal transformations in linear elasticity include rotations and reflections, but not shape changing transformations and can be represented, in orthonormal coordinates, by a formula_10 matrix formula_37 given by\nIn Voigt notation, the transformation matrix for the stress tensor can be expressed as a formula_39 matrix formula_40 given by\nThe transformation for the strain tensor has a slightly different form because of the choice of notation. This transformation matrix is\nIt can be shown that formula_43.\nThe elastic properties of a continuum are invariant under an orthogonal transformation formula_37 if and only if\nAn orthotropic elastic material has three orthogonal symmetry planes. If we choose an orthonormal coordinate system such that the axes coincide with the normals to the three symmetry planes, the transformation matrices are\nWe can show that if the matrix formula_34 for a linear elastic material is invariant under reflection about two orthogonal planes then it is also invariant under reflection about the third orthogonal plane.\n\nIf we consider the reflection formula_48 about the formula_17 plane, then we have\nThen the requirement formula_45 implies that\nThe above requirement can be satisfied only if\nLet us next consider the reflection formula_54 about the formula_21 plane. In that case\nUsing the invariance condition again, we get the additional requirement that\nNo further information can be obtained because the reflection about third symmetry plane is not independent of reflections about the planes that we have already considered. Therefore, the stiffness matrix of an orthotropic linear elastic material can be written as\n\nThe inverse of this matrix is commonly written as\nwhere formula_60 is the Young's modulus along axis formula_61, formula_62 is the shear modulus in direction formula_63 on the plane whose normal is in direction formula_61, and formula_65 is the Poisson's ratio that corresponds to a contraction in direction formula_63 when an extension is applied in direction formula_61.\n\nThe strain-stress relation for orthotropic linear elastic materials can be written in Voigt notation as\nwhere the compliance matrix formula_69 is given by\nThe compliance matrix is symmetric and must be positive definite for the strain energy density to be positive. This implies from Sylvester's criterion that all the principal minors of the matrix are positive, i.e., \nwhere formula_72 is the formula_73 principal submatrix of formula_69.\n\nThen, \nWe can show that this set of conditions implies that\nor\nHowever, no similar lower bounds can be placed on the values of the Poisson's ratios formula_78.\n\n\n"}
{"id": "51886508", "url": "https://en.wikipedia.org/wiki?curid=51886508", "title": "Phytobiome", "text": "Phytobiome\n\nPhytobiome is a term that relates to a plant (phyto) in a specific ecological area (biome). It includes the plant itself, the environment and all micro- and macro-organisms living in, on, or around the plant. These organisms include microbes, animals and other plants. The environment includes the soil, air and climate. Examples of ecological areas are fields, rangelands, forests.\n\nKnowledge of the interactions within a phytobiome can be used to create tools for agriculture, crop management, increased health, preservation, productivity, and sustainability of cropping and forest systems.\n\nIn 2015 the American Phytopathological Society (APS) launched a research framework, the Phytobiomes Initiative, to facilitate the organization of research into phytobiome. As part of this effort, in 2016 it launched \"Phytobiomes\", an open-access journal. The journal focuses on transdisciplinary research that impacts the entire plant ecosystem. An overall research strategy has been published in the Phytobiomes Roadmap, a document developed by a group of scientific societies, companies, research institutes, and governmental agencies. It is intended to present a strategic plan to study phytobiomes and propose an action plan to apply phytobiome studies. The connected Phytobiomes Alliance is an international, nonprofit consortium of academic institutions, large and small companies, and governmental agencies coordinating public-private research projects on various aspects of agriculturally relevant phytobiomes.\n\n"}
{"id": "9962853", "url": "https://en.wikipedia.org/wiki?curid=9962853", "title": "Pipeline and Hazardous Materials Safety Administration", "text": "Pipeline and Hazardous Materials Safety Administration\n\nThe Pipeline and Hazardous Materials Safety Administration (PHMSA) is a United States Department of Transportation agency created in 2004, responsible for developing and enforcing regulations for the safe, reliable, and environmentally sound operation of the US' 2.6 million mile pipeline transportation. It is responsible for nearly 1 million daily shipments of hazardous materials by land, sea, and air. It oversees the nation's pipeline infrastructure, which accounts for 64 percent of the energy commodities consumed in the United States. Made up of the Office of Pipeline Safety (OPS) and the Office of Hazardous Materials Safety (OHMS).\n\nPHMSA was created within the U.S. DOT under the Norman Y. Mineta Research and Special Programs Improvement Act of 2004, which former United States President George W. Bush signed into law on November 30, 2004.\n\nHoward R. Elliott was sworn into office in October 2017, and is the current Administrator. \n\nThe PHMSA enforces the Natural Gas Pipeline Safety Act of 1968(), which was enacted in response to the Richmond, Indiana, explosion, as well as the Hazardous Liquid Pipeline Act of 1979 (), the Pipeline Safety Improvement Act of 2002, the Pipeline Inspection, Protection, Safety and Enforcement Act (PIPES) Act of 2006, the Pipeline Safety, Regulatory Certainty, and Job Creation Act of 2011 (P.L. 112), regulations (49 CFR Parts 190-199) and other statutes.\n\nPrior to 2005 the U.S. Department of Transportation had no focused research organization and no separately operating administration for pipeline safety and hazardous materials Transportation safety in the United States. The Norman Y. Mineta Research and Special Programs Improvement Act of 2004 provided these, with an opportunity to establish mode government budget and information practices in support of then president Bush's 'Management Agenda' initiatives. Prior to the Special Programs Act of 2004, PHMSA's hazmat and pipeline safety programs were housed within the Transportation Department's Research and Special Programs Administration, known as RSPA.\n\nOHMS oversees the transportation of hazardous materials by air, rail, highway, and water, with the exception of bulk transportation of hazmat by vessel. OHMS promulgates a national safety program, which consists of:\nevaluating safety risks, developing and enforcing standards for transporting hazardous materials, educating shippers and carriers, investigating hazmat incidents and failures, conducting research, providing grants to improve emergency response to incidents.\n\nThe OHMS website includes OHM guidance documents, hazmat carriers' special permits and approvals information, reports and incidents summaries, penalty action reports, registration information and forms, the Emergency Response Guidebook for First Responders, Freedom of Information Act requests, and the Hazardous Materials Emergency Preparedness (HMEP) grants program.\n\nOPS oversees the 2.3 million miles of the natural gas pipeline system in the United States and its hazardous liquid pipelines.\n\nSeveral agencies collaborate on the 'federal pipeline safety program', authorized through the fiscal year ending September 30,\n2015 such as the Transportation Security Administration,the Department of Homeland Security, and the Federal Energy Regulatory Commission.\n\nThe Pipeline Risk Management Information System (PRIMIS) consists of integrity management programs originally created for transmission pipelines and has led to a reduced amount of pipeline accidents.\nIn 2001 the Liquid Integrity Management Program (LIMP)came into law, followed by the 2003 Transmission Integrity Management Program (TIMP) and the 2008 Distribution Integrity Management Program (DIMP).\n\nThe 'Accountable Pipeline Safety and Partnership Act of 1996' requires that OPS adopt rules requiring interstate gas pipeline operators to provide maps of their facilities to the governing body of each municipality, in which the pipeline is located. The National Pipeline Mapping System (NPMS) was removed for a number of months from public use after September 11, due to security concerns. In 2012, it returned with restriction of use. National Pipeline Maps can still be bought from PennWell Corporation.\n\nAs of 2014, OPS and PMHSA respectively, have not set minimum qualifications for state inspectors, who lead inspection teams. In one state, for example, an inspector with less than one year's experience was allowed to lead inspections.\n\nIn the years 1996-2015, Pipeline and Hazardous Materials Safety Administration (PHMSA) reported a total of 11,199 pipeline incidents in a system that includes a total of of gas pipelines and of hazardous liquids pipelines in the United States as of 2015. 856 of those were considered \"serious incidents\", 86 were serious incidents involving hazardous liquids pipelines. This is an average of approximately 560 incidents per year in the last 20 years for all types of pipelines. These incidents caused a total of 360 fatalities and 1,376 injuries. More recent data however reveals that pipeline incidents as well as the total number of releases has decreased by over 50% since PHMSA was created and pipelines now safely deliver over 99.999% of all shipments, the highest safety percentage of any transportation system.\n\nThe 2010 San Bruno pipeline explosion of a Pacific Gas and Electric Company (PG&E) gasline in San Bruno, California, a suburb south of San Francisco, killed eight people, injured 58 and destroyed much of a subdivision. Investigators from the National Transportation Safety Board found weak state and federal oversight. The long term costs for pipeline inspection and safety upgrades will be borne at 55% by electricity rate payers per California Public Utilities Commission judgement On 1 April 2014 PG&E was indicted in U.S. District Court, San Francisco, California, for multiple violations of the Natural Gas Pipeline Safety Act of 1968 relating to its record keeping and pipeline \"integrity management\" practices.\n\nIn 2011, the PHMSA came under criticism for not releasing a Canadian company’s plans for managing oil spills and estimating a worst-case scenario in the event its pipeline burst in the US. Then Transportation Secretary Ray LaHood, who oversaw the pipeline agency, acknowledged weaknesses in the program and asked Congress to pass legislation that would increase penalties for negligent operators and authorize the hiring of additional inspectors. Federal records showed that although the pipeline industry reported 25 percent fewer significant incidents from 2001 through 2010 than in the prior decade, the amount of hazardous liquids being spilled remained substantial. There were more than 100 significant spills each year, a trend that dates back more than 20 years. The percentage of dangerous liquids recovered by pipeline operators after a spill dropped considerably.\n\nIn 2013, PHMSA's Jeffrey Wiese told several hundred oil and gas pipeline compliance officers that his agency has \"very few tools to work with\", in enforcing safety rules, even after Congress in 2011 allowed it to impose higher fines on companies that cause major accidents. \"Do I think I can hurt a major international corporation with a $2 million civil penalty? No,\" he said. These comments reflect the growing trend toward creating industry standards, in order to \"drive safety through the adoption of Safety Management Systems (SMS) in regulated communities.\"\n\nA May 2014 report by the Office of Inspector General for the Department of Transportation found the PHMSA did not ensure \"that key state inspectors are properly trained, inspections are being conducted frequently enough and inspections target the most risky pipelines\".\n\nAt the end of FY2012, PHMSA employed 203 staff in total, including 135 inspection and enforcement staff. Timothy P. Butters is the acting Administrator, since Cynthia L. Quarterman left in October 2014.\n\nThe PMHSA has a 'senior leadership team' of eight people with the following positions: Administrator, Deputy Administrator, Chief Safety Officer, Chief Counsel, two Directors and three Associate Administrators. The current leadership team includes:\nPast leadership includes\n\n\n"}
{"id": "2574067", "url": "https://en.wikipedia.org/wiki?curid=2574067", "title": "Radiative process", "text": "Radiative process\n\nIn particle physics, a radiative process refers to one elementary particle emitting another and continuing to exist. This typically happens when a fermion emits a boson such as a gluon or photon.\n\n"}
{"id": "577162", "url": "https://en.wikipedia.org/wiki?curid=577162", "title": "Relativistic wave equations", "text": "Relativistic wave equations\n\nIn physics, specifically relativistic quantum mechanics (RQM) and its applications to particle physics, relativistic wave equations predict the behavior of particles at high energies and velocities comparable to the speed of light. In the context of quantum field theory (QFT), the equations determine the dynamics of quantum fields.\n\nThe solutions to the equations, universally denoted as or (Greek psi), are referred to as \"wave functions\" in the context of RQM, and \"fields\" in the context of QFT. The equations themselves are called \"wave equations\" or \"field equations\", because they have the mathematical form of a wave equation or are generated from a Lagrangian density and the field-theoretic Euler–Lagrange equations (see classical field theory for background).\n\nIn the Schrödinger picture, the wave function or field is the solution to the Schrödinger equation;\n\none of the postulates of quantum mechanics. All relativistic wave equations can be constructed by specifying various forms of the Hamiltonian operator \"Ĥ\" describing the quantum system. Alternatively, Feynman's path integral formulation uses a Lagrangian rather than a Hamiltonian operator.\n\nMore generally – the modern formalism behind relativistic wave equations is Lorentz group theory, wherein the spin of the particle has a correspondence with the representations of the Lorentz group.\n\nThe failure of classical mechanics applied to molecular, atomic, and nuclear systems and smaller induced the need for a new mechanics: \"quantum mechanics\". The mathematical formulation was led by De Broglie, Bohr, Schrödinger, Pauli, and Heisenberg, and others, around the mid-1920s, and at that time was analogous to that of classical mechanics. The Schrödinger equation and the Heisenberg picture resemble the classical equations of motion in the limit of large quantum numbers and as the reduced Planck constant , the quantum of action, tends to zero. This is the correspondence principle. At this point, special relativity was not fully combined with quantum mechanics, so the Schrödinger and Heisenberg formulations, as originally proposed, could not be used in situations where the particles travel near the speed of light, or when the number of each type of particle changes (this happens in real particle interactions; the numerous forms of particle decays, annihilation, matter creation, pair production, and so on).\n\nA description of quantum mechanical systems which could account for \"relativistic\" effects was sought for by many theoretical physicists; from the late 1920s to the mid-1940s. The first basis for relativistic quantum mechanics, i.e. special relativity applied with quantum mechanics together, was found by all those who discovered what is frequently called the Klein–Gordon equation:\n\nby inserting the energy operator and momentum operator into the relativistic energy–momentum relation:\n\nThe solutions to () are scalar fields. The KG equation is undesirable due to its prediction of \"negative\" energies and probabilities, as a result of the quadratic nature of () – inevitable in a relativistic theory. This equation was initially proposed by Schrödinger, and he discarded it for such reasons, only to realize a few months later that its non-relativistic limit (what is now called the Schrödinger equation) was still of importance. Nevertheless, – () is applicable to spin-0 bosons.\n\nNeither the non-relativistic nor relativistic equations found by Schrödinger could predict the fine structure in the Hydrogen spectral series. The mysterious underlying property was \"spin\". The first two-dimensional \"spin matrices\" (better known as the Pauli matrices) were introduced by Pauli in the Pauli equation; the Schrödinger equation with a non-relativistic Hamiltonian including an extra term for particles in magnetic fields, but this was \"phenomenological\". Weyl found a relativistic equation in terms of the Pauli matrices; the Weyl equation, for \"massless\" spin- fermions. The problem was resolved by Dirac in the late 1920s, when he furthered the application of equation () to the electron – by various manipulations he factorized the equation into the form:\n\nand one of these factors is the Dirac equation (see below), upon inserting the energy and momentum operators. For the first time, this introduced new four-dimensional spin matrices and in a relativistic wave equation, and explained the fine structure of hydrogen. The solutions to () are multi-component spinor fields, and each component satisfies (). A remarkable result of spinor solutions is that half of the components describe a particle while the other half describe an antiparticle; in this case the electron and positron. The Dirac equation is now known to apply for all massive spin- fermions. In the non-relativistic limit, the Pauli equation is recovered, while the massless case results in the Weyl equation.\n\nAlthough a landmark in quantum theory, the Dirac equation is only true for spin- fermions, and still predicts negative energy solutions, which caused controversy at the time (in particular – not all physicists were comfortable with the \"Dirac sea\" of negative energy states).\n\nThe natural problem became clear: to generalize the Dirac equation to particles with \"any spin\"; both fermions and bosons, and in the same equations their antiparticles (possible because of the spinor formalism introduced by Dirac in his equation, and then-recent developments in spinor calculus by van der Waerden in 1929), and ideally with positive energy solutions.\n\nThis was introduced and solved by Majorana in 1932, by a deviated approach to Dirac. Majorana considered one \"root\" of ():\n\nwhere is a spinor field now with infinitely many components, irreducible to a finite number of tensors or spinors, to remove the indeterminacy in sign. The matrices and are infinite-dimensional matrices, related to infinitesimal Lorentz transformations. He did not demand that each component of to satisfy equation (), instead he regenerated the equation using a Lorentz-invariant action, via the principle of least action, and application of Lorentz group theory.\n\nMajorana produced other important contributions that were unpublished, including wave equations of various dimensions (5, 6, and 16). They were anticipated later (in a more involved way) by de Broglie (1934), and Duffin, Kemmer, and Petiau (around 1938–1939) see Duffin–Kemmer–Petiau algebra. The Dirac–Fierz–Pauli formalism was more sophisticated than Majorana’s, as spinors were new mathematical tools in the early twentieth century, although Majorana’s paper of 1932 was difficult to fully understand; it took Pauli and Wigner some time to understand it, around 1940.\n\nDirac in 1936, and Fierz and Pauli in 1939, built equations from irreducible spinors and , symmetric in all indices, for a massive particle of spin for integer (see Van der Waerden notation for the meaning of the dotted indices):\n\nB_{\\gamma\\epsilon_1\\epsilon_2\\cdots\\epsilon_n}^{\\dot{\\beta}_1\\dot{\\beta}_2\\cdots\\dot{\\beta}_n} = mcA_{\\epsilon_1\\epsilon_2\\cdots\\epsilon_n}^{\\dot{\\alpha}\\dot{\\beta}_1\\dot{\\beta}_2\\cdots\\dot{\\beta}_n} \n\nwhere is the momentum as a covariant spinor operator. For , the equations reduce to the coupled Dirac equations and and together transform as the original Dirac spinor. Eliminating either or shows that and each fulfill ().\n\nIn 1941, Rarita and Schwinger focussed on spin- particles and derived the Rarita–Schwinger equation, including a Lagrangian to generate it, and later generalized the equations analogous to spin for integer . In 1945, Pauli suggested Majorana's 1932 paper to Bhabha, who returned to the general ideas introduced by Majorana in 1932. Bhabha and Lubanski proposed a completely general set of equations by replacing the mass terms in () and () by an arbitrary constant, subject to a set of conditions which the wave functions must obey.\n\nFinally, in the year 1948 (the same year as Feynman's path integral formulation was cast), Bargmann and Wigner formulated the general equation for massive particles which could have any spin, by considering the Dirac equation with a totally symmetric finite-component spinor, and using Lorentz group theory (as Majorana did): the Bargmann–Wigner equations. In the early 1960s, a reformulation of the Bargmann–Wigner equations was made by H. Joos and Steven Weinberg, the Joos–Weinberg equation. Various theorists at this time did further research in relativistic Hamiltonians for higher spin particles.\n\nThe relativistic description of spin particles has been a difficult problem in quantum theory. It is still an area of the present-day research because the problem is only partially solved; including interactions in the equations is problematic, and paradoxical predictions (even from the Dirac equation) are still present.\n\nThe following equations have solutions which satisfy the superposition principle, that is, the wave functions are additive.\n\nThroughout, the standard conventions of tensor index notation and Feynman slash notation are used, including Greek indices which take the values 1, 2, 3 for the spatial components and 0 for the timelike component of the indexed quantities. The wave functions are denoted \", and are the components of the four-gradient operator.\n\nIn matrix equations, the Pauli matrices are denoted by \" in which , where is the identity matrix:\n\nand the other matrices have their usual representations. The expression\n\nis a matrix operator which acts on 2-component spinor fields.\n\nThe gamma matrices are denoted by \"\", in which again , and there are a number of representations to select from. The matrix is \"not\" necessarily the identity matrix. The expression\n\nis a matrix operator which acts on 4-component spinor fields.\n\nNote that terms such as \"\" scalar multiply an identity matrix of the relevant dimension, the common sizes are or , and are \"conventionally\" not written for simplicity.\n\nThe Duffin–Kemmer–Petiau equation is an alternative equation for spin-0 and spin-1 particles:\n\nStart with the standard special relativity (SR) 4-vectors\n\nNote that each 4-vector is related to another by a Lorentz scalar:\n\nNow, just apply the standard Lorentz scalar product rule to each one:\n\nThe last equation is a fundamental quantum relation.\n\nWhen applied to a Lorentz scalar field formula_21, one gets the Klein–Gordon equation, the most basic of the quantum relativistic wave equations.\n\nThe Schrödinger equation is the low-velocity limiting case (\"v\" « \"c\") of the Klein–Gordon equation.\n\nWhen the relation is applied to a four-vector field formula_25 instead of a Lorentz scalar field formula_21, then one gets the Proca equation (in Lorenz gauge):\n\nIf the rest mass term is set to zero (light-like particles), then this gives the free Maxwell equation (in Lorenz gauge)\n\nUnder a proper orthochronous Lorentz transformation in Minkowski space, all one-particle quantum states of spin with spin z-component locally transform under some representation of the Lorentz group:\n\nwhere is some finite-dimensional representation, i.e. a matrix. Here is thought of as a column vector containing components with the allowed values of . The quantum numbers and as well as other labels, continuous or discrete, representing other quantum numbers are suppressed. One value of may occur more than once depending on the representation. Representations with several possible values for are considered below.\n\nThe irreducible representations are labeled by a pair of half-integers or integers . From these all other representations can be built up using a variety of standard methods, like taking tensor products and direct sums. In particular, space-time itself constitutes a 4-vector representation so that . To put this into context; Dirac spinors transform under the representation. In general, the representation space has subspaces that under the subgroup of spatial rotations, SO(3), transform irreducibly like objects of spin \"j\", where each allowed value:\n\noccurs exactly once. In general, \"tensor products of irreducible representations\" are reducible; they decompose as direct sums of irreducible representations.\n\nThe representations and can each separately represent particles of spin . A state or quantum field in such a representation would satisfy no field equation except the Klein–Gordon equation.\n\nThere are equations which have solutions that do not satisfy the superposition principle.\n\n\n\n\n"}
{"id": "26374197", "url": "https://en.wikipedia.org/wiki?curid=26374197", "title": "Slaughter tapping", "text": "Slaughter tapping\n\nSlaughter tapping is an obsolete method of extracting large quantities of natural latex from rubber trees in a forest environment. Prior to commercial exploitation of latex-bearing trees such as \"Hevea brasiliensis\" in the Amazon Basin and \"Funtumia elastica\" in the Congo, native populations limited harvesting to non-lethal tapping of the latex. However, with the rising demand for rubber worldwide in the late nineteenth century, debt-slave \"hunters\" began tapping more intensively, using ladders to extract as much latex as possible from all areas of the tree, killing the tree as a result. This \"slaughter tapping\" resulted in the destruction of all latex-bearing trees across large swathes of sub-Saharan Africa and South America. Eventually, the establishment of rubber plantations in the Far East made \"hunting\" of naturally occurring rubber trees unprofitable, and the practice largely ceased in the early twentieth century.\n\n"}
{"id": "140731", "url": "https://en.wikipedia.org/wiki?curid=140731", "title": "Snake handling in religion", "text": "Snake handling in religion\n\nSnake handling as a religious rite in the United States, also called serpent handling, is observed in a small number of isolated churches, mostly in the United States, usually characterized as rural and part of the Holiness movement. The practice began in the early 20th century in Appalachia and plays only a small part in the church service. Participants are either Holiness, Pentecostals, Charismatics, or other evangelicals.\n\nMany writers have attempted to designate George Went Hensley (1880–1955) as both the progenitor and popularizer of Appalachian religious snake handling, but his role in initiating the practice has been disputed by academic studies. Kimbrough notes that claims of Hensley being the originator of snake handling are usually found to be unsubstantiated by research, and the origins of the observance are unclear. Hood and Williamson similarly argue that the beginnings of Pentecostal snake handling rituals cannot be ascribed to a single person, and that the observance arose independently on multiple occasions.\n\nHistorians agree that Hensley helped spread Pentecostal snake handling throughout the southeast United States, and that coverage of Hensley's ministry was influential in prompting various churches to include the practice in their services. The media has focused on popular snake handlers such as Hensley, and the deaths of ministers due to snakebite have received particular attention.\n\nGeorge Went Hensley is often credited with introducing snake handling practices into the Church of God Holiness, an association of autonomous Christian Methodist congregations, founding the Dolly Pond Church of God in Birchwood, Tennessee, around 1910. He later traveled the Southeast promoting the practice, eventually resigning his ministry to start the first holiness movement church to require snake handling as evidence of salvation. If believers truly had the Holy Spirit within them, Hensley argued, they should be able to handle rattlesnakes and any number of other venomous serpents. They should also be able to drink poison and suffer no harm whatsoever. Snake handling as a test or demonstration of faith became popular wherever Hensley traveled and preached in the small towns of Tennessee, Kentucky, the Carolinas, Virginia, Ohio, and Indiana. Sister-churches later sprang up throughout the Appalachian region. In July 1955, Hensley died following a snakebite received during a service he was conducting in Altha, Florida.\n\nHensley's life was unusual for a clergyman. He had four wives (the first three marriages ended in divorce) and was frequently drunk. Practitioners of snake handling continue to view him as a great man. Kimbrough recorded a discussion with an advocate of snake handling who dismissed Hensley's personal failings as slanderous fabrications. His advocacy, leadership, and particularly his personal charisma, were important factors in the advancement of the movement.\n\nMost religious snake handlers are still found in the Appalachian Mountains and other parts of the southeastern United States, especially in Alabama, Mississippi, Georgia, Kentucky, North Carolina, Tennessee, West Virginia, and South Carolina. In 2001, about 40 small churches practiced snake handling, most of them considered to be either Holiness, Pentecostals, or Charismatics. In 2004, there were four snake-handling congregations in the provinces of Alberta and British Columbia, Canada.\n\nRalph Hood, professor of social psychology and the psychology of religion at the University of Tennessee, who has studied the snake handling movement, indicated in 2003 that the practice is \"currently at a fairly low ebb of popularity\". A 2013 article by National Public Radio gave a figure of \"about 125\" churches where snakes are handled, but also indicated that \"snake handlers are notoriously private\".\n\nPractitioners believe serpent handling dates to antiquity and quote the Gospel of Mark and the Gospel of Luke to support the practice:\n\nAnd these signs shall follow them that believe: In my name shall they cast out devils; they shall speak with new tongues. They shall take up serpents; and if they drink any deadly thing, it shall not hurt them; they shall lay hands on the sick, and they shall recover.()\nBehold, I give unto you power to tread on serpents and scorpions, and over all the power of the enemy: and nothing shall by any means hurt you. ()\n\nAnother passage from the New Testament used to support snake handlers' beliefs is , which relates that Paul was bitten by a venomous viper and suffered no harm:\n\nAnd when they were escaped, then they knew that the island was called Melita. And the barbarous people shewed us no little kindness: for they kindled a fire, and received us every one, because of the present rain, and because of the cold. And when Paul had gathered a bundle of sticks, and laid them on the fire, there came a viper out of the heat, and fastened on his hand. And when the barbarians saw the venomous beast hang on his hand, they said among themselves, No doubt this man is a murderer, whom, though he hath escaped the sea, yet vengeance suffereth not to live. And he shook off the beast into the fire, and felt no harm. Howbeit they looked when he should have swollen, or fallen down dead suddenly: but after they had looked a great while, and saw no harm come to him, they changed their minds, and said that he was a god.\n\nAs in the early days, worshipers are still encouraged to lay hands on the sick, speak in tongues, provide testimony of miracles, and occasionally consume poisons such as strychnine. Snake handlers do not worship snakes, instead using the snakes to show non-Christians that God protects them from harm. In church services, when they feel the anointing of the Holy Spirit come upon them, these Christians reach into boxes, pick up venomous snakes and hold them up as they pray, sing, and dance. Gathering mainly in homes and converted buildings, snake handlers generally adhere to strict dress codes such as uncut hair, ankle-length dresses, and no cosmetics for women; and short hair and long-sleeved shirts for men. Most preach against any use of tobacco or alcohol.\n\nLike their predecessors, today's snake handlers believe in a strict and literal interpretation of the Bible, and most Church of God with Signs Following churches are non-denominational, believing that denominations are human-made and carry the Mark of the Beast. Worshipers attend services several nights a week, where if the Holy Spirit \"intervenes\", services can last up to five hours; the minimum is usually 90 minutes. Those who die from snakebites are never criticized for lack of adequate faith; it is believed that it was simply the deceased’s time to die.\n\nThe final 12 verses of Mark 16 are a point of controversy. Most scholars, following the approach of the textual critic Bruce Metzger, believe that verses 9-20 were not part of the original text. Chronologically, the Gospel of Mark was the first of the four gospels, and the last 12 verses of Mark are absent from the two earliest manuscripts. Early third-century theologians like Origen and Clement of Alexandria also make no mention of them. Because of patristic evidence from the late 2nd century for the existence of copies of Mark with the longer ending, it is contended by a majority of scholars that the longer ending must have been written and attached no later than the early 2nd century.\n\nAll Appalachian states except West Virginia outlawed the snake-handling ritual when it first emerged. The states of Alabama, Kentucky, and Tennessee have passed laws against the use of venomous snakes and/or other reptiles that endangers the lives of others, or without a permit. The Kentucky law specifically mentions religious services; in Kentucky snake handling is a misdemeanor and punishable by a $50 to $100 fine.\n\nSnake handling is legal in the state of West Virginia, as the current state constitution does not allow any law to impede upon nor promote a religious practice. Snake handling was made a felony punishable by death under Georgia law in 1941, following the death of a seven-year-old from a rattlesnake bite. However, the punishment was so severe that juries would refuse to convict, and the law was repealed in 1968. The American Civil Liberties Union has defended the religious freedom of snake handlers against various attempts to have the practice banned.\n\nIn 1992, Glen Summerford, a serpent-handling preacher, was convicted of attempted murder of his wife with a rattlesnake. In his book, \"Salvation on Sand Mountain\", Dennis Covington, a journalist covering the Summerford trial, discusses his first-hand, investigative experiences at a snake-handling church in Appalachia. Because of their snake-handling beliefs, the congregation does not feel that Glen Summerford should be convicted. They fully believe that adherents need to handle the snakes as a demonstration of their having the Holy Spirit within. And, if they get bit by the snake, then they lack the true Spirit. Moreover, if they are bitten, then the congregation prays over them. If they die, then God intended for that to happen. This congregation did not care to put themselves in harm's way. Covington submerges himself into this congregation, and begins to care tremendously for their beliefs. That then forms into caring for Glen Summerford, himself.\n\nIn July 2008, ten people were arrested and 125 venomous snakes were confiscated as part of an undercover sting operation titled \"Twice Shy.\" Pastor Gregory James Coots of the Full Gospel Tabernacle in Jesus' Name (FGTJN) in Middlesboro, Kentucky, was arrested and 74 snakes seized from his home as part of the sting.\n\nJamie Coots (son of Gregory Coots) was cited in 2013 for illegal possession and transportation of venomous snakes when three rattlesnakes and two copperheads were discovered in his vehicle during a vehicle check in Knoxville, Tennessee. Later in 2013, Coots published an op-ed in \"The Wall Street Journal\" making an argument for U.S. Constitutional protection regarding religious freedom, especially freedom to practice the unique variety of religion found in snake-handling churches. Coots died on 15 February 2014 from a snakebite.\n\nAndrew Hamblin, who appeared alongside Jamie Coots in \"Snake Salvation\", was cited for having dangerous wildlife in 2014, but a grand jury declined to indict him.\n\nThe handling of venomous snakes has significant risks. Ralph Hood observes, \"If you go to any serpent-handling church, you'll see people with atrophied hands, and missing fingers. All the serpent-handling families have suffered such things\". Jamie Coots, a pastor who subsequently died from a snakebite, said, \"Handlers get bitten all the time, and every few years someone dies\".\n\nVarious figures for the total number of deaths from snakebite during religious services have been proposed:\nAnother source indicates that 35 people died between 1936 and 1973.\n\nHood also notes that the practice does not present a danger to observers. There is no documented case of a non-handling member being bitten by a serpent handled by another believer. Those who handle are consenting adults and as few as ten to fifteen percent of congregants handle the snakes in services. Children do not participate, and those not handling the serpents sit apart from the ritual as it proceeds.\n\nA number of films and television programs have been made about religious snake handling.\n\n\nAlabama\n\nGeorgia\n\nIndiana\n\nKentucky\n\nNorth Carolina\n\nSouth Carolina\n\nTennessee\n\nWest Virginia\n\n\n\n\nBooks\n\nArticles\n"}
{"id": "47599442", "url": "https://en.wikipedia.org/wiki?curid=47599442", "title": "Tapti Pushkaravahini", "text": "Tapti Pushkaravahini\n\nPushkaram is the Hindu festival about the 12 auspicious rivers in India. One of the 12 rivers is the river Tapti. Usually, one river is worshipped every year, but sometimes that changes, one of these rivers have a celebration and thousands of people dive in to take a bath as worship. The Rashi or the Hindu zodiac sign for this river is Dhanus or Sagittarius.\n"}
{"id": "23906554", "url": "https://en.wikipedia.org/wiki?curid=23906554", "title": "Taupo Volcano", "text": "Taupo Volcano\n\nLake Taupo, in the centre of New Zealand’s North Island, is the caldera of a large rhyolitic supervolcano called the Taupo Volcano. This huge volcano has produced two of the world’s most violent eruptions in geologically recent times.\n\nThe Taupo Volcano forms part of the Taupo Volcanic Zone, a region of volcanic activity that extends from Ruapehu in the South, through the Taupo and Rotorua regions, to White Island, in the Bay of Plenty.\n\nTaupo began erupting about 300,000 years ago, but the main eruptions that still affect the surrounding landscape are the Oruanui eruption, about 26,500 years ago, which is responsible for the shape of the modern caldera, and the Hatepe eruption, about 1,800 years ago. However, there have been many more eruptions, with major ones every thousand years or so (see timeline of last 10,000 years of eruptions).\n\nConsidering recent history alone, the volcano has been inactive for an unusually long period of time, but considering its long-term activity, it was inactive for much longer between 8100 and 5100 BC (3,000 year inactivity, compared to the current 1,800 years)\n\nThe Taupo Volcano erupts rhyolite, a viscous magma, with a high silica content.\n\nIf the magma does not contain much gas, rhyolite tends to just form a lava dome. However, when mixed with gas or steam, rhyolitic eruptions can be extremely violent. The magma froths to form pumice and ash, which is thrown out with great force.\n\nIf the volcano creates a stable plume, high in the atmosphere, the pumice and ash is blown sideways, and eventually falls to the ground, draping the landscape like snow.\n\nIf the material thrown out cools too rapidly, and becomes denser than the air, it cannot rise as high, and suddenly collapses back to the ground, forming a pyroclastic flow, hitting the surface like water from a waterfall, and spreading sideways across the land at enormous speed. When the pumice and ash settles, it is sufficiently hot to stick together as a rock called Ignimbrite. Pyroclastic flows can travel hundreds of kilometres an hour.\n\nEarlier ignimbrite eruptions occurred further north than Taupo. Some of these were enormous, and two eruptions around 1.25 and 1.0 million years ago were big enough to generate an ignimbrite sheet that covered the North Island from Auckland to Napier.\n\nWhile Taupo has been active for 300,000 years, explosive eruptions became more common 65,000 years ago.\n\nThe Oruanui eruption of the Taupo Volcano was the world's largest known eruption in the past 70,000 years, with a Volcanic Explosivity Index of 8. It occurred around 26,500 years ago and generated approximately 430 km³ of pyroclastic fall deposits, 320 km³ of pyroclastic density current (PDC) deposits (mostly ignimbrite) and 420 km³ of primary intracaldera material, equivalent to 530 km³ of magma.\n\nModern Lake Taupo partly fills the caldera generated during this eruption.\n\nTephra from the eruption covered much of the central North Island with ignimbrite up to 200 metres deep. Most of New Zealand was affected by ash fall, with even an 18 cm ash layer left on the Chatham Islands, 1,000 km away. Later erosion and sedimentation had long-lasting effects on the landscape, and caused the Waikato River to shift from the Hauraki Plains to its current course through the Waikato to the Tasman Sea.\n\nThe Taupo eruption (also known as the Hatepe eruption) represents the most recent major eruption of the Taupo Volcano, and occurred about 1,800 years ago. It represents the most violent eruption in the world in the last 5,000 years.\n\nThe eruption went through several stages.\n\nThe main pyroclastic flow devastated the surrounding area, climbing over 1500 metres (5000 ft) to overtop the nearby Kaimanawa Ranges and Mount Tongariro, and covering the land within with ignimbrite from Rotorua to Waiouru. Only Ruapehu was high enough to divert the flow.\n\nThe power of the pyroclastic flow was so strong that in some places it eroded more material off the ground surface than it replaced with ignimbrite. Valleys were filled with ignimbrite, evening out the shape of the land.\n\nAll vegetation within the area was flattened. Loose pumice and ash deposits formed lahars down all the main rivers.\n\nThe eruption further expanded the lake, which had formed after the much larger Oruanui eruption. The previous outlet was blocked, raising the lake 35 metres above its present level until it broke out in a huge flood, flowing for more than a week at roughly 200 times the Waikato River's current rate.\n\nMany dates have been given for the Taupo eruption. One estimated date is 181 CE from ice cores in Greenland and Antarctica. It is possible that the meteorological phenomena described by Fan Ye in China and by Herodian in Rome were due to this eruption, which would give a date of exactly 186 however ash from volcanic activity does not normally cross hemispheres, and recent radiocarbon dating by R. Sparks has put the date at 233 CE +/- 13 (95% confidence).\n\nThere was no local population at this time, so the nearest humans would have been those in Australia, more than 2000 km to the west.\n\n"}
{"id": "54160066", "url": "https://en.wikipedia.org/wiki?curid=54160066", "title": "Violet Lake", "text": "Violet Lake\n\nViolet Lake (), is a small high-elevation lake located at above sea level on Mauna Kahalawai (the West Maui Mountains), situated in the western part of the island of Maui. It is located in the boggy slopes near the ʻEke Crater and Puʻu Kukui, the highest peak of the West Maui Mountains. It is approximately in size.\n\nThe lake's English name derive from the reflected color of the lake's surface and also the Maui violet (\"Viola mauiensis\") which grows on its banks. The Hawaiian language name Kiʻowaiokihawahine means the \"pond of Kihawahine\".\n\nThe lake was important to the traditional Hawaiian religion. During ancient times, the lake and surrounding summit area was protected by \"kapu\" and regarded as the meeting place of heaven and earth. The lake was believed to be the home of the Hawaiian \"moʻo\" (lizard) goddess Kihawahine, who was associated with the \"aliʻi nui\" (high chiefs or kings) of Maui and an \"ʻaumakua\" (family deity) of Queen Keōpūolani, a descendant of the Maui royal line and the highest-ranking wife of King Kamehameha I, who established the Kingdom of Hawaii, and their son Kamehameha III. Kihawahine was also associated with the wetland and former royal complex at Mokuʻula located in the watersheds below the lake in Lahaina.\n\nThe surrounding montane rainforest ecosystem on the slopes of Puʻu Kukui Watershed Management Area (the second wettest spot in the Hawaiian Islands) is extremely diverse and home to many endemic species. The lake's own unique ecosystem have not been well studied. \nSpecies include the dwarfed ʻōhiʻa lehua tree (\"Metrosideros polymorpha\"), the Maui violet (\"Viola mauiensis\"), a variety of Hawaiian lobelioids (\"Lobelia gloriamontis\"), the Hawaiian damsel flies (\"Megalagrion spp.\"), and others. It has been described as an \"extremely rare gem\" which have \"residents and visiting scientists alike\".\n\n"}
{"id": "49801999", "url": "https://en.wikipedia.org/wiki?curid=49801999", "title": "Wild Seasons (Kay Young)", "text": "Wild Seasons (Kay Young)\n\nWild Seasons: Gathering and Cooking Wild Plants of the Great Plains is a 1993 non-fiction book by author, illustrator, and ethno-botanist Kay Young. It features a variety of wild plants of the great plains area and how to prepare them in appetizing ways. The book includes a number of recipes as well as Young's enthusiasm and advocacy for eating wild crops. It was published by the University of Nebraska Press.\n\n"}
