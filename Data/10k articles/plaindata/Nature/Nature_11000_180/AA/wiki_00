{"id": "15674073", "url": "https://en.wikipedia.org/wiki?curid=15674073", "title": "2008 Central Asia energy crisis", "text": "2008 Central Asia energy crisis\n\nThe 2008 Central Asia energy crisis was an energy shortage in Central Asia, which, combined with the severe weather of the 2007-08 winter (the coldest since 1969) and high prices for food and fuel, caused considerable hardship for many. The abnormally cold weather has pushed demand up for electricity, exacerbating the crisis. The situation was most dire in Tajikistan. An international appeal was made by the United Nations, NGOs, and the Red Cross and Red Crescent for around US$25 million to assist the government. At the time, The UN warned that millions face starvation during the 2008-09 winter.\n\nAt the beginning of January 2008, officials announced an electricity price hike of 20 percent to allow the \"government [to] repay its debt to the World Bank.\" According to an official at Barqi Tojik, a national power company, limits will become stricter, and the price for electricity is expected to rise until 2010.\n\nOn April 2008, Pradeep Mitra, World Bank chief economist for Europe and Central Asia, issued an uncharacteristic statement, urging the worse-hit countries to spend more on social assistance and \"top up\" their social programs.\n\nNonetheless, Mitra focus remained centred \"on inflation management,\" suggesting that the affected countries \"especially refrain from imposing controls on trade\" (measures recently undertaken by many countries to protect their populations from food price inflation and keep food available domestically), arguing that \"it could work against the food supply in the longer term.\"\n\nStarting on January 13, 2008, many villages received only one to three hours of electricity per day, and the capital Dushanbe cut power to residential areas overnight. On January 26, 2008, Dushanbe cut power to places of entertainment (including restaurants, shops, pharmacies, markets, and public bathhouses), causing many to close until spring. There were only few visible lights in the city which were from the owners of generators, factories, or people who had illegally tapped the power lines. The restriction was set to end February 10, however it was subsequently extended. There were only few exemptions to the restrictions which included government offices, hospitals, and certain industrial cities, such as Tursunzoda, which had a large aluminum plant. Because of inoperable central heating systems in Dushanbe and other cities, residents in apartment blocks had no means other than electricity to heat their homes.\n\nThe situation was exacerbated by the cold winter, with temperatures reaching -20 degrees Celsius. Dushanbe residents reported wearing several jackets and overcoats to sleep and all family members sleeping under a single blanket to share warmth. The UN's World Food Programme also declared the food situation as being in emergency shortage, in both cities and rural areas.\n\nAs of mid-January 2008, the state-run media did not discuss the problem. Subsequently throughout the month of February, there emerged numerous Western media reports of children dying in maternity wards of hospitals during blackouts. The Tajik government maintains that the blackouts were not responsible for any deaths. The Tajik government has appealed for international aid. Meanwhile, aid workers and diplomats urged the government to declare a state of emergency. The handling of the crisis has raised questions about the competence of the political leadership.\n\nOn March 3, 2008 it was reported that the crisis in Tajikistan has eased: \"From now on (at least till the next winter) Dushanbe is not going to have problems with electricity and the tough schedule introduced in the beginning of this winter was abandoned on March 1, 2008 in Dushanbe by the decision of Barqi Tojik.\" The Christian Science Monitor, neweurasia, and other media observers predicted that a nascent hunger crisis will erupt into a full famine as a consequence of the energy shortages, which subsequently happened. UN experts announced on 10 October that almost one-third of Tajikistan’s 6.7 million inhabitants did not have enough to eat for the winter of 2008-09.\n\nIn Kyrgyzstan, also rich in hydroelectric resources, the cold weather had made demand 10% higher than in the winter of 2006/2007, which depleted the main Toktogul reservoir for hydroelectric power.\n\nBeginning in late December 2007, the unusually harsh weather had frozen the gas supply to numerous homes and businesses across Uzbekistan. As a result, there had been numerous demonstrations and protests against the government, in favor of an insured uninterrupted supply of gas and electricity. The government response was varied, and in Karakalpakstan, they met protesters and promised to rectify the situation, while the local government head of Hazarasp responded to a complaint by one woman by cutting off gas altogether to all the houses on her street.\n\nSome in Uzbekistan were able to turn to \"traditional methods\" for heating, and it was reported that some villages which had no trees left because villagers have cut them down to heat their homes and cook food. This had an expected negative effect on the economy, because the leaves are essential to the local silk industry, and the fruit grown on these trees are the main source of income for many villagers.\n\nIn some provinces of Turkmenistan, villagers were burning saxaul plants, a traditional Turkmen way to heat homes, since in the cities, the central heating pipes have been neglected and failed to produce adequate heat.\n\n"}
{"id": "25793481", "url": "https://en.wikipedia.org/wiki?curid=25793481", "title": "2010 AL30", "text": "2010 AL30\n\nItalian scientists Ernesto Guido and Giovanni Sostero told RIA Novosti that it had an orbital period of almost exactly one year and might be a spent rocket booster. However, it was determined that it is a near-Earth asteroid.\n\nOn January 13, 2010 at 1246 UT it passed Earth at , about 1/3 of the distance from the Earth to the Moon (or 0.33 LD).\n\nBased an estimated diameter of , if 2010 AL30 had entered the Earth's atmosphere, it would have created an air burst equivalent to between 50 kT and 100 kT (kilotons of TNT). The Nagasaki \"Fat Man\" atom bomb had a yield between 13–18 kT.\n\nIt has an uncertainty parameter of 2 and has been observed by radar. Radar observations show the asteroid is elongated and is about 30 meters in diameter. It may be a contact binary.\n\n\n"}
{"id": "9146522", "url": "https://en.wikipedia.org/wiki?curid=9146522", "title": "A field guide to the birds of Hawaii and the Tropical Pacific", "text": "A field guide to the birds of Hawaii and the Tropical Pacific\n\nA Field Guide to the Birds of Hawaii and the Tropical Pacific is a 1987 book by Harold Douglas Pratt, Jr., Phillip L. Bruner and Delwyn G. Berrett (with illustrations by Pratt). It is published by Princeton University Press and is produced as both hardback () and softback () editions. The book is primarily a field guide to birds found in the Hawaiian islands, Micronesia, Fiji and tropical Polynesia (plus some northern subtropical Polynesian islands), including some distribution and status data. It was the first identification work to cover the birds of the whole of this region.\n\nThe book is 190 mm high by 128 mm wide. It comprises xx + 409 pages, with 45 colour plates (43 of which contain Pratt's illustrations of birds; the remaining two contain photographs of plants important to birds in the region). The main text is divided up into three sections:\n\nAppendix A (pages 321 - 328) contains a hypothetical list for the region, and Appendix B (pages 329 - 258) a series of species checklists for the individual subdivisions. A map of the region covered is given on pages xvi - xvii, with more detailed maps of individual parts of the region in Appendix C (pages 359 - 372). These appendices are followed by a glossary, bibliography and index.\n\nA list of extinct species is given on page 36, and a number of these are illustrated in the book's artwork section.\n\nThe book is dedicated to Dr. Robert J. Newman.\n"}
{"id": "28856255", "url": "https://en.wikipedia.org/wiki?curid=28856255", "title": "Ancient Greek flood myths", "text": "Ancient Greek flood myths\n\nGreek mythology describes three floods, the flood of Ogyges, the flood of Deucalion, and the flood of Dardanus. Two of the Greek Ages of Man concluded with a flood: The Ogygian Deluge ended the Silver Age, and the flood of Deucalion ended the First Bronze Age. In addition to these floods, Greek mythology says the world was also periodically destroyed by fire. See Phaëton.\n\nThe Ogygian flood is so called because it occurred in the time of Ogyges, a mythical king of Attica. The name \"Ogyges\" and \"Ogygian\" is synonymous with \"primeval\", \"primal\" and \"earliest dawn\". Others say he was the founder and king of Thebes. In many traditions the Ogygian flood is said to have covered the whole world and was so devastating that Attica remained without kings until the reign of Cecrops.\n\nPlato in his Laws, Book III, argues that this flood had occurred \nten thousand years before his time, as opposed to only \"one or two thousand years that have elapsed\" since the discovery of music, and other inventions. Also in \"Timaeus\" (22) and in \"Critias\" (111-112) he describes the \"great deluge of all\" as having been preceded by 9,000 years of history before the time of his contemporary Solon, during the 10th millennium BCE. In addition, the texts report that \"many great deluges have taken place during the nine thousand years\" since Athens and Atlantis were preeminent.\n\nThe Deucalion legend as told by the \"Bibliotheca\" has some similarity to other deluge myths such as the Epic of Gilgamesh and the story of Noah's Ark. The Titan Prometheus advised his son Deucalion to build a chest. All other men perished except for a few who escaped to high mountains. The mountains in Thessaly were parted, and all the world beyond the Isthmus and Peloponnese was overwhelmed. Deucalion and his wife Pyrrha, after floating in the chest for nine days and nights, landed on Parnassus. An older version of the story told by Hellanicus has Deucalion's \"ark\" landing on Mount Othrys in Thessaly. Another account has him landing on a peak, probably Phouka, in Argolis, later called Nemea. When the rains ceased, he sacrificed to Zeus. Then, at the bidding of Zeus, he threw stones behind him, and they became men, and the stones Pyrrha threw became women. The \"Bibliotheca\" gives this as an etymology for Greek \"Laos\" \"people\" as derived from \"laas\" \"stone\". The Megarians told that Megarus, son of Zeus and a Sithnid nymph, escaped Deucalion's flood by swimming to the top of Mount Gerania, guided by the cries of cranes.\n\nAccording to the \"Theogony\" of the \"Bibliotheca\", Prometheus moulded men out of water and earth and gave them fire which, unknown to Zeus, he had hidden in a stalk of fennel. When Zeus learned of it, he ordered Hephaestus to nail Prometheus to Mount Caucasus, a Scythian mountain. Prometheus was nailed to the mountain and kept bound for many years. Every day an eagle swooped on him and devoured the lobes of his liver, which grew by night. That was the penalty that Prometheus paid for the theft of fire until Heracles afterwards released him.\n\nPrometheus had a son Deucalion. He, reigning in the regions about Phthia, married Pyrrha, the daughter of Epimetheus and Pandora, the first woman fashioned by the gods. And when Zeus would destroy the men of the Bronze Age, Deucalion, by the advice of Prometheus, constructed a chest. Having stocked it with provisions, he embarked in it with Pyrrha. Zeus, by pouring heavy rain from heaven, flooded the greater part of Greece, so that all men were destroyed, except a few who fled to the high mountains in the neighbourhood as Peloponnesus was overwhelmed. But Deucalion, floating in the chest over the sea for nine days and as many nights, drifted to Parnassus, and there, when the rain ceased, he landed and made a sacrifice to Zeus, the god of Escape. And Zeus sent Hermes to him and allowed him to choose what he would, and he chose to get men.\n\nAt the bidding of Zeus he took up stones and threw them over his head, and the stones Deucalion threw became men, and the stones Pyrrha threw became women. Hence people were called metaphorically people (Laos) from laas, \"a stone.\" And Deucalion had children by Pyrrha, first Hellen, whose father some say was Zeus, and second Amphictyon, who reigned over Attica after Cranaus, and third a daughter Protogonia, who became the mother of Aethlius by Zeus. Hellen had Dorus, Xuthus, and Aeolus by a nymph Orseis. Those who were called Greeks he named Hellenes after himself, and divided the country among his sons. Xuthus received Peloponnese and begat Achaeus and Ion by Creusa, daughter of Erechtheus, and from Achaeus and Ion the Achaeans and Ionians derive their names. Dorus received the country over against Peloponnese and called the settlers Dorians after himself.\n\nAeolus reigned over the regions about Thessaly and named the inhabitants Aeolians. He married Enarete, daughter of Deimachus, and begat seven sons, Cretheus, Sisyphus, Athamas, Salmoneus, Deion, Magnes, Perieres, and five daughters, Canace, Alcyone, Pisidice, Calyce, Perimede. Perimede had Hippodamas and Orestes by Achelous; and Pisidice had Antiphus and Actor by Myrmidon. Alcyone was married by Ceyx, son of Lucifer (?). These perished by reason of their pride, for he said that his wife was Hera, and she said that her husband was Zeus. But Zeus turned them into birds; her he made a kingfisher (alcyon) and him a gannet (ceyx).\n\nThis one has the same basic story line.\nAccording to Dionysius of Halicarnassus, Dardanus left Pheneus in Arcadia to colonize a land in the North-East Aegean Sea. When Dardanus' deluge occurred, the land was flooded and the mountain where he and his family survived formed the island of Samothrace. He left Samothrace on an inflated skin to the opposite shores of Asia Minor and settled on Mount Ida. Due to the fear of another flood, they refrained from building a city and lived in the open for fifty years. His grandson Tros eventually moved from the highlands down to a large plain, on a hill that had many rivers flowing down from Ida above. There he built a city, which was named Troy after him. Today, we call the area \"the Dardanelles\" (formerly known as Hellespont), a narrow strait in northwestern Turkey that connects the Aegean Sea to the Sea of Marmara. The name is derived from Dardania, an ancient land on the Asian shore of the strait which in turn takes its name from Dardanus, the mythical son of Zeus and Electra.\n"}
{"id": "902", "url": "https://en.wikipedia.org/wiki?curid=902", "title": "Atom", "text": "Atom\n\nAn atom is the smallest constituent unit of ordinary matter that has the properties of a chemical element. Every solid, liquid, gas, and plasma is composed of neutral or ionized atoms. Atoms are extremely small; typical sizes are around 100 picometers (a ten-billionth of a meter, in the short scale).\n\nAtoms are small enough that attempting to predict their behavior using classical physics – as if they were billiard balls, for example – gives noticeably incorrect predictions due to quantum effects. Through the development of physics, atomic models have incorporated quantum principles to better explain and predict this behavior.\n\nEvery atom is composed of a nucleus and one or more electrons bound to the nucleus. The nucleus is made of one or more protons and typically a similar number of neutrons. Protons and neutrons are called nucleons. More than 99.94% of an atom's mass is in the nucleus. The protons have a positive electric charge, the electrons have a negative electric charge, and the neutrons have no electric charge. If the number of protons and electrons are equal, that atom is electrically neutral. If an atom has more or fewer electrons than protons, then it has an overall negative or positive charge, respectively, and it is called an ion.\n\nThe electrons of an atom are attracted to the protons in an atomic nucleus by this electromagnetic force. The protons and neutrons in the nucleus are attracted to each other by a different force, the nuclear force, which is usually stronger than the electromagnetic force repelling the positively charged protons from one another. Under certain circumstances, the repelling electromagnetic force becomes stronger than the nuclear force, and nucleons can be ejected from the nucleus, leaving behind a different element: nuclear decay resulting in nuclear transmutation.\n\nThe number of protons in the nucleus defines to what chemical element the atom belongs: for example, all copper atoms contain 29 protons. The number of neutrons defines the isotope of the element. The number of electrons influences the magnetic properties of an atom. Atoms can attach to one or more other atoms by chemical bonds to form chemical compounds such as molecules. The ability of atoms to associate and dissociate is responsible for most of the physical changes observed in nature and is the subject of the discipline of chemistry.\n\nThe idea that matter is made up of discrete units is a very old idea, appearing in many ancient cultures such as Greece and India. The word \"atom\" (; \"\"), meaning \"uncuttable\", was coined by the ancient Greek philosophers Leucippus and his pupil Democritus ( 460 – 370 BC). Democritus taught that atoms were infinite in number, uncreated, and eternal, and that the qualities of an object result from the kind of atoms that compose it. Democritus's atomism was refined and elaborated by the later philosopher Epicurus (341–270 BC). During the Early Middle Ages, atomism was mostly forgotten in western Europe, but survived among some groups of Islamic philosophers. During the twelfth century, atomism became known again in western Europe through references to it in the newly-rediscovered writings of Aristotle.\n\nIn the fourteenth century, the rediscovery of major works describing atomist teachings, including Lucretius's \"De rerum natura\" and Diogenes Laërtius's \"Lives and Opinions of Eminent Philosophers\", led to increased scholarly attention on the subject. Nonetheless, because atomism was associated with the philosophy of Epicureanism, which contradicted orthodox Christian teachings, belief in atoms was not considered acceptable. The French Catholic priest Pierre Gassendi (1592–1655) revived Epicurean atomism with modifications, arguing that atoms were created by God and, though extremely numerous, are not infinite. Gassendi's modified theory of atoms was popularized in France by the physician François Bernier (1620–1688) and in England by the natural philosopher Walter Charleton (1619–1707). The chemist Robert Boyle (1627–1691) and the physicist Isaac Newton (1642–1727) both defended atomism and, by the end of the seventeenth century, it had become accepted by portions of the scientific community.\n\nIn the early 1800s, John Dalton used the concept of atoms to explain why elements always react in ratios of small whole numbers (the law of multiple proportions). For instance, there are two types of tin oxide: one is 88.1% tin and 11.9% oxygen and the other is 78.7% tin and 21.3% oxygen (tin(II) oxide and tin dioxide respectively). This means that 100g of tin will combine either with 13.5g or 27g of oxygen. 13.5 and 27 form a ratio of 1:2, a ratio of small whole numbers. This common pattern in chemistry suggested to Dalton that elements react in multiples of discrete units — in other words, atoms. In the case of tin oxides, one tin atom will combine with either one or two oxygen atoms.\n\nDalton also believed atomic theory could explain why water absorbs different gases in different proportions. For example, he found that water absorbs carbon dioxide far better than it absorbs nitrogen. Dalton hypothesized this was due to the differences between the masses and configurations of the gases' respective particles, and carbon dioxide molecules (CO) are heavier and larger than nitrogen molecules (N).\n\nIn 1827, botanist Robert Brown used a microscope to look at dust grains floating in water and discovered that they moved about erratically, a phenomenon that became known as \"Brownian motion\". This was thought to be caused by water molecules knocking the grains about. In 1905, Albert Einstein proved the reality of these molecules and their motions by producing the first statistical physics analysis of Brownian motion. French physicist Jean Perrin used Einstein's work to experimentally determine the mass and dimensions of atoms, thereby conclusively verifying Dalton's atomic theory.\n\nThe physicist J.J. Thomson measured the mass of cathode rays, showing they were made of particles, but were around 1800 times lighter than the lightest atom, hydrogen. Therefore, they were not atoms, but a new particle, the first \"subatomic\" particle to be discovered, which he originally called \"corpuscle\" but was later named \"electron\", after particles postulated by George Johnstone Stoney in 1874. He also showed they were identical to particles given off by photoelectric and radioactive materials. It was quickly recognized that they are the particles that carry electric currents in metal wires, and carry the negative electric charge within atoms. Thomson was given the 1906 Nobel Prize in Physics for this work. Thus he overturned the belief that atoms are the indivisible, ultimate particles of matter. Thomson also incorrectly postulated that the low mass, negatively charged electrons were distributed throughout the atom in a uniform sea of positive charge. This became known as the plum pudding model.\n\nIn 1909, Hans Geiger and Ernest Marsden, under the direction of Ernest Rutherford, bombarded a metal foil with alpha particles to observe how they scattered. They expected all the alpha particles to pass straight through with little deflection, because Thomson's model said that the charges in the atom are so diffuse that their electric fields could not affect the alpha particles much. However, Geiger and Marsden spotted alpha particles being deflected by angles greater than 90°, which was supposed to be impossible according to Thomson's model. To explain this, Rutherford proposed that the positive charge of the atom is concentrated in a tiny nucleus at the center of the atom.\n\nWhile experimenting with the products of radioactive decay, in 1913 radiochemist Frederick Soddy discovered that there appeared to be more than one type of atom at each position on the periodic table. The term isotope was coined by Margaret Todd as a suitable name for different atoms that belong to the same element. J.J. Thomson created a technique for isotope separation through his work on ionized gases, which subsequently led to the discovery of stable isotopes.\n\nIn 1913 the physicist Niels Bohr proposed a model in which the electrons of an atom were assumed to orbit the nucleus but could only do so in a finite set of orbits, and could jump between these orbits only in discrete changes of energy corresponding to absorption or radiation of a photon. This quantization was used to explain why the electrons orbits are stable (given that normally, charges in acceleration, including circular motion, lose kinetic energy which is emitted as electromagnetic radiation, see \"synchrotron radiation\") and why elements absorb and emit electromagnetic radiation in discrete spectra.\n\nLater in the same year Henry Moseley provided additional experimental evidence in favor of Niels Bohr's theory. These results refined Ernest Rutherford's and Antonius Van den Broek's model, which proposed that the atom contains in its nucleus a number of positive nuclear charges that is equal to its (atomic) number in the periodic table. Until these experiments, atomic number was not known to be a physical and experimental quantity. That it is equal to the atomic nuclear charge remains the accepted atomic model today.\n\nChemical bonds between atoms were now explained, by Gilbert Newton Lewis in 1916, as the interactions between their constituent electrons. As the chemical properties of the elements were known to largely repeat themselves according to the periodic law, in 1919 the American chemist Irving Langmuir suggested that this could be explained if the electrons in an atom were connected or clustered in some manner. Groups of electrons were thought to occupy a set of electron shells about the nucleus.\n\nThe Stern–Gerlach experiment of 1922 provided further evidence of the quantum nature of atomic properties. When a beam of silver atoms was passed through a specially shaped magnetic field, the beam was split in a way correlated with the direction of an atom's angular momentum, or spin. As this spin direction is initially random, the beam would be expected to deflect in a random direction. Instead, the beam was split into two directional components, corresponding to the atomic spin being oriented up or down with respect to the magnetic field.\n\nIn 1925 Werner Heisenberg published the first consistent mathematical formulation of quantum mechanics (Matrix Mechanics). One year earlier, in 1924, Louis de Broglie had proposed that all particles behave to an extent like waves and, in 1926, Erwin Schrödinger used this idea to develop a mathematical model of the atom (Wave Mechanics) that described the electrons as three-dimensional waveforms rather than point particles.\n\nA consequence of using waveforms to describe particles is that it is mathematically impossible to obtain precise values for both the position and momentum of a particle at a given point in time; this became known as the uncertainty principle, formulated by Werner Heisenberg in 1927. In this concept, for a given accuracy in measuring a position one could only obtain a range of probable values for momentum, and vice versa.\nThis model was able to explain observations of atomic behavior that previous models could not, such as certain structural and spectral patterns of atoms larger than hydrogen. Thus, the planetary model of the atom was discarded in favor of one that described atomic orbital zones around the nucleus where a given electron is most likely to be observed.\n\nThe development of the mass spectrometer allowed the mass of atoms to be measured with increased accuracy. The device uses a magnet to bend the trajectory of a beam of ions, and the amount of deflection is determined by the ratio of an atom's mass to its charge. The chemist Francis William Aston used this instrument to show that isotopes had different masses. The atomic mass of these isotopes varied by integer amounts, called the whole number rule. The explanation for these different isotopes awaited the discovery of the neutron, an uncharged particle with a mass similar to the proton, by the physicist James Chadwick in 1932. Isotopes were then explained as elements with the same number of protons, but different numbers of neutrons within the nucleus.\n\nIn 1938, the German chemist Otto Hahn, a student of Rutherford, directed neutrons onto uranium atoms expecting to get transuranium elements. Instead, his chemical experiments showed barium as a product. A year later, Lise Meitner and her nephew Otto Frisch verified that Hahn's result were the first experimental \"nuclear fission\". In 1944, Hahn received the Nobel prize in chemistry. Despite Hahn's efforts, the contributions of Meitner and Frisch were not recognized.\n\nIn the 1950s, the development of improved particle accelerators and particle detectors allowed scientists to study the impacts of atoms moving at high energies. Neutrons and protons were found to be hadrons, or composites of smaller particles called quarks. The standard model of particle physics was developed that so far has successfully explained the properties of the nucleus in terms of these sub-atomic particles and the forces that govern their interactions.\n\nThough the word \"atom\" originally denoted a particle that cannot be cut into smaller particles, in modern scientific usage the atom is composed of various subatomic particles. The constituent particles of an atom are the electron, the proton and the neutron; all three are fermions. However, the hydrogen-1 atom has no neutrons and the hydron ion has no electrons.\n\nThe electron is by far the least massive of these particles at , with a negative electrical charge and a size that is too small to be measured using available techniques. It was the lightest particle with a positive rest mass measured, until the discovery of neutrino mass. Under ordinary conditions, electrons are bound to the positively charged nucleus by the attraction created from opposite electric charges. If an atom has more or fewer electrons than its atomic number, then it becomes respectively negatively or positively charged as a whole; a charged atom is called an ion. Electrons have been known since the late 19th century, mostly thanks to J.J. Thomson; see history of subatomic physics for details.\n\nProtons have a positive charge and a mass 1,836 times that of the electron, at . The number of protons in an atom is called its atomic number. Ernest Rutherford (1919) observed that nitrogen under alpha-particle bombardment ejects what appeared to be hydrogen nuclei. By 1920 he had accepted that the hydrogen nucleus is a distinct particle within the atom and named it proton.\n\nNeutrons have no electrical charge and have a free mass of 1,839 times the mass of the electron, or , the heaviest of the three constituent particles, but it can be reduced by the nuclear binding energy. Neutrons and protons (collectively known as nucleons) have comparable dimensions—on the order of —although the 'surface' of these particles is not sharply defined. The neutron was discovered in 1932 by the English physicist James Chadwick.\n\nIn the Standard Model of physics, electrons are truly elementary particles with no internal structure. However, both protons and neutrons are composite particles composed of elementary particles called quarks. There are two types of quarks in atoms, each having a fractional electric charge. Protons are composed of two up quarks (each with charge +) and one down quark (with a charge of −). Neutrons consist of one up quark and two down quarks. This distinction accounts for the difference in mass and charge between the two particles.\n\nThe quarks are held together by the strong interaction (or strong force), which is mediated by gluons. The protons and neutrons, in turn, are held to each other in the nucleus by the nuclear force, which is a residuum of the strong force that has somewhat different range-properties (see the article on the nuclear force for more). The gluon is a member of the family of gauge bosons, which are elementary particles that mediate physical forces.\n\nAll the bound protons and neutrons in an atom make up a tiny atomic nucleus, and are collectively called nucleons. The radius of a nucleus is approximately equal to 1.07  fm, where \"A\" is the total number of nucleons. This is much smaller than the radius of the atom, which is on the order of 10 fm. The nucleons are bound together by a short-ranged attractive potential called the residual strong force. At distances smaller than 2.5 fm this force is much more powerful than the electrostatic force that causes positively charged protons to repel each other.\n\nAtoms of the same element have the same number of protons, called the atomic number. Within a single element, the number of neutrons may vary, determining the isotope of that element. The total number of protons and neutrons determine the nuclide. The number of neutrons relative to the protons determines the stability of the nucleus, with certain isotopes undergoing radioactive decay.\n\nThe proton, the electron, and the neutron are classified as fermions. Fermions obey the Pauli exclusion principle which prohibits \"identical\" fermions, such as multiple protons, from occupying the same quantum state at the same time. Thus, every proton in the nucleus must occupy a quantum state different from all other protons, and the same applies to all neutrons of the nucleus and to all electrons of the electron cloud.\n\nA nucleus that has a different number of protons than neutrons can potentially drop to a lower energy state through a radioactive decay that causes the number of protons and neutrons to more closely match. As a result, atoms with matching numbers of protons and neutrons are more stable against decay. However, with increasing atomic number, the mutual repulsion of the protons requires an increasing proportion of neutrons to maintain the stability of the nucleus, which slightly modifies this trend of equal numbers of protons to neutrons.\n\nThe number of protons and neutrons in the atomic nucleus can be modified, although this can require very high energies because of the strong force. Nuclear fusion occurs when multiple atomic particles join to form a heavier nucleus, such as through the energetic collision of two nuclei. For example, at the core of the Sun protons require energies of 3–10 keV to overcome their mutual repulsion—the coulomb barrier—and fuse together into a single nucleus. Nuclear fission is the opposite process, causing a nucleus to split into two smaller nuclei—usually through radioactive decay. The nucleus can also be modified through bombardment by high energy subatomic particles or photons. If this modifies the number of protons in a nucleus, the atom changes to a different chemical element.\n\nIf the mass of the nucleus following a fusion reaction is less than the sum of the masses of the separate particles, then the difference between these two values can be emitted as a type of usable energy (such as a gamma ray, or the kinetic energy of a beta particle), as described by Albert Einstein's mass–energy equivalence formula, \"E\" = \"mc\", where \"m\" is the mass loss and \"c\" is the speed of light. This deficit is part of the binding energy of the new nucleus, and it is the non-recoverable loss of the energy that causes the fused particles to remain together in a state that requires this energy to separate.\n\nThe fusion of two nuclei that create larger nuclei with lower atomic numbers than iron and nickel—a total nucleon number of about 60—is usually an exothermic process that releases more energy than is required to bring them together. It is this energy-releasing process that makes nuclear fusion in stars a self-sustaining reaction. For heavier nuclei, the binding energy per nucleon in the nucleus begins to decrease. That means fusion processes producing nuclei that have atomic numbers higher than about 26, and atomic masses higher than about 60, is an endothermic process. These more massive nuclei can not undergo an energy-producing fusion reaction that can sustain the hydrostatic equilibrium of a star.\n\nThe electrons in an atom are attracted to the protons in the nucleus by the electromagnetic force. This force binds the electrons inside an electrostatic potential well surrounding the smaller nucleus, which means that an external source of energy is needed for the electron to escape. The closer an electron is to the nucleus, the greater the attractive force. Hence electrons bound near the center of the potential well require more energy to escape than those at greater separations.\n\nElectrons, like other particles, have properties of both a particle and a wave. The electron cloud is a region inside the potential well where each electron forms a type of three-dimensional standing wave—a wave form that does not move relative to the nucleus. This behavior is defined by an atomic orbital, a mathematical function that characterises the probability that an electron appears to be at a particular location when its position is measured. Only a discrete (or quantized) set of these orbitals exist around the nucleus, as other possible wave patterns rapidly decay into a more stable form. Orbitals can have one or more ring or node structures, and differ from each other in size, shape and orientation.\n\nEach atomic orbital corresponds to a particular energy level of the electron. The electron can change its state to a higher energy level by absorbing a photon with sufficient energy to boost it into the new quantum state. Likewise, through spontaneous emission, an electron in a higher energy state can drop to a lower energy state while radiating the excess energy as a photon. These characteristic energy values, defined by the differences in the energies of the quantum states, are responsible for atomic spectral lines.\n\nThe amount of energy needed to remove or add an electron—the electron binding energy—is far less than the binding energy of nucleons. For example, it requires only 13.6 eV to strip a ground-state electron from a hydrogen atom, compared to 2.23 \"million\" eV for splitting a deuterium nucleus. Atoms are electrically neutral if they have an equal number of protons and electrons. Atoms that have either a deficit or a surplus of electrons are called ions. Electrons that are farthest from the nucleus may be transferred to other nearby atoms or shared between atoms. By this mechanism, atoms are able to bond into molecules and other types of chemical compounds like ionic and covalent network crystals.\n\nBy definition, any two atoms with an identical number of \"protons\" in their nuclei belong to the same chemical element. Atoms with equal numbers of protons but a different number of \"neutrons\" are different isotopes of the same element. For example, all hydrogen atoms admit exactly one proton, but isotopes exist with no neutrons (hydrogen-1, by far the most common form, also called protium), one neutron (deuterium), two neutrons (tritium) and more than two neutrons. The known elements form a set of atomic numbers, from the single proton element hydrogen up to the 118-proton element oganesson. All known isotopes of elements with atomic numbers greater than 82 are radioactive, although the radioactivity of element 83 (bismuth) is so slight as to be practically negligible.\n\nAbout 339 nuclides occur naturally on Earth, of which 254 (about 75%) have not been observed to decay, and are referred to as \"stable isotopes\". However, only 90 of these nuclides are stable to all decay, even in theory. Another 164 (bringing the total to 254) have not been observed to decay, even though in theory it is energetically possible. These are also formally classified as \"stable\". An additional 34 radioactive nuclides have half-lives longer than 80 million years, and are long-lived enough to be present from the birth of the solar system. This collection of 288 nuclides are known as primordial nuclides. Finally, an additional 51 short-lived nuclides are known to occur naturally, as daughter products of primordial nuclide decay (such as radium from uranium), or else as products of natural energetic processes on Earth, such as cosmic ray bombardment (for example, carbon-14).\n\nFor 80 of the chemical elements, at least one stable isotope exists. As a rule, there is only a handful of stable isotopes for each of these elements, the average being 3.2 stable isotopes per element. Twenty-six elements have only a single stable isotope, while the largest number of stable isotopes observed for any element is ten, for the element tin. Elements 43, 61, and all elements numbered 83 or higher have no stable isotopes.\n\nStability of isotopes is affected by the ratio of protons to neutrons, and also by the presence of certain \"magic numbers\" of neutrons or protons that represent closed and filled quantum shells. These quantum shells correspond to a set of energy levels within the shell model of the nucleus; filled shells, such as the filled shell of 50 protons for tin, confers unusual stability on the nuclide. Of the 254 known stable nuclides, only four have both an odd number of protons \"and\" odd number of neutrons: hydrogen-2 (deuterium), lithium-6, boron-10 and nitrogen-14. Also, only four naturally occurring, radioactive odd–odd nuclides have a half-life over a billion years: potassium-40, vanadium-50, lanthanum-138 and tantalum-180m. Most odd–odd nuclei are highly unstable with respect to beta decay, because the decay products are even–even, and are therefore more strongly bound, due to nuclear pairing effects.\n\nThe large majority of an atom's mass comes from the protons and neutrons that make it up. The total number of these particles (called \"nucleons\") in a given atom is called the mass number. It is a positive integer and dimensionless (instead of having dimension of mass), because it expresses a count. An example of use of a mass number is \"carbon-12,\" which has 12 nucleons (six protons and six neutrons).\n\nThe actual mass of an atom at rest is often expressed using the unified atomic mass unit (u), also called dalton (Da). This unit is defined as a twelfth of the mass of a free neutral atom of carbon-12, which is approximately . Hydrogen-1 (the lightest isotope of hydrogen which is also the nuclide with the lowest mass) has an atomic weight of 1.007825 u. The value of this number is called the atomic mass. A given atom has an atomic mass approximately equal (within 1%) to its mass number times the atomic mass unit (for example the mass of a nitrogen-14 is roughly 14 u). However, this number will not be exactly an integer except in the case of carbon-12 (see below). The heaviest stable atom is lead-208, with a mass of .\n\nAs even the most massive atoms are far too light to work with directly, chemists instead use the unit of moles. One mole of atoms of any element always has the same number of atoms (about ). This number was chosen so that if an element has an atomic mass of 1 u, a mole of atoms of that element has a mass close to one gram. Because of the definition of the unified atomic mass unit, each carbon-12 atom has an atomic mass of exactly 12 u, and so a mole of carbon-12 atoms weighs exactly 0.012 kg.\n\nAtoms lack a well-defined outer boundary, so their dimensions are usually described in terms of an atomic radius. This is a measure of the distance out to which the electron cloud extends from the nucleus. However, this assumes the atom to exhibit a spherical shape, which is only obeyed for atoms in vacuum or free space. Atomic radii may be derived from the distances between two nuclei when the two atoms are joined in a chemical bond. The radius varies with the location of an atom on the atomic chart, the type of chemical bond, the number of neighboring atoms (coordination number) and a quantum mechanical property known as spin. On the periodic table of the elements, atom size tends to increase when moving down columns, but decrease when moving across rows (left to right). Consequently, the smallest atom is helium with a radius of 32 pm, while one of the largest is caesium at 225 pm.\n\nWhen subjected to external forces, like electrical fields, the shape of an atom may deviate from spherical symmetry. The deformation depends on the field magnitude and the orbital type of outer shell electrons, as shown by group-theoretical considerations. Aspherical deviations might be elicited for instance in crystals, where large crystal-electrical fields may occur at low-symmetry lattice sites. Significant ellipsoidal deformations have been shown to occur for sulfur ions and chalcogen ions in pyrite-type compounds.\n\nAtomic dimensions are thousands of times smaller than the wavelengths of light (400–700 nm) so they cannot be viewed using an optical microscope. However, individual atoms can be observed using a scanning tunneling microscope. To visualize the minuteness of the atom, consider that a typical human hair is about 1 million carbon atoms in width. A single drop of water contains about 2 sextillion () atoms of oxygen, and twice the number of hydrogen atoms. A single carat diamond with a mass of contains about 10 sextillion (10) atoms of carbon. If an apple were magnified to the size of the Earth, then the atoms in the apple would be approximately the size of the original apple.\n\nEvery element has one or more isotopes that have unstable nuclei that are subject to radioactive decay, causing the nucleus to emit particles or electromagnetic radiation. Radioactivity can occur when the radius of a nucleus is large compared with the radius of the strong force, which only acts over distances on the order of 1 fm.\n\nThe most common forms of radioactive decay are:\n\nOther more rare types of radioactive decay include ejection of neutrons or protons or clusters of nucleons from a nucleus, or more than one beta particle. An analog of gamma emission which allows excited nuclei to lose energy in a different way, is internal conversion—a process that produces high-speed electrons that are not beta rays, followed by production of high-energy photons that are not gamma rays. A few large nuclei explode into two or more charged fragments of varying masses plus several neutrons, in a decay called spontaneous nuclear fission.\n\nEach radioactive isotope has a characteristic decay time period—the half-life—that is determined by the amount of time needed for half of a sample to decay. This is an exponential decay process that steadily decreases the proportion of the remaining isotope by 50% every half-life. Hence after two half-lives have passed only 25% of the isotope is present, and so forth.\n\nElementary particles possess an intrinsic quantum mechanical property known as spin. This is analogous to the angular momentum of an object that is spinning around its center of mass, although strictly speaking these particles are believed to be point-like and cannot be said to be rotating. Spin is measured in units of the reduced Planck constant (ħ), with electrons, protons and neutrons all having spin ½ ħ, or \"spin-½\". In an atom, electrons in motion around the nucleus possess orbital angular momentum in addition to their spin, while the nucleus itself possesses angular momentum due to its nuclear spin.\n\nThe magnetic field produced by an atom—its magnetic moment—is determined by these various forms of angular momentum, just as a rotating charged object classically produces a magnetic field. However, the most dominant contribution comes from electron spin. Due to the nature of electrons to obey the Pauli exclusion principle, in which no two electrons may be found in the same quantum state, bound electrons pair up with each other, with one member of each pair in a spin up state and the other in the opposite, spin down state. Thus these spins cancel each other out, reducing the total magnetic dipole moment to zero in some atoms with even number of electrons.\n\nIn ferromagnetic elements such as iron, cobalt and nickel, an odd number of electrons leads to an unpaired electron and a net overall magnetic moment. The orbitals of neighboring atoms overlap and a lower energy state is achieved when the spins of unpaired electrons are aligned with each other, a spontaneous process known as an exchange interaction. When the magnetic moments of ferromagnetic atoms are lined up, the material can produce a measurable macroscopic field. Paramagnetic materials have atoms with magnetic moments that line up in random directions when no magnetic field is present, but the magnetic moments of the individual atoms line up in the presence of a field.\n\nThe nucleus of an atom will have no spin when it has even numbers of both neutrons and protons, but for other cases of odd numbers, the nucleus may have a spin. Normally nuclei with spin are aligned in random directions because of thermal equilibrium. However, for certain elements (such as xenon-129) it is possible to polarize a significant proportion of the nuclear spin states so that they are aligned in the same direction—a condition called hyperpolarization. This has important applications in magnetic resonance imaging.\n\nThe potential energy of an electron in an atom is negative, its dependence of its position reaches the minimum (the most absolute value) inside the nucleus, and vanishes when the distance from the nucleus goes to infinity, roughly in an inverse proportion to the distance. In the quantum-mechanical model, a bound electron can only occupy a set of states centered on the nucleus, and each state corresponds to a specific energy level; see time-independent Schrödinger equation for theoretical explanation. An energy level can be measured by the amount of energy needed to unbind the electron from the atom, and is usually given in units of electronvolts (eV). The lowest energy state of a bound electron is called the ground state, i.e. stationary state, while an electron transition to a higher level results in an excited state. The electron's energy raises when \"n\" increases because the (average) distance to the nucleus increases. Dependence of the energy on is caused not by electrostatic potential of the nucleus, but by interaction between electrons.\n\nFor an electron to transition between two different states, e.g. grounded state to first excited level (ionization), it must absorb or emit a photon at an energy matching the difference in the potential energy of those levels, according to Niels Bohr model, what can be precisely calculated by the Schrödinger equation.\nElectrons jump between orbitals in a particle-like fashion. For example, if a single photon strikes the electrons, only a single electron changes states in response to the photon; see Electron properties.\n\nThe energy of an emitted photon is proportional to its frequency, so these specific energy levels appear as distinct bands in the electromagnetic spectrum. Each element has a characteristic spectrum that can depend on the nuclear charge, subshells filled by electrons, the electromagnetic interactions between the electrons and other factors.\n\nWhen a continuous spectrum of energy is passed through a gas or plasma, some of the photons are absorbed by atoms, causing electrons to change their energy level. Those excited electrons that remain bound to their atom spontaneously emit this energy as a photon, traveling in a random direction, and so drop back to lower energy levels. Thus the atoms behave like a filter that forms a series of dark absorption bands in the energy output. (An observer viewing the atoms from a view that does not include the continuous spectrum in the background, instead sees a series of emission lines from the photons emitted by the atoms.) Spectroscopic measurements of the strength and width of atomic spectral lines allow the composition and physical properties of a substance to be determined.\n\nClose examination of the spectral lines reveals that some display a fine structure splitting. This occurs because of spin–orbit coupling, which is an interaction between the spin and motion of the outermost electron. When an atom is in an external magnetic field, spectral lines become split into three or more components; a phenomenon called the Zeeman effect. This is caused by the interaction of the magnetic field with the magnetic moment of the atom and its electrons. Some atoms can have multiple electron configurations with the same energy level, which thus appear as a single spectral line. The interaction of the magnetic field with the atom shifts these electron configurations to slightly different energy levels, resulting in multiple spectral lines. The presence of an external electric field can cause a comparable splitting and shifting of spectral lines by modifying the electron energy levels, a phenomenon called the Stark effect.\n\nIf a bound electron is in an excited state, an interacting photon with the proper energy can cause stimulated emission of a photon with a matching energy level. For this to occur, the electron must drop to a lower energy state that has an energy difference matching the energy of the interacting photon. The emitted photon and the interacting photon then move off in parallel and with matching phases. That is, the wave patterns of the two photons are synchronized. This physical property is used to make lasers, which can emit a coherent beam of light energy in a narrow frequency band.\n\nValency is the combining power of an element. It is equal to number of hydrogen atoms that atom can combine or displace in forming compounds. The outermost electron shell of an atom in its uncombined state is known as the valence shell, and the electrons in\nthat shell are called valence electrons. The number of valence electrons determines the bonding\nbehavior with other atoms. Atoms tend to chemically react with each other in a manner that fills (or empties) their outer valence shells. For example, a transfer of a single electron between atoms is a useful approximation for bonds that form between atoms with one-electron more than a filled shell, and others that are one-electron short of a full shell, such as occurs in the compound sodium chloride and other chemical ionic salts. However, many elements display multiple valences, or tendencies to share differing numbers of electrons in different compounds. Thus, chemical bonding between these elements takes many forms of electron-sharing that are more than simple electron transfers. Examples include the element carbon and the organic compounds.\n\nThe chemical elements are often displayed in a periodic table that is laid out to display recurring chemical properties, and elements with the same number of valence electrons form a group that is aligned in the same column of the table. (The horizontal rows correspond to the filling of a quantum shell of electrons.) The elements at the far right of the table have their outer shell completely filled with electrons, which results in chemically inert elements known as the noble gases.\n\nQuantities of atoms are found in different states of matter that depend on the physical conditions, such as temperature and pressure. By varying the conditions, materials can transition between solids, liquids, gases and plasmas. Within a state, a material can also exist in different allotropes. An example of this is solid carbon, which can exist as graphite or diamond. Gaseous allotropes exist as well, such as dioxygen and ozone.\n\nAt temperatures close to absolute zero, atoms can form a Bose–Einstein condensate, at which point quantum mechanical effects, which are normally only observed at the atomic scale, become apparent on a macroscopic scale. This super-cooled collection of atoms\nthen behaves as a single super atom, which may allow fundamental checks of quantum mechanical behavior.\n\nThe scanning tunneling microscope is a device for viewing surfaces at the atomic level. It uses the quantum tunneling phenomenon, which allows particles to pass through a barrier that would normally be insurmountable. Electrons tunnel through the vacuum between two planar metal electrodes, on each of which is an adsorbed atom, providing a tunneling-current density that can be measured. Scanning one atom (taken as the tip) as it moves past the other (the sample) permits plotting of tip displacement versus lateral separation for a constant current. The calculation shows the extent to which scanning-tunneling-microscope images of an individual atom are visible. It confirms that for low bias, the microscope images the space-averaged dimensions of the electron orbitals across closely packed energy levels—the Fermi level local density of states.\n\nAn atom can be ionized by removing one of its electrons. The electric charge causes the trajectory of an atom to bend when it passes through a magnetic field. The radius by which the trajectory of a moving ion is turned by the magnetic field is determined by the mass of the atom. The mass spectrometer uses this principle to measure the mass-to-charge ratio of ions. If a sample contains multiple isotopes, the mass spectrometer can determine the proportion of each isotope in the sample by measuring the intensity of the different beams of ions. Techniques to vaporize atoms include inductively coupled plasma atomic emission spectroscopy and inductively coupled plasma mass spectrometry, both of which use a plasma to vaporize samples for analysis.\n\nA more area-selective method is electron energy loss spectroscopy, which measures the energy loss of an electron beam within a transmission electron microscope when it interacts with a portion of a sample. The atom-probe tomograph has sub-nanometer resolution in 3-D and can chemically identify individual atoms using time-of-flight mass spectrometry.\n\nSpectra of excited states can be used to analyze the atomic composition of distant stars. Specific light wavelengths contained in the observed light from stars can be separated out and related to the quantized transitions in free gas atoms. These colors can be replicated using a gas-discharge lamp containing the same element. Helium was discovered in this way in the spectrum of the Sun 23 years before it was found on Earth.\n\nAtoms form about 4% of the total energy density of the observable Universe, with an average density of about 0.25 atoms/m. Within a galaxy such as the Milky Way, atoms have a much higher concentration, with the density of matter in the interstellar medium (ISM) ranging from 10 to 10 atoms/m. The Sun is believed to be inside the Local Bubble, a region of highly ionized gas, so the density in the solar neighborhood is only about 10 atoms/m. Stars form from dense clouds in the ISM, and the evolutionary processes of stars result in the steady enrichment of the ISM with elements more massive than hydrogen and helium. Up to 95% of the Milky Way's atoms are concentrated inside stars and the total mass of atoms forms about 10% of the mass of the galaxy. (The remainder of the mass is an unknown dark matter.)\n\nElectrons are thought to exist in the Universe since early stages of the Big Bang. Atomic nuclei forms in nucleosynthesis reactions. In about three minutes Big Bang nucleosynthesis produced most of the helium, lithium, and deuterium in the Universe, and perhaps some of the beryllium and boron.\n\nUbiquitousness and stability of atoms relies on their binding energy, which means that an atom has a lower energy than an unbound system of the nucleus and electrons. Where the temperature is much higher than ionization potential, the matter exists in the form of plasma—a gas of positively charged ions (possibly, bare nuclei) and electrons. When the temperature drops below the ionization potential, atoms become statistically favorable. Atoms (complete with bound electrons) became to dominate over charged particles 380,000 years after the Big Bang—an epoch called recombination, when the expanding Universe cooled enough to allow electrons to become attached to nuclei.\n\nSince the Big Bang, which produced no carbon or heavier elements, atomic nuclei have been combined in stars through the process of nuclear fusion to produce more of the element helium, and (via the triple alpha process) the sequence of elements from carbon up to iron; see stellar nucleosynthesis for details.\n\nIsotopes such as lithium-6, as well as some beryllium and boron are generated in space through cosmic ray spallation. This occurs when a high-energy proton strikes an atomic nucleus, causing large numbers of nucleons to be ejected.\n\nElements heavier than iron were produced in supernovae through the r-process and in AGB stars through the s-process, both of which involve the capture of neutrons by atomic nuclei. Elements such as lead formed largely through the radioactive decay of heavier elements.\n\nMost of the atoms that make up the Earth and its inhabitants were present in their current form in the nebula that collapsed out of a molecular cloud to form the Solar System. The rest are the result of radioactive decay, and their relative proportion can be used to determine the age of the Earth through radiometric dating. Most of the helium in the crust of the Earth (about 99% of the helium from gas wells, as shown by its lower abundance of helium-3) is a product of alpha decay.\n\nThere are a few trace atoms on Earth that were not present at the beginning (i.e., not \"primordial\"), nor are results of radioactive decay. Carbon-14 is continuously generated by cosmic rays in the atmosphere. Some atoms on Earth have been artificially generated either deliberately or as by-products of nuclear reactors or explosions. Of the transuranic elements—those with atomic numbers greater than 92—only plutonium and neptunium occur naturally on Earth. Transuranic elements have radioactive lifetimes shorter than the current age of the Earth and thus identifiable quantities of these elements have long since decayed, with the exception of traces of plutonium-244 possibly deposited by cosmic dust. Natural deposits of plutonium and neptunium are produced by neutron capture in uranium ore.\n\nThe Earth contains approximately atoms. Although small numbers of independent atoms of noble gases exist, such as argon, neon, and helium, 99% of the atmosphere is bound in the form of molecules, including carbon dioxide and diatomic oxygen and nitrogen. At the surface of the Earth, an overwhelming majority of atoms combine to form various compounds, including water, salt, silicates and oxides. Atoms can also combine to create materials that do not consist of discrete molecules, including crystals and liquid or solid metals. This atomic matter forms networked arrangements that lack the particular type of small-scale interrupted order associated with molecular matter.\n\nWhile isotopes with atomic numbers higher than lead (82) are known to be radioactive, an \"island of stability\" has been proposed for some elements with atomic numbers above 103. These superheavy elements may have a nucleus that is relatively stable against radioactive decay. The most likely candidate for a stable superheavy atom, unbihexium, has 126 protons and 184 neutrons.\n\nEach particle of matter has a corresponding antimatter particle with the opposite electrical charge. Thus, the positron is a positively charged antielectron and the antiproton is a negatively charged equivalent of a proton. When a matter and corresponding antimatter particle meet, they annihilate each other. Because of this, along with an imbalance between the number of matter and antimatter particles, the latter are rare in the universe. The first causes of this imbalance are not yet fully understood, although theories of baryogenesis may offer an explanation. As a result, no antimatter atoms have been discovered in nature. However, in 1996 the antimatter counterpart of the hydrogen atom (antihydrogen) was synthesized at the CERN laboratory in Geneva.\n\nOther exotic atoms have been created by replacing one of the protons, neutrons or electrons with other particles that have the same charge. For example, an electron can be replaced by a more massive muon, forming a muonic atom. These types of atoms can be used to test the fundamental predictions of physics.\n\n"}
{"id": "36231456", "url": "https://en.wikipedia.org/wiki?curid=36231456", "title": "Beni Abbes Museum", "text": "Beni Abbes Museum\n\nBeni Abbes Museum also known as (Béni Abbès Museum) () is an art museum located in oasis town of Beni Abbes, Béchar Province, Algeria. It is \"a resource dedicated to desert fauna, fossils and Algerian arts and crafts.\"\n\nBéni Abbès Museum is very popular in Algeria. It is supported by the Saharan Research Center. The museum houses displays desert fauna, fossils and traditional arts and crafts, such as carpets, wall hangings, ceramic items, woodcarvings and jewelry. The museum, which is supported by the Saharan Research Center, displays an extensive range of different types of dates. Dates are one of the mainstay crops of oasis towns and villages in Algeria.\n\nThe oasis of Beni Abbes has its own myth. Local people claim that:\n\nThe town of Beni Abbès is a popular tourist destination and the museum provides insights into its history and culture.\n\n\n"}
{"id": "13294262", "url": "https://en.wikipedia.org/wiki?curid=13294262", "title": "Climate change in the Arctic", "text": "Climate change in the Arctic\n\nThe effects of global warming in the Arctic, or climate change in the Arctic include rising air and water temperatures, loss of sea ice, and melting of the Greenland ice sheet with a related cold temperature anomaly, observed since the 1970s. Related impacts include ocean circulation changes, increased input of freshwater, and ocean acidification. Indirect effects through potential climate teleconnections to mid latitudes may result in a greater frequency of extreme weather events (flooding, fires and drought), ecological, biological and phenology changes, biological migrations and extinctions, natural resource stresses and as well as human health, displacement and security issues. Potential methane releases from the region, especially through the thawing of permafrost and methane clathrates, may occur. Presently, the Arctic is warming twice as fast compared to the rest of the world. The pronounced warming signal, the amplified response of the Arctic to global warming, is often seen as a leading indicator of global warming. The melting of Greenland's ice sheet is linked to polar amplification. According to a study published in 2016, about 0.5◦C of the warming in the Arctic has been attributed to reductions in sulfate aerosols in Europe since 1980.\n\nAccording to the Intergovernmental Panel on Climate Change, \"warming in the Arctic, as indicated by daily maximum and minimum temperatures, has been as great as in any other part of the world.\" The period of 1995–2005 was the warmest decade in the Arctic since at least the 17th century, with temperatures above the 1951–1990 average. Some regions within the Arctic have warmed even more rapidly, with Alaska and western Canada's temperature rising by . This warming has been caused not only by the rise in greenhouse gas concentration, but also the deposition of soot on Arctic ice. A 2013 article published in Geophysical Research Letters has shown that temperatures in the region haven't been as high as they currently are since at least 44,000 years ago and perhaps as long as 120,000 years ago. The authors conclude that \"anthropogenic increases in greenhouse gases have led to unprecedented regional warmth.\"\n\nThe poles of the Earth are more sensitive to any change in the planet's climate than the rest of the planet. In the face of ongoing global warming, the poles are warming faster than lower latitudes. The primary cause of this phenomenon is ice-albedo feedback, whereby melting ice uncovers darker land or ocean beneath, which then absorbs more sunlight, causing more heating. The loss of the Arctic sea ice may represent a tipping point in global warming, when 'runaway' climate change starts, but on this point the science is not yet settled. According to a 2015 study, based on computer modelling of aerosols in the atmosphere, up to 0.5 degrees Celsius of the warming observed in the Arctic between 1980 and 2005 is due to aerosol reductions in Europe.\n\nBlack carbon deposits (from the exhaust system of marine engines that often run on bunker fuel) reduce the albedo when deposited on snow and ice, and thus accelerate the effect of the melting of snow and sea ice.\n\nAccording to a 2015 study, reductions in black carbon emissions and other minor greenhouse gases, by roughly 60 percent, could cool the Arctic up to 0.2 °C by 2050.\n\nSea ice is currently in decline in area, extent, and volume and may cease to exist sometime during the 21st century. Sea ice area refers to the total area covered by ice, whereas sea ice extent is the area of ocean with at least 15% sea ice, while the volume is the total amount of ice in the Arctic.\n\nReliable measurement of sea ice edges began with the satellite era in the late 1970s. Before this time, sea ice area and extent were monitored less precisely by a combination of ships, buoys and aircraft. The data show a long-term negative trend in recent years, attributed to global warming, although there is also a considerable amount of variation from year to year. Some of this variation may be related to effects such as the Arctic oscillation, which may itself be related to global warming.\n\nThe Arctic sea ice September minimum extent (i.e., area with at least 15% sea ice coverage) reached new record lows in 2002, 2005, 2007, and 2012. The 2007 melt season let to a minimum 39% below the 1979–2000 average, and for the first time in human memory, the fabled Northwest Passage opened completely. The dramatic 2007 melting surprised and concerned scientists.\n\nFrom 2008 to 2011, Arctic sea ice minimum extent was higher than 2007, but it did not return to the levels of previous years. In 2012 however, the 2007 record low was broken in late August with three weeks still left in the melt season. It continued to fall, bottoming out on 16 September 2012 at 3.41 million square kilometers (1.32 million square miles), or 760,000 square kilometers (293,000 square miles) below the previous low set on 18 September 2007 and 50% below the 1979–2000 average.\n\nThe rate of the decline in entire Arctic ice coverage is accelerating. From 1979–1996, the average per decade decline in entire ice coverage was a 2.2% decline in ice extent and a 3% decline in ice area. For the decade ending 2008, these values have risen to 10.1% and 10.7%, respectively. These are comparable to the September to September loss rates in year-round ice (i.e., perennial ice, which survives throughout the year), which averaged a retreat of 10.2% and 11.4% per decade, respectively, for the period 1979–2007.\n\nThe sea ice thickness field and accordingly the ice volume and mass, is much more difficult to determine than the extension. Exact measurements can be made only at a limited number of points. Because of large variations in ice and snow thickness and consistency air- and spaceborne-measurements have to be evaluated carefully. Nevertheless, the studies made support the assumption of a dramatic decline in ice age and thickness. While the Arctic ice area and extent show an accelerating downward trend, arctic ice volume shows an even sharper decline than the ice coverage. Since 1979, the ice volume has shrunk by 80% and in just the past decade the volume declined by 36% in the autumn and 9% in the winter.\n\nThe IPCC's Fourth Assessment Report in 2007 summarized the current state of sea ice projections: \"the projected reduction <nowiki>[in global sea ice cover]</nowiki> is accelerated in the Arctic, where some models project summer sea ice cover to disappear entirely in the high-emission A2 scenario in the latter part of the 21st century.″ However, current climate models frequently underestimate the rate of sea ice retreat. A summertime ice-free Arctic would be unprecedented in recent geologic history, as currently scientific evidence does not indicate an ice-free polar sea anytime in the last 700,000 years.\n\nThe Arctic ocean will likely be free of summer sea ice before the year 2100, but many different dates have been projected. One study suggests 2060–2080 and another suggests 2030. A 2013 study showed that simply extending summertime ice melting trends into the future in a straight line predicts an ice-free summertime Arctic as early as by 2020.\n\nThis century, thawing of the various types of Arctic permafrost could release large amounts of carbon into the atmosphere. It has been estimated that about two-thirds of released carbon escapes to the atmosphere as carbon dioxide, originating primarily from ancient ice deposits along the ~7,000 kilometer long coastline of the East Siberian Arctic Shelf (ESAS) and shallow subsea permafrost. Following thaw, collapse and erosion of coastline and seafloor deposits may accelerate with Arctic amplification of climate warming.\n\nClimate models suggest that during periods of rapid sea-ice loss, temperatures could increase as far as inland, accelerating the rate of terrestrial permafrost thaw, with consequential effects on carbon and methane release.\n\nAs of 2018, modeling of the permafrost carbon feedback has focused on gradual surface thawing, models have yet to account for deeper soil layers. A new study used field observations, radiocarbon dating, and remote sensing to account for thermokarst lakes, the authors concluded that, \"..methane and carbon dioxide emissions from abrupt thaw beneath thermokarst lakes will more than double radiative forcing from circumpolar permafrost-soil carbon fluxes this century.\"\n\nSubsea permafrost occurs beneath the seabed and exists in the continental shelves of the polar regions. This source of methane is different from methane clathrates, but contributes to the overall outcome and feedbacks.\n\nSea ice serves to stabilise methane deposits on and near the shoreline, preventing the clathrate breaking down and venting into the water column and eventually reaching the atmosphere. From sonar measurements in recent years researchers quantified the density of bubbles emanating from the subsea permafrost into the Ocean (a process called ebullition), and found that 100–630 mg methane per square meters is emitted daily along the East Siberian Shelf, into the water column. They also found that during storms, methane levels in the water column drop dramatically, when wind-driven air-sea gas exchange accelerates the ebullition process into the atmosphere. This observed pathway suggests that methane from seabed permafrost will progress rather slowly, instead of abrupt changes. However, Arctic cyclones, fueled by global warming and further accumulation of greenhouse gases in the atmosphere could contribute to more release from this methane cache, which is really important for the Arctic. An update to the mechanisms of this permafrost degradation, implying the possibility of being close to an acceleration of methane release was published in 2017.\n\nChanges in vegetation are associated with the increases in landscape scale methane emissions.\n\nThe growing season has lengthened in the far northern latitudes, bringing major changes to plant communities in tundra and boreal (also known as taiga) ecosystems.\n\nFor decades, NASA and NOAA satellites have continuously monitored vegetation from space. The Moderate Resolution Imaging Spectroradiometer (MODIS) and Advanced Very High-Resolution Radiometer (AVHRR) instruments measure the intensity of visible and near-infrared light reflecting off of plant leaves. Scientists use the information to calculate the Normalized Difference Vegetation Index (NDVI), an indicator of photosynthetic activity or “greenness” of the landscape.\n\nThe maps above show the Arctic Vegetation Index Trend between July 1982 and December 2011 in the Arctic Circle. Shades of green depict areas where plant productivity and abundance increased; shades of brown show where photosynthetic activity declined. The maps show a ring of greening in the treeless tundra ecosystems of the circumpolar Arctic—the northernmost parts of Canada, Russia, and Scandinavia. Tall shrubs and trees started to grow in areas that were previously dominated by tundra grasses. The researchers concluded that plant growth had increased by 7 to 10 percent overall.\n\nHowever, boreal forests, particularly those in North America, showed a different response to warming. Many boreal forests greened, but the trend was not as strong as it was for tundra of the circumpolar Arctic. In North America, some boreal forests actually experienced “browning” (less photosynthetic activity) over the study period. Droughts, forest fire activity, animal and insect behavior, industrial pollution, and a number of other factors may have contributed to the browning.\n\n\"Satellite data identify areas in the boreal zone that are warmer and drier and other areas that are warmer and wetter,\" explained co-author Ramakrishna Nemani of NASA’s Ames Research Center. \"Only the warmer and wetter areas support more growth.\"\n\n\"We found more plant growth in the boreal zone from 1982 to 1992 than from 1992 to 2011, because water limitations were encountered in the later two decades of our study,\" added co-author Sangram Ganguly of the Bay Area Environmental Research Institute and NASA Ames.\n\nThe less severe winters in tundra areas allow shrubs such as alders and dwarf birch to replace moss and lichens. The impact on mosses and lichens is unclear as there exist very few studies at species level, also climate change is more likely to cause increased fluctuation and more frequent extreme events. The feedback effect of shrubs on the tundra's permafrost is unclear. In the winter they trap more snow which insulates the permafrost from extreme cold spells, but in the summer they shade the ground from direct sunlight. The warming is likely to cause changes in the plant communities. Except for an increase in shurbs, warming may also cause a decline in cushion plants such as moss campion. Since cushion plants act as facilitator species across trophic level and fill important roles in severe environments this could cause cascading effects in the ecosystems. Rising summer temperature melts on Canada's Baffin Island have revealed moss previously covered which has not seen daylight in 44,000 years.\n\nThe reduction of sea ice has boosted the productivity of phytoplankton by about twenty percent over the past thirty years. However, the effect on marine ecosystems is unclear, since the larger types of phytoplankton, which are the preferred food source of most zooplankton, do not appear to have increased as much as the smaller types. So far, Arctic phytoplankton have not had a significant impact on the global carbon cycle. In summer, the melt ponds on young and thin ice have allowed sunlight to penetrate the ice, in turn allowing phytoplankton to bloom in unexpected concentrations, although it is unknown just how long this phenomenon has been occurring.\n\nThe northward shift of the subarctic climate zone is allowing animals that are adapted to that climate to move into the far north, where they are replacing species that are more adapted to a pure Arctic climate. Where the Arctic species are not being replaced outright, they are often interbreeding with their southern relations. Among slow-breeding vertebrate species, this often has the effect of reducing the genetic diversity of the genus. Another concern is the spread of infectious diseases, such as brucellosis or phocine distemper virus, to previously untouched populations. This is a particular danger among marine mammals who were previously segregated by sea ice.\n\n3 April 2007, the National Wildlife Federation urged the United States Congress to place polar bears under the Endangered Species Act.\nFour months later, the United States Geological Survey completed a year-long study which concluded in part that the floating Arctic sea ice will continue its rapid shrinkage over the next 50 years, consequently wiping out much of the polar bear habitat. The bears would disappear from Alaska, but would continue to exist in the Canadian Arctic Archipelago and areas off the northern Greenland coast. Secondary ecological effects are also resultant from the shrinkage of sea ice; for example, polar bears are denied their historic length of seal hunting season due to late formation and early thaw of pack ice.\n\nModels predict a sea-level contribution of about from melting in Greenland during the 21st century. It is also predicted that Greenland will become warm enough by 2100 to begin an almost complete melt during the next 1,000 years or more. In early July 2012, 97% percent of the Ice Sheet experienced some form of surface melt including the summits.\n\nIce thickness measurements from the GRACE satellite indicate that ice mass loss is accelerating. For the period 2002–2009, the rate of loss increased from −137 Gt/yr to −286 Gt/yr, with an acceleration of −30 gigatonnes per year per year.\n\nAlthough this is now thought unlikely in the near future, it has also been suggested that there could be a shutdown of thermohaline circulation, similar to that which is believed to have driven the Younger Dryas, an abrupt climate change event. There is also potentially a possibility of a more general disruption of ocean circulation, which may lead to an ocean anoxic event, although these are believed to be much more common in the distant past. It is unclear whether the appropriate pre-conditions for such an event exist today.\n\nGrowing evidence that global warming is shrinking polar ice has added to the urgency of several nations' Arctic territorial claims in hopes of establishing resource development and new shipping lanes, in addition to protecting sovereign rights.\n\nDanish Foreign Minister Per Stig Møller and Greenland's Premier Hans Enoksen invited foreign ministers from Canada, Norway, Russia and the United States to Ilulissat, Greenland for a summit in May 2008 to discuss how to divide borders in the changing Arctic region, and a discussion on more cooperation against climate change affecting the Arctic. At the Arctic Ocean Conference, Foreign Ministers and other officials representing the five countries announced the Ilulissat Declaration on 28 May 2008.\n\nPeople are affecting the geographic space of the Arctic and the Arctic is affecting the population. Much of the climate change in the Arctic can be attributed to humans influences on the atmosphere, such as an increased greenhouse effect caused by the increase in due to the burning of fossil fuels. Climate change is having a direct impact on the people that live in the Arctic, as well as other societies around the world.\n\nThe warming environment presents challenges to local communities such as the Inuit. Hunting, which is a major way of survival for some small communities, will be changed with increasing temperatures. The reduction of sea ice will cause certain species populations to decline or even become extinct. In good years, some communities are fully employed by the commercial harvest of certain animals. The harvest of different animals fluctuates each year and with the rise of temperatures it is likely to continue changing and creating issues for Inuit hunters. Unsuspected changes in river and snow conditions will cause herds of animals, including reindeer, to change migration patterns, calving grounds, and forage availability.\n\nOther forms of transportation in the Arctic have seen negative impacts from the current warming, with some transportation routes and pipelines on land being disrupted by the melting of ice. Many Arctic communities rely on frozen roadways to transport supplies and travel from area to area. The changing landscape and unpredictability of weather is creating new challenges in the Arctic.\n\nThe Transpolar Sea Route is a future Arctic shipping lane running from the Atlantic Ocean to the Pacific Ocean across the center of the Arctic Ocean. The route is also sometimes called Trans-Arctic Route. In contrast to the Northeast Passage (including the Northern Sea Route) and the North-West Passage it largely avoids the territorial waters of Arctic states and lies in international high seas.\n\nGovernments and private industry have shown a growing interest in the Arctic. Major new shipping lanes are opening up: the northern sea route had 34 passages in 2011 while the Northwest Passage had 22 traverses, more than any time in history. Shipping companies may benefit from the shortened distance of these northern routes. Access to natural resources will increase, including valuable minerals and offshore oil and gas. Finding and controlling these resources will be difficult with the continually moving ice. Tourism may also increase as less sea ice will improve safety and accessibility to the Arctic.\n\nThe melting of Arctic ice caps is likely to increase traffic in and the commercial viability of the Northern Sea Route. One study, for instance, projects, \"remarkable shifts in trade flows between Asia and Europe, diversion of trade within Europe, heavy shipping traffic in the Arctic and a substantial drop in Suez traffic. Projected shifts in trade also imply substantial pressure on an already threatened Arctic ecosystem.\"\n\nIndividual countries within the Arctic zone, Canada, Denmark (Greenland), Finland, Iceland, Norway, Russia, Sweden, and the United States (Alaska) conduct independent research through a variety of organizations and agencies, public and private, such as Russia's Arctic and Antarctic Research Institute. Countries who do not have Arctic claims, but are close neighbors, conduct Arctic research as well, such as the Chinese Arctic and Antarctic Administration (CAA). The United States's National Oceanic and Atmospheric Administration (NOAA) produces an Arctic Report Card annually, containing peer-reviewed information on recent observations of environmental conditions in the Arctic relative to historical records.\n\nInternational cooperative research between nations has become increasingly important:\n\n\n\n"}
{"id": "381774", "url": "https://en.wikipedia.org/wiki?curid=381774", "title": "Compressed air", "text": "Compressed air\n\nCompressed air is air kept under a pressure that is greater than atmospheric pressure. Compressed air is an important medium for transfer of energy in industrial processes. Compressed air is used for power tools such as air hammers, drills, wrenches and others. Compressed air is used to atomize paint, to operate air cylinders for automation, and can also be used to propel vehicles. Brakes applied by compressed air made large railway trains safer and more efficient to operate. Compressed air brakes are also found on large highway vehicles.\n\nCompressed air is used as a breathing gas by underwater divers. It may be carried by the diver in a high pressure diving cylinder, or supplied from the surface at lower pressure through an air line or diver's umbilical. Similar arrangements are used in breathing apparatus used by firefighters, mine rescue workers and industrial workers in hazardous atmospheres.\n\nIn Europe, 10 percent of all industrial electricity consumption is to produce compressed air—amounting to 80 terawatt hours consumption per year.\n\nIndustrial use of piped compressed air for power transmission was developed in the mid 19th century; unlike steam, compressed air could be piped for long distances without losing pressure due to condensation. An early major application of compressed air was in the drilling of the Mont Cenis Tunnel in Switzerland in 1861, where a 600 kPa (87 psi) compressed air plant provided power to pneumatic drills, increasing productivity greatly over previous manual drilling methods. Compressed air drills were applied at mines in the United States in the 1870s. George Westinghouse invented air brakes for trains starting in 1869; these brakes considerably improved the safety of rail operations. In the 19th century, Paris had a system of pipes installed for municipal distribution of compressed air to power machines and to operate generators for lighting. Early air compressors were steam-driven, but in certain locations a trompe could directly obtain compressed air from the force of falling water.\n\nAir for breathing may be stored at high pressure and gradually released when needed, as in scuba diving. Air for breathing must be free of oil and other contaminants; carbon monoxide, for example, in trace amounts that might not be dangerous at normal atmospheric pressure may have deadly effects when breathing pressurized air. Air compressors and supply systems intended for breathing air are not generally also used for pneumatic tools or other purposes.\n\nWorkers constructing the foundations of bridges or other structures may be working in a pressurized enclosure called a caisson, where water is prevented from entering the open bottom of the enclosure by filling it with air under pressure. It was known as early as the 17th century that workers in diving bells experienced shortness of breath and risked asphyxia, relieved by the release of fresh air into the bell. Such workers also experienced pain and other symptoms when returning to the surface, as the pressure was relieved. Denis Papin suggested in 1691 that the working time in a diving bell could be extended if fresh air from the surface was continually forced under pressure into the bell. By the 19th century, caissons were regularly used in civil construction, but workers experienced serious, sometimes fatal, symptoms on returning to the surface, a syndrome called caisson disease or decompression sickness. Many workers were killed by the disease on projects such as the Brooklyn Bridge and the Eads Bridge and it was not until the 1890s that it was understood that workers had to decompress slowly, to prevent the formation of dangerous bubbles in tissues.\n\nAir under moderately high pressure, such as is used when diving below about , has an increasing narcotic effect on the nervous system. Nitrogen narcosis is a hazard when diving. For diving much beyond , it is less safe to use air alone and special breathing mixes containing helium are often used.\n\nIn industry, compressed air is so widely used that it is often regarded as the fourth utility, after electricity, natural gas and water. However, compressed air is more expensive than the other three utilities when evaluated on a per unit energy delivered basis.\n\nCompressed air is used for many purposes, including:\n\nCompressor rooms must be designed with ventilation systems to remove waste heat produced by the compressors.\n\nWhen air at atmospheric pressure is compressed, it contains much more water vapor than the high-pressure air can hold. Relative humidity is governed by the properties of water and is not affected by air pressure. After compressed air cools, then the vaporized water turns to liquefied water., Management of the excessive moisture is a requirement of a compressed air distribution system. System designers must ensure that piping maintains a slope, to prevent accumulation of moisture in low parts of the piping system. Drain valves may be installed at multiple points of a large system to allow trapped water to be blown out. Taps from piping headers may be arranged at the tops of pipes, so that moisture is not carried over into piping branches feeding equipment.\n\n"}
{"id": "285246", "url": "https://en.wikipedia.org/wiki?curid=285246", "title": "Dunite", "text": "Dunite\n\nDunite ( or ) (also known as olivinite, not to be confused with the mineral olivenite) is an igneous, plutonic rock, of ultramafic composition, with coarse-grained or phaneritic texture. The mineral assemblage is greater than 90% olivine, with minor amounts of other minerals such as pyroxene, chromite, magnetite, and pyrope. Dunite is the olivine-rich end-member of the peridotite group of mantle-derived rocks. Dunite and other peridotite rocks are considered the major constituents of the Earth's mantle above a depth of about 400 kilometers. Dunite is rarely found within continental rocks, but where it is found, it typically occurs at the base of ophiolite sequences where slabs of mantle rock from a subduction zone have been thrust onto continental crust by obduction during continental or island arc collisions (orogeny). It is also found in alpine peridotite massifs that represent slivers of sub-continental mantle exposed during collisional orogeny. Dunite typically undergoes retrograde metamorphism in near-surface environments and is altered to serpentinite and soapstone.\nThe type of dunite found in the lowermost parts of ophiolites, alpine peridotite massifs, and xenoliths may represent the refractory residue left after the extraction of basaltic magmas in the upper mantle. However, a more likely method of dunite formation in mantle sections is by interaction between lherzolite or harzburgite and percolating silicate melts, which dissolve orthopyroxene from the surrounding rock, leaving a progressively olivine-enriched residue. Dunite may also form by the accumulation of olivine crystals on the floor of large basaltic or picritic magma chambers. These \"cumulate\" dunites typically occur in thick layers in layered intrusions, associated with cumulate layers of wehrlite, olivine pyroxenite, harzburgite, and even chromitite (a cumulate rock consisting largely of chromite). Small layered intrusions may be of any geologic age, for example, the Triassic Palisades Sill in New York and the larger Eocene Skaergaard complex in Greenland. The largest layered mafic intrusions are tens of kilometers in size and almost all are Proterozoic in age, e.g., the Stillwater igneous complex (Montana), the Muskox intrusion (Canada), and the Great Dyke (Zimbabwe). Cumulate dunite may also be found in ophiolite complexes, associated with layers of wehrlite, pyroxenite, and gabbro.\n\nDunite was named by the German geologist Ferdinand von Hochstetter in 1859, after Dun Mountain near Nelson, New Zealand. Dun Mountain was given its name because of the dun colour of the underlying ultramafic rocks. This color results from surface weathering that oxidizes the iron in olivine in temperate climates (weathering in tropical climates creates a deep red soil). The Dunite from Dun Mountain is part of the ultramfic section of the Dun Mountain Ophiolite Belt. \n\nA massive exposure of dunite in the United States can be found as Twin Sisters Mountain, near Mount Baker in the northern Cascade Range of Washington. In Europe it occurs in the Troodos mountains of Cyprus. In southern British Columbia, Canada dunite rocks form the core of an ultramafic rock complex located near the small community of Tulameen. The rocks are locally enriched in platinum group metals, chromite and magnetite.\n\nDunite could be used to sequester CO and help mitigate global climate change via accelerated chemical rock weathering. This would involve the mining of dunite rocks in quarries followed by crushing and grinding as to create fine ground rock that would react with the atmospheric carbon dioxide. The resulting products are magnesite and silica which could be commercialized.\n\n"}
{"id": "2719436", "url": "https://en.wikipedia.org/wiki?curid=2719436", "title": "Dybbøl", "text": "Dybbøl\n\nDybbøl is a small town with a population of 2,495 (1 January 2014) in the southeastern corner of South Jutland, Denmark. It is around west of Sønderborg. It is mainly known for being the site of a famous last stand battle in 1864.\n\nDuring the Second Schleswig War in 1864, the Danish Army withdrew from the traditional fortified defence line, the Dannevirke (after waters and marshes which supported its flanks froze solid in a hard winter), and marched for Dybbøl to find a more defensible position. Although much artillery was abandoned and the evacuation was executed through a snow-laden north gale in winter, the army arrived almost intact. It entrenched itself at the Dybbøl Trenches, which became the scene of the siege and subsequent Battle of Dybbøl (7 April – 18 April 1864). This battle resulted in a Prussian-Austrian victory over Denmark.\n\nIn the following peace settlement, Denmark surrendered Schleswig. Following World War I, Denmark recovered the northern part of Schleswig as a result of the Schleswig Plebiscites as described in the Treaty of Versailles.\n\nThe Dybbøl Mill is considered a Danish national symbol.\n\nDybbøl is also the birthplace of American landscape architect Jens Jensen.\n\nDybbøl has gone under a myriad of names throughout history, but it is theorized to have started as Dyttis Bol; after the founder Dytti, with Bol being an old Danish word for a single farm. The name would later evolve into its first written form, Duttebul, as recorded in a Schleswig tax registry from 1352. This name would be used for many years, until the T's started to get dropped, leading to the words eventual change to Dyppell in, for example, Johannes Mejer's atlas. The name would continue to evolve in this trend, eventually changing out Bol/Bel in favour of the newer word Bøl, to finally produce Dybbøl.\n\nThe town was also renamed to Düppel while under the rule of Prussia and later German Confederation and Empire, from 1864 to 1920.\n\nThe town of Dybbøl started as part of a larger wave of expansionism during the Viking Age in Denmark, where hundreds of new land areas were settled, both in geographic Denmark as well as its many settlements abroad in, for example, England. The first traces of human settlement in Dybbøl go back to around 4.500 BC, and the town itself is estimated to have been founded around 800 AD.\n\nThe town was, during the pre-war times, quite typical of the area. It's oldest building, from around 1100 AD, is a part of the local church structure, and the local peasants were serfs tied to Sandbjerg Castle. The ownership of the castle changed hands to the Reventlow Family, which meant that the serfs in the area got to benefit from being some of the first serfs able to buy their land and become independent when Conrad Georg Reventlow started to sell his property after the Stavnsbånd was lifted. Conrad Georg was one of the first lords to do this, which makes Dybbøl home of some of the very first self-bought free peasants in Denmark.\n\nDuring the First Schleswig War, Dybbøl was used as a flanking position for the Danish army in case of an attack from the south. The first battle of Dybbøl was fought on the 5. July 1848, when Prussian troops were driven back from Dybbøl by the Danish troops garrisoned there. During the month of April, there were regular skirmishes in and around the Dybbøl area, leading to the famous Dybbøl Mill being burnt down, resulting in it being out of commission for 4 years.\n\nDuring later years between the two Schleswig Wars, namely in 1861, Danish engineers began construction of Dybbøls trench system, which was finished in 1862. The system consisted of 10 redoubts in a 3 km long half-circle that stretched from Vemmingbund to the Als Sound. The redoubts were small earthen constructions with large powder stashes of concrete, as well as wooden blockhouses for soldiers.\n\nAs part of the Second Schleswig War, Danish forces retreating from the Danevirke arrived in Dybbøl on the 5. February. The massive influx of soldiers and officers meant that the Dybbøl Mill became a temporary military headquarter, a role that the owners of the mill (a married couple) were famously happy to fulfil, to the point of them being honoured by veterans of the later battle at their wedding anniversary a century later.\n\nOn the 15. March the Prussian forces arrived to Dybbøl as part of their larger advance up Jutland. They began a month long bombardment of the position, something they could do with impunity as they had rifled cannons, something the Danish army did not. During the bombardment, the Prussian army worked to dig their own trenches towards the Danish ones as part of their assault preparations.\n\nOn the 18. April 1864, at 10:00, the Prussian army assaulted the Danish trench system. This was after 6 hours of continual bombardment, with more than 8.000 shells falling on the Danish trenches.\n\nThe assault was successful, and the Danish forces had to fall back to Als.\n\nAfter the Danish defeat in the Second Schleswig War, the resulting Treaty of Vienna meant that Dybbøl was now German territory as part of Prussia's conquest of the Duchy of Schleswig.\n\nAfter the Unification of Germany, the German Empire erected a large monument called the Düppel Denkmal, which became a tourist attraction for Germans all the way up to the First World War. The monument was later destroyed in 1945 by Danish rebels during the Nazi-German occupation of Denmark. The mill in Dybbøl became a monument for the Danish-speaking part of Dybbøl however, which was the majority at that time as well, going so far as becoming the subject of several poems by Danish poet Holger Drachmann.\n\nThe German Empire also rebuilt the trench system in Dybbøl, making it much bigger and more expansive. These additional reinforcements never got to be used however, as Denmark did not participate in World War I. These newer fortifications are still visible at the Dybbøl Museum to this day.\n\nIn 1914, right before the advent of WW1, the German Empire celebrated the anniversary of their victory at Als. The celebrations were held at the newly constructed trench system at Dybbøl, where about 2.000 war veterans from both Germany and Austria-Hungary attended, along with the Emperors brother, Prince Heinrich. This would be the last German celebration in Dybbøl, as WW1 would break out shortly after.\n\nAfter the end of World War I, populations in the former Duchy of Schleswig were given the opportunity through the Versailles Treaty to vote for which country they would rather be part of; Germany (now the Weimar Republic) or Denmark. The votes resulted in the borders as they are to this day.\n\nThe reunification resulted in celebration in Dybbøl, culminating in a visit by King Christian X on the 11. July 1920. A massive party was held while the king visited in the 10th redoubt in the newer German trenches. This redoubt would later become known as Kongeskansen (The Royal Redoubt). Around 50.000 people were present for the celebrations, along with the King and the entire royal Danish family. The climax of the celebrations were the handing over of an old Dannebrog to the king by a veteran of the Battle of Dybbøl.\n\nDybbøl continues to be a symbol of pride in Denmark, with it often being associated with a heroic last, similar to the Alamo in American conscience. Because of this, the entire town and most of the surrounding area has gradually become protected area, with it being illegal to disturb the trenches, mill and surrounding area.\n\nThe most notable institution in the town today is the museum, which is a popular school trip destination.\n\nThe site is a national memorial and museum of the Battle of Dybbøl and was therefore included in the 'National Park Dybbøl Skanser,' inaugurated in 1924. This park is not included in the Danish National Park laws of 2007, but it can still use the name National Park. The area is today administered as a 'Historiecenter Dybbøl Banke' (Dybbøl Banke Museum and History Centre).\n"}
{"id": "329115", "url": "https://en.wikipedia.org/wiki?curid=329115", "title": "Fulgurite", "text": "Fulgurite\n\nFulgurites (from the Latin \"fulgur\", meaning \"lightning\") are natural tubes, clumps, or masses of sintered, vitrified, and/or fused soil, sand, rock, organic debris and other sediments that can form when lightning discharges into ground. They can therefore be referred to as petrified lightning. They are classified as a variety of the mineraloid lechatelierite, although their absolute chemical composition is dependent on the physical and chemical properties of the usually granular-crystalline material providing an electrically and thermally conductive dissipation network for lightning-facilitated energy transfer. They are commonly hollow and/or branching assemblages of glassy, protocrystalline, and heterogeneously microcrystalline tubes, crusts, slags, vesicular masses, and clusters of refractory materials that often form during the discharge phase of lightning strikes propagating into silica-rich quartzose sand, mixed soil, clay, or other sediments. Fulgurites are homologous to Lichtenberg figures, which are the branching patterns produced on surfaces of insulators during dielectric breakdown by high-voltage discharges, such as lightning.\n\nFulgurites are formed when lightning strikes the ground, fusing and vitrifying mineral grains. Peak temperatures within a lightning channel are known to exceed 30,000 K, with sufficient pressure to produce planar deformation features, or \"shock lamellae,\" in SiO, a kind of polymorphism. This is also known colloquially as shocked quartz. \"Artificial fulgurites\" can also be produced when the controlled arcing of electricity into an appropriate medium. Downed high voltage power lines have produced fulgurite-like lechatelierites, occasionally colored by copper from the power lines themselves.\n\nThe primary SiO phase in common tube fulgurites is lechatelierite, a silica glass also found in impactites, but many other glasses may result from the processes involved with the production of fulgurites given their chemical context. Because their groundmass is generally amorphous in structure, fulgurites are classified as mineraloids.\n\nThe optical properties (color, surface texture modulation) of fulgurites vary widely, depending on bulk composition, refractory inclusions, interface dynamics, and chemical \"impurities,\" among other possible sources of variation. Most natural fulgurites fall on a spectrum from colorless (transparent), to white, to black; moderate iron oxide content, among other factors, can result in a deep brownish-green coloration. More colorful variants are usually synthetic and reflect incorporation of synthetic materials. The interior of Type I (sand) fulgurites normally is very smooth or lined with fine bubbles, while other types are often both vesicular and dense, almost pore-free, or even scoria-like; their exteriors generally can be coated with rough sedimentary particles or small rocks and can be porous, smooth, or topologically-complex. Branching fulgurites display fractal-like self-similarity and structural scale invariance as a macroscopic or microscopic network of root-like branches, and can display this texture without central channels or obvious divergence from morphology of context or target (e.g. sheet-like melt, rock fulgurites). Fulgurites formed in sand or loose soil are mechanically fragile, making the field collection of large specimens difficult. Other fulgurite classes are often very durable and may withstand a long residence in the geologic record, an issue of some contention in planetary sciences, such as impact geology.\n\nFulgurites can exceed tens of centimeters in diameter and can penetrate deep into the subsoil, sometimes occurring as far as below the surface that was struck, but may form directly on appropriate sedimentary surfaces. One of the longest fulgurites to have been found in modern times was a little over in length, and was found in northern Florida. The Yale University Peabody Museum of Natural History displays one of the longest known preserved fulgurites, approximately in length. Charles Darwin in \"The Voyage of the Beagle\" recorded that tubes such as these found in Drigg, Cumberland, UK reached a length of . The Winans Lake fulgurite[s] (Winans Lake, Livingston County, Michigan), extended discontinuously throughout a 30 m range, and arguably includes the largest reported fulgurite mass ever recovered and described - its largest section extending approximately 16 ft (4.88 m) in length by 1 ft in diameter (30 cm).\n\nFulgurites have been classified by Pasek et al. (2012) into five types related to the type of sediment in which the fulgurite formed, as follows:\n\n\nThe presence of fulgurites in an area can be used to predict the prevalence of lightning over the particular area over a certain period of time, which in turn can help in understanding the past climates, in the study of paleolightning. For instance, the fact that fulgurites are abundant in the central Sahara Desert, where thunderstorm activity is very rare, demonstrates that thunderstorms were once more frequent in that region. As their spatial distributions guide reconstruction of convection and precipitation patterns, their bulk chemistry may lead to inferences about surface sediments and generally porous structures enclose and preserve samples of the ancient atmosphere in which they formed.\n\nAnalysis of LIRM anomaly (see paleomagnetism) and reconstruction of ancient environmental conditions are motivations for the identification and study of fulgurites.\n\nMany observations have been made in fulgurites of high-pressure, high-temperature materials more commonly assumed to be exclusive to meteoritic sources, products of asteroid impacts, comet airbursts, or cosmic dust. Such materials - as a suite - formerly considered to be unique to hypervelocity impacts, have been identified in fulgurites, including highly reduced silicon-metal alloys (silicides), the fullerene allotropes C (buckminsterfullerene) and C, as well as high-pressure polymorphs of SiO, collectively known as shocked quartz, in fulgurites.\n\nReduced phosphorus as phosphides and phosphites has been identified through quantitative analyses of a representative sample of 10 fulgurites recovered from most continents, in the form of schreibersite (FeP, (Fe,Ni)P) - otherwise extremely rare on Earth, but relatively common in meteorites, comets, interplanetary dust, and some planetary bodies - and TiP, which is unique to fulgurites, among other unusual compounds resulting from supercritical non-equilibrium.\n\nA fulgurite was found within the contents of the ash altar at the temple of Lykaian Zeus at Mount Lykaion in Greece. It may be associated with ritual activity performed there.\n\nFulgurites are appreciated by many for their value as tangible evidence of lightning strikes. \nFulgurites are also popular among hobbyists and collectors of natural specimens.\n\n\n"}
{"id": "17164901", "url": "https://en.wikipedia.org/wiki?curid=17164901", "title": "Galapagos Triple Junction", "text": "Galapagos Triple Junction\n\nThe Galapagos Triple Junction is a geological area in the eastern Pacific Ocean several hundred miles west of the Galapagos Islands where three tectonic plates - the Cocos Plate, the Nazca Plate and the Pacific Plate - meet. It is an unusual type of triple junction in which the three plates do not meet at a simple intersection. Instead, the junction includes two small microplates, the Galapagos Microplate and the Northern Galapagos Microplate, caught in the junction, turning synchronously with respect to each other and separated by the Hess Deep rift.\n"}
{"id": "787928", "url": "https://en.wikipedia.org/wiki?curid=787928", "title": "George Shelvocke", "text": "George Shelvocke\n\nGeorge Shelvocke (baptised 1 April 167530 November 1742) was an English Royal Navy officer and later privateer who in 1723 wrote \"A Voyage Round the World by Way of the Great South Sea\" based on his exploits. It includes an account of how his second captain, Simon Hatley, shot an albatross off Cape Horn, an incident which provided the dramatic motive in Samuel Taylor Coleridge's poem \"The Rime of the Ancient Mariner\".\n\nBorn into a farming family in Shropshire and christened at St Mary's, Shrewsbury, on 1 April 1675, Shelvocke joined the Royal Navy when he was fifteen years old. During two long wars with France he rose through the ranks to become a sailing master and finally second lieutenant of a flagship serving under Rear Admiral Sir Thomas Dilkes in the Mediterranean. However, when war ended in 1713 he was beached without even half-pay for support. By the time he was offered a commission as captain of the privateering ship \"Speedwell\", he was living in poverty.\n\nAlongside the \"Success\", captained by John Clipperton, the \"Speedwell\" was involved in a 1719 expedition to loot Spanish ships and settlements along the Pacific coast of the Americas. The English had just renewed hostilities with Spain in the War of the Quadruple Alliance, and the ships carried letters of marque which gave them official permission to wage war on the Spanish and keep the profits. Shelvocke broke away from Clipperton shortly after leaving British waters and appears to have avoided contact as much as possible for the rest of the voyage.\n\nOn 25 May 1720 the \"Speedwell\" was wrecked on an island of Juan Fernández called Más a Tierra by the Spanish. Shelvocke and his crew were marooned there for five months but managed to build a 20-ton boat using some timbers and hardware salvaged from the wreck, in addition to wood obtained from locally felled trees. Leaving the island on 6 October, they transferred into their first prize, renamed the \"Happy Return\", and resumed privateering, despite the war having ended in February and rendered their letter of marque invalid. They continued up the coast of South America from Chile to Baja California, capturing more vessels along the way, before crossing the Pacific to Macao and returning to England in July 1722.\n\nIn England Shelvocke was arrested on charges of fraud at the instigation of the principal shareholders of the voyage, though he avoided conviction through out-of-court settlements with two of the complainants. They suspected, probably with reason, that he had failed to let them know about a significant portion of the loot obtained from the voyage, and planned to keep it for himself and other members of his crew. In this he likely succeeded. The self-justifying version of events given by Shelvocke in the book \"A Voyage Round the World by Way of the Great South Sea\" was disputed by some who had accompanied him on that expedition, in particular by his captain of marines, William Betagh.\n\nShelvocke nevertheless went on to re-establish his reputation and died on 30 November 1742 at the age of 67, a wealthy man as a result of his buccaneering activity. His chest tomb (since removed) in the churchyard of St Nicholas, Deptford, London, by the east wall eulogised \"a gentleman of great abilities in his profession and allowed to have been one of the bravest and most accomplished seamen of his time.\" A wall tablet in the chancel commemorates his son, also George Shelvocke, who died in 1760 and accompanied his father on the journey round the world before becoming Secretary of the General Post Office and a Fellow of the Royal Society.\n\nIn his book Shelvocke described an event wherein his second captain, Simon Hatley, shot a black albatross while the \"Speedwell\" was attempting to round Cape Horn in severe storms. Hatley took the giant sea bird to be a bad omen, and hoped that by killing it he might bring about a break in the weather. Some seventy years later the episode would become the inspiration for the central plot device in Samuel Taylor Coleridge's narrative poem \"The Rime of the Ancient Mariner\". Coleridge's friend and fellow poet William Wordsworth shared the following reminiscences on the origins of the poem:\n\nMuch the greatest part of the story was Mr Coleridge's invention; but certain parts I myself suggested: for example, some crime was to be committed which should bring upon the old navigator, as Coleridge afterwards delighted to call him, the spectral persecution, as a consequence of that crime, and his own wanderings. I had been reading in Shelvock's \"Voyages\" a day or two before that while doubling Cape Horn they frequently saw albatrosses in that latitude, the largest sort of sea-fowl, some extending their wings twelve or fifteen feet. \"Suppose,\" I said, \"you represent him as having killed one of these birds on entering the South Sea, and that the tutelary spirits of those regions take upon them to avenge the crime. The incident was thought fit for the purpose and adopted accordingly.\"\n\n\n"}
{"id": "12867", "url": "https://en.wikipedia.org/wiki?curid=12867", "title": "George Vancouver", "text": "George Vancouver\n\nCaptain George Vancouver (22 June 1757 – 10 May 1798) was a British officer of the Royal Navy, best known for his 1791–95 expedition, which explored and charted North America's northwestern Pacific Coast regions, including the coasts of contemporary British Columbia, Canada and Alaska, Washington, and Oregon, United States. He also explored the Hawaiian Islands and the southwest coast of Australia.\n\nVancouver Island and the city of Vancouver, British Columbia are named for him, as is Vancouver, Washington. Mount Vancouver of Yukon and Alaska, on the Canadian-American border and New Zealand's sixth highest mountain, are also named for him.\n\nGeorge Vancouver was born in the seaport town of King's Lynn (Norfolk, England) on 22 June 1757 as the sixth, and youngest, child of John Jasper Vancouver, a Deputy Collector of Customs, and Bridget Berners.\n\nIn 1771, at the age of 13, George Vancouver entered the Royal Navy as a \"young gentleman,\" a future candidate for midshipman. He was selected to serve as a midshipman aboard , on James Cook's second voyage (1772–1775) searching for \"Terra Australis\". He also accompanied Cook's third voyage (1776–1780), this time aboard \"Resolution\"'s companion ship, , and was present during the first European sighting and exploration of the Hawaiian Islands. Upon his return to Britain in October 1780, Vancouver was commissioned as a lieutenant and posted aboard the sloop initially on escort and patrol duty in the English Channel and North Sea. He accompanied the ship when it left Plymouth on 11 February 1782 for the West Indies. On 7 May 1782 he was appointed fourth Lieutenant of the 74-gun ship of the line HMS \"Fame\" which was at the time part of the British West Indies Fleet and assigned to patrolling the French-held Leeward Islands. Vancouver returned to England in June 1783.\n\nIn the late 1780s the Spanish Empire commissioned an expedition to the Pacific Northwest. The 1789 Nootka Crisis developed, and Spain and Britain came close to war over ownership of the Nootka Sound on contemporary Vancouver Island, and of greater importance, the right to colonise and settle the Pacific Northwest coast. Henry Roberts had recently taken command of the survey ship (a new vessel named in honour of the ship on Cook's voyage), which was to be used on another round-the-world voyage, and Roberts selected Vancouver as his first lieutenant, but they were then diverted to other warships due to the crisis. Vancouver went with Joseph Whidbey to the 74-gun ship of the line . When the first Nootka Convention ended the crisis in 1790, Vancouver was given command of \"Discovery\" to take possession of Nootka Sound and to survey the coasts.\n\nDeparting England with two ships, HMS \"Discovery\" and , on 1 April 1791, Vancouver commanded an expedition charged with exploring the Pacific region. In its first year the expedition travelled to Cape Town, Australia, New Zealand, Tahiti, and Hawaii, collecting botanical samples and surveying coastlines along the way. He formally claimed at Possession Point, King George Sound Western Australia, now the town of Albany, Western Australia for the British. Proceeding to North America, Vancouver followed the coasts of present-day Oregon and Washington northward. In April 1792 he encountered American Captain Robert Gray off the coast of Oregon just prior to Gray's sailing up the Columbia River.\n\nVancouver entered the Strait of Juan de Fuca, between Vancouver Island and the Washington state mainland on 29 April 1792. His orders included a survey of every inlet and outlet on the west coast of the mainland, all the way north to Alaska. Most of this work was in small craft propelled by both sail and oar; manoeuvring larger sail-powered vessels in uncharted waters was generally impractical and dangerous.\n\nVancouver named many features for his officers, friends, associates, and his ship \"Discovery\", including:\n\nVancouver was the second European to enter Burrard Inlet on 13 June 1792, naming it for his friend Sir Harry Burrard. It is the present day main harbour area of the City of Vancouver beyond Stanley Park. George Vancouver surveyed Howe Sound and Jervis Inlet over the next nine days. Then, on his 35th birthday on 22 June 1792, he returned to Point Grey, the present-day location of the University of British Columbia. Here he unexpectedly met a Spanish expedition led by Dionisio Alcalá Galiano and Cayetano Valdés y Flores. Vancouver was \"mortified\" (\"his word\") to learn they already had a crude chart of the Strait of Georgia based on the 1791 exploratory voyage of José María Narváez the year before, under command of Francisco de Eliza. For three weeks they cooperatively explored the Georgia Strait and the Discovery Islands area before sailing separately towards Nootka Sound.\n\nAfter the summer surveying season ended, in August 1792, Vancouver went to Nootka, then the region's most important harbour, on contemporary Vancouver Island. Here he was to receive any British buildings and lands returned by the Spanish from claims by Francisco de Eliza for the Spanish crown. The Spanish commander, Juan Francisco Bodega y Quadra, was very cordial and he and Vancouver exchanged the maps they had made, but no agreement was reached; they decided to await further instructions. At this time, they decided to name the large island on which Nootka was now proven to be located as \"Quadra and Vancouver Island\". Years later, as Spanish influence declined, the name was shortened to simply Vancouver Island.\n\nWhile at Nootka Sound Vancouver acquired Robert Gray's chart of the lower Columbia River. Gray had entered the river during the summer before sailing to Nootka Sound for repairs. Vancouver realised the importance of verifying Gray's information and conducting a more thorough survey. In October 1792, he sent Lieutenant William Robert Broughton with several boats up the Columbia River. Broughton got as far as the Columbia River Gorge, sighting and naming Mount Hood.\n\nVancouver sailed south along the coast of Spanish Alta California, visiting Chumash villages at Point Conception and near Mission San Buenaventura. Vancouver spent the winter in continuing exploration of the Sandwich Islands, the contemporary islands of Hawaii.\n\nThe next year, 1793, he returned to British Columbia and proceeded further north, unknowingly missing the overland explorer Alexander Mackenzie by only 48 days. He got to 56°30'N, having explored north from Point Menzies in Burke Channel to the northwest coast of Prince of Wales Island. He sailed around the latter island, as well as circumnavigating Revillagigedo Island and charting parts of the coasts of Mitkof, Zarembo, Etolin, Wrangell, Kuiu and Kupreanof Islands. With worsening weather, he sailed south to Alta California, hoping to find Bodega y Quadra and fulfil his territorial mission, but the Spaniard was not there. He again spent the winter in the Sandwich Islands.\n\nIn 1794, he first went to Cook Inlet, the northernmost point of his exploration, and from there followed the coast south. Boat parties charted the east coasts of Chichagof and Baranof Islands, circumnavigated Admiralty Island, explored to the head of Lynn Canal, and charted the rest of Kuiu Island and nearly all of Kupreanof Island. He then set sail for Great Britain by way of Cape Horn, returning in September 1795, thus completing a circumnavigation of South America.\n\nImpressed by the view from Richmond Hill, Vancouver retired to Petersham, London.\n\nVancouver faced difficulties when he returned home to England. The accomplished and politically well-connected naturalist Archibald Menzies complained that his servant had been pressed into service during a shipboard emergency; sailing master Joseph Whidbey had a competing claim for pay as expedition astronomer; and Thomas Pitt, 2nd Baron Camelford, whom Vancouver had disciplined for numerous infractions and eventually sent home in disgrace, proceeded to harass him publicly and privately.\n\nPitt's allies, including his cousin, Prime Minister William Pitt the Younger, attacked Vancouver in the press. Thomas Pitt took a more direct approach; on 29 August 1796 he sent Vancouver a letter heaping many insults on the head of his former captain, and challenging him to a duel. Vancouver gravely replied that he was unable \"in a private capacity to answer for his public conduct in his official duty,\" and offered instead to submit to formal examination by flag officers. Pitt chose instead to stalk Vancouver, ultimately assaulting him on a London street corner. The terms of their subsequent legal dispute required both parties to keep the peace, but nothing stopped Vancouver's civilian brother Charles from interposing and giving Pitt blow after blow until onlookers restrained the attacker. Charges and counter-charges flew in the press, with the wealthy Camelford faction having the greater firepower until Vancouver, ailing from his long naval service, died.\n\nVancouver, at one time amongst Britain's greatest explorers and navigators, died in obscurity on 10 May 1798 at the age of 40, less than three years after completing his voyages and expeditions. No official cause of death was stated, as the medical records pertaining to Vancouver were destroyed; one doctor named John Naish claimed Vancouver died from kidney failure, while others believed it was a hyperthyroid condition. His grave is in the churchyard of St Peter's Church, Petersham, in the London Borough of Richmond upon Thames, England. The Hudson's Bay Company placed a memorial plaque in the church in 1841. His grave in Portland stone, renovated in the 1960s, is now Grade II listed in view of its historical associations.\n\nVancouver determined that the Northwest Passage did not exist at the latitudes that had long been suggested. His charts of the North American northwest coast were so extremely accurate that they served as the key reference for coastal navigation for generations. Robin Fisher, the academic Vice-President of Mount Royal University in Calgary and author of two books on Vancouver, states:\nHowever, Vancouver failed to discover two of the largest and most important rivers on the Pacific coast, the Fraser River and the Columbia River. He also missed the Skeena River near Prince Rupert in northern British Columbia. Vancouver did eventually learn of the river before he finished his survey—from Robert Gray, captain of the American merchant ship that conducted the first Euroamerican sailing of the Columbia River on 11 May 1792, after first sighting it on an earlier voyage in 1788. However it and the Fraser River never made it onto Vancouver's charts.\nStephen R. Bown, noted in \"Mercator's World\" magazine (November/December 1999) that:\n\nWhile it is difficult to comprehend how Vancouver missed the Fraser River, much of this river's delta was subject to flooding and summer freshet which prevented the captain from spotting any of its great channels as he sailed the entire shoreline from Point Roberts, Washington to Point Grey in 1792. The Spanish expeditions to the Pacific Northwest, with the 1791 Francisco de Eliza expedition preceding Vancouver by a year, had also missed the Fraser River although they knew from its muddy plume that there was a major river located nearby.\n\nVancouver generally established a good rapport with both Indigenous peoples and European trappers. Historical records show Vancouver enjoyed good relations with native leaders both in Hawaii – where King Kamehameha I ceded Hawaii to Vancouver in 1794 – as well as the Pacific Northwest and California. Vancouver's journals exhibit a high degree of sensitivity to natives. He wrote of meeting the Chumash people, and of his exploration of a small island on the Californian coast on which an important burial site was marked by a sepulchre of \"peculiar character\" lined with boards and fragments of military instruments lying near a square box covered with mats. Vancouver states:\n\nVancouver also displayed contempt in his journals towards unscrupulous western traders who provided guns to natives by writing:\n\nRobin Fisher notes that Vancouver's \"relationships with aboriginal groups were generally peaceful; indeed, his detailed survey would not have been possible if they had been hostile.\" While there were hostile incidents at the end of Vancouver's last season – the most serious of which involved a clash with Tlingits at Behm Canal in southeast Alaska in 1794 – these were the exceptions to Vancouver's exploration of the US and Canadian Northwest coast.\n\nDespite a long history of warfare between Britain and Spain, Vancouver maintained excellent relations with his Spanish counterparts and even fêted a Spanish sea captain aboard his ship during his 1792 trip to the Vancouver region.\n\n\nMany places around the world have been named after George Vancouver, including:\n\n\n\n\n\n\nMany collections were made on the voyage: one was donated by Archibald Menzies to the British Museum 1796; another made by surgeon George Goodman Hewett (1765–1834) was donated by A. W. Franks to the British Museum in 1891. An account of these has been published.\n\n Canada Post issued a $1.55 postage stamp to commemorate the 250th anniversary of Vancouver's birth, on 22 June 2007. The stamp has an embossed image of Vancouver seen from behind as he gazes forward towards a mountainous coastline. This may be the first Canadian stamp not to show the subject's face.\n\nThe City of Vancouver in Canada organised a celebration to commemorate the 250th anniversary of Vancouver's birth, in June 2007 at the Vancouver Maritime Museum. The one-hour festivities included the presentation of a massive 63 by 114 centimetre carrot cake, the firing of a gun salute by the Royal Canadian Artillery's 15th Field Regiment and a performance by the Vancouver Firefighter's Band.\nVancouver's then-mayor, Sam Sullivan, officially declared 22 June 2007 to be \"George Day\".\n\nThe Musqueam (xʷməθkʷəy̓əm) Elder sɁəyeɬəq (Larry Grant) attended the festivities and acknowledged that some of his people might disapprove of his presence, but also noted:\n\nThere has been some debate about the origins of the Vancouver name. It is now commonly accepted that the name Vancouver derives from the expression van Coevorden, meaning \"(originating) from Coevorden\", a city in the northeast of the Netherlands. This city is apparently named after the \"Coeverden\" family of the 13th–15th century.\n\nIn the 16th century, a number of businessmen from the Coevorden area (and the rest of the Netherlands) moved to England. Some of them were known as \"Van Coeverden\". Others adopted the surname Oxford, as in oxen fording (a river), which is approximately the English translation of \"Coevorden\". However, it is not the exact name of the noble family mentioned in the history books that claim Vancouver's noble lineage: that name was Coeverden not Coevorden.\n\nIn the 1970s, Adrien Mansvelt, a former consul general of the Netherlands based in Vancouver, published a collation of information in both historical and genealogical journals and in the \"Vancouver Sun\" newspaper. Mansvelt's theory was later presented by the city during the Expo 86 World's Fair, as historical fact. The information was then used by historian W. Kaye Lamb in his book \"A Voyage of Discovery to the North Pacific Ocean and Round the World, 1791–1795\" (1984).\n\nW. Kaye Lamb, in summarising Mansvelt's 1973 research, observes evidence of close family ties between the Vancouver family of Britain and the Van Coeverden family of the Netherlands as well as George Vancouver's own words from his diaries in referring to his Dutch ancestry:\n\nIn 2006 John Robson, a librarian at the University of Waikato, conducted his own research into George Vancouver's ancestry, which he published in an article published in the British Columbia History journal. Robson theorises that Vancouver's forebears may have been Flemish rather than Dutch; he believes that Vancouver is descended from the Vangover family of Ipswich and Colchester in Suffolk. Those towns had a significant Flemish population in the 16th and 17th centuries.\n\nGeorge Vancouver named the south point of what is now Couverden Island, Alaska, as \"Point Couverden\" during his exploration of the North American Pacific coast, in honour of his family's hometown of Coevorden. It is located at the western point of entry to Lynn Canal in southeastern Alaska.\n\nThe Admiralty instructed Vancouver to publish a narrative of his voyage which he started to write in early 1796 in Petersham. At the time of his death the manuscript covered the period up to mid-1795. The work, \"A Voyage of Discovery to the North Pacific Ocean, and Round the World\", was completed by his brother John and published in three volumes in the autumn of 1798. A second edition was published in 1801 in six volumes. \n\nA modern annotated edition (1984) by W. Kaye Lamb was renamed \"The Voyage of George Vancouver 1791–1795\", and published in four volumes by the Hakluyt Society of London, England.\n\n\n\n"}
{"id": "22690388", "url": "https://en.wikipedia.org/wiki?curid=22690388", "title": "Geoscientific Model Development", "text": "Geoscientific Model Development\n\nGeoscientific Model Development is a peer-reviewed open access scientific journal published by Copernicus Publications on behalf of the European Geosciences Union. It covers the description, development, and evaluation of numerical models of the Earth system and its components.\n\nThe journal has a two-stage publication process. In the first stage, papers that pass a rapid access peer-review are immediately published on the \"Geoscientific Model Development Discussions\" website. They are then subject to interactive public discussion, during which the referees' comments (anonymous or attributed), additional short comments by other members of the scientific community (attributed), and the authors' replies are published. In the second stage, the peer-review process is completed and, if accepted, the final revised papers are published in \"Geoscientific Model Development\".\n\nThe journal is abstracted and indexed in the Science Citation Index Expanded, Scopus, Astrophysics Data System, and Current Contents/Physical, Chemical & Earth Sciences. According to the \"Journal Citation Reports\", the journal has a 2014 impact factor of 3.654.\n"}
{"id": "38119814", "url": "https://en.wikipedia.org/wiki?curid=38119814", "title": "Good Hope Jet", "text": "Good Hope Jet\n\nThe Good Hope Jet is the northward-running shelf edge frontal jet of the Southern Benguela Current off the Cape Peninsula of South Africa's west coast. The jet, an intrusion of water from the Agulhas Current, was first described by South African oceanographers, Nils Bang and W.R.H. (Bill) Andrews in 1974. This warm water jet forms a sharp front as it comes into contact with the colder upwelled water over the shelf and plays a key role in carrying fish eggs and larvae from their food-poor Agulhas Bank spawning grounds to inshore nurseries.\n"}
{"id": "5212093", "url": "https://en.wikipedia.org/wiki?curid=5212093", "title": "Gran Desierto de Altar", "text": "Gran Desierto de Altar\n\nThe Gran Desierto de Altar is one of the major sub-ecoregions of the Sonoran Desert, located in the State of Sonora, Northwest Mexico. It includes the only active erg dune region in North America. The desert extends across much of the northern border of the Gulf of California, reaching more than east to west, and over north to south. It constitutes the largest continuous wilderness area within the Sonoran Desert.\n\nThe eastern portion of the area contains the volcanic Pinacate Peaks region, and with the western Gran Desierto de Altar area, together they form the El Pinacate y Gran Desierto de Altar Biosphere Reserve and a UNESCO World Heritage Site.\n\nThe Gran Desierto covers approximately , most of it in the Mexican state of Sonora. The northernmost edges overlap the border into Organ Pipe Cactus National Monument and Cabeza Prieta National Wildlife Refuge in southwestern Arizona. The dominant sand sheets and dunes range in thickness from less than to greater than . The total volume of sand in the Gran Desierto is about 60 cubic km. Most of that volume was delivered by the Pleistocene Colorado River which flowed through the present-day Gran Desierto area ~120,000 years before present. This Pleistocene delta migrated westward concomitant with strike-slip faulting and rifting associated with the opening of the Salton Trough and the Gulf of California.\n\nThe eastern margin of the Gran Desierto abuts the Cenozoic volcanic complex of the Sierra Pinacate, a composite volcanic field covering more than with a summit elevation of . Aeolian sands have climbed onto many of the western slopes of the Sierras Pinacate, defining the eastern limit of the dune field. To the north, the sands thin out against the distal margins of alluvial fans from the Tinajas Altas and Tule Mountains along the Arizona-Sonora border. The southern border of the sand sea is the northern shore of the Gulf of California.\n\nThe southernmost extension of the San Andreas Fault cuts across the area and lies beneath several prominent granite inselbergs, most notably the Sierra del Rosario mountains which are surrounded by the erg on all sides. The Sierra Enternada is a smaller inselberg almost completely buried by the sand near the boundary of the Gran Desierto and the Pinacate volcanic complex.\n\nThe Gran Desierto is best known for its magnificent Star Dunes, many in excess of high. More than two-thirds of the Gran Desierto is covered by sand sheets and sand streaks. The remaining area is split equally between a western population of star dunes and an eastern set of transverse or crescentic dunes. Some of the larger crescentic dunes in the northeastern sand sea exhibit reversing crests, a transitional morphologic feature associated with star dunes.\n\nVegetation assemblages of the Gran Desierto are typical of the lower Sonoran Desert with a marked difference in vegetation type and density with location. Large areas of the southern and eastern sand sea, especially near the margins, have a moderately dense (up to 20%) cover of perennial low shrubs and herbs such as bursage (\"Ambrosia dumosa\") and longleaf jointfir (\"Ephedra trifurca\") with creosote bush (\"Larrea tridentata\") in areas of thin sand cover. Palo verde/acacia/ocotillo communities occur on alluvial slopes on the northern side of the sand sea, particularly in arroyos and washes. Its estimated total vegetation cover at 15% in the star dunes and about 10% in the low transverse or crescentic dunes areas. These percentages are substantially greater than in most active dune fields where vegetation covers of 15% are more typical.\n\nSeveral teams have examined the nests (middens) built by pack rats as a proxy for ancient vegetation regimes. All have concluded that the Gran Desierto has been a refugia for desert plants since at least the late Pleistocene. The Gran Desierto has served as a refuge for most dominant Mojave Desert plant species during cooler pluvial epochs as well. C14 dates for a midden from the Tinajas Altas Mountains showing assemblages of juniper and Joshua trees coexisting with contemporary Gran Desierto flora and fauna more than 43,000 years before present. Although midden studies do not provide information beyond the late Pleistocene, they do indicate that, in gross form, the climate of the Gran Desierto as recorded by plant communities has been desert-like since at least the peak of the Wisconsinan glaciation.\n\nThe Gran Desierto has a warm-to-hot arid climate. Mean annual rainfall, most of which occurs between September and December, is 73 mm at Puerto Peñasco, Sonora (located on the southeastern margin of the sand sea) and decreases northward toward Yuma, Arizona (on the northwestern edge) to 62 mm per year. Mid-summer highs in excess of 45 °C are common in the central sand sea. Mid-winter lows of less than 10 °C are rare. Winds are controlled in part by the position and strength of the Sonoran Low in summer, creating southerly winds, and by the Great Basin High in winter, with north-to-northeasterly winds.\n\nThe well-documented pluvial epochs which occurred over much of the southwestern United States during the most-recent (Wisconsin) ice age may not have extended as far south as the Gran Desierto. It appears that the climatic regime of the past 150,000 years at this site has been one of gradually increasing aridity with current hyperarid conditions being firmly in place by at least 43,000 years ago.\n\nAs a minimum, it may be assumed that onshore coastal winds from the south were less important to sand movement when the Wisconsin shoreline was located 45 km seaward of its current position.\n\nThe Gran Desierto is located adjacent to a rapidly subsiding tectonic basin, the Salton Trough, which is a northern extension of the Gulf of California, an embayment created by rifting initiated during the Pliocene along the East Pacific Rise and the San Andreas fault system Regional subsidence has propagated to the northwest as rifting and strike-slip faulting continues into the present day. The central portion of the nearby Salton Trough is more than below sea level; it is protected from marine embayment only by the natural dike of the Colorado River Delta.\nOngoing tectonic activity modifies the Gran Desierto today. The southernmost extension of the San Andreas fault system, the Cerro Prieto Fault, passes directly through the area before continuing offshore into the Gulf of California. Strike-slip movement in the area is as high as 60 mm/year.\nSince 1900, one magnitude 6.3 and two magnitude 7.1 earthquakes have originated within the erg. Most seismicity within the Gran Desierto originates at depths of five to six kilometers, corresponding to the transition between deltaic deposits and basement crystalline rocks. Local uplift is still occurring along the Mesa Arenosa, a drag folded fault block forming the coastal boundary.\n\nThe geological history of the Gran Desierto is intimately linked to the opening of the Gulf of California and the capture of the ancestral Colorado River; source areas that were adjacent to the Gran Desierto have shifted in position, basement topography has been altered continuously, and bedforms have been created, modified or completely destroyed and then reworked.\n\nThe Gran Desierto sand sheets and dunes are located atop deltaic deposits of the Pleistocene Colorado River. The lower Colorado River was captured by the Gulf of California 1.2 million years before present. This event places an upper bound on the age of the Gran Desierto with the Colorado's major clastic sediment sources. Conglomeritic sands and silts beneath the Mesa Arenosa were examined by Colletta and Ortlieb and dated at between 700,000 and 120,000 years before present.\n\nVertebrate fossils found by Merriam within the deltaic deposits include Equus, Gomphotherium and Bison and were assigned to Irvingtonian age (0.5 to 1.8 million years before present); dates consistent with the aforementioned capture of the lower Colorado River. Evidence of a giant anteater Myrmecophaga tridactyla was found in the deltaic deposits in the southern Gran Desierto. Van Devender notes that the specimen was found in association with fossils of mammoths, sloths and boa constrictors; a tropical faunal assemblage which supports a contention that the Colorado River delta of a previous interglacial period (>120,000 years ago) was much warmer and wetter than the present interglacial.\n\nPaleo-deltaic deposits near Salina Grande correlate with a ubiquitous indurated shell deposit dated by Io/U radiometric methods at 146,000 + 13,000/-11, 000 years of age. Slate obtained K-Ar ages for basalt flows in the western Pinacates. Based on this work, some aeolian activity may have been present as early as 700,000 years ago, as evidenced by the dated accretionary mantles on basalt flows of the Pinacate volcanic field.\n\nBlount and Lancaster proposed that by late Pleistocene time, the Colorado River was a highly competent stream flowing through the area which is occupied today by the massive western star dune zone. The seashore at this time was at least south of its present-day location. Primary bed loads of poorly sorted gravel were deposited from present-day Yuma, Arizona to an area south of the present-day Sierra del Rosario mountains.\nAs rifting of the Gulf of California progressed to the northwest, and uplift along the coast began, the river channel shifted westward, leaving primary bedload deposits in the former channel and floodplain. Deltaic sediments beneath the Gran Desierto may be as much as deep.\n\nAnnual sediment loads prior to the damming of the Colorado River were prodigious. A single flood event deposited an estimated 100,000,000 m of coarse to medium sand as a sheet deposit on the modern delta just south of the international boundary. Events like this, even if rare, could fill up the Gran Desierto in only a few millennia.\n\nThe synchronous development of the Colorado River Delta and the associated Gran Desierto sand sink continues offshore into the Gulf of California. Reports on the submarine topography of the Gulf of California by van Andel, describe three former river channels on the seafloor: one originating at the present-day Colorado delta, another from the area of the paleo-delta between El Golfo and Salina Grande, and a third to the area of present-day Puerto Penasco. Rusnak reported on sonar soundings which discovered the valleys and also describe two elongate depressions, each about in length, into which the valley networks terminate at a depth of ~ below sea level. Those incised valley systems were also interpreted as fluvial in origin.\n\n\n\n"}
{"id": "1125112", "url": "https://en.wikipedia.org/wiki?curid=1125112", "title": "Hans Jenny (cymatics)", "text": "Hans Jenny (cymatics)\n\nHans Jenny (16 August 1904, Basel – 23 June 1972, Dornach) was a physician and natural scientist who coined the term cymatics to describe acoustic effects of sound wave phenomena.\n\nJenny was born in Basel, Switzerland. After completing a doctorate he taught science at the Rudolph Steiner School in Zürich for four years before beginning medical practice.\n\nIn 1967, Jenny published the first volume of \"Cymatics: The Study of Wave Phenomena.\" The second volume came out in 1972, the year he died. This book was a written and photographic documentation of the effects of sound vibrations on fluids, powders and liquid paste. He concluded, \"This is not an unregulated chaos; it is a dynamic but ordered pattern.\"\n\nJenny made use of crystal oscillators and his so-called tonoscope to set plates and membranes vibrating. He spread quartz sand onto a black drum membrane 60 cm in diameter. The membrane was caused to vibrate by singing loudly through a cardboard pipe, and the sand produced symmetrical Chladni patterns, named after Ernst Chladni, who had discovered this phenomenon in 1787. Low tones resulted in rather simple and clear pictures, while higher tones formed more complex structures.\n\nChladni's and Jenny's work influenced Alvin Lucier and helped lead to his composition \"Queen of the South\". Cymatics was also followed up by Center for Advanced Visual Studies (CAVS) founder György Kepes at MIT. His work in this area included an acoustically vibrated piece of sheet metal in which small holes had been drilled in a grid. Small flames of gas burned through these holes and thermodynamic patterns were made visible by this setup. A special edition of the Hafler Trio's work \"Exactly As I Say\" includes a DVD containing material said to be \"based on and extended from techniques suggested by Prof. Hans Jenny\".\nPhotographer Alexander Lauterwasser has also captured imagery of water surfaces set into motion by sound sources ranging from sine waves to music by Beethoven, Karlheinz Stockhausen and overtone singing.\n\n\n"}
{"id": "14482797", "url": "https://en.wikipedia.org/wiki?curid=14482797", "title": "Hin Namno", "text": "Hin Namno\n\nHin Namno or Hin Namno National Biodiversity Conservation Area is a nature reserve in Khammouane Province, Laos. This area borders Phong Nha-Ke Bang of Vietnam to the east.\n\nThe area protects the biodiversity of Laos and Southeast Asia. Hin Namno topography and geology feature karst formation. If this area and Phong Nha-Ke Bang are combined into a contiguous area, then this would be one of the largest karst region in the world.\n\nKey species living in the reserve include Douc and Francois’s langur, giant muntjac, fruit bat, harlequin bat, great evening bat, wreathed and great hornbills and the sooty babbler.\n\nCertain habitats and forestry in the reserve include evergreens, both mixed deciduous and dipterocarp forest. Due to Hin Namno being located between the Central Indochina Limestone and Annamite Chain there are many caves and limestone escarpments including a 5 km cave along the Xe Bangfai River.\n\n\n\n"}
{"id": "2127941", "url": "https://en.wikipedia.org/wiki?curid=2127941", "title": "IEC 61850", "text": "IEC 61850\n\nIEC 61850 is an international standard defining communication protocols for intelligent electronic devices at electrical substations. It is a part of the International Electrotechnical Commission's (IEC) Technical Committee 57 reference architecture for electric power systems. The abstract data models defined in IEC 61850 can be mapped to a number of protocols. Current mappings in the standard are to MMS (Manufacturing Message Specification), GOOSE (Generic Object Oriented Substation Event), SMV (Sampled Measured Values), and soon to Web Services. These protocols can run over TCP/IP networks or substation LANs using high speed switched Ethernet to obtain the necessary response times below four milliseconds for protective relaying.\n\nMultiple protocols exist for substation automation, which include many proprietary protocols with custom communication links. Interoperation of devices from different vendors would be an advantage to users of substation automation devices. An IEC project group of about 60 members from different countries worked in three IEC working groups from 1995. They responded to all the concerns and objectives and created IEC 61850. The objectives set for the standard were:\n\nIEC 61850 consists of the following parts detailed in separate IEC 61850 standard documents:\n\nIEC 61850 features include:\n\n\n\n"}
{"id": "1078772", "url": "https://en.wikipedia.org/wiki?curid=1078772", "title": "Largest naval battle in history", "text": "Largest naval battle in history\n\nThe title of the \"largest naval battle in history\" is disputed between adherents of different criteria which include the numbers of personnel and/or vessels involved in the battle, and the total tonnage of the vessels involved. While battles fought in modern times are comparatively well-documented, the figures from those in pre-Renaissance times are generally believed to be exaggerated by contemporary chroniclers.\n\n\nNotes\n\nBibliography\n"}
{"id": "51171860", "url": "https://en.wikipedia.org/wiki?curid=51171860", "title": "List of Local Nature Reserves in Cambridgeshire", "text": "List of Local Nature Reserves in Cambridgeshire\n\nCambridgeshire is a county in eastern England, with an area of and a population as of 2011 of 708,719. It is crossed by two major rivers, the Nene and the Great Ouse. The main manufacturing area is Peterborough, and the foundation of the University of Cambridge in the thirteenth century made the county one of the country's most important intellectual centres. A large part of the county is in The Fens, and drainage of this habitat, which was probably commenced in the Roman period and largely completed by the seventeenth century, considerably increased the area available for agriculture.\n\nThe administrative county was formed in 1974, incorporating most of the historic county of Huntingdonshire. Local government is divided between Cambridgeshire County Council and Peterborough City Council, which is a separate unitary authority. Under the county council, there are five district councils, Cambridge City Council, South Cambridgeshire District Council, East Cambridgeshire District Council, Huntingdonshire District Council and Fenland District Council.\n\nLocal Nature Reserves (LNRs) are designated by local authorities, which must have a legal control over the site, by owning or leasing it or having an agreement with the owner. LNRs are sites which have a special local interest either biologically, geologically or education. Local authorities can either manage sites themselves or through other groups such as \"friends of\" and wildlife trusts, and can apply local bye-laws to manage and protect LNRs.\n\nThere are twenty-seven LNRs in Cambridgeshire. Four are Sites of Special Scientific Interest, and five are managed by the Wildlife Trust for Bedfordshire, Cambridgeshire and Northamptonshire. The largest is Little Paxton Pits at sixty hectares, which is of national importance for wintering wildfowl, and the smallest is St Denis Churchyard, East Hatley, which has grassland with diverse flowers. There is public access to all sites.\n\n"}
{"id": "2045145", "url": "https://en.wikipedia.org/wiki?curid=2045145", "title": "List of Peperomia species", "text": "List of Peperomia species\n\nHere is a list of the species belonging to the \"Peperomia\" genus. The list follows the alphabetical order of the species name.\n\n\n\n\n\n\n\n\n\n"}
{"id": "32020163", "url": "https://en.wikipedia.org/wiki?curid=32020163", "title": "List of Ranunculus species", "text": "List of Ranunculus species\n\nRanunculus is a genus of about 600 species of plants in the Ranunculaceae. Members of the genus include the buttercups, spearworts, water crowfoots and the lesser celandine.\n\n"}
{"id": "5894154", "url": "https://en.wikipedia.org/wiki?curid=5894154", "title": "List of Synaphridae species", "text": "List of Synaphridae species\n\nThis page lists all described species of the spider family Synaphridae as of June 10, 2011.\n\n\"Africepheia\" \n\n\"Cepheia\" \n\n\"Synaphris\" \n\n"}
{"id": "8639024", "url": "https://en.wikipedia.org/wiki?curid=8639024", "title": "List of rivers of Mendoza Province", "text": "List of rivers of Mendoza Province\n\nThis is a list of rivers that are part of the Mendoza Province of Argentina. All rivers born on the area of the Andes range, except the Desaguadero River mainly located in the provinces of San Juan, San Luis and Mendoza in the Argentine region called Cuyo.\n\n\n"}
{"id": "9012665", "url": "https://en.wikipedia.org/wiki?curid=9012665", "title": "List of soybean diseases", "text": "List of soybean diseases\n\nSoybean plants (\"Glycine max\") are subject to a variety of diseases and pests.\n\n\n"}
{"id": "145736", "url": "https://en.wikipedia.org/wiki?curid=145736", "title": "Læsø", "text": "Læsø\n\nLæsø (\"Isle of Hlér\") is the largest island in the North Sea bay of Kattegat, and is located off the northeast coast of the Jutland Peninsula, the Danish mainland. Læsø is also the name of the municipality (Danish, \"kommune\") on that island. The island is a location mentioned in several instances in Norse mythology, including as the dwelling of the sea jötunn Ægir and as a feasting place of the Norse gods, the Æsir.\n\nThe municipality is in Region Nordjylland in northern Denmark. The municipality, Denmark's least populous, covers Læsø and neighboring small islands for a total area of , and has a total population of 1,793 as of 1 January 2017. The population has been steadily declining, and according to Danmarks Statistik (Statistikbanken.dk) was: 2091 inhabitants (year:2006); 2056 inh. (2007); 2003 inh. (2008);1993 inh.(2009);1969 inh. (2010);1949 inh. (2011);1897 inh. (2012);1839 inh. (2013);1808 inh. (2014);1795 inh. (2015);1817 inh. (2016);1793 inh. (2017). Its mayor is Karsten Nielsen as of 1 January 2018. He is a member of the Danish People's Party political party.\n\nThe main town and the site of its municipal council is Byrum.\n\nBecause Læsø is an island and lies in the Kattegat, its neighboring municipality, Frederikshavn on the Jutland peninsula, is separated by water, the \"Læsø Rende\", from the island municipality.\n\nFerry service connects Frederikshavn on the Jutland peninsula to the municipality at the town of Vesterø Havn while Østerby Havn is the island's fishing harbour.\n\nLæsø Municipality was not merged with any adjacent municipality under the municipal reform of 2007, as it agreed to enter into a \"municipal cooperation agreement\" with Frederikshavn Municipality.\n\nLæsø has an outstanding botanical interest. The nature-types on and around Læsø includes open water, extensive mudflats, sand banks, heathland, islets and areas of arable land. It houses Denmarks largest tidal saltmarsh outside the Wadden Sea but the decline in grazing animals has led to a gradual vegetational succession. Invasive species are colonizing the site, especially Japanese Rose, and scrub clearance has been implemented to re-establish the former pastures open heathland. Seals like the Harbor seal are breeding around Læsø and the whole area is an internationally important area for wintering, molting and staging waterbirds. Therefore, a Ramsar protection was put into force in 1977 (number 149) and today it encompass 66,548 ha.\n\nTogether with Anholt, Læsø belongs to the Danish \"desert belt\"; during the summer months there is so little rain that streams and ponds partly dry up.\n\nIn the Middle Ages, the island was known for its salt industry. The ground water can reach over 15 percent salt, and this was naturally concentrated in flat salt meadows during the hot dry summers. The final concentration, carried out in hundreds of salt kilns, consumed large amounts of wood. Eventually the island became deforested, sandstorms buried villages, and salt extraction was banned. Since the end of the 1980s it has been resumed on a small scale as an archaeological experiment and a tourist attraction.\n\nLæsø is home to the bee subspecies European dark bee. The species is protected by Danish law which prohibits the import of other species to the island. The law has not been enforced and today normal bees and brown bees are both used for the production of honey. The island has been split in two parts for bee management, one for each species.\n\nLæsø is home to unique styles of Danish traditional music. Most of it is not played any more but has been preserved through intense documentation and research in the 1980s and 1990s.\n\nThe HVDC powerline Kontiskan crosses Læsø as overhead line. On Læsø, there is also a 160 metres tall radio relay mast.\n\nModern Danish \"Læsø\" is first attested in Old Norse as Hlésey, meaning \"Isle of Hlér\". According to the \"Prose Edda\" book \"Skáldskaparmál\", \"Hlér\" is another name for the sea jötunn Ægir who, according to the same book, there held feasts for the gods.\n\nIn the \"Poetic Edda\" poem \"Hárbarðsljóð\", the god Thor comments that it was on Hlésey that he was attacked by and so fought \"berserk women\" or \"brides of berserks\" who had bewitched all of the men on the island. Thor details that, upon beaching his ship, the women battered it, threatened him with iron clubs and chased his servant, Þjálfi:\n\nThese \"women\" are either personified waves or jötnar. The island is also a setting in the poems \"Helgakviða Hundingsbana II\" and \"Oddrúnargrátr\", the saga \"Örvar-Odds saga\", in two skaldic kennings, and the aforementioned (see etymology section above) \"Prose Edda\" book \"Skáldskaparmál\".\n\nSince 1992 the island has been home to the noted chef Mogens Bay Esbensen. The artist Per Kirkeby owns a house and studio on the island.\n\n\n"}
{"id": "23584530", "url": "https://en.wikipedia.org/wiki?curid=23584530", "title": "Maple River (Michigan)", "text": "Maple River (Michigan)\n\nThe Maple River is the name of three rivers in the U.S. state of Michigan:\n\n\n"}
{"id": "27704329", "url": "https://en.wikipedia.org/wiki?curid=27704329", "title": "Marathon Large Igneous Province", "text": "Marathon Large Igneous Province\n\nThe Marathon Large Igneous Province is a Paleoproterozoic large igneous province along the southern Superior craton of Ontario, Canada, located around the northern margin of Lake Superior. It consists of three diabase dike swarms known as Marathon, Kapuskasing and Fort Frances. The Kapuskasing and Marathon dike swarms range in age from about 2,126 to 2,101 million years old while the Fort Frances dike swarm is between 2,076 and 2,067 million years old.\n\nA single, periodically active mantle plume was responsible for the creation of the Marathon Large Igneous Province due to the lack of apparent polar wander during the formation of the igneous province. The large magmatic event covers an area of at least and the entire large igneous province was constructed in 60 million years.\n"}
{"id": "19834219", "url": "https://en.wikipedia.org/wiki?curid=19834219", "title": "Masuyite", "text": "Masuyite\n\nMasuyite is a uranium/lead oxide mineral with formula Pb[(UO)O(OH)]·3HO.\n\nMasuyite was first described in 1947 for an occurrence in Katanga and named to honor Belgian geologist Gustave Masuy (1905–1945).\n\n"}
{"id": "613582", "url": "https://en.wikipedia.org/wiki?curid=613582", "title": "Mount Lao", "text": "Mount Lao\n\nMount Lao, or Laoshan () is a mountain located near the East China Sea on the southeastern coastline of the Shandong Peninsula in China. The mountain is culturally significant due to its long affiliation with Taoism and is often regarded as one of the \"cradles of Taoism\". It is the highest coastal mountain in China and the second highest mountain in Shandong, with the highest peak (Jufeng) reaching . The mountain lies about to the northeast of the downtown area of the City of Qingdao and is protected by the Qingdao Laoshan National Park that covers an area of 446 square kilometers.\n\nMount Lao consists of granite. The mountain's landforms were formed due to the action of glaciers during the Quaternary and erosion by meltwater released from the icecap that covered a large portion of Shandong during the late Pleistocene.\n\nLaoshan is known as one of the birthplaces of Taoism. It is the place where the Complete Perfection School of Taoism () developed. In 412 CE the Chinese Buddhist pilgrim Faxian landed near Laoshan on his return from India. In the course of history, the mountain has been known by various other names, which includes different spellings of \"Laoshan\" (劳山, 牢山) as well as entirely different names such as Mount Futang and Mount Ao (鰲山). The latter name was used by the Taoist Qiu Chuji who served as the top religious affairs official to Genghis Khan.\n\nIn the course of history, numerous palaces, Taoist temples, and nunneries have been constructed on Mount Lao. At the peak of Taoist worship, Mount Lao was home to about 1000 monks and nuns. However, many of these structures have not survived to the present. Major sites on Mount Lao are:\n\nThe largest temple complex on Mount Lao is that of the Temple of Supreme Purity (), a Taoist temple that was first built during the Northern Song dynasty with the present structures dating to the reign of the Wanli Emperor in the Ming dynasty. The temple is located near the coast, below Pantao Peak on the southeastern foot of Mount Lao and is hence also known as the Lower Temple (), The main structure of the temple is the Hall of the Three Pure Ones () with houses statues of the Taoist Trinity (the Grand Pure One, the Supreme Pure One, the Jade Pure One). It is flanked by the Three Emperors Hall (to the left, ) and the Three Officials Hall (to the left, ). The Three Emperors Hall enshrines statues of Fuxi, Shennong, and the Yellow Emperor. In the Three Officials Halls are the statues of the Three Gods (the God of Heaven, the God of Earth, and the God of Water) along with other statues, such as of the warrior god Xuan Wu and of Lei Gong, the God of Thunder. Two ancient Cypress trees in front of the Three Emperors Hall are said to have been planted during the Han Dynasty. Two old camellia trees are standing in front of the Three Officials Hall. On the walls outside of the Three Emperors Hall an imperial decree by Genghis Khan granting Daoism protection. A natural feature on the grounds of the temple is the Shenshui (Immortal Water) Spring that is fed by Mount Lao's considerable underground waters. The Qing dynasty writer Pu Songling is said to have resided in the Temple of Supreme Purity in his later years. His stories \"The Taoist Priest of Laoshan\" () and \"The Flower Nymphs\" () from the collection Strange Stories from a Chinese Studio are set on Mount Lao with the latter story specifically referring to peonies and camellias in the Temple of Supreme Purity.\n\nThe Temple of Great Purity () also known as the Upper Temple () is located on the southeastern slope of Mount Lao above the Temple of Supreme Purity. It was established during the Song Dynasty and rebuilt during the Yuan Dynasty, in the years 1297 to 1307. It is one of the oldest extant structures on Mount Lao. Like the Temple of Supreme Purity, the Temple of Great Purity features a spring. The spring in the Temple of Great Purity is called the \"Shengshuiyang (Ocean of Holy Water) Spring\".\n\nThe Longtan Waterfall () has a height of about 20 meters and is located to the south of the Temple of Great Purity.\n\nThe Temple of Supreme Peace () is located on the northern slope of Mount Lao. It was established during the Song Dynasty, but has been rebuilt several times.\n\nThe Hualou Temple () was established by the Daoist Liu Zhijian during the Yuan Dynasty (in 1325) and subsequently rebuilt during the Ming and Qing Dynasties, as well as during the Republican era.\n\nThe Huayan Temple () is the only Buddhist temple on Mount Lao and is located on the eastern slope of the mountain.\n\nTraditionally, 12 sceneries () on Mount Lao have been regarded as particularly beautiful:\n\n\nMount Lao is featured in many legends and local traditions. For example, the mountain is said to have been visited by the emperor Qin Shi Huang and Emperor Wu of Han, both hoping to meet immortals and gain immortality there.\n\nThe martial arts style of the Northern Praying Mantis is attributed to Wang Lang, who is said to have developed while living on Mount Lao. Wang Lang is commemorated by a contemporary stone statue on the mountain.\n\nMany gamblers visit Mount Lao for the famous tree located in the mountain, as it is said that three pats of the tree's trunk will bring instant luck for those seeking money.\n\nThe beer of the Tsingtao Brewery is brewed with Laoshan spring water.\n\nMount Lao is also famous for its green tea.\n\nLaoshan is a popular tourist site with a 5A government rating. During the 2012 Golden Week, it received about 176,000 visitors. There are six designated scenic areas on Mount Lao: Taiqing (太清景区), Chessboard Stone (棋盘石景区, Yangkou（仰口景区), Jufeng (巨峰景区), Beijiushui (北九水景区), and Hualou (华楼景区).\n\n"}
{"id": "56087100", "url": "https://en.wikipedia.org/wiki?curid=56087100", "title": "NOAA-20", "text": "NOAA-20\n\nNOAA-20, designated JPSS-1 prior to launch, is the first of the United States National Oceanic and Atmospheric Administration's latest generation of U.S. polar-orbiting, non-geosynchronous, environmental satellites called the Joint Polar Satellite System. NOAA-20 was launched on November 18, 2017 and joined the Suomi National Polar-orbiting Partnership satellite in the same orbit. NOAA-20 operates about 50 minutes ahead of Suomi NPP, allowing important overlap in observational coverage. Circling the Earth from pole-to-pole, it crosses the equator about 14 times daily, providing full global coverage twice a day. This will give meteorologists information on \"atmospheric temperature and moisture, clouds, sea-surface temperature, ocean color, sea ice cover, volcanic ash, and fire detection\" so as to enhance weather forecasting including hurricane tracking, post-hurricane recovery by detailing storm damage and mapping of power outages.\n\nThe project incorporates five instruments, and these are substantially upgraded since previous satellite equipment. The project's greater-detailed observations will provide better predictions and emphasize climate behavior in cases like El Niño and La Nina.\n\nThe project's satellite bus and Ozone Mapping and Profiler equipment, was designed by Ball Aerospace. The Visible Infrared Imaging Radiometer Suite and the Common Ground System were built by Raytheon, and the Cross-track Infrared Sounder was by Harris. The Advanced Technology Microwave Sounder and the Clouds and the Earth’s Radiant Energy System instrument were built by Northrop Grumman Aerospace Systems.\n\nThe NOAA-20 launch was delayed several times. When the contract was awarded in 2010, launch was scheduled for 2014. By 2011, launch had slipped to 2016, and by 2012 that had slipped to 2017. In August 2016, following environmental testing, launch slipped from January 20, 2017 to March 16, 2017 due to problems with ATMS and the ground system. In January 2017, launch was delayed from March 2017 to the fourth quarter of fiscal year 2017, or July to September for the same reasons. The launch was delayed from September 2017 to November 10, 2017 to provide extra time for engineers to complete testing of the spacecraft and electronics as well as the Advanced Technology Microwave Sounder (ATMS).\n\nIt also experienced several brief launch delays in the final weeks before launch. Originally scheduled to launch on November 10, 2017, it was delayed to the 14th following the discovery of a faulty battery on the Delta 2 launch vehicle. The launch was then delayed to November 15, 2017 due to boats being in the launch safety zone minutes before the launch and due to a bad reading on the first stage of the launch vehicle. It was delayed a 3rd time to November 18 due to high winds.\n\nNOAA-20 successfully launched on November 18, 2017. It represented the penultimate, and 99th consecutive successful launch of the Delta 2 rocket. It was launched along with 5 CubeSats that conducted research in \"3D-printed polymers for in-space manufacturing, weather data collection, bit flip memory testing, radar calibration and the effects of space radiation on electronic components\".\n\nBetween Nov 29, 2017, when ATMS produced its \"first light\" image and Jan 5, 2018 when VIIRS and OMPS produced theirs, the satellite went through activation, outgassing and decontamination on the path to operation.\n\nOn May 30, 2018, after six months of on-orbit checkout, NOAA declared the spacecraft fully operational.\n\nNOAA-20 Sensors/Instruments:\n"}
{"id": "50346935", "url": "https://en.wikipedia.org/wiki?curid=50346935", "title": "Oddgeir Bruaset", "text": "Oddgeir Bruaset\n\nOddgeir Bruaset (born 10 December 1944) is a Norwegian journalist and non-fiction writer.\n\nBruaset was born in Rauma, and was assigned with the Norwegian Broadcasting Corporation from 1971 to 2014. He is particularly known for hosting the television documentary series, \"Der ingen skulle tru at nokon kunne bu\", about people who live far off the beaten track. His books include \"Folket langs Storfjorden\" (two volumes, 1991 and 2004), \"Orkanen\" from 1992, \"Jostedalsbreen\" from 1996, and \"Sunnmøre og sunnmøringen\" from 1999. He won a Gullruten award in 2009, and the Gullruten honorary award in 2015. In 2009 he was awarded the King's Medal of Merit.\n"}
{"id": "3169278", "url": "https://en.wikipedia.org/wiki?curid=3169278", "title": "Phase angle (astronomy)", "text": "Phase angle (astronomy)\n\nPhase angle in astronomical observations is the angle between the light incident onto an observed object and the light reflected from the object. In the context of astronomical observations, this is usually the angle Sun-object-observer.\n\nFor terrestrial observations, \"Sun–object–Earth\" is often nearly the same thing as \"Sun–object–observer\", since the difference depends on the parallax, which in the case of observations of the Moon can be as much as 1°, or two full Moon diameters. With the development of space travel, as well as in hypothetical observations from other points in space, the notion of phase angle became independent of Sun and Earth.\n\nThe etymology of the term is related to the notion of planetary phases, since the brightness of an object and its appearance as a \"phase\" is the function of the phase angle.\n\nThe phase angle varies from 0° to 180°. The value of 0° corresponds to the position where the illuminator, the observer, and the object are collinear, with the illuminator and the observer on the same side of the object. This is known as astronomical opposition. The value of 180° is the position where the object is between the illuminator and the observer. Values less than 90° represent backscattering; values greater than 90° represent forward scattering.\n\nFor some objects, such as the Moon (see lunar phases), Venus and Mercury the phase angle (as seen from the Earth) covers the full 0–180° range. The superior planets cover shorter ranges. For example, for Mars the maximum phase angle is about 45°.\n\nThe brightness of an object is a function of the phase angle, which is generally smooth, except for the so-called opposition spike near 0°, which does not affect gas giants or bodies with pronounced atmospheres, and when the object becomes fainter as the angle approaches 180°. This relationship is referred to as the phase curve.\n\n\n"}
{"id": "47664587", "url": "https://en.wikipedia.org/wiki?curid=47664587", "title": "Photogeochemistry", "text": "Photogeochemistry\n\nPhotogeochemistry merges photochemistry and geochemistry into the study of light-induced chemical reactions that occur or may occur among natural components of Earth's surface. The first comprehensive review on the subject was published in 2017 by the chemist and soil scientist Timothy A Doane, but the term photogeochemistry appeared a few years earlier as a keyword in studies that described the role of light-induced mineral transformations in shaping the biogeochemistry of Earth; this indeed describes the core of photogeochemical study, although other facets may be admitted into the definition.\n\nThe context of a photogeochemical reaction is implicitly the surface of Earth, since that is where sunlight is available (although other sources of light such as chemiluminescence would not be strictly excluded from photogeochemical study). Reactions may occur among components of land such as rocks, soil and detritus; components of surface water such as sediment and dissolved organic matter; and components of the atmospheric boundary layer directly influenced by contact with land or water, such as mineral aerosols and gases. Visible and medium- to long-wave ultraviolet radiation is the main source of energy for photogeochemical reactions; wavelengths of light shorter than about 290 nm are completely absorbed by the present atmosphere, and are therefore practically irrelevant, except in consideration of atmospheres different from that of Earth today.\n\nPhotogeochemical reactions are limited to chemical reactions not facilitated by living organisms. The reactions comprising photosynthesis in plants and other organisms, for example, are not considered photogeochemistry, since the physiochemical context for these reactions is installed by the organism, and must be maintained in order for these reactions to continue (i.e. the reactions cease if the organism dies). In contrast, if a certain compound is produced by an organism, and the organism dies but the compound remains, this compound may still participate independently in a photogeochemical reaction even though its origin is biological (e.g. biogenic mineral precipitates or organic compounds released from plants into water).\n\nThe study of photogeochemistry is primarily concerned with naturally occurring materials, but may extend to include other materials, inasmuch as they are representative of, or bear some relation to, those found on Earth. For example, many inorganic compounds have been synthesized in the laboratory to study photocatalytic reactions. Although these studies are usually not undertaken in the context of environmental or Earth sciences, the study of such reactions is relevant to photogeochemistry if there is a geochemical implication (i.e. similar reactants or reaction mechanisms occur naturally). Similarly, photogeochemistry may also include photochemical reactions of naturally occurring materials that are not touched by sunlight, if there is the possibility that these materials may become exposed (e.g. deep soil layers uncovered by mining).\n\nExcept for several isolated instances, studies that fit the definition of photogeochemistry have not been explicitly specified as such, but have been traditionally categorized as photochemistry, especially at the time when photochemistry was an emerging field or new facets of photochemistry were being explored. Photogeochemical research, however, may be set apart in light of its specific context and implications, thereby bringing more exposure to this \"poorly explored area of experimental geochemistry\". Past studies that fit the definition of photogeochemistry may be designated retroactively as such.\n\nThe first efforts that can be considered photogeochemical research can be traced to the \"formaldehyde hypothesis\" of Adolf von Baeyer in 1870, in which formaldehyde was proposed to be the initial product of plant photosynthesis, formed from carbon dioxide and water through the action of light on a green leaf. This suggestion inspired numerous attempts to obtain formaldehyde \"in vitro\", which can retroactively be considered photogeochemical studies. Detection of organic compounds such as formaldehyde and sugars was reported by many workers, usually by exposure of a solution of carbon dioxide to light, typically a mercury lamp or sunlight itself. At the same time, many other workers reported negative results. One of the pioneer experiments was that of Bach in 1893, who observed the formation of lower uranium oxides upon irradiation of a solution of uranium acetate and carbon dioxide, implying the formation of formaldehyde. Some experiments included reducing agents such as hydrogen gas, and others detected formaldeyhde or other products in the absence of any additives, although the possibility was admitted that reducing power may have been produced from the decomposition of water during the experiment. In addition to the main focus on synthesis of formaldehyde and simple sugars, other light-assisted reactions were occasionally reported, such as the decomposition of formaldehyde and subsequent release of methane, or the formation of formamide from carbon monoxide and ammonia.\n\nIn 1912 Benjamin Moore summarized the main facet of photogeochemistry, that of inorganic photocatalysis: \"the inorganic colloid must possess the property of transforming sunlight, or some other form of radiant energy, into chemical energy.\" Many experiments, still focused on how plants assimilate carbon, did indeed explore the effect of a \"transformer\" (catalyst); some effective \"transformers\" were similar to naturally occurring minerals, including iron(III) oxide or colloidal iron hydroxide; cobalt carbonate, copper carbonate, nickel carbonate; and iron(II) carbonate. Working with an iron oxide catalyst, Baly concluded in 1930 that \"the analogy between the laboratory process and that in the living plant seems therefore to be complete,\" referring to his observation that in both cases, a photochemical reaction takes place on a surface, the activation energy is supplied in part by the surface and in part by light, efficiency decreases when the light intensity is too great, the optimal temperature of the reaction is similar to that of living plants, and efficiency increases from the blue to the red end of the light spectrum.\n\nAt this time, however, the intricate details of plant photosynthesis were still obscure, and the nature of photocatalysis in general was still actively being discovered; Mackinney in 1932 stated that \"the status of this problem [photochemical CO reduction] is extraordinarily involved.\" As in many emerging fields, experiments were largely empirical, but the enthusiasm surrounding this early work did lead to significant advances in photochemistry. The simple but challenging principle of transforming solar energy into chemical energy capable of performing a desired reaction remains the basis of application-based photocatalysis, most notably artificial photosynthesis (production of solar fuels).\n\nAfter several decades of experiments centered around the reduction of carbon dioxide, interest began to spread to other light-induced reactions involving naturally occurring materials. These experiments usually focused on reactions analogous to known biological processes, such as soil nitrification, for which the photochemical counterpart \"photonitrification\" was first reported in 1930.\n\nPhotogeochemical reactions may be classified based on thermodynamics and/or the nature of the materials involved. In addition, when ambiguity exists regarding an analogous reaction involving light and living organisms (phototrophy), the term \"photochemical\" may be used to distinguish a particular abiotic reaction from the corresponding photobiological reaction. For example, \"photooxidation of iron(II)\" can refer to either a biological process driven by light (phototrophic or photobiological iron oxidation) or a strictly chemical, abiotic process (photochemical iron oxidation). Similarly, an abiotic process that converts water to O under the action of light may be designated \"photochemical oxidation of water\" rather than simply \"photooxidation of water\", in order to distinguish it from photobiological oxidation of water potentially occurring in the same environment (by algae, for example).\n\nPhotogeochemical reactions are described by the same principles used to describe photochemical reactions in general, and may be classified similarly:\n\nAny reaction in the domain of photogeochemistry, either observed in the environment or studied in the laboratory, may be broadly classified according to the nature of the materials involved.\n\nDirect photogeochemical catalysts act by absorbing light and subsequently transferring energy to reactants.\n\nThe majority of observed photogeochemical reactions involve a mineral catalyst. Many naturally occurring minerals are semiconductors that absorb some portion of solar radiation. These semiconducting minerals are frequently transition metal oxides and sulfides and include abundant, well-known minerals such as hematite (FeO), magnetite (FeO), goethite and lepidocrocite (FeOOH), and pyrolusite (MnO). Radiation of energy equal to or greater than the band gap of a semiconductor is sufficient to excite an electron from the valence band to a higher energy level in the conduction band, leaving behind an electron hole (h); the resulting electron-hole pair is called an exciton. The excited electron and hole can reduce and oxidize, respectively, species having suitable redox potentials relative to the potentials of the valence and conduction bands. Semiconducting minerals with appropriate band gaps and appropriate band energy levels can catalyze a vast array of reactions, most commonly at mineral-water or mineral-gas interfaces.\n\nOrganic compounds such as \"bio-organic substances\" and humic substances are also able to absorb light and act as catalysts or sensitizers, accelerating photoreactions that normally occur slowly or facilitating reactions that might not normally occur at all.\n\nSome materials, such as certain silicate minerals, absorb little or no solar radiation, but may still participate in light-driven reactions by mechanisms other than direct transfer of energy to reactants.\n\nIndirect photocatalysis may occur via the production of a reactive species which then participates in another reaction. For example, photodegradation of certain compounds has been observed in the presence of kaolinite and montmorillonite, and this may proceed via the formation of reactive oxygen species at the surface of these clay minerals. Indeed, reactive oxygen species have been observed when soil surfaces are exposed to sunlight. The ability of irradiated soil to generate singlet oxygen was found to be independent of the organic matter content, and both the mineral and organic components of soil appear to contribute to this process. Indirect photolysis in soil has been observed to occur at depths of up to 2 mm due to migration of reactive species; in contrast, direct photolysis (in which the degraded compound itself absorbs light) was restricted to a \"photic depth\" of 0.2 to 0.4 mm. Like certain minerals, organic matter in solution, as well as particulate organic matter, may act as an indirect catalyst via formation of singlet oxygen which then reacts with other compounds.\n\nIndirect catalysts may also act through surface sensitization of reactants, by which species sorbed to a surface become more susceptible to photodegradation.\n\nStrictly speaking, the term \"catalysis\" should not be used unless it can be shown that the number of product molecules produced per number of active sites is greater than one; this is difficult to do in practice, although it is often assumed to be true if there is no loss in the photoactivity of the catalyst for an extended period of time. Reactions that are not strictly catalytic may be designated \"assisted photoreactions\". Furthermore, phenomena that involve complex mixtures of compounds (e.g. soil) may be hard to classify unless complete reactions (not just individual reactants or products) can be identified.\n\nThe great majority of photogeochemical research is performed in the laboratory, as it is easier to demonstrate and observe a particular reaction under controlled conditions. This includes confirming the identity of materials, designing reaction vessels, controlling light sources, and adjusting the reaction atmosphere. However, observation of natural phenomena often provides initial inspiration for further study. For example, during the 1970s it was generally agreed that nitrous oxide (NO) has a short residence time in the troposphere, although the actual explanation for its removal was unknown. Since NO does not absorb light at wavelengths greater than 280 nm, direct photolysis had been discarded as a possible explanation. It was then observed that light would decompose chloromethanes when they were absorbed on silica sand, and this occurred at wavelengths far above the absorption spectra for these compounds. The same phenomenon was observed for NO, leading to the conclusion that particulate matter in the atmosphere is responsible for the destruction of NO via surface-sensitized photolysis. Indeed, the idea of such a sink for atmospheric NO was supported by several reports of low concentrations of NO in the air above deserts, where there is a high amount of suspended particulate matter. As another example, the observation that the amount of nitrous acid in the atmosphere greatly increases during the day lead to insight into the surface photochemistry of humic acids and soils and an explanation for the original observation.\n\nThe following table lists some reported reactions that are relevant to photogeochemical study, including reactions that involve only naturally occurring compounds as well as complementary reactions that involve synthetic but related compounds. The selection of reactions and references given is merely illustrative and may not exhaustively reflect current knowledge, especially in the case of popular reactions such as nitrogen photofixation for which there is a large body of literature. Furthermore, although these reactions have natural counterparts, the probability of encountering optimal reaction conditions may be low in some cases; for example, most experimental work concerning CO photoreduction is intentionally performed in the absence of O, since O almost always suppresses the reduction of CO. In natural systems, however, it is uncommon to find an analogous context where CO and a catalyst are reached by light but there is no O present.\n"}
{"id": "46422201", "url": "https://en.wikipedia.org/wiki?curid=46422201", "title": "Racal suit", "text": "Racal suit\n\nA racal suit (also known as racal space suit) is a protective suit with powered air-purifying respirator (PAPR). It consists of a plastic suit and a battery-operated blower with HEPA filters that supplied filtered air to a positive-pressure hood (also known as racal hood). Racal suits were among the protective suits used by Aeromedical Isolation Team (AIT) to evacuate patients with highly infectious diseases for treatment.\n\nThe main body of the protective suit consists of a lightweight coverall made of polyvinyl chloride, rubber gloves, and rubber boots. Originally, the coverall was in a bright orange color, and the racal suit was known as the orange suit.\n\nThe hood is a separate component from the protective suit. The racal hood is a type of PAPR consisting of a transparent hood connected to a respirator, which is powered by a rechargeable battery. The respirator has three HEPA filters that are certified to remove 99.7% of particles of 0.03 to 3.0 microns in diameter. The filtered air is supplied at the rate of 170 L/min to the top of hood under positive pressure for breathing and cooling. The air is forced out through an air exhaust valve at the base of the hood. A two-way radio system is installed inside the hood for communication.\n\nOriginally, the hood was manufactured by Racal Health & Safety, Inc. located in Frederick, Maryland, the same city in which the AIT unit of the United States Army Medical Research Institute of Infectious Diseases was based. The division of Racal responsible for the suit's manufacture later became part of 3M, and the respirator product line was branded as 3M/Racal. The AIT later switched from using transparent bubble hoods to butyl rubber hoods.\n\nThe main purpose of the AIT was to evacuate a patient from the field to a specialized isolation unit. As part of their procedures, AIT members wore racal suits while transporting the patients. They were trained to take a bathroom break before suiting up, since the time they would be in the suits could be 1 hour and 45 minutes for a training session and 4 to 6 hours for an actual mission. The patient was placed in a mobile stretcher isolator during transit. After the patient was delivered to the isolation unit, the members would leave the unit and enter into an anteroom with an airlock. They were then sprayed with glutaraldehyde solution to disinfect before the suit was cut away and sent to an on-site incinerator for complete destruction.\n\nThe racal suit is similar to other positive pressure personnel suits such as Chemturion, in that there is an air supply to provide positive pressure to reduce the chance of airborne agents entering the suit. However, several components are different. The positive pressure section for the racal suit is only available at the hood. The air supply for racal suits comes from a battery-operated blower that makes the suit portable, whereas other suits must be connected to an air hose that is part of the building, such as in biosafety level 4 laboratories. The main body part of the racal suit is also more lightweight and can be disposed of by burning after use.\n\nRacal suits were used in films such as \"Outbreak\" in 1995. The term is also used in many literatures related to situations with infectious diseases, such as in \"The Hot Zone: A Terrifying True Story\", \"Infected\", and \"Executive Orders\".\n"}
{"id": "477175", "url": "https://en.wikipedia.org/wiki?curid=477175", "title": "Radiant energy", "text": "Radiant energy\n\nIn physics, and in particular as measured by radiometry, radiant energy is the energy of electromagnetic and gravitational radiation. As energy, its SI unit is the joule (J). The quantity of radiant energy may be calculated by integrating radiant flux (or power) with respect to time. The symbol \"Q\" is often used throughout literature to denote radiant energy (\"e\" for \"energetic\", to avoid confusion with photometric quantities). In branches of physics other than radiometry, electromagnetic energy is referred to using \"E\" or \"W\". The term is used particularly when electromagnetic radiation is emitted by a source into the surrounding environment. This radiation may be visible or invisible to the human eye.\n\nThe term \"radiant energy\" is most commonly used in the fields of radiometry, solar energy, heating and lighting, but is also sometimes used in other fields (such as telecommunications). In modern applications involving transmission of power from one location to another, \"radiant energy\" is sometimes used to refer to the electromagnetic waves \"themselves\", rather than their \"energy\" (a property of the waves). In the past, the term \"electro-radiant energy\" has also been used.\n\nThe term \"radiant energy\" also applies to gravitational radiation. For example, the first gravitational waves ever observed were produced by a black hole collision that emitted about 5.3 joules of gravitational-wave energy.\n\nBecause electromagnetic (EM) radiation can be conceptualized as a stream of photons, radiant energy can be viewed as photon energy – the energy carried by these photons. Alternatively, EM radiation can be viewed as an electromagnetic wave, which carries energy in its oscillating electric and magnetic fields. These two views are completely equivalent and are reconciled to one another in quantum field theory (see wave-particle duality).\n\nEM radiation can have various frequencies. The bands of frequency present in a given EM signal may be sharply defined, as is seen in atomic spectra, or may be broad, as in blackbody radiation. In the photon picture, the energy carried by each photon is proportional to its frequency. In the wave picture, the energy of a monochromatic wave is proportional to its intensity. This implies that if two EM waves have the same intensity, but different frequencies, the one with the higher frequency \"contains\" fewer photons, since each photon is more energetic.\n\nWhen EM waves are absorbed by an object, the energy of the waves is converted to heat (or converted to electricity in case of a photoelectric material). This is a very familiar effect, since sunlight warms surfaces that it irradiates. Often this phenomenon is associated particularly with infrared radiation, but any kind of electromagnetic radiation will warm an object that absorbs it. EM waves can also be reflected or scattered, in which case their energy is redirected or redistributed as well.\n\nRadiant energy is one of the mechanisms by which energy can enter or leave an open system. Such a system can be man-made, such as a solar energy collector, or natural, such as the Earth's atmosphere. In geophysics, most atmospheric gases, including the greenhouse gases, allow the Sun's short-wavelength radiant energy to pass through to the Earth's surface, heating the ground and oceans. The absorbed solar energy is partly re-emitted as longer wavelength radiation (chiefly infrared radiation), some of which is absorbed by the atmospheric greenhouse gases. Radiant energy is produced in the sun as a result of nuclear fusion.\n\nRadiant energy is used for radiant heating. It can be generated electrically by infrared lamps, or can be absorbed from sunlight and used to heat water. The heat energy is emitted from a warm element (floor, wall, overhead panel) and warms people and other objects in rooms rather than directly heating the air. Because of this, the air temperature may be lower than in a conventionally heated building, even though the room appears just as comfortable.\n\nVarious other applications of radiant energy have been devised. These include treatment and inspection, separating and sorting, medium of control, and medium of communication. Many of these applications involve a source of radiant energy and a detector that responds to that radiation and provides a signal representing some characteristic of the radiation. Radiant energy detectors produce responses to incident radiant energy either as an increase or decrease in electric potential or current flow or some other perceivable change, such as exposure of photographic film.\n\n"}
{"id": "11970483", "url": "https://en.wikipedia.org/wiki?curid=11970483", "title": "Rebaptism", "text": "Rebaptism\n\nRebaptism in Christianity is the baptism of a person who has previously been baptized, usually in association with a denomination that does not recognize the validity of the previous baptism. When a denomination rebaptizes members of another denomination, it is a sign of significant differences in theology. Churches that practice exclusive adult baptism, including Baptists, Churches of Christ and Christian Churches, rebaptize those who were baptized as infants because they do not consider infant baptism to be valid.\n\nRebaptism is generally associated with:\n\nIn the 4th century, controversy was provoked by the Donatist sect's practice of re-baptizing Christians who had renounced their faith under persecution. The mainstream church decided that the lapsi could not be rebaptized, because the sacrament of baptism was irrevocable, leaving an indelible mark on the soul of the baptized. Later, rebaptism of Arians was deemed necessary because Arians did not believe in the identical Holy Trinity as defined by the Council of Nicea and their baptism was therefore not in the name of the Trinity as understood by the Council.\n\nThe Catholic Church does not admit the possibility of rebaptism:1272. Incorporated into Christ by Baptism, the person baptized is configured to Christ. Baptism seals the Christian with the indelible spiritual mark (\"character\") of his belonging to Christ. No sin can erase this mark, even if sin prevents Baptism from bearing the fruits of salvation. Given once for all, Baptism cannot be repeated.The baptisms of those to be received into the Catholic Church from other Christian communities are held to be valid if administered using the Trinitarian formula. As the \"Catechism of the Catholic Church\" explains:1256. The ordinary ministers of Baptism are the bishop and priest and, in the Latin Church, also the deacon. In case of necessity, anyone, even a non-baptized person, with the required intention, can baptize, by using the Trinitarian baptismal formula. The intention required is to will to do what the Church does when she baptizes. The Church finds the reason for this possibility in the universal saving will of God and the necessity of Baptism for salvation...1284. In case of necessity, any person can baptize provided that he have the intention of doing that which the Church does and provided that he pours water on the candidate's head while saying: \"I baptize you in the name of the Father, and of the Son, and of the Holy Spirit.\"The \"1983 Code of Canon Law\" (1983 CIC) addresses cases in which the validity of a person's baptism is in doubt:Can. 869 §1. If there is a doubt whether a person has been baptized or whether baptism was conferred validly and the doubt remains after a serious investigation, baptism is to be conferred conditionally.§2. Those baptized in a non-Catholic ecclesial community must not be baptized conditionally unless, after an examination of the matter and the form of the words used in the conferral of baptism and a consideration of the intention of the baptized adult and the minister of the baptism, a serious reason exists to doubt the validity of the baptism.§3. If in the cases mentioned in §§1 and 2 the conferral or validity of the baptism remains doubtful, baptism is not to be conferred until after the doctrine of the sacrament of baptism is explained to the person to be baptized, if an adult, and the reasons of the doubtful validity of the baptism are explained to the person or, in the case of an infant, to the parents.In cases where a valid baptism is performed subsequent to an invalid attempt, it is held that only one baptism actually occurred, namely the valid one. Thus baptism is never repeated.\n\nSome have claimed the Orthodox Church members have rebaptized Catholics, but no evidence (besides rumors) has come to light. Greek Orthodox practice changed in 1755, when Patriarch Cyril V of Constantinople issued the \"Definition of the Holy Church of Christ Defending the Holy Baptism Given from God, and Spitting upon the Baptisms of the Heretics Which Are Otherwise Administered\", however, the Greek Orthodox does not now insist on rebaptizing Catholics.\n\nLatter Day Saints practice rebaptism, as they do not accept the doctrine of the Trinity, in whose name mainstream Christian baptisms are conducted. They also believe they are the only church with the priesthood authority to perform saving ordinances.\n\nJehovah's Witnesses do not recognize previous baptisms conducted by any other denomination.\n\n"}
{"id": "164483", "url": "https://en.wikipedia.org/wiki?curid=164483", "title": "Scattering", "text": "Scattering\n\nScattering is a general physical process where some forms of radiation, such as light, sound, or moving particles, are forced to deviate from a straight trajectory by one or more paths due to localized non-uniformities in the medium through which they pass. In conventional use, this also includes deviation of reflected radiation from the angle predicted by the law of reflection. Reflections that undergo scattering are often called \"diffuse reflections\" and unscattered reflections are called \"specular\" (mirror-like) reflections. \n\nScattering may also refer to particle-particle collisions between molecules, atoms, electrons, photons and other particles. Examples include: cosmic ray scattering in the Earth's upper atmosphere; particle collisions inside particle accelerators; electron scattering by gas atoms in fluorescent lamps; and neutron scattering inside nuclear reactors. \n\nThe types of non-uniformities which can cause scattering, sometimes known as \"scatterers\" or \"scattering centers\", are too numerous to list, but a small sample includes particles, bubbles, droplets, density fluctuations in fluids, crystallites in polycrystalline solids, defects in monocrystalline solids, surface roughness, cells in organisms, and textile fibers in clothing. The effects of such features on the path of almost any type of propagating wave or moving particle can be described in the framework of scattering theory.\n\nSome areas where scattering and scattering theory are significant include radar sensing, medical ultrasound, semiconductor wafer inspection, polymerization process monitoring, acoustic tiling, free-space communications and computer-generated imagery. Particle-particle scattering theory is important in areas such as particle physics, atomic, molecular, and optical physics, nuclear physics and astrophysics.\n\nWhen radiation is only scattered by one localized scattering center, this is called \"single scattering\". It is very common that scattering centers are grouped together; in such cases, radiation may scatter many times, in what is known as \"multiple scattering\". The main difference between the effects of single and multiple scattering is that single scattering can usually be treated as a random phenomenon, whereas multiple scattering, somewhat counterintuitively, can be modeled as a more deterministic process because the combined results of a large number of scattering events tend to average out. Multiple scattering can thus often be modeled well with diffusion theory.\n\nBecause the location of a single scattering center is not usually well known relative to the path of the radiation, the outcome, which tends to depend strongly on the exact incoming trajectory, appears random to an observer. This type of scattering would be exemplified by an electron being fired at an atomic nucleus. In this case, the atom's exact position relative to the path of the electron is unknown and would be unmeasurable, so the exact trajectory of the electron after the collision cannot be predicted. Single scattering is therefore often described by probability distributions.\n\nWith multiple scattering, the randomness of the interaction tends to be averaged out by the large number of scattering events, so that the final path of the radiation appears to be a deterministic distribution of intensity. This is exemplified by a light beam passing through thick fog. Multiple scattering is highly analogous to diffusion, and the terms \"multiple scattering\" and \"diffusion\" are interchangeable in many contexts. Optical elements designed to produce multiple scattering are thus known as \"diffusers\". Coherent backscattering, an enhancement of backscattering that occurs when coherent radiation is multiply scattered by a random medium, is usually attributed to weak localization.\n\nNot all single scattering is random, however. A well-controlled laser beam can be exactly positioned to scatter off a microscopic particle with a deterministic outcome, for instance. Such situations are encountered in radar scattering as well, where the targets tend to be macroscopic objects such as people or aircraft.\n\nSimilarly, multiple scattering can sometimes have somewhat random outcomes, particularly with coherent radiation. The random fluctuations in the multiply scattered intensity of coherent radiation are called speckles. Speckle also occurs if multiple parts of a coherent wave scatter from different centers. In certain rare circumstances, multiple scattering may only involve a small number of interactions such that the randomness is not completely averaged out. These systems are considered to be some of the most difficult to model accurately.\n\nThe description of scattering and the distinction between single and multiple scattering are tightly related to wave–particle duality.\n\n\"Scattering theory\" is a framework for studying and understanding the scattering of waves and particles. Prosaically, wave scattering corresponds to the collision and scattering of a wave with some material object, for instance sunlight scattered by rain drops to form a rainbow. Scattering also includes the interaction of billiard balls on a table, the Rutherford scattering (or angle change) of alpha particles by gold nuclei, the Bragg scattering (or diffraction) of electrons and X-rays by a cluster of atoms, and the inelastic scattering of a fission fragment as it traverses a thin foil. More precisely, scattering consists of the study of how solutions of partial differential equations, propagating freely \"in the distant past\", come together and interact with one another or with a boundary condition, and then propagate away \"to the distant future\".\n\nElectromagnetic waves are one of the best known and most commonly encountered forms of radiation that undergo scattering. Scattering of light and radio waves (especially in radar) is particularly important. Several different aspects of electromagnetic scattering are distinct enough to have conventional names. Major forms of elastic light scattering (involving negligible energy transfer) are Rayleigh scattering and Mie scattering. Inelastic scattering includes Brillouin scattering, Raman scattering, inelastic X-ray scattering and Compton scattering.\n\nLight scattering is one of the two major physical processes that contribute to the visible appearance of most objects, the other being absorption. Surfaces described as \"white\" owe their appearance to multiple scattering of light by internal or surface inhomogeneities in the object, for example by the boundaries of transparent microscopic crystals that make up a stone or by the microscopic fibers in a sheet of paper. More generally, the gloss (or lustre or sheen) of the surface is determined by scattering. Highly scattering surfaces are described as being dull or having a matte finish, while the absence of surface scattering leads to a glossy appearance, as with polished metal or stone. \n\nSpectral absorption, the selective absorption of certain colors, determines the color of most objects with some modification by elastic scattering. The apparent blue color of veins in skin is a common example where both spectral absorption and scattering play important and complex roles in the coloration. Light scattering can also create color without absorption, often shades of blue, as with the sky (Rayleigh scattering), the human blue iris, and the feathers of some birds (Prum et al. 1998). However, resonant light scattering in nanoparticles can produce many different highly saturated and vibrant hues, especially when surface plasmon resonance is involved (Roqué et al. 2006).\n\nModels of light scattering can be divided into three domains based on a dimensionless size parameter, \"α\" which is defined as:\nwhere π\"D\" is the circumference of a particle and \"λ\" is the wavelength of incident radiation. Based on the value of \"α\", these domains are:\n\nRayleigh scattering is a process in which electromagnetic radiation (including light) is scattered by a small spherical volume of variant refractive index, such as a particle, bubble, droplet, or even a density fluctuation. This effect was first modeled successfully by Lord Rayleigh, from whom it gets its name. In order for Rayleigh's model to apply, the sphere must be much smaller in diameter than the wavelength (\"λ\") of the scattered wave; typically the upper limit is taken to be about 1/10 the wavelength. In this size regime, the exact shape of the scattering center is usually not very significant and can often be treated as a sphere of equivalent volume. The inherent scattering that radiation undergoes passing through a pure gas is due to microscopic density fluctuations as the gas molecules move around, which are normally small enough in scale for Rayleigh's model to apply. This scattering mechanism is the primary cause of the blue color of the Earth's sky on a clear day, as the shorter blue wavelengths of sunlight passing overhead are more strongly scattered than the longer red wavelengths according to Rayleigh's famous 1/\"λ\" relation. Along with absorption, such scattering is a major cause of the attenuation of radiation by the atmosphere. The degree of scattering varies as a function of the ratio of the particle diameter to the wavelength of the radiation, along with many other factors including polarization, angle, and coherence.\n\nFor larger diameters, the problem of electromagnetic scattering by spheres was first solved by Gustav Mie, and scattering by spheres larger than the Rayleigh range is therefore usually known as Mie scattering. In the Mie regime, the shape of the scattering center becomes much more significant and the theory only applies well to spheres and, with some modification, spheroids and ellipsoids. Closed-form solutions for scattering by certain other simple shapes exist, but no general closed-form solution is known for arbitrary shapes.\n\nBoth Mie and Rayleigh scattering are considered elastic scattering processes, in which the energy (and thus wavelength and frequency) of the light is not substantially changed. However, electromagnetic radiation scattered by moving scattering centers does undergo a Doppler shift, which can be detected and used to measure the velocity of the scattering center/s in forms of techniques such as lidar and radar. This shift involves a slight change in energy.\n\nAt values of the ratio of particle diameter to wavelength more than about 10, the laws of geometric optics are mostly sufficient to describe the interaction of light with the particle, and at this point the interaction is not usually described as scattering.\n\nFor modeling of scattering in cases where the Rayleigh and Mie models do not apply such as irregularly shaped particles, there are many numerical methods that can be used. The most common are finite-element methods which solve Maxwell's equations to find the distribution of the scattered electromagnetic field. Sophisticated software packages exist which allow the user to specify the refractive index or indices of the scattering feature in space, creating a 2- or sometimes 3-dimensional model of the structure. For relatively large and complex structures, these models usually require substantial execution times on a computer.\n\n\n"}
{"id": "26655994", "url": "https://en.wikipedia.org/wiki?curid=26655994", "title": "Southern African Power Pool", "text": "Southern African Power Pool\n\nThe Southern African Power Pool (SAPP) is a cooperation of the national electricity companies in Southern Africa under the auspices of the Southern African Development Community (SADC). The members of SAPP have created a common power grid between their countries and a common market for electricity in the SADC region. SAPP was founded in 1995.\n\nThe Southern African Power Pool has many long-term goals it wishes to achieve. \nMain goals include increasing the accessibility of electricity to rural communities, better the relationships between the involved countries, create strategies that will support sustainable development priorities, and to co-ordinate the planning of electric power.\n\nAlong with industrial productivity, electricity generation can assist in the Southern African Development Community (SADC)’s mandate of poverty elimination across Southern Africa. Only 5% of rural areas in Southern Africa have access to electricity, which prevents their ability to control sanitation, clean water, and food. In 2010, SADC passed the Regional Energy Access Strategy and Action Plan, which aims to combine regional energy resources as a means of ensuring the entire SADC region has access to affordable, sustainable electricity. The plan’s goal is to within ten years reduce half the number of people in the region without access to energy, and then halving it every five years until the region has universal access.\n\nThe most recent developments to the Southern African Power Pool came between the years of 1995 and 2005, where multiple interconnections were added. In 1995, an interconnector that ran from South Africa to Zimbabwe was completed, a Mozambique-South Africa interconnector was fixed in 1997, a Mozambique-Zimbabwe interconnector was finished, and most recently two power lines connecting South Africa to Maputo were finished.\n\nCurrently being planned for future development is a Zambia-Tanzania power line, a Mozambique-Malawi power line, the fixing of the old Zambia power line, and the construction of a third Inga hydropower station by means of the Westcor Project.\n\nMember Status\n\nThe Botswansa Power Corporation, Electricidade de Mocambique, ESKOM, Lesotho Electricity Corporation, NAMPOWER, Societe Nationale d'Electricitite, Swaziland Electricity Board, Zesco Limited, and the Zimbabwe Electricity Supply Authority are operating members of the Power Pool. The Electricity Supply Corporation of Malawi, Empresa Nacional de Electricidade, and the Tanzania Electricity Supply Company Ltd are the non-operating members of the Power Pool. The Copperbelt Energy Corporation is the only Independent Transmission Company of the Power Pool.\n\nPower trading between regions started in 1950 after the signed agreement between the Democratic Republicans of Congo and Zambia. Other parts of the continent also started a power trading business until 1995. In 1995 SAPP was founded and it was the first and advanced power pool in Africa. Since then the SAPP influenced the energy market and started competitive energy markets such as a day-ahead market (DAM). Every year, each member donates a certain amount of money to the group as pre-determined in the document that set up the Power Pool.\nToday, most of the money that funds the Pool comes from donors, such as the World Bank and the Development Bank of Southern Africa.\n\nDespite being the most advanced power plant ever developed in Africa, the SAPP has some challenges and shortcomings. Its takes money to keep the plant running and the SAPP does not have the funds to create new investments. The SAPP also suffered from lack of infrastructure across the continent along with lack of maintenance for the existing infrastructure. There is not enough production because of the additional power that began running out by 2007. One major challenge that the Power Pool encounters is the establishment of the dependable power supply in a location that lacks the surplus of invention ability and low costs. Another significant challenge that the SAPP is confronted with is the skills development and preservation of qualified workers. The Southern African Power Pool \nfaces the problem that power pools can only be sustained in areas with developed grid interconnections, an already decent amount of generating capacity that could be difficult for some other countries to meet, a legal framework that the members can agree to, and most importantly trust between the members. As of right now the legal documents and framework that hold the Southern African Power Plant is from the PEAC legal framework which includes the Intergovernmental MOU, and the Inter-Utility MOU.\n\nDespite recent challenges, the SAPP have long committed goals that can help in its expansion and company’s growth. The following plans are investment projects, interconnected grid, electricity access, and competitive market. Investment projects that creates new revenue includes hydropower and clean coal power plant alternative over nuclear. They also plan to improve SAPP central grid to improve the link between North and South Africa regarding to the new alternative power plant, hydropower and clean coal. Interconnected grids can expand SAPP activities. Along with the current SAPP members, there are future plans to involve other areas such as Tanzania, Malawi and Angola. Electricity access would expand the use of electricity to more areas in order to make it accessible to new borders. Lastly, strengthen and regulating competitive markets plan that sets up new rules that help meets consumers and investors satisfaction.\n\nThere are five African Power Pools, SAPP (Southern African Power Pool), EAPP (East African Power Pool), CAPP (Central African Power Pool), WAPP (West African Power Pool) & COMELC. \n\nThe DRC is the only member that is part of three pools, SAPP, EAPP and CAPP\nBurundi is part of EAPP and CAPP. Tanzania is a member of SAPP and EAPP. Guinea is part of CAPP and WAPP\n\n"}
{"id": "36228195", "url": "https://en.wikipedia.org/wiki?curid=36228195", "title": "St Mary the Virgin, Mortlake", "text": "St Mary the Virgin, Mortlake\n\nSt Mary the Virgin, Mortlake is a parish church in Mortlake, in the London Borough of Richmond upon Thames. It is part of the Church of England and the Anglican Communion. The rector is The Revd Canon Dr Ann Nickson.\n\nThe building, on Mortlake High Street, London SW14, dates from 1543 and is Grade II* listed.\n\nThe first chapel in Mortlake, founded in 1348, stood on the river side of the High Street, on a site later occupied by Mortlake Brewery. The only surviving relic is a 15th-century font presented to this church by Archbishop Bourchier (c.1404–86).\n\nThe present churchyard and church were given to the parish by King Henry VIII in 1543, an event commemorated by a stone in the west front of the tower. Its inscription \"VIVAT RH8 1543\" is dismissed by Cherry and Pevsner as \"bogus\".\n\nThe 1543 building has undergone many alterations and enlargements during its long history and, of the original Tudor church, only the tower remains. The belfry and the cupola are a distinctive feature of the tower which appears as a landmark in many historic prints and pictures of the Thames bank. The current appearance of the church is mostly the work of local architect Sir Arthur Blomfield, who built the chancel in 1885; his firm built the nave in 1905.\n\nThe vestry house dates from 1670. It was restored in 1979/80.\n\nThe church's pulpit was installed in 1902 in memory of Albert Shadwell Shutt, who had been the church's vicar from 1866 to 1896.\n\nThe earliest surviving tomb in the churchyard is that of the astrologer John Partridge, who died in 1715. Memorials to other famous people include a British Prime Minister, Henry Addington, 1st Viscount Sidmouth (1757–1844) and three Lord Mayors of London. A memorial to John Dee (1527–1609), who lived opposite the church and is buried in an unmarked spot beneath the chancel, was unveiled in June 2013.\n\nTogether with Christ Church, East Sheen and All Saints' Church, East Sheen, St Mary's forms the parish of Mortlake with East Sheen. The parish publishes a monthly magazine, \"Parish Link\". The church stands in the Central and Liberal traditions of the Church of England. Services are held on Tuesday, Saturday and Sunday mornings.\n\nMortlake Quiet Gardens are based around the landscaped churchyard and are affiliated to The Quiet Garden Trust.\n\n"}
{"id": "1201321", "url": "https://en.wikipedia.org/wiki?curid=1201321", "title": "Superposition principle", "text": "Superposition principle\n\nThe superposition principle, also known as superposition property, states that, for all linear systems, the net response caused by two or more stimuli is the sum of the responses that would have been caused by each stimulus individually. So that if input \"A\" produces response \"X\" and input \"B\" produces response \"Y\" then input (\"A\" + \"B\") produces response (\"X\" + \"Y\").\n\nThe homogeneity and additivity properties together are called Superposition. A linear function is one that satisfies the properties of superposition. It is defined as \n\nThis principle has many applications in physics and engineering because many physical systems can be modeled as linear systems. For example, a beam can be modeled as a linear system where the input stimulus is the load on the beam and the output response is the deflection of the beam. The importance of linear systems is that they are easier to analyze mathematically; there is a large body of mathematical techniques, frequency domain linear transform methods such as Fourier, Laplace transforms, and linear operator theory, that are applicable. Because physical systems are generally only approximately linear, the superposition principle is only an approximation of the true physical behaviour.\n\nThe superposition principle applies to \"any\" linear system, including algebraic equations, linear differential equations, and systems of equations of those forms. The stimuli and responses could be numbers, functions, vectors, vector fields, time-varying signals, or any other object that satisfies certain axioms. Note that when vectors or vector fields are involved, a superposition is interpreted as a vector sum.\n\nBy writing a very general stimulus (in a linear system) as the superposition of stimuli of a specific, simple form, often the response becomes easier to compute.\n\nFor example, in Fourier analysis, the stimulus is written as the superposition of infinitely many sinusoids. Due to the superposition principle, each of these sinusoids can be analyzed separately, and its individual response can be computed. (The response is itself a sinusoid, with the same frequency as the stimulus, but generally a different amplitude and phase.) According to the superposition principle, the response to the original stimulus is the sum (or integral) of all the individual sinusoidal responses.\n\nAs another common example, in Green's function analysis, the stimulus is written as the superposition of infinitely many impulse functions, and the response is then a superposition of impulse responses.\n\nFourier analysis is particularly common for waves. For example, in electromagnetic theory, ordinary light is described as a superposition of plane waves (waves of fixed frequency, polarization, and direction). As long as the superposition principle holds (which is often but not always; see nonlinear optics), the behavior of any light wave can be understood as a superposition of the behavior of these simpler plane waves.\n\nWaves are usually described by variations in some parameter through space and time—for example, height in a water wave, pressure in a sound wave, or the electromagnetic field in a light wave. The value of this parameter is called the amplitude of the wave, and the wave itself is a function specifying the amplitude at each point.\n\nIn any system with waves, the waveform at a given time is a function of the sources (i.e., external forces, if any, that create or affect the wave) and initial conditions of the system. In many cases (for example, in the classic wave equation), the equation describing the wave is linear. When this is true, the superposition principle can be applied.\nThat means that the net amplitude caused by two or more waves traversing the same space is the sum of the amplitudes that would have been produced by the individual waves separately. For example, two waves traveling towards each other will pass right through each other without any distortion on the other side. (See image at top.)\n\nWith regard to wave superposition, Richard Feynman wrote:\nOther authors elaborate:\nYet another source concurs:\nThe phenomenon of interference between waves is based on this idea. When two or more waves traverse the same space, the net amplitude at each point is the sum of the amplitudes of the individual waves. In some cases, such as in noise-cancelling headphones, the summed variation has a smaller amplitude than the component variations; this is called \"destructive interference\". In other cases, such as in a line array, the summed variation will have a bigger amplitude than any of the components individually; this is called \"constructive interference\".\n\nIn most realistic physical situations, the equation governing the wave is only approximately linear. In these situations, the superposition principle only approximately holds. As a rule, the accuracy of the approximation tends to improve as the amplitude of the wave gets smaller. For examples of phenomena that arise when the superposition principle does not exactly hold, see the articles nonlinear optics and nonlinear acoustics.\n\nIn quantum mechanics, a principal task is to compute how a certain type of wave propagates and behaves. The wave is described by a wave function, and the equation governing its behavior is called the Schrödinger equation. A primary approach to computing the behavior of a wave function is to write it as a superposition (called \"quantum superposition\") of (possibly infinitely many) other wave functions of a certain type—stationary states whose behavior is particularly simple. Since the Schrödinger equation is linear, the behavior of the original wave function can be computed through the superposition principle this way.\n\nThe projective nature of quantum-mechanical-state space makes an important difference: it does not permit superposition of the kind that is the topic of the present article. A quantum mechanical state is a \"ray\" in projective Hilbert space, not a \"vector\". The sum of two rays is undefined. To obtain the relative phase, we must decompose or split the ray into components\nwhere the formula_4 and the formula_5 belongs to an orthonormal basis set. The equivalence class of formula_6 allows a well-defined meaning to be given to the relative phases of the formula_7.\n\nThere are some likenesses between the superposition presented in the main on this page, and quantum superposition. Nevertheless, on the topic of quantum superposition, Kramers writes: \"The principle of [quantum] superposition ... has no analogy in classical physics.\" According to Dirac: \"\"the superposition that occurs in quantum mechanics is of an essentially different nature from any occurring in the classical theory\" [italics in original].\"\n\nA common type of boundary value problem is (to put it abstractly) finding a function \"y\" that satisfies some equation\nwith some boundary specification\nFor example, in Laplace's equation with Dirichlet boundary conditions, \"F\" would be the Laplacian operator in a region \"R\", \"G\" would be an operator that restricts \"y\" to the boundary of \"R\", and \"z\" would be the function that \"y\" is required to equal on the boundary of \"R\".\n\nIn the case that \"F\" and \"G\" are both linear operators, then the superposition principle says that a superposition of solutions to the first equation is another solution to the first equation:\nwhile the boundary values superpose:\nUsing these facts, if a list can be compiled of solutions to the first equation, then these solutions can be carefully put into a superposition such that it will satisfy the second equation. This is one common method of approaching boundary value problems.\n\nConsider a simple linear system :<br>\nformula_12<br>\nBy superposition principle, the system can be decomposed into<br>\nformula_13 <br>\nformula_14 <br>\nwith<br>\nformula_15\nSuperposition principle is only available for linear systems. However, the Additive state decomposition can be applied not only to linear systems but also nonlinear systems. Next, consider a nonlinear system<br>\nformula_16<br>\nwhere formula_17 is a nonlinear function. By the additive state decomposition, the system can be ‘additively’ decomposed into<br>\nformula_18 <br>\nformula_19 <br>\nwith<br>\nformula_15<br>\nThis decomposition can help to simplify controller design.\n\n\nAccording to Léon Brillouin, the principle of superposition was first stated by Daniel Bernoulli in 1753: \"The general motion of a vibrating system is given by a superposition of its proper vibrations.\" The principle was rejected by Leonhard Euler and then by Joseph Lagrange. Later it became accepted, largely through the work of Joseph Fourier.\n\n\n"}
{"id": "242228", "url": "https://en.wikipedia.org/wiki?curid=242228", "title": "The Language Instinct", "text": "The Language Instinct\n\nThe Language Instinct is a 1994 book by Steven Pinker, written for a general audience. Pinker argues that humans are born with an innate capacity for language. He deals sympathetically with Noam Chomsky's claim that all human language shows evidence of a universal grammar, but dissents from Chomsky's skepticism that evolutionary theory can explain the human language instinct.\n\nPinker criticizes a number of common ideas about language, for example that children must be taught to use it, that most people's grammar is poor, that the quality of language is steadily declining, that language has a heavy influence on a person's possible range of thoughts (the Sapir–Whorf hypothesis), and that nonhuman animals have been taught language (see Great Ape language). Pinker sees language as an ability unique to humans, produced by evolution to solve the specific problem of communication among social hunter-gatherers. He compares language to other species' specialized adaptations such as spiders' web-weaving or beavers' dam-building behavior, calling all three \"instincts\".\n\nBy calling language an instinct, Pinker means that it is not a human invention in the sense that metalworking and even writing are. While only some human cultures possess these technologies, all cultures possess language. As further evidence for the universality of language, Pinker—mainly relying on the work of Derek Bickerton—notes that children spontaneously invent a consistent grammatical speech (a creole) even if they grow up among a mixed-culture population speaking an informal trade pidgin with no consistent rules. Deaf babies \"babble\" with their hands as others normally do with voice, and spontaneously invent sign languages with true grammar rather than a crude \"me Tarzan, you Jane\" pointing system. Language (speech) also develops in the absence of formal instruction or active attempts by parents to correct children's grammar. These signs suggest that rather than being a human invention, language is an innate human ability. Pinker also distinguishes language from humans' general reasoning ability, emphasizing that it is not simply a mark of advanced intelligence but rather a specialized \"mental module\". He distinguishes the linguist's notion of grammar, such as the placement of adjectives, from formal rules such as those in the American English writing style guide. He argues that because rules like \"a preposition is not a proper word to end a sentence with\" must be explicitly taught, they are irrelevant to actual communication and should be ignored. \n\nPinker attempts to trace the outlines of the language instinct by citing his own studies of language acquisition in children, and the works of many other linguists and psychologists in multiple fields, as well as numerous examples from popular culture. He notes, for instance, that specific types of brain damage cause specific impairments of language such as Broca's aphasia or Wernicke's aphasia, that specific types of grammatical construction are especially hard to understand, and that there seems to be a critical period in childhood for language development just as there is a critical period for vision development in cats. Much of the book refers to Chomsky's concept of a universal grammar, a meta-grammar into which all human languages fit. Pinker explains that a universal grammar represents specific structures in the human brain that recognize the general rules of other humans' speech, such as whether the local language places adjectives before or after nouns, and begin a specialized and very rapid learning process not explainable as reasoning from first principles or pure logic. This learning machinery exists only during a specific critical period of childhood and is then disassembled for thrift, freeing resources in an energy-hungry brain.\n\nPinker's assumptions about the innateness of language have been challenged; English linguist Geoffrey Sampson claims that \"either the logic is fallacious, or the factual data are incorrect (or, sometimes, both)\".\n\nRichard Webster, writing in \"Why Freud Was Wrong\" (1995), concludes that Pinker argues cogently that the human capacity for language is part of the genetic endowment associated with the evolution through natural selection of specialised neural networks within the brain, and that its attack on the 'standard social science model' of human nature is effective. Webster accepts Pinker's argument that, for ideological motives, twentieth-century social scientists have minimized the extent to which human nature is influenced by genetics. However, Webster finds Pinker's speculation about other specialized neural networks that may have evolved within the human brain—such as \"intuitive mechanics\" and \"intuitive biology\"—to be questionable, and believes that there is a danger that they will be treated by others as science. Webster believes that such speculations strengthen supporters of extreme genetic determinism.\n\n"}
{"id": "40522430", "url": "https://en.wikipedia.org/wiki?curid=40522430", "title": "The Paradisus Londinensis", "text": "The Paradisus Londinensis\n\nThe Paradisus Londonensis (full title The Paradisus Londonensis : or Coloured Figures of Plants Cultivated in the Vicinity of the Metropolis) is a book dated 1805–1808. It consists of coloured illustrations of 117 plants drawn by William Hooker, with explanatory text by Richard Anthony Salisbury.\n\n\"The Paradisus Londinensis\" was constructed as two volumes, each of two parts. The plates were in one part, the text in the other. The title page of the first volume and part bears the date 1805 and identifies the illustrator and publisher as Hooker. The title page of the second part identifies the author of the text as Salisbury. It has often been catalogued as 1805–1807, although some later plates are dated 1808. The International Plant Names Index dates the parts as follows:\n\n\"The Paradisus Londinensis\" (abbreviated to \"Parad. Lond.\" in botanical citations) is a significant source of botanical names, with around 150 attributed to Salisbury on the basis of this publication. Some have been superseded, but others are still in use. The names include:\n\n\"The Paradisus Londinensis\" preserves a record of a dispute between Salisbury and James Edward Smith which had (and still has) consequences for the acceptance of botanical names first published by Salisbury, in this work and others. Smith and Salisbury had become friends while studying at the University of Edinburgh. Later in life, in 1802, they quarrelled. Smith was a strong supporter of Linnaeus's \"systema sexuale\" (sexual system) for classifying plants. He had bought Linnaeus's entire collection of books, manuscripts and specimens, and founded the Linnean Society in 1788. Salisbury on the other hand was a supporter of the natural system of classification, in particular that of Antoine Laurent de Jussieu, published in \"Genera Plantarum\" in 1789 – the system used in \"Paradisus Londinensis\".\n\nIn his 1807 work, \"An introduction to physiological and systematical botany\", Smith had used newly discovered plants from the west coast of British Columbia, Canada (plants which he did not name) to support the view that the tepals of lilioid monocots were actually sepals, since their flowers had what Smith regarded as six internal petals. In \"The Paradisus Londinensis\", in the notes to number 98, dated 1 March 1808, Salisbury named these plants as the genus \"Hookera\" with two species \"H. coronaria\" and \"H. pulchella\". (The latter is illustrated as number 117.) Salisbury pointed out that the supposed internal petals were actually stamens (three sterile ones and the bases of three normal ones). He also disputed Smith's association of the plant with \"Agapanthus\", placing it instead with \"Allium\". He says \"I regret much to dissent so often from the celebrated lecturer [Smith]\".\n\nShortly afterwards in 1808, Smith named a moss genus \"Hookeria\" and read to the Linnean Society a formal description of a new genus, based on the same species as Salisbury's \"Hookera coronaria\", naming the genus \"Brodiaea\" in honour of Scottish botanist James Brodie. (The presentation was not published until 1810.) George Boulger, writing in the \"Dictionary of National Biography\", says that Smith's actions were deliberately intended to deprive Salisbury of credit for the genus \"Hookera\". In the text for figure 117, dated 1 September 1808, Salisbury says that \"Smith, to suit his own purpose, was utterly silent\" on the prior naming of the genus by Salisbury, and that Smith's \"multiplied acts of injustice to me whether open or concealed, I sincerely forgive.\"\n\nIf it was Smith's deliberate intention to suppress Salisbury's botanical names by giving a moss a confusingly similar name and by renaming \"Hookera\" to \"Brodiaea\", he was, initially at least, successful. James Britten, writing much later in 1886, argued that the then established name \"Brodiaea\" should be replaced by \"Hookera\", which had priority. (He also showed how many of Salisbury's other names had been ignored.) However, Smith's \"Hookeria\" and \"Brodiaea\" had become so widely used that they were made conserved names, and \"Hookera\" was not reinstated. Salisbury's epithet \"coronaria\" in his \"Hookera coronaria\" is still used in the combination \"Brodiaea coronaria\". , \"Hookera pulchella\" is considered as a synonym, either in full or in part, for \"Dichelostemma congestum\" or \"Dichelostemma capitatum\", and the epithet \"pulchella\" is not used, although Britten had argued for its priority.\n\nThe 117 plants illustrated in \"The Paradisus Londinensis\" are listed below. Salisbury's original orthography is given in brackets where different from modern usage. Accepted names are taken from The Plant List, unless otherwise referenced. An asterisk indicates that no accepted name has been found.\n\n\n"}
{"id": "32325621", "url": "https://en.wikipedia.org/wiki?curid=32325621", "title": "Top cap", "text": "Top cap\n\nIn vacuum tube technology, a top cap is a terminal at the top of the tube envelope that connects one of the electrodes, the other electrodes being connected via the tube socket.\n\nTop caps have most commonly been used for:\n\n\nA few amplifier tubes used two top caps, symmetrically placed, one for anode and the other for grid.\n\nIn audio amplifier tube application, the top cap was originally used for the grid connection, and a serviceman could apply a moist finger to the terminal to confirm that the stage and subsequent circuits were working by listening for the hum this produced in the loudspeaker. This practice led to some nasty accidents when anode top caps were first introduced to amplifier stages (they had been used on rectifiers for some time).\n"}
{"id": "4205746", "url": "https://en.wikipedia.org/wiki?curid=4205746", "title": "Vacuum engineering", "text": "Vacuum engineering\n\nVacuum engineering deals with technological processes and equipment that use vacuum to achieve better results than those run under atmospheric pressure. The most widespread applications of vacuum technology are:\n\nVacuum coaters are capable of applying various types of coatings on metal, glass, plastic or ceramic surfaces, providing high quality and uniform thickness and color. Vacuum dryers can be used for delicate materials and save significant quantities of energy due to lower drying temperatures.\n\nVacuum systems usually consist of gauges, vapor jet and pumps, vapor traps and valves along with other extensional piping’s. A vessel that is operating under vacuum system may be any of these types such as processing tank, steam simulator, particle accelerator, or any other type of space that has an enclosed chamber to maintain the system in less than atmospheric gas pressure. Since a vacuum is created in an enclosed chamber, the consideration of being able to withstand external atmospheric pressure are the usual precaution for this type of design. Along with the effect of buckling or collapsing, the outer shell of vacuum chamber will be carefully evaluated and any sign of deterioration will be corrected by the increase of thickness of the shell itself.The main material used for vacuum design are usually mild steel, stainless steel, and aluminum.Other sections such as glass are used for gauge glass or view ports or sometimes electrical insulation. The interior of vacuum chamber should always be smooth and free of rush and defections.\nHigh pressure solvent are usually used to remove any excess oil and contaminants that will affect vacuum in any way.Due to vacuum chamber is in an enclosed space, very specific detergents can be used to prevent any hazards or danger during cleaning. Any vacuum chamber should always have a certain number of access and viewing ports. These are usually served as a flange connection to the attachment of pumps, piping or any other parts required for system operation. The most importance is the fabrication of vacuum chambers sealing capability.The chamber itself must be airtight to maintain perfect vacuum, this is ensure through the process of leaking hunting generally using mass spectrometer leak detector. All openings and connections are also assembled with O rings and gaskets to prevent any further possible leakage of air in the system.\n\nVacuum engineering uses techniques and equipment that vary greatly depending on the level of vacuum used. Pressure slightly reduced from atmospheric pressure may be used to control airflow in ventilation systems, or in material handling systems. Lower-pressure vacuums may be used in vacuum evaporation in processing of food stuffs without excessive heating. Higher grades of vacuum are used for degassing, vacuum metallurgy, and in the production of light bulbs and cathode ray tubes. So-called \"ultrahigh\" vacuums are required for certain semiconductor processing; the \"hardest\" vacuums with the lowest pressure are produced for experiments in physics, where even a few stray atoms of air would interfere with the experiment in progress.\n\nApparatus used varies with decreasing pressure. Blowers give way to various kinds of reciprocating and rotary pumps. For some important applications, a steam ejector can quickly evacuate a large process vessel to a rough vacuum, sufficient for some processes or as a preliminary to more complete pumping processes. The invention of the Sprengel pump was a critical step in the development of the incandescent light bulb as it allowed creation of a vacuum that was higher than previously available, which extended the life of the bulbs. At higher vacuum levels (lower pressures), diffusion pumps, absorption, cyrogenic pumps are used. Pumps are more like \"compressors\" since they gather the rarefied gases in the vacuum vessel and push them into a much higher pressure, smaller volume, exhaust. A chain of two or more different kinds of vacuum pumps may be used in a vacuum system, with one \"roughing\" pump removing most of the mass of air from the system, and the additional stages handling relatively smaller amounts of air at lower and lower pressures. In some applications, a chemical element is used to combine with the air remaining in an enclosure after pumping. For example, in electronic vacuum tubes, a metallic \"getter\" was heated by induction to remove the air left after initial pump down and closure of the tubes. The \"getter\" would also slowly remove any gas evolved within the tube during its remaining life, maintaining sufficiently good vacuum.\n\nVacuum technology is a method used to evacuate air from a closed volume by creating a pressure differential from the closed volume to some vent, the ultimate vent being the open atmosphere. When using an industrial vacuum system, a vacuum pump or generator creates this pressure differential. A variety of technical inventions were created based on the idea of vacuum discovered during the 17th century. These range from vacuum generation pumps to X-ray tubes, which were later introduced to the medical field for use as sources of X-ray radiation. The vacuum environment has come to play an important role in scientific research as new discoveries are being made by looking back to the basic fundamentals of pressure. The idea of “perfect vacuum” cannot be realized, but very nearly approximated by the technological discoveries of the early 20th century. Vacuum engineering today uses a range of different material, from aluminum to zirconium and just about everything in between. There may be the popular belief that vacuum technology deals only with valves, flanges, and other vacuum components, but novel scientific discoveries are often made with the assistance of these traditional vacuum technologies, especially in the realm of high-tech. Vacuum engineering is used for compound semiconductors, power devices, memory logic, and photovoltaics.\n\nAnother technical invention is the vacuum pump. Such invention is used to remove gas molecules from sealed volume thus leaving behind a partial vacuum. More than one vacuum pumps are used in a single application to create fluent flow. Fluent flow is used to allow a clear path made using vacuum to remove any air molecules in the way of the process. Vacuum will be used in this process to attempted to create a perfect vacuum. A type of vacuum such as partial vacuum can be caused by the usage of positive displacement type pumps. A positive displacement pump is able to transfer gas load from the entrance to the exit port, but due to its limitation of design it can only achieve a relatively low vacuum. In order to reach a higher vacuum, other techniques must be used. By using a series of pump such as following up a fast pump down with a positive displacement type pump will create a way better vacuum than using a single pump. These combination of pump used is usually determined by the need of vacuum in the system.\n\nMaterials for use in vacuum systems must be carefully evaluated. Many materials have a degree of porosity, unimportant at ordinary pressures, but which would continually admit minute amounts of air into a vacuum system if incorrectly used. Some items, such as rubber and plastic, give off gases into a vacuum that can contaminate the system. At high and ultrahigh vacuum levels, even metals must be carefully selected - air molecules and moisture can cling to the surface of metals, and any trapped gas within the metal may percolate to the surface under vacuum. In some vacuum systems, a simple coating of low-volatile grease is sufficient to seal gaps in joints, but at ultrahigh vacuum, fittings must be carefully machined and polished to minimize trapped gas. It is usual practice to bake components of a high-vacuum system; at high temperatures, any gases or moisture adhering to the surface is driven off. However, this requirement affects which materials can be used.\n\nParticle accelerators are the largest ultrahigh vacuum systems and can be up to kilometres in length.\n\nThe word “Vacuum” is originated from the Latin word “vacua”, which is translated to the word “empty”.Physicist use vacuum to describe a partially empty space, where air or some other gases are being removed from one container. The idea of vacuum relating to the empty space has been speculated as early as 5th century from Greek philosophers, Aristotle (384-322 B.C.) was the one who came up with the relation of vacuum being an empty space in nature would be impossible to ever create. This idea had stuck around for over centuries until the 17th century, when vacuum technology and physics was discovered. In the mid 17th century, Evangelista Torricelli studied the properties of a vacuum generated by a mercury column in a glass tube; this became the barometer, an instrument to observe variations in atmospheric air pressure. Otto von Guericke spectacularly demonstrated the effect of atmospheric pressure in 1654, when teams of horses could not separate two 20-inch diameter hemispheres, which had been placed together and evacuated. In 1698, Thomas Savery patented a steam pump that relied on condensation of steam to produce a low-grade vacuum, for pumping water out of mines. The apparatus was improved in the Newcomen atmospheric engine of 1712; while inefficient, it allowed coal mines to be exploited that otherwise would flood by ground water. During the years of 1564-1642, the famous scientist Galileo was one of the first physicist to conduct experiments to develop measured forces to develop vacuum using a piston in a cylinder.This was a big discovery for scientist and was shared among others. French scientist and philosopher Blaise Pascal used the idea that was discovered to look into further research of vacuum. Pascal discoveries were similar to Torricelli’s research as Pascal used similar methods to pull vacuum using mercury. It was until the year 1661, when the mayor of the city of Magdeburg used this discovery to invent or retrofit new ideas. The mayor Otto von Guericke created the first air pump, modified the idea of water pumps, and also modified manometers.Vacuum engineering today, provides the solution for all thin film needs in the mechanical industry. This method of engineering is typically used for R&D needs or large scale material production.\n\nVacuum was used to propel trains experimentally.\n\nPump technology hit a plateau until Geissler and Sprengle in the mid 19th century, who finally gave access to the high-vacuum regime. This led to the study of electrical discharges in vacuum, discovery of cathode rays,discovery of X-rays and the discovery of the electron. The photoelectric effect was observed in high vacuum, which was a key discovery that lead to the formulation of quantum mechanics and much of modern physics.\n\n"}
{"id": "32539443", "url": "https://en.wikipedia.org/wiki?curid=32539443", "title": "Verified Carbon Standard", "text": "Verified Carbon Standard\n\nThe Verified Carbon Standard (VCS), formerly the Voluntary Carbon Standard, is a standard for certifying carbon emissions reductions. VCS is administered by Verra, a 501(c)(3) not-for-profit organization.\n\nIn 2005, The Climate Group, International Emissions Trading Association (IETA) and The World Economic Forum - convened a team of global carbon market experts to draft the first VCS requirements. The World Business Council for Sustainable Development (WBCSD) joined the effort soon after. These experts soon formed the VCS Steering Committee, which worked to draft the first and subsequent versions of the VCS Standard. Many of the members of the original steering committee went on to be on the original Board of Directors, which now has evolved into a body of 12 members that offers input and guidance to the organization.\n\nBy 2008, with the VCS Standard becoming more widely adopted, the Board of Directors named David Antonioli the organization’s first Chief Executive Officer. Soon after in 2009, VCS incorporated in Washington D.C. as a non-profit NGO.\n\nOn February 15, 2018, the organization that maintains the Verified Carbon Standard changed its name from Verified Carbon Standard to Verra.\n\n\n"}
{"id": "49610959", "url": "https://en.wikipedia.org/wiki?curid=49610959", "title": "Volcano Museum, Daun", "text": "Volcano Museum, Daun\n\nThe Volcano Museum () in the old district administrative office (\"Landratsamt\") in Daun, Germany, was set up as an extension of the existing 'geopaths' at Hillesheim, Manderscheid and Gerolstein and is part of the Volcanic Eifel Nature and Geopark.\n\nNumerous information boards, photographs and exhibits from the Volcanic Eifel, as well as of currently active volcanoes in Europe and Asia, offer insights into the geological development of the Volcanic Eifel region.\n\nIn addition there are several other volcano museums that cover the Eifel, such as the Maar Museum in Manderscheid and the Volcano House, Strohn.\n\nIn the exhibition rooms are interactive computer models about the geology and volcanism of the Eifel. On a large overview map at the entrance, the phenomena of continental movement (plate tectonics) is explained, and currently active volcanoes can be viewed by means of fluorescent lights. A computer simulation shows the distribution of the continents in the geological eras of the past and also in the future. Other exhibits explain the origin and evolution of the rocks. In the exhibition room \"Eifel Volcanism\" the visitor can even simulate a volcanic eruption using a model of a cinder cone.\n\nOther models show the entire region of the volcanic West Eifel and the formation of volcano types in Eifel region. The models are supplemented in turn by numerous exhibits from the Volcanic Eifel. In the Volcanic Eifel Geo Centre all the geologically-related facilities of the Volcanic Eifel are summarized on an information board. These include the Devonian Route, the Bunter Sandstone Route, the Volcano Route, the Mineral Spring Route as well as mining and other museum facilities. Once again, visitors can illuminate geologically interesting places on a clear wall map using buttons. In the basement of the museum, the training centre of the Geo Centre offers a wide range of media and exhibits relating to geology.\n\n"}
{"id": "547324", "url": "https://en.wikipedia.org/wiki?curid=547324", "title": "West Siberian Plain", "text": "West Siberian Plain\n\nThe West Siberian Plain, also known as Zapadno-sibirskaya Ravnina, () is a large plain that occupies the western portion of Siberia, between the Ural Mountains in the west and the Yenisei River in the east, and by the Altay Mountains on the southeast. Much of the plain is poorly drained and consists of some of the world's largest swamps and floodplains. Important cities include Omsk, Novosibirsk, Tomsk and Chelyabinsk.\n\nThe West Siberian Plain is located east of the Ural Mountains mostly in the territory of Russia. It has been described as the world's largest unbroken lowland—more than 50 percent is less than 100 metres (330 ft) above sea level—and covers an area of about 2.6– which is about one third of Siberia, extending from north to south for 2,400 km (1,490 mi), from the Arctic Ocean to the foothills of the Altay Mountains, and from east to west for 1,900 km (1,180 mi) from the Ural Mountains to the Yenisei River.\n\nThe plain has eight distinct vegetation regions: tundra, forest-tundra, northern taiga, middle taiga, southern taiga, sub-taiga forest, forest-steppe, and steppe. The number of animal species in the West Siberian Plain ranges from at least 107 in the tundra to 278 or more in the forest-steppe region. The long Yenisei river flows broadly south to north, a distance of 3,530 km (2,195 mi) to the Arctic Ocean, where it discharges more than 20 million litres (5 million gallons) of water per second at its mouth. Together with its tributary Angara, the two rivers flow 5,530 km (3,435 mi). The valley formed by the Yenisei acts as a rough dividing line between the West Siberian Plain and the Central Siberian Plateau. Glacial deposits extend as far south as the Ob-Irtysh confluence, forming occasional low hills and ridges, but otherwise the plain is exceedingly flat and featureless.\n\nWinters on the West Siberian Plain are harsh and long. The climate of most of the plains is either subarctic or continental. Two of the larger cities on the plain are Surgut and Nizhnevartovsk.\n\nThe West Siberian Plain consists mostly of Cenozoic alluvial deposits and is extraordinarily flat. A rise of fifty metres in sea level would cause all land between the Arctic Ocean and Novosibirsk to be inundated (see also Turgai Straits, West Siberian Glacial Lake). It is a region of the Earth’s crust that has undergone prolonged subsidence and is composed of horizontal deposits from as much as 65,000,000 years ago. Many of the deposits on this plain result from ice dams that reversed the flow of the Ob and Yenisei rivers, redirecting them into the Caspian Sea, and perhaps the Aral Sea as well. It is very swampy and soils are mostly peaty Histosols and, in the treeless northern part, Histels.\n\nThis is one of the world's largest areas of peatlands, which are characterized by raised bogs. It is believed that Vasyugan Swamp is the world’s largest single raised bog, covering approximately .\n\nIn the south of the plain, where permafrost is largely absent, rich grasslands that are an extension of the Kazakh Steppe formed the original vegetation, which has almost all been cleared as of the early 21st century.\n\nLarge regions of the plains are flooded in the spring, and marshlands make much of the area unsuitable for agriculture. The principal rivers in the West Siberian Plain are from west to east the Irtysh, Ob, Nadym, Pur, Taz and Yenisei. There are many lakes and swamps. This area had large petroleum and natural gas reserves. Most of Russia’s oil and gas production was extracted from this area during the 1970s and 80s.\n\n\n"}
