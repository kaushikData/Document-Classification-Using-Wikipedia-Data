{"id": "6568326", "url": "https://en.wikipedia.org/wiki?curid=6568326", "title": "2006 Mumbai sweet seawater incident", "text": "2006 Mumbai sweet seawater incident\n\nThe 2006 Mumbai \"sweet\" seawater incident was a strange phenomenon during which residents of Mumbai claimed that the water at Mahim Creek had suddenly turned sweet. Within hours, residents of Gujarat claimed that seawater at Teethal beach had turned sweet as well. This caused a mass hysteria among people who started coming in large numbers to drink the sea water.\n\nIn the aftermath of the incidents, local authorities feared the possibility of a severe outbreak of water-borne diseases, such as gastroenteritis. The Maharashtra Pollution Control Board had warned people not to drink the water, but despite this many people had collected it in bottles, even as plastic and rubbish had drifted by on the current. The Municipal Corporation of Greater Mumbai had ordered a bacteriological report into the \"sweet\" water, but suspected that \"contamination in the water might have been reduced due to the waters from Mithi river flowing into the mouth of Mahim Bay\".\n\nGeologists at the Indian Institute of Technology Bombay offered the explanation that water turning sweet is a natural phenomenon. Continuous rainfall over the preceding few days had caused a large pool of fresh water to accrue in an underground rock formation near to the coast, which then discharged into the sea as a large \"plume\" as fractures in the rocks widened. Because of the differences in density, the discharged fresh water floated on top of the salt water of the sea and spread along the coast. Over time, the two would mix to become normal sea water once more.\n\nAccording to Valsad District Collector D Rawal the reason for the water in Gujarat tasting less salty than usual was that because of the monsoon, two rivers Auranga and Banki were in spate and were flowing into the sea in the region.\n\nSimilar natural phenomenon is also observed in the case of halocline.\n\n\n"}
{"id": "46302885", "url": "https://en.wikipedia.org/wiki?curid=46302885", "title": "Ants of Sri Lanka (book)", "text": "Ants of Sri Lanka (book)\n\nAnts of Sri Lanka by R.K. Sriyani Dias is a zoology book about all the ant species in Sri Lanka. This is the first complete zoological book ever published in Sri Lanka after about 100 years, about ants. During the past decades, the taxonomic evidences about ants of Sri Lanka is full of doubts due to misidentification of various species and placement of them into various genera and subfamilies. At present, this book is the only publication about ants within Sri Lanka, which was published after Bingham's ant publication on 1903 on island.\n\nDr. R.K. Sriyani Dias is the first myrmecologist in Sri Lanka, which experimented and revealed the hidden informations of Sri Lankan ant diversity. She is currently worked as a senior lecturer in Zoology of University of Kelaniya. She has conducted many researches throughout the country to reveal the exact number of ant species recorded from Sri Lanka. Dr. Dias is the first person to undergo extensive biological nature about Sri Lanka endemic ant species - \"Aneuretus simoni\". She also has kept many ant specimens at the National Museum of Colombo and Department of Zoology in the University of Kelaniya.\n\nThe book describes all the ant species recorded in Sri Lanka known up to date, with 125+ species, both terrestrial and fossorial. The book also highlight major ants in Sri Lanka with microscopic and hand drawings. It features about many aspects of myrmecology, such as identification of an ant from other insects, general life cycle of ant, taxonomically important morphological features of worker ants.\n\nThe field and laboratory methods for study ants are also listed in well documented fashion with a easily understandable way. \n\nThe forward of the Book was written by Prof. Seiki Yamane, who is a leading scientist about ants. During his forward, he stated that:\n\n\"It can be said that now Sri Lanka has an active ant researcher who is expected to clarify the entire ant fauna of the country. For this goal to be achieved, the networking of Sri Lankan entomologists interested in ant study, and corporation with myrmecologists outside the country, particularly of India, may be indispensable. I (Yamane) hope this book will inspire native scientists to start studying these fascinating creatures.\"\n\nAfter this book, Prof. Dias completed much advanced, scientific record for ants of Sri Lanka, first published by Biodiversity Secretariat of Sri Lanka in 2014 with 273 pages. This has much focused on their biology, identifying keys, full details of each subfamily and relevant updated classifications and photographs. \n\n"}
{"id": "19916559", "url": "https://en.wikipedia.org/wiki?curid=19916559", "title": "Atomic nucleus", "text": "Atomic nucleus\n\nThe atomic nucleus is the small, dense region consisting of protons and neutrons at the center of an atom, discovered in 1911 by Ernest Rutherford based on the 1909 Geiger–Marsden gold foil experiment. After the discovery of the neutron in 1932, models for a nucleus composed of protons and neutrons were quickly developed by Dmitri Ivanenko and Werner Heisenberg. An atom is composed of a positively-charged nucleus, with a cloud of negatively-charged electrons surrounding it, bound together by electrostatic force. Almost all of the mass of an atom is located in the nucleus, with a very small contribution from the electron cloud. Protons and neutrons are bound together to form a nucleus by the nuclear force.\n\nThe diameter of the nucleus is in the range of () for hydrogen (the diameter of a single proton) to about for the heaviest atom uranium. These dimensions are much smaller than the diameter of the atom itself (nucleus + electron cloud), by a factor of about 26,634 (uranium atomic radius is about ()) to about 60,250 (hydrogen atomic radius is about ).\n\nThe branch of physics concerned with the study and understanding of the atomic nucleus, including its composition and the forces which bind it together, is called nuclear physics.\n\nThe nucleus was discovered in 1911, as a result of Ernest Rutherford's efforts to test Thomson's \"plum pudding model\" of the atom. The electron had already been discovered earlier by J.J. Thomson himself. Knowing that atoms are electrically neutral, Thomson postulated that there must be a positive charge as well. In his plum pudding model, Thomson suggested that an atom consisted of negative electrons randomly scattered within a sphere of positive charge. Ernest Rutherford later devised an experiment with his research partner Hans Geiger and with help of Ernest Marsden, that involved the deflection of alpha particles (helium nuclei) directed at a thin sheet of metal foil. He reasoned that if Thomson's model were correct, the positively charged alpha particles would easily pass through the foil with very little deviation in their paths, as the foil should act as electrically neutral if the negative and positive charges are so intimately mixed as to make it appear neutral. To his surprise, many of the particles were deflected at very large angles. Because the mass of an alpha particle is about 8000 times that of an electron, it became apparent that a very strong force must be present if it could deflect the massive and fast moving alpha particles. He realized that the plum pudding model could not be accurate and that the deflections of the alpha particles could only be explained if the positive and negative charges were separated from each other and that the mass of the atom was a concentrated point of positive charge. This justified the idea of a nuclear atom with a dense center of positive charge and mass.\n\nThe term nucleus is from the Latin word \"nucleus\", a diminutive of \"nux\" (\"nut\"), meaning the kernel (i.e., the \"small nut\") inside a watery type of fruit (like a peach). In 1844, Michael Faraday used the term to refer to the \"central point of an atom\". The modern atomic meaning was proposed by Ernest Rutherford in 1912. The adoption of the term \"nucleus\" to atomic theory, however, was not immediate. In 1916, for example, Gilbert N. Lewis stated, in his famous article \"The Atom and the Molecule\", that \"the atom is composed of the \"kernel\" and an outer atom or \"shell\"\"\n\nThe nucleus of an atom consists of neutrons and protons, which in turn are the manifestation of more elementary particles, called quarks, that are held in association by the nuclear strong force in certain stable combinations of hadrons, called baryons. The nuclear strong force extends far enough from each baryon so as to bind the neutrons and protons together against the repulsive electrical force between the positively charged protons. The nuclear strong force has a very short range, and essentially drops to zero just beyond the edge of the nucleus. The collective action of the positively charged nucleus is to hold the electrically negative charged electrons in their orbits about the nucleus. The collection of negatively charged electrons orbiting the nucleus display an affinity for certain configurations and numbers of electrons that make their orbits stable. Which chemical element an atom represents is determined by the number of protons in the nucleus; the neutral atom will have an equal number of electrons orbiting that nucleus. Individual chemical elements can create more stable electron configurations by combining to share their electrons. It is that sharing of electrons to create stable electronic orbits about the nucleus that appears to us as the chemistry of our macro world.\n\nProtons define the entire charge of a nucleus, and hence its chemical identity. Neutrons are electrically neutral, but contribute to the mass of a nucleus to nearly the same extent as the protons. Neutrons can explain the phenomenon of isotopes (same atomic number with different atomic mass.) The main role of neutrons is to reduce electrostatic repulsion inside the nucleus.\n\nProtons and neutrons are fermions, with different values of the strong isospin quantum number, so two protons and two neutrons can share the same space wave function since they are not identical quantum entities. They are sometimes viewed as two different quantum states of the same particle, the \"nucleon\". Two fermions, such as two protons, or two neutrons, or a proton + neutron (the deuteron) can exhibit bosonic behavior when they become loosely bound in pairs, which have integer spin.\n\nIn the rare case of a hypernucleus, a third baryon called a hyperon, containing one or more strange quarks and/or other unusual quark(s), can also share the wave function. However, this type of nucleus is extremely unstable and not found on Earth except in high energy physics experiments.\n\nThe neutron has a positively charged core of radius ≈ 0.3 fm surrounded by a compensating negative charge of radius between 0.3 fm and 2 fm. The proton has an approximately exponentially decaying positive charge distribution with a mean square radius of about 0.8 fm.\n\nNuclei can be spherical, rugby ball-shaped (prolate deformation), discus-shaped (oblate deformation), triaxial (a combination of oblate and prolate deformation) or pear-shaped.\n\nNuclei are bound together by the residual strong force (nuclear force). The residual strong force is a minor residuum of the strong interaction which binds quarks together to form protons and neutrons. This force is much weaker \"between\" neutrons and protons because it is mostly neutralized within them, in the same way that electromagnetic forces \"between\" neutral atoms (such as van der Waals forces that act between two inert gas atoms) are much weaker than the electromagnetic forces that hold the parts of the atoms together internally (for example, the forces that hold the electrons in an inert gas atom bound to its nucleus).\n\nThe nuclear force is highly attractive at the distance of typical nucleon separation, and this overwhelms the repulsion between protons due to the electromagnetic force, thus allowing nuclei to exist. However, the residual strong force has a limited range because it decays quickly with distance (see Yukawa potential); thus only nuclei smaller than a certain size can be completely stable. The largest known completely stable nucleus (i.e. stable to alpha, beta, and gamma decay) is lead-208 which contains a total of 208 nucleons (126 neutrons and 82 protons). Nuclei larger than this maximum are unstable and tend to be increasingly short-lived with larger numbers of nucleons. However, bismuth-209 is also stable to beta decay and has the longest half-life to alpha decay of any known isotope, estimated at a billion times longer than the age of the universe.\n\nThe residual strong force is effective over a very short range (usually only a few femtometres (fm); roughly one or two nucleon diameters) and causes an attraction between any pair of nucleons. For example, between protons and neutrons to form [NP] deuteron, and also between protons and protons, and neutrons and neutrons.\n\nThe effective absolute limit of the range of the strong force is represented by halo nuclei such as lithium-11 or boron-14, in which dineutrons, or other collections of neutrons, orbit at distances of about (roughly similar to the radius of the nucleus of uranium-238). These nuclei are not maximally dense. Halo nuclei form at the extreme edges of the chart of the nuclides—the neutron drip line and proton drip line—and are all unstable with short half-lives, measured in milliseconds; for example, lithium-11 has a half-life of .\n\nHalos in effect represent an excited state with nucleons in an outer quantum shell which has unfilled energy levels \"below\" it (both in terms of radius and energy). The halo may be made of either neutrons [NN, NNN] or protons [PP, PPP]. Nuclei which have a single neutron halo include Be and C. A two-neutron halo is exhibited by He, Li, B, B and C. Two-neutron halo nuclei break into three fragments, never two, and are called \"Borromean nuclei\" because of this behavior (referring to a system of three interlocked rings in which breaking any ring frees both of the others). He and Be both exhibit a four-neutron halo. Nuclei which have a proton halo include B and P. A two-proton halo is exhibited by Ne and S. Proton halos are expected to be more rare and unstable than the neutron examples, because of the repulsive electromagnetic forces of the excess proton(s).\n\nAlthough the standard model of physics is widely believed to completely describe the composition and behavior of the nucleus, generating predictions from theory is much more difficult than for most other areas of particle physics. This is due to two reasons:\n\n\nHistorically, experiments have been compared to relatively crude models that are necessarily imperfect. None of these models can completely explain experimental data on nuclear structure.\n\nThe nuclear radius (\"R\") is considered to be one of the basic quantities that any model must predict. For stable nuclei (not halo nuclei or other unstable distorted nuclei) the nuclear radius is roughly proportional to the cube root of the mass number (\"A\") of the nucleus, and particularly in nuclei containing many nucleons, as they arrange in more spherical configurations:\n\nThe stable nucleus has approximately a constant density and therefore the nuclear radius R can be approximated by the following formula,\n\nwhere \"A\" = Atomic mass number (the number of protons \"Z\", plus the number of neutrons \"N\") and \"r\" = 1.25 fm = 1.25 × 10 m. In this equation, the \"constant\" \"r\" varies by 0.2 fm, depending on the nucleus in question, but this is less than 20% change from a constant.\n\nIn other words, packing protons and neutrons in the nucleus gives \"approximately\" the same total size result as packing hard spheres of a constant size (like marbles) into a tight spherical or almost spherical bag (some stable nuclei are not quite spherical, but are known to be prolate).\n\nModels of nuclear structure include :\n\nEarly models of the nucleus viewed the nucleus as a rotating liquid drop. In this model, the trade-off of long-range electromagnetic forces and relatively short-range nuclear forces, together cause behavior which resembled surface tension forces in liquid drops of different sizes. This formula is successful at explaining many important phenomena of nuclei, such as their changing amounts of binding energy as their size and composition changes (see semi-empirical mass formula), but it does not explain the special stability which occurs when nuclei have special \"magic numbers\" of protons or neutrons.\n\nThe terms in the semi-empirical mass formula, which can be used to approximate the binding energy of many nuclei, are considered as the sum of five types of energies (see below). Then the picture of a nucleus as a drop of incompressible liquid roughly accounts for the observed variation of binding energy of the nucleus:\n\nVolume energy. When an assembly of nucleons of the same size is packed together into the smallest volume, each interior nucleon has a certain number of other nucleons in contact with it. So, this nuclear energy is proportional to the volume.\n\nSurface energy. A nucleon at the surface of a nucleus interacts with fewer other nucleons than one in the interior of the nucleus and hence its binding energy is less. This surface energy term takes that into account and is therefore negative and is proportional to the surface area.\n\nCoulomb Energy. The electric repulsion between each pair of protons in a nucleus contributes toward decreasing its binding energy.\n\nAsymmetry energy (also called Pauli Energy). An energy associated with the Pauli exclusion principle. Were it not for the Coulomb energy, the most stable form of nuclear matter would have the same number of neutrons as protons, since unequal numbers of neutrons and protons imply filling higher energy levels for one type of particle, while leaving lower energy levels vacant for the other type.\n\nPairing energy. An energy which is a correction term that arises from the tendency of proton pairs and neutron pairs to occur. An even number of particles is more stable than an odd number.\n\nA number of models for the nucleus have also been proposed in which nucleons occupy orbitals, much like the atomic orbitals in atomic physics theory. These wave models imagine nucleons to be either sizeless point particles in potential wells, or else probability waves as in the \"optical model\", frictionlessly orbiting at high speed in potential wells.\n\nIn the above models, the nucleons may occupy orbitals in pairs, due to being fermions, which allows explanation of even/odd \"Z\" and \"N\" effects well-known from experiments. The exact nature and capacity of nuclear shells differs from those of electrons in atomic orbitals, primarily because the potential well in which the nucleons move (especially in larger nuclei) is quite different from the central electromagnetic potential well which binds electrons in atoms. Some resemblance to atomic orbital models may be seen in a small atomic nucleus like that of helium-4, in which the two protons and two neutrons separately occupy 1s orbitals analogous to the 1s orbital for the two electrons in the helium atom, and achieve unusual stability for the same reason. Nuclei with 5 nucleons are all extremely unstable and short-lived, yet, helium-3, with 3 nucleons, is very stable even with lack of a closed 1s orbital shell. Another nucleus with 3 nucleons, the triton hydrogen-3 is unstable and will decay into helium-3 when isolated. Weak nuclear stability with 2 nucleons {NP} in the 1s orbital is found in the deuteron hydrogen-2, with only one nucleon in each of the proton and neutron potential wells. While each nucleon is a fermion, the {NP} deuteron is a boson and thus does not follow Pauli Exclusion for close packing within shells. Lithium-6 with 6 nucleons is highly stable without a closed second 1p shell orbital. For light nuclei with total nucleon numbers 1 to 6 only those with 5 do not show some evidence of stability. Observations of beta-stability of light nuclei outside closed shells indicate that nuclear stability is much more complex than simple closure of shell orbitals with magic numbers of protons and neutrons.\n\nFor larger nuclei, the shells occupied by nucleons begin to differ significantly from electron shells, but nevertheless, present nuclear theory does predict the magic numbers of filled nuclear shells for both protons and neutrons. The closure of the stable shells predicts unusually stable configurations, analogous to the noble group of nearly-inert gases in chemistry. An example is the stability of the closed shell of 50 protons, which allows tin to have 10 stable isotopes, more than any other element. Similarly, the distance from shell-closure explains the unusual instability of isotopes which have far from stable numbers of these particles, such as the radioactive elements 43 (technetium) and 61 (promethium), each of which is preceded and followed by 17 or more stable elements.\n\nThere are however problems with the shell model when an attempt is made to account for nuclear properties well away from closed shells. This has led to complex \"post hoc\" distortions of the shape of the potential well to fit experimental data, but the question remains whether these mathematical manipulations actually correspond to the spatial deformations in real nuclei. Problems with the shell model have led some to propose realistic two-body and three-body nuclear force effects involving nucleon clusters and then build the nucleus on this basis. Three such cluster models are the 1936 Resonating Group Structure model of John Wheeler, Close-Packed Spheron Model of Linus Pauling and the 2D Ising Model of MacGregor.\n\nAs with the case of superfluid liquid helium, atomic nuclei are an example of a state in which both (1) \"ordinary\" particle physical rules for volume and (2) non-intuitive quantum mechanical rules for a wave-like nature apply. In superfluid helium, the helium atoms have volume, and essentially \"touch\" each other, yet at the same time exhibit strange bulk properties, consistent with a Bose–Einstein condensation. The nucleons in atomic nuclei also exhibit a wave-like nature and lack standard fluid properties, such as friction. For nuclei made of hadrons which are fermions, Bose-Einstein condensation does not occur, yet nevertheless, many nuclear properties can only be explained similarly by a combination of properties of particles with volume, in addition to the frictionless motion characteristic of the wave-like behavior of objects trapped in Erwin Schrödinger's quantum orbitals.\n\n"}
{"id": "54022472", "url": "https://en.wikipedia.org/wiki?curid=54022472", "title": "Aurorasaurus", "text": "Aurorasaurus\n\nAurorasaurus is a citizen science project which tracks auroras with social media, namely Twitter and Facebook, but largely with its mobile app. The project was created by Liz MacDonald, a space physicist at NASA Goddard Space Flight Center in Greenbelt, Maryland, after a 2011 solar storm produced red auroras visible from Alabama, whereupon she enlisted people as aurora observers using social media. It receives funding from the National Science Foundation. The sheer ubiquity of smartphones in particular have made them a modern version of pioneering aurora scientist's Victor Hessler's bell; reportage of the heretofore undescribed unusual atmospheric optical phenomenon dubbed \"Steve\" went viral.\n\n"}
{"id": "16602928", "url": "https://en.wikipedia.org/wiki?curid=16602928", "title": "Azores–Gibraltar Transform Fault", "text": "Azores–Gibraltar Transform Fault\n\nThe Azores–Gibraltar Transform Fault (AGFZ), also called a fault zone and a fracture zone, is a major seismic fault in the Central Atlantic Ocean west of the Strait of Gibraltar. It is the product of the complex interaction between the African, Eurasian, and Iberian plates.\nThe AGFZ produced the large-magnitude 1755 Lisbon and 1969 Horseshoe earthquakes and, consequently, a number of large tsunamis.\n\nForming the Atlantic segment of the boundary between the African and Eurasian plates, the AGFZ is largely dominated by compressional forces between these converging () plates, but it is subject of a dynamic tectonic regime also involving extension and transform faulting. The oceanic lithosphere in the area is directly related to the opening of the North Atlantic Ocean and one of the oldest preserved on Earth.\n\nThe western end of the AGFZ, the Azores Triple Junction on the Mid-Atlantic Ridge (MAR), is where the North American, African, and Eurasian plates meet. Spreading in the MAR is faster south of the AGFZ than north of it, which results in a trancurrent movement along the AGFZ at about .\nThe eastern segment of the fault is complex and characterised by a series of seamounts and ridges separating the Tores and Horseshoe abyssal plains. The active compressional deformation in this segment is an extremely rare example of compression between two oceanic litospheres.\n\nThe Atlantic Ocean is surrounded by passive margins with the exception of three subduction zones: the Lesser Antilles Arc in the Caribbean, the Scotia Arc in the South Atlantic, and the Gibraltar Arc in the western Mediterranean. The Gibraltar Arc is propagating westward into the Atlantic over an east-dipping oceanic slab (one of the remainders of the Tethys Ocean). This subduction/back-arc basin system is developing in front of the Alboran Block (under the Alboran Sea) at a rate faster than that of the Africa-Iberia convergence. Consequently, this area is a rare case of a passive margin slowly being transformed into an active margin.\nThe extension of this subduction system, known as the \"allochthonous unit of the Gulf of Cadiz\" (AUGZ), marks the continuing propagation of the Alpide belt into the Atlantic along the AGFZ.\nIn the context of the Wilson Cycle, this suggests that the beginning of the closure of the Atlantic is taking place in front of the three Atlantic subduction zones.\n\n"}
{"id": "1614121", "url": "https://en.wikipedia.org/wiki?curid=1614121", "title": "Barotropic fluid", "text": "Barotropic fluid\n\nIn fluid dynamics, a barotropic fluid is a fluid whose density is a function of pressure only. The barotropic fluid is a useful model of fluid behavior in a wide variety of scientific fields, from meteorology to astrophysics.\n\nThe density of most liquids is nearly constant (isopycnic), so it can be stated that their densities vary only weakly with pressure and temperature. Water, which varies only a few percent with temperature and salinity, may be approximated as barotropic. In general, air is not barotropic, as it is a function of temperature and pressure; but, under certain circumstances, the barotropic assumption can be useful. \n\nIn astrophysics, barotropic fluids are important in the study of stellar interiors or of the interstellar medium. One common class of barotropic model used in astrophysics is a polytropic fluid. Typically, the barotropic assumption is not very realistic.\n\nIn meteorology, a barotropic atmosphere is one that for which the density of the air depends only on pressure, as a result isobaric surfaces (constant-pressure surfaces) are also constant-density surfaces. Such isobaric surfaces will also be isothermal surfaces, hence (from the thermal wind equation) the geostrophic wind will not vary with depth. Hence, the motions of a rotating barotropic air mass is strongly constrained. The tropics are more nearly barotropic than mid-latitudes because temperature is more nearly horizontally uniform in the tropics.\n\nA barotropic flow is a generalization of a barotropic atmosphere. It is a flow in which the pressure is a function of the density only and vice versa. In other words, it is a flow in which isobaric surfaces are isopycnic surfaces and vice versa. One may have a barotropic flow of a non-barotropic fluid, but a barotropic fluid will always follow a barotropic flow. Examples include barotropic layers of the oceans, an isothermal ideal gas or an isentropic ideal gas. \n\nA fluid which is not barotropic is baroclinic, i. e., pressure is not the only factor to determine density. For a barotropic fluid or a barotropic flow (such as a barotropic atmosphere), the baroclinic vector is zero.\n\n\n"}
{"id": "26488448", "url": "https://en.wikipedia.org/wiki?curid=26488448", "title": "Biosorption", "text": "Biosorption\n\nBiosorption is a physiochemical process that occurs naturally in certain biomass which allows it to passively concentrate and bind contaminants onto its cellular structure. Biosorption can be defined as the ability of biological materials to accumulate heavy\nmetals from wastewater through metabolically mediated or physico-chemical pathways of uptake. Though using biomass in environmental cleanup has been in practice for a while, scientists and engineers are hoping this phenomenon will provide an economical alternative for removing toxic heavy metals from industrial wastewater and aid in environmental remediation.\n\nPollution interacts naturally with biological systems. It is currently uncontrolled, seeping into any biological entity within the range of exposure. The most problematic contaminants include heavy metals, pesticides and other organic compounds which can be toxic to wildlife and humans in small concentration. There are existing methods for remediation, but they are expensive or ineffective. However, an extensive body of research has found that a wide variety of commonly discarded waste including eggshells, bones, peat, fungi, seaweed, yeast and carrot peels can efficiently remove toxic heavy metal ions from contaminated water. Ions from metals like mercury can react in the environment to form harmful compounds like methylmercury, a compound known to be toxic in humans. In addition, adsorbing biomass, or biosorbents, can also remove other harmful metals like: arsenic, lead, cadmium, cobalt, chromium and uranium. Biosorption may be used as an environmentally friendly filtering technique. Chitosan is among the biological adsorbents used for heavy metals removal without negative environmental impacts.\n\nThe idea of using biomass as a tool in environmental cleanup has been around since the early 1900s when Arden and Lockett discovered certain types of living bacteria cultures were capable of recovering nitrogen and phosphorus from raw sewage when it was mixed in an aeration tank. This discovery became known as the activated sludge process which is structured around the concept of bioaccumulation and is still widely used in wastewater treatment plants today. It wasn't until the late 1970s when scientists noticed the sequestering characteristic in dead biomass which resulted in a shift in research from bioaccumulation to biosorption.\n\nThough bioaccumulation and biosorption are used synonymously, they are very different in how they sequester contaminants:\n\nBiosorption is a metabolically passive process, meaning it does not require energy, and the amount of contaminants a sorbent can remove is dependent on kinetic equilibrium and the composition of the sorbents cellular surface. Contaminants are adsorbed onto the cellular structure. \n\nBioaccumulation is an active metabolic process driven by energy from a living organism and requires respiration. \n\nBoth bioaccumulation and biosorption occur naturally in all living organisms however, in a controlled experiment conducted on living and dead strains of \"bacillus sphaericus\" it was found that the biosorption of chromium ions was 13–20% higher in dead cells than living cells.\n\nIn terms of environmental remediation, biosorption is preferable to bioaccumulation because it occurs at a faster rate and can produce higher concentrations. Since metals are bound onto the cellular surface, biosorption is a reversible process whereas bioaccumulation is only partially reversible.\n\nSince biosorption is determined by equilibrium, it is largely influenced by pH, the concentration of biomass and the interaction between different metallic ions.\n\nFor example, in a study on the removal of pentachlorophenol (PCP) using different strains of fungal biomass, as the pH changed from low pH to high pH (acidic to basic) the amount of removal decreased by the majority of the strains, however one strain was unaffected by the change. In another study on the removal of copper, zinc and nickel ions using a composite sorbent as the pH increased from low to high the sorbent favored the removal of copper ions over the zinc and nickel ions. Because of the variability in sorbent this might be a drawback to biosorption, however, more research will be necessary.\n\nEven though the term biosorption may be relatively new, it has been put to use in many applications for a long time. One very widely known use of biosorption is seen in activated carbon filters. They can filter air and water by allowing contaminants to bind to their incredibly porous and high surface area structure. The structure of the activated carbon is generated as the result of charcoal being treated with oxygen. Another type of carbon, sequestered carbon, can be used as a filtration media. It is made by carbon sequestration, which uses the opposite technique as for creating activated carbon. It is made by heating biomass in the absence of oxygen. The two filters allow for biosorption of different types of contaminants due to their chemical compositions—one with infused oxygen and the other without.\n\nMany industrial effluents contain toxic metals that must be removed. Removal can be accomplished with biosorption techniques. It is an alternative to using man-made ion-exchange resins, which cost ten times more than biosorbents. The cost is so much less, because the biosorbents used are often waste from farms or they are very easy to regenerate, as is the case with seaweed and other unharvested biomass. \n\nIndustrious biosorption is often done by using sorption columns as seen in Figure 1. Effluent containing heavy metal ions is fed into a column from the top. The biosorbents adsorb the contaminants and let the ion-free effluent to exit the column at the bottom. The process can be reversed to collect a highly concentrated solution of metal contaminants. The biosorbents can then be re-used or discarded and replaced.\n"}
{"id": "44401", "url": "https://en.wikipedia.org/wiki?curid=44401", "title": "Brown dwarf", "text": "Brown dwarf\n\nBrown dwarfs are substellar objects that occupy the mass range between the heaviest gas giant planets and the lightest stars, having masses between approximately 13 to 75–80 times that of Jupiter (), or approximately to about . Below this range are the sub-brown dwarfs, and above it are the lightest red dwarfs (M9 V). Brown dwarfs may be fully convective, with no layers or chemical differentiation by depth.\n\nUnlike the stars in the main sequence, brown dwarfs are not massive enough to sustain nuclear fusion of ordinary hydrogen (H) to helium in their cores. They are, however, thought to fuse deuterium (H) and to fuse lithium (Li) if their mass is above a debated threshold of and , respectively. It is also debated whether brown dwarfs would be better defined by their formation processes rather than by their supposed nuclear fusion reactions.\n\nStars are categorized by spectral class, with brown dwarfs designated as types M, L, T, and Y. Despite their name, brown dwarfs are of different colors. Many brown dwarfs would likely appear magenta to the human eye, or possibly orange/red. Brown dwarfs are not very luminous at visible wavelengths.\n\nThere are planets known to orbit brown dwarfs: 2M1207b, MOA-2007-BLG-192Lb, and 2MASS J044144b.\n\nAt a distance of about 6.5 light years, the nearest known brown dwarf is Luhman 16, a binary system of brown dwarfs discovered in 2013. HR 2562 b is listed as the most-massive known exoplanet (as of December 2017) in NASA's exoplanet archive, despite having a mass () more than twice the 13-Jupiter-mass cutoff between planets and brown dwarfs.\n\nThe objects now called \"brown dwarfs\" were theorized to exist in the 1960s by Shiv S. Kumar and were originally called black dwarfs, a classification for dark substellar objects floating freely in space that were not massive enough to sustain hydrogen fusion. However: (a) the term black dwarf was already in use to refer to a cold white dwarf; (b) red dwarfs fuse hydrogen; and (c) these objects may be luminous at visible wavelengths early in their lives. Because of this, alternative names for these objects were proposed, including planetar and substar. In 1975, Jill Tarter suggested the term \"brown dwarf\", using \"brown\" as an approximate color.\n\nThe term \"black dwarf\" still refers to a white dwarf that has cooled to the point that it no longer emits significant amounts of light. However, the time required for even the lowest-mass white dwarf to cool to this temperature is calculated to be longer than the current age of the universe; hence such objects are expected to not yet exist.\n\nEarly theories concerning the nature of the lowest-mass stars and the hydrogen-burning limit suggested that a population I object with a mass less than 0.07 solar masses () or a population II object less than would never go through normal stellar evolution and would become a completely degenerate star. The first self-consistent calculation of the hydrogen-burning minimum mass confirmed a value between 0.08 and 0.07 solar masses for population I objects.\n\nThe discovery of deuterium burning down to 0.012 solar masses and the impact of dust formation in the cool outer atmospheres of brown dwarfs in the late 1980s brought these theories into question. However, such objects were hard to find because they emit almost no visible light. Their strongest emissions are in the infrared (IR) spectrum, and ground-based IR detectors were too imprecise at that time to readily identify any brown dwarfs.\n\nSince then, numerous searches by various methods have sought these objects. These methods included multi-color imaging surveys around field stars, imaging surveys for faint companions of main-sequence dwarfs and white dwarfs, surveys of young star clusters, and radial velocity monitoring for close companions.\n\nFor many years, efforts to discover brown dwarfs were fruitless. In 1988, however, a faint companion to a star known as GD 165 was found in an infrared search of white dwarfs. The spectrum of the companion GD 165B was very red and enigmatic, showing none of the features expected of a low-mass red dwarf. It became clear that GD 165B would need to be classified as a much cooler object than the latest M dwarfs then known. GD 165B remained unique for almost a decade until the advent of the Two Micron All-Sky Survey (2MASS) which discovered many objects with similar colors and spectral features.\n\nToday, GD 165B is recognized as the prototype of a class of objects now called \"L dwarfs\".\n\nAlthough the discovery of the coolest dwarf was highly significant at the time, it was debated whether GD 165B would be classified as a brown dwarf or simply a very-low-mass star, because observationally it is very difficult to distinguish between the two.\n\nSoon after the discovery of GD 165B, other brown-dwarf candidates were reported. Most failed to live up to their candidacy, however, because the absence of lithium showed them to be stellar objects. True stars burn their lithium within a little over 100 Myr, whereas brown dwarfs (which can, confusingly, have temperatures and luminosities similar to true stars) will not. Hence, the detection of lithium in the atmosphere of an object older than 100 Myr ensures that it is a brown dwarf.\n\nIn 1995, the study of brown dwarfs changed substantially with the discovery of two indisputable substellar objects – Teide 1 and Gliese 229B – which were identified by the presence of the 670.8 nm lithium line. The latter was found to have a temperature and luminosity well below the stellar range.\n\nIts near-infrared spectrum clearly exhibited a methane absorption band at 2 micrometres, a feature that had previously only been observed in the atmospheres of giant planets and that of Saturn's moon Titan. Methane absorption is not expected at any temperature of a main-sequence star. This discovery helped to establish yet another spectral class even cooler than L dwarfs, known as \"T dwarfs\", for which Gliese 229B is the prototype.\n\nThe first confirmed brown dwarf was discovered by Spanish astrophysicists (head of team), María Rosa Zapatero Osorio, and Eduardo Martín in 1994. This object, found in the Pleiades open cluster, received the name Teide 1. The discovery article was submitted to \"Nature\" in May 1995, and published on 14 September 1995. \"Nature\" highlighted \"Brown dwarfs discovered, official\" in the front page of that issue.\n\nTeide 1 was discovered in images collected by the IAC team on 6 January 1994 using the 80 cm telescope (IAC 80) at Teide Observatory and its spectrum was first recorded in December 1994 using the 4.2 m William Herschel Telescope at Roque de los Muchachos Observatory (La Palma). The distance, chemical composition, and age of Teide 1 could be established because of its membership in the young Pleiades star cluster. Using the most advanced stellar and substellar evolution models at that moment, the team estimated for Teide 1 a mass of , which is below the stellar-mass limit. The object became a reference in subsequent young brown dwarf related works.\n\nIn theory, a brown dwarf below is unable to burn lithium by thermonuclear fusion at any time during its evolution. This fact is one of the lithium test principles used to judge the substellar nature of low-luminosity and low-surface-temperature astronomical bodies.\n\nHigh-quality spectral data acquired by the Keck 1 telescope in November 1995 showed that Teide 1 still had the initial lithium abundance of the original molecular cloud from which Pleiades stars formed, proving the lack of thermonuclear fusion in its core. These observations confirmed that Teide 1 is a brown dwarf, as well as the efficiency of the spectroscopic lithium test.\n\nFor some time, Teide 1 was the smallest known object outside the Solar System that had been identified by direct observation. Since then, over 1,800 brown dwarfs have been identified, even some very close to Earth like Epsilon Indi Ba and Bb, a pair of brown dwarfs gravitationally bound to a Sun-like star 12 light-years from the Sun, and Luhman 16, a binary system of brown dwarfs at 6.5 light-years from the Sun.\n\nThe standard mechanism for star birth is through the gravitational collapse of a cold interstellar cloud of gas and dust. As the cloud contracts it heats due to the Kelvin–Helmholtz mechanism. Early in the process the contracting gas quickly radiates away much of the energy, allowing the collapse to continue. Eventually, the central region becomes sufficiently dense to trap radiation. Consequently, the central temperature and density of the collapsed cloud increases dramatically with time, slowing the contraction, until the conditions are hot and dense enough for thermonuclear reactions to occur in the core of the protostar. For most stars, gas and radiation pressure generated by the thermonuclear fusion reactions within the core of the star will support it against any further gravitational contraction. Hydrostatic equilibrium is reached and the star will spend most of its lifetime fusing hydrogen into helium as a main-sequence star.\n\nIf, however, the mass of the protostar is less than about , normal hydrogen thermonuclear fusion reactions will not ignite in the core. Gravitational contraction does not heat the small protostar very effectively, and before the temperature in the core can increase enough to trigger fusion, the density reaches the point where electrons become closely packed enough to create quantum electron degeneracy pressure. According to the brown dwarf interior models, typical conditions in the core for density, temperature and pressure are expected to be the following:\n\nThis means that the protostar is not massive enough and not dense enough to ever reach the conditions needed to sustain hydrogen fusion. The infalling matter is prevented, by electron degeneracy pressure, from reaching the densities and pressures needed.\n\nFurther gravitational contraction is prevented and the result is a \"failed star\", or brown dwarf that simply cools off by radiating away its internal thermal energy.\n\nLithium is generally present in brown dwarfs and not in low-mass stars. Stars, which reach the high temperature necessary for fusing hydrogen, rapidly deplete their lithium. Fusion of lithium-7 and a proton occurs producing two helium-4 nuclei. The temperature necessary for this reaction is just below that necessary for hydrogen fusion. Convection in low-mass stars ensures that lithium in the whole volume of the star is eventually depleted. Therefore, the presence of the lithium spectral line in a candidate brown dwarf is a strong indicator that it is indeed a substellar object.\n\nThe use of lithium to distinguish candidate brown dwarfs from low-mass stars is commonly referred to as the lithium test, and was pioneered by , Eduardo Martín and Antonio Magazzu. However, lithium is also seen in very young stars, which have not yet had enough time to burn it all.\n\nHeavier stars, like the Sun, can also retain lithium in their outer layers, which never get hot enough to fuse lithium, and whose convective layer does not mix with the core where the lithium would be rapidly depleted. Those larger stars are easily distinguishable from brown dwarfs by their size and luminosity.\n\nConversely, brown dwarfs at the high end of their mass range can be hot enough to deplete their lithium when they are young. Dwarfs of mass greater than can burn their lithium by the time they are half a billion years old, thus the lithium test is not perfect.\n\nUnlike stars, older brown dwarfs are sometimes cool enough that, over very long periods of time, their atmospheres can gather observable quantities of methane which cannot form in hotter objects. Dwarfs confirmed in this fashion include Gliese 229B.\n\nMain-sequence stars cool, but eventually reach a minimum bolometric luminosity that they can sustain through steady fusion. This varies from star to star, but is generally at least 0.01% that of the Sun. Brown dwarfs cool and darken steadily over their lifetimes: sufficiently old brown dwarfs will be too faint to be detectable.\n\nIron rain as part of atmospheric convection processes is possible only in brown dwarfs, and not in small stars. The spectroscopy research into iron rain is still ongoing, but not all brown dwarfs will always have this atmospheric anomaly. In 2013, a heterogeneous iron-containing atmosphere was imaged around the B component in the close Luhman 16 system.\n\nLike stars, brown dwarfs form independently, but, unlike stars, lack sufficient mass to \"ignite\". Like all stars, they can occur singly or in close proximity to other stars. Some orbit stars and can, like planets, have eccentric orbits.\n\nBrown dwarfs are all roughly the same radius as Jupiter. At the high end of their mass range (), the volume of a brown dwarf is governed primarily by electron-degeneracy pressure, as it is in white dwarfs; at the low end of the range (), their volume is governed primarily by Coulomb pressure, as it is in planets. The net result is that the radii of brown dwarfs vary by only 10–15% over the range of possible masses. This can make distinguishing them from planets difficult.\n\nIn addition, many brown dwarfs undergo no fusion; those at the low end of the mass range (under ) are never hot enough to fuse even deuterium, and even those at the high end of the mass range (over ) cool quickly enough that after 10 million years they no longer undergo fusion.\n\nX-ray and infrared spectra are telltale signs of brown dwarfs. Some emit X-rays; and all \"warm\" dwarfs continue to glow tellingly in the red and infrared spectra until they cool to planet-like temperatures (under 1000 K).\n\nGas giants have some of the characteristics of brown dwarfs. Like the Sun, Jupiter and Saturn are both made primarily of hydrogen and helium. Saturn is nearly as large as Jupiter, despite having only 30% the mass. Three of the giant planets in the Solar System (Jupiter, Saturn, and Neptune) emit much more heat than they receive from the Sun. And all four giant planets have their own \"planetary\" systems – their moons.\n\nCurrently, the International Astronomical Union considers an object above (the limiting mass for thermonuclear fusion of deuterium) to be a brown dwarf, whereas an object under that mass (and orbiting a star or stellar remnant) is considered a planet.\n\nThe 13 Jupiter-mass cutoff is a rule of thumb rather than something of precise physical significance. Larger objects will burn most of their deuterium and smaller ones will burn only a little, and the 13 Jupiter mass value is somewhere in between. The amount of deuterium burnt also depends to some extent on the composition of the object, specifically on the amount of helium and deuterium present and on the fraction of heavier elements, which determines the atmospheric opacity and thus the radiative cooling rate.\n\nThe Extrasolar Planets Encyclopaedia includes objects up to 25 Jupiter masses, and the Exoplanet Data Explorer up to 24 Jupiter masses.\n\nObjects below , called sub-brown dwarf or planetary-mass brown dwarf, form in the same manner as stars and brown dwarfs (i.e. through the collapse of a gas cloud) but have a mass below the limiting mass for thermonuclear fusion of deuterium.\n\nSome researchers call them free-floating planets, whereas others call them planetary-mass brown dwarfs.\n\nThese are brown dwarfs with a spectral class of M6.5 or later; they are also called late-M dwarfs.\n\nThe defining characteristic of spectral class M, the coolest type in the long-standing classical stellar sequence, is an optical spectrum dominated by absorption bands of titanium(II) oxide (TiO) and vanadium(II) oxide (VO) molecules. However, GD 165B, the cool companion to the white dwarf GD 165, had none of the hallmark TiO features of M dwarfs. The subsequent identification of many objects like GD 165B ultimately led to the definition of a new spectral class, the L dwarfs, defined in the red optical region of the spectrum not by metal-oxide absorption bands (TiO, VO), but by metal hydride emission bands (FeH, CrH, MgH, CaH) and prominent atomic lines of alkali metals (NaI, KI, CsI, RbI). , over 900 L dwarfs have been identified, most by wide-field surveys: the Two Micron All Sky Survey (2MASS), the Deep Near Infrared Survey of the Southern Sky (DENIS), and the Sloan Digital Sky Survey (SDSS). This spectral class contains not only the brown dwarfs, because the coolest main-sequence stars above brown dwarfs (> 80 M) have the spectral \nclass L2 or L3.\n\nAs GD 165B is the prototype of the L dwarfs, Gliese 229B is the prototype of a second new spectral class, the T dwarfs. Whereas near-infrared (NIR) spectra of L dwarfs show strong absorption bands of HO and carbon monoxide (CO), the NIR spectrum of Gliese 229B is dominated by absorption bands from methane (CH), features that were only found in the giant planets of the Solar System and Titan. CH, HO, and molecular hydrogen (H) collision-induced absorption (CIA) give Gliese 229B blue near-infrared colors. Its steeply sloped red optical spectrum also lacks the FeH and CrH bands that characterize L dwarfs and instead is influenced by exceptionally broad absorption features from the alkali metals Na and K. These differences led Kirkpatrick to propose the T spectral class for objects exhibiting H- and K-band CH absorption. , 355 T dwarfs are known. NIR classification schemes for T dwarfs have recently been developed by Adam Burgasser and Tom Geballe. Theory suggests that L dwarfs are a mixture of very-low-mass stars and sub-stellar objects (brown dwarfs), whereas the T dwarf class is composed entirely of brown dwarfs. Because of the absorption of sodium and potassium in the green part of the spectrum of T dwarfs, the actual appearance of T dwarfs to human visual perception is estimated to be not brown, but the color of magenta. T-class brown dwarfs, such as WISE 0316+4307, have been detected over 100 light-years from the Sun.\n\nThere is some doubt as to what, if anything, should be included in the class Y dwarfs. They are expected to be much cooler than T-dwarfs. They have been modelled, though there is no well-defined spectral sequence yet with prototypes.\n\nIn 2009, the coolest known brown dwarfs had estimated effective temperatures between 500 and 600 K, and have been assigned the spectral class T9. Three examples are the brown dwarfs CFBDS J005910.90-011401.3, ULAS J133553.45+113005.2, and ULAS J003402.77−005206.7. The spectra of these objects have absorption peaks around 1.55 micrometers. Delorme et al. have suggested that this feature is due to absorption from ammonia and that this should be taken as indicating the T–Y transition, making these objects of type Y0. However, the feature is difficult to distinguish from absorption by water and methane, and other authors have stated that the assignment of class Y0 is premature.\n\nIn April 2010, two newly discovered ultracool sub-brown dwarfs (UGPS 0722-05 and SDWFS 1433+35) were proposed as prototypes for spectral class Y0.\n\nIn February 2011, Luhman et al. reported the discovery of a \"brown dwarf\" companion to a nearby white dwarf with a temperature of and mass of . Though of planetary mass, Rodriguez et al. suggest it is unlikely to have formed in the same manner as planets.\n\nShortly after that, Liu et al. published an account of a \"very cold\" brown dwarf orbiting another very-low-mass brown dwarf and noted that \"Given its low luminosity, atypical colors and cold temperature, CFBDS J1458+10B is a promising candidate for the hypothesized Y spectral class.\"\n\nIn August 2011, scientists using data from NASA's Wide-field Infrared Survey Explorer (WISE) discovered six \"Y dwarfs\"—star-like bodies with temperatures as cool as the human body.\n\nWISE data has revealed hundreds of new brown dwarfs. Of these, fourteen are classified as cool Ys. One of the Y dwarfs, called WISE 1828+2650, was, as of August 2011, the record holder for the coldest brown dwarf – emitting no visible light at all, this type of object resembles free-floating planets more than stars. WISE 1828+2650 was initially estimated to have an atmospheric temperature cooler than 300 K—for comparison, the upper end of room temperature is . Its temperature has since been revised and newer estimates put it in the range of .\n\nIn April 2014, WISE 0855−0714 was announced with a temperature profile estimated around 225 to 260 K (−48 to −13 °C; −55 to 8 °F) and a mass of . It was also unusual in that its observed parallax meant a distance close to 7.2±0.7 light years from the Solar System.\n\nThe majority of flux emitted by L and T dwarfs is in the 1 to 2.5 micrometre near-infrared range. Low and decreasing temperatures through the late M-, L-, and T-dwarf sequence result in a rich near-infrared spectrum containing a wide variety of features, from relatively narrow lines of neutral atomic species to broad molecular bands, all of which have different dependencies on temperature, gravity, and metallicity. Furthermore, these low temperature conditions favor condensation out of the gas state and the formation of grains.\n\nTypical atmospheres of known brown dwarfs range in temperature from 2200 down to 750 K. Compared to stars, which warm themselves with steady internal fusion, brown dwarfs cool quickly over time; more massive dwarfs cool more slowly than less massive ones.\n\nCoronagraphs have recently been used to detect faint objects orbiting bright visible stars, including Gliese 229B.\n\nSensitive telescopes equipped with charge-coupled devices (CCDs) have been used to search distant star clusters for faint objects, including Teide 1.\n\nWide-field searches have identified individual faint objects, such as Kelu-1 (30 ly away).\n\nBrown dwarfs are often discovered in surveys to discover extrasolar planets. Methods of detecting extrasolar planets work for brown dwarfs as well, although brown dwarfs are much easier to detect.\n\nBrown dwarfs can be powerful emitters of radio emission due to their strong magnetic fields. Observing programs at the Arecibo Observatory and the Very Large Array have detected over a dozen such objects, which are also called ultracool dwarfs because they share common magnetic properties with other objects in this class. The detection of radio emission from brown dwarfs permits their magnetic field strengths to be measured directly.\n\n\nFirst methane brown dwarf verified. Gliese 229B is discovered orbiting red dwarf Gliese 229A (20 ly away) using an adaptive optics coronagraph to sharpen images from the 60-inch (1.5 m) reflecting telescope at Palomar Observatory on Southern California's Mt. Palomar; follow-up infrared spectroscopy made with their 200-inch (5 m) Hale telescope shows an abundance of methane.\n\nX-ray flares detected from brown dwarfs since 1999 suggest changing magnetic fields within them, similar to those in very-low-mass stars.\n\nWith no strong central nuclear energy source, the interior of a brown dwarf is in a rapid boiling, or convective state. When combined with the rapid rotation that most brown dwarfs exhibit, convection sets up conditions for the development of a strong, tangled magnetic field near the surface. The flare observed by Chandra from LP 944-20 could have its origin in the turbulent magnetized hot material beneath the brown dwarf's surface. A sub-surface flare could conduct heat to the atmosphere, allowing electric currents to flow and produce an X-ray flare, like a stroke of lightning. The absence of X-rays from LP 944-20 during the non-flaring period is also a significant result. It sets the lowest observational limit on steady X-ray power produced by a brown dwarf, and shows that coronas cease to exist as the surface temperature of a brown dwarf cools below about 2800K and becomes electrically neutral.\n\nUsing NASA's Chandra X-ray Observatory, scientists have detected X-rays from a low-mass brown dwarf in a multiple star system. This is the first time that a brown dwarf this close to its parent star(s) (Sun-like stars TWA 5A) has been resolved in X-rays. \"Our Chandra data show that the X-rays originate from the brown dwarf's coronal plasma which is some 3 million degrees Celsius\", said Yohko Tsuboi of Chuo University in Tokyo. \"This brown dwarf is as bright as the Sun today in X-ray light, while it is fifty times less massive than the Sun\", said Tsuboi. \"This observation, thus, raises the possibility that even massive planets might emit X-rays by themselves during their youth!\"\n\nBrown dwarfs can maintain magnetic fields of up to 6 kG in strength. Approximately 5-10% of brown dwarfs appear to have strong magnetic fields and emit radio waves, and there may be as many as 40 magnetic brown dwarfs within 25 pc of the Sun based on Monte Carlo modeling and their average spatial density. The regular, periodic reversal of radio wave orientation may indicate that brown dwarf magnetic fields periodically reverse orientation. These reversals may be the result of a brown dwarf magnetic activity cycle, similar to the solar cycle.\n\nThe brown dwarf Cha 110913-773444, located 500 light years away in the constellation Chamaeleon, may be in the process of forming a miniature planetary system. Astronomers from Pennsylvania State University have detected what they believe to be a disk of gas and dust similar to the one hypothesized to have formed the Solar System. Cha 110913-773444 is the smallest brown dwarf found to date (), and if it formed a planetary system, it would be the smallest known object to have one. Their findings were published in the December 10, 2005 issue of Astrophysical Journal Letters.\n\nRecent observations of known brown dwarf candidates have revealed a pattern of brightening and dimming of infrared emissions that suggests relatively cool, opaque cloud patterns obscuring a hot interior that is stirred by extreme winds. The weather on such bodies is thought to be extremely violent, comparable to but far exceeding Jupiter's famous storms.\n\nOn January 8, 2013 astronomers using NASA's Hubble and Spitzer space telescopes probed the stormy atmosphere of a brown dwarf named 2MASS J22282889-431026, creating the most detailed \"weather map\" of a brown dwarf thus far. It shows wind-driven, planet-sized clouds. The new research is a stepping stone toward a better understanding not only brown dwarfs, but also of the atmospheres of planets beyond the Solar System.\n\nNASA's WISE mission has detected 200 new brown dwarfs. There are actually fewer brown dwarfs in our cosmic neighborhood than previously thought. Rather than one star for every brown dwarf, there may be as many as six stars for every brown dwarf.\n\nIn a study published in Aug 2017 NASA's Spitzer Space Telescope monitored infrared brightness variations in brown dwarfs caused by cloud cover of variable thickness. The observations revealed that large-scale waves propagating in the atmospheres of brown dwarfs (similarly to the atmosphere of Neptune and other Solar System giant planets). These atmospheric waves modulate the thickness of the clouds and propagate with different velocities (probably due to differential rotation).\n\nThe super-Jupiter planetary-mass objects 2M1207b and 2MASS J044144 that are orbiting brown dwarfs at large orbital distances may have formed by cloud collapse rather than accretion and so may be sub-brown dwarfs rather than planets, which is inferred from relatively large masses and large orbits. The first discovery of a low-mass companion orbiting a brown dwarf (ChaHα8) at a small orbital distance using the radial velocity technique paved the way for the detection of planets around brown dwarfs on orbits of a few AU or smaller. However, with a mass ratio between the companion and primary in ChaHα8 of about 0.3, this system rather resembles a binary star. Then, in 2013, the first planetary-mass companion (OGLE-2012-BLG-0358L b) in a relatively small orbit was discovered orbiting a brown dwarf.\nIn 2015, the first terrestrial-mass planet orbiting a brown dwarf was found, OGLE-2013-BLG-0723LBb.\n\nDisks around brown dwarfs have been found to have many of the same features as disks around stars; therefore, it is expected that there will be accretion-formed planets around brown dwarfs. Given the small mass of brown dwarf disks, most planets will be terrestrial planets rather than gas giants. If a giant planet orbits a brown dwarf across our line of sight, then, because they have approximately the same diameter, this would give a large signal for detection by transit. The accretion zone for planets around a brown dwarf is very close to the brown dwarf itself, so tidal forces would have a strong effect.\n\nPlanets around brown dwarfs are likely to be carbon planets depleted of water.\n\nA 2016 study, based upon observations with Spitzer estimates that 175 brown dwarfs need to be monitored in order to guarantee (95%) at least one detection of a planet.\n\nHabitability for hypothetical planets orbiting brown dwarfs has been studied. Computer models suggesting conditions for these bodies to have habitable planets are very stringent, the habitable zone being narrow and decreasing with time, due to the cooling of the brown dwarf. The orbits there would have to be of extremely low eccentricity (of the order of 10) to avoid strong tidal forces that would trigger a greenhouse effect on the planets, rendering them uninhabitable.\n\n\n\n\n\n\n"}
{"id": "8046937", "url": "https://en.wikipedia.org/wiki?curid=8046937", "title": "Caucasus mixed forests", "text": "Caucasus mixed forests\n\nThe Caucasus mixed forests ecoregion, in the Temperate broadleaf and mixed forests Biome, of Eurasia in Western Asia and Eastern Europe. It covers the Caucasus Mountains range, which forms the traditional border between Europe and Asia, as well as the adjacent Lesser Caucasus range and the eastern end of the Pontic Mountains. As such, its highest point is Mount Elbrus.\n\nHaving the Lagodekhi Protected Areas, Borjomi-Kharagauli and Tusheti National Parks, and Zakatala and Caucasus Biosphere Reserves, the ecoregion covers an area of , extending across portions of:\n\n"}
{"id": "52896780", "url": "https://en.wikipedia.org/wiki?curid=52896780", "title": "Corrobinnie Hill Conservation Park", "text": "Corrobinnie Hill Conservation Park\n\nCorrobinnie Hill Conservation Park is a protected area in the Australian state of South Australia located on the Eyre Peninsula in the gazetted locality of Pinkawillinie about west of the town centre in Kyancutta.\n\nThe conservation park was proclaimed on 25 August 1983 under the state’s \"National Parks and Wildlife Act 1972\". Its name was derived from Corrobinnie Hill which is located within its boundaries. As of June 2016, the conservation park covered an area of .\n\nThe surrounding the conservation park which is crown land and which is described in cadastral terms as “allotment 1 of Deposited Plan No. 33659, Hundred of Hill” was dedicated on 11 November 1993 under the \"Crown Lands Act 1929\" as part of the conservation reserve known as the Pinkawillinie Conservation Reserve. On 21 November 2002, the surrounding land was added by proclamation to the Pinkawillinie Conservation Park.\n\nAs of 2010, the conservation park was operated in association with the Pinkawillinie Conservation Park. It can only be accessed by four-wheel drive vehicle only via the “Number 17 Stock Route.” It is reported as being “one of the most popular attractions” in the Pinkawillinie Conservation Park due to the presence of Corrobinnie Hill, a hill “consisting of unusually shaped, weathered, granite rocks.”\n\nThe conservation park is classified as an IUCN Category III protected area.\n\n\n"}
{"id": "47687", "url": "https://en.wikipedia.org/wiki?curid=47687", "title": "Cosmic ray", "text": "Cosmic ray\n\nCosmic rays are high-energy radiation, mainly originating outside the Solar System and even from distant galaxies. Upon impact with the Earth's atmosphere, cosmic rays can produce showers of secondary particles that sometimes reach the surface. Composed primarily of high-energy protons and atomic nuclei, they are of uncertain origin. Data from the Fermi Space Telescope (2013) have been interpreted as evidence that a significant fraction of primary cosmic rays originate from the supernova explosions of stars. Active galactic nuclei also appear to produce cosmic rays, based on observations of neutrinos and gamma rays from blazar TXS 0506+056 in 2018.\n\nThe term \"ray\" is somewhat of a misnomer due to a historical accident, as cosmic rays were at first, and wrongly, thought to be mostly electromagnetic radiation. In common scientific usage, high-energy particles with intrinsic mass are known as \"cosmic\" rays, while photons, which are quanta of electromagnetic radiation (and so have no intrinsic mass) are known by their common names, such as \"gamma rays\" or \"X-rays\", depending on their photon energy.\n\nIn current usage, the term \"cosmic ray\" almost exclusively refers to massive particles – those that have rest mass – as opposed to photons, which have no rest mass. Massive particles have additional, kinetic, mass-energy when they are moving, due to relativistic effects. Through this process, some particles acquire tremendously high mass-energies. These are significantly higher than the photon energy of even the highest-energy photons detected to date. The energy of the massless photon depends solely on frequency, not speed, as photons always travel at the same speed. At the higher end of the energy spectrum, relativistic kinetic energy is the main source of the mass-energy of cosmic rays.\n\nThe highest-energy fermionic cosmic rays detected to date, such as the Oh-My-God particle, had an energy of about , while the highest-energy gamma rays to be observed, very-high-energy gamma rays, are photons with energies of up to . Hence, the highest-energy detected fermionic cosmic rays are about times as energetic as the highest-energy detected cosmic photons.\n\nOf primary cosmic rays, which originate outside of Earth's atmosphere, about 99% are the nuclei of well-known atoms (stripped of their electron shells), and about 1% are solitary electrons (similar to beta particles). Of the nuclei, about 90% are simple protons (i.e., hydrogen nuclei); 9% are alpha particles, identical to helium nuclei; and 1% are the nuclei of heavier elements, called HZE ions. A very small fraction are stable particles of antimatter, such as positrons or antiprotons. The precise nature of this remaining fraction is an area of active research. An active search from Earth orbit for anti-alpha particles has failed to detect them.\n\nCosmic rays attract great interest practically, due to the damage they inflict on microelectronics and life outside the protection of an atmosphere and magnetic field, and scientifically, because the energies of the most energetic ultra-high-energy cosmic rays (UHECRs) have been observed to approach , about 40 million times the energy of particles accelerated by the Large Hadron Collider. One can show that such enormous energies might be achieved by means of the centrifugal mechanism of acceleration in active galactic nuclei. At 50J, the highest-energy ultra-high-energy cosmic rays have energies comparable to the kinetic energy of a baseball. As a result of these discoveries, there has been interest in investigating cosmic rays of even greater energies. Most cosmic rays, however, do not have such extreme energies; the energy distribution of cosmic rays peaks on .\n\nAfter the discovery of radioactivity by Henri Becquerel in 1896, it was generally believed that atmospheric electricity, ionization of the air, was caused only by radiation from radioactive elements in the ground or the radioactive gases or isotopes of radon they produce. Measurements of ionization rates at increasing heights above the ground during the decade from 1900 to 1910 showed a decrease that could be explained as due to absorption of the ionizing radiation by the intervening air.\n\nIn 1909, Theodor Wulf developed an electrometer, a device to measure the rate of ion production inside a hermetically sealed container, and used it to show higher levels of radiation at the top of the Eiffel Tower than at its base. However, his paper published in \"Physikalische Zeitschrift\" was not widely accepted. In 1911, Domenico Pacini observed simultaneous variations of the rate of ionization over a lake, over the sea, and at a depth of 3 metres from the surface. Pacini concluded from the decrease of radioactivity underwater that a certain part of the ionization must be due to sources other than the radioactivity of the Earth. \n\nIn 1912, Victor Hess carried three enhanced-accuracy Wulf electrometers to an altitude of 5,300 metres in a free balloon flight. He found the ionization rate increased approximately fourfold over the rate at ground level. Hess ruled out the Sun as the radiation's source by making a balloon ascent during a near-total eclipse. With the moon blocking much of the Sun's visible radiation, Hess still measured rising radiation at rising altitudes. He concluded that \"The results of the observations seem most likely to be explained by the assumption that radiation of very high penetrating power enters from above into our atmosphere.\" In 1913–1914, Werner Kolhörster confirmed Victor Hess's earlier results by measuring the increased ionization enthalpy rate at an altitude of 9 km. Hess received the Nobel Prize in Physics in 1936 for his discovery.\n\nThe Hess balloon flight took place on 7 August 1912. By sheer coincidence, exactly 100 years later on 7 August 2012, the Mars Science Laboratory rover used its Radiation Assessment Detector (RAD) instrument to begin measuring the radiation levels on another planet for the first time. On 31 May 2013, NASA scientists reported that a possible manned mission to Mars may involve a greater radiation risk than previously believed, based on the amount of energetic particle radiation detected by the RAD on the Mars Science Laboratory while traveling from the Earth to Mars in 2011–2012.\n\nBruno Rossi wrote that:\nIn the late 1920s and early 1930s the technique of self-recording electroscopes carried by balloons into the highest layers of the atmosphere or sunk to great depths under water was brought to an unprecedented degree of perfection by the German physicist Erich Regener and his group. To these scientists we owe some of the most accurate measurements ever made of cosmic-ray ionization as a function of altitude and depth.\n\nErnest Rutherford stated in 1931 that \"thanks to the fine experiments of Professor Millikan and the even more far-reaching experiments of Professor Regener, we have now got for the first time, a curve of absorption of these radiations in water which we may safely rely upon\".\n\nIn the 1920s, the term \"cosmic rays\" was coined by Robert Millikan who made measurements of ionization due to cosmic rays from deep under water to high altitudes and around the globe. Millikan believed that his measurements proved that the primary cosmic rays were gamma rays; i.e., energetic photons. And he proposed a theory that they were produced in interstellar space as by-products of the fusion of hydrogen atoms into the heavier elements, and that secondary electrons were produced in the atmosphere by Compton scattering of gamma rays. But then, sailing from Java to the Netherlands in 1927, Jacob Clay found evidence, later confirmed in many experiments, of a variation of cosmic ray intensity with latitude, which indicated that the primary cosmic rays are deflected by the geomagnetic field and must therefore be charged particles, not photons. In 1929, Bothe and Kolhörster discovered charged cosmic-ray particles that could penetrate 4.1 cm of gold. Charged particles of such high energy could not possibly be produced by photons from Millikan's proposed interstellar fusion process.\n\nIn 1930, Bruno Rossi predicted a difference between the intensities of cosmic rays arriving from the east and the west that depends upon the charge of the primary particles—the so-called \"east-west effect.\" Three independent experiments found that the intensity is, in fact, greater from the west, proving that most primaries are positive. During the years from 1930 to 1945, a wide variety of investigations confirmed that the primary cosmic rays are mostly protons, and the secondary radiation produced in the atmosphere is primarily electrons, photons and muons. In 1948, observations with nuclear emulsions carried by balloons to near the top of the atmosphere showed that approximately 10% of the primaries are helium nuclei (alpha particles) and 1% are heavier nuclei of the elements such as carbon, iron, and lead.\n\nDuring a test of his equipment for measuring the east-west effect, Rossi observed that the rate of near-simultaneous discharges of two widely separated Geiger counters was larger than the expected accidental rate. In his report on the experiment, Rossi wrote \"... it seems that once in a while the recording equipment is struck by very extensive showers of particles, which causes coincidences between the counters, even placed at large distances from one another.\" In 1937 Pierre Auger, unaware of Rossi's earlier report, detected the same phenomenon and investigated it in some detail. He concluded that high-energy primary cosmic-ray particles interact with air nuclei high in the atmosphere, initiating a cascade of secondary interactions that ultimately yield a shower of electrons, and photons that reach ground level.\n\nSoviet physicist Sergey Vernov was the first to use radiosondes to perform cosmic ray readings with an instrument carried to high altitude by a balloon. On 1 April 1935, he took measurements at heights up to 13.6 kilometres using a pair of Geiger counters in an anti-coincidence circuit to avoid counting secondary ray showers.\n\nHomi J. Bhabha derived an expression for the probability of scattering positrons by electrons, a process now known as Bhabha scattering. His classic paper, jointly with Walter Heitler, published in 1937 described how primary cosmic rays from space interact with the upper atmosphere to produce particles observed at the ground level. Bhabha and Heitler explained the cosmic ray shower formation by the cascade production of gamma rays and positive and negative electron pairs.\n\nMeasurements of the energy and arrival directions of the ultra-high-energy primary cosmic rays by the techniques of \"density sampling\" and \"fast timing\" of extensive air showers were first carried out in 1954 by members of the Rossi Cosmic Ray Group at the Massachusetts Institute of Technology. The experiment employed eleven scintillation detectors arranged within a circle 460 metres in diameter on the grounds of the Agassiz Station of the Harvard College Observatory. From that work, and from many other experiments carried out all over the world, the energy spectrum of the primary cosmic rays is now known to extend beyond 10 eV. A huge air shower experiment called the Auger Project is currently operated at a site on the pampas of Argentina by an international consortium of physicists, led by James Cronin, winner of the 1980 Nobel Prize in Physics from the University of Chicago, and Alan Watson of the University of Leeds. Their aim is to explore the properties and arrival directions of the very highest-energy primary cosmic rays. The results are expected to have important implications for particle physics and cosmology, due to a theoretical Greisen–Zatsepin–Kuzmin limit to the energies of cosmic rays from long distances (about 160 million light years) which occurs above 10 eV because of interactions with the remnant photons from the Big Bang origin of the universe.\n\nHigh-energy gamma rays (>50MeV photons) were finally discovered in the primary cosmic radiation by an MIT experiment carried on the OSO-3 satellite in 1967. Components of both galactic and extra-galactic origins were separately identified at intensities much less than 1% of the primary charged particles. Since then, numerous satellite gamma-ray observatories have mapped the gamma-ray sky. The most recent is the Fermi Observatory, which has produced a map showing a narrow band of gamma ray intensity produced in discrete and diffuse sources in our galaxy, and numerous point-like extra-galactic sources distributed over the celestial sphere.\n\nEarly speculation on the sources of cosmic rays included a 1934 proposal by Baade and Zwicky suggesting cosmic rays originated from supernovae. A 1948 proposal by Horace W. Babcock suggested that magnetic variable stars could be a source of cosmic rays. Subsequently, in 1951, Y. Sekido \"et al.\" identified the Crab Nebula as a source of cosmic rays. Since then, a wide variety of potential sources for cosmic rays began to surface, including supernovae, active galactic nuclei, quasars, and gamma-ray bursts.\nLater experiments have helped to identify the sources of cosmic rays with greater certainty. In 2009, a paper presented at the International Cosmic Ray Conference (ICRC) by scientists at the Pierre Auger Observatory showed ultra-high energy cosmic rays (UHECRs) originating from a location in the sky very close to the radio galaxy Centaurus A, although the authors specifically stated that further investigation would be required to confirm Cen A as a source of cosmic rays. However, no correlation was found between the incidence of gamma-ray bursts and cosmic rays, causing the authors to set upper limits as low as 3.4 × 10 erg·cm on the flux of cosmic rays from gamma-ray bursts.\n\nIn 2009, supernovae were said to have been \"pinned down\" as a source of cosmic rays, a discovery made by a group using data from the Very Large Telescope. This analysis, however, was disputed in 2011 with data from PAMELA, which revealed that \"spectral shapes of [hydrogen and helium nuclei] are different and cannot be described well by a single power law\", suggesting a more complex process of cosmic ray formation. In February 2013, though, research analyzing data from \"Fermi\" revealed through an observation of neutral pion decay that supernovae were indeed a source of cosmic rays, with each explosion producing roughly 3 × 10 – 3 × 10J of cosmic rays. However, supernovae do not produce all cosmic rays, and the proportion of cosmic rays that they do produce is a question which cannot be answered without further study.\n\nCosmic rays can be divided into two types, galactic cosmic rays (GCR), high-energy particles originating outside the solar system, and solar energetic particles, high-energy particles (predominantly protons) emitted by the sun, primarily in solar particle events. However, the term \"cosmic ray\" is often used to refer to only the GCR flux. Despite the nomenclature \"galactic\", GCRs may originate within or outside the galaxy (as discussed in the source section above).\nCosmic rays originate as primary cosmic rays, which are those originally produced in various astrophysical processes. Primary cosmic rays are composed primarily of protons and alpha particles (99%), with a small amount of heavier nuclei (~1%) and an extremely minute proportion of positrons and antiprotons. Secondary cosmic rays, caused by a decay of primary cosmic rays as they impact an atmosphere, include neutrons, pions, positrons, and muons. Of these four, the latter three were first detected in cosmic rays.\n\nPrimary cosmic rays primarily originate from outside the Solar system and sometimes even the Milky Way. When they interact with Earth's atmosphere, they are converted to secondary particles. The mass ratio of helium to hydrogen nuclei, 28%, is similar to the primordial elemental abundance ratio of these elements, 24%. The remaining fraction is made up of the other heavier nuclei that are typical nucleosynthesis end products, primarily lithium, beryllium, and boron. These nuclei appear in cosmic rays in much greater abundance (~1%) than in the solar atmosphere, where they are only about 10 as abundant as helium. Cosmic rays made up of charged nuclei heavier than helium are called HZE ions. Due to the high charge and heavy nature of HZE ions, their contribution to an astronaut's radiation dose in space is significant even though they are relatively scarce.\n\nThis abundance difference is a result of the way secondary cosmic rays are formed. Carbon and oxygen nuclei collide with interstellar matter to form lithium, beryllium and boron in a process termed cosmic ray spallation. Spallation is also responsible for the abundances of scandium, titanium, vanadium, and manganese ions in cosmic rays produced by collisions of iron and nickel nuclei with interstellar matter.\n\nSatellite experiments have found evidence of positrons and a few antiprotons in primary cosmic rays, amounting to less than 1% of the particles in primary cosmic rays. These do not appear to be the products of large amounts of antimatter from the Big Bang, or indeed complex antimatter in the universe. Rather, they appear to consist of only these two elementary particles, newly made in energetic processes.\n\nPreliminary results from the presently operating Alpha Magnetic Spectrometer (\"AMS-02\") on board the International Space Station show that positrons in the cosmic rays arrive with no directionality. In September, 2014, new results with almost twice as much data were presented in a talk at CERN and published in Physical Review Letters. A new measurement of positron fraction up to 500 GeV was reported, showing that positron fraction peaks at a maximum of about 16% of total electron+positron events, around an energy of . At higher energies, up to 500GeV, the ratio of positrons to electrons begins to fall again. The absolute flux of positrons also begins to fall before 500GeV, but peaks at energies far higher than electron energies, which peak about 10GeV. These results on interpretation have been suggested to be due to positron production in annihilation events of massive dark matter particles.\n\nCosmic ray antiprotons also have a much higher average energy than their normal-matter counterparts (protons). They arrive at Earth with a characteristic energy maximum of 2 GeV, indicating their production in a fundamentally different process from cosmic ray protons, which on average have only one-sixth of the energy.\n\nThere is no evidence of complex antimatter atomic nuclei, such as antihelium nuclei (i.e., anti-alpha particles), in cosmic rays. These are actively being searched for. A prototype of the \"AMS-02\" designated \"AMS-01\", was flown into space aboard the on STS-91 in June 1998. By not detecting any antihelium at all, the \"AMS-01\" established an upper limit of for the antihelium to helium flux ratio.\nWhen cosmic rays enter the Earth's atmosphere they collide with atoms and molecules, mainly oxygen and nitrogen. The interaction produces a cascade of lighter particles, a so-called air shower secondary radiation that rains down, including x-rays, muons, protons, alpha particles, pions, electrons, and neutrons. All of the produced particles stay within about one degree of the primary particle's path.\n\nTypical particles produced in such collisions are neutrons and charged mesons such as positive or negative pions and kaons. Some of these subsequently decay into muons, which are able to reach the surface of the Earth, and even penetrate for some distance into shallow mines. The muons can be easily detected by many types of particle detectors, such as cloud chambers, bubble chambers or scintillation detectors. The observation of a secondary shower of particles in multiple detectors at the same time is an indication that all of the particles came from that event.\n\nCosmic rays impacting other planetary bodies in the Solar System are detected indirectly by observing high-energy gamma ray emissions by gamma-ray telescope. These are distinguished from radioactive decay processes by their higher energies above about 10 MeV.\n\nThe flux of incoming cosmic rays at the upper atmosphere is dependent on the solar wind, the Earth's magnetic field, and the energy of the cosmic rays. At distances of ~94 AU from the Sun, the solar wind undergoes a transition, called the termination shock, from supersonic to subsonic speeds. The region between the termination shock and the heliopause acts as a barrier to cosmic rays, decreasing the flux at lower energies (≤1 GeV) by about 90%. However, the strength of the solar wind is not constant, and hence it has been observed that cosmic ray flux is correlated with solar activity.\n\nIn addition, the Earth's magnetic field acts to deflect cosmic rays from its surface, giving rise to the observation that the flux is apparently dependent on latitude, longitude, and azimuth angle.\n\nThe combined effects of all of the factors mentioned contribute to the flux of cosmic rays at Earth's surface. The following table of participial frequencies reach the planet and are inferred from lower energy radiation reaching the ground.\n\nIn the past, it was believed that the cosmic ray flux remained fairly constant over time. However, recent research suggests one-and-a-half- to two-fold millennium-timescale changes in the cosmic ray flux in the past forty thousand years.\n\nThe magnitude of the energy of cosmic ray flux in interstellar space is very comparable to that of other deep space energies: cosmic ray energy density averages about one electron-volt per cubic centimetre of interstellar space, or ~1eV/cm, which is comparable to the energy density of visible starlight at 0.3eV/cm, the galactic magnetic field energy density (assumed 3 microgauss) which is ~0.25eV/cm, or the cosmic microwave background (CMB) radiation energy density at ~ 0.25eV/cm.\n\nThere are several ground-based methods of detecting cosmic rays currently in use. The first detection method is called the air Cherenkov telescope, designed to detect low-energy (<200 GeV) cosmic rays by means of analyzing their Cherenkov radiation, which for cosmic rays are gamma rays emitted as they travel faster than the speed of light in their medium, the atmosphere. While these telescopes are extremely good at distinguishing between background radiation and that of cosmic-ray origin, they can only function well on clear nights without the Moon shining, and have very small fields of view and are only active for a few percent of the time. Another Cherenkov telescope uses water as a medium through which particles pass and produce Cherenkov radiation to make them detectable.\n\nExtensive air shower (EAS) arrays, a second detection method, measure the charged particles which pass through them. EAS arrays measure much higher-energy cosmic rays than air Cherenkov telescopes, and can observe a broad area of the sky and can be active about 90% of the time. However, they are less able to segregate background effects from cosmic rays than can air Cherenkov telescopes. EAS arrays employ plastic scintillators in order to detect particles.\n\nAnother method was developed by Robert Fleischer, P. Buford Price, and Robert M. Walker for use in high-altitude balloons. In this method, sheets of clear plastic, like 0.25 mm Lexan polycarbonate, are stacked together and exposed directly to cosmic rays in space or high altitude. The nuclear charge causes chemical bond breaking or ionization in the plastic. At the top of the plastic stack the ionization is less, due to the high cosmic ray speed. As the cosmic ray speed decreases due to deceleration in the stack, the ionization increases along the path. The resulting plastic sheets are \"etched\" or slowly dissolved in warm caustic sodium hydroxide solution, that removes the surface material at a slow, known rate. The caustic sodium hydroxide dissolves the plastic at a faster rate along the path of the ionized plastic. The net result is a conical etch pit in the plastic. The etch pits are measured under a high-power microscope (typically 1600× oil-immersion), and the etch rate is plotted as a function of the depth in the stacked plastic.\n\nThis technique yields a unique curve for each atomic nucleus from 1 to 92, allowing identification of both the charge and energy of the cosmic ray that traverses the plastic stack. The more extensive the ionization along the path, the higher the charge. In addition to its uses for cosmic-ray detection, the technique is also used to detect nuclei created as products of nuclear fission.\n\nA fourth method involves the use of cloud chambers to detect the secondary muons created when a pion decays. Cloud chambers in particular can be built from widely available materials and can be constructed even in a high-school laboratory. A fifth method, involving bubble chambers, can be used to detect cosmic ray particles.\n\nAnother method detects the light from nitrogen fluorescence caused by the excitation of nitrogen in the atmosphere by the shower of particles moving through the atmosphere. This method allows for accurate detection of the direction from which the cosmic ray came.\n\nMore recently, the CMOS devices in pervasive smartphone cameras have been proposed as a practical distributed network to detect air showers from ultra-high-energy cosmic rays (UHECRs) which is at least comparable with that of conventional cosmic ray detectors. The first app, to exploit this proposition was the CRAYFIS (Cosmic RAYs Found In Smartphones) experiment. Then, in 2017, the CREDO (Cosmic Ray Extremely Distributed Observatory) Collaboration released the first version of its completely open source app for Android devices. Since then the collaboration has attracted the interest and support of many scientific institutions, educational institutions and members of the public around the world .\n\nCosmic rays ionize the nitrogen and oxygen molecules in the atmosphere, which leads to a number of chemical reactions. Cosmic rays are also responsible for the continuous production of a number of unstable isotopes in the Earth's atmosphere, such as carbon-14, via the reaction:\n\nCosmic rays kept the level of carbon-14 in the atmosphere roughly constant (70 tons) for at least the past 100,000 years, until the beginning of above-ground nuclear weapons testing in the early 1950s. This is an important fact used in radiocarbon dating used in archaeology.\n\n\nCosmic rays constitute a fraction of the annual radiation exposure of human beings on the Earth, averaging 0.39mSv out of a total of 3mSv per year (13% of total background) for the Earth's population. However, the background radiation from cosmic rays increases with altitude, from 0.3mSv per year for sea-level areas to 1.0mSv per year for higher-altitude cities, raising cosmic radiation exposure to a quarter of total background radiation exposure for populations of said cities. Airline crews flying long distance high-altitude routes can be exposed to 2.2mSv of extra radiation each year due to cosmic rays, nearly doubling their total exposure to ionizing radiation.\n\nCosmic rays have sufficient energy to alter the states of circuit components in electronic integrated circuits, causing transient errors to occur (such as corrupted data in electronic memory devices or incorrect performance of CPUs) often referred to as \"soft errors.\" This has been a problem in electronics at extremely high-altitude, such as in satellites, but with transistors becoming smaller and smaller, this is becoming an increasing concern in ground-level electronics as well. Studies by IBM in the 1990s suggest that computers typically experience about one cosmic-ray-induced error per 256 megabytes of RAM per month. To alleviate this problem, the Intel Corporation has proposed a cosmic ray detector that could be integrated into future high-density microprocessors, allowing the processor to repeat the last command following a cosmic-ray event.\n\nIn 2008, data corruption in a flight control system caused an Airbus A330 airliner to twice plunge hundreds of feet, resulting in injuries to multiple passengers and crew members. Cosmic rays were investigated among other possible causes of the data corruption, but were ultimately ruled out as being very unlikely.\n\nGalactic cosmic rays are one of the most important barriers standing in the way of plans for interplanetary travel by crewed spacecraft. Cosmic rays also pose a threat to electronics placed aboard outgoing probes. In 2010, a malfunction aboard the Voyager 2 space probe was credited to a single flipped bit, probably caused by a cosmic ray. Strategies such as physical or magnetic shielding for spacecraft have been considered in order to minimize the damage to electronics and human beings caused by cosmic rays.\n\nFlying high, passengers and crews of jet airliners are exposed to at least 10 times the cosmic ray dose that people at sea level receive. Aircraft flying polar routes near the geomagnetic poles are at particular risk.\n\nCosmic rays have been implicated in the triggering of electrical breakdown in lightning. It has been proposed that essentially all lightning is triggered through a relativistic process, \"runaway breakdown\", seeded by cosmic ray secondaries. Subsequent development of the lightning discharge then occurs through \"conventional breakdown\" mechanisms.\n\nA role for cosmic rays in climate was suggested by Edward P. Ney in 1959 and by Robert E. Dickinson in 1975. It has been postulated that cosmic rays may have been responsible for major climatic change and mass-extinction in the past. According to Adrian Mellott and Mikhail Medvedev, 62-million-year cycles in biological marine populations correlate with the motion of the Earth relative to the galactic plane and increases in exposure to cosmic rays. The researchers suggest that this and gamma ray bombardments deriving from local supernovae could have affected cancer and mutation rates, and might be linked to decisive alterations in the Earth's climate, and to the mass-extinctions of the Ordovician.\n\nDanish physicist Henrik Svensmark has controversially argued that because solar variation modulates the cosmic ray flux on Earth, they would consequently affect the rate of cloud formation and hence be an indirect cause of global warming. Svensmark is one of several scientists outspokenly opposed to the mainstream scientific assessment of global warming, leading to concerns that the proposition that cosmic rays are connected to global warming could be ideologically biased rather than scientifically based. Other scientists have vigorously criticized Svensmark for sloppy and inconsistent work: one example is adjustment of cloud data that understates error in lower cloud data, but not in high cloud data; another example is \"incorrect handling of the physical data\" resulting in graphs that do not show the correlations they claim to show. Despite Svensmark's assertions, galactic cosmic rays have shown no statistically significant influence on changes in cloud cover, and demonstrated to have no causal relationship to changes in global temperature.\n\nThere are a number of cosmic-ray research initiatives, listed below.\n\n\n\n\n"}
{"id": "78978", "url": "https://en.wikipedia.org/wiki?curid=78978", "title": "Daphne", "text": "Daphne\n\nDaphne (; , meaning \"laurel\") a minor figure in Greek mythology, is a naiad, a variety of female nymph associated with fountains, wells, springs, streams, brooks and other bodies of freshwater. She is said by ancient sources variously to have been a daughter of the river god Peneus and the nymph Creusa in Thessaly (Hyginus \"Fabulae\" 203) or of Ladon (the river Ladon in Arcadia) or Pineios, and to Ge (or Gaia) (Pausanias and others).\nThere are several versions of the myth in which she appears, but the general narrative appears in Greco-Roman mythology, is that due to a curse made by the god Cupid, son of Venus, on the god Apollo (Phoebus), she became the unwilling object of the infatuation of Apollo, who chased her against his wish. Just before being overtaken by him, Daphne pleaded to her rivergod father for help, who transformed her into a laurel tree, thus foiling Apollo.\n\nThenceforth Apollo developed a special reverence for laurel. At the Pythian Games which were held every four years in Delphi in honour of Apollo, a wreath of laurel gathered from the Vale of Tempe in Thessaly was given as a prize. Hence it later became customary to award prizes in the form of laurel wreaths to victorious generals, athletes, poets and musicians, worn as a chaplet on the head. The Poet Laureate is a well-known modern example of such a prize-winner, dating from the early Renaissance in Italy. According to Pausanias the reason for this was \"simply and solely because the prevailing tradition has it that Apollo fell in love with the daughter of Ladon (Daphne)\". Most artistic depictions of the myth focus on the moment of Daphne's transformation.\n\nThe earliest source of the myth of Daphne and Apollo is Phylarchus, quoted by Parthenius. Later, the Roman poet Ovid does a retelling of this Greek legend, which appears in his work Metamorphoses.\n\nThe pursuit of a local nymph by an Olympian god, part of the archaic adjustment of religious cult in Greece, was given an arch anecdotal turn in the \"Metamorphoses\" by the Roman poet Ovid (died AD 17). According to this version Apollo's infatuation was caused by a golden-tipped arrow shot at him by Cupid, son of Venus, who wanted to punish Apollo for having insulted his archery skills by commenting \"What hast thou to do with the arms of men, thou wanton boy?\", and to demonstrate the power of love's arrow. Eros also shot Daphne, but with a leaden-tipped arrow, the effect of which was to make her flee from Apollo.\n\nElated with sudden love, Apollo chased Daphne contuinally. He tried to make her cease her flight by saying he doesn't wish to hurt her. When she continued fleeing, Apollo lamented that even though he had the knowledge of medicinal herbs, he had failed to cure himself from the wound of Cupid's arrow. When Apollo had caught up with her, Daphne prayed for help to her father, the river god Peneus of Thessaly, who immediately commenced her transformation into a laurel tree (\"Laurus nobilis\"): \n\nEven this did not quench Apollo's ardour, and as he embraced the tree, he felt her heart still beating. He then declared:\n\nUpon hearing his words, Daphne bends her branches, approving his decision.\n\nA version of the attempt on Daphne's sworn virginity that has been less familiar since the Renaissance was narrated by the Hellenistic poet Parthenius, in his \"Erotica Pathemata\", \"The Sorrows of Love\". Parthenius' tale, based on the Hellenistic historian Phylarchus, was known to Pausanias, who recounted it in his \"Description of Greece\" (2nd century AD). In this, which is the earliest written account, Daphne is a mortal girl, daughter of Amyclas, fond of hunting and determined to remain a virgin; she is pursued by the boy Leucippus (\"white stallion\"), who disguises himself in a girl's outfit in order to join her band of huntresses. He is also successful in gaining her innocent affection. This makes Apollo angry and he puts it into the girl's mind to stop to bathe in the river Ladon; there, as all strip naked, the ruse is revealed, as in the myth of Callisto, and the affronted huntresses plunge their spears into Leucippus. At this moment Apollo's attention becomes engaged, and he begins his own pursuit. Daphne, fleeing to escape Apollo's advances, prays to Zeus to help. Zeus turns her into laurel tree. Parthenius' modern editor remarks on the rather awkward transition, linking two narratives.\n\n\"Why should she wish to escape? Because she is Artemis Daphnaia, the god's sister,\" observed the Freudian anthropologist Géza Róheim, and Joseph Fontenrose concurs; boldly stating such a one-to-one identity doubtless oversimplifies the picture: \"the equation of Artemis and Daphne in the transformation myth itself clearly cannot work\". The laurel became sacred to Apollo, and crowned the victors at the Pythian Games.\n\nThe name Daphne, in greek Δάφνη, means \"laurel. While the story of Daphne is traditionally connected with the bay laurel (\"Laurus nobilis\"), almost 90 species of evergreen shrubs noted for their scented flowers and poisonous berries are grouped under the genus Daphne—including the garland flower (\"Daphne cneorum\"); the February Daphne or mezereon (\"Daphne mezereum\"); and spurge laurel or wood laurel (\"Daphne laureola\"). These genera are categorized in the family Thymelaeaceae and are native to Asia, Europe and North Africa\n\nArtemis Daphnaia, who had her temple among the Lacedemonians, at a place called Hypsoi in antiquity, on the slopes of Mount Cnacadion near the Spartan frontier, had her own sacred laurel trees.\n\nAt Eretria the identity of an excavated 7th- and 6th-century BCE temple to \"Apollo Daphnephoros\", \"Apollo, laurel-bearer\", or \"carrying off Daphne\", a \"place where the citizens are to take the oath\", is identified in inscriptions.\n\n\n"}
{"id": "12452729", "url": "https://en.wikipedia.org/wiki?curid=12452729", "title": "Densey Clyne", "text": "Densey Clyne\n\nDensey Clyne (born Dorothy Denise Bell, 4 December 1922) is an Australian naturalist, photographer and writer, especially well known for her studies of spiders and insects. \nShe was born in Risca, Wales, United Kingdom, and moved to Australia in 1936. During World War II she served as a commissioned officer in the Australian Women's Army Service, after a year in the Land Army. She married Peter Clyne (1927–1987) in 1950. Clyne lives in Wauchope, New South Wales.\n\nAs a naturalist, conservationist and communicator, Clyne has written 30 books on natural history subjects, in particular on insects and spiders (see list below). She has written scripts for her own and other television documentaries on natural history, and published numerous papers and articles dealing with invertebrate lives and behaviour in professional journals and popular magazines. She delivered talks and addresses on invertebrate behaviour and the pleasures of insect-watching to schools, adult groups, and professional organisations. She has also taken part in seminars on natural history writing and on wildlife filming, and also acted as a consultant on local wildlife for Australian and overseas television film productions, including several of Sir David Attenborough's natural history series. She has also served as a juror at Japan's Environmental Film Festival (1995), and presented regular natural history segments for eight years on Channel 9's \"Burke's Backyard\" lifestyle show. Densey Clyne is a Fellow of the Royal Entomological Society of London.\nFor her contributions to arachnology Densey Clyne has had two new species of spider named for her.\n\nClyne has written several regular columns on natural history for the print media for:\n\nClyne's scientific contributions include the first detailed description of the netmaking behaviour and sperm induction of the spider \"Dinopis subrufa\", (\"Australian Zoologist\", 1967); the web structure of the spider \"Poecilopachys bispinosa\" (\"Journal of the Australian Entomological Society\", 1973); and a joint paper with D. Rentz, CSIRO Insect Division, on \"Anthophiloptera dryas\", a new orthopteran genus and species, studied and recorded over several years by Clyne in her Sydney garden (\"Journal of the Australian Entomological Society\", 1983).\n\n\nBooks authored or coauthored by Clyne include:\nThe following are the TV and other documentary productions in which Densey Clyne has been involved as researcher, writer, narrator and/or adviser, in partnership with award-winning cinematographer Jim Frazier, OAM: \n\n"}
{"id": "2877923", "url": "https://en.wikipedia.org/wiki?curid=2877923", "title": "Department of Aerospace Science and Technology", "text": "Department of Aerospace Science and Technology\n\nThe Brazilian Department of Science and Aerospace Technology (; DCTA) is the national military research center for aviation and space flight. It is subordinated to the Brazilian Air Force (FAB).\n\nIt coordinates all technical and scientific activities related to the aerospace sector in which there are interests by the Ministry of Defence. It was established in 1953. It currently employs several thousand civilian and military personnel.\n\nThe DCTA has four institutes within its campus.\n\nIn Portuguese, \"Instituto de Aeronaútica e Espaço\". It develops projects in the aeronautical, airspace and defense sectors, co-responsible for the execution of the Brazilian Space Mission.\n\nIn Portuguese, \"Instituto Tecnológico de Aeronáutica\". It is one of the main educational colleges of the Brazilian Air Force.\n\nIn Portuguese, \"Instituto de Estudos Avançados\". Responsible for the development of pure and applied sciences: photonics, nuclear energy, applied physics, remote sensor systems and decision support systems. In 2006, the IEAv inaugurated the T3 Hypersonic wind tunnel, the largest in Latin America.\n\nIn Portuguese, \"Instituto de Fomento e Coodernação Industrial\". It provides military aeronautical certification and aerospace equipment approval, acting as an interface between the institutes and the industry. Until 2006, it carried out the civil aircraft certification activities, today under the National Civil Aviation Agency responsibilities.\n\nIn Portuguese, \"Instituto de Pesquisas e Ensaios em Voo\". This institute is responsible for the instruction and fulfillment of flight testing campaigns.\n\nThe DCTA is also responsible managing for the Brazilian Aerospace Memorial (Memorial Aerospacial Brasileiro - MAB). It is located in São José dos Campos, São Paulo, Brazil.\n\n\n"}
{"id": "56279651", "url": "https://en.wikipedia.org/wiki?curid=56279651", "title": "Dudleyite", "text": "Dudleyite\n\nDudleyite is a mineral, named after Dudleyville, Alabama. It is a vermiculite, hydrous mica, derived from margarite, or phlogopite.\n\n"}
{"id": "3270043", "url": "https://en.wikipedia.org/wiki?curid=3270043", "title": "Electric power", "text": "Electric power\n\nElectric power is the rate, per unit time, at which electrical energy is transferred by an electric circuit. The SI unit of power is the watt, one joule per second.\n\nElectric power is usually produced by electric generators, but can also be supplied by sources such as electric batteries. It is usually supplied to businesses and homes (as domestic mains electricity) by the electric power industry through an electric power grid. Electric power is usually sold by the kilowatt hour (3.6 MJ) which is the product of the power in kilowatts multiplied by running time in hours. Electric utilities measure power using an electricity meter, which keeps a running total of the electric energy delivered to a customer.\n\nElectrical power provides a low entropy form of energy and can be carried long distances and converted into other forms of energy such as motion, light or heat with high energy efficiency.\n\nElectric power, like mechanical power, is the rate of doing work, measured in watts, and represented by the letter \"P\". The term \"wattage\" is used colloquially to mean \"electric power in watts.\" The electric power in watts produced by an electric current \"I\" consisting of a charge of \"Q\" coulombs every \"t\" seconds passing through an electric potential (voltage) difference of \"V\" is\nwhere\n\nElectric power is transformed to other forms of energy when electric charges move through an electric potential (voltage) difference, which occurs in electrical components in electric circuits. From the standpoint of electric power, components in an electric circuit can be divided into two categories: \nSome devices can be either a source or a load, depending on the voltage and current through them. For example, a rechargeable battery acts as a source when it provides power to a circuit, but as a load when it is connected to a battery charger and is being recharged, or a generator as a power source and a motor as a load.\n\nSince electric power can flow either into or out of a component, a convention is needed for which direction represents positive power flow. Electric power flowing \"out\" of a circuit \"into\" a component is arbitrarily defined to have a positive sign, while power flowing \"into\" a circuit from a component is defined to have a negative sign. Thus passive components have positive power consumption, while power sources have negative power consumption. This is called the \"passive sign convention\".\n\nIn the case of resistive (Ohmic, or linear) loads, Joule's law can be combined with Ohm's law (\"V\" = \"I·R\") to produce alternative expressions for the amount of power that is dissipated:\n\nwhere \"R\" is the electrical resistance.\n\nIn alternating current circuits, energy storage elements such as inductance and capacitance may result in periodic reversals of the direction of energy flow. The portion of power flow that, averaged over a complete cycle of the AC waveform, results in net transfer of energy in one direction is known as real power (also referred to as active power). That portion of power flow due to stored energy, that returns to the source in each cycle, is known as reactive power. The real power P in watts consumed by a device is given by\nwhere\n\nThe relationship between real power, reactive power and apparent power can be expressed by representing the quantities as vectors. Real power is represented as a horizontal vector and reactive power is represented as a vertical vector. The apparent power vector is the hypotenuse of a right triangle formed by connecting the real and reactive power vectors. This representation is often called the \"power triangle\". Using the Pythagorean Theorem, the relationship among real, reactive and apparent power is:\n\nReal and reactive powers can also be calculated directly from the apparent power, when the current and voltage are both sinusoids with a known phase angle θ between them:\n\nThe ratio of real power to apparent power is called power factor and is a number always between 0 and 1. Where the currents and voltages have non-sinusoidal forms, power factor is generalized to include the effects of distortion.\n\nElectrical energy flows wherever electric and magnetic fields exist together and fluctuate in the same place. The simplest example of this is in electrical circuits, as the preceding section showed. In the general case, however, the simple equation \"P\" = \"IV\" must be replaced by a more complex calculation, the integral of the cross-product of the electrical and magnetic field vectors over a specified area, thus:\n\nThe result is a scalar since it is the \"surface integral\" of the \"Poynting vector\".\n\nThe fundamental principles of much electricity generation were discovered during the 1820s and early 1830s by the British scientist Michael Faraday. His basic method is still used today: electricity is generated by the movement of a loop of wire, or disc of copper between the poles of a magnet.\n\nFor electric utilities, it is the first process in the delivery of electricity to consumers. The other processes, electricity transmission, distribution, and electrical power storage and recovery using pumped-storage methods are normally carried out by the electric power industry.\n\nElectricity is mostly generated at a power station by electromechanical generators, driven by heat engines heated by combustion, geothermal power or nuclear fission. Other generators are driven by the kinetic energy of flowing water and wind. There are many other technologies that are used to generate electricity such as photovoltaic solar panels.\n\nA battery is a device consisting of one or more electrochemical cells that convert stored chemical energy into electrical energy. Since the invention of the first battery (or \"voltaic pile\") in 1800 by Alessandro Volta and especially since the technically improved Daniell cell in 1836, batteries have become a common power source for many household and industrial applications. According to a 2005 estimate, the worldwide battery industry generates US$48 billion in sales each year, with 6% annual growth. There are two types of batteries: primary batteries (disposable batteries), which are designed to be used once and discarded, and secondary batteries (rechargeable batteries), which are designed to be recharged and used multiple times. Batteries come in many sizes, from miniature cells used to power hearing aids and wristwatches to battery banks the size of rooms that provide standby power for telephone exchanges and computer data centers.\n\nThe electric power industry provides the production and delivery of power, in sufficient quantities to areas that need electricity, through a grid connection. The grid distributes electrical energy to customers. Electric power is generated by central power stations or by distributed generation. The electric power industry has gradually been trending towards deregulation - with emerging players offering consumers competition to the traditional public utility companies.\n\nElectric power, produced from central generating stations and distributed over an electrical transmission grid, is widely used in industrial, commercial and consumer applications. The per capita electric power consumption of a country correlates with its industrial development. Electric motors power manufacturing machinery and propel subways and railway trains. Electric lighting is the most important form of artificial light. Electrical energy is used directly in processes such as extraction of aluminum from its ores and in production of steel in electric arc furnaces. Reliable electric power is essential to telecommunications and broadcasting. Electric power is used to provide air conditioning in hot climates, and in some places electric power is an economically competitive source of energy for building space heating. Use of electric power for pumping water ranges from individual household wells to irrigation projects and energy storage projects.\n\n\n\n"}
{"id": "4132411", "url": "https://en.wikipedia.org/wiki?curid=4132411", "title": "Energy in Taiwan", "text": "Energy in Taiwan\n\nTaiwan lacks energy resources and highly depends on import, so it is a top priority to develop clean, sustainable, and independent energy and achieve the balance among energy security, environmental protection, and industrial competitiveness, and reduce emissions through various strategies. Taiwan relies on imports for more than 99.23% of its energy in 2008, which leaves the island's energy supply vulnerable to external disruption. In order to reduce this dependence, the Ministry of Economic Affairs' Bureau of Energy has been actively promoting energy research at several universities since the 1990s.\n\n, in Taiwan, oil accounts for 48.5% of the total energy consumption. Coal comes next with 29.2%, followed by natural gas (indigenous and liquefied) with 12.2%, nuclear energy with 8.3%, biomass and waste with 1.2%, and energy from other renewable sources with 0.5%. Taiwan has 6 nuclear reactors.\n\nThe 2016 election was won by the Pan-Green Coalition with policies that included a move toward a nuclear-free society, and is considering legislating to phase out nuclear power generation within nine years.\n\nThe 2018 referendum shown a decisive victory of pro-nuclear movement, where 59% of voters rejected the idea of nuclear-free society and phase-out of nuclear power.\n\nNuclear energy is controversial, and the privatization of the energy market (with Taipower that is owned by the state), originally planned in 2001, has been postponed to 2006. Nuclear energy provides one quarter of base load power generation and 16% from overall generation in Taiwan. In 2012, nuclear power accounted for a total 38,890 GWh of electricity generation in Taiwan.\n\nThe annual output of liquefied natural gas (LNG) from exploration and production within Taiwan is 350-400 million m. While on the other hand, Taiwan imported 18.4 billion m of LNG, the fifth largest LNG importer in the world, primarily from Qatar, Malaysia and Indonesia. LNG-fired power plants in Taiwan produce electricity at a cost of NT$3.91/kWh, more than the electricity cost charged to the customers.\n\nThe Democratic Progressive Party Government of the Republic of China under Chen Shui-bian was elected in early 2000 promising to approve only liquefied natural gas power projects in the future, and to increase the share of liquefied natural gas of Taiwan's power generation to roughly one-third by 2010. President Chen's administration tried to stop the 2,600 MW Longmen Nuclear Power Plant, currently under construction, but a court has ruled the construction could not be aborted.\n\nIn January 2013, China National Offshore Oil Corporation (CNOOC) and Shinfox signed an agreement to supply Kinmen with liquefied natural gas (LNG) from Mainland China. The delivery of LNG is expected to be started in early 2015 to industrial companies. At a later stage, the supply will be increased up to 100,000 tonnes per year to include power plants and households. The agreement was witnessed by Minister of Environmental Protection Administration Stephen Shen. Shen said that the cooperation is helpful to help Taiwan in realizing Kinmen to be a tourism-focused low-carbon county. During the third Cross-Strait Entrepreneurs Summit in Nanjing on 2–3 November 2015, executives from CNOOC and Shinfox gave a briefing on the joint venture project. Under the cooperation framework, CNOOC facilities in Fujian will also supply compressed natural gas, along with production technology.\n\nCurrently, the safety stock of LNG imported to Taiwan is seven days.\n\nIn June 2009, the Legislative Yuan passed a renewable energy act aimed at promoting the use of renewable energy, boosting energy diversification and helping reduce greenhouse gases. The new law authorizes the government to enhance incentives for the development of renewable energy via a variety of methods, including the acquisition mechanism, incentives for demonstration projects, and the loosening of regulatory restrictions in order to increase Taiwan’s renewable energy generation capacity by 6.5 gigawatts to 10 gigawatts within 20 years.\n\nIn July 2009, the Executive Yuan approved a proposal consisting of 16 measures to transform Taiwan into a “low carbon” country by 2020. The Ministry of Economic Affairs’ (MOEA) proposal set a long-term goal of cutting total annual greenhouse gas emissions to 2000 levels by the year 2025. The Executive Yuan also requested the legislature to complete a bill on reducing carbon emissions and another bill on developing renewable energy. NT$226 million will be used to promote renewable energy and facilities in homes and public buildings. Over the next five years NT$20 billion would be invested into advanced technologies in seven industries: solar energy, LED lighting, wind power, hydrogen energy and fuel cells, biofuel, energy information and communications technology and electric vehicles. Cooperation between local governments and the central government will be enhanced by providing incentives for conserving energy and cutting emissions. Two pilot communities will be created per county or city over 2009-2011, with 50 percent of the energy supply in those areas coming from renewable sources.\n\nIn August 2009, Taiwan's government has announced that it will invest T$45 billion ($1.4 billion) in the island's domestic renewable energy sector in an attempt to help the sector grow nearly eight-fold by 2015 thereby increasing industry production value to T$1.158 trillion in 2015 compared to T$160.3 billion in mid-2009. Proponents hope that \"the green energy sector will help Taiwan become a major power in energy technology and production, as well as provide the creation of green jobs.\"\n\nIn May 2016 Economics Minister Lee Shih-guang stated that the government expects renewable energy to account for 20% of electricity generation by 2025, to support the government's ambition to phase out nuclear power generation.\n\nTotal carbon dioxide emissions nationwide were 277.645 million tonnes in 2006, representing 124.68 percent growth over 1990’s 123.574 million tonnes, according to the Environmental Protection Administration (EPA). Energy conversion industry contributed to 6.9 percent of emissions in 2006, while heavy industry contributed 52.5 percent, the transportation sector contributed 14.3 percent, the commercial sector 6.3 percent and private households 12.1 percent.\n\nTaiwan ranked third in Asia and 32nd worldwide in the 2009 Climate Change Performance Index (CCPI) for carbon dioxide emissions, published by Climate Action Network Europe (CAN-Europe) and Germanwatch.\n\nTaiwan produces electricity from fossil fuels, wind, nuclear and hydro power. Taiwan’s energy consumption the equivalent of 10.5 million kiloliters of oil, or about 2.2 million barrels a day.\n\nConsumption of petroleum products account for about half of Taiwan’s energy supply equivalent of 4.5 million kiloliters of oil. Demand for diesel declined 21 percent, while that for gasoline dropped 8.7 percent. Monthly Power consumption is around 20.9 billion kilowatt-hours. Formosa Petrochemical Corp. and CPC Corp are Taiwan’s only oil refiners.\n\nEnergy use in the first six months of the year rose 6.7 percent to the equivalent of 61.6 million kiloliters of oil, the energy bureau said.\n\nCrude oil processing: 4.59 million kiloliters in June.\n\nCoal imports: 5.23 million metric tons purchases of liquefied natural gas increased 13 percent to 1.06 billion cubic meters. LNG accounted for 97 percent of gas supply.\n\nImports of crude oil: 26.9 million kiloliters\n\nLNG purchases: 5.58 billion cubic meters.\n\nIn 2013, coal imports to Taiwan amounted 65.96 million tons, which consists of 58.96 million tons of steam coal and 7.02 million tons of cooking coal. The largest coal exporter country to Taiwan is Indonesia (41.64%) and Australia (40.20%).\n\nLNG is natural gas that is chilled to liquid form, reducing it to one six-hundredth of its original volume at minus 161 degrees Celsius (minus 259 Fahrenheit) for transportation by ship to destinations not connected by pipeline. It is turned back into gas for distribution to power plants, factories and households.\n\nThe Taiwan government has been active in promoting energy efficiency, and set a target of energy efficiency of 33% by 2025. This target is higher than Japan's commitment to APEC with the target of 25%-26% efficiency. The government is currently assisting 200 major energy users (companies and organizations) in implementing energy-saving measures.\n\nTaiwan is preparing for the age of high oil prices, and is proactively developing clean energy, such as solar and wind power and biofuels. The efforts would help reduce Taiwan's reliance on imported oil, while contributing to the reduction of greenhouse gases.\n\nThe government aims for renewable energy to account for 15% of the nation's energy by 2025. It would amount to 8.45 million kilowatts, capable of producing 28.7 billion kilowatt hours of electricity. Wind-generated power could create as much as 8.9 billion kilowatt hours of electricity by 2025, comparable to 2.3 times the capacity of Linkou's thermal power plants. Many domestic companies are now beginning to work on the development of solar energy, and conservative estimates are projecting that 1.2 billion kilowatt hours of electricity will be produced through solar power by 2025.\n\nUnder the Energy Management Law and the underlying Implementing Regulations and related measures, companies are encouraged to improve the energy efficiency of their operations and products. Mandatory programs have been established for the purpose of energy conservation, including energy audit and energy efficiency standards for certain electrical and electronic products.\n\nThe Energy Commission under the Ministry of Economic Affairs is responsible for formulating and implementing energy policy and laws, including the programs instituted under the Energy Management Law. The principal responsibilities of the Energy Commission include:\n\n\nThe day-to-day work of the Energy Commission involves activities such as the development of policies and regulations, planning and conservation, research and development of technology, and data collection, processing, and publication.\n\nThrough the Greenmark program set up by the Fundamentals for Promoting the Use of the Taiwan Ecolabel, a number of energy-using appliances have been granted Ecolabels. For the electronic products, energy efficiency is one of the most important criteria for granting Green Marks. For further details, refer to the section on packaging and labeling. The Statute for Upgrading Industries also provides incentives for the improvement of energy efficiency.\n\nGreen energy technologies are a top priority on the Taiwan government's list of new industries to promote. President Ma Ying-jeou has highlighted solar and LED (light emitting diode) technologies as two areas where the island has potential to lead competitors.\n\nClean energy-related industries in Taiwan produced US$4.7 billion worth of goods in 2008, and the government hopes to grow that figure to US$14 billion by 2012.\n\nSeveral companies in Taiwan have produced solar panels for years, including Motech Solar and E-Ton Solar. LED makers have also sprouted on the island, including Epistar and Everlight Electronics.\n\nGroups from Taiwan have discussed using LED in public areas, including street lights and elsewhere, to take advantage of the energy-saving aspect of the technology.\n\nHydrogen-powered fuel cell research started fairly late in Taiwan when the Industrial Technology Research Institute (ITRI) began its research on fuel cell technology in 2001. The ITRI's current focus is on proton exchange membrane fuel cells, especially 3-kilowatt class fuel cell power systems, which are suitable for most Taiwanese households' average energy consumption. Presently, the high cost of manufacturing this type of fuel cell is still the most serious obstacle. It is estimated that Taiwan will catch up with other nations and mass-produce fuel cells between 2015 and 2020.\n\nBiodiesel, a diesel-equivalent made from vegetable oils or animal fats, is considered a practical option for the island. Biodiesel's greatest advantages are that it can be distributed through existing diesel infrastructure and be used in conjunction with petro-diesel after an inexpensive engine conversion.\n\nThe Center for Energy Research (CER) at National Central University has initiated a plan to educate energy professionals. It would coordinate professors from related disciplines and build a diversified teaching platform to recruit young students and researchers. Educating young scientists in the field of green technology and encouraging them to create innovative products will provide Taiwan with an edge in the international market.\n\n\nCurrently there are four nuclear research centers in Taiwan ranging up to 2.8 MW.\n\nDr. Yuan Tseh Lee is alumni of National Tsing Hua University and actual Taiwanese Nobel Prize scientist at Lawrence Berkeley National Laboratory using with scientific method and with understanding on laws of thermodynamics looking for energy sources with high net capacity factor as base load power source and low carbon footprint. \n\nTaiwan has to step up its pace in fusion power research if it wishes to develop more sources of \"clean\" energy according to Dr. Cheng Chio-Zong\n\n\n\n"}
{"id": "262256", "url": "https://en.wikipedia.org/wiki?curid=262256", "title": "Gasification", "text": "Gasification\n\nGasification is a process that converts organic- or fossil fuel-based carbonaceous materials into carbon monoxide, hydrogen and carbon dioxide. This is achieved by reacting the material at high temperatures (>700 °C), without combustion, with a controlled amount of oxygen and/or steam. The resulting gas mixture is called syngas (from synthesis gas) or producer gas and is itself a fuel. The power derived from gasification and combustion of the resultant gas is considered to be a source of renewable energy if the gasified compounds were obtained from biomass.\n\nThe advantage of gasification is that using the syngas (synthesis gas H2/CO) is potentially more efficient than direct combustion of the original fuel because it can be combusted at higher temperatures or even in fuel cells, so that the thermodynamic upper limit to the efficiency defined by Carnot's rule is higher or (in case of fuel cells) not applicable. Syngas may be burned directly in gas engines, used to produce methanol and hydrogen, or converted via the Fischer–Tropsch process into synthetic fuel. Gasification can also begin with material which would otherwise have been disposed of such as biodegradable waste. In addition, the high-temperature process refines out corrosive ash elements such as chloride and potassium, allowing clean gas production from otherwise problematic fuels. Gasification of fossil fuels is currently widely used on industrial scales to generate electricity.\n\nThe process of producing energy using the gasification method has been in use for more than 180 years. In the early time coal and peat were used to power these plants. Initially developed to produce town gas for lighting and cooking in the 1800s, this was replaced by electricity and natural gas, it was also used in blast furnaces but the bigger role was played in the production of synthetic chemicals where it has been in use since the 1920s.\n\nDuring both world wars, especially the World War II, the need for fuel produced by gasification reemerged due to the shortage of petroleum. Wood gas generators, called Gasogene or Gazogène, were used to power motor vehicles in Europe. By 1945 there were trucks, buses and agricultural machines that were powered by gasification. It is estimated that there were close to 9,000,000 vehicles running on producer gas all over the world.\n\nIn a gasifier, the carbonaceous material undergoes several different processes:\n\nIn essence, a limited amount of oxygen or air is introduced into the reactor to allow some of the organic material to be \"burned\" to produce carbon dioxide and energy, which drives a second reaction that converts further organic material to hydrogen and additional carbon dioxide. Further reactions occur when the formed carbon monoxide and residual water from the organic material react to form methane and excess carbon dioxide (formula_4). This third reaction occurs more abundantly in reactors that increase the residence time of the reactive gases and organic materials, as well as heat and pressure. Catalysts are used in more sophisticated reactors to improve reaction rates, thus moving the system closer to the reaction equilibrium for a fixed residence time.\n\nSeveral types of gasifiers are currently available for commercial use: counter-current fixed bed, co-current fixed bed, fluidized bed, entrained flow, plasma, and free radical.\n\nA fixed bed of carbonaceous fuel (e.g. coal or biomass) through which the \"gasification agent\" (steam, oxygen and/or air) flows in counter-current configuration. The ash is either removed in the dry condition or as a slag. The slagging gasifiers have a lower ratio of steam to carbon, achieving temperatures higher than the ash fusion temperature. The nature of the gasifier means that the fuel must have high mechanical strength and must ideally be non-caking so that it will form a permeable bed, although recent developments have reduced these restrictions to some extent. The throughput for this type of gasifier is relatively low. Thermal efficiency is high as the temperatures in the gas exit are relatively low. However, this means that tar and methane production is significant at typical operation temperatures, so product gas must be extensively cleaned before use. The tar can be recycled to the reactor.\n\nIn the gasification of fine, undensified biomass such as rice hulls, it is necessary to blow air into the reactor by means of a fan. This creates very high gasification temperature, as high as 1000 C. Above the gasification zone, a bed of fine and hot char is formed, and as the gas is blow forced through this bed, most complex hydrocarbons are broken down into simple components of hydrogen and carbon monoxide.\n\nSimilar to the counter-current type, but the gasification agent gas flows in co-current configuration with the fuel (downwards, hence the name \"down draft gasifier\"). Heat needs to be added to the upper part of the bed, either by combusting small amounts of the fuel or from external heat sources. The produced gas leaves the gasifier at a high temperature, and most of this heat is often transferred to the gasification agent added in the top of the bed, resulting in an energy efficiency on level with the counter-current type. Since all tars must pass through a hot bed of char in this configuration, tar levels are much lower than the counter-current type.\n\nThe fuel is fluidized in oxygen and steam or air. The ash is removed dry or as heavy agglomerates that defluidize. The temperatures are relatively low in dry ash gasifiers, so the fuel must be highly reactive; low-grade coals are particularly suitable. The agglomerating gasifiers have slightly higher temperatures, and are suitable for higher rank coals. Fuel throughput is higher than for the fixed bed, but not as high as for the entrained flow gasifier. The conversion efficiency can be rather low due to elutriation of carbonaceous material. Recycle or subsequent combustion of solids can be used to increase conversion. Fluidized bed gasifiers are most useful for fuels that form highly corrosive ash that would damage the walls of slagging gasifiers. Biomass fuels generally contain high levels of corrosive ash.\n\nA dry pulverized solid, an atomized liquid fuel or a fuel slurry is gasified with oxygen (much less frequent: air) in co-current flow. The gasification reactions take place in a dense cloud of very fine particles. Most coals are suitable for this type of gasifier because of the high operating temperatures and because the coal particles are well separated from one another.\n\nThe high temperatures and pressures also mean that a higher throughput can be achieved, however thermal efficiency is somewhat lower as the gas must be cooled before it can be cleaned with existing technology. The high temperatures also mean that tar and methane are not present in the product gas; however the oxygen requirement is higher than for the other types of gasifiers. All entrained flow gasifiers remove the major part of the ash as a slag as the operating temperature is well above the ash fusion temperature.\n\nA smaller fraction of the ash is produced either as a very fine dry fly ash or as a black colored fly ash slurry. Some fuels, in particular certain types of biomasses, can form slag that is corrosive for ceramic inner walls that serve to protect the gasifier outer wall. However some entrained flow type of gasifiers do not possess a ceramic inner wall but have an inner water or steam cooled wall covered with partially solidified slag. These types of gasifiers do not suffer from corrosive slags.\n\nSome fuels have ashes with very high ash fusion temperatures. In this case mostly limestone is mixed with the fuel prior to gasification. Addition of a little limestone will usually suffice for the lowering the fusion temperatures. The fuel particles must be much smaller than for other types of gasifiers. This means the fuel must be pulverized, which requires somewhat more energy than for the other types of gasifiers. By far the most energy consumption related to entrained flow gasification is not the milling of the fuel but the production of oxygen used for the gasification.\n\nIn a plasma gasifier a high-voltage current is fed to a torch, creating a high-temperature arc. The inorganic residue is retrieved as a glass like substance.\n\nThere are a large number of different feedstock types for use in a gasifier, each with different characteristics, including size, shape, bulk density, moisture content, energy content, chemical composition, ash fusion characteristics, and homogeneity of all these properties. Coal and petroleum coke are used as primary feedstocks for many large gasification plants worldwide. Additionally, a variety of biomass and waste-derived feedstocks can be gasified, with wood pellets and chips, waste wood, plastics and aluminium, Municipal Solid Waste (MSW), Refuse-derived fuel (RDF), agricultural and industrial wastes, sewage sludge, switch grass, discarded seed corn, corn stover and other crop residues all being used.\n\nChemrec has developed a process for gasification of black liquor.\n\nWaste gasification has several advantages over incineration:\n\nA major challenge for waste gasification technologies is to reach an acceptable (positive) gross electric efficiency. The high efficiency of converting syngas to electric power is counteracted by significant power consumption in the waste preprocessing, the consumption of large amounts of pure oxygen (which is often used as gasification agent), and gas cleaning. Another challenge becoming apparent when implementing the processes in real life is to obtain long service intervals in the plants, so that it is not necessary to close down the plant every few months for cleaning the reactor.\n\nEnvironmental advocates have called gasification \"incineration in disguise\" and argue that the technology is still dangerous to air quality and public health. \"Since 2003 numerous proposals for waste treatment facilities hoping to use... gasification technologies failed to receive final approval to operate when the claims of project proponents did not withstand public and governmental scrutiny of key claims,\" according to the Global Alliance for Incinerator Alternatives. One facility which operated from 2009–2011 in Ottawa had 29 \"emissions incidents\" and 13 \"spills\" over those three years. It was also only able to operate roughly 25% of the time.\n\nSeveral waste gasification processes have been proposed, but few have yet been built and tested, and only a handful have been implemented as plants processing real waste, and most of the time in combination with fossil fuels.\n\nOne plant (in Chiba, Japan using the Thermoselect process) has been processing industrial waste with natural gas and purified oxygen since year 2000, but has not yet documented positive net energy production from the process.\n\nIn 2007 Ze-gen erected a waste gasification demonstration facility in New Bedford, Massachusetts. The facility was designed to demonstrate gasification of specific non-MSW waste streams using \"liquid metal gasification\". This facility came after widespread public opposition shelved plans for a similar plant in Attleboro, Massachusetts. Today Ze-gen appears to be defunct, and the company website was taken down in 2014.\n\nAlso in the US, in 2011 a plasma system delivered by PyroGenesis Canada Inc. was tested to gasify municipal solid waste, hazardous waste and biomedical waste at the Hurlburt Field Florida Special Operations Command Air Force base. The plant, which cost $7.4 million to construct, was closed and sold at a government liquidation auction in May 2013. The opening bid was $25. The winning bid was sealed.\n\nSyngas can be used for heat production and for generation of mechanical and electrical power. Like other gaseous fuels, producer gas gives greater control over power levels when compared to solid fuels, leading to more efficient and cleaner operation.\n\nSyngas can also be used for further processing to liquid fuels or chemicals.\n\nGasifiers offer a flexible option for thermal applications, as they can be retrofitted into existing gas fueled devices such as ovens, furnaces, boilers, etc., where syngas may replace fossil fuels. Heating values of syngas are generally around 4–10 MJ/m.\n\nCurrently Industrial-scale gasification is primarily used to produce electricity from fossil fuels such as coal, where the syngas is burned in a gas turbine. Gasification is also used industrially in the production of electricity, ammonia and liquid fuels (oil) using Integrated Gasification Combined Cycles (IGCC), with the possibility of producing methane and hydrogen for fuel cells. IGCC is also a more efficient method of CO capture as compared to conventional technologies. IGCC demonstration plants have been operating since the early 1970s and some of the plants constructed in the 1990s are now entering commercial service.\n\nIn small business and building applications, where the wood source is sustainable, 250–1000 kWe and new zero carbon biomass gasification plants have been installed in Europe that produce tar free syngas from wood and burn it in reciprocating engines connected to a generator with heat recovery. This type of plant is often referred to as a wood biomass CHP unit but is a plant with seven different processes: biomass processing, fuel delivery, gasification, gas cleaning, waste disposal, electricity generation and heat recovery.\n\nDiesel engines can be operated on dual fuel mode using producer gas. Diesel substitution of over 80% at high loads and 70–80% under normal load variations can easily be achieved. Spark ignition engines and solid oxide fuel cells can operate on 100% gasification gas. Mechanical energy from the engines may be used for e.g. driving water pumps for irrigation or for coupling with an alternator for electrical power generation.\n\nWhile small scale gasifiers have existed for well over 100 years, there have been few sources to obtain a ready to use machine. Small scale devices are typically DIY projects. However, currently in the United States, several companies offer gasifiers to operate small engines.\n\nIn principle, gasification can proceed from just about any organic material, including biomass and plastic waste. The resulting syngas can be combusted. Alternatively, if the syngas is clean enough, it may be used for power production in gas engines, gas turbines or even fuel cells, or converted efficiently to dimethyl ether (DME) by methanol dehydration, methane via the Sabatier reaction, or diesel-like synthetic fuel via the Fischer–Tropsch process. In many gasification processes most of the inorganic components of the input material, such as metals and minerals, are retained in the ash. In some gasification processes (slagging gasification) this ash has the form of a glassy solid with low leaching properties, but the net power production in slagging gasification is low (sometimes negative) and costs are higher.\n\nRegardless of the final fuel form, gasification itself and subsequent processing neither directly emits nor traps greenhouse gases such as carbon dioxide. Power consumption in the gasification and syngas conversion processes may be significant though, and may indirectly cause CO emissions; in slagging and plasma gasification, the electricity consumption may even exceed any power production from the syngas.\n\nCombustion of syngas or derived fuels emits exactly the same amount of carbon dioxide as would have been emitted from direct combustion of the initial fuel. Biomass gasification and combustion could play a significant role in a renewable energy economy, because biomass production removes the same amount of CO from the atmosphere as is emitted from gasification and combustion. While other biofuel technologies such as biogas and biodiesel are carbon neutral, gasification in principle may run on a wider variety of input materials and can be used to produce a wider variety of output fuels.\n\nThere are at present a few industrial scale biomass gasification plants. Since 2008 in Svenljunga, Sweden, a biomass gasification plant generates up to 14 MW, supplying industries and citizens of Svenljunga with process steam and district heating, respectively. The gasifier uses biomass fuels such as CCA or creosote impregnated waste wood and other kinds of recycled wood to produces syngas that is combusted on site. In 2011 a similar gasifier, using the same kinds of fuels, is being installed at Munkfors Energy's CHP plant. The CHP plant will generate 2 MW (electricity) and 8 MW (district heating).\n\nExamples of demonstration projects include:\n\n\n"}
{"id": "48120666", "url": "https://en.wikipedia.org/wiki?curid=48120666", "title": "Grayite", "text": "Grayite\n\nGrayite, ThPO • (HO), is a thorium phosphate mineral of the Rabdophane group first discovered in 1957 by S.H.U. Bowie in Rhodesia. It is of moderate hardness occurring occasionally in aggregates of hexagonal crystals occasionally but more commonly in microgranular/cryptocrystalline masses. Due to its thorium content, grayite displays some radioactivity although it is only moderate and the mineral displays powder XRD peaks without any metamict-like effects. The color of grayite is most commonly observed as a light to dark reddish brown but has also been observed as lighter yellows with grayish tints. It has a low to moderate hardness with a Mohs hardness of 3-4 and has a specific gravity of 3.7-4.3. It has been found in both intrusive igneous and sedimentary environments.\n\nFormations including grayite were originally documented in Rhodesia (now Zimbabwe) in 1957 and subsequently around the globe. Some of these locales include the states of Wyoming and Colorado as well as Madagascar. Grayite has often been found in pegmatitic environments amongst other thorium minerals, particularly monazite ((Ce,La)PO). Recent work has shown widespread occurrences in Wisconsin pegmatitic environments. Other notable finds of pegmatitic grayite occur in Bulgaria. Grayite has also been found in sedimentary environments with an observation of high concentrations in cracks raising the possibility of the mineral as a precipitate from fluid mobilized ions. Formation of grayite and other rhabdophane minerals in this context has been documented in literature.\n\nGrayite is isostructural with members of the Rhabdophane group such as brockite and rhabdrophane. While previous work has identified grayite as a pseudohexagonal orthorhombic member of the rhabdophane group along with ningyoite, more contemporary work seems to maintain a hexagonal crystal structure. These hydrated phosphate minerals often include radioactive elements such as thorium, uranium, and cerium. Powder XRD analysis produces peaks matching those of rhabdophane.\n\nIn the identification of new hydrated phosphate minerals related to rhabdophane XRD peak information is usually recorded through different sample preparation methods. Besides standard powder XRD, samples are often heated to ~850 °C so that the structure changes. The peak information is analyzed again and upon doing this hydrated thorium phosphate minerals will show a monazite-like structure indicating a possible alteration relationship.\n\n"}
{"id": "23686446", "url": "https://en.wikipedia.org/wiki?curid=23686446", "title": "Gulf Stream", "text": "Gulf Stream\n\nThe Gulf Stream, together with its northern extension the North Atlantic Drift, is a warm and swift Atlantic ocean current that originates in the Gulf of Mexico and stretches to the tip of Florida, and follows the eastern coastlines of the United States and Newfoundland before crossing the Atlantic Ocean. The process of western intensification causes the Gulf Stream to be a northward accelerating current off the east coast of North America. At about , it splits in two, with the northern stream, the North Atlantic Drift, crossing to Northern Europe and the southern stream, the Canary Current, recirculating off West Africa.\n\nThe Gulf Stream influences the climate of the east coast of North America from Florida to Newfoundland, and the west coast of Europe. Although there has been recent debate, there is consensus that the climate of Western Europe and Northern Europe is warmer than it would otherwise be due to the North Atlantic drift which is the northeastern section of the Gulf Stream. It is part of the North Atlantic Gyre. Its presence has led to the development of strong cyclones of all types, both within the atmosphere and within the ocean. The Gulf Stream is also a significant potential source of renewable power generation. The Gulf Stream may be slowing down as a result of climate change.\n\nThe Gulf Stream is typically 100 kilometres (62 mi) wide and 800 metres (2,600 ft) to 1,200 metres (3,900 ft) deep. The current velocity is fastest near the surface, with the maximum speed typically about 2.5 metres per second (9 kph; 5.6 mph).\n\nEuropean discovery of the Gulf Stream dates to the 1512 expedition of Juan Ponce de León, after which it became widely used by Spanish ships sailing from the Caribbean to Spain. A summary of Ponce de León's voyage log, on April 22, 1513, noted, \"A current such that, although they had great wind, they could not proceed forward, but backward and it seems that they were proceeding well; at the end it was known that the current was more powerful than the wind.\" Its existence was also known to Peter Martyr d'Anghiera.\n\nBenjamin Franklin became interested in the North Atlantic Ocean circulation patterns. In 1768, while in England, Franklin heard a curious complaint from the Colonial Board of Customs: Why did it take British packets several weeks longer to reach New York from England than it took an average American merchant ship to reach Newport, Rhode Island, despite the merchant ships leaving from London and having to sail down the River Thames and then the length of the English Channel before they sailed across the Atlantic, while the packets left from Falmouth in Cornwall?\n\nFranklin asked Timothy Folger, his cousin twice removed (Nantucket Historical Society), a Nantucket Island whaling captain, for an answer. Folger explained that merchant ships routinely crossed the then-unnamed Gulf Stream—identifying it by whale behavior, measurement of the water's temperature, and changes in the water's color—while the mail packet captains ran against it. Franklin had Folger sketch the path of the Gulf Stream on an old chart of the Atlantic and add written notes on how to avoid the Stream when sailing from England to America. Franklin then forwarded the chart to Anthony Todd, secretary of the British Post Office. Franklin's Gulf Stream chart was printed in 1769 in London, but it was mostly ignored by British sea captains. A copy of the chart was printed in Paris circa 1770-1773, and a third version was published by Franklin in Philadelphia in 1786. The inset in the upper left part of the 1786 chart is an illustration of the migration pattern of herring and not an ocean current.\n\nThe Gulf Stream proper is a western-intensified current, driven largely by wind stress. The North Atlantic Drift, in contrast, is largely thermohaline circulation–driven. In 1958 the oceanographer Henry Stommel noted that \"very little water from the Gulf of Mexico is actually in the Stream\". By carrying warm water northeast across the Atlantic, it makes Western and especially Northern Europe warmer than it otherwise would be.\n\nHowever, the extent of its contribution to the actual temperature differential between North America and Europe is a matter of dispute, as there is a recent minority opinion within the science community that this temperature difference (beyond that caused by contrasting maritime and continental climates) is mainly due to atmospheric waves created by the Rocky Mountains.\n\nA river of sea water, called the Atlantic North Equatorial Current, flows westward off the coast of Central Africa. When this current interacts with the northeastern coast of South America, the current forks into two branches. One passes into the Caribbean Sea, while a second, the Antilles Current, flows north and east of the West Indies. These two branches rejoin north of the Straits of Florida.\n\nThe trade winds blow westward in the tropics, and the westerlies blow eastward at mid-latitudes. This wind pattern applies a stress to the subtropical ocean surface with negative curl across the north Atlantic Ocean. The resulting Sverdrup transport is equatorward.\n\nBecause of conservation of potential vorticity caused by the northward-moving winds on the subtropical ridge's western periphery and the increased relative vorticity of northward moving water, transport is balanced by a narrow, accelerating poleward current, which flows along the western boundary of the ocean basin, outweighing the effects of friction with the western boundary current, known as the Labrador current. The conservation of potential vorticity also causes bends along the Gulf Stream, which occasionally break off due to a shift in the Gulf Stream's position, forming separate warm and cold eddies. This overall process, known as western intensification, causes currents on the western boundary of an ocean basin, such as the Gulf Stream, to be stronger than those on the eastern boundary.\n\nAs a consequence, the resulting Gulf Stream is a strong ocean current. It transports water at a rate of 30 million cubic meters per second (30 sverdrups) through the Florida Straits. As it passes south of Newfoundland, this rate increases to 150 million cubic metres per second. The volume of the Gulf Stream dwarfs all rivers that empty into the Atlantic combined, which barely total 0.6 million cubic metres per second. It is weaker, however, than the Antarctic Circumpolar Current. Given the strength and proximity of the Gulf Stream, beaches along the East Coast of the United States may be more vulnerable to large sea-level anomalies, which significantly impact rates of coastal erosion.\n\nThe Gulf Stream is typically wide and to deep. The current velocity is fastest near the surface, with the maximum speed typically about . As it travels north, the warm water transported by the Gulf Stream undergoes evaporative cooling. The cooling is wind-driven: Wind moving over the water causes evaporation, cooling the water and increasing its salinity and density. When sea ice forms, salts are left out of the ice, a process known as brine exclusion. These two processes produce water that is denser and colder (or, more precisely, water that is still liquid at a lower temperature). In the North Atlantic Ocean, the water becomes so dense that it begins to sink down through less salty and less dense water. (The convective action is not unlike that of a lava lamp.) This downdraft of cold, dense water becomes a part of the North Atlantic Deep Water, a southgoing stream. Very little seaweed lies within the current, although seaweed lies in clusters to its east.\n\nIn April 2018, two studies published in \"Nature\" found the Gulf Stream to be at its weakest for at least 1,600 years.\n\nThe Gulf Stream is influential on the climate of the Florida peninsula. The portion off the Florida coast, referred to as the Florida current, maintains an average water temperature at or above during the winter. East winds moving over this warm water move warm air from over the Gulf Stream inland, helping to keep temperatures milder across the state than elsewhere across the Southeast during the winter. Also, the Gulf Stream's proximity to Nantucket, Massachusetts adds to its biodiversity, as it is the northern limit for southern varieties of plant life, and the southern limit for northern plant species, Nantucket being warmer during winter than the mainland.\n\nThe North Atlantic Current of the Gulf Stream, along with similar warm air currents, helps keep Ireland and the western coast of Great Britain a couple of degrees warmer than the east. However, the difference is most dramatic in the western coastal islands of Scotland. A noticeable effect of the Gulf Stream and the strong westerly winds (driven by the warm water of the Gulf Stream) on Europe occurs along the Norwegian coast. Northern parts of Norway lie close to the Arctic zone, most of which is covered with ice and snow in winter. However, almost all of Norway's coast remains free of ice and snow throughout the year. Weather systems warmed by the Gulf Stream drift into Northern Europe, also warming the climate behind the Scandinavian mountains.\n\nThe warm water and temperature contrast along the edge of the Gulf Stream often increase the intensity of cyclones, tropical or otherwise. Tropical cyclone generation normally requires water temperatures in excess of . Tropical cyclone formation is common over the Gulf Stream, especially in the month of July. Storms travel westward through the Caribbean and then either move in a northward direction and curve toward the eastern coast of the United States or stay on a north-westward track and enter the Gulf of Mexico. Such storms have the potential to create strong winds and extensive damage to the United States' Southeast Coastal Areas. Hurricane Sandy in 2012 was a recent example of a hurricane passing over the Gulf Stream and gaining strength.\n\nStrong extratropical cyclones have been shown to deepen significantly along a shallow frontal zone, forced by the Gulf Stream itself, during the cold season. Subtropical cyclones also tend to generate near the Gulf Stream. 75 percent of such systems documented between 1951 and 2000 formed near this warm water current, with two annual peaks of activity occurring during the months of May and October. Cyclones within the ocean form under the Gulf Stream, extending as deep as beneath the ocean's surface.\n\nThe theoretical maximum energy dissipation from the Gulf Stream by turbines is in the range of 20-60 GW. One suggestion, which could theoretically supply power comparable to several nuclear power plants, would deploy a field of underwater turbines placed 300 meters (980 ft) under the center of the core of the Gulf Stream. Ocean thermal energy could also be harnessed to produce electricity using the temperature difference between cold deep water and warm surface water.\n\n\n"}
{"id": "408801", "url": "https://en.wikipedia.org/wiki?curid=408801", "title": "Helioseismology", "text": "Helioseismology\n\nHelioseismology, a term coined by Douglas Gough, is the study of the structure and dynamics of the Sun through its oscillations. These are principally caused by sound waves that are continuously driven and damped by convection near the Sun's surface. It is similar to geoseismology, or asteroseismology (also coined by Gough), which are respectively the studies of the Earth or stars through their oscillations. While the Sun's oscillations were first detected in the early 1960s, it was only in the mid-1970s that it was realised that the oscillations propagated throughout the Sun and could allow scientists to study the Sun's deep interior. The modern field is separated into global helioseismology, which studies the Sun's resonant modes, and local helioseismology, which studies all the waves propagating at the Sun's surface.\n\nHelioseismology has contributed to a number of scientific breakthroughs. The most notable was to show the predicted neutrino flux from the Sun could not be caused by flaws in stellar models and must instead be a problem of particle physics. The so-called solar neutrino problem was ultimately resolved by neutrino oscillations.\nThe experimental discovery of neutrino oscillations was recognized by the 2015 Nobel Prize for Physics.\nHelioseismology also allowed accurate measurements of the quadrupole (and higher-order) moments of the Sun's gravitational potential, which are consistent with general relativity. The first helioseismic calculations of the Sun's internal rotation profile showed a rough separation into a rigidly-rotating core and differentially-rotating envelope. The boundary layer is now known as the tachocline and is thought to be a key component for the solar dynamo. Although it roughly coincides with the base of the solar convection zone—also inferred through helioseismology—it is conceptually a distinct entity.\n\nHelioseismology benefits most from continuous monitoring of the Sun, which began first with uninterrupted observations from near the South Pole over the southern summer. In addition, observations over multiple solar cycles have allowed helioseismologists to study changes in the Sun's structure over decades. These studies are made possible by global telescope networks like the Global Oscillations Network Group (GONG) and the Birmingham Solar Oscillations Network (BiSON), which have been operating for over 20 years.\n\nSolar oscillation modes are interpreted (to first order) as vibrations of a spherically symmetric self-gravitating fluid in hydrostatic equilibrium. Each mode is then characterised by three numbers:\n\n\nThe latter two correspond to the quantum numbers of the spherical harmonics. Under these assumptions, it can be shown that\nthe oscillations are separated into two categories of interior oscillations and a third special category of surface modes.\n\nPressure modes are in essence standing sound waves. The dominant restoring force is the pressure (rather than buoyancy), hence the name. All the solar oscillations that are used for inferences about the interior are p-modes, with frequencies between about 1 and 5 millihertz. They span angular degrees from zero (purely radial motion) to several hundred.\n\nGravity modes are lower-frequency modes that are confined to the convectively-stable interior. The restoring force is buoyancy and thus indirectly gravity, from which they take their name. They are evanescent in the convection zone and therefore have tiny amplitudes at the surface. No individual g-modes have been detected but indirect detections have been claimed.\n\nSurface gravity waves are analogous to waves on deep water. To good approximation, they follow the same dispersion law: formula_4, where formula_5 is the angular frequency, formula_6 is the surface gravity and formula_7 is the horizontal wavenumber. Surface gravity modes observed on the Sun are all of very high degree (formula_8).\n\nThe chief tool for analysing data in global helioseismology is the Fourier transform. To good approximation, each mode is a damped harmonic oscillator, for which the power as a function of frequency is a Lorentz function. Resolved data is usually integrated over the desired spherical harmonic to obtain a single timeseries that can be Fourier transformed. In this way, helioseismologists combine a power spectrum for each spherical harmonic into a two-dimensional power spectrum.\n\nThe lower frequency range of the oscillations is dominated by the variations caused by granulation. This must first be filtered out before (or at the same time that) the modes are analysed. Granular flows at the solar surface are mostly horizontal, from the centres of the rising granules to the narrow downdrafts between them. Relative to the oscillations, granulation produces a stronger signal in intensity than line-of-sight velocity, so the latter is preferred for helioseismic observatories.\n\nLocal helioseismology—a term coined by Charles Lindsey, Doug Braun and Stuart Jefferies in 1993—employs several different analysis methods to make inferences from the observational data.\n\n\nThe Sun's oscillation modes represent a discrete set of observations that are sensitive to its continuous structure. This allows scientists to formulate inverse problems for the Sun's interior structure and dynamics. Given a reference model of the Sun, the differences in the mode frequencies are weighted averages of the differences between the structure of the reference model and the real Sun. The difference in the mode frequencies can then be used to infer the differences in the structures. The weighting functions of these averages are known as \"kernels\".\n\nThe first inversions of the Sun's structure were made using Duvall's law and later using Duvall's law linearised about a reference solar model. These results were superseded by analyses that use the full set of equations describing the stellar oscillations and are now the standard way to compute inversions. The inversions demonstrated differences in solar models that were greatly reduced by implementing \"gravitational settling\": the gradual separation of heavier elements towards the solar centre (and lighter elements to the surface to replace them).\n\nIf the Sun were perfectly spherical, the modes with different azimuthal orders \"m\" would have the same frequencies. Rotation, however, breaks this degeneracy, and the modes frequencies differ by \"rotational splittings\" that are weighted-averages of the rotation rate throughout the Sun. \nDifferent modes are sensitive to different parts of the Sun and, given enough data, these differences can be used to infer the rotation rate throughout the Sun. For example, if the Sun rotated with the same rotational frequency throughout, then all the modes would be split by the same amount. In reality, different layers of the Sun rotate at different speeds, as can be seen at the surface, where the equator rotates faster than the poles.\nThe Sun rotates slowly enough that a spherical, non-rotating model serves as close enough to reality to derive the rotational kernels.\n\nHelioseismology has shown that the Sun has a rotation profile with several features:\n\nHelioseismology was born from analogy with geoseismology but several important differences remain. First, the Sun lacks a solid surface and therefore cannot support shear waves. From the data analysis perspective, global helioseismology differs from geoseismology by studying only normal modes. Local helioseismology is thus somewhat closer in spirit to geoseismology in the sense that it studies the complete wavefield.\n\nBecause the Sun is a star, helioseismology is closely related to the study of oscillations in other stars, known as asteroseismology. Helioseismology is most closely related to the study of stars whose oscillations are also driven and damped by their outer convection zones, known as solar-like oscillators, but the underlying theory is broadly the same for other classes of variable star.\n\nThe principal difference is that oscillations in distant stars cannot be resolved. Because the brighter and darker sectors of the spherical harmonic cancel out, this restricts asteroseismology almost entirely to the study of low degree modes (angular degree formula_9). This makes inversion much more difficult but upper limits can still be achieved by making more restrictive assumptions.\n\nSolar oscillations were first observed in the early 1960s as a quasi-periodic intensity and line-of-sight velocity variation with a period of about 5 minutes. Scientists gradually realised that the oscillations might be global modes of the Sun and predicted that the modes would form clear ridges in two-dimensional power spectra. The ridges were subsequently confirmed in observations in the mid 1970s although individual mode frequencies were not measured until several years later.\nAt a similar time, Jørgen Christensen-Dalsgaard and Douglas Gough suggested the potential of using individual mode frequencies to infer the interior structure of the Sun.\n\nHelioseismology developed rapidly in the 1980s. A major breakthrough was made when observations from Antarctica linked the frequencies for modes of low and intermediate angular degree, from which the radial orders of the modes could be identified. In addition, new methods of inversion developed, allowing researchers to infer the sound speed profile of the Sun. Towards the end of the decade, observations also began to show that the oscillation mode frequencies vary with the Sun's magnetic activity cycle.\n\nTo overcome the problem of not being able to observe the Sun at night, several groups had begun to assemble networks of telescopes (e.g. the Birmingham Solar Oscillations Network, or BiSON, and the Global Oscillation Network Group) from which the Sun would always be visible to at least one node. Long, uninterrupted observations brought the field to maturity, and the state of the field was summarized in a 1996 special issue of \"Science magazine\". This coincided with the start of normal operations of the Solar and Heliospheric Observatory (SoHO), which began producing high-quality data for helioseismology.\n\nThe subsequent years saw the resolution of the solar neutrino problem and the long observations began to allow analysis of multiple solar activity cycles. The agreement between standard solar models and helioseismic inversions was disrupted by new measurements of the heavy element content of the solar photosphere based on detailed three-dimensional models. Though the results later shifted back towards the traditional values used in the 1990s, the new abundances significantly worsened the agreement between the models and helioseismic inversions. The cause of the discrepancy remains unsolved and is known as the \"solar abundance problem\".\n\nSpace-based observations by SoHO have continued and SoHO was joined in 2010 by the Solar Dynamics Observatory (SDO), which has also been monitoring the Sun continuously since its operations began. In addition, ground-based networks (notably BiSON and GONG) continue to operate, providing nearly continuous data from the ground too.\n\n\n\n"}
{"id": "28858009", "url": "https://en.wikipedia.org/wiki?curid=28858009", "title": "International Commission on Non-Ionizing Radiation Protection", "text": "International Commission on Non-Ionizing Radiation Protection\n\nThe International Commission on Non-Ionizing Radiation Protection (ICNIRP) is an international commission specialized in non-ionizing radiation protection. The organization's activities include determining exposure limits for electromagnetic fields used by devices such as cellular phones.\n\nICNIRP is an independent non profit scientific organization chartered in Germany. It was founded in 1992 by the International Radiation Protection Association (IRPA) to which it maintains close relations. \n\nThe mission of ICNIRP is to screen and evaluate scientific knowledge and recent findings toward providing protection guidance on non-ionizing radiation, i.e. radio, microwave, UV and infrared. The commission produces reviews of the current scientific knowledge and guidelines summarizing its evaluation. ICNIRP provides its science-based advice free of charge. In the past, national authorities in more than 50 countries and multinational authorities such as the European Union have adopted the ICNIRP guidelines and translated them into their own regulatory framework on protection of the public and of workers from established adverse health effects caused by exposure to non-ionizing radiation.\n\nICNIRP consists of a main commission which membership is limited to fourteen to ensure efficiency and four Standing Committees of each up to 8 members covering the fields of epidemiology, biology and medicine, physics and dosimetry and optical radiation. Its members are scientists employed typically by universities or radiation protection agencies. They do not represent their country of origin, nor their institute and can not be employed by commercial companies. \n\nICNIRP is widely connected to a large community working on non-ionizing radiation protection around the world. Its conferences and workshops are widely attended. ICNIRP presents its draft guidelines online for public review and comment before publication. It has ties to IRPA and is formally recognized by the World Health Organization (WHO) and the International Labour Office (ILO) as partners in the field of non-ionizing radiation. Its advice is requested by many national and multinational organizations such as the European Union (EU). Standard bodies also refer to ICNIRP health protection guidance for setting appliance standards. \n\nTo preserve its independence from vested interests ICNIRP applies fundamental principles as provided by its Charter and statutes: it does not receive financial support from commercial entities. Its fundings consist solely of periodical or project grants from national and international public bodies and to a lesser extent of the income derived from its publications and scientific congresses and workshops. The members are not allowed to be employed by commercial entities. To enforce this rule, they are requested to fill in a declaration of personal interests and report any changes as they occur. Declarations of interests are publicly available on the ICNIRP website.\n\nICNIRP's activities are of scientific nature and deal with health risk assessment only. Policy or national or international risk management are considered outside of its scope. Balanced evidence based health risk assessment requires to screen the totality of the available science in an evaluation process. In this process the published literature is carefully read and interpreted in light of a set of quality criteria widely agreed by the scientific community.\n\nDespite or because of the wide use of ICNIRP guidance, it also encounters criticism. The Council of Europe says: \"it is most curious, to say the least, that the applicable official threshold values for limiting the health impact of extremely low frequency electromagnetic fields and high frequency waves were drawn up and proposed to international political institutions (WHO, European Commission, governments) by the ICNIRP, an NGO whose origin and structure are none too clear and which is furthermore suspected of having rather close links with the industries whose expansion is shaped by recommendations for maximum threshold values for the different frequencies of electromagnetic fields\".\n\n\n"}
{"id": "1440511", "url": "https://en.wikipedia.org/wiki?curid=1440511", "title": "Isotope geochemistry", "text": "Isotope geochemistry\n\nIsotope geochemistry is an aspect of geology based upon the study of natural variations in the relative abundances of isotopes of various elements. Variations in isotopic abundance are measured by isotope ratio mass spectrometry, and can reveal information about the ages and origins of rock, air or water bodies, or processes of mixing between them.\n\nStable isotope geochemistry is largely concerned with isotopic variations arising from mass-dependent isotope fractionation, whereas radiogenic isotope geochemistry is concerned with the products of natural radioactivity.\n\nFor most stable isotopes, the magnitude of fractionation from kinetic and equilibrium fractionation is very small; for this reason, enrichments are typically reported in \"per mil\" (‰, parts per thousand). These enrichments (δ) represent the ratio of heavy isotope to light isotope in the sample over the ratio of a standard. That is,\n\nHydrogen isotope biogeochemistry\n\nCarbon has two stable isotopes, C and C, and one radioactive isotope, C.\n\nThe stable carbon isotope ratio, \"δ\"C, is measured against Vienna Pee Dee Belemnite (VPDB). The stable carbon isotopes are fractionated primarily by photosynthesis (Faure, 2004). The C/C ratio is also an indicator of paleoclimate: a change in the ratio in the remains of plants indicates a change in the amount of photosynthetic activity, and thus in how favorable the environment was for the plants. During photosynthesis, organisms using the C pathway show different enrichments compared to those using the C pathway, allowing scientists not only to distinguish organic matter from abiotic carbon, but also what type of photosynthetic pathway the organic matter was using. Occasional spikes in the global C/C ratio have also been useful as stratigraphic markers for chemostratigraphy, especially during the Paleozoic.\n\nThe C ratio has been used to track ocean circulation, among other things.\n\nNitrogen has two stable isotopes, N, and N. The ratio between these is measured relative to nitrogen in ambient air. Nitrogen ratios are frequently linked to agricultural activities. Nitrogen isotope data has also been used to measure the amount of exchange of air between the stratosphere and troposphere using data from the greenhouse gas NO.\n\nOxygen has three stable isotopes, O, O, and O. Oxygen ratios are measured relative to Vienna Standard Mean Ocean Water (VSMOW) or Vienna Pee Dee Belemnite (VPDB). Variations in oxygen isotope ratios are used to track both water movement, paleoclimate, and atmospheric gases such as ozone and carbon dioxide. Typically, the VPDB oxygen reference is used for paleoclimate, while VSMOW is used for most other applications. Oxygen isotopes appear in anomalous ratios in atmospheric ozone, resulting from mass-independent fractionation. Isotope ratios in fossilized foraminifera have been used to deduce the temperature of ancient seas.\n\nSulfur has four stable isotopes, with the following abundances: S (0.9502), S (0.0075), S (0.0421) and S (0.0002). These abundances are compared to those found in Cañon Diablo troilite. Variations in sulfur isotope ratios are used to study the origin of sulfur in an orebody and the temperature of formation of sulfur–bearing minerals.\n\nRadiogenic isotopes provide powerful tracers for studying the ages and origins of Earth systems. They are particularly useful to understand mixing processes between different components, because (heavy) radiogenic isotope ratios are not usually fractionated by chemical processes.\n\nRadiogenic isotope tracers are most powerful when used together with other tracers: The more tracers used, the more control on mixing processes. An example of this application is to the evolution of the Earth's crust and Earth's mantle through geological time.\n\nLead has four stable isotopes - Pb, Pb, Pb, Pb and one common radioactive isotope Pb with a half-life of ~53,000 years.\n\nLead is created in the Earth via decay of transuranic elements, primarily uranium and thorium.\n\nLead isotope geochemistry is useful for providing isotopic dates on a variety of materials. Because the lead isotopes are created by decay of different transuranic elements, the ratios of the four lead isotopes to one another can be very useful in tracking the source of melts in igneous rocks, the source of sediments and even the origin of people via isotopic fingerprinting of their teeth, skin and bones.\n\nIt has been used to date ice cores from the Arctic shelf, and provides information on the source of atmospheric lead pollution.\n\nLead–lead isotopes has been successfully used in forensic science to fingerprint bullets, because each batch of ammunition has its own peculiar Pb/Pb vs Pb/Pb ratio.\n\nSamarium–neodymium is an isotope system which can be utilised to provide a date as well as isotopic fingerprints of geological materials, and various other materials including archaeological finds (pots, ceramics).\n\nSm decays to produce Nd with a half life of 1.06x10 years.\n\nDating is achieved usually by trying to produce an isochron of several minerals within a rock specimen. The initial Nd/Nd ratio is determined.\n\nThis initial ratio is modelled relative to CHUR - the Chondritic Uniform Reservoir - which is an approximation of the chondritic material which formed the solar system. CHUR was determined by analysing chondrite and achondrite meteorites.\n\nThe difference in the ratio of the sample relative to CHUR can give information on a model age of extraction from the mantle (for which an assumed evolution has been calculated relative to CHUR) and to whether this was extracted from a granitic source (depleted in radiogenic Nd), the mantle, or an enriched source.\n\nRhenium and osmium are siderophile elements which are present at very low abundances in the crust. Rhenium undergoes radioactive decay to produce osmium. The ratio of non-radiogenic osmium to radiogenic osmium throughout time varies.\n\nRhenium prefers to enter sulfides more readily than osmium. Hence, during melting of the mantle, rhenium is stripped out, and prevents the osmium–osmium ratio from changing appreciably. This \"locks in\" an initial osmium ratio of the sample at the time of the melting event. Osmium–osmium initial ratios are used to determine the source characteristic and age of mantle melting events.\n\nNatural isotopic variations amongst the noble gases result from both radiogenic and nucleogenic production processes. Because of their unique properties, it is useful to distinguish them from the conventional radiogenic isotope systems described above.\n\nHelium-3 was trapped in the planet when it formed. Some He is being added by meteoric dust, primarily collecting on the bottom of oceans (although due to subduction, all oceanic tectonic plates are younger than continental plates). However, He will be degassed from oceanic sediment during subduction, so cosmogenic He is not affecting the concentration or noble gas ratios of the mantle.\n\nHelium-3 is created by cosmic ray bombardment, and by lithium spallation reactions which generally occur in the crust. Lithium spallation is the process by which a high-energy neutron bombards a lithium atom, creating a He and a He ion. This requires significant lithium to adversely affect the He/He ratio.\n\nAll degassed helium is lost to space eventually, due to the average speed of helium exceeding the escape velocity for the Earth. Thus, it is assumed the helium content and ratios of Earth's atmosphere have remained essentially stable.\n\nIt has been observed that He is present in volcano emissions and oceanic ridge samples. How He is stored in the planet is under investigation, but it is associated with the mantle and is used as a marker of material of deep origin.\n\nDue to similarities in helium and carbon in magma chemistry, outgassing of helium requires the loss of volatile components (water, carbon dioxide) from the mantle, which happens at depths of less than 60 km. However, He is transported to the surface primarily trapped in the crystal lattice of minerals within fluid inclusions.\n\nHelium-4 is created by radiogenic production (by decay of uranium/thorium-series elements). The continental crust has become enriched with those elements relative to the mantle and thus more He is produced in the crust than in the mantle.\n\nThe ratio (R) of He to He is often used to represent He content. R usually is given as a multiple of the present atmospheric ratio (Ra).\n\nCommon values for R/Ra:\n\nHe/He isotope chemistry is being used to date groundwaters, estimate groundwater flow rates, track water pollution, and provide insights into hydrothermal processes, igneous geology and ore genesis. \n\nU-series isotopes are unique amongst radiogenic isotopes because, being in the U-series decay chains, they are both radiogenic and radioactive. Because their abundances are normally quoted as activity ratios rather than atomic ratios, they are best considered separately from the other radiogenic isotope systems.\n\nUranium is well mixed in the ocean, and its decay produces Pa and Th at a constant activity ratio (0.093). The decay products are rapidly removed by adsorption on settling particles, but not at equal rates. Pa has a residence equivalent to the residence time of deep water in the Atlantic basin (around 1000 yrs) but Th is removed more rapidly (centuries). Thermohaline circulation effectively exports Pa from the Atlantic into the Southern Ocean, while most of the Th remains in Atlantic sediments. As a result, there is a relationship between Pa/Th in Atlantic sediments and the rate of overturning: faster overturning produces lower sediment Pa/Th ratio, while slower overturning increases this ratio. The combination of δC and Pa/Th can therefore provide a more complete insight into past circulation changes.\n\nTritium was released to the atmosphere during atmospheric testing of nuclear bombs. Radioactive decay of tritium produces the noble gas helium-3. Comparing the ratio of tritium to helium-3 (H/He) allows estimation of the age of recent ground waters.\n\n\n\n\n\n\n"}
{"id": "5906197", "url": "https://en.wikipedia.org/wiki?curid=5906197", "title": "List of Hawaii hurricanes", "text": "List of Hawaii hurricanes\n\nA Hawaiian hurricane is a tropical cyclone that forms in the Pacific Ocean and affects the Hawaiian Islands. Hawaii lies in the central Pacific, where about four or five tropical cyclones appear each year, although as many as fifteen have occurred, such as in the 2015 season; rarely do these storms actually affect Hawaii. Tropical cyclone records were not kept before the 1950s. Earlier windstorms that struck Hawaii were not labeled as hurricanes.\n\nThis list contains every tropical cyclone that had a somewhat notable effect on the State of Hawaii.\n\n\n\n\n\n\n\n\n\nIn total, 63 tropical cyclones have affected Hawaii since official record-keeping began in 1949.\n\nAt least 29 people have died in Hawaii as a result of tropical cyclones since 1949. \n\nThe islands of Hawaii, with Kauai as the notable exception, appear to be remarkably immune from direct hurricane hits. The USGS states that \"more commonly, near-misses that generate large swell and moderately high winds causing varying degrees of damage are the hallmark of hurricanes passing close to the islands.\" This has also drawn media attention.\nOne notion is that Hawaii’s volcanic peaks slow down or divert storms. A partial source of this idea may be the long list of hurricanes in the above paragraphs that dissipated into tropical storms or depressions upon approaching the islands. Satellite images of Hurricane Flossie's breakup when approaching Hawaii Island fueled this idea. Another example may be Hurricane Felicia which dropped from Category 4 down to a tropical depression with residual winds predicted at only .\n\nWind data in particular supports the USGS assertion that hurricane damage has been low on all islands except for Kauai. Data collected by the Western Regional Climate Center show no hurricane-strength winds on any Hawaii Islands with the exception of Kauai. Despite this data, FEMA classified all of Hawaii as being in a \"Wind-Borne Debris Region\".\n\nBefore Hurricane Iniki in 1992, a standard homeowner's insurance policy with extended coverage provided hurricane coverage. Since Iniki, many insurance policies exclude hurricane and a separate hurricane policy is required to obtain hurricane coverage.\n\n\n"}
{"id": "4262900", "url": "https://en.wikipedia.org/wiki?curid=4262900", "title": "List of Lepidoptera that feed on Aster", "text": "List of Lepidoptera that feed on Aster\n\nAster species are used as food plants by the larvae of a number of Lepidoptera species including:\n\n\n\n\n\n"}
{"id": "25663067", "url": "https://en.wikipedia.org/wiki?curid=25663067", "title": "List of coal power stations", "text": "List of coal power stations\n\nThe following page lists all coal-fired power stations (including lignite-fired) that are larger than in current net capacity, which are currently operational or under construction. If station has also non-coal-fired blocks, only coal-fired capacity is listed. Those power stations that are smaller than , and those that are only at a planning/proposal stage may be found in regional lists, listed at the end of the page. \n\n"}
{"id": "17964574", "url": "https://en.wikipedia.org/wiki?curid=17964574", "title": "List of ecoregions in North America (CEC)", "text": "List of ecoregions in North America (CEC)\n\nThis list of ecoregions of North America provides an overview of North American ecoregions designated by the Commission for Environmental Cooperation (CEC) in its North American Environmental Atlas. It should not be confused with Wikipedia articles based on the classification system developed by the World Wildlife Fund, such as List of ecoregions (WWF) and Lists of ecoregions by country.\n\nThe commission was established in 1994 by the member states of Canada, Mexico, and the United States to address regional environmental concerns under the North American Agreement on Environmental Cooperation (NAAEC), the environmental side accord to the North American Free Trade Agreement (NAFTA). The Commission's 1997 report, \"Ecological Regions of North America\", provides a framework that may be used by government agencies, non-governmental organizations, and academic researchers as a basis for risk analysis, resource management, and environmental study of the continent's ecosystems. Ecoregions may be identified by similarities in geology, physiography, vegetation, climate soils, land use, wildlife distributions, and hydrology.\n\nThe classification system has four levels. Only the first three levels are shown on this list. \"Level I\" divides North America into 15 broad ecoregions. \"Level II\" subdivides the continent into 52 smaller ecoregions. \"Level III\" subdivides those regions again into 182 ecoregions. \"Level IV\" is a further subdivision of Level III ecoregions. Level IV mapping is still underway but is complete across most of the United States. For an example of Level IV data, see List of ecoregions in Oregon and the associated articles.\n\nThe Arctic Cordillera is one of the world's fifteen diverse ecoregions, characterized by a vast mountain chain spanning the spine of the range. The geographic range is composed along the provinces of Labrador: including Eastern Baffin, The Devon Islands, Ellesmere, Bylot Islands, the Thorngat Mountains, and some parts of the Northeastern fringe. The landscape is dominated by massive polar icefields, alpine glaciers, inland fjords, and large bordering bodies of water, distinctive of many similar arctic regions in the world. Although the terrain is infamous for its unforgiving conditions, humans maintained an established population of 1000 people – 80% of which were Inuit. In addition, the landscape is 75% covered by ice or exposed bedrock, with a continuous permafrost that persists throughout the year, making plant and animal life somewhat scarce. The temperature of the Arctic Cordillera ranges from 6 °C in summer, down to −16 °C in winter. Vegetation is largely absent in this area due to permanent ice and snow.\n\nThe Arctic Cordillera is a cold, harsh environment making plant life and animal-life sparse; even soil is rare in this ecoregion. Moss, cottongrass, and Arctic heather are examples of plant life that can be found in valleys. Meanwhile, polar bears, seals, and walruses roam the shores and survive off the thriving marine ecosystem. Fish, clams, and shrimp are just a few of the resources the local Inuit communities of Nunavut use in the highly productive waters to support their economy. Nunavut’s government is also investing in exploration of mineral resources; Breakwater Resources, for example, is a zinc-lead mine in Arctic Bay that just reopened in April 2003 after closing the year before due to declining resources. Climate change is the strongest human influence in the Arctic Cordillera. Rising temperatures in the Arctic are causing ice shelves, and the habitats they provide, to shrink from year to year. Researchers of global warming also express concern for the economic, political, and social consequences of the resulting decline in fisheries stocks expected because of the changing climate.\n\nThe Arctic Cordillera is one of Canada’s most inhospitable climates. The northern part is covered by the ice caps and glaciers cover a large part of the south. It was not always as cold as it is today. 40 million-year-old tree stumps found in 1985 on Axel Heiberg Island suggest that the area used to be warmer and wetter with much more biodiversity. Today the weather is generally very cold and dry with a few weeks of sun and rain in the summer. Snow is the most common form of precipitation in the Cordillera. The region only gets 20−60 centimeters of precipitation annually. The temperature in this ecoregion averages around 4 degrees Celsius during the summer. In the winter the temperature is −35 degrees Celsius on average. A polar cell is a system of winds that influence the climate of the Cordillera. It is made up of the Westerlies, which are winds that blow warm air east to west from 30 to 60 degrees latitude up to the poles, and the Polar Easterlies, which blow cold air back south where it will repeat the process.\n\nThis region can be divided up into three major areas Ellesmere Island, Baffin Island, and the coastline of the most northern part of Labrador. Nearly 75% of the land within this ecoregion is exposed bedrock or ice. The majority of the water in this ecoregion is locked up in frozen ice and snow, therefore there are very few named rivers or other bodies of water within this region. The annual amount precipitation is about 200 mm, which usually falls down as snow or ice. Huge ice caps dominate the landscape, and they spawn large glaciers that are pushed down steep fjords and into the sea. When the temperature gets above freezing for an extended period time a little amount of runoff is created, which is generally under 200 mm annually.\n\nThe Arctic Cordillera is dominated by vast mountain ranges stretching for thousands of miles, virtually untouched by man. These mountains were formed millions of years ago during the mid-Mesozoic when the North American Plate moved northward, pushing earth and rock upwards. The mountains of the north contain metamorphic and igneous rock, and are predominantly sedimentary rock. On the other hand, the southern mountains are greater, composed of granite gneiss and magmatic volcanic rock. These mountains are characterized as being highly erodible with very steep and jagged cliffs with narrow ledges. The highest peak in the Arctic Cordillera mountain range is Barbeau Peak – standing almost nine thousand feet tall. In general, the Arctic Cordillera Mountain Range is most similar (in composition and age) to the Appalachian Mountain Range of the United States. However, as the Appalachian Mountains are slightly older, their cliffs have been eroded, and are less jagged than those of the Arctic Cordillera. This ecoregion is also home to very limited amounts of exposed soil. Only in extremely sheltered places – such as that of caves – is surface soil present. The remaining soil is hidden beneath deep snow and ice, and is kept in a constant state of permafrost.\n\nThe Arctic Cordillera is a very high stress environment for plants to try and grow and regenerate. Vegetation is largely absent due to permanent ice and snow. Due to the extremely cold, dry climate, along with the ice-fields and lack of soil materials, the high and mid-elevations are largely devoid of significant populations of plants. In the warmer valleys at low elevations and along coastal margins, the plant cover is more extensive, consisting of herbaceous and shrub-type communities. Stream-banks and coastlines are the most biologically productive areas here. The plants in this region have a history of being survivors and stress tolerant to high winds, low temperatures, few available macronutrients like nitrogen and phosphorus. Plants have adaptations such as fluffy seed masses, staying low to the ground, and use of other plant masses for extra insulation.\n\nDue to the harsh environments and extremely low temperatures that encompass the Arctic Cordillera, there is not a large variety of plants and animals that are able to survive and exist as a population. However, some animal species, both herbivores and carnivores, are able to survive the extreme weather and terrain. Among these animals are wolves, polar bears, Arctic foxes, musk-oxen, and caribou. For the most part, the large carnivores are the dominant species in the ecoregion, mainly the polar bear. It is the keystone species for the area due to many of its habits, including its diet and hunting strategies. In addition, the life history of the 22,000 polar bears in the Arctic clearly defines its current existence in the Arctic Cordillera.\n\nThe large carnivorous species defines the ecoregion due to its intimate relationship with the ice as well as its extremely intelligent hunting tactics. No other predatory animal defines the Arctic Cordillera as well as the large white polar bear and that is why when people think about arctic animals, they think about the polar bear. As long as the polar bear remains existent, it will be the keystone species of the Arctic Cordillera. However, this existence relies solely on the degree of ice melt that is encountered in the future.\n\nThe polar bear is one of the most notably affected species in the Arctic Cordillera, mainly due to their heavy reliance on arctic ice for hunting and bedding grounds. Habitat loss, caused by global warming, has led to many dangerous behavioral changes including a new behavior called long swims. These are swims lasting as long as ten days performed by mother bears to attempt to find food for their cubs, which generally lead to the death of the cub. Because of their stature and aggressiveness, direct conservation practices are not very useful to the polar bear. Instead, scientific observation to better understand these animals is the largest form of traditional conservation.\n\nThe Arctic black spruce is an example of a plant native to the Arctic Cordillera that is considered to be in ecological decline. The black spruce is a species of least concern because of habitat loss and deforestation from the spruce budworm moth. In the Arctic Cordillera however, the black spruce population is in good health, and is slowly gaining habitat through the retreat of polar ice.\n\nAnother species that is of great importance to this ecoregion is the endangered Bowhead whale (\"Balaena mysticetus\"). Five total stocks of this species exist in the region within the arctic oceans and adjacent seas: the Spitsbergen stock, Baffin Bay/Davis Strait, stock and Hudson Bay/Foxe Basin Stock, Sea of Okhotsk Stock, and the Bering/Chukchi/Beaufort Stock. Historically, these whales have served as a cultural icon, and an important source of food and fuel to the Inuit people. At this point in time, their populations were estimated between 30,000 and 50,000 individuals.\n\nHowever, with the expansion of commercial whaling in the 16th and 17th century, this species was exploited to dangerously low numbers. Commercial hunting of bowheads was officially ended in 1921, when moratoria were established to protect the remaining 3,000 individuals left in the wild.\n\nToday, those same moratoria are still in effect, but the Bowhead population has been reinstated to a manageable population of between 7,000 and 10,000 individuals. Nonetheless, these whales have been (and remain) on the IUCN Red List since 1984. One of the most important conservation efforts for this species is \"legal\" protection by the International Convention for the Regulation of Whaling, which came into force in 1935. This convention was further strengthened and ratified by Canada in 1977 to support the International Whaling Commission’s (IWC) recommendation for full protection of the bowhead whale. Further conservation efforts have involved more physically demanding solutions, including the recommended funding of specialized technical machines that have the capability to remove debris that commonly kills these whales due to entanglement and accidental indigestion.\n\nOne of the planet's most recent biomes, a result of the last ice age only 10,000 years ago, the tundra contains unique flora and fauna formed during the last glaciation in areas unrestricted by permanent ice. The tundra region is found in high latitudes, primarily in Alaska, Canada, Russia, Greenland, Iceland, Scandinavia, as well as the Antarctic Islands. Consisting of the arctic, alpine and Antarctic regions, and stemming from the Samer language, tundra literally means a \"high and dry place\".\n\nThe adversity of soil and climatic conditions proves for low production levels, as well as little biomass accumulation due to slow rates of nutrient release in cold and wet soils, specifically as a result of limited nitrogen and phosphorus (Nadelhoffer et al. 1996) Additionally, there are low temperatures and strong winds in the tundra causing most vegetation to be dominated by woody plants that hug the soil. Within the tundra, some dominant plant species include lichen, cotton grass, and Arctic willow.\nLichens dominate the tundra as the regions major primary producer. A symbiotic combination of algae and fungi, a lichen is able to survive in the harsh conditions of the tundra (Biodiversity Institute of Ontario et al. 2010). Their unique structure and survivability makes lichen a prominent and keystone plant species in the tundra ecosystem.\n\nCotton grass is another dominant plant species in the tundra producing much of its growth early in the summer. Being a member of the sedge family, it forms a large part of the vegetation in the tundra because it is able to deal with harsh and cold temperatures. This perennial plant contains flowering heads with dense brittles that are spread during heavy winds, enabling pollination (Wein and Bliss 1974). Additionally, its survivability in the tundra can be attributed to cotton grass’s ability to photosynthesize in low temperatures and low light.\n\nThe Arctic willow, commonly named rock willow, is found in the North American tundra. Most uniquely, the Arctic willow often has long trailing branches that root where they intersect with the surface of the ground, and the roots are shallow as to thrive in the frozen ground of the tundra (Wielgolaski 1972).\n\nIn addition to species such as lichens, cotton grass, and Arctic willows, shrubs, sedges, lichens, mosses, and vascular plants dominate the tundra plant community (Folch and Camarasa 2000). Despite the tundra eco-region’s reputation of being a cold and desolate ‘polar desert’, it is actually a varying landscape supporting a diverse amount of plant and animal species.\n\nSince the tundra has such a harsh environment, the animals who live here have adapted in a way to call the tundra their home. The keystone species of the tundra can be as small as a lemming to as large as a musk ox. The low biodiversity means that fluctuation in individual animals can substantially affect the entire ecosystem. The main predators of the tundra are the polar bear, the Arctic wolf and the Arctic fox. They all have thick white coats that help them blend into their environment and stalk prey. The polar bear spends majority of its time out on the ice hunting seals and sometimes when small rodents are scarce on land the Arctic fox will follow the bears and eat their scraps. Wolves use teamwork to attack herds of caribou or musk ox for food. Lemming are small rodents that fluctuate every three to four years and with their fluctuations also comes the fluctuation of their predators such as the Arctic fox and the snowy owl. The keystone herbivores are the musk ox and the caribou. They have thick shaggy coats that they shed during the warmer months. Caribou use their nimble legs to escape quickly from predators while the musk ox use each other to make a fierce wall of horns. These animals help keep each other alive as well as the ecosystem around them.\n\nThe tundra is an extremely harsh, cold, windy and unique ecosystem found on the extreme north and south latitudes of our Earth. The soil consists mostly of frozen permafrost, which makes it difficult for extended root systems to grow, water to drain and support of a wide variety of plant life. This permafrost is also responsible for creating an extremely unusual topography. The land of the tundra is constantly changing as permafrost and snow melts and refreezes through the changing seasons. Land slumps and depressions occur as a result of melting permafrost that takes up less space when the soil was frozen. Depressions that occur as a result of melting permafrost are known as thermokarst, and are often in the form of pits, funnel-shaped sinkholes, valleys, ravines and sometimes caves. Pingos are another feature of the tundra, and can be defined as a cone shaped hill or mound of soil with a core of ice. Lastly, polygons make up a crucial part of the tundra and are created when two large cracks create a large ice wedge and slowly slumps into itself filling with water as heat from sunlight melts the permafrost. Often small lakes are formed from polygons on the surface of the tundra.\n\nThe flora and fauna must adapt to extremely harsh conditions, however has been able to do so successfully through evolutionary change. Many threats exist today to the tundra biome including mining, oil drilling, increased habitat loss, human habitations moving farther north and global warming which is melting more and more permafrost and changing the delicate balance of the soils. It is imperative that we fully understand how our ecosystems function in order to monitor their stability through our changing climate.\n\nThe tundra is characterized by a harsh, frost-laden landscape with negative temperatures, a lack of precipitation and nutrients, and extremely short seasons. In the winter it is cold and dark, and in the summer when the snow and the top layer of permafrost melt, it is very soggy and the tundra is covered with marshes, lakes, bogs and streams. Spring and fall are only short periods between winter and summer. In the peak of winter, average temperatures can reach −30 °F. In arctic regions, there generally is not a great difference between daytime highs and nighttime lows, as the sun generally never rises or simply hangs briefly on the horizon. Summers in the tundra, on the other hand, are very short, in some locations only lasting a few weeks. Daily temperatures can reach up to 60 °F but overnight lows go down into the 30s, 20s or lower, depending on the region. This results in daily average temperatures to come out to around 50 °F. It may rain or snow, and frost still occurs. The average annual temperature is −18 °F. Nights can last for weeks, and when the sun barely rises during some months in the winter, the temperature can drop to −94 °F. During the summer the sun shines almost 24 hours a day. Temperatures can get up to 54 °F but it can get as cold as 37 °F. Average summer temperatures range from 37 °F to 60 °F. The tundra is very much like a desert in terms of precipitation. Yearly average precipitation varies by region, but generally there is only about of precipitation per year and in some regions it can have up to . This precipitation usually falls in the form of light, fluffy snow.\n\nDue to its vulnerable state, the powerful forces of climate change, ozone depletion, air pollution, and construction threaten the tundra's survival. The melting of permafrost increases as a result of global warming, which could drastically alter both the landscape and the biodiversity of the region. The ozone depletion at both the North and South Poles increase the strength of ultraviolet rays that harm the tundra. Air pollution around the world creates smog clouds that contaminate the lichen in the ecosystem, which is a major food source in the region. The construction of pipelines and roads to obtain oil, gas, and minerals cause physical disturbances and habitat fragmentation. There are a number of possible solutions, according to National Geographic, including switching to alternative energy, establishing protected areas and park reserves to restrict human influence, limit road construction, mining activities, and the building of pipelines in tundra habitat, and limiting tourism and respecting local cultures. The creation of the Arctic National Refuge is an example of a measure being enacted to protect the North American tundra. The Arctic Refuge was originally created in 1960 by the Public Land Order 2214, which was created \"for the purpose of preserving unique wildlife, wilderness and recreational values\" and \"withdrawn from all forms of appropriation under the public land laws, including the mining but not the mineral leasing laws, nor disposals of materials\". In 1980, the Alaska National Interest Lands Conservation Act (ANILCA) re-designated the Range as a part of the larger Arctic National Wildlife Refuge, and declared \"that the ‘production of oil and gas from the Arctic National Wildlife Refuge is prohibited and no leasing or other development leading to production of oil and gas from the [Refuge] shall be undertaken until authorized by an act of Congress’\".\n\nThough species have adapted to the harsh climate of the tundra, several species have become endangered due to changing environmental factors. Both plant species and animal species have become endangered. The Aleutian shield fern is a plant species that has been endangered due to caribou tramping and grazing, slumping from growing substrate, and human foot traffic. Animal species that are endangered in the tundra include the Arctic fox, caribou, and polar bears. These animals have been endangered due to overhunting, infestation of disease, loss of diet and habitat due to climate change, and human destructive activities, such as searches for natural gas and oil, mining, and road building. In an effort to conserve these endangered species, many regulations and standards are being put into action along with establishing prohibition of unauthorized plant collecting. Standards are being set in regards to mining and mineral explorations. This will help in not disturbing the habitats as much. In addition to this, protection of caribou grounds has been established along with regulations in regards to removal of gravel roads for airstrips and road fill, which takes away from many of the animals’ critical territories.\n\nThe tundra is one of the first places on Earth we have noted the effects of climate change. As an indicator biome, the tundra is a crucial part of the whole global climate system and can help predict the changes the rest of the world will face. The Earth depends on regulating mechanisms and air circulation patterns the tundra provides to keep climates steady worldwide. Human-induced climate change is devastating the tundra because intense complications are present in remote areas, free from human interference. Changes in climate, permafrost, ice pack, and glacier formations pose a serious threat to the stability of global climate because these conditions are influenced and reinforced by positive feedback loops. Temperatures in the tundra are rising to the highest temperatures recorded in four centuries and are rising more rapidly than anywhere worldwide The land surfaces in the tundra are no longer reflecting radiation from the sun out of the atmosphere. Soils and open water are absorbing heat from the sun and leading to more warming. Changes in the tundra influence climate change in lower latitudes because air pressure changes are shifting global air and ocean circulation patterns. Sea ice extent in the tundra has reached lowest recorded levels in centuries and this will dramatically affect people and wildlife worldwide. Changes in climate will be noticed first and seen most intensely in the northern regions of the planet. The tundra will show effects from climate change the soonest and will hopefully serve as a catalyst for action for people all over the world.\n\nAccording to the US Energy Information Administration, the arctic tundra holds an estimated 13% or 90 billion barrels of the world's undiscovered conventional oil sources. However, there are a number of challenges to oil exploration, drilling, and transportation in an arctic tundra environment that limits the profitability of the venture. Oil and gas fields in the arctic need to be large, with lots of proven reserves, because oil companies need that money to make the investment profitable. Natural gas is a more recoverable resource than oil in tundra eco-regions. It is estimated that there are 221.4 million undiscovered, technically recoverable cubic feet of natural gas in the Arctic. Oil sands, often pejoratively referred to as tar sands, are a phenomenon unique to the tundra environment and are profitable and plentiful in the Athabasca region of the Alberta sands. Oil sands consist of bitumen, which contains petroleum, found in a natural state combined with clays, sands, and water. Oil sands must be heavily processed and refined to yield synthetic crude oil, similar to conventional crude oil. Arctic tundra may contain minerals such as coal, copper, gold, iron, nickel, diamonds and the base feedstock for uranium oxide called pitchblende.\n\nThe arctic tundra has an exceptionally short growing period, minimal sunlight and limited resources, creating a brutal environment for plants and animals. By adapting to these harsh conditions, animals and plants represent iconic characteristics of the tundra. Plants grow in aggregated formations which provide shelter from wind, ice and also improves seed success. Animals have adapted with specialized organs, such as a rete mirabile, an organ that efficiently transfers heat. Frogs and amphibians use \"anti-freeze\" to prevent organ damage while hibernating. Polar bears, foxes and owls use insulated fur and feathers to protect for the cold conditions. These complex interactions between plants, animals and abiotic factors in the tundra are held together by the permafrost layer, located under the soil. However climate change is causing this crucial layer of frozen soil to melt. As a result, tundra communities are becoming unstable and basic processes are breaking down. Other factors such as oil development and drilling in tundra ecosystems has completely disheveled the wildlife and vegetation populations. Tundra exploration vehicles used for oil development and polar bear tours (\"an eco-friendly\" industry) leave traces of tire marks for 20-plus years after disturbance occurs. Other factors such as high CO2 emissions from tourism and from warming tundra soil, creates a positive feedback loop, acceleration changes to the tundra.\n\n\n\n\n\nThe taiga ecoregion includes much of the interior Alaska as well as the Yukon forested area, and extends on the west from the Bering Sea to the Richardson Mountains in on the east, with the Brooks Range on the north and the Alaska Range on the south end. It is a region with a vast mosaic of habitats and a fragile yet extensive patchwork of ecological characteristics. All aspects of the region such as soils and plant species, hydrology, and climate interact, and are affected by climate change, new emerging natural resources, and other environmental threats such as deforestation. These threats alter the biotic and abiotic components of the region, which lead to further degradation and to various endangered species.\nThe main type of soil in the taiga is a Spodosol. These soils contain a Spodic horizon, a sandy layer of soil that has high accumulations of iron and aluminum oxides, which lays underneath a leached A horizon. The color contrast between the Spodic horizon and the overlying horizon is very easy to identify. The color change is the result of the migration of iron and aluminum oxides from small, but consistent amounts of rainfall from the top horizon to the lower horizon of soil.\n\nThe decomposition of organic matter is very slow in the taiga because of the cold climate and low moisture. With slow decomposition of organic matter nutrient cycling is very slow and the nutrient level of the soil is also very low. The soils in the taiga are quite acidic as well. A relatively small amount of rainfall coupled with slow decomposition of organic material allows the acidic plant debris to sit and saturate the top horizons of the soil profile.\n\nAs a result of the infertile soil only a few plant species can really thrive in taiga. The common plant species in the taiga are coniferous trees. Not only do conifer trees thrive in acidic soils, they actually make the soil more acidic. Acidic leaflitter (or needles) from conifers falls to the forest floor and the precipitation leaches the acids down into the soil. Other species that can tolerate the acidic soils of the taiga are lichens and mosses, yellow nutsedge and water horsetail. The depth to bedrock has an effect on the plants that grow well in the taiga as well. A shallow depth to bedrock forces the plants to have shallow roots, limiting overall stability and water uptake.\n\nBeaver, Canadian lynx, bobcat, wolverine, and snowshoe hare are all keystone species in the taiga area. These species are keystone because they have learned to adapt to the cold climate of the area and are able to survive year round.\n\nThese species survive year round in taiga by changing fur color and growing extra fur. They have adapted to use each other to survive too. All of the predators depend on the snowshoe hare at some point during the year. All of the species also depend on forests in the area for shelter.\n\nWatersheds characterize much of the taiga ecoregion as interconnecting rivers, streams, lakes and coastline. Due to a cool climate, low evaporation levels keeps moisture levels high and enables water to have serious influences for ecosystems. The vast majority of water in the taiga is freshwater, occupying lakes and rivers.\n\nMany watersheds are dominated by large rivers that dump huge amounts of freshwater into the ocean such as the Lena river in Central Siberia. This exportation of freshwater helps control the thermohaline circulation and the global climate. Flow rates of taiga rivers are variable and \"flashy\" due to the presence of a permafrost that keeps water from percolating deep into the soil. Due to global warming, flow rates have increased as more of the permafrost melts every year. In addition to \"flashy\" flow levels, the permafrost in the taiga allows dissolved inorganic nitrogen and organic carbon levels in the water to be higher while calcium, magnesium, sulfate, and hydrogen bicarbonate levels are shown to be much lower. As a dominant characteristic in the soil, the permafrost also influences the degree to which water percolates into the soil. Where there is a year-long permafrost, the water table is located much deeper in the soil and is less available to organisms, while a discontinuous permafrost provides much shallower access.\n\nLakes that cover the taiga are characteristically formed by receding glaciers, and therefore have many unique features. The vast majority of lakes and ponds in the taiga ecoregion are oligotrophic, and have much higher levels of allochthonous versus autochthonous matter. This is due to glacier formation and has implications in how trophic levels interact with limiting nutrients. These oligotrophic lakes show organic nitrogen and carbon as more limiting nutrients for trophic growth over phosphorus. This contrasts sharply with mesotrophic or eutrophic lakes from similar climates.\n\nWhen we look at the climate of the taiga, we are looking at average temperatures, abiotic factors such as precipitation, and circulatory patterns. According to the study in Global Change Biology, the average yearly temperatures across the Alaskan and Canadian taiga ranged from −26.6 °C to 4.8 °C. This indicates the extreme cold weather the taiga has for the majority of the year. As for precipitation, the majority of it is snow, but rain is also an important factor. According to \"The International Journal of Climatology\", precipitation in the form of rain ranged from 40 mm average in August, to 15 mm average in April over a multi-year study. Rain is not the only kind of precipitation that affects the taiga; the main factor in precipitation is usually snow. According to CEC Ecological Regions of North America, snow and freshwater ice can occupy the taiga for half to three quarters of the year. A CEC Ecological Regions of North America document states that the lowest average precipitation is on the western side of taiga; can be as little as 200 mm and on the east coast it can be as high as exceeding 1,000 mm. As for circulatory patterns, we're finding that the temperature increases have led to a seasons shift. Global Change Biology also has noted with the change in temperature over time, as well as the overall climate change, the growing season has lengthened. Their findings illustrate that the growing season has grown 2.66 days per ten years. This growing season change as a result of global warming is having an extreme effect on the taiga.\n\nClimate change has played its role in threatening the taiga ecoregion. Equally as harmful are the human effects like deforestation, however many associations and regulations are working to protect the taiga and reverse the damage. Climate change is resulting in rising temperatures, and decreases in moisture, which cause parasites and other insects to be more active thus causing tree stress and death. Thawing permafrost has led to many forests experiencing less stability and they become \"drunken forests\" (the decrease in soil stability causes the trees to lean or fall over). Increased tree death then leads to a carbon dioxide outflux, thus further propagating the increases in global warming. It is essential for climate change to be combated with global action, which is what the Kyoto Protocol in 1997 was created to do. Other measures to protect the taiga would be to prohibit unsustainable deforestation, switch to renewable energy, and protect old growth forests, (they sequester the most carbon dioxide). The taiga also suffers from more direct human effects such as logging and mining sites. Logging has been a very profitable business in the region, however fragmentation of forests leads to loss of habitats, relocation of keystone species, increases in erosion, increases in magnitude and frequency of flooding, and altered soil composition. Regions in which permafrost has thawed and trees have fallen take centuries to recover. Canadian and Russian governments enacted a Protection Belt, which covers 21.1 million ha, and initiatives like the Far East Association for the use of non-timber forest products, gives economic significance to the forests while avoiding logging. In addition to logging, studies have measured over 99,300 tones of airborne pollutants from just one metal extracting plant over a 50-year span. These pollutants are 90% sulfur dioxide, which is a precursor to acid rain. Other emissions include nitrogen oxides, sulfurous anhydrides, and inorganic dust. Forests in a radius of these sites can serve little to no biological services once affected, and there has been little appearance of protection measures to regulate mining plants.\n\nThe taiga is inhabited by many species, some of which are endangered, and include the Canadian lynx, gray wolf, and grizzly bear. The Canadian lynx is one well-known animal to inhabit the North American taiga region and is listed as threatened in the U.S. The mother lynx will have a litter of about 4 kittens in the spring. Following the birth, the female is the sole caretaker, not letting them out of her sight until 12 months when they begin to learn to hunt. According to the USDS Forest Service, protection for the lynx has increased since 2000, which marks the date it became protected under the Endangered Species Act. Since much of the lynx’s habitat is land managed by the agency, efforts to maintain and increase the habitat for the Canadian lynx using forest management plans are underway.\n\nThe taiga region is also interspersed with various plant species. The endangered or threatened species include Labrador tea, lady’s slipper orchid, helleborine orchid, long leaf pine, ligonberry plant, Newfoundland pine marten, Methuselahs beard, lodgepole pine, and Scots pine. The life history of the long leaf pine is a tree species that has been around for quite sometime, and can reach more than 250 years in age. To begin the tree’s life, a seed falls from the parent in October to late November awaiting water to begin germination in a few weeks. For those individuals that make it, they will enter what is known as the grass stage. During this stage the roots are being established, and the bud of the tree is protected from fire. Years later, the long leaf will reach about in height and the diameter will increase with time. Somewhere around 30 years after the trees will begin to produce cones with fertile seeds and average about at maturity. One recent study discusses the effects of logging in the 1950s on pine species. Since then, conservation efforts have increased the number of pine (and other) tree species. The Nature Conservancy is prioritizing its protection efforts to rebuild long leaf pine forests through land purchases, conservation easements, and management of land sites. Restoration is also a large part of efforts to ensure the long leaf pine remains extant. By planting seedlings, controlling competitive vegetation, and controlled burning methods, scientists and volunteers are working to increase the number of the long leaf pine.\nOver the next 100 years, global annual mean temperatures are expected to rise by 1.4−5.8 °C, but changes in high latitudes where the boreal biome exists will be much more extreme (perhaps as much as a 10 °C rise). Warming observed at high latitudes over the past 50 years exceeds the global average by as much as a factor of 5 (2–3 °C in Alaska versus the 0.53° global mean).\n\nThe effects of increased temperature on boreal forest growth has varied, often depending on tree species, site type and region, as well as whether or not the warming is accompanied by increases or decreases in precipitation. However, studies of tree rings from all parts of the boreal zone have indicated an inverse growth response to temperature, likely as a result of direct temperature and drought stress. As global warming increases, negative effects on growth are likely to become more widespread as ecosystems and species will be unable to adapt to increasingly extreme environmental conditions.\n\nPerhaps the most significant effect of climate change on the boreal region is the increase in severity of disturbance regimes, particularly fire and insect outbreaks. Fire is the dominant type of disturbance in boreal North America, but the past 30 plus years have seen a gradual increase in fire frequency and severity as a result of warmer and drier conditions. From the 1960s to the 1990s, the annual area burned increased from an average of 1.4 to 3.1 million hectares per year. Insect outbreaks also represent an increasingly significant threat. Historically, temperatures have been low enough in the wintertime to control insect populations, but under global warming, many insects are surviving and reproducing during the winter months, causing severe damage to forests across the North American boreal. The main culprits are the mountain pine beetle in the western provinces of British Columbia and Alberta, and the spruce bark beetle in Alaska.\n\nTaiga (boreal forests) have amazing natural resources that are being exploited by humans. Human activities have a huge effect on the taiga ecoregions mainly through extensive logging, natural gas extraction and mine-fracking. This results in loss of habitat and increases the rate of deforestation. It is important to use the natural resources but its key to use natural resources sustainably and not over exploit them. In recent years rules and regulations have been set in place to conserve the forests in order to reduce the amount of trees that are cut. There has been an increase in oil extraction and mining throughout the United States and Canada. Exploitation of tar sands oil reserves has increased mining. This is a large operation that started in Alberta Canada. Oil extraction has a direct effect on the taiga forests because the most valuable and abundant oil resources come from taiga forests. Tar sands have affected over 75% of the habitat in Alberta taiga forest due to the clearing of the forests and the oil ponds that come from the extraction. These tar sands also create awful toxic oil ponds that affect the wildlife and surrounding vegetation. Oil extraction also affects the forest soil, which harms tree and plant growth.\n\nToday, the world population has an increasingly high ecological footprint and a large part of that has to do with the populations carbon footprint. As a result of that, oil supplies have increased, which has spread across the U.S. and into other countries. This is detrimental to natural ecosystems. Taiga being the largest region is seeing major consequences of our actions on extracting oil and natural gas. This is also causing climate change temperatures to increase at a rapid rate, which is affecting wildlife and forests. However, even though Human activities are responsible for the exploitation of these natural resources humans are the solution and have the tools to fix this issue. It is crucial that humans reduce the consumption rate of these natural resources in order to increase environmental conditions.\n\n\n\n\n\n\n\n\n\n\n\nMost of the water in this ecoregion is fresh water and contained in rivers, lakes, and ground water. Washington, Oregon, and Idaho are mainly drained by the Columbia River, its tributaries, and other streams that flow to the Pacific Ocean. The Columbia River Basin is the fourth largest watershed in North America. According to a 2004 GIS inventory by the Environmental Protection Agency, there are approximately 10,535 lakes and reservoirs in the Pacific Northwest. The largest lakes in the Pacific Northwest include Lake Washington, Lake Roosevelt, Lake Chelan, Upper Klamath Lake, Lake Pend Oreille, Priest Lake, and Lake Coeur d’Alene.\n\nIn British Columbia the Fraser River watershed covers one-fourth of the land and extends from Mount Robson to the Georgia Strait and Gulf Islands. This basin is the fifth largest drainage basin in Canada and contains thirteen main sub-watersheds, each consisting of small rivers, streams, creeks, marshes, bogs, and swamps. The largest lake in British Columbia is Williston Lake which covers 680 square miles.\n\nAlaska contains abundant natural resources which include ground and surface water. The southwestern part of Alaska is drained by the Yukon River and its tributaries that include the Porcupine, Tanana, and Koyukuk Rivers. The Yukon River is the third longest river and fourth largest drainage basin in North America with a drainage area of 832,700 square kilometers. Alaska contains over three million lakes and the largest is Lake Iliamna which covers an area of 1,000 square miles.\n\nVegetative cover is extremely diverse within the northwestern forested mountain ecological region as the region can be broken down into different zones based on elevation, temperature and mean annual rainfall. Alpine communities; areas of high elevation (> 8,200 feet) can support the growth of herbs, grasses, lichen, and shrubs well adapted for these harsh conditions. Common plants here include mountain sorrel, capitate sedge, mat muhly, Newberry knotweed, and red huckleberry. Lichens such as the witch’s hair lichen and cup lichen also persist here. Subalpine communities; located below the alpine communities (6,500-8,200 feet) support the presence of lodgepole pine, subalpine fir, pacific silver fir, grand fir, and Engelmann spruce. The Engelmann spruce–subalpine fir forest association occupies the greatest water-yielding areas in the Rocky Mountains and the natural adaptations of these trees are important in maintaining stable vegetation. The mountainous slopes and rolling plains slope from about 5,500 feet at the foot of the Rocky Mountains to about 2,000 feet in the lowest elevations. The dominant trees present in the region consist of; ponderosa pine, Rocky Mountain Douglas fir, lodgepole pine, and quaking aspen the drier southeast and central portions. Western hemlock, western red cedar, Douglas fir, and western white pine make up the majority of the moist west and southwest portions. White spruce is also found at this elevation and is a keystone tree species found in the Alaskan interior. The dry southern interior grasslands and forests generally occur at low elevations (under 4000 feet) and usually have a lower canopy closure than forests at higher elevations that receive more precipitation They are characterized by very warm to hot, dry summers, and moderately cool winters with little snowfall. Frequent low-severity, \"stand-maintaining\" fires are thought to have played a key historic role in shaping these ecosystems. Much of this area consists of small scrub like ponderosa pine with bluebunch wheatgrass, blue grass, June-grass, and big sagebrush dominating the understory.\n\nThis ecoregion is abundant with varying types of mammals, fish, and birds. Many dominant animal species, such as the bighorn sheep and hoary marmot, have adapted to the terrain of the region. The talus slopes provide burrowing shelters for the hoary marmot, and the bighorn sheep have adapted to climb the steep slopes in order to find shelter from predators (National Park Service). Top carnivorous predators include coyotes, wolves, and cougars. The grizzly bear is a keystone species found in this region. As an \"ecosystem engineer\", they regulate the species they prey on, disperse plant seeds, aerate the soil as they dig, and bring salmon carcasses into the forest (Suzuki). The dominant fish species of the region, in which the grizzly bear preys on, is pacific salmon. The typical bird species that can be found here include blue grouse, Steller’s jay, and black-billed magpie (Commission for Environmental Cooperation, 2008).\n\nThe northern spotted owl (\"Strix occidentalis caurina\") is considered a species of utmost concern in the Northwestern Forested Mountains region. This small raptor was listed as threatened under the Endangered Species Act of 1973. The current population is 15,000 birds, all of which are located in North America. Over 70% of the species’ habitat was destroyed in the 19th and 20th centuries, and the timber industry is causing that number to increase. Both northern spotted owls and the timber industry prefer old-growth forests, so as demand for timber products increases, the spotted owl’s habitat decreases. Forest management plans that stress limits on timber harvest and suggest alternative options are being formed, along with plans to prevent habitat fragmentation.\n\nThe barred owl is also causing a decrease in the population numbers of the northern spotted owl, as they are a larger, more competitive species that have begun to use the same habitat, however, no major plans have been formed to manage this situation.\n\nMalheur wire-lettuce (\"Stephanomeria malheurensis\") is also an endangered species in the region. Only one population of this plant survives in the wild, located in Harney, Oregon. The self-pollinating shrub is found at high elevations in volcanic soils. Because the range is so small, any disturbance in the habitat could be detrimental. One of the main threats is Cheatgrass, which can expand to completely cover the ground and use up resources also needed by Malheur wire-lettuce. It is generally agreed that in order to protect the species, efforts must be focused on forming new populations, and more importantly, maintaining the condition of the current site in Oregon.\n\nThe Northwestern Forested Mountain ecoregion is rich in natural resources. Historically the most sought after resources were the minerals found here. The presence of gold drove much of the early settlers to this ecoregion. These early settlers extracted gold from the streams, and timber for building, flora, and fauna. Today many more resources are utilized by the economies of this area. Large scale mining operations have become less common throughout the entirety of the region. There are a few prospective industrial mines lobbying for permitting to dig in both Canada and Alaska. Canada is the 6th-largest petroleum producer in the world. The largest point of extraction within this ecoregion is in Alberta, Canada. This area is abundant in tar sands, a crude form of petroleum. In order to begin this operation large tracts of boreal forest are removed. After the large pits are dug there is a constant risk of further environmental degradation through oil spillage. Logging in the past was often conducted through large clear cuts. The environmental effects of large clearcuts became apparent and are now less common. There are logging techniques that can benefit the ecological integrity of a system. Group selection can mimic natural processes and increase both horizontal and vertical structure to a forest. As well as increase biotic diversity of both flora and fauna. Tourism generates a considerable amount of revenue for the different economies of this area. Tourists come to these areas for a multitude of outdoor activities. In the winter tourists travel from all across the globe to ski the Rocky Mountains, British Columbia, and Alaska ranges. In the summer the national parks draw in millions. Other summer activities include but not limited to hunting, fishing, mountain biking, backpacking, rafting, kayaking, and wildlife viewing/ photography. Resource use and extraction is sustainable when a system can replenish resources faster than they are being used. A practice is unsustainable when usage exceeds this threshold thereby damaging the ecological integrity of the ecoregion.\n\nExtending from the lower Yukon of Canada all the way into northern California and Nevada, the northwestern-forested mountains range in different about three climate zones; moist maritime, arid dry, and sub arctic.\n\nThe moist maritime climate of the Northwestern Forested Mountains is found along a narrow strip of coastal Oregon, Washington, British Columbia, and southern Alaska in North America. It is formed by westerly winds coming off of the Pacific Ocean, which hit the mountains and rise to a cooler atmosphere. This causes rainy, cloudy, and moist atmospheric conditions where up to 100 inches of rain per year can be seen, and is a temperate zone ranging from about 15 °F in the winter to about 65 °F in the winter.\n\nThe arid dry zone is west of the mountain ranges and doesn't receive much rain due to the north to south orientation of the mountains, which block clouds and precipitation. It can range from the upper 80s (°F) in the summer to single digits in the winter. It generally only receives about of rain per year.\n\nThe sub arctic region ranges from Fairbanks, Alaska to the Yukon of Canada and averages a mean of 50 °F. in the summer and is often negative 13 in the winter. On the mountain tops it can receive up to of precipitation per year, and often considered the snowiest place on earth.\n\nThe Northwestern Forested Mountains experience phenomena called decadal oscillations, the La Niña and El Niño. This is a shift in temperatures from warmer (La Niña) to colder (El Niño) and each phase generally last about a decade. These phases are caused by many factors including, jet streams, trade winds, precipitation, land surface, temperature, ocean surface temperature, and sea level pressure.\n\nThe biggest threats to this region are fires and invasive pests. As fires occur, they alter the forest composition dramatically. Fire scars create entry for heart rot and other fatal conditions. Burned soils repel water and the runoff creates sediment and ash polluting rivers and streams, harming fish and wildlife that depend on these water sources. An especially troubling aspect of fires’ aftermath is the increased vulnerability of trees to non-native invasive pests. Burned stands create a perfect habitat for pests who will find shelter in the regrowth. These pests create tunneling galleries that further weaken a tree’s ability to fend off pathogens that lead to mortality.\n\nPreventing forest fires and controlling pest populations go hand-in-hand, which leaves room for any combination of treatment plans. Especially helpful is the use of prescribed burns, which consists of randomly dropping a match on a grid that has been divided and planted at scattered time periods. After the fire, workers must go in to peel bark off felled logs, and, if possible, remove dead, dying, and severely damaged/stressed trees as soon as possible.\n\nThe effects of fossil fuels emissions, the largest contributor to climate change, cause rising CO2 levels in the earth’s atmosphere. This raises atmospheric temperatures and levels of precipitation in the Northwestern Forested Mountains. Being a very mountainous region, weather patterns contribute higher levels of precipitation. This can cause landslides, channel erosion and floods. The warmer air temperatures also create more rain and less snow, something dangerous for many animal and tree species; with less snow pack comes more vulnerability for trees and insects.\n\nA large contributor to fire susceptible forests is past land use; the higher air temperatures make wildfires more common. Wildfires are extremely detrimental for species inhabiting the landscape; they destroy habitats and it takes many years to restore the land to how it used to be.\n\nThese effects caused by climate change can destroy animal habitats and species diversity. Not only will these climate catastrophes directly reduce animal populations, but it will indirectly disrupt trophic levels by reducing food sources for many keystone species. Climate change contributes to a worsening economy in this region as well by taking away valuable resources for recreational uses, like snow for skiing and fish for fishing.\n\n\n\nThe region is strongly influenced by the large mountain ranges stretching throughout most of the coast. Changes in elevation cause changes in plant/animal diversity, this can be exemplified through observing the alpine tundra's vegetation which consists of shrubs, herbs, mosses, and lichens; while lower elevations, the temperate coastal forest hold magnificently large trees such as western hemlock, California redwood, and the red alder. These differences are in direct correlation with the availability of oxygen, and other nutrients at higher elevations. The mountains also create rain-shadow areas due to the clouds having to release their precipitation in order to get over the mountains, or be blocked all together. Trees, which perform better under stress, grow in these areas such as the Douglas fir (www.countriesquest.com). As for the soil, the region generally has a thin podzol soil, causing it to be extremely acidic. Farmers must compensate by applying fertilizers and lime to lower the acidic levels for agricultural viability. Digging even deeper the then soil within the region will reveal mostly igneous and sedimentary rock. Colluvium and morainal deposits make up most of the surface materials. Mountains, which so intensely affect the region, are massive formations resulting from upheaval caused by continental collisions\n\nThe climate of the marine west coast forests is humid. According to the Köppen climate classification System, this climate is very damp throughout most of the year, receiving a great amount of rainfall along with heavy cloud cover. The marine climate can also be defined with its narrow range of temperatures throughout the year. Precipitation is ample and consistent in the marine west coast, with many days of rainfall and a large annual accumulation. Many areas in the marine west coast climate have more than 150 days of rainfall per a year, along with averaging around 50 to 250 centimeters per a year of total rainfall (Britannica, 2013). The average temperatures of areas within the marine west coast forests usually range from 10 °C to 15 °C (Britannica, 2013).\n\nThese mild temperatures are in collaboration with the moderating effect of ocean bodies on air temperatures due to the constant influx of oceanic air influencing the marine west coast throughout the year (Ritter, 2009). The marine west coast is located in the path of westerly winds from the ocean that contribute to its cloudy skies, significant amount of precipitation, and mild temperatures (Hollow, 2001). The rainfall, seasons, and temperature are all dependent on each other and are all affected by the global circulatory pattern.\n\nThe main watersheds in the region are the Puget Sound and Columbia River Watershed. Due to the region’s proximity to the Pacific Ocean, this ecoregion experiences large amounts of precipitation annually, creating a very humid and wet climate. The majority of river and stream activity is directly influenced by the annual precipitation patterns. In the rainy season from October to May, most of the low elevation rivers and streams experience peak run off levels. Rivers and streams at higher elevation are more influenced by snow melt and therefore experience peak run off from late spring into early summer due to the snowmelt. The permeability levels of bedrock in the area of interest dictate surface water in the region. Volcanic parent material, as found in Oregon, tends to result in lower levels of ground water due to the low permeability of the rock. Although areas with volcanic parent material may have fewer ground water aquifers, these areas tend to have better developed stream networks and higher stream drainage levels (Moore, 765). Areas with newer volcanic bedrock have higher levels of permeability, and are therefore more likely to have ground water aquifers. These areas will experience lower stream drainage densities and less developed stream networks due to the greater rate of ground water recharge (Moore, 765).\n\nThe plants in this region are responsible for holding the geography and geology of the area intact. The North-South orientation of the mountain ranges combines with the moist polar air masses and mild westerlies coming eastward off the Pacific Ocean to form a weather pattern that dominates the area. This pattern consists of a temperate moist zone on the west side of the mountains and a drier moderate climate on the east side. The moist conditions along with glacial valleys cut by the glaciers allow for a variety of plant life to thrive.\n\nThe softwood stands of the highlands are keystone species in maintaining land integrity. The ability of the firs and spruces to populate the high altitude and shallow soil works like glue to hold the soil in place. As you drop in altitude pines and cedars do the same for the lower slopes. Erosion control is key to keeping the glacial valleys and their rivers free from silt build up, which has the ability to devastate the salmon population, as well as holding the integrity of the mountain ranges.\n\nMarine West Coast Forests combine aquatic ecosystems with temperate rainforests to provide habitat for an abundance of wildlife. The sea otter is considered a keystone species because of the critical role it plays in maintaining the structure of the ecosystem. Sea otters feed on sea urchins, which are herbivores of kelps. A large mass of kelp can become an underwater kelp forest, which is considered by many to be one of the most productive and dynamic ecosystems on Earth. Two more dominant species found in the Marine West Coast Forest are the gray wolf and grizzly bears. Grizzly bears provide a connection between the marine coast and the forests when they eat nitrogen-rich salmon and transfer the nutrients to the forests. The Pacific salmon provide strong sources of nitrogen for the aquatic ecosystems. Due to the high precipitation in this Eco region, the nitrogen levels can be very low. The Pacific salmon helps to normalize the nitrogen levels. Without anyone of these species, the ecosystem would fall apart. The Marine West Coast Forests are a unique habitat for a diverse group of species.\n\nSeveral species struggle to survive in the ever disappearing and degrading ecosystems of the northwest. These species face a high risk of extinction; some iconic examples of those listed as threatened or endangered in this ecoregion include the giant sequoia, coast redwood, and marbled murrelet.\n\nThe giant sequoia and coast redwood are listed as a vulnerable under the IUCN Red List standards (Conifer Specialist Group 1998). Large-scale logging, felling 90 to 95 percent of the old-growth forest between 1856 and 1955, is primarily to blame for these species’ now limited range. The remainder of most populations of giant sequoias and coast redwoods is now almost entirely in parks and reserves (Farjon & Page 1999). Fire prevention policy, however, is most to blame for the continued declining of populations, as the build-up of undergrowth hampers the regeneration of both species (Vankat 1977). Luckily, plans to improve management and plant trees on cleared land are in place (Farjon & Page 1999).\n\nThough the marbled murrelet is still considered abundant, its population has undergone a rapid decline, principally because the old-growth forests in which they breed are subject to logging (Piatt et al. 2006). Current estimates are nearly half of historic numbers, suggesting just 350,000 to 420,000 remain (Piatt et al. 2007). The IUCN has listed the species as endangered (BirdLife International 2012). Hard forest edges resulting from forest fragmentation greatly subject murrelet nests to corvid predation and other associated disturbances (Peery et al. 2004). Declines in areas where logging is not an issue can be explained by the overexploitation and subsequent collapse of the pacific sardine fishery. Nylon gill-nets in shallow waters and oil spills have cause considerable mortality, as well (Piatt & Naslund 1995). In response, conservation measures have been implemented to slow the species’ decline, including: the prevention of logging within identified breeding areas (Nelson 1997), the development of detailed research and recovery plans (Kaiser et al. 1994, CMMRT 2003, Escene 2007), and the protection of 179 square kilometers on Afognak Island by the Exxon Valdex Trustee Council (EVOSTC 1995).\n\nThe Marine West Coast Forest's primary environmental threats are human development and population growth, logging, spruce bark beetle populations, and invasive species. This ecological region is home to large cities like Vancouver, Portland, Anchorage, and Seattle. As these cities continue to grow in population, greater tracts of land are being developed, and more resources are needed to accommodate these higher populations. Logging is another large human induced environmental threat to the ecoregion. Logging causes habitat fragmentation and adversely affects important species such as spotted owl, grizzly bear, and Kermode \"spirit\" bears, who all require large tracts of land to survive (Demarchi, Nelson, Kavanagh, Sims, Mann, 2013). The spruce-bark beetle is an insect that destroys spruce trees by tunneling into the bark of the trees. These beetles are widespread in the northern part of the ecoregion in states such as Alaska (Alaska Department of Fish and Game, 2013). The beetle’s distribution and survival rate has increased in the last decade due to climate change. Invasive species are also rampant in the ecoregion. These foreign plants and animals disrupt naturally occurring species in the ecoregion. Several solutions have been enacted to solve the environmental threats of the Marine West Coast Forest. Public land ownership is positively correlated with environmental preservation, as seen by the parts of the ecoregion located in Alaska (Alaska Department of Fish and Game, 2013). When land is privately owned, the most effective measures are education of the beautiful natural areas, smart land use, and planned efficient growth (Oregon Department of Fish and Wildlife, 2006).\n\nThe Marine West Coast Forests are located along the coast and some islands of northern California up to Alaska. The rise of the sea level will increase soil erosion of these marine areas (Coastal Areas Impacts and Adaptation). Depending on to what degree the sea level will rise, the introduction of salt water to the soil in the marine forest can slow and or destroy the growth of marine forest plants as well as the habitat of forest animals (Oberrecht). Freshwater flow will greatly disrupt the ecology of the Marine West Coast Forest. The trend seems to be that wet regions are getting wetter and the dry regions are getting drier (Song). The Marine West Coastal Region is a wet region that will most likely see these increases in precipitation levels.\n\nThe precipitation level increasing will change the stream chemistry of vital spawning areas for salmon. Spawning salmon are most successful when the water is cold and with a steady flow (Coastal Areas Impacts & Adaptation). The rising temperature of the streams from rainfall instead of snowfall will be more likely to also develop and spread disease through salmon (Coastal Areas Impacts & Adaptation). The estuaries, where the ocean and river water meet is a very vulnerable area. The rising sea level will bring more salt water into the estuaries (Oberrecht). The salinity of the water will increase further up rivers and this can alter the mixing and flushing rates of the estuary, increasing pollution dramatically (Oberrecht). The change of balance in an estuary will also decrease the buffer effect that estuaries have against storms (Oberrecht).\n\n\n\nVery few places in the world have the Mediterranean climate of California. It is one of the more rare in the world, with only five locations: the Mediterranean Basin, Southwest Australia, the Cape Province—Western Cape of South Africa, the Chilean Matorral, and the California chaparral and woodlands ecoregion of California and the Baja California Peninsula. The region is typified by warm dry summers and mild wet winters. This is unusual as most climates have more precipitation in the summer. There are three variations to the Mediterranean climate in California, a cool summer/cool winter variation, a cool summer/cool winter with summer fog variation, and a hot summer/cool winter variation. The average temperatures for the cool summer variations are below 71 °F in the summer and between 64 and 27 degrees Fahrenheit in the winter. Average summer temperatures for the hot summer variation are above 71 degrees Fahrenheit. Average annual precipitation for this climate is per year.\n\nDefined by the Pacific Coast on the west, the Sierra Nevada (mountains) and the deserts of California on the east, and the Northern California Coast Ranges on the north, the Mediterranean California ecoregion has unique physical characteristics that play a large role in the natural systems of the region, including hydrology.\n\nThe unusual precipitation pattern of the Mediterranean climate is due to subtropical high-pressure systems in the summer and the polar jet stream in the winter. Rainfall in the summer is uncommon because the marine layer becomes capped with dry sinking air. The marine layer is an air mass over a large body of water brought about by a temperature inversion from the cooling effect of the water on the warmer air. The marine layer is often accompanied by fog. The polar jet stream in the winter brings with it rain and snow. The jet stream is an extremely powerful air current flowing west to east often at over 100 miles per hour.\n\nThe precipitation in the region is closely associated with winter frontal storms from the Pacific Ocean, which bring cool air and rain to the area. The annual rainfall varies in different elevations, but the average range is between annually. Much of the rain in Central and Northern California flows out the Sacramento and San Joaquin Rivers, which with numerous tributaries run through an upper part of the ecoregion.\n\nFog is also an important aspect of the hydrologic cycle in this ecoregion; the cooling of air over the warm seawater create an dense fog that covers large areas of the coast. This fog affects the vegetation and overall environment on the coast. On the contrary, fire also influences this region. The fire-flood sequence that occurs post-fire can greatly effect populations of species in the region. The combination of the geophysical characteristics, little rainfall, and the bodies of water in the region make it a unique, distinct environment.\n\nMediterranean climate California's geology is characterized by the meeting of the North American Plate and Pacific Plate, with much of its region near or influenced by the San Andreas Fault along the junction. When the two plates collided the Pacific Plate was pushed under the North American Plate, and the California Coast Ranges and Sierra Nevada were uplifted. The Coast Ranges are largely metamorphic rock formed from the submergence of the Pacific Plate, and the Sierra are uplifted granite batholiths. Not along the San Andreas Fault, the granitic Peninsular Ranges system also uplifted with the collision, and runs from Southern California, down the Baja California Peninsula, into Baja California Sur state, northwest Mexico. The Transverse Ranges are another major Southern California mountain system primarily in the Mediterranean climate zone. Large earthquakes can do considerable damage to populated areas, and to the state's water, transportation, and energy infrastructure.\n\nThe Central Valley of California is a significant feature of Mediterranean climate California. It was an ancient oceanic inlet that eventually sediment filled in, the deposition supplied by erosion of the surrounding mountain ranges. The soil is composed of both the metamorphic, oceanic crust-like Coastal Range sediment and the mineral-rich granitic Sierra sediment. The combination creates very fertile soil. The flatness and fertility of the soil, along with the almost year-round sunshine has attracted much agriculture to the area. As a result, native species no longer dominate the landscape. The southern portion, named the San Joaquin Valley, also produces two-thirds of California’s oil from underground reserves. Fossils are found where adjacent tar pits occur.\n\n\nThe Mediterranean California ecoregion, is well known for its large variety and abundance of animals. One of these important animals is the American golden eagle, which plays a massive role in maintaining the ecoregion’s ecosystem through its top-down predation on smaller, more abundant animals. The golden eagle is considered to be the apex predator of this community, and there are no other species bigger than them on the food chain. Their lifespan can be up to around 30 years in the wild and even longer in captivity. Native to mountain areas and grasslands, California is a great region for this bird of prey to thrive in. The main reason for the golden eagle being a keystone species of this ecoregion is their ability to keep small herbivorous mammal populations in line. \"Prairie Dogs, ground squirrels, other rodents, hares, and rabbits, all of which eat grass and seeds, constitute 77.9% of the golden eagles diet.\" They also are known to prey on animals such as, cranes, black-tailed jack rabbits, swans, deer, coyotes, badgers, mountain goats, bobcats, and various fish species.\n\nAnother less popular species, but yet still keystone to this region, is the kangaroo rat. Studies have shown that kangaroo rats play very large roles in maintaining the population sizes and animal diversity throughout the region. Although they are small and on the verge of extinction, these animals play a large role in maintaining plant diversity, which helps the various herbivores with food supply, and also protection for other small animals seeking shelter. kangaroo rats occupy many land habitats ranging from desserts, and grasslands, to chaparral areas making them present in all areas of the Mediterranean California ecoregion. Kangaroo rats like to feed on many various grass seeds, as well as mesquite beans and thus is the reason that plants tend to not grow as well when sharing the same community with these rats. On occasions though, these animals like to feed on green vegetation, and insects. Unfortunately for the rat though, it is preyed upon by many predators. These predators include, owls, snakes, bobcats, foxes, badgers, coyotes, cats and dogs, and many more. Other dominant species in the region include, mountain lions, coyotes, sea otters, brown bears, and various large birds of prey.\n\n\nThe vegetation in the Mediterranean California ecoregion is a mixture of grasses and shrubs called chaparral with some oak forests as well. This area is very highly populated and agriculture is prevalent in the valleys (Comm. of Env. Coop. 2011). Evergreen trees and shrubs—such as heaths—mainly dominate Mediterranean vegetation with a shrubby to herbaceous understory. Mediterranean vegetation embodies less than 5% of terrestrial ecosystems around the world. A very important aspect of this ecosystem is its frequent wildfires leading to most of its vegetation adapting fire response mechanisms (Vilà and Sardans 1999). Common shrubs within this region are chamise or greasewood (\"Adenostoma fasciculatum\"), manzanita (\"Arctostaphylos\" spp.), coast sagebrush (\"Artemisia californica\"), and California-lilacs (\"Ceanothus \"spp.) (Conrad 1987).\n\nBecause the climate is so dry and experiences frequent fires, competition is high among plants within this ecoregion. The Mediterranean community found in southern California is said to have a successional stage after wildfires. The fire leaves patches of bare ground which then are quickly filled with newly germinated seeds. Native and introduced herbs persist for the first year following a fire. Shrubs and subshrubs slowly fill in and hit their peak at four to eight years after the fire. Extinctions, unlike many other communities are frequently the cause of environmental extremes rather than competitive invasive species (Zedler et al. 1983). Human disturbance can increase wildfires with the introduction of grasses such as Bromus rubens which can be readily established in the newly burned, cleared patches. These grasses are more densely compacted and create more fuel for fires. Agricultural grazing can also greatly decrease the chaparral (tangled shrubby brush habitat), which is the home of many native endemic species (Fleming et al. 2009, Zedler et al. 1983).\n\nAn endangered species is a species of organisms, either flora or fauna, which face a very high risk of extinction in a proximate time frame, much sooner than the long-term horizon in which species typically persist. There are many species of birds, mammals, reptiles, amphibians and plants that live in the Mediterranean California chaparral and woodlands ecoregion. Yet due to a variety of factors including habitat loss due to the 30 million humans who share the land, some species are endangered.\n\nEndangered, threatened, and vulnerable species of the Mediterranean California chaparral and woodlands ecoregion include:\n\nThe California condor (\"Gymnogyps californianus\") is one of the most iconic species in the state. With over a wingspan, condors are the largest flying land bird in North America. They are opportunistic scavengers that prey on large dead mammals. The main factors that led to the species endangered status were settlement of the west, shooting, poisoning from lead and DDT, egg collecting, and general habitat degradation. Serious conservation efforts have been made since the 1960s and this severely endangered species has begun a recovery path. A condor recovery program has been started and a wild population is steadily growing.\n\nAnother species is the tiny and secretive San Joaquin kit fox (\"Vulpes macrotis\" subsp. \"mutica\") is one of the most endangered animals in California. The kit fox is the size of a cat, with big ears, a long bushy tail and furry toes that help to keep it cool in its hot and dry Californian Mediterranean environment. Biologists state that there are fewer than 7,000 San Joaquin kit foxes. San Joaquin kit fox populations rise and fall with the amount of annual rainfall: more rain means more kit foxes. Changes in precipitation patterns, including reduced rainfall and increase changes of drought, all caused by climate change, would affect San Joaquin kit fox populations. The change in the Central Valley from open grasslands to farms, orchards, houses and roads has most affected San Joaquin kit foxes, causing death, illness, injury, difficulty in finding a mate and difficulty in finding food. These kit foxes also are killed and out competed for resources by coyotes and red foxes. Another threat is poison used to kill rats and mice. A recent decision by the federal government to limit to use of these poisons outdoors may keep kit foxes safe.\n\nHumans have used resources of this ecoregion for many years, dating all the way back to early Native Americans. Some traditional resources that are still used today are in danger of being overharvested. These include the Pacific Ocean fisheries, the dwindling timber industry, the rivers flowing from the mountains and the grasslands. All of these resources are either being over harvested or destroyed through agricultural and industrial development. Grasslands hold many native oak trees that are being lost due to overgrazing or forest fires. The overgrazing is attributed to the increasing number of cattle farms while the forest fires come from the use of natural water for human and agricultural use. As more water is used, oak trees lose out without this key component and fires increase due to drying out of the grasslands and forests. The government has tried to install conservation programs to halter the increased use of the land and waterways, but more must be done to create a truly sustainable environment.\n\nEmerging resources from the region are mainly high value agricultural crops. These include stone fruits, sugar beets, rice, nuts, grapes, cotton and specialized cattle systems. Many of these cannot be grown in other parts of the country and thrive in this type of climate. However, because of the dry seasons, these products require large amounts of water as well as varied chemicals and fertilizers to increase production. Many of these farming enterprises are enormous and not sustainable. They leach out chemicals, bring in mass amounts of inputs, and degrade a lot of the land. As with the traditional resources, the government has implemented conservation programs, but only a limited amount.\n\nClimate change in the Mediterranean California ecoregion is expected to ultimately have negative effects on the ecosystem and the region's biodiversity. The coast of California is expected to warm by as much as 2 °C in the next 50 years. This is going to cause hotter and drier seasons; the normally wet winters (when a majority of the ecosystem's rain in received) will be drier, and the summers will be especially hotter as well. Increased wildfires will result from the region's warming – mainly in the summer. The shrubbery and trees characteristic of the California chaparral will not fare well in the warmer (and increased fire) region; grasses that are able to regrow asexually or from special off chutes will fare the best. Ultimately the soil quality is going to degrade due to the increased burnings and increased temperatures. Overall, climate change does not bode well for the Mediterranean California ecosystem.\n\nThere are several large threats to this region. Many of California’s large population centers are located within it which causes stress on the surrounding environment because people have a desire to move to California so new homes and industry have to be established in order to accommodate all of the people moving into the region and this requires expansion. Research shows that this eco region is already 20% urban environments and 15% agricultural lands. The research also concluded that population density and urban area has increased by 13% between 1990 and 2000 while agricultural lands in the region have only expanded by 1%. The study conducted also showed direct relationships between the growth of the population and the number of species that were threatened in the area. Expansion will break up the contiguous landscape and move humans closer to the native flora and fauna which will over pressure species that need large open tracts of land to thrive and harm the species diversity of the region. Prevailing winds coming from the west off of the Pacific Ocean all of the pollution created gets carried up to these higher inland sites and causes the species there to suffer with the pollution generated.\n\nThe region is also plagued by wildfires. The area is becoming arid species diversity will drop as organisms adapted for dryer climates thrive. No current management plans are in place, a Species refugia to save struggling species that inhabit this region has been proposed by some. Forests similar to these are more resilient to such events due to the spatial arrangement, it would be possible to replicate this in the current forest and make it resilient to the fires that will increase in the near future.\n\n\n25 February 2013.\n\nThe Eastern Temperate Forests of North America are a vast and diverse region. Stretching inland from the Atlantic coast about 385 miles (620 km), they reach from Michigan in the north and Texas in the south; they cover the land of New England to Florida, Alabama to Michigan, and Missouri to the Appalachian Mountains. This ecoregion enjoys a mild and moist climate, though it is generally warmer as latitude decreases and drier as longitude increases. Warm summers and mild to cool winters have provided favorable growing conditions for a number of plant species, the dominant being large, broadleaf, deciduous trees and (to a lesser extent) needle-leaf, coniferous, evergreen trees. Indeed, before the arrival of Europeans, this area was almost completely forested. After their arrival a few centuries ago, much of the eastern forests had been cleared for timber and to make way for cropland. In more recent time, however, these open areas have been abandoned and are slowly returning to forest. Although heavily influenced by people, the Eastern Temperate Forests have proven to be a very resilient region; these great forests still provide habitat for many birds, animals, reptiles, amphibians, and insects, as well as recreational and economic benefits for the people of the region.\n\nThe Eastern Temperate Forest region has a wide range of fluctuating temperatures dependent on time of year. In this region, there are four distinct seasons- winter, spring, summer, and fall. This seasonal variation is caused by exposure to both warm and cold air masses due to the biomes mid-latitude positioning between the polar regions and the tropics and is reflected in both the seasonal temperatures and precipitation levels. The highest temperatures, averaging 21 °C, occur during the summer months of July and August, and the lowest temperatures, averaging 0 °C, occur during the winter months of December, January, and February. The year-round average temperature within the region is 10 °C. Levels of precipitation vary with the seasons as well, with the highest levels of precipitation, averaging 95 mm/month, occurring in May and August, and the lowest, averaging 60 mm/month, occurring in June and the winter months of January, February, March, and December. The Eastern Temperate Forest region can thus be described as \"warm, humid, and temperate\" with abundant levels of precipitation year-round.\n\nThere are many global patterns that affect and contribute to the climate of the Eastern Temperate Forest region, such as global ocean currents, El Nino, La Nina, the Gulf Stream current, and global air circulation patterns. El Niño, caused by warmer sea-surface temperatures in the Pacific Ocean, can lead to \"wet winters\" and warm episodes occurring between the months of December and February in the southeastern region of the United States Eastern Temperate Forest. La Niña is caused by cooler than normal sea-surface temperatures in the central and eastern tropical Pacific Ocean, it leads to drier than normal conditions in the winter months in the Southeast region of the Eastern Temperate Forest. The global ocean current that effects the Eastern Temperate Forest most is the Gulf Stream current which brings a warm flow of water from South to North along the eastern coast of North America in the Atlantic Ocean, it keeps temperatures in this region relatively warm. The winds that have the greatest effect on the climate of the region are the prevailing westerlies and the tropical easterlies. The prevailing westerlies, caused by the Coriolis Effect, explain why most major events that occur in North America come from the west and proceed east, which is where the majority of the Eastern Temperate Forest is located.\n\nThe Eastern Temperate Forest Ecoregion has favorable growing conditions for a number of plant species, the dominant being large, broadleaf, deciduous trees. Before the arrival of Europeans, this area was almost completely forested. After their arrival a few centuries ago, much of these forests had been cleared for timber and to make way for cropland. In more recent time, however, these open areas have been abandoned and are slowly returning to forest. Of the many plant species that inhabit the Eastern Temperate Forests today, those of the oak (Quercus), beech (Fagus), maple (Acer), basswood (Tilia), and pine (Pinus) genera are the most characteristic and defining of this ecoregion. These plants can be broken down into several main communities: northern hardwood, beech-maple, maple-basswood, mixed mesophytic, oak-hickory, and southern mixed hardwood forests. With the exception of Pinus, all of these species are angiosperms, meaning that they produce flowers and fruits, an important food source to many animals who inhabit the region. The flowers of angiosperms provide nectar, their leaves are important vegetable matter for herbivores, and their seeds are rich in fat and protein rich that allow many animals to fatten up for their winter hibernation. The trees of the Eastern Temperate Forests provide food, shelter, and a suitable habitat for countless species of both flora and fauna; they yield lumber, fuel, recreation, and aesthetic enjoyment to not only the people who live in this region, but also those who visit and enjoy products produced from the resources gleaned from these vast forests.\n\nArboreal species are widely found in the region due to the high density of tree cover, providing a suitable habitat and food source for the animals; this includes birds and many ground squirrels. Migratory songbirds are common in the eastern temperate forests once the canopy opens up in the spring. Mammals that are native to the eastern forests are white-tailed deer, black bears, ground squirrels (gray squirrels and chipmunks), as well as red and grey foxes. Bird species include, the black-throated warbler, piping plover, and the yellow- breasted chat. Amphibious species that are common to the region are the American toad and the box turtle.\n\nWhite-tailed deer populations are very large across the eastern US, making it both a dominant and defining species. The white-tailed deer competes with other herbivores for limited food resources directly affecting the ecosystem, as well as indirectly affecting the area by altering habitats for small vertebrates and mammals. According to the Virginia Journal of Science’s research on white-tailed deer, deer are grazers primarily, feeding on the leaves of shrubs and such; however in the winter months they are found browsing the woody stems of shrubs and saplings. White-tailed deer have four stomachs, each with their own specific digestive action. The complex breaking down of food allows the deer to each woody plants and other things that most animals cannot digest. Areas with high deer populations, will see a dramatic shift in forest cover because small saplings and shrubs growth will be retarded on hindered due to their browsing habits. White tailed deer are polygamous; in the northern parts of the region they will mate in November and for more southern dwelling populations mating occurs in January. A female will give birth to one to three fawns, after a 6-month gestation period. After about 3 months, the young will leave their parents. White tailed deer typically live about three years but can live up to 15 years. White-tailed deer exemplify a \"k-selection\" species. They have long gestation periods, can reproduce more than once in a lifetime and are only a few offspring are produced at once.\n\nThe United States has more endangered species than all of the other continents combined, the Eastern Temperate Forest’s endangered and threatened species make up a little less than a quarter of that number. Endangered and threatened mammals (but not limited to) include, the Louisiana black bear, the red wolf, the Key deer, the eastern puma (cougar) the West Indian manatee, the North Atlantic right whale, the Mississippi sandhill crane, the piping plover, and the leatherback sea turtle. Endangered and threatened flowering/non-flowering plants include, the Virginia round-leaf birch, the Tennessee yellow-eyed grass, the Michaux's sumac, the Florida torreya and the Louisiana quillwort, among many others. The region is also home to the only two endangered lichen species, rock gnome lichen and Florida perforate reindeer lichen.\nThe piping plover is a bird that has been on the endangered species list since 1985 in the Great Lakes watershed (including: NY, PA, IL, MI, and WI.) This species nearly became extinct after over hunting in the 19th and early 20th century due to use of feathers for fashion hats. Current potential sources of endangerment include, the development of coastlines for recreation, and detrimental material washing up to shore. The management of the habitat sites, closing off sections of the beach where birds are nesting, creation of a mimic habitat, predation management, restriction of beach vehicles, and vegetation control are current conservation efforts being enforced.\n\nThe Louisiana quillwort has been on the list of endangered species since 1992; contrary to its name it is only now found in MI and AL. Threats to this species include, pollution (herbicides and chemicals), construction in proximity to stream, vehicle traffic on or near stream, changes in flow rate and erosion (these two factors most likely caused from climate change.) Conservation efforts being enforces are, updates to where the population status is, permanently protecting existing habitats (through local and federal levels), look for potential populations that are not accounted for, preserve the genetic stock of the species remaining, and more in-depth habitat studies leading to population fluctuation.\n\nThe Appalachian Mountains are a main topic of research, regarding the geology of the surrounding area. They formed when the ancestral continents of North America and Africa collided together and are about 480 million years old. The folded and thrust faulted igneous rocks, marine sedimentary rock and rocks that look like that of the ancient ocean floor, reveal that they got pushed up during plate collisions.\nIce ages, during the Pleistocene epoch (after the Appalachians formed), contributed a great deal to the current appearance of the surrounding area. Surfaces that were once covered by ice were eroded and smoothed out during glacier movement. Therefore, the Appalachians used to be much taller when they formed, than they are today. Glaciers also deposited parent materials of the underlying bedrocks, which contribute to the formation of soils later on.\n\nThere are very clear soil horizons, when looking at a cross section of this land. These are labeled and described (see Figure 2) as: O: organic matter, A: fine particles of organic matter and mineral material, B: material layer where most nutrients accumulate, C: parent material, and R: bedrock1. The U.S. Soil Taxonomy classifies Inceptisols, Mollisols, and Spodosols as good soils that can support temperate forests that like mature soils that can support deep root systems1. Different levels of nitrogen also have a big effect on a soils capability of supporting life. The presence of too much nitrogen can cause declines in species richness and abundance. The types of vegetation that exist in the Appalachian area heavily rely on the existing soil types and amount of nutrients available.\n\nThe Eastern Temperate region has a vast wealth of natural resources that are utilized by people. The two most common traditional resources include timber and coal. Timber specifically hardwoods, which make up the majority of timber from this region, are utilized widely for furniture production. In 1997 there was about 6 billion dollars worth of solid wood exports with 36% coming from the eastern United States. Coal is the other major traditional resource of the region. Coal is found on the western slopes of the Appalachian mountain range as well as in parts of Illinois and Indiana. In 2003 U.S. coal production was about 1.07 billion short tons and while not all of this comes from the eastern region a large portion of it does as 6 of the top 10 coal producing states are from within this region as of 2012.\n\nNatural gas and oil from hydraulic fracturing is an interesting relatively new emerging resource from the region. \"Fracking\" as it is commonly known involves sending pressurized water or sand into shale deposits into order to open up more cracks for which natural gas and oil can flow through, into the pipes and out of the ground. There were 8.982 drills as of 2011 in Pennsylvania alone that operated under hydraulic fracturing. Though this is an intriguing emerging resource for the region it also is extremely controversial as oil and gas from the \"fracking\" process can sometimes seep into ground water and contaminate it.\n\nThere are three major current threats to the Eastern Temperate Forest. These include agriculture, invasive species and overpopulation/urbanization. A major use of land in the eastern temperate forest is for agricultural purposes due to the rich soils which are easily converted to farmland. Pesticides in particular threaten the health of the eastern temperate forest region because they are used in massive quantities for agricultural production but are also widely popular in homes, businesses, schools, hospitals, and parks to maintain lawns or fields.\n\nAnother problem with no easy solution that faces the eastern temperate forest are non-native invasive species like the emerald ash borer. The emerald ash borer is thought to have been introduced to Michigan from China about 15 years ago. The adult beetles target ash trees as places to lay their eggs, when the larvae hatch they bore through the bark and kill the tree. The health of the ash population is of major concern because they provide habitat for many wildlife species and edible seeds for birds, mammals, and insects.\n\nThe biggest threat besides climate change to the eastern temperate forest is its high density of human inhabitants. According to the Commission for Environmental Cooperation approximately 160 million people or over 40 percent of North America’s population, lives within the ecological region of the eastern temperate forest12. Such population density can be attributed to the concentration of the continents economic, political, and industrial power in this region. Major cities and sprawling suburban communities between them have drastically changed the regions landscape and fragmented local habitat. Roads and highways divide habitat and limit migration while urbanization and deforestation completely eliminate suitable habitat and food sources. Studies conducted by Kansas State University have shown that fragmentation can decrease population productivity by isolating populations, crowding species, and causing edge effect.\n\nAs the planet faces more intense and severe effects of climate changes, each element of the eastern temperate region will be affected, from flora and fauna, to soil and water. Vegetation mortality, soil content, species existence, water levels, and overall functionality of the Eco region will continue to change and be altered as global warming and the concentration of greenhouse gases increases. Climate change correlates with disturbances such as insect outbreaks, harsh weather, and susceptibility of forests to invasive species, all of which can affect the functions of a forest. Insect breakouts can completely destroy an entire habitat within one season. With increased drought and higher temperatures, the weakened forest can suffer from multiple tree species loss, along with the loss of animals and creatures that serve vital predatory roles within the ecosystem. Plants that are considered to be moist-forest herbs, such as Cohoosh and Clintonia, are threatened by the lack of available water that is vital to their survival. As climate change more rapidly progresses, temperature increases will affect the length of the growing season. Tree species growing range will shift to adapt to the new climates, typically moving to higher altitudes or more northern regions. For example, mountaintop tree species like the red spruce will potentially die out because there is no higher altitude that is available for relocation. In addition to the northern migration, southern species such as the red oak have expanded their territories. Therefore, as species that thrive in the lower areas of the region are expanding into a greater space, they are beginning to compete for resources and nutrients with pre-existing native species. This can be said for many bird species as well. A study conducted by the USDA Forest Service confirms that 27 out of 38 bird species that inhabit eastern temperate forests, have expanded their territory further north. The water cycle is also incredibly susceptible to the effects of climate change. The water quality and ecosystems within lakes, streams, and rivers are all greatly affected by the alterations of precipitation patterns. Increases in runoff potentially increase the chemical contents within the water, such as nitrate and acid pulses. Aquatic species are stressed by not only the warmer temperatures themselves, but also the low flows and timing of ice-outs and thaws. Such factors affect oxygenation cycles, productive cycles, and reproductive cycles. Seeing as though the Eastern Temperate Forest region is considered to be a significant evolutionary zone for fauna, the effects of climate change can substantially alter the balances and chains of not only the Ecoregion, but the planet as well.\n\nThe Eastern Temperate Forest ecoregion is divided into five Level II ecoregions: Mixed Wood plains, Central USA plains, Southeastern USA plains, Ozark and Ouachita- Appalachian Forests, and Mississippi Alluvial and Southeastern Coastal Plains.\n\nThe land formation of the area of the Mixed Wood plains is predominantly plains, with some hills, and the bodies of water are many small lakes. The surface materials of the region are moraines and lacustrine and the soil composition includes forest soils and fine textured soils. The mean annual precipitation of the area ranges from and the mean annual temperature generally varies between 4–10 °C. In this area, human activity includes fruit and dairy agriculture, major urban areas, and some forestry and tourism attractions. The most prominent wildlife observed are white tailed deer, moose, and the grey squirrel, and vegetation includes a wide range of trees such as oak, hickory, maple, beech, and some pine and basswood species.\n\nThe second sub-ecoregion is the Central USA Plains, anarea of , that has a landform of smooth plains. The majority of this region’s surface material is moraine with some lacustrine, and the soil consists of calcium enriched prairie soils and forest soils on moraine. The climate consists of a mean annual precipitation of 760–1,100 mm and average temperatures varying from 7–13 °C. Human activities largely include corn and soybean agriculture, major urban areas, and local dairy operations. Vegetation is mostly prairie type in the west, but also includes oak, hickory, elm, ash, beech, and maple. White tailed deer, cottontail rabbits, and grey squirrels are the most commonly represented wildlife.\n\nThe Southeastern USA plains are the third Level II ecoregion and have a land area of . The majority of this land consists of irregular plains with low hills, which is made up of predominantly residuum and some loess on weakly developed soils. The climate of this region is an annual precipitation of and average temperatures of 13−19 °C. Human activities include predominantly forestry with tobacco, hog, and cotton agriculture, along with major urban areas. There is a wide array of wildlife which can include white-tailed deer, grey squirrels, armadillos, wild turkeys, northern cardinals, and mockingbirds. The vegetation of the area is less diverse and includes oak, hickory, loblolly, and shortleaf pines.\n\nThe Ozark and Ouachita-Appalachian Forests region is an area mostly consisting of hills and low mountains, with some wild valleys that make up the of land. This land is primarily residuum and colluvium matter on weakly developed soils and is put to use by humans through forestry, coal mining, some local agriculture, and tourism operations. The temperature averages around 17–18 °C annually and precipitation can be anywhere from , which provides a suitable environment for mixed oaks and hickory, white pine, birch, beech, maple, and hemlock trees. In this environment, black bears, white tailed deer, chipmunks, and wild turkeys are commonly found\n\nThe final of the five Level II ecoregions in the Eastern Temperate Forest is Mississippi Alluvial and Southeastern Coastal Plains. The of land in this region is home to a very vast amount of organisms including animals such as white-tailed deer, opossums, armadillos, American alligators, mockingbirds, and egrets, along with varying vegetation from bottomland forests (ash, oak, tupelo, bald cypress) and southern mixed forests (beech, sweet gum, magnolias, oaks, pine, saw palmetto). The climate of 13−27 °C and precipitation varying between annually provides adequate conditions for forestry, citrus, soybean, and cotton agriculture, fishing, and tourism.\n\n\n\n\n\n\nThe Tropical Wet Forests ecoregion in North America includes the southern tip of the Florida Peninsula in the United States; within Mexico, the Gulf Coastal Plain, the western and southern part of the Pacific Coastal Plain, most of the Yucatán Peninsula and the lowlands of the Chiapas Sierra Madre, which continue south to Central and South America.\n\nThe tropical wet forests of North America have an average year round temperatures between 68−78.8 °F. Thus, frost does not occur under these conditions. The temperatures remain fairly uniform throughout the year; therefore there is not a change of seasons. There is also no dry season, as all months experience precipitation. The average annual precipitation ranges from eight to fourteen feet per year. The high levels of precipitation usually cause poor soil quality because soluble nutrients are lost due to the nutrient leaching process. The average humidity is between 77−88%. Nine out of twelve months of the year are considered \"wet\" months. The overall climate of the tropical wet forests ecoregion can best be described as humid, warm, and wet. George Hadley, a scientist who researched during the 18th century suggested that warm tropical air rises and moves north. Colder high latitude air flows south nearer to the Earth’s surface where it displaces the former air. Hadley’s explanation is highly accepted and still expanded upon today. The warm, moist air in tropical wet forests is unstable; meaning as soon as the air rises it becomes saturated. In addition, there are large amounts of heat, or convection occurring at the same time. The vast bulk of vertical movement of air occurs in the Hadley cell and thus provides an explanation for the global circulation patterns.\n\nThe direction of the wind at various levels of the atmosphere determines local climate and can result in severe weather patterns. For example, in an El Nino winter the presence of warm water in the eastern Pacific Ocean can shift the position of a subtropical jet stream. This results in heavy rainfall in the tropical wet forest ecoregion. Also, in a warming climate the Hadley cell could increase the severity of climate. As a result, the ecoregion may become hotter and wetter for longer periods of time.\n\nHydrology in Tropical Wet Rainforests keeps the flow of water throughout the ecosystem, which nourishes plants, keeps local aquifers and rivers active and the environment in the area functioning. The watershed and basin pattern have three major contexts; first, low-gradient drainage, second, typically high ground water table, and third, extensive drainage canal network. This idea applies to all areas, but have unique outcomes in Tropical Wet Rain Forests in North America specifically. Tropical Wet Rainforests have an excess of vegetation, compared to many other ecoregion types such as savannahs, and therefore have a much slower drainage rate than other ecosystems. When an ecosystem has a high ground water table it separates the time between drainage and absorption of water in an area. It helps organisms to absorb nutrients, while also slowly filling up aquifers in the ecosystem. So primarily the down time between rainfall and drainage is slowed due to vegetation and climate, but now due to the vastness of the ecosystem, the drainage canal network is large and water can fall in one place, and end up in many other places at the end of the draining process.\n\nWet tropical forests in North America span from sea level to an altitude of . They have particular geologic, topographic and soil conditions that characterize them. These characteristics influence biotic structures and relationships and have contributed to the high biodiversity of the ecoregion.\n\nThe geology of these forests is primarily composed of folded and metamorphic hills, which are covered by a thin layer of alluvium (loose sediments and soil). The bedrock is sedimentary and rich in silica and dates back to the Precenozoic periods when much of the region was underwater.\n\nThe topography of wet tropical forests includes valleys, hills, ridges and low mountains. Depending on elevation and the location of such features, areas as referred to as either lowland or highland. These elevation and topographical changes allow for a higher variety of specialized conditions, which increases habitat. The inclination changes (or slope) of the forest floor greatly affects water drainage and the leaching of nutrients, and valleys can have an accumulation of sediments and nutrients versus plateaus and ridges. But the most important topographic characteristic is the extensive network of rivers that weave across the landscape, acting as a drainage system to the forest that can receive upwards of 250 inches of rain a year.\nThe soils in wet tropical forests are some of the most diverse of any region, and they are the cause for many biological adaptations. There is a combination of highly weathered and leached soils as well as less weathered alluvial soils, categorized as \"oxisols\" and \"ultisols\". Their pH can vary immensely, sometimes being as acidic as 4.0. The soils are generally shallow, often only a few inches deep.\n\nThe soil is produced from decomposing organic matter and the breakdown of bedrock, but is generally poor in nutrients; most nutrients are found as superficial detritus and within the living components of the ecosystem. There are multiple reasons for why the soil is generally very poor in nutrients. Firstly, the warm and humid climate allows for a rapid decomposition rate, meaning that nutrients do not stay present in or on top of the soil for long before being absorbed by the biota. Secondly, the acidity of the soil, caused by the few cation exchange sites to be occupied by hydrogen ions, increases the loss of minerals such as iron, aluminium oxides and phosphorus. Thirdly, leaching, which is the continuous downward movement and loss of solutes and minerals from the soil, happens regularly due to the heavy rainfall. You wouldn’t be able to tell that the soil is poor from the lush, dense vegetation in these wet tropical forests; but shortly after an area of forest is cleared for agriculture (usually through slash-and-burn) the small amount of nutrients wash away and the soil becomes infertile.\n\nThe ecosystems have developed highly specialized ways of mitigating effects such as leaching, but these functions are fragile, and need to be protected. This includes tree adaptations such as buttress roots and thick root mats that grow laterally along the forest floor. These adaptations mitigate nutrient loss by capturing the nutrients in falling detritus, before the nutrients are absorbed and decomposed into the soil, and lost from leaching by the heavy rains. The geologic, topographic and soil changes across wet tropical forest ecosystems has contributed to the astonishing biodiversity in biota we see today.\nThe plant communities of the tropical wet forest are the most diverse, abundant, and lush plant life in the world. The plants define the tropical wet forest by contributing to ecosystem functions, such as producing nourished rainfall and storing atmospheric carbon. Tropical wet forests are characterized by the complex, physical structure of the ecosystem. There are many layers of plant communities, though they are rarely visible from the ground. Shrubs and creepers fill the forest floor with saplings dispersed throughout. Large trees hold their full crowns in the canopy, prohibiting sunlight to plants below. Beneath the canopy of trees lies a network of stout branches, thick limbs, and climbers. Sometimes even above these trees, the largest of canopies fill the sky like individual islands.\n\nLarge trees, such as the pacque, allspice, and breadnut tree, provide habitat for most animal species and other plant species. The leaves are usually oval, thick, and waxy with pointed drip-tips to alleviate water collection. Roots are often buttressed (flaring from above ground), radiated across the forest floor, or stilted as prop roots. Lichens, orchids, and mosses cover the trunks of trees, retaining moisture and hosting small invertebrates. Most tropical trees have large, colorful, fragrant blossoms and plump fruits, perfect feeding for animals and insects. Climbers, hemiepiphytes, and epiphytes are the major groups of non-tree species, although they tend to inhabit trees. Climbers provide a road system in canopies for motile animals. Vines are large in biomass and are an essential food source to many fauna. Hemiepiphytes have the most unusual growth forms and are parasitic to larger trees. Epiphytes claim space on a branch and set roots, trap minimal soil, and photosynthesize. They adhere tightly to the bark of trees but, are not internally parasitic. As rain forests become drier and more disturbed, these native species become more rare. The loss of these plant communities severely affects the world, in regard to increase of carbon dioxide, high floods, and impure water.\n\nThe two main keystone species of the Tropical Wet Forest ecoregion are the American crocodile and the Mexican jaguar. They are both top predators and influence the population of their pray. American crocodiles create habitat for many creatures through their water holes and the paths they create. Their diet consists of fish, snails, birds, frogs, and mammals that come to the water’s edge. Males can grow up to 15 feet long and weigh up to 2,000 pounds while females range from 8–13 feet. Their average life span is around 45 years. Females lay a clutch between 20−60 eggs which hatch after an average of 85 days. The mother leaves the young to fend for themselves after a few days. The jaguar is the third largest cat in the world and the largest in North America. It is between 5 and 8 feet, nose to tail, and weighs between 140 and 300 pounds. Their average lifespan in the wild is 12–16 years while in captivity it ranges from 20–27 years. They have been observed to prey on around 85 different species, the most common of which are terrestrial mammals, they prefer giant anteaters, capybaras. Females become sexually mature around 2–3 years while males become sexually mature around 3–4 years. They have a gestation period about 100 days and give birth to an average litter of 2 cubs. The cubs are able to open their eyes after about 8 days and are able to walk 10 days after that. They stay with their mother for a year and half.\n\nTropical wet forests are known for their wide diversity of natural resources. Historically, the primary harvestable products they produce are from plants including exotic lumber such as mahogany, red cedar, and also gum tree for rubber. Other plants that can be utilized from this region include common food items such as bananas, cacao, oranges, coffee, sesame, alfalfa, cotton, and a variety of peppers.\n\nFollowing Spanish and English colonization of the area there was a shift towards larger scaled agriculture plantations. With these plantations came increased production of sugar cane, beans, pineapples, and chiles as well as an increase in harvesting of precious lumbers. This trend continued largely up into the 1960s when large swaths of land were cleared to make room for cattle ranches.\n\nConsecutively came the influx from the petrochemical industry to extract the vast reservoirs of oil that exist underground. This new development led to even larger portions of land being cleared for oil drilling sites and roads compounding the existing problem of deforestation in the region.\n\nOne ray of hope for the future of natural resource procurement in tropical wet forests is the search for medicinally valuable plant secondary compounds. Plants that contain compounds that can treat ailments ranging from analgesics, antibiotics, heart drugs, enzymes, hormones, diuretics, anti-parasitics, denitifrices, laxatives, dysentery treatments, anti-coagulants and hundreds more exist and could prove to be a valuable economically viable as well as sustainable alternative to current resources being utilized in the area.\n\nDeforestation is the main threat to the North Americans tropical wet forests and has devastating consequences. Deforestation causes habitat loss and habitat fragmentation which have drastic effects on biodiversity. Deforestation of tropical wet forests has caused many native species to become endangered or extinct at an alarming rate. The Tropical Wet Forests around the global are being deforested at an alarming rate. For example, some counties like Florida have lost 50% of their tropical wet forest habitat and Costa Rica has lost about 90%.\n\nProtection of the tropical wet forests we have left is very important for its continued existence. Many Reserves have been created in an attempt to protect the little we have left of these forests. Some examples of this in the United States are Florida's Everglades National Park and the Big Cypress National Preserve.\n\nAnother important tool for the continued existence of tropical wet forests is Restoration. There have been successful restoration projects of a tropical wet forest with native species in Costa Rica. These restoration projects have been shown to significantly improve the native animal and plant species survival. It is necessary for good management plans to be developed if we are to use tropical wet forests sustainably.\n\nThe IUCN Red List has 65,521 species listed as threatened in the tropical wet forests. The \"Harpia harpyja\", harpy eagle is one threatened species in the tropical wet forests, they are the largest neotropical bird of prey, nest in the tallest trees, prey mostly on animals that live in trees, lay between 1−2 eggs but only allowing 1 egg to hatch, reproduce every 2–4 years, and reaches sexual maturity between the ages of 4 and 5. The harpy eagle is suffering because of slow reproductive rates, hunting, food competition, fragmentation, and habitat destruction. There are many orchid species that are threatened in the tropical wet forests. Orchids are a smart plant that manipulate other species into pollinating them, and once pollinated they produce seeds that are eventually released in hopes to be carried to a specific type of fungi (depending on the orchid) where it will attach for mycorrhizal symbioses, and then bloom after a few years or decades depending on the environment and species. Many orchid species are suffering because of overharvesting, burning, clearing, and development. Many efforts are being done to help save both species. Spreading knowledge (educating), creating reserves, and coming up with alternatives are the top three actions being done to conserve both species.\n\nOver the last 100 years the Earth's temperature has increased 0.6 degrees Celsius and it is predicted to increase an additional 3.5 degrees over the next century. Tropical wet forests account for only 6% of earth's land surface yet are responsible for 40% of earth’s oxygen production. Any type of change to this system can prove to have significant detrimental effects in terms of global oxygen availability. In addition, due to the sensitivity and fragile interactions between organisms and the atmosphere, ecosystem services such as carbon sequestration rates, will experience even larger adverse effects.\n\nAmounts of precipitation and moisture availability are also areas of concern. Global precipitation is expected to rise two-fold in tropical areas. This will cause shifts in vegetation as moist forest species expand into new areas of moisture. Increasing atmospheric emissions also plays an integral role in precipitation patterns. Annual rainfall is projected to decrease across the Everglades National Park causing a hydrologic change across the entire region. Dry vegetative communities will outnumber hydric vegetative communities in this particular area.\n\nFurthermore, a one degree increase in atmospheric temperature is the result from a doubling of atmospheric CO2. Effects of this increase on forest soil temperature include reduced tree growth and higher decomposition rates of deep soil organic matter. Ultimately, as the forests become a larger carbon source to the atmosphere, ecosystem services cease to function, and the delicate balance found in the tropics is disrupted, the climate warming cycle intensifies.\n\nAn iconic ecosystem of this region is the complex interaction and the variety of biota along with fairly consistent abiotic factors; even though this eco region covers roughly seven percent of the earth's surface, its tree community is the most diverse on the planet. It would not be unusual to have 100 different tree species coexisting within a one-hectare plot. The tree community contains many broad-leafed evergreen trees, which form a high canopy (30–40 meters) above the ground. The understory contains a variety of more shade tolerant plants, which is a necessity for survival due to the thick canopy above. The vegetation is \"spatially heterogeneous\". This plant community survives in nutrient-poor soils conditions making disturbances (such as deforestation) to have greater effects because regeneration of the forest takes much longer. Tributaries and river systems have formed from the large amount of rainfall and typically carry a lot of sediments, but increase water demands and the construction of dams can further alter and strain these ecosystems.\n\n\n\n\n\n\n\nTropical rain forests. (n.d.). Retrieved from http://www.marietta.edu/~biol/biomes/troprain.htm\n\nThe North American Deserts include both cold and hot deserts, which supply a variety of climates. Due to this fact, they are often used for agricultural, business, or petroleum purposes. These factors have been taking a toll on the desert climate, organisms, and landscape. These deserts are the Mojave, Sonoran, Chihuahuan and the Great Basin.\n\nThe North American Deserts are home to a variety of plant species. These plants are categorized as either xerophytes, adapted to the arid conditions of the desert, or phreatophytes, which are plants with very deep roots that are dependent on a permanent water supply and survive by tapping groundwater.\n\nThese species have come to possess several adaptations that allow them to survive and thrive in these dry and harsh conditions. One of the most common of these species is the barrel cactus (Echinocactus and Ferocactus). This plant was important to Native Americans and served a number of purposes, including use for food and water and creating fish hooks from the spines. Another common species is the Shin Digger (Agave lechuguilla).\n\nWith its shallow roots, it is able to take in a large quantity of water and store it in its pedals for extended periods of time. The Ocotillo (Fouquieria splendens) is another plant frequently found in this area, which is a very unusually shaped plant. Because of this, it is often referred to as a \"vine cactus.\" This plant has an adaptive ability to photosynthesize during very dry conditions and gather large quantities of water when it is available. The Great Basin is also home to the oldest species in the world, the bristlecone pine (\"Pinus longaeva\"). Its needles allow it to retain water and use very little of it during its lifetime. It is able to grow on exposed rocky surfaces in higher elevations about forested areas. With these advantages come some drawbacks, including its very slow growth rate, which leaves it vulnerable to being out-competed by faster growing trees.\n\nThere are a variety of mammals that define the North American Deserts such as the bighorn sheep, mule deer, white-tailed deer, ground squirrel, coyote, prairie dog, cottontail rabbit, desert packrat, and mountain lion. There are a number of birds and reptiles that thrive in these ecosystems as well. The cactus wren, Gambel's quail, burrowing owl, red-tailed hawk, hummingbird, desert tortoise, and vulture to name a few.\n\nAn example of a keystone species in the North American deserts would be the coyote or mountain lion. These two predators can control the population and distribution of a large number of prey species. A single mountain lion can roam an area of hundreds of kilometers, in which deer, rabbits, and bird species are partly controlled by a predator of this caliber. They will change the feeding behavior or where they decide to nest or burrow is largely a reaction to the mountain lions activity. Another example, such as the hummingbird, new plants or animals could also come into the habitat and push out native species. In the Sonoran Desert, the hummingbird pollinates many native species of cactus and other plants. The hummingbirds in this region, such as the Costa's hummingbird, have evolved to have very long beaks and tongues that wrap around the skull in order to reach the nectar for that sweet sugar staple.\n\nThe Great Basin Desert is the only Cold desert, bordered by the Rocky Mountain range to the east, and the Sierra Nevada – Cascade to the west. The northernmost part of the desert lies above sea level, and due to high summer temperatures, not all of the fallen precipitation is fully absorbed into the soil, resulting in a high sodium concentration. In other areas, mountain erosion has caused deep soils of fine particles, which allows for standing lakes.\n\nThe Mojave lies between the Sonoran (south) and the Great Basin (north). Here, soil is shallow, rocky, and dry. The average elevation is between above sea level. The Mojave has several mountain range boundaries, the Garlock and the San Andres. They are made up of the two largest faults in the state of California.\n\nThe Sonoran is referred to as the Base and Range geologic province. Here, the Mogollon rim exists of sandstone and limestone piled over millions of years. The basin and valley were made from volcanic eruption 40 million years ago, and the underlying rock is made primarily of cretaceous (aged granites).\n\nThe Chihuahuan desert is made up of calcareous soils that have a high pH and calcium concentration. The soil is thin, sandy, and gravel like, and rests atop deep layers of limestone. Higher elevations allow water to sink deeper into soils that are made of finer particles, and deep sedimentary fans exist. Limestone beds show that this desert was at one point fully submerged beneath the sea. This desert features elevations ranging from above sea level, to below.\n\nThere are common patterns of hydrological cycles throughout the North American Deserts, but specifics of times and source of water range. All four deserts rely on rivers, precipitation, and underground aquifers to replenish their water supply. The water in the North American desert is mainly freshwater. There is an ephemeral flow of underground water during the wet seasons that slows during each sub-desert’s dry season. Oases form in all four deserts when the groundwater reaches the surface and pools in the hollows of the desert basins. Being surrounded by mountains provides a rain shadow effect that contributes to the dry climate and creates the desert ecosystem. All four deserts experience times of drought and times of intense precipitation. The Colorado River flows through the Mojave, Great Basin, and Sonoran desert.\n\nBut, differences in seasonal rain create the differing hydrological cycles. The Great Basin receives most its rainfall in the winter. This leads to creation of playa lakes in the spring, as the snowfall melts and flows down surrounding mountains. The Sonoran Desert has a bimodal precipitation pattern that includes winter storms and summer monsoons, which help sustain flora. The Chihuahuan Desert relies primarily on its intense summer monsoon for water. During the summer is when the area sees the accumulation of playa lakes. They may all have similar characteristics, but the difference in location and evaluation attribute to the diversity of their hydrological sources and cycles. Although the Northern American Deserts are characteristically dry, they still contain the water necessary to fuel their ecosystem and sustain the life of humans, animals, and plants alike.\n\nNorth American deserts can be categorized by climate categories of hot and cold deserts. The cold deserts include the Thompson Okanagan Plateau, Columbian Plateau, Northern and Central Basins, Colorado Plateaus, and the Snake River Plane.\nAll of these North American Deserts are included in the cold category, which indicates that they have a dry mid-latitude steppe or desert climate. These areas are affected by their interior position within the continent leading to broader temperature ranges and considerable rainfall. More specifically, these areas are affected by the rain shadow created by neighboring mountain ranges, acting as a barrier to westerly flowing air carrying moisture. All of these cold deserts experience about 100–300 mm of precipitation in a year indicating a semi-arid climate.\n\nThe warm deserts of North America include The Mojave Basin and Range, the Sonoran desert, and the Chihuahuan desert. These areas have a tropical desert climate, and are known as the hottest and driest place on the continent. This is due to the continental interior location on the leeward side of mountains, with constant subtropical high pressures. The high temperatures throughout the year are due to the high percentage of sunshine caused by high sun angles. Increased distance from a body of water leads to a lack of clouds, which is associated with much cooler nighttime temperatures because all the heat of the day is lost. The only source of water in the warm deserts is an oasis; this creates an arid climate in the area distinguishable by the lack of moisture in the soil due to annual precipitation being less than half of the annual potential evapotranspiration.\n\nThe North American Desert biome is facing a variety of ecological threats. Human disturbance poses the number one concern to this fragile ecosystem. The Sonoran desert contains the two large cities of Tucson and Phoenix, Arizona, which contain over 3 million people. These dense human populations deplete the water table of the entire desert and are sending the desert towards desertification. Also, the Chihuahuan desert is seeing the effects of agricultural expansions, invasive species, illegal poaching, and extractions of resources such as salt, lime, and sand. These activities in the desert lead to eventual desertification and a loss of overall biodiversity. A number of organizations such as the United States Nature Conservancy and the World Wildlife Fund have begun working together to conserve the threatened desert ecosystem. The less heavily populated areas of the desert are being sought out and conserved in order to prevent future human habitation and disturbance. Also, several organizations are now monitoring the use and health of the Rio Grande system located in the Chihuahuan desert, while also building new low tech water treatment facilities that will help to prevent overall water table depletion. The World Wildlife Fund is replanting disturbed, upland vegetation in order to retain species habitat and biodiversity. These measures are helping to protect and preserve the four North American Desert ecosystems.\n\nThe giant kangaroo rat is one of the most peculiar looking rodents around. The Dipodomys ingens can grow up to 34.7 centimeters in length and have a tail of up to 19.8 centimeters long. They can weigh up to 180 grams. It is mainly found in the San Joaquin Valley in California. The giant kangaroo rat forages for food from sunset to sunrise. Its diet consists mainly of seeds, that are sun dried and some greenery. They store food in their cheeks until they bring it back to their burrow systems, where they store food that could last them up to 2 years of drought. The giant kangaroo rats develop rather quickly. Depending on the environmental conditions, they can reproduce after about 5 months. Their litter size varies but averages about 3.75 offspring. These rodents are rather resilient when it comes to surviving under natural conditions, such as drought and low plant productivity. However, when the human factor is introduced, they have a much less successful survival rate. Aqueducts and other water projects started crisscrossing the giant kangaroo rat habitat. Agriculture moved in because of the new water routes and suddenly the habitat of many species became agricultural land. Kangaroo rats became a pest for farmers and rodenticide-treated grain became common practice which took out another chunk of their population.\n\nNichol's Turk's head cactus (\"Echinocactus horizonthalonius\" var. \"nicholii\") is one of multiple species of \"Echinocactus horizonthalonius\". The Nichol’s Turk’s head cactus ranges from blue-green to yellow-green. It tends to be around 46 centimeters tall and has about a 20 centimeter diameter. It has 8 ribs that are lined with spines. The cactus blooms from April to May with a purple flower and white, hairy fruit. Like many cacti, it is rather slow growing at a rate of just 2 inches in 10 years, due to minimal nutrient input. Its habitat is located mainly in the Vekol and Waterman Mountains in Arizona and it has a population in the Sierra del Viejo Mountains of northwestern Sonora. The cactus is particularly fond of Horquilla limestone outcrops. The biggest threats to these cacti are habitat loss to new development, vehicle/off-roading damage, mining, and human collection. Among other threats, erosion from foot traffic from drug and human trafficking in the area.\n\nNorth American Deserts, as in most arid systems, experience water and temperature change as the most limiting factors in this ecoregion. Climate change's major effects thus far have been an increase in average annual temperature as well as an increase in average annual rainfall.\n\nThe most prevalent factor is the increase in rainfall events and the severity of the events. Between 1931 and 2000, there have been measurable increases in seasonal rainfall during the summertime monsoon in the southern United States and northern Mexico. Because of this increase in rainfall, changes in the vegetative cover have caused native species to disappear and invasive species populations to rise. The kangaroo rat, which also supported Mojave rattlesnake and burying owl populations, has essentially disappeared from the Chihuahan Desert, while the non-native Bailey’s pocket mouse has colonized the area. Increased rainfall has also led to decrease in soil quality and less vegetative cover, which leads to increasingly higher temperatures. In the Sonoran Desert, anthropogenic land degradation as well as natural erosion from increased rainfall has caused a 4–5 degree increase in average afternoon temperatures, which means for many species less available water and nutrients they need to survive. These effects will lead to less biodiversity in the area, which is one of the main combatant factors that biota have against climate change.\n\nAs the effects of climate change continue to develop, North American Deserts will be increasingly affected, leading worsening biodiversity loss and decreases in the ecoregion productivity. Deserts are one of the most delicate ecosystems, relying on limited water and nutrient sources to survive. When these careful relationships are disturbed by the unpredictable and worsening effects of climate change, it will be very hard for these ecosystems to recover or endure.\nIn the North American Deserts there are emerging natural resources within the ecosystem. A few natural resources within the desert consist of oil, sunlight, copper, zinc, and water. Some of these resources are renewable and some are non-renewable. Most of these resources are being exploited by humans and most actions are not sustainable.\nSunlight is one of the deserts most important resource as it is renewable and has sustainable exploitations. Deserts within North America tend to have fields of solar panels, so they can reuse the sun as energy. Areas such as New Mexico, Texas, Arizona, and the Great Basin area, put up fields for green energy. We monitored how the sun provides energy for resources such as plants and animals; we decided to make solar panels to produce energy for us. Water is also a resource found in the desert that can be reused and has sustainable exploitations.\n\nOil is the most exploited resource within the deserts. In the North American desert most of the oil is found within the Great Basin region and this resource is non-renewable. Oil is mined out of rocks and creates massive holes that disrupt the ecosystem. The process with taking oil is not sustainable and this resource is scarce.\nAnother resource that is mined is copper. Along with oil, this resource is also scarce as it is non-renewable and also has the same mining affects as oil does. This resource can be used for things such as computers, TVs, cell phones, and other electronics. Copper is mainly found in California. Other mined resources consist of zinc, uranium, rocks, jade, crystals, gold, and quartz.\n\n\n\n"}
{"id": "55998971", "url": "https://en.wikipedia.org/wiki?curid=55998971", "title": "List of mammals that perform mass migrations", "text": "List of mammals that perform mass migrations\n\nMass migrations take place, or used to take place, in the following mammals:\n\nAfrica:\nNorth America:\nNorth America and Eurasia:\nEurasia:\n\nOf these migrations, those of the springbok, black wildebeest, blesbok, scimitar-horned oryx, and kulan have ceased.\n"}
{"id": "894791", "url": "https://en.wikipedia.org/wiki?curid=894791", "title": "List of national parks of Finland", "text": "List of national parks of Finland\n\nThere are 40 national parks in Finland. They are all managed by the Metsähallitus. The national parks cover a total area of – 2.7% of Finland's total land area. A total of 1.7 million people visited the parks in 2007.\n\n\nOther references on Wikipedia:\n\n"}
{"id": "10374104", "url": "https://en.wikipedia.org/wiki?curid=10374104", "title": "List of past presumed highest mountains", "text": "List of past presumed highest mountains\n\nThe following is a list of mountains that have been presumed, at one time, to be the highest mountain in the world. How general were the following presumptions is unclear. Before the age of exploration, no geographer could make any plausible assumption.\n\n\n"}
{"id": "50881499", "url": "https://en.wikipedia.org/wiki?curid=50881499", "title": "List of plants with dehiscent fruits", "text": "List of plants with dehiscent fruits\n\nThis is a list of species of plants having a specialized seed dispersal characteristic known as dehiscence in their or plant genera where dehiscent fruit is a defining characteristic.\n\nIn order for plant species articles to be included in the list they should meet the following criteria :\n\n\nIn order for plant genus articles to be included in the list they should meet the following criteria :\n\n\nsee also : List of plants with indehiscent fruits\n\n"}
{"id": "48212992", "url": "https://en.wikipedia.org/wiki?curid=48212992", "title": "List of star systems within 60–65 light-years", "text": "List of star systems within 60–65 light-years\n\nThis is a list of star systems within 60-65 light years of Earth.\n\n"}
{"id": "8591759", "url": "https://en.wikipedia.org/wiki?curid=8591759", "title": "List of stars in Monoceros", "text": "List of stars in Monoceros\n\nThis is the list of notable stars in the constellation Monoceros, sorted by decreasing brightness.\n\n\n"}
{"id": "50748031", "url": "https://en.wikipedia.org/wiki?curid=50748031", "title": "Lists of breeds", "text": "Lists of breeds\n\nThe Lists of breeds refer to listed breeds of domesticated animals.\n\n"}
{"id": "475199", "url": "https://en.wikipedia.org/wiki?curid=475199", "title": "Low-pressure area", "text": "Low-pressure area\n\nA low-pressure area, low, depression or cyclone is a region on the topographic map where the atmospheric pressure is lower than that of surrounding locations. Low-pressure systems form under areas of wind divergence that occur in the upper levels of the troposphere. The formation process of a low-pressure area is known as cyclogenesis. Within the field of meteorology, atmospheric divergence aloft occurs in two areas. The first area is on the east side of upper troughs, which form half of a Rossby wave within the Westerlies (a trough with large wavelength that extends through the troposphere). A second area of wind divergence aloft occurs ahead of embedded shortwave troughs, which are of smaller wavelength. Diverging winds aloft ahead of these troughs cause atmospheric lift within the troposphere below, which lowers surface pressures as upward motion partially counteracts the force of gravity.\n\nThermal lows form due to localized heating caused by greater sunshine over deserts and other land masses. Since localized areas of warm air are less dense than their surroundings, this warmer air rises, which lowers atmospheric pressure near that portion of the Earth's surface. Large-scale thermal lows over continents help drive monsoon circulations. Low-pressure areas can also form due to organized thunderstorm activity over warm water. When this occurs over the tropics in concert with the Intertropical Convergence Zone, it is known as a monsoon trough. Monsoon troughs reach their northerly extent in August and their southerly extent in February. When a convective low acquires a well-hot circulation in the tropics it is termed a tropical cyclone. Tropical cyclones can form during any month of the year globally, but can occur in either the northern or southern hemisphere during November.\n\nAtmospheric lift will also generally produce cloud cover through adiabatic cooling once the air becomes saturated as it rises, although the low-pressure area typically brings cloudy skies, which act to minimize diurnal temperature extremes. Since clouds reflect sunlight, incoming shortwave solar radiation decreases, which causes lower temperatures during the day. At night the absorptive effect of clouds on outgoing longwave radiation, such as heat energy from the surface, allows for warmer diurnal low temperatures in all seasons. The stronger the area of low pressure, the stronger the winds experienced in its vicinity. Globally, low-pressure systems are most frequently located over the Tibetan Plateau and in the lee of the Rocky mountains. In Europe (particularly in the British Isles and Netherlands), recurring low-pressure weather systems are typically known as \"depressions\".\n\nCyclogenesis is the development and strengthening of cyclonic circulations, or low-pressure areas, within the atmosphere. Cyclogenesis is the opposite of , and has an anticyclonic (high-pressure system) equivalent which deals with the formation of high-pressure areas—anticyclogenesis. Cyclogenesis is an umbrella term for several different processes, all of which result in the development of some sort of cyclone. Meteorologists use the term \"cyclone\" where circular pressure systems flow in the direction of the Earth's rotation, which normally coincides with areas of low pressure. The largest low-pressure systems are cold-core polar cyclones and extratropical cyclones which lie on the synoptic scale. Warm-core cyclones such as tropical cyclones, mesocyclones, and polar lows lie within the smaller mesoscale. Subtropical cyclones are of intermediate size. Cyclogenesis can occur at various scales, from the microscale to the synoptic scale. Larger-scale troughs, also called Rossby waves, are synoptic in scale. Shortwave troughs embedded within the flow around larger scale troughs are smaller in scale, or mesoscale in nature. Both Rossby waves and shortwaves embedded within the flow around Rossby waves migrate equatorward of the polar cyclones located in both the Northern and Southern hemispheres. All share one important aspect, that of upward vertical motion within the troposphere. Such upward motions decrease the mass of local atmospheric columns of air, which lowers surface pressure.\n\nExtratropical cyclones form as waves along weather fronts due to a passing by shortwave aloft or upper level jet streak before occluding later in their life cycle as cold-core cyclones. Polar lows are small-scale, short-lived atmospheric low-pressure systems that occur over the ocean areas poleward of the main polar front in both the Northern and Southern Hemispheres. They are part of the larger class of mesoscale weather-systems. Polar lows can be difficult to detect using conventional weather reports and are a hazard to high-latitude operations, such as shipping and gas- and oil-platforms. They are vigorous systems that have near-surface winds of at least .\nTropical cyclones form due to latent heat driven by significant thunderstorm activity, and are warm-core with well-defined circulations. Certain criteria need to be met for their formation. In most situations, water temperatures of at least are needed down to a depth of at least ; waters of this temperature cause the overlying atmosphere to be unstable enough to sustain convection and thunderstorms. Another factor is rapid cooling with height, which allows the release of the heat of condensation that powers a tropical cyclone. High humidity is needed, especially in the lower-to-mid troposphere; when there is a great deal of moisture in the atmosphere, conditions are more favorable for disturbances to develop. Low amounts of wind shear are needed, as high shear is disruptive to the storm's circulation. Lastly, a formative tropical cyclone needs a pre-existing system of disturbed weather, although without a circulation no cyclonic development will take place. Mesocyclones form as warm core cyclones over land, and can lead to tornado formation. Waterspouts can also form from mesocyclones, but more often develop from environments of high instability and low vertical wind shear.\n\nIn deserts, lack of ground and plant moisture that would normally provide evaporative cooling can lead to intense, rapid solar heating of the lower layers of air. The hot air is less dense than surrounding cooler air. This, combined with the rising of the hot air, results in a low-pressure area called a thermal low. Monsoon circulations are caused by thermal lows which form over large areas of land and their strength is driven by how land heats more quickly than the surrounding nearby ocean. This generates a steady wind blowing toward the land, bringing the moist near-surface air over the oceans with it. Similar rainfall is caused by the moist ocean-air being lifted upwards by mountains, surface heating, convergence at the surface, divergence aloft, or from storm-produced outflows at the surface. However the lifting occurs, the air cools due to expansion in lower pressure, which in turn produces condensation. In winter, the land cools off quickly, but the ocean keeps the heat longer due to its higher specific heat. The hot air over the ocean rises, creating a low-pressure area and a breeze from land to ocean while a large area of drying high pressure is formed over the land, increased by wintertime cooling. Monsoons resemble sea and land breezes, terms usually referring to the localized, diurnal (daily) cycle of circulation near coastlines everywhere, but they are much larger in scale - also stronger and seasonal.\n\nLarge polar cyclones help determine the steering of systems moving through the mid-latitudes, south of the Arctic and north of the Antarctic. The Arctic oscillation provides an index used to gauge the magnitude of this effect in the Northern Hemisphere. Extratropical cyclones tend to form east of climatological trough positions aloft near the east coast of continents, or west side of oceans. A study of extratropical cyclones in the Southern Hemisphere shows that between the 30th and 70th parallels there are an average of 37 cyclones in existence during any 6-hour period. A separate study in the Northern Hemisphere suggests that approximately 234 significant extratropical cyclones form each winter. In Europe, particularly in the United Kingdom and in the Netherlands, recurring extratropical low-pressure weather systems are typically known as depressions.\nThese tend to bring wet weather throughout the year. \"Thermal lows\" also occur during the summer over continental areas across the subtropics - such as the Sonoran Desert, the Mexican plateau, the Sahara, South America, and Southeast Asia. The lows are most commonly located over the Tibetan plateau and in the lee of the Rocky mountains.\n\nElongated areas of low pressure form at the monsoon trough or intertropical convergence zone as part of the Hadley cell circulation. Monsoon troughing in the western Pacific reaches its zenith in latitude during the late summer when the wintertime surface ridge in the opposite hemisphere is the strongest. It can reach as far as the 40th parallel in East Asia during August and 20th parallel in Australia during February. Its poleward progression is accelerated by the onset of the summer monsoon which is characterized by the development of lower air pressure over the warmest part of the various continents. The large-scale thermal lows over continents help create pressure gradients which drive monsoon circulations. In the southern hemisphere, the monsoon trough associated with the Australian monsoon reaches its most southerly latitude in February, oriented along a west-northwest/east-southeast axis. Many of the world's rainforests are associated with these climatological low-pressure systems.\n\nTropical cyclones generally need to form more than or poleward of the 5th parallel north and 5th parallel south, allowing the Coriolis effect to deflect winds blowing towards the low-pressure center and creating a circulation. Worldwide, tropical cyclone activity peaks in late summer, when the difference between temperatures aloft and sea surface temperatures is the greatest. However, each particular basin has its own seasonal patterns. On a worldwide scale, May is the least active month while September is the most active month. November is the only month that activity in all the tropical cyclone basins is possible. Nearly one-third of the world's tropical cyclones form within the western Pacific Ocean, making it the most active tropical cyclone basin on Earth.\n\nWind is initially accelerated from areas of high pressure to areas of low pressure. This is due to density (or temperature and moisture) differences between two air masses. Since stronger high-pressure systems contain cooler or drier air, the air mass is denser and flows towards areas that are warm or moist, which are in the vicinity of low-pressure areas in advance of their associated cold fronts. The stronger the pressure difference, or pressure gradient, between a high-pressure system and a low-pressure system, the stronger the wind. Thus, stronger areas of low pressure are associated with stronger winds.\n\nThe Coriolis force caused by the Earth's rotation is what gives winds around low-pressure areas (such as in hurricanes, cyclones, and typhoons) their counter-clockwise (anticlockwise) circulation in the northern hemisphere (as the wind moves inward and is deflected right from the center of high pressure) and clockwise circulation in the southern hemisphere (as the wind moves inward and is deflected left from the center of high pressure). A cyclone differs from a hurricane or typhoon only on the basis of location. A hurricane is a storm that occurs in the Atlantic Ocean and northeastern Pacific Ocean, a typhoon occurs in the northwestern Pacific Ocean, and a cyclone occurs in the south Pacific or Indian Ocean. Friction with land slows down the wind flowing into low-pressure systems and causes wind to flow more inward, or flowing more ageostrophically, toward their centers. A low-pressure area is commonly associated with inclement weather, while a high-pressure area is associated with light winds and fair skies. Tornados are often too small, and of too short duration, to be influenced by the Coriolis force, but may be so-influenced when arising from a low-pressure system.\n\n"}
{"id": "1221071", "url": "https://en.wikipedia.org/wiki?curid=1221071", "title": "Manu (Hinduism)", "text": "Manu (Hinduism)\n\nManu () is a term found with various meanings in Hinduism. In early texts, it refers to the archetypal man, or to the first man (progenitor of humanity).The Sanskrit term for 'human', मानव (IAST: mānava) means 'of Manu' or 'children of Manu'. In later texts, Manu is the title or name of fourteen mystical Kshatriya rulers of earth, or alternatively as the head of mythical dynasties that begin with each cyclic \"kalpa\" (aeon) when the universe is born anew. The title of the text \"Manusmriti\" uses this term as a prefix, but refers to the first Manu – Svayambhuva, the spiritual son of Brahma.\n\nIn some Puranic mythology, each \"kalpa\" consists of fourteen Manvantaras, and each Manvantara is headed by a different Manu. The current universe, in this mythology, is asserted to be ruled by the 7th Manu named Vaivasvata.\n\nIn Vishnu Purana, Vaivasvata, also known as Sraddhadeva or Satyavrata, was the king of Dravida before the great flood. He was warned of the flood by the Matsya (fish) avatar of Vishnu, and built a boat that carried the Vedas, Manu's family and the seven sages to safety, helped by Matsya. The myth is repeated with variations in other texts, including the Mahabharata and a few other Puranas. It is similar to other flood myths such as that of Gilgamesh and Noah.\n\nThe 14 Manus of the current aeon are:\n\nIn this Manvantara, the Saptarshis were Marichi, Atri, Angiras, Pulaha, Kratu, Pulastya, and Vashishtha.\nIn Svayambhuva-manvantara, Lord Vishnu's avatar was called Yajna.\n\nThe first Manu was Svayambhuva Manu. He had three daughters, namely Akuti, Devahuti and Prasuti. Devahuti was given in marriage to sage Kardama and she gave birth to nine daughters, and a single son named Kapila. Prasuti gave birth to Yajna and Akuti gave birth to one son and one daughter. Both Kapila and Yajna, who were sons of Devahuti and Prasuti respectively, were incarnations of Vishnu. Svayambhuva Manu, along with his wife, Satarupa, went into the forest to practice austerities on the bank of the River Sunanda. At some point in time, Rakshasas attacked them, but Yajna, accompanied by his sons, the demigods, swiftly killed them. Then Yajna personally took the post of Indra, the King of the heavenly planets.\n\nThe Saptarshis were Urjastambha, Agni, Prana, Danti, Rishabha, Nischara, and Charvarivan.\nIn Svarocisha-manvantara, Lord Vishnu's avatar was called Vibhu.\n\nThe second Manu, whose name was Svarocisha, was the son of Agni, and His sons were headed by Dyumat, Sushena and Rochishmat. In the age of this Manu, Rochana became Indra, the ruler of the heavenly planets, and there were many demigods, headed by Tushita. There were also many saintly persons, such as Urjastambha. Among them was Vedasira, whose wife, Tushita, gave birth to Vibhu. Vibhu was the incarnation of Vishnu for this Manvantara. He remained a Brahmachari all his life and never married. He instructed eighty-eight thousand dridha-vratas, or saintly persons, on sense-control and austerity.\n\nThe Saptarshis for this Manvantara were Kaukundihi, Kurundi, Dalaya, Sankha, Pravahita, Mita, and Sammita.\nIn Uttama-manvantara, Lord Vishnu's avatar was called Satyasena.\n\nUttama, the son of Priyavrata, was the third Manu. Among his sons were Pavana, Srinjaya and Yajnahotra. During the reign of this Manu, the sons of Vashista, headed by Pramada, became the seven saintly persons. The Satyas, Devasrutas and Bhadras became the demigods, and Satyajit became Indra. From the womb of Sunrita, the wife of Dharma, the Supreme Lord Narayana appeared as Satyasena, and killed all the evil Rakshasas who created havoc in all the worlds, along with Satyajit, who was Indra at that time.\n\nSaptarshis list: Jyotirdhama, Prithu, Kavya, Chaitra, Agni, Vanaka, and Pivara.\nIn Tapasa-manvantara, Lord Vishnu's avatar was called Hari.\n\nTapasa/Tamasa, the brother of the third Manu, was the fourth Manu, and he had ten sons, including Prithu, Khyati, Nara and Ketu. During his reign, the Satyakas, Haris, Viras and others were demigods, the seven great saints were headed by Jyotirdhama, and Trisikha became Indra. Harimedha begot a son named Hari, who was the incarnation of Vishnu for this Manvantara, by his wife Harini. Hari was born to liberate the devotee Gajendra.\n\nSaptarshis list: Hirannyaroma, Vedasrí, Urddhabahu, Vedabahu, Sudhaman, Parjanya, and Mahámuni.\nIn Raivata-manvantara, Lord Vishnu's avatar was called Vaikuntha, not to be confused with Vishnu’s divine realm, of the same name.\n\nVaikuntha came as Raivata Manu, the twin brother of Tamasa. His sons were headed by Arjuna, Bali and Vindhya. Among the demigods were the Bhutarayas, and among the seven brahmanas who occupied the seven planets were Hiranyaroma, Vedasira and Urdhvabahu.\n\nSaptarshis list: Sumedhas, Virajas, Havishmat, Uttama, Madhu, Abhináman, and Sahishnnu.\nIn Chakshusha-manvantara, Lord Vishnu's avatar was called Ajita.\n\nAjita came as Chakshsusa Manu, the son of the demigod Chakshu. He had many sons, headed by Puru, Purusa and Sudyumna. During the reign of Chakshusa Manu, the King of heaven was known as Mantradruma. Among the demigods were the Apyas, and among the great sages were Havisman and Viraka.\n\nSaptarshis list: Kashyapa, Atri, Vashista, Angira, Gautama, Agastya, Bharadvaja. During Vaivasvata-manvantara, Lord Vishnu's avatar is called Vamana\n\nThe seventh Manu, who is the son of Vivasvan, is known as Sraddhadeva(or satyavrata ) or Vaivasvat(son of Vivasvan). He has ten sons, named Ikshvaku, Nabhaga, Dhrsta, Saryati, Narisyanta, Dista, Tarusa, Prsadhra and Vasuman. In this manvantara, or reign of Manu, among the demigods are the Adityas, Vasus, Rudras, Visvedevas, Maruts, Asvini-kumaras and Rbhus. The king of heaven, Indra, is known as Purandara, and the seven sages are known as Kashyapa, Atri, Vashista, Angira, Gautama, Agastya and Bharadwaja. During this period of Manu, Lord Vishnu took birth from the womb of Aditi, the wife of Kashyapa.\n\nSaptarshis list: Diptimat, Galava, Parasurama, Kripa, Drauni or Ashwatthama, Vyasa, and Rishyasringa. In Savarnya-manvantara, Lord Vishnu's avatar will be called Sarvabhauma.\n\nIn the period of the eighth Manu, the Manu is Surya Savarnika Manu. His sons are headed by Nirmoka, and among the demigods are the Sutapas. Bali, the son of Virochana, is Indra, and Galava and Parasurama are among the seven sages. In the age of this Manu, Lord Vishnu's avatar will be called Sarvabhauma, the son of Devaguhya and Sarasvati.\n\nSaptarshis list: Savana, Dyutimat, Bhavya, Vasu, Medhatithi, Jyotishmán, and Satya.\nIn Daksha-savarnya-manvantara, Lord Vishnu's avatar will be called Rishabha.\n\nThe ninth Manu is Daksha-savarni. His sons are headed by Bhutaketu, and among the demigods are the Maricigarbhas. Adbhuta is Indra, and among the seven sages is Dyutiman. Rishabha would be born of Ayushman and Ambudhara.\n\nSaptarshis list: Havishmán, Sukriti, Satya, Apámmúrtti, Nábhága, Apratimaujas, and Satyaket.\nIn Brahma-savarnya-manvantara, Lord Vishnu's avatar will be called Vishvaksena.\n\nIn the period of the tenth Manu, the Manu is Brahma-savarni. Among his sons is Bhurishena, and the seven sages are Havishman and others. Among the demigods are the Suvasanas, and Sambhu is Indra. Vishvaksena would be a friend of Sambhu and will be born from the womb of Vishuci in the house of a brahmana named Visvasrashta.\n\nSaptarshis list: Niśchara, Agnitejas, Vapushmán, Vishńu, Áruni, Havishmán, and Anagha.\nIn Dharma-savarnya-manvantara, Lord Vishnu's avatar will be called Dharmasetu.\n\nIn the period of the eleventh Manu, the Manu is Dharma-savarni, who has ten sons, headed by Satyadharma. Among the demigods are the Vihangamas, Indra is known as Vaidhrita, and the seven sages are Aruna and others. Dharmasetu will be born of Vaidhrita and Aryaka.\n\nSaptarshis list: Tapaswí, Sutapas, Tapomúrtti, Taporati, Tapodhriti, Tapodyuti, and Tapodhan.\nIn Rudra-savarnya-manvantara, Lord Vishnu's avatar will be called Sudhama.\n\nIn the period of the twelfth Manu, the Manu is Rudra-savarni, whose sons are headed by Devavan. The demigods are the Haritas and others, Indra is Ritadhama, and the seven sages are Tapomurti and others. Sudhama, or Svadhama, who will be born from the womb of Sunrita, wife of a Satyasaha.\n\nSaptarshis list: Nirmoha, Tatwadersín, Nishprakampa, Nirutsuka, Dhritimat, Avyaya, and Sutapas.\nIn Deva-savarnya-manvantara, Lord Vishnu's avatar will be called Yogeshwara.\n\nIn the period of the thirteenth Manu, the Manu is Deva-savarni. Among his sons is Chitrasena, the demigods are the Sukarmas and others, Indra is Divaspati, and Nirmoka is among the sages. Yogeshwara will be born of Devahotra and Brihati.\n\nSaptarshis list: Agnibáhu, Śuchi, Śhukra, Magadhá, Gridhra, Yukta, and Ajita.\nIn Indra-savarnya-manvantara, Lord Vishnu's avatar will be called Brihadbhanu.\n\nIn the period of the fourteenth Manu, the Manu is Indra-savarni. Among his sons are Uru and Gambhira, the demigods are the Pavitras and others, Indra is Suci, and among the sages are Agni and Bahu. Brihadbhanu will be born of Satrayana from the womb of Vitana.\n\nAlmost all literature refers to the first 9 Manus with the same names but there is a lot of disagreement on names after that, although all of them agree with a total of 14.\n\nThe texts ascribed to the Svayambhuva Manu include \"Manava Grihyasutra\", \"Manava Sulbasutra\" and \"Manava Dharmashastra\" (\"Manusmṛti\" or \"rules of Manu\").\n\nJain mythology mentions the 14th patriarch named Nabhiraja, mentioning him also as Manu. This, state scholars, links ancient Jain tradition to Hindu mythologies, because the 14 patriarchs in Jain myths are similar to the 14 Manus in Hindu myths. The Manu of Jainism is the father of 1st \"Tirthankara\" Rishabhanatha (Adinatha). This ancient story is significant as it includes one of earliest mentions of \"ikshu\" (sugarcane) processing.\n\nIn the Victor Hugo novel, \"The Hunchback of Notre Dame\", Claude Frollo is seen to be studying Manu's works in his study of alchemy.\n\n\n"}
{"id": "69726", "url": "https://en.wikipedia.org/wiki?curid=69726", "title": "Moriah", "text": "Moriah\n\nMoriah (, ) is the name given to a mountainous region by the Book of Genesis, in which context it is the location of the sacrifice of Isaac. The Vulgate renders the location specified by God for the sacrifice as \"terram Visionis\", traditionally rendered \"land of Vision\" in Catholic translations. Through association with the biblical Mount Moriah (the Temple Mount), Mount Moriah has traditionally been interpreted as the name of the specific mountain at which this occurred, although this identification is typically rejected by scholarship.\n\nMuslims believe the historical mount is \"Marwah\" in Arabic, as mentioned in the Qur'an, located close to the Kaaba in Mecca, Saudi Arabia. There has been an historical account of rams' horns preserved in the Kaaba until the year 683, which are believed to be the remains of the sacrifice of Ishmael.\n\nIn the Hebrew Bible, the name \"Moriah\" () occurs twice. Tradition has interpreted these as the same place:\n\n\nIn the book of Chronicles it is reported that the location of Araunah's threshing floor is on \"Mount Moriah\" and that the Temple of Solomon was built over Araunah's threshing floor. This has led to the classical rabbinical supposition that this is at the peak of Moriah.\n\nThere is debate as to whether the two references (Genesis 22:2 and Chronicles 3:1) are correctly translated as the same word. For example, in the LXX, these verses are translated as:\n\n\nSome interpretations of a biblical passage concerning Melchizedek, king of Salem, would indicate Jerusalem was already a city with a priest at the time of Abraham, and thus is unlikely to have been founded after this, at the site of a sacrifice made by Abraham in the wilderness. However the view that Salem refers to Jerusalem (in David's time, Jebus) and not peace (shalome, shelomo) is of heavy debate between many sects of Jews and Christians.\n\nIn consequence of these traditions, Classical Rabbinical Literature theorised that the name was a (linguistically corrupted) reference to the Temple, suggesting translations like \"the teaching-place\" (referring to the Sanhedrin that met there), \"the place of fear\" (referring to the supposed fear that non-Israelites would have at the Temple), \"the place of myrrh\" (referring to the spices burnt as incense). Targum Pseudo-Jonathan interprets the name as \"land of worship\", while the Samaritan Targum regards it as being \"land of vision\".\n\nMost modern biblical scholars, however, regard the name as a reference to the Amorites, having lost the initial \"a\" via aphesis; the name is thus interpreted as meaning \"land of the Amorites\". This agrees with the Septuagint, where, for example, 2 Chronicles 3:1 refers to the location as – \"Amōriā\". This would give it the same etymological root as \"Hamor\", a person's name in the narrative at Genesis 34 which concerns Shechem. Some scholars also identify it with \"Moreh\", the location near Shechem at which Abraham built an altar, according to Genesis 12:6. Hence a number of scholars believe that \"Moriah\" refers to a hill near Shechem, supporting the Samaritan belief that the near-sacrifice of Isaac occurred on Mount Gerizim – a location near Shechem.\n\nSome scholars reference the conversation Jesus had with the Samaritan woman at the well, where He stated that the Samaritans were inaccurate in their knowledge of the worship of God (John 4:21–24). Acknowledging the intended similarity between the sacrifice of Isaac and the crucifixion of Jesus, they make the connection that Moriah would be the same location where Jews made sacrifices at the Temple of Solomon. Isaac carrying the wood for the sacrificial fire as Jesus carried the cross; the reference in Hebrews 11:17–19 to Abraham believing God could raise Isaac from the dead; Isaac being Abraham's \"only\" son and Jesus being God's only begotten Son; all make the correlation between the two events point to Moriah being the Temple site.\n\nFrom a Muslim point of view, the well-known site of Mount Marwah (Arabic مروة), the hill just outside the perimeter of the Kaaba in Mecca, may be identified with the biblical Moriah (Hebrew מוריה).\n\n\n"}
{"id": "33971340", "url": "https://en.wikipedia.org/wiki?curid=33971340", "title": "Mountains of Bhutan", "text": "Mountains of Bhutan\n\nThe mountains of Bhutan are some of the most prominent natural geographic features of the kingdom. Located on the southern end of the Eastern Himalaya, Bhutan has one of the most rugged mountain terrains in the world, whose elevations range from to more than above sea level, in some cases within distances of less than of each other. Bhutan's highest peak, at above sea level, is north-central Gangkhar Puensum, close to the border with China; the third highest peak, Jomolhari, overlooking the Chumbi Valley in the west, is above sea level; nineteen other peaks exceed . Weather is extreme in the mountains: the high peaks have perpetual snow, and the lesser mountains and hewn gorges have high winds all year round, making them barren brown wind tunnels in summer, and frozen wastelands in winter. The blizzards generated in the north each winter often drift southward into the central highlands.\n\nThe mountains of Bhutan define its three main geographic zones: the Great Himalaya, the Lower Himalayan Range (or Inner Himalaya), and the Sub-Himalayan Range. The snowcapped Great Himalaya in the north ranges from about to peaks of over above sea level, extending along the Bhutan-China border. The northern region consists of an arc of glaciated mountain peaks with an arctic climate at the highest elevations. Watered by snow-fed rivers, alpine valleys in this region provide pasturage for livestock tended by a sparse population of migratory shepherds. Spur-like mountain ranges of the Lower Himalaya, between and , run northwest to southeast in western Bhutan, and northeast to southwest in eastern Bhutan. These mountains, and especially their western valleys, make up the economic and cultural heart of the kingdom, including most of its dzongs. These mountainous areas are contrasted with the hilly Sub-Himalaya, with elevations of up to , and the lower Duars. Many lower mountain ranges are composed of coarse granite sandstone, while rocks at the highest elevations consist of gneiss among upheaved strata of mica and talcose slate. Many ranges are abundant in limestone.\n\nBhutan's valleys are carved into the Himalaya by its rivers, fed by glacial melt and monsoon rains. Much of the Bhutanese population is concentrated in valleys and lowlands, separated by the rugged southward spurs of the Inner Himalaya. Despite modernization and development of transport in Bhutan, including a national highway system, travel from one valley to the next remains difficult. Western valleys are bound to the east by the Black Mountains in central Bhutan, which form a watershed between two major river systems, the Mo Chhu (Sankosh River) and the Drangme Chhu. Central valleys are separated from the east by the Donga Range. The more isolated mountain valleys protect several tiny, distinct cultural and linguistic groups.\n\nBhutan controls several strategic Himalayan mountain passes including routes between Tibet and Assam. These routes, being the only way into the kingdom, along with centuries-old policies of isolationism, have gained Bhutan the nickname \"Mountain Fortress of the Gods.\" Although the British established a protectorate over Bhutan and occupied its lowlands, the mountainous interior has never been successfully invaded.\n\nThe mountains of Great Himalaya dominate the north of Bhutan, where peaks can easily reach . The tallest peaks range, from west to east, along northern Haa, Paro and Thimphu Districts; the bulk of Gasa District; northernmost Wangdue Phodrang District; and northern Bumthang and Lhuentse Districts. The highest point in Bhutan is Gangkhar Puensum, which has the distinction of being the highest unclimbed mountain in the world, at . Some massive summits such as Gangkhar Puensum, Kula Kangri, and Tongshanjiabu lie in territory disputed among Bhutan and China. According to Bhutanese claims, these giants should be part of Gasa District. Other peaks of the Great Himalaya, such as Mount Jitchu Drake, lie squarely within Gasa.\n\nThe Great Himalaya contains most of the glaciers of Bhutan. This region contains the vast majority of Bhutan's 677 glaciers and 2,674 glacial lakes and subsidiary lakes, out of which 25 pose a risk of GLOFs. The vast number of glaciers in Bhutan are classed as \"valley\" and \"mountain glaciers,\" although significant numbers of \"ice apron,\" and \"niche glacier\" types also exist. Some glacial lakes, such as Thorthormi Lake in Lunana Gewog, are not single bodies of water but collections supraglacial ponds.\n\nThe Lower Himalayan Ranges, also called the Inner Himalaya, are southward spurs of the Great Himalaya, dominating the midsection of Bhutan. The Dongkya Range forms the trijunction of the Bhutan-Sikkim-Tibet border, separating Sikkim from the Chumbi Valley The Black Mountains in central Bhutan form a watershed between two major river systems, the Mo Chhu and the Drangme Chhu. Peaks in the Black Mountains range between and above sea level. Eastern Bhutan is divided by another southward spur, the Donga Range, whose valleys tend to be steeper ravines.\n\nUnlike the Great Himalaya, there are no glaciers in the Inner Himalayan ranges, though some summits and upper slopes are covered with moraines.\n\nThe Dongkya Range, also called Chola, occupies the three-way border with Bhutan, Sikkim, and Tibet, and reaches southward into West Bengal as the Singalila Ridge. Gipmochi is located on the trijunction point of Bhutan, Sikkim and Tibet. One of the range's spurs extends southward from Jomolhari on the northwest border, forming the watersheds between the Chumbi Valley Teesta, Torsa (Amo), and Raidāk (Wong) systems in all three countries.\n\nTo the west lies Thimphu Valley, bound by another Inner Himalaya spur. In the midst of this range, Thimphu Valley is accessible to Punakha via Dochu La pass along the national highway at . This second range forms a watershed between the Raidāk (Wong) and Sankosh (Mo Chhu) Rivers.\n\nThe Black Mountains, lie to the east of the Sankosh River. Midway between Punakha and Trongsa, they separate western Bhutan from the ethnolinguistically diverse central regions and the densely populated eastern regions. The Black Mountains themselves spur ramifications to the southwest and southeast, reaching into Trongsa District. Pele La pass at is historically and modernly the most important pass in the Black Mountains.\n\nThe isolation of populations in the Black Mountain area has produced great linguistic and ethnic diversity: it is the home of the Lakha, Nyenkha, and 'Olekha languages, representing distantly related Tibetan and East Bodish language groups.\n\nBetween Trongsa and Jakar runs another mountain range, crossed by Yuto La pass (also called Yotong La). Continuing east, there is another ridge between Jakar and the Kuri Chhu valley, crossed by Ura La pass in Ura Gewog at . \n\nThe steep Donga Range separates Bumthang and Lhuentse Districts and forms the watershed between the Raidāk and Manas River systems. Northeastern portions of the Donga are known as Kurtoe (modern Kurtoe Gewog, historical Kurtoed Province). Thrumshing La pass, also called Donga Pass, provides the only road access across the Donga Range at . The steep Rodang La further north provides non-motor communication, and several southerly passes including Thebong La are used by herders. At , a major peak stands over Thrumshing La.\n\nTo the east runs another lesser spur of mountains separating Lhuentse and Trashiyangse Valleys.\n\nTo the east of the Manas River system, the Tawang Range (also called Kollong) forms the eastern boundary of Bhutan. The Tawang Range originates in Tibet, to the northeast of Arunachal Pradesh.\n\n"}
{"id": "7195477", "url": "https://en.wikipedia.org/wiki?curid=7195477", "title": "Nylon-eating bacteria", "text": "Nylon-eating bacteria\n\nNylon-eating bacteria are a strain of \"Flavobacterium\" that are capable of digesting certain by-products of nylon 6 manufacture. This strain of \"Flavobacterium\" sp. K172, became popularly known as nylon-eating bacteria, and the enzymes used to digest the man-made molecules became popularly known as nylonase.\n\nIn 1975, a team of Japanese scientists discovered a strain of \"Flavobacterium\", living in ponds containing waste water from a nylon factory, that was capable of digesting certain byproducts of nylon 6 manufacture, such as the linear dimer of 6-aminohexanoate. These substances are not known to have existed before the invention of nylon in 1935.\n\nFurther study revealed that the three enzymes the bacteria were using to digest the byproducts were significantly different from any other enzymes produced by other \"Flavobacterium\" strains (or, for that matter, any other bacteria), and not effective on any material other than the manmade nylon byproducts.\n\nThis discovery led geneticist Susumu Ohno in a paper published in April 1984 to speculate that the gene for one of the enzymes, 6-aminohexanoic acid hydrolase, had come about from the combination of a gene duplication event with a frameshift mutation. Ohno suggested that many unique new genes have evolved this way.\n\nA 2007 paper that described a series of studies by a team led by Seiji Negoro of the University of Hyogo, Japan, suggested that in fact no frameshift mutation was involved in the evolution of the 6-aminohexanoic acid hydrolase. However, many other genes have been discovered which did evolve by gene duplication followed by a frameshift mutation affecting at least part of the gene.\n\nA 1995 paper showed that scientists have also been able to induce another species of bacterium, \"Pseudomonas aeruginosa\", to evolve the capability to break down the same nylon byproducts in a laboratory by forcing them to live in an environment with no other source of nutrients. The \"P. aeruginosa\" strain did not seem to use the same enzymes that had been utilized by the original \"Flavobacterium\" strain.\n\nAs described in a 1983 publication, other scientists were able to get the ability to generate the enzymes to transfer from the \"Flavobacterium\" strain to a strain of \"E. coli\" bacteria via a plasmid transfer.\n\nThere is scientific consensus that the capacity to synthesize nylonase most probably developed as a single-step mutation that survived because it improved the fitness of the bacteria possessing the mutation. More importantly, the enzyme involved was produced by a mutation completely randomizing the original gene. Despite this, the new gene still had a novel, albeit weak, catalytic capacity. This is seen as a good example of how mutations easily can provide the raw material for evolution by natural selection.\n\n\n"}
{"id": "54512571", "url": "https://en.wikipedia.org/wiki?curid=54512571", "title": "Paleointensity", "text": "Paleointensity\n\nIn geomagnetism, paleointensity (or palaeointensity) is the study of changes in the strength of the geomagnetic field over Earth's history. Émile and Odette Thellier were the first to make laboratory measurements to determine the strength of the ancient field responsible for producing remanent magnetization in a rock or archeological artifacts.\n\nAbsolute paleointensity determinations involve measurements that attempt to quantity the past field strength that produced a magnetization in a rock or other material that has cooled from a high temperature. Most of these methods involve progressively removing the natural remanent magnetization (NRM) by thermal demagnetization and replacing it with a laboratory thermoremanent magnetization given in a magnetic field of known strength and direction. Various methods for measuring absolute paleointensity include:\n\nRelative paleointensity determinations are often used in materials that may be destroyed or strongly altered during heating, such as lake and marine sediments. A measure of relative changes in paleointensity may be obtained by normalizing the NRM by a factor that represents the concentration of magnetic grains in a sample, such as the anhysteretic remanent magnetization. \n"}
{"id": "1174047", "url": "https://en.wikipedia.org/wiki?curid=1174047", "title": "Rock magnetism", "text": "Rock magnetism\n\nRock magnetism is the study of the magnetic properties of rocks, sediments and soils. The field arose out of the need in paleomagnetism to understand how rocks record the Earth's magnetic field. This remanence is carried by minerals, particularly certain strongly magnetic minerals like magnetite (the main source of magnetism in lodestone). An understanding of remanence helps paleomagnetists to develop methods for measuring the ancient magnetic field and correct for effects like sediment compaction and metamorphism. Rock magnetic methods are used to get a more detailed picture of the source of distinctive striped pattern in marine magnetic anomalies that provides important information on plate tectonics. They are also used to interpret terrestrial magnetic anomalies in magnetic surveys as well as the strong crustal magnetism on Mars.\n\nStrongly magnetic minerals have properties that depend on the size, shape, defect structure and concentration of the minerals in a rock. Rock magnetism provides non-destructive methods for analyzing these minerals such as magnetic hysteresis measurements, temperature-dependent remanence measurements, Mössbauer spectroscopy, ferromagnetic resonance and so on. With such methods, rock magnetists can measure the effects of past climate change and human impacts on the mineralogy (see environmental magnetism). In sediments, a lot of the magnetic remanence is carried by minerals that were created by magnetotactic bacteria, so rock magnetists have made significant contributions to biomagnetism.\n\nUntil the 20th century, the study of the Earth's field (geomagnetism and paleomagnetism) and of magnetic materials (especially ferromagnetism) developed separately.\n\nRock magnetism had its start when scientists brought these two fields together in the laboratory. Koenigsberger (1938), Thellier (1938) and Nagata (1943) investigated the origin of remanence in igneous rocks. By heating rocks and archeological materials to high temperatures in a magnetic field, they gave the materials a thermoremanent magnetization (TRM), and they investigated the properties of this magnetization. Thellier developed a series of conditions (the Thellier laws) that, if fulfilled, would allow the determination of the intensity of the ancient magnetic field to be determined using the Thellier-Thellier method. In 1949, Louis Néel developed a theory that explained these observations, showed that the Thellier laws were satisfied by certain kinds of single-domain magnets, and introduced the concept of blocking of TRM.\n\nWhen paleomagnetic work in the 1950s lent support to the theory of continental drift, skeptics were quick to question whether rocks could carry a stable remanence for geological ages.\nRock magnetists were able to show that rocks could have more than one component of remanence, some soft (easily removed) and some very stable. To get at the stable part, they took to \"cleaning\" samples by heating them or exposing them to an alternating field. However, later events, particularly the recognition that many North American rocks had been pervasively remagnetized in the Paleozoic, showed that a single cleaning step was inadequate, and paleomagnetists began to routinely use stepwise demagnetization to strip away the remanence in small bits.\n\nThe contribution of a mineral to the total magnetism of a rock depends strongly on the type of magnetic order or disorder. Magnetically disordered minerals (diamagnets and paramagnets) contribute a weak magnetism and have no remanence. The more important minerals for rock magnetism are the minerals that can be magnetically ordered, at least at some temperatures. These are the ferromagnets, ferrimagnets and certain kinds of antiferromagnets. These minerals have a much stronger response to the field and can have a remanence.\n\nDiamagnetism is a magnetic response shared by all substances. In response to an applied magnetic field, electrons precess (see Larmor precession), and by Lenz's law they act to shield the interior of a body from the magnetic field. Thus, the moment produced is in the opposite direction to the field and the susceptibility is negative. This effect is weak but independent of temperature. A substance whose only magnetic response is diamagnetism is called a diamagnet.\n\nParamagnetism is a weak positive response to a magnetic field due to rotation of electron spins. Paramagnetism occurs in certain kinds of iron-bearing minerals because the iron contains an unpaired electron in one of their shells (see Hund's rules). Some are paramagnetic down to absolute zero and their susceptibility is inversely proportional to the temperature (see Curie's law); others are magnetically ordered below a critical temperature and the susceptibility increases as it approaches that temperature (see Curie-Weiss law).\n\nCollectively, strongly magnetic materials are often referred to as ferromagnets. However, this magnetism can arise as the result of more than one kind of magnetic order. In the strict sense, ferromagnetism refers to magnetic ordering where neighboring electron spins are aligned by the exchange interaction. The classic ferromagnet is iron. Below a critical temperature called the Curie temperature, ferromagnets have a spontaneous magnetization and there is hysteresis in their response to a changing magnetic field. Most importantly for rock magnetism, they have remanence, so they can record the Earth's field.\n\nIron does not occur widely in its pure form. It is usually incorporated into iron oxides, oxyhydroxides and sulfides. In these compounds, the iron atoms are not close enough for direct exchange, so they are coupled by indirect exchange or superexchange. The result is that the crystal lattice is divided into two or more sublattices with different moments.\n\nFerrimagnets have two sublattices with opposing moments. One sublattice has a larger moment, so there is a net unbalance. Magnetite, the most important of the magnetic minerals, is a ferrimagnet. Ferrimagnets often behave like ferromagnets, but the temperature dependence of their spontaneous magnetization can be quite different. Louis Néel identified four types of temperature dependence, one of which involves a reversal of the magnetization. This phenomenon played a role in controversies over marine magnetic anomalies.\n\nAntiferromagnets, like ferrimagnets, have two sublattices with opposing moments, but now the moments are equal in magnitude. If the moments are exactly opposed, the magnet has no remanence. However, the moments can be tilted (spin canting), resulting in a moment nearly at right angles to the moments of the sublattices. Hematite has this kind of magnetism.\n\nMagnetic remanence is often identified with a particular kind of remanence that is obtained after exposing a magnet to a field at room temperature. However, the Earth's field is not large, and this kind of remanence would be weak and easily overwritten by later fields. A central part of rock magnetism is the study of magnetic remanence, both as natural remanent magnetization (NRM) in rocks obtained from the field and remanence induced in the laboratory. Below are listed the important natural remanences and some artificially induced kinds.\n\nWhen an igneous rock cools, it acquires a \"thermoremanent magnetization (TRM)\" from the Earth's field. TRM can be much larger than it would be if exposed to the same field at room temperature (see isothermal remanence). This remanence can also be very stable, lasting without significant change for millions of years. TRM is the main reason that paleomagnetists are able to deduce the direction and magnitude of the ancient Earth's field.\n\nIf a rock is later re-heated (as a result of burial, for example), part or all of the TRM can be replaced by a new remanence. If it is only part of the remanence, it is known as \"partial thermoremanent magnetization (pTRM)\". Because numerous experiments have been done modeling different ways of acquiring remanence, pTRM can have other meanings. For example, it can also be acquired in the laboratory by cooling in zero field to a temperature formula_1 (below the Curie temperature), applying a magnetic field and cooling to a temperature formula_2, then cooling the rest of the way to room temperature in zero field.\n\nThe standard model for TRM is as follows. When a mineral such as magnetite cools below the Curie temperature, it becomes ferromagnetic but is not immediately capable of carrying a remanence. Instead, it is superparamagnetic, responding reversibly to changes in the magnetic field. For remanence to be possible there must be a strong enough magnetic anisotropy to keep the magnetization near a stable state; otherwise, thermal fluctuations make the magnetic moment wander randomly. As the rock continues to cool, there is a critical temperature at which the magnetic anisotropy becomes large enough to keep the moment from wandering: this temperature is called the \"blocking temperature\" and referred to by the symbol formula_3. The magnetization remains in the same state as the rock is cooled to room temperature and becomes a thermoremanent magnetization.\n\nMagnetic grains may precipitate from a circulating solution, or be formed during chemical reactions, and may record the direction of the magnetic field at the time of mineral formation. The field is said to be recorded by \"chemical remanent magnetization (CRM)\". The mineral recording the field commonly is hematite, another iron oxide. Redbeds, clastic sedimentary rocks (such as sandstones) that are red primarily because of hematite formation during or after sedimentary diagenesis, may have useful CRM signatures, and magnetostratigraphy can be based on such signatures.\n\nMagnetic grains in sediments may align with the magnetic field during or soon after deposition; this is known as detrital remanent magnetization (DRM). If the magnetization is acquired as the grains are deposited, the result is a depositional detrital remanent magnetization (dDRM); if it is acquired soon after deposition, it is a \"post-depositional detrital remanent magnetization (pDRM)\".\n\n\"Viscous remanent magnetization (VRM)\", also known as viscous magnetization, is remanence that is acquired by ferromagnetic minerals by sitting in a magnetic field for some time. The natural remanent magnetization of an igneous rock can be altered by this process. To remove this component, some form of stepwise demagnetization must be used.\n\n"}
{"id": "15513065", "url": "https://en.wikipedia.org/wiki?curid=15513065", "title": "Teleconnection", "text": "Teleconnection\n\nTeleconnection in atmospheric science refers to climate anomalies being related to each other at large distances (typically thousands of kilometers). The most emblematic teleconnection is that linking sea-level pressure at Tahiti and Darwin, Australia, which defines the Southern Oscillation.\n\nTeleconnections were first noted by the British meteorologist Sir Gilbert Walker in the late 19th century, through computation of the correlation between time series of atmospheric pressure, temperature and rainfall. They served as a building block for the understanding of climate variability, by showing that the latter was not purely random.\n\nIndeed, the term El Niño–Southern Oscillation (ENSO) is an implicit acknowledgment that the phenomenon underlies variability in several locations at once. It was later noticed that associated teleconnections occurred all over North America, as embodied by the Pacific–North American teleconnection pattern.\n\nIn the 1980s, improved observations allowed to detect teleconnections at larger distances throughout the troposphere. Concomitantly, the theory emerged that such patterns could be understood through the dispersion of Rossby waves due to the spherical geometry of the Earth. This is sometimes called the \"proto-model\".\n\nTeleconnections within the tropical Pacific began to be understood thanks to the idealized calculations of A.E. Gill and later through more complex models.\n\nBuilding upon the \"proto-model\", much of the early theory of teleconnections dealt with barotropic, linearized model of atmospheric flow about a constant mean state. However, the model was soon invalidated when it was discovered that actual teleconnection patterns were nearly insensitive to the location of the forcing, in direct contradiction with the predictions offered by this simple picture. \nSimmons and collaborators showed that if a more realistic background state was prescribed, it would become unstable, leading to a similar pattern regardless of the location of the forcing, in accordance to observations. This \"modal\" property turned out to be an artifact of the barotropicity of the model, though it has appeared for more subtle reasons in more realistic models.\n\nMore recent work has shown that most teleconnections from the tropics to the extratropics can be understood to surprising accuracy by the propagation of linear, planetary waves upon a 3-dimensional seasonally-varying basic state. Because the patterns are persistent over time and somewhat \"locked\" to geographical features such as mountain ranges, these waves are called \"stationary\".\n\nAnother mechanism of teleconnection between tropical oceans and midlatitude regions is symmetric along latitude circles (i.e. \"zonal\") and between hemispheres, unlike the stationary wave mechanism. It relies on interactions between transient eddies and the mean atmospheric flow that are mutually reinforcing (i.e. non-linear). It has been shown to explain some aspects of ENSO teleconnections in temperature and rainfall. Other authors suggested, as well, a correlation between many teleconnection patterns and local climate change factors.\n\nSince tropical sea surface temperatures are predictable up to two years ahead of time, knowledge of teleconnection patterns gives some amount of predictability in remote locations with an outlook sometimes as long as a few seasons. \nFor instance, predicting El Niño enables prediction of North American rainfall, snowfall, droughts or temperature patterns with a few weeks to months lead time. In Sir Gilbert Walker's time, A strong El Niño usually meant a weaker Indian monsoon, but this anticorrelation has weakened in the 1980s and 1990s, for controversial reasons.\n\n\n"}
{"id": "383652", "url": "https://en.wikipedia.org/wiki?curid=383652", "title": "Telluric current", "text": "Telluric current\n\nA telluric current (from Latin \"tellūs\", \"earth\"), or Earth current, is an electric current which moves underground or through the sea. Telluric currents result from both natural causes and human activity, and the discrete currents interact in a complex pattern. The currents are extremely low frequency and travel over large areas at or near the surface of the Earth.\n\nTelluric currents are phenomena observed in the Earth's crust and mantle. In September 1862, an experiment to specifically address Earth currents was carried out in the Munich Alps (Lamont, 1862). Including minor processes, there are at least 32 different mechanisms which cause telluric currents. The strongest are primarily geomagnetically induced currents, which are induced by changes in the outer part of the Earth's magnetic field, which are usually caused by interactions between the solar wind and the magnetosphere or solar radiation effects on the ionosphere. Telluric currents flow in the surface layers of the earth. The electric potential on the Earth's surface can be measured at different points, enabling the calculation of the magnitudes and directions of the telluric currents and hence the Earth's conductance. These currents are known to have diurnal characteristics wherein the general direction of flow is towards the sun. Telluric currents continuously move between the sunlit and shadowed sides of the earth, toward the equator on the side of the earth facing the sun (that is, during the day), and toward the poles on the night side of the planet.\n\nBoth telluric and magnetotelluric methods are used for exploring the structure beneath the Earth's surface (such as in industrial prospecting). For mineral exploration the targets are any subsurface structure with a distinguishable resistance in comparison to its surroundings. Uses include geothermal exploration, mining exploration, petroleum exploration, mapping of fault zones, ground water exploration and monitoring, investigation of magma chambers, and investigation of boundaries of tectonic plates. Earth batteries tap a useful low voltage current from telluric currents, and were used for telegraph systems as far back as the 1840s.\n\nIn industrial prospecting activity that uses the telluric current method, electrodes are properly located on the ground to sense the voltage difference between locations caused by the oscillatory telluric currents. It is recognized that a low frequency window (LFW) exists when telluric currents pass through the earth's substrata. In the frequencies of the LFW, the earth acts as a conductor.\n\nIn William Hope Hodgson's novel \"The Night Land\", the \"Earth-Current\", a powerful telluric current, is the source of power for the Last Redoubt, the arcology home of man after the Sun has died. Hodgson's Earth-Current is a spiritual force as well as an electrical one, warding off the monsters of the Night Lands.\n\nThe main plot of the novel \"Foucault's Pendulum\" by Umberto Eco revolves around search for the \"Umbilicus Mundi\" (Latin: \"The Navel of the World\"), the mystic \"Center of The Earth\" which is supposed to be a certain point from where a person could control the energies and shapes of the Earth, thus reforming it at will. The novel takes this even further by suggesting that monuments like the Eiffel Tower are nothing more than giant antennas to catalyze these energies.\n\nTelluric currents are also used as a means of travel by the woman Hsien-Ko and her minions in the Doctor Who \"Missing Adventures\" novel, \"The Shadow of Weng-Chiang\", by David A. McIntee.\n\nTelluric currents, along what are effectively ley lines, are discovered to be a means of mysterious communication in Thomas Pynchon's \"Mason and Dixon\", and are associated with the book's Chinese-Jesuit subplot. As with Eco, cited above, Pynchon also reflects upon hollow Earth theories in this work.\n\nIn Michel Houellebecq's novel \"The Possibility of an Island\", it is claimed that New Age literature generally holds human beings to be especially sensitive to the telluric currents that underlie volcanic areas, and that they incite sexual promiscuity.\n\nIn the television series of \"Teen Wolf\", telluric currents are used as a source of power given off by the Nemeton, an ancient worship ground for Druids. The darach uses the telluric currents to move from place to place, killing each victim at the center of each current, the Nemeton.\n\n\n"}
{"id": "21208171", "url": "https://en.wikipedia.org/wiki?curid=21208171", "title": "Theodore Roosevelt Conservation Partnership", "text": "Theodore Roosevelt Conservation Partnership\n\nThe Theodore Roosevelt Conservation Partnership, or TRCP, is a non-profit 501(c)(3) coalition of conservation organizations, grassroots partners and outdoor related businesses, the main goal of which is increased federal funding for conservation while preserving access for hunters and fishers. It has lobbied policymakers to do more to address the challenge of climate change, which they say will have a significant impact on the sportsmen community and has teamed with the National Wildlife Federation and Trout Unlimited to form \"Sportsmen for Responsible Energy Development,\" which works to preserve natural resources and forests.\n\nAs of December 2014, the President and CEO was Whit Fosburgh. In 2013, the organization's revenue was $3.7 million and it spent 84.1% of its income on programs.\n\n\n"}
{"id": "285236", "url": "https://en.wikipedia.org/wiki?curid=285236", "title": "Trachyte", "text": "Trachyte\n\nTrachyte is an igneous volcanic rock with an aphanitic to porphyritic texture. It is the volcanic equivalent of syenite. The mineral assemblage consists of essential alkali feldspar; relatively minor plagioclase and quartz or a feldspathoid such as nepheline may also be present. (See the QAPF diagram). Biotite, clinopyroxene and olivine are common accessory minerals.\n\nChemically, trachyte contains 60 to 65% silica content; less SiO than rhyolite and more (NaO plus KO) than dacite. These chemical differences are consistent with the position of trachyte in the TAS classification, and they account for the feldspar-rich mineralogy of the rock type.\n\nTrachytes usually consist mainly of sanidine feldspar. Very often they have minute irregular steam cavities which make the broken surfaces of specimens of these rocks rough and irregular, and from this character they have derived their name. It was first given to certain rocks of this class from Auvergne, and was long used in a much wider sense than that defined above; in fact it included quartz-trachytes (now known as liparites and rhyolites) and oligoclase-trachytes, which are now more properly assigned to andesites. The trachytes are often described as being the volcanic equivalents of the plutonic syenites. Their dominant mineral, sanidine feldspar, very commonly occurs in two generations, i.e. both as large well-shaped porphyritic crystals and in smaller imperfect rods or laths forming a finely crystalline groundmass. With this there is practically always a smaller amount of plagioclase, usually oligoclase; but the potassium feldspar (sanidine) often contains a considerable proportion of the sodium feldspar (albite), and has rather the characteristics of anorthoclase or cryptoperthite than of pure sanidine. Rhomb porphyry is an example with usually large porphyritic rhomb shaped phenocrysts embedded in a very fine-grained matrix.\n\nQuartz is typically rare in trachyte, but tridymite (which likewise consists of silica) is by no means uncommon. It is rarely in crystals large enough to be visible without the aid of the microscope, but in thin sections it may appear as small hexagonal plates, which overlap and form dense aggregates, like a mosaic or like the tiles on a roof. They often cover the surfaces of the larger feldspars or line the steam cavities of the rock, where they may be mingled with amorphous opal or fibrous chalcedony. In the older trachytes, secondary quartz is not rare, and probably sometimes results from the recrystallization of tridymite.\n\nOf the mafic minerals present, augite is the most common. It is usually of pale green color, and its small crystals are often very perfect in form. Brown hornblende and biotite occur also, and are usually surrounded by black corrosion borders composed of magnetite and pyroxene; sometimes the replacement is complete and no hornblende or biotite is left, though the outlines of the cluster of magnetite and augite may clearly indicate from which of these minerals it was derived. Olivine is unusual, though found in some trachytes, like those of the Arso in Ischia. Basic varieties of plagioclase, such as labradorite, are known also as phenocrysts in some Italian trachytes. Dark brown varieties of augite and rhombic pyroxene (hypersthene or bronzite) have been observed but are not common. Apatite, zircon and magnetite are practically always present as accessory minerals.\n\nTrachytes, being very rich in potassium feldspar, necessarily contain considerable amounts of alkali; in this character they approach the phonolites. Occasionally minerals of the feldspathoid group, such as nepheline, sodalite and leucite, occur, and rocks of this kind are known as phonolitic trachytes. The sodium-bearing amphiboles and pyroxenes so characteristic of the phonolites may also be found in some trachytes; thus aegirine or aegirine augite forms outgrowths on diopside crystals, and riebeckite may be present in spongy growths among the feldspars of the groundmass (as in the trachyte of Berkum on the Rhine). Trachytic rocks are typically porphyritic, and some of the best known examples, such as the trachyte of Drachenfels on the Rhine, show this character excellently, having large sanidine crystals of tabular form an inch or two in length scattered through their fine-grained groundmass. In many trachytes, however, the phenocrysts are few and small, and the groundmass comparatively coarse. The ferromagnesian minerals rarely occur in large crystals, and are usually not conspicuous in hand specimens of these rocks. Two types of groundmass are generally recognized: the trachytic, composed mainly of long, narrow, subparallel rods of sanidine, and the orthophyric, consisting of small squarish or rectangular prisms of the same mineral. Sometimes granular augite or spongy riebeckite occurs in the groundmass, but as a rule this part of the rock is highly feldspathic. Glassy forms of trachyte (obsidian) occur, as in Iceland, and pumiceous varieties are known (in Tenerife and elsewhere), but these rocks as contrasted with the rhyolites have a remarkably strong tendency to crystallize, and are rarely to any considerable extent vitreous.\n\nTrachytes are well represented among the Cenozoic volcanic rocks of Europe. In Britain they occur in Skye as lava flows and as dikes or intrusions, but they are much more common on the continent of Europe, as in the Rhine district and the Eifel, also in Auvergne, Bohemia and the Euganean Hills. In the neighborhood of Rome, Naples and the island of Ischia trachytic lavas and tuffs are of common occurrence. Trachytes are also found on the island of Pantelleria. In the United States, trachytes crop out extensively in the Davis Mountains, Chisos Mountains, and Big Bend Ranch State Park in the Big Bend (Texas) region, as well as southern Nevada and South Dakota (Black Hills). There is one known voluminous flow from Pu'u Wa'awa'a on the north flank of Hualalai in Hawaii. In Iceland, the Azores, Tenerife and Ascension there are recent trachytic lavas, and rocks of this kind occur also in New South Wales (Cambewarra Range), Queensland (Main Range), East Africa, Madagascar, Yemen and in many other districts.\n\nAmong the older volcanic rocks trachytes also are not scarce, though they have often been described under the names orthophyre and orthoclase-porphyry, while trachyte was reserved for Tertiary and recent rocks of similar composition. In England there are Permian trachytes in the Exeter district, and Carboniferous trachytes are found in many parts of the central valley of Scotland. The latter differ in no essential respect from their modern representatives in Italy and the Rhine valley, but their augite and biotite are often replaced by chlorite and other secondary products. Permian trachytes occur also in Thuringia and the Saar district in Germany.\n\nClosely allied to trachyte is the rock type called keratophyre, which is the sodium-rich-plagioclase equivalent of trachyte.\n\n\n"}
{"id": "46214758", "url": "https://en.wikipedia.org/wiki?curid=46214758", "title": "USAEE", "text": "USAEE\n\nUnited States Association for Energy Economics (USAEE) was founded in 1994 to provide a forum for the exchange of ideas, experience and issues among professionals interested in energy economics. It is the largest affiliate of the International Association for Energy Economics. USAEE has 1000 members from diverse backgrounds - corporate, academic, scientific and government.\n\nThe USAEE publishes a newsletter, \"Dialogue\", an annual salary survey, as well as a \"Working Paper Series\".\n\nThe main annual conference for USAEE is the \"USAEE North American Conference\" that is organized at diverse locations in North America. In addition, local affiliate chapters hold regular meetings as well as regional conferences. Recent annual conferences have been held in New York City, Anchorage, Alaska, and Calgary, Canada.\n\n"}
{"id": "39006647", "url": "https://en.wikipedia.org/wiki?curid=39006647", "title": "Washed Ashore", "text": "Washed Ashore\n\nWashed Ashore is a project of the Artula Institute for Art and Environmental Education U.S. 501(c)(3) grassroots non-profit environmental organization that works to bring awareness to the world's growing plastic pollution problem through art. Washed Ashore is a large touring exhibit composed of massive sculptures made up of marine debris collected by volunteers primarily along the southern coast of Oregon.\n\nThe mission statement is \n\"To bring awareness to the ocean’s plastic pollution problem and influence consumer habits by creating community built aesthetically powerful art.\"\n\nFounded in 2010 by Angela Haseltine Pozzi, an artist and educator for over 30 years, the unique non-profit organization has built over 66 giant sculptures from over 17 tons of ocean garbage, and the exhibit, including educational signage, has appeared at numerous venues including SeaWorld Parks throughout the USA, The Virginia Aquarium, San Francisco Zoo, The Marine Mammal Center in Sausalito, California Newport Visual Arts Center, the Chula Vista, California Nature Center, Portland Community College, America's Cup Healthy Oceans Exhibit and the Oregon Coast Aquarium.\n\nBased in Bandon, Oregon, the project uses local and area volunteers in workshops held at Bandon's Harbortown Event Center Thursdays through Saturdays (winter hours). The workshops consist of assembling \"piece work\" which involves drilling holes through plastic and stitching the plastic to wire mesh. These panels will later be fastened by Lead Artist, Angela Haseltine Pozzi, and her team, to welded structures and other artwork as part of Washed Ashore's Traveling Exhibits.\n"}
{"id": "12364704", "url": "https://en.wikipedia.org/wiki?curid=12364704", "title": "White River (Michigan)", "text": "White River (Michigan)\n\nWhite River may refer to the following streams in the U.S. state of Michigan:\n\n\n"}
