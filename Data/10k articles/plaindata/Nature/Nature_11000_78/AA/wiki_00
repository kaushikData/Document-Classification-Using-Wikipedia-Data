{"id": "4235559", "url": "https://en.wikipedia.org/wiki?curid=4235559", "title": "1991 BA", "text": "1991 BA\n\n1991 BA is a sub-kilometer asteroid, classified as near-Earth object of the Apollo group that was first observed by Spacewatch on 18 January 1991, and passed within 160,000 km (100,000 mi) of Earth. This is a little less than half the distance to the Moon.\n\n1991 BA is approximately 5 to 10 meters (15 to 30 ft) in diameter and is listed on the Sentry Risk Table. It follows a highly eccentric (0.68), low-inclination (2.0°) orbit of 3.3 years duration, ranging between 0.71 and 3.7 AU from the Sun. 1991 BA was, at the time of its discovery, the smallest and closest confirmed asteroid outside of Earth's atmosphere. 1991 BA is too faint to be observed except during close approaches to Earth and is considered lost.\n\nVirtual clones of the asteroid that fit the uncertainty region in the known trajectory show a 1 in 1,961,000 chance that the asteroid will impact Earth on 2023 January 18. It is estimated that an impact would produce an upper atmosphere air burst equivalent to 19 kt TNT, roughly equal to Nagasaki's Fat Man. The asteroid would appear as a bright fireball and fragment in the air burst into smaller pieces that would hit the ground at terminal velocity producing a meteorite strewn field. Impacts of objects this size are estimated to occur approximately once a year. Asteroid was an object of similar size that was discovered less than a day before its impact on Earth on October 7, 2008 and produced a fireball and meteorite strewn field in the Sudan.\n\n"}
{"id": "1729464", "url": "https://en.wikipedia.org/wiki?curid=1729464", "title": "6V6", "text": "6V6\n\nThe 6V6 is a beam-power tetrode vacuum tube. The first of this family of tubes to be introduced was the 6V6G by Ken-Rad Tube & Lamp Corporation in late 1936, with the availability by December of both Ken-Rad and Raytheon 6V6G tubes announced. It is still in use in audio applications, especially electric guitar amplifiers. Following the introduction in July 1936 of the 6L6, the potential of the scaled down version that became the 6V6 was soon realized. The lower-powered 6V6 was better suited for average home use, and became common in the audio output stages of \"farmhouse\" table-top radios, where power pentodes such as the 6F6 had previously been used. The 6V6 required less heater power and produced less distortion than the 6F6, while yielding higher output in both single-ended and push-pull configurations. Although the 6V6 was originally designed especially for use in automobile radios, the clip-in Loctal base 7C5, from early 1939, or the lower heater current 12V6GT, both with the identical characteristics to the 6V6, but with the smaller T-9 glass envelope, soon became the tubes of choice for many automotive radios manufacturers. Additionally, the 6V6 had applications in portable battery-operated radios.\n\nThe data sheet information supplied by the tube manufacturers design centers, list the typical operation of an audio output stage for a single 6V6 as producing about 5W of continuous power, and a push-pull pair about 14W. Amplifier manufacturers soon realized that the tube was capable of being used at ratings above the recommended maximums, and guitar amplifiers with 400V on the plates of a pair of 6V6GTA claim to produce an output power of 20W RMS at 5%THD with 40W Peak Music Power, and with 490V on the plates, as much as 30W RMS.\n\nFollowing the 6V6G, RMA Release #96 – 09 Nov. 1936, sponsored by Ken-Rad Tube & Lamp Corporation, with the ST 14 shouldered glass envelope, the 6V6 was announced with a metal mantel in January 1937 by Hygrade Sylvania Corporation. The RMA Release #125 – 03 Jan.1938, Sponsored by RCA. for the 6V6 tube has led to some confusion as to the origins of the 6V6. The 6V6G but not the 6V6 is in the RCA manual RC-13 from July 1937, but the 6V6 is to be found in the 1937 tube manuals of other manufacturers, such as Raytheon. Tube manufacturers were keen to promote the superiority of the metal tube construction that was introduced on April 1, 1935, and large quantities of the 6V6 tube were produced in the following decade, many as military supply JAN tubes, and the price of the metal and glass versions were held to closely the same retail price level for the first few years of their production. The introduction of the 6V6GT, RMA Release #201 – 10 July 1939, was sponsored by Hytron Corporation. By 1940, the 6V6G production was largely superseded by this smaller \"GT\" T-9 glass envelope. On April 17, 1942, the War Production Board ordered\nradio tube manufacturers to discontinue within seven days the production for civilian use of 349 of the 710 types of radio tubes on the market, amongst these was the 6V6G and 6V6GX. By 1943, we find that the price of the metal version is almost twice that of the GT version, and this proportional difference in price seems to have remained constant, right through to the end of the 1970s. The 6V6GTA – RMA Release #1681 – 2 July 1956, sponsored by Hygrade Sylvania Corporation, has a controlled warm-up period. The various different NOS (new old stock) tubes of the 6V6 family, depending on manufacturer, model, series, strength and condition, will vary enormously in scarcity and therefore usually in price. The metal NOS 6V6 tube, once costing almost twice the price of its now highly valued glass enveloped counterparts, is now considered to be fairly common, and is usually the cheapest NOS tube available, with many current production tubes costing more than its 60 to 80 year older classic predecessor. In the final years of U.S. production, several of the major manufacturers switched to using the so-called \"coin\" based GT bulb.\n\nNow, eighty years after its introduction, and still retaining its original characteristics, the 6V6 has one of the longest active lifetimes of any electronic component, having never been out of production in all this long period of time. Although historically widely used in all manner of electronic goods, many of which are still in service, it is in guitar amplifiers where its use has become archetypal. Not only are there very many existing amplifiers in regular use that rely on the 6V6, with contemporary reproductions of the more iconic models still being made, modern designers are still keen to develop new creations that rely on its use.\nGenerally speaking, 6V6 tubes are sturdy and can be operated beyond their published specifications (the Soviet made 6P6S, and Chinese 6V6 versions being possible exceptions, although current production has improved). Because of this, the 6V6 soon proved itself to be suitable for use in consumer-market musical instrument amplifiers, particularly combo-style guitar amps such as the Gibson GA-40, and the Fender Amplifiers; Champ, Princeton, and Deluxe, some of which drive their 6V6s well in excess of the datasheet specified maximum rating. This ongoing demand encourages Chinese, Slovakian and Russian tube factories not only to keep the 6V6 in production to this day, but to further develop the supply.\n\n6V6G – Glass \"Shouldered Tube\" ST envelope.\n\n6V6GX – Glass \"Shouldered Tube\" ST envelope, Ceramic Base.\n\n6V6 – Metal jacketed envelope.\nThe metal envelope of 6V6 is connected to pin 1 of the base, and was normally used as a ground.\nPin 1 of the other members of the 6V6 family of tubes are usually not internally connected, although some may have the gray RF shield connected.\n\n6V6GT – smaller \"Glass Tube\" T-9 envelope.\n\n6V6GTA – with a controlled warm-up period.\n\n6V6GTY – a GT with a low loss micanol brown base.\n\n6V6GTX = HY6V6GTX – a GT \"Bantam\" selected for high gain, with a ceramic base, 15W plate dissipation rating, produced for a limited period around 1941 by Hytron.\n\n5871 – Ruggedized 6V6GT for operation under severe vibrations found in aircraft and similar applications. Radio Valve Co. of Canada Ltd., 1954 : RMA #859A.\n\n5992 – Premium, ruggedized 6V6GT with heater current raised to 600mA. Bendix and GE known manufacturers.\n\n7408 – 6V6GT with additional zero-bias characteristics.\n\n6V6S – A modern production, large plated tube, heater current 500mA, with a higher plate and screen voltage rating. Made by JJ Electronic.\n\n6V6GT(A)(B)-STR – Modern production valve, STR signifying \"Special Test Requirement.\" Claiming to be heavy duty, suitable for high plate voltage.\n\nMilitary specification 6V6 tubes and their equivalents\n\nAmerican military services contracted tubes from many sources through the U.S. War Department. They used a Joint Army-Navy Nomenclature System (AN System. JAN) Most of these tubes bear the JAN marking as well as a VT number.\n\nVT-107 – Metal 6V6.\n\n6V6Y – Metal, with a low loss micanol brown base.\n\nVT-107A – 6V6GT.\n\nVT-107B – 6V6G.\n\nBritish Ministry Of Supply valves for the Military & other governmental agencies have a CV number (CV = common valve). Supplied by Mullard & Brimar.\n\nCV509 – 6V6G\n\nCV510 – 6V6\n\nCV511 – 6V6GT & 6V6GTY\n\nThe British GPO also used their own VT (Valve - Thermionic) numbering system\n\nVT196 = CV509 = 6V6G – General Post Office (GPO)\n\nSwedish Military supplier Bofors, had tubes made by Standard Radiofabrik (SRF) at the Ulvsunda plant in Stockholm.\n\n5S2D - Premium, ruggedized 6V6GT with triple micas, low loss micanol brown base.\n\nOther tubes cited as being equivalent\n\n6P6S (6П6С in Cyrillic.) Also 6П2 - In the Soviet Union a version of the 6V6GT was produced since the late 1940s which appears to be a close copy of the 1940s Sylvania-issue 6V6GT – initially under its American designation (in both Latin and Cyrillic lettering), but later, the USSR adopted its own system of designations.\n\n6P11S (6П11С in Cyrillic.) = 6П6С-Y2 - Military consignment, ruggedized 6P6S. OTK tested. Higher Voltage ratings.\n\n1515 - Premium version of the 6P6S (6П6С).\n\n6P6P - Chinese version of the 6V6GT made by Shuguang, but now obsolete, different from Shuguang's current production 6V6GT.\n\n3106 – East German production 6V6GT from OSW (Oberspreewerk Berlin, later HF, then WF with 6П6С & 6V6 marking). Open, split-plate design.\n\n6AY5 – East German production 6V6GT\n\nVT227 = 7184 – Cited equivalent, made by Ken-Rad, inadequate documentation, no RMA registration.\n\nWT-210-00-82 – Cited equivalent, inadequate documentation, no RMA registration.\n\nWTT-123 – Cited equivalent, inadequate documentation, no RMA registration.\n\n6V6HD – NOT a 6V6, but relabeled Sovtek 6L6GA / 6P3S.\n\nThese tubes have very similar characteristics to the 6V6, but differ either in the heater rating, or use a different socket and pin-out\n\n5V6GT – Same as the 6V6GT, but with different heater ratings - 4.7V, 0.6A, controlled 11 sec. warm-up time.\n\n12V6GT – Same as the 6V6GT, but with different heater ratings - 12.6V, 0.225A, suitable for automotive receiver applications.\n\n7C5 – Clip-in Loctal B8G base, T-9 Bulb. Raytheon – 1939 RMA #162. Other version of this tube are 7C5-TV, 2C48, 2C50, N148, CV885\n\n7C5LT = CV886 – Same as the 7C5, except with a small wafer Octalox base 8-pin T-9 Bulb. RCA – 1940 RMA #234.\n\n14C5 – Same as 7C5 but with 12.6V Heater.\n\n6BW6 = CV2136 – British made miniature-tube 12W equivalent of the 6V6, 9-pin base B9A.\n\n6061 – Premium, ruggedized 6BW6. Brimar STC London. 1951 : RMA #965\n\nCV4043 – British made miniature-tube 13.5W equivalent of the 6BW6, 9-pin base B9A.\n\n6AQ5 – A pentode with similar specifications to the 6V6GT, miniature glass envelope, 7-pin base B7G. Other equivalents of this tube are the CV1862, EL90, 6005, 6095, 6669, 6928, BPM 04, CK-6005, M8249, N7277C5, 6L31\n\n12AQ5 - Same as the 6AQ5, but with different heater ratings.\n\n6P1P (6П1П in Cyrillic.) – 9-pin Soviet version of the 6AQ5.\n\n6973 – US version, 9 pin Noval base tube, higher plate voltage ratings, intended for high fidelity output applications.\n\n6CM6 – 9 pin Noval base tube, equivalent type primarily intended for vertical deflection amplifiers in television receivers.\n\n12CM6 - Same as the 6CM6, but with different heater ratings.\n\n12AB5 – 9 pin Noval base tube with 12V heater, suitable for automotive receiver applications. Equivalent to the 7061\n\nThe pentode EL84/6BQ5 - 9 pin Noval base tube, that although different enough from the 6V6 not to justify rating it as an equivalent, because of its popularity and ready availability, plus having a close enough similarity to make it possible, adapters have been developed commercially to allow an amplifier designed for 6V6 use, to accept the noval based EL84 tube. Likewise, the inverse adapter is also available.\n\n\n\n"}
{"id": "30784907", "url": "https://en.wikipedia.org/wiki?curid=30784907", "title": "ACEGES", "text": "ACEGES\n\nThe ACEGES model (Agent-based Computational Economics of the Global Energy System) is a decision support tool for energy policy by means of controlled computational experiments. The ACEGES tool is designed to be the foundation for large custom-purpose simulations of the global energy system. The ACEGES methodological framework, developed by Voudouris (2011) by extending Voudouris (2010), is based on the agent-based computational economics (ACE) paradigm. ACE is the computational study of economies modeled as evolving systems of autonomous interacting agents.\n\nThe ACEGES tool is written in Java and runs on Windows, Mac OS and Linux platforms. The ACEGES tool is based on:\n\nIt is important to clarify that although the ACEGES model builds on the available scholarly and policy literature, it does not strictly follow any existing approach.\n\nThe ACEGES project was conceived by Vlasios Voudouris (with contributions by Michael Jefferson). Voudouris is now Affiliate Professor (Energy and Commodities) at the ESCP Europe. The first version of the ACEGES decision-support tool was written in 2010. The ACEGES models energy demand and supply of 218 countries. The ACEGES tool was the main output of the ACEGES Project. The overall aim of the ACEGES project was to develop, test and disseminate an agent-based computational laboratory for the systematic experimental study of the global energy system through the mechanism of Energy Scenarios. In particular, the intention was to show how the ACEGES framework and prototype can be used to help leaders in government, business and civil society better understand the challenging outlook for energy through controlled computational experiments.\n\nThe ACEGES tool has been used, for example, to test the peak oil theory and to develop plausible scenarios of conventional oil production by means of demonstration at:\n\nDetails about the ACEGES decision-support tool (including supporting documentation) are available from ABM Analytics.\n"}
{"id": "233013", "url": "https://en.wikipedia.org/wiki?curid=233013", "title": "Absurdism", "text": "Absurdism\n\nIn philosophy, \"the Absurd\" refers to the conflict between the human tendency to seek inherent value and meaning in life and the human inability to find any in a purposeless, meaningless or chaotic and irrational universe. The universe and the human mind do not each separately cause the Absurd, but rather, the Absurd arises by the contradictory nature of the two existing simultaneously. \n\nAs a philosophy, absurdism furthermore explores the fundamental nature of the Absurd and how individuals, once becoming conscious of the Absurd, should respond to it. The absurdist philosopher Albert Camus stated that individuals should embrace the absurd condition of human existence while also defiantly continuing to explore and search for meaning.\n\nAbsurdism shares some concepts, and a common theoretical template, with existentialism and nihilism. It has its origins in the work of the 19th-century Danish philosopher Søren Kierkegaard, who chose to confront the crisis that humans face with the Absurd by developing his own existentialist philosophy. Absurdism as a belief system was born of the European existentialist movement that ensued, specifically when Camus rejected certain aspects of that philosophical line of thought and published his essay \"The Myth of Sisyphus\". The aftermath of World War II provided the social environment that stimulated absurdist views and allowed for their popular development, especially in the devastated country of France.\n\nIn absurdist philosophy, the Absurd arises out of the fundamental disharmony between the individual's search for meaning and the meaninglessness of the universe. As beings looking for meaning in a meaningless world, humans have three ways of resolving the dilemma. Kierkegaard and Camus describe the solutions in their works, \"The Sickness Unto Death\" (1849) and \"The Myth of Sisyphus\" (1942), respectively:\n\n\nAbsurdism originated from (as well as alongside) the 20th-century strains of existentialism and nihilism; it shares some prominent starting points with both, though also entails conclusions that are uniquely distinct from these other schools of thought. All three arose from the human experience of anguish and confusion stemming from the Absurd: the apparent meaninglessness in a world in which humans, nevertheless, are compelled to find or create meaning. The three schools of thought diverge from there. Existentialists have generally advocated the individual's construction of his or her own meaning in life as well as the free will of the individual. Nihilists, on the contrary, contend that \"it is futile to seek or to affirm meaning where none can be found.\" Absurdists, following Camus's formulation, hesitantly allow the possibility for some meaning or value in life, but are neither as certain as existentialists are about the value of one's own constructed meaning nor as nihilists are about the total inability to create meaning. Absurdists following Camus also devalue or outright reject free will, encouraging merely that the individual live defiantly and authentically \"in spite of\" the psychological tension of the Absurd.\n\nCamus himself passionately worked to counter nihilism, as he explained in his essay \"The Rebel,\" while he also categorically rejected the label of \"existentialist\" in his essay \"Enigma\" and in the compilation \"The Lyrical and Critical Essays of Albert Camus\", though he was, and still is, often broadly characterized by others as an existentialist. Both existentialism and absurdism entail consideration of the practical applications of becoming conscious of the truth of existential nihilism: i.e., how a driven seeker of meaning should act when suddenly confronted with the seeming concealment, or downright absence, of meaning in the universe. Camus's own understanding of the world (e.g., \"a benign indifference\", in \"The Stranger\"), and every vision he had for its progress, however, sets him apart from the general existentialist trend.\n\nSuch a chart represents some of the overlap and tensions between existentialist and absurdist approaches to meaning. While absurdism can be seen as a kind of response to existentialism, it can be debated exactly how substantively the two positions differ from each other. The existentialist, after all, doesn't deny the reality of death. But the absurdist seems to reaffirm the way in which death ultimately nullifies our meaning-making activities, a conclusion the existentialists seem to resist through various notions of posterity or, in Sartre's case, participation in a grand humanist project.\n\nA century before Camus, the 19th century Danish philosopher Søren Kierkegaard wrote extensively about the absurdity of the world. In his journals, Kierkegaard writes about the absurd:\n\nHere is another example of the Absurd from his writings: \n\nHow can this absurdity be held or believed? Kierkegaard says: \n\nKierkegaard provides an example in \"Fear and Trembling\" (1843), which was published under the pseudonym \"Johannes de Silentio\". In the story of Abraham in the Book of Genesis, Abraham is told by God to kill his son Isaac. Just as Abraham is about to kill Isaac, an angel stops Abraham from doing so. Kierkegaard believes that through virtue of the absurd, Abraham, defying all reason and ethical duties (\"you cannot act\"), got back his son and reaffirmed his faith (\"where I have to act\").\n\nAnother instance of absurdist themes in Kierkegaard's work appears in \"The Sickness Unto Death\", which Kierkegaard signed with pseudonym \"Anti-Climacus\". Exploring the forms of despair, Kierkegaard examines the type of despair known as defiance. In the opening quotation reproduced at the beginning of the article, Kierkegaard describes how such a man would endure such a defiance and identifies the three major traits of the Absurd Man, later discussed by Albert Camus: a rejection of escaping existence (suicide), a rejection of help from a higher power and acceptance of his absurd (and despairing) condition.\n\nAccording to Kierkegaard in his autobiography \"The Point of View of My Work as an Author\", most of his pseudonymous writings are not necessarily reflective of his own opinions. Nevertheless, his work anticipated many absurdist themes and provided its theoretical background.\n\nThough the notion of the 'absurd' pervades all Albert Camus's writing, \"The Myth of Sisyphus\" is his chief work on the subject. In it, Camus considers absurdity as a confrontation, an opposition, a conflict or a \"divorce\" between two ideals. Specifically, he defines the human condition as absurd, as the confrontation between man's desire for significance, meaning and clarity on the one hand – and the silent, cold universe on the other. He continues that there are specific human experiences evoking notions of absurdity. Such a realization or encounter with the absurd leaves the individual with a choice: suicide, a leap of faith, or recognition. He concludes that recognition is the only defensible option.\n\nFor Camus, suicide is a \"confession\" that life is not worth living; it is a choice that implicitly declares that life is \"too much.\" Suicide offers the most basic \"way out\" of absurdity: the immediate termination of the self and its place in the universe.\n\nThe absurd encounter can also arouse a \"leap of faith,\" a term derived from one of Kierkegaard's early pseudonyms, \"Johannes de Silentio\" (although the term was not used by Kierkegaard himself), where one believes that there is more than the rational life (aesthetic or ethical). To take a \"leap of faith,\" one must act with the \"virtue of the absurd\" (as \"Johannes de Silentio\" put it), where a suspension of the ethical may need to exist. This faith has no expectations, but is a flexible power initiated by a recognition of the absurd. (Although at some point, one recognizes or encounters the existence of the Absurd and, in response, actively ignores it.) However, Camus states that because the leap of faith escapes rationality and defers to abstraction over personal experience, the leap of faith is not absurd. Camus considers the leap of faith as \"philosophical suicide,\" rejecting both this and physical suicide.\n\nLastly, a person can choose to embrace the absurd condition. According to Camus, one's freedom – and the opportunity to give life meaning – lies in the recognition of absurdity. If the absurd experience is truly the realization that the universe is fundamentally devoid of absolutes, then we as individuals are truly free. \"To live without appeal,\" as he puts it, is a philosophical move to define absolutes and universals subjectively, rather than objectively. The freedom of humans is thus established in a human's natural ability and opportunity to create their own meaning and purpose; to decide (or think) for him- or herself. The individual becomes the most precious unit of existence, representing a set of unique ideals that can be characterized as an entire universe in its own right. In acknowledging the absurdity of seeking any inherent meaning, but continuing this search regardless, one can be happy, gradually developing meaning from the search alone.\n\nCamus states in \"The Myth of Sisyphus\": \"Thus I draw from the absurd three consequences, which are my revolt, my freedom, and my passion. By the mere activity of consciousness I transform into a rule of life what was an invitation to death, and I refuse suicide.\" \"Revolt\" here refers to the refusal of suicide and search for meaning despite the revelation of the Absurd; \"Freedom\" refers to the lack of imprisonment by religious devotion or others' moral codes; \"Passion\" refers to the most wholehearted experiencing of life, since hope has been rejected, and so he concludes that every moment must be lived fully.\n\nAccording to absurdism, humans historically attempt to find meaning in their lives. Traditionally, this search results in one of two conclusions: either that life is meaningless, or life contains within it a purpose set forth by a higher power—a belief in God, or adherence to some religion or other abstract concept. \n\nCamus perceives filling the void with some invented belief or meaning as a mere \"act of eluding\"—that is, avoiding or escaping rather than acknowledging and embracing the Absurd. To Camus, elusion is a fundamental flaw in religion, existentialism, and various other schools of thought. If the individual eludes the Absurd, then he or she can never confront it.\nCamus also concedes that elusion is the most common.\n\nEven with a spiritual power as the answer to meaning, another question arises: What is the purpose of a belief in God? Kierkegaard believed that there is no human-comprehensible purpose of God, making faith in God absurd itself. Camus on the other hand states that to believe in God is to \"deny one of the terms of the contradiction\" between humanity and the universe (and is therefore not absurd but what he calls \"philosophical suicide\"). Camus (as well as Kierkegaard), though, suggests that while absurdity does not lead to belief in God, neither does it lead to the denial of God. Camus notes, \"I did not say 'excludes God', which would still amount to asserting\".\n\nFor Camus, the beauty people encounter in life makes it worth living. People may create meaning in their own lives, which may not be the objective meaning of life (if there is one), but can still provide something to strive for. However, he insisted that one must always maintain an ironic distance between this invented meaning and the knowledge of the absurd, lest the fictitious meaning take the place of the absurd.\n\nFreedom cannot be achieved beyond what the absurdity of existence permits; however, the closest one can come to being absolutely free is through acceptance of the Absurd. Camus introduced the idea of \"acceptance without resignation\" as a way of dealing with the recognition of absurdity, asking whether or not man can \"live without appeal\", while defining a \"conscious revolt\" against the avoidance of absurdity of the world. In a world devoid of higher meaning or judicial afterlife, the human nature becomes as close to absolutely free as is humanly possible.\n\nThe rejection of hope, in absurdism, denotes the refusal to believe in anything more than what this absurd life provides. Hope, Camus emphasizes, however, has nothing to do with despair (meaning that the two terms are not opposites). One can still live fully while rejecting hope, and, in fact, can only do so \"without\" hope. Hope is perceived by the absurdist as another fraudulent method of evading the Absurd, and by not having hope, one is motivated to live every fleeting moment to the fullest. In the words of Nikos Kazantzakis's epitaph: \"I hope for nothing. I fear nothing. I am free.\"\n\nThe absurdist is not guided by morality, but rather, by their own integrity. The absurdist is, in fact, amoral (though not necessarily immoral). The Absurdist's view of morality implies an unwavering sense of definite right and wrong at all times, while integrity implies honesty with one's self and consistency in the motivations of one's actions and decisions.\n\n\n"}
{"id": "39286355", "url": "https://en.wikipedia.org/wiki?curid=39286355", "title": "BIRD (satellite)", "text": "BIRD (satellite)\n\nBIRD (Bispectral and Infrared Remote Detection) is a satellite launched by ISRO in 2001 for DLR. This small (92 kg) boxlike system, with solar panel two collectors on stub wings, has remarkable fire-detection qualities. It hosts a two-channel infrared sensor system in combination with a Wide-Angle Optoelectronic Stereo Scanner (WAOSS). It also features a neuronal network classificator in orbit to reduce downlink bandwidth and cost.\n\nThe unique combination of a stereo camera and two infrared cameras gives the opportunity to acquire:\n\n\nThe attitude&control system of the BIRD satellite was reused in the TET-1 satellite.\n\nA BIRD satellite architecture (in German) (pdf)\n\nO. Maibaum, T. Terzibaschian, \"Lessons learned from the Object-Oriented Design of the BIRD Attitude Control System Software\", 16th IFAC Symposium on Automatic Control in Aerospace (ACA'2004), ACA'2004 Preprints (Vol.I), S. 156-161, St.Petersburg, 14–18 June 2004\n\n"}
{"id": "52159000", "url": "https://en.wikipedia.org/wiki?curid=52159000", "title": "Baldwin effect (astronomy)", "text": "Baldwin effect (astronomy)\n\nThe Baldwin effect in astronomy describes a relationship between continuum and emission-line fluxes observed in the electromagnetic spectra of quasars and active galactic nuclei, namely an anticorrelation between the equivalent width, W, of a spectral line and the continuum luminosity, L, in broad UV optical emission lines. This means that the ratio of brightness of the emission line to the brightness of the nearby continuum decreases with increasing luminosity of the continuum.\n\nThe effect was discovered in observations of high-redshift quasars reported in 1977, and has been commonly named after the discoverer Jack Allen Baldwin, an astronomer at Lick Observatory. It was found that the equivalent width of the broad ultraviolet C IV (λ 1549) emission line, caused by the presence of thrice ionised carbon C, decreased with increasing continuum luminosity of the source. For flat-spectrum, radio-loud quasars the relation\n\nformula_1\n\nwas found.\n\nThe same effect has been observed for other spectral lines such as the Ly-α hydrogen line and the C III] (λ 1909) line of twice ionised carbon. It has also been found for spectral lines in the infrared range, e.g. in several Seyfert galaxies.\n\nMore or less satisfactory explanations of the phenomenon have been attempted using photoionization models, continuum beaming, variability, or continuum shape. An adequate, but not exclusive explanation is offered as a consequence of the presence of massive accretion disks in the centres of active galaxies and quasars. However, a complete explanation is still outstanding.\n\n"}
{"id": "4240854", "url": "https://en.wikipedia.org/wiki?curid=4240854", "title": "Bipolaron", "text": "Bipolaron\n\nIn physics, a bipolaron is a type of quasiparticle consisting of two polarons. In organic chemistry, it is a molecule or a part of a macromolecular chain containing two positive charges in a conjugated system.\n\nIn physics, a bipolaron is a bound pair of two polarons. An electron in a material may cause a distortion in the underlying lattice. The combination of electron and distortion (which may also be understood as a cloud of phonons) is known as a polaron (in part because the interaction between electron and lattice is via a polarization). When two polarons are close together, they can lower their energy by sharing the same distortions, which leads to an effective attraction between the polarons. If the interaction is sufficiently large, then that attraction leads to a bound bipolaron. For strong attraction, bipolarons may be small. Small bipolarons have integer spin and thus share some of the properties of bosons. If many bipolarons form without coming too close, they might be able to form a Bose–Einstein condensate. This has led to a suggestion that bipolarons could be a possible mechanism for high-temperature superconductivity. For example, they can lead to a very direct interpretation of the isotope effect.\n\nRecently, bipolarons have been found in a Bose-Einstein condensate. Two polarons interchange sound waves and they attract to each other, forming a bound-state when the strength coupling between the single polarons and the condensate is strong in comparison with the interactions of the host gas.\n\nIn organic chemistry, a bipolaron is a molecule or part of a macromolecular chain containing two positive charges in a conjugated system. The charges can be located in the centre of the chain or at its termini. Bipolarons and polarons are encountered in doped conducting polymers such as polythiophene.\n\nIt is possible to synthesize and isolate bipolaron model compounds for X-ray diffraction studies. The diamagnetic bis(triaryl)amine dication 2 in \"scheme 1\" is prepared from the neutral precursor 1 in dichloromethane by reaction with 4 equivalents of antimony pentachloride. Two resonance structures exist for the dication. Structure 2a is a (singlet) diradical and 2b is the closed shell quinoid. The experimental bond lengths for the central vinylidene group in 2 are 141 pm and 137 pm compared to 144 pm and 134 pm for the precursor 1 implying some contribution from the quinoid structure.\n\nOn the other hand, when a thiophene unit is added to the core in the structure depicted in \"scheme 2\", these bond lengths are identical (around 138 pm) making it a true hybrid.\n\n"}
{"id": "8819939", "url": "https://en.wikipedia.org/wiki?curid=8819939", "title": "Business Action for Energy", "text": "Business Action for Energy\n\nBusiness Action for Energy (BAE) is a business network set up to give input to the UN Commission on Sustainable Development.\n\nThe Business Action for Sustainable Development (BASD) was a temporary initiative developed around the World Summit on Sustainable Development (WSSD, Johannesburg, 2002). This initiative was a resounding success and this model for positive business co-operation has since received much support around the world. \n\nBusinesses positive involvement must be continued in the international debate and on-the-ground projects, through partnerships and initiatives that can make a significant contribution to the achievement of the Millennium Development Goals and the Johannesburg Declaration.\n\nSet up in January 2005 by three founding organizations - the International Chamber of Commerce (ICC), the World Business Council for Sustainable Development (WBCSD) and the World Energy Council (WEC) - Business Action for Energy (BAE) is an ad hoc, temporary business initiative bringing together a comprehensive network of businesses, large and small, from around the world, through the representation of their associations, drawn from many sectors and regions.\n\nBAE has been established to coordinate business input into the 14th and 15th sessions (May 2006 and 2007) of the UN Commission on Sustainable Development (UNCSD), which will focus on energy, industrial development, atmosphere and climate change, and to other important forums in that timeframe. \n\nIt will do so by coordinating the delivery of international business views, positions, or recommendations, and by communicating the depth and variety of activities and projects undertaken by businesses, often in partnership with governments, international, regional and local agencies, and NGOs, in pursuit of a more sustainable energy future. \n\nBAE brings together international, regional and sector organizations, representing oil, gas, nuclear, coal, electricity and other major energy producers and consumers. At the national level, members include national committees, chambers, chapters of international business organizations and sector associations. The current list of confirmed participating organizations, including the founding organizations, are the:\n\n\n"}
{"id": "1653007", "url": "https://en.wikipedia.org/wiki?curid=1653007", "title": "Cellularization", "text": "Cellularization\n\nThe theory of cellularization, also known as Ciliate-acoel theory, is one of the theories explaining the origin of the metazoans. It was first based on Ernst Haeckel's assumption that these beasts are formed from the jaws of fish and that the earliest animals derived from ciliate protozoans. Haeckel later abandoned this idea which is revived by Hadzi in 1953. According to the theory, a multinucleated unicellular ciliate ancestor would give rise to organisms similar to modern turbellarian flatworms by cellularization of the external layer. Recent molecular and morphologic data add increasing evidence against this view, and the alternative colonial theory, also proposed by Haeckel in the 1870s is gaining widespread acceptance.\n\n"}
{"id": "79900", "url": "https://en.wikipedia.org/wiki?curid=79900", "title": "Cydippe", "text": "Cydippe\n\nThe name Cydippe (Ancient Greek: Κυδίππη, \"Kudíppē\") is attributed to four individuals in Greek mythology.\n\n\n"}
{"id": "873829", "url": "https://en.wikipedia.org/wiki?curid=873829", "title": "Derecho", "text": "Derecho\n\nA derecho (, from , \"straight\") is a widespread, long-lived, straight-line wind storm that is associated with a land-based, fast-moving group of severe thunderstorms.\n\nDerechos can cause hurricane-force winds, tornadoes, heavy rains, and flash floods. Convection-induced winds take on a bow echo (backward \"C\") form of squall line, forming in an area of wind divergence in upper levels of the troposphere, within a region of low-level warm air advection and rich low-level moisture. They travel quickly in the direction of movement of their associated storms, similar to an outflow boundary (gust front), except that the wind is sustained and increases in strength behind the front, generally exceeding hurricane-force.\n\nA warm-weather phenomenon, derechos occur mostly in summer, especially during June, July, and August in the Northern Hemisphere, within areas of moderately strong instability and moderately strong vertical wind shear. They may occur at any time of the year and occur as frequently at night as during the daylight hours.\n\n\"Derecho\" comes from the Spanish word in adjective form for \"straight\" (or \"direct\"), in contrast with a \"tornado\" which is a \"twisted\" wind. The word was first used in the \"American Meteorological Journal\" in 1888 by Gustavus Detlef Hinrichs in a paper describing the phenomenon and based on a significant derecho event that crossed Iowa on 31 July 1877.\n\nOrganized areas of thunderstorm activity reinforce pre-existing frontal zones, and can outrun cold fronts. The resultant mesoscale convective system (MCS) forms at the point of the best upper level divergence in the wind pattern in the area of best low level inflow. The convection then moves east and toward the equator into the warm sector, parallel to low-level thickness lines with the mean tropospheric flow. When the convection is strong linear or curved, the MCS is called a squall line, with the feature placed at the leading edge of the significant wind shift and pressure rise.\n\nDerechos are squall lines that are bow- or spearhead-shaped on weather radar and, thus, also are called \"bow echoes\" or \"spearhead radar echoes\". Squall lines typically bow out due to the formation of a mesoscale high pressure system which forms within the stratiform rain area behind the initial line. This high pressure area is formed due to strong descending motion behind the squall line, and could come in the form of a downburst. The size of the bow may vary, and the storms associated with the bow may die and redevelop.\n\nDuring the cold season within the Northern Hemisphere, derechos generally develop within a pattern of southwesterly winds at mid levels of the troposphere in an environment of low to moderate atmospheric instability (caused by heat and moisture near ground level or cooler air moving in aloft and measured by convective available potential energy) and high values of vertical wind shear ( within the lowest of the atmosphere). \n\nWarm season derechos in the Northern Hemisphere form in west to northwesterly flow at mid levels of the troposphere with moderate to high levels of instability. Derechos form within environments of low-level warm air advection and significant low-level moisture.\n\nIts common definition is a thunderstorm complex that produces a damaging wind swath of at least , featuring a concentrated area of convectively-induced wind gusts exceeding . According to the National Weather Service criterion, a derecho is classified as a band of storms that have winds of at least along the entire span of the storm front, maintained over a time span of at least six hours. Some studies add a requirement that no more than two or three hours separate any two successive wind reports. \n\nDerechos typically possess a high or rapidly increasing forward speed. They have a distinctive appearance on radar (known as a bow echo) with several unique features, such as the rear inflow notch and bookend vortices, and usually they manifest two or more downbursts.\n\nThere are four types of derechos:\n\n\nWinds in a derecho can be enhanced by downburst clusters embedded inside the storm. These straight-line winds may exceed , reaching in past events. Tornadoes sometimes form within derecho events, although such events are often difficult to confirm due to the additional damage caused by straight-line winds in the immediate area.\n\nWith the average tornado in the United States and Canada rating in the low end of the F/EF1 classification at peak winds and most or all of the rest of the world even lower, derechos tend to deliver the vast majority of extreme wind conditions over much of the territory in which they occur. Datasets compiled by the United States National Weather Service and other organizations show that a large swath of the north-central United States, and presumably at least the adjacent sections of Canada and much of the surface of the Great Lakes, can expect winds above 85 mph to as high as 120 mph (135 to 190 km/h) over a significant area at least once in any 50-year period, including both convective events and extra-tropical cyclones and other events deriving power from baroclinic sources. Only in 40 to 65 percent or so of the United States resting on the coast of the Atlantic basin, and a fraction of the Everglades, are derechos surpassed in this respect — by landfalling hurricanes, which at their worst may have winds as severe as EF3 tornadoes.\n\nCertain derecho situations are the most common instances of severe weather outbreaks which may become \"less\" favorable to tornado production as they become more violent; the height of 30–31 May 1998 upper Middle West-Canada-New York State derecho and the latter stages of significant tornado and severe weather outbreaks in 2003 and 2004 are only three examples of this. Some upper-air measurements used for severe-weather forecasting may reflect this point of diminishing return for tornado formation, and the mentioned three situations were instances during which the rare Particularly Dangerous Situation severe thunderstorm variety of severe weather watches were issued from the Storm Prediction Center of the U.S. National Oceanic & Atmospheric Administration.\n\nSome derechos develop a radar signature resembling that of a hurricane in the low levels. They may have a central \"eye\" free of precipitation, with a minimum central pressure and surrounding bands of strong convection, but are really associated with an MCS developing multiple squall lines, and are not tropical in nature. These storms have a warm core, like other mesoscale convective systems. One such derecho occurred across the midwestern U.S. on 21 July 2003. An area of convection developed across eastern Iowa near a weak stationary/warm front and ultimately matured, taking on the shape of a wavy squall line across western Ohio and southern Indiana. The system re-intensified after leaving the Ohio Valley, starting to form a large hook, with occasional hook echoes appearing along its eastern side. A surface low pressure center formed and became more impressive later in the day.\n\nDerechos in North America form predominantly from April to August, peaking in frequency from May into July. During this time of year, derechos are mostly found in the Midwestern United States and the U.S. Interior Highlands most commonly from Oklahoma and across the Ohio Valley. During mid-summer when a hot and muggy airmass covers the north-central U.S., they will often develop farther north into Manitoba or Northwestern Ontario, sometimes well north of the Canada–US border. \n\nNorth Dakota, Minnesota and upper Michigan are also vulnerable to derecho storms when such conditions are in place. They often occur along stationary fronts on the northern periphery of where the most intense heat and humidity bubble exists. Late-year derechos are normally confined to Texas and the Deep South, although a late-summer derecho struck upper parts of the New York State area after midnight on 7 September 1998. Warm season derechos have greater instability than their cold season counterpart, while cool season derechos have greater shear than their warm season counterpart.\n\nAlthough these storms most commonly occur in North America, derechos can occur elsewhere in the world, although infrequently. Outside North America, they sometimes are called by different names. For example, in Bangladesh and adjacent portions of India, a type of storm known as a \"Nor'wester\" may be a progressive derecho. One such event occurred on 10 July 2002 in Germany: a serial derecho killed eight people and injured 39 near Berlin. They have occurred in Argentina and South Africa as well, and on rarer occasions, close to or north of the 60th parallel in northern Canada. Primarily a mid-latitudes phenomenon, derechos do occur in the Amazon Basin of Brazil. On 8 August 2010, a derecho struck Estonia and tore off the tower of Väike-Maarja Church.\n\nUnlike other thunderstorms, which typically can be heard in the distance when approaching, a derecho seems to strike suddenly. Within minutes, extremely high winds can arise, strong enough to knock over highway signs and topple large trees. These winds are accompanied by spraying rain and frequent lightning from all directions. It is dangerous to drive under these conditions, especially at night, because of blowing debris and obstructed roadways. Downed wires and widespread power outages are likely but not always a factor. A derecho moves through quickly, but can do much damage in a short time.\n\nSince derechos occur during warm months and often in places with cold winter climates, people who are most at risk are those involved in outdoor activities. Campers, hikers, and motorists are most at risk because of falling trees toppled over by straight-line winds. Wide swaths of forest have been felled by such storms. People who live in mobile homes are also at risk; mobile homes that are not anchored to the ground may be overturned from the high winds. Across the United States, Michigan and New York have incurred a significant portion of the fatalities from derechos. Prior to Hurricane Katrina, the death tolls from derechos and hurricanes were comparable for the United States.\n\nDerechos may also severely damage an urban area's electrical distribution system, especially if these services are routed above ground. The derecho that struck Chicago, Illinois on 11 July 2011 left more than 860,000 people without electricity. The June 2012 North American derecho took out electrical power to more than 3.7 million customers starting in the Midwestern United States, across the central Appalachians, into the Mid-Atlantic States during a heat wave.\n\n\nNotes\nFurther reading\n\n"}
{"id": "8097930", "url": "https://en.wikipedia.org/wiki?curid=8097930", "title": "Eastern Mediterranean conifer-sclerophyllous-broadleaf forests", "text": "Eastern Mediterranean conifer-sclerophyllous-broadleaf forests\n\nThe Eastern Mediterranean conifer-sclerophyllous-broadleaf forests ecoregion, in the Mediterranean forests, woodlands, and scrub biome, is of the eastern Mediterranean Basin.\n\nThe ecoregion covers an area of , and covers portions of Israel, the Palestinian territories, Jordan, Lebanon, Syria, and Turkey.\n\nMajor plant communities in the ecoregion include broadleaf sclerophyllous shrublands (maquis), pine forests (chiefly of Aleppo Pine \"(Pinus halepensis)\" and Turkish Pine \"(Pinus brutia\")), and dry oak (\"Quercus\" spp.) woodlands and steppes.\n\n"}
{"id": "3780269", "url": "https://en.wikipedia.org/wiki?curid=3780269", "title": "Energy current", "text": "Energy current\n\nEnergy current is a flow of energy defined by the Poynting vector (), as opposed to normal current (flow of charge). It was originally postulated by Oliver Heaviside. It is also an informal name for Energy flux.\n\n\"Energy current\" is a somewhat informal term that is used, on occasion, to describe the process of energy transfer in situations where the transfer can usefully be viewed in terms of a flow. It is particularly used when the transfer of energy is more significant to the discussion than the process by which the energy is transferred. For instance, the flow of fuel oil in a pipeline could be considered as an energy current, although this would not be a convenient way of visualising the fullness of the storage tanks.\n\nThe units of energy current are those of power (W). This is closely related to energy flux, which is the energy transferred per unit area per unit time (measured in W/m).\n\nA specific use of the concept of energy current was promulgated by Oliver Heaviside in the last quarter of the 19th century. Against heavy resistance from the engineering community, Heaviside worked out the physics of signal velocity/impedance/distortion on telegraph, telephone, and undersea cables. He invented the inductor-loaded \"distortionless line\" later patented by Michael Pupin in the USA.\nBuilding on the concept of the Poynting vector, which describes the flow of energy in a transverse electromagnetic wave as the vector product of its electric and magnetic fields (), Heaviside sought to extend this by treating the transfer of energy due to the electric current in a conductor in a similar manner. In doing so he reversed the contemporary view of current, so that the electric and magnetic fields due to the current are the \"prime movers\", rather than being a result of the motion of the charge in the conductor.\n\nHeaviside's approach had some adherents at the time—enough, certainly, to quarrel with the \"traditionalists\" in print. However, the \"energy current\" view presented a number of difficulties, most notably that in asserting that the energy flowed in the electric and magnetic fields around the conductor the theory is unable to explain why the charge appears to flow in the conductor. Another major flaw is that electrical science and engineering are built on solutions of Maxwell's Equations in which the electric current - expressed through the current-density vector J – is a fundamental quantity, while a so-called 'energy current' does not appear. Moreover, there are no equivalent equations describing the physical behaviour of the Poynting vector on which the concept of energy current is based.\n\nAfter the discovery of the electron in 1897, the Drude model, which describes electrical conduction in metals, was developed very quickly. By associating the somewhat abstract concept of moving charge with the rather more concrete motion of the charged electrons, the Drude model effectively deals with the traditional \"charge current\" and the Heaviside \"energy current\" views simultaneously. \nWith this achievement of \"unification\", the energy current approach has largely lost favour, because in omitting the concepts related to conduction it has no direct model for (for example) Ohm's Law. In consequence it is less convenient to use than the \"traditional\" charge current approach, which defines the concepts of current, voltage, resistance, etc., as commonly used for electrical work.\n\nPoynting-flow diagrams are part of E&M engineering, transmission line theory, and antenna design, but rare in electronics texts.\n"}
{"id": "187933", "url": "https://en.wikipedia.org/wiki?curid=187933", "title": "Hugo Eckener", "text": "Hugo Eckener\n\nDr. Hugo Eckener (10 August 186814 August 1954) was the manager of the Luftschiffbau Zeppelin during the inter-war years, and also the commander of the famous \"Graf Zeppelin\" for most of its record-setting flights, including the first airship flight around the world, making him the most successful airship commander in history. He was also responsible for the construction of the most successful type of airships of all time. An anti-Nazi who was invited to campaign as a moderate in the German presidential elections, he was blacklisted by that regime and eventually sidelined.\n\nEckener was born in Flensburg as the first child of Johann Christoph Eckener from Bremen and Anna Lange, daughter of a shoemaker. As a youth he was judged an \"indifferent student\", and he spent summers sailing and winters ice skating.\n\nNevertheless, by 1892 under Professor Wilhelm Wundt, Eckener had earned a doctorate \"magna cum laude\" in what today might be deemed experimental psychology. at the University of Leipzig.\n\nEckener then began his military service in the Infantry Regiment 86 in Flensburg.\n\nEckener's early career was as a journalist and editor; by August 1893 he was working for the \"\"; in October 1897 he married Johanna, daughter of the publisher family Maaß. He later became a correspondent for the Frankfurter Zeitung in 1905 and 1906, whilst writing a book on the social effects of capitalism.\n\nAsked to cover the first flights of the Zeppelins LZ 1 and LZ 2, Eckener was critical of both airships' marginal performances, but praised Count Ferdinand von Zeppelin's dedication to his cause. Because several scientists and engineers had criticized his airship plans, the Count sought to speak to his critic and Eckener was so impressed by him that during October 1908 he agreed to be a part-time publicist for the Zeppelin Company. He became extremely interested in airships, and joined the company on a full-time basis.\n\nHis aptitude at flying was noticed early on in his career, and he became an airship captain, obtaining his airship license in 1911. However, when Eckener attempted his first flight on 16 May 1911 in the LZ 8, christened \"Deutschland II\", he decided to launch it in a strong wind which pushed the craft into the hangar wall, damaging it seriously. Nonetheless, he became a very successful airshipman.\n\nEckener was responsible for training most of Germany's airship pilots both during and after World War I. Despite his protestations, he was not allowed on operational missions due to his value as an instructor.\n\nAfter the War, Eckener succeeded Count Ferdinand von Zeppelin, who had died on 8 March 1917. After considerable conflict with Zeppelin's business manager, Alfred Colsman, who wanted to replace the production of airships with production of other (and likely more profitable) products, Eckener was able to keep the Zeppelin factory at Friedrichshafen on Bodensee (Lake Constance) in Württemberg, southern Germany, from being retooled. Colsman left the company soon afterwards.\n\nThe Treaty of Versailles had forbidden Germans to construct airships of the size needed to operate the profitable trans-Atlantic service that was Eckener's goal. However, after much skilful lobbying, he persuaded the US and German governments to allow the company to build LZ 126, later rechristened the \"USS Los Angeles (ZR-3)\", for the US Navy as part of Germany's war reparations. Eckener himself captained the airship on its delivery flight to Lakehurst, New Jersey. The \"Los Angeles\" became the longest-serving rigid airship ever operated by the US Navy.\n\nRefused funds by the penniless Weimar government, Eckener and his colleagues began a nationwide fund-raising lecture tour in order to commence construction of \"Graf Zeppelin\", which became the most successful rigid airship ever built.\n\nThe first flight to America was fraught with drama. Near Bermuda on the outbound flight the airship was nearly lost after becoming caught in a severe storm during which off the left fin. The ship was saved only by Eckener's skilled piloting and the courage of his son, Knut Eckener, and other crew members who climbed out onto the fin to repair the damage. Upon arrival in America, a country which Eckener grew to love, he and the crew were subject to the first of two New York ticker tape parades.\n\nEckener captained \"Graf Zeppelin\" during most of its record-setting flights, including the 1928 first intercontinental passenger airship flight, the 1929 flight around the world (the only such flight by an airship, and the second by an aircraft of any type) and the 1931 Arctic flight.\n\nA master of publicity as well as a master airship captain, Eckener used the \"Graf Zeppelin\" to establish the Zeppelin as a symbol of German pride and engineering.\n\nAfter these flights the public treated Eckener as a national hero. During the early 1930s, Eckener was one of the most well-known and respected figures in Weimar Republic Germany. In the 1932 presidential election Eckener was a potential unity candidate against Adolf Hitler, encouraged to campaign by leaders of both the SPD and the Zentrum, but he bowed out when Paul von Hindenburg decided to run for a second term. However, his potential candidacy had already angered the Nazi party. In supposed anger and fear of Eckener, Hitler's de facto deputy, Hermann Esser, once called him the \"director of the flying weisswurst\", a greyish-white Bavarian sausage.\n\nThe Nazis came to power in January 1933. A planned arrest of Eckener in 1933 was blocked by Hindenburg. Hitler met Eckener only once, in July 1933, but the two barely spoke. Eckener did not make any secret of his dislike of the Nazis and the disastrous events he foresaw. He criticised the regime frequently, and refused to allow the Nazis to use the large hangars at Frankfurt for a rally. Eventually the Nazis declared Eckener to be \"persona non grata\" and his name was no longer allowed to appear in print.\n\nDuring the 1930s, the Nazi government nationalized the Zeppelin operation under the name Deutsche Zeppelin-Reederei GmbH (DZR). The Nazis sidelined Eckener in favour of men who were more compliant with their wishes. In their haste to please the Nazi regime, these newly promoted airshipmen did not always obey Eckener's safety procedures. For example, the maiden voyage of the Hindenburg nearly resulted in disaster when Captain Ernst Lehmann brought the ship out in strong winds in order to undertake a Nazi propaganda flight. The ship was damaged, and there was an argument between Eckener, Lehmann and the Nazi propaganda ministry.\n\nHugo Eckener had always made safety his absolute priority during his many years managing airship operations. With Eckener's management, the Zeppelin company had a perfect safety record with no passenger ever sustaining a serious injury on any of the more than 1 million air miles that the rigid airships flew, until the Hindenburg disaster of 1937. . Eckener was in Graz, Austria when he heard news of the Hindenburg disaster on 6 May 1937. In the official inquiry he concluded that a static spark ignited leaking hydrogen in the aft section of the ship. The leak would have been caused by a sharp turn, which he believed overstrained a bracing wire, causing it to snap and rip open an adjacent gas cell.\n\nAfter the destruction of the \"Hindenburg\", the nearly-completed LZ-130 \"Graf Zeppelin\" was redesigned as a helium-filled ship, although, owing to geo-political considerations, the American helium was not available.. Thus the ship never began commercial service. However, under the command of Captain Albert Sammt, who had previously survived the fiery destruction of the Hindenburg, albeit with severe burns, the ship performed an espionage mission off the coast Great Britain, intended to investigate the radar defences. Eckener, however, had by this time little influence on the Zeppelin Company.\n\nEckener survived World War II despite his disagreements with the Nazis. Post war, he was involved in a plan by the Goodyear Zeppelin Corporation to build large rigid airships. However, nothing came to pass of this project.\n\nIn 1945, Johannes Weyl and Eckener co-founded the \"\" regional newspaper and Eckener started writing for German-French co-operation. In November 1945 Eckener was confronted with the charge of collaboration with Nazi Germany. In 1947 the French occupying powers fined him 100,000 German Reichsmarks. Many personalities lobbied for Eckener's rehabilitation. The judgement was rejected in July 1948 and Eckener was rehabilitated.\n\nEckener's home town of Flensburg had a Danish-oriented majority in its council since 1945, with the goal of unification with Denmark. Eckener remained active in local politics campaigning for a German majority in Flensburg, while at the same time, during a \"thundering\" one-hour speech in 1951, warning against small-mindedness in border concerns.\n\nEckener died in Friedrichshafen on 14 August 1954 just after his 86th birthday.\n\nEckener was responsible for many innovative aviation developments, notably the trans-Atlantic passenger services offered by the airships \"Graf Zeppelin\" and \"Hindenburg\". \n\nSince his death his achievements have been remembered by airship enthusiasts and historians. Additionally, the town of Friedrichshafen, scene of his many triumphant homecomings in \"Graf Zeppelin\", has recognised his memory by naming a large new conference centre after him.\n\nEckener wrote or contributed to 24 publications, including two books in English: \n\nEckener features as a character in the 2012 novel \"Flight from Berlin\" by David John and in the novels \"Vango: Between Sky and Earth\" (2010) and \"A Prince Without Kingdom\" (2011) by Timothée de Fombelle.\n\n\n\n"}
{"id": "31789317", "url": "https://en.wikipedia.org/wiki?curid=31789317", "title": "Indian Energy Exchange", "text": "Indian Energy Exchange\n\nThe Indian Energy Exchange (IEX) is an electronic system based power trading exchange regulated by the Central Electricity Regulatory Commission (CERC). IEX started its operations on June 27, 2008. Indian Energy Exchange pioneered the development of power trading in India and provides an electronic platform to the various participants in power market, comprising State Electricity Boards, Power producers, Power Traders and Open Access Consumers (both Industrial & Commercial).\n\nIEX is one of the two operational Power Exchanges in India. Ever since its incorporation, it has held an influential market share. IEX operates a day-ahead market based on closed auctions with double-sided bidding and uniform pricing; it has over 3,800 registered clients, over 300 private generators and more than 3,300 industrial electricity consumers.\n\nFinancial Technologies (India) Limited (FTIL) made an impulsive thrust into India's Power Sector by launching Indian Energy Exchange, along with Power Trading Corporation of India Limited. The aim of the promoters was to magnify the unexplored potential of Indian Power market and abridge the demand supply gap. Thereby creating a marketplace wherein players from different purposes may garner the business potential.\n\nThe concept of power exchange in India had its genesis in the year of 2005, post which, Financial Technologies Limited and Multi Commodity Exchange together yielded an application for setting up the nation's first power exchange. Since its development on June 9, 2008, IEX had recorded 37.4 million transactions, the same month that electricity prices on the exchange hit a record low 13 paisa/KWh. In November 2009, IEX won Best E-Enabled Customer Platform at the second annual India Power Awards for developing what was described as \"a robust platform for energy trading. \"\n\nIEX was envisioned as a neutral platform where participants from different segments of the sector may participate and utilize the range of products and services to match their needs. The participants are required to bear a membership and participate in the bid for the respective product/service. The exchange has launched a range of products and are broadly classified as:\n\nDay Ahead\n\nTerm Ahead\n\nRenewable Energy Certificates\n\nEnergy Saving Certificates(Es Certs)\n\n"}
{"id": "14882004", "url": "https://en.wikipedia.org/wiki?curid=14882004", "title": "International Global Atmospheric Chemistry", "text": "International Global Atmospheric Chemistry\n\nThe International Global Atmospheric Chemistry (IGAC) project is a non-profit organization created in the late 1980s to address growing international concerns over rapid changes observed in Earth's atmosphere. \n\nIt developed under joint sponsorship of the Commission on Atmospheric Chemistry and Global Pollution (CACGP) of the International Association of Meteorology and Atmospheric Sciences (IAMAS), and the International Geosphere-Biosphere Programme (IGBP).\n\n"}
{"id": "15056", "url": "https://en.wikipedia.org/wiki?curid=15056", "title": "Isoelectric point", "text": "Isoelectric point\n\nThe isoelectric point (pI, pH(I), IEP), is the pH at which a particular molecule carries no net electrical charge or is electrically neutral in the statistical mean. The standard nomenclature to represent the isoelectric point is pH(I), although pI is also commonly seen, and is used in this article for brevity. The net charge on the molecule is affected by pH of its surrounding environment and can become more positively or negatively charged due to the gain or loss, respectively, of protons (H).\n\nSurfaces naturally charge to form a double layer. In the common case when the surface charge-determining ions are H/OH, the net surface charge is affected by the pH of the liquid in which the solid is submerged.\n\nThe pI value can affect the solubility of a molecule at a given pH. Such molecules have minimum solubility in water or salt solutions at the pH that corresponds to their pI and often precipitate out of solution. Biological amphoteric molecules such as proteins contain both acidic and basic functional groups. Amino acids that make up proteins may be positive, negative, neutral, or polar in nature, and together give a protein its overall charge. At a pH below their pI, proteins carry a net positive charge; above their pI they carry a net negative charge. Proteins can, thus, be separated by net charge in a polyacrylamide gel using either preparative gel electrophoresis, which uses a constant pH to separate proteins or isoelectric focusing, which uses a pH gradient to separate proteins. Isoelectric focusing is also the first step in 2-D gel polyacrylamide gel electrophoresis.\n\nIn biomolecules, proteins can be separated by ion exchange chromatography. Biological proteins are made up of zwitterionic amino acid compounds; the net charge of these proteins can be positive or negative depending on the pH of the environment. The specific pI of the target protein can be used to model the process around and the compound can then be purified from the rest of the mixture. Buffers of various pH can be used for this purification process to change the pH of the environment. When a mixture containing a target protein is loaded into an ion exchanger, the stationary matrix can be either positively-charged (for mobile anions) or negatively-charged (for mobile cations). At low pH values, the net charge of most proteins in the mixture is positive - in cation exchangers, these positively-charged proteins bind to the negatively-charged matrix. At high pH values, the net charge of most proteins is negative, where they bind to the positively-charged matrix in anion exchangers. When the environment is at a pH value equal to the protein's pI, the net charge is zero, and the protein is not bound to any exchanger, and therefore, can be eluted out.\n\nFor an amino acid with only one amine and one carboxyl group, the pI can be calculated from the mean of the pKas of this molecule.\n\nThe pH of an electrophoretic gel is determined by the buffer used for that gel. If the pH of the buffer is above the pI of the protein being run, the protein will migrate to the positive pole (negative charge is attracted to a positive pole). If the pH of the buffer is below the pI of the protein being run, the protein will migrate to the negative pole of the gel (positive charge is attracted to the negative pole). If the protein is run with a buffer pH that is equal to the pI, it will not migrate at all. This is also true for individual amino acids.\n\nIn the two examples (on the right) the isoelectric point is shown by the green vertical line. In glycine the pK values are separated by nearly 7 units so the concentration of the neutral species, glycine (GlyH), is effectively 100% of the analytical glycine concentration. Glycine may exist as a zwitterion at the isoelectric point, but the equilibrium constant for the isomerization reaction in solution\nis not known.\n\nThe other example, adenosine monophosphate is shown to illustrate the fact that a third species may, in principle, be involved. In fact the concentration of (AMP)H is negligible at the isoelectric point in this case.\nIf the pI is greater than the pH, the molecule will have a positive charge.\n\nA number of algorithms for estimating isoelectric points of peptides and proteins have been developed. Most of them use Henderson–Hasselbalch equation with different pK values. For instance, within the model proposed by Bjellqvist and co-workers the pK's were determined between closely related immobilines, by focusing the same sample in overlapping pH gradients. Some improvements in the methodology (especially in the determination of the pK values for modified amino acids) have been also proposed. More advanced methods take into account the effect of adjacent amino acids ±3 residues away from a charged aspartic or glutamic acid, the effects on free C terminus, as well as they apply a correction term to the corresponding pK values using genetic algorithm. Other recent approaches are based on a support vector machine algorithm and pKa optimization against experimentally known protein/peptide isoelectric points.\n\nMoreover, experimentally measured isoelectric point of proteins were aggregated into the databases. Recently, a database of isoelectric points for all proteins predicted using most of the available methods had been also developed.\n\nThe isoelectric points (IEP) of metal oxide ceramics are used extensively in material science in various aqueous processing steps (synthesis, modification, etc.). In the absence of chemisorbed or physisorbed species particle surfaces in aqueous suspension are generally assumed to be covered with surface hydroxyl species, M-OH (where M is a metal such as Al, Si, etc.). At pH values above the IEP, the predominate surface species is M-O, while at pH values below the IEP, M-OH species predominate. Some approximate values of common ceramics are listed below:\n\n\"Note: The following list gives the isoelectric point at 25 °C for selected materials in water. The exact value can vary widely, depending on material factors such as purity and phase as well as physical parameters such as temperature. Moreover, the precise measurement of isoelectric points can be difficult, thus many sources often cite differing values for isoelectric points of these materials.\"\n\nMixed oxides may exhibit isoelectric point values that are intermediate to those of the corresponding pure oxides. For example, a synthetically prepared amorphous aluminosilicate (AlO-SiO) was initially measured as having IEP of 4.5 (the electrokinetic behavior of the surface was dominated by surface Si-OH species, thus explaining the relatively low IEP value). Significantly higher IEP values (pH 6 to 8) have been reported for 3AlO-2SiO by others. Similarly, also IEP of barium titanate, BaTiO was reported in the range 5-6 while others got a value of 3. Mixtures of titania (TiO) and zirconia (ZrO) were studied and found to have an isoelectric point between 5.3-6.9, varying non-linearly with %(ZrO). The surface charge of the mixed oxides was correlated with acidity. Greater titania content led to increased Lewis acidity, whereas zirconia-rich oxides displayed Br::onsted acidity. The different types of acidities produced differences in ion adsorption rates and capacities.\n\nThe terms isoelectric point (IEP) and point of zero charge (PZC) are often used interchangeably, although under certain circumstances, it may be productive to make the distinction.\n\nIn systems in which H/OH are the interface potential-determining ions, the point of zero charge is given in terms of pH. The pH at which the surface exhibits a neutral net electrical charge is the point of zero charge at the surface. Electrokinetic phenomena generally measure zeta potential, and a zero zeta potential is interpreted as the point of zero net charge at the shear plane. This is termed the isoelectric point. Thus, the isoelectric point is the value of pH at which the colloidal particle remains stationary in an electrical field. The isoelectric point is expected to be somewhat different than the point of zero charge at the particle surface, but this difference is often ignored in practice for so-called pristine surfaces, i.e., surfaces with no specifically adsorbed positive or negative charges. In this context, specific adsorption is understood as adsorption occurring in a Stern layer or chemisorption. Thus, point of zero charge at the surface is taken as equal to isoelectric point in the absence of specific adsorption on that surface.\n\nAccording to Jolivet, in the absence of positive or negative charges, the surface is best described by the point of zero charge. If positive and negative charges are both present in equal amounts, then this is the isoelectric point. Thus, the PZC refers to the absence of any type of surface charge, while the IEP refers to a state of neutral net surface charge. The difference between the two, therefore, is the quantity of charged sites at the point of net zero charge. Jolivet uses the intrinsic surface equilibrium constants, p\"K\" and p\"K\" to define the two conditions in terms of the relative number of charged sites:\n\nFor large Δp\"K\" (>4 according to Jolivet), the predominant species is MOH while there are relatively few charged species - so the PZC is relevant. For small values of Δp\"K\", there are many charged species in approximately equal numbers, so one speaks of the IEP.\n\n\n\n"}
{"id": "39474796", "url": "https://en.wikipedia.org/wiki?curid=39474796", "title": "Latin Bloc (proposed alliance)", "text": "Latin Bloc (proposed alliance)\n\nThe Latin Bloc (Italian: \"Blocco Latino\", French: \"Bloc Latin\", Spanish: \"Bloque Latino\", Portuguese: \"Bloco Latino\") was a proposal for an alliance made the 1920s to the 1940s that began with Italy's \"Duce\" Benito Mussolini proposing such a bloc in 1927 between Italy, France, Spain, and Portugal, that would be an alliance based upon common Latin civilization and culture. The proposal was publicly discussed between the governments of Italy, Spain, and France, during World War II.\n\nIn the 1930s, French Prime Minister Pierre Laval alongside French conservatives expressed support for a Latin Bloc with Italy and Spain. \n\nDuring World War II the proposal was discussed between Mussolini, Spain's \"Caudillo\" Francisco Franco, and Vichy France's head of state Philippe Petain. However the alliance failed to materialize. The planned bloc would have united Italy, France, Spain, Portugal, and Vatican City together as a bloc alliance based upon unity of the Latin culture European states that would be within the Axis powers that was designed to balance the power between them and Germany in the Axis by combining together. The main effort was to create a \"Rome-Madrid axis\", Franco took a major role in promoting the proposal, and Franco with Vichy French leader Petain in Montpellier, France in 1940 to discuss the proposal, and Franco met with Mussolini in Bordighera, Italy in 1941 to discuss it. Germany supported the proposal for the Latin Bloc during World War II and German propaganda assisted Italian propaganda in promoting the bloc However the alliance failed to materialize. Germany's \"Führer\" Adolf Hitler promoted the Latin Bloc and in October 1940, travelled to Hendaye, France on the border with Spain to meet Franco in which he promoted Spain forming a Latin bloc with Italy and Vichy France to join Italy's fight against Britain in the Mediterranean region. \n\n"}
{"id": "16925974", "url": "https://en.wikipedia.org/wiki?curid=16925974", "title": "List of Baeckea species", "text": "List of Baeckea species\n\nThis is a list of Baeckea species.\n\n"}
{"id": "2446119", "url": "https://en.wikipedia.org/wiki?curid=2446119", "title": "List of Minnesota state forests", "text": "List of Minnesota state forests\n\nMinnesota State Forests are State forests located within the U.S. State of Minnesota. The 59 state forests were established by the Minnesota Legislature in order to conserve and manage the forest resources, including: Timber management, Wildlife management, Water resources management, and Public recreation. Acreage of Minnesota's State Forests is over .\n\nMinnesota's state forests are generally managed by the Minnesota Department of Natural Resources, Division of Forestry - headquartered in Saint Paul, Minnesota. Some forest land is managed entirely or in-part by the counties in which they are located in, or by the United States Forest Service in cases where state forests are located within the boundaries of either Chippewa National Forest or Superior National Forest. \n\nThe following is a list of state forests in Minnesota:\n\n\n\n"}
{"id": "7787909", "url": "https://en.wikipedia.org/wiki?curid=7787909", "title": "List of Southern Hemisphere tornadoes and tornado outbreaks", "text": "List of Southern Hemisphere tornadoes and tornado outbreaks\n\n\"Parent article:\" List of tornadoes and tornado outbreaks\n\nThese are some notable tornadoes, tornado outbreaks, and tornado outbreak sequences that have occurred in the Southern Hemisphere including Oceania, and, for the purposes of this list, all of South America and Africa.\n\n\n\n"}
{"id": "30741901", "url": "https://en.wikipedia.org/wiki?curid=30741901", "title": "List of books about renewable energy", "text": "List of books about renewable energy\n\nThis is a bibliography of renewable energy.\n\nRenewable energy is energy which comes from natural resources such as sunlight, wind, rain, tides, and geothermal heat, which are renewable (naturally replenished). About 16% of global final energy consumption comes from renewables, with 10% coming from traditional biomass, which is mainly used for heating, and 3.4% from hydroelectricity. New renewables (small hydro, modern biomass, wind, solar, geothermal, and biofuels) account for another 3% and are growing very rapidly.\n\nTotal investment in renewable energy reached $244 billion in 2012. The top countries for investment in recent years were China, Germany, Spain, the United States, Italy, and Brazil. Leading renewable energy companies include BrightSource Energy, Enercon, First Solar, Gamesa, GE Energy, Goldwind, Nordex, Sinovel, Suntech, Trina Solar, Vestas and Yingli.\n\n\nAcademic journals\n\nOther\n\nBook of Solar Power Systems Design: From the Sun into Electricity Website.\n"}
{"id": "309910", "url": "https://en.wikipedia.org/wiki?curid=309910", "title": "List of chordate orders", "text": "List of chordate orders\n\nThis page contains a list of all of the classes and orders that are located in the Phylum Chordata.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "20478586", "url": "https://en.wikipedia.org/wiki?curid=20478586", "title": "List of energy abbreviations", "text": "List of energy abbreviations\n\nThis is a list of acronyms found in the context of energy issues.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe following table lists a number of terms that are used in the United States for residential energy audits.\n\n\n\n"}
{"id": "1999546", "url": "https://en.wikipedia.org/wiki?curid=1999546", "title": "List of environmental disasters", "text": "List of environmental disasters\n\nThis page is a list of environmental disasters. In this context it is an annotated list of specific events caused by human activity that results in a negative effect on the environment.\n\n\n\n\nCoordinates of the Industrial Environmental Disasters found on this page, shown in Google. Complete with the Wikipedia descriptions listed below built into each location.\n\n\n\n\n\n\n\n\n\n"}
{"id": "35282712", "url": "https://en.wikipedia.org/wiki?curid=35282712", "title": "List of forestry journals", "text": "List of forestry journals\n\nThis list includes representative academic, peer-reviewed journals in forestry, forest science and related fields. Included are several historic but still-publishing journals of forestry, traditional scientific forestry journals, and several newer, open-access journals. More than 180 forestry journals were being published in 2008.\n\nForestry journals established more than a century ago and still publishing include:\n\n\nContemporary, peer-reviewed scientific journals of forestry include:\n\nOpen access forestry journals include:\n\n\nForestry journals that have ceased publication or merged with other journals include:\n\n\n\n\n"}
{"id": "46877842", "url": "https://en.wikipedia.org/wiki?curid=46877842", "title": "List of mountains in Sweden", "text": "List of mountains in Sweden\n\nSweden has 12 peaks above 2000 metres in height.\n\nSweden, Highest elevation per historical province:\n"}
{"id": "13974860", "url": "https://en.wikipedia.org/wiki?curid=13974860", "title": "List of prehistoric foraminifera genera", "text": "List of prehistoric foraminifera genera\n\nThis is a list of fossil genera of foraminiferans.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "22903217", "url": "https://en.wikipedia.org/wiki?curid=22903217", "title": "List of rivers of Uruguay", "text": "List of rivers of Uruguay\n\nThis is a list of rivers in Uruguay. This list is arranged by drainage basin, with respective tributaries indented under each larger stream's name. All rivers in Uruguay drain to the Atlantic Ocean.\n\n\n"}
{"id": "6193118", "url": "https://en.wikipedia.org/wiki?curid=6193118", "title": "List of stars with proplyds", "text": "List of stars with proplyds\n\nThis is a list of stars with proplyds (protoplanetary discs) and includes whether they have gaps or planets.\n\n\"(list is very incomplete)\" \n\"When planets are confirmed (if no planet before), please move to the List of extrasolar planets\"\n\nThis list contains the largest, smallest, oldest, youngest.\n\nThis list contains firsts.\n\n\n"}
{"id": "12889755", "url": "https://en.wikipedia.org/wiki?curid=12889755", "title": "Maria Sinukuan", "text": "Maria Sinukuan\n\nApúng Sinukuan, is the Kapampangan sun god of war and death who lived in Mount Arayat. During the colonial period, the Spanish rebranded him into Maria Sinukuan, the diwata (fairy) or mountain goddess associated with Mount Arayat in Pampanga, Philippines, and later became a prominent example of the mountain-goddesses motif in Philippine mythology, other prominent examples being Maria Makiling of Los Baños and Maria Cacao of Cebu.\n\nPrior to Spanish colonization, Sinukuan was known as a powerful male Kapampangan god, named Aring Sinukûan, who was in par with the Kapampangan god of Pinatubo, Apûng Malyari. The two were the second most powerful deities in Kampampangan mythology, next only to Mangechay (sometimes called Mangacha), the great elder and creator goddess. Aring Sinukûan was the sun god of war and death, taught the early inhabitants the industry of metallurgy, wood cutting, rice culture and waging war. He had three children, namely, Munag Sumalâ, the golden serpent god who represent dawn, Lakandanup, the god of gluttony who represented the sun at noon time, and Gatpanapun, the noble god who only knew pleasure and represented the afternoon. He also had a winged assistant named Galurâ, a giant eagle deity believed to be the bringer of storms, and a wife named Mingan. However, when the Spanish arrived, they 'rebranded' Sinukuan as a woman, thinking that the people would not revere the deity if he was a female, not knowing that the great elder deity of the Kapampangan was a woman named Mangechay. Additionally, Sinukuan's wife, Mingan, was rebranded as 'male'. Despite this, the natives continued to revere Sinukuan. Furious, the Spanish added 'Maria' to Sinukuan's name to somewhat turn her into Catholic in a bid to further subjugate the natives and convert them into Roman Catholicism.\n\nThe basic legend is similar to those of many mountain guardian goddesses, notably \"Maria Makiling\".\n\nSinukuan is associated with the unusual bounty of the forests in Arayat, and with the profusion of animals there. Watching over the needs of the people in the nearby town, she used to regularly leave fruits and animals at the doorstep of locals who needed food during hard times. At one point, though, a group of young men got greedy. They sought out where Sinukuan’s home was in the mountains, and when they found it, they asked for more than what they actually needed. Sinukuan did not object to this, and allowed them to pick a great load of fruits. She warned them, however, not to get any fruits from the forest without her permission. On their way back home, they decided they would get more. Why not? They asked each other. “She won't know we took home fruits and animals. They're so plentiful, she won't know the difference.\" But she did. As soon as they had started picking more fruit, their packs began to feel heavier. They soon discovered that all the fruit and meat they were carrying had turned into rocks. The young men ran away, but before they managed to escape the forest, Sinukuan appeared before them. As punishment, she said, she would turn them into swine. And so she did.\n\nBut the other people in the village were also getting greedy. More and more, they stole from Sinukuan’s forests. Angered, Sinukuan stopped leaving food at their doorsteps. She made the fruit trees and animals in the mountain to disappear. And she also never allowed the villagers to see her again.\n\nLocal tradition describes Sinukuan, the Spanish-imposed female counterpart, as:\n\nSinukuan or SINyang, KUnnie, and ANing is a fictional character in Dyosa series, a telefantasya being aired by ABS-CBN Network portrayed by Filipino actress Mickey Ferriols.\n\nMariang Sinukuan is a Fictional Character Voiced By \"Kylie Padilla\" With her Animals Kuneho \"Pekto\" and Hunyango \"John Feir\" in \"Alamat\" by GMA Network\n\n"}
{"id": "21186", "url": "https://en.wikipedia.org/wiki?curid=21186", "title": "Northwest Territories", "text": "Northwest Territories\n\nThe Northwest Territories (NT or NWT; French: \"les Territoires du Nord-Ouest\", \"TNO\"; Athabaskan languages: \"Denendeh\"; Inuinnaqtun: \"Nunatsiaq\"; Inuktitut: ᓄᓇᑦᓯᐊᖅ) is a federal territory of Canada. At a land area of approximately and a 2011 population of 41,462, it is the second-largest and the most populous of the three territories in Northern Canada. Its estimated population as of 2016 is 44,291. Yellowknife became the territorial capital in 1967, following recommendations by the Carrothers Commission.\n\nThe Northwest Territories, a portion of the old North-Western Territory, entered the Canadian Confederation on July 15, 1870, but the current borders were formed on April 1, 1999, when the territory was subdivided to create Nunavut to the east, via the \"Nunavut Act\" and the \"Nunavut Land Claims Agreement\". While Nunavut is mostly Arctic tundra, the Northwest Territories has a slightly warmer climate and is both boreal forest (taiga), and tundra, and its most northern regions form part of the Canadian Arctic Archipelago.\n\nThe Northwest Territories is bordered by Canada's two other territories, Nunavut to the east and Yukon to the west, and by the provinces of British Columbia, Alberta, and Saskatchewan to the south.\n\nThe name is descriptive, adopted by the British government during the colonial era to indicate where it lay in relation to Rupert's Land. It is shortened from North-Western Territory (\"see\" History). In Inuktitut, the Northwest Territories are referred to as ᓄᓇᑦᓯᐊᖅ (\"Nunatsiaq\"), \"beautiful land.\"\n\nThere was some discussion of changing the name of the Northwest Territories after the splitting off of Nunavut, possibly to a term from an Aboriginal language. One proposal was \"Denendeh\" (an Athabaskan language word meaning \"our land\"), as advocated by the former premier Stephen Kakfwi, among others. One of the most popular proposals for a new name – one to name the territory \"Bob\" – began as a prank, but for a while it was at or near the top in the public-opinion polls.\n\nIn the end a poll conducted prior to division showed that strong support remained to keep the name \"Northwest Territories\". This name arguably became more appropriate following division than it had been when the territories extended far into Canada's north-central and northeastern areas.\n\nLocated in northern Canada, the territory borders Canada's two other territories, Yukon to the west and Nunavut to the east, and three provinces: British Columbia to the southwest, and Alberta and Saskatchewan to the south. It possibly meets Manitoba at a quadripoint to the extreme southeast, though surveys have not been completed. It has a land area of .\n\nGeographical features include Great Bear Lake, the largest lake entirely within Canada, and Great Slave Lake, the deepest body of water in North America at , as well as the Mackenzie River and the canyons of the Nahanni National Park Reserve, a national park and UNESCO World Heritage Site. Territorial islands in the Canadian Arctic Archipelago include Banks Island, Borden Island, Prince Patrick Island, and parts of Victoria Island and Melville Island. Its highest point is Mount Nirvana near the border with Yukon at an elevation of .\n\nThe Northwest Territories extends for more than and has a large climate variant from south to north. The southern part of the territory (most of the mainland portion) has a subarctic climate, while the islands and northern coast have a polar climate.\n\nSummers in the north are short and cool, with daytime highs of 14-17 Celsius (60° to 70 °F), and lows of 1-5 Celsius (45° to 55 °F). Winters are long and harsh, daytime highs in the mid and lows around . Extremes are common with summer highs in the south reaching and lows reaching into the negatives. In winter in the south, it is not uncommon for the temperatures to reach , but they can also reach the low teens during the day. In the north, temperatures can reach highs of , and lows can reach into the low negatives.\nIn winter in the north it is not uncommon for the temperatures to reach but they can also reach the single digits during the day. Thunderstorms are not rare in the south. In the north they are very rare, but do occur. Tornadoes are extremely rare but have happened with the most notable one happening just outside Yellowknife that destroyed a communications tower. The Territory has a fairly dry climate due to the mountains in the west.\n\nAbout half of the territory is above the tree line. There are not many trees in most of the eastern areas of the territory, or in the north islands.\n\nThe present-day territory came under government authority in July 1870, after the Hudson's Bay Company transferred Rupert's Land and the North-Western Territory to the British Crown, which subsequently transferred them to the Dominion of Canada, giving it the name the North-west Territories. This immense region comprised all of today's Canada except that which was encompassed within the early signers of Canadian Confederation, that is, British Columbia, early forms of present-day Ontario and Quebec (which encompassed the coast of the Great Lakes, the Saint Lawrence River valley and the southern third of Quebec), the Maritimes (NS, PEI and NB), Newfoundland, the Labrador coast, and the Arctic Islands, except the southern half of Baffin Island (the Arctic Islands remained under direct British claim until 1880).\n\nAfter the 1870 transfer, some of the North-west Territories was whittled away. The province of Manitoba was created on July 15, 1870, at first a small square area around Winnipeg, and then enlarged in 1881 to a larger rectangular region composing the modern province's south. By the time British Columbia joined Confederation on July 20, 1871, it had already (1866) been granted the portion of North-Western Territory south of 60 degrees north and west of 120 degrees west, an area that comprised most of the Stickeen Territories.\nIn the meantime, the Province of Ontario was enlarged northwestward in 1882. Quebec was also extended northwards in 1898. The Yukon was made a separate territory that year, due to the Klondike Gold Rush, to free the North-west Territories government in Regina from the burden of addressing the problems caused by the sudden boom of population and economic activity, and the influx of non-Canadians.\n\nThe provinces of Alberta and Saskatchewan were created in 1905. In 1906, the Parliament of Canada renamed the \"North-West Territories\" as the Northwest Territories, dropping all hyphenated forms of it.\n\nManitoba, Ontario, and Quebec acquired the last addition to their modern landmass from the Northwest Territories in 1912. This left only the districts of Mackenzie, Franklin (which absorbed the remnants of Ungava in 1920), and Keewatin within what was then given the name Northwest Territories. In 1925, the boundaries of the Northwest Territories were extended all the way to the North Pole on the sector principle, vastly expanding its territory onto the northern ice cap. Between 1925 and 1999, the Northwest Territories covered a land area of – larger than that of India.\n\nOn April 1, 1999, the existing Northwest Territories was split into two parts, with a separate Nunavut territory being formed to represent the Inuit people.\n\nThe NWT is one of two jurisdictions in Canada – Nunavut being the other – where Aboriginal peoples are in the majority, constituting 50.4% of the population.\n\nAccording to the 2016 Canadian census, the 10 major ethnic groups were:\n\nPopulation of the Northwest Territories since 1871\n\nThe largest denominations by number of adherents according to the 2001 census were Roman Catholic with 16,940 (46.7%); the Anglican Church of Canada with 5,510 (14.9%); and the United Church of Canada with 2,230 (6.0%), while a total of 6,465 (17.4%) people stated no religion.\n\nFrench was made an official language in 1877 by the territorial government. After a lengthy and bitter debate resulting from a speech from the throne in 1888 by Lieutenant Governor Joseph Royal the members of the day voted on more than one occasion to nullify that and make English the only language used in the assembly. After some conflict with the Confederation Government in Ottawa, and a decisive vote on January 19, 1892, the assembly members voted for an English-only territory.\n\nThe Northwest Territories' Official Languages Act recognizes the following eleven official languages:\nNWT residents have a right to use any of the above languages in a territorial court, and in the debates and proceedings of the legislature. However, the laws are legally binding only in their French and English versions, and the NWT government only publishes laws and other documents in the territory's other official languages when the legislature asks it to. Furthermore, access to services in any language is limited to institutions and circumstances where there is a significant demand for that language or where it is reasonable to expect it given the nature of the services requested. In practical terms, English language services are universally available, and there is no guarantee that other languages, including French, will be used by any particular government service, except for the courts.\n\nThe 2016 census returns showed a population of 41,786. Of the 40,565 singular responses to the census question regarding each inhabitant's \"mother tongue\", the most reported languages were the following:\nThere were also 630 responses of both English and a \"non-official language\"; 35 of both French and a \"non-official language\"; 145 of both English and French, and about 400 people who either did not respond to the question, or reported multiple non-official languages, or else gave some other unenumerable response. The Northwest Territories' official languages are shown in bold.\n\nAs of 2014 there are 33 official communities in the NWT. These range in size from Yellowknife with a population of 19,569 to Kakisa with 36 people. Governance of each community differs, some are run under various types of First Nations control, while others are designated as a city, town, village or hamlet, but most communities are municipal corporations. Yellowknife is the largest community and has the largest number of Aboriginal peoples, 4,520 (23.4%) people. However, Behchokǫ̀, with a population of 1,874, is the largest First Nations community, 1,696 (90.9%), and Inuvik with 3,243 people is the largest Inuvialuit community, 1,315 (40.5%). There is one Indian reserve in the NWT, Hay River Reserve, located on the south shore of the Hay River.\n\nThe NWT's geological resources include gold, diamonds, natural gas and petroleum. BP is the only oil company currently producing oil in the Territory. NWT diamonds are promoted as an alternative to purchasing blood diamonds. Two of the biggest mineral resource companies in the world, BHP Billiton and Rio Tinto mine many of their diamonds from the NWT. In 2010, NWT accounted for 28.5% of Rio Tinto's total diamond production (3.9 million carats, 17% more than in 2009, from the Diavik Diamond Mine) and 100% of BHP's (3.05 million carats from the EKATI mine).\n\nThe Northwest Territories has the highest per capita GDP of all provinces or territories in Canada, C$76,000 in 2009. However, as production at the current mines started to wind down, no new mines opened and the public service shrank, the territory lost 1,200 jobs between November 2013 and November 2014.\n\n\nAs a territory, the NWT has fewer rights than the provinces. During his term, Premier Kakfwi pushed to have the federal government accord more rights to the territory, including having a greater share of the returns from the territory's natural resources go to the territory. Devolution of powers to the territory was an issue in the 20th general election in 2003, and has been ever since the territory began electing members in 1881.\n\nThe Commissioner of the NWT is the chief executive and is appointed by the Governor-in-Council of Canada on the recommendation of the federal Minister of Aboriginal Affairs and Northern Development. The position used to be more administrative and governmental, but with the devolution of more powers to the elected assembly since 1967, the position has become symbolic. The Commissioner had full governmental powers until 1980 when the territories were given greater self-government. The Legislative Assembly then began electing a cabinet and \"Government Leader\", later known as the Premier. Since 1985 the Commissioner no longer chairs meetings of the Executive Council (or cabinet), and the federal government has instructed commissioners to behave like a provincial Lieutenant Governor. Unlike Lieutenant Governors, the Commissioner of the Northwest Territories is not a formal representative of the Queen of Canada.\n\nUnlike provincial governments and the government of Yukon, the government of the Northwest Territories does not have political parties, except for the period between 1898 and 1905. It is a consensus government called the Legislative Assembly. This group is composed of one member elected from each of the nineteen constituencies. After each general election, the new Assembly elects the Premier and the Speaker by secret ballot. Seven MLAs are also chosen as cabinet ministers, with the remainder forming the opposition.\n\nThe current Legislative Assembly is the 18th and the most recent election was held November 23, 2015. The Premier is Bob McLeod. The member of Parliament for the Northwest Territories is Michael McLeod (Liberal Party). The Commissioner of the Northwest Territories is George Tuccaro and the Deputy Commissioner is Margaret Thom.\n\nIn the Parliament of Canada, the NWT comprises a single Senate division and a single House of Commons electoral district, titled Northwest Territories (\"Western Arctic\" until 2014).\n\nThe Government of Northwest Territories comprises the following departments:\n\nThe Northwest Territories is divided into five administrative regions (with regional seat):\n\nNorthwest Territories has eight numbered highways. The longest is the Mackenzie Highway which stretches from the Alberta Highway 35's northern terminus in the south at the Alberta – Northwest Territories border at the 60th parallel to Wrigley, Northwest Territories in the north. Ice roads and winter roads are also prominent and provide road access in winter to towns and mines which would otherwise be fly-in locations. Yellowknife Highway branches out from Mackenzie Highway and connects it to Yellowknife. Dempster Highway is the continuation of Klondike Highway. It starts just west of Dawson City, Yukon and continues east for over to Inuvik.\n\nYellowknife did not have an all-season road access to the rest of Canada's highway network until the completion of Deh Cho Bridge in 2012. Prior to that, traffic relied on ferry service in summer and ice road in winter to cross the Mackenzie River. This became a problem during spring and fall time when the ice was not thick enough to handle vehicle load but the ferry could not pass through the ice, which would require all goods from fuel to groceries to be airlifted during the transition period.\n\nYellowknife Transit is the public transportation agency in the city, and is the only transit system within the Northwest Territories.\n\nYellowknife Airport is the largest airport in the territory in terms of aircraft movements and passengers. It is the gateway airport to other destinations within the Northwest Territories. As the airport of the territory capital, it is part of the National Airports System. It is the hub of multiple regional airlines. Major airlines serving destinations within Northwest Territories include Buffalo Airways, Canadian North, First Air, North-Wright Airways.\n\nAboriginal issues in the Northwest Territories include the fate of the Dene who, in the 1940s, were employed to carry radioactive uranium ore from the mines on Great Bear Lake. Of the thirty plus miners who worked at the Port Radium site, at least fourteen have died due to various forms of cancer. A study was done in the community of Deline, called \"A Village of Widows\" by Cindy Kenny-Gilday, which indicated that the number of people involved were too small to be able to confirm or deny a link.\n\nThere has been racial tension based on a history of violent conflict between the Dene and the Inuit, who have now taken recent steps towards reconciliation.\n\nLand claims in the NWT began with the Inuvialuit Final Agreement, signed on June 5, 1984. It was the first Land Claim signed in the Territory, and the second in Canada. It culminated with the creation of the Inuit homeland of Nunavut, the result of the Nunavut Land Claims Agreement, the largest land claim in Canadian history.\n\nAnother land claims agreement with the Tłı̨chǫ people created a region within the NWT called Tli Cho, between Great Bear and Great Slave Lakes, which gives the Tłı̨chǫ their own legislative bodies, taxes, resource royalties, and other affairs, though the NWT still maintains control over such areas as health and education. This area includes two of Canada's three diamond mines at Ekati and Diavik.\n\nAmong the festivals in the region are the Great Northern Arts Festival, the Snowking Winter Festival, Folk on the Rocks music festival in Yellowknife, and Rockin the Rocks.\n\nDuring the winter, many international visitors go to Yellowknife to watch the auroras.\n\nAulavik National Park is in the northern part of Northwest Territories.\n\n\n"}
{"id": "2383878", "url": "https://en.wikipedia.org/wiki?curid=2383878", "title": "Norwegian Institute for Air Research", "text": "Norwegian Institute for Air Research\n\nThe Norwegian Institute for Air Research () or NILU is one of the leading specialized scientific laboratories in Europe researching issues related to air pollution, climate change and health. NILU has a staff of scientists, engineers and technicians with specialized expertise for working on air pollution problems. The staff do more than two hundred projects annually for research councils, industries, international banks and local, national and international authorities and organizations. Its director since 2009 is Kari Nygaard.\n\nNILU was founded in 1969 and the institute conducts environmental research with emphasis on the sources of air pollution and on air pollution dispersion, transport, transformation and deposition. It is also involved in the assessment of the effects of pollution on ecosystems, human health and materials. Integrated environmental assessments and optimal abatement strategy planning has been a field of priority during the last few years. Assessment of transboundary transport of air pollutants, acid rain and global air quality are important tasks.\n\nNILU has the responsibility as a national research institution for air pollution in Norway and is also being used as an international air pollution expert by the World Bank, the Asian Development Bank and the World Health Organization.\n\nNILU has developed an automatic surveillance program for air quality in cities and background areas. NILU have specialized in computerized automatic air pollution surveillance, planning and optimal abatement strategy planning. This AirQUIS system is an air pollution management and planning system designed for managers and decision-makers.\n\nNILU's head office is at Kjeller on the outskirt of Oslo in Norway. A specialised office for Arctic related matters is an integrated part of The Fram Centre (FRAM - High North Research Centre for Climate and the Environment) in Tromsø. NILU also own NILU Polska, an office in Krakow, Poland, working within areas of air quality monitoring, biomass, waste management and emission reduction; and uMoya NILU, an Air Quality Consulting company in Durban, South Africa. Innovation nilu is a holding company for NILU's various commercial interests and subsidiaries, such as Portseye (air quality and climate monitoring within maritime and industrial sector).\n\n"}
{"id": "55997248", "url": "https://en.wikipedia.org/wiki?curid=55997248", "title": "Oasis effect", "text": "Oasis effect\n\nThe oasis effect refers to the creation of a local microclimate that is cooler than the surrounding dry area due to evaporation or evapotranspiration of a water source or plant life and higher albedo of plant life than bare ground. The oasis effect is so-named because it occurs in desert oases. Urban planners can design a city's layout to optimize the oasis effect to combat the urban heat island effect. Since it depends on evaporation, the oasis effect differs by season.\n\nAn oasis contains moisture from a water source and/or plants. When that water evaporates or transpirates, heat from the surroundings is used to convert liquid to gas in an endothermic reaction, which results in cooler local temperatures. Moreover, vegetation has a higher albedo than bare ground, and reflects more sunlight, leading to lower land temperatures, lower air temperatures, and a cooler local microclimate. \n\nThe oasis effect occurs most prominently during the summertime because warmer temperatures lead to more evaporation. In the winter, the oasis effect operates differently. Instead of making the oasis cooler, the oasis effect makes it warmer at night. This occurs through the fact that trees block heat from leaving the land. Basically, radiation cannot be emitted back into the atmosphere because the trees intercept and absorb it.\n\nThe oasis effect plays a role in urban development because plants and bodies of water result in cooler cities. Accordingly, cities with parks will have lower temperatures because plants have higher albedo than bare ground or roads. Areas with higher albedo reflect more light than they absorb, leading to cooler temperatures. Normally, cities are hotter than their suburbs due to dense population, dark buildings and roads, and pollution; this is known as the urban heat island effect. However, by careful placement of trees, parks, and plant life, cities can create their own oasis effect. By maintaining plant life throughout a city, urban planners can produce an oasis effect to offer the urban heat island effect; even a small scattering of trees can significantly reduce local temperatures. However, concerns can arise in arid regions with limited water sources where city planners may not want to leave water sources out in the open to evaporate, and may not want to sacrifice water for upkeep of plants.\n\n"}
{"id": "28714062", "url": "https://en.wikipedia.org/wiki?curid=28714062", "title": "Orpheus Descending (film)", "text": "Orpheus Descending (film)\n\nOrpheus Descending is a 1990 American television film starring Vanessa Redgrave and directed by Peter Hall. It is an adaptation of Tennessee Williams' play of the same name. Hall had directed Redgrave in an acclaimed Broadway production of the play a year earlier. The stage actors such as Redgrave, Kevin Anderson and Brad Sullivan reprised their roles for the film.\n\nLady Torrance (Redgrave) is introduced as a disillusioned character; her Sicilian father was murdered by the Ku Klux Klan and she's been trapped in a miserable marriage for twenty years. Her tyrannical and bigoted husband is struck down by cancer. Then Lady Torrance meets the orpheuse, a young man named Val Xavier (Kevin Anderson), an Elvis Presley-inspired drifter. A romantic entanglement ensues as Lady Torrance puts behind the misery of marriage behind her and finds passion and happiness with Val.\n\n"}
{"id": "1470503", "url": "https://en.wikipedia.org/wiki?curid=1470503", "title": "Paleoarchean", "text": "Paleoarchean\n\nThe Paleoarchean (), also spelled Palaeoarchaean (formerly known as early Archean), is a geologic era within the Archaean Eon. It spans the period of time —the era is defined chronometrically and is not referenced to a specific level of a rock section on Earth. The name derives from Greek \"Palaios\" \"ancient\". The oldest ascertained life form of fossilized bacteria in microbial mats, old, found in Western Australia, is from this era. The first supercontinent Vaalbara formed during this period.\n\nDuring this era, a large asteroid, about wide, collided with the Earth in the area of South Africa about 3.26 billion years ago, creating the features known as the Barberton greenstone belt.\n"}
{"id": "26302177", "url": "https://en.wikipedia.org/wiki?curid=26302177", "title": "Plate reconstruction", "text": "Plate reconstruction\n\nPlate reconstruction is the process of reconstructing the positions of tectonic plates relative to each other (relative motion) or to other reference frames, such as the earth's magnetic field or groups of hotspots, in the geological past. This helps determine the shape and make-up of ancient supercontinents and provides a basis for paleogeographic reconstructions.\n\nAn important part of reconstructing past plate configurations is to define the edges of areas of the lithosphere that have acted independently at some time in the past.\n\nMost present plate boundaries are easily identifiable from the pattern of recent seismicity. This is now backed up by the use of GPS data, to confirm the presence of significant relative movement between plates.\n\nIdentifying past (but now inactive) plate boundaries within current plates is generally based on evidence for an ocean that has now closed up. The line where the ocean used to be is normally marked by pieces of the crust from that ocean, included in the collision zone, known as ophiolites. The line across which two plates became joined to form a single larger plate, is known as a suture.\n\nIn many orogenic belts, the collision is not just between two plates, but involves the sequential accretion of smaller terranes. Terranes are smaller pieces of continental crust that have been caught up in an orogeny, such as continental fragments or island arcs.\n\nPlate motions, both those observable now and in the past, are referred ideally to a reference frame that allows other plate motions to be calculated. For example, a central plate, such as the African plate, may have the motions of adjacent plates referred to it. By composition of reconstructions, additional plates can be reconstructed to the central plate. In turn, the reference plate may be reconstructed, together with the other plates, to another reference frame, such as the earth's magnetic field, as determined from paleomagnetic measurements of rocks of known age. A global hotspot reference frame has been postulated (see, e.g., W. Jason Morgan) but there is now evidence that not all hotspots are necessarily fixed in their locations relative to one another or the earth's spin axis. However, there are groups of such hotspots that appear to be fixed within the constraints of available data, within particular mesoplates.\n\nThe movement of a rigid body, such as a plate, on the surface of a sphere can be described as rotation about a fixed axis (relative to the chosen reference frame). This pole of rotation is known as an Euler pole. The movement of a plate is completely specified in terms of its Euler pole and the angular rate of rotation about the pole. Euler poles defined for current plate motions can be used to reconstruct plates in the recent past (few million years). At earlier stages of earth's history, new Euler poles need to be defined.\n\nIn order to move plates backward in time it is necessary to provide information on either relative or absolute positions of the plates being reconstructed such that an Euler pole can be calculated. These are quantitative methods of reconstruction.\n\nCertain fits between continents, particularly that between South America and Africa, were known long before the development of a theory that could adequately explain them. The reconstruction before Atlantic rifting by Bullard based on a least-squares fitting at the 500 fathom contour still provides the best match to paleomagnetic pole data for the two sides from the middle of Paleozoic to Late Triassic.\n\nPlate reconstructions in the recent geological past mainly use the pattern of magnetic stripes in oceanic crust to remove the effects of seafloor spreading. The individual stripes are dated from magnetostratigraphy so that their time of formation is known. Each stripe (and its mirror image) represents a plate boundary at a particular time in the past, allowing the two plates to be repositioned relative to one another. The oldest oceanic crust is Jurassic, providing a lower age limit of about 175 Ma for the use of such data. Reconstructions derived in this way are only relative.\n\nPaleomagnetic data are obtained by taking oriented samples of rocks and measuring their remanent magnetizations in the laboratory. Good quality data can be recovered from different rock types. In igneous rocks, magnetic minerals crystallize from the melt, and when the rock is cooled below their Curie temperature, it acquires a thermoremanent magnetization (TRM) in the direction of the Earth’s magnetic field. In sedimentary rocks, magnetic grains will align their magnetic moments with the direction of the magnetic field during or soon after the deposition, resulting in a detrital or post-detrital remanent magnetization (DRM). A common difficulty with the use of clastic sediments for defining directions of the magnetic field in the past is that the direction of DRM may rotate toward the bedding plane due to the compaction of sediment, resulting in an inclination, which is shallower than the inclination of the field during the deposition. The inclination flattening error can nevertheless be estimated and corrected for through re-deposition experiments, measurements of magnetic anisotropy, and the use of theoretical models for the dispersion of paleomagnetic directions. Metamorphic rocks are not normally used for paleomagnetic measurements due to the complexities related to the acquisition of remanence, uncertainties in magnetization age, and high magnetic anisotropy. \n\nA typical paleomagnetic study would sample a large number of independent rock units of similar age at nearby locations and collect multiple samples from each unit in order to estimate measurement errors and assess how well the obtained paleomagnetic dataset samples geomagnetic secular variation. Progressive demagnetization techniques are used to identify secondary magnetization components (e.g., magnetic overprints that could have been imparted on the rock due to chemical alteration or reheating) and to isolate the primary magnetization, which records the direction of the magnetic field at the time when the rock was formed. Various rock-magnetic and paleomagnetic tests are normally performed to establish the primary nature of the isolated remanent magnetization. The recovered paleomagnetic directions are used to derive paleomagnetic poles, which provide constrains on the latitudinal position of the crustal block from which the rock samples were taken, and its original orientation with respect to the lines of longitude.\n\nGood quality paleomagnetic data are available from the \"Global Paleomagnetic Database\", which is accessible from the World Data Center A in the USA at Boulder, Colorado.\n\nA paleomagnetic pole is defined by taking the average direction of the primary remanent magnetization for the sampled rocks (expressed as the mean declination and inclination) and calculating the position of a geomagnetic pole for the field of a geocentric magnetic dipole that would produce the observed mean direction at the sampled locality in its present geographic coordinates. An alternative way of defining paleomagnetic poles is to calculate a virtual geomagnetic pole (VGP) for each individual rock unit and then estimate the mean location for all VGPs. Fisher statistics on the sphere is normally used to obtain the mean direction of magnetization, or the mean VGP location, and to estimate their uncertainties. Both approaches are used in paleomagnetic studies, but it has been recognized that averaging directions instead of full remanence vectors can lead to biased estimates of the mean direction of the paleomagnetic field, so that the calculation of paleomagnetic poles by averaging VGPs is currently the preferred technique. \n\nPaleomagnetic studies of geologically recent lavas (Pliocene to Quaternary, 0-5 Ma) indicate that when the geomagnetic field is averaged on time scales of tens of thousands to millions of years – over a time period long enough to fully sample geomagnetic secular variation, the time-averaged field can be accurately approximated by the field of a geocentric axial dipole (GAD) – that is, a magnetic dipole placed in the center of the Earth and aligned with the Earth’s rotation axis. Hence, if a paleomagnetic dataset has sampled enough time to average secular variation, the paleomagnetic pole derived from it can be interpreted as an estimate for the location of the geographic pole with respect to the sampling locality fixed in the present geographic position. \n\nThe difference between the paleomagnetic pole and the present geographic pole reflects the paleogeographic position of the crustal block containing the sampled area at the time when the studied rocks were formed, including its original latitude (paleolatitude) and orientation. Under the assumption that the mean paleomagnetic direction corresponds to that of the GAD field, the paleolatitude of the sampling location (λ) can be derived from the inclination (I) of the mean direction using a simple equation: \n\nformula_1\n\nThe mean declination (D) gives the sense and amount of rotation about a vertical axis passing through the sampling area, which needs to be applied to restore its original orientation with respect to the lines of longitude. The paleolatitude for any specific location belonging to the same crustal block can be computed as 90° minus the angular distance between this location and the paleomagnetic pole, and the local vertical axis rotation can be estimated by computing the declination expected from the position of the pole. Thus, a paleomagnetic pole defines the paleo-latitudinal position and orientation of the entire tectonic block at a specific time in the past. However, because the GAD field is azimuthally symmetric about the Earth’s rotation axis, the pole doesn’t set any constraint on the absolute longitude. From the perspective of paleomagnetic directions, the GAD field has the same values of inclination and declination along a line of constant latitude at all longitudes, so that any conceivable longitude would be an equally viable option for the reconstruction of a tectonic element if its paleogeographic position is constrained by paleomagnetic data alone.\n\nConsidering that a paleomagnetic pole approximates the position of the geographic pole with respect to the continent or geologic terrane from which it was determined, the paleolatitude and orientation can be restored by finding a rotation (Euler pole and rotation angle) that reconstructs the paleomagnetic pole to the geographic pole, and applying this rotation to the continent or terrane. By doing so, the crustal block and its paleomagnetic pole are reconstructed using the same Euler rotation, so that they do not move relative to each other, the paleomagnetic pole is placed at the geographic pole, and the crustal block is correctly restored in latitude and orientation (i.e., with respect to the geographic pole). Noting that a further rotation around the geographic pole will only change the longitude of the block, but its latitude and orientation with respect to the lines of longitude will not be affected, the absolute paleolongitude cannot be determined in reconstructions based on paleomagnetism. However, relative longitudes of different crustal blocks can be defined using other types of geological and geophysical data constraining relative motions of tectonic plates, including the histories of seafloor spreading recorded my marine magnetic anomalies, matching of continental borders and geologic terranes, and paleontological data.\n\nPoles from different ages in a single continent, lithospheric plate, or any other tectonic block can be used to construct an apparent polar wander path (APWP). If paths from adjacent crustal fragments are identical, this is taken to indicate that there has been no relative movement between them during the period covered by the path. Divergence of APW paths indicates that the areas in question have acted independently in the past with the point of divergence marking the time at which they became joined. Combined or synthetic APWPs can be constructed by rotating paleomagnetic poles from different plates into the reference frame fixed to a single plate, using estimates of relative plate motions. For the times postdating the assembly of Pangea (320 Ma), synthetic APWPs are often constructed in the reference frame fixed to the African plate because Africa has occupied a central position in the Pangea configuration and has been dominantly surrounded by spreading ridges after the Pangea breakup, which commenced in the early Jurassic (ca. 180 Ma). \n\nFor a single lithospheric plate, the APWP reflects the motion of the plate with respect to the geographic pole (changes in latitude) and changes of its orientation with respect to paleomeridians. The longitudes of paleogeographic reconstructions based on APWPs are uncertain, but it has been argued that the uncertainty can be minimized by selecting a reference plate that is expected to move the least in longitude from the consideration of the plate tectonics theory and by linking the reconstructions of the remaining plates to this reference plate using the estimates of relative plate motion. For example, and it was shown that assuming no significant longitudinal motion of Africa since the time of the Pangea assembly results in a reasonable plate tectonic scenario, in which no large, coherent east-west motions of the continental lithosphere are observed in paleogeographic reconstructions. \n\nAPWPs can be interpreted as records of a combined signal from two sources of plate motion: (1) motion of lithospheric plates with respect to the Earth’s mantle and (2) motion of the entire solid Earth (mantle and lithosphere) with respect to the Earth’s rotation axis. The second component is commonly referred to as true polar wander (TPW) and on geologic time scales results from gradual redistribution of mass heterogeneities due to convective motions in the Earth’s mantle. By comparing plate reconstructions based on paleomagnetism with reconstructions in the mantle reference frame defined by hotspots for the last 120 Ma, the TPW motions can be estimated, which allows tying paleogeographic reconstructions to the mantle and hence constraining them in paleolongitude. For the earlier times in the Mesozoic and Paleozoic, TPW estimates can be obtained through the analysis of coherent rotations of the continental lithosphere, which allows linking the reconstructed paleogeography to the large-scale structures in the lower mantle, commonly referred to as Large Low Shear-wave Velocity Provinces (LLSVPs). It has been argued that the LLSVPs have been stable over at least the past 300 Ma, and possibly longer, and that the LLSVP margins have served as generation zones for the mantle plumes responsible for eruptions of Large Igneous Provinces (LIPs) and kimberlites. Correlating the reconstructed locations of LIPs and kimberlites with the margins of LLSVPs using the estimated TPW rotations makes it possible to develop a self-consistent model for plate motions relative to the mantle, true polar wander, and the corresponding changes of paleogeography constrained in longitude for the entire Phanerozoic, although the origin and long-term stability of LLSVPs are the subject of the on-going scientific debate. \n\nPaleomagnetic Euler poles derived by geometrizing apparent polar wander paths (APWPs) potentially allows constraining paleolongitudes from paleomagnetic data. This method could extend absolute plate motion reconstructions deeply into the geologic history as long as there are reliable APWPs.\n\nThe presence of chains of volcanic islands and seamounts interpreted to have formed from fixed hotspots allows the plate on which they sit to be progressively restored so that a seamount is moved back over the hotspot at its time of formation. This method can be used back to the Early Cretaceous, the age of the oldest evidence for hotspot activity. This method gives an absolute reconstruction of both latitude and longitude, although before about 90 Ma there is evidence of relative motion between hotspot groups.\n\nOnce oceanic plates subduct in the lower mantle (slabs), they are assumed to sink in a near-vertical manner. With the help of seismic wave tomography, this can be used to constrain plate reconstructions at first order back to the Permian.\n\nSome plate reconstructions are supported by other geological evidence, such as the distribution of sedimentary rock types, the position of orogenic belts and faunal provinces shown by particular fossils. These are semi-quantitative methods of reconstruction.\n\nSome types of sedimentary rock are restricted to certain latitudinal belts. Glacial deposits for instance are generally confined to high latitudes, whereas evaporites are generally formed in the tropics.\n\nOceans between continents provide barriers to plant and animal migration. Areas that have become separated tend to develop their own fauna and flora. This is particularly the case for plants and land animals but is also true for shallow water marine species, such as trilobites and brachiopods, although their planktonic larvae mean that they were able to migrate over smaller deep water areas. As oceans narrow before a collision occurs, the faunas start to become mixed again, providing supporting evidence for the closure and its timing.\n\nWhen supercontinents break up, older linear geological structures such as orogenic belts may be split between the resulting fragments. When a reconstruction effectively joins up orogenic belts of the same age of formation, this provides further support for the reconstruction's validity.\n\n"}
{"id": "18605319", "url": "https://en.wikipedia.org/wiki?curid=18605319", "title": "Quark–gluon plasma", "text": "Quark–gluon plasma\n\nA quark–gluon plasma (QGP) or quark soup is a state of matter in quantum chromodynamics (QCD) which exists at extremely high temperature and/or density. This state is thought to consist of asymptotically free strong-interacting quarks and gluons, which are ordinarily confined by color confinement inside atomic nuclei or other hadrons. This is in analogy with the conventional plasma where nuclei and electrons, confined inside atoms by electrostatic forces at ambient conditions, can move freely. Artificial quark matter, which has been produced at Brookhaven National Laboratory’s Relativistic Heavy Ion Collider and CERN's Large Hadron Collider, can only be produced in minute quantities and is unstable and impossible to contain, and will radioactively decay within a fraction of a second into stable particles through hadronization; the produced hadrons or their decay products and gamma rays can then be detected. In the quark matter phase diagram, QGP is placed in the high-temperature, high-density regime, whereas ordinary matter is a cold and rarefied mixture of nuclei and vacuum, and the hypothetical quark stars would consist of relatively cold, but dense quark matter. It is believed that up to a few milliseconds after the Big Bang, known as the quark epoch, the Universe was in a quark–gluon plasma state.\n\nThe strength of the color force means that unlike the gas-like plasma, quark–gluon plasma behaves as a near-ideal Fermi liquid, although research on flow characteristics is ongoing. Liquid or even near-perfect liquid flow with almost no frictional resistance or viscosity was claimed by research teams at RHIC and LHC's Compact Muon Solenoid detector. QGP differs from a \"free\" collision event by several features; for example, its particle content is indicative of a temporary chemical equilibrium producing an excess of middle-energy strange quarks vs. a nonequilibrium distribution mixing light and heavy quarks (\"strangeness production\"), and it does not allow particle jets to pass through (\"jet quenching\").\n\nExperiments at CERN's Super Proton Synchrotron (SPS) first tried to create the QGP in the 1980s and 1990s: the results led CERN to announce indirect evidence for a \"new state of matter\" in 2000. In 2010, scientists at Brookhaven National Laboratory’s Relativistic Heavy Ion Collider announced they had created quark–gluon plasma by colliding gold ions at nearly the speed of light, reaching temperatures of 4 trillion degrees Celsius. Current experiments (2017) at the Brookhaven National Laboratory's Relativistic Heavy Ion Collider (RHIC) on Long Island (NY, USA) and at CERN's recent Large Hadron Collider near Geneva (Switzerland) are continuing this effort, by colliding relativistically accelerated gold and other ion species (at RHIC) or lead (at LHC) with each other or with protons. Three experiments running on CERN's Large Hadron Collider (LHC), on the spectrometers ALICE, ATLAS and CMS, have continued studying the properties of QGP. CERN temporarily ceased colliding protons, and began colliding lead ions for the ALICE experiment in 2011, in order to create a QGP. A new record breaking temperature was set by at CERN on August, 2012 in the ranges of 5.5 trillion (5.5×10) kelvin as claimed in their Nature PR.\n\nQuark–gluon plasma is a state of matter in which the elementary particles that make up the hadrons of baryonic matter are freed of their strong attraction for one another under extremely high energy densities. These particles are the quarks and gluons that compose baryonic matter. In normal matter quarks are \"confined\"; in the QGP quarks are \"deconfined\". In classical QCD quarks are the fermionic components of hadrons (mesons and baryons) while the gluons are considered the bosonic components of such particles. The gluons are the force carriers, or bosons, of the QCD color force, while the quarks by themselves are their fermionic matter counterparts.\n\nAlthough the experimental high temperatures and densities predicted as producing a quark–gluon plasma have been realized in the laboratory, the resulting matter does \"not\" behave as a quasi-ideal state of free quarks and gluons, but, rather, as an almost perfect dense fluid. Actually, the fact that the quark–gluon plasma will not yet be \"free\" at temperatures realized at present accelerators was predicted in 1984 as a consequence of the remnant effects of confinement.\n\nA plasma is matter in which charges are screened due to the presence of other mobile charges. For example: Coulomb's Law is suppressed by the screening to yield a distance-dependent charge, formula_1, i.e., the charge Q is reduced exponentially with the distance divided by a screening length α. In a QGP, the color charge of the quarks and gluons is screened. The QGP has other analogies with a normal plasma. There are also dissimilarities because the color charge is non-abelian, whereas the electric charge is abelian. Outside a finite volume of QGP the color-electric field is not screened, so that a volume of QGP must still be color-neutral. It will therefore, like a nucleus, have integer electric charge.\n\nBecause of the extremely high energies involved, quark-antiquark pairs are produced by pair production and thus QGP is a roughly equal mixture of quarks and antiquarks of various flavors, with only a slight excess of quarks. This property is not a general feature of conventional plasmas, which may be too cool for pair production (see however pair instability supernova).\n\nOne consequence of this difference is that the color charge is too large for perturbative computations which are the mainstay of QED. As a result, the main theoretical tools to explore the theory of the QGP is lattice gauge theory. The transition temperature (approximately ) was first predicted by lattice gauge theory. Since then lattice gauge theory has been used to predict many other properties of this kind of matter. The AdS/CFT correspondence conjecture may provide insights in QGP, moreover the ultimate goal of the fluid/gravity correspondence is to understand QGP. The QGP is believed to be a phase of QCD which is completely locally thermalized and thus suitable for an effective fluid dynamic description.\n\nThe QGP can be created by heating matter up to a temperature of , which amounts to per particle. This can be accomplished by colliding two large nuclei at high energy (note that is not the energy of the colliding beam). Lead and gold nuclei have been used for such collisions at CERN SPS and BNL RHIC, respectively. The nuclei are accelerated to ultrarelativistic speeds (contracting their length) and directed towards each other, creating a \"fireball\", in the rare event of a collision. Hydrodynamic simulation predicts this fireball will expand under its own pressure, and cool while expanding. By carefully studying the spherical and elliptic flow, experimentalists put the theory to test.\n\nQCD is one part of the modern theory of particle physics called the Standard Model. Other parts of this theory deal with electroweak interactions and neutrinos. The theory of electrodynamics has been tested and found correct to a few parts in a billion. The theory of weak interactions has been tested and found correct to a few parts in a thousand. Perturbative forms of QCD have been tested to a few percent. Perturbative models assume relatively small changes from the ground state, i.e. relatively low temperatures and densities, which simplifies calculations at the cost of generality. In contrast, non-perturbative forms of QCD have barely been tested. The study of the QGP, which has both a high temperature and density, is part of this effort to consolidate the grand theory of particle physics.\n\nThe study of the QGP is also a testing ground for finite temperature field theory, a branch of theoretical physics which seeks to understand particle physics under conditions of high temperature. Such studies are important to understand the early evolution of our universe: the first hundred microseconds or so. It is crucial to the physics goals of a new generation of observations of the universe (WMAP and its successors). It is also of relevance to Grand Unification Theories which seek to unify the three fundamental forces of nature (excluding gravity).\n\nThe cross-over temperature from the normal hadronic to the QGP phase is about . This \"crossover\" may actually \"not\" be only a qualitative feature, but instead one may have to do with a true (second order) phase transition, e.g. of the universality class of the three-dimensional Ising model. The phenomena involved correspond to an energy density of a little less than . For relativistic matter, pressure and temperature are not independent variables, so the equation of state is a relation between the energy density and the pressure. This has been found through lattice computations, and compared to both perturbation theory and string theory. This is still a matter of active research. Response functions such as the specific heat and various quark number susceptibilities are currently being computed.\n\nThe equation of state is an important input into the flow equations. The speed of sound is currently under investigation in lattice computations. The mean free path of quarks and gluons has been computed using perturbation theory as well as string theory. Lattice computations have been slower here, although the first computations of transport coefficients have recently been concluded. These indicate that the mean free time of quarks and gluons in the QGP may be comparable to the average interparticle spacing: hence the QGP is a liquid as far as its flow properties go. This is very much an active field of research, and these conclusions may evolve rapidly. The incorporation of dissipative phenomena into hydrodynamics is another recent development that is still in an active stage.\n\nThe study of thermodynamic and flow properties indicate that the assumption of QGP consisting almost entirely of free quarks and gluons is an over-simplification. Many ideas are currently being developed and will be put to test in the near future. It has been hypothesized recently that some mesons built from heavy quarks do not dissolve until the temperature reaches about . This has led to speculation that many other kinds of bound states may exist in the plasma. Some static properties of the plasma (similar to the Debye screening length) constrain the excitation spectrum.\n\nSince 2008, there is a discussion about a hypothetical precursor state of the Quark–gluon plasma, the so-called \"Glasma\", where the dressed particles are condensed into some kind of glassy (or amorphous) state, below the genuine transition between the confined state and the plasma liquid. This would be analogous to the formation of metallic glasses, or amorphous alloys of them, below the genuine onset of the liquid metallic state.\n\nThose forms of the QGP that are easiest to compute are not those that are easiest to verify experimentally. While the balance of evidence points towards the QGP being the origin of the detailed properties of the fireball produced at SPS (CERN), in the RHIC and at LHC, this is the main barrier which prevents experimentalists from declaring a sighting of the QGP.\n\nThe important classes of experimental observations are\n\nIn short, a quark–gluon plasma flows like a splat of liquid, and because it's not \"transparent\" with respect to quarks, it can attenuate jets emitted by collisions. Furthermore, once formed, a ball of quark–gluon plasma, like any hot object, transfers heat internally by radiation. However, unlike in everyday objects, there is enough energy available that gluons (particles mediating the strong force) collide and produce an excess of the heavy (i.e. high-energy) strange quarks. Whereas, if the QGP didn't exist and there was a pure collision, the same energy would be converted into a nonequilibrium mixture containing even heavier quarks such as charm quarks or bottom quarks.\n\nIn April 2005, formation of quark matter was tentatively confirmed by results obtained at Brookhaven National Laboratory's Relativistic Heavy Ion Collider (RHIC). The consensus of the four RHIC research groups was that they had created a quark–gluon liquid of very low viscosity. However, contrary to what was at that time still the widespread assumption, it is yet unknown from theoretical predictions whether the QCD \"plasma\", especially close to the transition temperature, should behave like a gas or liquid. Authors favoring the weakly interacting interpretation derive their assumptions from the lattice QCD calculation, where the entropy density of quark–gluon plasma approaches the weakly interacting limit. However, since both energy density and correlation shows significant deviation from the weakly interacting limit, it has been pointed out by many authors that there is in fact no reason to assume a QCD \"plasma\" close to the transition point should be weakly interacting, like electromagnetic plasma (see, e.g.,). That being said, systematically improvable perturbative QCD quasiparticle models do a very good job of reproducing the lattice data for thermodynamical observables (pressure, entropy, quark susceptibility), including the aforementioned \"significant deviation from the weakly interacting limit\", down to temperatures on the order of 2 to 3 times the critical temperature for the transition.\n\n"}
{"id": "1040953", "url": "https://en.wikipedia.org/wiki?curid=1040953", "title": "Recuperator", "text": "Recuperator\n\nA recuperator is a special purpose counter-flow energy recovery heat exchanger positioned within the supply and exhaust air streams of an air handling system, or in the exhaust gases of an industrial process, in order to recover the waste heat. Generally, they are used to extract heat from the exhaust and use it to preheat air entering the combustion system. In this way they use waste energy to heat the air, offsetting some of the fuel, and thereby improves the energy efficiency of the system as a whole.\n\nIn many types of processes, combustion is used to generate heat, and the recuperator serves to recuperate, or reclaim this heat, in order to reuse or recycle it. The term recuperator refers as well to liquid-liquid counterflow heat exchangers used for heat recovery in the chemical and refinery industries and in closed processes such as ammonia-water or LiBr-water absorption refrigeration cycle.\n\nRecuperators are often used in association with the burner portion of a heat engine, to increase the overall efficiency. For example, in a gas turbine engine, air is compressed, mixed with fuel, which is then burned and used to drive a turbine. The recuperator transfers some of the waste heat in the exhaust to the compressed air, thus preheating it before entering the fuel burner stage. Since the gases have been pre-heated, less fuel is needed to heat the gases up to the turbine inlet temperature. By recovering some of the energy usually lost as waste heat, the recuperator can make a heat engine or gas turbine significantly more efficient.\n\nNormally the heat transfer between airstreams provided by the device is termed as \"sensible heat\", which is the exchange of energy, or enthalpy, resulting in a change in temperature of the medium (air in this case), but with no change in moisture content. However, if moisture or relative humidity levels in the return air stream are high enough to allow condensation to take place in the device, then this will cause \"latent heat\" to be released and the heat transfer material will be covered with a film of water. Despite a corresponding absorption of latent heat, as some of the water film is evaporated in the opposite airstream, the water will reduce the thermal resistance of the boundary layer of the heat exchanger material and thus improve the heat transfer coefficient of the device, and hence increase efficiency. The energy exchange of such devices now comprises both sensible and latent heat transfer; in addition to a change in temperature, there is also a change in moisture content of the exhaust air stream.\n\nHowever, the film of condensation will also slightly increase pressure drop through the device, and depending upon the spacing of the matrix material, this can increase resistance by up to 30%. If the unit is not laid to falls, and the condensate not allowed to drain properly, this will increase fan energy consumption and reduce the seasonal efficiency of the device.\n\nIn heating, ventilation and air-conditioning systems, HVAC, recuperators are commonly used to re-use waste heat from exhaust air normally expelled to atmosphere. Devices typically comprises a series of parallel plates of aluminium, plastic, stainless steel, or synthetic fiber, alternate pairs of which are enclosed on two sides to form twin sets of ducts at right angles to each other, and which contain the supply and extract air streams. In this manner heat from the exhaust air stream is transferred through the separating plates, and into the supply air stream. Manufacturers claim gross efficiencies of up to 80% depending upon the specification of the unit.\n\nThe characteristics of this device are attributable to the relationship between the physical size of the unit, in particular the air path distance, and the spacing of the plates. For an equal air pressure drop through the device, a small unit will have a narrower plate spacing and a lower air velocity than a larger unit, but both units may be just as efficient. Because of the cross-flow design of the unit, its physical size will dictate the air path length, and as this increases, heat transfer will increase but pressure drop will also increase, and so plate spacing is increased to reduce pressure drop, but this in turn will reduce heat transfer.\n\nAs a general rule a recuperator selected for a pressure drop of between will have a good efficiency, while having a small effect on fan power consumption, but will have in turn a higher seasonal efficiency than that for physically smaller, but higher pressure drop recuperator.\n\nWhen heat recovery is not required, it is typical for the device to be bypassed by use of dampers arranged within the ventilation distribution system. Assuming the fans are fitted with inverter speed controls, set to maintain a constant pressure in the ventilation system, then the reduced pressure drop leads to a slowing of the fan motor and thus reducing power consumption, and in turn improves the seasonal efficiency of the system.\n\nRecuperators have also been used to recover heat from waste gasses to preheat combustion air and fuel for many years by metallic recuperators to reduce energy costs and carbon footprint of operation. Compared to alternatives such as regenerative furnaces, initial costs are lesser, there are no valves to be switching back and forth, there are no induced-draft fans and it does not require a web of gas ducts spread up all over the furnace.\n\nHistorically the recovery ratios of recuperators compared to regenerative burners were low. However, recent improvements to technology have allowed recuperators to recover 70-80% of the waste heat and pre-heated air up to is now possible.\n\nRecuperators can be used to increase the efficiency of gas turbines for power generation, provided the exhaust gas is hotter than the compressor outlet temperature. The exhaust heat from the turbine is used to pre-heat the air from the compressor before further heating in the combustor, reducing the fuel input required. The larger the temperature difference between turbine out and compressor out, the greater the benefit from the recuperator. Therefore, microturbines (<1MW), which typically have low pressure ratios, have the most to gain from the use of a recuperator. In practice, a doubling of efficiency is possible through the use of a recuperator. The major practical challenge for a recuperator in microturbine applications is coping with the exhaust gas temperature, which can exceed .\n\n\n"}
{"id": "10699036", "url": "https://en.wikipedia.org/wiki?curid=10699036", "title": "Recycling codes", "text": "Recycling codes\n\nRecycling codes are used to identify the material from which an item is made, to facilitate easier recycling or other reprocessing. Having a recycling code, the chasing arrows logo or a resin code on an item is not an automatic indicator that a material is recyclable but rather an explanation of what the item is. Such symbols have been defined for batteries, biomatter/organic material, glass, metals, paper, and plastics. Various countries have adopted different codes. For example, the table below shows the polymer resin codes (plastic) for a country. In the United States there are fewer, as ABS is grouped in with others in group 7. Other countries have a more granular recycling code system. For example, China's polymer identification system has seven different classifications of plastic, five different symbols for post-consumer paths, and 140 identification codes. The lack of codes in some countries has encouraged those who can fabricate their own plastic products, such as RepRap and other prosumer 3-D printer users, to adopt a voluntary recycling code based on the more comprehensive Chinese system.\n\nThe Standardization Administration of the People’s Republic of China (SAC) has defined material codes for different types of plastics in the document GB16288,2008.\n\n\n"}
{"id": "50352834", "url": "https://en.wikipedia.org/wiki?curid=50352834", "title": "Regrowth inside ballast tanks", "text": "Regrowth inside ballast tanks\n\nAfter delivering their cargo, empty commercial ships need to take up water from the port of arrival in order to maintain stability and ensure safe navigation conditions before heading back to the port of departure. This water, called ballast water, which contains aquatic organisms typical of the port of arrival, is stored in ballast tanks and is ultimately discharged at the port of departure when the ship is ready to be re-loaded. During this process, aquatic organisms capable of surviving in ballast water are released into new environments and can therefore become invasive species, causing serious economic and public health issues.\nFollowing the ratification of the International Maritime Organization (IMO)’s Ballast Water Management (BWM) Convention, commercial ships will have to treat their ballast water in order to comply with maximum discharge standards established for organisms in different size categories (IMO’s D-2 Standards and US Coast Guard Standards). However, some organisms are capable of surviving or even recovering after harsh treatments, leading to regrowth in ballast water tanks before discharge. In order to minimise regrowth and hence avoid exceeding discharge limits, different criteria such as duration of the journey, ballast water tanks capacity and water flow rate at intake and discharge, among others, should be considered when choosing an appropriate type-approved ballast water treatment systems (BWTS).\n\n, no single ballast water treatment method, or even a combination of primary (e.g. mechanical/physical separation) and secondary (e.g. active chemical substances) methods, can remove or inactivate all organisms in ballast water. Furthermore, some treatments are more effective in removing microorganisms such as bacteria, whereas others are better at killing larger organisms such as phytoplankton (e.g. diatoms) and zooplankton (e.g. copepods).\n\nThere are currently over 50 IMO type-approved BWTS on the market to choose from. These include the use of technologies such as UV irradiation, ozonation, electrochlorination, hydrodynamic cavitation and ultrasound, among others, applied as stand-alone or combined treatments. The US Coast Guard requires BWTS as per 46 CFR 162.060, and began approving in 2016.\n\nSurviving organisms have the potential to regrow after treatment and, depending on the duration of the voyage and the prevailing conditions, this regrowth could lead to exceeding the maximum number of aquatic organisms that can be discharged according to the established standards.\n\nBoth phytoplankton and zooplankton have been shown to be capable of surviving in ballast water tanks for up to 23 days. There is also evidence showing that different phytoplanktonic organisms can regrow within 4 to 20 days of incubating in favourable conditions. Bacteria that survive treatment have an even higher potential for regrowth, as they benefit from the death of other organisms in two different ways (i) nutrients essential for bacterial growth are released in the form of dissolved organic matter and, (ii) there is a decrease in the number of predators that would otherwise eat them. Overall, bacterial regrowth has been observed after 18 hrs to 7 days of applying different treatments.\n\nTherefore, the scientific evidence currently available supports the idea that it is not an issue of “IF regrowth” but “WHEN regrowth”.\n\nTimescales are very important when considering regrowth. For instance, if ballast water is treated at intake and held in ballast tanks for over a week before discharge, then the organisms surviving the treatment could have enough time to increase in numbers and potentially exceed discharge standards before the end of the voyage.\n\nEvery BWTS has its advantages and disadvantages and ship owners and ship operators should consider the benefits and limitations of different systems before making an educated choice based on their requirements. The issue of regrowth should be taken seriously and should be taken into account when choosing an appropriate BWTS.\n\n"}
{"id": "24016215", "url": "https://en.wikipedia.org/wiki?curid=24016215", "title": "Reykjavík Green Days", "text": "Reykjavík Green Days\n\nThe Reykjavík Green Days is an annual event which takes place in Reykjavík, Iceland. This event was founded by a group of students at the University of Iceland in the Environmental and Natural resources program.\n\n\"Gaia\", as the group is known, aims to raise awareness and stimulate citizens, businesses, non-governmental organizations, local authorities and other urban actors to improve the environment's quality by implementing changes in behavior.\n\nIt's considered by the Reykjavík town Council an environmental education initiative that encourages all stakeholders to take steps towards becoming more environmentally responsible.\n\nThe initiave has been organized by the SEED Iceland association in collaboration with other minor local institutions. The staff has been made up of people with 9 nationalities and so the event has been a relevant tourist attraction.\nThe most important happening of the 2009 edition was the Reykjavík critical mass, which was actually the first one in Iceland.\n\n"}
{"id": "22032425", "url": "https://en.wikipedia.org/wiki?curid=22032425", "title": "Runwise", "text": "Runwise\n\nRunWise is an energy recovery project within the Parker Hannifin Corporation. The plans of this project are: being part of the powertrain on trucks performing multiple stop and go cycles, the Runwise system will store energy from vehicle braking into accumulators, releasing the power again for acceleration.\n\nAutocar, maker of severe service trucks, is partnering with Parker Hannifin to release a refuse truck utilizing the RunWise system.\n"}
{"id": "16210264", "url": "https://en.wikipedia.org/wiki?curid=16210264", "title": "Seasonally adjusted annual rate", "text": "Seasonally adjusted annual rate\n\nThe seasonally adjusted annual rate (SAAR) is a rate that is adjusted to take into account typical seasonal fluctuations in data and is expressed as an annual total. SAARs are used for data affected by seasonality, when it could be misleading to directly compare different times of the year.\n\nSAARs are often used for car sales. Other examples of when SAARs could be used are occupancy rates at ski resorts, which are higher in the winter, or sales of ice cream, which are higher in the summer. Sales between seasons can be more easily compared using seasonally adjusted rates. The SAAR is calculated by dividing the unadjusted annual rate for the month by its seasonality factor and multiplying by 12 to create an annual rate. (Quarterly data would be multiplied by 4.)\n\n"}
{"id": "2030045", "url": "https://en.wikipedia.org/wiki?curid=2030045", "title": "Specific energy", "text": "Specific energy\n\nSpecific energy is energy per unit mass. (It is also sometimes called \"energy density,\" though \"energy density\" more precisely means energy per unit volume.) It is used to quantify, for example, stored heat and other thermodynamic properties of substances such as specific internal energy, specific enthalpy, specific Gibbs free energy, and specific Helmholtz free energy. It may also be used for the kinetic energy or potential energy of a body. Specific energy is an intensive property, whereas energy and mass are extensive properties.\n\nThe SI unit for specific energy is the joule per kilogram (J/kg). Other units still in use in some contexts are the kilocalorie per gram (Cal/g or kcal/g), mostly in food-related topics, watt hours per kilogram in the field of batteries, and the Imperial unit BTU per pound (BTU/lb), in some engineering and applied technical fields. The gray and sievert are specialized measures for specific energy absorbed by body tissues in the form of radiation. The following table shows the factors for converting to J/kg:\n\nThe concept of specific energy is related to but distinct from the chemical notion of \"molar\" energy, that is energy per mole of a substance, which uses units of energy per mole, such as J/mol, kJ/mol, or the older (but still widely used) kcal/mol.\n\nFor a table giving the specific energy of many different fuels as well as batteries, see the article Energy density.\n\nEnergy density is the amount of energy per mass or volume of food. The energy density of a food can be determined from the label by dividing the energy per serving (usually in kilojoules or food calories) by the serving size (usually in grams, milliliters or fluid ounces). Energy density is thus expressed in cal/g, kcal/g, J/g, kJ/g, cal/mL, kcal/mL, J/mL, or kJ/mL. The \"calorie\" commonly used in nutritional contexts is the kilogram-calorie (abbreviated \"Cal\" and sometimes called the \"dietary calorie\", \"food calorie\" or \"Calorie\" with a capital \"C\"). This is equivalent to a thousand gram-calories (abbreviated \"cal\") or one kilocalorie (kcal). Because food energy is commonly measured in Calories, the energy density of food is commonly called \"caloric density\".\n\nEnergy density measures the energy released when the food is metabolized by a healthy organism when it ingests the food (see food energy for calculation) and the food is metabolized with oxygen, into waste products such as carbon dioxide and water. Besides alcohol the only sources of food energy are carbohydrates, fats and proteins, which make up ninety percent of the dry weight of food. Therefore, water content is the most important factor in energy density. Carbohydrates provide four calories per gram (17 kJ/g), and proteins offer slightly less at 16kJ/g whereas fat provides nine calories per gram (38 kJ/g), times as much energy. Fats contain more carbon-carbon and carbon-hydrogen bonds than carbohydrates or proteins and are therefore richer in energy. Foods that derive most of their energy from fat have a much higher energy density than those that derive most of their energy from carbohydrates or proteins, even if the water content is the same. Nutrients with a lower absorption, such as fiber or sugar alcohols, lower the energy density of foods as well. A moderate energy density would be 1.6 to 3 calories per gram (7–13 kJ/g); salmon, lean meat, and bread would fall in this category. High-energy foods would have more than three calories per gram and include crackers, cheese, dark chocolate, and peanuts.\n\nEnergy density is sometimes useful for comparing fuels. For example, liquid hydrogen fuel has a higher specific energy (energy per unit mass) than gasoline does, but a much lower volumetric energy density.\n\nSpecific mechanical energy, rather than simply energy, is often used in astrodynamics, because gravity changes the kinetic and potential specific energies of a vehicle in ways that are independent of the mass of the vehicle, consistent with the conservation of energy in a Newtonian gravitational system.\n\nThe specific energy of an object such as a meteoroid falling on the earth from outside the earth's gravitational well is at least one half the square of the escape velocity of 11.2 km/s. This comes to 63 MJ/kg (15 kcal/g, or 15 tonnes TNT equivalent per tonne). Comets have even more energy, typically moving with respect to the sun, when in our vicinity, at about the square root of two times the speed of the earth. This comes to 42 km/s, or a specific energy of 882 MJ/kg. The speed relative to the earth may be more or less, depending on direction. Since the speed of the earth around the sun is about 30 km/s, a comet's speed relative to the earth can range from 12 to 72 km/s, the latter corresponding to 2592 MJ/kg. If a comet with this speed fell to the earth it would gain another 63 MJ/kg, yielding a total of 2655 MJ/kg with a speed of 72.9 km/s. Since the equator is moving at about 0.5 km/s, the impact speed has an upper limit of 73.4 km/s, giving an upper limit for the specific energy of a comet hitting the earth of about 2690 MJ/kg.\n\nIf the Hale-Bopp comet (50 km in diameter) had hit the earth, it would have vaporized the oceans and sterilized the surface of the earth.\n\n\n"}
{"id": "40523295", "url": "https://en.wikipedia.org/wiki?curid=40523295", "title": "Specific mechanical energy", "text": "Specific mechanical energy\n\nSpecific mechanical energy is the mechanical energy of an object per unit of mass. Similar to mechanical energy, the specific mechanical energy of an object in an isolated system subject only to conservative forces will remain constant.\n\nIt is defined as:\n\nformula_1\n\nwhere\n\n\n\nIn the gravitational two-body problem, the specific mechanical energy formula_4 is given as:\n\nformula_5\n\nwhere\n\n\nWhen calculating the specific mechanical energy of a satellite in orbit around a celestial body, the mass of the satellite is assumed to be negligible:\n\nformula_12\n\nwhere formula_13 is the mass of the celestial body.\n"}
{"id": "23584640", "url": "https://en.wikipedia.org/wiki?curid=23584640", "title": "Sturgeon River (Michigan)", "text": "Sturgeon River (Michigan)\n\nSturgeon River may refer to any of the following streams in the U.S. state of Michigan:\n\n\n"}
{"id": "1221489", "url": "https://en.wikipedia.org/wiki?curid=1221489", "title": "The Flight of the Phoenix", "text": "The Flight of the Phoenix\n\nThe Flight of the Phoenix is a 1964 novel by Elleston Trevor. The plot involves the crash of a transport aircraft in the middle of a desert and the survivors' desperate attempt to save themselves. The book was the basis for the 1965 film \"The Flight of the Phoenix\" starring James Stewart and the 2004 remake entitled \"Flight of the Phoenix\".\n\"The Flight of the Phoenix\" came at the midpoint of Trevor's career and led to a bidding war over its film rights.\n\nPilot Frank Towns and navigator Lew Moran are ferrying a mixed bag of passengers out of the Jebel oil town of the Libyan desert, among them oil workers, a couple of British soldiers, and a German who was visiting his brother. An unexpected sandstorm forces the aircraft down, damaging it, killing two of the men, and severely injuring the German. In the book, the action takes place in the Libyan part of the Sahara.\n\nThe survivors wait for rescue but begin to worry, as the storm has blown them far off course, away from where searchers would look for them. After several days, Captain Harris marches toward a distant oasis together with another passenger. His aide, Sergeant Watson feigns a sprained ankle and does not join Harris. A third man follows after them. Days later, Harris barely manages to return to the crash site. The others are lost.\n\nAs the water begins to run out, Stringer, a precise, arrogant English aeronautical engineer, proposes a radical solution. He claims they can rebuild a new aircraft from the wreckage of the old twin-boom aircraft, using the undamaged boom and adding skids to take off. They set to work.\n\nAt one point, they spot a party of nomadic tribesmen. Captain Harris decides to ask them for help, but Sergeant Watson refuses to accompany him. Instead another survivor, a Texan named Loomis, goes with him. The next day, Towns finds their looted bodies, throats cut, and the nomads gone.\n\nLater, Towns finds out that Stringer's job is designing \"model\" aircraft, not real, full-scale ones. Afraid of the effect on morale, he and Moran keep their discovery secret, though they now believe Stringer's plan is doomed. However, they turn out to be wrong. The aircraft is reborn, like the mythical Phoenix. It flies the passengers, strapped to the outside of the fuselage, to an oasis and civilization.\n\nThe book was the basis of a 1965 film starring James Stewart, Richard Attenborough, and Hardy Krüger. The 2004 remake featuring Dennis Quaid was also based on the novel, although it was re-located to the Gobi Desert in Asia.\n\n"}
{"id": "15190312", "url": "https://en.wikipedia.org/wiki?curid=15190312", "title": "Theistic rationalism", "text": "Theistic rationalism\n\nTheistic rationalism is a hybrid of natural religion, Christianity, and rationalism, in which rationalism is the predominant element.\nAccording to Henry Clarence Thiessen, the concept of theistic rationalism first developed during the eighteenth century as a form of English and German Deism.\nThe term \"theistic rationalism\" occurs as early as 1856, in the English translation of a German work on recent religious history.\nSome scholars have argued that the term properly describes the beliefs of some of the prominent Founding Fathers of the United States, including George Washington, John Adams, Benjamin Franklin, James Wilson, and Thomas Jefferson.\n\nTheistic rationalists believe natural religion, Christianity, and rationalism typically coexist compatibly, with rational thought balancing the conflicts between the first two aspects. They often assert that the primary role of a person's religion should be to bolster morality, a fixture of daily life. \n\nTheistic rationalists believe that God plays an active role in human life, rendering prayer effective.\nThey accept parts of the Bible as divinely inspired, using reason as their criterion for what to accept or reject.\nTheir belief that God intervenes in human affairs and their approving attitude toward parts of the Bible distinguish theistic rationalists from Deists.\n\nAnthony Ashley-Cooper, 3rd Earl of Shaftesbury (1671-1713), has been described as an early theistic rationalist. According to Stanley Grean, \n\n\n"}
{"id": "10545493", "url": "https://en.wikipedia.org/wiki?curid=10545493", "title": "Wuluwait", "text": "Wuluwait\n\nWuluwait is a god from northern Arnhem Land (in northern Australia) and is known to work with Bunbulama as a rainmaker. He is also recorded by Charles Mountford and Ainslie Roberts as a boatman who ferries the souls of the dead to Purelko, the aboriginal afterlife.\n\n\"The Dreamtime\" (1965) Ainslie Roberts and Charles P Mountford, Adelaide, Rigby Pty Ltd.\n"}
