{"id": "2707177", "url": "https://en.wikipedia.org/wiki?curid=2707177", "title": "2004 Argentine energy crisis", "text": "2004 Argentine energy crisis\n\nThe Argentine energy crisis was a natural gas supply shortage experienced by Argentina in 2004. After the recession triggered by the economic crisis and ending in 2002, Argentina's energy demands grew quickly as industry recovered, but extraction and transportation of natural gas, a cheap and relatively abundant fossil fuel, did not match the surge.\n\nAccording to estimates, 50% of the electricity generated in Argentina depends on gas-powered plants. The national energy matrix has no emergency reserves and by 2004 it was functioning at the top of its capacity. At this point, barely emerging from the seasonal low demand caused by summer, a large number of industrial facilities and power plants started suffering intermittent cuts in their supply of natural gas. Between February and May the cuts amounted to an average of 9.5 million m³ a day, about 13% of industrial demand, and by the end of May they grew to a maximum of 22 million m³. The most seriously affected regions were the capital, certain regions of the province of Buenos Aires, and the province of La Pampa.\n\nAs winter approached, the Argentine government announced that it would restrict natural gas exports in order to preserve the supply for internal consumption, both domestic and industrial, in compliance with the Hydrocarbons Law. These export cuts would seriously harm Chile and affect Uruguay and Brazil.\n\nThe Chilean Minister of Economy and Energy, Jorge Rodríguez, warned Argentina that supply contracts with Chilean companies must be fulfilled. This caused a mild diplomatic crisis. Chile imports more than 90% of its natural gas from Argentina and depends heavily on it to generate electricity; it has shifted the focus from coal and oil towards gas, and had five gas pipelines built for the specific purpose of getting gas from Argentina.\n\nThe energy crisis was blamed on a number of factors. Former Argentine President Néstor Kirchner attributed it on lack of investment on the part of the private companies that extract the resource (such as Repsol YPF), and the concomitant lack of pressure from past governments on those companies.\n\nThe private corporations contended that their profits after the collapse of the Argentine economy were severely hurt by the freezing of domestic and industrial fees since 2002. Natural gas remained at the same price during the inflationary process caused by the devaluation of the Argentine peso, while the prices of gasoline and diesel were adjusted upwards, which increased the demand for gas as a cheap alternative fuel and at the same time discouraged its production. In addition to this, a larger part of the supply of natural gas was required to compensate for a smaller yield of hydroelectricity.\n\nThe exporters complained that heavy export tariffs compounded with the price freezing and prevented them from investing on more surveyance and further exploitation, thus leaving them unable to keep up with demand. However, the government and critics of the neoliberal model of the Menem administration point out that the privatized companies obtained huge profits during the 1990s.\n\nIn order to diminish the impact of the crisis, three measures were suggested: buying natural gas from Bolivia, which has abundant reserves of it; directly buying electricity from Brazil, which generates a large part of it using hydroelectric power plants; and importing oil from Venezuela.\n\nFor historical reasons, Bolivia would not sell natural gas to Chile. Moreover, it lacks the infrastructure to convey it. A projected gas pipeline that would transport massive amounts of gas to Argentina was delayed by the critical political situation in Bolivia during 2003. Moreover, some people and organizations in Bolivia have expressed strong disagreement about the idea of exporting gas, calling the energy crisis \"a fiction\".\n\nThe Venezuelan Chávez administration, which at the time was politically close to the Argentine government, signed energy accords that including sending fuel oil tankers to Argentina at reduced costs, through PDVSA (the Venezuelan state oil company). Fuel oil (imported or otherwise) is, in any case, considerably more expensive than natural gas.\n\nIn addition to industrial supply, Argentina employs Compressed Natural Gas for stoves, ovens, etc., and as fuel for over 1.4 million natural gas vehicles. While the possibility of restricting domestic usage was considered, it was deemed unnecessary and disruptive.\n\nAs a response to the 2001 economic crisis, electricity tariffs were converted to the Argentine peso and frozen in January 2002 through the Public Emergency and Exchange Regime Law. Together with high inflation (see Economy of Argentina) and the devaluation of the peso, many companies in the sector had to deal with high levels of debt in foreign currency under a scenario in which their revenues remained stable while their costs increased. This situation has led to severe underinvestment and unavailability to keep up with an increasing demand, factors that contributed to the 2003-2004 energy crisis. Since 2003, the government has been in the process of introducing modifications that allow for tariff increases. Industrial and commercial consumers' tariffs have already been raised (near 100% in nominal terms and 50% in real terms), but residential tariffs still remain the same. Nevertheless, the national government even tried to profit from the crisis by creating a new oil company, Enarsa, with 53% of state control and full exploitation rights over offshore areas.\n\nAs 2004 passed with no major disruptions, some people claimed that the so-called \"energy crisis\" had in fact turned out a minor complication, inflated by the government and the media. In a broader context, though, it is still true that investments on exploitation of energy resources, as well as energy production and distribution, are insufficient. In March 2005, President Kirchner admitted that \"for a long time the possibility will remain that we must move on the brink <nowiki>[</nowiki>of a crisis<nowiki>]</nowiki>\". However, the government also pointed out that remedies are on the works, and that Argentina is better prepared than in 2004 to face problems with energy generation.\n\nIn the meantime, fuel oil supply from Venezuela has continued, amounting to 50 million tonnes sent in two ships (in April and May) by PDVSA, in a coordinated effort with the Brazilian oil company Petrobras and the Electrical Market Management Company of Argentina (Cammesa).\n\nAnalysts and officials, such as former President of Uruguay Jorge Batlle, have remarked that a full-fledged protocol for energetic integration of Mercosur should be outlined and brought into action as soon as possible to coordinate energy production and distribution in the region.\n\n"}
{"id": "92873", "url": "https://en.wikipedia.org/wiki?curid=92873", "title": "Alignak", "text": "Alignak\n\nIn Inuit mythology, Alignak is a lunar deity and god of weather, water, tides, eclipses, and earthquakes.\n"}
{"id": "264009", "url": "https://en.wikipedia.org/wiki?curid=264009", "title": "Amiidae", "text": "Amiidae\n\nThe Amiidae are a family of basal ray-finned fishes. The bowfin is the only species to survive today, although additional species in all four subfamilies of Amiidae are known from Jurassic, Cretaceous, and Eocene fossils.\n\nBowfins are now found throughout eastern North America, typically in slow-moving backwaters, canals, and ox-bow lakes. When the oxygen level is low (as often happens in still waters), the bowfin can rise to the surface and gulp air into its swim bladder, which is lined with blood vessels and can serve as a primitive lung.\nThe family is divided into four subfamilies, with 11 genera described:\n"}
{"id": "236717", "url": "https://en.wikipedia.org/wiki?curid=236717", "title": "Anticyclone", "text": "Anticyclone\n\nAn anticyclone (that is, opposite to a cyclone) is a weather phenomenon defined by the United States National Weather Service's glossary as \"a large-scale circulation of winds around a central region of high atmospheric pressure, clockwise in the Northern Hemisphere, counterclockwise in the Southern Hemisphere\". Effects of surface-based anticyclones include clearing skies as well as cooler, drier air. Fog can also form overnight within a region of higher pressure. Mid-tropospheric systems, such as the subtropical ridge, deflect tropical cyclones around their periphery and cause a temperature inversion inhibiting free convection near their center, building up surface-based haze under their base. Anticyclones aloft can form within warm core lows such as tropical cyclones, due to descending cool air from the backside of upper troughs such as polar highs, or from large scale sinking such as the subtropical ridge.\nThe evolution of an anticyclone depends on a few variables such as its size, intensity, moist-convection, Coriolis force etc .\nSir Francis Galton first discovered anticyclones in the 1860s. Preferred areas within a synoptic flow pattern in higher levels of the hydrosphere are beneath the western side of troughs, or dips in the Rossby wave pattern. High-pressure systems are alternatively referred to as anticyclones. Their circulation is sometimes referred to as cum sole. Subtropical high pressure zones form under the descending portion of the Hadley cell circulation. Upper-level high-pressure areas lie over tropical cyclones due to their warm core nature.\nSurface anticyclones form due to downward motion through the troposphere, the atmospheric layer where weather occurs. Preferred areas within a synoptic flow pattern in higher levels of the troposphere are beneath the western side of troughs. On weather maps, these areas show converging winds (isotachs), also known as confluence, or converging height lines near or above the level of non-divergence, which is near the 500 hPa pressure surface about midway up the troposphere. Because they weaken with height, these high-pressure systems are cold.\n\nHeating of the earth near the equator forces upward motion and convection along the monsoon trough or intertropical convergence zone. The divergence over the near-equatorial trough leads to air rising and moving away from the equator aloft. As air moves towards the mid-latitudes, it cools and sinks leading to subsidence near the 30° parallel of both hemispheres. This circulation known as the Hadley cell forms the subtropical ridge. Many of the world's deserts are caused by these climatological high-pressure areas. Because these anticyclones strengthen with height, they are known as warm core ridges.\n\nThe development of anticyclones aloft occurs in warm core cyclones such as tropical cyclones when latent heat caused by the formation of clouds is released aloft increasing the air temperature; the resultant thickness of the atmospheric layer increases high pressure aloft which evacuates their outflow.\n\nIn the absence of rotation, the wind tends to blow from areas of high pressure to areas of low pressure. The stronger the pressure difference (pressure gradient) between a high-pressure system and a low-pressure system, the stronger the wind. The coriolis force caused by Earth's rotation gives winds within high-pressure systems their clockwise circulation in the northern hemisphere (as the wind moves outward and is deflected right from the center of high pressure) and anticlockwise circulation in the southern hemisphere (as the wind moves outward and is deflected left from the center of high pressure). Friction with land slows down the wind flowing out of high-pressure systems and causes wind to flow more outward (more ageostrophically) from the center.\n\nHigh-pressure systems are frequently associated with light winds at the surface and subsidence of air from higher portions of the troposphere. Subsidence will generally warm an air mass by adiabatic (compressional) heating. Thus, high pressure typically brings clear skies. Because no clouds are present to reflect sunlight during the day, there is more incoming solar radiation and temperatures rise rapidly near the surface. At night, the absence of clouds means that outgoing longwave radiation (i.e. heat energy from the surface) is not blocked, giving cooler diurnal low temperatures in all seasons. When surface winds become light, the subsidence produced directly under a high-pressure system can lead to a buildup of particulates in urban areas under the high pressure, leading to widespread haze. If the surface level relative humidity rises towards 100 percent overnight, fog can form.\n\nThe movement of continental arctic air masses to lower latitudes produces strong but vertically shallow high-pressure systems. The surface level, sharp temperature inversion can lead to areas of persistent stratocumulus or stratus cloud, colloquially known as anticyclonic gloom. The type of weather brought about by an anticyclone depends on its origin. For example, extensions of the Azores high pressure may bring about anticyclonic gloom during the winter because they pick up moisture as they move over the warmer oceans. High pressures that build to the north and move southwards often bring clear weather because they are cooled at the base (as opposed to warmed) which helps prevent clouds from forming.\n\nOnce arctic air moves over an unfrozen ocean, the air mass modifies greatly over the warmer water and takes on the character of a maritime air mass, which reduces the strength of the high-pressure system. When extremely cold air moves over relatively warm oceans, polar lows can develop. However, warm and moist (or maritime tropical) air masses which move poleward from tropical sources are slower to modify than arctic air masses.\n\nThe circulation around mid-level (altitude) ridges, and the air subsidence at their center, act to steer tropical cyclones around their periphery. Due to the subsidence within this type of system, a \"cap\" can develop which inhibits free convection and hence mixing of the lower with the middle level troposphere. This limits thunderstorm activity near their centers and traps low-level pollutants such as ozone as haze under their base, which is a significant problem in large urban centers during summer months such as Los Angeles, California and Mexico City.\n\nThe existence of upper-level (altitude) high pressure allows upper level divergence which leads to surface convergence. If a capping mid-level ridge does not exist, this leads to free convection and the development of showers and thunderstorms if the lower atmosphere is humid. Because a positive feedback loop develops between the convective tropical cyclone and the upper level high, the two system are strengthened. This loop stops once ocean temperatures cool to below , reducing the thunderstorm activity, which then weakens the upper level high pressure system.\n\nWhen the subtropical ridge in the Northwest Pacific is stronger than normal, it leads to a wet monsoon season for Asia. The subtropical ridge position is linked to how far northward monsoon moisture and thunderstorms extend into the United States. Typically, the subtropical ridge across North America migrates far enough northward to begin monsoon conditions across the Desert Southwest from July to September. When the subtropical ridge is farther north than normal towards the Four Corners, monsoon thunderstorms can spread northward into Arizona. When suppressed to the south, the atmosphere dries out across the Desert Southwest, causing a break in the monsoon regime.\n\nOn weather maps, high-pressure centers are associated with the letter H in English, within the isobar with the highest pressure value. On constant-pressure upper-level charts, anticyclones are located within the highest height line contour.\n\nOn Jupiter, there are two examples of an extraterrestrial anticyclonic storm; the Great Red Spot and the recently formed Oval BA. They are powered by smaller storms merging unlike any typical anticyclonic storm that happens on Earth where water powers them. Another theory is that warmer gases rise in a column of cold air, creating a vortex as is the case of other storms that include Anne's Spot on Saturn, and the Great Dark Spot on Neptune. Anticyclones have been detected near the poles of Venus.\n\n\n\n<br>\n"}
{"id": "15051416", "url": "https://en.wikipedia.org/wiki?curid=15051416", "title": "Barranca de Oblatos", "text": "Barranca de Oblatos\n\nBarranca de Oblatos (), also known as Barranca de Huentitán, is a canyon carved by the Río Grande de Santiago in Mexico in the state of Jalisco. It lies on the northeast side of the municipality of Guadalajara and on the edge of the municipalities of Tonalá, Zapotlanejo, Ixtlahuacán del Río and Zapopan in the Guadalajara Metropolitan Area. Its beauty and structure make it a slightly smaller scale version of the Grand Canyon in the United States, and Barranca del Cobre in Chihuahua.\n\nIt includes approximately 1.137 hectares and it has an average depth of 600 meters. The difference in elevation between the rim of the canyon (1.520 msnm) and the river (1.000 mvsl) is 520 meters (1,706 feet) at the point of a funicular. This canyon is also named Oblatos-Huentitán due to the areas in the city crossed by it, called Oblatos and Huentitán respectively.\n\nSeveral important events in the history of Guadalajara occurred in the canyon. In the 19th century, during the Spanish Conquest combat between the indigenous natives of Huentitán and the Spaniards took place in this area. This was also the location of many battles of the Mexican Revolution and Cristero War. But perhaps the most interesting story about this canyon is that of a flooding that took place during the \"Porfiriato\" era (1876-1911).\n\nThe Canyon is considered a biogeographic corridor since it is home to four types of vegetation: Tropical Forest, Deciduous, Riparian forest, Rupicolous Gerbil vegetation and secondary vegetation. Several species of flora and fauna are endemic to the canyon. It is often visited by national and international investigators since it is includes great biological diversity. \nOn June 5, 1997 the canyon was declared a Nature reserve, under the category of Zone subject to Ecological Conservation Protected Area by the World Conservation Union of all the area belonging to the Guadalajara Metropolitan Area because it is the most urbanized area. Great Horned Owls, Collared Peccaries, Bobcats, Gray Foxes, Opossums, Red-tailed Boas, Barn Owls, Leaf Cutter Ants and Vampire Bats are among the species making a home in the canyon.\n\nThere are several urbanized areas that cover the eastern side of the canyon, complexes such as the University of Guadalajara campus of the CUAAD Center of Art, Architecture and Design, the Guadalajara Zoo, and the now closed Guadalajara Planetarium. In addition to several residential areas, there are also sporting and recreational facilities that include soccer fields, basketball, tennis and fronton courts, picnic spaces, a running strip, a recreational park and an outdoor theater. There were plans to build the next Guggenheim Museum in early 2008, the controversial Arcediano dam project and the high-rise project \"Puerta Guadalajara\" (Guadalajara Gate) which included a shopping mall, a convention center, two hotels, two museums, 9 residencial towers and two more corporate towers. None of these projects have been started and it seems likely that they have now all been cancelled or postponed indefinitely. However, work is in progress on the construction of an art museum in the Mirador park, on the site formerly intended for the Guggenheim museum project.\n\n"}
{"id": "35325932", "url": "https://en.wikipedia.org/wiki?curid=35325932", "title": "Bigu (grain avoidance)", "text": "Bigu (grain avoidance)\n\nBigu () is a Daoist fasting technique associated with achieving \"xian\" \"transcendence; immortality\". Grain avoidance is related to multifaceted Chinese cultural beliefs. For instance, \"bigu\" fasting was the common medical cure for expelling the \"sanshi\" 三尸 \"Three Corpses\", the malevolent, grain-eating spirits that live in the human body (along with the hun and po souls), report their host's sins to heaven every 60 days, and carry out punishments of sickness and early death. Avoiding \"grains\" has been diversely interpreted to mean not eating particular foodstuffs (food grain, cereal, the Five Grains, \"wugu\", or staple food), or not eating any food (inedia, breatharianism, or aerophagia). In the historical context of traditional Chinese culture within which the concept of \"bigu\" developed, there was great symbolic importance connected with the five grains and their importance in sustaining human life, exemplified in various myths and legends from ancient China and throughout subsequent history. The concept of \"bigu\" developed in reaction to this tradition, and within the context of Daoist philosophy.\n\nThe Chinese word \"bigu\" compounds \"bi\" 辟 \"ruler; monarch; avoid; ward off; keep away\" and \"gu\" 穀 or 谷 \"cereal; grain; (穀子) millet\". The \"bi\" 辟 meaning in \"bigu\" is a variant Chinese character for \"bi\" 避 \"avoid; shun; evade; keep away\" (e.g., \"bixie\" 辟邪 or避邪 \"ward off evil spirits; talisman; amulet\"). The alternate pronunciation of \"pi\" 辟 \"open up; develop; refute; eliminate\" is a variant character for 闢. The complex 14-stroke traditional Chinese character \"gu\" 穀 \"grain\" has a 7-stroke simplified Chinese character \"gu\" 谷 \"valley; gorge.\" Although a few Chinese dictionaries (e.g., Liang & Chang 1971, Lin 1972) gloss the pronunciation of \"bigu\" 辟穀 as \"pigu\", the definitive \"Hanyu Da Cidian\" (1997) gives \"bigu\".\n\nEnglish lexicographic translations of \"bigu\" are compared in this table. \nCatherine Despeux (2008:233) lists synonyms for \"bigu\" \"abstention from cereals\": \"duangu\" 斷穀 \"stopping cereals\" (with \"duan\" 斷 \"cut off; sever; break; give up\"), \"juegu\" 絕穀 \"discontinuing cereals\" (\"jue\" 絕 \"cut off; sever; refuse; reject\"), \"quegu\" 卻穀 \"refraining from cereals\" (\"que\" 卻 \"retreat; decline; reject; refuse\"), and \"xiuliang\" 修糧 \"stopping grains\" (with \"xiu\" 修 \"repair; trim; prune' cultivate\" and \"liang\" 糧 \"grain; food\").\n\n\"Juegu\", unlike these other alternative expressions, had meanings besides Daoist dietary practices. For instance, the (c. 139 BCE) \"Huainanzi\" uses \"juegu\" in a traditional saying (tr. Major et al. 2010:775): \"Now, rejecting study because those who study have faults is like taking one instance of choking to refuse grain and not eat or taking one problem with stumbling to stop walking and not go [anywhere].\" About one century later, Liu Xiang's \"Shuoyuan\" 說苑 \"Garden of Stories\" rephrases this simile about choking once and discontinuing grains.\n\nChinese folklore and mythology associated several divinities with agriculture and grains.\nIn ancient times, the people fed on herbaceous plants and drank [only] water, picked fruit from shrubs and trees and ate the meat of oysters and clams. They frequently suffered tribulations from feverish maladies and injurious poisons. Consequently, the Divine Farmer first taught the people to plant and cultivate the five grains. He evaluated the suitability of the land, [noting] whether it was dry or wet, fertile or barren, high or low. He tried the taste and flavor of the one hundred plants and the sweetness or bitterness of the streams and springs, issuing directives so the people would know what to avoid and what to accept. At the time [he was doing this], he suffered poisoning [as many as] seventy times a day. (19, tr. Major et al. 2010:766-767)\n\nWhile traditional Chinese mythology depicted cooking and agriculture as key elements of civilization, the Daoists created a \"counter-narrative\" (Campany 2005:16) to justify the idea of grain avoidance. For example, the Confucianist \"Xunzi\" and Legalist \"Hanfeizi\" describe Suiren as cultural folk hero.\nIn the earliest times ... the people lived on fruit, berries, mussels, and clams – things that sometimes became so rank and fetid that they hurt people's stomachs, and many became sick. Then a sage appeared who created the boring of wood to produce fire so as to transform the rank and putrid foods. The people were so delighted by this that they made him ruler of the world and called him the Fire-Drill Man (Suiren 燧人). (\"Hanfeizi\" 49, tr. Campany 2005:15) \nIn contrast, the \"Zhuangzi\" \"Mending Nature\" chapter mentions Suiren first in a list of mythic sage-rulers – Fu Xi, Shennong, Yellow Emperor, Tang of Shang, and Yu the Great, traditionally credited with advancing civilization – but depicts them as villains who began the destruction of the primal harmony of the Dao. Campany (2005:16) calls this \"the decline of Power and the ever-farther departure from the natural Dao into systems of social constraint and what passes for culture.\" \nThe ancients, in the midst of chaos, were tranquil together with the whole world. At that time, yin and yang were harmoniously still, ghosts and spirits caused no disturbances; the four seasons came in good time; the myriad things went unharmed; the host of living creatures escaped premature death. … This condition persisted until integrity deteriorated to the point that Torchman [Suiren] and Fuhsi arose to manage all under heaven, whereupon there was accord, but no longer unity. Integrity further declined until the Divine Farmer and the Yellow Emperor arose to manage all under heaven, whereupon there was repose, but no longer accord. Integrity declined still further until T'ang and Yu arose to manage all under heaven. They initiated the fashion of governing by transformation, whereby purity was diluted and simplicity dissipated. (tr. Mair 1994:149) \n\nThe traditional Chinese symbol for civilization and state was \"gu\" \"grains; cereals\" (a synecdoche for \"agricultural products\").\n\nThe \"Wangzhi\" \"Royal Regulations\" chapter of the \"Liji\" uses cooking food and eating grains to culturally classify the Chinese \"Middle Kingdom\" bordered by the \"Four Barbarians\" (eastern Yi, southern Man, western Rong, and northern Di).\nThus the people of the five regions … each had their several natures, which they could not be made to alter. Those of the east were called Yi; they wore their hair unbound and tattooed their bodies, and some of them ate their food without cooking it. [The people of] the south were called Man; they tattooed their foreheads and had their feet turned in toward each other, and some among them ate their food without cooking it. [The people of] the west were called Rong; they wore their hair unbound and wore skins, and some of them did not eat grain. [The people of] the north were called Di; they wore feathers and furs and lived in caves, and some of them did not eat grain. (tr. Campany 2005:7-8)\nKwang-chih Chang (1977:42) interprets this \"Liji\" context to mean, \"One could eat grain but also eat raw meat or one could eat his meat cooked but eat no grain. Neither was fully Chinese. A Chinese by definition ate grain and cooked his meat.\"\n\nDuring the first dynasties of the Qin and Han, when Daoism simultaneously became a mass movement, Chinese agricultural techniques were revolutionized. Applying methods from the (256 BCE) Dujiangyan Irrigation System, arable land was converted into rice fields, with two or more harvests annually, resulting in widespread deforestation.\n\nThe \"nong\" 農 \"peasant; farmer\" was second-highest of the Four Occupations under the traditional Chinese feudal system. Kristofer Schipper says,\nThe peasants depended entirely on agriculture and were forever tied to their land through all kinds of fiscal and administrative measures. As a result, the rural communities became an easy prey to all the ills of sedentary civilization: ever-higher taxes, enslavement to the government through corvée labor and military draft, epidemics, periodic shortages and famines, and wars and raids by non-Chinese tribes from across the borders. (Schipper 1993:168) \nWhen natural or human catastrophes occurred, the peasants could take refuge in non-arable regions of mountains and survive on wild foods other than grains.\n\nThe \"sheji\" 社稷 \"altars to soil and grain gods\" were the ritual center of a Chinese state. Originally, \"she\" 社 was the \"god of the land\" and \"ji\" 稷 the \"god of the harvest\" (cf. Houji above), and the compound \"sheji\" \"gods of soil and grain\" metaphorically means \"the state; the nation\". The \"Shiji\" (Campany 2005:21) says establishing a new dynasty required eliminating the \"sheji\" altars of the preceding dynasty and erecting one's own.\n\nOfferings of grain, liquor (a grain product), and meat were necessary not only for \"sheji\" sacrifices but for ancestral sacrifices. The obligation to feed the ancestral dead was fundamental to Chinese society. Campany summarizes the cultural importance of sacrificing \"grains\" to feed both natural and ancestral spirits.\nGrain was, in short, a symbol and summation of culture itself, or rather of nature acculturated, as well as of the completely human community. A natural locus of nutritive \"essence\" (\"jing\"), grain nevertheless required cooperative, communal and differentiated stages of production—planting, tending, harvesting, storing, thrashing, milling, mixing, and cooking—to be transformed into food. Thus transformed, it was perhaps the most culturally celebrated food of humans (both living and dead) and of gods. (2005:24) \nThe Chinese character for \"jing\" 精 \"spirit; essence of life; energy\" is written with the rice radical 米.\n\nThe first textual references to \"avoiding grains/cereals\" are found in Chinese classics from the Warring States period (475–221 BCE), Qin Dynasty (221 BCE–206 BCE), and Han Dynasty (206 BCE–220 CE).\n\nA (c. 3rd century BCE) \"Zhuangzi\" chapter describes a \"shenren\" 神人 \"divine person\" who does not eat grains but mysteriously helps them grow.\nFar away on Mount Kuyeh there dwells a spirit man whose skin is like congealed snow and who is gentle as a virgin. He does not eat any of the five grains, but inhales the wind and drinks the dew. He rides on the clouds, drives a flying dragon, and wanders beyond the four seas. His spirit is concentrated, saving things from corruption and bringing a bountiful harvest every year. (1, tr. Mair 1994:6-7) \nIn this passage, Maspero (1981:417) recognizes the principal Daoist practices that were current during the Six Dynasties period: \"(1) abstention from Cereals, (2) respiratory exercises, and (3) concentration and meditation. The \"journey beyond the Four Seas\" (4) corresponds to a manner of directing ecstasy,\" resembling astral projection.\n\nThe (168 BCE) \"Quegu shiqi\" 卻穀食氣 \"Eliminating Grain and Eating \"Qi\"\" manuscript, which was discovered in 1973 among the Mawangdui Silk Texts, is the oldest documented grain-avoidance diet (tr. by Harper 1998:305-9). This Chinese medical manual outlines a method for replacing grains with \"qi\" circulations, and consuming medicinal herbs, notably the fern \"shiwei\" 石韋 \"Pyrrosia lingua\" as a diuretic to treat urine retention resulting from eliminating grains. This text (tr. Campany 2005:31) dichotomizes diets with the square-earth round-heaven model from Chinese cosmography and fengshui, \"Those who eat grain eat what is square; those who eat \"qi\" eat what is round. Round is heaven; square is earth.\"\n\nThe (139 BCE) \"Huainanzi\" chapter on topography (4, tr. Major et al. 2010:161) correlates diet and lifespan. \"Those that feed on flesh are brave and daring but are cruel. Those that feed on \"qi\" [attain] spirit illumination and are long-lived. Those that feed on grain are knowledgeable and clever but short-lived. Those that do not feed on anything do not die and are spirits.\"\n\nSima Qian's (c. 91 BCE) \"Records of the Grand Historian\" (26, tr. Watson 1993 1:113; cf. Yü 1965:92) mentions \"bigu\" in connection with Zhang Liang (262-189 BCE), or the Marquis of Liu, who served as teacher and strategist for Emperor Gaozu of Han (r. 202-195 BCE). Zhang officially requested \"to lay aside the affairs of this world, and join the Master of the Red Pine in immortal sport\" (referring to Chisongzi 赤松子 \"Master Red Pine\", a legendary \"xian\" who, like Guiguzi, abstained from grains), and the emperor permitted it. Zhang Liang \"set about practising dietary restrictions and breathing and stretching exercises to achieve levitation\" (namely, \"bigu\", \"daoyin\", and \"qingshen\" 輕身\"lightening the body\"). After Gaozu died, Empress Lü Zhi urged Zhang to eat, saying, \"Man's life in this world is as brief as the passing of a white colt glimpsed through a crack in the wall. Why should you punish yourself like this?\" Zhang \"had no other recourse but to listen to her advice and begin eating again. Eight years later he died.\" Based upon this account (which is also found in the \"Lunheng\"), Campany (2005:37) concludes that by the late 2nd and 1st centuries BCE, \"the idea that some practitioners were abstaining from grains while practicing methods for consuming, directing, and cultivating \"qi\" as alternate nourishment was ubiquitous and commonplace.\"\n\nThe (ca. 111 CE) \"Book of Han\" mentions \"bigu\" in context with the fangshi \"alchemist; magician\" Li Shaojun teaching Emperor Wu of Han (r. 141-87 BCE) a \"method of worshipping the furnace and abstaining from cereals to prevent old age\" (tr. Despeux 2008:233). Since grains were cooked on the stove, in raw/cooked logic, grain avoidance was traditionally linked with worship of Zaoshen 灶神 The Stove God (Pregadio 2008). In a reversal of not eating the Five Grains to obtain immortality, the \"Book of Han\" also records that in 10 CE, the usurper Wang Mang paid the \"fangshi\" Su Lo蘇樂, who claimed to know the \"xian\" secrets of longevity, to plant some \"immortality grain\" (Yu 1965:115).\n[T]he five grains were planted within the palace in plots facing according to the color of each one. The seeds had been soaked in (a liquid made from) the marrow of the bones of cranes, tortoise-shell (\"tu mao\"), rhinoceros (horn), and jade, in all more than twenty constituents. One bushel of this grain cost one piece of gold. This was called Huang Ti's cereal method for becoming a holy immortal. (tr. Needham 1997:37) \n\nThe Confucian scholar Liu Xiang (79-8 BCE) edited several classical texts, including the (c. 26 BCE) \"Guanzi\" that repeatedly praises grain eating. The first chapter \"Neiye\" \"Inner Training\" begins by comparing the \"jing\" 精 \"essence\" in grains and stars. \nThe vital essence of all things: it is this that brings them to life. It generates the five grains below and becomes the constellated stars above. When flowing amid the heavens and earth, we call it ghostly and numinous. When stored within the chests of human beings, we call them sages. (tr. Roth 2004 1:01) \nCampany (2005:18) knows of \"no text that exalts grains more highly or insists on their importance more strongly than the \"Guanzi\".\" Compare: \"The five grains and the eating of rice are the people's Director of Allotted Lifespans\" (i.e., Siming) and \"In all cases the five grains are the controllers of all things\" (meaning the market price of grains affects all economic values).\n\nLiu Xiang's hagiography of Daoist \"xian\", the \"Liexian Zhuan\" \"Collected Biographies of Immortals\", tells the famous \"Hairy Woman\" legend in terms of grain avoidance.\nDuring the reign of Emperor Cheng of the Han, hunters in the Zhongnan Mountains saw a person who wore no clothes, his body covered with black hair. Upon seeing this person, the hunters wanted to pursue and capture him, but the person leapt over gullies and valleys as if in flight, and so could not be overtaken. The hunters then stealthily observed where the person dwelled, surrounded and captured him, whereupon they determined that the person was a woman. Upon questioning, she said, \"I was originally a woman of the Qin palace. When I heard that invaders from the east had arrived, that the King of Qin would go out and surrender, and that the palace buildings would be burned, I fled in fright into the mountains. Famished, I was on the verge of dying by starvation when an old man taught me to eat the resin and nuts of pines. At first, they were bitter, but gradually I grew accustomed to them. They enabled me to feel neither hunger nor thirst; in winter I was not cold, in summer I was not hot.\" Calculation showed that the woman, having been a member of the Qin King Ziying's harem, must be more than two hundred years old in the present time of Emperor Cheng. The hunters took the woman back in. They offered her grain to eat. When she first smelled the stink of the grain, she vomited, and only after several days could she tolerate it. After little more than two years of this [diet], her body hair fell out; she turned old and died. Had she not been caught by men, she would have become a transcendent. (tr. Campany 2005:38) \nCampany states, \"Few narratives more succinctly summarize the argument that ordinary foods or \"grains\" block the path to transcendence.\" Ge Hong's (3rd century) \"Shenxian zhuan\" gives a different version – including the Hairy Woman's name of Yu Qiang and not mentioning her being captured or fed grains.\n\nTwo chapters of Wang Chong's (c. 80 CE) \"Lunheng\" criticize the practice of avoiding grains as mistaken. The \"Daoist Untruths\" chapter uses Li Shao Jun, who \"knew some clever maneuvers and some fine tricks, which did not fail to produce a wonderful effect\" (tr. Forke 1907:344), to exemplify confusing Daoist \"xian\" immortality techniques with natural longevity (see Yü 1965:111).\nThere are no instances of any one having obtained Tao, but there have been very long-lived persons. People remarking that those persons, while studying Tao and the art of immortality, become over a hundred years old without dying, call them immortals, as the following example will show. At the time of Han Wu Ti there lived a certain Li Shao Chün, who pretended that by sacrificing to the \"Hearth\" and [\"bigu\"] abstaining from eating grain he could ward off old age. He saw the emperor, who conferred high honours upon him. (tr. Forke 1907:344) \nThis context also mentions Wang Ziquiao 王子僑, a son of King Ling of Zhou (r. 571-545 BCE).\nThe idea prevails that those who [\"bigu\"] abstain from eating grain are men well versed in the art of Tao. They say e.g. that Wang Tse Ch'iao and the like, because they did not touch grain, and lived on different food than ordinary people, had not the same length of life as ordinary people, in so far as having passed a hundred years, they transcended into another state of being, and became immortals. That is another mistake. Eating and drinking are natural impulses, with which we are endowed at birth. Hence, the upper part of the body has a mouth and teeth, the inferior part orifices. With the mouth and teeth one chews and eats, the orifices are for the discharge. Keeping in accord with one's nature, one follows the law of heaven, going against it, one violates one's natural propensities, and neglects one's natural spirit before heaven. How can one obtain long life in this way? (tr. Forke 1907:347)\nThe \"Lunheng\" \"Meaning of Sacrifice\" chapter mentions \"juegu\" in criticizing the tradition of presenting food and wine sacrifices to ancestral spirits.\nThe votaries of Taoism studying the art of immortality abstain from eating cereals and take other food than other people with a view to purifying themselves. Ghosts and spirits, however, are still more ethereal than immortals, why then should they use the same food as man? One assumes that after death man loses his consciousness, and that his soul cannot become a spirit. But let us suppose that he did, then he would use different food, and using different food, he would not have to eat human food. Not eating human food, he would not ask us for it, and having nothing to ask at the hands of man, he could not give luck or mishap. (tr. Forke 1907:524)\n\nLu Jia 陸賈's (c. 191 BCE) \"Xinyu\" 新語 \"New Sayings\" criticizes \"bigu\" among other early Daoist \"xian\" transcendental practices.\n\nThe (c. 190-220 CE) \"Xiang'er\" commentary to the \"Daodejing\" contrasts \"qi\"-eaters and grain-eaters.\nTranscendent nobles (\"xianshi\" 仙士) differ from the vulgar in that they do not value glory, rank, or wealth. They value only \"drawing sustenance from the mother\"—that is, [from] their own bodies. In the interior of the body, the \"mother\" is the stomach, which governs the \"qi\" of the five viscera. Commoners eat grain, and when the grain is gone, they die. Transcendent nobles eat grain when they have it, and when they do not, they consume \"qi\". The \"qi\" returns to the stomach, which is the layered sack of the bowels. (tr. Campany 2005:37) \n\nGe Hong's (c. 320 CE) \"Baopuzi\" contains classical discussions of \"bigu\" techniques. For instance, chapter 6, \"The Meaning of 'Subtle'\" (微旨), equates grain avoidance with the supernatural abilities of a \"xian\" transcendent. \nTherefore, by giving up starches one can become immune to weapons, exorcize demons, neutralize poisons, and cure illnesses. On entering a mountain, he can render savage beasts harmless. When he crosses streams, no harm will be done to him by dragons. There will be no fear when plague strikes; and when a crisis or difficulty suddenly arises, you will know how to cope with it. (6, tr. Ware 1966:114-5) \n\nChapter 15, \"Miscellanea\" (雜應), describes \"avoiding grains\" in terms that Campany (2005:39) says are \"tantamount to not eating food at all\" and \"merely swallowing saliva and \"qi\" and ingesting medicinal preparations to suppress appetite and strengthen the body.\" The chapter begins with the interlocutor asking about \"duangu\" \"cutting off grains\" and \"changsheng\" 長生 \"longevity\" (meaning \"eternal life\" in Daoist terminology). \"I should like to inquire whether a man can attain Fullness of Life by merely dispensing with starches. How many methods for this are there altogether, and which is the best?\" Ge Hong gives a lengthy answer, citing both personal observations and textual records. Practitioners medicinally used \"huangqing\" 黃精 \"yellow essence\" (\"polygonatum; Solomon's Seal\") and \"yuyu\" 禹餘糧 \"Yu's leftover grain\" (\"limonite\").\nBy dispensing with starches a man can only stop spending money on grains, but by that alone he cannot attain Fullness of Life. When I inquired of people who had been doing without starches for a long time, they replied that they were in better health than when they were eating starches. When they took thistle and nibbled mercury and when they also took pills of brown hematite twice a day, this triple medication produced an increase in breaths, so that they gained the strength to carry loads on long trips, for their bodies became extremely light in weight. One such full treatment protected the patients' inner organs for five to ten years, but when they swallowed their breaths, took amulets, or drank brine, only loss of appetite resulted, and they did not have the strength for hard work. The Taoist writings may say that if one wishes Fullness of Life the intestines must be clean, and if immortality is desired the intestines must be without feces; but they also say that those eating greens will be good walkers, but at the same time stupid; that those eating meat will be very strong, and also brave. Those eating starches will be wise, but they will not live to an old age, while those eating breath will have gods and spirits within them that never die. This last, however, is only a biased claim advanced by the school that teaches the circulation of breaths. One has no right to claim to use this method exclusively. If you wish to take the great medicines of gold or cinnabar, they will act more quickly if you fast for the preceding hundred days or so. If you cannot fast that long, take them straightway; this will do no great harm, but it will take more time to acquire geniehood. (15, tr. Ware 1966:243-4) \n\nWarning that abandoning grains is difficult – \"If you consider it inconvenient to break with the world, abandon your household, and live high on a peak, you will certainly not succeed\" – Ge Hong notes the popularity of alternative dietary techniques. \nIf you would not distress yourself, it is best not to dispense with starches but merely to regulate the diet, for which there are about a hundred methods. Sometimes, after a few dozen pills of interior-protecting medicines have been taken, it is claimed that appetite is lost for forty or fifty days. (Other times, one or two hundred days are claimed, or the pills must be taken for days or months.) Refined pine and cypress as well as thistle can also protect the interior, but they are inferior to the great medicines, and last only ten years or less. At other times, fine foods are first prepared and consumed to utter satiation, and then medicines are taken to nurture the things that have been eaten, so that they may not be digested. This is claimed to remain valid for three years. If you then wish to revert to the eating of starches, it is necessary to start by swallowing mallows and lard, so that the fine food you prepared will pass from you undigested. (15, tr. Ware 1966:244) \n\nGe Hong chronicles the effects of grain avoidance. \nI have personally observed for two or three years men, who were foregoing starches, and in general their bodies were slight and their complexions good. They could withstand wind, cold, heat, or dampness, but there was not a fat one among them. I admit that I have not yet met any who had not eaten starches in several decades, but if some people cut off from starches for only a couple of weeks die while these others look as well as they do after years, why should we doubt that the (deliberate) fasting could be prolonged still further? If those cut off from starches grow progressively weaker to death, one would normally fear that such a diet simply cannot be prolonged, but inquiry of those pursuing this practice reveals that at first all of them notice a lessening of strength, but that later they gradually get stronger month by month and year by year. Thus, there is no impediment to the possibility of prolongation. All those who have found the divine process for attaining Fullness of Life succeeded by taking medicines and swallowing breath; on this they are all in perfect agreement. A moment of crisis, however, generally occurs at an early stage when medicines are being taken and starches abandoned and it is only after forty days of progressive weakening, as one uses only holy water and feeds solely on breath, that one regains strength. (15, tr. Ware 1966:246-7) \nThis \"holy water\" refers to a Daoist \"fu\" 符 \"talisman\" dissolved in water. Ge Hong further cites an Eastern Wu historical example to show that drinking holy water cannot prevent death. When Emperor Jing of Wu (r. 258-264) heard about Shi Chun 石春, a Daoist healer \"who would not eat in order to hasten the cure when he was treating a sick person,\" he exclaimed, \n\"In a short time this man is going to starve to death.\" Then he had him locked up and guarded, and all that Shih Ch'un requested was two or three quarts of water for making holy water. It went on like this for more than a year, while his complexion became ever fresher and his strength remained normal. The emperor then asked him how much longer he could continue like this, and Shih Ch'un replied that there was no limit; possibly several dozen years, his only fear being that he might die of old age, but it would not be of hunger. The emperor then discontinued the experiment and sent him away. Note that Shih Ch'un's statement shows that giving up starches cannot protract one's years. Some today possess Shih Ch'un's method. (15, tr. Ware 1966:248-9) \n\nIn the \"Baopuzi\", Ge Hong criticizes contemporary charlatans who claimed to have \"duangu\" \"cut off grains\".\nI have also frequently seen ignorant processors who, wishing to boast and amaze and acquire a reputation for not eating when they really knew nothing about such procedures, merely claimed not to eat gruel. Meanwhile, they would drink more than a gallon of wine daily, and dried meats, puddings, jujubes, chestnuts, or eggs were never out of their mouths. Sometimes they would eat large quantities of meat – several dozen pounds daily – swallowing its juices and spitting out anything that was unpleasant. This, however, is actually feasting. Wine drinkers will eat dried meats with their wine but not starches, and they can keep this up for six months to a year without stumbling or falling. Never yet, however, have they claimed that this was \"cut off from starches!\" (15, tr. Ware 1966:248) \n\nThe (c. 4th-5th century) \"Taishang Lingbao Wufuxu\" 太上靈寶五符序 \"Explanations of the Five Numinous Treasure Talismans\", attributed to the Han Daoist Lezichang 樂子長, gives instructions for practicing \"bigu\", swallowing saliva, and ingesting the \"five wonder plants\" (pine resin, sesame, pepper, ginger, and calamus). This \"Explanations\" text includes the (c. 280) \"Lingbao wufu jing\" 靈寶五符經 \"Scripture of the Five Numinous Treasure Talismans\", which says:\nThe Third Immortal King told the Emperor: In the old days I followed a dietetic regimen and attained immortality. My teacher made me increase the sweet spring in my mouth and swallow it in accordance with the following incantation: \"The white stones, hard and rocky, are rolling on and on. The gushing spring, bubbling and pervasive, becomes a thick juice. Drink it and attain long life – Longevity forever longer!\" \nThese twenty-two words—you should follow them! If you can actually do this and nourish on the True One without stopping, swallow from your flowery pond without interruption, then your inner energy will grow and remain strong, never to be weakened. You attain the Tao by avoiding all grains. You will never again have to follow the rhythm of the moon and plant or harvest. Now, the people of mysterious antiquity, they reached old age because they remained in leisure and never ate any grains. As the \"Dayou zhang\" [大有章] (Verse of Great Existence) says: \"The five grains are chisels cutting life away, making the five organs stink and shorten our spans. Once entered into our stomach, there's no more chance to live quite long. To strive for complete avoidance of all death, keep your intestines free of excrement!\" (tr. Kohn 1993:149-150)\nCampany uses internalism and externalism to analyze how early texts justified the idea that \"shiqi\" 食氣 \"eating \"qi\"\" is better than \"shigu\" \"eating grains\". For examples (2005:2), \"We eat X because X makes us live long\" is an internalist rationale based upon essential properties or benefits; \"We eat X and not Y, which is what those other people eat\" is an externalist claim based upon cultural stereotypes. After comprehensive analysis of how early texts describe \"grain\" (i.e., \"mainstream food\") avoidance, from the (c. 320 BCE) \"Zhuangzi\" to the (c. 320 CE) \"Baopuzi\", Campany (2005:43) concludes the (c. 280 CE) \"Lingbao wufu jing\" is the earliest passage \"in which grains are attacked as a food source based on what we might call negative internalist reasons—that is, on the grounds that they cause actual harm to the body in specific, theorized ways.\" Before the 3rd century, Chinese classical texts did not claim that \"grains\" actually harm the body, they argued that \" \"qi\" and other more refined substances, when ingested and circulated in esoterically prescribed ways, give superior and (for some texts at least) longevity-inducing nourishment.\"\nOne of the striking things about the texts we have reviewed is that most of them offer very little by way of an internalist critique of grains or other everyday foods. That is, they all recommend avoiding grains and offer what they tout as superior alternatives, but on the question of precisely \"why\" grains are such inferior nourishment they have little or nothing to say. What little internalist critique we do find comes quite late — apparently Eastern Han at the earliest — and does not seem well developed: ordinary foods, described as rotten and smelly, impurify a body that must be brought into \"qi\"-based resonance with heaven. This impurity is located specifically in the intestines. […] In most discussions, then, it is not that prescribers and practitioners of transcendence arts portrayed ordinary food as harmful; it is rather that they had what they considered superior alternatives to offer. [… But,] \"why\" these diets of \"qi\" or of rare herbs and minerals should be regarded as superior to one of ordinary food is a question that very often remains unanswered; we are merely, but repeatedly and in diverse ways, told \"that\" they are superior. (2005:48)\nEchoing Claude Lévi-Strauss, Campany (2005:50) suggests that grains, inexorably linked with all their cultural and institutional symbolisms, were \"good to oppose\" rather than being seen as intrinsically \"bad to eat.\" One of the major reasons for consuming wild plants and exotic foods was the inherent contrast with eating everyday \"grains\".\n\nThe avoidance of \"grain\" signifies the Daoist rejection of common social practices. According to Kohn (1993:149), \"It is a return to a time in the dawn of humanity when there were as yet no grains; it is also a return to a more primitive and simple way of eating.\"\n\nDaoist \"bigu\" practices created excuses to quit the agricultural-based Chinese society in which grains were necessary for basic food, ancestral sacrifices, and tax payments. \nThe \"cutting off\" of grains, which were the basic staple food for the peasants, was also a rejection of their sedentary life and the peasant condition as such. This refusal should not solely be interpreted in the light of the miseries endured by farmers, but also in a much more fundamental way. Agriculture has occasioned, since Neolithic times, a radical break with the way of life that prevailed for almost the entire prehistory of humankind. Agriculture has also been the main culprit of the imbalances of human civilization over the last ten thousand years or so: the systematic destruction of the natural environment, overpopulation, capitalization, and other evils that result from sedentariness. (Schipper 1993:170) \n\nGrain abstention was prerequisite for the Daoist practice of \"yangxing\" 養性 \"nourishing the inner nature\". Maspero explains.\nNourishing the Vital Principle consists in suppressing the causes of death and creating in oneself the immortal body which will replace the mortal body. The causes of death are especially the Breath of Grains and the Breath of Bloody Food: hence the alimentary regimens which are designated by the generic name Abstinence from Grains. One must succeed in replacing vulgar food with the Food of the Breath, like an aerophagia which consists of breathing air in, holding it in as long as possible without allowing it to escape and, while it is held in, making it pass, in identical mouthfuls with great gulps of water, from the trachea into the esophagus, so that it can be sent on into the stomach like real food. The body is made of Breaths, like all things; but it is made of coarse breaths, whereas air is a light, subtle and pure Breath. Vulgar food, after digestion, supplies the body with the Breaths of the Five Flavors, common and impure Breaths which make it heavy. By contrast, Food of the Breath little by little replaces the coarse matter of the body with light, pure Breaths; and when the transformation is completed, the body is immortal. (1981:255) \n\nSome versions of \"grain avoidance\" could result in health problems, as discussed by Maspero. \nThis very severe diet was not without its painful moments. Without grains and meat, whoever practices it is undernourished; and the Taoist authors admit that at the beginning one may have numerous troubles, some of them general (vertigo, weakness, sleepiness, difficulties in moving), others local (diarrhea, constipation, and so on). Nevertheless, they advise persevering, insisting that these disappear after several weeks and that the body soon feels as before, and even better: more calm and more at ease. They also advise practicing it only gradually, and they recommend a number of drugs for the period of transition and adaptation which, according to them, lasts thirty to forty days. The recipes for drugs to help in the practice of Abstention from Cereals are numerous: ginseng, cinnamon, pachyma cocos [i.e., Fu Ling], sesame, digitalis, licorice, and all the traditional Chinese tonics play a preponderant role in them. (1981:335) \n\nChinese Buddhism adopted Daoist grain abstention as a preparation for self-immolation. For instance (Benn 2007:36-7), the monk Huiyi 慧益 (d. 463), who vowed to burn his body in sacrifice to the Buddha, began preparations by \"queli\" \"abstaining from grains\" (eating only sesame and wheat) for two years, then consumed only oil of thyme, and finally ate only pills made of incense. Although Emperor Xiaowu of Liu Song (r. 453–464) tried to dissuade Huiyi, he publicly immolated himself in a cauldron full of oil, wearing an oil-soaked cap to act as a wick, while chanting the \"Lotus Sutra\".\n\nAvoiding grains was the primary medical cure for eliminating the \"sanshi\" 三尸 \"Three Corpses\" or \"sanchong\" 三蟲 \"Three Worms\", which are evil spirits believed to live in the human body and hasten death. Livia Kohn (1993:148) describes the Three Corpses as \"demonic supernatural creatures who feed on decay and are eager for the body to die altogether so they can devour it. Not only do they thus shorten the lifespan but they also delight in the decaying matter produced by the grains as they are digested in the intestines. If one is to attain long life, the three worms have to be starved, and the only way to do so is to avoid all grain.\"\n\nTraditional Chinese medicine links the mythological Three Corpses/Worms with the intestinal \"jiuchong\" 九蟲 \"Nine Worms\", which \"correspond to parasites such as roundworms or tapeworms, weaken the host's body and cause a variety of physical symptoms\" (Cook 2008:844).\n\nThe Three Corpses allegedly enter the human body at birth, and reside in the Upper, Middle, and Lower \"Dantian\" \"Cinnabar Fields\" within the brain, heart, and abdomen, respectively. After their host dies, they become ghosts and are free to roam about stealing sacrificial offerings. These pernicious corpse-worms seek to harm both their host's body and fate. First, they weaken the bodily Dantian energy centers. Second, the Three Corpses keep records or their host's misdeeds, ascend to tian \"heaven\" bimonthly on the Chinese sexagenary cycle day \"gengshen\" 庚申 \"57th of the 60\", and file reports to the Siming 司命 \"Director of Destinies\" who assigns punishments to shorten the host's lifespan. For\"genghsen\" days, the (4th century) \"Huangtingjing\" 黃庭經 \"Yellow Court Scripture\" (tr. Schipper 1978:372) says, \"Do not sleep either day or night, and you shall become immortal.\"\n\nIn addition to the Three Corpses making a bimonthly report to the Director of Fate, the \"Baopuzi\" records the Hearth God making one.\nIt is also said that there are Three Corpses in our bodies, which, though not corporeal, actually are of a type with our inner, ethereal breaths, the powers, the ghosts, and the gods. They want us to die prematurely. (After death they become a man's ghost and move about at will to where sacrifices and libations are being offered.) Therefore, every fifty-seventh day of the sixty-day cycle they mount to heaven and personally report our misdeeds to the Director of Fates. Further, during the night of the last day of the month the hearth god also ascends to heaven and makes an oral report of a man's wrongs. For the more important misdeeds a whole period of three hundred days is deducted. For the minor ones they deduct one reckoning, a reckoning being three days. Personally, I have not yet been able to determine whether this is really so or not, but that is because the ways of heaven are obscure, and ghosts and gods are hard to understand. (6, tr. Ware 1966:115-6) \n\n\"Bigu\" abstinence from grains and cereals, which allegedly makes the Three Corpses waste away, is the basis for many Daoist dietetic regimens, which can also exclude wine, meat, onion, and garlic. The \"Jinjian yuzi jing\" 金簡玉字經 \"Classic of Jade Characters on Slips of Gold\" (tr. Maspero 1981:334) specifies, \"Those who, in their food, cut off cereals must not take wine, nor meat, nor plants of the five strong flavors; they must bathe, wash their garments, and burn incense.\" Practicing \"bigu\" alone cannot eliminate the Three Corpses, but will weaken them to the point where they can be killed with alchemical drugs, particularly cinnabar (Schipper 1993:167).\n\nEarly Daoist texts and traditions portray the Three Corpses in both \"zoomorphic and bureaucratic metaphors\" (Campany 2005:43). The (4th century CE) \"Ziyang zhenren neizhuan\" 紫陽真人內傳 \"Inner Biography of the True Person of Purple Yang\" (tr. Maspero 1981:332) described them living in the Three Cinnabar Fields.\nCompare the (9th century) \"Chu sanshi jiuchong baosheng jing\" \"Scripture on Expelling the Three Corpses and Nine Worms to Protect Life\" description.\nThis text's woodblock illustrations depict the upper corpse as a scholarly man, the middle as a short quadruped, and the lower corpse as \"a monster that looks like a horse's leg with a horned human head\" (tr. Maspero 1981:333).\n\nThe Japanese folk tradition of Kōshin (namely, the Japanese pronunciation of \"gengshen\" 庚申 \"57th\") combines the Daoist Three Corpses with Shintō and Buddhist beliefs, including the Three Wise Monkeys. People attend \"Kōshin-Machi\" 庚申待 \"57th Day Waiting\" events to stay awake all night and prevent the \"Sanshi\" 三尸 \"Three Corpses\" from leaving the body and reporting misdeeds to heaven.\n\nFamine food plants, which are not normally considered as crops, are consumed during times of extreme poverty, starvation, or famine. \"Bigu\" diets were linked with mountain wilderness areas in which one relied upon non-grain foods, including famine foodstuffs and underutilized crops. Despeux (2008:234) said, \"Abstention from cereals should also be situated in the historical context of social unrest and famine.\"\n\nThe \"Mouzi Lihuolun\" introduction describes people who fled China after the death of Emperor Ling of Han and moved south to Cangwu in Jiaozhou (present day Tonkin).\nIt happened that, after the death of Emperor Ling (189 C.E.), the world was in disorder. Since only Chiao-chou [a colonial district in the far south] remained relatively peaceful, otherworldly people from the north came en masse and settled there. Many of them practiced the methods of the spirit immortals, abstaining from grains to prolong life. These methods were popular then, but Mou-tzu unceasingly refuted them by employing the Five Classics, and none among the Taoist adepts or the Magicians dared engage him in debate. (1, tr. Keenan 1994:52) \nThese refutations of grain avoidance are found in \"Mouzi Lihuolun\" Article 30 (Keenan 1994:151-153).\n\nThe \"Baopuzi\" discussion of grain abstention notes, \nShould you take to the mountains and forests during political troubles, you will avoid dying of starvation by observing the rule about starches. Otherwise, do not rush into this practice, for rushing cannot be very beneficial. If you dispense with meat while living among others, you will find it impossible not to desire it deep in your heart when you smell its fat or freshness. (15, tr. Ware 1966:244) \n\nThe Chinese published the oldest book on famine foods: the \"Jiuhuang Bencao\" 救荒本草 \"Materia Medica for the Relief of Famine\". Zhu Su 朱橚 (1361–1425), the fifth son of the Hongwu Emperor, compiled this treatise describing 414 famine food plants. Bernard Read (1946) translated the \"Jiuhuang bencao\" into English.\n\nThe ancient Daoist practice of \"bigu\" grain avoidance resonates in present-day trends such as low-carbohydrate diets, gluten-free diets, cyclic ketogenic diets.\n\nSchipper uses medical terminology to explain grain avoidance. \nOne can advance positive explanations for this belief, and the practice that derives from it, if one thinks, for example, of the relative abundance of feces produced by cereals as compared to that produced by a diet of meat. The conclusion of recent studies on the harmful effect of excessive amounts of carbohydrates in the form of sugar and bread, have led some to see the Taoist abstinence from cereals as the result of an ancient empiricism in matters of diet. (Schipper 1993:167) \n\nSome contemporary researchers (such as Yan et al. 2002) are clinically investigating \"bigu\" fasting.\n\n\n\n"}
{"id": "7933107", "url": "https://en.wikipedia.org/wiki?curid=7933107", "title": "Bird Neighbors", "text": "Bird Neighbors\n\nBird Neighbors, published in 1897, was the first major work by nature writer Neltje Blanchan. The book combined scientific data with color illustrations, accessible language, and personal experience reflecting Blanchan's joy in nature. In his introduction, naturalist John Burroughs praised it as \"reliable\" as well as \"written in a vivacious strain by a real bird lover.\"\n\nAfter discussing the scientific classification of birds by families, Blanchan lists 19 \"habitats\" where birds can be found (such as \"birds seen near the edges of woods\" and \"birds found near salt water\"), and groups the birds themselves by size and color. The 52 color illustrations were produced by photographing stuffed birds in front of appropriate backgrounds, since cameras of the time could not take good photographs of living birds. The published photographs were also limited by the color printing technologies of the time, and some of the darker birds have been described as looking like \"they had been dipped in shoe polish.\"\n\nAmong the \"birds conspicuously black\" in the book is the red-winged blackbird, which is introduced this way:\nIn oozy pastures where a brook lazily finds its way through the farm is the ideal pleasure ground of this \"bird of society.\" His notes, \"h'-wa-ker-ee\" or \"con-quer-ee\" (on an ascending scale), are liquid in quality, suggesting the sweet, moist, cool retreats where he nests ... satisfied with cut-worms, grubs, and insects, or with fruit and grain for his food – the blackbird is an impressive and helpful example of how to get the best out of life.\nOn publication, \"The New York Times\" praised the book's color prints and its ability to be \"understood by all readers,\" and the following year included it in a list of \"150 books for summer reading.\" \"Bird Neighbors\" sold over 250,000 copies, helping to make the author the best-selling female nature writer of her time.\n\nOriginally published by Doubleday, the book was republished by the Project Gutenberg Literary Archive Foundation in 1999.\n\n\n"}
{"id": "56968848", "url": "https://en.wikipedia.org/wiki?curid=56968848", "title": "Caill Tomair", "text": "Caill Tomair\n\nCaill Tomair (Old Irish 'Thor's Grove') was a sacred grove dedicated to the North Germanic god Thor. Located near the Norse-Gaelic city of Dublin, the grove was destroyed by forces led by Brian Boru early in the year 1000 CE.\n\nAccording to scholar Poul Holm, the grove was likely targeted due to its role among the local population:\n\n\n"}
{"id": "312864", "url": "https://en.wikipedia.org/wiki?curid=312864", "title": "Cattle in religion and mythology", "text": "Cattle in religion and mythology\n\nDue to the multiple benefits from cattle, there are varying beliefs about cattle in societies and religions. In some regions, especially Nepal and most states of India, the slaughter of cattle is prohibited and their meat may be taboo.\n\nCattle are considered sacred in world religions such as Hinduism, Jainism, Buddhism, Zoroastrianism, and others. Religions in ancient Egypt, ancient Greece, ancient Rome, and ancient Germany held similar beliefs.\n\nLegislation against cattle slaughter is in place throughout most states of India except Kerala, West Bengal, and parts of the North-East.\n\nMajority of scholars explain the veneration for cows among Hindus in economic terms, which includes the importance of dairy in the diet, use of cow dung as fuel and fertilizer, and the importance that cattle have historically played in agriculture. Ancient texts such as Rig Veda, Puranas highlight the importance of the cattle. The scope, extent and status of cows throughout during ancient India is a subject of debate. According to D. N. Jha, cattle including cows were neither inviolable nor revered in the ancient times as they were later. A \"Gryhasutra\" recommends that beef be eaten by the mourners, after a funeral ceremony as a ritual rite of passage. In contrast, according to Marvin Harris, the Vedic literature is contradictory, with some suggesting ritual slaughter and meat consumption, while others suggesting a taboo on meat eating.\n\nThe \"Chandogya Upanishad\" (~ 800 BCE) mentions the ethical value of Ahimsa, or non-violence towards all beings. By mid 1st millennium BCE, all three major Indian religions – Buddhism, Hinduism, and Jainism – were championing non-violence as an ethical value, and something that impacted one's rebirth. According to Harris, by about 200 CE, food and feasting on animal slaughter were widely considered as a form of violence against life forms, and became a religious and social taboo.\n\nMilk cows are called \"aghnya\" \"that which may not be slaughtered\" in Rigveda. Yaska, the early commentator of the Rigveda, gives nine names for cow, the first being \"aghnya\".\n\nAccording to Nanditha Krishna, the cow veneration in ancient India during the Vedic era, the religious texts written during this period called for non-violence towards all bipeds and quadrupeds, and often equated killing of a cow with the killing of a human being especially a Brahmin. Nanditha Krishna stated that the hymn 8.3.25 of the Hindu scripture \"Atharvaveda\" (~1200–1500 BCE) condemns all killings of men, cattle, and horses, and prays to god Agni to punish those who kill.\n\nMany ancient and medieval Hindu texts debate the rationale for a voluntary stop to cow slaughter and the pursuit of vegetarianism as a part of a general abstention from violence against others and all killing of animals. According to Harris, the literature relating to cow veneration became common in 1st millennium CE, and by about 1000 CE vegetarianism, along with a taboo against beef, became a well accepted mainstream Hindu tradition. This practice was inspired by the beliefs in Hinduism that a soul is present in all living beings, life in all its forms is interconnected, and non-violence towards all creatures is the highest ethical value. Vegetarianism is a part of the Hindu culture. The god Krishna and his Yadav kinsmen are associated with cows, adding to its endearment.\n\nAccording to Ludwig Alsdorf, \"Indian vegetarianism is unequivocally based on \"ahimsa\" (non-violence)\" as evidenced by ancient \"smritis\" and other ancient texts of Hinduism. He adds that the endearment and respect for cattle in Hinduism is more than a commitment to vegetarianism and has become integral to its theology. The respect for cattle is widespread but not universal. According to Christopher Fuller, animal sacrifices have been rare among the Hindus outside a few eastern states. To the majority of modern Indians, states Alsdorf, respect for cattle and disrespect for slaughter is a part of their ethos and there is \"no \"ahimsa\" without renunciation of meat consumption\".\n\nThe interdiction of the meat of the bounteous cow as food was regarded as the first step to total vegetarianism.\n\n\nThe earth-goddess Prithvi was, in the form of a cow, successively milked of beneficent substances for the benefit of humans, by deities starting with the first sovereign: Prithu milked the cow to generate crops for humans to end a famine.\n\nKamadhenu, the miraculous \"cow of plenty\" and the \"mother of cows\" in certain versions of the Hindu mythology, is believed to represent the generic sacred cow, regarded as the source of all prosperity. In the 19th-century, a form of Kamadhenu was depicted in poster-art that depicted all major gods and goddesses in it.\n\nThe reverence for the cow played a role in the Indian Rebellion of 1857 against the British East India Company. Hindu and Muslim sepoys in the army of the East India Company came to believe that their paper cartridges, which held a measured amount of gunpowder, were greased with cow and pig fat. The consumption of swine is forbidden in Islam and Judaism. Because loading the gun required biting off the end of the paper cartridge, they concluded that the British were forcing them to break edicts of their religion.\n\nA historical survey of major communal riots in India between 1717 and 1977 revealed that 22 out of 167 incidents of rioting between Hindus and Muslims were attributable directly to cow slaughter.\n\nThe cow protection was a symbol of animal rights and of non-violence against all life forms for Gandhi. He venerated cows, and suggested ending cow slaughter to be the first step to stopping violence against all animals. He said: \"I worship it and I shall defend its worship against the whole world\", and stated that \"The central fact of Hinduism is cow protection.\" \nYet even Gandhi never called for the banning of cow slaughter in India. He said,\n\n“How can I force anyone not to slaughter cows unless he is himself so disposed? It is not as if there were only Hindus in the Indian Union. There are Muslims, Parsis, Christians and other religious groups here.”\n\nJainism is against violence to all living beings, including cattle. According to the Jaina sutras, humans must avoid all killing and slaughter because all living beings are fond of life, they suffer, they feel pain, they like to live, and long to live. All beings should help each other live and prosper, according to Jainism, not kill and slaughter each other.\n\nIn the Jain religious tradition, neither monks nor laypersons should cause others or allow others to work in a slaughterhouse. Jains believe that vegetarian sources can provide adequate nutrition, without creating suffering for animals such as cattle. According to some Jain scholars, slaughtering cattle increases ecological burden from human food demands since the production of meat entails intensified grain demands, and reducing cattle slaughter by 50 percent would free up enough land and ecological resources to solve all malnutrition and hunger worldwide. The Jain community leaders, states Christopher Chapple, has actively campaigned to stop all forms of animal slaughter including cattle.\n\nThe texts of Buddhism state ahimsa to be one of five ethical precepts, which requires a practicing Buddhist to \"refrain from killing living beings\". Slaughtering cow has been a taboo, with some texts suggest taking care of a cow is a means of taking care of \"all living beings\". Cattle are seen in some Buddhist sects as a form of reborn human beings in the endless rebirth cycles in samsara, protecting animal life and being kind to cattle and other animals is good karma. Not only do some, mainly Mahayana, Buddhist texts state that killing or eating meat is wrong, it urges Buddhist laypersons to not operate slaughterhouses, nor trade in meat. Indian Buddhist texts encourage a plant-based diet.\n\nAccording to Saddhatissa, in the \"Brahmanadhammika Sutta\", the Buddha \"describes the ideal mode of life of Brahmins in the Golden Age\" before him as follows:\n\nSaving animals from slaughter for meat, is believed in Buddhism to be a way to acquire merit for better rebirth. According to Richard Gombrich, there has been a gap between Buddhist precepts and practice. Vegetarianism is admired, states Gombrich, but often it is not practiced. Nevertheless, adds Gombrich, there is a general belief among Theravada Buddhists that eating beef is worse than other meat and the ownership of cattle slaughterhouses by Buddhists is relatively rare.\n\nMeat eating remains controversial within Budhhism, with most Theravada sects allowing it, reflecting early Buddhist practise, and most Mayahayana sects forbidding it. Early Suttas indicate that the Buddha himself ate meat and was clear that no rule should be introduced to forbid meat eating to monks.The consumption, however, appears to have been limited to pork, chicken and fish and may well have excluded cattle. \n\nThe term \"geush urva\" means \"the spirit of the cow\" and is interpreted as the soul of the earth. In the Ahunavaiti Gatha, Zarathustra (or Zoroaster) accuses some of his co-religionists of abusing the cow. Ahura Mazda tells Zarathustra to protect the cow.\n\nThe lands of Zarathustra and the Vedic priests were those of cattle breeders. The 9th chapter of the \"Vendidad\" of the \"Avesta\" expounds the purificatory power of \"gōmēz\" – cow urine. It is declared to be a panacea for all bodily and moral evils, understood as which it features prominently in the 9-night purification ritual Barashnûm.\n\nAccording to the Hebrew Bible, an unblemished red cow was an important part of ancient Jewish rituals. The cow was sacrificed and burned in a precise ritual, and the ashes were added to water used in the ritual purification of a person who had come in to contact with a human corpse. The ritual is described in the Book of Numbers in Chapter 19, verses 1–14.\n\nObservant Jews study this passage every year in early summer as part of the weekly Torah portion called Chukat. A contemporary Jewish organization called the Temple Institute is trying to revive this ancient religious observance.\n\nTraditional Judaism considers beef kosher and permissible as food, as long as the cow is slaughtered in a religious ritual called \"shechita\", and the meat is not served in a meal that includes any dairy foods.\n\nSome Jews committed to Jewish vegetarianism believe that the logical conclusion to the Jewish rules for animal care and slaughter is to refrain from slaughtering animals altogether .\n\nIslam allows the slaughter of cows and consumption of beef, as long as the cow is slaughtered in a religious ritual called \"dhabīḥah\" or \"zabiha\" similar to the Jewish \"shechita\".\n\nAlthough slaughter of cattle plays a role in a major Muslim holiday, Eid al-Adha, many rulers of the Mughal Empire had imposed a ban on the slaughter of cows owing to the large Hindu and Jain populations living under their rule.\n\nThe second and longest surah of the Quran is named Al-Baqara (\"The Cow\"). Out of the 286 verses of the surah, 7 mention cows (Al Baqarah 67–73). The name of the surah derives from this passage in which Moses orders his people to sacrifice a cow in order to resurrect a man murdered by an unknown person. Per the passage, the \"Children of Israel\" quibbled over what kind of cow was meant when the sacrifice was ordered. Classical Sunni and Shia commentators recount several variants of this tale. Per some of the commentators though any cow would have been acceptable but after they \"created hardships for themselves\" and the cow was finally specified, it was necessary to obtain it any cost.\n\nThe ancient Egyptians sacrificed animals but not the cow, because it was sacred to goddess Hathor, and due to the contemporary Greek myth of Io, who had the form of a cow.\n\nIn Egyptian mythology, Hesat was the manifestation of Hathor, the divine sky-cow, in earthly form. Like Hathor, she was seen as the wife of Ra. In hieroglyphs, she is depicted as a cow with a headdress.\n\nApis bull\n\n\nToday, in Hindu-majority countries like India and Nepal, bovine milk holds a key part of religious rituals. For some, it is customary to boil milk on a stove or lead a cow through the house as part of a housewarming ceremony. In honor of their exalted status, cows often roam free, even along (and in) busy streets in major cities such as Delhi. In some places, it is considered good luck to give one a snack or fruit before breakfast.\n\nConstitution of India mandates the protection of cows in India. The slaughter of cattle is allowed with restrictions (like a 'fit-for-slaughter' certificate which may be issued depending on factors like age and gender of cattle, continued economic viability, etc.), but only for bulls and buffaloes and not cows in fourteen states. It is completely banned in six states with pending litigation in the supreme court to overturn the ban, while there is no restriction in many states. This has created communal disharmony in India and frequently leads to unwanted incidents.\n\nGopastami, a holiday celebrated by the Hindus once a year, is one of the few instances where cows receive prayers in modern-day India. While the cow is still respected and honored by most of the Indian population, there has been controversy over the treatment of the cows during the holiday.\n\nIn Nepal, the cow is the national animal. Cows give milk from which the people produce dahi (yogurt), ghee, butter, etc. In Nepal, a Hindu-majority country, slaughtering of cows and bulls is completely banned. Cows are considered like the Goddess Lakshmi (goddess of wealth and prosperity). The Nepalese have a festival called Tihar (Diwali) during which, on one day called Gaipuja, they perform prayers for cows.\n\nAccording to a Lodi News-Sentinel news story written in the 1960s, in then contemporary Nepal an individual could serve three months in jail for killing a pedestrian, but one year for injuring a cow, and life imprisonment for killing a cow.\n\nCows roam freely and are sacred. Buffalo slaughtering was done in Nepal at specific Hindu events, such as at the Gadhimai festival, last held in 2014. In 2015, Nepal's temple trust on announced to cancel all future animal sacrifice at the country's Gadhimai festival.\n\nThe beef taboo is fairly widespread in Myanmar, particularly in the Buddhist community. In Myanmar, beef is typically obtained from cattle that are slaughtered at the end of their working lives (16 years of age) or from sick animals. Cattle is rarely raised for meat; 58% of cattle in the country is used for draught animal power (DAP). Few people eat beef, and there is a general dislike of beef (especially among the Bamar and Burmese Chinese), although it is more commonly eaten in regional cuisines, particularly those of ethnic minorities like the Kachin. Buddhists, when giving up meat during the Buddhist (Vassa) or Uposatha days, will forego beef first. Almost all butchers are Muslim because of the Buddhist doctrine of ahimsa (no harm).\n\nDuring the country's last dynasty, the Konbaung dynasty, habitual consumption of beef was punishable by public flogging.\n\nIn 1885, Ledi Sayadaw, a prominent Buddhist monk wrote the \"Nwa-myitta-sa\" (), a poetic prose letter that argued that Burmese Buddhists should not kill cattle and eat beef, because Burmese farmers depended on them as beasts of burden to maintain their livelihoods, that the marketing of beef for human consumption threatened the extinction of buffalo and cattle, and that the practice and was ecologically unsound. He subsequently led successful beef boycotts during the colonial era, despite the presence of beef eating among locals, and influenced a generation of Burmese nationalists in adopting this stance.\n\nOn 29 August 1961, the Burmese Parliament passed the State Religion Promotion Act of 1961, which explicitly banned the slaughtering of cattle nationwide (beef became known as \"todo tha\" (); lit. hush hush meat). Religious groups, such as Muslims, were required to apply for exemption licences to slaughter cattle on religious holidays. This ban was repealed a year later, after Ne Win led a coup d'état and declared martial law in the country.\n\nIn Sri Lanka, in May 2013, 30-year-old Buddhist monk Bowatte Indrarathana Thera of the Sri Sugatha Purana Vihara self immolated to protest the government allowing religious minorities to slaughter cows.\n\nA beef taboo in Ancient China, known as \"niú jiè\" (牛戒), was historically a dietary restriction, particularly among the Han Chinese, as oxen and buffalo (bovines) are useful in farming and are respected. During the Zhou Dynasty, they were not often eaten, even by emperors. Some emperors banned killing cows. Beef is not recommended in Chinese medicine, as it is considered a hot food and is thought to disrupt the body's internal balance.\n\nIn written sources (including anecdotes and Daoist liturgical texts), this taboo first appeared in the 9th to 12th centuries (Tang-Song transition, with the advent of pork meat.) By the 16th to 17th centuries, the beef taboo had become well accepted in the framework of Chinese morality and was found in morality books (善書), with several books dedicated exclusively to this taboo. The beef taboo came from a Chinese perspective that relates the respect for animal life and vegetarianism (ideas shared by Buddhism, Confucianism, and Daoism, and state protection for draught animals.) In Chinese society, only ethnic and religious groups not fully assimilated (such as the Muslim Huis and the Miao) and foreigners consumed this meat. This taboo, among Han Chinese, led Chinese Muslims to create a niche for themselves as butchers who specialized in slaughtering oxen and buffalo.\n\nOccasionally, some cows seen weeping before slaughter are often released to temples nearby.\nHistorically, there was a beef taboo in Ancient Japan, as a means of protecting the livestock population and due to Buddhist influence. Meat-eating had long been taboo in Japan, beginning with a decree in 675 that banned the consumption of cattle, horses, dogs, monkeys, and chickens, influenced by the Buddhist prohibition of killing. In 1612, the shōgun declared a decree that specifically banned the killing of cattle.\n\nThis official prohibition was in place until 1872, when it was officially proclaimed that Emperor Meiji consumed beef and mutton, which transformed the country's dietary considerations as a means of modernizing the country, particularly with regard to consumption of beef. With contact from Europeans, beef increasingly became popular, even though it had previously been considered barbaric.\n\nIn Kudus, Indonesia, Muslims still maintain the tradition of not slaughtering or eating cows, out of respect for their ancestors, who were Hindus, allegedly imitating Sunan Kudus who also did as such.\n\nIn religiously diverse countries, leather vendors are typically careful to clarify the kinds of leather used in their products. For example, leather shoes will bear a label identifying the animal from which the leather was taken. In this way, a Muslim would not accidentally purchase pigskin leather, and a Hindu could avoid cow leather. Many Hindus who are vegetarians will not use any kind of leather.\n\nJudaism forbids the wearing of shoes made with leather on Yom Kippur, Tisha B'Av, and during mourning.\n\nJainism prohibits the use of leather because it is obtained by killing animals.\n\n\n"}
{"id": "7139514", "url": "https://en.wikipedia.org/wiki?curid=7139514", "title": "Charles Gaines", "text": "Charles Gaines\n\nCharles Latham Gaines, Jr. (born January 6, 1942) is an American writer and outdoorsman, notable for numerous works in both the fiction and non-fiction genres. His writing most typically concerns the outdoors sports of fishing in general and fly fishing in particular, as well as upland bird hunting and mountaineering, often with an intellectual and philosophical bent, and an eye towards the various cultures and traditions surrounding different forms of fishing around the world.\n\nIn addition to his outdoors writings, Gaines covered the \"Golden Age\" of professional bodybuilding and is the author of \"Pumping Iron\", considered the definitive journalistic work in that field, and credited in large part for bringing greater public awareness to what was formerly a little-known subculture, as well as helping to launch the career of Arnold Schwarzenegger. Gaines also narrated and contributed to the documentary film of the same name.\n\nGaines is active in the conservation movement and in particular the stewardship of the North American Atlantic Salmon fisheries. He has served on the board of directors of the Atlantic Salmon Federation.\n\nGaines was born in Jacksonville, Florida on January 6, 1942, the son of Margaret (\"née\" Shook) and Charles Latham Gaines. At the age of ten, he and his family moved to Birmingham, Alabama. As a teenager, Gaines took up bodybuilding. He briefly attended Washington and Lee University but left school to travel around the country. He later received his BA from Birmingham-Southern College and his MFA in writing from the University of Iowa. In 1970, he moved to New Hampshire, where he taught creative writing at New England College.\n\nIn 1972, Gaines published his first novel, \"Stay Hungry\". This work of fiction focused on the subculture of bodybuilding during the early 1970s, and revolved around the lives of three characters in Birmingham, Alabama: Craig Blake, a young heir from a family of Southern gentry; Mary Tate Farnsworth, a young woman of working-class background; and Joe Santo, a bodybuilder, athlete and Renaissance man figure of French Canadian and Menominee Indian ancestry. As well as exploring the culture of bodybuilding in great detail, the novel also dealt with themes of class difference, spirituality and self-identity. In 1976, \"Stay Hungry\" was made into a motion picture starring Jeff Bridges, Sally Field and Arnold Schwarzenegger (the latter in one of his earliest roles.)\n\nIn 1974 Gaines created \"Pumping Iron: The Art and Sport of Bodybuilding\", a photo-essay with photographs by George Butler which focused in even further detail on the professional bodybuilding scene during the 1970s. It featured such bodybuilding greats as Arnold Schwarzenegger, Franco Columbu, Serge Nubret, Ken Waller, Mike Katz and Ed Corney, and provided a factual and philosophical exposition of the sport of bodybuilding. It is considered to be a seminal work in that field, and still enjoys significant popularity.\n\n\"Pumping Iron\" was adapted by Butler into a documentary film of the same name in 1977, this time focusing on the 1975 Mr. Olympia bodybuilding contest and the rivalry between Schwarzenegger and up-and-comer Lou Ferrigno, though also including all of the bodybuilders from the original book. The film enjoyed great success, bringing both Ferrigno and Schwarzenegger into the spotlight and increasing their public profiles tremendously. It also helped to further popularize bodybuilding as a sport and bring it into the mainstream.\n\nGaines continued writing on the topic of bodybuilding, publishing an exercise manual, \"Staying Hard: The Only Exercise Book You Will Ever Need\" in 1980 and writing a biography of Charles Atlas in 1982.\n\nIn 1995 he wrote the autobiographical novel \"A Family Place\", which documented the building of a family cabin in Nova Scotia with the help of his wife and children. In 2001 he published The Next Valley Over: An Angler's Progress, a creative nonfiction work which detailed his fly fishing experiences all over the world. He also co-authored Leaper: The Wonderful World of Atlantic Salmon Fishing with Monte Burke.\n\nGaines currently teaches writing part-time at the Spalding University Master of Fine Arts program, and continues to contribute articles to outdoors publications.\n\nIn 1976, Gaines returned to New Hampshire from an African buffalo-hunting safari, and discussed with his friend Bob Gurnsey the idea of a game in which the participants would stalk each other through the woods. The concept was inspired in part by the short story \"The Most Dangerous Game\" by Richard Connell. In 1981, Gaines and eleven others played the first game of paintball, using Nel-Spot pistols which were intended for marking trees and livestock by ranchers. This initial game was a capture the flag style scenario. Later, Bob Gurnsey formed the National Survival Game company, which was the first firm to sell paintball equipment.\n\nIn 1997 Gaines published a novel entitled \"Survival Games\" which features inventors of paintball, probably based upon Gaines and Gurnsey, as the main characters.\n"}
{"id": "25779100", "url": "https://en.wikipedia.org/wiki?curid=25779100", "title": "Climograph", "text": "Climograph\n\nA climograph is a graphical representation of a location's basic climate. Climographs display data for two variables: (a) monthly average temperature and (b) monthly average precipitation. These are useful tools to quickly describe a location's climate.\n\nOne form of representation uses an overlapped combination of a bar and line chart used to show the climate of a place over a 12-month period. The horizontal axis (x-axis) displays the 12 months while the vertical axis contains the precipitation scale on one side and the temperature scale on the other.\n\nWhile temperature is typically visualized using a line, some climographs opt to visualize the data using a bar. This method's advantage allows the climograph to display the average range in temperature (average minimum and average maximum temperatures) rather than a simple monthly average.\n\nThe patterns in a climograph describe not just a location's climate but also provide evidence for that climate's relative location. For example, a climograph with a narrow range in temperature over the year might represent a location close to the equator, or alternatively a location adjacent to a large body of water exerting a moderating effect on the temperature range. Meanwhile, a wide range in annual temperature might suggest the opposite. We could also derive information about a site's ecological conditions through a climograph. For example, if precipitation is consistently low year-round, we might suggest the location reflects a desert; if there is a noticeable seasonal pattern to the precipitation, we might suggest the location experiences a monsoon season. When combining the temperature and precipitation patterns together, we have even better clues as to the local conditions. Despite this, it is important to note that a number of local factors contribute to the patterns observed in a particular place; therefore, a climograph is not a foolproof tool that captures all the geographic variation that might exist.\n\n"}
{"id": "54389113", "url": "https://en.wikipedia.org/wiki?curid=54389113", "title": "Deep Core (film)", "text": "Deep Core (film)\n\nDeep Core is a 2000 American Action, Sci-Fi, Thriller film. The film was generally poorly received.\n\nBrian, the head drilling engineer of an un-named company, sabotages the drilling operation as he is concerned that Alan the CEO is drilling too deep without taking necessary precautions.\n\nAlan restarts the drilling operation on an island off the coast of China and while drilling using a mole like machine, the drilling team hits a magma flow which kills them. This then causes a large volcanic eruption and earthquake.\n\nAlan and Brian start investigating the effects of the drilling, with help from Allison Saunders, head of research and development at the company. They realize that they have affected the stability of the Earth’s core, which is going to cause natural disasters all over the world. They figure the only way to stop the magma flow and earthquakes is to use a series of nuclear explosions to reset the spinning of the Earth’s core. They plan to drop these bombs using a new drilling machine.\n\nThe earthquakes start to spread all over the world, affecting major cities. Quito, Ecuador is hit and many important buildings are destroyed.\n\nDarryl obtains the nuclear bombs from the Chinese government. However, they want their own team to do the drilling. Brian disagrees and assembles his own team including Allison and his drilling engineer colleges Brian and Rodney. Alan also volunteers to be part of the team.\n\nThey prepare to drill down into the earth to position the nuclear bombs at strategic positions ready for a coordinated sequence of explosions. They start drilling down into the ground, and successfully drop the first 2 nuclear bombs. They have an issue with the cabin pressure, but after running diagnostics they realize the sensors malfunctioned and they manage to reset them.\n\nA large piece of rock breaks one of the connections to the feet, which needs to be repaired. Allison and Rodney go outside to fix it. Meanwhile Brian realizes they have been drilling into a Magma flow. The Magma comes through the roof of the tunnel, killing Rodney. Brian goes outside and rescues Allison. They continue drilling to escape the Magma flow. And drop the third nuclear bomb.\n\nThere is another large volcanic eruption from Mount Pinatubo in the Philippines. As the destabilization of the Earth’s core gets worse.\n\nAlan confesses that he has partnered with the Chinese government, that they had a lot of input into the design of the drilling vehicle, and that the vehicle will go to the government after the drilling mission so they can use it as a weapon. Darryl is contacted by Colonel Po, a Chinese government official who threatens him to ensure the government get control of the vehicle after the mission has been completed.\n\nMeanwhile the drilling team hit a cave full of diamonds, which they can’t drill through so they must go around it. The drilling team successfully drop the forth nuclear bomb, but due to their detour around the diamond cave they run out of communication cable, that they have been laying to stay in contact with the command center. This initiates a self-destruct countdown of the bomb pod within the drilling vehicle.\n\nThey eject the bomb pod from the main mining vehicle, but Alan stays in the pod and delays the self-destruct by tying up the computer with a diagnostics program. Brian and his team find a magma flow, to get them to the surface away from the nuclear bombs before they detonate. The nuclear bombs all explode successfully. Brian and his team make it to the surface of the Sea. They are rescued by the coast guard.\n\nDarryl tries to cover up the fact that the drilling vehicle has survived and not fallen into the hands of the Chinese, by destroying all the evidence of the drilling vehicle. He then tries to escape in his car, but Colonel Po finds him and kills him.\n\n\nImdb gave the film a 2.6 out of 10, All Movie gave the film a 1.5 out of 5, Contact Music gave the film a 1 out of 5, TV Guide gave the film a 2 out of 5 Disastermovieworld gave the film a 1 out of 5.\n\nThe Core\n\n"}
{"id": "283829", "url": "https://en.wikipedia.org/wiki?curid=283829", "title": "Ebonite", "text": "Ebonite\n\nEbonite is a brand name for very hard rubber first obtained by vulcanizing natural rubber for prolonged periods. Besides natural rubber, Ebonite contains about 25–80% sulfur and linseed oil. Its name comes from its intended use as an artificial substitute for ebony wood.The material is known generically as hard rubber and has formerly been called \"vulcanite\", although that name now refers to the mineral vulcanite.\n\nCharles Goodyear's brother Nelson Goodyear experimented with the chemistry of ebonite composites. In 1851 he used zinc oxide as a filler.\n\nThe sulfur percentage and the applied temperatures and duration during vulcanizing are the main variables that determine the technical properties of the hard rubber polysulfide elastomer. The occurring reaction is basically addition of sulfur at the double bonds, forming intramolecular ring structures, so a large portion of the sulfur is highly cross-linked in the form of intramolecular addition. As a result of having a maximum sulfur content up to 40%, it may be used to resist swelling and minimize dielectric loss. The strongest mechanical properties and greatest heat resistance is obtained with sulfur contents around 35% while the highest impact strength can be obtained with a lower sulfur content of 30%. The rigidity of hard rubber at room temperature is attributed to the van der Waals forces between the intramolecular sulfur atoms. Raising the temperature gradually increases the molecular vibrations that overcome the van der Waals forces making it elastic. Hard rubber has a content mixture dependent density around 1.1 to 1.2. When reheated hard rubber exhibits shape-memory effect and can be fairly easy reshaped within certain limits. Depending on the sulfur percentage hard rubber has a thermoplastic transition or softening temperature of .\n\nThe material is brittle, which produces problems in its use in battery cases for example, where the integrity of the case is vital to prevent leakage of sulfuric acid. It has now been generally replaced by carbon black -filled polypropylene.\n\nUnder the influence of the ultraviolet portion in daylight hard rubber oxidizes and exposure to moisture bonds water with free sulfur on the surface creating sulfates and sulfuric acid at the surface that are very hygroscopic. The sulfates condense water from the air, forming a hydrophilic film with favorable wettability characteristics on the surface. These aging processes will gradually discolor the surface grayish green to brown and cause rapid deterioration of electric surface resistivity.\n\nEbonite contamination was problematic when it was used for electronics. The ebonite was rolled between metal foil sheets, which were peeled off, leaving traces of metal behind. For electronic use the surface was ground to remove metal particles.\n\nHard rubber was used in early 20th century bowling balls; however, it was phased out in favor of other materials (the Ebonite name remains as a trade name for one of the major manufacturers of polymer balls). It has been used in electric plugs, smoking pipe mouthpieces (in competition with lucite), hockey pucks, fountain pen bodies and nib feeds, and saxophone and clarinet mouthpieces as well as complete humidity-stable clarinets . Hard rubber is often seen as the wheel material in casters. It is also commonly used in physics classrooms to demonstrate static electricity because it is at or near the negative end of the triboelectric series.\n\nHard rubber was used in the cases of automobile batteries for years, thus establishing black as their traditional colour even long after stronger modern plastics like polypropylene were substituted. It was used for decades in hair combs made by Ace, now part of Newell Rubbermaid, although the current models are known to be produced solely with plastics. (An easy way to identify a hard rubber comb is to rub part of its surface vigorously, then immediately smell the comb. Hard rubber's scent, resulting from the sulfur in the ebonite, can usually be detected temporarily. The same effect can often be produced by running the comb under hot tap water.)\n\nEbonite is used as an anticorrosive lining for various (mainly storage) vessels that contain diluted hydrochloric acid. It forms bubbles when storing hydrofluoric acid at temperatures above room temperature, or for prolonged durations.\n"}
{"id": "246019", "url": "https://en.wikipedia.org/wiki?curid=246019", "title": "Elastic-rebound theory", "text": "Elastic-rebound theory\n\nIn geology, the elastic-rebound theory is an explanation for how energy is released during an earthquake.\n\nAs the Earth's crust deforms, the rocks which span the opposing sides of a fault are subjected to shear stress. Slowly they deform, until their internal rigidity is exceeded. Then they separate with a rupture along the fault line; the sudden movement releases accumulated energy, and the rocks snap back almost to their original shape. The previously solid mass is divided between the two slowly moving plates, the energy released through the surroundings in a seismic wave.\n\nAfter the great 1906 San Francisco earthquake, geophysicist Harry Fielding Reid examined the displacement of the ground surface along the San Andreas Fault in the 50 years before the earthquake. He found evidence for 3.2 m of bending during that period. He concluded that the quake must have been the result of the elastic rebound of the strain energy stored in the rocks on either side of the fault. Later measurements using the global positioning system largely support Reid’s theory as the basis of seismic movement.\n\nThe two sides of an active but locked fault are slowly moving in different directions, where elastic strain energy builds up in any rock mass that adjoins them. Thus, if a road is built straight across the fault as in Time 1 of the figure panel, it is perpendicular to the fault trace at point E, where the fault is locked. The overall fault movement (large arrows) causes the rocks across the locked fault to accrue elastic deformation, as in Time 2. This deformation may build at the rate of a few centimeters per year. When the accumulated strain is great enough to overcome the strength of the rocks, the result is a sudden break, or a springing back to the original shape as much as possible, a jolt which is felt on the surface as an earthquake. This sudden movement results in the shift of the roadway's surface, as shown in Time 3. The stored energy is released partly as heat, partly in alteration of the rock, and partly as a seismic wave.\n\n"}
{"id": "21026476", "url": "https://en.wikipedia.org/wiki?curid=21026476", "title": "Fabre's Book of Insects", "text": "Fabre's Book of Insects\n\nFabre's Book of Insects is a non-fiction book that is a retelling of Alexander Teixeira de Mattos' translation of Jean-Henri Fabre's \"Souvenirs entomologiques\". It was retold by Mrs. Rodolph Stawell and illustrated by Edward Detmold. It talks about insects in real life, mythology and folklore.\n\nA \"Times Higher Education\" review says \"It was \"Fabre's Book of Insects\", extracts from that extraordinary man's Souvenirs entomologiques, \"retold\" – and with an exemplary clarity and simplicity which made me feel enlisted and embraced but never patronised – by a Mrs Rudolph Stawell. Years later I read the full Souvenirs themselves and wondered why I had not done so long before, as soon as I could read French. The magic is Fabre's own, not imported by Mrs Stawell.\" It was reviewed by \"The New York Times\".\n"}
{"id": "10531634", "url": "https://en.wikipedia.org/wiki?curid=10531634", "title": "Ferndene State Reserve", "text": "Ferndene State Reserve\n\nThe Ferndene State Reserve is a state reserve in the Dial Range of northwest Tasmania, Australia. It comprises and is managed by the Tasmania Parks and Wildlife Service. It was established on 2 August 1939 and is described by the Parks and Wildlife Service as a \"scenic fern glade\".\n\n"}
{"id": "2374282", "url": "https://en.wikipedia.org/wiki?curid=2374282", "title": "Ice blink", "text": "Ice blink\n\nIce blink is a white light seen near the horizon, especially on the underside of low clouds, resulting from reflection of light off a field of ice immediately beyond. The ice blink was used by both the Inuit and explorers looking for the Northwest Passage to help them navigate safely.\n\n"}
{"id": "14602813", "url": "https://en.wikipedia.org/wiki?curid=14602813", "title": "Iddingsite", "text": "Iddingsite\n\nIddingsite is a microcrystalline rock that is derived from alteration of olivine. It is usually studied as a mineral, and consists of a mixture of remnant olivine, clay minerals, iron oxides and ferrihydrites. Debates over iddingsite's non-definite crystal structure caused it to be de-listed as an official mineral by the IMA; thus it is properly referred to as a rock.\n\nIddingsite forms from the weathering of basalt in the presence of liquid water and can be described as a phenocryst, i.e. it has megascopically visible crystals in a fine-grained groundmass of a porphyritic rock. It is a pseudomorph that has a composition that is constantly transforming from the original olivine, passing through many stages of structural and chemical change to create a fully altered iddingsite.\n\nBecause iddingsite is constantly transforming it does not have a definite structure or a definite chemical composition. The chemical formula for iddingsite has been approximated as MgO * FeO * 3SiO * 4 HO where MgO can be substituted by CaO. The geologic occurrence of iddingsite is limited to extrusive or subvolcanic rocks that are formed by injection of magma near the surface. It is absent from deep-seated rocks and is found on meteorites. As it has been found on Martian meteorites, its ages have been calculated to obtain absolute ages when liquid water was at or near the surface of Mars.\n\nIt was named after Joseph P. Iddings, an American petrologist. \n\nIddingsite is a pseudomorph, and during the alteration process the olivine crystals had their internal structure or chemical composition changed, although the external form has been preserved. This is not true for all phases of the alteration of olivine because the atomic arrangement becomes distorted and causes a non-definite structure to form. Iddingsite has a composition that is constantly transforming from the original olivine passing through many stages of structural and chemical change.\n\nIddingsite has been a subject researched in recent years because of its presence in the Martian meteorites. The formation of iddingsite requires liquid water, giving scientists an estimate as to when there has been liquid water on Mars. Potassium-argon dating of the meteorite samples showed that Mars had water on its surface anywhere from 1300 Ma to 650 Ma ago.\n\nIddingsite is a mineral that lacks a definite chemical composition, so exact compositions cannot be calculated. An approximated composition for a hypothetical end product of iddingsite has been calculated as being SiO = 16%, AlO = 8%, FeO = 62% and HO = 14%. Throughout the alteration process of olivine, there is a decrease in SiO, FeO and MgO and an increase in AlO and HO. The chemical process associated with the alteration consists of the addition of FeO and the removal of MgO (Gay and Le Maitre 1961). The chemical formula for iddingsite is approximated as MgO * FeO * 4 HO where MgO can be substituted by CaO by a ratio of 1:4. There are also some trace constituents of NaO and KO that enter iddingsite as the alteration process progresses.\n\nThe geologic occurrence of Iddingsite is limited to extrusive or hypabyssal rocks, and it is absent from deep-seated rocks. Iddingsite is an epimagmatic mineral derived during the final cooling of lava in which it occurs from a reaction between gases, water and olivine. The formation of iddingsite is not dependent on the original composition of the olivine. It is however dependent on oxidation conditions, hydration and the magma from which iddingsite forms must be rich in water vapor. The alteration of olivine to iddingsite occurs in a highly oxidizing environment under low pressure and at intermediate temperatures. Temperature needed for the alteration process has to be above temperatures that could cause the olivine to solidify, but below temperatures that would cause structural reorganization.\n\nThe structure of iddingsite is difficult to characterize because of the complexity of the possible alterations that can occur from olivine. Iddingsite has the tendency to be optically homogeneous which indicates that there is some structural control. Structural rearrangements are controlled by hexagonal sequences of approximately close-packed oxygen sheets. These oxygen layers are perpendicular to the x-axis of an olivine cell. One of the close-packed directions is parallel to the z-axis of an olivine cell. These ion arrangements within olivine control the structural orientation of the alteration products. X-ray diffraction patterns found that there are five structural types of iddingsite that can occur during different stages of alteration. They are: olivine-like structures, goethite-like structures, hematite structures, spinel structures and silicate structures.\n\nOlivine has an orthorhombic structure with a space group of Pbnm. Olivine-like structures represent the stage that breaks down olivine with chemical changes introduced by alterations. These structures have the cell dimensions a = 4.8, b = 10.3 and c = 6.0 Å, a space group Pbnm and a d-spacing of 2.779 Å. Olivine axes are oriented in the following way: a is parallel to X-axis, b is parallel to Y-axis and c is parallel to Z-axis. X-ray diffraction patterns taken from iddingsite vary from true olivine pattern to patterns that are very diffuse spots. This is an indication of a distorted structure caused by atomic replacement creating a distorted atomic arrangement.\n\nGoethite-like structures are common because goethite is in the same space group as olivine. This allows for goethite to grow within the olivine making the close packed planes common for both structures. Goethite-like structures have cell dimensions a=4.6, b= 10.0 and c = 3.0 Å. Diffraction spots caused by goethite are diffuse even though the material is well oriented. These structures are aligned parallel to the original olivine with a-axis (goethite) parallel to a-axis (olivine), b-axis (goethite) parallel to b-axis (olivine) and c-axis (goethite) parallel to c-axis (olivine). The preferred orientation of olivine and goethite are when they are parallel with their z-axis.\n\nHematite-like structures occur in a similar fashion as goethite. Hematite has a triagonal crystal system and experiences twinning by having an approximately hexagonal close-packed oxygen framework and has a structural orientation similar to olivine. When twinning occurs, the orientation of hematite-like iddingsite is as follows: a-axis of olivine is parallel to the c-axis of hematite, the b-axis of olivine is parallel to the +/− [010] plane of hematite and the c-axis of olivine is parallel to the +/− [210] plane of hematite. This hematite structure is very well oriented and occurs because of the high stability of the anion framework and because the cations can be made to migrate throughout the structure.\n\nSpinel structures consist of multiple oxide structures that are cubic and have cubic close packing. The spinel structures have a twined orientation and are controlled by close packed sheets. This twined orientation is can be described as: the a-axis of olivine is parallel to the (111) spinel face. The b-axis of olivine is parallel to +/− (112) and the c-axis of olivine is parallel to +/− (110) spinel face. These alterations tend to be rare in iddingsite but when they are present they show a sharp diffraction spot making them easily identified.\n\nSilicate structures are the most variable among all of the structures discussed. A common silicate structure consists of a hexagonal array of cylinders whose length is parallel to the x-axis of the olivine and the side of the hexagonal cell is parallel to the z-axis of olivine. Diffraction effects caused by this structure can be attributed to the formation of sheet silicate structures that have a very disordered stacking of layers.\nIddingsite is a pseudomorph that usually has crystals rimmed by a thin zone of yellowish brown or greenish cryptocrystalline material. The color of iddingsite varies from red-brown to orange-brown to deep ruby red to orange-red. The color of iddingsite in plane polarized light is the same until the later alteration stages when it turns into a darker color due to the strengthening effect of pleochroism. An increase in beta refractive index, which typically is 1.9 can be seen in most types of iddingsite, as the alteration process proceeds. Iddingsite also exhibits an increase in birefringence and dispersion as the alteration process proceeds.\n\nSome samples that have completed their alterations have miscellaneous cleavage thereby making it not a very good diagnostic tool. Most samples have no cleavage at all. Thin sections of Lismore, New South Wales, Australia, have a lamellar habit with one well developed cleavage and two subsidiary cleavages at right angles to each other. It has an alpha of 1.7 to 1.68 and a gamma of 1.71 to 1.72 and a birefringence of 0.04. On average iddingsite has a density of about 2.65 g/cm and a hardness of 3 (calcite). Variability in these values are expected due to the differences in crystal structure that can occur from different stages in the alteration process.\n\n"}
{"id": "7739128", "url": "https://en.wikipedia.org/wiki?curid=7739128", "title": "Islanding", "text": "Islanding\n\nIslanding is the condition in which a distributed generator (DG) continues to power a location even though electrical grid power is no longer present. Islanding can be dangerous to utility workers, who may not realize that a circuit is still powered, and it may prevent automatic re-connection of devices. Additionally, without strict frequency control the balance between load and generation in the islanded circuit is going to be violated, leading to abnormal frequencies and voltages. For those reasons, distributed generators must detect islanding and immediately disconnect from the circuit; this is referred to as anti-islanding.\n\nA common example of islanding is a distribution feeder that has solar panels attached to it. In the case of a power outage, the solar panels will continue to deliver power as long as irradiance is sufficient. In this case, the circuit detached by the outage becomes an \"island\". For this reason, solar inverters that are designed to supply power to the grid are generally required to have some sort of automatic anti-islanding circuitry.\n\nSome designs, commonly known as a microgrid, allow for intentional islanding. In case of an outage, a microgrid controller disconnects the local circuit from the grid on a dedicated switch and forces the distributed generator(s) to power the entire local load.\n\nElectrical inverters are devices that convert direct current (DC) to alternating current (AC). Grid-interactive inverters have the additional requirement that they produce AC power that matches the existing power presented on the grid. In particular, a grid-interactive inverter must match the voltage, frequency and phase of the power line it connects to. There are numerous technical requirements to the accuracy of this tracking.\n\nConsider the case of a house with an array of solar panels on the roof. Inverter(s) attached to the panels convert the varying DC current provided by the panels into AC power that matches the grid supply. If the grid is disconnected, the voltage on the grid line might be expected to drop to zero, a clear indication of a service interruption. However, consider the case when the house's load exactly matches the output of the panels at the instant of the grid interruption. In this case the panels can continue supplying power, which is used up by the house's load. In this case there is no obvious indication that an interruption has occurred.\n\nNormally, even when the load and production are exactly matched, the so-called \"balanced condition\", the failure of the grid will result in several additional transient signals being generated. For instance, there will almost always be a brief decrease in line voltage, which will signal a potential fault condition. However, such events can also be caused by normal operation, like the starting of a large electric motor.\n\nMethods that detect islanding without a large number of false positives is the subject of considerable research. Each method has some threshold that needs to be crossed before a condition is considered to be a signal of grid interruption, which leads to a \"non-detection zone\" (NDZ), the range of conditions where a real grid failure will be filtered out.\n\nGiven the activity in the field, and the large variety of methods that have been developed to detect islanding, it is important to consider whether or not the problem actually demands the amount of effort being expended. Generally speaking, the reasons for anti-islanding are given as (in no particular order):\n\n\nThe first issue has been widely dismissed by many in the power industry. Line workers are already constantly exposed to unexpectedly live wires in the course of normal events (i.e. is a house blacked out because it has no power, or because they pulled the main breaker inside?). Normal operating procedures under hot-line rules or dead-line rules require line workers to test for power as a matter of course, and it has been calculated that active islands would add a negligible risk. However, other emergency workers may not have time to do a line check, and these issues have been extensively explored using risk-analysis tools. A UK-based study concluded that \"The risk of electric shock associated with islanding of PV systems under worst-case PV penetration scenarios to both network operators and customers is typically <10 per year.\"\n\nThe second possibility is also considered extremely remote. In addition to thresholds that are designed to operate \"quickly\", islanding detection systems also have absolute thresholds that will trip long before conditions are reached that could cause end-user equipment damage. It is, generally, the last two issues that cause the most concern among utilities. Reclosers are commonly used to divide up the grid into smaller sections that will automatically, and quickly, re-energize the branch as soon as the fault condition (a tree branch on lines for instance) clears. There is some concern that the reclosers may not re-energize in the case of an island, or that the rapid cycling they cause might interfere with the ability of the DG system to match the grid again after the fault clears.\n\nIf an islanding issue does exist, it appears to be limited to certain types of generators. A 2004 Canadian report concluded that synchronous generators, installations like microhydro, were the main concern. These systems may have considerable mechanical inertia that will provide a useful signal. For inverter based systems, the report largely dismissed the problem; \"Anti-islanding technology for inverter based DG systems is much better developed, and published risk assessments suggest that the current technology and standards provide adequate protection while penetration of DG into the distribution system remains relatively low.\" The report also noted that \"views on the importance \nof this issue tend to be very polarized,\" with utilities generally considering the possibility of occurrence and its impacts, while those supporting DG systems generally use a risk based approach and the very low probabilities of an island forming.\n\nAn example of such an approach, one that strengthens the case that islanding is largely a non-issue, is a major real-world islanding experiment that was carried out in the Netherlands in 1999. Although based on then-current anti-islanding system, typically the most basic voltage jump detection methods, the testing clearly demonstrated that islands could not last longer than 60 seconds. Moreover, the theoretical predictions were true; the chance of a balance condition existing were on the order of 10 a year, and that the chance that the grid would disconnect at that point in time was even less. As an island can only form when both conditions are true, they concluded that the \"Probability of encountering an islanding is virtually zero\"\n\nNevertheless, utility companies have continued to use islanding as a reason to delay or refuse the implementation of distributed generation systems. In Ontario, Hydro One recently introduced interconnection guidelines that refused connection if the total distributed generation capacity on a branch was 7% of the maximum yearly peak power. At the same time, California sets a limit of 15% only for review, allowing connections up to 30%, and is actively considering moving the review-only limit to 50%.\n\nThe issue can be hotly political. In Ontario a number of potential customers taking advantage of a new Feed-in tariff program were refused connection only after building their systems. This was a problem particularly in rural areas where numerous farmers were able to set up small (10 kWp) systems under the \"capacity exempt\" microFIT program only to find that Hydro One had implemented a new capacity regulation after the fact, in many cases after the systems had been installed.\n\nDetecting an islanding condition is the subject of considerable research. In general, these can be classified into passive methods, which look for transient events on the grid, and active methods, which probe the grid by sending signals of some sort from the inverter or the grid distribution point. There are also methods that the utility can use to detect the conditions that would cause the inverter-based methods to fail, and deliberately upset those conditions in order to make the inverters switch off. A Sandia Labs Report covers many of these methodologies, both in-use and future developments. These methods are summarized below.\n\nPassive methods include any system that attempts to detect transient changes on the grid, and use that information as the basis as a probabilistic determination of whether or not the grid has failed, or some other condition has resulted in a temporary change.\n\nAccording to Ohm's law, the voltage in an electrical circuit is a function of electric current (the supply of electrons) and the applied load (resistance). In the case of a grid interruption, the current being supplied by the local source is unlikely to match the load so perfectly as to be able to maintain a constant voltage. A system that periodically samples voltage and looks for sudden changes can be used to detect a fault condition.\n\nUnder/over voltage detection is normally trivial to implement in grid-interactive inverters, because the basic function of the inverter is to match the grid conditions, including voltage. That means that all grid-interactive inverters, by necessity, have the circuitry needed to \"detect\" the changes. All that is needed is an algorithm to detect sudden changes. However, sudden changes in voltage are a common occurrence on the grid as loads are attached and removed, so a threshold must be used to avoid false disconnections.\n\nThe range of conditions that result in non-detection with this method may be large, and these systems are generally used along with other detection systems.\n\nThe frequency of the power being delivered to the grid is a function of the supply, one that the inverters carefully match. When the grid source is lost, the frequency of the power would fall to the natural resonant frequency of the circuits in the island. Looking for changes in this frequency, like voltage, is easy to implement using already required functionality, and for this reason almost all inverters also look for fault conditions using this method as well.\n\nUnlike changes in voltage, it is generally considered highly unlikely that a random circuit would naturally have a natural frequency the same as the grid power. However, many devices deliberately synchronize to the grid frequency, like televisions. Motors, in particular, may be able to provide a signal that is within the NDZ for some time as they \"wind down\". The combination of voltage and frequency shifts still results in a NDZ that is not considered adequate by all.\n\nIn order to decrease the time in which an island is detected, rate of change of frequency has been adopted as a detection method. The rate of change of frequency is given by the following expression:\n\nformula_1\n\nwhere formula_2 is the system frequency, formula_3 is the time, formula_4 is the power imbalance (formula_5), formula_6 is the system capacity, and formula_7 is the system inertia.\n\nShould the rate of change of frequency, or ROCOF value, be greater than a certain value, the embedded generation will be disconnected from the network.\n\nLoads generally have power factors that are not perfect, meaning that they do not accept the voltage from the grid perfectly, but impede it slightly. Grid-tie inverters, by definition, have power factors of 1. This can lead to changes in phase when the grid fails, which can be used to detect islanding.\n\nInverters generally track the phase of the grid signal using a phase locked loop (PLL) of some sort. The PLL stays in sync with the grid signal by tracking when the signal crosses zero volts. Between those events, the system is essentially \"drawing\" a sine-shaped output, varying the current output to the circuit to produce the proper voltage waveform. When the grid disconnects, the power factor suddenly changes from the grid's (1) to the load's (~1). As the circuit is still providing a current that would produce a smooth voltage output given the known loads, this condition will result in a sudden change in voltage. By the time the waveform is completed and returns to zero, the signal will be out of phase.\n\nThe main advantage to this approach is that the shift in phase will occur even if the load exactly matches the supply in terms of Ohm's law - the NDZ is based on power factors of the island, which are very rarely 1. The downside is that many common events, like motors starting, also cause phase jumps as new impedances are added to the circuit. This forces the system to use relatively large thresholds, reducing its effectiveness.\n\nEven with noisy sources, like motors, the total harmonic distortion (THD) of a grid-connected circuit is generally unmeasurable due to the essentially infinite capacity of the grid that filters these events out. Inverters, on the other hand, generally have much larger distortions, as much as 5% THD. This is a function of their construction; some THD is a natural side-effect of the switched-mode power supply circuits most inverters are based on.\n\nThus, when the grid disconnects, the THD of the local circuit will naturally increase to that of the inverters themselves. This provides a very secure method of detecting islanding, because there are generally no other sources of THD that would match that of the inverter. Additionally, interactions within the inverters themselves, notably the transformers, have non-linear effects that produce unique 2nd and 3rd harmonics that are easily measurable.\n\nThe drawback of this approach is that some loads may filter out the distortion, in the same way that the inverter attempts to. If this filtering effect is strong enough, it may reduce the THD below the threshold needed to trigger detection. Systems without a transformer on the \"inside\" of the disconnect point will make detection more difficult. However, the largest problem is that modern inverters attempt to lower the THD as much as possible, in some cases to unmeasurable limits.\n\nActive methods generally attempt to detect a grid failure by injecting small signals into the line, and then detecting whether or not the signal changes.\n\nThis method is an active islanding detection method which can be used by three-phase electronically coupled distributed generation (DG) units. The method is based on injecting a negative-sequence current through the voltage-sourced converter (VSC) controller and detecting and quantifying the corresponding negative-sequence voltage at the point of common coupling (PCC) of the VSC by means of a unified three-phase signal processor (UTSP). The UTSP system is an enhanced phase-locked loop (PLL) which provides a high degree of immunity to noise, and thus enables islanding detection based on injecting a small negative-sequence current. The negative-sequence current is injected by a negative-sequence controller which is adopted as the complementary of the conventional VSC current controller. The negative-sequence current injection method detects an islanding event within 60 ms (3.5 cycles) under UL1741 test conditions, requires 2% to 3% negative-sequence current injection for islanding detection, can correctly detect an islanding event for the grid short circuit ratio of 2 or higher, and is insensitive to variations of the load parameters of UL1741 test system. \nImpedance Measurement attempts to measure the overall impedance of the circuit being fed by the inverter. It does this by slightly \"forcing\" the current amplitude through the AC cycle, presenting too much current at a given time. Normally this would have no effect on the measured voltage, as the grid is an effectively infinitely stiff voltage source. In the event of a disconnection, even the small forcing would result in a noticeable change in voltage, allowing detection of the island.\n\nThe main advantage of this method is that it has a vanishingly small NDZ for any given single inverter. However, the inverse is also the main weakness of this method; in the case of multiple inverters, each one would be forcing a slightly different signal into the line, hiding the effects on any one inverter. It is possible to address this problem by communication between the inverters to ensure they all force on the same schedule, but in a non-homogeneous install (multiple installations on a single branch) this becomes difficult or impossible in practice. Additionally, the method only works if the grid is effectively infinite, and in practice many real-world grid connections do not sufficiently meet this criterion.\n\nAlthough the methodology is similar to Impedance Measurement, this method, also known as \"harmonic amplitude jump\", is actually closer to Harmonics Detection. In this case, the inverter deliberately introduces harmonics at a given frequency, and as in the case of Impedance Measurement, expects the signal from the grid to overwhelm it until the grid fails. Like Harmonics Detection, the signal may be filtered out by real-world circuits.\n\nThis is one of the newest methods of islanding detection, and in theory, one of the best. It is based on forcing the phase of the inverter's output to be slightly mis-aligned with the grid, with the expectation that the grid will overwhelm this signal. The system relies on the actions of a finely tuned phase-locked loop to become unstable when the grid signal is missing; in this case, the PLL attempts to adjust the signal back to itself, which is tuned to continue to drift. In the case of grid failure, the system will quickly drift away from the design frequency, eventually causing the inverter to shut down.\n\nThe major advantage of this approach is that it can be implemented using circuitry that is already present in the inverter. The main disadvantage is that it requires the inverter to always be slightly out of time with the grid, a lowered power factor. Generally speaking, the system has a vanishingly small NDZ and will quickly disconnect, but it is known that there are some loads that will react to offset the detection.\n\nFrequency bias forces a slightly off-frequency signal into the grid, but \"fixes\" this at the end of every cycle by jumping back into phase when the voltage passes zero. This creates a signal similar to Slip Mode, but the power factor remains closer to that of the grid's, and resets itself every cycle. Moreover, the signal is less likely to be filtered out by known loads. The main disadvantage is that every inverter would have to agree to shift the signal back to zero at the same point on the cycle, say as the voltage crosses back to zero, otherwise different inverters will force the signal in different directions and filter it out.\n\nThere are numerous possible variations to this basic scheme. The Frequency Jump version, also known as the \"zebra method\", inserts forcing only on a specific number of cycles in a set pattern. This dramatically reduces the chance that external circuits may filter the signal out. This advantage disappears with multiple inverters, unless some way of synchronizing the patterns is used.\n\nThe utility also has a variety of methods available to it to force systems offline in the event of a failure.\n\nMost small generator connections require a mechanical disconnect switch, so at a minimum the utility could send a repairman to pull them all. For very large sources, one might simply install a dedicated telephone hotline that can be used to have an operator manually shut down the generator. In either case, the reaction time is likely to be on the order of minutes, or hours.\n\nManual disconnection could be automated through the use of signals sent though the grid, or on secondary means. For instance, power line carrier communications could be installed in all inverters, periodically checking for signals from the utility and disconnecting either on command, or if the signal disappears for a fixed time. Such a system would be highly reliable, but expensive to implement.\n\nAs the utility can be reasonably assured that they will always have a method for discovering a fault, whether that be automated or simply looking at the recloser, it is possible for the utility to use this information and transmit it down the line. This can be used to force the tripping of properly equipped DG systems by deliberately opening a series of recloser in the grid to force the DG system to be isolated in a way that forces it out of the NDZ. This method can be guaranteed to work, but requires the grid to be equipped with automated recloser systems, and external communications systems that guarantee the signal will make it through to the reclosers.\n\nA related concept is to deliberately force a section of the grid into a condition that will guarantee the DG systems will disconnect. This is similar to the transfer-trip method, but uses active systems at the head-end of the utility, as opposed to relying on the topology of the network.\n\nA simple example is a large bank of capacitors that are added to a branch, left charged up and normally disconnected by a switch. In the event of a failure, the capacitors are switched into the branch by the utility after a short delay. This can be easily accomplished through automatic means at the point of distribution. The capacitors can only supply current for a brief period, ensuring that the start or end of the pulse they deliver will cause enough of a change to trip the inverters.\n\nThere appears to be no NDZ for this method of anti-islanding. Its main disadvantage is cost; the capacitor bank has to be large enough to cause changes in voltage that will be detected, and this is a function of the amount of load on the branch. In theory, very large banks would be needed, an expense the utility is unlikely to look on favourably.\n\nAnti-islanding protection can be improved through the use of the Supervisory Control and Data Acquisition (SCADA) systems already widely used in the utility market. For instance, an alarm could sound if the SCADA system detects voltage on a line where a failure is known to be in progress. This does not affect the anti-islanding systems, but may allow any of the systems noted above to be quickly implemented.\n\nDistributed Resource Unit, IEEE Trans. on Power Electronics, VOL. 23, NO. 1, JANUARY 2008.\n\n"}
{"id": "40496840", "url": "https://en.wikipedia.org/wiki?curid=40496840", "title": "Jennifer Francis", "text": "Jennifer Francis\n\nJennifer Ann Francis is a research professor at Rutgers University's Institute of Marine and Coastal Sciences since 1994.\n\nFrancis received her PhD in atmospheric sciences from the University of Washington in 1994.\n\nFrom 1987 to 1988, she was a research assistant at the Ames Research Center. From 1988 to 1994, while attending the University of Washington, she was a research assistant at the department of Polar Science Center there.\n\nFrancis's research focuses on climate change in the Arctic, and has published over 40 scientific papers on the topic. It is also her opinion that warming in the Arctic may be changing the jet stream, which, in turn, may be leading to abnormal weather patterns such as an unusually long winter in the United Kingdom, the 2013 Colorado floods, and the unusually cold conditions across much of the southern United States in early 2014. Specifically, Francis argues that the heating and cooling of Arctic seawater (the Arctic is warming much faster than the rest of the world) has slowed down the jet stream, resulting in weather conditions persisting for longer than they usually would. That the warming in the Arctic is linked to extreme weather elsewhere in the world is a view supported by some of Francis's research, such as a study published in Geophysical Research Letters in 2012.\n\n"}
{"id": "1627865", "url": "https://en.wikipedia.org/wiki?curid=1627865", "title": "Jomolhari", "text": "Jomolhari\n\nJomolhari or Chomolhari (; ) sometimes known as \"the bride of Kangchenjunga”, is a mountain in the Himalayas, straddling the border between Yadong County of Tibet, China and the Thimphu district of Bhutan. The north face rises over above the barren plains. The mountain is the source of the Paro Chu (Paro river) which flows from the south side and the Amo Chu which flows from the north side.\n\nThe mountain is sacred to Tibetan Buddhists who believe it is the abode of one of the Five Tsheringma Sisters; \"(jo mo tshe ring mched lnga)\" — female protector goddesses (Jomo) of Tibet and Bhutan, who were bound under oath by Padmasambhava to protect the land, the Buddhist faith and the local people.\n\nOn the Bhutanese side is a Jomolhari Temple, toward the south side of the mountain about a half-day's journey from the army outpost between Thangthangkha and Jangothang at an altitude of 4150 meters. Religious practitioners and pilgrims visiting Mt. Jomolhari stay at this temple. There are several other sacred sites near Jomolhari Temple, including meditation caves of Milarepa and Gyalwa Lorepa. Within an hour's walk up from the temple at an altitude of c. 4450 meters is Tseringma Lhatso, the \"spirit lake\" of Tsheringma.\n\nIn Tibet there is an annual pilgrimage from Pagri to a holy lake, Jomo Lharang, which lies at c. elevation, just north of the mountain.\n\nBecause Jomolhari was sacred and the home of goddesses, those living nearby believed it was impossible to climb, and that anyone who climbed too high would be thrown down.\n\nDespite its notability and spectacular visibility from the old trade route between India and Lhasa that passes through the Chumbi Valley, the mountain has seen little climbing activity. It was known to climbers passing by on the way to Everest and was scouted by Odell as early as 1924. In 1937 a permission to climb the sacred mountain was granted to a British expedition headed by Freddie Spencer Chapman by both \"the Tibetans\" and the \"Maharajah of Bhutan. Although no refusals are known of earlier climbing requests, Chapman believed this was the reason it had gone unclimbed until 1937. Six porters accompanied the 5 man climbing team from Phari across Sur La into Bhutan. Chapman and Sherpa Pasang Dawa Lama (of the American K2 expedition fame) reached the summit via the southeast spur on 21 May 1937. The protracted and epic descent, which they were fortunate to survive, is described in detail in Chapman’s \"Helvellyn to Himalaya\" published in 1940.\n\nThe second ascent was only on 24 April 1970 -over the same route- by a joint Bhutanese-Indian military expedition led by Colonel Narendra Kumar. This ascent was notable also for the disappearance of two climbing members and a sherpa in the second summit party the following day. Dorjee Lhatoo (Nanda Devi East 1975, West 1981) led the route, partnered with Prem Chand (2nd ascent Kanchenjunga 1977) all the way to the summit via two camps. Lhatoo was charged with laying a \"Sachu Bumter\" offering on the summit by the Bhutanese King in order to \"appease\" mountain deities - apparently a pot containing gold, silver and precious stones. The following day, the second party of three were spotted close to the ridge when they became obscured by cloud. When the cloud lifted, they were gone. A telephoto lens and fruit cans were found on the ridge by a search party. Prem Chand went up to the ridge and reported gunshots thudding into the ice and whipping up ice chips - thus ending any further attempts in locating the missing bodies. Lhatoo and Prem Chand, on their way up during their successful summit attempt had reported seeing a lot of PLA activity on the Lhasa-Chumbi highway. The reason for their disappearance remains speculative - did they fall or were they shot? All three were relatively inexperienced climbers and Lhatoo later speculated on the exposure on the knife-edged ridge leading to the summit slope as a possible incident site. He (an ex-Gurkha himself) is quoted as believing the shooting theory to be unlikely but possible, citing his difficulty in estimating the distance between the ridge and possible Chinese positions on the Tibetan side. An account of the expedition is available in the Himalayan Journal 2000. Prem Chand has not spoken publicly on the matter. Chinese displeasure with Bhutan over the expedition and sensitivities in New Delhi led to a complete media blackout of what was otherwise a notable Indian climb.\n\nThe third ascent was made in 1996 by a joint Japanese-Chinese expedition which reached the south col from the Tibetan side and climbed the peak over the south ridge. On 7 May 2004, British climbers Julie-Ann Clyma and Roger Payne reached the summit via the c. 5800 m south col as well, in a single day's dash from the col, after attempts to climb the impressive northwest pillar were thwarted by strong winds.\n\nIn October 2006, a six-member Slovenian team climbed two new routes, registering the fifth and sixth ascents. Rok Blagus, Tine Cuder, Samo Krmelj and Matej Kladnik took the left couloir of the north face to the East ridge at c. 7100 m, from which they followed the ridge to the top, while Marko Prezelj and Boris Lorencic climbed the northwest ridge in a six-day round trip. This climb earned Prezelj and Lorencic the Piolet d'Or in January 2007.\n\n\n\n"}
{"id": "16921463", "url": "https://en.wikipedia.org/wiki?curid=16921463", "title": "Leaching (pedology)", "text": "Leaching (pedology)\n\nIn pedology, leaching is the loss of mineral and organic solutes due to very heavy rainfall, high temperature and percolation. It is a mechanism of soil formation distinct from the soil forming process of eluviation, which is the loss of mineral and organic colloids. Leached and elluviated materials tend to be lost from topsoil and deposited in subsoil. A soil horizon accumulating leached and eluviated materials is referred to as a zone of illuviation.\n\nLaterite soil, which develops in regions with high temperature and heavy rainfall, is an example of this process in action.\n\n"}
{"id": "5072827", "url": "https://en.wikipedia.org/wiki?curid=5072827", "title": "List of Lepidoptera that feed on cinquefoils", "text": "List of Lepidoptera that feed on cinquefoils\n\nCinquefoils and silverweeds of the genera \"Argentina\", \"Comarum\" \"Dasiphora\", \"Drymocallis\", \"Potentilla\" and \"Sibbaldiopsis\" (formerly all included in \"Potentilla\") are used as food plants by the caterpillars of some Lepidoptera species, including:\n\nSpecies which feed exclusively on cinquefoils\n\nColeophoridae\n\nHesperiidae\n\nSpecies which feed on cinquefoils and other plants\n\nColeophoridae\n\nHesperiidae\n\nLasiocampidae\n\nLycaenidae\n\nSaturniidae\n\n"}
{"id": "45633505", "url": "https://en.wikipedia.org/wiki?curid=45633505", "title": "List of Ngorongoro Crater plants", "text": "List of Ngorongoro Crater plants\n\nAn alphabetical list of plants occurring in the Ngorongoro Crater in Tanzania\nNgorongoro Crater covers an area of 265 square kilometres and was formed some 2.5 million years ago from the collapse of a volcanic mountain which had become inactive. Its rim lies some 2280 meters above sea level, while the floor is at 1800 meters. The diversity of landforms and altitude has produced similarly diverse habitats of mountains, gallery forests, craters, grassy plains, swamps, lakes and woodlands, ranging from arid and semi-arid communities below 1,300 m with abundant grazing to montane vegetation with tall grassland, open moors and relict evergreen montane forests on the steep slopes. There is a large stand of bamboo, \"Yushania alpina\", on Oldeani (an extinct volcano of 3,188 meters) and Pencil Cedar, \"Juniperus procera\", on Makarut Mountain. There are four extinct volcanic peaks, all over 3,000 meters, the highest of which is Loolmalasin (3,648 m). Within the crater lies Lake Magadi, a small soda lake.\n\nThe crater floor consists of shortgrass savanna punctuated by shallow, fresh and brackish lakes, marshes and swamps. Two notable forests are Lerai Forest and Laiyanai Forest, while the surrounds of Lake Eyasi are dominated by \"Acacia mellifera\" and \"Dalbergia melanoxylon\" during the dry season.\n\n\n\n\n"}
{"id": "19660381", "url": "https://en.wikipedia.org/wiki?curid=19660381", "title": "List of Preserve America Communities", "text": "List of Preserve America Communities\n\nThis is a list of United States municipalities, counties, neighborhoods, and tribal communities that have been designated as \"Preserve America Communities\" under the federal government's Preserve America program. As of 2017, more than 900 communities, representing all 50 states and two U.S. territories, had been so designated.\n"}
{"id": "6185148", "url": "https://en.wikipedia.org/wiki?curid=6185148", "title": "List of Sites of Special Scientific Interest in Mid Argyll and Cowal", "text": "List of Sites of Special Scientific Interest in Mid Argyll and Cowal\n\nThe following is a list of Sites of Special Scientific Interest in the Mid Argyll and Cowal Area of Search. For other areas, see List of SSSIs by Area of Search.\n\n"}
{"id": "10834544", "url": "https://en.wikipedia.org/wiki?curid=10834544", "title": "List of Superfund sites in Alabama", "text": "List of Superfund sites in Alabama\n\nThis is a list of Superfund sites in Alabama designated under the Comprehensive Environmental Response, Compensation, and Liability Act (CERCLA) environmental law. The CERCLA federal law of 1980 authorized the United States Environmental Protection Agency (EPA) to create a list of polluted locations requiring a long-term response to clean up hazardous material contaminations. These locations are known as Superfund sites, and are placed on the National Priorities List (NPL). \n\nThe NPL guides the EPA in \"determining which sites warrant further investigation\" for environmental remediation. As of November 13, 2014, there were 14 Superfund sites on the National Priorities List in Alabama. Two additional sites have been proposed for entry on the list. One site has been cleaned up and removed from the list.\n\n\n"}
{"id": "40060954", "url": "https://en.wikipedia.org/wiki?curid=40060954", "title": "List of asteroid close approaches to Earth in 2013", "text": "List of asteroid close approaches to Earth in 2013\n\nBelow is the list of asteroid close approaches to Earth in 2013. This was the year of the Chelyabinsk impact, in addition to the other NEO flybys\n\nA list of known Near-Earth asteroid close approaches less than 1 lunar distance (384,400 km or 0.00256 AU) from Earth in 2013.\n\nThis list does not include any of the 24 objects that collided with earth in 2013, none of which were discovered in advance, but were recorded by sensors designed to detect detonation of nuclear devices. Of the 24 objects so detected, 5 had an impact energy greater than that of a 1 kiloton device including the 440 kiloton Chelyabinsk meteor, estimated at 20 m in diameter, which injured around 1500 people and damaged over 7000 buildings. \n\nAn example list of near-Earth asteroids that passed more than 1 lunar distance (384,400 km or 0.00256 AU) from Earth in 2013.\n\n\n"}
{"id": "11469982", "url": "https://en.wikipedia.org/wiki?curid=11469982", "title": "List of crop plants pollinated by bees", "text": "List of crop plants pollinated by bees\n\nThis is a list of crop plants pollinated by bees. Most of them are pollinated in whole or part by honeybees and by the crop's natural pollinators such as bumblebees, orchard bees, squash bees, and solitary bees. Where the same plants have non-bee pollinators such as birds or other insects, these are also indicated.\n\nPollination by insects is called entomophily. Entomophily is a form of plant pollination whereby pollen is distributed by insects, particularly bees, Lepidoptera (butterflies and moths), flies and beetles. Honey bees pollinate many plant species that are not native to their natural habitat but are often inefficient pollinators of such plants; if they are visiting ten different species of flower, only a tenth of the pollen they carry may be the right species. Other bees tend to favor one species at a time, therefore do most of the actual pollination. \n\nThe most essential staple food crops on the planet, like corn, wheat, rice, soybeans and sorghum, need no insect help at all; they are wind-pollinated or self-pollinating. Other staple food crops, like bananas and plantains, are sterile and propagated from cuttings, requiring no pollination of any form, ever. Further, foods such as root vegetables and salad crops will produce a useful food crop without pollination, though they may not set seed; and hybrids do not even require insect pollination to produce seeds for the next generation, because hybrid production is always human-pollinated. Many of the most desirable and common non-hybrid crops, like heirloom tomatoes, are self-pollinated, which is what makes their cultivar stable.\n\nBuckwheat\n\n\n"}
{"id": "45708390", "url": "https://en.wikipedia.org/wiki?curid=45708390", "title": "List of power stations in Telangana", "text": "List of power stations in Telangana\n\nThe below is list of all the power plants installed and operated by central, private and state govt of Telangana.\n\nThermal power is the \"largest\" source of power in Telangana state. There are different types of Thermal power plants based on the fuel used to generate the steam such as coal, gas, diesel etc.\n\nThis is a list of hydroelectric power plants in Telangana.\n\nThis is a list of all solar power plants in Telangana.\n\nShankarampalli Gas based power generation station presently not working\n\n"}
{"id": "15530478", "url": "https://en.wikipedia.org/wiki?curid=15530478", "title": "List of rivers of Vietnam", "text": "List of rivers of Vietnam\n\nThis is a list of streams and rivers in Vietnam:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "43068147", "url": "https://en.wikipedia.org/wiki?curid=43068147", "title": "Major soil deposits of India", "text": "Major soil deposits of India\n\nThere are about five soil deposits in India.\n\nThese soils are formed by the sediments brought down by the rivers. They are also rich in chemical ingredients. The rivers deposit very fine particles of soil called alluvium in their plains during their long course of journey. Alluvial soil is also known as riverine soil because it is mainly found in the river basin.Alluvial soils are very fertile.They contains potash,phosphoric acid and lime which are ideal for the growth of sugarcane,paddy,wheat and other cereal and pulse crops.Soils in the drier areas are more alkaline and can be productive after proper treatment and irrigation.\n\nThese soils are found in Thar desert in the Indian state of Rajasthan. This soil is formed from arid conditions with practically negligible rainfall. This type of soil is highly pervious and has a low density. It requires densification to increase its bearing capacity and shearing strength. Commonly recognised plants that grow in these soils are cacti.\n\nThese soils are also called as regur soils. Central India and Deccan plateau mainly constitutes this type of soil.\nThe soil is suitable for growing cottons. It is believed that the climatic conditions along with the parent rock material are the important factors for the formation of black soil. This type of soil is typically of the Deccan trap region spread over Northwest Deccan plateau and is made from lava flows. They cover the plateaus of Maharashtra, Saurashtra, Malwa, Madhya Pradesh, Chhattisgarh and extend in South-East direction along Godavari and Krishna valleys. These soils contain essential clay minerals as montmorillonite. These soils cover an extensive area of 300,000 km. The engineering properties of such soils are as follows:\nThey are made up of clayey materials.They are well known for their capacity to hold moisture.They are rich in calcium carbonate,magnesium,potash and lime.\n\nLaterite soil\"s are formed from chemical decomposition of rocks. These soils mainly contain iron oxide which gives them characteristic pink or red color. These soils are found in Central, Eastern and South India. These are residual soils formed from basalt and have high specific gravity. These soils are mostly composed as calcite depositions.\n\nThese soils are found in narrow belt near the south-west coast of India. These soils have low shearing strength and high compressibility. The marine clays are soft and highly plastic.They contain large amount of organic matter and are not suitable for construction of megastructures like buildings, cranes etc.\n\n"}
{"id": "19731", "url": "https://en.wikipedia.org/wiki?curid=19731", "title": "Midgard", "text": "Midgard\n\nMidgard (an anglicised form of Old Norse ; Old English , Swedish and Danish \"Midgård\", Old Saxon , Old High German , Gothic \"Midjun-gards\"; \"middle yard\") is the name for Earth (equivalent in meaning to the Greek term , \"inhabited\") inhabited by and known to humans in early Germanic cosmology, and specifically one of the Nine Worlds in Norse mythology.\n\nThis name occurs in Old Norse literature as . In Old Saxon \"Heliand\" it appears as and in Old High German poem \"Muspilli\" it appears as . The Gothic form is attested in the Gospel of Luke as a translation of the Greek word . The word is present in Old English epic and poetry as ; later transformed to or (\"Middle-earth\") in Middle English literature.\n\nAll these forms are from a Common Germanic \"*midja-gardaz\" (\"*meddila-\", \"*medjan-\"), a compound of \"*midja-\" \"middle\" and \"*gardaz\" \"yard, enclosure\".\nIn early Germanic cosmology, the term stands alongside \"world\" (Old English \"weorold\", Old Saxon \"werold\", Old High German \"weralt\", Old Frisian \"warld\" and Old Norse \"verǫld\"), from a Common Germanic compound \"*wira-alđiz\", the \"age of men\".\n\nMidgard is a realm in Norse mythology. It is one of the Nine Worlds—the only one that is completely visible to mankind (the others may intersect with this visible realm but are mostly invisible). Pictured as placed somewhere in the middle of Yggdrasil, Midgard is between the land of Niflheim—the land of ice—to the north and Muspelheim—the land of fire—to the south. Midgard is surrounded by a world of water, or ocean, that is impassable. The ocean is inhabited by the great sea serpent Jörmungandr (Miðgarðsormr), who is so huge that he encircles the world entirely, grasping his own tail. The concept is similar to that of the Ouroboros. Midgard was also connected to Asgard, the home of the gods, by the Bifröst, the rainbow bridge, guarded by Heimdallr.\n\nIn Norse mythology, \"Miðgarðr\" became applied to the wall around the world that the gods constructed from the eyebrows of the giant Ymir as a defense against the Jotuns who lived in Jotunheim, east of \"Manheimr\", the \"home of men\", a word used to refer to the entire world. The gods slew the giant Ymir, the first created being, and put his body into the central void of the universe, creating the world out of his body: his flesh constituting the land, his blood the oceans, his bones the mountains, his teeth the cliffs, his hairs the trees, and his brains the clouds. Aurgelmir's skull was held by four dwarfs, Nordri, Sudri, Austri, and Vestri, who represent the four points on the compass and became the dome of heaven. The sun, moon, and stars were said to be scattered sparks in the skull.\nAccording to the Eddas, Midgard will be destroyed at Ragnarök, the battle at the end of the world. Jörmungandr will arise from the ocean, poisoning the land and sea with his venom and causing the sea to rear up and lash against the land. The final battle will take place on the plane of Vígríðr, following which Midgard and almost all life on it will be destroyed, with the earth sinking into the sea, only to rise again, fertile and green when the cycle repeats and the creation begins again.\n\nAlthough most surviving instances of the word Midgard refer to spiritual matters, it was also used in more mundane situations, as in the Viking Age runestone poem from the inscription Sö 56 from Fyrby:\n\nThe Danish and Swedish form or , the Norwegian or , as well as the Icelandic and Faroese form , all derive from the Old Norse term.\n\nThe name \"middangeard\" occurs six times in the Old English epic poem \"Beowulf\", and is the same word as Midgard in Old Norse. The term is equivalent in meaning to the Greek term Oikoumene, as referring to the known and inhabited world.\n\nThe concept of Midgard occurs many times in Middle English. The association with \"earth\" (OE \"eorðe\") in Middle English \"middellærd\", \"middelerde\" is by popular etymology; the continuation of \"geard\" \"enclosure\" is \"yard\". An early example of this transformation is from the Ormulum:\n\nThe usage of \"Middle-earth\" as a name for a setting was popularized by Old English scholar J. R. R. Tolkien in his \"The Lord of the Rings\" and other fantasy works; he was originally inspired by the references to \"middangeard\" and \"Éarendel\" in the Old English poem \"Crist\".\n\n\"Mittilagart\" is mentioned in the 9th-century Old High German \"Muspilli\" (v. 54) meaning \"the world\" as opposed to the sea and the heavens:\n"}
{"id": "57484903", "url": "https://en.wikipedia.org/wiki?curid=57484903", "title": "Museum of the Flat Earth", "text": "Museum of the Flat Earth\n\nThe Museum of the Flat Earth is a small museum dedicated to the history of the Canadian Flat Earth Society, located on Fogo Island, Newfoundland. \n\nIt has a variety of historical collections covering the life of Bartholomew Seeker, as well as other individuals associated with the Canadian Flat Earth Society, and a series of more contemporary displays which deal with debates around the notion of a Flat Earth.\n\nThe Museum was formally opened in May 2016 by Dr Iris Taylor and Kay Burns as the culmination of 14 years formal research and public education. \n\nIt has a varied educational outreach programme, working with both the local community as well as visiting researchers, writers and creative practitioners. In 2017, the Museum launched a formal visiting artist programme, with the theme of the Great Auk. Artists participating in the programme included Yvonne Mullock, Marcus Coates and Michael Waterman. \n\nIt is based in a small building in Shoal Bay that includes a coffee bar and Museum Shop carrying unique flat earth items, with ample visitor parking. The Museum's location also has strong links to Flat Earth theory, with Brimstone Head, labelled as one of the corners of the Flat Earth, which is a prominent geological feature on Fogo Island.\n\nThe Museum has its roots in the 1970s revival of the Flat Earth Society of Canada, by a group of academics at the St.Thomas University, New Brunswick, which included Leo Ferrari, Alden Nowlan and Ray Fraser. However, the Society's activities declined in the early 1980s and it was only revived in 2003 by independent researcher Iris Taylor as a result of several visits to Fogo Island and extensive research at the University of New Brunswick Special Collections archives.\n\nThe Museum's Collections cover several areas and include: \n\n\n"}
{"id": "1489559", "url": "https://en.wikipedia.org/wiki?curid=1489559", "title": "Net energy gain", "text": "Net energy gain\n\nNet Energy Gain (NEG) is a concept used in energy economics that refers to the difference between the energy expended to harvest an energy source and the amount of energy gained from that harvest. The net energy gain, which can be expressed in joules, differs from the net financial gain that may result from the energy harvesting process, in that various sources of energy (e.g. natural gas, coal, etc.) can be priced differently for the same amount of energy.\n\nA net energy gain is achieved by expending less energy acquiring a source of energy than is contained in the source to be consumed. That is\n\nFactors to consider when calculating NEG is the type of energy, the way energy is used and acquired, and the methods used to store or transport the energy. It is also possible to overcomplicate the equation by an infinite number of externalities and inefficiencies that may be present during the energy harvesting process.\n\nThe definition of an energy source is not rigorous. Anything that can provide energy to anything else can qualify. Wood in a stove is full of potential thermal energy; in a car, mechanical energy is acquired from the combustion of gasoline, and the combustion of coal is converted from thermal to mechanical, and then to electrical energy.\nExamples of energy sources include:\n\n\nThe term net energy gain can be used in slightly different ways:\n\nThe usual definition of net energy gain compares the energy required to extract energy (that is, to find it, remove it from the ground, refine it, and ship it to the energy user) with the amount of energy produced and transmitted to a user from some (typically underground) energy resource. To better understand this, assume an economy has a certain amount of finite oil reserves that are still underground, unextracted. To get to that energy, some of the extracted oil needs to be consumed in the extraction process to run the engines driving the pumps, therefore after extraction the net energy produced will be less than the amount of energy in the ground before extraction, because some had to be used up.\n\nThe extraction energy can be viewed in one of two ways: profitable extractable (NEG>0) or nonprofitable extractable (NEG<0). For instance, in the Athabasca Oil Sands, the highly diffuse nature of the tar sands and low price of crude oil rendered them uneconomical to mine until the late 1950s (NEG<0). Since then, the price of oil has risen and a new steam extraction technique has been developed, allowing the sands to become the largest oil provider in Alberta (NEG>0).\n\nThe situation is different with sustainable energy sources, such as hydroelectric, wind, solar, and geothermal energy sources, because there is no bulk reserve to account for (other than the Sun's lifetime), but the energy continuously trickles, so only the energy required for extraction is considered.\n\nIn all energy extraction cases, the life cycle of the energy-extraction device is crucial for the NEG-ratio. If an extraction device is defunct after 10 years, its NEG will be significantly lower than if it operates for 30 years. Therefore, the \"'energy payback time\" (sometimes referred to as energy amortization) can be used instead, which is the time, usually given in years, a plant must operate until the running NEG becomes positive (i.e. until the amount of energy needed for the plant infrastructure has been harvested from the plant).\n\nNet energy gain of biofuels has been a particular source of controversy for ethanol derived from corn (bioethanol). The actual net energy of biofuel production is highly dependent on both the bio source that is converted into energy, how it is grown and harvested (and in particular the use of petroleum-derived fertilizer), and how efficient the process of conversion to usable energy is. Details on this can be found in the Ethanol fuel energy balance article. Similar considerations also apply to biodiesel and other fuels.\n\nISO 13602-1 provides methods to analyse, characterize and compare technical energy systems (TES) with all their inputs, outputs and risk factors. It contains rules and guidelines for the methodology for such analyses.\n\nISO 13602-1 describes a means of to establish relations between inputs and outputs (net energy) and thus to facilitate certification, marking, and labelling, comparable characterizations, coefficient of performance, energy resource planning, environmental impact assessments, meaningful energy statistics and forecasting of the direct natural energy resource or energyware inputs, technical energy system investments and the performed and expected future energy service outputs.\n\nIn ISO 13602-1:2002, renewable resource is defined as \"natural resource for which the ratio of the creation of the natural resource to the output of that resource from nature to the technosphere is equal to or greater than one\".\n\nDuring the 1920s, of crude oil were extracted for every barrel of crude used in the extraction and refining process. Today only are harvested for every barrel used. When the net energy gain of an energy source reaches zero, then the source is no longer contributing energy to an economy.\n\n\n"}
{"id": "348869", "url": "https://en.wikipedia.org/wiki?curid=348869", "title": "North Atlantic oscillation", "text": "North Atlantic oscillation\n\nThe North Atlantic Oscillation (NAO) is a weather phenomenon in the North Atlantic Ocean of fluctuations in the difference of atmospheric pressure at sea level (SLP) between the Icelandic Low and the Azores High. Through fluctuations in the strength of the Icelandic low and the Azores high, it controls the strength and direction of westerly winds and location of storm tracks across the North Atlantic. It is part of the Arctic oscillation, and varies over time with no particular periodicity.\n\nThe NAO was discovered through several studies in the late 19th and early 20th centuries. Unlike the El Niño-Southern Oscillation phenomenon in the Pacific Ocean, the NAO is a largely atmospheric mode. It is one of the most important manifestations of climate fluctuations in the North Atlantic and surrounding humid climates.\n\nThe North Atlantic Oscillation is closely related to the Arctic oscillation (AO) (or Northern Annular Mode (NAM)), but should not be confused with the Atlantic Multidecadal Oscillation (AMO).\n\nThe NAO has multiple possible definitions. The easiest to understand are those based on measuring the seasonal average air pressure difference between stations, such as:\nThese definitions all have in common the same northern point (because this is the only station in the region with a long record) in Iceland; and various southern points. All are attempting to capture the same pattern of variation, by choosing stations in the \"eye\" of the two stable pressure areas, the Azores high and the Icelandic low (shown in the graphic).\n\nA more complex definition, only possible with more complete modern records generated by numerical weather prediction, is based on the principal empirical orthogonal function (EOF) of surface pressure. This definition has a high degree of correlation with the station-based definition. This then leads onto a debate as to whether the NAO is distinct from the AO/NAM, and if not, which of the two is to be considered the most physically based expression of atmospheric structure (as opposed to the one that most clearly falls out of mathematical expression).\n\nWesterly winds blowing across the Atlantic bring moist air into Europe. In years when westerlies are strong, summers are cool, winters are mild and rain is frequent. If westerlies are suppressed, the temperature is more extreme in summer and winter leading to heat waves, deep freezes and reduced rainfall.\n\nA permanent low-pressure system over Iceland (the Icelandic Low) and a permanent high-pressure system over the Azores (the Azores High) control the direction and strength of westerly winds into Europe. The relative strengths and positions of these systems vary from year to year and this variation is known as the NAO. A large difference in the pressure at the two stations (a high index year, denoted NAO+) leads to increased westerlies and, consequently, cool summers and mild and wet winters in Central Europe and its Atlantic facade. In contrast, if the index is low (NAO-), westerlies are suppressed, northern European areas suffer cold dry winters and storms track southwards toward the Mediterranean Sea. This brings increased storm activity and rainfall to southern Europe and North Africa.\n\nEspecially during the months of November to April, the NAO is responsible for much of the variability of weather in the North Atlantic region, affecting wind speed and wind direction changes, changes in temperature and moisture distribution and the intensity, number and track of storms. Research now suggests that the NAO may be more predictable than previously assumed and skillful winter forecasts may be possible for the NAO.\n\nAlthough having a less direct influence than for Western Europe, the NAO is also believed to affect the weather over much of eastern North America. During the winter, when the index is high (NAO+), the Icelandic low draws a stronger south-westerly circulation over the eastern half of the North American continent which prevents Arctic air from plunging southward. In combination with the El Niño, this effect can produce significantly warmer winters over the northeastern United States and southeastern Canada. Conversely, when the NAO index is low (NAO-), the eastern seaboard and southeastern United States can incur winter cold outbreaks more than the norm with associated heavy snowstorms in Appalachia/mid-Atlantic region and sub-freezing conditions into Florida. Another condition that occurs as result of NAO- is increased polar front activity plunging farther south than usual, beyond the Gulf of Mexico/Caribbean Sea basins resulting in a break in tropical weather for those regions, as instance of this is when Tehuantepecer winds blow outward into the Pacific Ocean away from the coastline of Southern Mexico. In summer, a strong NAO- is thought to contribute to a weakened jet stream that normally pulls zonal systems into the Atlantic Basin contributing significantly to excessively long lasting heat waves over Europe.\n\nUnder a positive NAO index (NAO+), regional reduction in atmospheric pressure results in a regional rise in sea level due to the 'inverse barometer effect'. This effect is important to both the interpretation of historic sea level records and predictions of future sea level trends, as mean pressure fluctuations of the order of millibars can lead to sea level fluctuations of the order of centimeters.\n\nBy controlling the position of the Azores high, the NAO also influences the direction of general storm paths for major North Atlantic tropical cyclones: a position of the Azores high farther to the south tends to force storms into the Gulf of Mexico, whereas a northern position allows them to track up the North American Atlantic Coast.\n\nAs paleotempestological research has shown, few major hurricanes struck the Gulf coast during 3000–1400 BC and again during the most recent millennium. These quiescent intervals were separated by a hyperactive period during 1400 BC and 1000 AD, when the Gulf coast was struck frequently by catastrophic hurricanes and their landfall probabilities increased by 3–5 times.\n\nUntil recently, the NAO had been in an overall more positive regime since the late 1970s, bringing colder conditions to the North-West Atlantic, which has been linked with the thriving populations of Labrador Sea snow crabs, which have a low temperature optimum.\n\nThe NAO+ warming of the North Sea reduces survival of cod larvae which are at the upper limits of their temperature tolerance, as does the cooling in the Labrador Sea, where the cod larvae are at their lower temperature limits. Though not the critical factor, the NAO+ peak in the early 1990s may have contributed to the collapse of the Newfoundland cod fishery.\n\nOn the East Coast of the United States an NAO+ causes warmer temperatures and increased rainfall, and thus warmer, less saline surface water. This prevents nutrient-rich upwelling which has reduced productivity. Georges Bank and the Gulf of Maine are affected by this reduced cod catch.\n\nThe strength of the NAO is also a determinant in the population fluctuations of the intensively studied Soay sheep.\n\nStrangely enough, Jonas and Joern (2007) found a strong signal between NAO and grasshopper species composition in the tall grass prairies of the midwestern United States. They found that, even though NAO does not significantly affect the weather in the midwest, there was a significant increase in abundance of common grasshopper species (i.e. \"Hypochlora alba, Hesperotettix\" spp., \"Phoetaliotes nebrascensis, M. scudderi, M. keeleri, and Pseudopomala brachyptera\") following winters during the positive phase of NAO and a significant increase in the abundance of less common species (i.e. \"Campylacantha olivacea, Melanoplus sanguinipes, Mermiria picta, Melanoplus packardii, and Boopedon gracile\") following winters during a negative phase of the NAO. This is thought to be the first study showing a link between NAO and terrestrial insects in North America.\n\nThe winter of 2009–10 in Europe was unusually cold. It is hypothesized that this may be due to a combination of low solar activity, a warm phase of the El Niño Southern Oscillation and a strong easterly phase of the Quasi-Biennial Oscillation all occurring simultaneously. The Met Office reported that the UK, for example, had experienced its coldest winter for 30 years. This coincided with an exceptionally negative phase of the NAO. Analysis published in mid-2010 confirmed that the concurrent 'El Niño' event and the rare occurrence of an extremely negative NAO were involved, this has become known as a \"Hybrid El Niño\".\n\nHowever, during the winter of 2010-11 in Northern and Western Europe, the Icelandic low, typically positioned west of Iceland and east of Greenland, appeared regularly to the east of Iceland and so allowed exceptionally cold air into Europe from the Arctic. A strong area of high pressure was initially situated over Greenland, reversing the normal wind pattern in the northwestern Atlantic, creating a blocking pattern driving warm air into northeastern Canada and cold air into Western Europe, as was the case during the previous winter. This occurred during a La Niña season, and is connected to the rare Arctic dipole anomaly.\n\nIn the north western part of the Atlantic, both of these winters were mild, especially 2009-2010, which was the warmest recorded in Canada. The winter of 2010-2011 was particularly above normal in the northern Arctic regions of that country.\n\nThe probability of cold winters with much snow in Central Europe rises when the Arctic is covered by less sea ice in summer. Scientists of the Research Unit Potsdam of the Alfred Wegener Institute for Polar and Marine Research in the Helmholtz Association have decrypted a mechanism in which a shrinking summertime sea ice cover changes the air pressure zones in the Arctic atmosphere and effects on European winter weather.\n\nIf there is a particularly large-scale melt of Arctic sea ice in summer, as observed in recent years, two important effects are intensified. Firstly, the retreat of the light ice surface reveals the darker ocean, causing it to warm up more in summer from the solar radiation (ice-albedo feedback mechanism). Secondly, the diminished ice cover can no longer prevent the heat stored in the ocean being released into the atmosphere (lid effect). As a result of the decreased sea ice cover the air is warmed more greatly than it used to be particularly in autumn and winter because during this period the ocean is warmer than the atmosphere.\n\nThe warming of the air near to the ground leads to rising movements and the atmosphere becomes less stable. One of these patterns is the air pressure difference between the Arctic and mid-latitudes: the so-called Arctic oscillation with the Azores highs and Iceland lows known from the weather reports. If this difference is high, a strong westerly wind will result which in winter carries warm and humid Atlantic air masses right down to Europe. In the negative phase when pressure differences are low, cold Arctic air can then easily penetrate southward through Europe without being interrupted by the usual westerlies, as has been the case frequently over the last three winters. Model calculations show that the air pressure difference with decreased sea ice cover in the Arctic summer is weakened in the following winter, enabling Arctic cold to push down to mid-latitudes.\n\nDespite one of the strongest El Nino ever recorded in the Pacific Ocean, a largely positive North Atlantic Oscillation prevailed over Europe during the Winter of 2015-2016. For example, Cumbria in England registered one of the wettest months on record. Meanwhile, the Maltese Islands in the Mediterranean Sea registered one of the driest years ever recorded up to beginning of March as the Island's national average was only 235mm to date with some areas registering even less than 200mm.\n\n"}
{"id": "7977991", "url": "https://en.wikipedia.org/wiki?curid=7977991", "title": "Oily water separator (marine)", "text": "Oily water separator (marine)\n\nAn oily water separator (OWS) (marine) is a piece of equipment specific to the shipping or marine industry. It is used to separate oil and water mixtures into their separate components. This page deals exclusively oily water separators aboard marine vessels. They are found on board ships where they are used to separate oil from oily waste water such as bilge water before the waste water is discharged into the environment. These discharges of waste water must comply with the requirements laid out in Marpol 73/78. For information on more general oil water separators Oily Water Separators (general).\n\nBilge water is a near-unavoidable product of shipboard operations. Oil leaks from running machinery such as diesel generators, air compressors, and the main propulsion engine. Modern OWSs have alarms and automatic closure devices which are activated when the oil storage content of the waste water exceeds a certain limit.\n\nThe primary purpose of a shipboard oily water separator (OWS) is to separate oil and other contaminants that could be harmful for the oceans. The International Maritime Organization (IMO) publishes regulations through the Marine Environment Protection Committee (MEPC). On July 18, 2003, the MEPC issued new regulations that each vessel built after this date had to follow. This document is known as MEPC 107(49) and it details revised guidelines and specifications for pollution prevention equipment for machinery space bilges of ships. Each OWS must be able to achieve clean bilge water under 15 ppm of type C oil or heavily emulsified oil, and any other contaminates that may be found. All oil content monitors (OCM) must be tamper-proof. Also whenever the OWS is being cleaned out the OCM must be active. An OWS must be able to clear out contaminants as well as oil. Some of these contaminating agents include lubricating oil, cleaning product, soot from combustion, fuel oil, rust, sewage, and several other things that can be harmful to the ocean environment.\n\nThe bilge area is the lowest area on a ship. The bilge water that collects here include drain water or leftover water from the boilers, water collecting tanks, drinking water and other places where water can not overflow. However, bilge water doesn't just include water drainage. Another system that drains into the Bilge system comes from the propulsion area of the ship. Here fuels, lubricants, hydraulic fluid, antifreeze, solvents, and cleaning chemicals drain into the engine room bilges in small quantities. The OWS is intended to remove a large proportion of these contaminants before discharge to the environment (overboard to the sea).\n\nAll OWS equipment, new or old, can, in a laboratory setting, separate oil and water, do so automatically, and produce clean water for discharge overboard that contains no more than 15 parts per million oil. OWS equipment is approved by testing it with specific cocktails of mixed oil and water. Initially these combinations were very simple, basically no more than a mixture of clean water and diesel fuel, but they have become more sophisticated under MARPOL MEPC 107(49). The vast majority of these many equipment models, manufacturers, and types start with some sort of gravity separation of bilge water. Simply letting oil and water sit is called decanting, and this does not always meet the 15 ppm criterion, which is why each manufacturer has added additional features to his equipment to ensure that this criterion can be met. The separation that takes place inside the OWS allows oil that floats to the top to be automatically skimmed off to a sludge tank or dirty oil holding tank. There is no official standard for tank naming convention but there are some proposals for that.\n\nAn OWS needs to be fitted with an oil content meter (OCM) that samples the OWS overboard discharge water for oil content. If the oil content is less than 15 ppm, the OCM allows the water to be discharged overboard. If the oil content is higher than 15 ppm, the OCM will activate an alarm and move a three-way valve that, within a short period of time, will recirculate the overboard discharge water to a tank on the OWS suction side.\n\nAn OCM takes a trickle sample from the OWS overboard discharge line and shines a light through the sample to an optical sensor. Since small oil droplets will diffract and diffuse light, a change in signal at the sensor will indicate the presence of oil. At a certain signal setting that is roughly equivalent to 15 ppm, the sensor will conclude that there is too much oil going through the discharge line. This calibration generally takes place in a lab, but can be tested by use of a three-sample liquid aboard the vessel. If the OCM ends up sampling a certain amount of heavy oil, the OCM will be fouled and it will need to be flushed or cleaned.\n\nThe cleaning can be done by running fresh water through the OCM via a permanent connection or can be performed by opening the OCM sample area and scrubbing the sample area with a bottle brush.\n\nThe water removed by the OWS flows to oil collecting spaces. There can be two stages. The first-stage filter removes physical impurities present and promotes some fine separation. The second-stage filter uses coalescer inserts to achieve the final de-oiling. Coalescence is the breakdown of surface tension between oil droplets in an oil/water mixture which causes them to join and increase in size. The oil from the collecting spaces is drained away automatically or manually. In most modern ships, the oil from collecting spaces is drained away automatically.\n\nAll Cargo vessels where MARPOL Convention is applicable must have an oil record book where the chief engineer will record all oil or sludge transfers and discharges within the vessel. This is necessary in order for authorities to be able to monitor if a vessel's crew has performed any illegal oil discharges at sea.\n\nWhen making entries in the oil record book Part I, the date, operational code, and item number are inserted in the appropriate columns and the required particulars shall be recorded in chronological order as they have been executed on board. Each operation is to be fully recorded without delay so that all the entries in the book appropriate to that operation are completed.\n\nIn 1948 in the USA, a Water Pollution Control Act (WPA) was passed by the federal government. This act gave rights to the surgeon general of the public health service to make programs to decrease the amount of pollution in the world's waters. The main concern was to save water, protect fish, and have clean water for agricultural usage. The WPA also helped to start the process of building water treatment plants. This is to guard against sewage from polluting drinking water. In 1972 the WPA was amended to include more requirements in order to insure that the water is chemically sound. This amendment also furthered regulations to insure the quality of the water was up to par. In 1987 the WPA was amended again to put an even more strict control on water supply pollution. With this new amendment water sources had to fit a specific set of criteria to fight against pollution.\n\nMarpol 73/78 is the International Convention for the Prevention of Pollution from Ships, 1973 as modified by the Protocol of 1978. (\"Marpol\" is short for marine pollution and 73/78 short for the years 1973 and 1978.)\n\nMarpol 73/78 is one of the most important international marine environmental conventions. It was designed to minimize pollution of the seas, including dumping, oil and exhaust pollution. Its stated object is to preserve the marine environment through the complete elimination of pollution by oil and other harmful substances and the minimization of accidental discharge of such substances.\n\nThe regulations in the Clean Water Act limit what may be discharged to sea from an OWS in USA waters. Current limits are oil for discharges within 12 nautical miles of shore or 100 mg/l outside that limit.\n\nEuropean countries and Canada have stricter rules on discharge and discharges must contain less than 5 mg/l of contaminants.\n\nThe discharge of oil contaminated waters are also subject to international controls such as the International Convention for the Prevention of Pollution from Ships (MARPOL), and International Maritime Organization (IMO). These organizations set strict limits to protect marine life and coastal environments. These agency require logs to be kept of any discharges of contaminated water.\n\nA gravity plate separator contains a series of plates through which the contaminated water flows. The oil in the water coalesces on the underside of the plate eventually forming droplets before coalescing into liquid oil which floats off the plates and accumulates at the top of the chamber. The oil accumulating at the top is then transferred to waste oil tank on the vessel where it is later discharged to a treatment facility shore side. This type of Oily Water Separator is very common in ships but it has some flaws that decrease efficiency. Oil particles that are twenty micrometers or smaller do not get separated. The variety of oily wastes in bilge water can limit removal efficiency especially when very dense and highly viscous oils such as bunker oil are present. Plates must be replaced when fouled, which increases the costs of operation.\n\nWastewater purification of oils and contaminates by electrochemical emulsification is actively in research and development. Electrochemical emulsification involves the generation of electrolytic bubbles that attract pollutants such as sludge and carry them to the top of the treatment chamber. Once at the top of the treatment chamber the oil and other pollutants are transferred to a waste oil tank.\n\nBioremediation is the use of microorganisms to treat contaminated water. A carefully managed environment is needed for the microorganisms which includes nutrients and hydrocarbons such as oil or other contaminates, and oxygen.\n\nIn pilot scale studies, bio-remediation was used as one stage in a multi-stage purification process involving a plate separator to remove the majority of the contaminants and was able to treat pollutants at very low concentrations including organic contaminates such as glycerol, solvents, jet fuel, detergents, and phosphates. After treatment of contaminated water, carbon dioxide, water and an organic sludge were the only residual products.\n\nA centrifugal water-oil separator, \"centrifugal oil-water separator\" or \"centrifugal liquid-liquid separator\" is a device designed to separate oil and water by centrifugation. It generally contains a cylindrical container that rotates inside a larger stationary container. The denser liquid, usually water, accumulates at the periphery of the rotating container and is collected from the side of the device, whereas the less dense liquid, usually oil, accumulates at the rotation axis and is collected from the center. Centrifugal oil-water separators are used for waste water processing and for cleanup of oil spills on sea or on lake. Centrifugal oil-water separators are also used for filtering diesel and lubricating oils by removing the waste particles and impurity from them.\n\nOn a properly operated vessel only small amounts of bilge would be present as long as there are no equipment failures. But even the best-operated vessels suffer equipment failures, which then quickly results in contaminated bilges. Sometimes these contaminates are massive and pose a serious challenge to the crew to deal with in a legal fashion.\n\nA properly designed OWS system will make it clear and easy for regulatory enforcement agencies to determine if OWS system regulations are being violated. At present, there is no clear and efficient method of determining whether regulations are violated or not. At the most basic level, the absolute absence of any type of standardization of OWS systems makes the initial investigation confusing, dirty, time consuming and sometimes plain incorrect. In the marine industry there is a long-standing and important tradition of \"jointness\" in marine forensic investigations, where all parties at interest examine the same things at the same time. However, due to the criminal character of OWS violations the jointness concept is abandoned, which leads to very poor technical investigative methods and severe unnecessary disruptions to vessel operations.\n\nVarious efforts are being made to improve the overall OWS system approach. In 2015, at the MAX1 Studies Conference held in Wilmington, North Carolina, maritime leaders from many different sectors gathered to discuss problems potential solutions regarding waste stream management.\n\n"}
{"id": "53897874", "url": "https://en.wikipedia.org/wiki?curid=53897874", "title": "PTAA GMB Model", "text": "PTAA GMB Model\n\nPTAAGMB (Precipitation-Temperature-Area-Altitude Glacier Mass Balance)\n\nThe PTAAGMB Model is used for calculating a glacier's mass balance, the primary indicator of its health, and plots the changes to its mass balance over time to predict its future.\n\nDeveloped in the mid-1990's by glaciologist Wendell Tangborn, the PTAAGMB model provides an easy and reliable alternative to the challenging task of manually measuring glaciers using snow pits and ablation stakes.\n\nThe PTAAGMB model only requires data from the precipitation and temperature (PT) observations from nearby low-altitude weather stations and the glacier's area-altitude (AA) distribution.\n\nGlaciers are ultra-sensitive to minute changes in the climate and respond by changing their size and by advancing or retreating. The mass balance, or the difference between snow accumulation and snow and ice ablation, is crucial to glacier health and its survival. The Columbia Glacier in Alaska is a large tidewater glacier that began a drastic retreat in the 1970s due to climate fluctuations and began discharging large quantities of icebergs into Prince William Sound. These icebergs were responsible for a massive oil spill in 1989 when an oil tanker captain tried to avoid them and went aground.\n\nThe key to the PTAAGMB model is the glacier’s area-altitude distribution, which is simply the glacier’s surface area as a function of elevation. The AA profile is a unique feature of a glacier that has been shaped by thousands of years of erosion of the bedrock underlying the glacier. Thus, the area altitude distribution has embedded within it the past climate history that has formed the glacier.\n\nThe PTAAGMB model uses daily values of such balance variables as snowline altitude, zero balance altitude, glacier balance, balance flux and the accumulation area ratio are correlated throughout the ablation season using two-degree polynomial regressions to obtain the lowest fitting error. When the minimum average error (or maximum R2) is attained, the generated balances and other variables are considered to be real. A simplex optimization technique is used to determine the optimal coefficient values that are used in algorithms to convert meteorological observations to snow accumulation and snow and ice ablation.\n\nThe PTAAGMB model has been used successfully on a number of glaciers in various parts of the world: in the United States, the Alaskan glaciers Bering, Gulkana, Lemon Creek, Mendenhall, Wolverine and Wrangell Range; in Washington State, on the South Cascade Glacier; in Europe, the Austrian glaciers Hintereisferner, Kesselwanferner and Vernagt Ferner.\n\nThe mass balance and runoff of Langtang Glacier in Nepal was determined with the PTAAGMB model using daily meteorological observations observed at Kathmandu. This is the only Himalayan glacier for which mass balance and runoff have been calculated.\n\nAnother feature of the PTAAGMB model is the capability to estimate glacier thickness from ice flow velocity and mass balance measurements. The average thickness of South Cascade Glacier was found to be 83 meters in 1965, based on flow velocity and balance measurements. Borehole depth measurements of the glacier made later approximately agree with this estimate.\n\nThe mass balance, runoff and surges of Bering Glacier were calculated with the PTAAGMB model using weather observations at Cordova and Yakutat, Alaska. Ice volume loss measured with the PTAAGMB model agrees within 0.8% of the loss measured with the geodetic method. Runoff from Bering Glacier (derived from simulated ablation and rain) correlates with four of the glacier surges that have occurred since 1951.\n\nComparison of glacier mass balance by glaciological, hydrological and mapping methods revealed that glaciers internally store a significant amount of liquid water. Stored water in glaciers is now considered the key to understanding the disintegration of Antarctic and Greenland Ice Caps.\n\nA website with PTAAGMB results reported from 9 different glaciers, 5 of which are compared with available manual measurements, can be seen at www.ptaagmb.com.\n\n\n"}
{"id": "318083", "url": "https://en.wikipedia.org/wiki?curid=318083", "title": "Pacific Rim", "text": "Pacific Rim\n\nThe Pacific Rim comprises the lands around the rim of the Pacific Ocean. The \"Pacific Basin\" includes the Pacific Rim and the islands in the Pacific Ocean. The Pacific Rim roughly overlaps with the geologic Pacific Ring of Fire.\n\nThis is a list of countries that are generally considered to be a part of the Pacific Rim, since they lie along the Pacific Ocean.\n\n\nThe Pacific is a hotbed of overseas shipping. The top 10 busiest container ports, with the exception of Dubai's Port of Jebel Ali (9th), are in the Rim nations. They are home to 29 of the world's 50 busiest container shipping ports:\n\nVarious intergovernmental and non-governmental organizations focus on the Pacific Rim, including APEC, the East-West Center, Sustainable Pacific Rim Cities and the Institute of Asian Research. In addition, the RIMPAC naval exercises are coordinated by United States Pacific Command.\n\n\n"}
{"id": "23222", "url": "https://en.wikipedia.org/wiki?curid=23222", "title": "Pennsylvanian (geology)", "text": "Pennsylvanian (geology)\n\nThe Pennsylvanian (also known as Upper Carboniferous or Late Carboniferous) is, in the ICS geologic timescale, the younger of two subperiods (or upper of two subsystems) of the Carboniferous Period. It lasted from roughly Ma (million years ago). As with most other geochronologic units, the rock beds that define the Pennsylvanian are well identified, but the exact date of the start and end are uncertain by a few hundred thousand years. The Pennsylvanian is named after the U.S. state of Pennsylvania, where the coal-productive beds of this age are widespread.\n\nThe division between Pennsylvanian and Mississippian comes from North American stratigraphy. In North America, where the early Carboniferous beds are primarily marine limestones, the Pennsylvanian was in the past treated as a full-fledged geologic period between the Mississippian and the Permian. In Europe, the Mississippian and Pennsylvanian are one more-or-less continuous sequence of lowland continental deposits and are grouped together as the Carboniferous Period. The current internationally used geologic timescale of the ICS gives the Mississippian and Pennsylvanian the rank of subperiods, subdivisions of the Carboniferous Period.\n\nAll modern classes of fungi have been found in rocks of Pennsylvanian age.\n\nAmphibians were diverse and common; some were several meters long as adults. The collapse of the rainforest ecology in the mid-Pennsylvanian (between the Moscovian and the Kasimovian) removed many amphibian species that did not survive as well in the cooler, drier conditions. Reptiles, however, prospered due to specific key adaptations. One of the greatest evolutionary innovations of the Carboniferous was the amniote egg, which allowed for the further exploitation of the land by certain tetrapods. These included the earliest sauropsid reptiles (\"Hylonomus\"), and the earliest known synapsid (\"Archaeothyris\"). Small lizard-like animals quickly gave rise to many descendants. Reptiles underwent a major evolutionary radiation, in response to the drier climate that followed the rainforest collapse.\n\nThe major forms of life at this time were the arthropods. Due to the high levels of oxygen, arthropods were far larger than modern ones. \"Arthropleura\", a giant millipede relative, was a common sight and the giant dragonfly \"Meganeura\" \"flew the skies\".\n\nThe Pennsylvanian has been variously subdivided. The international timescale of the ICS follows the Russian subdivision into four stages:\n\n\nNorth American subdivision is into five stages, but not precisely the same, with additional (older) Appalachian series names following:\n\nThe Virgilian or Conemaugh corresponds to the Gzhelian plus the uppermost Kasimovian.\nThe Missourian or Monongahela corresponds to the rest of the Kasimovian.\nThe Desmoinesian or Allegheny corresponds to the upper half of the Moscovian.\nThe Atokan or upper Pottsville corresponds to the lower half of the Moscovian.\nThe Morrowan corresponds to the Bashkirian.\n\nIn the European subdivision, the Carboniferous is divided into two epochs: Dinantian (early) and Silesian (late). The Silesian starts earlier than the Pennsylvanian and is divided in three ages: \n\n"}
{"id": "58472865", "url": "https://en.wikipedia.org/wiki?curid=58472865", "title": "Phosphaethynolate", "text": "Phosphaethynolate\n\nThe phosphaethynolate anion is the phosphorus-containing analogue of the cyanate anion with the chemical formula [PCO] or [OCP]. The anion has a linear geometry and is commonly isolated as a salt. When used as a ligand, the phosphaethynolate anion is ambidentate in nature meaning it forms complexes by coordinating via either the phosphorus or oxygen atoms. This versatile character of the anion has allowed it to be incorporated into many transition metal and actinide complexes but now the focus of the research around phosphaethynolate has turned to utilising the anion as a synthetic building block to organophosphanes.\n\nThe first reported synthesis and characterisation of phosphaethynolate came from Becker et al. in 1992. They were able to isolate the anion as a lithium salt (in 87% yield) by reacting lithium bis(trimethylsilyl)phosphide with dimethyl carbonate (see Scheme 1). The x-ray crystallographic analysis of the anion determined the P-C bond length to be 1.555 Å (indicative of a phosphorus-carbon triple bond) and the C-O bond length to be 1.198 Å. Similar studies were performed on derivatives of this structure and the results indicated that dimerisation to form a four membered Li ring is favoured by this molecule.\n\nTen years later, in 2002, Westerhausen et al. published the use of Becker’s method to make a family of alkaline earth metal salts of PCO (see Scheme 2); this work involved the synthesis of the magnesium, calcium, strontium and barium bis-phosphaethynolates. Like the salts previously reported by Becker, the alkali-earth metal analogues were unstable to moisture and air and thus were required to be stored at low temperatures (around – 20 °C) in dimethoxyethane solutions.\n\nIt wasn’t until 2011 that the first stable salt of the phosphaethynolate anion was reported by Grutzmacher and co-workers (see Scheme 3). They managed to isolate the compound as a brown solid in 28% yield. The structure of the stable sodium salt, formed by carbonylation of sodium phosphide, contains bridging PCO units in contrast to the terminal anions found in the previously reported structures. The authors noted that this sodium salt could be handled in air as well as water without major decomposition; this emphasises the significance of the accompanying counter cation in stabilisation of PCO. \n\nDirect carbonylation was a method also employed by Goicoechea in 2013 in order to synthesis a phosphaethynolate anion stabilised by a potassium cation sequestered in 18-crown-6 (see Scheme 4). This method required the carbonylation of solutions of KP at 150 °C and produced by-products that were readily separated during aqueous work ups. The use of aqueous work ups reflects the high stability of the salt in water. This method afforded the PCO anion in reasonable yields around 43%. Characterisation of the compound involved infra-red spectroscopy; the band indicative of the PC triple bond stretch was observed at 1730 cm. \n\nThe phosphaethynolate anion is the heavier isoelectronic congener of the cyanate anion. It has been shown that it behaves in a similar way to its lighter analogue, as an ambidentate nucleophile. This ambidentate character of the anion means that it is able to bind via both the phosphorus and oxygen atoms depending on the nature of the centre being coordinated. \n\nComputational studies carried out on the anion such as Natural Bond Orbital (NBO) and Natural Resonance Theory (NRT) analyses can go part way to explain why PCO can react in such a manner \"(see Figure 1)\". The two dominant resonance forms of the phosphaethynolate anion localise negative charge on either the phosphorus or oxygen atoms meaning both are sites of nucleophilicity. The same applies for the cyanate anion hence why PCO is observed to have similar pseudo-halogenic behaviour. \n\nCoordination via the oxygen atom is favoured by hard, highly electropositive centres. This is due to the fact that oxygen is a more electronegative atom and thus prefers to bind via more ionic interactions. Examples of this type of coordination were presented in the work of Goicoechea et al. from 2015. The group found that actinide complexes of PCO involving Uranium and Thorium both coordinated through the oxygen. This is the result of the contracted nature of the actinide orbitals which makes the metal centres more ‘core-like’ thus favouring ionic interactions. \n\nOn the other hand, softer, more polarisable centres prefer to coordinate in a more covalent manner through the phosphorus atom. Examples of this include complexes accomodating a neutral or sparsely charged transition metal centre. The first example of this nature of PCO binding was published by Grutzmacher and co-workers in 2012. The group’s studies used a Re(I) complex and the analysis of its bonding parameters and electronic structure showed that the phosphaethynolate anion coordinated in a bent fashion. This suggested the Re(I) – P bond possessed a highly covalent character thus the complex would be best described as a metallaphosphaketene. It wasn’t until four years later that a second example of this coordination nature of PCO was identified. This time it came in the form of a W(0) pentacarbonyl complex produced by the Goicoechea group. \n\nThere is one particular reaction studied by Grutzmacher et al. that exhibits the rearrangement of coordination character of PCO. Initially when reacting the anion with triorganyl silicon compounds, it binds via the oxygen forming the kinetic oxyphosphaalkyne product. The thermodynamic silyl phosphaketene product is generated when the kinetic product rearranges to allow PCO to coordinate through phosphorus. \n\nThe formation of the kinetic product is charged controlled and thus explains why it is formed by oxygen coordination. The oxygen atom favours a larger degree of ionic interactions as a result of its greater electronegativity. Contrastingly, the thermodynamic product of the reaction is generated under orbital control. This comes in the form of phosphorus coordination as the largest contribution in the HOMO of the anion resides on the phosphorus atom; this is clearly visible in \"Figure 3\". \n\nExtensive studies involving the phosphaethynolate anion have shown that it can react in a variety of ways. It has documented use in cycloadditions, as a phosphorus transfer agent, a synthetic building block and as pseudo halide ligands \"(as described above)\".\n\nIn these types of reactions, CO is released as the phosphaethynolate anion acts as either a mild nucleophilic source of phosphorus or a Brønsted base. Examples of these types of reactions involving PCO include work conducted by Grutzmacher and Goicoechea. .\n\nIn 2014, Grutzmacher et al. reported that an imidazolium salt would react with the phosphaethynolate anion to produce a phosphinidine carbene adduct. Computational mechanistic studies were conducted on this reaction using density functional theory at the B3LYP/6-31+G* level. The results of these investigations eluded to the fact that the lowest energy and therefore most likely pathway involves PCO acting as a Brønstead base initially deprotonating the acidic imidazolium cation to generate the intermediate phosphaketene, HPCO. The highly unstable protonated PCO remains hydrogen bonded to the newly produced N-heterocylic carbene prior to rearrangement and formation of the observed product. In this case, PCO doesn’t act as a mild nucleophile due to the augmented stability of the starting imidazolium cation. \n\nOn the other hand, in the work published by Goicoechea and co-workers in 2015, the phosphaethynolate anion can be seen to act as a source of nucleophilic phosphide \"(P)\". The anion was seen to add across the Si-Si double bond of cyclotrisilene thus introducing a phosphorus vertex into its scaffold (after undergoing decarbonylation). \n\nAfter synthesising the potassium salt of the phosphaethynolate anion in 2013 \"(see above)\", Goicoechea et al. began to look into the potential of PCO towards cycloadditions. They found that the anion could react in a [2+2] fashion with a diphenyl ketene to produce the first isolatable example of a four-membered monoanionic phosphorus containing heterocycle. They employed the same method to test other unsaturated substrates such as carbodiimides and found that the likelihood of cyclisation heavily relies on the nature of the substituents on the unsaturated substrate. \n\nCycloaddition reactions involving the phosphaethynolate anion have also been shown by Grutzmacher and co-workers to be a viable synthetic route to other heterocycles. One simple example is the reaction between the NaPCO and an α-pyrone. This reaction yields the sodium phosphinin-2-olate salt which is stable to both air and moisture. \n\nA large part of the research involving PCO is now looking into utilising the anion as a synthetic building block to derive phosphorus containing analogues of small molecules.\nThe first major breakthrough in this area came from Goicoechea et al. in 2013; they published the reaction between the PCO anion and ammonium salts which yielded the phosphorus containing analogue of urea in which phosphorus replaces a nitrogen atom. The group predict that this heavier congener could have applications in new materials, anion sensing and coordination chemistry. \n\nGoicoechea and co-workers were also able to isolate the heavily sought after phosphorus containing analogue of isocyanic acid, HPCO, in 2017. This molecule is thought to be a crucial intermediate in a lot of reactions involving PCO \"(including P-transfer to an imidazolium cation; see above)\". \n\nMoreover, the most recent addition to this class of small molecules is the phosphorus containing analogue of N,N-dimethylformamide. This work in which the phosphorus again replaces a nitrogen atom was published in 2018 by Stephan and co-workers. Generating acylphosphines in this manner is considered a much milder route than other current strategies that require multi-step syntheses involving toxic, volatile and pyrophoric reagents. \n\nThe other analogues of the phosphaethynolate anion all obey the general formulae E-C-X and are made by varying E and X. When changing either atom, unique trends amongst the different analogues become apparent.\n\nAs ‘E’ is varied by descending group 15, there is a clear shift in the weights of the resonance structures towards the phosphaketene analogue \"(see Figure 5)\". This reflects the decrease in effective orbital overlap between E and C which in turn disfavours multiple bond formation. This increasing tendency to form double and not triple E-C bonds is also reflected in calculated E-C bond lengths \"(see Table 1)\". The data from \"Table 1\" is evidence of E-C bond elongation which correlates with the change from triple to double bond. \n\nIn addition, NBO analysis highlights that the greatest electron delocalisation within the anions stems from the donation of an oxygen lone pair into the E-C π antibonding orbital. The energy value associated with this donation is seen to increase down the group \"(see Table 1)\". This explains the increasing resonance weight towards the ketene isomers as populating antibonding orbitals usually suggests the breaking of a bond. \n\nThe shift towards the ketene isomer will also cause an increase in charge density on the elemental ‘E’ atom; this makes the elemental atom an increasing source of E \"(see Figures 5 and 6)\". \n\nThe simplest analogue that can be formed as ‘X’ is varied is PCS. This anion was first isolated by Becker et al. by reacting the phosphaethynolate anion with carbon disulphide. Unlike PCO, PCS shows ambidentate nucleophilic tendencies towards the W(0) complex mentioned above. \n\nThis is the result of a reduced difference in electronegativity between E and X thus neither atom offers a substantial advantage over the other in terms of providing ionic contributions to bonding. As a result, the average electron density in PCS is spread over the entire anion (see Figure 8) whereas in PCO, most electron density is localised on th phosphorus atom (see Figure 7) as this is the atom which bonds to form the thermodynamically favourable product. \n"}
{"id": "2387970", "url": "https://en.wikipedia.org/wiki?curid=2387970", "title": "Polar ecology", "text": "Polar ecology\n\nPolar ecology is the relationship between plants and animals in a polar environment. Polar environments are in the Arctic and Antarctic regions. Arctic regions are in the Northern Hemisphere, and it contains land and the islands that surrounds it. Antarctica is in the Southern Hemisphere and it also contains the land mass, surrounding islands and the ocean. Polar regions also contain the subantarctic and subarctic zone which separate the polar regions from the temperate regions. Antarctica and the Arctic lie in the polar circles. The polar circles are not visible on the earth but it is shown on maps to be the areas that receives less sunlight due to less radiation. These areas either receive sunlight (midnight sun) or shade (polar night) 24 hours a day because of the earth's tilt. Plants and animals in the polar regions are able to withstand living in harsh weather conditions but are facing environmental threats that limit their survival.\n\nPolar climates are cold, windy and dry. Because of the lack of precipitation and low temperatures the Arctic and Antarctic are considered the world's largest deserts or Polar deserts. Much of the radiation from the sun that is received is reflected off the snow making the polar regions cold. When the radiation is reflected, the heat is also reflected. The polar regions reflect 89-90% of the sun radiation that the earth receives. And because Antarctica is closer to the sun at perihelion, it receives 7% more radiation than the Arctic. Also in the polar region, the atmosphere is thin. Because of this the UV radiation that gets to the atmosphere can cause fast sun tanning and snow blindness.\n\nPolar regions are dry areas; there is very little precipitation due to the cold air. There are some times when the humidity may be high but the water vapor present in the air may be low. Wind is also strong in the polar region. Wind carries snow creating blizzard like conditions. Winds may also move small organisms or vegetation if it is present. The wind blows the snow making snowdrifts or snow dunes which may exist even in the spring when the snow is thawing out. It is hard for meteorologists to measure the amount of precipitation. This is because it is expensive to take care of the stations that collect weather data and it hard for them to measure snow fall amounts because the wind blows the snow too much to calculate exact amounts.\n\nThe temperatures are similar between the Arctic and Antarctic. The temperatures in the Arctic are different depending on the location. Temperatures in the Arctic have a higher range than in the Antarctic. Temperatures can range as much as . Along the coast in the Arctic temperatures average in December, January and February. The ice melts along the coast during the summer months which are around June, July and August and the temperature may rise a few degrees above freezing causing there to be some vegetation. During these same months in the northern regions there will be 24 hours of daylight. Arctic regions also receive a lot of snowfall. The Arctic Basin has snow 320 days out of the year while the Arctic Seas have snow cover 260 days a year. The thickness of the snow averages . In Greenland, temperatures have an average temperature of in the winter and in the summer the temperatures reach . Iceland on the other hand is in a subarctic region meaning it is near the temperate zone. Because of this the temperatures are above the freezing point throughout much of the year. In Russia temperatures are extremely cold. In Verkhoyansk, Siberia it has reached the coldest temp of in the Northern Hemisphere. The temperatures in the summer in Siberia can get to .\n\nIn the Antarctic there is less temperature variations. Temperatures only range by around . The winter months are May till September while the summer months will be October till April. The sun reappears in September which then starts the 24 hours of daylight. The temperatures are different between the plateaus in Antarctica and between the coasts. The plateaus are the coldest regions of Antarctica. In the summer months there is low precipitation with light winds. Vostok has received the lowest temperature worldwide getting as low as in 1960. The West Antarctica plateau reaches snow levels of around . This area is also warmer but it receives the heaviest snow and receives more wind. Because of the cold desert like conditions on the plateaus there are very little plants and animals. Some species of birds though have been seen.\n\nOn the coasts in the summer there is more wind, and it is cloudier. Coasts with higher latitudes have a temperature of in the winter months whereas lower latitude coasts get down to . Coastal areas may receive or more of snow.\n\nWater is an important part of human survival. Because of its cold temperature much of the earth's water comes from the polar regions. 90% of the world's water comes from the Antarctic ice cap although a lot of this water is not used. Water environments are important for many species around the world. Many bacteria thrive there as well as algae and flora.\n\nMany of the ponds or lakes in polar regions are frozen over or snow covered for most of the year. Larger lakes thaw out around the edges during the warmer months while the smaller lakes thaw entirely. There are few rivers in the polar regions. The Arctic has more rivers compared to Antarctica. The regions also have ponds. The ponds that attract birds tend to be rich in nutrients. This is because of the bird droppings or bird feathers. There are two different types of lakes in polar regions including Arctic lakes and Antarctic lakes. Of the Arctic lakes they include glacial lakes and permafrost lakes.\n\nThe polar regions include the Arctic Ocean and the Southern Ocean. The Arctic Ocean covers . In the spring the ice covers an area of and in the winter it is twice that. In this area it is never totally ice-covered. This is due to the winds breaking up the ice. Because of these cracks in the ice there is more biological productivity in the ocean.\n\nThe Southern Ocean is . This ocean contains the Weddell Sea and Ross Sea. The ocean contains large packs of ice that surrounds Antarctica.\n\nBecause of the cold weather it is hard for plants to grow. Frozen ground covers most of the polar regions for the majority of the year. Permafrost reaches a thickness of deep. Large amounts of permafrost can lead to poor water drainage. Due to the permafrost the water in the soil remains frozen for most of the year. In the summer the top of the permafrost may be covered with water due to melting in the area. Weathering is also common in polar regions. There is rubble from rocks that are scattered on the land due to movement of glaciers. Also quick temperature change causes weathering.\n\nThe main type of soil in the polar regions is ahumic soil. This includes the cold desert soil. This soil consists of sand that is frozen. These soils tend to not have a lot of vegetation but bacteria has been found.\n\nThe other type of soil is organic soil. This type of soil is found in areas that are warmer and have more moisture. Some vegetation that lives here are algae, fungi and mosses. One type of organic soil is the brown soils, which have drainage.\n\nDue to the harsh weather in the polar regions, there are not many animals. The animals that do exist in the polar region are similar between the Antarctic and Arctic regions. The animals do differ by the temperature. In the Arctic some invertebrates include spiders, mites, mosquitoes and flies. In warmer areas of the polar regions moths, butterflies and beetles can be found. Some of the larger animals that exist are foxes, wolves, rabbits, hares, polar bears, reindeer/caribou. There are various bird species that have been spotted in the Arctic. Eight species of birds reside on the polar tundra year round while 150 breed in the Arctic. The birds that do breed go to the Arctic between May and July. One of the known birds is the snowy owl, which has enough fat on it to be able to survive in the cold temperatures.\n\nIn the Antarctic some invertebrates that exist are mites, fleas and ticks. Antarctica is the only continent that does not have a land mammal population. There are also no birds that reside in Antarctica. Though, it has been known that various birds from South America have been spotted in Antarctica.\n\nFor animals to be able to live in the polar region they have to have adaptations which allow them to live in the cold and windy environments. These animals have originated with these adaptations, and animals that live in these regions are accumulating adaptations to be able to live in this type of environment. Some of these adaptations may be to be big and insolated, have a lot of fur, and to be darker. Also, many animals live in groups to be able to protect themselves from the cold. Animals also tend to be homeotherms which are animals that maintain a high temperature. Smaller invertebrates also tend to be smaller in polar regions which helps them conserve energy.\n\nThere are also many different animals that live in the sea water near polar regions. Squids are one animal that live in both Antarctica and the Arctic. They are the food source for other large animals such as the male sperm whale. There is also a wide variety of fish in the polar regions. Arctic cod is a major species in the Arctic. Halibut, cod, herring, and Alaska pollock (walleye pollock) are some other types of fish. In Antarctica there is not a lot of diversity among the fish; there is a lot of the same kind. Antarctic silverfish and lanternfish are some examples of fish that live in Antarctica.\n\nSeals are also found in polar regions and number around 2.5 million. They are known to breed on land in the polar regions. Whales are also in the polar regions and can be found near the surfaces of water where they prey.\n\nThere are also birds that breed in the polar regions. In the Arctic, 95% of the birds breeding here consist of only four different species. These include the northern fulmar, kittiwake, the little auk and the thick-billed murre. These birds breed here when the ice starts to thaw and when there is cracks in the ice so the birds are able to feed. In the Antarctic there are two different birds that live there including the penguin and the procellariiformes.\n\nThere is a wide source of vegetation in the polar region but there are few species in common in the southern and northern polar regions. The Arctic consists of desert and tundra vegetations. The desert vegetation consists of algae, lichens, and mosses. Lichens are the most dominant plants. The ground is bare with patchy cover of lichens and mosses. Flowering plants are also seen but not as common. It only contains 60 species of flowering plants. The Arctic tundra vegetation also consists of lichens and mosses, but it includes shrubs, grasses and forbs as well. The amount of vegetation in the tundra consists on how much sun, or snow cover is in the area. The vegetation in this area may grow as tall as . In the southern part of the Arctic there tend to be more shrubs whereas the northern parts there is less plant cover. In wet areas of the tundra there is tussock grasses and cotton grasses. In moist areas there are short grasses, mosses, willows, and birches.\n\nThe Antarctic vegetation consists of algae or lichens, and some bacteria and fungi. Mosses and lichens dominate though. The algae and lichens grow where there is moisture, and they hide in cracks to be protected from the wind. The dominate grassland is the tussock. These grasses get to be high, so they provide habitat for many mammals. Of the of land that makes up Antarctic, less than 2% of it does not have snow or ice.\n\nOne example of a type of vegetation is a crustose lichen. These lichens are found in moist areas that are hidden from wind. They hide on the surface of rocks in the cracks. They survive off the water that melts from above. These lichen occur in Canada and Alaska, as well as Greenland and IceLand These lichens can be red or orange colored and are known to defoliate rocks.\n\nThere exist many threats to the polar regions. One threat is whaling. Whaling started in the 16th century. People hunted whales to sell the meat. By 1925 the number of whales being killed rose from 14,000 to 40,000. The International Whaling Commission tried to stop whaling in the 20th century, but was unsuccessful.\n\nOverfishing is another threat to the polar regions. In the Bering Sea there is a lot of fishing due to the high populations of halibut and Alaskan Pollock. Around the 1970s krill began to become a popular crustacean to catch. The Soviet Union started advertising food with krill in it and they started overfishing krill. It has been estimated that 40 tonnes of krill per hour were caught during this time. In 1982, the exclusive economic zone was established. This said that a certain country can fish off the shore. The country is now able to control who fishes in their EEZ area. But the EEZ has been unsuccessful.\n\nAnother threat is pollution. There are many land and water areas within the polar regions that are contaminated. This can be due to the transport of oil by large ships. Siberia is one example of a place that has had major pollution in its rivers.\n\nDepletion on the ozone layer is one more threat. An ozone hole has been detected above Antarctica. The cause of the depletion of the ozone layer is due to chlorofluorocarbons and other greenhouse gases. The other main reason is due to man made gases that are released into the atmosphere. There are many environmental effects due to this because the gases that are being released five times faster than they are destroyed.\n\nGlobal warming is also having effect on the Arctic and Antarctic regions. Global warming is causing the temperature on the earth to increase. In \"Plan B 2.0\" Lester R. Brown talks about how the Arctic is warming twice as fast as the rest of the world. He goes on to say that the temperature in the Arctic region has increased by within the last half century. And with the increase in temperatures some worry that this will cause the sea level to rise. Scientists believe that if the Greenland ice sheet melts then the sea level could rise by The melting of this ice sheet or others could have an effect on ocean currents. It could cause lower temperatures in northern North America. Rising of the sea level will also impact coastal areas. One example is in Bangladesh. If there was a increase in sea level then millions of people would have to migrate from the coast. Global warming is also affecting Antarctica. The Larsen Ice Shelf or Larsen A is an ice sheet on the Antarctic Peninsula. The sheet broke in 1995, and then in 2000 an iceberg that is broke off the Ross Ice Shelf in Antarctica. In 2002 Larsen B, which was , broke off.\n\nGlobal warming affects plants and animals. For plants, the warmer temperatures induce stress on the plants. For animals, there has been a decrease in the number of polar bears in the Hudson Bay area. Since 1981, the polar bear population has been declining. This is because global warming causes the ice to break up faster so the polar bears are going to the coasts when there are poor conditions.\n\nWhoever owns the land is responsible or managing it. And the owners of the land differ between the Antarctic and Arctic and also within the polar regions. In the Arctic, there are six nations that own the land about 60°N. These nations include: Canada, Russia, Finland, USA, Denmark, Iceland and Norway. There have been international treaties set up so there are no disputes. These nations have also have set their government to manage the land properly. They have set up national parks, land for wilderness, and also land for research. In the polar regions there have been laws set up to manage the number of visitors. There have been rules set up allowing only a certain amount of mining done and other measures to protect the environment from damages.\n\nIn the Antarctic the owners of the land are less clear. Some areas of Antarctica are controlled by the French, while other areas are controlled by South Africa, Australia, New Zealand, and the UK. Whoever owns Antarctic is still unclear therefore many other countries have put out scientific stations. The Antarctic Treaty System of 1961 was established to make sure all the conflicts were resolved about who owned the land. This and other treaties have shown interest in helping to conserve the Antarctic region. All of these countries have conservation laws. These laws manage the amount of hunting in the area, monitor invasive species, and control burning and settlement.\n\nIn a meta-analysis of the published work in aquatic ecosystems since the term biodiversity appeared in the bibliography, the Arctic and Antarctic Polar regions were found to still be unexplored. In addition, the north Pacific Ocean (Pacific Northeast and Pacific Northwest), still has few citations in comparison to its large size. This limits our perception of the world's aquatic biodiversity. Consequently, we do not have sufficient information about biodiversity in most places on earth. Even though biodiversity declines from the equator to the poles in terrestrial ecosystems, this is still a hypothesis to be tested in aquatic and especially marine ecosystems where causes of this phenomenon are unclear. In addition, particularly in marine ecosystems, there are several well stated cases where diversity in higher latitudes actually increases. Therefore, the lack of information on biodiversity of Polar Regions prevents scientific conclusions on the distribution of the world's aquatic biodiversity.\n\n\n"}
{"id": "38051839", "url": "https://en.wikipedia.org/wiki?curid=38051839", "title": "Postglacial vegetation", "text": "Postglacial vegetation\n\nPostglacial vegetation refers to plants that colonize the newly exposed substrate after a glacial retreat. The term \"postglacial\" typically refers to processes and events that occur after the departure of glacial ice or glacial climates.\n\nClimate change is the main force behind changes in species distribution and abundance. Repeated changes in climate throughout the Quaternary Period are thought to have had a significant impact on the current vegetative species diversity present today. Functional and phylogenetic diversity are considered to be closely related to changing climatic conditions, this indicates that trait differences are extremely important in long term responses to climate change. During the transition from the last glaciation of the Pleistocene to the Holocene period, climate warming resulted in the expansion of taller plants and larger seed bearing plants which resulted in lower proportions of vegetative regeneration. Hence, low temperatures can be strong environmental filters that prevent tall and large-seeded plants from establishing in postglacial environments.\nThroughout Europe vegetation dynamics within the first half of the Holocene appear to have been influenced mainly by climate and the reorganization of atmospheric circulation associated with the disappearance of the North American ice sheet. This is evident in the rapid increase of forestation and changing biomes during the postglacial period between 11500ka and 8000ka before the present.\nVegetation development periods of post-glacial land forms on Ellesmere Island, Northern Canada, is assumed to have been at least ca. 20,000 years in duration. This slow progression is mostly due to climatic restrictions such as an estimated annual rainfall amount of only 64mm and a mean annual temperature of -19.7 degrees Celsius. The length in time of vegetation development observed on Ellesmere Island is evidence that post glacial vegetation development is much more restricted in the Arctic and colder climates as compared to milder climatic regions such as the boreal, temperate and tropical zones.\n\nAs land became exposed following the glaciation of the last ice age, a variety of geographic settings ranging from the tropics to the Arctic and Antarctic became available for the establishment of vegetation. Species that now exist on formerly glaciated terrain must have undergone a change in distribution of hundreds to thousands of kilometers, or have evolved from other taxa that have once done so in the past. In a newly developing environment, plant growth is often strongly influenced by the introduction of new organisms into that environment, where competitive or mutuallistic relationships may develop. Often, competitive balances are eventually reached and species abundances remain somewhat constant over a period of generations. \nStudies done on the Norwegian Island of Svalbard, have been very useful in understanding the behavior of postglacial vegetation. Studies show that many vascular plants that are considered pioneers of vegetation development, eventually become less frequent. For example, the abundance of species such as Braya purpurascens has fallen nearly 30% due to the introduction of new species in the area.\n\nArctic vegetation has distinct postglacial development characteristics compared to more temperate zones of lower latitudes. A study of postglacial moraines conducted in the Canadian Arctic on Ellesmere Island have found that dwarf shrubs of Dryas integrifolia and Cassiope tetragona are often good indicators of vegetation development and progression. Dwarf shrubs have been found to increase with the age of the moraine, with Dryas integrifolia becoming the most predominant. As well the cover of vegetation, including lichens and bryophytes showed consistent increase with the moraine age, suggesting directional vegetation development. It is also suggested that part of the high proportions of polypoids occurring in arctic floras is the result of speciation as continental ice-sheets withdrew.\nPollen diagrams from northern Quebec, Canada, show advances throughout the Holocene of post-glacial vegetation development. The initial phase of open vegetation began about 6000 years before the present. Following deglaciation, shrub and herbaceous tundra plants dominated for a brief period of time. Plants such as the Larix laricina, Populus and Juniperus, were also important in the initial vegetation development. Some species that followed later include: Alnus crispa, and Betula. Though later vegetation development was mainly dominated by Picea, shortly following deglaciation, they reached their present day limit. Today black spruce is mainly dominant throughout much of northern Quebec.\nContinental U.S. is considered to have strongly contributed to the re-establishment of postglacial vegetation in Canada following the last ice age. Roughly 300 taxa of vascular plants and mosses that were found to have existed below the extent of the last glacial period within the United States are also found to have migrated to Canada. These patterns are recorded within either pollen or macro fossils.\n\nStudies done by Reitalu, (2015) have found that human impact throughout much of Europe has negatively influenced plant diversity by suppressing the establishment of tall-growing, large seeded taxa. Although human influence has facilitated many Ruderal species, this is believed to have led to an overall decrease in phylogenetic diversity.\n\nMany pollen diagrams around the world indicate that major climate changes caused the last continental ice sheets to retreat, leading to dramatic effects on the distribution and abundance of plants. By converting pollen data into plant functional type (PFT) assemblages and interpolating the data, researchers have been able to reconstruct postglacial vegetation patterns around the world. Core sampling and analysis of lake sediments that contain pollen and other plant remains are often used to obtain good records of past pollination cycles. Such paleorecords preserved in lake sediments can be used to reconstruct the history of post glacial vegetation. Lake sediments have an advantage over other core sampling sites, such as fen and bog peats, as they provide no overwhelming local pollen components. As well, lake sediments contain stratigraphic changes in soil character, which are useful for understanding changes in vegetation development over a period of time. Macrofossils that are obtained from sedimentary deposits are also useful for constructing the history of changing postglacial vegetation.\n\n\n"}
{"id": "182075", "url": "https://en.wikipedia.org/wiki?curid=182075", "title": "Richard Lindzen", "text": "Richard Lindzen\n\nRichard Siegmund Lindzen (born February 8, 1940) is an American atmospheric physicist known for his work in the dynamics of the middle atmosphere, atmospheric tides, and ozone photochemistry. He has published more than 200 scientific papers and books. From 1983 until his retirement in 2013, he was Alfred P. Sloan Professor of Meteorology at the Massachusetts Institute of Technology. He was a lead author of Chapter 7, \"Physical Climate Processes and Feedbacks,\" of the Intergovernmental Panel on Climate Change's Third Assessment Report on climate change. He has criticized the scientific consensus about climate change and what he has called \"climate alarmism.\"\n\nLindzen was born on February 8, 1940 in Webster, Massachusetts. His father, a shoemaker, had fled Hitler's Germany with his mother. He moved to the Bronx soon after his birth and grew up in a Jewish household in a predominantly Catholic neighborhood there. Lindzen attended the Bronx High School of Science (winning Regents' and National Merit Scholarships), Rensselaer Polytechnic Institute,and Harvard University. From Harvard, he received an A.B. in physics in 1960, followed by an S.M. in applied mathematics in 1961 and a PhD in applied mathematics in 1964. His doctoral thesis, \"Radiative and photochemical processes in strato- and mesospheric dynamics\", concerned the interactions of ozone photochemistry, radiative transfer, and dynamics in the middle atmosphere.\n\nLindzen has published papers on Hadley circulation, monsoon meteorology, planetary atmospheres, hydrodynamic instability, , global heat transport, the water cycle, ice ages and seasonal atmospheric effects. His main contribution to the academic literature on anthropogenic climate change is his proposal of the iris hypothesis in 2001, with co-authors Ming-Dah Chou and Arthur Y. Hou. He is a member of the National Academy of Sciences and the Science, Health, and Economic Advisory Council at the Annapolis Center for Science-Based Public Policy. He joined MIT in 1983, prior to which he held positions at the University of Washington (1964–65), Institute for Theoretical Meteorology, University of Oslo (1965–67), National Center for Atmospheric Research (NCAR) (1966–67), University of Chicago (1968–72) and Harvard University (1972–83). He also briefly held a position of Visiting Lecturer at UCLA in 1967. As of January 2010, his publications list included 230 papers and articles published between 1965 and 2008, with five in process for 2009. He is the author of a standard textbook on atmospheric dynamics, and co-authored the monograph \"Atmospheric Tides\" with Sydney Chapman.\n\nHe was Alfred P. Sloan Professor of Meteorology at MIT from 1983, until his retirement which was reported in the Spring 2013 newsletter of MIT's Department of Earth, Atmospheric and Planetary Sciences (EAPS). On December 27, 2013 the Cato Institute announced that he is a Distinguished Senior Fellow in their \"Center for the Study of Science\".\n\nLindzen's early work was concerned with ozone photochemistry, the aerodynamics of the middle atmosphere, the theory of atmospheric tides, and planetary waves. His work in these areas led him to a number of fundamental scientific discoveries, including the discovery of negative equivalent depths in classical tidal theory, explanations for both the quasi-biennial oscillation of the Earth's stratosphere and the four-day period of the superrotation of the Venus atmosphere above the cloud top.\n\nHis PhD thesis of 1964 concerned the interactions of ozone photochemistry, radiative transfer and the dynamics of the middle atmosphere. This formed the basis of his seminal \"Radiative and Photochemical Processes in Mesospheric Dynamics\" that was published in four parts in the \"Journal of the Atmospheric Sciences\" between 1965 and 1966. The first of these, \"Part I: Models for Radiative and Photochemical Processes\", was co-authored with his Harvard colleague and former PhD thesis advisor, Richard M. Goody, who is well known for his 1964 textbook \"Atmospheric Radiation\". The Lindzen and Goody (1965) study has been widely cited as foundational in the exact modeling of middle atmosphere ozone photochemistry. This work was extended in 1973 to include the effects of nitrogen and hydrogen reactions with his former PhD student, Donna Blake, in \"Effect of photochemical models on calculated equilibria and cooling rates in the stratosphere\".\n\nLindzen's work on ozone photochemistry has been important in studies that look at the effects that anthropogenic ozone depletion will have on climate.\n\nSince the time of Laplace (1799), scientists had been puzzled as to why pressure variations measured at the Earth's surface associated with the semi-diurnal solar tide dominate those of the diurnal tide in amplitude, when intuitively one would expect the diurnal (daily) passage of the sun to dominate. Lord Kelvin (1882) had proposed the so-called \"resonance\" theory, wherein the semi-diurnal tide would be \"selected\" over the diurnal oscillation if the atmosphere was somehow able to oscillate freely at a period of very close to 12 hours, in the same way that overtones are selected on a vibrating string. By the second half of the twentieth century, however, observations had failed to confirm this hypothesis, and an alternative hypothesis was proposed that something must instead suppress the diurnal tide. In 1961, Manfred Siebert suggested that absorption of solar insolation by tropospheric water vapour might account for the reduction of the diurnal tide. However, he failed to include a role for stratospheric ozone. This was rectified in 1963 by the Australian physicist Stuart Thomas Butler and his student K.A. Small who showed that stratospheric ozone absorbs an even greater part of the solar insolation.\n\nNevertheless, the predictions of classical tidal theory still did not agree with observations. It was Lindzen, in his 1966 paper, \"On the theory of the diurnal tide\", who showed that the solution set of Hough functions given by Bernhard Haurwitz to Laplace's tidal equation was incomplete: modes with negative equivalent depths had been omitted. Lindzen went on to calculate the thermal response of the diurnal tide to ozone and water vapor absorption in detail and showed that when his theoretical developments were included, the surface pressure oscillation was predicted with approximately the magnitude and phase observed, as were most of the features of the diurnal wind oscillations in the mesosphere. In 1967, along with his NCAR colleague, Douglas D. McKenzie, Lindzen extended the theory to include a term for Newtonian cooling due to emission of infrared radiation by carbon dioxide in the stratosphere along with ozone photochemical processes, and then in 1968 he showed that the theory also predicted that the semi-diurnal oscillation would be insensitive to variations in the temperature profile, which is why it is observed so much more strongly and regularly at the surface.\n\nWhile holding the position of Research Scientist at the National Center for Atmospheric Research (NCAR) in Boulder, CO Lindzen was noticed and befriended by Professor Sydney Chapman, who had contributed to the theory of atmospheric tides in a number of papers from the 1920s through to the 1940s. This led to their joint publication in 1969 of a 186-page monograph (republished in 1970 as a book) \"Atmospheric Tides\".\n\nAlthough it wasn't realized at the time, the quasi-biennial oscillation (QBO) was observed during the 1883 eruption of Krakatoa, when the ash from the volcano was transported around the globe from east to west by stratospheric winds in about two weeks. These winds became known as the \"Krakatoa easterlies\". It was observed again in 1908, by the German meteorologist Arthur Berson, who saw that winds blow from the west at altitude in tropical Africa from his balloon experiments. These became known as the \"Berson westerlies\". However, it was not until the early 1960s that the ~ 26-month cycle of the QBO was first described, independently by Richard J. Reed in 1960 and Veryhard and Ebdon in 1961.\n\nLindzen recalls his discovery of the mechanism underlying the QBO in the semi-autobiographical review article, \"On the development of the theory of the QBO\". His interest in the phenomenon began in 1961 when his PhD advisor, Richard M. Goody, speculated that the 26-month relaxation time for stratospheric ozone at in the tropics might somehow be related to the 26-month period of the QBO, and suggested investigation of this idea as a thesis topic. In fact, Lindzen's, \"Radiative and photochemical processes in mesospheric dynamics, Part II: Vertical propagation of long period disturbances at the equator\", documented the failure of this attempt to explain the QBO.\n\nLindzen's work on atmospheric tides led him to the study of planetary waves and the general circulation of atmospheres. By 1967, he had contributed a number of papers on the theory of waves in the middle atmosphere. In \"Planetary waves on beta planes\", he developed a beta plane approximation for simplifying the equations of classical tidal theory, whilst at the same time developing planetary wave relations. He noticed from his equations that eastward-traveling waves (known as \"Rossby waves\" since their discovery in 1939 by Carl-Gustav Rossby) and westward-traveling waves (which Lindzen himself helped in establishing as \"atmospheric Kelvin waves\") with periods less than five days were \"vertically trapped.\" At the same time, an important paper by Booker and Bretherton appeared, which Lindzen read with great interest. Booker and Bretherton showed that vertically propagating gravity waves were completely absorbed at a critical level.\n\nIn his 1968 paper with James R. Holton, \"A theory of the quasi-biennial oscillation\", Lindzen presented his theory of the QBO after testing it in a two-dimensional (2-D) numerical model that had been developed by Holton and John M. Wallace. They showed that the QBO could be driven by vertically propagating gravity waves with phase speeds in both westward and eastward directions and that the oscillation arose through a mechanism involving a two-way feedback between the waves and the mean flow. It was a bold conjecture, given that there was very little observational evidence available to either confirm or confute the hypothesis. In particular, there was still no observational evidence of the westward-traveling \"Kelvin\" waves; Lindzen postulated their existence theoretically.\n\nIn the years following the publication of Lindzen and Holton (1968), more observational evidence became available, and Lindzen's fundamental insight into the mechanism driving the QBO was confirmed. However, the theory of interaction via critical level absorption was found to be incomplete and was modified to include the importance of attenuation due to radiative cooling. The revised theory was published in the Holton and Lindzen (1972) paper, \"An updated theory for the quasibiennial cycle of the tropical stratosphere\".\n\nSince the 1960s a puzzling phenomenon has been observed in the atmosphere of Venus. The atmosphere above the cloud base is seen to travel around the planet about 50 times faster than the rotation of the planet surface, or in only four to five Earth-days. In 1974 a theory was proposed by Stephen B. Fels and Lindzen to explain this so-called \"superrotation\" which held that the rotation is driven by the thermal atmospheric tide. An alternative theory was proposed by Peter J. Gierasch in the following year which held instead that the meridional (Hadley) circulation may transport the momentum by eddy-mixing. The actual cause of this phenomenon continues to be debated in the literature, with General Circulation Model experiments suggesting that both the Fels/Lindzen and Gierasch mechanisms are involved.\n\nFrom 1972 to 1982 Lindzen was a professor of dynamic meteorology at Harvard University. From February to June 1975 he was a visiting professor of dynamic meteorology at MIT, and during part of 1979 Lindzen was a visiting professor at the Hebrew University of Jerusalem, before switching affiliations to MIT as the Alfred P. Sloan Professor of Meteorology in 1983.\n\nDuring this time, Lindzen published some research on gravity waves, as well as Hadley circulations. He is named as one of 16 Scientific Members of the team authoring the National Academy of Sciences 1975 publication \"Understanding Climatic Change: A Program for Action\".\n\nLindzen hypothesized that the Earth may act like an infrared iris. A sea surface temperature increase in the tropics would result in reduced cirrus clouds and thus more infrared radiation leakage from Earth's atmosphere. Additionally, rising temperatures would cause more extensive drying due to increased areas of atmospheric subsidence. This hypothesis suggests a negative feedback which would counter the effects of warming by lowering the climate sensitivity. Satellite data from CERES has led researchers investigating Lindzen's theory to conclude that the Iris effect would instead warm the atmosphere. Lindzen disputed this, claiming that the negative feedback from high-level clouds was still larger than the weak positive feedback estimated by Lin et al.\n\nLindzen has expressed his concern over the validity of computer models used to predict future climate change. Lindzen said that predicted warming may be overestimated because of their handling of the climate system's water vapor feedback. The feedback due to water vapor is a major factor in determining how much warming would be expected to occur with increased atmospheric concentrations of carbon dioxide, and all existing computer models assume positive feedback — that is, that as the climate warms, the amount of water vapour held in the atmosphere will increase, leading to further warming. By contrast, Lindzen believes that temperature increases will actually cause more extensive drying due to increased areas of atmospheric subsidence as a result of the Iris effect, nullifying future warming. This claim was criticized by climatologist Gavin Schmidt, Director of NASA's Goddard Institute for Space Studies, who notes the more generally-accepted understanding of the effects of the Iris effect and cites empirical cases where large and relatively rapid changes in the climate such as El Niño events, the Ultra Plinian eruption of Mount Pinatubo in 1991, and recent trends in global temperature and water vapor levels to show that, as predicted in the generally-accepted view, water vapor increases as the temperature increases, and decreases as temperatures decrease.\n\nContrary to the IPCC's assessment, Lindzen said that climate models are inadequate. Despite accepted errors in their models, e.g., treatment of clouds, modelers still thought their climate predictions were valid. Lindzen has stated that due to the non-linear effects of carbon dioxide in the atmosphere, CO levels are now around 30% higher than pre-industrial levels but temperatures have responded by about 75% of the expected value for a doubling of CO. The IPCC (2007) estimates that the expected rise in temperature due to a doubling of CO to be about , ± 1.5°. Lindzen has given estimates of the Earth's climate sensitivity to be 0.5 °C based on ERBE data. These estimates were criticized by Kevin E. Trenberth and others, and Lindzen accepted that his paper included \"some stupid mistakes\". When interviewed, he said \"It was just embarrassing\", and added that \"The technical details of satellite measurements are really sort of grotesque.\" Lindzen and Choi revised their paper and submitted it to \"PNAS\". The four reviewers of the paper, two of whom had been selected by Lindzen, strongly criticized the paper and PNAS rejected it for publication. Lindzen and Choi then succeeded in getting a little known Korean journal to publish it as a 2011 paper. Andrew Dessler published a paper which found errors in Lindzen and Choi 2011, and concluded that the observations it had presented \"are not in fundamental disagreement with mainstream climate models, nor do they provide evidence that clouds are causing climate change. Suggestions that significant revisions to mainstream climate science are required are therefore not supported.\"\n\nIn 2001, Lindzen served on an 11-member panel organized by the National Academy of Sciences. The panel's report, titled \"Climate Change Science: An Analysis of Some Key Questions\", has been widely cited. Lindzen subsequently publicly criticized the report summary for not referring to the statement in the full report that twenty years of temperature measurements was \"too short a period for estimating long term trends\".\n\nLindzen worked on Chapter 7 of 2001 IPCC Working Group 1, which considers the physical processes that are active in real world climate. He had previously been a contributor to Chapter 4 of the 1995 \"IPCC Second Assessment\". He described the full 2001 IPCC report as \"an admirable description of research activities in climate science\" although he criticized the Summary for Policymakers. Lindzen stated in May 2001 that it did not truly summarize the IPCC report but had been amended to state more definite conclusions. He also emphasized the fact that the summary had not been written by scientists alone. The NAS panel on which Lindzen served says that the summary was the result of dialogue between scientists and policymakers.\n\nIn an announcement on December 27, 2013, the Cato Institute said that at Cato, Lindzen's focus would be on \"the interaction between science and policymakers\" and that he would study \"whether the move from largely private funding to public support has introduced biases into science and the public policies informed by science.\"\n\nHe has criticized the scientific consensus on global climate change, pointing out that scientists are just as liable to err when the science appears to point in just one direction. He drew an analogy in 1996 between the consensus in the early and mid-twentieth century on eugenics and the current consensus about global warming. In a 2007 interview on The Larry King Show, Lindzen said:\nIn a 2009 editorial in the Wall Street Journal, Lindzen said that the earth was just emerging from the \"Little Ice Age\" in the 19th century and says that it is \"not surprising\" to see warming after that. He goes on to state that the IPCC claims were\n\nAccording to an April 30, 2012 \"New York Times\" article, \"Dr. Lindzen accepts the elementary tenets of climate science. He agrees that carbon dioxide is a greenhouse gas, calling people who dispute that point \"nutty.\" He agrees that the level of it is rising because of human activity and that this should warm the climate.\" He also believes that decreasing tropical cirrus clouds in a warmer world will allow more longwave radiation to escape the atmosphere, counteracting the warming. Lindzen first published this \"iris\" theory in 2001, and offered more support in a 2009 paper.\n\nStarting in 1991, Lindzen has provided testimonies to the U.S. Senate and House committees regarding his understandings of the current state of research on climate change for multiple times.\n\nIn 2001, Lindzen along with 11 scientists prepared a report urging the Bush Administration not to ratify the Kyoto Protocol. In the report, they stated that there are still considerable uncertainties regarding the understanding of the climate system as well as unresolved difficulties in predicting the future climate, and the Kyoto Protocol \"would not result in a substantial reduction in global warming\". Lindzen has also said that he believed the Kyoto Protocol would increase the cost of electricity for no gain and put signatory states at a competitive disadvantage in a letter to Mayor David B. Cohen of Newton, Massachusetts.\n\nIn 2017, Lindzen has sent a petition to President Trump, asking the President to withdraw the United States from the United Nations Convention on Climate Change. The petition contains the names of \"around 300 eminent scientists and other qualified individuals\" and calls on the United States and other nations to “change course on an outdated international agreement that targets minor greenhouse gases,” starting with carbon dioxide. It receives considerable media coverage and 22 then current or retired MIT professors promptly issued an open letter addressed to Trump informing him that Lindzen’s petition doesn’t represent their views or those of the vast majority of other climate scientists. Lindzen and his supporting signers then published another rebuttal letter in response.\n\nAn April 30, 2012 article in \"The New York Times\" included the comments of several other experts. Christopher S. Bretherton, an atmospheric researcher at the University of Washington, said Lindzen is \"feeding upon an audience that wants to hear a certain message, and wants to hear it put forth by people with enough scientific reputation that it can be sustained for a while, even if it's wrong science. I don't think it's intellectually honest at all.\" Kerry A. Emanuel, another M.I.T. scientist, said of Lindzen's views \"Even if there were no political implications, it just seems deeply unprofessional and irresponsible to look at this and say, 'We're sure it's not a problem.' It's a special kind of risk, because it's a risk to the collective civilization.\"\n\nA 1996 article in \"The New York Times\" included the comments of several other experts. Jerry D. Mahlman, director of the Geophysical Fluid Dynamics Laboratory, did not accept Lindzen's assessment of the science, and said that Lindzen had \"sacrificed his luminosity by taking a stand that most of us feel is scientifically unsound.\" Mahlman did, however, admit that Lindzen was a \"formidable opponent\". William Gray of Colorado State University basically agreed with Lindzen, describing him as \"courageous\". He said, \"A lot of my older colleagues are very skeptical on the global warming thing\". He added that while he regarded some of Lindzen's views as flawed, he said that, \"across the board he's generally very good\". John Wallace of the University of Washington agreed with Lindzen that progress in climate change science had been exaggerated, but said there are \"relatively few scientists who are as skeptical of the whole thing as Dick [Lindzen] is\".\n\nThe November 10, 2004 online version of \"Reason\" magazine reported that Lindzen is \"willing to take bets that global average temperatures in 20 years will in fact be lower than they are now\". However, on June 8, 2005 they reported that Lindzen insisted that he had been misquoted, after James Annan contacted Lindzen to make the bet but claimed that \"Lindzen would take only 50 to 1 odds\".\n\n\"The Guardian\" reported in June 2016 that Lindzen has been a beneficiary of Peabody Energy, a coal company that has funded multiple groups contesting the climate consensus.\n\nLindzen has been called a contrarian, in relation to climate change and other issues. Lindzen's graduate students describe him as \"fiercely intelligent, with a deep contrarian streak.\"\n\nThe characterization of Lindzen as a contrarian has been reinforced by reports that he claims that lung cancer has only been weakly linked to smoking. However, when asked about this during an interview as part of an Australian Broadcasting Company documentary, Lindzen said that while \"the case for second-hand tobacco is not very good ... the World Health Organization also said that” (referencing a 1998 study by the International Agency for Research on Cancer (IARC) on environmental tobacco smoke (ETS)), on the other hand \"With first-hand smoke it's a more interesting issue ... The case for lung cancer is very good but it also ignores the fact that there are differences in people's susceptibilities which the Japanese studies have pointed to.\" Again, when asked to clarify his position by a climate skeptic blogger, Lindzen wrote, \"there was a reasonable case for the role of cigarette smoking in lung cancer, but that the case was not so strong that one should rule that any questions were out of order ... the much, much weaker case against second hand smoke [is] also being treated as dogma.\"\n\nLindzen is a recipient of the American Meteorological Society's Meisinger and Charney Awards, American Geophysical Union's Macelwane Medal, and the Leo Prize from the Wallin Foundation in Goteborg, Sweden. He is a member of the National Academy of Sciences (NAS), and the Norwegian Academy of Science and Letters, and was named Fellow of the American Academy of Arts and Sciences, the American Association for the Advancement of Sciences, the American Geophysical Union, and the American Meteorological Society. He is a corresponding member of the NAS Committee on Human Rights, and a member of the United States National Research Council Board on Atmospheric Sciences and Climate. He was a consultant to the Global Modeling and Simulation Group at NASA's Goddard Space Flight Center, and a Distinguished Visiting Scientist at California Institute of Technology's Jet Propulsion Laboratory. Lindzen is an ISI highly cited researcher, and his biography has been included in American Men and Women of Science.\n\nRichard Lindzen and his wife, Nadine, have two sons. Lindzen's interests include amateur radio (call letters: WO1I), photography, and oriental rugs.\n\n\n\n\n"}
{"id": "213555", "url": "https://en.wikipedia.org/wiki?curid=213555", "title": "Solar updraft tower", "text": "Solar updraft tower\n\nThe solar updraft tower (SUT) is a design concept for a renewable-energy power plant for generating electricity from low temperature solar heat. Sunshine heats the air beneath a very wide greenhouse-like roofed collector structure surrounding the central base of a very tall chimney tower. The resulting convection causes a hot air updraft in the tower by the chimney effect. This airflow drives wind turbines, placed in the chimney updraft or around the chimney base, to produce electricity. \n\nAs of mid 2018, although several prototype models have been built, no full-scale practical units are in operation. Scaled-up versions of demonstration models are planned to generate significant power. They may also allow development of other applications, such as to agriculture or horticulture, to water extraction or distillation, or to improvement of urban air pollution.\n\nCommercial investment may have been discouraged by the high initial cost of building a very large novel structure, the large land area required and by the risk of investment. However, there is renewed interest in solar updraft towers, especially in sunny remote areas. A few prototypes have recently been built, and projects are proposed for parts of Africa, the US and Australia.\n\nIn 2014, National Geographic published a popular update, including an interview with an informed engineering proponent. A solar updraft tower power plant can generate electricity from the low temperature atmospheric heat gradient between ground or surface level and structurally reachable altitude. Functional or mechanical feasibility is now less of an issue than capitalisation.\n\nA comprehensive review of theoretical and experimental aspects of \"solar updraft tower power plant\" (SUTPP) development is available, recommending commercial development.\n\nPower output depends primarily on two factors: collector area and chimney height. A larger area collects and warms a greater volume of air to flow up the chimney; collector areas as large as in diameter have been discussed. A larger chimney height increases the pressure difference via the stack effect; chimneys as tall as have been discussed.\n\nHeat is stored inside the collector area allowing SUTs to operate 24 hours a day. The ground beneath the solar collector, water in bags or tubes, or a saltwater thermal sink in the collector could add thermal capacity and inertia to the collector. Humidity of the updraft and condensation in the chimney could increase the energy flux of the system.<ref name=\"Solar pond tower for 5 €ct/kWh\"></ref>\n\nTurbines with a horizontal axis can be installed in a ring around the base of the tower, as once planned for an Australian project and seen in the diagram above; or—as in the prototype in Spain—a single vertical axis turbine can be installed inside the chimney.\n\nA near negligible amount of Carbon dioxide is produced as part of operations, while construction material manufacturing can create emissions. Net energy payback is estimated to be 2–3 years.\n\nSince solar collectors occupy significant amounts of land, deserts and other low-value sites are most likely. Improvements in the solar heat collection efficiency by using unglazed transpired collector can significantly reduce the land required for the solar array.\n\nA small-scale solar updraft tower may be an attractive option for remote regions in developing countries. The relatively low-tech approach could allow local resources and labour to be used for construction and maintenance.\n\nLocating a tower at high latitudes could produce up to 85 per cent of the output of a similar plant located closer to the equator, if the collection area is sloped significantly toward the equator. The sloped collector field, which also functions as a chimney, is built on suitable mountainsides, with a short vertical chimney on the mountaintop to accommodate the vertical axis air turbine. The results showed that solar chimney power plants at high latitudes may have satisfactory thermal performance.\n\nA chimney turbine was envisioned as a smoke jack, and illustrated 500 years ago by Leonardo da Vinci. An animal spitted above a fire or in an oven could be turned by a vertical axis turbine with four angled vanes in the chimney updraft. \n\nIn 1896, Mr. Alfred Rosling Bennett published the first patent describing a \"Convection Mill\" . Even if in the title of the Patent and in the claims the word \"Toy\" clearly appears and even if in the overall description made inside the Patent it is evident that the idea was to produce small devices, in page 3 at lines 49-54 Bennett envisions much larger devices for bigger scale applications. A model of this \"convection mill\", built in 1919 by Albert H. Holmes & Son (London) to demonstrate the phenomenon of convection currents, is on display in the Science Museum, London.\n\nIn 1903, Isidoro Cabanyes, a colonel in the Spanish army, proposed a solar chimney power plant in the magazine \"La energía eléctrica\". Another early description was published in 1931 by German author Hanns Günther. Beginning in 1975, Robert E. Lucier applied for patents on a solar chimney electric power generator; between 1978 and 1981 patents (since expired) were granted in Australia, Canada, Israel, and the US.\n\nIn 1926 Prof Engineer Bernard Dubos proposed to the French Academy of Sciences the construction of a Solar Aero-Electric Power Plant in North Africa with its solar chimney on the slope of a large mountain. \n\nIn 1982, a small-scale experimental model of a solar draft tower was built in Manzanares, Ciudad Real, 150 km south of Madrid, Spain at . The power plant operated for approximately eight years. The tower's guy-wires were not protected against corrosion and failed due to rust and storm winds. The tower blew over and was decommissioned in 1989.\n\nInexpensive materials were used in order to evaluate their performance. The solar tower was built of iron plating only thick under the direction of a German engineer, Jörg Schlaich. The project was funded by the German government.\n\nThe chimney had a height of and a diameter of with a collection area (greenhouse) of and a diameter of , obtaining a maximum power output of about 50 kW. Various materials were used for testing, such as single or double glazing or plastic (which turned out not to be durable enough). One section was used as an actual greenhouse. During its operation, 180 sensors measured inside and outside temperature, humidity and wind speed data was collected on a second-by-second basis. This experimental setup did not sell energy.\n\nIn December 2010, a tower in Jinshawan in Inner Mongolia, China started operation, producing 200 kilowatts. The 1.38 billion RMB (USD 208 million) project was started in May 2009. It was intended to cover and produce 27.5 MW by 2013, but had to be scaled back. The solar chimney plant was expected to improve the climate by covering loose sand, restraining sandstorms. Critics have said that the 50m tall tower is too short to work properly and that it was a mistake to use glass in metal frames for the collector, as many of them cracked and shattered in the heat.\n\nA proposal to construct a solar updraft tower in Fuente el Fresno, Ciudad Real, Spain, entitled \"Ciudad Real Torre Solar\" would be the first of its kind in the European Union and would stand tall – nearly twice as tall as the Belmont TV Mast, which was once the tallest structure in the European Union, before being shortened by several hundred feet – covering an area of .\nIt is expected to produce 40 MW.\n\nIn 2001, EnviroMission proposed to build a solar updraft tower power generating plant known as \"Solar Tower Buronga\" near Buronga, New South Wales. The company did not complete the project. They have plans for a similar plant in Arizona, and most recently (December 2013) in Texas, but there is no sign of 'breaking ground' in any of Enviromission's proposals.\n\nIn December 2011, Hyperion Energy, controlled by Western Australians Tony Sage and Dallas Dempster, was reported to be planning to build a 1-km-tall solar updraft tower near Meekatharra to supply power to Mid-West mining projects.\n\nBased on the need for plans for long-term energy strategies, Botswana's Ministry of Science and Technology designed and built a small-scale research tower. This experiment ran from 7 October to 22 November 2005. It had an inside diameter of and a height of , manufactured from glass-reinforced polyester, with an area of approximately . The roof was made of a 5 mm thick clear glass supported by a steel framework.\n\nIn mid-2008, the Namibian government approved a proposal for the construction of a 400 MW solar chimney called the 'Greentower'. The tower is planned to be tall and in diameter, and the base will consist of a greenhouse in which cash crops can be grown.\n\nA model solar updraft tower was constructed in Turkey as a civil engineering project. Functionality and outcomes are obscure.\n\nA second solar updraft tower using a transpired collector is operating at Trakya University in Edirne Turkey and is being used to test various innovations in SUT designs including the ability to recover heat from photovoltaic (PV) arrays.\nA grade-school pupil's home do-it-yourself SUT demonstration for a school science fair was constructed and studied in 2012, in a suburban Connecticut setting. With a 7-metre stack and 100 square metre collector, this generated a daily average 6.34 mW, from a computer fan as a turbine. Insolation and wind were the major factors on variance (range from 0.12 to 21.78 mW) in output.\n\nIn Xian, central China, a 60 metre urban chimney with surrounding collector has significantly reduced urban air pollution. This demonstration project was led by Cao Junji, a chemist at the Chinese Academy of Sciences’ Key Laboratory of Aerosol Chemistry and Physics.\n\nThe traditional solar updraft tower has a power conversion rate considerably lower than many other designs in the (high temperature) solar thermal group of collectors. The low conversion rate is balanced to some extent by the lower cost per square metre of solar collection.\n\nModel calculations estimate that a 100 MW plant would require a 1,000 m tower and a greenhouse of . A 200 MW tower with the same tower would require a collector 7 kilometres in diameter (total area of about 38 km²). One 200MW power station will provide enough electricity for around 200,000 typical households and will abate over 900,000 tons of greenhouse producing gases from entering the environment annually. The glazed collector area is expected to extract about 0.5 percent, or 5 W/m² of 1 kW/m², of the solar energy that falls upon it. If a transpired solar collector is used in place of the glazed collector, the efficiency is doubled. Additional efficiency improvements are possible by modifying the turbine and chimney design to increase air speed using a venturi configuration. Concentrating thermal (CSP) or photovoltaic (CPV) solar power plants range between 20% to 31.25% efficiency (dish Stirling). Overall CSP/CPV efficiency is reduced because collectors do not cover the entire footprint. Without further tests, the accuracy of these calculations is uncertain. Most of the projections of efficiency, costs and yields are calculated theoretically, rather than empirically derived from demonstrations, and are seen in comparison with other collector or solar heat transducing technologies.\n\nAn innovative concept recombining a thermal power plant dry cooling tower with a solar chimney was first introduced by Zandian and Ashjaee in 2013 to increase the efficiency of the solar updraft towers. This hybrid cooling-tower-solar-chimney (HCTSC) system was shown to be able to produce an over ten times increase in output power compared to the conventional solar chimney power plants like Manzanares, Ciudad Real, with similar geometrical dimensions. In addition, it was shown that with an increase in chimney diameter, the power generation can reach to MW-graded power output without the necessity of building huge individual solar chimney panels. The results showed a maximum of 3 MW power output from the HCTSC system which resulted in 0.37% increase in the thermal efficiency of a typical 250 MW fossil fuel power plant, with a chimney diameter of only . The new hybrid design made the solar updraft tower feasible again, and proved it to be economical in saving lots of construction cost and time. This concept also recaptures the heat of radiators that are thrown out into the atmosphere without efficient utilization, and prevents generation of excessive greenhouse gasses.\n\nThe performance of an updraft tower may be degraded by factors such as atmospheric winds, by drag induced by the bracings used for supporting the chimney, and by reflection off the top of the greenhouse canopy.\n\n\n\n\n\nA solar updraft power station would require a large initial capital outlay, but would have relatively low operating cost.\n\nCapital outlays would be roughly the same as next-generation nuclear plants such as the AP-1000 at roughly $5 per Watt of capacity. As with other renewable power sources, towers have no need for fuel. Overall costs are largely determined by interest rates and years of operation, varying from 5 eurocent per kWh for 4% and 20 years to 15 eurocent per kWh for 12% and 40 years.\n\nEstimates of total costs range from 7 (for a 200 MW plant) and 21 (for a 5 MW plant) euro cents per kWh to 25–35 cents per kWh. Levelized cost are approximately 3 Euro cents per KWh for a 100 MW wind or natural gas plant. No actual data are available for a utility scale power plant.\n\n\n"}
{"id": "41961028", "url": "https://en.wikipedia.org/wiki?curid=41961028", "title": "The Martian (Weir novel)", "text": "The Martian (Weir novel)\n\nThe Martian is a 2011 science fiction novel written by Andy Weir. It was his debut novel under his own name. It was originally self-published in 2011; Crown Publishing purchased the rights and re-released it in 2014. The story follows an American astronaut, Mark Watney, as he becomes stranded alone on Mars in the year 2035 and must improvise in order to survive. \"The Martian\", a film adaptation directed by Ridley Scott and starring Matt Damon, was released in October 2015.\n\nIn 2035, the crew of NASA's Ares 3 mission have arrived at Acidalia Planitia for a planned month-long stay on Mars. After only six sols, an intense dust and wind storm threatens to topple their Mars Ascent Vehicle (MAV), trapping them on the planet. During the hurried evacuation, an antenna tears loose and impales astronaut Mark Watney, a botanist and engineer, also disabling his spacesuit radio. He is flung out of sight by the wind and presumed dead. As the MAV teeters dangerously, mission commander Melissa Lewis has no choice but to take off without searching for Watney.\n\nHowever, Watney is not dead. His injury proves relatively minor, but with no long-range radio, he cannot communicate with anyone. He must rely on his own resourcefulness to survive. He begins a log of his experiences. His philosophy is to \"work the problem\", solving each challenge in turn as it confronts him. With food a critical, though not immediate, problem, he begins growing potatoes in the crew's Martian habitat (the Hab), and burns hydrazine to generate water for the plants.\n\nNASA eventually discovers that Watney is alive when satellite images of the landing site show evidence of his activities; they begin devising ways to rescue him, but withhold the news of his survival from the rest of the Ares 3 crew, on their way back to Earth aboard the \"Hermes\" spacecraft, so as not to distract them.\n\nWatney plans to drive to Schiaparelli crater where the next mission, Ares 4, will land in four years and their MAV is already pre-positioned. He begins modifying one of the rovers for the journey, adding solar cells and an additional battery. He makes a three-week test drive to recover the unmanned \"Pathfinder\" lander and \"Sojourner\" rover and brings them back to the Hab, enabling him to contact NASA. Mitch Henderson, the Ares 3 flight director, convinces NASA Administrator Teddy Sanders to allow him to inform the Ares 3 crew of Watney's survival; they are thrilled, except for Lewis, who is guilt-stricken at leaving him behind.\n\nThe canvas at one of the Hab airlocks tears due to Watney's repeated use of the airlock, decompressing the Hab and nearly killing him. He repairs the Hab, but his plants are dead, threatening him again with eventual starvation. Setting aside safety protocols due to time constraints, NASA hastily prepares an unmanned probe to send Watney supplies, but the rocket disintegrates after liftoff. A deal with the China National Space Administration provides a ready booster—planned for use with the \"Taiyang Shen,\" an unmanned probe—to try again. With no time to build a probe with a soft-landing system, NASA is faced with the prospect of building a capsule whose cargo can survive crashing into the Martian surface at .\n\nHowever, astrodynamicist Rich Purnell devises a \"slingshot\" trajectory around Earth for a gravity assist that could get \"Hermes\" back to Mars on a much-extended mission to save Watney, using the Chinese rocket booster to send a simpler resupply probe to \"Hermes\" as it passes Earth. Sanders vetoes the \"Rich Purnell Maneuver\", as it would entail risking the other crewmembers, but Henderson secretly emails the details to \"Hermes.\" All five of Watney's crewmates approve the plan. Once they begin the maneuver (and disable NASA's remote overrides), NASA has no choice but to support them. The resupply ship docks with \"Hermes\" successfully.\n\nWatney resumes modifying the rover because the new rescue plan requires him to lift off from Mars in the Ares 4 MAV. While working on the rover, Watney accidentally shorts out the electronics of \"Pathfinder\", losing the ability to communicate with Earth (except for spelling out Morse code with rocks).\n\nAfter Watney leaves for Schiaparelli, NASA discovers that a dust storm is approaching his path, but has no way to warn him. The rover's solar cells will be less and less able to recharge, endangering both the rendezvous and his immediate survival (if there is not enough power to run his life-support equipment). While crossing Arabia Terra, Watney becomes aware of the darkening sky and improvises a rough measurement of the storm's shape and direction of movement, enabling him to go around it.\n\nSurviving a rover rollover on his descent into Schiaparelli, Watney reaches the MAV and reestablishes contact with NASA. He receives instructions on the radical modifications necessary to reduce the MAV's weight to enable it to intercept \"Hermes\" during its flyby. The modifications include removing the front of the MAV, which Watney has to cover with Hab canvas. After takeoff, the canvas tears, creating extra drag and leaving the MAV too low for the rendezvous. Lewis hastily improvises a plan to intercept the MAV by firing \"Hermes\" attitude thrusters and then blowing a hole in the front airlock with an improvised sugar-and-liquid-oxygen bomb, using the thrust from the escaping air to reduce speed. Beck, the \"Hermes\" EVA specialist, uses a Manned Maneuvering Unit, MMU, on a tether to reach Watney and bring him back to \"Hermes\". In a final log entry, Watney expresses his joy at being rescued, reflecting on the human instinct to help those in need.\n\nThe major characters in the novel are:\n\n\nAndy Weir, the son of a particle physicist, has a background in computer science. He began writing the book in 2009, researching related material so that it would be as realistic as possible and based on existing technology. Weir studied orbital mechanics, astronomy, and the history of human spaceflight. He said he knows the exact date of each day in the book. He specifically avoided physically describing the characters when not necessary for the plot.\n\nHaving been rebuffed by literary agents when trying to get prior books published, Weir decided to put the book online in serial format one chapter at a time for free at his website. At the request of fans, he made an Amazon Kindle version available at 99 cents (the minimum allowable price he could set). The Kindle edition rose to the top of Amazon's list of best-selling science-fiction titles, where it sold 35,000 copies in three months, more than had been previously downloaded free. This garnered the attention of publishers: Podium Publishing, an audiobook publisher, signed for the audiobook rights in January 2013. Weir sold the print rights to Crown in March 2013 for over US$100,000.\n\nThe book debuted on the \"New York Times\" Best Seller list on March 2, 2014, in the hardcover fiction category at twelfth position and remained on this list for four weeks without going above eleventh position. The trade paperback edition of the novel debuted on \"The New York Times\" Best Seller list on November 16, 2014, in the paperback trade fiction category at eighth position. It gradually rose to the top position for the week of June 28, 2015, before dropping down to number two for nine weeks, during which it was displaced by E. L. James' \"Grey\", before returning to the top position on September 6, 2015. The book remained continuously at the number one position for 12 weeks before it was displaced on November 22, 2015, by Nora Roberts' \"Stars of Fortune\" for two weeks. The trade paperback returned to the top position for the third and final time on December 6, 2015, for six weeks before it was finally replaced on January 24, 2016. The trade paperback's final appearance on the list occurred on April 24, 2016, 76 weeks after its debut in this category. Overall, the trade paperback edition was on the top of its \"New York Times\" best seller category for a total of 19 out of 76 weeks that the edition was listed.\n\n\"The Martian\" was published in print by Crown on February 11, 2014. An audiobook edition, narrated by R. C. Bray and released by Podium Publishing, preceded the print release in March 2013 on Audible.com, and was later followed with an MP3 CD in association with Brilliance Audio. The audiobook was nominated and won an Audie Award (2014) in the Science Fiction category. A Classroom Edition, published by Broadway Books in May 2016, contains educational materials and uses school-appropriate language.\n\nIn a starred review, \"Publishers Weekly\" said that \"Weir laces the technical details with enough keen wit to satisfy hard science fiction fan and general reader alike.\" \"Kirkus Reviews\" called \"The Martian\" \"Sharp, funny and thrilling, with just the right amount of geekery\". \"The Wall Street Journal\" called the book \"the best pure sci-fi novel in years.\" \"Entertainment Weekly\" gave the novel a grade of \"B\", describing it as \"an impressively geeky debut novel\" but saying Weir \"stumbles with his secondary characters\". \"USA Today\" rated \"The Martian\" three out of four stars, calling it \"terrific stuff, a crackling good read\" but noting that \"Mark's unflappability, perhaps the book's biggest asset, is also its greatest weakness. He's a wiseacre with a tendency to steer well clear of existential matters.\" \"Amazing Stories\" commented, \"Andy Weir's \"The Martian\" will leave you as breathless as if you'd been dropped on the Martian surface without a suit\".\n\nThe Japanese translation of the novel won the Seiun Award for Best Translated Long Story in 2015.\n\n\"Solanum watneyi\", a species of bush tomato from Australia, was named after the fictional botanist. It is a member of the same genus as the potato, \"Solanum\".\n\nAt the 2016 Hugo Awards, Andy Weir won the John W. Campbell Award for Best New Writer for \"The Martian\". The screenplay adapted from the novel additionally won Best Dramatic Presentation, Long Form.\n\nIn March 2013, Twentieth Century Fox optioned the film rights, and hired screenwriter Drew Goddard to adapt and direct the film. In May 2014, it was reported that Ridley Scott was in negotiations to direct an adaptation that would star Matt Damon as Mark Watney. On September 3, 2014, Jessica Chastain joined the film as Commander Lewis. The ensemble cast also includes Kristen Wiig, Jeff Daniels, Michael Peña, Kate Mara, Sean Bean, Sebastian Stan and Chiwetel Ejiofor. The film was released on October 2, 2015.\n\nOn December 5, 2014, the Orion spacecraft took the cover page of \"The Martian\" script on the first test flight of the unmanned Exploration Flight Test 1 (EFT-1). The script was launched atop a Delta IV Heavy on the flight lasting 4 hours and 24 minutes, landing at its target in the Pacific Ocean.\n\nIn October 2015, NASA presented a new web tool to follow Watney's trek across Mars, and details of NASA's next steps, as well as a health hazards report, for a real-world human journey to Mars.\n\n\n"}
{"id": "642896", "url": "https://en.wikipedia.org/wiki?curid=642896", "title": "Thermoacoustics", "text": "Thermoacoustics\n\nThermoacoustics is the interaction between temperature, density and pressure variations of acoustic waves. Thermoacoustic heat engines can readily be driven using solar energy or waste heat and they can be controlled using proportional control. They can use heat available at low temperatures which makes it ideal for heat recovery and low power applications. The components included in thermoacoustic engines are usually very simple compared to conventional engines. The device can easily be controlled and maintained.\n\nThermoacoustic effects can be observed when partly molten glass tubes are connected to glass vessels. Sometimes spontaneously a loud and monotone sound is produced. A similar effect is observed if a stainless steel tube is with one side at room temperature (293 K) and with the other side in contact with liquid helium at 4.2 K. In this case, spontaneous oscillations are observed which are named \"Taconis oscillations\". The mathematical foundation of thermoacoustics is by Nikolaus Rott. Later, the field was inspired by the work of John Wheatley and Swift and his co-workers. Technologically thermoacoustic devices have the advantage that they have no moving parts which makes them attractive for applications where reliability is of key importance.\n\nThermoacoustic-induced oscillations have been observed for centuries. Glass blowers produced heat generated sound when blowing a hot bulb at the end of a cold narrow tube. This phenomenon also has been observed in cryogenic storage vessels, where oscillations are induced by the insertion of a hollow tube open at the bottom end in liquid helium, called Taconis oscillations, but the lack of heat removal system causes the temperature gradient to diminish and acoustic wave to weaken and then to stop completely. Byron Higgins made the first scientific observation of heat energy conversion into acoustical oscillations. He investigated the \"singing flame\" phenomena in a portion of a hydrogen flame in a tube with both ends open.\n\nPhysicist Pieter Rijke introduced this phenomenon into a greater scale by using a heated wire screen to induce strong oscillations in a tube (the Rijke tube). Feldman mentioned in his related review that a convective air current through the pipe is the main inducer of this phenomenon. The oscillations are strongest when the screen is at one fourth of the tube length. Research performed by Sondhauss in 1850 is known to be the first to approximate the modern concept of thermoacoustic oscillation. Sondhauss experimentally investigated the oscillations related to glass blowers. Sondhauss observed that sound frequency and intensity depends on the length and volume of the bulb. Lord Rayleigh gave a qualitative explanation of the Sondhauss thermoacoustic oscillations phenomena, where he stated that producing any type of thermoacoustic oscillations needs to meet a criterion: \"If heat be given to the air at the moment of greatest condensation or taken from it at the moment of greatest rarefaction, the vibration is encouraged\". This shows that he related thermoacoustics to the interplay of density variations and heat injection. The formal theoretical study of thermoacoustics started by Kramers in 1949 when he generalized the Kirchhoff theory of the attenuation of sound waves at constant temperature to the case of attenuation in the presence of a temperature gradient. Rott made a breakthrough in the study and modeling of thermodynamic phenomena by developing a successful linear theory. After that, the acoustical part of thermoacoustics was linked in a broad thermodynamic framework by Swift.\n\nUsually sound is understood in terms of pressure variations accompanied by an oscillating motion of a medium (gas, liquid or solid). In order to understand thermoacoustic machines, it is of importance to focus on the temperature-position variations rather than the usual pressure-velocity variations.\n\nThe sound intensity of ordinary speech is 65 dB. The pressure variations are about 0.05 Pa, the displacements 0.2 μm, and the temperature variations about 40 μK. So, the thermal effects of sound cannot be observed in daily life. However, at sound levels of 180 dB, which are normal in thermoacoustic systems, the pressure variations are 30 kPa, the displacements more than 10 cm, and the temperature variations 24 K.\n\nThe one-dimensional wave equation for sound reads\nwith \"t\" time, \"v\" the gas velocity, \"x\" the position, and \"c\" the sound velocity given by \"c²=γp₀/ρ₀\". For an ideal gas, \"c²=γRT₀/M\" with \"M\" the molar mass. In these expressions, \"p₀\", \"T₀\", and \"ρ₀\" are the average pressure, temperature, and density respectively. In monochromatic plane waves, with angular frequency \"ω\" and with \"ω=kc\", the solution is\nThe pressure variations are given by\nThe deviation \"δx\" of a gas-particle with equilibrium position \"x\" is given by\nand the temperature variations are\nThe last two equations form a parametric representation of a tilted ellipse in the \"δT – δx\" plane with \"t\" as the parameter.\n\nIf formula_4, we are dealing with a pure standing wave. Figure 1a gives the dependence of the velocity and position amplitudes (red curve) and the pressure and temperature amplitudes (blue curve) for this case. The ellipse of the \"δT – δx\" plane is reduced to a straight line as shown in Fig. 1b. At the tube ends \"δx\" =0, so the \"δT – δx\" plot is a vertical line here. In the middle of the tube the pressure and temperature variations are zero, so we have a horizontal line. It can be shown that the power, transported by sound, is given by\nwhere \"γ\" is the ratio of the gas specific heat at fixed pressure to the specific heat at fixed volume and \"A\" is the area of the cross section of the sound duct.\nSince in a standing wave, formula_4, the average energy transport is zero.\n\nIf formula_7 or formula_8, we have a pure traveling wave. In this case, Eqs.(1) and (2) represent circles in the \"δT – δx\" diagram as shown in Fig. 1c, which applies to a pure traveling wave to the right. The gas moves to the right with a high temperature and back with a low temperature, so there is a net transport of energy.\n\nThe thermoacoustic effect inside the stack takes place mainly in the region that is close to the solid walls of the stack. The layers of gas too far away from the stack walls experience adiabatic oscillations in temperature that result in no heat transfer to or from the walls, which is undesirable.Therefore, an important characteristic for any thermoacoustic element is the value of the thermal and viscous penetration depths. The thermal penetration depth \"δ\" is the thickness of the layer of the gas where heat can diffuse through during half a cycle of oscillations. Viscous penetration depth δv is the thickness of the layer where viscosity effect is effective near the boundaries. In case of sound, the characteristic length for thermal interaction is given by the thermal penetration depth \"δ\"\nHere \"κ\" is the thermal conductivity, \"V\" the molar volume, and \"C\" the molar heat capacity at constant pressure. Viscous effects are determined by the viscous penetration depth \"δ\"\nwith \"η\" the gas viscosity and \"ρ\" its density. The Prandtl number of the gas is defined as\nThe two penetration depths are related as follows\nFor many working fluids, like air and helium, \"P\" is of order 1, so the two penetration depths are about equal. For helium at normal temperature and pressure, P≈0.66.\nFor typical sound frequencies the thermal penetration depth is ca. 0.1 mm. That means that the thermal interaction between the gas and a solid surface is limited to a very thin layer near the surface. The effect of thermoacoustic devices is increased by putting a large number of plates (with a plate distance of a few times the thermal penetration depth) in the sound field forming a stack. Stacks play a central role in so-called standing-wave thermoacoustic devices.\n\nAcoustic oscillations in a medium are a set of time depending properties, which may transfer energy along its path. Along the path of an acoustic wave, pressure and density are not the only time dependent property, but also entropy and temperature. Temperature changes along the wave can be invested to play the intended role in the thermoacoustic effect. The interplay of heat and sound is applicable in both conversion ways. The effect can be used to produce acoustic oscillations by supplying heat to the hot side of a stack, and sound oscillations can be used to induce a refrigeration effect by supplying a pressure wave inside a resonator where a stack is located. In a thermoacoustic prime mover, a high temperature gradient along a tube where a gas media is contained induces density variations. Such variations in a constant volume of matter force changes in pressure. The cycle of thermoacoustic oscillation is a combination of heat transfer and pressure changes in a sinusoidal pattern. Self-induced oscillations can be encouraged, according to Lord Raleigh, by the appropriate phasing of heat transfer and pressure changes.\n\nThe thermoacoustic engine (TAE) is a device that converts heat energy into work in the form of acoustic energy. A thermoacoustic engine operates using the effects that arise from the resonance of a standing-wave in a gas. A standing-wave thermoacoustic engine typically has a thermoacoustic element called the \"stack\". A stack is a solid component with pores that allow the operating gas fluid to oscillate while in contact with the solid walls. The oscillation of the gas is accompanied with the change of its temperature. Due to the introduction of solid walls into the oscillating gas, the plate modifies the original, unperturbed temperature oscillations in both magnitude and phase for the gas about a thermal penetration depth δ=√(2k/ω) away from the plate, where k is the thermal diffusivity of the gas and ω=2πf is the angular frequency of the wave. Thermal penetration depth is defined as the distance that heat can diffuse though the gas during a time 1/ω. In air oscillating at 1000 Hz, the thermal penetration depth is about 0.1 mm. Standing-wave TAE must be supplied with the necessary heat to maintain the temperature gradient on the stack. This is done by two heat exchangers on both sides of the stack.\n\nIf we put a thin horizontal plate in the sound field, the thermal interaction between the oscillating gas and the plate leads to thermoacoustic effects. If the thermal conductivity of the plate material would be zero, the temperature in the plate would exactly match the temperature profiles as in Fig. 1b. Consider the blue line in Fig. 1b as the temperature profile of a plate at that position. The temperature gradient in the plate would be equal to the so-called critical temperature gradient. If we would fix the temperature at the left side of the plate at ambient temperature \"T\" (e.g. using a heat exchanger), then the temperature at the right would be below \"T\". In other words: we have produced a cooler. This is the basis of thermoacoustic cooling as shown in Fig. 2b which represents a thermoacoustic refrigerator. It has a loudspeaker at the left. The system corresponds with the left half of Fig. 1b with the stack in the position of the blue line. Cooling is produced at temperature \"T\".\n\nIt is also possible to fix the temperature of the right side of the plate at \"T\" and heat up the left side so that the temperature gradient in the plate would be larger than the critical temperature gradient. In that case, we have made an engine (prime mover) which can e.g. produce sound as in Fig. 2a. This is a so-called thermoacoustic prime mover. Stacks can be made of stainless steel plates but the device works also very well with loosely packed stainless steel wool or screens. It is heated at the left, e.g., by a propane flame and heat is released to ambient temperature by a heat exchanger. If the temperature at the left side is high enough, the system starts to produces a loud sound.\n\nThermoacoustic engines still suffer from some limitations, including that:\nThe performance of thermoacoustic engines usually is characterized through several indicators as follows:\n\nFigure 3 is a schematic drawing of a travelling-wave thermoacoustic engine. It consists of a resonator tube and a loop which contains a regenerator, three heat exchangers, and a bypass loop. A regenerator is a porous medium with a high heat capacity. As the gas flows back and forth through the regenerator, it periodically stores and takes up heat from the regenerator material. In contrast to the stack, the pores in the regenerator are much smaller than the thermal penetration depth, so the thermal contact between gas and material is very good. Ideally, the energy flow in the regenerator is zero, so the main energy flow in the loop is from the hot heat exchanger via the pulse tube and the bypass loop to the heat exchanger at the other side of the regenerator (main heat exchanger). The energy in the loop is transported via a travelling wave as in Fig. 1c, hence the name travelling-wave systems. The ratio of the volume flows at the ends of the regenerator is \"T\"/\"T\", so the regenerator acts as a volume-flow amplifier.\nJust like in the case of the standing-wave system, the machine \"spontaneously\" produces sound if the temperature \"T\" is high enough. The resulting pressure oscillations can be used in a variety of ways, such as in producing electricity, cooling, and heat pumping.\n\n\n"}
{"id": "34945713", "url": "https://en.wikipedia.org/wiki?curid=34945713", "title": "Vittra (folklore)", "text": "Vittra (folklore)\n\nA Vittra (plural: Vittror) is a type of Vættr (wight) from northern Sweden. A Vættr is a nature spirit, a type of mythological creature very common in Scandinavian mythology. Elves, Dwarves and Jötnar are Vættir. Experts are not certain if Vittra, Huldra and Näcken can be grouped together or not because all three of them support nature against humans and dangers.\n\nThe Vittra is part of Scandinavian folklore in Sweden, Norway, Denmark, Iceland, the Faroe Islands, and the Swedish speaking parts of Finland. In tales told in the north of Sweden, Vittror often take the place that trolls, tomte and vättar hold in the same stories told in other parts of the country.\n\nVittror live underground, are invisible most of the time and have their own cattle (which are also invisible). Most of the time Vittror are rather distant and do not meddle in human affairs, but are fearsome when enraged. This can be achieved by not respecting them properly, for example by neglecting to perform certain rituals (such as saying \"look out\" when putting out hot water or urinating, so they can move out of the way) or building your home to close to or, even worse, on top of their home, disturbing their cattle or blocking their roads. They can make your life very miserable or even dangerous – they do whatever it takes to drive you away, even arrange accidents that will harm or even kill you. Even in modern days, people have rebuilt or moved houses in order not to block a \"Vittra-way\", or moved from houses that are deemed a \"Vittra-place\" (Vittraställe) because of bad luck – although this is rather uncommon. It is said the Vittra dwells in Norrland, in the high woods or in the fields. However it is unclear if they rule over the field as the skogsrå rules over the grand woods.\n\nThey are experts at milking both their own cows and men's cows. They usually live underground but sometimes they also live in abandoned human chalets. People believed they could sometimes hear the Vittror calling for their cattle or milking, and sometimes even the tinkling of their cow's bell. Vittror are believed to sometimes \"borrow\" cattle that later would be returned to the owner with the ability to give more milk as a sign of gratitude. This tradition is heavily influenced by the fact that it was developed during a time when people let their cattle graze on mountains or in the forest for long periods of the year.\n\n"}
{"id": "306773", "url": "https://en.wikipedia.org/wiki?curid=306773", "title": "Water quality", "text": "Water quality\n\nWater quality refers to the chemical, physical, biological, and radiological characteristics of water. It is a measure of the condition of water relative to the requirements of one or more biotic species and or to any human need or purpose. It is most frequently used by reference to a set of standards against which compliance, generally achieved through treatment of the water, can be assessed. The most common standards used to assess water quality relate to health of ecosystems, safety of human contact, and drinking water.\n\nIn the setting of standards, agencies make political and technical/scientific decisions about how the water will be used. In the case of natural water bodies, they also make some reasonable estimate of pristine conditions. Natural water bodies will vary in response to environmental conditions. Environmental scientists work to understand how these systems function, which in turn helps to identify the sources and fates of contaminants. Environmental lawyers and policymakers work to define legislation with the intention that water is maintained at an appropriate quality for its identified use.\n\nThe vast majority of surface water on the Earth is neither potable nor toxic. This remains true when seawater in the oceans (which is too salty to drink) is not counted. Another general perception of water quality is that of a simple property that tells whether water is polluted or not. In fact, water quality is a complex subject, in part because water is a complex medium intrinsically tied to the ecology of the Earth. Industrial and commercial activities (e.g. manufacturing, mining, construction, transport) are a major cause of water pollution as are runoff from agricultural areas, urban runoff and discharge of treated and untreated sewage.\n\nThe parameters for water quality are determined by the intended use. Work in the area of water quality tends to be focused on water that is treated for human consumption, industrial use, or in the environment.\n\nContaminants that may be in untreated water include microorganisms such as viruses, protozoa and bacteria; inorganic contaminants such as salts and metals; organic chemical contaminants from industrial processes and petroleum use; pesticides and herbicides; and radioactive contaminants. Water quality depends on the local geology and ecosystem, as well as human uses such as sewage dispersion, industrial pollution, use of water bodies as a heat sink, and overuse (which may lower the level of the water).\n\nThe United States Environmental Protection Agency (EPA) limits the amounts of certain contaminants in tap water provided by US public water systems. The Safe Drinking Water Act authorizes EPA to issue two types of standards:\n\nThe U.S. Food and Drug Administration (FDA) regulations establish limits for contaminants in bottled water that must provide the same protection for public health. Drinking water, including bottled water, may reasonably be expected to contain at least small amounts of some contaminants. The presence of these contaminants does not necessarily indicate that the water poses a health risk.\n\nIn urbanized areas around the world, water purification technology is used in municipal water systems to remove contaminants from the source water (surface water or groundwater) before it is distributed to homes, businesses, schools and other recipients. Water drawn directly from a stream, lake, or aquifer and that has no treatment will be of uncertain quality.\n\nDissolved minerals may affect suitability of water for a range of industrial and domestic purposes. The most familiar of these is probably the presence of ions of calcium (Ca) and magnesium (Mg) which interfere with the cleaning action of soap, and can form hard sulfate and soft carbonate deposits in water heaters or boilers. Hard water may be softened to remove these ions. The softening process often substitutes sodium cations. Hard water may be preferable to soft water for human consumption, since health problems have been associated with excess sodium and with calcium and magnesium deficiencies. Softening decreases nutrition and may increase cleaning effectiveness. Various industries' wastes and effluents can also pollute the water quality in receiving bodies of water.\n\nEnvironmental water quality, also called ambient water quality, relates to water bodies such as lakes, rivers, and oceans. Water quality standards for surface waters vary significantly due to different environmental conditions, ecosystems, and intended human uses. Toxic substances and high populations of certain microorganisms can present a health hazard for non-drinking purposes such as irrigation, swimming, fishing, rafting, boating, and industrial uses. These conditions may also affect wildlife, which use the water for drinking or as a habitat. Modern water quality laws generally specify protection of fisheries and recreational use and require, as a minimum, retention of current quality standards.\n\nThere is some desire among the public to return water bodies to pristine, or pre-industrial conditions. Most current environmental laws focus on the designation of particular uses of a water body. In some countries these designations allow for some water contamination as long as the particular type of contamination is not harmful to the designated uses. Given the landscape changes (e.g., land development, urbanization, clearcutting in forested areas) in the watersheds of many freshwater bodies, returning to pristine conditions would be a significant challenge. In these cases, environmental scientists focus on achieving goals for maintaining healthy ecosystems and may concentrate on the protection of populations of endangered species and protecting human health.\n\nThe complexity of water quality as a subject is reflected in the many types of measurements of water quality indicators. The most accurate measurements of water quality are made on-site, because water exists in equilibrium with its surroundings. Measurements commonly made on-site and in direct contact with the water source in question include temperature, pH, dissolved oxygen, conductivity, oxygen reduction potential (ORP), turbidity, and Secchi disk depth.\n\nMore complex measurements are often made in a laboratory requiring a water sample to be collected, preserved, transported, and analyzed at another location. The process of water sampling introduces two significant problems:\nSample preservation may partially resolve the second problem. A common procedure is keeping samples cold to slow the rate of chemical reactions and phase change, and analyzing the sample as soon as possible; but this merely minimizes the changes rather than preventing them. A useful procedure for determining influence of sample containers during delay between sample collection and analysis involves preparation for two artificial samples in advance of the sampling event. One sample container is filled with water known from previous analysis to contain no detectable amount of the chemical of interest. This sample, called a \"blank\", is opened for exposure to the atmosphere when the sample of interest is collected, then resealed and transported to the laboratory with the sample for analysis to determine if sample holding procedures introduced any measurable amount of the chemical of interest. The second artificial sample is collected with the sample of interest, but then \"spiked\" with a measured additional amount of the chemical of interest at the time of collection. The blank and spiked samples are carried with the sample of interest and analyzed by the same methods at the same times to determine any changes indicating gains or losses during the elapsed time between collection and analysis.\n\nInevitably after events such as earthquakes and tsunamis, there is an immediate response by the aid agencies as relief operations get underway to try and restore basic infrastructure and provide the basic fundamental items that are necessary for survival and subsequent recovery. Access to clean drinking water and adequate sanitation is a priority at times like this. The threat of disease increases hugely due to the large numbers of people living close together, often in squalid conditions, and without proper sanitation.\n\nAfter a natural disaster, as far as water quality testing is concerned there are widespread views on the best course of action to take and a variety of methods can be employed. The key basic water quality parameters that need to be addressed in an emergency are bacteriological indicators of fecal contamination, free chlorine residual, pH, turbidity and possibly conductivity/total dissolved solids. There are a number of portable water test kits on the market widely used by aid and relief agencies for carrying out such testing.\n\nAfter major natural disasters, a considerable length of time might pass before water quality returns to pre-disaster levels. For example, following the 2004 Indian Ocean tsunami the Colombo-based International Water Management Institute (IWMI) monitored the effects of saltwater and concluded that the wells recovered to pre-tsunami drinking water quality one and a half years after the event. IWMI developed protocols for cleaning wells contaminated by saltwater; these were subsequently officially endorsed by the World Health Organization as part of its series of Emergency Guidelines.\n\nThe simplest methods of chemical analysis are those measuring chemical elements without respect to their form. Elemental analysis for oxygen, as an example, would indicate a concentration of 890 g/L (grams per litre) of water sample because oxygen (O) has 89% mass of the water molecule (HO). The method selected to measure dissolved oxygen should differentiate between diatomic oxygen and oxygen combined with other elements. The comparative simplicity of elemental analysis has produced a large amount of sample data and water quality criteria for elements sometimes identified as heavy metals. Water analysis for heavy metals must consider soil particles suspended in the water sample. These suspended soil particles may contain measurable amounts of metal. Although the particles are not dissolved in the water, they may be consumed by people drinking the water. Adding acid to a water sample to prevent loss of dissolved metals onto the sample container may dissolve more metals from suspended soil particles. Filtration of soil particles from the water sample before acid addition, however, may cause loss of dissolved metals onto the filter. The complexities of differentiating similar organic molecules are even more challenging.\n\nMaking these complex measurements can be expensive. Because direct measurements of water quality can be expensive, ongoing monitoring programs are typically conducted by government agencies. However, there are local volunteer programs and resources available for some general assessment. Tools available to the general public include on-site test kits, commonly used for home fish tanks, and biological assessment procedures.\n\nAlthough water quality is usually sampled and analyzed at laboratories, nowadays, citizens demand real-time information about the water they are drinking. During the last years, several companies are deploying worldwide real-time remote monitoring systems for measuring water pH, turbidity or dissolved oxygen levels.\n\nThe following is a list of indicators often measured by situational category:\n\n\n\n\nBiological monitoring metrics have been developed in many places, and one widely used measure is the presence and abundance of members of the insect orders Ephemeroptera, Plecoptera and Trichoptera (common names are, respectively, mayfly, stonefly and caddisfly). EPT indexes will naturally vary from region to region, but generally, within a region, the greater the number of taxa from these orders, the better the water quality. Organisations in the United States, such as EPA. offer guidance on developing a monitoring program and identifying members of these and other aquatic insect orders. Many US wastewater dischargers (e.g., factories, power plants, refineries, mines, municipal sewage treatment plants) are required to conduct periodic whole effluent toxicity (WET) tests.\n\nIndividuals interested in monitoring water quality who cannot afford or manage lab scale analysis can also use biological indicators to get a general reading of water quality. One example is the IOWATER volunteer water monitoring program of Iowa, which includes a benthic macroinvertebrate indicator key.\n\nBivalve molluscs are largely used as bioindicators to monitor the health of aquatic environments in both fresh water and the marine environments. Their population status or structure, physiology, behaviour or the level of contamination with elements or compounds can indicate the state of contamination status of the ecosystem. They are particularly useful since they are sessile so that they are representative of the environment where they are sampled or placed. A typical project is the U.S. Mussel Watch Programme, but today they are used worldwide.\n\nThe Southern African Scoring System (SASS) method is a biological water quality monitoring system based on the presence of benthic macroinvertebrates. The SASS aquatic biomonitoring tool has been refined over the past 30 years and is now on the fifth version (SASS5) which has been specifically modified in accordance with international standards, namely the ISO/IEC 17025 protocol. The SASS5 method is used by the South African Department of Water Affairs as a standard method for River Health Assessment, which feeds the national River Health Programme and the national Rivers Database.\n\n\nThe water policy of the European Union is primarily codified in three directives:\n\n\nWater quality guidelines for South Africa are grouped according to potential user types (e.g. domestic, industrial) in the 1996 Water Quality Guidelines. Drinking water quality is subject to the South African National Standard (SANS) 241 Drinking Water Specification.\n\nIn England and Wales acceptable levels for drinking water supply are listed in the \"Water Supply (Water Quality) Regulations 2000.\"\n\nIn the United States, Water Quality Standards are defined by state agencies for various water bodies, guided by the desired uses for the water body (e.g., fish habitat, drinking water supply, recreational use). The Clean Water Act (CWA) requires each governing jurisdiction (states, territories, and covered tribal entities) to submit a set of biennial reports on the quality of water in their area. These reports are known as the 303(d) and 305(b) reports, named for their respective CWA provisions, and are submitted to, and approved by, EPA. These reports are completed by the governing jurisdiction, typically a . EPA recommends that each state submit a single \"Integrated Report\" comprising its list of impaired waters and the status of all water bodies in the state. The \"National Water Quality Inventory Report to Congress\" is a general report on water quality, providing overall information about the number of miles of streams and rivers and their aggregate condition. The CWA requires states to adopt standards for each of the possible designated uses that they assign to their waters. Should evidence suggest or document that a stream, river or lake has failed to meet the water quality criteria for one or more of its designated uses, it is placed on a list of impaired waters. Once a state has placed a water body on this list, it must develop a management plan establishing Total Maximum Daily Loads (TMDLs) for the pollutant(s) impairing the use of the water. These TMDLs establish the reductions needed to fully support the designated uses.\n\nDrinking water standards, which are applicable to public water systems, are issued by EPA under the Safe Drinking Water Act.\n\n\n\n\n\n"}
{"id": "50578652", "url": "https://en.wikipedia.org/wiki?curid=50578652", "title": "Δ34S", "text": "Δ34S\n\nThe δS (pronounced \"delta 34 S\") value is a standardized method for reporting measurements of the ratio of two stable isotopes of sulfur, S:S, in a sample against the equivalent ratio in a known reference standard. Presently, the most commonly used standard is Vienna-Canyon Diablo Troilite (VCDT). Results are reported as variations from the standard ratio in parts per thousand, per mil or \"per mille\", using the ‰ symbol. Heavy and light sulfur isotopes fractionate at different rates and the resulting δS values, recorded in marine sulfate or sedimentary sulfides, have been studied and interpreted as records of the changing sulfur cycle throughout the earth's history.\n\nOf the 25 known isotopes of sulfur, four are stable. In order of their abundance, those isotopes are S (94.93%), S (4.29%), S (0.76%), and S (0.02%). The δS value refers to a measure of the ratio of the two most common stable sulfur isotopes, S:S, as measured in a sample against that same ratio as measured in a known reference standard. The lowercase delta character is used by convention, to be consistent with use in other areas of stable isotope chemistry. That value can be calculated in per mil (‰, parts per thousand) as:\n\nLess commonly, if the appropriate isotope abundances are measured, similar formulae can be used to quantify ratio variations between S and S, and S and S, reported as δS and δS, respectively.\n\nSulfur from meteorites was determined in the early 1950s to be an adequate reference standard because it exhibited a small variability in isotopic ratios. It was also believed that because of their extraterrestrial provenances, meteors represented primordial terrestrial isotopic conditions. During a meeting of the National Science Foundation in April 1962, troilite from the Canyon Diablo meteorite found in Arizona, US, was established as the standard with which δS values (and other sulfur stable isotopic ratios) could be calculated. Known as Canyon Diablo Troilite (CDT), the standard was established as having a S:S ratio of 22.220 and was used for around three decades. In 1993, the International Atomic Energy Agency (IAEA) established a new standard, Vienna-CDT (VCDT), based on artificially prepared silver sulfide (IAEA-S-1) that was defined to have a δS value of −0.3‰. In 1994, the original CDT material was found not to be isotopically homogeneous, with internal variations as great as 0.4‰, confirming its unsuitability as a reference standard.\n\nTwo mechanisms of fractionation occur that alter sulfur stable isotope ratios: kinetic effects, especially due to the metabolism of sulfate-reducing bacteria, and isotope exchange reactions that occur between sulfide phases based on temperature. With VCDT as the reference standard, natural δS value variations have been recorded between +120‰ and -65‰.\n\nThe presence of sulfate-reducing bacteria, which reduce sulfate () to hydrogen sulfide (HS), has played a significant role in the oceanic δS value throughout the earth's history. Sulfate-reducing bacteria metabolize S more readily than S, resulting in an increase in the value of the δS in the remaining sulfate in the seawater. Archean pyrite found in barite in the Warrawoona Group, Western Australia, with sulfur fractionations as great as 21.1‰ hint at the presence of sulfate-reducers as early as .\n\nThe δS value, recorded by sulfate in marine evaporites, can be used to chart the sulfur cycle throughout earth's history. The Great Oxygenation Event around altered the sulfur cycle radically, as increased atmospheric oxygen permitted an increase in the mechanisms that could fractionate sulfur isotopes, leading to an increase in the δS value from ~0‰ pre-oxygenation. Approximately , the δS values in seawater sulfates began to vary more and those in sedimentary sulfates grew more negative. Researchers have interpreted this excursion as indicative of an increase in water column oxygenation with continued periods of anoxia in the deepest waters. Modern seawater sulfate δS values are consistently 21.0 ± 0.2‰ across the world's oceans, while sedimentary sulfides vary widely. Seawater sulfate δS and δO values exhibit similar trends not seen in sedimentary sulfide minerals.\n\n\nCitations\n"}
