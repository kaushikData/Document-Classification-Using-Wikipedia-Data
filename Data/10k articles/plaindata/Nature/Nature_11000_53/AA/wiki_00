{"id": "33375541", "url": "https://en.wikipedia.org/wiki?curid=33375541", "title": "(416151) 2002 RQ25", "text": "(416151) 2002 RQ25\n\n orbits the Sun at a distance of 0.8–1.5 AU once every 1 years and 2 months (428 days). Its orbit has an eccentricity of 0.31 and an inclination of 5° with respect to the ecliptic.\n\nThe asteroid's minimum orbit intersection distance with Earth is , which is currently exactly at the threshold limit of 0.05 AU (or about 19.5 lunar distances) to make it a potentially hazardous object.\n\nThe carbonaceous C-type asteroid is also classified as a C/X-type body according to the survey carried out by NASA's Spitzer Space Telescope.\n\nA rotational lightcurve of was obtained from photometric observations made by American astronomer Brian Warner at his Palmer Divide Observatory, Colorado, in February 2015. The ambiguous lightcurve rendered a rotation period of hours with a brightness variation of 0.72 magnitude (), while a second solution gave 6.096 hours (or half of the first period) with an amplitude of 0.43.\n\nThe \"Collaborative Asteroid Lightcurve Link\" assumes a standard albedo for stony asteroids of 0.20 and calculates diameter of 225 meters with an absolute magnitude of 20.6.\n\nAs of 2017, this minor planet remains unnamed.\n\n \n"}
{"id": "42113039", "url": "https://en.wikipedia.org/wiki?curid=42113039", "title": "2014 DX110", "text": "2014 DX110\n\n came to opposition (furthest elongation in the sky from the Sun) on 15 February 2014, but the asteroid had a very faint apparent magnitude of about 23 and was only 10 degrees from the full moon. The asteroid was discovered on 28 February 2014 by Pan-STARRS at an apparent magnitude of 20 using a Ritchey–Chrétien telescope.\n\nOn 5 March 2014 at 21:00 UT the asteroid passed from Earth and reached about apparent magnitude 15. At 22:22 UT it passed from the Moon. By 6 March 2014 18:00 UT, the asteroid was less than 30 degrees from the Sun and dimming significantly.\n\nIt has an observation arc of 5 days with an uncertainty parameter of 6. It was removed from the JPL Sentry Risk Table on 5 March 2014 using JPL solution 3 with an observation arc of 5 days. When the asteroid only had an observation arc of 4 days, virtual clones of the asteroid that fit the uncertainty region in the known trajectory showed a 1 in 10 million chance that the asteroid could impact Earth on 4 March 2046. With a 2046 Palermo Technical Scale of −7.11, the odds of impact by in 2046 were about 13 million times less than the background hazard level of Earth impacts which is defined as the average risk posed by objects of the same size or larger over the years until the date of the potential impact. Using the nominal orbit, NEODyS shows that the asteroid will be from Earth on 4 March 2046.\n\n"}
{"id": "48493924", "url": "https://en.wikipedia.org/wiki?curid=48493924", "title": "Acmella nana", "text": "Acmella nana\n\nAcmella nana is a species of land snail discovered from Borneo, Malaysia, in 2015. It was described by Jaap J. Vermeulen of the JK Art and Science in Leiden, Thor-Seng Liew of the Institute for Tropical Biology and Conservation at the Universiti Malaysia Sabah, and Menno Schilthuizen of the Naturalis Biodiversity Center in Leiden. It was named \"nana\" (Latin for \"dwarf\") due to its minute size. Measuring only 0.7 millimeters in size, it is the smallest known land snail as of 2015. It surpasses the earlier record attributed to \"Angustopila dominikae\", which is 0.86 mm in size, described from China in September 2015.\n\nThe genus name \"Acmella\" is derived from a Greek word \"akme\" meaning \"(the highest) point, edge or peak of anything.\" The species name \"nana\" was derived from a Latin word \"nanus\" meaning \"dwarf\", and was chosen because of its small size.\n\n\"Acmella nana\" has a shell, which is whitish in colour and has a shiny appearance. The shell is translucent and measures 0.50 to 0.60 mm in width, and 0.60 to 0.79 mm in height. In average the size is 0.7 mm. Due to its size, it cannot be directly noticed by naked eye, and can be seen clearly under microscope. It has 2 to 3 whorls, and the aperture opening is 0.26-0.30 mm wide and 0.30-0.37 mm high.\n\n\"Acmella nana\" was discovered from limestone hills in Borneo. Knowing that limestone and snail shells are both composed of calcium carbonate, the research team led by two Dutch biologists Jaap J. Vermeulen and Menno Schilthuizen, and a Malaysian biologist Thor-Seng Liew collected soil and litter and dirt from the cliffs. Then they separated the larger particles from the smaller ones using sieves. They put the larger particles in a bucket of water and stirred them. The minerals such as clay and sand settled at the bottom, while the shells, being buoyant, float up on the surface. This is because, although the shells are chemically same as the minerals, their internal cavities contain air pockets. The shells were then examined under microscope. The taxonomic description was published in the 2 November 2015 issue of \"ZooKeys\", and the paper also included a report of other 47 new species of snails. Schilthuizen remarked, saying, \"Our paper was in review when that paper on \"Angustopila dominikae\" came out and it was only then that we realized that one of 'our' species was actually smaller.\" The main specimen (holotype) was collected by Vermeulen from the Niah Caves in Sarawak, Malaysia. Other specimens were from Sabah.\n\nSince the specimens are only the shells, and not living snails, it is not possible to know details of the biology. But a closely related species \"Acmella polita\" is known in the same area. This snail eats thin films of bacteria and fungi growing on the limestone walls inside the caves. The researchers speculated that \"Acmella nana\" does the same. The presence of an opening called operculum on the shell suggests that it may also possess gills. Gills would be the respiratory organs for their wet environment. Such gills are known in aquatic snails. It is also assumed that the species is distributed in other parts of Borneo, and that their survival is not a serious threat. However, according to Schilthuizen, they can become threatened due to heavy quarrying in these limestone hills.\n"}
{"id": "9433475", "url": "https://en.wikipedia.org/wiki?curid=9433475", "title": "Atlantic dry forests", "text": "Atlantic dry forests\n\nThe Atlantic dry forests are a tropical dry forest ecoregion of the Atlantic Forest Biome, located in eastern Brazil.\n\nThe Atlantic dry forests cover an area of , lying between the Cerrado savannas of central Brazil and the Caatinga dry shrublands of northeastern Brazil. The Atlantic dry forests stretch from northern Minas Gerais state across western Bahia state into central Piauí. The Atlantic dry forests generally lie along the upper São Francisco River of Minas Gerais and Bahia, and in the basin of the Gurguéia River in Piauí. A large enclave of Atlantic dry forest lies on the Chapada Diamantina of east-central Bahia.\n\nThe Atlantic dry forests are dense, with deciduous and semi-deciduous trees reaching up to 25 to 30 meters in height. The Barriguda Tree, \"Cavanillesia arborea\", is a dry forest tree species distinguished by its huge, bottle-shaped trunk which reaches up to 1.5 meters in diameter.\n\n"}
{"id": "11726208", "url": "https://en.wikipedia.org/wiki?curid=11726208", "title": "Blue Latitudes", "text": "Blue Latitudes\n\nBlue Latitudes: Boldly Going Where Captain Cook Has Gone Before (United States), or Into the Blue: Boldly Going Where Captain Cook Has Gone Before (Australia), is a travel book by Tony Horwitz, published in 2002.\n\nIn it, the Pulitzer Prize–winning journalist travels to various parts of the world, following in the footsteps of explorer James Cook. The book compares the current conditions of the places Cook visited to what Cook documented at the time, and describes the different legacies Cook has left behind.\n\nHorowitz begins with his experience as a volunteer deckhand on the replica of HM Bark \"Endeavour\". Some of the places Horowitz visits in his travels include Australia, the small island nation of Niue, the Society Islands, Tonga, New Zealand, the birthplace and home of Cook in North Yorkshire England, Alaska, and Hawaii.\n"}
{"id": "15053805", "url": "https://en.wikipedia.org/wiki?curid=15053805", "title": "Bogotá savanna", "text": "Bogotá savanna\n\nThe Bogotá savanna is a montane savanna, located in the southwestern part of the Altiplano Cundiboyacense in the center of Colombia. The Bogotá savanna has an extent of and an average altitude of . The savanna is situated in the Eastern Ranges of the Colombian Andes.\n\nThe Bogotá savanna is crossed from northeast to southwest by the long Bogotá River, which at the southwestern edge of the plateau forms the Tequendama Falls (\"Salto del Tequendama\"). Other rivers, such as the Subachoque, Bojacá, Fucha, Soacha and Tunjuelo Rivers, tributaries of the Bogotá River, form smaller valleys with very fertile soils dedicated to agriculture and cattle-breeding.\n\nBefore the Spanish conquest of the Bogotá savanna, the area was inhabited by the indigenous Muisca, who formed a loose confederation of various \"caciques\", named the Muisca Confederation. The Bogotá savanna, known as \"Bacatá\", was ruled by the \"zipa\". The people specialised in agriculture, the mining of emeralds, trade and especially the extraction of rock salt from rocks in Zipaquirá, Nemocón, Tausa and other areas on the Bogotá savanna. The salt extraction, a task exclusively of the Muisca women, gave the Muisca the name \"The Salt People\". \n\nIn April 1536, a group of around 800 conquistadors left the relative safety of the Caribbean coastal city of Santa Marta to start a strenuous expedition up the Magdalena River, the main fluvial artery of Colombia. Word got around among the Spanish colonisers that deep in the unknown Andes, a rich area with an advanced civilisation must exist. These tales bore the -not so much- legend of \"El Dorado\"; the city or man of gold. The Muisca, skilled goldworkers, held a ritual in Lake Guatavita where the new \"zipa\" would cover himself in gold dust and jump from a raft into the cold waters of the high lake to the northeast of the Bogotá savanna.\n\nAfter a journey of almost a year, where the Spanish lost over 80% of their soldiers, the conquistadors following the Suárez River, reached the Bogotá savanna in March 1537. The \"zipa\" who ruled the Bogotá savanna at the arrival of the Spanish was Tisquesusa. The Muisca posed little resistance to the Spanish strangers and Tisquesusa was defeated in April 1537 in Funza, in the centre of the savanna. He fled towards the western hills and died of his wounds in Facatativá, on the southwestern edge of the Bogotá savanna. The Spanish conquistador Gonzalo Jiménez de Quesada established the New Kingdom of Granada with capital Santa Fe de Bogotá on August 6, 1538. This started a process of colonisation, evangelisation and submittance of the Muisca to the new rule. Between 65 and 80% of the indigenous people perished due to European diseases as smallpox and typhus. The Spanish introduced new crops, replacing many of the New World crops that the Muisca cultivated.\n\nOver the course of the 16th to early 20th century, the Bogotá savanna was sparsely populated and industrialised. The rise in population during the twentieth century and the expansion of agriculture and urbanisation reduced the biodiversity and natural habitat of the Bogotá savanna severely. Today, the Metropolitan Area of Bogotá on the Bogotá savanna hosts more than ten million people. Bogotá is the biggest city worldwide at altitudes above . The many rivers on the savanna are highly contaminated and efforts to solve the environmental problems are conducted in the 21st century.\n\nThe Bogotá savanna is named after Bogotá, which is derived from Muysccubun \"Bacatá\", which means \"(Enclosure) outside of the farm fields\".\n\nThe Bogotá savanna is the southwestern part of the larger Andean plateau, the Altiplano Cundiboyacense. The savanna is a montane savanna, bordered to the east by the Eastern Hills, the Sumapaz mountains in the south, the hills of Tausa and Suesca in the north and western hills of Cundinamarca in the west. The total surface area is .\n\nThe average temperature of the plateau is , but this can fluctuate between . The dry and rainy seasons alternate frequently during the year. The driest months are December, January, February and March. During the rainy months, the temperature tends to be more stable with variations between . June, July and August are the months that present the largest variations of temperature, and during the morning frost in the higher terrains surrounding the savanna is possible. Sometimes also ground frost is present, which has a negative impact on agriculture. Hail is a relatively common phenomenon on the savanna.\n\n\n\n\n\nThere is a system of wetlands (\"humedales\") that regulate the soil moisture acting like sponges for the rain waters. Fifteen wetlands have a protected status, with various wetlands as unprotected. In 1950, the total surface area of the wetlands amounted to , but due to the urbanisation of the Colombian capital the total area has been reduced to .\n\nDespite the continuous urbanisation and industrial activities, the Bogotá savanna is a rich biodiverse area with many bird species registered. The diversity of mammals, amphibians and reptiles is much lower. Before the arrival of the European colonisers, the savanna was populated predominantly by white-tailed deer, the main ingredient of the Muisca cuisine. Today, this species of deer, as well as the once common spectacled bear, is restricted to protected areas surrounding the Bogotá savanna. The Thomas van der Hammen Natural Reserve is a protected area in the north of Bogotá.\n\nThe earliest confirmed inhabitation of present-day Colombia was on the Bogotá savanna with sites El Abra, Tequendama and Tibitó, where semi-nomadic hunter-gatherers lived in caves and rock shelters. One of the first evidences of settlement in open area space was Aguazuque, whose oldest dated remains are analysed to be 5000 years old. This prehistorical preceramic period was followed by the Herrera Period, commonly defined from 800 BCE to 800 AD.\n\nAt the arrival of the Spanish conquistadors, the region was inhabited by the Muisca who lived in hundreds of small villages scattered across the plateau. These villages were individually ruled by \"caciques\" who at the same time paid tribute to the \"zipa\", ruler of Bacatá. The Muisca were known as \"The Salt People\", thanks to their extraction of rock salt from brines in large pots heated over fires. This process was the exclusive task of the Muisca women.\n\nThe economy of the Muisca, meaning \"person\" or \"people\" in their indigenous version of Chibcha; Muysccubun, was self-sufficient due to the advanced agriculture on the fertile soils of the frequently flooding Bogotá savanna. More tropical and subtropical agricultural products as avocadoes and cotton were traded with their neighbours, in particular the Guane and Lache in the north and northeast and the Guayupe, Achagua and Tegua in the east.\n\nThe Muisca were known as skilled goldworkers, represented in the famous Muisca raft, that symbolises the initiation ritual of the new \"zipa\" in Lake Guatavita. This ritual, where the \"zipa\" covered himself in gold dust and jumped in the altitude lake, gave rise to the -not so much- legend of \"El Dorado\".\n\nIn April 1536, a group of around 800 conquistadors left the relative safety of the Caribbean coastal city of Santa Marta to start a strenuous expedition up the Magdalena River, the main fluvial artery of Colombia. Word got around among the Spanish colonisers that deep in the unknown Andes, a rich area with an advanced civilisation must exist. These tales bore the -not so much- legend of \"El Dorado\"; the city or man of gold. The Muisca, skilled goldworkers, held a ritual in Lake Guatavita where the new \"zipa\" would cover himself in gold dust and jump from a raft into the cold waters of the high lake to the northeast of the Bogotá savanna.\n\nAfter a journey of almost a year, where the Spanish lost over 80% of their soldiers, the conquistadors following the Suárez River, reached the Bogotá savanna in March 1537. The \"zipa\" who ruled the Bogotá savanna at the arrival of the Spanish was Tisquesusa. The Muisca posed little resistance to the Spanish strangers and Tisquesusa was defeated in April 1537 in Funza, in the centre of the savanna. He fled towards the western hills and died of his wounds in Facatativá, on the southwestern edge of the Bogotá savanna. The Spanish conquistador Gonzalo Jiménez de Quesada established the New Kingdom of Granada with capital Santa Fe de Bogotá on August 6, 1538. This started a process of colonisation, evangelisation and submittance of the Muisca to the new rule. Between 65 and 80% of the indigenous people perished due to European diseases as smallpox and typhus. The Spanish introduced new crops, replacing many of the New World crops that the Muisca cultivated.\n\nThe Spanish colonizers engaged in the construction of Spanish-style towns to replace all the indigenous villages and in the process of assimilation and religious convert of the Muisca. The majority of those villages kept their indigenous names, but some were slightly modified in time, like \"Suacha\" which became Soacha, \"Hyntiba\" becoming Fontibón and \"Bacatá\" becoming Bogotá.\n\nOver the course of the 16th to early 20th century, the Bogotá savanna was sparsely populated and industrialised. The rise in population during the twentieth century and the expansion of agriculture and urbanisation reduced the biodiversity and natural habitat of the Bogotá savanna severely. Today, the Metropolitan Area of Bogotá on the Bogotá savanna hosts more than ten million people. Bogotá is the biggest city worldwide at altitudes above . The many rivers on the savanna are highly contaminated and efforts to solve the environmental problems are conducted in the 21st century.\n\nThe main cities of the Bogotá savanna, in addition to the capital city of Bogotá, are: Mosquera, Soacha, Madrid, Funza, Facatativá, Subachoque, El Rosal, Tabio, Tenjo, Cota, Chía, Cajicá, Zipaquirá, Nemocón, Sopó, Tocancipá, Gachancipá, Sesquilé, Suesca, Chocontá and Guatavita.\n\n\n\n\n\n\n"}
{"id": "7811", "url": "https://en.wikipedia.org/wiki?curid=7811", "title": "Cernunnos", "text": "Cernunnos\n\nCernunnos is the conventional name given in Celtic studies to depictions of the \"horned god\" of Celtic polytheism. Cernunnos was a Celtic god of fertility, life, animals, wealth, and the underworld. The name itself is only attested once, on the 1st-century Pillar of the Boatmen, but he appears all over Gaul, and among the Celtiberians. Cernunnos is depicted with the antlers of a stag, sometimes carries a purse filled with coin, often seated cross-legged and often associated with animals and holding or wearing torcs, are known from over 50 examples in the Gallo-Roman period, mostly in north-eastern Gaul.\n\nNot much is known about the god from literary sources, and details about his name, his followers or his significance in Celtic religion are unknown. Speculative interpretations identify him as a god of nature, life or fertility.\n\nThe theonym \"[C]ernunnos\" appears on the Pillar of the Boatmen, a Gallo-Roman monument dating to the early 1st century CE, to label a god depicted with stag's antlers in their early stage of annual growth. Both antlers have torcs hanging from them.\n\nThe name has been compared to a divine epithet \"Carnonos\" in a Celtic inscription written in Greek characters at Montagnac, Hérault (as καρνονου, \"karnonou\", in the dative case).\nA Gallo-Latin adjective \"carnuātus\", \"horned,\" is also found.\n\nThe Proto-Celtic form of the theonym is reconstructed as either *\"Cerno-on-os\" or *\"Carno-on-os\". The augmentative \"-on-\" is characteristic of theonyms, as in Maponos, Epona, Matronae, and Sirona.\nMaier (2010) states that the etymology of \"Cernunnos\" is unknown, as the Celtic word for \"horn\" has an \"a\" (as in \"Carnonos\").\n\nGaulish \"karnon\" \"horn\"is cognate with Latin \"cornu\" and Germanic \"*hurnaz\", English \"horn\", ultimately from Proto-Indo-European \"\".\nThe etymon \"karn-\" \"horn\" appears in both Gaulish and Galatian branches of Continental Celtic. Hesychius of Alexandria glosses the Galatian word \"karnon\" (κάρνον) as \"Gallic trumpet\", that is, the Celtic military horn listed as the carnyx (κάρνυξ) by Eustathius of Thessalonica, who notes the instrument's animal-shaped bell. The root also appears in the names of Celtic polities, most prominent among them the Carnutes, meaning something like \"the Horned Ones,\" and in several personal names found in inscriptions.\n\nThe name \"Cernunnos\" occurs only on the \"Pillar of the Boatmen\" (\"Pilier des nautes\"), now displayed in the Musée National du Moyen Age in Paris. Constructed by Gaulish sailors probably in 14 CE, it was discovered in 1710 within the foundations of the cathedral of Notre-Dame de Paris, site of ancient Lutetia, the \"civitas\" capital of the Celtic Parisii. The distinctive stone pillar is an important monument of Gallo-Roman religion. Its low reliefs depict and label by name several Roman deities such as Jupiter, Vulcan, and Castor and Pollux, along with Gallic deities such as Esus, Smertrios, and Tarvos Trigaranus. The name \"Cernunnos\" can be read clearly on 18th century drawings of the inscriptions, but the initial letter has been obscured since, so that today only a reading \"[_]ernunnos\" can be verified\n\nAdditional evidence is given by one inscription on a metal plaque from Steinsel-Rëlent in Luxembourg, in the territory of the Celtic Treveri. This inscription read \"Deo Ceruninco\", \"to the God Cerunincos\", assumed to be the same deity. The Gaulish inscription from Montagnac reads αλλετ[ει]νος καρνονου αλ[ι]σο[ντ]εας (\"Alletinos [dedicated this] to Carnonos of Alisontea\"), with the last word possibly a place name based on \"Alisia\", \"service-tree\" or \"rock\" (compare Alesia, Gaulish \"Alisiia\").\n\nThe god labelled \"[C]ernunnos\" on the Pillar of the Boatmen is depicted with stag's antlers in their early stage of annual growth. Both antlers have torcs hanging from them. The lower part of the relief is lost, but the dimensions suggest that the god was sitting cross-legged, providing a direct parallel to the antlered figure on the Gundestrup cauldron.\n\nIn spite of the name \"Cernunnos\" being attested nowhere else, it is commonly used in Celtological literature as describing all comparable depictions of horned/antlered deities.\n\nThis \"Cernunnos\" type in Celtic iconography is often portrayed with animals, in particular the stag, and also frequently associated with the ram-horned serpent, and less frequently bulls (at Rheims), dogs and rats. Because of his frequent association with creatures, scholars often describe Cernunnos as the \"Lord of the Animals\" or the \"Lord of Wild Things\", and Miranda Green describes him as a \"peaceful god of nature and fruitfulness\".\n\nThe \"Pilier des nautes\" links him with sailors and with commerce, suggesting that he was also associated with material wealth as does the coin pouch from the Cernunnos of Rheims (Marne, Champagne, France)—in antiquity, Durocortorum, the \"civitas\" capital of the Remi tribe—and the stag vomiting coins from Niedercorn-Turbelslach (Luxembourg) in the lands of the Treveri. The god may have symbolized the fecundity of the stag-inhabited forest.\n\nOther examples of \"Cernunnos\" images include a petroglyph in Val Camonica in Cisalpine Gaul. The antlered human figure has been dated as early as the 7th century BCE or as late as the 4th. An antlered child appears on a relief from Vendeuvres, flanked by serpents and holding a purse and a torc. The best known image appears on the Gundestrup cauldron found on Jutland, dating to the 1st century BCE, thought to depict Celtic subject matter though usually regarded as of Thracian workmanship.\n\nAmong the Celtiberians, horned or antlered figures of the Cernunnos type include a \"Janus-like\" god from Candelario (Salamanca) with two faces and two small horns; a horned god from the hills of Ríotinto (Huelva); and a possible representation of the deity Vestius Aloniecus near his altars in Lourizán (Pontevedra). The horns are taken to represent \"aggressive power, genetic vigor and fecundity.\"\n\nDivine representations of the Cernunnos type are exceptions to the often-expressed view that the Celts only began to picture their gods in human form after the Roman conquest of Gaul.\nThe Celtic \"horned god\", while well attested in iconography, cannot be identified in description of Celtic religion in Roman ethnography and does not appear to have been given any \"interpretatio romana\", perhaps due to being too distinctive to be translatable into the Roman pantheon.\nWhile Cernunnos was never assimilated, scholars have sometimes compared him functionally to Greek and Roman divine figures such as Mercury, Actaeon, specialized forms of Jupiter, and Dis Pater, the latter of whom Julius Caesar said was considered the ancestor of the Gauls.\n\nThere have been attempts to find the \"cern\" root in the name of Conall Cernach, the foster brother of the Irish hero Cuchulainn in the Ulster Cycle. In this line of interpretation, \"Cernach\" is taken as an epithet with a wide semantic field—\"angular; victorious; bearing a prominent growth\"—and Conall is seen as \"the same figure\" as the ancient Cernunnos.\n\nThere is even greater evidence available to connect Conall Cernach to Cernunnos than the similarity in the names. A brief passage involving Conall in an eighth-century story entitled \"Táin Bó Fraích\" (\"The Cattle Raid on Fraech\") has been questioned before for its anti-climatic conclusion to an epic Celtic tale. In this passage Conall Cernach is portrayed as a hero and mighty warrior who assists the protagonist Fraech in rescuing his wife and son, and in reclaiming for Fraech his cattle. The fort that Conall must penetrate is guarded by a mighty serpent. The supposed anti-climax of this tale is when the fearsome serpent, instead of attacking Conall, darts to Conall's waist and girdles him as a belt. Rather than killing the serpent, Conall allows it to live, and then proceeds to attack and rob the fort of its great treasures the serpent previously protected.\n\nCernunnos, as the conjectured Gaulish manifestation of the Roman Dis Pater, is considered to share the latter's attributes of ruling over the hidden treasures of the underworld. Subterranean treasures were commonly linked in Medieval Bestiaries to the serpent, the occupant of the underground, or otherworld, and the keeper of its treasures and mysteries. This aspect of Cernunnos is depicted on a stone statue from a well in Sommerécourt, Haute-Marne, France, and on a bronze figurine from Autun. Both statue and figurine portray Cernunnos with the two ram-headed serpents encircling his waist. This is more than just a small similarity to the instance of the serpent that guarded the treasure of the fort in \"Táin Bó Fraích\" surrendering to Conall Cernach and becoming his belt. Cernunno's connections to the deity Mars serve to underline Conall's role as hero-warrior in the tale.\n\nThe anti-climatic nature of the eighth-century Irish tale then gains significant clarity in the light of the relationship between a horned or antler-bearing deity, warrior, or progenitor, and the chthonic dwelling, treasure-guarding serpent that encircled the waist of the one it chose to protect. This universal Celtic concept comes down to us as a mere echo of its ancient self through centuries of the Christianization of Ireland. The Gaelic Cernunnos may now possibly only be found in the slight similarity of a name and the peculiarity of a single passage from a Middle Ages Irish epic.\n\nSome see the qualities of Cernunnos subsumed into the \"life\" of Saint Ciarán of Saighir, one of the Twelve Apostles of Ireland. When he was building his first tiny cell, as his hagiograph goes, his first disciple and monk was a boar that had been rendered gentle by God. This was followed by a fox, a badger, a wolf and a stag.\n\nIn Wicca and other forms of Neopaganism a Horned God is revered; this divinity syncretises a number of horned or antlered gods from various cultures, including Cernunnos. The Horned God reflects the seasons of the year in an annual cycle of life, death and rebirth.\n\nIn the tradition of Gardnerian Wicca, the Horned God is sometimes specifically referred to as Cernunnos, or sometimes also as Kernunno.\n\n\n\n"}
{"id": "207820", "url": "https://en.wikipedia.org/wiki?curid=207820", "title": "Compact star", "text": "Compact star\n\nIn astronomy, the term \"compact star\" (or \"compact object\") refers collectively to white dwarfs, neutron stars, and black holes. It would grow to include exotic stars if such hypothetical dense bodies are confirmed.\n\nMost compact stars are the endpoints of stellar evolution, and thus often referred to as stellar remnants, the form of the remnant depending primarily on the mass of the star when it formed. All of these objects have a high mass relative to their radius, giving them a very high density. The term \"compact star\" is often used when the exact nature of the star is not known, but evidence suggests that it is very massive and has a small radius, thus implying one of the above-mentioned categories. A compact star that is not a black hole may be called a degenerate star.\n\nThe usual endpoint of stellar evolution is the formation of a compact star.\n\nMost stars will eventually come to a point in their evolution when the outward radiation pressure from the nuclear fusions in its interior can no longer resist the ever-present gravitational forces. When this happens, the star collapses under its own weight and undergoes the process of stellar death. For most stars, this will result in the formation of a very dense and compact stellar remnant, also known as a compact star.\n\nCompact stars have no internal energy production, but will—with the exception of black holes—usually radiate for millions of years with excess heat left from the collapse itself.\n\nAccording to the most recent understanding, compact stars could also form during the phase separations of the early Universe following the Big Bang. Primordial origins of known compact objects have not been determined with certainty.\n\nAlthough compact stars may radiate, and thus cool off and lose energy, they do not depend on high temperatures to maintain their structure, as ordinary stars do. Barring external disturbances and proton decay, they can persist virtually forever. Black holes are however generally believed to finally evaporate from Hawking radiation after trillions of years. According to our current standard models of physical cosmology, all stars will eventually evolve into cool and dark compact stars, by the time the Universe enters the so-called degenerate era in a very distant future.\n\nThe somewhat wider definition of \"compact objects\" often includes smaller solid objects such as planets, asteroids, and comets. There is a remarkable variety of stars and other clumps of hot matter, but all matter in the Universe must eventually end as some form of compact stellar or substellar object, according to the theory of thermodynamics.\n\nThe stars called white or degenerate dwarfs are made up mainly of degenerate matter; typically carbon and oxygen nuclei in a sea of degenerate electrons. White dwarfs arise from the cores of main-sequence stars and are therefore very hot when they are formed. As they cool they will redden and dim until they eventually become dark black dwarfs. White dwarfs were observed in the 19th century, but the extremely high densities and pressures they contain were not explained until the 1920s.\n\nThe equation of state for degenerate matter is \"soft\", meaning that adding more mass will result in a smaller object. Continuing to add mass to what is now a white dwarf, the object shrinks and the central density becomes even larger, with higher degenerate-electron energies. The star's radius has now shrunk to only a few thousand kilometers, and the mass is approaching the theoretical upper limit of the mass of a white dwarf, the Chandrasekhar limit, about 1.4 times the mass of the Sun ().\n\nIf we were to take matter from the center of our white dwarf and slowly start to compress it, we would first see electrons forced to combine with nuclei, changing their protons to neutrons by inverse beta decay. The equilibrium would shift towards heavier, neutron-richer nuclei that are not stable at everyday densities. As the density increases, these nuclei become still larger and less well-bound. At a critical density of about 4 kg/m), called the neutron drip line, the atomic nucleus would tend to fall apart into protons and neutrons. Eventually we would reach a point where the matter is on the order of the density (c. 2 kg/m) of an atomic nucleus. At this point the matter is chiefly free neutrons, with a small amount of protons and electrons.\n\nIn certain binary stars containing a white dwarf, mass is transferred from the companion star onto the white dwarf, eventually pushing it over the Chandrasekhar limit. Electrons react with protons to form neutrons and thus no longer supply the necessary pressure to resist gravity, causing the star to collapse. If the center of the star is composed mostly of carbon and oxygen then such a gravitational collapse will ignite runaway fusion of the carbon and oxygen, resulting in a Type Ia supernova that entirely blows apart the star before the collapse can become irreversible. If the center is composed mostly of magnesium or heavier elements, the collapse continues. As the density further increases, the remaining electrons react with the protons to form more neutrons. The collapse continues until (at higher density) the neutrons become degenerate. A new equilibrium is possible after the star shrinks by three orders of magnitude, to a radius between 10 and 20 km. This is a \"neutron star\".\n\nAlthough the first neutron star was not observed until 1967 when the first radio pulsar was discovered, neutron stars were proposed by Baade and Zwicky in 1933, only one year after the neutron was discovered in 1932. They realized that because neutron stars are so dense, the collapse of an ordinary star to a neutron star would liberate a large amount of gravitational potential energy, providing a possible explanation for supernovae. This is the explanation for supernovae of types Ib, Ic, and II. Such supernovae occur when the iron core of a massive star exceeds the Chandrasekhar limit and collapses to a neutron star.\n\nLike electrons, neutrons are fermions. They therefore provide neutron degeneracy pressure to support a neutron star against collapse. In addition, repulsive neutron-neutron interactions provide additional pressure. Like the Chandrasekhar limit for white dwarfs, there is a limiting mass for neutron stars: the Tolman-Oppenheimer-Volkoff limit, where these forces are no longer sufficient to hold up the star. As the forces in dense hadronic matter are not well understood, this limit is not known exactly but is thought to be between 2 and . If more mass accretes onto a neutron star, eventually this mass limit will be reached. What happens next is not completely clear.\n\nAs more mass is accumulated, equilibrium against gravitational collapse reaches its breaking point. The star's pressure is insufficient to counterbalance gravity and a catastrophic gravitational collapse occurs in milliseconds. The escape velocity at the surface, already at least 1/3 light speed, quickly reaches the velocity of light. No energy nor matter can escape: a black hole has formed. All light will be trapped within an event horizon, and so a black hole appears truly black, except for the possibility of Hawking radiation. It is presumed that the collapse will continue.\n\nIn the classical theory of general relativity, a gravitational singularity occupying no more than a point will form. There may be a new halt of the catastrophic gravitational collapse at a size comparable to the Planck length, but at these lengths there is no known theory of gravity to predict what will happen. Adding any extra mass to the black hole will cause the radius of the event horizon to increase linearly with the mass of the central singularity. This will induce certain changes in the properties of the black hole, such as reducing the tidal stress near the event horizon, and reducing the gravitational field strength at the horizon. However, there will not be any further qualitative changes in the structure associated with any mass increase.\n\n\nAn \"exotic star\" is a hypothetical compact star composed of something other than electrons, protons, and neutrons balanced against gravitational collapse by degeneracy pressure or other quantum properties. These include strange stars (composed of strange matter) and the more speculative preon stars (composed of preons).\n\nExotic stars are hypothetical, but observations released by the Chandra X-Ray Observatory on April 10, 2002 detected two candidate strange stars, designated RX J1856.5-3754 and 3C58, which had previously been thought to be neutron stars. Based on the known laws of physics, the former appeared much smaller and the latter much colder than they should, suggesting that they are composed of material denser than neutronium. However, these observations are met with skepticism by researchers who say the results were not conclusive.\n\nIf neutrons are squeezed enough at a high temperature, they will decompose into their component quarks, forming what is known as a quark matter. In this case, the star will shrink further and become denser, but instead of a total collapse into a black hole, it is possible, that the star may stabilize itself and survive in this state indefinitely, as long as no extra mass is added. It has, to some extent, become a very large nucleon. A-type star in this hypothetical state is called a \"quark star\" or more specifically a \"strange star\". The pulsars RX J1856.5-3754 and 3C58 have been suggested as possible quark stars. Most neutron stars are thought to hold a core of quark matter, but it has proven hard to determine observationally.\n\nA \"preon star\" is a proposed type of compact star made of preons, a group of subatomic particles. Preon stars would be expected to have huge densities, exceeding 10 kilogram per cubic meter – intermediate between quark stars and black holes.\nPreon stars could originate from supernova explosions or the Big Bang; however, current observations from particle accelerators speak against the existence of preons.\n\n\"Q stars\" are hypothetical compact, heavier neutron stars with an exotic state of matter where particle numbers are preserved with radii less than 1.5 times the corresponding Schwarzschild radius. Q stars are also called \"gray holes\".\n\nAn \"electroweak star\" is a theoretical type of exotic star, whereby the gravitational collapse of the star is prevented by radiation pressure resulting from electroweak burning, that is, the energy released by conversion of quarks to leptons through the electroweak force. This process occurs in a volume at the star's core approximately the size of an apple, containing about two Earth masses.\n\nA boson star is a hypothetical astronomical object that is formed out of particles called bosons (conventional stars are formed out of fermions). For this type of star to exist, there must be a stable type of boson with repulsive self-interaction. As of 2016 there is no significant evidence that such a star exists. However, it may become possible to detect them by the gravitational radiation emitted by a pair of co-orbiting boson stars.\n\nBased on the generalized uncertainty principle (GUP), proposed by some approaches to quantum gravity such as string theory and doubly special relativity, the effect of GUP on the thermodynamic properties of compact stars with two different components has been studied, recently. Tawfik et al. noted that the existence of quantum gravity correction tends to resist the collapse of stars if the GUP parameter is taking values between Planck scale and electroweak scale. Comparing with other approaches, it was found that the radii of compact stars should be smaller and increasing energy decreases the radii of the compact stars.\n\n"}
{"id": "1624646", "url": "https://en.wikipedia.org/wiki?curid=1624646", "title": "Crude oil washing", "text": "Crude oil washing\n\nCrude oil washing (COW) is washing out the residue from the tanks of an oil tanker using the crude oil cargo itself, after the cargo tanks have been emptied. Crude oil is pumped back and preheated in the slop tanks, then sprayed back via high pressure nozzles in the cargo tanks onto the walls of the tank. Due to the sticky nature of the crude oil, the oil clings to the tank walls, and such oil adds to the cargo 'remaining on board' (the ROB). By COWing the tanks, the amount of ROB is significantly reduced, and with the current high cost of oil, the financial savings are significant, both for the Charterer and the Shipowner. If the cargo ROB is deemed as 'liquid and pumpable' then the charterers can claim from the owner for any cargo loss for normally between 0.3% up to 0.5%. It replaced the \"load on top\" and \"seawater washing\" systems, both of which involved discharging oil-contaminated water into the sea. MARPOL 73/78 made this mandatory equipment for oil tankers of 20,000 tons or greater deadweight.\n\nAlthough COWing is most notable for actual tankers, the current chairman for Hashimoto Technical Service, Hashimoto Akiyoshi, applied this method in washing refinery plant oil tanks in Japan. Hashimoto is currently using this method in the Kyushu, Chugoku, and Tohouku regions in Japan. \nOriginally oil tankers used one set of tanks for cargo and about one third of the same tanks were for water ballast on their empty trips. High pressure, hot, seawater jets were used to clean the tanks and the mixture of seawater and residue called \"slops\" discharged into the sea, as was the oil-contaminated ballast water. The 1954 OILPOL Convention attempted to reduce the harm by prohibiting such discharges within of most land and of certain particularly sensitive areas.\n\nThe discharges from seawater washing were still considered a problem and during the 1960s the load on top approach began to be adopted. The mixture of cleaning water and residue was pumped into a \"slop tank\" and allowed to separate by their different densities into oil and water during the journey. The water portion was then discharged, leaving only crude oil in the slop tank. This was pumped into the main tanks and the new cargo loaded on top of it, recovering as much as 800 tons of oil which was formerly discarded.\n\nEven with load on top there is still some oil in the discharged water from the slop tank. Starting in the 1970s, equipment capable of using crude oil itself for washing began to replace the water-based washing, leading to the current technique of \"crude oil washing\". This reduces the remaining deliberate discharge of oil-contaminated water and increases the amount of cargo discharged, providing a further benefit to the cargo owner.\n\nCrude oil washing equipment became mandatory for new tankers of 20,000 tons or more deadweight with the 1978 Protocol to the 1973 MARPOL Convention. Revised specifications for the equipment were introduced in 1999.\n\nModern tankers also use segregated ballast tanks and these remove the problem of discharge of oily ballast water.\n\n\n2.http://www.hts-g.co.jp/maintenance-eng.html\n"}
{"id": "16971924", "url": "https://en.wikipedia.org/wiki?curid=16971924", "title": "Economic restructuring", "text": "Economic restructuring\n\nEconomic restructuring refers to the phenomenon of Western urban areas shifting from a manufacturing to a service sector economic base. This transformation has affected demographics including income distribution, employment, and social hierarchy; institutional arrangements including the growth of the corporate complex, specialized producer services, capital mobility, informal economy, nonstandard work, and public outlays; as well as geographic spacing including the rise of world cities, spatial mismatch, and metropolitan growth differentials.\n\nAs cities experience a loss of manufacturing jobs and growth of services, sociologist Saskia Sassen affirms that a widening of the social hierarchy occurs where high-level, high-income, salaried professional jobs expands in the service industries alongside a greater incidence of low-wage, low-skilled jobs, usually filled by immigrants and minorities. A \"missing middle\" eventually develops in the wage structure. Several effects of this social polarization include the increasing concentration of poverty in large U.S. cities, the increasing concentration of black and Hispanic populations in large U.S. cities, and distinct social forms such as the underclass, informal economy, and entrepreneurial immigrant communities. In addition, the declining manufacturing sector leaves behind strained blue-collared workers who endure chronic unemployment, economic insecurity, and stagnation due to the global economy's capital flight. Wages and unionization rates for manufacturing jobs also decline. One other qualitative dimension involves the feminization of the job supply as more and more women enter the labor force usually in the service sector.\n\nBoth costs and benefits are associated with economic restructuring. Greater efficiency, job creation, gentrification, and enhanced national competitiveness are associated with social exclusion and inclusion. The low-skilled, low-income population faces the loss of opportunities, full participation in society, lack of access in labor market and school, weak position in housing markets, limited political participation, and restricted social-cultural integration. Conversely, high-skilled, high-income professionals enjoy social inclusion with modern amenities, conveniences, social participation, and full access to public resources.\n\nFurthermore, sociologist William Julius Wilson argues that the deindustrialization of manufacturing employment have exacerbated joblessness in impoverished African American communities correlating with a rise in single-mother households, high premature mortality rates, and increasing incarceration rates among African American males. With some African Americans gaining professional upward mobility through affirmative action and equal opportunity sanctions in education and employment, African Americans without such opportunities fall behind. This creates a growing economic class division among the African American demographic accentuated by global economic restructuring without government response to the disadvantaged. Furthermore, Wilson asserts that as the black middle class leave the predominantly black inner city neighborhoods, informal employment information networks are eroded. This isolates poor, inner city residents from the labor market compounding the concentration of poverty, welfare dependency, rise of unemployment, and physical isolation in these areas.\n\nCity youth are also affected such as in New York City. The declines in education, health care, and social services and the dearth of jobs for those with limited education and training along with the decay of public environments for outdoor play and recreation have all contributed to fewer autonomous outdoor play or \"hanging out\" places for young people. This in turn affects their gross motor development, cultural build-up, and identity construction. Children become prisoners of home relying on television and other outlets for companionship. Contemporary urban environments restricts the opportunities for children to forge and negotiate peer culture or acquire necessary social skills. Overall, their ecologies have eroded in recent years brought about by global restructuring.\n\nWhen the 1973 oil crisis affected the world capitalist economy, economic restructuring was used to remedy the situation by geographically redistributing production, consumption, and residences.\nCity economies across the globe moved from goods-producing to service-producing outlets.\nBreakthroughs in transportation and communications made industrial capital much more mobile. Soon, producer services emerged as a fourth basic economic sector where routine low-wage service employment moved to low-cost sites and advanced corporate services centralized in cities. These technological upheavals brought about changes in institutional arrangements with the prominence of large corporations, allied business and financial services, nonprofit and public sector enterprises. Global cities such as New York and London become centers for international finance and headquarters for multinational corporations offering cross currency exchange services as well as buildup of foreign banking and trading. Other cities become regional headquarters centers of low-wage manufacturing. In all these urban areas the corporate complex grows offering banking, insurance, advertising, legal council, and other service functions. Economic restructuring allows markets to expand in size and capacity from regional to national to international scopes.\n\nAltogether, these institutional arrangements buttressed by improved technology reflect the interconnectedness and internationalization of firms and economic processes. Consequently, capital, goods, and people rapidly flow across borders. Where the mode of regulation began with Fordism and Taylorization in the industrial age then to mass consumption of Keynesian economics policies, it evolves to differentiated and specialized consumption through international competition. Additionally, in the labor market, nonstandard work arrangements develop in the form of part-time work, temporary agency and contract company employment, short-term employment, contingent work, and independent contracting. Global economic changes and technological improvements in communications and information systems encouraged competitive organizations to specialize in production easily and assemble temporary workers quickly for specific projects. Thus, the norm of standard, steady employment unravels beginning in the mid-1970s.\n\nAnother shift in institutional arrangement involves public resources. As economic restructuring encourages high-technology service and knowledge-based economies, massive public de-investment results. Across many parts of the U.S. and the industrialized Western nations, steep declines in public outlays occur in housing, schools, social welfare, education, job training, job creation, child care, recreation, and open space. To remedy these cutbacks, privatization is installed as a suitable measure. Though it leads to some improvements in service production, privatization leads to less public accountability and greater unevenness in the distribution of resources. With this reform in privatizing public services, neoliberalism has become the ideological platform of economic restructuring. Free market economic theory has dismantled Keynesian and collectivists’ strategies and promoted the Reagan and Thatcher politics of the 1980s. Soon free trade, flexible labor, and capital flight are used from Washington D.C. to London to Moscow. Moreover, economic restructuring requires decentralization as states hand down power to local governments. Where the federal government focuses on mainly warfare-welfare concerns, local governments focus on productivity. Urban policy reflects this market-oriented shift from once supporting government functions to now endorsing businesses.\n\nUrban landscapes especially in the U.S. have significantly altered in response to economic restructuring. Cities such as Baltimore, Detroit, St. Louis and others face population losses which result in thousands of abandoned homes, unused buildings, and vacant lots, contributing to urban decay. Such transformations frustrate urban planning and revitalization, fostering deviance in the forms of drug-related activity and vagrancy. Older, compact, industrial U.S. cities have been rendered obsolete. Urban spaces become playgrounds for the urban gentry, wastelands for low-paid service workers, and denizens for the underground economy. In some areas, gentrification projects have caused displacement of poverty-stricken residents. Sunbelt cities such as Miami and Atlanta rise to become key business centers while Snowbelt cities such as Buffalo and Youngstown decline. Even housing markets respond to economic restructuring with decaying housing stocks, escalating housing prices, depleting tax base, changes in financing, and reduction in federal support for housing. Soon, spatial divisions among wealthy and poor households exacerbate. Moreover, with the movement of blue-collared employment from central cities, geographically entrenched housing discrimination, and suburban land use policy, African American youths in inner cities become victims of spatial mismatch, where their residences provide only weak and negative employment growth and they usually lack access to intrametropolitan mobility. High-order services, an expanding sector in the industrialized world, become spatially concentrated in a relative small number of large metropolitan areas, particularly in suburban office agglomerations.\n\nIn cultural terms, economic restructuring has been associated with postmodernity as its counterpart concerning flexible accumulation. Additionally, the term carries with it three core themes: historical, radical rupture into post-industrial economic order; priority of economic forces over social/political forces; and structure over agency where the process is independent of human will, as it takes place according to economic logic (Logan & Swanstrom 1990). In addition, economic restructuring demonstrates the increasing complex and human-capital intensive modern society in Western nations.\n\n"}
{"id": "23588360", "url": "https://en.wikipedia.org/wiki?curid=23588360", "title": "Ecosystem of the North Pacific Subtropical Gyre", "text": "Ecosystem of the North Pacific Subtropical Gyre\n\nThe North Pacific Subtropical Gyre (NPSG) is the largest contiguous ecosystem on earth. In oceanography, a subtropical gyre is a ring-like system of ocean currents rotating clockwise in the Northern Hemisphere and counterclockwise in the Southern Hemisphere caused by the Coriolis Effect. They generally form in large open ocean areas that lie between land masses.\n\nThe NPSG is the largest of the gyres as well as the largest ecosystem on our planet. Like other subtropical gyres, it has a high-pressure zone in its center. Circulation around the center is clockwise around this high-pressure zone. Subtropical gyres make up 40% of the Earth’s surface and play critical roles in carbon fixation and nutrient cycling. This particular gyre covers most of the Pacific Ocean and comprises four prevailing ocean currents: the North Pacific Current to the north, the California Current to the east, the North Equatorial Current to the south, and the Kuroshio Current to the west. Its large size and distance from shore has caused the NPSG to be poorly sampled and thus poorly understood.\n\nThe life processes in open-ocean ecosystems are a sink for the atmosphere’s increasing. Gyres make up a large proportion, approximately 75%, of what we refer to as the open ocean, or the area of the ocean that does not consist of coastal areas. They are considered oligotrophic, or nutrient poor because they are far from terrestrial runoff. These regions were once thought to be homogenous and static habitats. However, there is increasing evidence that the NPSG exhibits substantial physical, chemical, and biological variability on a variety of time scales. Specifically, the NPSG exhibits seasonal and interannual variations in primary productivity (simply defined as the production of new plant material), which is important for the uptake of .\n\nThe NPSG is not only a sink for in the atmosphere, but also other pollutants. As a direct result of this circular pattern, gyres act like giant whirlpools and become traps for anthropogenic pollutants, such as marine debris. The NPSG has become recognized for the large quantity of plastic debris floating just below the surface in the center of the gyre. This area has recently received a lot of media attention and is commonly referred to as the Great Pacific Garbage Patch.\n\nThe NPSG is not often sampled because of its distance from the coast and its shortage of marine life. These vast and deep ocean waters, far from the influence of land, have historically been considered the oceanic equivalent of terrestrial deserts, with low standing stocks of biomass and low production rates. This perspective is derived from a dearth of comprehensive investigation of central gyre habitats. Over the past two decades these views have been challenged with a newfound understanding of the dynamics of the NPSG.\n\nDuring the early days of marine exploration, the HMS Challenger (1872-1876), on its leg from Yokohama to Honolulu, collected plant and animal specimens as well as numerous seawater samples. The goals of this expedition were to determine the chemical composition of seawater and the organic matter in suspension and to study the distribution and abundance of various communities of organisms. The motivation for studying open ocean ecosystems has changed over time, whereas today more modern studies focus on biodiversity and the effects of climate on ecosystem dynamics. Today, the Hawaii Ocean Time-series (HOT) program has assembled the largest and most comprehensive ecological data set for the NPSG and is scheduled to continue to the next millennium. Programs like HOT have debunked the hypothesis that this ecosystem is static and homogenous, finding that the NPSG exhibits dynamic seasonal patterns separating it from other open ocean systems.\n\nThe NPSG is the largest of the open ocean habitats and is considered to be the Earth’s largest contiguous biome. This great anticyclonic circulation feature extends from 15°N to 35°N latitude and from 135°E to 135°W longitude. Its surface area spans approximately 2 x 10 km. Its western portion, west of 180° longitude, has greater physical variability than the eastern portion. This variability, where different weather patterns affect subregions differently, is due to the large dimensions of this gyre.\n\nThis large variability is caused by discrete eddies, near-inertial motions, and internal tides. Climate patterns such as the North Pacific Gyre Oscillation (NPGO), El Nino/Southern Oscillation (ENSO) and the Pacific Decadal Oscillation (PDO) affect the interannual variability in primary productivity in the NPSG. DiLorenzo et al., 2008 These conditions can have profound effects on biological processes within this habitat, they have the ability to shift sea surface temperature (SST), chlorophyll patterns, nutrient patterns, oxygen concentrations, mixed layer depths, and thus the carrying capacity (amount of life this habitat can carry) of the NPSG.\n\nLow nutrient concentrations and thus a low density of living organisms characterize the surface waters of the NPSG. The low biomass results in clear water, allowing photosynthesis to occur to a substantial depth. The NPSG is classically described as a two-layered system. The upper, nutrient-limited layer accounts for most of the primary production, supported primarily by recycled nutrients. The lower layer has nutrients more readily available, but photosynthesis is light-limited.\n\nIn open-ocean systems, biological production depends on intense nutrient recycling within the euphotic (sunlit) zone, with only a small fraction supported by the input of “new” nutrients. Previously there was a perception that the NPSG was a marine desert and that “new” nutrients were not commonly added to this system. The outlook has changed, as scientists have begun to have a better understanding of this habitat. Although fairly high rates of primary production are maintained through rapid recycling of nutrients, physical processes such as internal waves and tides, cyclonic mesoscale eddies, wind-driven Ekman pumping, and atmospheric storms may carry in new nutrients.\n\nNutrients that do not get used up on the surface will eventually sink down and nourish the seafloor habitat. The deep benthic habitats of the ocean gyres have been thought to typically consist of some of the most food-poor regions on the planet. One of the sources of nutrients to this deep ocean habitat is marine snow. Marine snow consists of detritus, dead organic matter, which falls from the surface waters where productivity is highest and exports carbon and nitrogen from the surface mixed layer to the deep ocean. Data on the abundance of marine snow to the deep ocean floor is lacking in this large ecosystem. However, Pilskaln et al. found that in the NPSG, marine snow was at a higher abundance than expected and were surprisingly comparable to a deep coastal upwelling system.\n\nHigher nutrient value may be because of Rhizosolenia mats, which also play an important role in contributing to marine snow in subtropical gyres. These are generally multi-species associations of Rhizosolenia species of diatoms. This larger phytoplankton may reach up to 10s of centimeters in size. These mats are particularly abundant in the NPSG. Their abundance in this ecosystem suggests a higher flux of nutrients in the NPSG than was predicted in classic theories.\n\nWhile N is transported deeper by this mechanism, the surface waters are potentially cut off from this source. Nitrogen must be available for life at the surface. In order to account for this lack of nitrogen to the surface, there are organisms that are capable of nitrogen fixation in the NPSG. Trichodesmium is one species capable of nitrogen fixation that is found in many surface plankton blooms. Nitrogen fixation is the process where inert N is taken from the atmosphere and converted into a nitrogen compound that is available to organisms for use. In many oligotrophic marine ecosystems, nitrogen fixation is a common source of nitrogen.\n\nVertically migrating zooplankton can also actively transport nutrients to different zones of the water column. Zooplankton feed in the surface waters at night, and then by day release fecal pellets to the midwaters, which can transport C, N, and P to the deeper waters. In the NPSG the zooplankton community is not static but fluctuates seasonally and is dominated by copepods, euphausiids, and chaetognaths.\n\nRecently, classic theories about the lack of nutrients in the NPSG have been disproven and new theories suggest that the ecosystem actually is dynamic and characterized by strong seasonal, interannual, and even decadal variability It has also been deemed highly sensitive to climate change, scientists have observed increases in water column stratification and decreased inorganic nutrient availability. These changes are proposed as driving mechanisms that are changing the current trend in phytoplankton community structure from eukaryotic to prokaryotic populations, as these simpler organisms can withstand lower nutrient supply. Zooplankton and phytoplankton represent less than 10% of living organisms in this region, and it is now well documented that the NPSG is a “microbial ecosystem”.\n\nMicrobial organisms make up the majority of the primary producers in the NPSG. They are autotrophic, meaning they capture their own “food” from sunlight and chemicals, including . These organisms comprise the base of the food chain, and thus their presence in an ecosystem is fundamental. In the NPSG, primary productivity is often described as low.\n\nBefore 1978, scientists hypothesized that diatoms dominated plankton populations in the NPSG. The primary consumers were expected to be relatively large mesozooplankton. It is now well known that most of the algae in the NPSG are actually bacteria (unicellular organisms), dominated by cyanobacteria, or blue-green algae. These simple organisms make up the majority of the standing stock of photosynthesizing marine life in this ecosystem. Scientists have also recently discovered Archaea (also a single-celled microorganism, but more similar to a eukaryote than bacteria) genes in the NPSG, suggesting that additional diversity exists in this habitat. Many microorganisms may exist in this gyre because small body size has a competitive advantage in the ocean for resource (light and nutrients) acquisition. In the contemporary view of the NPSG, the microbial food web is always present, whereas the larger eukaryote-grazer food chain is seasonal and ephemeral.\n\nEukaryotic plankton in the gyre is dependent on “new” nutrients coming in from physical weather patterns. The classic two-layered model discussed in previous sections considers the upper layer to be equivalent to a “spinning wheel,” with little export of nutrients because they are constantly recycled. This model does not allow for the input of new nutrients, which is problematic because this would make any rapid increase, or bloom of phytoplankton impossible. Despite ever-present nutrient limitation in the upper portion, plankton biomass and rates of primary production have considerable temporal variability and do produce blooms in the NPSG.\n\nThis interannual variability has been attributed to alterations in upper ocean nutrient supply stemming from physical variations due to ENSO and PDO. Based on new data, it now appears that present rates of primary production in these low nutrient regions are much greater than had been considered, and can vary significantly on time scales ranging from daily to interdecadal. In the spring, rapid increases in surface phytoplankton are occasionally observed in association with cyclonic mesoscale eddies or intense atmospheric disturbances, both physical processes that bring in new nutrients. In the summer, blooms are seen more regularly and are typically dominated by diatoms and cyanobacteria. These regular summer blooms may be caused by variations in the PDO. Summer blooms have been observed in these waters as long as research vessels have been frequenting them. All of these blooms have been seen in the eastern part of the NSPG with none reported west of 160° W. Hypotheses to explain this phenomenon are that the gyre is characterized by low phosphate, but that the bloom region of the eastern NPSG has considerably higher phosphate concentrations than the western.\n\nVariations in primary production in the NPSG can significantly affect nutrient cycling, food-web dynamics, and global elemental fluxes. The size distribution of pelagic primary producers determines both the composition and magnitude of the exported nutrients to the deeper waters. This in turn affects the communities that live in the deeper waters of this system.\n\nThe mesopelagic zone is sometimes referred to as the twilight zone; it extends from 200m to around 1000m. In the deeper layers of the NPSG, species higher up on the food chain will migrate vertically or horizontally within or in and out of the gyre. Based on analyses of the zooplankton community, the Central North Pacific has a high species diversity (or high number of species) and high equitability (meaning relatively equal numbers of each exist). There is also a low degree of seasonal variability of densities of zooplankton.\n\nStudies of mesopelagic fishes of central subtropical waters are scarce. The few studies that do exist found that mesopelagic fish species are not uniformly distributed throughout the subtropical Pacific Ocean. Their geographic ranges conform to patterns shown by zooplankton. Some of the species found are restricted to these low-productivity central gyres. Some of the families of fish that are highly represented are Mytophids, Gonostomatids, Photichthyids, Sternoptychids, and Melamphaids. Our understanding of the mesopelagic community of the NPSG suffers from an insufficiency of data due to the difficulty of accessing the deeper zones of this system.\n\nThe deepest community in the NPSG is the benthic community. At the depths of the gyre lies a sea floor of fine-grained clay sediments. This sediment is home to a community of organisms, which generally receive their nutrients as a “rain” of productivity sinking from above. At depth under the gyre lies one of the most food-poor areas on the planet, which therefore supports very low densities and biomass of benthic infauna, or animals residing in the sediment. In the sediment itself, nutrients generally decline with depth, including carbon, chlorophyll, and nitrogen. Density of the benthic infauna is consistent with this nutrient pattern. Infauna are typically found in the shallower layers of sediments where the sediment-water interface lies and generally decrease in number with increasing depth in the sediment. Bacteria in the sediment show this pattern as well as macrofauna (infaunal organisms >0.5mm), which are dominated by agglutinating (soft-bodied) foraminifera and nematodes. Other prominent macrofauna found in the sediment are calcareous foraminifera, copepods, polychaetes, and bivalves. These benthic organisms rely heavily on the supply of nutrients that settle to the sea floor. Any change in primary production at the surface could pose a major threat to these organisms, as well as cause other potential negative outcomes to other parts of the NPSG.\n\nUntil recently the NPSG was considered to be a static part of a vast global marine desert. Recent discoveries have proved that this system is dynamic and contains physical, chemical, and biological variability on a variety of time scales. With the current changing climate, patterns in the atmosphere are shifting and causing changes in primary production in the NPSG. Variations in primary productivity can affect the ocean carbon cycle and potentially atmospheric and climate, because such variations can change the amount of carbon that is stored in the subsurface layers of the oceans. Because the NPSG is the largest contiguous biome on earth, it is not only important to a community of organisms, but also the rest of the planet.\n\nThe NPSG has received copious attention because of another issue it currently faces. The eddy effects of the gyre serve to retain pollutants in its center. If a pollutant gets trapped in a current that is headed toward a gyre, it will stay there indefinitely or as long as the life of the pollutant. One such pollutant that is persistent and common in the NPSG is plastic debris. The NPSG forces debris into its central area. This phenomenon has recently given this gyre the nickname, “The Pacific Garbage Patch.” The mean abundance and weight of plastic pieces in this area are currently the largest observed in the Pacific Ocean. It is rumored that this plastic “soup” is anywhere from the size of Texas to the size of the US. With increasing interest in pollution and climate change, the NPSG has gained more attention. It is important that our knowledge of this system continue to flourish for these reasons, as well as solely for the understanding of the world’s largest ecosystem.\n\n\n"}
{"id": "16982912", "url": "https://en.wikipedia.org/wiki?curid=16982912", "title": "Electricity sector in Brazil", "text": "Electricity sector in Brazil\n\nThe electricity sector in Brazil is the largest in South America. \nIts currently installed capacity by the end of 2016 was 150,338 MW, a 9.500 MW increase on 2015.\nThe installed capacity grew from 11,000 MW in 1970 with an average yearly growth of 5.8% per year.\nBrazil has the largest capacity for water storage in the world, being highly dependent on hydroelectricity generation capacity, which meets over 70% of its electricity demand. The national grid is composed 80% from renewable sources.\nThis dependence on hydropower makes Brazil vulnerable to power supply shortages in drought years, as was demonstrated by the 2001-2002 energy crisis.\n\nThe National Interconnected System (SIN) comprises the electricity companies in the South, South-East, Center-West, North-East and part of the North region. Only 3.4% of the country's electricity production is located outside the SIN, in small isolated systems located mainly in the Amazonian region.\n\nGeneration capacity in Brazil is dominated by hydroelectric plants, which account for 77% of total installed capacity, with 24 plants above 1,000 MW. About 88 percent of the electricity fed into the national grid is estimated to come from hydroelectric generation, with over 25% coming from a single hydropower plant, the massive 14 GW Itaipu dam facility, located between Brazil and Paraguay on the Paraná River. Natural gas generation is second in importance, representing about 10% of total capacity, close to the 12% goal for 2010 established in 1993 by the Ministry of Mines and Energy.\n\nThis reliance on abundant hydroelectric resources allegedly reduces the overall generation costs. However, this large dependence on hydropower makes the country especially vulnerable to supply shortages in low-rainfall years (See The 2001-2002 crisis below).\n\nBy the end of 2016, the breakdown of generation by source was:\n\"Source\": Ministry of Mines and Energy, 2016\n\nAs summarized in the table above, Brazil has two nuclear power plants, Angra 1 (657 MW) and Angra 2 (1,350 MW), both of them owned by Eletronuclear, a subsidiary of the state-owned Eletrobrás.\n\nBrazil needs to add 6000 MW of capacity every year in order to satisfy growing demand from an increasing and more prosperous population. The Brazilian Ministry of Energy has decided to generate 50% of new supplies from hydropower, 30% from wind and biomass such as bagasse, and 20% from gas and other sources. Wind in the North-East is strongest during the dry season when hydropower plants produce less, so the two energy sources are seasonally complementary.\n\nBrazil has an untapped hydropower potential of 180,000 MW, including about 80,000 MW in protected regions for which there are no development plans. The government expects to develop the rest by 2030. Most new hydropower plants are run-of-river plants that are less damaging to the environment, because their reservoirs are small. However, they are more vulnerable to droughts and less efficient, because only a fraction of their capacity can be used during the dry season.\n\nThe National Agency for Electricity (ANEEL) has commissioned feasibility studies for several hydroelectric plants (small, medium and large) in the period 2006-2008. These studies correspond to a total potential capacity of 31,000MW. In 2007, Ibama, the environmental agency, gave approval for the construction of two new dams, the Jirau Dam (3,300 MW) and the Santo Antônio Dam (3,150 MW), on the Madeira River in the state of Rondônia. The bid for the Santo Antônio plant was awarded in December 2007 to Madeira Energy, with a 39% participation from state-owned Furnas, while the bid for the Jirau plant will be launched in May 2008. The government is also pursuing development of the controversial 11,000 MW Belo Monte Dam in the state of Pará, on the Xingu River. IBAMA approved Belo Monte's provisional environmental license in February 2010 despite internal uproar from technicians over incomplete data.\n\nAlso in 2007, Electronuclear was granted permission to resume construction of Angra 3, a 1,350 MW plant, and is currently in the process of selecting a site for a fourth nuclear power plant. In February 2014, Eletrobras Eletronuclear awarded contracts to begin construction, with an estimated completion date of 2018.\n\nCurrently, the development of gas-fired thermoelectric power is somewhat jeopardized by the lack of secure gas supplies. In fact, having a secure gas contract is a prerequisite to build a new thermoelectric plant and to participate in a new energy auction (See below). In order to counter the risk of unavailability of gas supplies, Brazil is in the initial stages of planning to build two LNG terminals that would likely come on-stream around 2010. However, in the meantime, several thermoelectric plants are converting their machinery to dual-fuel capacity (oil and gas).\n\nTotal electricity consumed in 2007 was 410 TWh, while annual consumption per capita for the same year averaged 2,166 kWh. \nThe share of consumption by sector was as follows:\n\n\nElectricity demand is expected to grow an average of 3.6% in the next few years, leading to total estimated consumption of 504 TWh and average per capita consumption of 2,527 kWh.\n\nIn Brazil, capacity addition traditionally lagged behind demand growth. \nElectricity demand is expected to continue to grow at a quick pace. \nThe income elasticity of demand for electricity is estimated by Eletrobras at above unity. \nBetween 1980 and 2000, electricity demand increased on average by 5.4 per cent per year while GDP grew by 2.4 per cent on average per year. \nInvestment is therefore needed to boost generation and transmission capacity because there is limited excess supply, despite the reduction in demand following the energy rationing program implemented in 2001 in response to the energy crisis.\n\nBrazil, together with Chile, is the country with the highest access rate in Latin America. The power sector in Brazil serves more than 50 million customers, which corresponds to about 97% of the country's households, who have access to reliable electricity.\n\nInterruption frequency and duration are very close to the averages for the LAC region. In 2005, the average number of interruptions per subscriber was 12.5, while duration of interruptions per subscriber was 16.5 hours. The weighted averages for LAC were 13 interruptions and 14 hours respectively.\n\nDistribution losses in 2005 were 14%, well in line with the 13.5% average for the LAC region but about double that of an OECD country such as the Great Britain, with 7% distribution losses.\n\nThe Ministry of Energy and Mines (MME) has the overall responsibility for policy setting in the electricity sector while ANEEL, which is linked to the Ministry of Mines and Energy, is the Brazilian Electricity Regulatory Agency created in 1996 by Law 9427. ANEEL's function is to regulate and control the generation, transmission and distribution of power in compliance with the existing legislation and with the directives and policies dictated by the Central Government. The National Council for Energy Policies (CNPE), is an advisory body to the MME in charge of approving supply criteria and \"structural\" projects while the Electricity Industry Monitoring Committee (CMSE) monitors supply continuity and security.\n\nANEEL and the Ministry of Environment play almost no part in which investment projects go ahead, but they only influence how projects are executed once the decision has been taken. Both have had their bosses resign rather than supporting infrastructure projects in the Amazon.\n\nThe Operator of the National Electricity System (ONS) is a non-profit private entity created in August 1998 that is responsible for the coordination and control of the generation and transmission installations in the National Interconnected System (SIN). The ONS is under ANEEL's control and regulation.\n\nThe Power Commercialization Chamber (CCEE), successor of MAE (\"Mercado Atacadista de Energia Electrica\"), is the operator of the commercial market. The initial role of the operator was to create a single, integrated commercial electricity market, to be regulated under published rules. This role has become more active since now CCEE is in charge of the auction system. The rules and commercialization procedures that regulate CCEE's activities are approved by ANEEL.\n\nFinally, the Power Research Company (EPE) was created in 2004 with the specific mission of developing an integrated long-term planning for the power sector in Brazil. Its mission is to carry out studies and research services in the planning of the energy sector in areas such as power, oil and natural gas and its derivates, coal, renewable energy resources and energy efficiency, among others. Its work serves as input for the planning and implementation of actions by the Ministry of Energy and Mines in the formulation of the national energy policy\n\nThe Brazilian electricity model is fully deregulated, which allows generators to sell all of their \"assured energy\" via freely negotiated contracts with consumers above 3MW or via energy auctions administered by CCEE (See below). . Under this model, distributors are required to contract 100% of their expected demand. Currently, Brazilian generation supply can be sold under four types of markets:\n\n\nIn Brazil, large government-controlled companies dominate the electricity sector. Federally owned Eletrobras holds about 40% of capacity (including 50% of the Itaipu dam), with state-companies CESP, Cemig and Copel controlling 8%, 7% and 5% of generation capacity respectively.\n\nGeneration capacity is shared among the different companies as follows:\n\nCurrently, about 27 percent of the generation assets are in the hands of private investors. Considering the plants under construction, as well as the concessions and licenses already granted by ANEEL, this figure is expected to grow up to 31 percent in the medium term and to reach almost 44 percent over 5–6 years. Private capital participation in the generation business will likely represent 50 percent of the installed capacity in the years to come\n\nBrazil's transmission system is growing in importance since adequate transmission capacity is essential to manage the effects of regional droughts, allowing to move power from areas where rainfall is plentiful. As a matter of fact, the rationing that occurred in Brazil during 2001-2002 (See The 2001-2002 crisis below), could have largely been averted if there had been adequate transmission capacity between the south (excess supply) and the southeast (severe deficit).\n\nTransmission has remained almost exclusively under government control through both federal (Eletrobras) and state companies (mainly Sao-Paulo-CTEEP, Minas Gerais-Cemig, and Parana-Copel) until recently. However, under the new sector regulatory model, there are about 40 transmission concessions in Brazil. Most of them are still controlled by the government, with subsidiaries under federal company Eletrobras holding 69% of total transmission lines.\n\"Source\": Bear Stearns 2007\n\nIn Brazil, there are 49 utilities with distribution concessions and about 64% of Brazilian distribution assets are controlled by private sector companies. The following table lists Brazil's most important distribution companies:\n\"Source\": Bear Stearns, 2007\n\nIn Brazil, hydroelectricity supplies about 77% of total electricity demand. It is estimated that about 70% of the overall hydroelectricity potential of the country, is still unexploited.\n\nApart from biomass, which accounts for about 3.5% of total generation capacity, no other renewable energy source besides hydroelectricity plays a relevant role in the energy mix. However, the potential for wind energy, which is concentrated in the Northeast, is very large. It is about 143 GW, which exceeds current installed capacity and is second only to Argentina in the LAC region. There are projects for the development of biomass, solar and wind energy.\n\nIn 2002, the government of Brazil created a Program to Foster Alternative Sources of Electric Power (PROINFA). The program aims to increase the participation of wind power sources, biomass sources and small hydropower systems in the supply of the Brazilian grid system through Autonomous Independent Producers (PIA). The medium to long-term objective (i.e. 20 years) of the program is that the defined sources supply 15% of the annual market growth until they reach 10% of the nation's annual electric power demand/total consumption.\n\nThe power sector in Brazil was essentially in government's hands until the early 1990s. The sector had seen remarkable development in the 1970s. However, by the late 1980s, the state-ownership model was on the verge of collapse. This delicate situation was the result of heavily subsidized tariffs and a revenue shortfall in the sector of about US$35 billion, which led to the delay in the construction of about 15 large hydro plants due to lack of funds for investment. Efforts to address the deterioration of the sector were not successful, a situation that further intensified the need for deep reforms. A major commitment was made by President Cardoso to carry out a substantial reform of the Brazilian electricity sector. The first reforms introduced in the power sector were aimed to allow the participation of private capital and also to improve its economic situation.\n\nThe Project for Restructuring the Brazilian Electric Sector, RESEB, which laid down the first steps for the implementation of the power sector reform, was initiated in 1996 during the administration of President Cardoso. The objective of the reform was to build a more competitive power sector with the creation of a level playing field for private sector participation. In addition, state-owned utilities and assets were privatized. Although transmission assets were not privatized, most of the expansion of the transmission network has been carried out by private capital. This reform also led to the creation, in 1996, of ANEEL (Brazil's National Electricity Regulatory Agency), a quasi-independent regulatory body in charge of overseeing the electricity sector. However, the main restructuring steps were taken with the enactment of the 1998 Law (Law 9648/98). Those steps included the creation of an independent operator of the national transmission system (ONS) and an operator of the commercial market (MAE), which did not become operational until 2001. \nAs a result of the reforms of the power sector, new capital was attracted, both in terms of privatization and greenfield projects. Some of the state-owned generation capacity was acquired by foreign investors such as Tractebel, AES, Prisma Energy, El Paso and Duke, which became significant producers. In addition, local investors such as industrial groups, large customers, utilities and pension funds also invested heavily in the national generation sector. Other companies such as EdF (\"Électricité de France\"), Endesa and Chilectra focused on the distribution segment, a segment in which privatization brought improved quality of service and a reduction of theft, non-payments and technical losses.\n\nHowever, the reforms were not successful in preventing the energy crisis that was to unfold in 2001. Installed capacity expanded by only 28 per cent during 1990-99, whereas electricity demand increased by 45 per cent. In 1999, as the power shortage was already foreseen, the President Cardoso Administration made efforts to increase private investment in the electricity sector through a Priority Thermal Power Program (PPT) that aimed at the expeditious construction of more than 40 gas-fired thermal plants. Unfortunately, the needed investment did not materialize and the crisis became unavoidable.\n\nBrazil was faced with one of the most serious energy crises in its history in 2001-2002. The crisis was the direct result of a sequence of a few years drier than average in a country with over 80% of hydroelectric generation capacity. Additionally, several delays in the commissioning of new generation plants and transmission problems in the third circuit from the Itaipu hydropower plant accounted for a third of the energy deficit. Reservoir levels reached such low levels that supply could not be ensured for more than four months.\n\nIt was soon clear that strict demand reduction programs would be needed to avoid widespread blackouts. In June 2001, the government created the Crisis Management Board (CGE), chaired by President Cardoso himself. The CGE received special powers among which was the authority to set up special tariffs, implement compulsory rationing and blackouts, and bypass normal bidding procedures of the purchase of new plant equipment. Instead of resorting to rolling blackouts, the government chose to apply a quota system. Quotas were established for all the consumers based on historical and target consumption level, applying bonuses for consumption well below the prescribed level, penalties for over-consumption and some freedom for the large users to trade their quotas in a secondary market. The government's goal of reducing historical consumption levels by at least 20% for an eight-month period was successfully achieved, with the government having to pay over US$200 million in bonuses to residential, industrial, and commercial customers. This achievement allowed the system to overcome that long period without blackouts and brownouts and proved the potential of demand-side management and energy efficiency efforts, which were able to create a virtual capacity of 4,000MW, helping the country to bridge the supply demand gap in a very economic way. In addition, the government launched a program for contracting emergency generation capacity, with bids for a total of 2,100MW of new thermal capacity accepted.\n\nHowever, the crisis affected numerous actors. Generators and distributors experienced a 20% reduction in their revenues due to the contraction in consumption. This situation was eventually addressed by an increase of tariffs approved by the government. The financial situation of distributors was also damaged, with customers also suffering from the increase in electricity prices (140% in nominal terms between 1995 and 2002).\n\nIn January 2003, the new administration led by Luiz Inácio Lula da Silva took over among criticism of the reforms introduced in the electricity sector by the administration of President Cardoso, supporting a model in which the system should be fully regulated. The pending privatizations of three generation subsidiaries of the large state-owned utility, Eletrobras, were stopped. However, despite initial expectations, the new administration opted for a model that clearly aims to attract long-term private investment to the sector and that heavily relies on competition. In addition, the existing institutions were preserved and in some cases strengthened, with a new company, EPE, created with the specific mission of developing an integrated long-term planning for the power sector in Brazil.\n\nThe new legislative framework was defined by Law 10,848/2004, which established clear, stable and transparent rules aimed at ensuring supply and the continuous expansions of the intrinsic sector activities (generation, transmission and distribution). The expansion was linked to a fair return on investments and to universal service access, together with tariff adjustments. Decree 5,081/2004 approved the regulatory framework for the power sector, specifying specific provisions to achieve the objectives of the reform. \n\nOne of the defining elements of the model adopted by the new administration is the establishment of energy auctions as the main procurement mechanism for distribution companies to acquire energy to serve their captive consumers. This initiative assisted in the introduction of competition in the power sector and also helped to address some of the existing market imperfections. Under this system, auctions of capacity from new generation projects will be held three to five years in advance of delivery dates. The Ministry of Mines and Energy wants to ensure that the totality of future expansion needs is met and that plants are only built once they have won bids in energy auctions and are guaranteed long-term contracts. The first auction was held in December 2004, with contracts for a total of about 40GW traded.\n\nAverage electricity tariffs for the different sectors in 2007 were as follows:\n\n\nIn the last 20 years, Brazil has been one of the main recipients of private capital investment in its power sector. Total investment by private actors in the power sector between 1994 and 2006 amounted to US$56,586 million in 124 projects. However, despite Brazil's deregulation and higher tariffs in the \"new energy\" auction system, investment, particularly in generation, has slowed significantly. This situation is not considered to be the result of concerns about the regulatory model or auction pricing caps, but it reflects the lack of available projects. The existing delays in granting environmental licenses and the uncertainties on the Bolivian gas supply, explain to a great extent the lack of hydroelectric and gas-fired thermoelectric projects respectively.\n\nThe investment required in power generation over the next 10 years is R$40 billion or around US$24.2 billion (April 29, 2008). This high investment will only be realized if the government succeeds in attracting greater private-sector investment.\n\nIn Brazil, large government-controlled companies dominate the electricity sector. Federally owned Eletrobras holds about 40% of capacity (including 50% of Itaipu), with state-companies CESP, Cemig and Copel controlling 8%, 7% and 5% of generation capacity respectively. About 27% of generation assets are currently in the hands of private investors.\n\nTransmission, it has remained almost exclusively under government control through both federal (Eletrobras) and state companies (mainly Sao-Paulo-CTEEP, Minas Gerais-Cemig, and Parana-Copel) until recently. However, under the new sector regulatory model, there are about 40 transmission As for distribution, there are 49 utilities with distribution concessions and about 64% of distribution assets are controlled by private sector companies.\n\nThe Ministry of the Environment holds the environmental responsibilities in Brazil. One of its associated institutions is Ibama, the Brazilian Institute for the Environment and Renewable Natural Resources, which is in charge of executing the environmental policies dictated by the Ministry regarding environmental licensing; environmental quality control; authorization of the use of natural resources; and environmental monitoring and control among others.\n\nOLADE (Latin American Energy Association) estimated that CO emissions from electricity production in 2003 were 20 million tons of CO, which corresponds to less than 7% of total emissions from the energy sector. This low contribution to emissions from electricity production in comparison with other countries in the region is due to the high share of hydroelectric generation.\n\nBrazil is host to the largest number of CDM projects in the Latin America region. Registered projects represent 40% of the total in the region and account for 45% of Certified Emission Reductions (CERs) (up to 2012).\n\nAs for the power sector, there were 91 projects registered in March 2008, adding up to an estimated total of 9 million tons of CO per year. The distribution of projects by category is as follows:\n\"Source\": UNFCCC\n\nAn exergoeconomic assessment accounting for the total and non-renewable unit exergy costs and specific CO2 emissions of Brazilian electricity is performed by Flórez-Orrego et al. (2014), comprising thermal, nuclear, hydro, wind farms and biomass-fired power plants. The analysis starts from the fuel obtainment and continues through the different stages of construction, fuel transportation and processing, operation and decommissioning of the plant, with electricity generation as the desired output. This approach allows the calculation of direct CO2 emissions as well as the upstream and downstream emissions, which play an important role in some technologies. In this way, a better comparison between the utilization of different fuels in the electricity generation can be achieved. An iterative calculation procedure is used to determine the unit exergy costs of electricity and processed fuels, since both electricity and processed fuel are used in their own production routes.\n\nAs it was expected, fossil-fired power plants presents the highest specific CO2 emissions, with the coal-fired power plants leading the group. However, even though fossil-fired power plants presents the most marked environmental impacts, their total unit exergy costs are much lower than that presented by sugar cane bagasse-fired power plants. This shows that, although almost renewable, the typical configurations of sugar cane bagasse-fired power plants are far from being efficient technologies. Hydro and wind farms present the lowest specific CO2 emissions as well as the lowest unit exergy cost. Due to the high participation of renewable sources in the production of electricity (near to 89% of the total), Brazilian electricity mix emissions are found to be 7.5 and 11.8 times lower than Europe and World electricity mixes. Also, owed to the higher efficiency of hydroelectric power plants, which contribute to the major part of the electricity generation in Brazil, the total unit exergy cost is lower, and thus, exergy efficiency of electricity generation is higher if compared with countries based on fossil fuels for electricity generation.\n\nApparently, total exergy cost of wind and natural-gas fired technologies are almost the same, but contrarily to the wind power plants, the non-renewable unit exergy costs of NG-fired power plants is practically equal to the total cost. It should be noted that this result is a consequence of the efficiency assumed for wind power plants. If energy storage is to be taken into account for intermittent technologies such as wind farms, the total exergy cost could be slightly increased. The upstream and downstream CO2 emissions in the coal route represent a very small part of the total CO2 emissions, if compared with the direct emissions of coal burning in the power plant. Finally, it is pointed out that controversies related to the flooding dams of vast zones with complex ecosystems should be carefully analysed since, according to the results reported by Dones et al., the GHG emissions could be increased up to achieve emission levels comparable to those of gas combined cycles power plants.\n\nThe Inter-American Development Bank (IDB) is currently (April 2008) supporting several projects and contributing to various technical assistance initiatives in the power sector in Brazil. The most relevant projects with financing from the IDB are:\n\n\nThe World Bank is currently (April 2008) supporting three rural poverty reduction projects that include the provision of access to electricity services:\n\n\n\n"}
{"id": "13267735", "url": "https://en.wikipedia.org/wiki?curid=13267735", "title": "Erden Eruç", "text": "Erden Eruç\n\nErden Eruç ( \"Air-den Air-rooch\"; born 14 July 1961) is a Turkish-American adventurer who became the first person in history to complete an entirely \"solo\" and entirely human-powered circumnavigation of the Earth on 21 July 2012 in Bodega Bay, California, United States. The journey had started from Bodega Bay a little more than five years earlier on 10 July 2007. The modes of transport included a rowboat to cross the oceans, a sea kayak for shorelines, a bicycle on the roads and hiking on trails, along with canoes for a few river crossings. The route he followed was long, crossed the equator twice and all lines of longitude, and passed over twelve pairs of antipodal points, meeting all the requirements for a true circumnavigation of the globe. Guinness World Records has officially recognized Eruç for the \"First solo circumnavigation of the globe using human power\" on a journey that lasted 5 years 11 days 12 hours and 22 minutes, the current world record time for a solo human-powered circumnavigation.\n\nEruç's human-powered circumnavigation plan was expanded to include summitting the tallest mountains on six continents as a tribute to his friend and fellow adventurer Göran Kropp who died in 2002 while climbing with Eruç in Vantage, Washington. Eruç named his expedition the \"Six Summits Project\". So far he has summitted three of the peaks including Denali (also known as Mount McKinley) in North America on 29 May 2003 more than four years before he began his solo circumnavigation, then Mount Kosciuszko in Australia on 10 April 2010, and Mount Kilimanjaro in Africa on 14 June 2011 during the circumnavigation. Eruç still plans to climb the remaining three mountains on future journeys: Mount Everest in Asia, Mount Elbrus in Europe and Aconcagua in South America.\n\nBy the end of his circumnavigation, Eruç had set several ocean rowing world records including the first person to row three oceans, the first rower to cross the Indian Ocean from Australia to mainland Africa (in two segments), the longest distance rowed across the Indian Ocean, and the longest distance rowed across the Atlantic Ocean.\n\nA documentary film called \"Castaway With Purpose\" will feature Eruç's circumnavigation. The film has been in production since 2013 with no release date set.\n\nEruç was born in Nicosia, Cyprus on 14 July 1961 and raised in Turkey. He has been an avid outdoorsman from an early age. When he was 11, his father took him for a climbing trip to Mount Erciyes, an extinct stratovolcano in south central Turkey and the highest mountain in central Anatolia with a summit at . During his higher education years, Eruç studied mechanical engineering at Boğaziçi University in Istanbul where he earned both a Bachelor of Science degree and a Master of Science degree. In 1986, he moved to the United States where he continued his studies in engineering and business administration, earning a second Master of Science degree in Engineering Mechanics at Ohio State University and an MBA degree at George Mason University.\n\nWhen his schooling was complete, Eruç worked in various technical consulting projects in the U.S. for nine years, advancing into project management. At the age of 41 he left the corporate office world, allowing him all the time he needed to pursue a dream of far-flung outdoor adventures. His new occupation was educating and inspiring others, especially children, through the pursuit of entirely human-powered travels around the world.\n\nDuring his journey to Alaska to climb Mount McKinley in June 2003, Eruç married Nancy Board in a native Alaskan Haida-Tsimshian ceremony on a beach near Homer, Alaska, though the culture is foreign to both of them as Eruç is a Turkish Muslim while Board is an American Catholic. Board is also an outdoor enthusiast and the acting Vice President of the \"Around-n-Over\" nonprofit organization.\n\nIn December 2002, Eruç established a 501(c)(3) nonprofit organization called \"Around-n-Over\" based in Seattle which received IRS approval as a 501(c)(3) nonprofit entity late in 2003. The \"Around-n-Over\" organization has the following mission statement: \"All for Education and Inspiration... To accomplish, to inspire and to teach... We are in the business of realizing dreams, and helping others achieve theirs.\"\n\nThe organization was also formed to honor fellow adventurers who had lost their lives, especially Göran Kropp who fell and died while climbing with Eruç in September 2002. Additionally, \"Around-n-Over\" provides the necessary structure for handling funds for expedition expenses and charitable donations to other organizations. The Turkish \"İLKYAR Foundation\" is one such charity, which provides assistance to Turkish elementary and middle school children in rural parts of the country with a special emphasis on trying to keep girls in school. The Mateves Secondary School near Mount Kilimanjaro is also being assisted by funds donated to \"Around-n-Over\".\n\nThe organization's name is based on Eruç's plan of circumnavigating (going \"around\" an approximate great circle of) the Earth using only his own power and (-\"n\"-) also summitting (going \"over\") the highest peaks on each of the continents, excepting only Antarctica. Eruç serves as the President and Chief Exploration Officer of \"Around-n-Over\".\n\nIn September 2004, Eruç committed to purchasing a used and proven by oceangoing plywood rowboat, the same vessel which he would eventually row across three oceans to reach two more summits in his \"Six Summits Project\". The rowboat was christened \"Kaos\" by its first owners and was later renamed \"Calderdale – the Yorkshire Challenger\", or simply the \"Calderdale\", by its second owners. Before Eruç acquired it, the \"Calderdale\" had already successfully crossed the Atlantic Ocean twice with two-person teams aboard. The boat is listed as the \"Around-n-Over\" on the website of \"Guinness World Records\". Eruç has not officially renamed the boat and still refers to it as the \"Calderdale\". He has stated that the naming rights to the rowboat are available to a willing sponsor.\n\nThe bare and loaded rowboat was equipped with many advanced navigational, safety and communications systems as well as a pair of 12 volt gel batteries charged by a solar panel that powered them all. The loaded rowboat contained an Argos tracking beacon, an EPIRB distress beacon, satellite phone, GPS navigator, radar transponder, radar reflector, VHF radio, palm-size computer, one manually operated and two powered desalination units, a medical kit, a watertight cabin and a life raft with an emergency bag of supplies.\n\nWhen not in active use, the rowboat has been housed at the Foss Waterway Seaport's Working Waterfront Maritime Museum in Tacoma, Washington.\n\nPrior to his successful circumnavigation of 2007 to 2012, Eruç had developed a substantially different route plan. This initial plan still began with his roundtrip bicycle ride from Seattle to Mount McKinley in Alaska from 1 February to 24 August 2003 with the summit being reached on 29 May.\n\nEruç had planned to row south from Seattle to South America to continue the project with a climb of Aconcagua. On 3 October 2004, however, Eruç once again left Seattle riding his fully loaded bicycle and arrived in Miami on 25 December. He had selected Miami as a new and potentially better starting point for a circumnavigation attempt combined with the summits project. The plan at that time was to row from Miami through the Caribbean Sea and then the Panama Canal to the Pacific, row down the west coast of South America, bike to and climb Aconcagua, row to New Guinea, bike to and climb Carstensz Pyramid (an alternate to Kosciuszko for the highest peak in Oceania), row to the Asian mainland, bike to and climb Everest, row to Africa, bike to and climb Kilimanjaro, row to the Middle East, bike to and climb Elbrus, and finally row the Mediterranean Sea and Atlantic Ocean back to Miami.\n\nA change in plans occurred when Eruç learned about Tim Harvey, a fellow human-powered adventurer, and Harvey's desire for a way home to Vancouver, Canada from Europe. Eruç contacted Harvey and had the \"Calderdale\" shipped to Portugal. The two men began rowing from Lisbon on 16 October 2005 intending to cross the entire Atlantic Ocean together; however, conditions prevented them from rowing away from Las Palmas de Gran Canaria after their arrival on 11 December. Harvey decided to go ahead with another group travelling by sailboat, thereby ending the human-powered part of his expedition but remaining emission-free since no motor was used. By late 2005, Eruç had considered making the entire journey a two-person rowing circumnavigation and consequently had started sending out requests for new rowing partners and sponsors. He eventually continued on alone and completed his first solo Atlantic row starting from Las Palmas on 29 January and finishing in Guadeloupe on 5 May 2006. The idea of a circumnavigation by rowboat was nearly abandoned when sponsorships and partners failed to materialize. Early in 2007, however, the \"Aktaş Group\", a Turkish transportation and construction holding company, came forward as a principal sponsor and a new solo circumnavigation plan was set in motion.\n\nIn early May 2007, Eruç once again departed Seattle this time riding his bicycle to Tiburon, California on the north side of San Francisco Bay. After fighting very strong onshore winds while trying to row out of San Francisco Bay in early and mid-June, he departed instead from Bodega Bay, California on 10 July 2007. Eruç had bicycled from Tiburon to Bodega Bay while his rowboat was transported separately to the new starting point, as it was during all later segments of the journey. The intended destination for his first arrival port was Mooloolaba, which is located just north of Brisbane in Australia, after crossing the immensity of the Pacific Ocean. Eruç calculated that the straight-line distance was while rowing in the middle of the ocean, based on distances from his boat to both shores. Due to opposing winds and ocean currents during rest periods, the actual distance rowed would be longer.\n\nThe rowboat proved very capable in rough seas and was only capsized once in crossing three oceans. On 20 December 2007 in the middle of the Pacific Ocean, a large rogue wave tipped the boat about 120 to 150 degrees in the estimation of Eruç. The wave hit while he was asleep in the cabin and threw him to the ceiling and then back to the floor as the boat righted itself due to its ballast. The only losses of any value from the open deck were energy gels and fluid replacement packets, with all other valuables being either tied down or stowed securely.\nOn 10 January 2008, an emergency signal was received from Eruç's Argos tracking beacon. A search and rescue operation was almost started when a call from Eruç was received indicating it was a false alarm.\n\nUnusually strong currents in the middle and western Pacific made progress south very difficult for Eruç. After his launch from Bodega Bay, La Niña conditions had developed on the Pacific. He was ultimately unable to cross the equator on his first attempt due to the opposing currents and wind patterns in the Intertropical Convergence Zone. Running dangerously low on food supplies after fighting strong La Niña winds from the southeast in the early part of the main typhoon season, Eruç was effectively trapped in the northern hemisphere. He accepted assistance from Filipino fishermen of the \"Frabelle Fishing Corporation\" north of Papua New Guinea (PNG) on 17 May 2008. The first typhoon of that season had formed on 5 May due northwest of him and would turn into a Category-4 super typhoon named Rammasun. He had reached the PNG waters near Ninigo Islands before the winds carried him offshore.\n\nEight months later on 15 January 2009, after the typhoon season had ended, the same fishermen returned him to the exact location where they had found him. Continuing on the Bismarck Sea, Eruç crossed the equator and reached Finsch Harbor in PNG on 4 February. At this point, he took another break of nearly eight months to mend his injured back. Beginning on 22 September 2009 he continued on foot and by sea kayak along the Solomon Sea shores of PNG and then walked shore-to-shore from Oro Bay to Port Moresby over the historic Kokoda Track until 26 November 2009.\n\nDeparting from Port Moresby on 8 December 2009, Eruç rowed his boatwhich had been shipped around PNG for himacross the Coral Sea and reached the Cape York Peninsula, Australia on 10 January 2010. He sea kayaked down the coast from 28 January to 15 February 2010 until he reached Cooktown where he continued by bicycle on 18 February. Eruç bicycled along the east coast toward Mount Kosciuszko, the second peak of his \"Six Summits Project\", which he summitted on 10 April. He continued again by bicycle along the southern and western coastal areas of the country. After preparing and resupplying his rowboat in Perth through that June, Eruç had the rowboat shipped north to Carnarvon as a more favorable departure point. He bicycled to Carnarvon and arrived on 7 July 2010.\n\nEruç departed from Carnarvon on 13 July 2010 to cross the Indian Ocean to Africa. After three and a half months of solitary rowing, a frigate named \"TCG Gaziantep\" of the Turkish Navy rendezvoused with Eruç north of Madagascar on 30 October. Coincidentally, the frigate was named after the Turkish city where Eruç's mother had been raised, though she was born in nearby Kilis. After exchanging pleasantries and gifts via a small zodiac boat, the frigate continued escorting Eruç as he rowed westward until sunset that day. The area was being patrolled as part of a Combined Task Force to combat piracy and the commander of that task force, Rear Admiral Sinan Ertuğrul, was on the frigate. Ertuğrul and Eruç had been exchanging emails about his route and expected landfall in Africa, and were attempting to steer him clear of pirate activity.\n\nIn November 2010 as the cyclone season was starting, Eruç found that he could not continue toward the African coast because a mesoscale eddy, or vortex, had formed around him and was pushing him away from the coast. He witnessed waterspouts while rowing in the Mozambique Channel and changed direction to the south-southeast reaching Mahajanga in Madagascar on 26 November without encountering any pirates. The row from Australia to Madagascar had taken about four and a half months. Eruç took a four-month break until the cyclone season ended before rowing westward to the African coast from Mahajanga on 26 March 2011 making landfall at Angoche in Mozambique on 20 April.\nEruç began bicycling across the continent twelve days later on 2 May with a side excursion to prepare for and climb Mount Kilimanjaro in Tanzania, the third peak of his \"Six Summits Project\", from 8 to 15 June 2011. His climbing party of more than a dozen people, including his wife and his 78-year-old father Cemal Eruç, summitted the mountain on 14 June.\n\nWhile bicycling in northern Mozambique, Eruç had crashed his bicycle several times on an unexpectedly sandy road. On 26 June 2011, Eruç crashed over rumble strips in Kabuku, Tanzania while avoiding a passing bus. His bicycle was not damaged but his GPS unit and Argos beacon were broken, his left thumb was hyperextended, and his right hip and right forearm had hit the pavement hard. After treating the wounds and taping his swollen thumb to his forefinger, Eruç continued southwestward through Zambia and Namibia. By 21 August he had reached the west coast of Africa. His rowboat had been transported, and then prepped and resupplied during a seven-week break. He departed from Lüderitz, Namibia on 10 October 2011.\n\nThe crossing of the Atlantic Ocean to South America lasted about five months until 11 March 2012 when Eruç reached Güiria, Venezuela. To reach a more favorable departure point for rowing, he completed a relatively short bicycle trip of along the coast of Venezuela to Carúpano on 19 March. His rowboat had been moved to the port of Carúpano so he could continue from there. The final rowing segment was across the Caribbean Sea and the Gulf of Mexico to Cameron, Louisiana in the United States from 21 March to 27 May 2012. The final segment overall was a bicycle ride starting on 21 June and ending on 21 July 2012 at the same pier where he had started, in Bodega Bay, California.\nEruç logged while rowing across three oceansthe Pacific, Indian and Atlanticand cycling across three continentsAustralia, Africa and North America. He became the first person to solo circumnavigate the globe entirely on his own power. He crossed the equator two times, passed over twelve pairs of antipodal points and spent five years and eleven days of his life completing the endeavorthe world record time for a human-powered circumnavigation. The total elapsed time of over five years included several long periods of downtime spent away from the route, for a total of about 26 months, with Eruç always continuing again from the exact location where he had last stopped. Excluding the downtime periods, he had traveled a total of 1026 days, or about two years and ten months.\n\nThe remaining three mountains of the \"Six Summits Project\"Everest, Elbrus and Aconcaguawere skipped during the circumnavigation primarily due to a lower level of donations than was anticipated which led to budgetary constraints and the decision to shorten the route. Eruç's decision to bypass those mountains resulted in a route that more closely followed a great circle route than the one he had originally envisioned.\n\nTo finance the expedition, Eruç and his wife had sold condominium properties in Washington, D.C. and Seattle, as well as a second car, and moved into a rental property. Eruç also withdrew the funds from his 401k retirement plan. The total spent out of the couple's own assets was approximately $216,000. Their organization's sponsors and donors contributed a similar amount in cash and products, including his bicycle, bike trailer and panniers, a liferaft, desalinating watermakers, energy bars and freeze dried foods.\n\nEruç maintained a blog of his adventure by posting dispatches on his website every few days. He was able to communicate directly with family, friends and schoolchildren in their classrooms, as well as medical and scientific experts, via a satellite phone link with email capability.\n\nOccasional technical problems caused Eruç to record audio dispatches to keep his followers up-to-date. \nAll dispatches are stored on the \"humanedgetech\" website and are linked from the dispatch pages of \"Around-n-Over\".\n\nThe following three audio dispatches were recorded between Madagascar and the African coast:\n\nThese two were recorded while crossing the Caribbean Sea:\n\nThe last two were recorded while crossing the Gulf of Mexico:\n\n\"Castaway With Purpose\" is a documentary film in production since 2013 that will feature Eruç's circumnavigation. The documentary crew will travel to some of the same places Eruç passed through, filming those places along with the people he met. The crew will then combine the new film with highlights from many hours of existing video footage, still images and journal entries created by Eruç during the five-year voyage. A Kickstarter fundraising campaign was held in the summer of 2013 to help fund the film's production. The crowdsourced fundraiser ended successfully on 18 August 2013 with 286 backers pledging a total of $75,041 to the project.\n\n\n, Eruç has set the following Guinness World Records:\n\n\n\n\nEruç rowed with Louis Bird, the son of Peter Bird, as a substitute for a race partner who had withdrawn for health reasons. They set the fastest time in the \"classic pair\" class to row mid-Pacific east-to-west in the Great Pacific Race, from Monterey Bay, California to Waikiki, Hawaii, in 54 days, 3 hours and 45 minutes, finishing first among three boats in the same class. The race result is also a Guinness World Record.\n\n\n\nAll the following articles were written before the successful circumnavigation began and contain different plans from the actual route taken between 2007 and 2012.\n\n"}
{"id": "6070319", "url": "https://en.wikipedia.org/wiki?curid=6070319", "title": "European Broadcasting Area", "text": "European Broadcasting Area\n\nThe European Broadcasting Area (EBA) is defined by the International Telecommunication Union (ITU) as such:\n\nThe EBA includes territory outside Europe, and excludes some territory that is part of the European continent. For example,\nArmenia, Azerbaijan and Georgia were defined as outside the EBA borders until 2007. After EBA was expanded by the 2007 World Radiocommunication Conference (WRC-07) to include those three countries, the only ITU member state with territory in Europe while remaining outside the EBA is Kazakhstan. While Kazakhstan is excluded from EBA, it is still eligible to apply for membership of the Council of Europe.\n\nThe boundaries of the European Broadcasting Area have their origin in the regions served and linked by telegraphy cables in the 19th and early 20th centuries. The European Broadcasting Area plays a part in the definition of eligibility for active membership in the European Broadcasting Union (EBU) and thus participation in the Eurovision Song Contest. As of November 2017, the European Broadcasting Union has 73 members from 56 countries, 33 associates from 21 countries, and 7 approved participants.\n\n\nThe following jurisdictions also rest inside the EBA borders, but cannot join the ITU or EBU due their dependent status or limited recognition:\n\nOverview\n\nThe members of the \"European Broadcasting Union\" are able to provide their audience with a variety of channels in different countries. Every country included in the European Broadcasting Area consists of different companies that spread the news to the public in a multitude of ways. Below are some countries and the companies that deliver information and entertainment to their viewers in the European Broadcasting Area.\n\nCompanies\n\nAlgeria\n\nThe companies in Algeria are Etablissement Public de Radiodiffusion Sonore, Etablissment Public de Télévision Algérienne, and Télédiffusion d’Algérie. The Etablissement Public de Radiodiffusion Sonore is the main radio company in Algeria. It consists of three different radio stations that transmit programs talking about regional, local, or international life related to Algerian nationalism. There are three different radio national channels because each one transmits programs in different languages. The first channel transmits information in Arabic while the second and third channel use the French language. The Etablissment Public de Télévision Algérienne is the main television company in Algeria. The company’s main motive is the same as Algeria’s radio company which is to educate and entertain their audience with programs about regional, local, and international life along with current events from around the world. The company transmits their channel to Europe, North Africa, and parts of the Middle East. The Télédiffusion d’Algérie is in charge of distributing radio and television channels and programs through technological advancements.\n\nBelgium\n\nIn Belgium, the companies are Radio-Télévision Belge de la Communauté française and Vlaamse Radio en Televisieomroep. Radio-Télévision Belge de la Communauté française (RTBF) is a public corporation that is focused on the needs of French speaking Belgian citizens and their aspiration is to educate and entertain their audience. Their ways of distributing information are four television channels, six radio stations, their webpage, and social networks. The Vlaamse Radio en Televisieomroep is a public service broadcasting company for Flemish people. Their focus is to provide information about the Flemish culture and identity in an open and diverse way. The company consists of a few radio stations and television news and sports programs.\n\nDenmark\n\nIn Denmark, the companies are Danmarks Radio and TV 2 DANMARK. Danmarks Radio (DR) is the oldest Danish Broadcasting Corporation in Denmark with regard to electronic media business. Danmarks Radio broadcasts information about news and entertainment through six television channels, nine radio channels, orchestras, and apps. TV 2 DANMARK is a government-owned company and is Denmark’s most watched channel with a number of sister channels.\n\nIreland\n\nThe companies in Ireland are Raidió Teilifís Éireann and TG4. Raidió Teilifís Éireann (RTÉ) is a public media organization in Ireland that grants their audience extensive multi-media services. RTÉ distributes their programs over seven television channels, nine radio stations, and their website. TG4 is a television channel known for its use of the Irish language. TV3 is an independent channel that is available on every television in Ireland with an average of 650,000 people tuning in to watch it a day.\n\nFrance\n\nSome of the companies in France are France Télévisions, France Médias Monde, and Radio France. France Télévisions is a television company owned by the government. It became France Télévisions with the consolidation of the television channels France 2, France 3, France 4, France 5, and France Ô. France Médias Monde controls France 24, RFI, and Monte Carlo Doualiya. The company is an international broadcasting service that transmits from the South of France to different parts of the world. France 24 is the international news channel, RFI is the international radio station, and Monte Carol Doualiya is an Arab-speaking radio station. Radio France is a national broadcasting company made up of seven other channels and forty-four local stations.\n\nGermany\n\nSome of the companies in Germany are Deutsche Welle, Deutschland Radio, Rundfunk Berlin- Brandenburg, Saarländischer Rundfunk, and Südwestrundfunk. Deutsche Welle (DW) is Germany’s international broadcasting service. DW is made up of six channels that transmit in 30 different languages internationally. Deutschland Radio is the German radio that consists of three programs. The German radio is a sign of nationalism because Germany did not have radio like other countries. The three programs are Germany Spark, Germany Kultur, and DRadio Knowledge. Germany Spark focuses on broadcasting information and is based in Cologne. Germany Kultur focuses on culture in the nation. Dradio Knowledge is an entertainment program that broadcasts pop culture news and music. Rundfunk Berlin- Brandenburg (RBB) is a merger of Sander Freies Berlin (SFB) company and the East German Rundfunk Brandenburg (ORB). RBB consists of broadcasting information through television and radio channels. Saarländischer Rundfunk (SR) is a radio and television company in Germany. SR consists of four radio stations and three television programs. Südwestrundfunk (SWR) is a public media company with many radio and television programs. The SWR programs consist of SWR1, SWR 2, SWR 3, SWR 4, SWRinfo, DASDING, and SWR Classic.\n\nNetherlands\n\nIn the Netherlands, some companies are KRO-NCRV, NTR, Nederlandse Omroep Stichting, Nerderlandse Publieke Omroep, and Omroep MAX. KRO-NCRV is a collaboration broadcaster made up of the association of KRO and the NCRV association. The company provides their audience with information about their society, their culture, and social needs. NTR an independent public service broadcaster that provides their viewers with unique programs. Nederlandse Omroep Stichting (NOS) is a television company that transmits national Dutch news and international news. NOS provides the Dutch viewers with information about news, sports, and events with a number of television programs. Nerderlandse Publieke Omroep (NPO) is a public service broadcaster for any who wants to watch or listen. NPO broadcasts through television and radio programs and social media. Omroep MAX is a public broadcasting service specially designed with programs for people of 50 years of age and older.\n\nSweden\n\nIn Sweden, the companies are Sveriges Radio Ab, Sverieges Television Sb, Swedish Educational Broadcasting Company. Sveriges Radio Ab (SR) is an independent radio company for the Swedish population. The company consists of 13 radio stations that broadcasts different information. Radio station P1 is for qualified news, P2 is for classical, jazz, and folk music, P3 is for mostly for the young adults with a variety of programs, P4 is for national and international news and pop music, etc. Sverieges Television Ab (SVT) is Sweden’s public service television company. SVT consists of four channels which are SVT1, SVT2, SVT3, SVT4. Swedish Educational Broadcasting Company (UR) is in collaboration with SR and SVT and focuses on broadcasting educational programs.\n\nUnited Kingdom\n\nThe United Kingdom’s companies are British Broadcasting Corporation, Sianel 4 Cymru, UK STV, and United Kingdom Independent Broadcasting. British Broadcasting Corporation (BBC) is a public service broadcaster in the UK and the world through various television channels, radio stations, and online websites. Sianel 4 Cymru (S4C) is a public service broadcaster that transmits Welsh language programs. Also, it is the only Welsh-speaking channel in the whole world. The channel broadcasts more than 155 hours of programming a week. UK STV is Scotland’s digital media company. STV broadcasts their programs on TV, on demand, and online. United Kingdom Independent Broadcasting (ITV) is a producer broadcaster with the availability of many different channels for their viewers.\n\n"}
{"id": "1044338", "url": "https://en.wikipedia.org/wiki?curid=1044338", "title": "Evolution of Hawaiian volcanoes", "text": "Evolution of Hawaiian volcanoes\n\nThe fifteen volcanoes that make up the eight principal islands of Hawaii are the youngest in a chain of more than 129 volcanoes that stretch across the North Pacific Ocean, called the Hawaiian-Emperor seamount chain. Hawaii's volcanoes rise an average of to reach sea level from their base. The largest, Mauna Loa, is high. As shield volcanoes, they are built by accumulated lava flows, growing a few meters/feet at a time to form a broad and gently sloping shape.\n\nHawaiian islands undergo a systematic pattern of submarine and subaerial growth that is followed by erosion. An island's stage of development reflects its distance from the Hawaii hotspot.\n\nThe Hawaiian-Emperor seamount chain is remarkable for its length and its number of volcanoes. The chain is split into two subsections across a break, separating the older Emperor Seamount Chain from the younger Hawaiian Ridge; the \"V\" shape bend of the chain is easily noticeable on maps. The volcanoes are progressively younger to the southeast; the oldest dated volcano, located at the northern end, is 81 million years old. The break between the two sub-chains is 43 million years; in comparison, the oldest of the principal islands, Kauai, is little more than 5 million years.\n\nThe \"assembly line\" that forms the volcanoes is driven by a hotspot, a plume of magma deep within the Earth producing lava at the surface. As the Pacific Plate moves in a west-northwest direction, each volcano moves with it away from its place of origin above the hotspot. The age and location of the volcanoes are a record of the direction, rate of movement, and orientation of the Pacific Plate. The pronounced 43-million-year-old break separating the Hawaiian Ridge from the Emperor Chain marks a dramatic change in direction of plate movement. Initial, deeper-water volcanic eruptions are characterized by pillow lava, so named for their shape, while shallow-water eruptions tend to be composed mainly of volcanic ash. Once the volcano is high enough so as to eliminate interference from water, its lava flows become those of ropey pāhoehoe and blocky aa lava.\n\nOur current understanding of the process of evolution originates from the first half of the 20th century. The understanding of the process was advanced by frequent observation of volcanic eruptions, study of contrasting rock types, and reconnaissance mapping. More recently our understanding has been aided by geophysical studies, offshore submersible studies, the advent of radioactive dating, advances in petrology and geochemistry, advanced surveillance and monitoring, and detailed geological studies. The ratio of magnesium to silica in the lava is a sign of what stage the volcano is in, as over time the volcano's lavas shift from alkalic to tholeiitic lava, and then back to alkalic.\n\nAlthough volcanism and erosion are the chief factors in the growth and erosion of a volcano, other factors are also involved. Subsidence is known to occur. Changes in sea level, occurring mostly during the Pleistocene, have caused drastic changes; an example is the breakup of Maui Nui, initially a seven-volcano island, which was transformed into five islands as a result of subsidence. High rainfall due to the trade wind effect impacts on the severity of erosion on many of the major volcanoes. Coastline collapses, a notable part of the history of many of the Hawaiian volcanoes, are often devastating and destroy large parts of the volcanoes.\n\nWhen a volcano is created near the Hawaiian hotspot, it begins its growth in the submarine preshield stage, characterized by infrequent, typically low volume eruptions. The volcano is steep-sided, and it usually has a defined caldera and has two or more rift zones radiating from the summit. The type of lava erupted in this stage of activity is alkali basalt. Due to stretching forces, the development of two or more rift zones is common. The lava accumulates in a shallow magma storage reservoir.\n\nBecause the eruptions occur with the volcano underwater, the form of lava typically erupted is pillow lava. Pillow lava is rounded balls of lava that was given very little time to cool due to immediate exposure to water. Water pressure prevents the lava from exploding upon contact with the cold ocean water, forcing it to simmer and solidify quickly. This stage is thought to last about 200,000 years, but lavas erupted during this stage make up only a tiny fraction of the final volume of the volcano. As time progresses, eruptions become stronger and more frequent.\n\nThe only example of a Hawaiian volcano in this stage is Loihi Seamount, which is thought to be transitioning from the submarine preshield stage into the submarine phase of the shield stage. All older volcanoes have had their preshield stage lavas buried by younger lavas, so everything that is known about this stage comes from research done on Lōihi Seamount.\n\nThe shield stage of the volcano is subdivided into three phases: the submarine, explosive, and sub aerial. During this stage of growth, the volcano accumulates about 95 percent of its mass and it takes on the \"shield\" shape that shield volcanoes are named for. It is also the stage where the volcano's eruptive frequency reaches its peak.\n\nAs eruptions become more and more frequent at the end of the preshield stage, the composition of the lava erupted from the Hawaiian volcano changes from alkalic basalt to tholeiitic basalt and the volcano enters the submarine phase of the shield stage. In this phase, the volcano continues to erupt pillow lava. Calderas form, fill, and reform at the volcano's summit and the rift zones remain prominent. The volcano builds its way up to sea level. The submarine phase ends when the volcano is only shallowly submerged.\n\nThe only example of a volcano in this stage is Loihi Seamount, which is now transitioning into this phase from the preshield stage.\n\nThis volcanic phase, so named for the explosive reactions with lava that take place, begins when the volcano just breaches the surface. The pressure and instantaneous cooling of being underwater stops, replaced instead by contact with air. Lava and seawater make intermittent contact, resulting in a lot of steam. The change in environment also engenders a change in lava type, and the lava from this stage is mostly fragmented into volcanic ash. These explosive eruptions continue intermittently for several hundred thousand years. Calderas continually develop and fill, and rift zones remain prominent. The phase ends when the volcano has sufficient mass and height (about above sea level) that the interaction between sea water and erupting lava fades away.\n\nOnce a volcano has added enough mass and height to end frequent contact with water, the sub aerial substage begins. During this stage of activity, the explosive eruptions become much less frequent and the nature of the eruptions become much more gentle. Lava flows are a combination of pāhoehoe and ʻaʻā. It is during this stage, that the low-profile \"shield\" shape of Hawaiian volcanoes is formed, named for the shape of a warrior's shield. Eruption rates and frequencies peak, and about 95% of the volcano's eventual volume forms during a period of roughly 500,000 years.\n\nThe lava erupted in this stage form flows of pāhoehoe or ʻaʻā. During this subaerial stage, the flanks of the growing volcanoes are unstable and as a result, large landslides may occur. At least 17 major landslides have occurred around the major Hawaiian islands. This stage is arguably the most well-studied, as all eruptions that occurred in the 20th century on the island of Hawaii were produced by volcanoes in this phase.\n\nMauna Loa and Kīlauea volcanoes are in this phase of activity.\n\nAs the volcano reaches the end of the shield stage, the volcano goes through another series of changes as it enters the postshield stage. The type of lava erupted changes from tholeiitic basalt back to alkalic basalt and eruptions become slightly more explosive. Results from the Hawaii Scientific Drilling Project confirm that the eruption rates of the postshield stage volcanoes started decreasing between 600 and 400 thousand years (ka) ago.\n\nEruptions in the postshield stage cap the volcano with a carapace of lava, containing low silica and high alkali contents, the reverse of the stage before it. Some Hawaiian volcanoes diverge from this, however. Lava is erupted as stocky, pasty aa flows along with a lot of cinder. Caldera development stops, and the rift zones become more inactive. The new lava flows increase the slope grade, as the aā never reaches the base of the volcano. These lavas commonly fill and overflow the caldera. Eruption rate gradually decreases over a period of ca. 250,000 years, eventually stopping altogether as the volcano becomes dormant.\n\nMauna Kea, Hualālai, and Haleakalā volcanoes are in this stage of activity.\n\nAfter the volcano becomes dormant, the forces of erosion gain control of the mountain. The volcano subsides into the oceanic crust due to its immense weight and loses elevation. Meanwhile, rain also erodes the volcano, creating deeply incised valleys. Coral reefs grow along the shoreline. The volcano becomes a skeleton of its former self.\n\nKohala, Mahukona, Lanai, and Waianae volcanoes are examples of volcanoes in this stage of development.\n\nAfter a long period of dormancy and erosion of the surface, the volcano may become active again, entering a final stage of activity called the rejuvenated stage. During this stage, the volcano erupts small volumes of lava very infrequently. These eruptions are often spread out over several millions of years. The composition of the lavas erupted in this stage is usually alkalic. The stage commonly occurs between 0.6 and 2 million years after it has entered the weathering cycle.\n\nKoolau Range, West Maui, and Kahoolawe volcanoes are examples of volcanoes in this stage of development. Note, however, that because in this stage eruptions are very infrequent (occurring thousands or even tens of thousands of years apart), erosion is still the primary factor controlling the volcano's development.\n\nAfter the rejuvenated stage, the volcano is too far away from the hotspot to receive new magma, and therefore will never erupt again. The volcano continues to sink into the ocean, and become deeply eroded, leading to infrequent, but large collapses in its original structure. The volcano has no remaining magma in its chambers, and is truly dead.\n\nWest Molokai, Waialeale, and Niihau volcanoes are in this stage of development.\n\nEventually, erosion and subsidence break the volcano down to sea level. At this point, the volcano becomes an atoll, with a ring of coral and sand islands surrounding a lagoon. All the Hawaiian islands west of the Gardner Pinnacles in the Northwestern Hawaiian Islands are in this stage.\n\nAtolls are the product of the growth of tropical marine organisms, so these islands are only found in warm tropical waters. Eventually, the Pacific Plate carries the volcanic atoll into waters too cold for these marine organisms to maintain a reef by growth. Volcanic islands located beyond the warm water temperature requirements of reef building organisms become seamounts as they subside and are eroded away at the surface. An island that is located where the ocean water temperatures are just sufficiently warm for upward reef growth to keep pace with the rate of subsidence is said to be at the Darwin Point. Islands in more northerly latitudes evolve towards seamounts or guyots; islands closer to the equator evolve towards atolls (see Kure Atoll).\n\nAfter the reef dies, the volcano subsides or erodes below sea level and becomes a coral-capped seamount. These flat-topped seamounts are called guyots. Most, if not all, the volcanoes west of Kure Atoll as well as most, if not all, the volcanoes in the Emperor Seamount chain are guyots or seamounts.\n\nNot all Hawaiian volcanoes go through all of these stages of activity. An example is Koolau Range on Oahu, which was prehistorically devastated by a cataclysmic landslide, never underwent the postshield stage and went dormant for hundreds of thousands of years after the shield stage before coming back to life. Some volcanoes never made it above sea level; there is no evidence to suggest that West Molokai ever went through the rejuvenated stage, while its younger neighbors, East Molokai and West Maui, have evidently done so. It is currently unknown what stage of development the submerged volcano of Penguin Bank is in.\n\nIn recent years research at other seamounts, for instance Jasper Seamount, has confirmed that the Hawaiian model applies to other seamounts as well.\n\n\n"}
{"id": "1340668", "url": "https://en.wikipedia.org/wiki?curid=1340668", "title": "Gosaikunda", "text": "Gosaikunda\n\nGosaikunda (गोसाइँकुण्ड), also spelled Gosainkunda and Gosain Kunda is an alpine freshwater oligotrophic lake in Nepal's Langtang National Park, located at an altitude of in the Rasuwa District with a surface of . Together with associated lakes, the Gosaikunda Lake complex is in size and has been designated a Ramsar site on 29 September 2007.\n\nThe lake melts and sips down to form the Trishuli River and remains frozen for six months in winter October to June. There are 108 lakes in this area, small to medium in size. The challenging Lauribina La at an altitude of is on its outskirts.\n\nThe Gosaikunda area has been delineated as a religious site. Hindu mythology attributes Gosaikunda as the abode of the Hindu deities Shiva and Gauri. The Hindu scriptures Bhagavata Purana, Vishnu Purana and the epics Ramayana and Mahabharata refer to Samudra manthan, which is directly related to the origin of Gosaikunda. Its waters are considered holy and of particular significance during the Gangadashahara and the Janai Purnima festivals when thousands of pilgrims from Nepal and India visit the area.\nGosaikunda is believed to have been created by Lord Shiva when he thrust his Trishul (holy Trident) into a mountain to extract water so that he could cool his stinging throat after he had swallowed poison.\n\nGosaikunda is a significant place of interest on the Dhunche-Helambu trekking route. This trek adjoins the famous Langtang Valley trek in the same district. Both treks can be combined. Basic accommodation is quite easily available. Tea houses offer a variety of food and snacks.\n\nThe trek to Gosaikunda starts in Dhunche Village or Syabru Besi in the Langtang Himal, or in Sundarijal in the Kathmandu Valley.\nWhen starting from Dhunche, the first day involves a long steady climb to reach Chandan Bari at an altitude of about . Laurebinayak at about can be reached on the second day. At this point, some trekkers choose to climb ahead to Gosaikunda, though altitude sickness is a concern as the ascent is rather steep. Many trekkers choose to stay at Laurebinayak, which also provides sunset and sunrise views of the Langtang and Ganesh Himal. The descent from Gosaikund to Sundarijal (सुन्दरीजल) takes about four days and involves a short climb to Laurebina La at , a rapid descent to Phedi and onwards to Ghopte. Depending on pace, there are options to stay at Thadepati, Mangengoth, Kutumsang and at many villages farther downhill. The trails are well marked, except between Ghopte and Thadepati.\n\n"}
{"id": "3483450", "url": "https://en.wikipedia.org/wiki?curid=3483450", "title": "Joe Adamov", "text": "Joe Adamov\n\nJoe Adamov, Russian: Иосиф Адамов (Yosif Adamov), (7 January 1920 - 18 December 2005) was a journalist and presenter on Radio Moscow and its successor, the Voice of Russia, for over sixty years. Of Armenian descent, he was born in Batumi, Georgia . As a child, he lived with his family in England and attended British schools. Later, in the 1930s, he attended a special school for American expats in Moscow where he acquired familiarity with Americans. He was a graduate of Moscow State Pedagogical University. \n\nAn expert English-speaker who spoke with a neutral American accent, Adamov joined Radio Moscow as an announcer at the foreign language service of Radio Moscow in 1942. He lived in Moscow during most of his career.\nAmong English-speaking listeners, he is best known as the presenter of the programme \"Moscow Mailbag\" on Radio Moscow's North American shortwave broadcasts. The program answered questions from listeners on all aspects of Soviet life and the USSR's policies. At its height, \"Moscow Mailbag\" listeners sent Adamov thousands of letters yearly. In his capacity as a Radio Moscow journalist and presenter of \"Moscow Mailbag\", Adamov conducted interviews with many western politicians and journalists, including Dwight D. Eisenhower, Eleanor Roosevelt, Walter Cronkite and Larry King.\n\nDuring the Cold War, Adamov's broadcasts, like all other Radio Moscow output, were carefully studied by Western governments, a fact of which the Soviet authorities were well aware. Joe Adamov presented \"Moscow Mailbag\" from 1957 until shortly before his death, when ill-health forced him to relinquish the role permanently. The program remains on the air with different presenters.\n\nJoe Adamov was also the official Soviet translator at the trial of Gary Powers, shot down in a U-2 spy plane in 1960.\n\nThe signature poem of Moscow Mailbag was:\n\n<poem>\nYou can't do better\nthan send us that letter\nand in it tell Joe\nwhat you think of his show\n</poem>\n\nAdamov's death was reported on Voice of Russia's English-language news bulletins on December 21, 2005, although neither these bulletins nor the obituary on their website gave an exact date of death.\n\n"}
{"id": "19445498", "url": "https://en.wikipedia.org/wiki?curid=19445498", "title": "Leonid Teliga", "text": "Leonid Teliga\n\nLeonid Teliga (28 May 1917 – 21 May 1970) was a Polish sailor, writer, journalist, translator and the first Pole to single-handedly circumnavigate the globe on his yawl \"Opty\".\n\nAlthough he was born in Russia, his parents decided to settle back in Poland after it regained independence. He was raised in Grodzisk Mazowiecki. After failing to get into medical studies, he decided to attend military academy. In 1937, he finished a yachting course in Jastarnia.\n\nDuring the September Campaign Teliga fought in 44th Infantry Regiment, and was wounded at Tomaszów Mazowiecki. In 1940, he arrived in Azov, where he took a skipper course and became a fisherman. Eventually, he took part in the evacuation of harbours on Crimea. In 1942, he joined the newly-formed Anders Army, established in the Soviet Union and made up mainly of Polish POWs, with which he got through to Great Britain. After taking a navigation course in Canada, he fought as a gunner in the No. 300 Squadron, a unit of the Polish Air Forces in Great Britain.\n\nTeliga returned to Poland in 1947. He used every occasion to return to the sea, sometimes as a skipper or sailing instructor, later as a journalist. In the 50s and 60s, he published several short stories collections and novels based on his various voyages. In 1957, Teliga went to North Korea to participate in the works of UN Armistice Commission.\n\nThe yacht \"Opty\" was designed by engineer Leon Tumiłowicz, based on his earlier construction, the \"Tuńczyk\" class, but modified so that it would better fit the task of long, solitary cruise. \"Tuńczyk\"<nowiki>'</nowiki>s predecessor, the \"Konik Morski\" type, was actually the first Polish seagoing construction, designed back in 1936. The construction of \"Opty\" began in January 1966, and finished in October. Although he received some support from Polish Yachting Association and other sources, Teliga funded the construction mostly on his own. Even though masts and booms were wooden, the yacht was farily well equipped, as it had a pneumatic raft, a plastic boat for easier communication with the coast, and a wide set of sails for every kind of wind.\n\nAfter having \"Opty\" transported to Casablanca, Teliga began his journey on 25 January 1967, heading west. During the course of the cruise, he visited Canary Islands, Lesser Antilles, Panama Canal, Galápagos Islands, Marquesas Islands, Tahiti, Bora Bora, Fiji and Dakar. In Panama Canal, he experienced an unexpected delay as it took eleven days to persuade the Canal's administration to let him pass. Most likely that was the reason for him to skip Australia and do the final section of the circumnavigation without any landings, as after failing to get Australian visa he expected similar obstructions there.\n\nDuring his journey, Teliga became fairly well known, and acquired honorary membership of several yacht clubs. He experienced a hospitable welcome in nearly every port, and, incidentally, met compatriots in most of them.\n\nBy cruising non-stop from Fiji to Dakar for 165 days, he beat the world record previously held by Bernard Gilboy, who sailed single-handedly for 163 days in an attempt to cross the Pacific Ocean. However, when Teliga landed in Dakar on 9 January 1969, Robin Knox-Johnston has already been at sea for 210 days.\n\nOn 5 April 1969, he crossed his course from 1967, finishing the circumnavigation. It took him \"2 years, 13 days, 21 hours and 15 minutes\"\n\nDue to rapidly developing cancer, Teliga was forced to stop in Casablanca. He was transported back to Poland by plane. Despite having an operation, he died in May 1970.\n\n\n"}
{"id": "51229452", "url": "https://en.wikipedia.org/wiki?curid=51229452", "title": "Liberation Route Europe", "text": "Liberation Route Europe\n\nLiberation Route Europe is an international remembrance trail that connects the main regions along the advance of the Western Allied Forces during the liberation of Europe and final stage of the Second World War. The route started in 2008 as a Dutch regional initiative in the Arnhem-Nijmegen area and then developed into a transnational route that was officially inaugurated in Arromanches on June 6, 2014 during the Normandy D-day commemorations. The route goes from Southern England through France, Belgium, Luxembourg, the Netherlands to Berlin, Germany and then extends to the Czech Republic and Poland. The southern route starts in Italy. As a form of remembrance tourism LRE aims to unfold these Allied offensives of 1944 and 1945 in one narrative combining the different perspectives and points of view. By combining locations with personal stories of people who fought and suffered, it gives visitors the opportunity to follow the Allied march and visit significant sites from war cemeteries to museums and monuments but also events and commemorations.\n\nAfter the fall of France in 1940 and their own defeat on the continent, the British troops regrouped and took the air to defend their island, known as the Battle of Britain. The route starts in London and here goes through Kent (Operation Fortitude South, Operation Pluto and Operation Dynamo) and Hampshire (where Operation Overlord was prepared).\n\nImportant remembrance sites\n\nDuring Operation Overlord, the largest amphibious assault in history, Allied forces landed on five beaches along a 50-mile stretch of Normandy coast on D-Day. This operation marked the start of the liberation of Western Europe. The route here comprises the 5 landing beaches: Sword Beach (from Ouistreham to Saint-Aubin-sur-Mer), Gold Beach (between Port-en-Bessin and La Rivière), Utah Beach (Sainte-Marie-du-Mont), Omaha Beach (from Sainte-Honorine-des-Pertes to Vierville-sur-Mer), Juno Beach (from Courseulles to Saint-Aubin-sur-Mer) along with Pointe du Hoc (Criqueville-en-Bessin), Ranville (British Airlandings) or Longues-sur-Mer (German Battery).\n\nThe following Battle of Normandy resulted not only in the deaths of tens of thousands of soldiers, but also many French civilians. Almost all of the larger cities in the region were badly damaged.\nImportant remembrance sites\nAn uprising of the population against the Germans on August 19 forced the Allies to send troops to liberate Paris, while it was not a priority. The 2nd French Armoured Division entered Paris on the evening of August 24. The capitulation was signed on the Île de la Cité, and the German troops surrendered at the Montparnasse train station. Two days later a triumphal parade led by General Charles de Gaulle was held on the Champs-Elysées.\n\nImportant remembrance sites\n\nThe Battle of the Bulge was the last major German offensive campaign launched through the densely forested Ardennes region. It eventually was gradually pushed back by the Allied until the Germans retreated behind the Siegfried Line.\n\nImportant remembrance sites\nBrussels was liberated on September 3d by the Guards Armoured Division of General Allan Adair including the Belgian 'Piron brigade'.\n\nImportant remembrance sites\n\nLuxembourg was the theatre of the Battle of the Bulge offensive and had to wait until 12 February 1945 before being completely liberated.\n\nImportant remembrance sites\n\nThe Dutch section of Liberation Route Europe is concentrated in the provinces of Gelderland, North-Brabant, Overijssel, Zeeland and Limburg. In these provinces a large network of 176 'audiospots' has been developed to combine historical sites and personal stories.\nThe main historical places on the route here are Arnhem (Battle of Arnhem) Nijmegen (Operation Market Garden), Groesbeek (Operation Veritable), Wageningen (German capitulation), Oosterbeek (Operation Market Garden), Otterlo (Liberation of the East) and Lent (Men’s Island).\n\nImportant remembrance sites\n\nImportant remembrance sites\n\nImportant remembrance sites\n\nImportant remembrance sites\n\nImportant remembrance sites\n\nDuring the autumn and winter of 1944-45, the longest battle of the Second World War on German soil took place in the Hürtgen Forest. With this battle, which ended in an Allied victory, the war returned to Germany and open the road to Berlin.\n\nSeven 'audiospots' have been installed in the region.\n\nImportant remembrance sites\n\nBerlin is the endpoint of the route. The Battle of Berlin was one of the last battles of the Second World War in Europe. Many soldiers died in widespread house-to-house fighting where Soviet soldiers faced desperate German resistance. On May 2, 1945, the Berlin garrison surrendered to the Soviet army. The unconditional surrender of Germany was signed on the 8th.\nImportant remembrance sites\n\nOn the 1st of September 1939 the battleship Schleswig-Holstein opened fire on the Westerplatte in Gdańsk. This is regarded as the first shots of the Second World War. After the war Gdansk would become an important symbol of Polish resistance.\n\nImportant remembrance sites\n\nPilsen is the capital city of Czech Republic‘s western region of Bohemia. In May 1945, the US Third Army led by General Patton entered Pilsen to liberate the Czech people from six years of occupation from Nazi Germany. Still today, the locals remember these events and they remain immensely grateful to the US Army.\n\nImportant remembrance sites\n\nThe Liberation Route Europe in Italy will connect important remembrance sites connected to the landing in Sicily, the Gustav Line defense, the Battle of Montecassino, the landing in Anzio, the Gothic Line defense.\n\nThe Liberation Route Europe is developed and managed by the Liberation Route Europe Foundation. Its purpose is also to bring together all of the institutions related to World War II remembrance (museums, universities, regional and national governments, tourism authorities, veterans associations, war graves commissions etc.) and to coordinate their efforts at an international level.\n\nMartin Schulz, former President of the European Parliament, serves as the patron of the Liberation Route Europe Foundation.\n\n\n"}
{"id": "3441059", "url": "https://en.wikipedia.org/wiki?curid=3441059", "title": "List of Lepidoptera that feed on Helianthus", "text": "List of Lepidoptera that feed on Helianthus\n\n\"Helianthus\" species are used as food plants by the larvae of a number of Lepidoptera species including:\n\nSpecies which feed exclusively on \"Helianthus\"\n\n\nSpecies which feed on \"Helianthus\" and other plants\n\n\n"}
{"id": "9656817", "url": "https://en.wikipedia.org/wiki?curid=9656817", "title": "List of Panaeolus species", "text": "List of Panaeolus species\n\nThe following is a list of the about 98 \"Panaeolus\" mushroom species:\n"}
{"id": "36815032", "url": "https://en.wikipedia.org/wiki?curid=36815032", "title": "List of Sites of Special Scientific Interest in Gwynedd", "text": "List of Sites of Special Scientific Interest in Gwynedd\n\nThis is a list of the Sites of Special Scientific Interest (SSSIs) in the Gwynedd Area of Search (AoS).\n"}
{"id": "229623", "url": "https://en.wikipedia.org/wiki?curid=229623", "title": "List of animal names", "text": "List of animal names\n\nMany animals, particularly domesticated, have specific names for males, females, young, and groups.\n\nThe best-known source of many English words used for collective groupings of animals is \"The Book of Saint Albans\", an essay on hunting published in 1486 and attributed to Juliana Berners. Most terms used here may be found in common dictionaries and general information web sites.\n\nThe terms in this table apply to many or all taxa in a particular biological family, class, or clade.\n\n\n"}
{"id": "30860126", "url": "https://en.wikipedia.org/wiki?curid=30860126", "title": "List of brackish aquarium plant species", "text": "List of brackish aquarium plant species\n\nAquatic plants are used to give the aquarium a natural appearance, oxygenate the water, and provide habitat for fish, especially fry (babies) and for invertebrates. Some aquarium fish and invertebrates also eat live plants. Hobby aquarists use aquatic plants for aquascaping.\n\nBrackish plants are known to occur in brackish water.\n\nThe taxonomy of most plant genera is not final.\n\n"}
{"id": "17252746", "url": "https://en.wikipedia.org/wiki?curid=17252746", "title": "List of countries by energy consumption per capita", "text": "List of countries by energy consumption per capita\n\nThis is a list of countries by total energy consumption per capita. This is not the consumption of end-users but all energy needed as input to produce fuel and electricity for end-users. It is known as total primary energy supply (TPES), a term used to indicate the sum of production and imports subtracting exports and storage changes (see also Worldwide energy supply). \nNumbers are from The World Bank - World Development Indicators.\n\nThe data is given in kilogrammes of oil equivalent per year, and gigajoules per year, and in watts, as average equivalent power.\n\n"}
{"id": "15910445", "url": "https://en.wikipedia.org/wiki?curid=15910445", "title": "List of natural gas pipelines", "text": "List of natural gas pipelines\n\nThis is a list of pipelines used to transport natural gas.\n\n\n\nSource\n\n\nPlanned pipelines for intereuropean gas transport:\n\nAll African routes run from Hassi R'Mel the Algerian hub for natural gas, it supplies gas form the Hassi R'Mel gas field. (The planned Trans-Saharan gas pipeline will connect Nigeria to Hassi_R’Mel).\nPipelines from Hassi_R’Mel:\n\nPlanned pipeline for gas transport from Africa:\n\nPipelines connect mainly from the Shah Deniz gas field in Azerbaijan. Connections to the Middle East are possible:\n\nPlanned pipelines for transport from Azerbaijan and Middle East:\n\nA lecture was held at The Institute of World Politics to consider the economic and geopolitics of the rival pipelines and what's at stake for the concerned parties, namely TANAP and South Stream's replacement Turkish Stream.\n\n\n\nPlanned pipelines for gas transport form Russia:\n\nInterstate pipelines are regulated by the National Energy Board in Canada and the Federal Energy Regulatory Commission (FERC) in the US. Intrastate pipelines are regulated by state, provincial or local jurisdictions.\n\n\n\n\nFERC requires most interstate pipelines to maintain an interactive web site with standardized information regarding their operations under a heading of \"Informational Postings.\" The exact legal name of each company appears below. Many of these companies are wholly owned subsidiaries of larger publicly traded companies.\n\n\n\n\n\nAlthough these pipelines flow gas in interstate commerce, they are subject to state regulation.\n\n\n\n\n"}
{"id": "9018357", "url": "https://en.wikipedia.org/wiki?curid=9018357", "title": "List of pecan diseases", "text": "List of pecan diseases\n\nThis article is a list of diseases of pecans (\"Carya illinoinensis\").\n\nPecan mosaic-associated virus, potyvirus \n\n"}
{"id": "18025961", "url": "https://en.wikipedia.org/wiki?curid=18025961", "title": "List of rivers of El Salvador", "text": "List of rivers of El Salvador\n\nThis is a list of rivers in El Salvador.\n\nEl Salvador has 58 river basins which all drain to the Pacific Ocean. The total amount of water discharged into the Pacific is 19 million m³ in a normal year. \nAdditionally, there are several endorheic basins linked to a lake. Coatepeque Lake is the largest endorheic basin in the country.\n\nThis list is arranged by drainage basin, with respective tributaries indented under each larger stream's name.\n\n\n"}
{"id": "8591748", "url": "https://en.wikipedia.org/wiki?curid=8591748", "title": "List of stars in Lyra", "text": "List of stars in Lyra\n\nThis is the list of notable stars in the constellation Lyra, sorted by decreasing brightness.\n\n\n"}
{"id": "8591843", "url": "https://en.wikipedia.org/wiki?curid=8591843", "title": "List of stars in Sculptor", "text": "List of stars in Sculptor\n\nThis is the list of notable stars in the constellation Sculptor, sorted by decreasing brightness.\n\n\n"}
{"id": "2578114", "url": "https://en.wikipedia.org/wiki?curid=2578114", "title": "List of the vascular plants of Britain and Ireland 6", "text": "List of the vascular plants of Britain and Ireland 6\n\nList of the vascular plants of Britain and Ireland #6 — this page's list covers a group of dicotyledon families (Mimosaceae to Dipsacaceae). \n\nStatus key: \"*\" indicates an introduced species and \"e\" indicates an extinct species.\n\nGo to:\n"}
{"id": "52782026", "url": "https://en.wikipedia.org/wiki?curid=52782026", "title": "List of trees of Cambodia", "text": "List of trees of Cambodia\n\nThe following 'List of Tree Species' in Cambodia has been based on one prepared in 2004 with the support of DANIDA project; the original document gives Khmer names for species shown here (with corrections and links).\n"}
{"id": "319751", "url": "https://en.wikipedia.org/wiki?curid=319751", "title": "Maquis shrubland", "text": "Maquis shrubland\n\nMaquis (French) or macchia (Italian: \"macchia mediterranea\") is a shrubland biome in the Mediterranean region, typically consisting of densely growing evergreen shrubs. \n\n\n"}
{"id": "52948677", "url": "https://en.wikipedia.org/wiki?curid=52948677", "title": "Ministry of Energy &amp; Minerals (Somaliland)", "text": "Ministry of Energy &amp; Minerals (Somaliland)\n\nThe Ministry of Energy & Minerals, also Ministry of Energy, Oil and Mineral is one of the governmental bodies of Somaliland. The ministry has the function of developing and implementing policies related to electricity, minerals, petroleum and petroleum products.\n\n\nIn August 2012, Genel was awarded an exploration licence for onshore blocks SL-10-B and SL-13 in Somaliland, with a 75% working interest in both. Genel extended its presence in November 2012 with the acquisition of 50% participating interest in the Odewayne Production Sharing Agreement which covers blocks SL-6, SL-7, SL-10A.\n\nOnshore Somaliland is a relatively unexplored region, with few exploration wells drilled. The total size of the blocks is approximately equivalent to the entire Kurdistan Region of Iraq.\n\nGenel took the opportunity because of encouraging indications including onshore oil seeps and existing geological data showing favourable conditions for hydrocarbons to have accumulated in numerous large tilted fault blocks and sub-basins. In addition, the basins of Somaliland were contiguous to Yemen prior to the opening of the Gulf of Aden in the Oligocene-Miocene - similar sedimentary sequences and structural styles are expected in Somaliland.\n\nWe are targeting resources of over 1,000 mmbbls in blocks SL-10B and SL-13. The Odewayne block has a similar resource potential to this, targeting in order of 1,000 mmbbls.\n\nGravity and aeromag has been acquired and interpreted over the entire 40,000 square km acreage.\n\nOslo, 8 September 2014 - DNO ASA, the Norwegian oil and gas operator, today announced that it has been granted a two-year extension of the term of its production sharing agreement for Block SL18 in Somaliland. The first exploration period will now end on 8 November 2017.\n\nBlock SL18, in which DNO has a 50 percent stake as operator, is a frontier exploration block. The partners have completed field survey and environmental assessment studies over the block and will initiate a planned seismic acquisition program once the Government of Somaliland has put in place a planned Oil Protection Unit (OPU) to support the international oil companies operating in Somaliland. The OPU is expected to be operational in 2015.\n\nIn the interim, security conditions permitting, DNO will resume a development program focused on drilling water wells to provide local communities in the areas covered by Block SL18 with potable water.\n\n\n"}
{"id": "2614281", "url": "https://en.wikipedia.org/wiki?curid=2614281", "title": "Mount Ebal", "text": "Mount Ebal\n\nMount Ebal ( \"Jabal ‘Aybāl\"; \"Har ‘Eival\") is one of the two mountains in the immediate vicinity of the city of Nablus in the West Bank (biblical \"Shechem\"), and forms the northern side of the valley in which Nablus is situated, the southern side being formed by Mount Gerizim. The mountain is one of the highest peaks in the West Bank and rises to above sea level, some higher than Mount Gerizim. Mount Ebal is approximately in area, and is composed primarily of limestone. The slopes of the mountain contain several large caverns which were probably originally quarries, and at the base towards the north are several tombs.\n\nIn advance of the Israelites' entry to the Promised Land, records Moses' direction that \"when the Lord your God has brought you into the land which you go to possess, that you shall put the blessing on Mount Gerizim and the curse on Mount Ebal\".\n\nIn the masoretic text and the Septuagint version of Deuteronomy 27, an instruction is given to build an altar on Mount Ebal, constructed from natural (rather than cut) stones, to place stones there and whiten them with lime, to make peace offerings on the altar, eat there, and write the words of \"this law\" on the stone. According to the Samaritan Pentateuch version, this instruction actually concerns Mount Gerizim, which the Samaritans view as a holy site; some scholars believe that the Samaritan version is probably more accurate in this respect, the compilers of the masoretic text and authors of the Septuagint being likely to be biased against the Samaritans. Recent Dead Sea Scrolls work supports the accuracy of the Samaritan Pentateuch's designation of Mount Gerizim rather than Mount Ebal as the sacred site.\n\nAn instruction immediately subsequent to this orders that, once this is done, the Israelites should split into two groups, one to stay on Mount Ebal and pronounce curses, while the other goes to Mount Gerizim and pronounces blessings. The tribes of Simeon, Levi, Judah, Issachar, Joseph and Benjamin were to be sent to Gerizim, while those of Reuben, Gad, Asher, Zebulun, Dan and Naphtali, were to remain on Ebal. No attempts to explain this division of tribes either by their Biblical ethnology or by their geographical distribution have been generally accepted in academic circles. More recently, however, some argue that the tribes were divided as equally as possible given the population's census data given in the Book of Numbers. Specifically, the division found in the book of Deuteronomy is the most equal out of 462 possible divisions.\n\nThe text goes on to list twelve curses, which were to be pronounced by the Levite priesthood and answered by the people with \"Amen\". These \"curses\" heavily resemble laws (e.g. \"cursed be he who removes his neighbour's landmark\"), and they are not followed by a list of blessings described in a similarly liturgical framework; scholars believe that these more likely represent what was written on the stones, and that the later list of six explicit blessings, six near-corresponding explicit curses, were originally in this position in the text. The present position of these explicit blessings and curses, within a larger narrative of promise, and a far larger narrative of threat (respectively), is considered to have been an editorial decision for the post-exilic second version of Deuteronomy (\"Dtr2\"), to reflect the deuteronomist's worldview after the Babylonian exile had occurred.\n\nIn the Book of Joshua, after the Battle of Ai, Joshua built an altar of unhewn stones there, the Israelites then made peace offerings on it, the \"Law of Moses\" was written onto the stones, and the Israelites split into the two groups specified in Deuteronomy and pronounced blessings and cursings as instructed there. There is some debate between textual scholars as to whether this incident in Joshua is one account or spliced together two different accounts, where one account refers to Joshua building an altar, and making sacrifices on it, while the other account refers to Joshua placing large stone slabs there that had been whitened with lime and then had the \"Torah\" inscribed on them. Either way there is general agreement that the sources of Joshua predate Deuteronomy, and hence that the order to build the altar and make the inscription is likely based on these actions in the sources of Joshua, rather than the other way round, possibly to provide an aetiology for the site acceptable to the deuteronomist's theology.\n\nMuch later in the Book, when Joshua was old and dying, he gathered the people together at Shechem, and gave a farewell speech, and then wrote \"these words in the book of the Torah of God, and took a great stone, and set it under the doorpost which is in the sanctuary of the Lord\". Depending on the way in which the sources of Joshua were spliced together, this may just be another version of the earlier narrative Joshua placing the whitened stones slabs with the \"Torah\" inscribed on them, and some scholars believe that this narrative may have originally been in an earlier location within the Book of Joshua.\n\nIn the Biblical narrative, the \"terebinth\", seemingly next to the sanctuary, was evidently in existence as early as the time of the Patriarchs, as Jacob is described in the Book of Genesis as having buried the idols of \"strange gods\" (belonging to his uncle Laban) beneath it. According to a midrash, one of these idols, in the shape of a dove, was later recovered by the Samaritans, and used in their worship on Mount Gerizim.\n\nThe higher part of the mountain, on the west, contains the ruins of some massive walls called \"Al-Kal'ah\", and east of this are other ruins now called \"Kunaisah\". However, potentially much more significant remains have been found on the northern side.\n\nLike many other sites in the region, by the 20th century there was a large stone heap found on Mount Ebal; this one was known to locals as \"el Burnat\" (Arabic for \"the Hat\"), and was found by Adam Zertal, within a naturally shaped \"amphitheatre\". Upon archaeological investigation, several potsherds were found among this heap, and were dated to 1220-1000BC, a date for which no other remains are found nearby, and so a more substantial archaeological excavation was launched at the site in 1987. The excavation found a large walled structure, seemingly built direct into the bedrock without a doorway or floor, and had been infilled by layers of stone, ash, and earth; on the southwest were found two paved areas split apart by a further wide wall higher at one end than the other and with a surrounding oval wall. Slowly burnt bones were found at the site, and after analysis were discovered to originate from bullocks, goats, and fallow deer.\n\nAlthough the excavating archaeologist believed that the site was the compound containing Joshua's altar, the filled walled structure being the altar itself - the filling being a part of the altar rather than debris (and indicative of an Assyrian style altar, like that specified in the Book of Exodus as being \"hollow with boards\") - and the \"wall\" between the two courtyards being a ramp (in accordance with the \"no steps\" instruction in Exodus), most other archaeologists believe it to be something else. The site has a significant issue in regard to the Biblical account of Joshua's altar, as it is located on the north side of the mountain, and not the south side facing Mount Gerizim, making a curse & blessing ritual held there and on Gerizim somewhat difficult to hold antiphonally; the excavating archaeologist proposed that this could be resolved by identifying a mountain to the north as Gerizim rather than the usual location, though the suggestion was ridiculed by both the Samaritans, who found it offensive to move the centre of their religion, and by other scholars and archaeologists.\n\nThough some archaeologists agree with the consideration that the site was an altar compound (though not constructed by \"Joshua\"), and some (including Israel Finkelstein) at least agree that it was a cultic location (though not necessarily involving an altar), others believe that it was simply a farmhouse, a guard tower or the Biblical \"tower of Shechem\". Those considering it to be a farmhouse and/or the \"tower of Shechem\", argue that the paved areas are simply rooms, the \"sloping\" wall simply an eroded partition wall, and the infilled enclosure a room that was later changed into a tower - the foundation of the tower being the infill, and the rest of the tower now destroyed.\n\nIsraelis wishing to visit the site today must coordinate their activity with COGAT, the Israeli defence ministry unit which manages civilian affairs for Palestinians in the West Bank and liaises with Gaza, since Mount Ebal is located in what is now designated as Area B. In addition, Israeli citizens visiting the area are required to be escorted by IDF soldiers, to ensure their personal safety. The Shomron Regional Council, as of July 2016, was trying to promote the area as a tourist destination.\n\n"}
{"id": "5805507", "url": "https://en.wikipedia.org/wiki?curid=5805507", "title": "Nirmal Shah", "text": "Nirmal Shah\n\nNirmal Shah is an environmentalist, writer, fund raiser and civic leader from the Seychelles, and director of Nature Seychelles.\n\nShah, who was educated in India and the United States, advocates for conservation of biodiversity, sustainable development and civil society participation through local and regional NGOs, international forums and the media. He is the founder of the Wildlife Clubs of Seychelles, the major youth NGO based in schools and institutions of higher learning.\n\n"}
{"id": "17113327", "url": "https://en.wikipedia.org/wiki?curid=17113327", "title": "Orpheus and Eurydice", "text": "Orpheus and Eurydice\n\nThe ancient legend of Orpheus and Eurydice concerns the fateful love of Orpheus of Thrace, son of Apollo and the muse Calliope, for the beautiful Eurydice (from \"Eurudike\", \"she whose justice extends widely\"). It may be a late addition to the Orpheus myths, as the latter cult-title suggests those attached to Persephone. It may have been derived from a legend in which Orpheus travels to Tartarus and charms the goddess Hecate.\n\nIn Virgil's classic version of the legend, it completes his \"Georgics\", a poem on the subject of agriculture. Here the name of Aristaeus, or Aristaios, the keeper of bees, and the tragic conclusion, was first introduced.\n\nOvid's version of the myth, in his \"Metamorphoses\", was published a few decades later and employs a different poetic emphasis and purpose. It relates that Eurydice's death was not caused by fleeing from Aristaeus, but by dancing with naiads on her wedding day.\n\nOther ancient writers treated Orpheus' visit to the underworld more negatively. According to Phaedrus in Plato's \"Symposium\", the infernal deities only \"presented an apparition\" of Eurydice to him. Plato's representation of Orpheus is in fact that of a coward; instead of choosing to die in order to be with his love, he mocked the deities in an attempt to visit Hades, to get her back alive. As his love was not \"true\"—meaning that he was not willing to die for it—he was punished by the deities, first by giving him only the apparition of his former wife in the underworld and then by having him killed by en.\n\nApollo gives his son Orpheus a lyre and teaches him how to play. Orpheus played with such perfection that even Apollo was surprised. It is said that nothing could resist his beautiful melodies, neither enemies nor beasts. Even trees and rocks were entranced with his music.\n\nOrpheus fell in love with Eurydice, a woman of unique beauty and grace, whom he married and lived happily with for a short time. However, when Hymen was called to bless the marriage, he predicted that their perfection was not meant to last.\n\nA short time after this ominous prophecy, Eurydice was wandering in the forest with the Nymphs. In some versions of the story, Aristaeus, a shepherd, then saw her, was beguiled by her beauty, made advances towards her, and began to chase her. Other versions of the story relate that Eurydice was merely dancing with the Nymphs. In any case, while fleeing or dancing, she was bitten by a snake and died instantly. \n\nOrpheus sang his grief with his lyre and managed to move everything living or not in the world; both humans and gods were deeply touched by his sorrow and grief.\n\nAt some point, Orpheus decided to descend to Hades to see his wife. Ovid's version of the myth does not explain this decision, while other versions relate that the gods and nymphs or Apollo himself, Orpheus' father, suggest that he make this journey. Any other mortal would have died, but Orpheus, protected by the gods, went to Hades and arrived at the infamous Stygian realm, passing by ghosts and souls of people unknown. He also managed to charm Cerberus, the monster known to have three heads. Orpheus presented himself in front of the god of the Underworld Hades (Pluto) and his wife Persephone.\n\nOrpheus played his lyre, melting even Hades' heart. Hades told Orpheus that he could take Eurydice with him but under one condition; Eurydice would follow him while walking out to the light from the caves of the Underworld, but he should not look at her before coming out to the light or else he would lose her forever. If Orpheus was patient enough he would have Eurydice as a normal woman again by his side.\n\nThinking it a simple task for a patient man like himself, Orpheus was delighted; he thanked the gods and left to ascend back into the world. Unable to hear Eurydice’s footsteps, however, he began fearing the gods had fooled him. Eurydice was in fact behind him, but as a shade, having to come back into the light to become a full woman again. Only a few feet away from the exit, Orpheus lost his faith and turned to see Eurydice behind him, but her shadow was whisked back among the dead, now trapped in Hades forever.\n\nOrpheus tried to return to the Underworld, but a man cannot enter the realm of Hades twice while alive. According to various versions of the myth, Orpheus started playing a mourning song with his lyre, calling for death so that he could be united with Eurydice forever. Orpheus is ultimately killed either by beasts tearing him apart, or by the Maenads, in a frenzied mood. According to another version, Zeus decided to strike him with lightning knowing Orpheus would reveal the secrets of the Underworld to humans.\n\nIn any case, Orpheus died, but the Muses decided to save his head and keep it among the living people to sing forever, enchanting everyone with his lovely melodies and tones.\n\n\n\n\n\n\n\n"}
{"id": "19346963", "url": "https://en.wikipedia.org/wiki?curid=19346963", "title": "Osialfecanakmg", "text": "Osialfecanakmg\n\nOsialfecanakmg is an acronym made up of the chemical symbols of the 8 most abundant elements in the Earth's crust. Because they are so important, geologists sometimes refer to the elements named below as the BIG 8.\n\nElement Volume proportions are:\n\n"}
{"id": "28870586", "url": "https://en.wikipedia.org/wiki?curid=28870586", "title": "Pigeon River (Michigan)", "text": "Pigeon River (Michigan)\n\nPigeon River may refer to the following streams in the U.S. state of Michigan:\n\n\n"}
{"id": "85585", "url": "https://en.wikipedia.org/wiki?curid=85585", "title": "Pilumnus", "text": "Pilumnus\n\nIn Roman mythology, Pilumnus (\"staker\") was a nature deity, brother of Picumnus. He ensured children grew properly and stayed healthy. Ancient Romans made an extra bed after the birth of a child in order to ensure the help of Pilumnus. He also taught humanity how to grind grain. He was also sometimes identified as the husband of Danaë, and therefore the father of Danaus and the ancestor of Turnus.\n\nA ceremony to honour the deity involved driving a stake into the ground.\n\n"}
{"id": "623870", "url": "https://en.wikipedia.org/wiki?curid=623870", "title": "Raman scattering", "text": "Raman scattering\n\nRaman scattering or the Raman effect is the inelastic scattering of a photon by molecules which are excited to higher vibrational or rotational energy levels.\nIt was discovered in 1928 by C. V. Raman and his student K. S. Krishnan in liquids, \nand independently by Grigory Landsberg and Leonid Mandelstam in crystals.\nThe effect had been predicted theoretically by Adolf Smekal in 1923.\n\nWhen photons are scattered from an atom or molecule, most of them are elastically scattered (Rayleigh scattering), such that the scattered photons have the same energy (frequency and wavelength) as the incident photons. A small fraction of the scattered photons (approximately 1 in 10 million) are scattered \"inelastically\" by an excitation, with the scattered photons having a frequency and energy different from, and usually lower than, those of the incident photons. In a gas, Raman scattering can occur with a change in energy of a molecule due to a transition to another (usually higher) energy level. Chemists are primarily concerned with this \"transitional\" Raman effect.\n\nThe inelastic scattering of light was predicted by Adolf Smekal in 1923 (and in German-language literature it may be referred to as the Smekal-Raman effect). In 1922, Indian physicist C. V. Raman published his work on the \"Molecular Diffraction of Light,\" the first of a series of investigations with his collaborators that ultimately led to his discovery (on 28 February 1928) of the radiation effect that bears his name. The Raman effect was first reported by C. V. Raman and K. S. Krishnan, and independently by Grigory Landsberg and Leonid Mandelstam, on 21 February 1928 (that is why in the former Soviet Union the priority of Raman was always disputed; thus in Russian scientific literature this effect is usually referred to as \"combination scattering\" or \"combinatory scattering\"). Raman received the Nobel Prize in 1930 for his work on the scattering of light.\n\nIn 1998 the Raman effect was designated a National Historic Chemical Landmark by the American Chemical Society in recognition of its significance as a tool for analyzing the composition of liquids, gases, and solids.\n\nFor any given chemical compound, there are a total of 3N degrees of freedom, where N is the number of atoms in the compound. This number arises from the ability of each atom in a molecule to move in three different directions (x, y, and z). When dealing with molecules, it is more common to consider the movement of the molecule as a whole. Consequently, the 3N degrees of freedom are partitioned into molecular translational, rotational, and vibrational motion. Three of the degrees of freedom correspond to translational motion of the molecule as a whole (along each of the three spatial dimensions). Similarly, three degrees of freedom correspond to rotations of the molecule about the formula_1, formula_2, and formula_3-axes. Linear molecules only have two rotations because rotations along the bond axis do not change the positions of the atoms in the molecule. The remaining degrees of freedom correspond to molecular vibrational modes. These modes include stretching and bending motions of the chemical bonds of the molecule. For a linear molecule, the number of vibrational modes is:\n\nwhereas for a non-linear molecule the number of vibrational modes are\n\nThe frequencies of molecular vibrations range from less than 10 to approximately 10 Hz. These frequencies correspond to radiation in the infrared (IR) region of the electromagnetic spectrum. At any given instant, each molecule in a sample has a certain amount of vibrational energy. However, the amount of vibrational energy that a molecule has continually changes due to collisions and other interactions with other molecules in the sample.\n\nAt room temperature, most of the molecules will be in the lowest energy state, which is known as the ground state. A few molecules will be in higher energy states, which are known as excited states. The fraction of molecules occupying a given vibrational mode at a given temperature can be calculated using the Boltzmann distribution. Performing such a calculation shows that, for relatively low temperatures (such as those used for most routine spectroscopy), most of the molecules occupy the ground vibrational state. Such a molecule can be excited to a higher vibrational mode through the direct absorption of a photon of the appropriate energy. This is the mechanism by which IR spectroscopy operates: infrared radiation is passed through the sample, and the intensity of the transmitted light is compared with that of the incident light. A reduction in intensity at a given wavelength of light indicates the absorption of energy by a vibrational transition. The energy, formula_6, of a photon is\n\nwhere formula_8 is Planck's constant and formula_9 is the frequency of the radiation. Thus, the energy required for such a transition may be calculated if the frequency of the incident radiation is known.\n\nIt is also possible to observe molecular vibrations by an inelastic scattering process. In inelastic (Raman) scattering, an absorbed photon is re-emitted with lower energy; the difference in energy between the incident photons and scattered photons corresponds to the energy required to excite a molecule to a higher vibrational mode.\n\nTypically, in Raman spectroscopy high intensity laser radiation with wavelengths in either the visible or near-infrared regions of the spectrum is passed through a sample. Photons from the laser beam produce an oscillating polarization in the molecules, exciting them to a virtual energy state. The oscillating polarization of the molecule can couple with other possible polarizations of the molecule, including vibrational and electronic excitations. If the polarization in the molecule does not couple to these other possible polarizations, then it will not change the vibrational state that the molecule started in and the scattered photon will have the same energy as the original photon. This type of scattering is known as Rayleigh scattering.\n\nWhen the polarization in the molecules couples to a vibrational state that is higher in energy than the state they started in, then the original photon and the scattered photon differ in energy by the amount required to vibrationally excite the molecule. In perturbation theory, the Raman effect corresponds to the absorption and subsequent emission of a photon via an intermediate quantum state of a material. The intermediate state can be either a \"real\", i.e. stationary state, or a virtual state.\n\nThe Raman interaction leads to two possible outcomes: \nThe energy difference between the absorbed and emitted photon corresponds to the energy difference between two resonant states of the material and is independent of the absolute energy of the photon.\n\nThe spectrum of the scattered photons is termed the Raman spectrum. It shows the intensity of the scattered light as a function of its frequency difference \"Δν\" to the incident photons. The locations of corresponding Stokes and anti-Stokes peaks form a symmetric pattern around \"Δν=0\". The frequency shifts are symmetric because they correspond to the energy difference between the same upper and lower resonant states. The intensities of the pairs of features will typically differ, though. They depend on the populations of the initial states of the material, which in turn depend on the temperature. In thermodynamic equilibrium, the lower state will be more populated than the upper state. Therefore, the rate of transitions from the more populated lower state to the upper state (Stokes transitions) will be higher than in the opposite direction (anti-Stokes transitions). Correspondingly, Stokes scattering peaks are stronger than anti-Stokes scattering peaks. Their ratio depends on the temperature, and can therefore be exploited to measure it.\n\nThe Raman effect differs from the process of fluorescence in that it is a scattering process. For fluorescence, the incident light is completely absorbed, transferring the system to an excited state. After a certain resonance lifetime, the system de-excites to lower energy states via emission of photons. The result of both processes is in essence the same: A photon with a frequency different from that of the incident photon is produced and the molecule is brought to a higher or lower energy level. But the major difference is that the Raman effect can take place for any frequency of incident light. In contrast to the fluorescence effect, the Raman effect is therefore not a resonant effect. In practice, this means that a fluorescence peak is anchored at a specific frequency, whereas a Raman peak maintains a constant separation from the excitation frequency.\n\nA Raman transition from one state to another is allowed only if the molecular polarizability of those states is different. For a vibration, this means that the derivative of the polarizability with respect to the normal coordinate associated to the vibration is non-zero: formula_10. In general, a normal mode is Raman active if it transforms with the same symmetry of the quadratic forms (formula_11), which can be verified from the character table of the molecule's symmetry group.\n\nThe specific selection rules state that the allowed rotational transitions are formula_12, where formula_13 is the rotational state.\n\nThe allowed vibrational transitions are formula_14, where formula_9 is the vibrational state.\n\nThe Raman-scattering process as described above takes place spontaneously; i.e., in random time intervals, one of the many incoming photons is scattered by the material. This process is thus called \"spontaneous Raman scattering\".\n\nOn the other hand, \"stimulated Raman scattering\" can take place when some Stokes photons have previously been generated by spontaneous Raman scattering (and somehow forced to remain in the material), or when deliberately injecting Stokes photons (\"signal light\") together with the original light (\"pump light\"). In that case, the total Raman-scattering rate is increased beyond that of spontaneous Raman scattering: pump photons are converted more rapidly into additional Stokes photons. The more Stokes photons are already present, the faster more of them are added. Effectively, this \"amplifies\" the Stokes light in the presence of the pump light, which is exploited in Raman amplifiers and Raman lasers.\n\nStimulated Raman scattering is a nonlinear-optical effect. It can be described using a third-order nonlinear susceptibility formula_16.\n\nSuppose that the distance between two points A and B of an exciting beam is . Generally, as the exciting frequency is not equal to the scattered Raman frequency, the corresponding relative wavelengths and are not equal. Thus, a phase-shift appears. For , the scattered amplitudes are opposite, so that the Raman scattered beam remains weak.\n\n– A crossing of the beams may limit the path .\n\nSeveral tricks may be used to get a larger amplitude:\n\n– In an optically anisotropic crystal, a light ray may have two modes of propagation with different polarizations and different indices of refraction. If energy may be transferred between these modes by a quadrupolar (Raman) resonance, phases remain coherent along the whole path, transfer of energy may be large. It is an Optical parametric generation.\n\n– Light may be pulsed, so that beats do not appear.\n\nIt is the Impulsive Stimulated Raman Scattering (ISRS), in which the length of the pulses must be shorter than all relevant time constants. Interference of Raman and incident lights is too short to allow beats, so that it produces a frequency shift roughly, in best conditions, inversely proportional to cube of length of pulses. \nIn labs, femtosecond laser pulses must be used because the ISRS becomes very weak if the pulses are too long. Thus ISRS cannot be observed using nanosecond pulses making ordinary time-incoherent light.\n\nThe inverse Raman effect in optics (the branch of physics which deals with the properties and behavior of light) is a form of Raman scattering. It was first noted by W.J.Jones and B.P. Stoicheff.\n\nIf a material is simultaneously irradiated by intense monochromatic light of frequency ν (typically a laser beam) and light of a continuum of higher frequencies, among the possibilities for light scattering are:\n\n\nwhere ν is a Raman frequency of the material. The strength of these two scatterings depends (among other things) on the energy levels of the material, their occupancy, and the intensity of the continuum. In some circumstances Stokes scattering can exceed anti-Stokes scattering; in these cases the continuum (on leaving the material) is observed to have an absorption line (a dip in intensity) at ν+ν. This phenomenon is referred to as the \"inverse Raman effect\"; the application of the phenomenon is referred to as \"inverse Raman spectroscopy\", and a record of the continuum is referred to as an \"inverse Raman spectrum\".\n\nIn the original description of the inverse Raman effect, the authors discuss both absorption from a continuum of higher frequencies and absorption from a continuum of lower frequencies. They note that absorption from a continuum of lower frequencies will not be observed if the Raman frequency of the material is vibrational in origin and if the material is in thermal equilibrium.\n\nRaman spectroscopy employs the Raman effect for substances analysis. The spectrum of the Raman-scattered light depends on the molecular constituents present and their state, allowing the spectrum to be used for material identification and analysis. Raman spectroscopy is used to analyze a wide range of materials, including gases, liquids, and solids. Highly complex materials such as biological organisms and human tissue can also be analyzed by Raman spectroscopy.\n\nFor solid materials, Raman scattering is used as a tool to detect high-frequency phonon and magnon excitations.\n\nRaman lidar is used in atmospheric physics to measure the atmospheric extinction coefficient and the water vapour vertical distribution.\n\nStimulated Raman transitions are also widely used for manipulating a trapped ion's energy levels, and thus basis qubit states.\n\nRaman spectroscopy can be used to determine the force constant and bond length for molecules that do not have an infrared absorption spectrum.\n\nRaman amplification is used in optical amplifiers.\n\nThe Raman effect is also involved in producing the appearance of the blue sky (see Rayleigh Effect: 'Rayleigh scattering of molecular nitrogen and oxygen in the atmosphere includes elastic scattering as well as the inelastic contribution from rotational Raman scattering in air').\n\nFor high-intensity continuous wave (CW) lasers, SRS can be used to produce broad bandwidth spectra. This process can also be seen as a special case of four-wave mixing, wherein the frequencies of the two incident photons are equal and the emitted spectra are found in two bands separated from the incident light by the phonon energies. The initial Raman spectrum is built up with spontaneous emission and is amplified later on. At high pumping levels in long fibers, higher-order Raman spectra can be generated by using the Raman spectrum as a new starting point, thereby building a chain of new spectra with decreasing amplitude. The disadvantage of intrinsic noise due to the initial spontaneous process can be overcome by seeding a spectrum at the beginning, or even using a feedback loop as in a resonator to stabilize the process. Since this technology easily fits into the fast evolving fiber laser field and there is demand for transversal coherent high-intensity light sources (i.e., broadband telecommunication, imaging applications), Raman amplification and spectrum generation might be widely used in the near-future.\n\n"}
{"id": "690345", "url": "https://en.wikipedia.org/wiki?curid=690345", "title": "Season (sports)", "text": "Season (sports)\n\nIn an organized sports league, a typical season is the portion of one year in which regulated games of the sport are in session: for example, in Major League Baseball the season lasts approximately from April to October. In other team sports, like association football or basketball, it is generally from August or September to May although in some countries - such as Northern Europe or East Asia - the season starts in the spring and finishes in autumn, mainly due to weather conditions encountered during the winter.\n\nA year can often be broken up into several distinct sections (sometimes themselves called seasons). These are: a preseason, a series of exhibition games played for training purposes; a regular season, the main period of the league's competition; the postseason, a playoff tournament played against the league's top teams to determine the league's champion; and the offseason, the time when there is no official competition.\n\nIn sport, the term \"regular season\" or \"home and away season\" refers to the sport's league competition. The regular season is usually similar to a group tournament format: teams are divided into groups, conferences and/or divisions, and each club plays a set number of games against a set number of opponents. In most countries the league is played in a double round-robin format, where every team plays every other team twice, once at their home venue, and once away at the oppositions venue as visitors. The results over all games are accumulated and when every team has completed its full schedule of games, a winner is declared. \n\nIn North America, the scheduling is different. Rather than every team playing all others twice, teams usually play more games against local rivals than teams in other parts of the country. For example, the NBA's Los Angeles Lakers will play the Los Angeles Clippers (a team within their division, a subdivision of the conference) four times in a regular season, while both will only play the Boston Celtics, who are in the opposite Eastern Conference, twice. Part of this is due to the vast geographic distances between some teams in North America—measured in a straight line, Los Angeles is 2,606 miles (4,194 kilometers) from Boston, for instance—and a desire to limit travel expenses. In the scheduling system used in the NFL, it is possible for two teams to only meet every four years, and to only have 2 common opponents in a season. Major League Baseball has the most uneven schedules of all the four major North American sports. In MLB the conferences are called leagues instead, but have exactly the same effect as conferences (as with all North American major leagues, leagues, conferences and division are not based on skill, but instead geography, history and rivalries). Teams play 19 games against each of teams in their own division each year but will only play 20 games total against all of the teams in the other league. Because each of the interleague matchups is part of a 3-game series or a 2-game series, teams will play no games at all against most teams from the other league. They play 6 of the 15 teams in the other league, a historically high number (until 1997, interleague play was limited to exhibition matches and the postseason World Series, and thus MLB teams did not play the other league's teams at all).\n\nIn Australia, the two largest football leagues, the AFL (Australian rules football) and NRL (rugby league), both grew out of competitions held within a single city (respectively Melbourne and Sydney) and only began expanding to the rest of the country when inexpensive air travel made a national league possible. These leagues use a single table instead of being split into divisions. The term \"home and away season\" is sometimes used instead of regular season.\n\nMany football leagues in Latin America have a very different system. Because most Latin American countries never had a football cup competition, they instead split their season into two parts, typically known as the Apertura and Clausura (Spanish for \"opening\" and \"closing\"). Most countries that use this system, Argentina being one notable example, crown separate league champions for each part of the season, using only league play. A few others, such as Uruguay, crown one champion at the end of a playoff involving top teams from each half of the season. Mexico operates its Apertura and Clausura as separate competitions that both end in playoffs. Brazil have a different system, the season starts with the state championships in January (every Brazilian state have his own championship), these state championships ends in April. The Campeonato Brasileiro Série A itself starts in May and ends in early December, and is played in a double round-robin format in the same way as the European championships.\n\nA system similar to the Apertura and Clausura developed independently in Philippine professional basketball, with formerly two, now three tournaments (called \"conferences\") in one season, with each conference divided into an \"elimination round\" (the regular season) and the playoffs in the North American sense. Winning the playoffs is the ultimate goal of every team for every conference; while there is no season championship, winning all conferences within a single season is rare and has only happened five times since 1975, with the two most recent examples occurring in 1996 and 2013–14. The elimination round and playoffs setup has permeated down to the local level and in most team sports, although seasons are not divided into conferences.\n\nMany sports leagues have playoffs or \"finals\" that occur after the regular season is complete. A subset of the teams enter into a playoff tournament, usually a knockout tournament, generally a pre-determined number with better overall records (more wins, fewer losses) during the regular season. There are many variations used to determine the champion, the league's top prize. In many of these leagues, winning the league's top prize at the conclusion of the postseason is more important than winning the regular season. This includes the five major U.S. sports leagues (Super Bowl, Stanley Cup Finals, NBA Finals, World Series and MLS Cup), the major Australian sports (NBL Grand Final, A-League Grand Final, AFL Grand Final and NRL Grand Final) and the CFL's Grey Cup.\n\nEuropean leagues have also started holding playoffs after a double round-robin \"regular season\". The Football League started its promotion playoffs in 1987, with the third up to the sixth-ranked teams participating for the final promotion berth (the two top teams are automatically promoted). Elsewhere, relegation playoffs are also held to determine which teams would be relegated to the lower leagues. One prominent top-level football league, the Eredivisie of the Netherlands, uses two different playoffs—one for relegation purposes, and the other to determine one of the league's entrants in the following season's UEFA Europa League. In Superleague Greece, which currently has two places in the UEFA Champions League and three in the Europa League, the teams that finish second through fifth in the regular season enter a home-and-away \"playoff\" mini-league. Since one Europa League place is reserved for the country's cup winner, only three of the four teams are guaranteed a place in the next season's European competitions (unless both the cup winner and runner-up are already qualified for Europe by other means). The playoff determines the country's second Champions League participant, and the points at which the two or three Europa League entrants join that competition. Conversely, some leagues like the Premier League do not hold a postseason, and therefore these leagues' champions and relegation are instead based on the regular season records.\n\nAlthough rugby union did not become professional until 1995, that sport has a long history of playoffs, primarily in France and the Southern Hemisphere. The French national championship, now known as Top 14, staged a championship final in its first season of 1892, first used more than one round of playoffs in 1893, and has continuously operated a playoff system (except during the two World Wars) since 1899. South Africa's Currie Cup has determined its champions by playoffs since 1968, and New Zealand's National Provincial Championship, the top level of which is now known as the Mitre 10 Cup, has used playoffs since its creation in 1976. Argentina's Nacional de Clubes has determined its champion by playoffs since its inception in 1993. Currently, two separate competitions feed into the Nacional, the Torneo de la URBA (for Buenos Aires clubs, held since 1899) and Torneo del Interior (for the rest of the country); both use playoffs to determine their champions. Super Rugby, involving regional franchises from Australia, New Zealand, and South Africa and national franchises in Argentina and Japan, has used playoffs to determine its champions since its creation as Super 12 in 1996.\n\nBy contrast, other European countries were slow to adopt playoffs in rugby union. The English Premiership only began playoffs in 1999–2000, and did not use them to determine the league champion until 2002-03. The Celtic League, now known as Pro14, resisted a playoff system even longer; its champions were determined solely by league play from its inception in 2001–02 until playoffs began in 2009–10.\n\nWhen the UEFA Champions League reformatted in 1993, it added a \"knockout stage\" involving four teams that finished at the top two places in their respective groups. Like North American sports leagues, this setup prevented some participants from facing each other, necessitating a two-round knockout stage to determine the champions. It has since been expanded to the 4-round knockout stage today. The Copa Libertadores has applied a knockout stage since the 1988 tournament, expanding to the current four-round format next season. All intercontinental club football competitions now feature a knockout stage.\n\nThe off-season, vacation time, or close season is the time of year when there is no official competition. Although upper management continues to work, the athletes will take much vacation time off. Also, various events such as drafts, transfers and important off-season free agent signings occur. Generally, most athletes stay in shape during the off-season in preparation for the next season. Certain new rules in the league may be made during this time, and will become enforced during the next regular season.\n\nAs most countries which have a league in a particular sport will operate their regular season at roughly the same time as the others, international tournaments may be arranged during the off season.\n\nFor example, most European football league club competitions run from July or August to May, subsequently major international competitions such as the FIFA World Cup and UEFA European Football Championship are organised to occur in June and July.\n\nThe table represents typical seasons for some leagues by month. Blank or white denotes off-season and pre-season months and solid colors mark the rest of the year. Leagues in the same sport use the same color.\n\n"}
{"id": "41414610", "url": "https://en.wikipedia.org/wiki?curid=41414610", "title": "Soil Use Efficiency", "text": "Soil Use Efficiency\n\nSoil Use Efficiency (SUE) is the use of individual and inter-related factors (inherent and dynamic) related to soil quality, soil nutrient availability and nutrient uptake potential as effective reference points for improvement of crop productivity in individual and varying soil types. Assessing SUE involves a site evaluation of the land and pit excavation to examine the soil profile. Site characterization identifies impairments to biomass productivity and the provisioning of ecological services. Inherent impairments are from limits inherent to the land and soil such as steep slope, and subsoil claypan. Dynamic impairments are the result of land degradation, such as soil acidification, loss of soil carbon, water erosion, and wind erosion. Understanding these relationships informs land management decisions needed to restore land productivity. The determination of water-use efficiency (WUE) and Nutrient Use Efficiency (NUE) in agricultural production systems is governed primarily by the boundary conditions of Soil Use Efficiency (SUE).\n"}
{"id": "16823330", "url": "https://en.wikipedia.org/wiki?curid=16823330", "title": "Solar America Cities", "text": "Solar America Cities\n\nSolar America Cities is a U.S. Department of Energy (U.S. DOE) initiative to promote solar energy at the local level through city programs.\n\nThe DOE has named 25 U.S. cities as a Solar America City which are promoting solar technology adoption at the local level.\n\nThese cities will take a comprehensive, citywide approach to solar technology that facilitates its mainstream acceptance. Solar technologies promoted by Solar America Cities include photovoltaics and concentrating solar power (which both produce solar electricity) as well as solar water and air heating.\n\nThey represent 16 different states and have varying degrees of solar resources and experience with solar technologies. The cities were not selected for their number of sunny days. Rather, DOE selected these urban communities based on their long-term commitment to developing solar energy markets in their municipalities. Some have extensive experience with solar power and some are just getting started. Different cities are also taking different approaches to building a sustainable solar infrastructure. DOE expects that these cities’ creative efforts will serve as models for other urban communities in the future.\n\nThe 25 cities selected are: \n\nThe desired outcomes of the Solar America Cities are: \n\nThe Solar America Cities program has engaged over 180 organizations, including municipal, county, and state agencies, solar companies, universities, utilities, and non-profit organizations. These partners have made a commitment to power their cities with clean, safe, reliable energy—solar energy.\n\nThe Solar America Cities program has received a total of $4.9 million in federal financial assistance.\n\n"}
{"id": "21689894", "url": "https://en.wikipedia.org/wiki?curid=21689894", "title": "Sorong Fault", "text": "Sorong Fault\n\nSorong fault also (Sorong Fault Zone, SFZ) is an active, broad zone of inferred left lateral shear at the triple junction of the Australian plate, Eurasian plate, and Pacific plates, where many plate fragments exist, such as the Philippine Sea Plate, Bird's Head Plate, Halmahera Plate and the Molucca Sea Plate. It has been implicated in numerous large earthquakes. It is one of the two major faults created by the Australian and Pacific plate convergence, the other being the Ramu-Markham Fault zone.\n\nThe Sorong fault begins around Sulawesi and runs towards New Guinea. It marks the boundary between Halmahera plate and Bird's Head Plate. The Sorong fault system has been documented to extend westward more than 500 miles (800 km) from Teluk Sarera to Kepulauan Banggai.\n\nThe fault zone juxtaposes Mesozoic-Tertiary continental and volcanic arc/ophiolitic rocks.\n\nIt is widely believed that fragments of the northern Australian continental margin in New Guinea are being detached and translated westward in this shear zone until they collide with the eastern margin of Eurasia (Sundaland) in the region of Sulawesi Island. However, the details of terrane translation, amalgamation, and docking remain poorly documented at this time. In particular, the timing of events is very poorly constrained, with estimates for the commencement of the SFZ ranging from early Miocene or older to Pleistocene. Investigations in the 1990s of the SFZ and the adjacent regions of Sulawesi and Irian Jaya (Indonesian New Guinea), including fieldwork in several of the SFZ island-terranes (Waigeo Island, Halmahera, Bacan, Obi Islands, and Sula islands), suggest a less mobilist interpretation of the region than previous reconstructions. SFZ did not begin to develop in its present form before the late Miocene.\n\n"}
{"id": "23112262", "url": "https://en.wikipedia.org/wiki?curid=23112262", "title": "South Atlantic Convergence Zone", "text": "South Atlantic Convergence Zone\n\nThe South Atlantic convergence zone, or SACZ, is an elongated axis of clouds, precipitation, and convergent winds oriented in a northwest-southeast manner across southeast Brazil into the southwest Atlantic Ocean. By definition, the feature is a monsoon trough. It is strongest in the warm season. Thunderstorm activity along the feature magnifies over three or more days when the Madden–Julian oscillation passes into the region, due to the enhanced upper divergence. Low level winds over Rondônia are tied to the strength of this zone, where westerly wind anomalies correlate to active phases of the South American monsoon, while easterly wind anomalies indicate breaks of activity along the SACZ. The feature is also strongest with negative anomalies in the sea surface temperature pattern lie over the southern Atlantic Ocean, while opposite conditions prevail across the northern Atlantic Ocean.\n"}
{"id": "31156672", "url": "https://en.wikipedia.org/wiki?curid=31156672", "title": "Supermoon", "text": "Supermoon\n\nA supermoon is a full moon or a new moon that approximately coincides with the closest distance that the Moon reaches to Earth in its elliptic orbit, resulting in a slightly larger-than-usual apparent size of the lunar disk as seen from Earth. The technical name is the perigee syzygy of the Earth–Moon–Sun system or more simply full (or new) Moon at perigee. The term \"supermoon\" is astrological in origin and has no precise astronomical definition.\n\nThe real association of the Moon with both oceanic and crustal tides has led to claims that the supermoon phenomenon may be associated with increased risk of events like earthquakes and volcanic eruptions, but no such link has been found.\n\nThe opposite phenomenon, an apogee syzygy, has been called a micromoon.\n\nThe name \"supermoon\" was coined by astrologer Richard Nolle in 1979, in \"Dell Horoscope\" magazine arbitrarily defined as:\n\nHe came up with the name while reading “Strategic Role Of Perigean Spring Tides in Nautical History and Coastal Flooding” published in 1976 by NOAA Hydrologist Fergus Wood. Nolle never outlined why he chose 90%, but explained in 2011 that he based calculations on 90% of the difference in lunar apsis extremes for the solar year. In other words, a full or new moon is considered a supermoon if formula_1 where formula_2 is the lunar distance at syzygy, formula_3 is the lunar distance at apogee, and formula_4 is the lunar distance at perigee.\n\nIn practice, there is no official or even consistent definition of how near perigee the full Moon must occur to receive the supermoon label, and new moons rarely receive a supermoon label. Sky and Telescope magazine refers to full Moon which comes within , TimeandDate.com prefers a definition of . EarthSky uses Nolle's definition comparing their calculations to tables published by Nolle in 2000.\n\nThe term \"perigee-syzygy\" or \"perigee full/new moon\" is preferred in the scientific community. Perigee is the point at which the Moon is closest in its orbit to the Earth, and syzygy is when the Earth, the Moon and the Sun are aligned, which happens at every full or new moon. Astrophysicist Fred Espenak uses Nolle's definition but preferring the label of \"full Moon at perigee\". Wood used the definition of a full or new moon occurring within 24 hours of perigee and also used the label \"perigee-syzygy\".\n\nWood also coined the less used term \"proxigee\" where perigee and the full or new moon are separated by 10 hours or less.\n\nOf the possible 12 or 13 full (or new) moons each year, usually three or four may be classified as supermoons, as commonly defined.\n\nThe most recent full supermoon occurred on January 31, 2018, and the next one will be on December 22, 2018. The one on November 14, 2016 was the closest full supermoon since January 26, 1948, and will not be surpassed until November 25, 2034. The closest full supermoon of the 21st century will occur on December 6, 2052.\n\nThe oscillating nature of the distance to the full or new moon is due to the difference between the synodic and anomalistic months. The period of this oscillation is roughly 14 synodic months.\n\nOccasionally, a supermoon coincides with a total lunar eclipse. The most recent occurrence of this was in January 2018, and the next will be in January 2019.\n\nA full moon at perigee appears roughly 14% larger in diameter than at apogee. Many observers insist that the moon looks bigger to them. This is likely due to observations shortly after sunset when Moon is near the horizon and the moon illusion is at its most apparent.\n\nWhile the moon's surface luminance remains the same, because it is closer to the earth the illuminance is about 30% brighter than at its farthest point, or apogee. This is due to the inverse square law of light which changes the amount of light received on earth in inverse proportion to the distance from the moon.\nWhile a typical summer full moon at temperate latitudes provides only about 0.05-0.1 lux, a supermoon directly overhead in the tropics could provide up to 0.36 lux.\n\nClaims that supermoons can cause natural disasters, and the claim of Nolle that supermoons cause \"geophysical stress\", have been refuted by scientists.\n\nDespite lack of scientific evidence, there has been media speculation that natural disasters, such as the 2011 Tōhoku earthquake and tsunami and the 2004 Indian Ocean earthquake and tsunami, are causally linked with the 1–2 week period surrounding a supermoon. A large, 7.5 magnitude earthquake centred 15 km north-east of Culverden, New Zealand at 00:03 NZDT on November 14, 2016, also coincided with a supermoon.\n\nScientists have confirmed that the combined effect of the Sun and Moon on the Earth's oceans, the tide, is when the Moon is either new or full. and that during lunar perigee, the tidal force is somewhat stronger, resulting in perigean spring tides. However, even at its most powerful, this force is still relatively weak, causing tidal differences of inches at most.\n\nTotal lunar eclipses which fall on supermoon and micromoon days are relatively rare. In 21st century, there are 87 total lunar eclipses, of which 28 are supermoons and 6 are micromoons. Almost all total lunar eclipses in Lunar Saros 129 are micromoon eclipses. \n\n"}
{"id": "380721", "url": "https://en.wikipedia.org/wiki?curid=380721", "title": "Vodyanoy", "text": "Vodyanoy\n\nIn Slavic mythology, vodyanoy or vodyanoi (; lit. '[he] from the water' or 'watery') is a male water spirit. Vodník (or in Germanized form hastrman) in Czech fairy tales is the same creature as the Wassermann or nix of German fairy tales. \n\nVodyanoy is said to appear as a naked old man with a frog-like face, greenish beard, and long hair, with his body covered in algae and muck, usually covered in black fish scales. He has webbed paws instead of hands, a fish's tail, and eyes that burn like red-hot coals. He usually rides along his river on a half-sunk log, making loud splashes. Consequently, he is often dubbed \"grandfather\" or \"forefather\" by the local people. Local drownings are said to be the work of the vodyanoy (or rusalkas).\n\nWhen angered, the vodyanoy breaks dams, washes down water mills, and drowns people and animals. (Consequently, fishermen, millers, and also bee-keepers make sacrifices to appease him.) He would drag down people to his underwater dwelling to serve him as slaves.\n\nIn Czech, Slovene and Slovak folklore the features of the vodník are markedly different to the East-Slavic conception; he has a completely human constitution and habits, except for few differences – vodníci (plural of vodník) have gills, webbed membrane between their fingers and their skin is algae-green in colour (as well as their hair, which is typically of pale green tone). Their overall dress and appearance is weird, sometimes even resembling a vagrant; patchy shirts and (by modern standards) odd hats - often boaters with long speckled ribbons - are commonplace. They can withstand lingering for hours outside their ponds. When they do so, one can tell them unequivocally by their wet coat-tails from which water is dripping under all circumstances. The vodník's face is usually unshaven and it is not uncommon for a vodník to have a large, wet, tangled beard.\n\nCzech, Slovenian and Slovak tales have both evil and good vodníci (relative to human beings) who do (or don't, respectively) try to drown people when they happen to swim in their territory. Vodníci would store the souls of the drowned in porcelain teapots. They consider their teapots as the most valuable heritage and display their \"work\", and number these cups they see as proportional to their wealth and/or status among other vodníci. When the lid of such a cup is removed, the soul within (in a form of a bubble) will escape and be liberated. Except for fish (or perhaps fish spirits), they do not have servants. Otherwise, vodníci spend their time by running their territory or – in their spare time – playing cards, smoking pipes or just sitting at the water surface (on rocks, willows nearby) and loitering. Fishermen ask the vodník for help by placing a pinch of tobacco in the water and saying, \"Here's your tobacco, Lord Vodník, now give me a fish.\" In Czech, Slovak and Slovene tales vodníci live in ponds or rivers; there is no mention of a particular dwelling and the \"half-sunken log\" is unapparent. There are almost no references to vodníci in connection with sea water, which it is supposed would be dangerous, even deadly for them.\n\n\n\n"}
{"id": "56269979", "url": "https://en.wikipedia.org/wiki?curid=56269979", "title": "Vulnerable waters", "text": "Vulnerable waters\n\nVulnerable waters refer to geographically-isolated wetlands (GIWs) and to ephemeral and intermittent streams. Ephemeral and intermittent streams are seasonally flowing and are located in headwater position. They are the outer and smallest stems of hydrological networks. Isolated wetlands are located outside floodplain and show poor surface connection to tributaries or floodplains. Geographically isolated wetlands encompass saturated depressions that are the result of fluvial, aeolian, glacial and/or coastal geomorphological processes. They may be natural landforms or the result of human interventions. Vulnerable waters represent the major proportion of river networks.\n\nThese water bodies show vulnerability to natural and human disturbances because they are poorly hydrologically connected, and they are often located in the gray zone of countries and states' protected water regulatory frameworks. In the USA, the protection status of GIWs and ephemeral/intermittent streams in regard to the Clean Water Act is being revised. In the context of European Union (EU) Water Framework Directive (WFD), small headwater streams are neglected, especially in agricultural setting.\n\nThese water bodies play an essential hydrological and ecological role at the local-to catchment scale. They control storage of water, sediment in the drainage network, increase sediment filtering and biochemical transformation. Furthermore, vulnerable waters contribute to increases in landscape biodiversity as they serve as refuge to endemic species and conduits for migration. Headwater streams and isolated wetlands shows hydrological and ecological connectivity through intermittent surface processes and groundwater processes.\n\nHeadwater streams refer to the smallest channels of a river network, where streamflow begins. They are considered first- to third-order streams in the Strahler 's stream classification system. The designation of intermittent and ephemeral stream refers to the continuum of streamflow within a year. An ephemeral stream flows episodically, following a precipitation event, while an intermittent stream flow continuously during a portion of the year. In both cases, the drying of the channel results from the local water table declining below the bed surface. The majority of ephemeral and intermittent streams are in headwater positions, but in lowland settings, small tributaries along the river network can be ephemeral or intermittent.\n\nGeographically isolated wetlands (GIWs) are wetlands entirely surrounded by uplands. GIWs receive water from adjacent uplands and precipitation. However, no stream of any type supplies water to GIWs. Despite GIWs having poor hydrological connectivity with stream networks, they can exhibit subsurface connectivity or even temporary surface-water outflows toward other wetlands or streams. GIWs lacking complete surface or subsurface hydrological connectivity with any water body will lose water mainly by evapotranspiration or to groundwater that is not connected to a stream network. Despite the absence of hydrological connectivity, they can exhibit biological and chemical connectivity with fluvial systems.\n\nGIWs that are hydrologically connected (by subsurface connection or temporary surface connection) can be considered non-floodplain wetlands. Non-floodplain wetlands are located outside floodplains and display a unidirectional hydrological connectivity with streams, meaning the water is flowing only toward stream located at lower elevations. Hydrological connectivity between non-floodplain wetlands and streams occurs through surface or subsurface processes. Surface connections can be ephemeral and intermittent streams.\n\nIn the USA, the natural types of GIWS are: prairie pothole wetlands, playas, Nebraska’s Rainwater Basin and Sandhills wetlands, West Coast vernal pools, sinkhole wetlands, Carolina bays, intradunal and interdunal wetlands, desert springs, endorheic basin in the Great Basin, and kettle-hole in glaciated regions.\n\nNon-floodplain wetlands are classified in three categories which include GIWs: depressional wetlands, slope wetlands and flats wetlands. Depressional wetlands occur in topographic depressions with or without surface outlets. Depressional wetlands include kettle holes, potholes, vernal pools, playas lake and Carolina bays. Slope wetlands are located along hillslopes and are mainly recharged by groundwater inputs. Fens are the usual type of slope wetlands. Flats wetlands occur on large flat areas like interfluve, dried lake bottoms or large floodplain terraces. Large playas are a type of mineral soil-dominated flats. Flats wetlands can also be formed from organic soils, like peatbogs.\n\nGIWs and non-floodplain wetlands can emerge from one or a combination of geomorphological processes: aeolian (potholes, playas, Rainwater basin, Carolina Bays, interdunal wetlands), (peri-)glacial (kettle, fens), karstic (sinkholes) and lacustrine (Carolina Bays, endorheic basin).\n\nEphemeral and intermittent headwater streams and GIWs display the shortest drainage area and stream length, but together they can represent the major proportion of river networks and watersheds.\n\nIn the U.S., headwater streams represent more than 60% of the river network length and geographically isolated wetlands encompass about 16% of freshwater resources. In 17 states, there are streams with an intermittent-stream-length-over-total-length's ratio higher than 82%. North Dakota, South Dakota and Minnesota are the three states with the most hectares of geographically isolated wetlands. Many studies report that actual maps of U.S. hydrographic network underestimate the distribution of headwater streams \n\nThe approximate length of first to third order streams in the world is respectively 45 660 000, 22 061 000 and 10 660 100 km, and they represent the dominant Strahler' order of streams in the world.\n\nThe legal status of ephemeral and intermittent headwater streams and GIWs differ from one legislation to another.\n\nIn the USA, Environmental Protection Agency (EPA) has, since 1972, the responsibility to regulate the waters of the United States, under the Clean Water Act (CWA). The Clean Water Act, introduced by president Richard Nixon, made clear that continental waters should be \"swimmable and fishable\" for the American public. That was a great step toward protection of riverine habitats and improvement of water quality.\n\nBecause of the U.S Constitution, the federal government can only protect interstate waters that is used for navigation, which is defined as the \"waters of the United States\" (WOTUS). From 1972 to 2015, the EPA defined WOTUS as:\n\n\"`[...] traditional navigable waters, interstate waters (including interstate wetlands), all other waters that could affect interstate or foreign commerce, impoundments of waters of the United States, tributaries, the territorial seas, and adjacent wetlands\" (CWA, (33 CFR 328.3; 40 CFR 122.2)\n\nThe CWA defined wetlands as:\n\n\"[...] areas that are inundated or saturated by surface or ground water at a frequency and duration sufficient to support, and that under normal circumstances do support, a prevalence of vegetation typically adapted for life in saturated soil conditions. Wetlands generally include swamps, marshes, bogs, and similar areas.\" (CWA, section 404).\n\nThe definition of WOTUS was challenged in court several times, especially regarding the integration of isolated wetlands, but until the years 2000, the U.S. Federal Court stood by the initial definition saying that floodplain wetlands are bound to the streams they are adjacent to. Then, In 2001, a judgement ruled that isolated wetlands are not comprised in the WOTUS definition because they do not show \"significant nexus\" with navigable streams. In 2006, in the Rapanos vs. United States case, further confusion was created concerning the WOTUS definition. No majority decision was obtained, as two main opinions divided the judges. The first one, defended by Justice Antonin Scalia, was that the scope of the CWA only included permanent streams and wetlands with a significant surface connections with navigable streams. The second opinion, led by Justice Anthony Kennedy, was that waters have a significant nexus with navigable waters if they are significantly affecting their chemical, physical, and biological integrity.\n\nThe definition of WOTUS is actually under revision. The United States President, Donald Trump, signed the executive Order 13778 (82 FR 12495, March 3, 2017), asking the EPA and United States Army Corps of Engineers Corps to review the definition of WOTUS in a manner that is consistent with Justice Antonin Scalia's opinion. This opinion calls for protection of permanent waters and wetlands showing surface connections to relatively permanent waters, which exclude GIWS, and ephemeral/intermittent streams.\n\nIn the European Union (EU), since 2000, the Water Framework Directive (WFD) aims to achieve a number of objectives to improve water quality of lakes, rivers and coastal areas. Key objectives are general protection of the aquatic ecosystem, protection of drinking water resources, and protection of swimmable waters. These objectives are realized through river management measures at basin scale. The WFD requires the delineation of water bodies that will be targeted for specific diagnosis and management measures. The smallest area range considered to identify those water bodies is 10 to 100 km square. Due to their small size, headwater streams are not usually identified as one of the water bodies targeted by the WFD and, therefore, become more vulnerable to human activities.\n\nThe U.S. Environmental Protection Agency (EPA) classifies headwater streams (including ephemeral/intermittent streams) and GIWs' ecological functions in five categories: source, sink, refuge, transformation and lag functions. These functions depend on the level of connectivity (hydrological, sedimentological, biological) in-between the sub-components of a river system (channel, floodplain, wetlands). Many functions are common among streams riverine wetlands and non-floodplain wetlands. Many more studies have been conducted on the role of headwater streams compare to GIWs.\n\nHeadwaters streams and GIWs represent the dominant sources of material and energy in river networks. They export water, sediment, nutrients, organic debris and organisms from the upland areas to the downstream portion of the river network.\n\nHeadwater streams are the dominant source of water in a river network. They supply water downstream that is essential to aquatic habitats. They contribute to flooding, and in doing transfer sediments and nutrients to adjacent riverine habitats. Headwater streams are also typically erosion zones. They collect sediment from bank erosion and from colluvium in mountainous areas. Sediment inputs from headwater streams influence the dynamic equilibrium between runoff discharge and transport capacity, responsible for aggradation and degradation of channels. Headwater streams also collect wood, organic matter, nutrients and fine particles through erosion and flooding of riverine wetlands. Headwater streams provide rich-nutrient waters that benefit micro-organisms, like algae and invertebrates. For example, it was demonstrated that first-order streams contribute to 40% of nitrogen reaching fourth and fifth-order streams. It was also demonstrated that headwater streams are sources of invertebrates that benefit the whole food chain downstream: salmonidae are a good example of species that benefit. Headwater streams supply organic matter downstream that is essential to physical and biological processes throughout the river network. They also deliver wood logs and wood debris that exert an influence over channel morphology, runoff velocity and on the spatial distribution of ecological habitats.\n\nGIWs can be the source of headwater streams. GIWs can provide the major proportion of stream's water in dry period. However, the process of water transfer from a GIWs to a stream is depending on antecedent moisture conditions prevailing in the GIWs. Under saturated conditions, GIWs will supply water to other downstream water bodies, including streams. There are abundant and diverse microbial populations in GIWs. Low pH, low salinity and presence of organic matter create favorable conditions for the development of sulfate-reducing bacteria. These bacteria are responsible for the production of methylmercury. GIWs are thus source of methylmercury, and other dissolved organic compounds and acids that can be transported downstream by temporary surface flows. Despite the fact the methylmercury is a particularly toxic pollutant, dissolved organic matter is a major source of energy for aquatic organisms located downstream in the river network.\n\nSink function refers to the overall net import of energy and materials from the stream to the riparian environment or outside the stream network.\n\nIn natural settings, many exchanges of water occur between headwater streams and their riparian environments. Bed friction and friction with the riparian surface during overbank flows result in a net loss of flow energy, especially in mountainous streams with coarse bed loads. It results in net decrease of the erosion capacity of the stream in its downstream section. In the riparian zone, friction and shallow water depth contributes to decreases in flow velocity and to the deposition of suspended sediments. Meanwhile, a net loss of water from the flooded riparian environment to the atmosphere can occur through evaporation or by transpiration of vegetation. Intake of nitrogen by bacteria, as runoff water charged with nutrient penetrates the hyporheic zone, is another demonstration of headwater stream's sink function. Knight et al. (2010) determined that riverine wetlands acting as buffer are the most effective tool to mitigate the effect of non-point sources of pollution to streams.\n\nGIWs, like riverine wetlands, can intercept nutrients and other pollutants from point-sources (ex. ditchs, drainage pipes) or from diffuse (non-point) sources (ex. leaching from agricultural fields). Processes involved in capturing nutrients are various and include: the process of denitrification, phosphorus retention through plant assimilation and sorption or sedimentation processes. Removal of nutrients by GIWs has a great influence over water quality in stream networks. A study by Dierberg and Brezonik (1984) demonstrated that a forested non-floodplain wetland was responsible for removing 95% of phosphorus, nitrate, ammonium and total nitrogen after human sewage was applied. Under low-saturated conditions, GIWs will store water instead of releasing it. Storage of water and subsequent evapotranspiration will result in an overall loss of water for the stream network.\n\nRefuge function refers to providing favorable conditions for many aquatic and terrestrial lifeforms.\n\nHeadwater streams and their riverine wetlands offer shelters from predation, drying, and extreme temperatures to many organisms. They provide habitats that are essential for the completion of a portion or of the full life cycle of fish species, macro-invertebrates, mammals, bird and amphibian species. Riverine wetlands display a mosaic of habitats due to the spatial heterogeneity of hydrological and morphological processes. The diversity of habitats and the abundance of food (see Source function) make riverine wetlands ideal feeding, breeding and shelter sites for fish species, amphibians and macroinvertebrates. Riverine wetlands also shelter a high diversity of plant species. Overbank flows within the floodplain are used by plants to disseminate their seeds In return, living organisms contribute to the spatial and temporal complexity of fluvial systems which is essential to maintaining a high level of connectivity between the streams and their riverine environment. For example, dams building by beaver create pools along headwater streams that eventually become suitable fish habitats and increase groundwater surface water interactions.\n\nGIWs have been identified as breeding site for birds, fish species, mammals (muskrats, otters), amphibians and reptiles. Fish species benefit from the temporary rise of water levels and creation of surface connections to migrate from GIWs to streams or others wetlands. Mammals and bird species serve as transport vectors for the dissemination of plants seeds, algae and invertebrates.\n\nTransformation function refers to the biogeochemical processing of organic and non-organic elements.\n\nNutrients entering headwater streams undergo many cycles of transformation through biological and chemical processes (absorption by algae, digestion by a fish, uptake by bacterias etc.). The cycling of nutrients through different forms and different compartments of the fluvial system is called “nutrient spiraling”. Organic matter will also undergo cycle of transformation in headwater streams, mainly through respiration by organisms and microbes. Other processes of transformation of organic matter, like dead leaves, include immersion, physical abrasion and photodegradation. The exchanges of water through the hyporheic zone of headwater streams can also mediate the form and mobility of pollutants, hereby decreasing pollutant concentrations downstream. In riverine wetlands, a lot of transformation processes occur in which nutrients and other compounds are lost to the atmosphere or sequestered in the soil or vegetation.\n\nTransformation of elemental mercury to methylmercury is performed by microbial communities living in acidic wetlands (see Source function). Methylmercury is a toxic form of mercury that is very mobile and that accumulates in the food chain. Denitrification is another transformation process occurring in GIWs.\n\nLag function refers to the transient storage of energy and materials. Because they are the dominant sources of water in stream network, headwater streams and wetlands have a great impact on the frequency, duration and magnitude of downstream transfer of materials and energy. The intensity of the lag function is correlated to the abundance and diversity of local storage components (wetlands, alluvial aquifers, stream banks and floodplains) and to the level of connectivity between these components.\n\nIn headwaters streams, flowing water interacts with channel bedforms, stream banks and vegetation. These interactions result in reduced flow velocity and transient storage of groundwater, which lessen the flood's magnitude during heavy precipitation events. On the other end, during the dry season, the transient storage and delayed transfer of groundwater to the stream will maintain a minimum baseflow essential for aquatic species. The same process applies for sediments, nutrients and organic matter being transported downstream from the upper areas of a watershed and being temporarily stored in the floodplain, including riverine wetlands. Living organisms present in headwater streams contribute to delaying the downstream transfer of sediments, nutrients and organic matter through consumption, assimilation, and bioconsolidation.\n\nTransient storage of water in GIWs contributes to a delay in input of precipitation water to streams or other connected water bodies. Such a function secures the base flow of streams and contributes to recharging local and regional aquifers, especially during dry periods Transient storage in GIWs contributes also to lessening flood magnitude during heavy precipitation events or during melt periods. In contrast, because storage capacity is largely determined by antecedent moisture conditions, a saturated GIWs will convey water downstream rapidly, which could increase flood magnitude. Following this idea, GIWs can also reduce base flow, through storage and evapotranspiration, when saturation conditions are low.\n"}
{"id": "33978", "url": "https://en.wikipedia.org/wiki?curid=33978", "title": "Weather", "text": "Weather\n\nWeather is the state of the atmosphere, describing for example the degree to which it is hot or cold, wet or dry, calm or stormy, clear or cloudy. Most weather phenomena occur in the lowest level of the atmosphere, the troposphere, just below the stratosphere. Weather refers to day-to-day temperature and precipitation activity, whereas climate is the term for the averaging of atmospheric conditions over longer periods of time. When used without qualification, \"weather\" is generally understood to mean the weather of Earth.\n\nWeather is driven by air pressure, temperature and moisture differences between one place and another. These differences can occur due to the sun's angle at any particular spot, which varies with latitude. The strong temperature contrast between polar and tropical air gives rise to the largest scale atmospheric circulations: the Hadley Cell, the Ferrel Cell, the Polar Cell, and the jet stream. Weather systems in the mid-latitudes, such as extratropical cyclones, are caused by instabilities of the jet stream flow. Because the Earth's axis is tilted relative to its orbital plane, sunlight is incident at different angles at different times of the year. On Earth's surface, temperatures usually range ±40 °C (−40 °F to 100 °F) annually. Over thousands of years, changes in Earth's orbit can affect the amount and distribution of solar energy received by the Earth, thus influencing long-term climate and global climate change.\n\nSurface temperature differences in turn cause pressure differences. Higher altitudes are cooler than lower altitudes, as most atmospheric heating is due to contact with the Earth's surface while radiative losses to space are mostly constant. Weather forecasting is the application of science and technology to predict the state of the atmosphere for a future time and a given location. The Earth's weather system is a chaotic system; as a result, small changes to one part of the system can grow to have large effects on the system as a whole. Human attempts to control the weather have occurred throughout history, and there is evidence that human activities such as agriculture and industry have modified weather patterns.\n\nStudying how the weather works on other planets has been helpful in understanding how weather works on Earth. A famous landmark in the Solar System, Jupiter's \"Great Red Spot\", is an anticyclonic storm known to have existed for at least 300 years. However, weather is not limited to planetary bodies. A star's corona is constantly being lost to space, creating what is essentially a very thin atmosphere throughout the Solar System. The movement of mass ejected from the Sun is known as the solar wind.\n\nOn Earth, the common weather phenomena include wind, cloud, rain, snow, fog and dust storms. Less common events include natural disasters such as tornadoes, hurricanes, typhoons and ice storms. Almost all familiar weather phenomena occur in the troposphere (the lower part of the atmosphere). Weather does occur in the stratosphere and can affect weather lower down in the troposphere, but the exact mechanisms are poorly understood.\n\nWeather occurs primarily due to air pressure, temperature and moisture differences between one place to another. These differences can occur due to the sun angle at any particular spot, which varies by latitude from the tropics. In other words, the farther from the tropics one lies, the lower the sun angle is, which causes those locations to be cooler due the spread of the sunlight over a greater surface. The strong temperature contrast between polar and tropical air gives rise to the large scale atmospheric circulation cells and the jet stream. Weather systems in the mid-latitudes, such as extratropical cyclones, are caused by instabilities of the jet stream flow (see baroclinity). Weather systems in the tropics, such as monsoons or organized thunderstorm systems, are caused by different processes.\n\nBecause the Earth's axis is tilted relative to its orbital plane, sunlight is incident at different angles at different times of the year. In June the Northern Hemisphere is tilted towards the sun, so at any given Northern Hemisphere latitude sunlight falls more directly on that spot than in December (see Effect of sun angle on climate). This effect causes seasons. Over thousands to hundreds of thousands of years, changes in Earth's orbital parameters affect the amount and distribution of solar energy received by the Earth and influence long-term climate. (See Milankovitch cycles).\n\nThe uneven solar heating (the formation of zones of temperature and moisture gradients, or frontogenesis) can also be due to the weather itself in the form of cloudiness and precipitation. Higher altitudes are typically cooler than lower altitudes, which the result of higher surface temperature and radiational heating, which produces the adiabatic lapse rate. In some situations, the temperature actually increases with height. This phenomenon is known as an inversion and can cause mountaintops to be warmer than the valleys below. Inversions can lead to the formation of fog and often act as a cap that suppresses thunderstorm development. On local scales, temperature differences can occur because different surfaces (such as oceans, forests, ice sheets, or man-made objects) have differing physical characteristics such as reflectivity, roughness, or moisture content.\n\nSurface temperature differences in turn cause pressure differences. A hot surface warms the air above it causing it to expand and lower the density and the resulting surface air pressure. The resulting horizontal pressure gradient moves the air from higher to lower pressure regions, creating a wind, and the Earth's rotation then causes deflection of this air flow due to the Coriolis effect. The simple systems thus formed can then display emergent behaviour to produce more complex systems and thus other weather phenomena. Large scale examples include the Hadley cell while a smaller scale example would be coastal breezes.\n\nThe atmosphere is a chaotic system. As a result, small changes to one part of the system can accumulate and magnify to cause large effects on the system as a whole. This atmospheric instability makes weather forecasting less predictable than tides or eclipses. Although it is difficult to accurately predict weather more than a few days in advance, weather forecasters are continually working to extend this limit through meteorological research and refining current methodologies in weather prediction. However, it is theoretically impossible to make useful day-to-day predictions more than about two weeks ahead, imposing an upper limit to potential for improved prediction skill.\n\nWeather is one of the fundamental processes that shape the Earth. The process of weathering breaks down the rocks and soils into smaller fragments and then into their constituent substances. During rains precipitation, the water droplets absorb and dissolve carbon dioxide from the surrounding air. This causes the rainwater to be slightly acidic, which aids the erosive properties of water. The released sediment and chemicals are then free to take part in chemical reactions that can affect the surface further (such as acid rain), and sodium and chloride ions (salt) deposited in the seas/oceans. The sediment may reform in time and by geological forces into other rocks and soils. In this way, weather plays a major role in erosion of the surface.\n\nWeather, seen from an anthropological perspective, is something all humans in the world constantly experience through their senses, at least while being outside. There are socially and scientifically constructed understandings of what weather is, what makes it change, the effect it has on humans in different situations, etc. Therefore, weather is something people often communicate about.\n\nWeather has played a large and sometimes direct part in human history. Aside from climatic changes that have caused the gradual drift of populations (for example the desertification of the Middle East, and the formation of land bridges during glacial periods), extreme weather events have caused smaller scale population movements and intruded directly in historical events. One such event is the saving of Japan from invasion by the Mongol fleet of Kublai Khan by the Kamikaze winds in 1281. French claims to Florida came to an end in 1565 when a hurricane destroyed the French fleet, allowing Spain to conquer Fort Caroline. More recently, Hurricane Katrina redistributed over one million people from the central Gulf coast elsewhere across the United States, becoming the largest diaspora in the history of the United States.\n\nThe Little Ice Age caused crop failures and famines in Europe. The 1690s saw the worst famine in France since the Middle Ages. Finland suffered a severe famine in 1696–1697, during which about one-third of the Finnish population died.\n\nWeather forecasting is the application of science and technology to predict the state of the atmosphere for a future time and a given location. Human beings have attempted to predict the weather informally for millennia, and formally since at least the nineteenth century. Weather forecasts are made by collecting quantitative data about the current state of the atmosphere and using scientific understanding of atmospheric processes to project how the atmosphere will evolve.\n\nOnce an all-human endeavor based mainly upon changes in barometric pressure, current weather conditions, and sky condition, forecast models are now used to determine future conditions. On the other hand, human input is still required to pick the best possible forecast model to base the forecast upon, which involve many disciplines such as pattern recognition skills, teleconnections, knowledge of model performance, and knowledge of model biases.\n\nThe chaotic nature of the atmosphere, the massive computational power required to solve the equations that describe the atmosphere, error involved in measuring the initial conditions, and an incomplete understanding of atmospheric processes mean that forecasts become less accurate as the difference in current time and the time for which the forecast is being made (the \"range\" of the forecast) increases. The use of ensembles and model consensus helps to narrow the error and pick the most likely outcome.\n\nThere are a variety of end users to weather forecasts. Weather warnings are important forecasts because they are used to protect life and property. Forecasts based on temperature and precipitation are important to agriculture, and therefore to commodity traders within stock markets. Temperature forecasts are used by utility companies to estimate demand over coming days.\n\nIn some areas, people use weather forecasts to determine what to wear on a given day. Since outdoor activities are severely curtailed by heavy rain, snow and the wind chill, forecasts can be used to plan activities around these events, and to plan ahead to survive through them.\n\nThe aspiration to control the weather is evident throughout human history: from ancient rituals intended to bring rain for crops to the U.S. Military Operation Popeye, an attempt to disrupt supply lines by lengthening the North Vietnamese monsoon. The most successful attempts at influencing weather involve cloud seeding; they include the fog- and low stratus dispersion techniques employed by major airports, techniques used to increase winter precipitation over mountains, and techniques to suppress hail. A recent example of weather control was China's preparation for the 2008 Summer Olympic Games. China shot 1,104 rain dispersal rockets from 21 sites in the city of Beijing in an effort to keep rain away from the opening ceremony of the games on 8 August 2008. Guo Hu, head of the Beijing Municipal Meteorological Bureau (BMB), confirmed the success of the operation with 100 millimeters falling in Baoding City of Hebei Province, to the southwest and Beijing's Fangshan District recording a rainfall of 25 millimeters.\n\nWhereas there is inconclusive evidence for these techniques' efficacy, there is extensive evidence that human activity such as agriculture and industry results in inadvertent weather modification:\n\nThe effects of inadvertent weather modification may pose serious threats to many aspects of civilization, including ecosystems, natural resources, food and fiber production, economic development, and human health.\n\nMicroscale meteorology is the study of short-lived atmospheric phenomena smaller than mesoscale, about 1 km or less. These two branches of meteorology are sometimes grouped together as \"mesoscale and microscale meteorology\" (MMM) and together study all phenomena smaller than synoptic scale; that is they study features generally too small to be depicted on a weather map. These include small and generally fleeting cloud \"puffs\" and other small cloud features.\n\nOn Earth, temperatures usually range ±40 °C (100 °F to −40 °F) annually. The range of climates and latitudes across the planet can offer extremes of temperature outside this range. The coldest air temperature ever recorded on Earth is , at Vostok Station, Antarctica on 21 July 1983. The hottest air temperature ever recorded was at 'Aziziya, Libya, on 13 September 1922, but that reading is queried. The highest recorded average annual temperature was at Dallol, Ethiopia. The coldest recorded average annual temperature was at Vostok Station, Antarctica.\n\nThe coldest average annual temperature in a permanently inhabited location is at Eureka, Nunavut, in Canada, where the annual average temperature is .\n\nStudying how the weather works on other planets has been seen as helpful in understanding how it works on Earth. Weather on other planets follows many of the same physical principles as weather on Earth, but occurs on different scales and in atmospheres having different chemical composition. The Cassini–Huygens mission to Titan discovered clouds formed from methane or ethane which deposit rain composed of liquid methane and other organic compounds. Earth's atmosphere includes six latitudinal circulation zones, three in each hemisphere. In contrast, Jupiter's banded appearance shows many such zones, Titan has a single jet stream near the 50th parallel north latitude, and Venus has a single jet near the equator.\n\nOne of the most famous landmarks in the Solar System, Jupiter's \"Great Red Spot\", is an anticyclonic storm known to have existed for at least 300 years. On other gas giants, the lack of a surface allows the wind to reach enormous speeds: gusts of up to 600 metres per second (about ) have been measured on the planet Neptune. This has created a puzzle for planetary scientists. The weather is ultimately created by solar energy and the amount of energy received by Neptune is only about of that received by Earth, yet the intensity of weather phenomena on Neptune is far greater than on Earth. The strongest planetary winds discovered so far are on the extrasolar planet HD 189733 b, which is thought to have easterly winds moving at more than .\n\nWeather is not limited to planetary bodies. Like all stars, the sun's corona is constantly being lost to space, creating what is essentially a very thin atmosphere throughout the Solar System. The movement of mass ejected from the Sun is known as the solar wind. Inconsistencies in this wind and larger events on the surface of the star, such as coronal mass ejections, form a system that has features analogous to conventional weather systems (such as pressure and wind) and is generally known as space weather. Coronal mass ejections have been tracked as far out in the solar system as Saturn. The activity of this system can affect planetary atmospheres and occasionally surfaces. The interaction of the solar wind with the terrestrial atmosphere can produce spectacular aurorae, and can play havoc with electrically sensitive systems such as electricity grids and radio signals.\n"}
