{"id": "46631115", "url": "https://en.wikipedia.org/wiki?curid=46631115", "title": "2009 BD", "text": "2009 BD\n\nDuring the 2011 opposition, the last opposition of that was observed, approached on June 2, 2011 within 0.00231 AU (346,000 km) of the Earth, which is less than 1 lunar distance. For comparison, the distance to the Moon is about 0.0026 AU (384,400 km).\n\nWith an orbital period of 369.99 days, is in a near 1:1 orbital resonance with Earth, and also has about the same orbit around the Sun as Earth. Other resonant near-Earth objects in addition to include (the first to be discovered), , , , , , , and (an Earth trojan).\n\nThe Jupiter Tisserand invariant, used to distinguish different kinds of orbits, is 6.039. The orbit has a small inclination of about 0.4 degrees.\n\nJPL and MPC give different parameters for the orbit of , affecting whether the orbit type should be considered an Apollo asteroid or an Amor asteroid. JPL includes non-gravitational acceleration parameters in the orbital solution.\n\nBecause is a very small multi-opposition near-Earth object, the effect of radiation pressure on the orbit caused by light from the Sun was able to be detected. The radiation-related acceleration allowed the Area to Mass Ratio (AMR) to be estimated at (2.97 ± 0.33) × 10 m/kg. Assuming an albedo of 0.12, a typical average for asteroids in the inner solar system, this AMR corresponds to a density of about 640 kg/m. This density is consistent with the density of very porous rock. For comparison, the asteroid 2006 RH120 has a measured density of about 400 kg/m, and the density of the asteroid 253 Mathilde as measured by the NEAR-Shoemaker space probe was 1300 kg/m. In contrast, the density of the man-made near-Earth object 6Q0B44E is 15 kg/m.\n"}
{"id": "53191245", "url": "https://en.wikipedia.org/wiki?curid=53191245", "title": "A Cancri", "text": "A Cancri\n\nThe Bayer designation A Cancri is shared by two stars/star systems in the constellation Cancer:\n\n\nα Cancri\n"}
{"id": "25313057", "url": "https://en.wikipedia.org/wiki?curid=25313057", "title": "Albany thickets", "text": "Albany thickets\n\nThe Albany thickets is an ecoregion of dense woodland in southern South Africa, which is concentrated around the Albany region of the Eastern Cape (whence the region's name originates).\n\nThe thickets grow on well-drained sandy soils in the wide valleys of the Great Fish, Sundays and Gamtoos River in the Eastern Cape and, extending further northwest, in the valleys of the Cape Fold Belt. Thicket is vulnerable to fire and to grazing so has always been restricted to valley areas where these are less of a threat than on open plains.\n\nThe climate is dry, especially as one proceeds inland, but the shady valleys are cooler than the surrounding terrain which is hot in summer, cold in winter and receives irregular rainfall.\n\nThe thickets contain many endemic plants, in particular succulent Euphorbia species and can be divided into three sections of varying habitat. The thicket is richest and most dense in the river valleys near the coast where it contains thorny shrubs with an undergrowth of creepers and succulent plants. As the river valleys climb inland and upstream the climate is drier and the vegetation less dense. Finally the shrubland in mountain valleys to the northwest consists of predominantly the porkbush (\"Portulacaria afra\") and jade plant (\"Crassula ovata\") succulents, along with boxthorn (\"Lycium austrinum\"), jacketplum (\"Pappea capensis\"), \"Euclea undulata\", \"Rhigozum obovatum\", aloes and \"Schotia afra\". Along with the fynbos ecoregions the Albany thickets comprise the Cape Floristic Region.\n\nBirds in this area include black goshawk, black-headed oriole and two species which are almost endemic to the Cape area, the orange-breasted sunbird and Cape siskin. There is one near-endemic mammal Duthie's golden mole (\"Chlorotalpat duthieae\") and in the inland valleys, Addo Elephant National Park is home to elephant (\"Loxodonta africana\"), black rhinoceros (\"Diceros bicornis\") and antelopes such as bushbuck (\"Tragelaphus scriptus\"), grey rhebok (\"Pelea capreolus\"), mountain reedbuck (\"Redunca fulvorufula\"), common eland (\"Taurotragus oryx\"), greater kudu (\"Tragelaphus strepsiceros\"), red hartebeest (\"Alcelaphus buselaphus\"), Cape grysbok (\"Raphicerus melanotis\") and common duiker (\"Sylvicapra grimmia\").\n\nA large part of the region has been converted for agriculture or reduced by grazing, especially by goats. This is a continuous threat especially in the river valleys near the coast, which are also vulnerable to clearance for urban areas and tourist resorts. Protected areas include Addo Elephant National Park near Port Elizabeth, the Groendal Wilderness Area near Uitenhage on the Swartkops River, and the Baviaanskloof Mega Reserve.\n\n"}
{"id": "19278451", "url": "https://en.wikipedia.org/wiki?curid=19278451", "title": "Bolus (radiation therapy)", "text": "Bolus (radiation therapy)\n\nIn radiation therapy, bolus is a material which has properties equivalent to tissue when irradiated. It is widely used in practice to reduce or alter dosing for targeted radiation therapy.\n\nIt must be possible to mould the bolus to fill the tissue space. Lincolnshire and Spier's bolus, which is loosely packed in polyethylene bags, is suitable as the bolus bags take the shape of the skin surface these bags are easily smoothed to achieve a flat surface.\n\nA specific thickness of bolus can be applied to the skin to alter the dose received at depth in the tissue and on the skin surface. A typical example of this is the application of a defined thickness of bolus to a chest wall for post-mastectomy chest wall treatment, to increase the skin dose. The thickness of bolus applied is dependent on the skin dose required and the angle of incidence of the treatment beams. For example if oblique 6 MV beams are used for tangential pair, 1 cm of bolus effectively becomes 1.5 cm, i.e., 'full bolus'.\n\nWhen a full bolus is applied, bolus thickness equal to the depth of the build-up region removes the skin-sparing effect of a megavoltage x-ray beam.\n\nSuitable material must be pliable and easily moulded to the skin surface, but retain a constant thickness. One example includes paraffin gauze.\n\nFor smaller areas which do not require the bolus to be moulded over the skin, Perspex can be used. The use of Perspex bolus is advantageous for electron set-ups because it is transparent. Since the f.s.d, for most electron fields is 95 cm, so that the movements of the couch are not isocentric, inaccuracies may arise for aligning angled fields when opaque bolus is inserted.\n\nTo ensure that the patient receives the required dose, bolus of the right thickness must be placed correctly. Therefore bolus requirements must be clearly documented in the setup sheets of the treatment card. When using bolus to compensate for missing tissue, the whole of the bolussed area must be level with the point on the patient where the f.s.d. is set, to ensure dose homogeneity.\n\nWhen the bolus is used to reduce the skin-sparing effect, the bolus does not necessarily need to touch the skin all over the bolussed area as the scatter is of sufficiently high energy to be unaffected by an air gap. However, it is important that the bolus is uniform thickness. Some bolus materials are easily squashed and must be carefully measured at regular intervals.\n\n"}
{"id": "255297", "url": "https://en.wikipedia.org/wiki?curid=255297", "title": "Coronal mass ejection", "text": "Coronal mass ejection\n\nA coronal mass ejection (CME) is a significant release of plasma and accompanying magnetic field from the solar corona. They often follow solar flares and are normally present during a solar prominence eruption. The plasma is released into the solar wind, and can be observed in coronagraph imagery.\n\nCoronal mass ejections are often associated with other forms of solar activity, but a broadly accepted theoretical understanding of these relationships has not been established. CMEs most often originate from active regions on the Sun's surface, such as groupings of sunspots associated with frequent flares. Near solar maxima, the Sun produces about three CMEs every day, whereas near solar minima, there is about one CME every five days.\n\nCoronal mass ejections release large quantities of matter and electromagnetic radiation into space above the Sun's surface, either near the corona (sometimes called a solar prominence), or farther into the planetary system, or beyond (interplanetary CME). The ejected material is a magnetized plasma consisting primarily of electrons and protons. While solar flares are very fast (being electromagnetic radiation), CMEs are relatively slow.\n\nCoronal mass ejections are associated with enormous changes and disturbances in the coronal magnetic field. They are usually observed with a white-light coronagraph.\n\nScientific research has shown that the phenomenon of magnetic reconnection is closely associated with CMEs and solar flares. In magnetohydrodynamic theory, the sudden rearrangement of magnetic field lines when two oppositely directed magnetic fields are brought together is called \"magnetic reconnection\". Reconnection releases energy stored in the original stressed magnetic fields. These magnetic field lines can become twisted in a helical structure, with a 'right-hand twist' or a 'left hand twist'. As the Sun's magnetic field lines become more and more twisted, CMEs appear to be a 'valve' to release the magnetic energy being built up, as evidenced by the helical structure of CMEs, that would otherwise renew itself continuously each solar cycle and eventually rip the Sun apart.\n\nOn the Sun, magnetic reconnection may happen on solar arcades—a series of closely occurring loops of magnetic lines of force. These lines of force quickly reconnect into a low arcade of loops, leaving a helix of magnetic field unconnected to the rest of the arcade. The sudden release of energy during this process causes the solar flare and ejects the CME. The helical magnetic field and the material that it contains may violently expand outwards forming a CME. This also explains why CMEs and solar flares typically erupt from what are known as the active regions on the Sun where magnetic fields are much stronger on average.\n\nWhen the ejection is directed towards Earth and reaches it as an interplanetary CME (ICME), the shock wave of traveling mass causes a geomagnetic storm that may disrupt Earth's magnetosphere, compressing it on the day side and extending the night-side magnetic tail. When the magnetosphere reconnects on the nightside, it releases power on the order of terawatt scale, which is directed back toward Earth's upper atmosphere.\n\nSolar energetic particles can cause particularly strong aurorae in large regions around Earth's magnetic poles. These are also known as the \"Northern Lights\" (aurora borealis) in the northern hemisphere, and the \"Southern Lights\" (aurora australis) in the southern hemisphere. Coronal mass ejections, along with solar flares of other origin, can disrupt radio transmissions and cause damage to satellites and electrical transmission line facilities, resulting in potentially massive and long-lasting power outages.\n\nEnergetic protons released by a CME can cause an increase in the number of free electrons in the ionosphere, especially in the high-latitude polar regions. The increase in free electrons can enhance radio wave absorption, especially within the D-region of the ionosphere, leading to Polar Cap Absorption (PCA) events.\n\nHumans at high altitudes, as in airplanes or space stations, risk exposure to relatively intense solar particle events. The energy absorbed by astronauts is not reduced by a typical spacecraft shield design and, if any protection is provided, it would result from changes in the microscopic inhomogeneity of the energy absorption events.\n\nA typical coronal mass ejection may have any or all of three distinctive features: a cavity of low electron density, a dense core (the prominence, which appears on coronagraph images as a bright region embedded in this cavity), and a bright leading edge.\n\nMost ejections originate from active regions on the Sun's surface, such as groupings of sunspots associated with frequent flares. These regions have closed magnetic field lines, in which the magnetic field strength is large enough to contain the plasma. These field lines must be broken or weakened for the ejection to escape from the Sun. However, CMEs may also be initiated in quiet surface regions, although in many cases the quiet region was recently active. During solar minimum, CMEs form primarily in the coronal streamer belt near the solar magnetic equator. During solar maximum, they originate from active regions whose latitudinal distribution is more homogeneous.\n\nCoronal mass ejections reach velocities from with an average speed of , based on SOHO/LASCO measurements between 1996 and 2003. These speeds correspond to transit times from the Sun out to the mean radius of Earth's orbit of about 13 hours to 86 days (extremes), with about 3.5 days as the average. The average mass ejected is . However, the estimated mass values for CMEs are only lower limits, because coronagraph measurements provide only two-dimensional data. The frequency of ejections depends on the phase of the solar cycle: from about one every fifth day near the solar minimum to 3.5 per day near the solar maximum. These values are also lower limits because ejections propagating away from Earth (backside CMEs) usually cannot be detected by coronagraphs.\n\nCurrent knowledge of coronal mass ejection kinematics indicates that the ejection starts with an initial pre-acceleration phase characterized by a slow rising motion, followed by a period of rapid acceleration away from the Sun until a near-constant velocity is reached. Some \"balloon\" CMEs, usually the slowest ones, lack this three-stage evolution, instead accelerating slowly and continuously throughout their flight. Even for CMEs with a well-defined acceleration stage, the pre-acceleration stage is often absent, or perhaps unobservable.\n\nCoronal mass ejections are often associated with other forms of solar activity, most notably:\nThe association of a CME with some of those phenomena is common but not fully understood. For example, CMEs and flares are normally closely related, but there was confusion about this point caused by the events originating beyond the limb. For such events no flare could be detected. Most weak flares do not have associated CMEs; most powerful ones do. Some CMEs occur without any flare-like manifestation, but these are the weaker and slower ones. It is now thought that CMEs and associated flares are caused by a common event (the CME peak acceleration and the flare impulsive phase generally coincide). In general, all of these events (including the CME) are thought to be the result of a large-scale restructuring of the magnetic field; the presence or absence of a CME during one of these restructures would reflect the coronal environment of the process (i.e., can the eruption be confined by overlying magnetic structure, or will it simply break through and enter the solar wind).\n\nIt was first postulated that CMEs might be driven by the heat of an explosive flare. However, it soon became apparent that many CMEs were not associated with flares, and that even those that were often started before the flare. Because CMEs are initiated in the solar corona (which is dominated by magnetic energy), their energy source must be magnetic.\n\nBecause the energy of CMEs is so high, it is unlikely that their energy could be directly driven by emerging magnetic fields in the photosphere (although this is still a possibility). Therefore, most models of CMEs assume that the energy is stored up in the coronal magnetic field over a long period of time and then suddenly released by some instability or a loss of equilibrium in the field. There is still no consensus on which of these release mechanisms is correct, and observations are not currently able to constrain these models very well. These same considerations apply equally well to solar flares, but the observable signatures of these phenomena differ.\n\nCMEs typically reach Earth one to five days after leaving the Sun. During their propagation, CMEs interact with the solar wind and the interplanetary magnetic field (IMF). As a consequence, slow CMEs are accelerated toward the speed of the solar wind and fast CMEs are decelerated toward the speed of the solar wind. The strongest deceleration or acceleration occurs close to the Sun, but it can continue even beyond Earth orbit (1 AU), which was observed using measurements at Mars and by the Ulysses spacecraft. CMEs faster than about eventually drive a shock wave. This happens when the speed of the CME in the frame of reference moving with the solar wind is faster than the local fast magnetosonic speed. Such shocks have been observed directly by coronagraphs in the corona, and are related to type II radio bursts. They are thought to form sometimes as low as 2 R (solar radii). They are also closely linked with the acceleration of solar energetic particles.\n\nOn 1 November 1994, NASA launched the \"Wind\" spacecraft as a solar wind monitor to orbit Earth's Lagrange point as the interplanetary component of the Global Geospace Science (GGS) Program within the International Solar Terrestrial Physics (ISTP) program. The spacecraft is a spin axis-stabilized satellite that carries eight instruments measuring solar wind particles from thermal to >MeV energies, electromagnetic radiation from DC to 13 MHz radio waves, and gamma-rays. Though the \"Wind\" spacecraft is over two decades old, it still provides the highest time, angular, and energy resolution of any of the solar wind monitors. It continues to produce relevant research as its data has contributed to over 150 publications since 2008 alone.\n\nOn 25 October 2006, NASA launched STEREO, two near-identical spacecraft which, from widely separated points in their orbits, are able to produce the first stereoscopic images of CMEs and other solar activity measurements. The spacecraft orbit the Sun at distances similar to that of Earth, with one slightly ahead of Earth and the other trailing. Their separation gradually increased so that after four years they were almost diametrically opposite each other in orbit.\n\nThe \"Parker Solar Probe\" was launched on 12 August 2018 to measure the mechanisms which accelerate and transport energetic particles i.e. the origins of the solar wind.\n\nThe largest recorded geomagnetic perturbation, resulting presumably from a CME, coincided with the first-observed solar flare on 1 September 1859. The resulting solar storm of 1859 is now referred to as the Carrington Event, The flare and the associated sunspots were visible to the naked eye (both as the flare itself appearing on a projection of the Sun on a screen and as an aggregate brightening of the solar disc), and the flare was independently observed by English astronomers R. C. Carrington and R. Hodgson. The geomagnetic storm was observed with the recording magnetograph at Kew Gardens. The same instrument recorded a \"crochet\", an instantaneous perturbation of Earth's ionosphere by ionizing soft X-rays. This could not easily be understood at the time because it predated the discovery of X-rays by Röntgen and the recognition of the ionosphere by Kennelly and Heaviside. The storm took down parts of the recently created US telegraph network, starting fires and shocking some telegraph operators.\n\nHistorical records were collected and new observations recorded in annual summaries by the Astronomical Society of the Pacific between 1953 and 1960.\n\nThe first detection of a CME as such was made on 14 December 1971, by R. Tousey (1973) of the Naval Research Laboratory using the seventh Orbiting Solar Observatory (OSO-7). The discovery image (256 × 256 pixels) was collected on a Secondary Electron Conduction (SEC) vidicon tube, transferred to the instrument computer after being digitized to 7 bits. Then it was compressed using a simple run-length encoding scheme and sent down to the ground at 200 bit/s. A full, uncompressed image would take 44 minutes to send down to the ground. The telemetry was sent to ground support equipment (GSE) which built up the image onto Polaroid print. David Roberts, an electronics technician working for NRL who had been responsible for the testing of the SEC-vidicon camera, was in charge of day-to-day operations. He thought that his camera had failed because certain areas of the image were much brighter than normal. But on the next image the bright area had moved away from the Sun and he immediately recognized this as being unusual and took it to his supervisor, Dr. Guenter Brueckner, and then to the solar physics branch head, Dr. Tousey. Earlier observations of \"coronal transients\" or even phenomena observed visually during solar eclipses are now understood as essentially the same thing.\n\nOn 9 March 1989 a coronal mass ejection occurred. On 13 March 1989 a severe geomagnetic storm struck the Earth. It caused power failures in Quebec, Canada and short-wave radio interference.\n\nOn 1 August 2010, during solar cycle 24, scientists at the Harvard–Smithsonian Center for Astrophysics (CfA) observed a series of four large CMEs emanating from the Earth-facing hemisphere of the Sun. The initial CME was generated by an eruption on 1 August that was associated with NOAA Active Region 1092, which was large enough to be seen without the aid of a solar telescope. The event produced significant aurorae on Earth three days later.\n\nOn 23 July 2012, a massive, and potentially damaging, solar superstorm (solar flare, CME, solar EMP) occurred but missed Earth, an event that many scientists consider to be Carrington-class event.\n\nOn 31 August 2012 a CME connected with Earth's magnetic environment, or magnetosphere, with a glancing blow causing aurora to appear on the night of 3 September. Geomagnetic storming reached the G2 (Kp=6) level on NOAA's Space Weather Prediction Center scale of geomagnetic disturbances.\n\n14 October 2014 ICME was photographed by the Sun-watching spacecraft PROBA2 (ESA), Solar and Heliospheric Observatory (ESA/NASA), and Solar Dynamics Observatory (NASA) as it left the Sun, and STEREO-A observed its effects directly at . ESA's \"Venus Express\" gathered data. The CME reached Mars on 17 October and was observed by the \"Mars Express\", MAVEN, \"Mars Odyssey\", and Mars Science Laboratory missions. On 22 October, at , it reached comet 67P/Churyumov–Gerasimenko, perfectly aligned with the Sun and Mars, and was observed by \"Rosetta\". On 12 November, at , it was observed by \"Cassini\" at Saturn. The \"New Horizons\" spacecraft was at approaching Pluto when the CME passed three months after the initial eruption, and it may be detectable in the data. \"Voyager 2\" has data that can be interpreted as the passing of the CME, 17 months after. The \"Curiosity\" rover's RAD instrument, \"Mars Odyssey\", \"Rosetta\" and \"Cassini\" showed a sudden decrease in galactic cosmic rays (Forbush decrease) as the CME's protective bubble passed by.\n\nAccording to a report published in 2012 by physicist Pete Riley of Predictive Science Inc., the chance of Earth being hit by a Carrington-class storm between 2012 and 2022 is 12%.\n\nThere have been a small number of CMEs observed on other stars, all of which have been found on M dwarfs. These have been detected by spectroscopy, most often by studying Balmer lines: the material ejected toward the observer causes asymmetry in the blue wing of the line profiles due to Doppler shift. This enhancement can be seen in absorption when it occurs on the stellar disc (the material is cooler than its surrounding), and in emission when it is outside the disc. The observed projected velocities of CMEs range from ≈. Compared to activity on the Sun, CME activity on other stars seems to be far less common.\n\n\n\n\n"}
{"id": "82854", "url": "https://en.wikipedia.org/wiki?curid=82854", "title": "Crinaeae", "text": "Crinaeae\n\nIn Greek mythology, the Crinaeae (; Ancient Greek: \"Κρηναῖαι\") were a type of Naiad nymphs associated with fountains or wells.\n\nThe number of Crinaeae includes but is not limited to:\n\n\n\n"}
{"id": "1530455", "url": "https://en.wikipedia.org/wiki?curid=1530455", "title": "Donar's Oak", "text": "Donar's Oak\n\nJove's Oak (\"interpretatio romana\" for Donar's Oak and therefore sometimes referred to as Thor's Oak) was a sacred tree of the Germanic pagans located in an unclear location around what is now the region of Hesse, Germany. According to the 8th century \"Vita Bonifatii auctore Willibaldi\", the Anglo-Saxon missionary Saint Boniface and his retinue cut down the tree earlier the same century. Wood from the oak was then reportedly used to build a church at the site dedicated to Saint Peter. Sacred trees and sacred groves were widely venerated by the Germanic peoples.\n\nAccording to Willibald's 8th century \"Life of Saint Boniface\", the felling of the tree occurred during Boniface's life earlier the same century at a location at the time known as \"Gaesmere\" (for details, see discussion below).\n\nAlthough no date is provided, the felling may have occurred around 723 or 724. Willibald's account is as follows (note that Robinson has translated \"robor Iobis\", \"tree of Jove\", as \"oak of Jupiter\"):\n\nSacred groves and sacred trees were venerated throughout the history of the Germanic peoples and were targeted for destruction by Christian missionaries during the Christianization of the Germanic peoples. Ken Dowden notes that behind this great oak dedicated to Donar, the Irminsul (also felled by Christian missionaries in the 8th century), and the Sacred tree at Uppsala (described by Adam of Bremen in the 11th century), stands a mythic prototype of an immense world tree, described in Norse mythology as Yggdrasil.\n\nIn the nineteenth century already \"Gaesmere\" was identified as in the Schwalm-Eder district, for instance by August Neander. There are a few dissenting voices: in his 1916 translation of Willibald's \"Vita Bonifacii\", George W. Robinson says \"The location [of the tree] is uncertain. There are in Hesse several places named Geismar.\" Historian Thomas F. X. Noble (2000) describes the location of the tree felling as \"still unidentified\". In the late 19th century, folklorist and philologist Francis Barton Gummere identifies the \"Gaesemere\" of the attestation as Geismar, a district of Frankenberg located in Hesse.\n\nHowever, most scholars agree that the site mentioned by Wilibald is Geismar near Fritzlar. In 1897 historian C. Neuber placed the Donar Oak \"im Kreise Fritzlar\". While Gregor Richter, in 1906, noted that one scholar considered Hofgeismar as a possible location, he himself comments that most people consider Geismar near Fritzlar as the right place. Unequivocal identification of Geismar near Fritzlar as the location of the Donar Oak is found in the Catholic Encyclopedia, in teaching materials for religious studies classes in Germany, in the work of Alexander Demandt, in histories of the Carolingians, and in the work of Lutz von Padberg. The Reallexikon der Germanischen Altertumskunde notes that for Willibald it was probably not necessary to specify the location any further because he presumed it widely known. This Geismar was close to Büraburg, then a hill castle and a Frankish stronghold.\n\nOne of the focal points of Boniface's life, the scene is frequently repeated, illustrated, and reimagined. Roberto Muller, for instance, in a retelling of Boniface's biography for young adults, has the four parts of the tree fall down to the ground and form a cross. In Hubertus Lutterbach's fictional expansion of the Boniface correspondence, Boniface relates the entire event in a long letter to Pope Gregory II, commenting that it took hours to cut the tree down, and that any account that says the tree fell down miraculously is a falsification of history.\n\n\n"}
{"id": "2809025", "url": "https://en.wikipedia.org/wiki?curid=2809025", "title": "Each-uisge", "text": "Each-uisge\n\nThe each-uisge (, literally \"water horse\") is a water spirit in Scottish folklore, known as the each-uisce (anglicized as \"aughisky\" or \"ech-ushkya\") in Ireland and cabyll-ushtey on the Isle of Man. It usually takes the form of a horse, and is similar to the kelpie but far more vicious.\n\nThe each-uisge, a supernatural water horse found in the Scottish Highlands, has been described as \"perhaps the fiercest and most dangerous of all the water-horses\" by the folklorist Katharine Briggs. Often mistaken for the kelpie (which inhabits streams and rivers), the each-uisge lives in the sea, sea lochs, and fresh water lochs. The each-uisge is a shape-shifter, disguising itself as a fine horse, pony, a handsome man or an enormous bird such as a boobrie. If, while in horse form, a man mounts it, he is only safe as long as the each-uisge is ridden in the interior of land. However, the merest glimpse or smell of water means the end of the rider, for the each-uisge's skin becomes adhesive and the creature immediately goes to the deepest part of the loch with its victim. After the victim has drowned, the each-uisge tears him apart and devours the entire body except for the liver, which floats to the surface.\n\nIn its human form it is said to appear as a handsome man, and can be recognised as a mythological creature only by the water weeds or profuse sand and mud in its hair. Because of this, people in the Highlands were often wary of lone animals and strangers by the water's edge, near where the each-uisge was reputed to live.\n\nCnoc-na-Bèist (\"Hillock of the Monster\") is the name of a knoll on the Isle of Lewis where an each-uisge was slain by the brother of a woman it tried to seduce, by the freshwater Loch a’ Mhuileinn (\"Loch of the Mill\").\n\nAlong with its human victims, cattle and sheep were also often prey to the each-uisge, and it could be lured out of the water by the smell of roasted meat. One story from John McKay's \"More West Highland Tales\" runs thus:\nThe Scottish folklorist John Gregorson Campbell recorded numerous tales and traditions concerning the each-uisge. In one account a man who was about to be carried by the water horse into the loch was able to save himself by placing both feet on either side of a narrow gateway the horse was running through, thereby wrenching himself off its back through sheer force. A boy who had touched the horse with his finger and gotten stuck was able to save himself by cutting it off. A Highland freebooter encountered a water horse in its human form and fired his gun at it twice with no effect, but when he loaded it with a coin made of silver and fired again the man retreated and plunged back into the loch. The each-uisge is unpredictable. It has been known to venture forth on land and attack solitary individuals, while in other accounts it will allow itself to be used as farm labour until its owner gets on its back and is carried into the loch. In their predatory hunger water horses may even turn on their own kind if the scent of a previous human rider is strong enough on the monster's body.\n\nThe each-uisge also has a particular desire for human women. Campbell states that \"any woman upon whom it set its mark was certain at last to become its victim.\" A young woman herding cattle encountered a water horse in the form of a handsome young man who laid his head in her lap and fell asleep. When he stretched himself she discovered that he had horse's hooves and quietly made her escape (in variations of the tale she finds the presence of water weeds or sand in his hair). In another account a water horse in human shape came to a woman's house where she was alone and attempted to court her, but all he got for his unwanted advances was boiling water hurled between his legs. He ran from the house roaring in pain. In a third tale a father and his three sons conspired to kill a water horse that came to the house to see the daughter. When they grabbed the young man he reverted to his equine form and would have carried them into the loch, but in the struggle they managed to slay him with their dirks. Despite its amorous tendencies, however, the each-uisge is just as likely to simply devour women in the same manner as its male victims.\n\nThe aughisky or Irish water horse is similar in many respects to the Scottish version. It sometimes comes out of the water to gallop on land and, despite the danger, if the aughisky can be caught and tamed then it will make the finest of steeds provided it is not allowed to glimpse the ocean.\n\nThe cabyll-ushtey (or cabbyl-ushtey), the Manx water horse, is just as ravenous as the each-uisge though there are not as many tales told about it. One of them recounts how a cabbyl-ushtey emerged from the Awin Dhoo (Black River) and devoured a farmer's cow, then later it took his teenaged daughter.\n\nThe appearance of the each-uisge on the Isle of Skye was described by Gordon in 1995 as having a parrot-like beak, and this, with its habit of diving suddenly, could be from real-life encounters with a sea turtle such as the leatherback sea turtle.\n"}
{"id": "9806397", "url": "https://en.wikipedia.org/wiki?curid=9806397", "title": "Food contact materials", "text": "Food contact materials\n\nFood contact materials are materials that are intended to be in contact with food. These can be things that are quite obvious like a glass, a can for soft drinks, but also machinery in a food factory or a coffee machine.\n\nFood contact materials can be constructed from a variety of materials like plastics, rubber, paper, coatings, metal etc. In many cases a combination is used; for example a carton box for juices can include (from the inside to the outside): plastic layer, aluminium, paper, printing and top coating.\n\nDuring contact with the food, molecules can migrate from the food contact material to the food, for example via blooming. Because of this, in many countries regulations are made to ensure food safety.\n\nThe international symbol for \"food safe\" material is a wine glass and a fork symbol. The symbol indicates that the material used in the product is considered safe for food contact. This includes food and water containers, packaging materials, cutlery etc. The regulation is applicable to any product intended for food contact whether it be made of metals, ceramics, paper and board, and plastics. Use of the symbol is more significant in products which should be explicitly identified whether food safe or not, i.e. wherever there is an ambuiguity whether the container could be used to hold food stuff. The symbol is used in North America, Europe and parts of Asia. It is mandatory for products sold in Europe after the Framework Regulation EC 1935/2004.\n\nIn plastic containers, over and above the prescribed resin identification codes (viz; , , , ), the food safe assurance is required because the resin identification codes do not explicitly communicate the food safe property (or more significantly, the lack of it). \n\nEven though the legal requirement in various nations would be different, the food safe symbol generally assures that:\n\nThe framework Regulation (EC) No. 1935/2004 applies to all food contact materials. Article 3 contains general safety requirements such as not endanger human health, no unacceptable change in the composition and no deterioration of the organoleptic characteristics. Article 4 set out special requirements for active and intelligent materials. Article 5 specifies measures for groups of materials that may be detailed in separate regulations of directives. Member States may maintain or adopt national provisions (Article 6). Articles 7-14 and 22-23 deal with the requirements and application for authorisation of a substance, modification of authorisation, the role of the European Food Safety Authority, the Member States and the Community. Article 15 is about labeling of food contact materials not yet in contact with food. Article 16 requires a declaration of compliance and appropriate documentation that demonstrate compliance. Articles 17-21 deal with traceability, safeguard measures, public access of applications, confidentiality, and data sharing. Article 24 sets out the inspection and control measures.\n\nSpecific measures for materials and articles such as ceramics, regenerated cellulose, plastics, gaskets and active and intelligent materials, and substances such as vinyl chloride, N-nitrosamines and N-nitrostable substances in rubber, and epoxy derivatives, exist.\n\nEU No 10/2011 is applicable regulation for all the Food contact material or Article made up of Plastics.\n\nThe U.S. Food and Drug Administration (FDA) considers three different types of food additives:\n\nThe food contact materials are described in the Code of Federal Legislation (CFR): 21 CFR 174 - 21 CFR 190. Important starting points are:\n\nTo this materials additives may be added. Which additives depend on the additive and on the material in which it is intended to be used. There must be a reference to the paragraph in which the additive is mentioned and the restrictions (for example only to be used in polyolefines) and limitations (max 0.5% in the final product) must be respected. See below for part in which additives are described: 21 CFR 170 Food additives\n\nPolymers or additives can also be regulated in other ways with exemptions; for example:\n\n\n"}
{"id": "55017", "url": "https://en.wikipedia.org/wiki?curid=55017", "title": "Fusion power", "text": "Fusion power\n\nFusion power is a theoretical form of power generation in which energy will be generated by using nuclear fusion reactions to produce heat for electricity generation. In a fusion process, two lighter atomic nuclei combine to form a heavier nucleus, and at the same time, they release energy. This is the same process that powers stars like our Sun. Devices designed to harness this energy are known as fusion reactors.\n\nFusion processes require fuel and a highly confined environment with a high temperature and pressure, to create a plasma in which fusion can occur. In stars, the most common fuel is hydrogen, and gravity creates the high temperature and confinement needed for fusion. Fusion reactors generally use hydrogen isotopes such as deuterium and tritium, which react more easily, and create a confined plasma of millions of degrees using inertial methods (laser) or magnetic methods (tokamak and similar), although many other concepts have been attempted. The major challenges in realising fusion power are to engineer a system that can confine the plasma long enough at high enough temperature and density, for a long term reaction to occur, and for the most common reactions, managing neutrons that are released during the reaction, which over time can degrade many common materials used within the reaction chamber.\n\nAs a source of power, nuclear fusion is expected to have several theoretical advantages over fission. These include reduced radioactivity in operation and little nuclear waste, ample fuel supplies, and increased safety. However, controlled fusion has proven to be extremely difficult to produce in a practical and economical manner. Research into fusion reactors began in the 1940s, but to date, no design has produced more fusion power output than the electrical power input; therefore, all existing designs have had a negative power balance.\n\nOver the years, fusion researchers have investigated various confinement concepts. The early emphasis was on three main systems: z-pinch, stellarator and magnetic mirror. The current leading designs are the tokamak and inertial confinement (ICF) by laser. Both designs are being built at very large scales, most notably the ITER tokamak in France, and the National Ignition Facility laser in the United States. Researchers are also studying other designs that may offer cheaper approaches. Among these alternatives there is increasing interest in magnetized target fusion and inertial electrostatic confinement.\n\nFusion reactions occur when two or more atomic nuclei come close enough for long enough that the nuclear force pulling them together exceeds the electrostatic force pushing them apart, fusing them into heavier nuclei. For nuclei lighter than iron-56, the reaction is exothermic, releasing energy. For nuclei heavier than iron-56, the reaction is endothermic, requiring an external source of energy. Hence, nuclei smaller than iron-56 are more likely to fuse while those heavier than iron-56 are more likely to break apart.\n\nThe strong force acts only over short distances. The repulsive electrostatic force acts over longer distances. In order to undergo fusion, the fuel atoms need to be given enough energy to approach each other close enough for the strong force to become active. The amount of kinetic energy needed to bring the fuel atoms close enough is known as the \"Coulomb barrier\". Ways of providing this energy include speeding up atoms in a particle accelerator, or heating them to high temperatures.\n\nOnce an atom is heated above its ionization energy, its electrons are stripped away (it is ionized), leaving just the bare nucleus (the ion). The result is a hot cloud of ions and the electrons formerly attached to them. This cloud is known as plasma. Because the charges are separated, plasmas are electrically conductive and magnetically controllable. Many fusion devices take advantage of this to control the particles as they are heated.\n\nA reaction's cross section, denoted σ, is the measure of the probability that a fusion reaction will happen. This depends on the relative velocity of the two nuclei. Higher relative velocities generally increase the probability, but the probability begins to decrease again at very high energies. Cross sections for many fusion reactions were measured (mainly in the 1970s) using particle beams.\n\nIn a plasma, particle velocity can be characterized using a probability distribution. If the plasma is thermalized, the distribution looks like a bell curve, or maxwellian distribution. In this case, it is useful to use the average particle cross section over the velocity distribution. This is entered into the volumetric fusion rate:\n\nwhere:\n\nThe Lawson Criterion shows how energy output varies with temperature, density, speed of collision, and fuel. This equation was central to John Lawson's analysis of fusion working with a hot plasma. Lawson assumed an energy balance, shown below.\n\n\nPlasma clouds lose energy through conduction and radiation. Conduction occurs when ions, electrons or neutrals impact other substances, typically a surface of the device, and transfer a portion of their kinetic energy to the other atoms. Radiation is energy that leaves the cloud as light in the visible, UV, IR, or X-ray spectra. Radiation increases with temperature. Fusion power technologies must overcome these losses.\n\nThe Lawson criterion argues that a machine holding a thermalized and quasi-neutral plasma has to meet basic criteria to overcome radiation losses, conduction losses and reach efficiency of 30 percent. This became known as the \"triple product\": the plasma density, temperature and confinement time. Attempts to increase the triple product led to targeting larger plants. Larger plants move structural materials further away from the centre of the plasma, which reduces conduction and radiation losses since more of the radiation is internally reflected. This emphasis on formula_10 as a metric of success has impacted other considerations such as cost, size, complexity and efficiency. This has led to larger, more complicated and more expensive machines such as ITER and NIF.\n\nPlasma is an ionized gas that conducts electricity. In bulk, it is modeled using magnetohydrodynamics, which is a combination of the Navier-Stokes equations governing fluids and Maxwell's equations governing how magnetic and electric fields behave. Fusion exploits several plasma properties, including:\n\n\nMultiple approaches have been proposed for energy capture. The simplest is to heat a fluid. Most designs concentrate on the D-T reaction, which releases much of its energy in a neutron. Electrically neutral, the neutron escapes the confinement. In most such designs, it is ultimately captured in a thick \"blanket\" of lithium surrounding the reactor core. When struck by a high-energy neutron, the lithium can produce tritium, which is then fed back into the reactor. The energy of this reaction also heats the blanket, which is then actively cooled with a working fluid and then that fluid is used to drive conventional turbomachinery.\n\nIt has also been proposed to use the neutrons to breed additional fission fuel in a blanket of nuclear waste, a concept known as a fission-fusion hybrid. In these systems, the power output is enhanced by the fission events, and power is extracted using systems like those in conventional fission reactors.\n\nDesigns that use other fuels, notably the p-B reaction, release much more of their energy in the form of charged particles. In these cases, alternate power extraction systems based on the movement of these charges are possible. Direct energy conversion was developed at LLNL in the 1980s as a method to maintain a voltage using the fusion reaction products. This has demonstrated energy capture efficiency of 48 percent.\n\n\n\n\n\n\nGas is heated to form a plasma hot enough to start fusion reactions. A number of heating schemes have been explored:\n\"Radiofrequency Heating\" A radio wave is applied to the plasma, causing it to oscillate. This is basically the same concept as a microwave oven. This is also known as electron cyclotron resonance heating or Dielectric heating.\n\n\"Electrostatic Heating\" An electric field can do work on charged ions or electrons, heating them.\n\n\"Neutral Beam Injection\" An external source of hydrogen is ionized and accelerated by an electric field to form a charged beam which is shone through a source of neutral hydrogen gas towards the plasma which itself is ionized and contained in the reactor by a magnetic field. Some of the intermediate hydrogen gas is accelerated towards the plasma by collisions with the charged beam while remaining neutral: this neutral beam is thus unaffected by the magnetic field and so shines through it into the plasma. Once inside the plasma the neutral beam transmits energy to the plasma by collisions as a result of which it becomes ionized and thus contained by the magnetic field thereby both heating and refuelling the reactor in one operation. The remainder of the charged beam is diverted by magnetic fields onto cooled beam dumps.\n\n\"Antiproton annihilation\" Theoretically a quantity of antiprotons injected into a mass of fusion fuel can induce thermonuclear reactions. This possibility as a method of spacecraft propulsion, known as Antimatter-catalyzed nuclear pulse propulsion, was investigated at Pennsylvania State University in connection with the proposed AIMStar project.\n\n\"Magnetic Oscillations\"\n\n\"Thomson Scattering\" Light scatters from plasma. This light can be detected and used to reconstruct the plasmas' behavior. This technique can be used to find its density and temperature. It is common in Inertial confinement fusion, Tokamaks and fusors. In ICF systems, this can be done by firing a second beam into a gold foil adjacent to the target. This makes x-rays that scatter or traverse the plasma. In Tokamaks, this can be done using mirrors and detectors to reflect light across a plane (two dimensions) or in a line (one dimension).\n\n\"Langmuir probe\" This is a metal object placed in a plasma. A potential is applied to it, giving it a positive or negative voltage against the surrounding plasma. The metal collects charged particles, drawing a current. As the voltage changes, the current changes. This makes a IV Curve. The IV-curve can be used to determine the local plasma density, potential and temperature.\n\n\"Neutron detectors\" Deuterium or tritium fusion produces neutrons. Neutrons interact with surrounding matter in ways that can be detected. Several types of neutron detectors exist which can record the rate at which neutrons are produced during fusion reactions. They are an essential tool for demonstrating success.\n\n\"Flux loop\" A loop of wire is inserted into the magnetic field. As the field passes through the loop, a current is made. The current is measured and used to find the total magnetic flux through that loop. This has been used on the National Compact Stellarator Experiment, the polywell and the LDX machines.\n\n\"X-ray detector\" All plasma loses energy by emitting light. This covers the whole spectrum: visible, IR, UV, and X-rays. This occurs anytime a particle changes speed, for any reason. If the reason is deflection by a magnetic field, the radiation is Cyclotron radiation at low speeds and Synchrotron radiation at high speeds. If the reason is deflection by another particle, plasma radiates X-rays, known as Bremsstrahlung radiation. X-rays are termed in both hard and soft, based on their energy.\n\nIt has been proposed that steam turbines be used to convert the heat from the fusion chamber into electricity. The heat is transferred into a working fluid that turns into steam, driving electric generators.\n\n\"Neutron blankets\" Deuterium and tritium fusion generates neutrons. This varies by technique (NIF has a record of 3E14 neutrons per second while a typical fusor produces 1E5–1E9 neutrons per second). It has been proposed to use these neutrons as a way to regenerate spent fission fuel or as a way to breed tritium using a breeder blanket consisting of liquid lithium or, as in more recent reactor designs, a helium cooled pebble bed consisting of lithium bearing ceramic pebbles fabricated from materials such as Lithium titanate, lithium orthosilicate or mixtures of these phases.\n\n\"Direct conversion\" This is a method where the kinetic energy of a particle is converted into voltage. It was first suggested by Richard F. Post in conjunction with magnetic mirrors, in the late sixties. It has also been suggested for Field-Reversed Configurations. The process takes the plasma, expands it, and converts a large fraction of the random energy of the fusion products into directed motion. The particles are then collected on electrodes at various large electrical potentials. This method has demonstrated an experimental efficiency of 48 percent.\n\nFusion records have been set by a number of devices. Here are some:\n\nThe ratio of energy extracted against the amount of energy supplied. This record is considered to be set by the Joint European Torus (JET) in 1997 when the device extracted 16 MW of power. However, this ratio can be seen three different ways.\n\nIn Field Reversed Configurations, the longest run time is 300 ms, set by the Princeton Field Reversed Configuration in August 2016. However this involved no fusion.\n\nThe fusion power trends as the plasma confinement raised to the fourth power. Hence, getting a strong plasma trap is of real value to a fusion power plant. Plasma has a very good electrical conductivity. This opens the possibility of confining the plasma with magnetic field, generally known as magnetic confinement. The field puts a magnetic pressure on the plasma, which holds it in. A widely used measure of magnetic trapping in fusion is the beta ratio:\n\nformula_11 \n\nThis is the ratio of the externally applied field to the internal pressure of the plasma. A value of 1 is ideal trapping. Some examples of beta values include:\n\nConfinement refers to all the conditions necessary to keep a plasma dense and hot long enough to undergo fusion. Here are some general principles.\n\n\nTo produce self-sustaining fusion, the energy released by the reaction (or at least a fraction of it) must be used to heat new reactant nuclei and keep them hot long enough that they also undergo fusion reactions.\n\nThe first human-made, large-scale fusion reaction was the test of the hydrogen bomb, Ivy Mike, in 1952. As part of the PACER project, it was once proposed to use hydrogen bombs as a source of power by detonating them in underground caverns and then generating electricity from the heat produced, but such a power station is unlikely ever to be constructed.\n\n\"Magnetic Mirror\" One example of magnetic confinement is with the magnetic mirror effect. If a particle follows the field line and enters a region of higher field strength, the particles can be reflected. There are several devices that try to use this effect. The most famous was the magnetic mirror machines, which was a series of large, expensive devices built at the Lawrence Livermore National Laboratory from the 1960s to mid 1980s. Some other examples include the magnetic bottles and Biconic cusp. Because the mirror machines were straight, they had some advantages over a ring shape. First, mirrors were easier to construct and maintain and second direct conversion energy capture, was easier to implement. As the confinement achieved in experiments was poor, this approach was abandoned.\n\n\"Magnetic Loops\" Another example of magnetic confinement is to bend the field lines back on themselves, either in circles or more commonly in nested toroidal surfaces. The most highly developed system of this type is the \"tokamak\", with the \"stellarator\" being next most advanced, followed by the Reversed field pinch. Compact toroids, especially the \"Field-Reversed Configuration\" and the spheromak, attempt to combine the advantages of toroidal magnetic surfaces with those of a simply connected (non-toroidal) machine, resulting in a mechanically simpler and smaller confinement area.\n\nInertial confinement is the use of rapidly imploding shell to heat and confine plasma. The shell is imploded using a direct laser blast (direct drive) or a secondary x-ray blast (indirect drive) or heavy ion beams. Theoretically, fusion using lasers would be done using tiny pellets of fuel that explode several times a second. To induce the explosion, the pellet must be compressed to about 30 times solid density with energetic beams. If direct drive is used—the beams are focused directly on the pellet—it can in principle be very efficient, but in practice is difficult to obtain the needed uniformity. The alternative approach, indirect drive, uses beams to heat a shell, and then the shell radiates x-rays, which then implode the pellet. The beams are commonly laser beams, but heavy and light ion beams and electron beams have all been investigated.\n\nThere are also electrostatic confinement fusion devices. These devices confine ions using electrostatic fields. The best known is the Fusor. This device has a cathode inside an anode wire cage. Positive ions fly towards the negative inner cage, and are heated by the electric field in the process. If they miss the inner cage they can collide and fuse. Ions typically hit the cathode, however, creating prohibitory high conduction losses. Also, fusion rates in fusors are very low because of competing physical effects, such as energy loss in the form of light radiation. Designs have been proposed to avoid the problems associated with the cage, by generating the field using a non-neutral cloud. These include a plasma oscillating device, a magnetically-shielded-grid, a penning trap, the polywell and the F1 cathode driver concept. The technology is relatively immature, however, and many scientific and engineering questions remain.\n\nResearch into nuclear fusion started in the early part of the 20th century. In 1920 the British physicist Francis William Aston discovered that the total mass equivalent of four hydrogen atoms (two protons and two neutrons) are heavier than the total mass of one helium atom (He-4), which implied that net energy can be released by combining hydrogen atoms together to form helium, and provided the first hints of a mechanism by which stars could produce energy in the quantities being measured. Through the 1920s, Arthur Stanley Eddington became a major proponent of the proton–proton chain reaction (PP reaction) as the primary system running the Sun.\n\nNeutrons from fusion was first detected by staff members of Ernest Rutherfords' at the University of Cambridge, in 1933. The experiment was developed by Mark Oliphant and involved the acceleration of protons towards a target at energies of up to 600,000 electron volts. In 1933, the Cavendish Laboratory received a gift from the American physical chemist Gilbert N. Lewis of a few drops of heavy water. The accelerator was used to fire heavy hydrogen nuclei \"deuterons\" at various targets. Working with Rutherford and others, Oliphant discovered the nuclei of Helium-3 (\"helions\") and tritium (\"tritons\").\n\nA theory was verified by Hans Bethe in 1939 showing that beta decay and quantum tunneling in the Sun's core might convert one of the protons into a neutron and thereby producing deuterium rather than a diproton. The deuterium would then fuse through other reactions to further increase the energy output. For this work, Bethe won the Nobel Prize in Physics.\n\nThe first patent related to a fusion reactor was registered in 1946 by the United Kingdom Atomic Energy Authority. The inventors were Sir George Paget Thomson and Moses Blackman. This was the first detailed examination of the Z-pinch concept. Starting in 1947, two UK teams carried out small experiments based on this concept and began building a series of ever-larger experiments.\n\nThe first successful man-made fusion device was the boosted fission weapon tested in 1951 in the Greenhouse Item test. This was followed by true fusion weapons in 1952's Ivy Mike, and the first practical examples in 1954's Castle Bravo. This was uncontrolled fusion. In these devices, the energy released by the fission explosion is used to compress and heat fusion fuel, starting a fusion reaction. Fusion releases neutrons. These neutrons hit the surrounding fission fuel, causing the atoms to split apart much faster than normal fission processes—almost instantly by comparison. This increases the effectiveness of bombs: normal fission weapons blow themselves apart before all their fuel is used; fusion/fission weapons do not have this practical upper limit.\n\nIn 1949 an expatriate German, Ronald Richter, proposed the Huemul Project in Argentina, announcing positive results in 1951. These turned out to be fake, but it prompted considerable interest in the concept as a whole. In particular, it prompted Lyman Spitzer to begin considering ways to solve some of the more obvious problems involved in confining a hot plasma, and, unaware of the z-pinch efforts, he developed a new solution to the problem known as the stellarator. Spitzer applied to the US Atomic Energy Commission for funding to build a test device. During this period, James L. Tuck who had worked with the UK teams on z-pinch had been introducing the concept to his new coworkers at the Los Alamos National Laboratory (LANL). When he heard of Spitzer's pitch for funding, he applied to build a machine of his own, the Perhapsatron.\n\nSpitzer's idea won funding and he began work on the stellarator under the code name Project Matterhorn. His work led to the creation of the Princeton Plasma Physics Laboratory. Tuck returned to LANL and arranged local funding to build his machine. By this time, however, it was clear that all of the pinch machines were suffering from the same issues involving instability, and progress stalled. In 1953, Tuck and others suggested a number of solutions to the stability problems. This led to the design of a second series of pinch machines, led by the UK ZETA and Sceptre devices.\n\nSpitzer had planned an aggressive development project of four machines, A, B, C, and D. A and B were small research devices, C would be the prototype of a power-producing machine, and D would be the prototype of a commercial device. A worked without issue, but even by the time B was being used it was clear the stellarator was also suffering from instabilities and plasma leakage. Progress on C slowed as attempts were made to correct for these problems.\n\nBy the mid-1950s it was clear that the simple theoretical tools being used to calculate the performance of all fusion machines were simply not predicting their actual behavior. Machines invariably leaked their plasma from their confinement area at rates far higher than predicted. In 1954, Edward Teller held a gathering of fusion researchers at the Princeton Gun Club, near the Project Matterhorn (now known as Project Sherwood) grounds. Teller started by pointing out the problems that everyone was having, and suggested that any system where the plasma was confined within concave fields was doomed to fail. Attendees remember him saying something to the effect that the fields were like rubber bands, and they would attempt to snap back to a straight configuration whenever the power was increased, ejecting the plasma. He went on to say that it appeared the only way to confine the plasma in a stable configuration would be to use convex fields, a \"cusp\" configuration.\n\nWhen the meeting concluded, most of the researchers quickly turned out papers saying why Teller's concerns did not apply to their particular device. The pinch machines did not use magnetic fields in this way at all, while the mirror and stellarator seemed to have various ways out. This was soon followed by a paper by Martin David Kruskal and Martin Schwarzschild discussing pinch machines, however, which demonstrated instabilities in those devices were inherent to the design.\n\nThe largest \"classic\" pinch device was the ZETA, including all of these suggested upgrades, starting operations in the UK in 1957. In early 1958, John Cockcroft announced that fusion had been achieved in the ZETA, an announcement that made headlines around the world. When physicists in the US expressed concerns about the claims they were initially dismissed. US experiments soon demonstrated the same neutrons, although temperature measurements suggested these could not be from fusion reactions. The neutrons seen in the UK were later demonstrated to be from different versions of the same instability processes that plagued earlier machines. Cockcroft was forced to retract the fusion claims, and the entire field was tainted for years. ZETA ended its experiments in 1968.\n\nThe first experiment to achieve controlled thermonuclear fusion was accomplished using Scylla I at the Los Alamos National Laboratory in 1958. Scylla I was a θ-pinch machine, with a cylinder full of deuterium. Electric current shot down the sides of the cylinder. The current made magnetic fields that pinched the plasma, raising temperatures to 15 million degrees Celsius, for long enough that atoms fused and produce neutrons. The sherwood program sponsored a series of Scylla machines at Los Alamos. The program began with 5 researchers and 100,000 in US funding in January 1952. By 1965, a total of 21 million had been spent on the program and staffing never reached above 65.\n\nIn 1950–1951 I.E. Tamm and A.D. Sakharov in the Soviet Union, first discussed a tokamak-like approach. Experimental research on those designs began in 1956 at the Kurchatov Institute in Moscow by a group of Soviet scientists led by Lev Artsimovich. The tokamak essentially combined a low-power pinch device with a low-power simple stellarator. The key was to combine the fields in such a way that the particles orbited within the reactor a particular number of times, today known as the \"safety factor\". The combination of these fields dramatically improved confinement times and densities, resulting in huge improvements over existing devices.\n\nA key plasma physics text was published by Lyman Spitzer at Princeton in 1963. Spitzer took the ideal gas laws and adapted them to an ionized plasma, developing many of the fundamental equations used to model a plasma.\n\nLaser fusion was suggested in 1962 by scientists at Lawrence Livermore National Laboratory, shortly after the invention of the laser itself in 1960. At the time, Lasers were low power machines, but low-level research began as early as 1965. Laser fusion, formally known as inertial confinement fusion, involves imploding a target by using laser beams. There are two ways to do this: indirect drive and direct drive. In direct drive, the laser blasts a pellet of fuel. In indirect drive, the lasers blast a structure around the fuel. This makes x-rays that squeeze the fuel. Both methods compress the fuel so that fusion can take place.\n\nAt the 1964 World's Fair, the public was given its first demonstration of nuclear fusion. The device was a θ-pinch from General Electric. This was similar to the Scylla machine developed earlier at Los Alamos.\n\nThe magnetic mirror was first published in 1967 by Richard F. Post and many others at the Lawrence Livermore National Laboratory. The mirror consisted of two large magnets arranged so they had strong fields within them, and a weaker, but connected, field between them. Plasma introduced in the area between the two magnets would \"bounce back\" from the stronger fields in the middle.\n\nThe A.D. Sakharov group constructed the first tokamaks, the most successful being the T-3 and its larger version T-4. T-4 was tested in 1968 in Novosibirsk, producing the world's first quasistationary fusion reaction. When this were first announced, the international community was highly skeptical. A British team was invited to see T-3, however, and after measuring it in depth they released their results that confirmed the Soviet claims. A burst of activity followed as many planned devices were abandoned and new tokamaks were introduced in their place — the C model stellarator, then under construction after many redesigns, was quickly converted to the Symmetrical Tokamak.\n\nIn his work with vacuum tubes, Philo Farnsworth observed that electric charge would accumulate in regions of the tube. Today, this effect is known as the Multipactor effect. Farnsworth reasoned that if ions were concentrated high enough they could collide and fuse. In 1962, he filed a patent on a design using a positive inner cage to concentrate plasma, in order to achieve nuclear fusion. During this time, Robert L. Hirsch joined the Farnsworth Television labs and began work on what became the fusor. Hirsch patented the design in 1966 and published the design in 1967.\n\nIn 1972, John Nuckolls outlined the idea of ignition. This is a fusion chain reaction. Hot helium made during fusion reheats the fuel and starts more reactions. John argued that ignition would require lasers of about 1 kJ. This turned out to be wrong. Nuckolls's paper started a major development effort. Several laser systems were built at LLNL. These included the argus, the Cyclops, the Janus, the long path, the Shiva laser and the Nova in 1984. This prompted the UK to build the Central Laser Facility in 1976.\n\nDuring this time, great strides in understanding the tokamak system were made. A number of improvements to the design are now part of the \"advanced tokamak\" concept, which includes non-circular plasma, internal diverters and limiters, often superconducting magnets, and operate in the so-called \"H-mode\" island of increased stability. Two other designs have also become fairly well studied; the compact tokamak is wired with the magnets on the inside of the vacuum chamber, while the spherical tokamak reduces its cross section as much as possible.\n\nIn 1974 a study of the ZETA results demonstrated an interesting side-effect; after an experimental run ended, the plasma would enter a short period of stability. This led to the reversed field pinch concept, which has seen some level of development since. On May 1, 1974, the KMS fusion company (founded by Kip Siegel) achieves the world's first laser induced fusion in a deuterium-tritium pellet.\n\nIn the mid-1970s, Project PACER, carried out at Los Alamos National Laboratory (LANL) explored the possibility of a fusion power system that would involve exploding small hydrogen bombs (fusion bombs) inside an underground cavity. As an energy source, the system is the only fusion power system that could be demonstrated to work using existing technology. It would also require a large, continuous supply of nuclear bombs, however, making the economics of such a system rather questionable.\n\nIn 1976, the two beam Argus laser becomes operational at livermore. In 1977, The 20 beam Shiva laser at Livermore is completed, capable of delivering 10.2 kilojoules of infrared energy on target. At a price of $25 million and a size approaching that of a football field, Shiva is the first of the megalasers. That same year, the JET project is approved by the European Commission and a site is selected.\n\nAs a result of advocacy, the cold war, and the 1970s energy crisis a massive magnetic mirror program was funded by the US federal government in the late 1970s and early 1980s. This program resulted in a series of large magnetic mirror devices including: 2X, Baseball I, Baseball II, the Tandem Mirror Experiment, the Tandem mirror experiment upgrade, the Mirror Fusion Test Facility and the MFTF-B. These machines were built and tested at Livermore from the late 1960s to the mid 1980s. A number of institutions collaborated on these machines, conducting experiments. These included the Institute for Advanced Study and the University of Wisconsin–Madison. The last machine, the Mirror Fusion Test Facility cost 372 million dollars and was, at that time, the most expensive project in Livermore history. It opened on February 21, 1986 and was promptly shut down. The reason given was to balance the United States federal budget. This program was supported from within the Carter and early Reagan administrations by Edwin E. Kintner, a US Navy captain, under Alvin Trivelpiece.\n\nIn Laser fusion progressed: in 1983, the NOVETTE laser was completed. The following December 1984, the ten beam NOVA laser was finished. Five years later, NOVA would produce a maximum of 120 kilojoules of infrared light, during a nanosecond pulse. Meanwhile, efforts focused on either fast delivery or beam smoothness. Both tried to deliver the energy uniformly to implode the target. One early problem was that the light in the infrared wavelength, lost lots of energy before hitting the fuel. Breakthroughs were made at the Laboratory for Laser Energetics at the University of Rochester. Rochester scientists used frequency-tripling crystals to transform the infrared laser beams into ultraviolet beams. In 1985, Donna Strickland and Gérard Mourou invented a method to amplify lasers pulses by \"chirping\". This method changes a single wavelength into a full spectrum. The system then amplifies the laser at each wavelength and then reconstitutes the beam into one color. Chirp pulsed amplification became instrumental in building the National Ignition Facility and the Omega EP system. Most research into ICF was towards weapons research, because the implosion is relevant to nuclear weapons.\n\nDuring this time Los Alamos National Laboratory constructed a series of laser facilities. This included Gemini (a two beam system), Helios (eight beams), Antares (24 beams) and Aurora (96 beams). The program ended in the early nineties with a cost on the order of one billion dollars.\n\nIn 1987, Akira Hasegawa noticed that in a dipolar magnetic field, fluctuations tended to compress the plasma without energy loss. This effect was noticed in data taken by Voyager 2, when it encountered Uranus. This observation would become the basis for a fusion approach known as the Levitated dipole.\n\nIn Tokamaks, the Tore Supra was under construction over the middle of the eighties (1983 to 1988). This was a Tokamak built in Cadarache, France. In 1983, the JET was completed and first plasmas achieved. In 1985, the Japanese tokamak, JT-60 was completed. In 1988, the T-15 a Soviet tokamak was completed. It was the first industrial fusion reactor to use superconducting magnets to control the plasma. These were Helium cooled.\n\nIn 1989, Pons and Fleischmann submitted papers to the \"Journal of Electroanalytical Chemistry\" claiming that they had observed fusion in a room temperature device and disclosing their work in a press release. Some scientists reported excess heat, neutrons, tritium, helium and other nuclear effects in so-called cold fusion systems, which for a time gained interest as showing promise. Hopes fell when replication failures were weighed in view of several reasons cold fusion is not likely to occur, the discovery of possible sources of experimental error, and finally the discovery that Fleischmann and Pons had not actually detected nuclear reaction byproducts. By late 1989, most scientists considered cold fusion claims dead, and cold fusion subsequently gained a reputation as pathological science. However, a small community of researchers continues to investigate cold fusion claiming to replicate Fleishmann and Pons' results including nuclear reaction byproducts. Claims related to cold fusion are largely disbelieved in the mainstream scientific community. In 1989, the majority of a review panel organized by the US Department of Energy (DOE) found that the evidence for the discovery of a new nuclear process was not persuasive. A second DOE review, convened in 2004 to look at new research, reached conclusions similar to the first.\n\nIn 1984, Martin Peng of ORNL proposed an alternate arrangement of the magnet coils that would greatly reduce the aspect ratio while avoiding the erosion issues of the compact tokamak: a Spherical tokamak. Instead of wiring each magnet coil separately, he proposed using a single large conductor in the center, and wiring the magnets as half-rings off of this conductor. What was once a series of individual rings passing through the hole in the center of the reactor was reduced to a single post, allowing for aspect ratios as low as 1.2. The ST concept appeared to represent an enormous advance in tokamak design. However, it was being proposed during a period when US fusion research budgets were being dramatically scaled back. ORNL was provided with funds to develop a suitable central column built out of a high-strength copper alloy called \"Glidcop\". However, they were unable to secure funding to build a demonstration machine, \"STX\". Failing to build an ST at ORNL, Peng began a worldwide effort to interest other teams in the ST concept and get a test machine built. One way to do this quickly would be to convert a spheromak machine to the Spherical tokamak layout. Peng's advocacy also caught the interest of Derek Robinson, of the United Kingdom Atomic Energy Authority fusion center at Culham. Robinson was able to gather together a team and secure funding on the order of 100,000 pounds to build an experimental machine, the Small Tight Aspect Ratio Tokamak, or START. Several parts of the machine were recycled from earlier projects, while others were loaned from other labs, including a 40 keV neutral beam injector from ORNL. Construction of START began in 1990, it was assembled rapidly and started operation in January 1991.\n\nIn 1991 the Preliminary Tritium Experiment at the Joint European Torus in England achieved the world's first controlled release of fusion power.\n\nIn 1992, a major article was published in Physics Today by Robert McCory at the Laboratory for laser energetics outlying the current state of ICF and advocating for a national ignition facility. This was followed up by a major review article, from John Lindl in 1995, advocating for NIF. During this time a number of ICF subsystems were developing, including target manufacturing, cryogenic handling systems, new laser designs (notably the NIKE laser at NRL) and improved diagnostics like time of flight analyzers and Thomson scattering. This work was done at the NOVA laser system, General Atomics, Laser Mégajoule and the GEKKO XII system in Japan. Through this work and lobbying by groups like the fusion power associates and John Sethian at NRL, a vote was made in congress, authorizing funding for the NIF project in the late nineties.\n\nIn the early nineties, theory and experimental work regarding fusors and polywells was published. In response, Todd Rider at MIT developed general models of these devices. Rider argued that all plasma systems at thermodynamic equilibrium were fundamentally limited. In 1995, William Nevins published a criticism arguing that the particles inside fusors and polywells would build up angular momentum, causing the dense core to degrade.\n\nIn 1995, the University of Wisconsin–Madison built a large fusor, known as HOMER, which is still in operation. Meanwhile, Dr George H. Miley at Illinois, built a small fusor that has produced neutrons using deuterium gas and discovered the \"star mode\" of fusor operation. The following year, the first \"US-Japan Workshop on IEC Fusion\", was conducted. At this time in Europe, an IEC device was developed as a commercial neutron source by Daimler-Chrysler and NSD Fusion.\n\nIn 1996, the Z-machine was upgraded and opened to the public by the US Army in August 1998 in Scientific American. The key attributes of Sandia's Z machine are its 18 million amperes and a discharge time of less than 100 nanoseconds. This generates a magnetic pulse, inside a large oil tank, this strikes an array of tungsten wires called a \"liner\". Firing the Z-machine has become a way to test very high energy, high temperature (2 billion degrees) conditions. In 1996, the Tore Supra creates a plasma for two minutes with a current of almost 1 million amperes driven non-inductively by 2.3 MW of lower hybrid frequency waves. This is 280 MJ of injected and extracted energy. This result was possible because of the actively cooled plasma-facing components \n\nIn 1997, JET produced a peak of 16.1MW of fusion power (65% of heat to plasma), with fusion power of over 10MW sustained for over 0.5 sec. Its successor, the International Thermonuclear Experimental Reactor (ITER), was officially announced as part of a seven-party consortium (six countries and the EU). ITER is designed to produce ten times more fusion power than the power put into the plasma. ITER is currently under construction in Cadarache, France.\n\nIn the late nineties, a team at Columbia University and MIT developed the Levitated dipole a fusion device which consisted of a superconducting electromagnet, floating in a saucer shaped vacuum chamber. Plasma swirled around this donut and fused along the center axis.\n\nIn the March 8, 2002 issue of the peer-reviewed journal \"Science\", Rusi P. Taleyarkhan and colleagues at the Oak Ridge National Laboratory (ORNL) reported that acoustic cavitation experiments conducted with deuterated acetone () showed measurements of tritium and neutron output consistent with the occurrence of fusion. Taleyarkhan was later found guilty of misconduct, the Office of Naval Research debarred him for 28 months from receiving Federal Funding, and his name was listed in the 'Excluded Parties List'.\n\n\"Fast ignition\" was developed in the late nineties, and was part of a push by the Laboratory for Laser Energetics for building the Omega EP system. This system was finished in 2008. Fast ignition showed such dramatic power savings that ICF appears to be a useful technique for energy production. There are even proposals to build an experimental facility dedicated to the fast ignition approach, known as HiPER.\n\nIn April 2005, a team from UCLA announced it had devised a way of producing fusion using a machine that \"fits on a lab bench\", using lithium tantalate to generate enough voltage to smash deuterium atoms together. The process, however, does not generate net power (see Pyroelectric fusion). Such a device would be useful in the same sort of roles as the fusor. In 2006, China's EAST test reactor is completed. This was the first tokamak to use superconducting magnets to generate both the toroidal and poloidal fields.\n\nIn the early 2000s, Researchers at LANL reasoned that a plasma oscillating could be at local thermodynamic equilibrium. This prompted the POPS and Penning trap designs. At this time, researchers at MIT became interested in fusors for space propulsion and powering space vehicles. Specifically, researchers developed fusors with multiple inner cages. Greg Piefer graduated from Madison and founded Phoenix Nuclear Labs, a company that developed the fusor into a neutron source for the mass production of medical isotopes. Robert Bussard began speaking openly about the Polywell in 2006. He attempted to generate interest in the research, before his death. In 2008, Taylor Wilson achieved notoriety for achieving nuclear fusion at 14, with a homemade fusor.\n\nIn March 2009, a high-energy laser system, the National Ignition Facility (NIF), located at the Lawrence Livermore National Laboratory, became operational.\n\nThe early 2000s saw the founding of a number of privately backed fusion companies pursuing innovative approaches with the stated goal of developing commercially viable fusion power plants. Secretive startup Tri Alpha Energy, founded in 1998, began exploring a field-reversed configuration approach. In 2002, Canadian company General Fusion began proof-of-concept experiments based on a hybrid magneto-inertial approach called Magnetized Target Fusion. These companies are funded by private investors including Jeff Bezos (General Fusion) and Paul Allen (Tri Alpha Energy). Toward the end of the decade, UK-based fusion company Tokamak Energy started exploring spherical tokamak devices.\n\nNIF, the French Laser Mégajoule and the planned European Union High Power laser Energy Research (HiPER) facility continued researching inertial (laser) confinement.\n\nIn 2010, NIF researchers conducted a series of \"tuning\" shots to determine the optimal target design and laser parameters for high-energy ignition experiments with fusion fuel. Firing tests were performed on October 31, 2010 and November 2, 2010. In early 2012, NIF director Mike Dunne expected the laser system to generate fusion with net energy gain by the end of 2012. However, that did not happen until August 2013. The facility reported that their next step involved improving the system to prevent the hohlraum from either breaking up asymmetrically or too soon.\n\nA 2012 paper demonstrated that a dense plasma focus had achieved temperatures of 1.8 billion degrees Celsius, sufficient for boron fusion, and that fusion reactions were occurring primarily within the contained plasmoid, a necessary condition for net power.\n\nIn April 2014, Lawrence Livermore National Laboratory ended the Laser Inertial Fusion Energy (LIFE) program and redirected their efforts towards NIF. In August 2014, Phoenix Nuclear Labs announced the sale of a high-yield neutron generator that could sustain 5×10 deuterium fusion reactions per second over a 24-hour period.\n\nIn October 2014, Lockheed Martin's Skunk Works announced the development of a high beta fusion reactor, the Compact Fusion Reactor, intending on making a 100-megawatt prototype by 2017 and beginning regular operation by 2022. Although the original concept was to build a 20-ton, container-sized unit, the team later conceded that the minimum scale would be 2,000 tons.\n\nIn January 2015, the polywell was presented at Microsoft Research.\n\nIn August, 2015, MIT announced a tokamak it named ARC fusion reactor using rare-earth barium-copper oxide (REBCO) superconducting tapes to produce high-magnetic field coils that it claimed produce comparable magnetic field strength in a smaller configuration than other designs.\n\nIn October 2015, researchers at the Max Planck Institute of Plasma Physics completed building the largest stellarator to date, named Wendelstein 7-X. On December 10, they successfully produced the first helium plasma, and on February 3, 2016 produced the device's first hydrogen plasma. With plasma discharges lasting up to 30 minutes, Wendelstein 7-X is attempting to demonstrate the essential stellarator attribute: continuous operation of a high-temperature hydrogen plasma.\n\nGeneral Fusion developed its plasma injector technology and Tri Alpha Energy constructed and operated its C-2U device.\n\nIn 2017 Helion Energy's fifth-generation plasma machine went into operation, seeking to achieve plasma density of 20 Tesla and fusion temperatures. In 2018 General Fusion was developing a 70% scale demo system to be completed around 2023.\n\nIn 2018, energy corporation Eni announced a $50 million investment in the newly founded Commonwealth Fusion Systems, to attempt to commercialize ARC technology using a test reactor (SPARC) in collaboration with MIT.\n\nBy firing particle beams at targets, many fusion reactions have been tested, while the fuels considered for power have all been light elements like the isotopes of hydrogen—protium, deuterium, and tritium. The deuterium and helium-3 reaction requires helium-3, an isotope of helium so scarce on Earth that it would have to be mined extraterrestrially or produced by other nuclear reactions. Finally, researchers hope to perform the protium and boron-11 reaction, because it does not directly produce neutrons, though side reactions can.\n\nThe easiest nuclear reaction, at the lowest energy, is:\n\nThis reaction is common in research, industrial and military applications, usually as a convenient source of neutrons. Deuterium is a naturally occurring isotope of hydrogen and is commonly available. The large mass ratio of the hydrogen isotopes makes their separation easy compared to the difficult uranium enrichment process. Tritium is a natural isotope of hydrogen, but because it has a short half-life of 12.32 years, it is hard to find, store, produce, and is expensive. Consequently, the deuterium-tritium fuel cycle requires the breeding of tritium from lithium using one of the following reactions:\n\nThe reactant neutron is supplied by the D-T fusion reaction shown above, and the one that has the greatest yield of energy. The reaction with Li is exothermic, providing a small energy gain for the reactor. The reaction with Li is endothermic but does not consume the neutron. At least some neutron multiplication reactions are required to replace the neutrons lost to absorption by other elements. Leading candidate neutron multiplication materials are beryllium and lead however the Li reaction above also helps to keep the neutron population high. Natural lithium is mainly Li however this has a low tritium production cross section compared to Li so most reactor designs use breeder blankets with enriched Li.\n\nSeveral drawbacks are commonly attributed to D-T fusion power:\n\nThe neutron flux expected in a commercial D-T fusion reactor is about 100 times that of current fission power reactors, posing problems for material design. After a series of D-T tests at JET, the vacuum vessel was sufficiently radioactive that remote handling was required for the year following the tests.\n\nIn a production setting, the neutrons would be used to react with lithium in the context of a breeder blanket comprising lithium ceramic pebbles or liquid lithium, in order to create more tritium. This also deposits the energy of the neutrons in the lithium, which would then be transferred to drive electrical production. The lithium neutron absorption reaction protects the outer portions of the reactor from the neutron flux. Newer designs, the advanced tokamak in particular, also use lithium inside the reactor core as a key element of the design. The plasma interacts directly with the lithium, preventing a problem known as \"recycling\". The advantage of this design was demonstrated in the Lithium Tokamak Experiment.\n\nThis is the second easiest fusion reaction, fusing two deuterium nuclei. The reaction has two branches that occur with nearly equal probability:\nThis reaction is also common in research. The optimum energy to initiate this reaction is 15 keV, only slightly higher than the optimum for the D-T reaction. The first branch does not produce neutrons, but it does produce tritium, so that a D-D reactor will not be completely tritium-free, even though it does not require an input of tritium or lithium. Unless the tritons can be quickly removed, most of the tritium produced would be burned before leaving the reactor, which would reduce the handling of tritium, but would produce more neutrons, some of which are very energetic. The neutron from the second branch has an energy of only , whereas the neutron from the D-T reaction has an energy of , resulting in a wider range of isotope production and material damage. When the tritons are removed quickly while allowing the He to react, the fuel cycle is called \"tritium suppressed fusion\" The removed tritium decays to He with a 12.5 year half life. By recycling the He produced from the decay of tritium back into the fusion reactor, the fusion reactor does not require materials resistant to fast neutrons.\n\nAssuming complete tritium burn-up, the reduction in the fraction of fusion energy carried by neutrons would be only about 18%, so that the primary advantage of the D-D fuel cycle is that tritium breeding would not be required. Other advantages are independence from scarce lithium resources and a somewhat softer neutron spectrum. The disadvantage of D-D compared to D-T is that the energy confinement time (at a given pressure) must be 30 times longer and the power produced (at a given pressure and volume) would be 68 times less.\n\nAssuming complete removal of tritium and recycling of He, only 6% of the fusion energy is carried by neutrons. The tritium-suppressed D-D fusion requires an energy confinement that is 10 times longer compared to D-T and a plasma temperature that is twice as high.\n\nA second-generation approach to controlled fusion power involves combining helium-3 (He) and deuterium (H):\n\nThis reaction produces a helium-4 nucleus (He) and a high-energy proton. As with the p-B aneutronic fusion fuel cycle, most of the reaction energy is released as charged particles, reducing activation of the reactor housing and potentially allowing more efficient energy harvesting (via any of several speculative technologies). In practice, D-D side reactions produce a significant number of neutrons, resulting in p-B being the preferred cycle for aneutronic fusion.\n\nIf aneutronic fusion is the goal, then the most promising candidate may be the hydrogen-1 (protium) and boron reaction, which releases alpha (helium) particles, but does not rely on neutron scattering for energy transfer.\n\nUnder reasonable assumptions, side reactions will result in about 0.1% of the fusion power being carried by neutrons. At 123 keV, the optimum temperature for this reaction is nearly ten times higher than that for the pure hydrogen reactions, the energy confinement must be 500 times better than that required for the D-T reaction, and the power density will be 2500 times lower than for D-T.\n\nBecause the confinement properties of conventional approaches to fusion such as the tokamak and laser pellet fusion are marginal, most proposals for aneutronic fusion are based on radically different confinement concepts, such as the Polywell and the Dense Plasma Focus. Results have been extremely promising:\n\nEven on smaller plasma production scales, the material of the containment apparatus will be intensely blasted with matter and energy. Designs for plasma containment must consider:\n\n\nDepending on the approach, these effects may be higher or lower than typical fission reactors like the pressurized water reactor (PWR). One estimate put the radiation at 100 times that of a typical PWR. Materials need to be selected or developed that can withstand these basic conditions. Depending on the approach, however, there may be other considerations such as electrical conductivity, magnetic permeability and mechanical strength. There is also a need for materials whose primary components and impurities do not result in long-lived radioactive wastes.\n\nFor long term use, each atom in the wall is expected to be hit by a neutron and displaced about a hundred times before the material is replaced. High-energy neutrons will produce hydrogen and helium by way of various nuclear reactions that tends to form bubbles at grain boundaries and result in swelling, blistering or embrittlement.\n\nOne can choose either a low-Z material, such as graphite or beryllium, or a high-Z material, usually tungsten with molybdenum as a second choice. Use of liquid metals (lithium, gallium, tin) has also been proposed, e.g., by injection of 1–5 mm thick streams flowing at 10 m/s on solid substrates.\n\nIf graphite is used, the gross erosion rates due to physical and chemical sputtering would be many meters per year, so one must rely on redeposition of the sputtered material. The location of the redeposition will not exactly coincide with the location of the sputtering, so one is still left with erosion rates that may be prohibitive. An even larger problem is the tritium co-deposited with the redeposited graphite. The tritium inventory in graphite layers and dust in a reactor could quickly build up to many kilograms, representing a waste of resources and a serious radiological hazard in case of an accident. The consensus of the fusion community seems to be that graphite, although a very attractive material for fusion experiments, cannot be the primary plasma-facing material (PFM) in a commercial reactor.\n\nThe sputtering rate of tungsten by the plasma fuel ions is orders of magnitude smaller than that of carbon, and tritium is much less incorporated into redeposited tungsten, making this a more attractive choice. On the other hand, tungsten impurities in a plasma are much more damaging than carbon impurities, and self-sputtering of tungsten can be high, so it will be necessary to ensure that the plasma in contact with the tungsten is not too hot (a few tens of eV rather than hundreds of eV). Tungsten also has disadvantages in terms of eddy currents and melting in off-normal events, as well as some radiological issues.\n\nUnlike nuclear fission, fusion requires extremely precise and controlled temperature, pressure and magnetic field parameters for any net energy to be produced. If a reactor suffers damage or loses even a small degree of required control, fusion reactions and heat generation would rapidly cease. Additionally, fusion reactors contain only small amounts of fuel, enough to \"burn\" for minutes, or in some cases, microseconds. Unless they are actively refueled, the reactions will quickly end. Therefore, fusion reactors are considered immune from catastrophic meltdown.\n\nFor similar reasons, runaway reactions cannot occur in a fusion reactor. The plasma is burnt at optimal conditions, and any significant change will simply quench the reactions. The reaction process is so delicate that this level of safety is inherent. Although the plasma in a fusion power station is expected to have a volume of or more, the plasma density is low and typically contains only a few grams of fuel in use. If the fuel supply is closed, the reaction stops within seconds. In comparison, a fission reactor is typically loaded with enough fuel for several months or years, and no additional fuel is necessary to continue the reaction. It is this large amount of fuel that gives rise to the possibility of a meltdown; nothing like this exists in a fusion reactor.\n\nIn the magnetic approach, strong fields are developed in coils that are held in place mechanically by the reactor structure. Failure of this structure could release this tension and allow the magnet to \"explode\" outward. The severity of this event would be similar to any other industrial accident or an MRI machine quench/explosion, and could be effectively stopped with a containment building similar to those used in existing (fission) nuclear generators. The laser-driven inertial approach is generally lower-stress because of the increased size of the reaction chamber. Although failure of the reaction chamber is possible, simply stopping fuel delivery would prevent any sort of catastrophic failure.\n\nMost reactor designs rely on liquid hydrogen as both a coolant and a method for converting stray neutrons from the reaction into tritium, which is fed back into the reactor as fuel. Hydrogen is highly flammable, and in the case of a fire it is possible that the hydrogen stored on-site could be burned up and escape. In this case, the tritium contents of the hydrogen would be released into the atmosphere, posing a radiation risk. Calculations suggest that at about , the total amount of tritium and other radioactive gases in a typical power station would be so small that they would have diluted to legally acceptable limits by the time they blew as far as the station's perimeter fence.\n\nThe likelihood of small industrial accidents, including the local release of radioactivity and injury to staff, cannot be estimated yet. These would include accidental releases of lithium or tritium or mishandling of decommissioned radioactive components of the reactor itself.\n\nA quench is an abnormal termination of magnet operation that occurs when part of the superconducting coil enters the normal (resistive) state. This can occur because the field inside the magnet is too large, the rate of change of field is too large (causing eddy currents and resultant heating in the copper support matrix), or a combination of the two.\n\nMore rarely a defect in the magnet can cause a quench. When this happens, that particular spot is subject to rapid Joule heating from the enormous current, which raises the temperature of the surrounding regions. This pushes those regions into the normal state as well, which leads to more heating in a chain reaction. The entire magnet rapidly becomes normal (this can take several seconds, depending on the size of the superconducting coil). This is accompanied by a loud bang as the energy in the magnetic field is converted to heat, and rapid boil-off of the cryogenic fluid. The abrupt decrease of current can result in kilovolt inductive voltage spikes and arcing. Permanent damage to the magnet is rare, but components can be damaged by localized heating, high voltages, or large mechanical forces.\n\nIn practice, magnets usually have safety devices to stop or limit the current when the beginning of a quench is detected. If a large magnet undergoes a quench, the inert vapor formed by the evaporating cryogenic fluid can present a significant asphyxiation hazard to operators by displacing breathable air.\n\nA large section of the superconducting magnets in CERN's Large Hadron Collider unexpectedly quenched during start-up operations in 2008, necessitating the replacement of a number of magnets. In order to mitigate against potentially destructive quenches, the superconducting magnets that form the LHC are equipped with fast-ramping heaters which are activated once a quench event is detected by the complex quench protection system. As the dipole bending magnets are connected in series, each power circuit includes 154 individual magnets, and should a quench event occur, the entire combined stored energy of these magnets must be dumped at once. This energy is transferred into dumps that are massive blocks of metal which heat up to several hundreds of degrees Celsius—because of resistive heating—in a matter of seconds. Although undesirable, a magnet quench is a \"fairly routine event\" during the operation of a particle accelerator.\n\nThe natural product of the fusion reaction is a small amount of helium, which is completely harmless to life. Of more concern is tritium, which, like other isotopes of hydrogen, is difficult to retain completely. During normal operation, some amount of tritium will be continually released.\n\nAlthough tritium is volatile and biologically active, the health risk posed by a release is much lower than that of most radioactive contaminants, because of tritium's short half-life (12.32 years) and very low decay energy (~14.95 keV), and because it does not bioaccumulate (instead being cycled out of the body as water, with a biological half-life of 7 to 14 days). Current ITER designs are investigating total containment facilities for any tritium.\n\nThe large flux of high-energy neutrons in a reactor will make the structural materials radioactive. The radioactive inventory at shut-down may be comparable to that of a fission reactor, but there are important differences.\n\nThe half-life of the radioisotopes produced by fusion tends to be less than those from fission, so that the inventory decreases more rapidly. Unlike fission reactors, whose waste remains radioactive for thousands of years, most of the radioactive material in a fusion reactor would be the reactor core itself, which would be dangerous for about 50 years, and low-level waste for another 100. Although this waste will be considerably more radioactive during those 50 years than fission waste, the very short half-life makes the process very attractive, as the waste management is fairly straightforward. By 500 years the material would have the same radiotoxicity as coal ash.\n\nAdditionally, the choice of materials used in a fusion reactor is less constrained than in a fission design, where many materials are required for their specific neutron cross-sections. This allows a fusion reactor to be designed using materials that are selected specifically to be \"low activation\", materials that do not easily become radioactive. Vanadium, for example, would become much less radioactive than stainless steel. Carbon fiber materials are also low-activation, as well as being strong and light, and are a promising area of study for laser-inertial reactors where a magnetic field is not required.\n\nIn general terms, fusion reactors would create far less radioactive material than a fission reactor, the material it would create is less damaging biologically, and the radioactivity \"burns off\" within a time period that is well within existing engineering capabilities for safe long-term waste storage.\n\nAlthough fusion power uses nuclear technology, the overlap with nuclear weapons would be limited. A huge amount of tritium could be produced by a fusion power station; tritium is used in the trigger of hydrogen bombs and in a modern boosted fission weapon, but it can also be produced by nuclear fission. The energetic neutrons from a fusion reactor could be used to breed weapons-grade plutonium or uranium for an atomic bomb (for example by transmutation of U to Pu, or Th to U).\n\nA study conducted 2011 assessed the risk of three scenarios:\n\n\nAnother study concludes that \"[..]large fusion reactors – even if not designed for fissile material breeding – could easily produce several hundred kg Pu per year with high weapon quality and very low source material requirements.\" It was emphasized that the implementation of features for intrinsic proliferation resistance might only be possible at this phase of research and development. The theoretical and computational tools needed for hydrogen bomb design are closely related to those needed for inertial confinement fusion, but have very little in common with the more scientifically developed magnetic confinement fusion.\n\nLarge-scale reactors using neutronic fuels (e.g. ITER) and thermal power production (turbine based) are most comparable to fission power from an engineering and economics viewpoint. Both fission and fusion power stations involve a relatively compact heat source powering a conventional steam turbine-based power station, while producing enough neutron radiation to make activation of the station materials problematic. The main distinction is that fusion power produces no high-level radioactive waste (though activated station materials still need to be disposed of). There are some power station ideas that may significantly lower the cost or size of such stations; however, research in these areas is nowhere near as advanced as in tokamaks.\n\nFusion power commonly proposes the use of deuterium, an isotope of hydrogen, as fuel and in many current designs also use lithium. Assuming a fusion energy output equal to the 1995 global power output of about 100 EJ/yr (= 1 × 10 J/yr) and that this does not increase in the future, which is unlikely, then the known current lithium reserves would last 3000 years. Lithium from sea water would last 60 million years, however, and a more complicated fusion process using only deuterium would have fuel for 150 billion years. To put this in context, 150 billion years is close to 30 times the remaining lifespan of the sun, and more than 10 times the estimated age of the universe.\n\nWhile fusion power is still in early stages of development, substantial sums have been and continue to be invested in research. In the EU almost was spent on fusion research up to the end of the 1990s, and the new ITER reactor alone is budgeted at €6.6 billion total for the timeframe between 2008 and 2020.\n\nIt is estimated that up to the point of possible implementation of electricity generation by nuclear fusion, R&D will need further promotion totalling around over a period of or so (of which within the EU) based on a report from 2002. Nuclear fusion research receives (excluding ITER funding) from the European Union, compared with for sustainable energy research, putting research into fusion power well ahead of that of any single rivaling technology. Indeed, the size of the investments and time frame of the expected results mean that fusion research is almost exclusively publicly funded, while research in other forms of energy can be done by the private sector. In spite of that, a number of start-up companies active in the field of fusion power have managed to attract private money.\n\nFusion power would provide more energy for a given weight of fuel than any fuel-consuming energy source currently in use, and the fuel itself (primarily deuterium) exists abundantly in the Earth's ocean: about 1 in 6500 hydrogen atoms in seawater is deuterium. Although this may seem a low proportion (about 0.015%), because nuclear fusion reactions are so much more energetic than chemical combustion and seawater is easier to access and more plentiful than fossil fuels, fusion could potentially supply the world's energy needs for millions of years.\n\nDespite being technically non-renewable, fusion power (like fission power using breeder reactors and reprocessing) has many of the benefits of renewable energy sources (such as being a long-term energy supply and emitting no greenhouse gases or air pollution) as well as some of the benefits of the resource-limited energy sources as hydrocarbons and nuclear fission (without reprocessing). Like these currently dominant energy sources, fusion could provide very high power-generation density and uninterrupted power delivery (because it is not dependent on the weather, unlike wind and solar power).\n\nAnother aspect of fusion energy is that the cost of production does not suffer from diseconomies of scale. The cost of water and wind energy, for example, goes up as the optimal locations are developed first, while further generators must be sited in less ideal conditions. With fusion energy the production cost will not increase much even if large numbers of stations are built, because the raw resource (seawater) is abundant and widespread.\n\nSome problems that are expected to be an issue in this century, such as fresh water shortages, can alternatively be regarded as problems of energy supply. For example, in desalination stations, seawater can be purified through distillation or reverse osmosis. Nonetheless, these processes are energy intensive. Even if the first fusion stations are not competitive with alternative sources, fusion could still become competitive if large-scale desalination requires more power than the alternatives are able to provide.\n\nA scenario has been presented of the effect of the commercialization of fusion power on the future of human civilization. ITER and later DEMO are envisioned to bring online the first commercial nuclear fusion energy reactor by 2050. Using this as the starting point and the history of the uptake of nuclear fission reactors as a guide, the scenario depicts a rapid take up of nuclear fusion energy starting after the middle of this century.\n\nFusion power could be used in interstellar space, where solar energy is not available.\n\n\n\n"}
{"id": "43893305", "url": "https://en.wikipedia.org/wiki?curid=43893305", "title": "Henry Neville Hutchinson", "text": "Henry Neville Hutchinson\n\nHenry Neville Hutchinson (1856, Chester – 1927) was an Anglican clergyman and, during the 1890s, a leading writer of popular books on geology, palaeontology, evolution and anthropology.\n\nHenry Neville Hutchinson was the eldest son of Thomas Neville Hutchinson, an Anglican clergyman and amateur naturalist. H. N. Hutchinson was educated at Rugby School and St John's College, Cambridge, where he earned a bachelor's degree in 1878. In 1879–1880 he was a student-master at Clifton College. In 1884 he was curate to St Saviour's, Redland Park, Bristol. In 1886–1887 he was private tutor to the sons of the Earl of Morley. In 1891 he began literary work in London. He was an amateur naturalist and photographer, whose collection of photographs was exhibited at The Anthropological Institute of Great Britain and Ireland where he encouraged members to collect quality photographs for ethnological purposes. He married in 1902.\n\n\n"}
{"id": "11448766", "url": "https://en.wikipedia.org/wiki?curid=11448766", "title": "John Webber", "text": "John Webber\n\nJohn Webber RA (London 6 October 1751 – 29 May 1793 London) was an English artist who accompanied Captain Cook on his third Pacific expedition. He is best known for his images of Australasia, Hawaii and Alaska.\n\nWebber was born in London, educated in Bern and studied painting at Paris. His father was Abraham Wäber, a Swiss sculptor who had moved to London, and changed his name to Webber before marrying a Mrs Mary Quant in 1744. \n\nWebber served as official artist on James Cook's third voyage of discovery around the Pacific (1776–80) aboard HMS Resolution. At Adventure Bay in January 1777 he did drawings of \"A Man of Van Diemen's Land\" and \"A Woman of Van Diemen's Land\". He also did many drawings of scenes in New Zealand and the South Sea islands. On this voyage, during which Cook lost his life in a fight in Hawaii, Webber became the first European artist to make contact with Hawaii, then called the Sandwich Islands. He made numerous watercolor landscapes of the islands of Kauai and Hawaii, and also portrayed many of the Hawaiian people.\n\nIn April 1778, Captain Cook's ships Resolution and Discovery anchored at Ship Cove, now known as Nootka Sound, Vancouver Island, Canada to refit. The crew took observations and recorded encounters with the local people. Webber made watercolour landscapes including \"Resolution and Discovery in Ship Cove, 1778\". His drawings and paintings were engraved for British Admiralty's account of the expedition, which was published in 1784.\nBack in England in 1780 Webber exhibited around 50 works at Royal Academy exhibitions between 1784 and 1792, and was elected an associate of the Royal Academy in 1785 and R.A. in 1791. Most of his work were landscapes. Sometimes figures were included as in \"A Party from H.M.S. Resolution shooting sea horses\", which was shown at the academy in 1784, and his \"The Death of Captain Cook\" became well known through an engraving of it. Another version of this picture is in the William Dixson gallery at Sydney.\n\nThe Anchorage Museum of History and Art (Alaska), the Bishop Museum (Honolulu), the Honolulu Museum of Art, the Peabody Essex Museum (Salem, Massachusetts), the Yale University Art Gallery, the British Museum, the Sir John Soane's Museum (London), The National Maritime Museum (London) the Museum of New Zealand Te Papa Tongarewa\nand the Mitchell Library (Australia) are among the public collections holding works by John Webber.\n\nWebber's art is held by a number of Australian institutions including the National Portrait Gallery (\"William Bligh\", c.1776, \"The Death of Captain Cook\" (engraving), 1784, and \"Portrait of Captain James Cook RN\", 1782); the Australian National Maritime Museum (\"View of Huaheine\", 1784); the Art Gallery of New South Wales (\"A View in Otaheite Peha\", 1785); the National Library of Australia (includes \n\"Sea Otter\", 1778, \"A Woman of Pulo Condore\", 1780, \"Portrait of Captain John Gore\", 1780, \"Poedua, Daughter of Orea, King of Ulietea, Society Islands\", 1782, \"Portrait of Captain James King\", 1782, \"A Dance in Otaheite\", 1784, \"A Woman of Van Diemen's Land\", 1784, \"A Chief of the Sandwich Islands\", 1787, and \"The Resolution Beating Through the Ice\", 1792,); the Dixson Library (includes \"An Opossum of Van Diemen's Land\", 1777, \"Red-tailed Tropic Bird\", 1777, and \"The Death of Captain Cook\", c.1781-83.); and the National Gallery of Victoria (\"A Night Dance by Men, in Hapaee\", 1784, \"A Young Woman of the Sandwich Islands\", 1784, and \"A Man of Van Diemen's Land\", 1784.).\n\n\n"}
{"id": "561564", "url": "https://en.wikipedia.org/wiki?curid=561564", "title": "Joss paper", "text": "Joss paper\n\nJoss paper (, , , or , also known as ghost money) are sheets of paper or papercrafts made into burnt offerings common in Chinese ancestral worship (such as the veneration of the deceased family members and relatives on holidays and special occasions). Joss paper, as well as other papier-mâché items, are also burned or buried in various Asian funerals, \"to ensure that the spirit of the deceased has lots of good things in the afterlife.\" In Taiwan alone, the annual revenue of temples received from burning ghost money was US$400 million (NT$13 billion) as of 2014.\n\nJoss paper is traditionally made from coarse bamboo paper, which feels handmade with many variances and imperfections, although rice paper is also commonly used. Traditional joss is cut into individual squares or rectangles. Depending on the region, Joss paper may be decorated with seals, stamps, pieces of contrasting paper, engraved designs or other motifs.\n\nDifferent types of spirit money are given to distinct categories of spirits. The three main types of spirit money are cash (also known as copper), silver and gold. Cash monies are given to newly deceased spirits and spirits of the unknown. Gold spirit money (jin) is given to both the deceased and higher gods such as the Jade Emperor. Silver spirit money (yin) is given exclusively to ancestral spirits as well as spirits of local deities. These distinctions between the three categories of spirit money must be followed precisely to avoid confusing or insulting the spirits.\nMore contemporary or westernized varieties of Joss paper include paper currency, credit cards, cheques, as well as papier-mâché clothes, houses, cars, toiletries, electronics and servants (together known as Mandarin \"zhǐzhā\" ). The designs on paper items vary from the very simple to very elaborate (with custom artwork and names).\n\nIn 2006, in response to the burning of \"messy sacrificial items\", such as paper cars, houses, and pills, Dou Yupei, China's deputy minister for civil affairs, announced that the ministry intended to ban at least the more extreme forms of joss paper.\n\nThe most well known joss paper item among Westerners is the Hell Bank Note. Much like the traditional gold and silver paper, Hell Bank Notes serve as the official currency for the afterlife. Living relatives offer them to dead ancestors by burning (or placing them in coffins in the case of funerals) the bank notes as a bribe to Yanluo for a shorter stay or to escape punishment, or for the ancestors themselves to use in spending on lavish items in the afterlife.\n\nThe word \"hell\" may have been derived from:\n\nHell Bank Notes are also known for their enormous denominations ranging from $10,000 to $5,000,000,000. The bills almost always feature an image of the Jade Emperor on the front and the \"headquarters\" of the Hell Bank on the back. Another common feature is the signatures of both the Jade Emperor and the lord of the Underworld, both of whom apparently also serve as the Hell bank's governor and deputy governor (as featured on the back).\n\nSpirit money is most often used for venerating those departed but has also been known to be used for other purposes such as a gift from a groom's family to the bride's ancestors. Spirit money has been said to have been given for the purpose of enabling their deceased family members to have all they will need or want in the afterlife. It has also been noted that these offerings have been given as a bribe to Yanluo to hold their ancestors for a shorter period of time.\nVenerating the ancestors is based on the belief that the spirits of the dead continue to dwell in the natural world and have the power to influence the fortune and fate of the living. The goal of ancestor worship is to ensure the ancestor's continued well-being and positive disposition towards the living and sometimes to ask for special favours or assistance. Rituals of ancestor worship most commonly consist of offerings to the deceased to provide for their welfare in the afterlife which is envisioned to be similar to the earthly life. The burning of spirit money enables the ancestor to purchase luxuries and necessities needed for a comfortable afterlife.\nMany temples have large furnaces outside the main gate to burn joss paper. Folding the paper is an important part of the burning ceremony as it distinguishes joss paper from actual money. Burning actual money would be untenable for most people, and is also considered unlucky in Asian cultures. The Joss paper may be folded into specific shapes which are meant to bring on good luck and people tend to burn lavish amounts to ensure that the offering is well received.\n\nEvery fifteen days business owners in Taiwan burn spirit money in red braziers and set out offering tables on the sidewalk for both gods and ghosts. This coincides with an ancient calendrical system divided into twenty-four fifteen-day periods.\n\nA simplified modern Chinese offering is made by drawing a circle with chalk on the sidewalk or the pavement between residential buildings and burning the paper offering within the circle. This is quite common in all Chinese cities and villages today.\n\nDue to environmental concerns, contemporary Joss paper burners are now fitted with a special cover which eliminates the spread of burning ashes. The cover allows enough oxygen in to ensure that all of the offering are completely burned.\n\nSpirit money is most commonly burned, but may also be offered by being held into the wind or placed into the deceased's coffin at funeral ceremonies.\n\nDepending on the type and status of the deity being worshiped, paper with metal foil or with ink seals of various sizes may be burned. Different regions of the world have different preferences for the type of Joss paper that is used. For instance, Hell Bank Notes are commonly found in regions where Cantonese populations dominate but are rarely seen or used in places such as Taiwan or Macau, which use \"gold paper\". The Joss paper is folded in half, or bought pre-folded into the shape of gold ingots before being burned in an earthenware pot or a specially built chimney. Joss paper burning is usually the last performed act in Chinese deity or ancestor worship ceremonies. The papers may also be folded and stacked into elaborate pagodas or lotuses.\n\nIn Taoist rituals, the practice of burning joss paper to deities or ancestors is acceptable. Some Buddhist groups, such as Kong Meng San Phor Kark See Temple in Singapore, have discouraged burning joss paper out of concern for the environment.\n\n\n"}
{"id": "52273414", "url": "https://en.wikipedia.org/wiki?curid=52273414", "title": "Kekerengu Fault", "text": "Kekerengu Fault\n\nThe Kekerengu Fault is an active dextral (right lateral) strike-slip fault in the northeastern part of South Island, New Zealand. It is closely associated with the Hope Fault and Jordan Thrust at its south-easternmost edge and likely joins with the Clarence Fault to form the Wairarapa Fault offshore in Cook Strait.\n\nEarly investigations immediately following the 14 November 2016 Kaikoura earthquake indicate that up to of motion may have occurred on the Kekerengu Fault during the 7.8 magnitude quake. During this earthquake the offshore continuation of the Kekerengu Fault to the north east, known as the Needles Fault, ruptured as well. NIWA marine geologist Dr Philip Barnes said the length of the Kekerengu–Needles Fault rupture may extend for about , consisting of on land and under the sea.\n"}
{"id": "2580900", "url": "https://en.wikipedia.org/wiki?curid=2580900", "title": "Liquefaction of gases", "text": "Liquefaction of gases\n\nLiquefaction of gases is physical conversion of a gas into a liquid state (condensation).\n\nThe processes are used for scientific, industrial and commercial purposes. Many gases can be put into a liquid state at normal atmospheric pressure by simple cooling; a few, such as carbon dioxide, require pressurization as well. Liquefaction is used for analyzing the fundamental properties of gas molecules (intermolecular forces), for storage of gases, for example: LPG, and in refrigeration and air conditioning. There the gas is liquefied in the \"condenser\", where the heat of vaporization is released, and evaporated in the \"evaporator,\" where the heat of vaporization is absorbed. Ammonia was the first such refrigerant, and is still in widespread use in industrial refrigeration, but it has largely been replaced by compounds derived from petroleum and halogens in residential and commercial applications. \n\nLiquid oxygen is provided to hospitals for conversion to gas for patients with breathing problems, and liquid nitrogen is used in the medical field for cryosurgery, and by inseminators to freeze semen. Liquefied chlorine is transported for eventual solution in water, after which it is used for water purification, sanitation of industrial waste, sewage and swimming pools, bleaching of pulp and textiles and manufacture of carbon tetrachloride, glycol and numerous other organic compounds as well as phosgene gas.\n\nLiquefaction of helium (He) with the precooled Hampson–Linde cycle led to a Nobel Prize for Heike Kamerlingh Onnes in 1913. At ambient pressure the boiling point of liquefied helium is . Below 2.17 K liquid He becomes a superfluid (Nobel Prize 1978, Pyotr Kapitsa) and shows characteristic properties such as heat conduction through second sound, zero viscosity and the fountain effect among others.\n\nThe liquefaction of gases is a complicated process that uses various compressions and expansions to achieve high pressures and very low temperatures, using, for example, turboexpanders. \n\nThe liquefaction of air is used to obtain nitrogen, oxygen, and argon and other atmospheric noble gases by separating the air components by fractional distillation in a cryogenic air separation unit.\n\nAir is liquefied by the Linde process, in which air is alternately compressed, cooled, and expanded, each expansion results in a considerable reduction in temperature. With the lower temperature the molecules move more slowly and occupy less space, so the air changes phase to become liquid.\n\nAir can also be liquefied by Claude's process in which the gas is allowed to expand isentropically twice in two chambers. While expanding, the gas has to do work as it is led through an expansion turbine. The gas is not yet liquid, since that would destroy the turbine. Commercial air liquefication plants bypass this problem by expanding the air at supercritical pressures. Final liquefaction takes place by isenthalpic expansion in a thermal expansion valve.\n\n"}
{"id": "31782155", "url": "https://en.wikipedia.org/wiki?curid=31782155", "title": "List of Amomum species", "text": "List of Amomum species\n\nThis list of \"Amomum\" species includes all recognized species in the \"Amomum\" plant genus.\n\n"}
{"id": "40840888", "url": "https://en.wikipedia.org/wiki?curid=40840888", "title": "List of Dungeons &amp; Dragons creatures (A)", "text": "List of Dungeons &amp; Dragons creatures (A)\n\nIn the \"Dungeons & Dragons\" fantasy tabletop role-playing game, the aarakocra ( are a race of bird-like monstrous humanoids dwelling in high mountains. First appearing in the \"Fiend Folio\" in 1981, they have since appeared in and been adapted to numerous campaign settings including \"Greyhawk\", \"Dragonlance\", \"Dark Sun\", and the \"Forgotten Realms\".\n\nAbishai ( ) are a fictional species of baatezu (devils) in the \"Dungeons & Dragons\" role-playing game.\n\nThe abishai were first introduced in the first edition \"AD&D\" article, \"From the Sorcerer's Scroll: New Denizens of Devildom\" by Gary Gygax in \"Dragon\" #75 (July 1983); the article detailed the black, blue, green, red, and white variants as species of lesser devil. The abishai later appeared in the first edition \"Monster Manual II\" (1983).\n\nThe black, green, and red abishai were re-categorized as lesser baatezu in second edition in the \"Monstrous Compendium Volume Outer Planes Appendix\" (1991), and next appear in the \"Monstrous Manual\" (1993). The same three variants are detailed as lesser baatezu for the Planescape setting in the first \"Planescape Monstrous Compendium Appendix\" (1994).\n\nThe black, blue, green, red, and white abishai appear in third edition of the Forgotten Realms setting in \"Monsters of Faerûn\" (2000). \"Fiendish Codex II: Tyrants of the Nine Hells\" (2006) includes the black, blue, green, red, and white abishai.\n\nThe abishai are a subgroup of Baatezu created through the joint efforts of Tiamat and Pearza of the Dark Eight. They are humanoid creatures that resemble gargoyles or humanoid dragons. There are five kinds, easily distinguishable by color (black, blue, green, red, and white). Most abishai are servitors of the dragon goddess Tiamat. They are the scouts, torturers, and wardens of the first two layers of Baator.\n\nRanked in power, the red abishai are the most powerful, followed by the blue, green, black, and white.\n\nIn the \"Forgotten Realms\" campaign setting for the \"Dungeons & Dragons\" fantasy role-playing game, the alaghi is a monstrous humanoid that lives in temperate mountains and forests. It is 6 feet tall and covered in shaggy white to brown hair. An alaghi is often neutral in alignment - however, villages of alaghi tend to be evil, while alaghi hermits tend to be nice and well faring creatures. Most alaghis are semi-nomadic hunter-gatherers.\n\nThe alaghi was introduced in the second edition in the \"Monstrous Compendium Forgotten Realms Appendix II\" (1991), which was later reprinted in \"Monstrous Compendium Annual Volume Three\" (1996). The alaghi is presented as a playable character race in \"The Complete Book of Humanoids\" (1993), and is later presented as a playable character race again in \"Player's Option: Skills & Powers\" (1995). The alaghi appeared in third edition in \"Monsters of Faerûn\" (2001).\n\nThe Ambush drake is a Dragon. Unlike traditional D&D Dragons, which are somewhat feline, Ambush drakes are lupine (wolf-like). Ambush drakes are short, squat and compact compared to normal Dragons, but still grow to be at least as large as an adult human. They have muscular limbs, and short spines on their necks and ugly heads. Their wings are disproportionate. Ambush drakes have grey bodies and back legs, with dark orangey-red heads, front legs and wings. They are far less intelligent than regular Dragons.\n\nAoa are fictional creatures in the \"Dungeons & Dragons\" role-playing game. Aoa resemble huge blobs of quicksilver that float above the surface of whatever environment they may be found. Their surface is like a mirror and reflects all light. Aoa are surrounded by tiny orbs that randomly separate from the sphere and reabsorb back into it. These outsiders are born from the friction caused as a result of the rare occurrences when the Negative Energy Plane and Positive Energy Plane graze each other.\n\nThe aoa droplet and the aoa sphere appeared in the third edition \"Fiend Folio\" (2003).\n\nAoa naturally reflect most attacks and spells. Since aoa reflect energy, scholars theorize that they may be a neutral counterpart to energons, such as the positive-energy xag-yas and negative-energy xeg-yis, which produce energy. Aoa can also release a pulse about three times per day that reflects magical energy back onto itself, which may destroy magical auras and shatter magic items.\n\nAoa are can be found floating around the Astral and Ethereal Planes, always seeking out large quantities of magic. They are most common at the borders of two or more planes where conflicting energies create magical maelstroms.\n\nAoa normally move slowly through the air, bobbing lazily. When they sense magic, they become excited or agitated and rush towards its source. When around spellcasting, an aoa will fly crazily around and try to intercept magical blasts and touch magical items. Aoa are sometimes summoned and used as guardians, kept content by low amounts of magic. Aoa do not appear to be very sentient; they do not speak nor seem to understand any languages.\n\nA full-sized aoa is called a sphere. Smaller aoa called droplets split off from a sphere when it reflects a large amount of magical energy. Eventually, a droplet will grow to become a full-sized sphere.\n\nIn the \"Dungeons & Dragons\" fantasy role-playing game, the aranea is a spider-like magical beast that lives in temperate forests. Its natural form is that of a spider of monstrous size, with two small humanlike arms below its mandibles.\n\nThe aranea first appeared in 1981 in the \"Dungeons & Dragons\" modules X1 \"Isle of Dread\", and X2 \"Castle Amber\". It description was later reprinted in AC9 \"Creature Catalogue\" (1986).\n\nThe aranea appeared in second edition \"Advanced Dungeons & Dragons\" in the \"Mystara Monstrous Compendium Appendix\" (1994), which was later reprinted in \"Monstrous Compendium Annual Three\" (1996).\n\nThe aranea appeared in the third edition in the \"Monster Manual\" (2000), and then in the revised 3.5 \"Monster Manual\" (2003). The aranea appeared as a player character race in the book \"Savage Species\" (2003), and later in \"Dragon\" #351 (January 2007).\n\nAn aranea is usually neutral in alignment. It has the ability to change its shape into that of a humanoid, or a spider-humanoid hybrid. It has the poison and webspinning ability of a spider, as well as the ability to cast spells like a sorcerer.\n\nThe aranea appeared in Paizo Publishing's book \"Pathfinder Roleplaying Game Bestiary 2\" (2010), on page 30.\n\nIn the \"Dungeons & Dragons\" fantasy role-playing game, the assassin vine is a dangerous plant which grows both in and underground.\n\nThe assassin vine first appeared in the third edition \"Monster Manual\" (2000), and then the 3.5 \"Monster Manual\".\n\nAn assassin vine is basically a tree which uses its vines to kill victims and deposit the bodies near the roots for fertilizer. It consists of a main vine attached to the tree, about 20 feet long, with smaller, 5 foot long vines breaking off. It has both leaves and berries. The berries have a bitter taste and are used to make a heady wine despite being widely believed to be poisonous. The underground version of the assassin vine is darker in coloration to the ground dwelling one.\n\nThe assassin vine appeared in Paizo Publishing's book \"Pathfinder Roleplaying Game Bestiary\" (2009), on page 22.\n\nAn azerblood is a type of planetouched that is the combination of a fire outsider known as an azer and dwarven blood, based in the campaign setting of the Forgotten Realms for \"Dungeons & Dragons\" fantasy role-playing game. As a planetouched, it is an outsider of the Native subtype with a collection of power based on its fiery bloodline as well as abilities based on its dwarven nature.\n\nBoth dwarves and azers pride law over chaos, but Azerbloods take after their outsider relatives in regard of good and evil by staying out of that conflict. So the common Azerblood is Lawful Neutral. Their favourite class is Fighter.\n\nThe azerblood appeared in the third edition for the Forgotten Realms campaign setting in \"Dragon\" #350 (December 2006).\n\nAzerblood resemble a dwarf of the shield dwarf subrace for the most part, with some variations based on the outsider blood that flows in their veins. Common traits are metallic brass-coloured skin, flame-red hair and irises that appear to move with flames. They favour the Azer way of dressing in metallic skirts of brass, bronze and copper. But they will dress for necessity, including heavier armours.\n\nAzerblood usually dwell in their own small communities or within larger communities of dwarves. They worship the Dwarven gods, especially Gorm Gulthyn and Dumathoin. They prefer coins and trade goods as treasure, most likely due to their dwarven natures. In combat, they are well known for team tactics and using their natural protections against fire to cast flaming magics.\n\nThe most common society of azerblood are those who live in the Small Teeth mountains of Amn, as they are descendants of Clan Azerkyn of the Adamant Kingdom of Xothaerin. Azerblood are also born to shield dwarves in locations near natural outlets to the Elemental Plane of Fire or places of great heat, like the Lake of Steam.\n"}
{"id": "6069300", "url": "https://en.wikipedia.org/wiki?curid=6069300", "title": "List of SSSIs in East Gwynedd", "text": "List of SSSIs in East Gwynedd\n\nSSSIs in the UK are notified using the concept of an Area of Search (AOS), an area of between and in size. The Areas of Search were conceived and developed between 1975 and 1979 by the Nature Conservancy Council (NCC), based on regions created by the Local Government Act 1972. Whereas England had its Areas of Search based on 46 counties, those in Wales were based on a combination of the counties and smaller districts. In 1974, Wales was divided into 8 counties, with 37 districts. The NCC created 12 Welsh Areas of Search; they mostly follow county borders, but the larger counties (Dyfed, Powys and Gwynedd) were divided into multiple Areas using district borders. Mid and South Glamorgan were merged into a single AOS, whilst Llanelli district was included in the West Glamorgan AOS.\n\nDue to subsequent local government reorganisation in the UK since 1972, many counties and districts have been divided, merged or renamed. Using the AOS system alone would make it difficult to search for individual SSSI citations via the Countryside Council for Wales (CCW) database without knowing 1972 region divisions. As a result, the CCW groups Welsh SSSIs using the subdivisions of Wales formed in April 1996 by the Local Government (Wales) Act 1994, resulting in 22 principal areas.\n\nEast Gwynedd AOS lies within the counties of Conwy and Gwynedd.\n\nFor SSSIs elsewhere in the UK, see List of SSSIs by Area of Search.\n\n"}
{"id": "6299420", "url": "https://en.wikipedia.org/wiki?curid=6299420", "title": "List of Storm Prediction Center high risk days", "text": "List of Storm Prediction Center high risk days\n\nA high risk severe weather event is the greatest threat level issued by the Storm Prediction Center (SPC) for convective weather events in the United States. High risks are issued only a few times a year when forecasters at the SPC are confident that a major severe weather outbreak, namely tornadoes and occasionally derechoes, will occur on the given day. These are typically reserved for the most extreme events.\n\nLimited details are available for days before the late 1990s, and it is probable that there were additional high risk days with no online documentation, especially in the 1980s.\n\nPrior to 1997, data on high risk events is relatively scarce due to a lack of online documentation by the Storm Prediction Center. Most of the listed events from 1984 to 1997 are constructed from case studies on certain outbreak, namely for those in North Carolina, as well as storm chaser accounts. During this time period, at least 42 high risk outlooks were issued for the United States.\n\nThere were no high risk days in 2000.\n\nThere were no high risk days (for two consecutive years) in 2015 or 2016. The 31 months between high risk days in June 2014 and January 2017 is the longest since at least the 1980s.\n\n"}
{"id": "46911919", "url": "https://en.wikipedia.org/wiki?curid=46911919", "title": "List of Tricholoma species", "text": "List of Tricholoma species\n\nThis is a list of species in the agaric genus \"Tricholoma\". , Index Fungorum lists 353 species in the genus.\nA B C D E F G H I J K L M N O P Q R S T U V U W X Y Z\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "49711392", "url": "https://en.wikipedia.org/wiki?curid=49711392", "title": "List of cities by sunshine duration", "text": "List of cities by sunshine duration\n\nThe following is a list of cities by sunshine duration. Sunshine duration is a climatological indicator, measuring duration of sunshine in given period (usually, a day or a year) for a given location on Earth, typically expressed as an averaged value over several years. It is a general indicator of cloudiness of a location, and thus differs from insolation, which measures the total energy delivered by sunlight over a given period.\n\nSunshine duration is usually expressed in hours per year, or in (average) hours per day. The first measure indicates the general sunniness of a location compared with other places, while the latter allows for comparison of sunshine in various seasons in the same location. Another often-used measure is percentage ratio of recorded bright sunshine duration and daylight duration in the observed period.\n\nAn important use of sunshine duration data is to characterize the climate of sites, especially of health resorts. This also takes into account the psychological effect of strong solar light on human well-being. It is often used to promote tourist destinations.\n\n"}
{"id": "55566492", "url": "https://en.wikipedia.org/wiki?curid=55566492", "title": "List of ecoregions in Albania", "text": "List of ecoregions in Albania\n\nTemperate broadleaf and mixed forests\n\nMediterranean forests, woodlands, and shrub\n"}
{"id": "26431931", "url": "https://en.wikipedia.org/wiki?curid=26431931", "title": "List of the major 4000-meter summits of the United States", "text": "List of the major 4000-meter summits of the United States\n\nThe following sortable table comprises the 104 mountain peaks of the United States with at least of topographic elevation and at least of topographic prominence.\n\nThe summit of a mountain or hill may be measured in three principal ways:\n\nIn the United States, only Denali exceeds elevation. Four major summits exceed , nine exceed , the following 104 major summits exceed , 220 exceed , and 302 exceed elevation.\n\nOf these 104 major 4000-meter summits of the United States, 55 are located in Colorado, 23 in Alaska, 16 in California, five in Wyoming, two in Hawaii, and one each in Washington, Utah, and New Mexico. Five of these summits are located on the international border between Alaska and Yukon, and one is located on the international border between Alaska and British Columbia. The ten highest major summits of the United States are all located in Alaska.\n\n\n"}
{"id": "9994297", "url": "https://en.wikipedia.org/wiki?curid=9994297", "title": "Load balancing (electrical power)", "text": "Load balancing (electrical power)\n\nLoad balancing, load matching, or daily peak demand reserve refers to the use of various techniques by electrical power stations to store excess electrical power during low demand periods for release as demand rises. The goal would be for the power supply system to see a load factor of 1.\n\nGrid energy storage stores electricity within the transmission grid beyond the customer. Alternatively, the storage can be distributed and involve the customer, for example in storage heaters running demand-response tariffs such as the United Kingdom's Economy 7, or in a vehicle-to-grid system to use storage from electric vehicles during peak times and then replenish it during off peak times. These require incentives for consumers to participate, usually by offering cheaper rates for off peak electricity.\n\nTelephone exchanges often have arrays of batteries in their basements to power equipment and in the past metro systems such as the London Underground had their own power stations, not only giving some redundancy but also using the grid for load balancing. Today these supplies often have been replaced by direct supply from the grid and so are no longer available for the purpose of load balancing.\n\nSolutions to the load balancing problem focus on \"smart grid\" technology, in which many consumer and industrial appliances would communicate with the utility using digital means, and could be switched on and off by the utility to run at off-peak hours. \n\nIn a very basic demand balancing system, the power company sends a signal down the line or by a dedicated phone chip to turn on a special circuit in the home. Typically, a storage device for space heating or a water heater will be connected to this circuit. The electricity is turned on after the evening peak demand, and turned off in the morning before the morning peak demand starts. The cost for such power is less than the \"on-demand\" power which makes it worthwhile for the user to subscribe to it. \n\nA nuanced system is possible with benefits for the power company and the electricity user. Once home devices contain the appropriate electronics, it will no longer be necessary to have devices connected to a special circuit. The power company can send a signal saying that power is now available at a better rate and this signal will turn on any device (dish washer for instance) that has the dial set for \"when available\" power (priority 2). Manufacturers can provide priority settings on their machines and the power company sending a number of signals as they need more demand to balance supply, or set the machine for lower priority to use lower cost energy. An electric car might even have a setting for \"charge and supply\"; charging when electricity is least expensive and returning energy when it is most expensive. \n\nThe power company benefits by selling more energy; consumer devices can receive signals via the internet when excess power is available, or when it is more expensive. Demand Side Response lessens the need to run expensive \"peaking capacity\" power stations when there is a high demand for power, and can encourage use when surplus electricity is available. \n\n"}
{"id": "2795967", "url": "https://en.wikipedia.org/wiki?curid=2795967", "title": "Maastrichtian", "text": "Maastrichtian\n\nThe Maastrichtian () is, in the ICS geologic timescale, the latest age (uppermost stage) of the Late Cretaceous epoch or Upper Cretaceous series, the Cretaceous period or system, and of the Mesozoic era or erathem. It spanned the interval from . The Maastrichtian was preceded by the Campanian and succeeded by the Danian (part of the Paleogene and Paleocene).\n\nAt the end of this period, there was a mass extinction known as the Cretaceous–Paleogene extinction event, (formerly known as the Cretaceous–Tertiary extinction event). At this extinction event, many commonly recognized groups such as non-avian dinosaurs, plesiosaurs and mosasaurs, as well as many other lesser-known groups, died out. The cause of the extinction is most commonly linked to an asteroid about wide colliding with Earth at the end of the Cretaceous.\n\nThe Maastrichtian was introduced into scientific literature by Belgian geologist André Hubert Dumont in 1849, after studying rock strata of the Chalk Group close to the Dutch city of Maastricht. These strata are now classified as the Maastricht Formation - both formation and stage derive their names from the city. The Maastricht Formation is known for its fossils from this age, most notably those of the giant sea reptile \"Mosasaurus\", which in turn also derives its name from the Dutch city (\"mosa\" being Latin for the river Maas).\n\nThe base of the Maastrichtian stage is at the first appearance of ammonite species \"Pachydiscus neubergicus\". At the original type locality near Maastricht, the stratigraphic record was later found to be incomplete. A reference profile for the base was then appointed in a section along the Ardour river called \"Grande Carrière\", close to the village of Tercis-les-Bains in southwestern France. The top of the Maastrichtian stage is defined to be at the iridium anomaly at the Cretaceous–Paleogene boundary (K–Pg boundary), which is also characterised by the extinction of many groups of life, such as certain foraminifers and calcareous nanoplankton, all ammonites and belemnites, etc.\n\nThe Maastrichtian is commonly subdivided into two substages (Upper and Lower) and three ammonite biozones. The biozones are (from young to old):\n\nThe Maastrichtian is roughly coeval with the Lancian North American Land Mammal Age.\n\nThe following are summaries of the characteristics of specific Maastrichtian aged formations.\n\nThe Bearpaw Formation, also called the Bearpaw Shale, is a sedimentary rock formation found in northwestern North America. It is exposed in the U.S. state of Montana, as well as the Canadian provinces of Alberta and Saskatchewan, east of the Rocky Mountains. It overlies the older Two Medicine, Judith River and Dinosaur Park Formations, and is in turn overlain by the Horseshoe Canyon Formation in Canada and the Fox Hills Sandstone in Montana. To the east and south it blends into the Pierre Shale.\n\nA marine formation composed mostly of shale, it represents the last major expansion of the Western Interior Seaway before it completely receded from northwestern North America by the end of the Cretaceous Period. It includes well-preserved ammonite fossils. Other fossils found in this formation include many types of shellfish, bony fish, sharks, rays, birds, and marine reptiles like mosasaurs, plesiosaurs and sea turtles. The occasional dinosaur remains have also been discovered, presumably from carcasses washed out to sea.\nThe Hell Creek Formation is an intensely studied division of Upper Cretaceous to lower Paleocene rocks in North America, named for exposures studied along Hell Creek, near Jordan, Montana. The Hell Creek Formation occurs in badlands of eastern Montana and portions of North Dakota, South Dakota, and Wyoming. In Montana, the Hell Creek Formation overlies the Fox Hills Formation and is the uppermost formation of the Cretaceous period.\n\nIt is a series of fresh and brackish-water clays, mudstones, and sandstones deposited during the Maastrichtian, the last part of the Cretaceous period, by fluvial activity in fluctuating river channels and deltas and very occasional peaty swamp deposits along the low-lying eastern continental margin fronting the late Cretaceous Western Interior Seaway. The climate was mild. The iridium-enriched Cretaceous–Paleogene boundary, which separates the Cretaceous from the Cenozoic, occurs as a discontinuous but distinct thin marker bedding within the Formation, near its uppermost strata.\n\nThe Horseshoe Canyon Formation is part of the Edmonton Group and is up to 230 m in depth. It is Late Campanian to Early Maastrichtian in age (Edmontonian Land Mammal Age) and is composed of mudstone, sandstone, and carbonaceous shales. There are a variety of environments, which have yielded a diversity of fossil material. The Horseshoe Canyon Formation outcrops extensively in the area of Drumheller, Alberta, as well as further north along the Red Deer River near Trochu, and also in the city of Edmonton.\n\nThe Sarir field was discovered in southern Cyrenaica during 1961 and is considered to be the largest oil field in Libya, with estimated oil reserves of . Sarir is operated by the Arabian Gulf Oil Company (AGOCO), a subsidiary of the state-owned National Oil Corporation (NOC).\n\nThe Sarir stratigraphic column generally resembles succession patterns throughout the Sirte Basin, with some variations. In the early regressive phase, basal sandstones were deposited on a Precambrian basement of igneous and metamorphic rocks. Sandstones are dated on angiosperm pollen as younger than Albian, probably from the Late Cretaceous. After a lengthy hiatus, represented by unconformity and sandstone erosion, a transgressive sequence of red, green, and purple Anhydrite shales was laid. Variegated bed remnants occur in crestal sections of many northern structures, such as in wells B-1-65 and C-1-80.\n\nAbove the transgressive sequence are Late Cretaceous shales with tight, micritic carbonate, marking the top of the Mesozoic. These shales thicken into troughs, providing the field's sole source rock. The youngest fauna are Maastrichtian, with an apparent disconformity between the Late Cretaceous and Paleocene, marked by high levels of gamma radiation on logs.\n\nThe Berivotra Formation is an Upper Cretaceous (Maastrichtian) sedimentary rock marine formation found in Madagascar, that is contemporary to the terrestrial Maevarano Formation.\n\nThe Maevarano Formation is an Upper Cretaceous sedimentary rock formation found in the Mahajanga Province of northwestern Madagascar. It is most likely Maastrichtian in age, and records a seasonal, semiarid environment with rivers that had greatly varying discharges. Notable animal fossils recovered include the theropod dinosaur \"Majungasaurus\" and the early birds \"Rahonavis\" and \"Vorona\", and the titanosaurian sauropod \"Rapetosaurus\".\n\nThe Lameta Formation is a sedimentary rock formation found in Madhya Pradesh, Gujarat, and Maharashtra, India. It is of Maastrichtian age (Upper Cretaceous), and is notable for its dinosaur fossils. Many dubious names have been created for isolated bones, but several genera of dinosaurs from these rocks are well-supported, including the titanosaur sauropod \"Isisaurus\", the abelisaurs \"Indosaurus\", \"Indosuchus\", \"Laevisuchus\", and \"Rajasaurus\" and possible stegosaurs.\n\nThe Barun Goyot Formation, dating from the Late Cretaceous Period, is located within and is widely represented in the Gobi Desert basin, in the Ömnögovi Province of Mongolia.\n\nIt was previously known as the \"Lower Nemegt Beds\", occurring beneath the Nemegt Formation and above the Djadokhta Formation. It is approximately in thickness and was laid down roughly 80-71 mya, making it uncertain whether it should be classed as Campanian or Maastrichtian, although the latter is less likely. The Barun Goyot Formation preserves an environment of sand dunes, created from wind-eroded rocks (aeolian dunes).\n\nThe Nemegt Formation is a Late Cretaceous sedimentary formation in the Gobi Desert of Mongolia. It overlies, and in some places interdigitates with, the Barun Goyot Formation. It consists of river channel sediments and contains fossils of fish, turtles, crocodilians, birds and a diverse fauna of dinosaurs. The climate was wetter than when the preceding formations were deposited; there seems to have existed at least some degree of forest cover.\n\nThe absolute age of the Nemegt Formation is unknown but it is thought to be Maastrichtian or maybe late Campanian in age, very roughly some 76-65 million years old. The presence of \"Saurolophus\", a taxon also known from the Horseshoe Canyon Formation of Alberta, may indicate an Early Maastrichtian age, about 70 million years old.\n\nSeveral lineages of modern birds existed, though many were of lineages that differed profoundly from their relatives of today and thus are not their ancestors, but became extinct in the Cenozoic. For example, \"Vegavis iaai\" indicates that at least four lineages of Anseriformes were distinct: Anhimidae, Anatidae, Anseranatidae and Presbyornithidae (and possibly Dromornithidae), and that there was at least one Galliformes lineage too, and apparently the Gastornithiformes were also distinct. It is also highly likely that there was at least one, and probably several, lineages of Charadriiformes, that penguins were about to split from their closest relatives or had already done so, and that if the Metaves are a valid group they would presumably have been distinct too. Also, paleognaths were of course long distinct from neognaths.\n\nTraditionally, pterosaur faunas of the Maastrichtian were assumed to be dominated by azhdarchids, with other pterosaur groups having become extinct earlier on. However, more recent findings suggest a fairly composite pterosaur diversity: at least six (\"Nyctosaurus\" \"lamegoi\", a Mexican humerus, a Jordan humerus and several taxa from Morocco) nyctosaurs date to this period, as do a few pteranodontids, and \"Navajodactylus\", tentatively assigned to Azhdarchidae, lacks any synapomorphies of the group. This seems to underscore a higher diversity of terminal Cretaceous pterosaurs than previously thought.\n\n\n\n"}
{"id": "53452581", "url": "https://en.wikipedia.org/wiki?curid=53452581", "title": "Melanocratic mineral", "text": "Melanocratic mineral\n\nA melanocratic mineral contains at least 60% dark and heavy ferro-magnesium minerals.<ref name=\"http://thesciencedictionary.org/melanocratic/\">What is Melanocratic? definition of Melanocratic (Science Dictionary): dark and heavy ferro-magnesian mineral, accessdate: March 10, 2017</ref>\n"}
{"id": "29007502", "url": "https://en.wikipedia.org/wiki?curid=29007502", "title": "Mesoamerican flood myths", "text": "Mesoamerican flood myths\n\nA large number of Mesoamerican flood myths have been documented in written form or passed down through in oral tradition. Some clearly have Christian influences, but others are believed by scholars to represent native flood myths of pre-Columbian origin. \n\nOne myth documented among the Tlapanec and Huaxtecs has a man and his dog as the sole survivors of the deluge, but the man finds out that the dog takes the shape of a woman during the day when he is away. The man and the dogwoman then repopulate the earth. Another myth found among the Aztec and Totonac peoples relates how a human couple survive by hiding in a hollow vessel and start to cook a fish when the water subsides. When the smoke reaches the heavens the gods become angry and punish them by turning them into dogs or monkeys depending on the version.\n\nIn Maya mythology as expressed in the Popol Vuh the creator gods attempted to create creatures who would worship them three times before finally succeeding in creating a race of humans that would pay proper homage to their creators. The three previous creations were destroyed. The third race of humans carved from wood were destroyed by a flood, mauled by wild animals and smashed by their own tools and utensils. Maya flood myths recorded by Diego de Landa and in the Chilam Balam of Chumayel holds that the only survivors of the flood were the four Bacabs who took their places as upholders of the four corners of the sky.\n\nIn Mesoamerican myth a variety of reasons are given for the occurrence of the flood: either the world was simply very old and needed to be renewed; the humans had neglected their duty to adore the gods; or they were punished for a transgression (cannibalism, for example). Many of the modern myths included obviously Christian references such as the murder of Abel by Cain as the reason. In Mesoamerican myth the flood was but one of several destructions of the creation — usually the first of three or four cataclysmic events, although there is some evidence that the Aztecs considered the flood to be the fourth. \n\nIn many Mesoamerican flood myths, especially recorded among the Nahua (Aztec), peoples tell that there were no survivors of the flood and creation had to start from scratch, while other accounts relate that current humans are descended from a small number of survivors. In some accounts the survivors transgress against the gods by lighting a fire and consequently are turned into animals. Horcasitas acknowledges that the dog-wife tale and the tale of transgression by fire and subsequent turning into animals of the flood survivors may be of pre-Columbian origin.\n"}
{"id": "28775600", "url": "https://en.wikipedia.org/wiki?curid=28775600", "title": "Microviscosity", "text": "Microviscosity\n\nMicroviscosity, also known as microscopic viscosity, is the friction experienced by a single particle undergoing diffusion because of its interaction with its environment at the micrometer length scale. The concept of microviscosity is intimately related to the concept of single particle diffusion and can be measured using microrheology. \n\nUnderstanding microviscosity requires an understanding of viscosity and diffusion i.e. macroscopic viscosity and bulk diffusion and where their assumptions break down at the micro to nanometer scale where physicists are still trying to replace phenomenological laws with physical laws governing the behavior of single particle mobility. \n\nIn the field of biophysics, a typical microviscosity problem is understanding how a biomolecule's mobility is hindered within a cellular compartment which will depend upon many factors such as the size, shape, charge, quantity and density of both the diffusing particle and all members of its environment. \n\nMicroviscosity can be probed by measuring the rotational correlation time of a probe molecule using either fluorescence correlation spectroscopy or the linewidths of the probe's electron spin resonance. The friction experienced by a single particle can be thought of as a microscopic viscosity (microviscosity) and should not necessarily agree with the bulk viscosity since it is a measure of the probe's local friction whereas bulk viscosity analogously would be the measure of an infinitely large probe. Both the crowding density and relative size of each co-solute in a mixture will contribute to the measured microviscosity as assessed by altered translational mobility.\n"}
{"id": "54173890", "url": "https://en.wikipedia.org/wiki?curid=54173890", "title": "Moca Cream", "text": "Moca Cream\n\nMoca Cream is a brand name for a type of limestone from Portugal. It has a beige homogeneous color and, if cut against the vein, a distinct parallel vein. When cut in the direction of the vein, the uniform background is highlighted with some shaded areas instead of the vein. It is suited for masonry, decoration, internal and external cladding and moderate traffic paving. There several varieties, including Moca Cream Fine Grain, Moca Cream Medium Grain, Moca Cream Gross Grain, Moca Cream Regular Cut and Moca Cream St. Hubert.\nThis stone can be used in projects that require large quantities because it is a stone that exists in abundance and it is explored by numerous quarries. On the other hand, it is an extremely versatile stone, can be used in flooring and facing slabs, for interior and exterior pavings, cladding, masonry, stonework, and many others.\n\nThe Moca Cream is extracted in Portugal, next to a small town called Alcanede. The city is located in the center of the country and it is in the center of “Serra de Aire e Candeiros”, which is the most important limestone extraction area in Portugal. The quarries of this stone are approximately 1 hour away from the capital of the country, Lisbon.\n\nMoca Cream is one of the most popular limestone stones in the world, mainly because of its vein and color that giving it a very elegant look. This stone is mainly used in exterior and interior claddings, coverings, and walls. The ease of choosing grain frequency increases the variety of uses and combinations. The uses in stonework for interior decoration and masonry are popular too. Moca Cream is also used in flooring, fireplaces, bathtubs, balusters and columns, garden decoration and many others.\n\nDue to the popularity of this stone in the Chinese market, when you search for Moca Cream is possible to find suppliers from China. The stone is only extracted in Portugal, but the stone export from Portugal to China was huge and so they have reserves of high quantities such as Portugal, however, you can find more stone variants in Portugal.\n"}
{"id": "57780274", "url": "https://en.wikipedia.org/wiki?curid=57780274", "title": "NOAAS Okeanos Explorer Gulf of Mexico 2018 Expedition", "text": "NOAAS Okeanos Explorer Gulf of Mexico 2018 Expedition\n\nThis expedition was the final of three expeditions on the NOAAS Okeanos Explorer intended to increase the understanding of the deep-sea environment in the Gulf of Mexico. Gulf of Mexico 2018 was a 23-day telepresence-enabled expedition to collect critical information and acquire data on priority exploration areas identified by the ocean management and scientific communities. The goal of the expedition was to use remotely operated vehicle (ROV) dives in combination with seafloor mapping operations to increase the understanding of deep-sea ecosystems and collect scientific information to support future management decisions.\n\n15 ROV dives were conducted ranging in depth from 305 to 3,010 meters to explore the diversity and distribution of deep-sea habitats and associated marine communities in the Gulf of Mexico basin. Operations were focused on characterizing deep-sea coral and sponge communities, bottomfish habitats, submarine canyons, shipwrecks, and chemosynthetic habitats such as brine pools, gas seeps, and mud volcanoes. Midwater exploration at depths ranging from 900 to 300 meters were also conducted during two dives to investigate the diversity and abundance of the largely unknown pelagic fauna.\n\nDuring these dives hundreds of different species of animals, including several potential new species, new behaviors, and numerous significant range extensions were observed. 67 Biological samples (22 primary and 45 associated and commensal taxa) were collected. 13 of the biological samples represented substantial range extensions, and several were new species to science. 12 rock samples were collected for geochemical composition analysis and age-dating. During dives 6 and 7 two new chemosynthetic communities were documented including a brine pool and extinct brine waterfall at Hidalgo Basin and gas seeps at Walker Ridge 488.\nDuring the first dive the shipwreck of the tug boat New Hope was explored for the first time and a 3D model of the site was created. The following dive explored an unidentified wooden vessel, a 3D model of the site was created as well.\n\nA smaller objective of the dives was to document the extent of deep sea debris in the area. Trash was found 1,600 meters deep and over 275 kilometers off shore.\n\nThe live video feeds of the expedition were shared publicly worldwide with the live video receiving more than 300,700 views via the NOAA Office of Ocean Exploration and Research (OER) YouTube channel. 85 Scientist, managers, and students from 35 institutions in the United States, Japan, Russia, Norway, United Kingdom, and Canada participated as members of the science team through telepresence. The expedition also conducted 16 live telepresence interactions with various groups engaging more than 400 individuals including the Exploratorium, National Aquarium, Boston College, Hawaii Pacific University, London Natural History Museum, and many more.\n\n"}
{"id": "23931489", "url": "https://en.wikipedia.org/wiki?curid=23931489", "title": "No load power", "text": "No load power\n\nNo load power consumption is the electric power consumed by a power charger when it is plugged into a wall socket but without the mobile phone or other handheld device being connected. Furthermore, it must be switched on at the plugs.\n\nThis wasted power is often incorrectly called standby power, which actually refers to the power consumed by a power adapter when a device such as a TV, DVD recorder or audio system is in a low-power mode. The power consumed in standby mode is used to maintain system functions such as responding to remote control, or a digital clock, whereas true no-load energy as consumed in a power charger does nothing of use.\n\nThe no-load power contribution to a country’s household energy demands is thought to be considerable. The average number of handsets per capita is frequently more than one in many countries , the proportion of chargers left accidentally or deliberately plugged in is thought to be significant and with current designs over 60% of a mobile phone’s energy use is wasted as no-load .\n\nAs a result, mobile phone manufacturers have become increasingly focused on reducing the no-load consumption of their power chargers. For example, in 2002, a typical design might use about 3 watts on average and in 2007 less than 0.5 watts.\n\nFor the purposes of estimating consumption, no-load and standby power are often lumped together, and it is difficult to distinguish them in statistics. Estimates of the magnitude are discussed with numerical detail in the article on standby power. Before energy-wastage initiatives were implemented, it was estimated that power used by devices plugged in but not in active use was about 10% of domestic electric energy power consumption, the small magnitude of the power drawn being offset by 24-hour drain.\n\nEnergy Star, the EU code of conduct on standby and other mandatory and regulatory standards are encouraging manufacturers to make further reductions in no-load energy demands.\n\nFor example: under the Energy Star V2.0 (level V) voluntary standard introduced in November 2008, the no-load consumption of a typical 4.25 W charger has to be less than 0.3 W. The EU code of conduct version 4, introduced in January 2009, has a limit of 0.25 W for mobile handheld battery-driven applications, reducing to 0.15 W in January 2011. Complete list of national and international standards.\n\nIn November 2008, the world’s five largest mobile phone makers – Nokia, Samsung, LG Electronics, Motorola and Sony Ericsson - announced their own five star energy rating system to help consumers more easily identify the most energy-efficient chargers. Chargers are being labeled with no stars for no-load consumptions exceeding 0.5 W, up to five stars for consumption of not more than 0.03 W.\n"}
{"id": "998070", "url": "https://en.wikipedia.org/wiki?curid=998070", "title": "Node (physics)", "text": "Node (physics)\n\nA node is a point along a standing wave where the wave has minimum amplitude. For instance, in a vibrating guitar string, the ends of the string are nodes. By changing the position of the end node through frets, the guitarist changes the effective length of the vibrating string and thereby the note played. The opposite of a node is an anti-node, a point where the amplitude of the standing wave is a maximum. These occur midway between the nodes.\n\nStanding waves result when two sinusoidal wave trains of the same frequency are moving in opposite directions in the same space and interfere with each other. They occur when waves are reflected at a boundary, such as sound waves reflected from a wall or electromagnetic waves reflected from the end of a transmission line, and particularly when waves are confined in a resonator at resonance, bouncing back and forth between two boundaries, such as in an organ pipe or guitar string.\n\nIn a standing wave the nodes are a series of locations at equally spaced intervals where the wave amplitude (motion) is zero (see animation above). At these points the two waves add with opposite phase and cancel each other out. They occur at intervals of half a wavelength (λ/2). Midway between each pair of nodes are locations where the amplitude is maximum. These are called the antinodes. At these points the two waves add with the same phase and reinforce each other.\n\nIn cases where the two opposite wave trains are not the same amplitude, they do not cancel perfectly, so the amplitude of the standing wave at the nodes is not zero but merely a minimum. This occurs when the reflection at the boundary is imperfect. This is indicated by a finite standing wave ratio (SWR), the ratio of the amplitude of the wave at the antinode to the amplitude at the node.\n\nIn resonance of a two dimensional surface or membrane, such as a drumhead or vibrating metal plate, the nodes become nodal lines, lines on the surface where the surface is motionless, dividing the surface into separate regions vibrating with opposite phase. These can be made visible by sprinkling sand on the surface, and the intricate patterns of lines resulting are called Chladni figures.\n\nIn transmission lines a voltage node is a current antinode, and a voltage antinode is a current node.\n\nNodes are the points of zero displacement, not the points where two constituent waves intersect.\n\nWhere the nodes occur in relation to the boundary reflecting the waves depends on the end conditions or boundary conditions. Although there are many types of end conditions, the ends of resonators are usually one of two types that cause total reflection:\n\n\n\nA sound wave consists of alternating cycles of compression and expansion of the wave medium. During compression, the molecules of the medium are forced together, resulting in the increased pressure and density. During expansion the molecules are forced apart, resulting in the decreased pressure and density.\n\nThe number of nodes in a specified length is directly proportional to the frequency of the wave.\n\nOccasionally on a guitar, violin, or other stringed instrument, nodes are used to create harmonics. When the finger is placed on top of the string at a certain point, but does not push the string all the way down to the fretboard, a third node is created (in addition to the bridge and nut) and a harmonic is sounded. During normal play when the frets are used, the harmonics are always present, although they are quieter. With the artificial node method, the overtone is louder and the fundamental tone is quieter. If the finger is placed at the midpoint of the string, the first overtone is heard, which is an octave above the fundamental note which would be played, had the harmonic not been sounded. When two additional nodes divide the string into thirds, this creates an octave and a perfect fifth (twelfth). When three additional nodes divide the string into quarters, this creates a double octave. When four additional nodes divide the string into fifths, this creates a double-octave and a major third (17th). The octave, major third and perfect fifth are the three notes present in a major chord.\n\nThe characteristic sound that allows the listener to identify a particular instrument is largely due to the relative magnitude of the harmonics created by the instrument.\n\nIn chemistry, quantum mechanical waves, or \"orbitals\", are used to describe the wave-like properties of electrons. Many of these quantum waves have nodes and antinodes as well. The number and position of these nodes and antinodes give rise to many of the properties of an atom or covalent bond. Atomic orbitals are classified according to the number of radial and angular nodes, while molecular orbitals are classified according to bonding character. Molecular orbitals with an antinode between nuclei are very stable, and are known as \"bonding orbitals\" which strengthen the bond. In contrast, molecular orbitals with a node between nuclei will not be stable due to electrostatic repulsion and are known as \"anti-bonding orbitals\" which weaken the bond. Another such quantum mechanical concept is the particle in a box where the number of nodes of the wavefunction can help determine the quantum energy state—zero nodes corresponds to the ground state, one node corresponds to the 1st excited state, etc. In general, \"If one arranges the eigenstates in the order of increasing energies, formula_1, the eigenfunctions likewise fall in the order of increasing number of nodes; the \"n\"th eigenfunction has \"n−1\" nodes, between each of which the following eigenfunctions have at least one node\".\n"}
{"id": "58232256", "url": "https://en.wikipedia.org/wiki?curid=58232256", "title": "Noelle Selin", "text": "Noelle Selin\n\nNoelle Eckley Selin is an atmospheric chemist and Associate Professor at Massachusetts Institute of Technology in the Institute for Data, Systems and Society and the Department of Earth, Atmospheric and Planetary Sciences.\n\nSelin received her Bachelor of Arts in Environmental Science and Public Policy and her Master of Arts in Earth and Planetary Sciences at Harvard University in 2000. Following graduation, Selin became a Fulbright Fellow, working as a visiting researcher in Copenhagen at the European Environment Agency. There, she studied ways to improve scientific assessments of chemicals and their environmental impacts.\n\nFollowing her Fulbright Fellowship, Selin returned to Harvard to receive her PhD in Earth and Planetary Sciences in 2007. There, she worked in the Atmospheric Chemistry Modeling Group with Daniel J. Jacob to understand how mercury cycles through the atmosphere, across land, and in water using a global 3-D chemical transport model. Her research extended to the politics and policy underlying mercury pollution, authoring articles in law and governance publications. Her graduate work was supported by a National Science Foundation Graduate Research Fellowship award, as well as a United States Environmental Protection Agency Science to Achieve Results Graduate Research Fellowship.\n\nIn 2007, Selin became a postdoctoral fellow at the Massachusetts Institute of Technology in the Center for Global Change Science and Joint Program on the Science and Policy of Global Change. Her research centered on atmospheric pollution and human health impacts, as well as continuing to focus on global efforts to regulate hazardous chemicals.\n\nIn 2010, Selin was appointed as an Assistant Professor at MIT in the Engineering Systems Division and Department of Earth, Atmospheric and Planetary Sciences, and was promoted to Associate Professor in 2015. She is also affiliated with the MIT Joint Program on the Science and Policy of Global Change and the MIT Center for Environmental Sciences. Her research centers on using atmospheric chemistry modeling to understand how atmospheric pollutants circulate and interact with the global environment. Her group has studied the financial and health benefits of reducing carbon emissions, finding that improving air quality led to reduced risk of health problems. The financial savings from avoiding health problems — and in turn avoiding the cost of medical care and reducing sick days — could recoup up to 10.5 times the cost of implementing a cap-and-trade program. The study, published in 2014, was the most detailed assessment of the effects of climate policy on the economy, air pollution, and human health. Her group has also found that global regulations on mercury pollution have a major economic benefit to the United States. Mercury is a major contaminant in the seafood market, and consumption leads to increased risk of cardiovascular disease and cognitive impairments. Decreasing the risk of mercury consumption through global policies to regulate mercury pollution can therefore have a large economic benefit by, for instance, saving individuals the cost of medical care over the course of their lifetime. Another study, also published in 2016, calculated the costs of IQ loss from lead emissions from aviation and won the award for Best Environmental Policy Paper from the journal Environmental Science & Technology.\n\nSelin has also worked to ensure that her research findings — and those of the greater scientific community — are employed to better inform policy around air pollution, climate change, and hazardous substances like mercury. In 2016, she became a Leshner Leadership Institute Public Engagement Fellow through the AAAS and began working with the newly formed MIT International Policy Lab, which works to connect scientists with the societal impacts of their work. She has published on the need to build policy literacy for climate scientists to close the gap between science and society and implement policies that mitigate the effects of climate change.\n\n"}
{"id": "31932415", "url": "https://en.wikipedia.org/wiki?curid=31932415", "title": "Penzance Natural History and Antiquarian Society", "text": "Penzance Natural History and Antiquarian Society\n\nPenzance Natural History and Antiquarian Society (1839–1961) was a local society founded in Penzance in Cornwall, England, UK, whose aim was \"the cultivation of the science of Natural History, and for the investigation of the Antiquities referring to the early inhabitants.\"\n\nThe Society was established at a public meeting on 20 November 1839, with an annual subscription of 10 shillings or life membership of £5. The society held meetings, where lectures were delivered and discussed, held an annual excursion to visit the antiquities of the area, and maintained a museum. Despite the initial enthusiasm, very little happened as the Secretaries and Curators' Report at the 1845 AGM concluded with \"The society having somewhat revived from the torpor which has for some time hung over it, the council hopes that its increased energy will be continued without further interruption\". The first volume of \"Reports and Transactions\" was finally published in 1851, covering the years 1845–51. Transactions continued with the publication of volume two in 1864 (covering the years 1851–55) and volume three in 1865 (the years 1862–65). The society then lay dormant apart from the annual excursions which continued until 1872 when it became defunct.\n\nFifty-three members joined the society after it was revived in June 1880 following a public meeting on 8 May. A petition, dated 26 April, had been made to the Mayor of Penzance, Charles Campbell Ross following concern that the museum had accumulated arrears of nearly £100 and was in danger of being sold. A new series of \"Reports and Transactions\" was published annually until 1890 and intermittently until 1899. The annual excursion and lectures continued into the 20th century and the 1935 AGM reported a membership of 160 and a credit balance of £55 10s 10d. The Penzance Natural History and Antiquarian Society ceased to exist in 1961.\n\nIn 1839 the Museum was housed in the dome of the Market House, (now Lloyds Bank) and moved to the newly built public buildings, now known as St John's Hall, in 1867, where it joined the Royal Geological Society of Cornwall and the Penzance Library (later to become the Morrab Library). In 1961 the collection, which included local flora and fauna passed into the ownership of the Borough of Penzance and is now housed at Penlee House.\n\n\n"}
{"id": "23743", "url": "https://en.wikipedia.org/wiki?curid=23743", "title": "Phanerozoic", "text": "Phanerozoic\n\nThe Phanerozoic Eon is the current geologic eon in the geologic time scale, and the one during which abundant animal and plant life has existed. It covers million years to the present, and began with the Cambrian Period when diverse hard-shelled animals first appeared. Its name was derived from the Ancient Greek words () and (), meaning \"visible life\", since it was once believed that life began in the Cambrian, the first period of this eon. The term \"Phanerozoic\" was coined in 1930 by the American geologist George Halcott Chadwick (1876–1953). The time before the Phanerozoic, called the \"Precambrian\", is now divided into the Hadean, Archaean and Proterozoic eons.\n\nThe time span of the Phanerozoic starts with what appears to be the rapid emergence of a number of animal phyla; the evolution of those phyla into diverse forms; the emergence and development of complex plants; the evolution of fish; the emergence of insects and tetrapods; and the development of modern fauna. Plant life on land appeared in the early Phanerozoic eon. During this time span, tectonic forces caused the continents to move and eventually collect into a single landmass known as Pangaea (the most recent supercontinent), which then separated into the current continental landmasses.\n\nThe Proterozoic-Phanerozoic boundary is at  million years ago. In the 19th century, the boundary was set at time of appearance of the first abundant animal (metazoan) fossils but several hundred groups (taxa) of metazoa of the earlier Proterozoic era have been identified since the systematic study of those forms started in the 1950s. Most geologists and paleontologists would probably set the Proterozoic-Phanerozoic boundary either at the classic point where the first trilobites and reef-building animals (archaeocyatha) such as corals and others appear; at the first appearance of a complex feeding burrow called \"Treptichnus pedum\"; or at the first appearance of a group of small, generally disarticulated, armored forms termed 'the small shelly fauna'. The three different dividing points are within a few million years of each other.\n\nIn the older literature, the term \"Phanerozoic\" is generally used as a label for the time period of interest to paleontologists, but that use of the term seems to be falling into disuse in more modern literature.\n\nThe Phanerozoic is divided into three eras: the Paleozoic, Mesozoic, and Cenozoic, which are further subdivided into 12 periods. The Paleozoic features the rise of fish, amphibians and reptiles. The Mesozoic is ruled by the reptiles, and features the evolution of mammals, birds and more famously, dinosaurs. The Cenozoic is the time of the mammals, and more recently, humans.\n\nThe Paleozoic is a time in Earth's history when complex life forms evolved, took their first breath of oxygen on dry land, and when the forerunners of all life on Earth began to diversify. There are six periods in the Paleozoic era: Cambrian, Ordovician, Silurian, Devonian, Carboniferous and Permian.\n\nThe Cambrian is the first period of the Paleozoic Era and starts from . The Cambrian sparked a rapid expansion in evolution in an event known as the Cambrian Explosion during which the greatest number of creatures evolved in a single period in the history of Earth. Plants like algae evolved, and the fauna was dominated by armored arthropods, such as trilobites. Almost all marine phyla evolved in this period. During this time, the super-continent Pannotia began to break up, most of which later recombined into the super-continent Gondwana.\n\nThe Ordovician spans from 485 million years to 440 million years ago. The Ordovician was a time in Earth's history in which many species still prevalent today evolved, such as primitive fish, cephalopods, and coral. The most common forms of life, however, were trilobites, snails and shellfish. More importantly, the first arthropods crept ashore to colonize Gondwana, a continent empty of animal life. By the end of the Ordovician, Gondwana had moved from the equator to the South Pole, and Laurentia had collided with Baltica, closing the Iapetus Ocean. The glaciation of Gondwana resulted in a major drop in sea level, killing off all life that had established along its coast. Glaciation caused a snowball Earth, leading to the Ordovician-Silurian extinction, during which 60% of marine invertebrates and 25% of families became extinct. This is considered the first mass extinction and the second deadliest in the history of Earth.\n\nThe Silurian spans from 440 million years to 415 million years ago, which saw a warming from Snowball Earth. This period saw the mass evolution of fish, as jaw-less fish became more numerous, jawed fish evolved, and the first freshwater fish evolved, though arthropods, such as sea scorpions, remained the apex predators. Fully terrestrial life evolved, which included early arachnids, fungi, and centipedes. The evolution of vascular plants (Cooksonia) allowed plants to gain a foothold on land. These early terrestrial plants are the forerunners of all plant life on land. During this time, there were four continents: Gondwana (Africa, South America, Australia, Antarctica, India), Laurentia (North America with parts of Europe), Baltica (the rest of Europe), and Siberia (Northern Asia). The recent rise in sea levels provided new habitats for many new species.\n\nThe Devonian spans from 415 million years to 360 million years ago. Also known as the \"Age of the Fish\", the Devonian features a huge diversification in fish, including armored fish like \"Dunkleosteus\" and lobe-finned fish which eventually evolved into the first tetrapods. On land, plant groups diversified; the first trees and seeds evolved. By the Middle Devonian, shrub-like forests of primitive plants existed: lycophytes, horsetails, ferns, and progymnosperm. This event also allowed the diversification of arthropod life as they took advantage of the new habitat. The first amphibians also evolved, and the fish were now at the top of the food chain. Near the end of the Devonian, 70% of all species became extinct in an event known as the Late Devonian extinction, which is the second mass extinction known to have happened.\n\nThe Carboniferous spans from 360 million to 300 million years ago. During this period, average global temperatures were exceedingly high: the early Carboniferous averaged at about 20 degrees Celsius (but cooled to 10 degrees during the Middle Carboniferous). Tropical swamps dominated the Earth, and the large amounts of trees created much of the carbon that became coal deposits (hence the name Carboniferous). The high oxygen levels caused by these swamps allowed massive arthropods, normally limited in size by their respiratory systems, to proliferate. Perhaps the most important evolutionary development of the time was the evolution of amniotic eggs, which allowed amphibians to move farther inland and remain the dominant vertebrates throughout the period. Also, the first reptiles and synapsids evolved in the swamps. Throughout the Carboniferous, there was a cooling pattern, which eventually led to the glaciation of Gondwana as much of it was situated around the south pole, in an event known as the Permo-Carboniferous glaciation or the Carboniferous Rainforest Collapse.\n\nThe Permian spans from 300 million to 250 million years ago and was the last period of the Paleozoic Era. At its beginning, all continents came together to form the super-continent Pangaea, surrounded by one ocean called Panthalassa. The Earth was very dry during this time, with harsh seasons, as the climate of the interior of Pangaea wasn't regulated by large bodies of water. Reptiles and synapsids flourished in the new dry climate. Creatures such as \"Dimetrodon\" and \"Edaphosaurus\" ruled the new continent. The first conifers evolved, then dominated the terrestrial landscape. Nearing the end of the period, \"Scutosaurus\" and gorgonopsids filled the arid landmass. Eventually, they disappeared, along with 95% of all life on Earth in an event simply known as \"the Great Dying\", the world's third mass extinction event and the largest in its history.\n\nThe Mesozoic ranges from 252 million to 66 million years ago. Also known as \"the Age of the dinosaurs\", the Mesozoic features the rise of reptiles on their 150 million year conquest of the Earth on the land, in the seas, and in the air. There are three periods in the Mesozoic: Triassic, Jurassic, and Cretaceous.\n\nThe Triassic ranges from 250 million to 200 million years ago. The Triassic is a transitional time in Earth's history between the Permian Extinction and the lush Jurassic Period. It has three major epochs: Early Triassic, Middle Triassic and Late Triassic.\n\nThe Early Triassic lasted between 250 million to 247 million years ago, and was dominated by deserts as Pangaea had not yet broken up, thus the interior was arid. The Earth had just witnessed a massive die-off in which 95% of all life became extinct. The most common life on Earth were \"Lystrosaurus\", labyrinthodonts, and \"Euparkeria\" along with many other creatures that managed to survive the Great Dying. Temnospondyli flourished during this time and were dominant predators for much of the Triassic. \n\nThe Middle Triassic spans from 247 million to 237 million years ago. The Middle Triassic featured the beginnings of the breakup of Pangaea, and the beginning of the Tethys Sea. The ecosystem had recovered from the devastation of the Great Dying. Phytoplankton, coral, and crustaceans all had recovered, and the reptiles began increasing in size. New aquatic reptiles, such as ichthyosaurs and nothosaurs, proliferated in the seas. Meanwhile, on land, pine forests flourished, as well as mosquitoes and fruit flies. The first ancient crocodilians evolved, which sparked competition with the large amphibians that had long ruled the freshwater world.\n\nThe Late Triassic spans from 237 million to 200 million years ago. Following the bloom of the Middle Triassic, the Late Triassic featured frequent rises of temperature, as well as moderate precipitation (10-20 inches per year). The recent warming led to a boom of reptilian evolution on land as the first true dinosaurs evolved, as well as pterosaurs. By the end of the period the first gigantic dinosaurs had evolved and advanced pterosaurs colonised Pangaea's deserts. The climactic change, however, resulted in a large die-out known as the Triassic-Jurassic extinction event, in which all archosaurs (excluding ancient crocodiles), synapsids, and almost all large amphibians became extinct, as well as 34% of marine life in the fourth mass extinction event. The extinction's cause is debated.\n\nThe Jurassic ranges from 200 million to 145 million years ago, and features three major epochs: Early Jurassic, Middle Jurassic, and Late Jurassic.\n\nThe Early Jurassic Epoch spans from 200 million to 175 million years ago. The climate was much more humid than the Triassic, and as a result, the world was very tropical. In the oceans, plesiosaurs, ichthyosaurs and ammonites dominated the seas. On land, dinosaurs and other reptiles dominated the land, with species such as \"Dilophosaurus\" at the apex. The first true crocodiles evolved, pushing the large amphibians to near extinction. The reptiles rose to rule the world. Meanwhile, the first true mammals evolved, but never exceeded the height of a shrew.\n\nThe Middle Jurassic Epoch spans from 175 million to 163 million years ago. During this epoch, dinosaurs flourished as huge herds of sauropods, such as \"Brachiosaurus\" and \"Diplodocus\", filled the fern prairies of the Middle Jurassic. Many other predators rose as well, such as \"Allosaurus\". Conifer forests made up a large portion of the world's forests. In the oceans, plesiosaurs were quite common, and ichthyosaurs were flourishing. This epoch was the peak of the reptiles. \n\nThe Late Jurassic Epoch spans from 163 million to 145 million years ago. The Late Jurassic featured a massive extinction of sauropods and ichthyosaurs due to the separation of Pangaea into Laurasia and Gondwana in an extinction known as the Jurassic-Cretaceous extinction. Sea levels rose, destroying fern prairies and creating shallows. Ichthyosaurs became extinct whereas sauropods, as a whole, did not; in fact, some species, like \"Titanosaurus\", lived until the K-T extinction. The increase in sea-levels opened up the Atlantic sea way which would continue to get larger over time. The divided world would give opportunity for the diversification of new dinosaurs.\n\nThe Cretaceous is the longest period in the Mesozoic, spans from 145 million to 66 million years ago, and is divided into two epochs: Early Cretaceous, and Late Cretaceous. \n\nThe Early Cretaceous Epoch spans from 145 million to 100 million years ago. The Early Cretaceous saw the expansion of seaways, and as a result, the decline and extinction of sauropods (except in South America). Many coastal shallows were created, and that caused ichthyosaurs to die out. Mosasaurs evolved to replace them as apex species of the seas. Some island-hopping dinosaurs, like \"Eustreptospondylus\", evolved to cope with the coastal shallows and small islands of ancient Europe. Other dinosaurs, such as \"Carcharodontosaurus\" and \"Spinosaurus\", rose to fill the empty space that the Jurassic-Cretaceous extinction had created. Of the most successful would be the \"Iguanodon\" which spread to every continent. Seasons came back into effect and the poles grew seasonally colder. Dinosaurs such as the \"Leaellynasaura\" inhabited the polar forests year-round, while many dinosaurs, such as the \"Muttaburrasaurus\", migrated there during summer . Since it was too cold for crocodiles, it was the last stronghold for large amphibians, such as the \"Koolasuchus\". In this epoch Pterosaurs reached their maximum diversity and grew larger, as species like \"Tapejara\" and \"Ornithocheirus\" took to the skies. The first true birds evolved, possibly sparking competition between them and the pterosaurs.\n\nThe Late Cretaceous Epoch spans from 100 million to 65 million years ago. The Late Cretaceous featured a cooling trend that would continue into the Cenozoic Era. Eventually, tropical ecology was restricted to the equator and areas beyond the tropic lines featured extreme seasonal changes of weather. Dinosaurs still thrived as new species such as \"Tyrannosaurus\", \"Ankylosaurus\", \"Triceratops\" and Hadrosaurs dominated the food web. Whether or not Pterosaurs went into a decline as birds radiated is debated, however many families survived until the end of the Cretaceous, alongside new species such as the gigantic \"Quetzalcoatlus\". Marsupials evolved within the large conifer forests as scavengers. In the oceans, Mosasaurs ruled the seas to fill the role of the ichthyosaurs, and huge plesiosaurs, such as \"Elasmosaurus\", evolved. Also, the first flowering plants evolved. At the end of the Cretaceous, the Deccan Traps and other volcanic eruptions were poisoning the atmosphere. As this was continued, it is thought that a large meteor smashed into Earth, creating the Chicxulub Crater creating the event known as the K-T Extinction, the fifth and most recent mass extinction event, during which 75% of life on Earth became extinct, including all non-avian dinosaurs. Every living thing with a body mass over 10 kilograms became extinct, and the age of the dinosaurs came to an end.\n\nThe Cenozoic featured the rise of mammals as the dominant class of animals, as the end of the age of the dinosaurs left significant evolutionary vacuums. There are three divisions of the Cenozoic: Paleogene, Neogene and Quaternary.\n\nThe Paleogene spans from the extinction of the non-avian dinosaurs, some 66 million years ago, to the dawn of the Neogene 23 million years ago. It features three epochs: Paleocene, Eocene and Oligocene. \n\nThe Paleocene Epoch began with the K-T extinction event caused by the impact of a metorite in the area of present-day Yucatan Peninsula and caused the destruction of 75% of all species on Earth. The Early Paleocene saw the recovery of the Earth from that event. The continents began to take their modern shape, but all continents (and India) were separated from each other. Afro-Eurasia was separated by the Tethys Sea, and the Americas were separated by the strait of Panama, as the Isthmus of Panama had not yet formed. This epoch featured a general warming trend, and jungles eventually reached the poles. The oceans were dominated by sharks as the large reptiles that had once ruled became extinct. Archaic mammals, such as creodonts and early primates that evolved during the Mesozoic filled the world. Mammals were still quite small, meanwhile enormous crocodiles and snakes like \"Titanoboa\" radiated to fill the niche of top predator.\n\nThe Eocene Epoch ranged from 56 million to 34 million years ago. In the early Eocene, most land mammals were small and living in cramped jungles, much like the Paleocene. Among them were early primates, whales and horses along with many other early forms of mammals. At the top of the food chains were huge birds, such as \"Gastornis\". Carnivorous flightless birds continued to be top predators for much of the rest of the Cenozoic, until their extinction in the Quaternary period. The temperature was 30 degrees Celsius with little temperature gradient from pole to pole. In the Middle Eocene Epoch, the circum-Antarctic current between Australia and Antarctica formed which disrupted ocean currents worldwide, resulting in global cooling, and caused the jungles to shrink. This allowed mammals to grow; some such as whales to mammoth proportions, which were, by now, almost fully aquatic. Mammals like \"Andrewsarchus\" were now at the top of the food-chain and sharks were replaced by \"Basilosaurus\", whales, as rulers of the seas. The late Eocene Epoch saw the rebirth of seasons, which caused the expansion of savanna-like areas, along with the evolution of grass. At the transition between the Eocene and Oligocene epochs there was a significant extinction event, the cause of which is debated.\n\nThe Oligocene Epoch spans from 33 million to 23 million years ago. The Oligocene was an important transitional period between the tropical world of the Eocene and more modern ecosystems. This period featured a global expansion of grass which had led to many new species to take advantage, including the first elephants, cats, dogs, marsupials and many other species still prevalent today. Many other species of plants evolved during this epoch also, such as the evergreen trees. The long term cooling continued and seasonal rains patterns established. Mammals continued to grow larger. \"Paraceratherium\", the largest land mammal to ever live evolved during this epoch, along with many other perissodactyls.\n\nThe Neogene spans from 23.03 million to 2.58 million years ago. It features 2 epochs: the Miocene, and the Pliocene.\n\nThe Miocene spans from 23.03 to 5.333 million years ago and is a period in which grass spread further across, effectively dominating a large portion of the world, diminishing forests in the process. Kelp forests evolved, leading to the evolution of new species, such as sea otters. During this time, perissodactyla thrived, and evolved into many different varieties. Alongside them were the apes, which evolved into a 30 species. Overall, arid and mountainous land dominated most of the world, as did grazers. The Tethys Sea finally closed with the creation of the Arabian Peninsula and in its wake left the Black, Red, Mediterranean and Caspian Seas. This only increased aridity. Many new plants evolved, and 95% of modern seed plants evolved in the mid-Miocene.\n\nThe Pliocene lasted from 5.333 to 2.58 million years ago. The Pliocene featured dramatic climactic changes, which ultimately led to modern species and plants. The Mediterranean Sea dried up for several thousand years in the Messinian salinity crisis. Along with these major geological events, \"Australopithecus\" evolved in Africa, beginning the human branch. The isthmus of Panama formed, and animals migrated between North and South America, wreaking havoc on the local ecology. Climatic changes brought savannas that are still continuing to spread across the world, Indian monsoons, deserts in East Asia, and the beginnings of the Sahara desert. The Earth's continents and seas moved into their present shapes. The world map has not changed much since, save for changes brought about by the glaciations of the Quaternary, such as the Great Lakes.\n\nThe Quaternary spans from 2.58 million years ago to present day, and is the shortest geological period in the Phanerozoic Eon. It features modern animals, and dramatic changes in the climate. It is divided into two epochs: the Pleistocene and the Holocene. \n\nThe Pleistocene lasted from 2.58 million to 11,700 years ago. This epoch was marked by ice ages as a result of the cooling trend that started in the Mid-Eocene. There were at least four separate glaciation periods marked by the advance of ice caps as far south as 40 degrees N latitude in mountainous areas. Meanwhile, Africa experienced a trend of desiccation which resulted in the creation of the Sahara, Namib, and Kalahari deserts. Many animals evolved including mammoths, giant ground sloths, dire wolves, saber-toothed cats, and most famously \"Homo sapiens\". 100,000 years ago marked the end of one of the worst droughts of Africa, and led to the expansion of primitive man. As the Pleistocene drew to a close, a major extinction wiped out much of the world's megafauna, including some of the hominid species, such as Neanderthals. All the continents were affected, but Africa to a lesser extent. That continent retains many large animals, such as hippos. The extent to which Homo Sapiens were involved in this extinction is debated.\n\nThe Holocene began 11,700 years ago and lasts until to present day. All recorded history and \"the history of the world\" lies within the boundaries of the Holocene epoch. Human activity is blamed for a mass extinction that began roughly 10,000 years ago, though the species becoming extinct have only been recorded since the Industrial Revolution. This is sometimes referred to as the \"Sixth Extinction\". More than 322 species have become extinct due to human activity since the Industrial Revolution.\n\nIt has been demonstrated that changes in biodiversity through the Phanerozoic correlate much better with the hyperbolic model (widely used in demography and macrosociology) than with exponential and logistic models (traditionally used in population biology and extensively applied to fossil biodiversity as well). The latter models imply that changes in diversity are guided by a first-order positive feedback (more ancestors, more descendants) or a negative feedback that arises from resource limitation, or both. The hyperbolic model implies a second-order positive feedback. The hyperbolic pattern of the human population growth arises from a second-order positive feedback, caused by the interaction of the population size and the rate of technological growth. The character of biodiversity growth in the Phanerozoic Eon can be similarly accounted for by a feedback between the diversity and community structure complexity. It is suggested that the similarity between the curves of biodiversity and human population probably comes from the fact that both are derived from the superposition on the hyperbolic trend of cyclical and random dynamics.\n\n"}
{"id": "323371", "url": "https://en.wikipedia.org/wiki?curid=323371", "title": "Pitch (resin)", "text": "Pitch (resin)\n\nPitch is a name for any of a number of viscoelastic polymers. Pitch can be natural or manufactured, derived from petroleum, coal tar, or plants. Various forms of pitch may also be called tar, bitumen, or asphalt. Pitch produced from plants is also known as resin. Some products made from plant resin are also known as rosin.\n\nPitch was traditionally used to help caulk the seams of wooden sailing vessels (see shipbuilding). Pitch may also be used to waterproof wooden containers and in the making of torches. Petroleum-derived pitch is black in colour, hence the adjectival phrase, \"pitch-black\".\n\nNaturally occurring asphalt/bitumen, a type of pitch, is a viscoelastic polymer. This means that even though it seems to be solid at room temperature and can be shattered with a hard impact, it is actually fluid and will flow over time, but extremely slowly. The pitch drop experiment taking place at University of Queensland is a long-term experiment which demonstrates the flow of a piece of pitch over many years. For the experiment, pitch was put in a glass funnel and allowed to slowly drip out. Since the pitch was allowed to start dripping in 1930, only nine drops have fallen. It was calculated in the 1980s that the pitch in the experiment has a viscosity approximately 230 billion (2.3×10) times that of water. The eighth drop fell on 28 November 2000, and the ninth drop fell on 17 April 2014. Another experiment was begun by a colleague of Nobel Prize winner Ernest Walton in the physics department of Trinity College in Ireland in 1944. Over the years, the pitch had produced several drops, but none had been recorded. On Thursday, July 11, 2013 scientists at Trinity College caught pitch dripping from a funnel on camera for the first time.\n\nThe viscoelastic properties of pitch make it well suited for the polishing of high-quality optical lenses and mirrors. In use, the pitch is formed into a lap or polishing surface, which is charged with iron oxide or cerium oxide. The surface to be polished is pressed into the pitch, then rubbed against the surface so formed. The ability of pitch to flow, albeit slowly, keeps it in constant uniform contact with the optical surface.\n\nThe heating (dry distilling) of wood causes tar and pitch to drip away from the wood and leave behind charcoal. Birchbark is used to make birch-tar, a particularly fine tar. The terms tar and pitch are often used interchangeably. However, pitch is considered more solid, while tar is more liquid. Traditionally, pitch that was used for waterproofing buckets, barrels and small boats was drawn from pine. It is used to make Cutler's resin.\n\n\n"}
{"id": "47163558", "url": "https://en.wikipedia.org/wiki?curid=47163558", "title": "Psai-Yah-hus", "text": "Psai-Yah-hus\n\nPsi-ya-hus (also spelled Psai-Yah-hus) is a spirit rock near the Fauntleroy ferry terminal in Seattle, Washington. Coast Salish peoples associate the rock with A'yahos, a \"malevolent and dangerous\" spirit, capable of shapeshifting, who sometimes appears in a two-headed serpent form, who is associated with other earthquake-related areas like landslides near the Seattle Fault.\n\nLIDAR imagery of the Seattle area revealed a previously unknown landslide in the Fauntleroy area. Another area associated with the a'yahos near Mercer Island could be related to the Lake Washington sunken forests, caused by landslides triggered by a Seattle Fault event around 900 CE.\n"}
{"id": "35036524", "url": "https://en.wikipedia.org/wiki?curid=35036524", "title": "Psalacantha", "text": "Psalacantha\n\nIn Greek mythology, Psalacantha (Ψαλάκανθα) was a nymph of the island Icaria. According to Ptolemy Hephaestion, she fell in love with Dionysus and promised to help him win the love of Ariadne on condition that he satisfy her own desires as well. Dionysus refused and Psalacantha went on to advise Ariadne against him, whereupon the god became enraged and changed Psalacantha into a plant known as \"psalakanthos\". Later, he repented and decided to commemorate Psalacantha by having the plant worked into Ariadne's wreath, the one that was changed into the constellation Corona Borealis.\n"}
{"id": "1763009", "url": "https://en.wikipedia.org/wiki?curid=1763009", "title": "Revetment", "text": "Revetment\n\nIn stream restoration, river engineering or coastal engineering, revetments are sloping structures placed on banks or cliffs in such a way as to absorb the energy of incoming water. In military engineering they are structures, again sloped, formed to secure an area from artillery, bombing, or stored explosives. River or coastal revetments are usually built to preserve the existing uses of the shoreline and to protect the slope, as defense against erosion.\n\nMany revetments are used to line the banks of freshwater rivers, lakes, and man-made reservoirs, especially to prevent damage during periods of floods or heavy seasonal rains (see riprap). Many materials may be used: wooden piles, loose-piled boulders or concrete shapes, or more solid banks.\n\nConcrete revetments are the most common type of infrastructure used to control the Mississippi River. More than of concrete matting has been placed in river bends between Cairo, Illinois and the Gulf of Mexico to slow the natural erosion that would otherwise frequently change small parts of the river's course.\n\nRevetments are used as a low-cost solution for coastal erosion defence in areas where crashing waves may otherwise deplete the coastline. Wooden revetments are made of planks laid against wooden frames so that they disrupt the force of the water. Although once popular, the use of wooden revetments has largely been replaced by modern concrete-based defence structures such as tetrapods.\n\nIn coastal engineering, a tetrapod is a four-legged concrete structure used as armour unit on breakwaters. The tetrapod's shape is designed to dissipate the force of incoming waves by allowing water to flow around rather than against it, and to reduce displacement by allowing a random distribution of tetrapods to mutually interlock.\n\nAccording to the U.S. National Park Service, and referring mostly to their employment in the American Civil War, a revetment is defined as a \"retaining wall constructed to support the interior slope of a parapet. Made of logs, wood planks, fence rails, fascines, gabions, hurdles, sods, or stones, the revetment provided additional protection from enemy fire, and, most importantly, kept the interior slope nearly vertical. Stone revetments commonly survive. A few log revetments have been preserved due to high resin pine or cypress and porous sandy soils. After an entrenchment was abandoned, many log or rail revetments were scavenged for other uses, causing the interior slope to slump more quickly. An interior slope will appear more vertical if the parapet eroded with the revetment still in place.\"\n\n\n\n"}
{"id": "13303895", "url": "https://en.wikipedia.org/wiki?curid=13303895", "title": "San Lucan xeric scrub", "text": "San Lucan xeric scrub\n\nThe San Lucan xeric scrub is a xeric shrubland ecoregion of the southernmost Baja California Peninsula, in Los Cabos Municipality and eastern La Paz Municipality of southern Baja California Sur state, Mexico. \n\nThe San Lucan xeric scrub covers an area of . It extends from sea level at the coasts up to elevation in the Sierra de la Laguna, where the Sierra de la Laguna dry forests ecoregion begins. Above that to the range's summits is the Sierra de la Laguna pine-oak forests ecoregion.\n"}
{"id": "2560961", "url": "https://en.wikipedia.org/wiki?curid=2560961", "title": "Scandinavian coastal conifer forests", "text": "Scandinavian coastal conifer forests\n\nThe Scandinavian coastal conifer forest ecoregion or the Norwegian coastal conifer forest ecoregion, a Palearctic ecoregion in the temperate coniferous forests biome, is located in along the coast of Norway. Within it are a number of small areas with botanical features and a local climate consistent with a temperate rainforest.\n\nThe Scandinavian coastal conifer forest PA0520 is a terrestrial ecoregion as defined by WWF. and National Geographic. The broad definition is based on climatic parameters and includes a long area along the western Norwegian coast from Lindesnes and north to approximately Senja (further north summers are too cool for pine to grow in coastal areas); in essence areas along the Norwegian coast where precipitation is high and winters are fairly mild. It might include areas lacking naturally occurring conifer forests (as in Lofoten, where the pine forest was cleared by man many centuries ago) and even islands and rocky headlands with little or no woodland and forest.\n\nAt somewhat higher elevations near the treeline in the Scandinavian mountains is the Scandinavian montane birch forest and grasslands ecoregion. In some areas along valleys, this ecoregion meet the taiga of the inland belonging to the Scandinavian and russian taiga ecoregion without mountain barriers. Examples of such valleys include the Rauma valley connecting Åndalsnes to Lesja and Dombås and the Namdalen valley connecting the Nord-Trøndelag coast to the cold interior with connection into Sweden. The ecoregion is naturally fragmented by fjords and mountains. The pine forests in the northern part have some of the oldest trees in Scandinavia, some more than 700 years old in Forfjord valley at Hinnøya.\n\nThis area has a long growing season for the latitude (140–215 days, longest in the south) with plentiful and reliable precipitation all year, from 1,200–3,000 mm. July 24-hr average temperatures typically range from 12–15 °C, with daytime highs of 14–20 °C (warmest days in sheltered fjord areas). Winters are fairly mild and rainy, January average range from −3° to 2 °C with daytime high at or above freezing. The mean annual temperature is approximately 7 °C on the southwestern coast (Bergen 7.6 °C, Stryn 6.4), 5.5 °C on the Trøndelag/central Norway coast (Åfjord 5.7 °C) and 4 °C in the northernmost area of this ecoregion (Bodø 4.5 °C, Gryllefjord/Senja 3.6 °C). This type of climate corresponds to the Köppen type Cfb and Cfc, and is comparable to the climate along the coast of northern British Columbia and the Alaska Panhandle.\n\nFor the smaller area classified as rainforest, there is at least 200 days/year with measurable precipitation. The minimum mean annual precipitation given in the sources varies somewhat but are generally around 1.400 mm, while the typical value on these locations are 1,500–2,200 mm annually. Summers are mild; warm weather usually does not last long, and really hot weather is virtually unknown or very short lasting. Winters are generally mild and rainy, sometimes with substantial snowfall, but the snow usually melts regularly throughout winter.\n\nThe lack of spruce north of the Arctic circle (Saltfjell) and along the southwestern coast is mainly due to barriers such as fjords and mountain ranges, and planted spruce grows well north of the arctic circle as in Tromsø. The southern limit of the Norway Spruce habitat in Norway is limited by mountains and fjords blocking the way, and also because of winters being too mild for Norway spruce near the outer seaboard along the southwestern coast. Along the southwestern coast and fjords (Vestlandet or Western Norway) is a temperate mixed forest with pine, some yew and deciduous trees (betula pendula, wych elm, linden, oak, aspen, hazel, holly) in the lowlands and more typical boreal forest at higher altitudes.\n\nThe botanically richest areas here (following the coast north to Ålesund, often the northern, south-facing shores of fjords, and patches further north along Trondheimsfjord), even if less diverse than the Oslofjord area due to migration barriers, is considered hemiboreal and might be considered as part of the Sarmatic mixed forests PA0436 or North Atlantic moist mixed forests PA0429 ecoregion. Some of the wettest areas in this region, where annual rainfall might exceed 1,500 mm and even 2,500 mm, are sometimes considered hemiboreal rainforest.\n\nIntroduced species include the Norway spruce, which has been planted for economic reasons in areas outside the natural range both on the southwestern coast and in the northernmost part of the ecoregion. Sitka spruce has also been planted extensively, especially near the outer seaboard, even north to Vesterålen and Harstad.\n\nSycamore maple was introduced to private gardens and church yards more than 150 years ago, and has spread profusely along the southwestern coast, along the central coast (Trøndelag) and, to a considerably lesser degree, north to Vesterålen. It is still most common near cities and villages, but seems certain to continue expansion along the coast and fjords.\n\nThere are many smaller introduced plants spreading, such as rosa rugosa. However, due to the often steep terrain, forestry has been somewhat limited in the coastal area, and many areas with original vegetation remain, but are often fragmented, especially so in the southern part of the ecoregion. American mink, originally escaped from fur farms, has colonized the whole country, and threaten sea bird colonies in some areas, but have not reached the outermost islands such as Røst, which have the largest sea bird colonies. The native otter seem to be dominant in areas with competition.\n\nWithin this long area is a smaller area classified as boreal rainforest based on botanical criteria. Much of the original forests have been destroyed, but a total of 250 forested areas, most of them not very large, have been classified as boreal rainforest. They are located from 63°20'N in Snillfjord in Sør-Trøndelag county and north along the coast to 66°N in Rana in Nordland county, but restricted to areas with high humidity; often shielded from the sun most of the day. Some inland locations are included; these are located in moist locations, often near waterfalls. This is the main area in Europe for boreal rainforest and Norway thus has a special responsibility for preservation. This coastal forest is mostly found at the lower elevations (below 200 m).\n\nDue to the long history of human settlement (millennia) with agriculture and more recently forestry management, only fragments remain of the original forest. The boreal rainforests are made up mostly of Norway Spruce (\"Picea abies\") but also included deciduous trees. Common Juniper (\"Juniperus communis\") is also common. There is a rich understory of mosses and ferns. However, the most distinguishing feature is the diversity of lichens, some of which are endemic for this forest, or have their only location in Europe here (they are often found on the northwest coast of North America). Approximately 15 of the most rare or typical species of lichens have been named \"Trøndelagselementet\" (named after the Trøndelag region). \"Pseudocyphella crocata\", \"Pannaria ahlneri\" and \"Erioderma padicellatum\" and Lobaria halli are examples of lichens. More than 60 unique species of lichen and moss can be found in the area.\n\nThere are two subtypes of this rainforest; the Namdalen type and the Brønnøy/Fosen type. There are also broadleaf trees scattered in this forest, especially in the Brønnøy/Fosen type. Broadleaf trees include birch (Silver Birch \"Betula pendula\" and Downy Birch \"Betula pubescens\"), European Rowan (\"Sorbus aucuparia\"), Aspen (\"Populus tremula\"), Goat Willow (\"Salix caprea\"), and Grey Alder (\"Alnus incana\"). Rarer species are Wych Elm (\"Ulmus glabra\"), Common Hazel (\"Corylus avellana\") and Black Alder (\"Alnus glutinosa\") (the latter only in the southern part of the area). The climate is also warm, and ya.\n\nThere are a large number of species of migrating birds in this ecoregion, as well as some that stay all year. Larger herbivore animals are moose and red deer (the latter only south of the Arctic circle), as well as the smaller roe deer. Reindeer might occasionally come down to the coast north of Trondheimsfjord, but they usually stay at the highlands outside this ecoregion. Predators are few, as they have been hunted by man for centuries, exterminating brown bear and grey wolf in the coastal area. In some areas, they roam further inland in the taiga ecoregion, and might on rare occasions get closer to the coast. Red fox and the sea eagle are common predators in the area, the latter now being very common after decades of protection. There are also some lynx, mostly in the northern part. Hares, otters are common and one can even see european beavers although more rarely. There are also some amphibians including the common frog and the smooth newt; the european viper can be seen south of the Arctic circle.\n\n\n"}
{"id": "31428172", "url": "https://en.wikipedia.org/wiki?curid=31428172", "title": "Steamboat Creek (Oregon)", "text": "Steamboat Creek (Oregon)\n\nSteamboat Creek is the name of several creeks in the U.S. state of Oregon:\n"}
{"id": "22878547", "url": "https://en.wikipedia.org/wiki?curid=22878547", "title": "Steven E. Koonin", "text": "Steven E. Koonin\n\nSteven E. Koonin (born December 12, 1951) is a theoretical physicist and Director of the Center for Urban Science and Progress at New York University. He is also a professor in the Department of Civil and Urban Engineering at NYU's Tandon School of Engineering.\n\nKoonin received his Bachelor of Science from Caltech and his Ph.D. from the Massachusetts Institute of Technology. In 1975, Koonin joined the faculty of the California Institute of Technology as a Professor of Theoretical Physics, and served as the Institute's provost from 1995 to 2004. In 2004, Koonin joined BP as their Chief Scientist where he was responsible for guiding the company’s long-range technology strategy, particularly in alternative and renewable energy sources. In 2009, he was appointed the U.S. Department of Energy’s second Senate-confirmed Under Secretary for Science serving from May 19, 2009 through November 18, 2011. He left that post in November 2011 for a position at the Institute for Defense Analyses. On April 23, 2012, Koonin was named director of NYU's Center for Urban Science and Progress (CUSP).\n\nHe has served on numerous advisory bodies for the National Science Foundation, the Department of Defense, and the Department of Energy and its various national laboratories, such as the JASON defense advisory group, which he has chaired. Koonin's research interests have included theoretical nuclear, many-body, and computational physics, nuclear astrophysics, and global environmental science.\n\nIn \"Climate Science Is Not Settled,\" a 2014 essay published in the Wall Street Journal, Koonin wrote that \"We are very far from the knowledge needed to make good climate policy,\" and that \"The impact today of human activity [on climate] appears to be comparable to the intrinsic, natural variability of the climate system itself.\" Koonin criticized the use of results from climate modelling to support the \"scientific consensus\" (quotes in original) about climate change, noting that, among other problems, \"The models differ in their descriptions of the past century's global average surface temperature by more than three times the entire warming recorded during that time.\" Regarding climate sensitivity, Koonin wrote that \"Today's best estimate of the sensitivity (between 2.7 degrees Fahrenheit and 8.1 degrees Fahrenheit) is no different, and no more certain, than it was 30 years ago. And this is despite an heroic research effort costing billions of dollars.\"\n\nIn 2017, Koonin urged interested parties to a drill-down debate with an article, \"A ‘Red Team’ Exercise Would Strengthen Climate Science.\" In support of such an approach, he wrote: \"The public is largely unaware of the intense debates within climate science. At a recent national laboratory meeting, I observed more than 100 active government and university researchers challenge one another as they strove to separate human impacts from the climate’s natural variability. At issue were not nuances but fundamental aspects of our understanding, such as the apparent—and unexpected—slowing of global sea-level rise over the past two decades.\"\n\n"}
{"id": "40413082", "url": "https://en.wikipedia.org/wiki?curid=40413082", "title": "Store Vildmose", "text": "Store Vildmose\n\nStore Vildmose (lit.: Large Wild-bog) is bogland located in northern Jutland, Vendsyssel, about 20 km north-west of Aalborg. It is the remnant of an extensive raised peat bog, in large part drained by constructed canals in the early 20th century. Some areas are still relatively untouched and give an impression of the original nature of this bog.\n\nThe area has both national and international importance, as it presents one of the largest contiguous areas of raised bog in Denmark, home to many rare animals and plants. Store Vildmose covers an area of about 6,000 hectares today, of which 1,895 hectares are protected areas. Most of the protections are designated for EU habitat, but the boglands are also to be protected as a scientific and archaeological important conservation zone.\n\nStore Vildmose is part of the Natura 2000 network.\n\nIn the Stone Age, the area was a big lake, which eventually dried up and was later cultivated by Iron Age farmers. In the middle ages, climate change had increased precipitation, stimulating the spread of peat moss and turning the area into one of the largest bogs in Europe. Store Vildmose eventually reached its maximum area in the 1800s, before drainage and peat cutting on a larger scale was initiated. As the peat has been dug up through the ages, various relics and artifacts from the Iron Age have been unearthed.\n\nBetween 1920 and 1945, most of the area became farmland, where so-called 'vildmose potatoes' are grown, considered a culinary speciality of Northern Jutland. Some of the benefits of potatoes from Store Vildmose are that the peels are very thin and smooth, that they do not soften easily with cooking and their taste. Several species are grown and sold under the name 'vildmose potatoes'. The potato-growers guild in Store Vildmose consists of six cooperating growers and they are working towards creating a regional- and event-center in one of the old stable farms in the area, through the fund of \"Vildmoseporten\" (\"The Vildmose Gate\").\n\nThere is a regional museum in the town of Brønderslev by the name of 'Vildmosemuseet', concentrating on the cultural history of the bog area.\n\nStore Vildmose is home to many rare and uncommon species, since the habitats it presents are threatened naturetypes. In the boglands, one can find cloudberry, the carnivorous great sundew and a breeding population of corn crake, that have otherwise seen a steep decline in western Europe. The marsh fritillary used to live here, but have not been observed for the last 20 years. Otter also used to roam here and might be establishing again in the near future, along with sea lamprey. Both species are to be protected in Denmark.\n\n"}
{"id": "5464288", "url": "https://en.wikipedia.org/wiki?curid=5464288", "title": "Telluric contamination", "text": "Telluric contamination\n\nTelluric contamination is contamination of the astronomical spectra by the Earth's atmosphere.\n\nMost astronomical observations are conducted by measuring photons (electromagnetic waves) which originate beyond the sky. The molecules in the Earth's atmosphere, however, absorb and emit their own light, especially in the visible and near-IR portion of the spectrum, and any ground-based observation is subject to contamination from these telluric (earth-originating) sources. Water vapor, oxygen, and OH are some of the more important molecules in telluric contamination. Water contamination was particularly pronounced in the Mount Wilson solar Doppler measurements.\n\nMany scientific telescopes have spectrographs, which measure photons as a function of wavelength or frequency, with typical resolution on the order of a nanometer. Spectroscopic observations can be used in myriad contexts, including measuring the chemical composition and physical properties of astronomical objects as well as measuring object velocities from the Doppler shift of spectral lines. Unless they are corrected for, telluric contamination can produce errors or reduce precision in such data.\n\nTelluric contamination can also be important for photometric measurements.\n\nIt is possible to correct for the effects of telluric contamination in an astronomical spectrum. This is done by preparing a telluric correction function, made by dividing a model spectrum of a star by an observation of an astronomical photometric standard star. This function can then be multiplied by an astronomical observation at each wavelength point.\n\nWhile this method can restore the original shape of the spectrum, the regions affected can be prone to high levels of noise due to the low number of counts in that area of the spectrum.\n\n\n"}
{"id": "41993095", "url": "https://en.wikipedia.org/wiki?curid=41993095", "title": "The Energy Journal", "text": "The Energy Journal\n\nThe Energy Journal is a quarterly peer-reviewed academic journal published by the International Association for Energy Economics and covering issues related to energy economics. It was established in 1980 and the editor-in-chief is Adonis Yatchew (University of Toronto).\n\nAccording to the \"Journal Citation Reports\", the journal has a 2014 impact factor of 1.772, ranking it 23rd out of 88 journals in the category \"Energy & Fuels\".\n\n"}
{"id": "58933", "url": "https://en.wikipedia.org/wiki?curid=58933", "title": "Timeline of white dwarfs, neutron stars, and supernovae", "text": "Timeline of white dwarfs, neutron stars, and supernovae\n\nTimeline of neutron stars, pulsars, supernovae, and white dwarfs\n\nNote that this list is mainly about the development of knowledge, but also about some supernovae taking place. For a separate list of the latter, see the article List of supernovae. All dates refer to when the supernova was observed on Earth or would have been observed on Earth had powerful enough telescopes existed at the time.\n\n"}
{"id": "21971285", "url": "https://en.wikipedia.org/wiki?curid=21971285", "title": "Umeme", "text": "Umeme\n\nUmeme Limited is the largest energy distributor in Uganda, distributing 97 percent of all electricity used in the country. The shares of the stock of the company are listed on the Uganda Securities Exchange (USE) and are cross listed on the Nairobi Stock Exchange (NSE). As of December 2017, the company's total assets were approximately UGX:2,349 trillion (US$628 million), with shareholder's equity of approximately UGX:617.7 billion (US$165 million). \n\nThe registered offices of the company are located at Rwenzori House, 1 Lumumba Avenue, in the central business district of Kampala, the capital and largest city of Uganda. The geographical coordinates of the company headquarters are 00°19'00.0\"N, 32°34'46.0\"E (Latitude:0.316667; Longitude:32.579444).\n\nUmeme was formed in 2004 when the government of Uganda leased the Uganda Electricity Distribution Company Limited to a consortium belonging to Globeleq (56 percent), a subsidiary of the Commonwealth Development Corporation of the United Kingdom, and Eskom of South Africa (44 percent). The transfer of assets did not take place until 1 March 2005.\n\nDuring 2006, the consortium formed by Globeleq and Eskom was restructured, with Globeleq becoming the sole owner of Umeme.\n\nOn 15 October 2012, Umeme became a listed company on the Uganda Securities Exchange (USE). A total of 622,378,000 shares, representing approximately 38 percent of its issued share capital at the time, became listed on the Ugandan bourse in an initial public offering (IPO). The shares of the company started trading on the USE on 30 November 2012. Umeme shares were first cross-listed on the NSE on 14 December 2012, with active trading commencing on 31 July 2013. The company expected to use the proceeds from the IPO, estimated at UGX:171 billion, to expand its power distribution network and payoff debt.\n\nAs of December 2017, the ten largest shareholders in the stock of the company were as illustrated in the table below: As of December 2017, the company shares were held by institutional investors (72 percent) and retail investors (28 percent).\n\nUmeme is governed by a ten-person board of directors, who included the following people, as of 17 May 2018.\n\n\nUmeme is structured into twelve management departments, each headed by a senior manager. The senior managers form the Executive Committee, whose members were the following as of 1 June 2018:\n\nUnder the Executive Committee, Umeme has 25 other managers who assist the executives to implement company policy and run the company on a daily basis. In March 2015, Selestino Babungi was appointed managing director, replacing Charles Chapman who served in that role from February 2009 until March 2015.\n\nWithin Uganda, Umeme is known for chronic unreliability and has been accused of corruption. Customers frequently face extended service outages, which are occasionally followed by protests, riots, and assaults on Umeme employees.\n\nIn 2010, protests erupted after a two-week power blackout in the town of Bugembe. Attempts to disperse the protesters led to violent clashes between police and irate residents. Nine were arrested, with eight being charged with inciting violence, staging an unlawful and violent assembly, and insulting the police.\n\nIn 2011, a two-month power outage led to protests in Jinja. Residents and business owners built barricades, set tires ablaze, and blocked traffic. Police responded with tear gas and live ammunition. Days later, over 100 people were dispersed by riot police using tear gas and rubber bullets after marching on Umeme's offices in protest of recurring outages in Masaka town.\n\nIn 2012, after a month-long outage along the Kampala–Gayaza Road, stone-throwing protesters caused property damage and injuries.\n\nIn 2014, a Umeme manager in Mubende was beaten severely by an infuriated mob during power-related protests at the Kasambya trading centre. The ensuing battle between rioters and security forces resulted in 15 arrests and one other injury, including a trader who was shot twice. Later that year, Kampala print and publishing shop owners went on strike, blocked Nasser road, and burnt paper and plastic to protest six-day power cuts along the road. Security forces used tear gas to disperse the protesters.\n\nIn 2013, a probe committee of Uganda's Parliament recommended cancellation of Umeme's contract due to its chronic failure to provide reliable services, which would trigger a contract severance payment of at least US $148 million.\n\nThe following year, Uganda's Parliament recommended that Umeme's \"contract should be terminated” due to the gross manipulations encountered in the procurement of the Umeme concession, and the scandalous provisions of the power distribution agreements signed between the government of Uganda and Umeme. Ultimately, this course of action was decided against because it would have been too expensive and politically complicated.\n\nIn June 2016, Umeme estimated its total power losses across its network at 19.1 percent. In September 2016, Umeme estimated that it lost UGX:106 billion annually from nonpayment of bills, vandalism, and illegal connections. Fifty-one percent of its losses come from the Bugisu sub-region in the Eastern Region.\n\nIn June 2015, Umeme estimated its power losses at 19.2 percent. Of that, 13 percent was attributable to technical losses (resulting from poor network configuration) and 6.2 percent was from commercial losses (such as power theft and under-billing). Umeme plans to reduce total power loss to 15 percent by 2018.\n\nIn May 2010, Umeme's communications director Charlotte Kemigyisha wrote a commentary in the \"Daily Monitor\" newspaper urging public support for President Yoweri Museveni's proposal to make electricity theft a capital offense.\n\nIn December 2010, Umeme announced plans to invest US$32 million during 2011 in new substations, improvements in grid connectivity, and the introduction of pre-payment systems.\n\nIn November 2013, Umeme announced that it had secured loans totaling US$190 million from the International Finance Corporation, Standard Chartered Bank, and Stanbic Bank to fund grid expansion and reduce energy losses.\n\nUmeme is spending US$440 million between 2013 and 2018 to overhaul equipment, buy technology, and add distribution points.\n\nIn March 2016, the \"Daily Monitor\" newspaper reported that Umeme had signed a contract with the Uganda Electricity Transmission Company Limited to distribute the power generated from the Isimba Hydroelectric Power Station, due online in 2018, and the Karuma Hydroelectric Power Station, due online in 2020.\n\nIn September 2016, a senior Umeme executive said that the company planned to spend US$2 billion over the next five years to expand the grid and increase access rates from an estimated 20 percent in 2016 (about 900,000 subscribers) to 40 percent in 2020 (about 3 million subscribers). In December 2017, Umeme announced plans to invest US$155 million in 2018, to improve the distribution network, build new substations and refurbish old networks across Uganda. In May 2018, in an interview with Reuters, Celestino Babungi, the CEO of Umeme, announced that the company planned to raise US$1.2 billion to revamp and expand the national distribution grid over the next seven years. A consultant was hired to advise and guide on how to raise the funds.\n\nAs of January 2016, Umeme's customer base was about 790,000, with approximately 16,000 customers being added every month. The company expects its customer base to exceed 1 million by the end of 2016.\n\nAs at 30 June 2017, Umeme exceeded 1,000,000 paying customers, with the numbers growing at a rate of 13 percent annually.\n\nAccordng to the \"Daily Monitor\", as of April 2018, Umeme had 1,125,291 customers. Seventy percent of those customers were on the pre-paid metering system, known as \"Yaka\". \n\nBy July 2018, pre-paid metering covered an estimated 80 percent of all Umeme's customers. At that time, the customer mix was as illustrated in the table below:\n\n\n"}
{"id": "1831024", "url": "https://en.wikipedia.org/wiki?curid=1831024", "title": "Wetlands International", "text": "Wetlands International\n\nWetlands International is a global organisation that works to sustain and restore wetlands and their resources for people and biodiversity. It is an independent, not-for-profit, global organisation, supported by government and NGO membership from around the world.\n\nBased mostly in the developing world, it has 20 regional, national or project offices in all continents and a head office in Ede, the Netherlands. \n\nThe NGO works in over 100 countries and at different scales to tackle problems affecting wetlands. With the support of dozens of governmental, NGO and corporate donors and partners, it supports about 80 projects.\n\nWetlands International's work ranges from research and community-based field projects to advocacy and engagement with governments, corporate and international policy fora and conventions. Wetlands International works through partnerships and is supported by contributions from an extensive specialist expert network and thousands of volunteers.\n\nIt was founded in 1937 as the International Wildfowl Inquiry and the organisation was focused on the protection of waterbirds. Later, the name became International Waterfowl & Wetlands Research Bureau (IWRB). The scope became wider; besides waterbirds, the organisation was also working on the protection of wetland areas.\n\nLater, organisations with similar objectives emerged in Asia and the Americas: the Asian Wetland Bureau (AWB) (initiated as INTERWADER in 1983) and Wetlands for the Americas (WA) (initiated in 1989). In 1991, the three organisations started to work closely together. \n\nIn 1995, the working relation developed into the global organisation Wetlands International.\n\nWetlands International works in many thematic areas throughout the world, including the links between peatlands and climate change, as well as wetlands and waterbird migration, based on extensive research as well as field projects. It coordinates the International Waterbird Census, a large-scale citizen science project with decades of data. \n\nCurrently, Wetlands International has four strong areas of work, which are:\n\nWetlands International's peatlands work is focused in Eurasia and southeast Asia. The peatswamp forests of Central Kalimantan are studied and worked on in particular for climate mitigation. The project Restoring peatlands in Russia is a joint-effort with the government and Greifeswald University, and was recognised by the UNFCCC with a Momentum for Change award in 2017, for storing up to 200,000 tonnes CO2e of carbon emissions every year by rewetting peatlands and preventing fires. \n\nIn the severely degraded peat lands of Central Kalimantan (Indonesia), drainage canals and logging have had disastrous impacts in an attempt to convert the unsuitable peatswamps into rice fields. By building small dams and blocks, the drainage of the area was stopped, preventing further oxidation of the peat soil. The area was then reforested with native tree species and community fire brigades to prevent the island's huge problem of peat fires.\n\nPrevious work in China has included the Ruoergai marshes, and new work is looking at the Yellow Sea/Bohai migratory bird flyway. Runoff from the glaciers in the Himalayas towards China’s lowland is regulated and stored in the Ruoergai marshes. The Wetlands International China office worked to have this peatland declared a Ramsar site, giving the Chinese government the obligation to protect the area. Furthermore, because of the work with the local Chinese authorities in measuring the impact of different management options, peat mining and drainage are now no longer allowed in Ruoergai and the neighbouring counties. This also leads to improved water supply to the Yellow River and Yangtze River.\n\nIn Tierra del Fuego, Argentina, the Wetlands International Latin America office built awareness of sustainable use of the peatlands from the local to the national level, which have contributed to their protection.\n\nCoastal wetlands such as mangrove forests and coral reefs reduce the impact of storms. Mangroves can even cope with sea-level rise and provide protection from impacts of waves. \n\nIn dryland regions such as sub-Saharan Africa, less rainfall and longer droughts increase the already huge importance of the Sahelian wetlands, and at the same time threaten overexploitation of these areas.\n\nWetlands International works in Mali to improve the livelihoods and water provision of communities in the Inner Niger Delta in a changing climate.\n\nWetlands International works to protect and restore the rich biodiversity of wetlands. Millions of waterbirds depend on wetlands like marshes, lakes and coastal zones. Wetlands International coordinates an International Waterbird Census in most countries of the world outside the USA where it approximates to the Christmas Bird Count run by the Audubon Society. The census takes place in 143 countries, divided into five regions, visiting 15,000 sites over 60 years. \n\nWetlands International promotes the establishment of ecological networks of well managed, protected wetlands, along the main flyway routes of migratory waterbirds. These wetlands provide stepping stones for migratory waterbirds; crucial for their survival. Wetlands International supports international governmental agreements to create these networks.\n\n\n"}
