{"id": "38000135", "url": "https://en.wikipedia.org/wiki?curid=38000135", "title": "Airlog", "text": "Airlog\n\nAIRLOG is a European Union FP7 project that was scheduled to run from 1 February 2012 to 31 January 2014. The aim of AIRLOG was to develop technology that would assist auditors of indoor air quality (IAQ) and educate the public about IAQ. AIRLOG was to also create an integrated platform for IAQ audit management. As such, the project was to provide best practice digital guide for the European Union.\n\nAir pollution is of concern in Europe because it may impair health and damage the environment. Long term exposure to air polluted with fine particles in Europe, especially that from proximity to traffic, is associated with deaths from natural causes and low birth weight (but not necessarily with non-malignant respiratory deaths). In the EU, it is estimated to cost the healthcare system 1 million euros per year (nine percent of the EU Gross Domestic Product (GDP). and 2.2 million disability-adjusted life years (\"DALYs\") are lost due to poor indoor air quality; it is estimated that 30% of those who spend a majority of their time within EU buildings suffer from Sick Building Syndrome (SBS).\n\nAlthough, from a global perspective, harmful indoor air pollution is caused by cooking and heating with solid fuels on open fires or traditional stoves, especially in poorly ventilated rooms, indoor air pollutants may also come from heating and cooling equipment, electronic appliances, cleaning products, air fresheners, insecticides, and construction materials. \n\nAccurately determining risk associated with exposure to indoor air pollution and controlling indoor air quality with regulatory instruments is difficult. Audits of Indoor Air Quality (IAQ) may assist in lessening risk to the health of people, help to improve productivity and support the use energy efficiency equipment and methods in buildings. To meet these goals, the European Commission's (EC) Scientific Committee on Health and Environmental Risks (SCHER) ruled that the EU should conduct a comprehensive review of existing data on pollutants of indoor air and begin recording data in a database.\n\nAIRLOG proposed to make a web-based audit management platform that would make audits less expensive, more accurate and easier to understand. The platform would include a Decision Support System that would use previously entered data to determine actions in improving IAQ that would eventually become best practice. The platform would also take into consideration the need for sustainable building design, efficient use of energy and other \"green\" elements.\n\nAn audit of IAQ is not a simple process for a number of reasons. The assessment must include measures of factors such as inefficient heating and air conditioning, growth of mould in moist areas, and the presence of volatile organic compounds (VOCs) from cleaning agents and objects such as new furniture. The IAQ audit may encounter unknown variables that influence its cost, time, and accuracy. In addition, each building is different in size, configuration, and exposure to air pollutants. The creation of an accurate IAQ audit must come from the combined efforts of engineers, chemists, health professionals, architects, building managers, maintenance personnel, building owners and consumers. IAQ auditors need to be trained and accredited in order to give a building an official certification.\n\nAn IAC audit can be divided into two parts. One part is about the level of comfort experienced in a building and the other is about measures of IAQ. Levels of comfort in a building are determined by factors such as temperature, humidity, air velocity, air renewal, and ventilation pressure. The second part, the IAQ, is determined by factors such as inorganic toxic gases, VOCs, particulate matter (PM10), electromagnetic fields and radiation, and microbiological contaminants (fungi and bacteria such as Legionella).\n\nThe audit is taken in steps. An initial assessment involves the collecting of information about the building and its ventilation and air conditioning (HVAC); its critical points; and its possible sources of contamination. This is done through visual inspection. Then, there is the standardised measurement and analysis of factors such as certain chemicals and microbes. This is done by collecting samples for laboratory analysis. A third step is researching possible corrective actions.\n\nAIRLOG aimed to create software to assist field engineers in audits of IAQ in buildings in the EU. It also aimed to develop an integrated management platform that could become more effective as data was entered. Thus, testing of IAQ in an individual building would be aligned with the monitoring of IAQ across the EU.\n\nThe problems in IAQ auditing that were to be overcome included: inefficient planning of audits, overdue reports, incompleteness, lack of transparency in reporting audit progress and slow accumulation of results for monitoring. They also included: risk of errors due to manual data entry, the generation of unreliable alerts, bulk data loss from Excel files and paper-based documents, delay or failure to implement the necessary corrective actions, and non-standardized auditing practices by building type, use and state. In addition, AIRLOG was challenged with considering the efficient use of energy and the training of IAQ experts in standard methodologies.\n\nAIRLOG aimed to integrate a number of data sources. These included: measurements from sensors; laboratory analysis; maintenance reports from building managers; and governmental and legislative decisions. AIRLOG planned to manage these sources of data using a platform based on Web 3.0. The platform would use artificial intelligence to automatically learn and propose actions for mitigation and control through the use of a risk simulation tool.\n\nIn its resolution of 4 September 2008, relating to the mid-term review of the European Agency for Health and Action Plan 2004–2010 (2007/2252 (INI)), the European parliament called on the EC to propose directives to address the following:\n\nThe resolution recommended that the EC encourage member states to improve IAQ through tax and other financial concessions and to reduce exposure to electromagnetic radiation in official buildings. Action 12 of the EU Environment and Health Action Plan 2004–2012 called for the development of ways to address factors affecting IAQ such as moisture, mould, building materials, consumer products, and indoor activities. Special focus would be placed on the IAQ of buildings used by people most at risk, the young, the sick and the elderly, for example schools and health centres.\n\nIn June 2010, an EU conference, \"Product Policy and Indoor Air Quality\", concluded that attention to IAQ would improve work performance, reduce absenteeism and reduce need for medical care and therefore have economic impact. Furthermore, the standardised testing, certification and labelling of indoor products that gave VOC emissions would facilitate international trade and reduce trade barriers within the 28 EU member states.\n\n"}
{"id": "7620920", "url": "https://en.wikipedia.org/wiki?curid=7620920", "title": "Archbald Pothole State Park", "text": "Archbald Pothole State Park\n\nArchbald Pothole State Park is a Pennsylvania state park in Archbald, Lackawanna County, Pennsylvania. The focal point of the park is Archbald Pothole. The pothole is a remnant of the Wisconsin Glacial Period, deep with a largest diameter of by . It has drawn tourists since just after it was discovered in 1884. Archbald Pothole State Park is on U.S. Route 6 Business in the borough of Archbald.\n\nA coal miner, Patrick Mahon, discovered Archbald Pothole in 1884. Mahon was extending a mine shaft. When he set off a blast of explosives, water and stones came pouring into the mine shaft. He and the other miners fled the scene fearing for their lives. The manager of the mining company, Edward Jones, came to investigate. Jones ordered that the area be cleared of the fallen debris. Almost 1,000 tons of small rounded stones were removed and Jones soon realized that the vertical tunnel discovered by the coal miners, was a large pothole.\n\nAfter serving as a ventilation shaft for the mine, the pothole was fenced in by the owner of the land, Colonel Hackley, for tourists to look at it. The pothole soon became a renowned tourist attraction. Edward Jones led the tours for the locals and famous geologists. Archbald Pothole was turned over to public ownership in 1914, when the widow of Colonel Hackley deeded surrounding the pothole to the Lackawanna Historical Society. Lackawanna County gained ownership of the pothole and the surrounding in 1940. Archbald Pothole was a county park until 1961 when the property was transferred to the Commonwealth of Pennsylvania. Archbald Pothole State Park was formally opened in 1964.\n\nWhile the pothole and surrounding park were long a popular tourist attraction, by the 1990s attendance had fallen and the facilities were in need of repair. The park was closed for a $170,000 \"facelift\" and when it reopened in 1997 it had been repaved and had new landscaping and new trash receptacles. Despite the improvements, attendance remained low and litter thrown into the pothole was still a problem, including \"bottles and paper bags... a parking meter, a park bench and a \"Wet Floor\" cone\". Another problem was the park's \"unsavory reputation\" as a place for \"men looking for sex\", with plainclothes police arresting 29 men there for \"lewd behavior\" in one 2002 sweep. In an attempt to address these issues, in 2002 the Pennsylvania State Legislature approved more improvements to the park, including \"least two soccer fields, a basketball court, a tennis court, a walking trail, a playground, roads and parking areas\".\n\nArchbald Pothole is deep and wide at its maximum diameter. The pothole cuts through layers of sandstone, shale and coal. A pothole, in geologic terms, is a hole that is worn into the bedrock of a stream in strong rapids or at the base of a waterfall. The force of the water spins rock fragments, sand and gravel into a small indentation in the bedrock. After years and years of constant spinning, the stones and sands carve out an elliptical hole. Potholes are also formed by the action of glacial meltwater. Archbald Pothole is an example of just such a pothole.\n\nArchbald Pothole was formed during the Wisconsin Glacial Period. As the glacier melted, a stream that flowed on top may have fallen into a crevasse and then fell to the bedrock. The force of the falling water created a pothole in much the same way that a waterfall creates a pothole. The pothole was filled by falling sand, rocks and gravel as the glacier retreated and created other potholes. Archbald Pothole was preserved underground for nearly 13,000 years until its discovery by Patrick Mahon. The park is at an elevation of .\n\n\nA small loop trail follows an old coal mine tram road for hiking. The trail passes along a rock ledge and through a forest.\n\nHunting is permitted on over of the park. The most common game species are squirrels, turkeys and white-tailed deer. The hunting of groundhogs is not permitted. Hunters are expected to follow the rules and regulations of the Pennsylvania State Game Commission.\n\nSome of the parkland was stripped off in the past by strip mining. This land is currently undergoing a reclamation process and there are plans to use the reclaimed land for recreation and to build athletic fields.\n\nThe following state parks are within of Archbald Pothole State Park:\n\n"}
{"id": "52331129", "url": "https://en.wikipedia.org/wiki?curid=52331129", "title": "BLIZZARD! The Storm That Changed America", "text": "BLIZZARD! The Storm That Changed America\n\nBLIZZARD! The Storm That Changed America is a 2000 Children's history book by Jim Murphy. It is about the Blizzard of 1888 that hit the north-east of North America, and concentrates on New York City.\n\n\"BookList\" called \"Blizzard\" \"an example of stellar nonfiction.\" and the \"School Library Journal\" wrote \"The narrative is a readable and seamless blend of history and adventure adapted from extensive first-person accounts and primary news sources. .. The text is exciting without being melodramatic .. Authentic photographs, drawings, and maps that demonstrate the course of the storm, all done in the same sepia tone as the text, perfectly illustrate the book. Overall, a superb piece of writing and history.\" In a star review \"Kirkus Reviews\" wrote \"Murphy’s ability to pull in details that lend context allows him to tell this story of a place in time through the lens of a single, dramatic episode that will engage readers. This is skillfully done: humorous, jaw-dropping, thought-provoking, and chilling.\" In a review of an audio version of the book \"Publishers Weekly\" wrote \"Murphy's well-rounded information about the various circumstances that worsened the effects of the storm make the tale both more fascinating and more tragic. Mali's (Taylor Mali, the reader) steady delivery is very well suited to the material; it allows listeners time to absorb this gripping history lesson.\"\n\n\"The Horn Book Magazine\" stated \"Murphy treats his subject with respect as he curbs the inherent sensationalism of the topic through an informal, journalistic style. To build urgency in the narrative, he creates cogent transitions from one event to another and from personal events to broader historical segments. Even with all of these connections, individual chapters stand alone, providing access for browsers and those searching for nonfiction read-alouds. Sepia-colored illustrations (archival photographs and original art from the period) reinforce the historical setting ..\" and \"Voice of Youth Advocates\" wrote \"this first annual Sibert Award honor book will appeal to both teens and adults interested in weather extremes and history.\"\n\nIn a review for \"The New York Times\", Mary Russell wrote \"Murphy's gift for dealing with disasters .. is that he manages to make them, simultaneously, both larger than we'd thought and smaller, more human than we'd imagined.\"\n"}
{"id": "1371845", "url": "https://en.wikipedia.org/wiki?curid=1371845", "title": "Blackwater River State Park", "text": "Blackwater River State Park\n\nBlackwater River State Park is a Florida State Park located fifteen miles northeast of Milton, near Harold, off U.S. 90. The address is 7720 Deaton Bridge Road.\n\nA favorite destination for canoeists and kayakers, Blackwater River offers opportunities for a variety of outdoor recreation. The river is one of the purest sand-bottom rivers in the nation, making this park a popular place for swimming, fishing, camping, and paddling. Shaded campsites are just a short walk from the river, and visitors can enjoy a picnic at a pavilion overlooking the river. Nature enthusiasts will enjoy strolling along trails through undisturbed natural communities. The park was certified a Registered State Natural Feature in 1980, for possessing exceptional value in illustrating the natural history of Florida. In 1982, an Atlantic white cedar there was recognized as a Florida Champion tree, one of the largest and oldest of its species.\n\nThe park has such amenities as birding, boating, canoeing, fishing, hiking, kayaking, picnicking areas, swimming, tubing, wildlife viewing and full camping facilities. The main picnicking area has covered picnicking pavilions, restrooms, and a spacious parking lot. Bring your tubes and enjoy a leisurely float down the river to the Deaton Bridge. It is a short 1 mile hike back to the parking lot to retrieve your vehicle. Very kid friendly park with a variety of areas for swimming.\n\n"}
{"id": "49731496", "url": "https://en.wikipedia.org/wiki?curid=49731496", "title": "Chemical cycling", "text": "Chemical cycling\n\nChemical cycling describes systems of repeated circulation of chemicals between other compounds, states and materials, and back to their original state, that occurs in space, and on many objects in space including the Earth. Active chemical cycling is known to occur in stars, many planets and natural satellites.\n\nChemical cycling plays a large role in sustaining planetary atmospheres, liquids and biological processes and can greatly influence weather and climate. Some chemical cycles release renewable energy, others may give rise to complex chemical reactions, organic compounds and prebiotic chemistry. On terrestrial bodies such as the Earth, chemical cycles involving the lithosphere are known as geochemical cycles. Ongoing geochemical cycles are one of the main attributes of geologically active worlds. A chemical cycle involving a biosphere is known as a biogeochemical cycle.\n\nIn most hydrogen-fusing stars, including the Sun, a chemical cycle involved in stellar nucleosynthesis occurs which is known as a carbon-nitrogen-oxygen or (CNO cycle). In addition to this cycle, stars also have a helium cycle. Various cycles involving gas and dust have been found to occur in galaxies.\n\nThe majority of known chemical cycles on Venus involve its dense atmosphere and compounds of carbon and sulphur, the most significant being a strong carbon dioxide cycle. The lack of a complete carbon cycle including a geochemical carbon cycle, for example, is thought to be a cause of its runaway greenhouse effect, due to the lack of a substantial carbon sink. Sulphur cycles including sulphur oxide cycles also occur, sulphur oxide in the upper atmosphere and results in the presence of sulfuric acid in turn returns to oxides through photolysis. Indications also suggest an ozone cycle on Venus similar to that of Earth's.\n\nA number of different types of chemical cycles geochemical cycles occur on Earth. Biogeochemical cycles play an important role in sustaining the biosphere.\nNotable active chemical cycles on Earth include:\n\n\nOther chemical cycles include hydrogen peroxide.\n\nRecent evidence suggests that similar chemical cycles to Earth's occur on a lesser scale on Mars, facilitated by the thin atmosphere, including carbon dioxide (and possibly carbon), water, sulphur, methane, oxygen, ozone, and nitrogen cycles. Many studies point to significantly more active chemical cycles on Mars in the past, however the faint young Sun paradox has proved problematic in determining chemical cycles involved in early climate models of the planet.\n\nJupiter, like all the gas giants, has an atmospheric methane cycle. Recent studies indicate a hydrological cycle of water-ammonia vastly different to the type operating on terrestrial planets like Earth and also a cycle of hydrogen sulfide.\n\nSignificant chemical cycles exist on Jupiter's moons. Recent evidence points to Europa possessing several active cycles, most notably a water cycle. Other studies suggest an oxygen and radiation induced carbon dioxide cycle. Io and Europa, appear to have radiolytic sulphur cycles involving their lithospheres. In addition, Europa is thought to have a sulfur dioxide cycle. In addition, the Io plasma torus contributes to a sulphur cycle on Jupiter and Ganymede. Studies also imply active oxygen cycles on Ganymede and oxygen and radiolytic carbon dioxide cycles on Callisto.\n\nIn addition to Saturn's methane cycle some studies suggest an ammonia cycle induced by photolysis similar to Jupiter's.\n\nThe cycles of its moons are of particular interest. Observations by Cassini–Huygens of Titan's atmosphere and interactions with its liquid mantle give rise to several active chemical cycles including a methane, hydrocarbon, hydrogen, and carbon cycles. Enceladus has an active hydrological, silicate and possibly a nitrogen cycle.\n\nUranus has an active methane cycle. Methane is converted to hydrocarbons through photolysis which condenses and as they are heated, release methane which rises to the upper atmosphere.\n\nStudies by Grundy et al. (2006) indicate active carbon cycles operates on Titania, Umbriel and Ariel and Oberon through the ongoing sublimation and deposition of carbon dioxide, though some is lost to space over long periods of time.\n\nNeptune's internal heat and convection drives cycles of methane, carbon, and a combination of other volatiles within Triton's lithosphere.\n\nModels predicted the presence of seasonal nitrogen cycles on the moon Triton, however this has not been supported by observations to date.\n\nModels predict a seasonal nitrogen cycle on Pluto and observations by New Horizons appear to support this.\n"}
{"id": "7680741", "url": "https://en.wikipedia.org/wiki?curid=7680741", "title": "ERICA", "text": "ERICA\n\nERICA, or the Experiment on Rapidly Intensifying Cyclones over the Atlantic, is a scientific field project that started in the winter of 1988/1989. Its aims were to better understand the processes involved in rapid cyclogenesis, and so improve understanding and forecasting of the situations that cause it.\n\n"}
{"id": "5933013", "url": "https://en.wikipedia.org/wiki?curid=5933013", "title": "Eachy", "text": "Eachy\n\nAn eachy is a name given to a species of lake monster from a variety of locations in Northern England and Scotland. An eachy is typically a large humanoid being of gruesome and slimy appearance seen to occasionally emerge from the lake. An eachy was reported from Windermere in 1873 and Bassenthwaite Lake as late as 1973 when it was supposedly photographed twice. A scientific expedition was launched to find the mystery creature when, in September 1961, three atomic scientists conducted an underwater exploration of the lake. However, they did not find the , triple-humped, python-headed creature that had recently been sighted.\n\nPresumably the name comes from the Middle English \"iker\" or \"eker\" (from the older \"niker\", from the Old English \"nicor\", meaning \"water-sprite\" or \"hippotamus\"), which was a type of sea monster. Although the name looks similar to the Scottish Gaelic each-uisge, the word is morphologically and phonetically dissimilar. The area of the each-uisge is traditionally quite distant from the traditional area of the eachy and the each-uisge was not anthropomorphic but instead a water horse.\n\n"}
{"id": "33544127", "url": "https://en.wikipedia.org/wiki?curid=33544127", "title": "Emergent organization", "text": "Emergent organization\n\nAn emergent organization (alternatively emergent organisation) is an organization that spontaneously emerges from and exists in a complex dynamic environment or market place, rather than being a construct or copy of something that already exists. The term first appeared in the late 1990s and was the topic of the Seventh Annual Washington Evolutionary Systems Conference at University of Ghent, Belgium in May, 1999. Emergent organizations and their dynamics pose interesting questions; for example, how does such an organization achieve closure and stability?\n\nAlternatively, James R. Taylor wrote in 2000 his seminal book, \"The Emergent Organization\", where he suggests that all organizations emerge from communication, especially from the interplay of conversation and text. This idea concerns human organizations, but is consistent with Leibniz or Gabriel Tarde's monadology, or with Alfred North Whitehead's process philosophy, which explains the macro—both in human and non-human \"societies\"—from the processes taking place between its constituent parts.\n\n"}
{"id": "47243366", "url": "https://en.wikipedia.org/wiki?curid=47243366", "title": "GPlates", "text": "GPlates\n\nGPlates is open-source application software offering a novel combination of interactive plate-tectonic reconstructions, geographic information system (GIS) functionality and raster data visualization.\n\nThe GPlates project was started by Professor Dietmar Müller in 2006. By the end of 2010, the GPlates 1.0.0 was released.\n\nThe latest release was GPlates 2.0 in November 2016. A user manual and tutorials are available online.\n\nBelow is a list of major releases of GPlates.\n\nGPlates enables both the visualization and the manipulation of plate-tectonic reconstructions and associated data through geological time:\n\nGPlates is developed by an international team of scientists and professional software developers at:\n\nGPlates is used by geophysicists, students and researchers in academic institutions, government departments and industry.\n\nGPlates runs on Mac OS X, Microsoft Windows and Ubuntu. GPlates is written in C++ and uses OpenGL to render its 3D globe and 2D map views. It uses Qt as a GUI framework. The Boost C++ library has also been widely used. Other libraries include GDAL, CGAL, proj, qwt and GLEW.\n\nGPlates uses the GPlates Geological Information Model (GPGIM) to represent geological data in a Plate tectonics context. The GPlates Markup Language (GPML) is an XML implementation of the GPGIM derived from the Geography Markup Language (GML).\n\nThe GPlates Python library (pyGPlates) enables access to GPlates functionality via the Python programming language. It allows users to use GPlates in a programmatic way and hence provides much more flexibility than the GPlates desktop interface can offer. The first beta release of pyGPlates is available for download. Reference documentation and tutorials are also available for download.\n\n\n\nGPlates is released under GNU General Public License version 2.0 (GPLv2) and the source code can be found on SourceForge\n\nThe GPlates Web Portal is a gateway to a series of GPlates-based web applications. The portal was launched in 2014. Michael Chin is the architect and chief programmer. Initially the portal was hosted on nectar cloud. Later on, it was migrated to Amazon Elastic Compute Cloud. Below is a list of applications in GPlates Web Portal.\n\nThe Cesium Javascript library is used to render the 3D globe in a web browser.\n\n\nBelow is a list of select publications of GPlates.\n\n\n\n\n"}
{"id": "589040", "url": "https://en.wikipedia.org/wiki?curid=589040", "title": "Geological Survey of Denmark and Greenland", "text": "Geological Survey of Denmark and Greenland\n\nThe Geological Survey of Denmark and Greenland () is the independent sector research institute under the Danish Ministry of Climate and Energy. GEUS is an advisory, research and survey institute in hydrogeology, geophysics, geochemistry, stratigraphy, glaciology, ore geology, marine geology, mineralogy, climatology, environmental history, air photo interpretation, geothermal energy fields concerning Denmark and Greenland.\n\nGEUS works in close corporation with Geologisk Institut and Geologisk Museum, both part of University of Copenhagen.\n\nIt publishes a service paper called \"Greenland Hydrocarbon Exploration Information Service\" (GHEXIS) and a newsletter called Greenland Mineral Exploration Newsletter (MINEX) in co-operation with the Bureau of Minerals and Petroleum (Råstofdirektoratet), a secretariat for the Joint Committee on Mineral Resources under Greenland’s home rule.\n\nIn 1888 (DGU) was founded. \n\nIn 1946, was created. \n\nOn 14 June 1965, law no. 238 created GGU.\n\nOn 23 December 1987, law no. 864 merged GGU into DGU, changing its name to DGGU ().\n\nOn 14 June 1995, Law no. 408 disbanded law no. 238. \n\nOn 20 December 1995, law no. 1076 concerning Danish sector research institutes created GEUS by merging DGU and GGU.\n\n\n"}
{"id": "43667", "url": "https://en.wikipedia.org/wiki?curid=43667", "title": "Georg Forster", "text": "Georg Forster\n\nJohann Georg Adam Forster (; November 27, 1754 – January 10, 1794) was a German naturalist, ethnologist, travel writer, journalist, and revolutionary. At an early age, he accompanied his father, Johann Reinhold Forster, on several scientific expeditions, including James Cook's second voyage to the Pacific. His report of that journey, \"A Voyage Round the World\", contributed significantly to the ethnology of the people of Polynesia and remains a respected work. As a result of the report, Forster was admitted to the Royal Society at the early age of twenty-two and came to be considered one of the founders of modern scientific travel literature.\n\nAfter returning to continental Europe, Forster turned toward academia. He traveled to Paris to seek out a discussion with the American revolutionary Benjamin Franklin in 1777. He taught natural history at the Collegium Carolinum in the Ottoneum, Kassel (1778–84), and later at the Academy of Vilna (Vilnius University) (1784–87). In 1788, he became head librarian at the University of Mainz. Most of his scientific work during this time consisted of essays on botany and ethnology, but he also prefaced and translated many books about travel and exploration, including a German translation of Cook's diaries.\n\nForster was a central figure of the Enlightenment in Germany, and corresponded with most of its adherents, including his close friend Georg Christoph Lichtenberg. His ideas and personality influenced Alexander von Humboldt, one of the great scientists of the 19th century. When the French took control of Mainz in 1792, Forster became one of the founders of the city's Jacobin Club and went on to play a leading role in the Mainz Republic, the earliest republican state in Germany. During July 1793 and while he was in Paris as a delegate of the young Mainz Republic, Prussian and Austrian coalition forces regained control of the city and Forster was declared an outlaw. Unable to return to Germany and separated from his friends and family, he died in Paris of illness in early 1794.\n\nGeorg Forster was born in the small village of Nassenhuben (Mokry Dwór) near Danzig (Gdańsk), in the region of Royal Prussia, in the Polish-Lithuanian Commonwealth. He was the oldest of seven surviving children of Johann Reinhold Forster and Justina Elisabeth (née Nicolai). His father was a naturalist, scientist and Reformed pastor. In 1765, the Russian empress Catherine II commissioned the pastor to travel through Russia on a research journey and investigate the situation of a German colony on the Volga River. Georg, then ten years old, joined him. On the journey, which reached the Kalmyk steppe on the lower Volga, they discovered several new species, and the young Forster learned how to conduct scientific research and practice cartography. He also became fluent in Russian.\n\nThe report of the journey, which included sharp criticism of the governor of Saratov, was not well received at court. The Forsters claimed they had not received fair payment for their work and had to move house. They chose to settle in England in 1766. The father took up teaching at the Dissenter's Academy in Warrington and also translation work. At the age of only thirteen, the young Forster published his first book: an English translation of Lomonosov's history of Russia, which was well received in scientific circles.\n\nIn 1772, Forster's father became a member of the Royal Society. This and the withdrawal of Joseph Banks resulted in his invitation by the British admiralty to join James Cook's second expedition to the Pacific (1772–75). Georg Forster joined his father in the expedition again and was appointed as a draughtsman to his father. Johann Reinhold Forster's task was to work on a scientific report of the journey's discoveries that was to be published after their return.\n\nThey embarked HMS \"Resolution\" on July 13, 1772, in Plymouth. The ship's route led first to the South Atlantic, then through the Indian Ocean and the Southern Ocean to the islands of Polynesia and finally around Cape Horn back to England, returning on July 30, 1775. During the three-year journey, the explorers visited New Zealand, the Tonga islands, New Caledonia, Tahiti, the Marquesas Islands and Easter Island. They went further south than anybody before them, almost discovering Antarctica. The journey conclusively disproved the \"Terra Australis Incognita\" theory, which claimed there was a big, habitable continent in the South.\n\nSupervised by his father, Georg Forster first undertook studies of the zoology and botanics of the southern seas, mostly by drawing animals and plants. However, Georg also pursued his own interests, which led to completely independent explorations in comparative geography and ethnology. He quickly learned the languages of the Polynesian islands. His reports on the people of Polynesia are well regarded today, as they describe the inhabitants of the southern islands with empathy, sympathy and largely without Western or Christian bias.\n\nUnlike Louis Antoine de Bougainville, whose reports from a journey to Tahiti a few years earlier had initiated uncritical \"noble savage\" romanticism, Forster developed a sophisticated picture of the societies of the South Pacific islands. He described various social structures and religions that he encountered on the Society Islands, Easter Island and in Tonga and New Zealand, and ascribed this diversity to the difference in living conditions of these people. At the same time, he also observed that the languages of these fairly widely scattered islands were similar. About the inhabitants of the Nomuka islands (in the Ha'apai island group of present-day Tonga), he wrote that their languages, vehicles, weapons, furniture, clothes, tattoos, style of beard, in short all of their being matched perfectly with what he had already seen while studying tribes on Tongatapu. However, he wrote, \"we could not observe any subordination among them, though this had strongly characterised the natives of Tonga-Tabboo, who seemed to descend even to servility in their obeisance to the king.\"\n\nThe journey was rich in scientific results. However, the relationship between the Forsters and Cook and his officers was often problematic, due to the elder Forster's fractious temperament as well as Cook's refusal to allow more time for botanical and other scientific observation. Cook refused scientists on his third journey after his experiences with the Forsters.\n\nThese conflicts continued after the journey with the problem of who should write the official account of the travels. Lord Sandwich, although willing to pay the promised money, was irritated with Johann Reinhold Forster's opening chapter and tried to have it edited. However, Forster did not want to have his writing corrected \"like a theme of a School-boy\", and stubbornly refused any compromise. As a result, the official account was written by Cook, and the Forsters were deprived of the right to compile the account and did not obtain payment for their work. During the negotiations, the younger Forster decided to release an unofficial account of their travels. In 1777, his book \"A Voyage Round the World in His Britannic Majesty's Sloop Resolution, Commanded by Capt. James Cook, during the Years, 1772, 3, 4, and 5\" was published. This report was the first account of Cook's second voyage (it appeared six weeks before the official publication) and was intended for the general public. The English version and his own translation into German (published 1778–80) earned the young author real fame. The poet Christoph Martin Wieland praised the book as the most important one of his time, and even today it remains one of the most important journey descriptions ever written. The book also had a significant impact on German literature, culture and science, influencing such scientists as Alexander von Humboldt and it inspired many ethnologists of later times.\n\nForster wrote well-polished German prose, which was not only scientifically accurate and objective, but also exciting and easy to read. This differed from conventional travel literature of the time, insofar as it presented more than a mere collection of data – it also demonstrated coherent, colourful and reliable ethnographical facts that resulted from detailed and sympathetic observation. He often interrupted the description to enrich it with philosophical remarks about his observations. His main focus was always on the people he encountered: their behavior, customs, habits, religions and forms of social organization. In \"A Voyage Round the World\" he even presented the songs sung by the people of Polynesia, complete with lyrics and notation. The book is one of the most important sources concerning the societies of the Southern Pacific from the times before European influence had become significant.\n\nBoth Forsters also published descriptions of their South Pacific travels in the Berlin-based \"Magazin von merkwürdigen neuen Reisebeschreibungen\" (\"\"Magazine of strange new travel accounts\"), and Georg published a translation of \"A Voyage to the South Sea, by Lieutenant William Bligh, London 1792\"\" in 1791–93.\n\nThe publication of \"A Voyage Round the World\" brought Forster scientific recognition all over Europe. The respectable Royal Society made him a member on January 9, 1777, though he was not even 23 years old. He was granted similar titles from academies ranging from Berlin to Madrid. These appointments, however, were unpaid.\n\nIn 1778, he went to Germany to take a teaching position as a Natural History professor at the Collegium Carolinum in Kassel, where he met Therese Heyne, the daughter of classicist Christian Gottlob Heyne. She later became one of the first independent female writers in Germany. They married in 1785 (which was after he left Kassel) and had three children, but their marriage was not happy. From his time in Kassel on, Forster actively corresponded with important figures of the Enlightenment, including Lessing, Herder, Wieland and Goethe. He also initiated cooperation between the Carolinum in Kassel and the University of Göttingen where his friend Georg Christoph Lichtenberg worked. Together, they founded and published the scientific and literary journal \"Göttingisches Magazin der Wissenschaften und Litteratur\". Forster's closest friend, Samuel Thomas von Sömmering, arrived in Kassel shortly after Forster, and both were soon involved with the Rosicrucians in Kassel.\n\nHowever, by 1783 Forster saw that his involvement with the Rosicrucians not only led him away from real science, but also deeper into debt (it is said he was not good at money); for these reasons Forster was happy to accept a proposal by the Polish–Lithuanian Commonwealth Komisja Edukacji Narodowej (Commission of National Education) and became Chair of Natural History at Vilnius University in 1784. Initially, he was accepted well in Vilnius, but he felt more and more isolated with time. Most of his contacts were still with scientists in Germany; especially notable is his dispute with Immanuel Kant about the definition of race. In 1785, Forster traveled to Halle where he submitted his thesis on the plants of the South Pacific for a doctorate in medicine. Back in Vilnius, Forster's ambitions to build a real natural history scientific center could not get appropriate financial support from the authorities in Polish–Lithuanian Commonwealth. Moreover, his famous speech on natural history in 1785 went almost unnoticed and was not printed until 1843. These events led to high tensions between him and the local community. Eventually, he broke the contract six years short of its completion as Catherine II of Russia had offered him a place on a journey around the world (the Mulovsky expedition) for a high honorarium and a position as a professor in Saint Petersburg. This resulted in a conflict between Forster and the influential Polish scientist Jędrzej Śniadecki. However, the Russian proposal was withdrawn and Forster left Vilnius. He then settled in Mainz, where he became head librarian of the University of Mainz, a position held previously by his friend Johannes von Müller, who made sure Forster would succeed him when Müller moved to the administration of Elector Friedrich Karl Josef von Erthal.\n\nForster regularly published essays on contemporary explorations and continued to be a very prolific translator; for instance, he wrote about Cook's third journey to the South Pacific, and about the Bounty expedition, as well as translating Cook's and Bligh's diaries from these journeys into German. From his London years, Forster was in contact with Sir Joseph Banks, the initiator of the Bounty expedition and a participant in Cook's first journey. While at the University of Vilnius he wrote the article \"Neuholland und die brittische Colonie in Botany-Bay\", published in the \"Allgemeines historisches Taschenbuch\" (Berlin, December 1786), an essay on the future prospects of the English colony founded in New South Wales in 1788.\n\nAnother interest of his was indology – one of the main goals of his failed expedition to be financed by Catherine II had been to reach India. He translated the Sanskrit play \"Shakuntala\" using a Latin version provided by Sir William Jones; this strongly influenced Herder and triggered German interest in the culture of India.\n\nIn the second quarter of 1790, Forster and the young Alexander von Humboldt started from Mainz on a long journey through the Southern Netherlands, the United Provinces, and England, eventually finishing in Paris. The impressions from the journey were described in a three volume publication \"Ansichten vom Niederrhein, von Brabant, Flandern, Holland, England und Frankreich im April, Mai und Juni 1790\" (\"Views of the Lower Rhine, from Brabant, Flanders, Holland, England, and France in April, May and June 1790\"), published 1791–94. Goethe said about the book: \"One wants, after one has finished reading, to start it over, and wishes to travel with such a good and knowledgeable observer.\" The book includes comments on the history of art that were as influential for the discipline as \"A Voyage Round the world\" was for ethnology. Forster was, for example, one of the first writers who gave just treatment to the Gothic architecture of Cologne Cathedral, which was widely perceived as \"barbarian\" at that time. The book conformed well to the early Romantic intellectual movements in German-speaking Europe.\n\nForster's main interest, however, was again focused on the social behavior of people, as 15 years earlier in the Pacific. The national uprisings in Flanders and Brabant and the revolution in France sparked his curiosity. The journey through these regions, together with the Netherlands and England, where citizens' freedoms were equally well developed, in the end helped him to resolve his own political opinions. From that time on he was to be a confident opponent of the ancien régime. With other German scholars, he welcomed the outbreak of the revolution as a clear consequence of the Enlightenment. As early as July 30, 1789, shortly after he heard about the Storming of the Bastille, he wrote to his father-in-law, philologist Christian Gottlob Heyne, that it was beautiful to see what philosophy had nurtured in people's minds and then had realized in the state. To educate people about their rights in this way, he wrote, was after all the surest way; the rest would then result as if by itself.\n\nThe French revolutionary army under General Custine gained control over Mainz on October 21, 1792. Two days later, Forster joined others in establishing a Jacobin Club called \"Freunde der Freiheit und Gleichheit\" (\"Friends of Freedom and Equality\") in the Electoral Palace. From early 1793 he was actively involved in organizing the Mainz Republic. This first republic located on German soil was constituted on the principles of democracy, and encompassed areas on the left bank of the Rhine between Landau and Bingen. Forster became vice-president of the republic's temporary administration and a candidate in the elections to the local parliament, the \"Rheinisch-Deutscher Nationalkonvent\" (\"Rhenish-German National Convention\"). From January to March 1793, he was an editor of \"Die neue Mainzer Zeitung oder Der Volksfreund\" (\"The new Mainz newspaper or The People's Friend\"). In his first article he wrote:\nThis freedom did not last long, though. The Mainz Republic existed only until the retreat of the French troops in July 1793 after the Siege of Mainz.\n\nForster was not present in Mainz during the siege. As representatives of the Mainz National Convention, he and Adam Lux had been sent to Paris to apply for Mainz – which was unable to exist as an independent state – to become a part of the French Republic. The application was accepted, but had no effect, since Mainz was conquered by Prussian and Austrian troops, and the old order was restored. Forster lost his library and collections and decided to remain in Paris.\n\nBased on a decree by Emperor Francis II inflicting punishments on German subjects who collaborated with the French revolutionary government, Forster was declared an outlaw and placed under the Imperial ban; a prize of 100 ducats was set on his head and he could not return to Germany. Devoid of all means of making a living and without his wife, who had stayed in Mainz with their children and her later husband Ludwig Ferdinand Huber, he remained in Paris. At this point the revolution in Paris had entered the Reign of Terror introduced by the Committee of Public Safety under the rule of Maximilien Robespierre. Forster had the opportunity to experience the difference between the promises of the revolution of happiness for all and its cruel practice. In contrast to many other German supporters of the revolution, like for instance Friedrich Schiller, Forster did not turn back from his revolutionary ideals under the pressure of the terror. He viewed the events in France as a force of nature that could not be slowed and that had to release its own energies to avoid being even more destructive.\n\nBefore the reign of terror reached its climax, Forster died after a rheumatic illness in his small attic apartment at Rue des Moulins in Paris in January 1794, at the age of thirty-nine. At the time, he was making plans to visit India.\n\nForster had partial Scottish roots and was born in Polish Royal Prussia, and therefore was by birth a Polish subject. He worked in Russia, England, Poland and in several German countries of his time. Finally, he finished his life in France. He worked in different milieus and traveled a lot from his youth on. It was his view that this, together with his scientific upbringing based on the principles of the Enlightenment, gave him a wide perspective on different ethnic and national communities:\n\nIn his opinion all human beings have the same abilities with regard to reason, feelings and imagination, but these basic ingredients are used in different ways and in different environments, which gives rise to different cultures and civilizations. According to him it is obvious that the culture on Tierra del Fuego is at a lower level of development than European culture, but he also admits that the conditions of life there are much more difficult and this gives people very little chance to develop a higher culture. Based on these opinions he was classified as one of the main examples of 18th-century German cosmopolitanism.\n\nIn contrast to the attitude expressed in these writings and to his Enlightenment background, he used insulting terms expressing prejudice against Poles in his private letters during his stay in Vilnius and in a diary from the journey through Poland, but he never published any manifestation of this attitude. These insults only became known after his death, when his private correspondence and diaries were released to the public. Since Forster's published descriptions of other nations were seen as impartial scientific observations, Forster's disparaging description of Poland in his letters and diaries was often taken at face value in Imperial and Nazi Germany, where it was used as a means of science-based support for a purported German superiority. The spreading of the \"Polnische Wirtschaft\" (Polish economy) stereotype is most likely due to the influence of his letters.\n\nForster's attitude brought him into conflict with the people of the different nations he encountered and made him welcome nowhere, as he was too revolutionary and antinational for Germans, proud and opposing in his dealings with Englishmen, too unconcerned about Polish science for Poles, and too insignificant politically and ignored while in France.\n\nAfter Forster's death, his works were mostly forgotten, except in professional circles. This was partly due to his involvement in the French revolution. However, his reception changed with the politics of the times, with different periods focusing on different parts of his work. In the period of rising nationalism after the Napoleonic era he was regarded in Germany as a \"traitor to his country\", overshadowing his work as an author and scientist. This attitude rose even though the philosopher Karl Wilhelm Friedrich Schlegel wrote about Forster at the beginning of the 19th century:\nSome interest in Forster's life and revolutionary actions was revived in the context of the liberal sentiments leading up to the 1848 revolution. But he was largely forgotten in the Germany of Wilhelm II and more so in the Third Reich, where interest in Forster was limited to his stance on Poland from his private letters. Interest in Forster resumed in the 1960s in East Germany, where he was interpreted as a champion of class struggle. The GDR research station in Antarctica that was opened on October 25, 1987, was named after him. In West Germany, the search for democratic traditions in German history also lead to a more diversified picture of him in the 1970s. The Alexander von Humboldt foundation named a scholarship program for foreign scholars from developing countries after him. His reputation as one of the first and most outstanding German ethnologists is indisputable, and his works are seen as crucial in the development of ethnology in Germany into a separate branch of science.\n\nThe ethnographical items collected by Georg and Johann Reinhold Forster are now presented as the \"Cook-Forster-Sammlung\" (\"Cook–Forster Collection\") in the Sammlung für Völkerkunde anthropological collection in Göttingen. Another collection of items collected by the Forsters is on display at the Pitt Rivers Museum in Oxford.\n\n\n\n\nThis article is partly based on a translation of the German Wikipedia article .\n\n"}
{"id": "178282", "url": "https://en.wikipedia.org/wiki?curid=178282", "title": "Geostationary transfer orbit", "text": "Geostationary transfer orbit\n\nA geosynchronous transfer orbit or geostationary transfer orbit (GTO) is a Hohmann transfer orbit—an elliptical orbit used to transfer between two circular orbits of different radii in the same plane—used to reach geosynchronous or geostationary orbit using high-thrust chemical engines.\n\nGeosynchronous orbits (GSO) are useful for various civilian and military purposes, but demand a great deal of delta-v to attain. Since, for station-keeping, satellites intended for this orbit typically carry highly efficient but low-thrust engines, total mass delivered to GSO is generally maximized if the launch vehicle provides only the delta-v required to be at high thrust, i.e., to escape Earth's atmosphere and overcome gravitational losses, and the satellite provides the delta-v required to turn the resulting intermediate orbit, which is the GTO, into the useful GSO.\n\nGTO is a highly elliptical Earth orbit with an apogee of , or above sea level, which corresponds to the geostationary altitude. The period of a standard geosynchronous transfer orbit is about 10.5 hours. The argument of perigee is such that apogee occurs on or near the equator. Perigee can be anywhere above the atmosphere, but is usually restricted to a few hundred kilometers above the Earth's surface to reduce launcher delta-V (formula_1) requirements and to limit the orbital lifetime of the spent booster so as to curtail space junk. If using low-thrust engines such as electrical propulsion to get from the transfer orbit to geostationary orbit, the transfer orbit can be supersynchronous (having an apogee above the final geosynchronous orbit). However, this method takes much longer to achieve due to the low thrust injected into the orbit. The typical launch vehicle injects the satellite to a supersynchronous orbit having the apogee above 42,164 km. The satellite's low-thrust engines are thrusted continuously around the geostationary transfer orbits in an inertial direction. This inertial direction is set to be in the velocity vector at apogee but with an outer plane direction. The outer plane direction removes the initial inclination set by the initial transfer orbit, while the inner plane direction raises simultaneously the perigee and lowers the apogee of the intermediate geostationary transfer orbit. In case of using the Hohmann transfer orbit, only a few days are required to reach the geosynchronous orbit. By using low-thrust engines or electrical propulsion, months are required until the satellite reaches its final orbit.\n\nThe orbital inclination of a GTO is the angle between the orbit plane and the Earth's equatorial plane. It is determined by the latitude of the launch site and the launch azimuth (direction). The inclination and eccentricity must both be reduced to zero to obtain a geostationary orbit. If only the eccentricity of the orbit is reduced to zero, the result may be a geosynchronous orbit but will not be geostationary. Because the formula_1 required for a plane change is proportional to the instantaneous velocity, the inclination and eccentricity are usually changed together in a single maneuver at apogee, where velocity is lowest.\n\nThe required formula_1 for an inclination change at either the ascending or descending node of the orbit is calculated as follows:\n\nFor a typical GTO with a semi-major axis of 24,582 km, perigee velocity is 9.88 km/s and apogee velocity is 1.64 km/s, clearly making the inclination change far less costly at apogee. In practice, the inclination change is combined with the orbital circularization (or \"apogee kick\") burn to reduce the total formula_1 for the two maneuvers. The combined formula_1 is the vector sum of the inclination change formula_1 and the circularization formula_1, and as the sum of the lengths of two sides of a triangle will always exceed the remaining side's length, total formula_1 in a combined maneuver will always be less than in two maneuvers. The combined formula_1 can be calculated as follows:\n\nwhere formula_12 is the velocity magnitude at the apogee of the transfer orbit and formula_13 is the velocity in GEO.\n\nEven at apogee, the fuel needed to reduce inclination to zero can be significant, giving equatorial launch sites a substantial advantage over those at higher latitudes. Baikonur Cosmodrome in Kazakhstan is at 46° north latitude. Kennedy Space Center is at 28.5° north. Guiana Space Centre, the Ariane launch facility, is at 5° north. Sea Launch launches from a floating platform directly on the equator in the Pacific Ocean.\n\nExpendable launchers generally reach GTO directly, but a spacecraft already in a low Earth orbit (LEO) can enter GTO by firing a rocket along its orbital direction to increase its velocity. This was done when geostationary spacecraft were launched from the space Shuttle; a \"perigee kick motor\" attached to the spacecraft ignited after the shuttle had released it and withdrawn to a safe distance.\n\nAlthough some launchers can take their payloads all the way to geostationary orbit, most end their missions by releasing their payloads into GTO. The spacecraft and its operator are then responsible for the maneuver into the final geostationary orbit. The 5-hour coast to first apogee can be longer than the battery lifetime of the launcher or spacecraft, and the maneuver is sometimes performed at a later apogee or split among multiple apogees. The solar power available on the spacecraft supports the mission after launcher separation. Also, many launchers now carry several satellites in each launch to reduce overall costs, and this practice simplifies the mission when the payloads may be destined for different orbital positions.\n\nBecause of this practice, launcher capacity is usually quoted as spacecraft mass to GTO, and this number will be higher than the payload that could be delivered directly into GEO.\n\nFor example, the capacity (adapter and spacecraft mass) of the Delta IV Heavy is 14,200kg to GTO, or 6,750kg directly to geostationary orbit.\n\nIf the manoeuvre from GTO to GEO is to be performed with a single impulse, as with a single solid-rocket motor, apogee must occur at an equatorial crossing and at synchronous orbit altitude. This implies an argument of perigee of either 0° or 180°. Because the argument of perigee is slowly perturbed by the oblateness of the Earth, it is usually biased at launch so that it reaches the desired value at the appropriate time (for example, this is usually the sixth apogee on Ariane 5 launches). If the GTO inclination is zero, as with Sea Launch, then this does not apply. (It also would not apply to an impractical GTO inclined at 63.4°; see Molniya orbit.)\n\nThe preceding discussion has primarily focused on the case where the transfer between LEO and GEO is done with a single intermediate transfer orbit. More complicated trajectories are sometimes used. For example, the Proton-M uses a set of three intermediate orbits, requiring five upper-stage rocket firings, to place a satellite into GEO from the high-inclination site of Baikonur Cosmodrome, in Kazakhstan. Because of Baikonur's high latitude and range safety considerations that block launches directly east, it requires less delta-v to transfer satellites to GEO by using a supersynchronous transfer orbit where the apogee (and the maneuver to reduce the transfer orbit inclination) are at a higher altitude than 35,786 km, the geosynchronous altitude. Proton even offers to perform a supersynchronous apogee maneuver up to 15 hours after launch.\n\n"}
{"id": "29953411", "url": "https://en.wikipedia.org/wiki?curid=29953411", "title": "Grand Technion Energy Program", "text": "Grand Technion Energy Program\n\nThe Nancy and Stephen Grand Technion Energy Program (GTEP) or Grand Technion Energy Program was established in 2007 at Technion - Israel Institute of Technology, which is Israel's first university, founded in 1912.\n\nGTEP's stated aim is to bring together Technion's researchers to discover and tap alternative and renewable energy sources, promote more efficient energy use, and reduce the environmental damage caused by the production of fossil fuels.\n\nGTEP is interdisciplinary, with members spanning the range from nano science through to applied engineering.\n\nThe stated GTEP mission is:\n\nMore than 40 faculty members from nine Technion faculties are involved in GTEP projects.\n\n\n\n\n\nIn 2011, GTEP submitted the winning proposal to the Israel Science Foundation, through the framework of the Israeli Centers for Research Excellence (I-CORE). As proposal coordinator, GTEP is coordinating top researchers to advance research into Solar fuels from Technion, the Weizmann Institute of Science and Ben-Gurion University of the Negev. The I-CORE for Solar Fuels includes nine existing faculty members from each university and 3 new faculty members in each school (a total of 36 members).\n\n\n\nGTEP houses Israel's only multidisciplinary graduate studies program in energy science and technology.\n\n"}
{"id": "24000964", "url": "https://en.wikipedia.org/wiki?curid=24000964", "title": "Iapetus Suture", "text": "Iapetus Suture\n\nThe Iapetus Suture is one of several major geological faults caused by the collision of several ancient land masses forming a suture. It represents in part the remains of what was once the Iapetus Ocean. Iapetus was the father of Atlas in Greek mythology, making his an appropriate name for what used to be called the 'Proto-Atlantic Ocean'. When the Atlantic Ocean opened, in the Cretaceous period, it took a slightly different line from that of the Iapetus suture, with some originally Laurentian rocks being left behind in north-west Europe and other, Avalonian, rocks remaining as part of Newfoundland.\n\nThe Iapetus Ocean was an ancient ocean which existed in the Southern Hemisphere approximately 600 million years ago and was bordered by several paleocontinents: Laurentia, Ganderia, Carolinia, Avalonia, and Baltica. During a series of geological events, the Salinic orogeny and Caledonian orogeny all three land masses began to converge upon each other slowly diminishing the ocean by subducting the oceanic crust. By the end of the Silurian period, approximately 420 million years ago, the ocean had disappeared. The geological fault zone resulting from the continental collision is known as the Iapetus Suture, named after the ocean it replaced.\n\nThe closure of Iapetus involved a complex and protracted collisional history of numerous micro-continents, volcanic arcs and back-arc basins that were accreted to Laurentia and Avalonia between the Early Ordovician and Late Silurian. The notion of a single suture zone in a complex orogen such as the Appalachian/Caledonian is unrealistic as several temporal and spatial distinct suture zones are present.\n\nThe Early Ordovician (Taconic) Baie Verte Line in Quebec and Newfoundland marks the boundary between the continental margin of Laurentia and Iapetan oceanic rocks, while the Early Ordovician (Penobscot) GRUB Line defines the contact between the vestiges of Iapetus and Ganderia. The Middle Ordovician Red Indian Line is considered the main Iapetan suture zone in that it separates peri-Laurentian and peri-Gondwanan oceanic elements. The Dog Bay Line is a younger feature in the Appalachians and delineates the terminal Iapetan suture in Newfoundland as it marks the Early Silurian (Salinic) collisional zone between Ganderia and Laurentia.\n\nThe suture runs through the north-eastern states of Maine, New Hampshire, Massachusetts and Connecticut. Rhode Island lies entirely to the east of the suture.\n\nThe estuary of the River Shannon (Ireland's largest river) follows the line of the Iapetus suture on the west coast. The suture reaches the east coast at Clogherhead in County Louth. Ireland's crust (and sedimentary rocks) to the north-west of the suture originally derives from Laurentia (proto-North America), while the crust to the south-east is Avalonian (\"European\").\n\nThe Niarbyl Fault is an exposed section of the Iapetus Suture as it crosses through the Irish Sea. It is readily visible in present-day Niarbyl on the southwest coast of Isle of Man. The fault is manifested by two major rock groups which abut each other, both deposited by their respective, aforementioned continents – the Manx Group to the south-east and the Dalby Group to the north-west. The Niarbyl fault is evidence of two paleocontinents colliding: Avalonia on which present-day England is located and Laurentia which contains both present-day North America and Scotland.\n\nThe fault is visible close to the shoreline downhill from the Niarbyl Cafe and Visitor Centre in Niarbyl.\n\nThe Caledonian orogeny united the northern and southern portions of present-day Great Britain. The Iapetus Suture runs from the Solway Firth to Lindisfarne.\n\n\n"}
{"id": "43087121", "url": "https://en.wikipedia.org/wiki?curid=43087121", "title": "Indian Institute of Ecology and Environment", "text": "Indian Institute of Ecology and Environment\n\nThe Indian Institute of Ecology and Environment (IIEE) is a Government of India sponsored, autonomous, non-profit making institution formed with a view to impart teaching and provide a sustained platform for research and consultancy in the areas of ecology and environment. The Institute is located at New Delhi, India.\n\nIIEE (ecology) as well as ASI (archaeology), BSI (botany), FSI (forests), FiSI (fisheries), GSI (geology), NIO (oceanography), RGCCI (Census of India), SI (cartography) and ZSI (zoology) are key national survey organisations of India.\n\nThe Indian Institute of Ecology and Environment, Paryavaran Complex, South of Saket, New Delhi owes its origin to the deliberations at Founes (1971), Stockholm (1972) and Belgrade (1975) and subsequent resolutions and recommendations of the Intergovernmental Conference on Environment at Tbilisi in 1977 organised by UNEP and UNESCO. The Institute \nwas established as a public charitable trust on 5 June 1980 for organising programs and providing learning platform in the areas of ecology, environment, pollution control, disaster management, sustainable development, ecological tourism and environmental education. The Institute also collaborates with the Government of Delhi in conducting ecological education in more than 1000 schools in New Delhi. The Institute arranges for technology transfer on environmental engineering, impact assessment, pollution monitoring and control, systems analysis, wildlife conservation, environmental communication, ecological education and environmental laws.\n\nIIEE is mandated to provide learning based assistance to the students and researchers to:\n\nIIEE has published over 150 books and monographs of which the most notable is a 30 volume International Encyclopaedia of Ecology and Environment which is considered as the only encyclopedia with a 10 volume section on Environment Laws.\n\nThe Institute has conducted over 180 national and international congresses and conventions on environment during the period 1981- 2013. It also assists Universities to design their curriculum on environment for graduate and postgraduate courses. It also conducts workshops worldwide in countries such as Sri Lanka, Italy, Spain, South Korea, Mongolia, Mauritius, USA, Tunisia, Russia, Poland, Uganda, Zambia, Ethiopia, England, Nepal and The Netherlands. Researches and consultancy assignments are another area IIEE engages in and has undertaken over 3500 such assignments.\n\nIIEE has the following facilities at the New Delhi campus. \n\nIIEE offers various master's degree courses, in affiliation with The Global Open University, Nagaland.\nThe Institute also offers MPhil programs on the subjects.\n\n"}
{"id": "49237898", "url": "https://en.wikipedia.org/wiki?curid=49237898", "title": "Indoor Environmental Quality Global Alliance", "text": "Indoor Environmental Quality Global Alliance\n\nThe Indoor Environmental Quality Global Alliance (IEQ-GA) was initiated in 2014 aiming to improve the actual, delivered indoor environmental quality in buildings through coordination, education, outreach and advocacy. The alliance works to supply information, guidelines and knowledge on the indoor environmental quality (IEQ) in buildings and workplaces, and to provide occupants in buildings and workplaces with an acceptable indoor environmental quality ((indoor air quality (IAQ), thermal conditions, visual quality, and acoustical quality)) and help promote implementation in practice of knowledge from research on the field.\n\nThe group has already begun work to collect and critique IEQ standards and is organising and presenting programmes at the conferences of member organisations and others.\n\nThe Alliance was officially launched on June 29, 2014 during ASHRAE’s 2014 Annual Conference in Seattle; a memorandum of understanding was signed and the Alliance was formed. Bill Bahnfleth, ASHRAE’s 2013-2014 president, appointed an ad-hoc committee to look at ways for industry groups to cooperate and address aspects of indoor environmental quality and health.\n\nFounding members of the Indoor Environmental Quality Global Alliance include organisations that broadly deal with the indoor environments as well as those specialised primarily in Indoor Air Quality(IAQ). \nThe Alliance’s current partners include the American Industrial Hygiene Association (AIHA), the Air Infiltration and Ventilation Centre (AIVC), the American Society of Heating, Refrigerating and Air-Conditioning Engineers (ASHRAE) the Air & Waste Management Association (A&WMA), the Indoor Air Quality Association (IAQA), and the Federation of European Heating and Air-Conditioning Associations (REHVA) . The Alliance is also supported by governmental organisations such as the World Health Organization.\n\n"}
{"id": "4520368", "url": "https://en.wikipedia.org/wiki?curid=4520368", "title": "Ionic transfer", "text": "Ionic transfer\n\nIonic transfer is the transfer of ions from one liquid phase to another. This is related to the phase transfer catalysts which are a special type of liquid-liquid extraction which is used in synthetic chemistry.\n\nFor instance nitrate anions can be transferred between water and nitrobenzene. One way to observe this is to use a cyclic voltammetry experiment where the liquid-liquid interface is the working electrode. This can be done by placing secondary electrodes in each phase and close to interface each phase has a reference electrode. One phase is attached to a potentiostat which is set to zero volts, while the other potentiostat is driven with a triangular wave. This experiment is known as a polarised Interface between Two Immiscible Electrolyte Solutions (ITIES) experiment.\n\nDiffusion potential\n"}
{"id": "3422423", "url": "https://en.wikipedia.org/wiki?curid=3422423", "title": "Isopycnic", "text": "Isopycnic\n\nAn isopycnic surface is a surface of constant density inside a fluid.\n\nIn geology, Isopycnic surfaces occur especially in connection with cratons which are very old geologic formations at the core of the continents, little affected by tectonic events. These formations are often known as shields or platforms. These formations are, relative to other lithospheric formations, cooler and less dense but much more isopycnic.\n\nIsopycnic surfaces contrast with isobaric or isothermal surfaces, which describe surfaces of constant pressure and constant temperature respectively. It is common in conversational use to hear isopycnic surfaces referred to simply as \"iso-density\" surfaces, which while strictly incorrect, is nonetheless abundantly more clear.\n\nThe term \"isopycnic\" is commonly encountered in the fluid dynamics of compressible fluids, such as in meteorology and geophysical fluid dynamics, astrophysics, or the fluid dynamics of explosions or high Mach number flows. It may also be applied to other situations where a continuous medium has smoothly-varying density, such as in the case of an inhomogeneous colloidal suspension. In general isopycnic surfaces will occur in fluids in hydrostatic equilibrium coinciding with equipotential surfaces formed by gravity.\n\nIsopycnic typically describes surfaces, not processes. Unless there is a flux of mass into or out of a control volume, a process which occurs at a constant density also occurs at a constant volume and is called an isochoric process and not an isopycnic process.\n\nThe term \"isopycnic\" is also encountered in biophysical chemistry, usually in reference to a process of separating particles, subcellular organelles, or other substances on the basis of their density. Isopycnic centrifugation refers to a method wherein a density gradient is either pre-formed or forms during high speed centrifugation. After this gradient is formed particles move within the gradient to the position having a density matching their own (this is in fact an incorrect description of the exact physical process but does describe the result in a meaningful way). This technique is extremely powerful.\n\n"}
{"id": "29854726", "url": "https://en.wikipedia.org/wiki?curid=29854726", "title": "Lanshan (Gansu)", "text": "Lanshan (Gansu)\n\nLanshan mountain range (Ch. 兰山) is a mountain range along northern shore of the Yellow River east from the capital Lanzhou of the Gansu province in northwestern China.\n\nThe area of the Lanshan mountains played a prominent role in the history of early China, \nchanging hands many times in the course of events, after the fall of the Han Dynasty there was located a capital of a succession of states, in the 4th century it was a capital of the independent state Earlier Liang, and from the 5th to the 11th century it became a center for Buddhist study in China.\n"}
{"id": "4448627", "url": "https://en.wikipedia.org/wiki?curid=4448627", "title": "List of Arizona hurricanes", "text": "List of Arizona hurricanes\n\nArizona has been affected by hurricanes on numerous occasions. Usually, these storms originate in the eastern Pacific Ocean, make landfall in the Mexican states of Baja California or Sonora, and dissipate before crossing into the United States. Thus, in most cases, it is only the tropical cyclones' remnant moisture that produces heavy rainfall—and in some occasions, flooding—in portions of Arizona. However, approximately every five years, a tropical cyclone retains sufficient strength to enter the state as a tropical storm or a tropical depression. Arizonans can expect indirect flash floods caused by the remnants of tropical cyclones to occur about every two years.\n\nTropical cyclones in Arizona are not common, since the predominant wind pattern steers most storms that form in the Eastern Pacific either parallel or away from the Pacific coast of northwestern Mexico. As a result, most storms that could affect Arizona are carried away from the United States, with only 6% of all Pacific hurricanes entering US territory. Not all Arizona hurricanes originate from the Pacific Ocean, however; in July 2008 an Atlantic hurricane named Hurricane Dolly produced rainfall in the eastern portion of the state, and another Atlantic storm reached Arizona as a tropical depression. Many, but not all, of these systems also impacted California.\n\nDespite their rarity, hurricanes are among Arizona's most significant weather makers. In years when Arizona is affected by a tropical cyclone, these can be responsible for up to 25% of the rainfall in areas along the Colorado River. Arizona hurricanes are also responsible for torrential rains in localized areas, with the state's 24-hour rainfall record— of precipitation—occurring during Hurricane Nora's landfall in 1997. The heavy rainfall can trigger extensive flash floods, such as the ones produced by the remnants of Tropical Storm Octave in 1983, or the lingering moisture from Tropical Storm Emilia in 2006.\n\nTropical cyclones are not common over Arizona, but on average, a tropical storm or a tropical depression enters the state approximately every five years. However, indirect flash floods caused by the remnants of tropical cyclones are more common, as they tend to occur about every two years.\n\nStorms that approach the southwestern United States, and by extension Arizona, generally form closer to the Mexican shoreline than average, making them more likely to recurve northwards under the influence of an approaching trough. These troughs tend to extend farther to the south during the latter part of the Pacific hurricane season, in the period between late August and early October. These pronounced troughs thus produce a synoptic-scale flow that is conducive to steering hurricanes towards the southwestern United States.\n\nThe infusions of tropical moisture from Arizona-bound tropical cyclones can be a significant portion of the rainfall in the region. In years when hurricanes approach Arizona, eastern and northern portions of the state receive on average 6–8% of the monsoon-season precipitation from tropical systems and their remnants. This percentage rises towards the southwestern corner of the state, which can receive up to a quarter of its monsoon-season rainfall from tropical cyclones.\n\nTropical storms are one of Arizona's main sources of rainfall, as they infuse the monsoon over the southwestern United States with moisture, producing large-scale floods in occasions. However, all of the storms that have impacted Arizona have formed in the latter parts of the Pacific hurricane season, and only storm remnants have affected the state before August.\n\nSeveral of these tropical cyclones have caused deaths or heavy property damage, usually due to flooding caused by rain.\n\nRecords of tropical cyclones in the East Pacific before 1950 are sparse, but there were still several storms that produced rainfall over Arizona in this period.\n\n\n\nThe 1980s saw destructive tropical cyclones pass through the state, as was the case with the previous decade.\n\nDuring the 1990s, several tropical systems affected Arizona even after losing all tropical characteristics. However, two hurricanes survived long enough to reach Arizona while still considered tropical systems.\n\n\n\nThe last decade saw no storms reach Arizona while retaining tropical characteristics; however, numerous remnant lows caused heavy rainfall and flooding throughout the state.\n\n\n"}
{"id": "2391648", "url": "https://en.wikipedia.org/wiki?curid=2391648", "title": "List of Croton species", "text": "List of Croton species\n\nThis is the alphabetical list of species belonging to the genus \"Croton\".\n"}
{"id": "5859260", "url": "https://en.wikipedia.org/wiki?curid=5859260", "title": "List of Sites of Special Scientific Interest in Cleveland", "text": "List of Sites of Special Scientific Interest in Cleveland\n\n<onlyinclude> This is a list of the Sites of Special Scientific Interest (SSSIs) in Cleveland, England, United Kingdom. In England the body responsible for designating SSSIs is Natural England, which chooses a site because of its fauna, flora, geological or physiographical features. , there are 18 sites designated within this Area of Search, of which 12 have been designated due to their biological interest, 4 due to their geological interest, and 2 (Durham Coast and Redcar Rocks) for both.\n</includeonly></onlyinclude>\n"}
{"id": "29160217", "url": "https://en.wikipedia.org/wiki?curid=29160217", "title": "List of earthquakes in the Philippines", "text": "List of earthquakes in the Philippines\n\nThe Philippines lies along the Pacific Ring of Fire, which causes the country to have frequent seismic and volcanic activity. Many earthquakes of smaller magnitude occur very regularly due to the meeting of major tectonic plates in the region. The largest was the 1918 Celebes Sea earthquake with .\n\n\n\n\n\n\n\nThe largest or most notable Philippine earthquakes per year since 2001. As for the repeated entries, the Moro Gulf is a seismically active area, the location of the devastating 1976 Moro Gulf earthquake which killed over 5,000 people, while Samar is near the Philippine Trench.\n\nAs the Philippines is subject to frequent seismic activity, to keep this list manageable, earthquakes with large magnitudes are prioritized (those with M < 6 are discouraged), unless the event has other noteworthy qualities such as causing fatalities, significant damage, or other notable consequences.\n\n\n\nThe table below is a tally of the ten deadliest recorded earthquakes in the Philippines since the 1600s:\n"}
{"id": "39040223", "url": "https://en.wikipedia.org/wiki?curid=39040223", "title": "List of ecoregions in Indiana", "text": "List of ecoregions in Indiana\n\nThe list of ecoregions in Indiana are listings of terrestrial ecoregions (see also, ecosystem) in the United States' State of Indiana, as defined separately by the United States Environmental Protection Agency (USEPA), and the World Wildlife Fund.\n\nThe USEPA ecoregion classification system has four levels, but only Levels I, III, and IV are shown on this list. Level I divides North America into 15 broad ecoregions (or biomes). Indiana is within the Eastern Temperate Forest environment, Level I region. Level IV ecoregions (denoted by numbers and letters) are a further subdivision of Level III ecoregions (denoted by numbers alone).\n\n\n"}
{"id": "15573397", "url": "https://en.wikipedia.org/wiki?curid=15573397", "title": "List of ecoregions in Nigeria", "text": "List of ecoregions in Nigeria\n\nThe following is a list of ecoregions in Nigeria, according to the Worldwide Fund for Nature (WWF).\n\n\"by major habitat type\"\n\n\n\n\n\n\n\"by bioregion\"\n\n\n\n\n"}
{"id": "8994257", "url": "https://en.wikipedia.org/wiki?curid=8994257", "title": "List of maize diseases", "text": "List of maize diseases\n\n"}
{"id": "58545056", "url": "https://en.wikipedia.org/wiki?curid=58545056", "title": "List of pipeline accidents in the United States in 2010", "text": "List of pipeline accidents in the United States in 2010\n\nThe following is a list of pipeline accidents in the United States in 2010. It is one of several lists of U.S. pipeline accidents. See also list of natural gas and oil production accidents in the United States.\n\nThis is not a complete list of all pipeline accidents. For natural gas alone, the Pipeline and Hazardous Materials Safety Administration (PHMSA), a United States Department of Transportation agency, has collected data on more than 3,200 accidents deemed serious or significant since 1987.\n\nA \"significant incident\" results in any of the following consequences:\n\nPHMSA and the National Transportation Safety Board (NTSB) post incident data and results of investigations into accidents involving pipelines that carry a variety of products, including natural gas, oil, diesel fuel, gasoline, kerosene, jet fuel, carbon dioxide, and other substances. Occasionally pipelines are repurposed to carry different products.\n\n"}
{"id": "18071628", "url": "https://en.wikipedia.org/wiki?curid=18071628", "title": "List of renewable energy companies by stock exchange", "text": "List of renewable energy companies by stock exchange\n\nSeveral renewable energy companies became listed on stock exchanges in the period after 2000. The early 21st century was a very productive time for the renewable energy industry, since many governments set long term renewable energy targets. Some chose to directly subsidize the renewables with feed-in tariffs and other temporary measures to bridge the gap to full cost accounting that would properly reward these technologies for their low emissions and lack of interference with ecosystem services, and also to ensure some capacity and motivation to install conservation-focused smart grid technologies.\n\nSmart grid policy in the United States was especially important in driving renewable energy in that country (see also load shedding, energy internet, home area network and cleantech for more on the structure of the industry and why renewables vendors are closely aligned industrially and politically with the vendors of networking technology), smart appliances and energy conservation software, and opposed in general to those of other \"energy\" firms, which effectively depend on the lack of this intelligence or accounting to appear competitive. For instance, coal vendors rely on a lack of accounting of about US$345 billion in harms done by coal to remain competitive in the US.\n\nIn 2004 Russia ratified the United Nations' Kyoto Protocol and the first order under this agreement came into force for the period 2008 to 2012. The period from 2002 to 2008 was a period of rapid growth for the renewable energy industry with the photovoltaics industry experiencing an average of 60% growth per annum, the biodiesel industry 42% and the wind industry 25%. As of the end of 2007 the renewable energy industry was worth an estimated US $77.3 billion. As a result of this growth several companies listed IPOs.\n\nMany public companies involved in the development of this industry and responsible for large market share do not participate exclusively in renewable energy and have been omitted for this list, most notable of these are BP, GE Energy and Sharp. This list consists of companies whose \"primary\" produce is either renewable energy or renewable energy products and services.\n\n\n"}
{"id": "4040150", "url": "https://en.wikipedia.org/wiki?curid=4040150", "title": "List of sovereign states and dependent territories in the Indian Ocean", "text": "List of sovereign states and dependent territories in the Indian Ocean\n\nThis is a list of sovereign states and dependencies in the Indian Ocean:\n\nHeading roughly clockwise, the states and territories (in italics) with a coastline on the Indian Ocean (including the Red Sea and Persian Gulf) are:\n\nAfrica\n\nAsia\nAustralasia\n\n\n\n"}
{"id": "7533718", "url": "https://en.wikipedia.org/wiki?curid=7533718", "title": "List of species of special concern in Rabun County, Georgia", "text": "List of species of special concern in Rabun County, Georgia\n\nLists of animal and plant species that are endangered, threatened, rare or uncommon in Rabun County, Georgia are maintained by both the United States Fish and Wildlife Service and the Georgia Department of Natural Resources.\n\nThis lists the animal species that are endangered, threatened, rare or uncommon in Rabun County.\n\nThis lists the animal species that are endangered, threatened, rare or uncommon in Rabun County.\n"}
{"id": "822008", "url": "https://en.wikipedia.org/wiki?curid=822008", "title": "Lists of invasive species", "text": "Lists of invasive species\n\nThese are lists of invasive species by country or region. A species is regarded as invasive if it has been introduced by human action to a location, area, or region where it did not previously occur naturally (i.e., is not a native species), becomes capable of establishing a breeding population in the new location without further intervention by humans, and becomes a pest in the new location, threatening agriculture and/or the local biodiversity.\n\nThe term invasive species refers to a subset of those species defined as introduced species, for which see List of introduced species.\n\n\n\n"}
{"id": "820289", "url": "https://en.wikipedia.org/wiki?curid=820289", "title": "London Convention on the Prevention of Marine Pollution by Dumping of Wastes and Other Matter", "text": "London Convention on the Prevention of Marine Pollution by Dumping of Wastes and Other Matter\n\nThe Convention on the Prevention of Marine Pollution by Dumping of Wastes and Other Matter 1972, commonly called the \"London Convention\" or \"LC '72\" and also abbreviated as Marine Dumping, is an agreement to control pollution of the sea by dumping and to encourage regional agreements supplementary to the Convention. It covers the deliberate disposal at sea of wastes or other matter from vessels, aircraft, and platforms. It does not cover discharges from land-based sources such as pipes and outfalls, wastes generated incidental to normal operation of vessels, or placement of materials for purposes other than mere disposal, providing such disposal is not contrary to aims of the Convention. It entered into force in 1975. As of September 2016, there were 89 Parties to the Convention.\n\nThe Convention was called for by the United Nations Conference on the Human Environment (June 1972, Stockholm), the treaty was drafted at the Intergovernmental Conference on the Convention on the Dumping of Wastes at Sea (13 November 1972, London) and it was opened for signature on 29 December 1972. It entered into force on 30 August 1975 when 15 nations ratified. As of 1 October 2001, there were 78 Contracting Parties to the Convention. International Administration of the Convention functions through Consultative Meetings held at International Maritime Organization (IMO) headquarters in London.\n\nThe London Convention consists of 22 Articles and three Annexes. It follows a \"black list/grey list\" approach to regulating ocean dumping; Annex I materials (black list) generally may not be ocean dumped (though for certain Annex I materials dumping may be permissible if present only as \"trace contaminants\" or \"rapidly rendered harmless\" and Annex II materials (grey list) require \"special care\". Annex III lays out general technical factors to be considered in establishing criteria for issuance of ocean dumping permits.\n\nThe main objective of the London Convention is to prevent indiscriminate disposal at sea of wastes that could be liable for creating hazards to human health; harming living resources and marine life; damaging amenities; or interfering with other legitimate uses of the sea. The 1972 Convention extends its scope over \"all marine waters other than the internal waters\" of the States and prohibits the dumping of certain hazardous materials. It further requires a prior special permit for the dumping of a number of other identified materials and a prior general permit for other wastes or matter.\n\nSince its entering into force in 1975, the convention has provided a framework for international control and prevention of marine pollution within which the contracting parties have achieved continuous progress in keeping the oceans clean. Among its milestones are the 1993 ban on ocean disposal of low-level radioactive wastes and the resolutions to end the dumping and incineration of industrial wastes. The efforts of the Parties are supported by a permanent secretariat hosted by the International Maritime Organization (IMO). The consultative meeting of the contracting parties to the London convention is the governing and political decision-making body of the convention. It takes advice on issues needing multidisciplinary expertise from the Joint Group of Experts on Scientific Aspects of Marine Environmental Protection (GESAMP) which is composed of specialised experts nominated by the IMO, FAO, UNESCO, IOC, WMO, WHO, IAEA, UN, and UNEP. A scientific group on dumping, composed of government experts from the parties to the convention a responsible to address any scientific requests from the consultative meeting, including the preparation of lists of hazardous substances, developing guide-lines on the implementation of the convention, and maintaining awareness of the impacts on the marine environments of inputs from all waste sources.\n\nThe convention is implemented in the United States through Title I of the Marine Protection, Research, and Sanctuaries Act (MPRSA) which directs that implementing regulations are to apply binding requirements of LC to the extent that this would not relax the MPRSA.\n\nOn 17 November 1996, a special meeting of the Contracting Parties adopted the \"1996 Protocol to the Convention on the Prevention of Marine Pollution by Dumping of Wastes and Other Matter, 1972\" which is to replace the 1972 Convention, subject to ratification. In line with UNCED's Agenda 21, the 1996 Protocol reflects the global trend towards precaution and prevention with the parties agreeing to move from controlled dispersal at sea of a variety of land-generated wastes towards integrated land-based solutions for most, and controlled sea disposal of few, remaining categories of wastes or other matter.\n\nAmong the most important innovations brought by the 1996 protocol is the codification of the \"precautionary approach\" and the \"polluter pays principle.\" Reflecting these principles, the protocol embodies a major structural revision of the convention the so-called \"reverse list\" approach. Now, instead of prohibiting the dumping of certain (listed) hazardous materials, the parties are obligated to prohibit the dumping of any waste or other matter that is not listed in Annex 1 (\"the reverse list\") of the 1996 protocol. Dumping of wastes or other matter on this reverse list requires a permit. Parties to the protocol are further obligated to adopt measures to ensure that the issuance of permits and permit conditions for the dumping of reverse list substances comply with Annex 2 (the Waste Assessment Annex) of the protocol. The substances on the reverse list include dredged material; sewage sludge; industrial fish processing waste; vessels and offshore platforms or other man-made structures at sea; inert, inorganic geological material; organic material of natural origin; and bulky items including iron, steel, concrete and similar materials for which the concern is physical impact, and limited to those circumstances where such wastes are generated at locations with no land-disposal alternatives. In addition, the 1996 protocol prohibits altogether the practice of incineration at sea, except for emergencies, and prohibits the exports of wastes or other matter to non-Parties for the purpose of dumping or incineration at sea.\n\nThe 1996 protocol has effectively moved the scope of the original London convention landwards, relating it to the policy and management issues of land as well as sea wastes disposal. Indicative for this shift are such elements as the codification of the precautionary approach and the establishment of requirements such as the \"waste prevention audit,\" the identification and control of the sources of contamination for certain materials, and the collaboration with relevant local and national agencies that are involved in point and non-point source pollution control. In this context, Integrated Coastal Management (ICM) comes as a natural framework for effective implementation of the objectives of the protocol. Relaying on its vast ICM technical expertise, the National Ocean Service (NOS) is to contribute to the creation of the necessary foundation for the US accession to the 1996 Protocol and, further on, to the protocol's implementation. Through its International Program Office, NOS would also contribute to the international co-operation efforts towards meeting the objectives of the 1996 Protocol.\n\n\"state parties -\" (87 as of 2013) Afghanistan, Antigua and Barbuda, Argentina, Australia, Azerbaijan, Barbados, Belarus (ratified as Byelorussian SSR), Belgium, Benin, Bolivia, Brazil, Bulgaria, Canada, Cape Verde, Chile, People's Republic of China, Democratic Republic of the Congo, Costa Rica, Côte d'Ivoire, Croatia, Cuba, Cyprus, Denmark, Dominican Republic, Egypt, Equatorial Guinea, Finland, France, Gabon, Germany, Greece, Guatemala, Haiti, Honduras, Hungary, Iceland, Iran, Ireland, Italy, Jamaica, Japan, Jordan, Kenya, Kiribati, South Korea, Libya, Luxembourg, Malta, Mexico, Monaco, Montenegro, Morocco, Nauru, Netherlands, New Zealand, Nigeria, Norway, Oman, Pakistan, Panama, Papua New Guinea, Peru, Philippines, Poland, Portugal, Russia (ratified as the Soviet Union), Saint Lucia, Saint Vincent and the Grenadines, Serbia (ratified as Serbia and Montenegro), Seychelles, Sierra Leone, Slovenia, Solomon Islands, South Africa, Spain, Suriname, Sweden, Switzerland, Syria, Tanzania, Tonga, Tunisia, Ukraine (ratified as Ukrainian SSR), United Arab Emirates, United Kingdom, United States, and Vanuatu.\n\n\n"}
{"id": "18309", "url": "https://en.wikipedia.org/wiki?curid=18309", "title": "Lucifer", "text": "Lucifer\n\nLucifer ( ; \"light-bringer\") was a Latin name for the planet Venus as the morning star in the ancient Roman era, and is often used for mythological and religious figures associated with the planet. Due to the unique movements and discontinuous appearances of Venus in the sky, mythology surrounding these figures often involved a fall from the heavens to earth or the underworld. Interpretations of a similar term in the Hebrew Bible, translated in the King James Version as \"Lucifer\", led to a Christian tradition of applying the name Lucifer and its associated stories of a fall from heaven to Satan. Most modern scholarship regards these interpretations as questionable, and translate the term in the relevant Bible passage as \"morning star\" or \"shining one\" rather than as a proper name, \"Lucifer\".\n\nAs a name for the Devil, the more common meaning in English, \"Lucifer\" is the rendering of the Hebrew word in Isaiah () given in the King James Version of the Bible. The translators of this version took the word from the Latin Vulgate, which translated הֵילֵל by the Latin word \"lucifer\" (uncapitalized), meaning \"the morning star, the planet Venus\", or, as an adjective, \"light-bringing\".\n\nAs a name for the morning star, \"Lucifer\" is a proper name and is capitalized in English. In Greco-Roman civilization the morning star was often personified and considered a god or the title of a deity associated with the planet.\n\nThe motif of a heavenly being striving for the highest seat of heaven only to be cast down to the underworld has its origins in the motions of the planet Venus, known as the morning star.\n\nThe Sumerian goddess Inanna (Babylonian Ishtar) is associated with the planet Venus. Inanna's actions in several of her myths, including \"Inanna and Shukaletuda\" and \"Inanna's Descent into the Underworld\" appear to parallel the motion of Venus as it progresses through its synodic cycle. For example, in \"Inanna's Descent to the Underworld\", Inanna is able to descend into the netherworld, where she is killed, and then resurrected three days later to return to the heavens. The three-day disappearance of Inanna refers to the three-day planetary disappearance of Venus between its appearance as a morning and evening star.\n\nA similar theme is present in the Babylonian myth of Etana. The \"Jewish Encyclopedia\" comments:\n\nThe fall from heaven motif also has a parallel in Canaanite mythology. In ancient Canaanite religion, the morning star is personified as the god Attar, who attempted to occupy the throne of Ba'al and, finding he was unable to do so, descended and ruled the underworld. The original myth may have been about a lesser god Helel trying to dethrone the Canaanite high god El who lived on a mountain to the north. Hermann Gunkel's reconstruction of the myth told of a mighty warrior called Hêlal, whose ambition was to ascend higher than all the other stellar divinities, but who had to descend to the depths; it thus portrayed as a battle the process by which the bright morning star fails to reach the highest point in the sky before being faded out by the rising sun. However, the Eerdmans Commentary on the Bible argues that no evidence has been found of any Canaanite myth or imagery of a god being forcibly thrown from heaven, as in the \"Book of Isaiah\" (see below). It argues that the closest parallels with \"Isaiah\"'s description of the king of Babylon as a fallen morning star cast down from heaven are to be found not in Canaanite myths but in traditional ideas of the Jewish people, echoed in the Biblical account of the fall of Adam and Eve, cast out of God's presence for wishing to be as God, and the picture in of the \"gods\" and \"sons of the Most High\" destined to die and fall. This Jewish tradition has echoes also in Jewish pseudepigrapha such as 2 Enoch and the \"Life of Adam and Eve\". The \"Life of Adam and Eve\", in turn, shaped the idea of Iblis in the Quran.\n\nThe Greek myth of Phaethon, a personification of the planet Jupiter, follows a similar pattern.\n\nIn classical mythology, Lucifer (\"light-bringer\" in Latin) was the name of the planet Venus, though it was often personified as a male figure bearing a torch. The Greek name for this planet was variously Phosphoros (also meaning \"light-bringer\") or Heosphoros (meaning \"dawn-bringer\"). Lucifer was said to be \"the fabled son of Aurora and Cephalus, and father of Ceyx\". He was often presented in poetry as heralding the dawn.\n\nThe second century Roman mythographer Pseudo-Hyginus said of the planet: \n\nOvid, in his first century epic \"Metamorphoses\", describes Lucifer as ordering the heavens:\n\nIn the classical Roman period, Lucifer was not typically regarded as a deity and had few, if any, myths, though the planet was associated with various deities and often poetically personified. Cicero pointed out that \"You say that Sol the Sun and Luna the Moon are deities, and the Greeks identify the former with Apollo and the latter with Diana. But if Luna (the Moon) is a goddess, then Lucifer (the Morning-Star) also and the rest of the Wandering Stars (Stellae Errantes) will have to be counted gods; and if so, then the Fixed Stars (Stellae Inerrantes) as well.\"\n\nIn the Book of Isaiah, chapter 14, the King of Babylon is condemned in a prophetic vision by the prophet Isaiah and is called (, Hebrew for \"shining one, son of the morning\"). who is addressed as הילל בן שחר (\"Hêlêl ben Šāḥar\"), The title \"Helel ben Shahar\" may refer to the planet Venus as the morning star, but the text in Isaiah 14 gives no indication that Helel is the name of a star or planet. The Hebrew word transliterated as \"Hêlêl\" or \"Heylel\" (pron. as \"Hay-LALE\"), occurs only once in the Hebrew Bible. The Septuagint renders הֵילֵל in Greek as Ἑωσφόρος (\"heōsphoros\"), \"bringer of dawn\", the Ancient Greek name for the morning star. According to the King James Bible-based Strong's Concordance, the original Hebrew word means \"shining one, light-bearer\", and the translation given in the King James text is the Latin name for the planet Venus, \"Lucifer\".\n\nHowever, the translation of הֵילֵל as \"Lucifer\" has been abandoned in modern English translations of Isaiah 14:12. Present-day translations render הֵילֵל as \"morning star\" (New International Version, New Century Version, New American Standard Bible, Good News Translation, Holman Christian Standard Bible, Contemporary English Version, Common English Bible, Complete Jewish Bible), \"daystar\" (New Jerusalem Bible, The Message), \"Day Star\" (New Revised Standard Version, English Standard Version), \"shining one\" (New Life Version, New World Translation, JPS Tanakh), or \"shining star\" (New Living Translation). \n\nIn a modern translation from the original Hebrew, the passage in which the phrase \"Lucifer\" or \"morning star\" occurs begins with the statement: \"On the day the Lord gives you relief from your suffering and turmoil and from the harsh labour forced on you, you will take up this taunt against the king of Babylon: How the oppressor has come to an end! How his fury has ended!\" After describing the death of the king, the taunt continues:\n\nJ. Carl Laney has pointed out that in the final verses here quoted, the king of Babylon is described not as a god or an angel but as a man; and that man may have been not Nebuchadnezzar II, but rather his son, Belshazzar. Nebuchadnezzar was gripped by a spiritual fervor to build a temple to the moon god Sin (possibly analogous with Hubal, the primary god of pre-Islamic Mecca), and his son ruled as regent. The Abrahamic scriptural texts could be interpreted as a weak usurping of true kingly power, and a taunt at the failed regency of Belshazzar.\n\nFor the unnamed \"king of Babylon\" a wide range of identifications have been proposed. They include a Babylonian ruler of the prophet Isaiah's own time the later Nebuchadnezzar II, under whom the Babylonian captivity of the Jews began, or Nabonidus, and the Assyrian kings Tiglath-Pileser, Sargon II and Sennacherib. Verse 20 says that this king of Babylon will not be \"joined with them [all the kings of the nations] in burial, because thou hast destroyed thy land, thou hast slain thy people; the seed of evil-doers shall not be named for ever\", but rather be cast out of the grave, while \"All the kings of the nations, all of them, sleep in glory, every one in his own house\", pointing to Nebuchadnezzar II as a possible interpretation. Herbert Wolf held that the \"king of Babylon\" was not a specific ruler but a generic representation of the whole line of rulers.\n\nIsaiah 14:12 became a source for the popular conception of the fallen angel motif seen later in 1 Enoch 86–90 and 2 Enoch 29:3–4. Rabbinical Judaism has rejected any belief in rebel or fallen angels. In the 11th century, the \"Pirqe de-Rabbi Eliezer\" illustrates the origin of the \"fallen angel myth\" by giving two accounts, one relates to the angel in the Garden of Eden who seduces Eve, and the other relates to the angels, the \"benei elohim\" who cohabit with the daughters of man (Genesis 6:1–4). An association of Isaiah 14:12–18 with a personification of evil, called the devil developed outside of mainstream Rabbinic Judaism in pseudepigrapha and Christian writings, particularly with the apocalypses.\n\nSome Christian writers have applied the name \"Lucifer\" as used in the Book of Isaiah, and the motif of a heavenly being cast down to the earth, to Satan. Sigve K Tonstad argues that the New Testament War in Heaven theme of , in which the dragon \"who is called the devil and Satan … was thrown down to the earth\", was derived from the passage about the Babylonian king in Isaiah 14. Origen (184/185 – 253/254) interpreted such Old Testament passages as being about manifestations of the Devil; but writing in Greek, not Latin, he did not identify the devil with the name \"Lucifer\". Tertullian (c. 160 – c. 225), who wrote in Latin, also understood (\"I will ascend above the tops of the clouds; I will make myself like the Most High\") as spoken by the Devil, but \"Lucifer\" is not among the numerous names and phrases he used to describe the devil. Even at the time of the Latin writer Augustine of Hippo (354–430), \"Lucifer\" had not yet become a common name for the Devil.\n\nSome time later, the metaphor of the morning star that Isaiah 14:12 applied to a king of Babylon gave rise to the general use of the Latin word for \"morning star\", capitalized, as the original name of the devil before his fall from grace, linking Isaiah 14:12 with (\"I saw Satan fall like lightning from heaven\") and interpreting the passage in Isaiah as an allegory of Satan's fall from heaven.\n\nAs a result, \"Lucifer has become a byword for Satan or the Devil in the church and in popular literature\", as in Dante Alighieri's \"Inferno\", Joost van den Vondel's \"Lucifer\", and John Milton's \"Paradise Lost\". However, unlike the English word, the Latin word was not used exclusively in this way and was applied to others also, including Jesus. \n\nAdherents of the King James Only movement and others who hold that Isaiah 14:12 does indeed refer to the devil have decried the modern translations. Jealousy of humans, created in the divine image and given authority over the world is the motive that a modern writer, who denies that there is any such person as Lucifer, says that Tertullian attributed to the devil, and, while he cited Tertullian and Augustine as giving envy as the motive for the fall, an 18th-century French Capuchin preacher himself described the rebel angel as jealous of Adam's exaltation, which he saw as a diminution of his own status.\n\nHowever, the understanding of the morning star in Isaiah 14:12 as a metaphor referring to a king of Babylon continued also to exist among Christians. Theodoret of Cyrus (c. 393 – c. 457) wrote that Isaiah calls the king \"morning star\", not as being the star, but as having had the illusion of being it. The same understanding is shown in Christian translations of the passage, which in English generally use \"morning star\" rather than treating the word as a proper name, \"Lucifer\". So too in other languages, such as French, German, Portuguese, and Spanish. Even the Vulgate text in Latin is printed with lower-case \"lucifer\" (morning star), not upper-case \"Lucifer\" (proper name).\n\nCalvin said: \"The exposition of this passage, which some have given, as if it referred to Satan, has arisen from ignorance: for the context plainly shows these statements must be understood in reference to the king of the Babylonians.\" Luther also considered it a gross error to refer this verse to the devil.\nIn the Bogomil and Cathar text \"Gospel of the secret supper\", Lucifer is a glorified angel and the older brother of Jesus, but fell from heaven to establish his own kingdom and became the Demiurge. Therefore, he created the material world and trapped souls from heaven inside matter. Jesus descended to earth to free the captured souls. In contrast to mainstream Christianity, the cross was denounced as a symbol of Lucifer and his instrument in an attempt to kill Jesus.\n\nLucifer is regarded within The Church of Jesus Christ of Latter-day Saints as the pre-mortal name of the devil. Mormon theology teaches that in a heavenly council, Lucifer rebelled against the plan of God the Father and was subsequently cast out. Mormon scripture reads:\"And this we saw also, and bear record, that an angel of God who was in authority in the presence of God, who rebelled against the Only Begotten Son whom the Father loved and who was in the bosom of the Father, was thrust down from the presence of God and the Son, and was called Perdition, for the heavens wept over him—he was Lucifer, a son of the morning. And we beheld, and lo, he is fallen! is fallen, even a son of the morning! And while we were yet in the Spirit, the Lord commanded us that we should write the vision; for we beheld Satan, that old serpent, even the devil, who rebelled against God, and sought to take the kingdom of our God and his Christ—Wherefore, he maketh war with the saints of God, and encompasseth them round about.\"After becoming Satan by his fall, Lucifer \"goeth up and down, to and fro in the earth, seeking to destroy the souls of men\". Mormons consider Isaiah 14:12 to be referring to both the king of the Babylonians and the devil.\n\nOther instances of \"lucifer\" in the Old Testament pseudepigrapha are related to the \"star\" Venus, in the Sibylline Oracles battle of the constellations (line 517) \"Lucifer fought mounted on the back of Leo\", or the entirely rewritten Christian version of the Greek Apocalypse of Ezra 4:32 which has a reference to Lucifer as Antichrist.\n\nIndications that in Christian tradition the Latin word \"lucifer\", unlike the English word, did not necessarily call a fallen angel to mind exist also outside the text of the Vulgate. Two bishops bore that name: Saint Lucifer of Cagliari, and Lucifer of Siena.\n\nIn Latin, the word is applied to John the Baptist and is used as a title of Jesus himself in several early Christian hymns. The morning hymn \"Lucis largitor splendide\" of Hilary contains the line: \"Tu verus mundi lucifer\" (you are the true light bringer of the world). Some interpreted the mention of the morning star (\"lucifer\") in Ambrose's hymn \"Aeterne rerum conditor\" as referring allegorically to Jesus and the mention of the cock, the herald of the day (\"praeco\") in the same hymn as referring to John the Baptist. Likewise, in the medieval hymn \"Christe qui lux es et dies\", some manuscripts have the line \"Lucifer lucem proferens\".\n\nThe Latin word \"lucifer\" is also used of Jesus in the Easter Proclamation prayer to God regarding the paschal candle: \"Flammas eius lucifer matutinus inveniat: ille, inquam, lucifer, qui nescit occasum. Christus Filius tuus, qui, regressus ab inferis, humano generi serenus illuxit, et vivit et regnat in saecula saeculorum\" (\"May this flame be found still burning by the Morning Star: the one Morning Star who never sets, Christ your Son, who, coming back from death's domain, has shed his peaceful light on humanity, and lives and reigns for ever and ever\"). In the works of Latin grammarians, Lucifer, like Daniel, was discussed as an example of a personal name.\n\nRudolf Steiner's writings, which formed the basis for Anthroposophy, characterised Lucifer as a spiritual opposite to Ahriman, with Christ between the two forces, mediating a balanced path for humanity. Lucifer represents an intellectual, imaginative, delusional, otherworldly force which might be associated with visions, subjectivity, psychosis and fantasy. He associated Lucifer with the religious/philosophical cultures of Egypt, Rome and Greece. Steiner believed that Lucifer, as a supersensible Being, had incarnated in China about 3000 years before the birth of Christ.\n\nLuciferianism is a belief system that venerates the essential characteristics that are affixed to Lucifer. The tradition, influenced by Gnosticism, usually reveres Lucifer not as the devil, but as a liberator, a guardian or guiding spirit or even the true god as opposed to Jehovah.\n\nIn Anton LaVey's \"The Satanic Bible\", Lucifer is one of the four crown princes of hell, particularly that of the East, the 'lord of the air', and is called the bringer of light, the morning star, intellectualism, and enlightenment. The title 'lord of the air' is based upon Ephesians 2:2, which uses the phrase 'prince of the power of the air' to refer to the pagan god Zeus, but that phrase later became conflated with Satan.\n\nAuthor Michael W. Ford has written on Lucifer as a \"mask\" of the adversary, a motivator and illuminating force of the mind and subconscious.\n\nLéo Taxil (1854–1907) claimed that Freemasonry is associated with worshipping Lucifer. In what is known as the Taxil hoax, he alleged that leading Freemason Albert Pike had addressed \"The 23 Supreme Confederated Councils of the world\" (an invention of Taxil), instructing them that Lucifer was God, and was in opposition to the evil god Adonai. Supporters of Freemasonry contend that, when Albert Pike and other Masonic scholars spoke about the \"Luciferian path,\" or the \"energies of Lucifer,\" they were referring to the Morning Star, the light bearer, the search for light; the very antithesis of dark, satanic evil. Taxil promoted a book by Diana Vaughan (actually written by himself, as he later confessed publicly) that purported to reveal a highly secret ruling body called the Palladium, which controlled the organization and had a satanic agenda. As described by \"Freemasonry Disclosed\" in 1897:\nTaxil's work and Pike's address continue to be quoted by anti-masonic groups.\n\nIn \"Devil-Worship in France\", Arthur Edward Waite compared Taxil's work to today's tabloid journalism, replete with logical and factual inconsistencies.\n\nIn a collection of folklore and magical practices supposedly collected in Italy by Charles Godfrey Leland and published in his \"Aradia, or the Gospel of the Witches\", the figure of Lucifer is featured prominently as both the brother and consort of the goddess Diana, and father of Aradia, at the center of an alleged Italian witch-cult. In Leland's mythology, Diana pursued her brother Lucifer across the sky as a cat pursues a mouse. According to Leland, after dividing herself into light and darkness:\n\nHere, the motions of Diana and Lucifer once again mirror the celestial motions of the moon and Venus, respectively. Though Leland's Lucifer is based on the classical personification of the planet Venus, he also incorporates elements from Christian tradition, as in the following passage: \n\nIn the several modern Wiccan traditions based in part on Leland's work, the figure of Lucifer is usually either omitted or replaced as Diana's consort with either the Etruscan god Tagni, or Dianus (Janus, following the work of folklorist James Frazer in \"The Golden Bough\").\n\n\n"}
{"id": "3276306", "url": "https://en.wikipedia.org/wiki?curid=3276306", "title": "Magnetic photon", "text": "Magnetic photon\n\nIn physics, a magnetic photon is a hypothetical particle. It is a mixture of even and odd C-parity states and, unlike the normal photon, does not couple to leptons. It is predicted by certain extensions of electromagnetism to include magnetic monopoles. There is no experimental evidence for the existence of this particle, and several versions have been ruled out by negative experiments.\n\nThe magnetic photon was predicted in 1966 by Nobel laureate Abdus Salam.\n"}
{"id": "3868736", "url": "https://en.wikipedia.org/wiki?curid=3868736", "title": "Mima mounds", "text": "Mima mounds\n\nMima mounds are low, flattened, circular to oval, domelike, natural mounds that are composed of loose, unstratified, often gravelly sediment that is an overthickened A horizon. These mounds range in diameter from 3 to more than 50 m; in height 30 cm to greater than 2 m; and in density from several to greater than 50 mounds per hectare, at times forming conspicuous natural patterns. Mima mounds can be seen at the Mima Mounds Natural Area Preserve in Washington State.\n\nTheories for the origin of Mima mounds include burrowing by pocket gophers; accumulation of wind-blown (aeolian) sediments around vegetation to form coppice dunes or nebkhas; seismic ground shaking by major earthquakes, though none have been observed to form Mima mounds; and shrinking and swelling of clays in hog-wallow or gilgai landforms.\n\nThough the definitive Mima mounds are common in North America, it has not been shown that all North American mounds result from the same causes. Similar or at least reminiscent phenomena occur on all continents and the proposed causes do not occur in all regions that have been studied. Nor is it clear that all such mounds really are the same, either physically or functionally; for example, the so-called fairy circles of Southern Africa tend to be less mound-like and occur in different climatic and ecological conditions. Furthermore, it has been argued that the possibly distinct heuweltjies of the South Western Cape region of South Africa are of an origin far different from either.\n\nWithin the northwestern United States, Mima mounds typically are part of what is commonly known as hog-wallow landscape. This type of landscape typically has a shallow basement layer such as bedrock, hardpan, claypan, or densely bedded gravel. In the northwestern United States, Mima mounds also occur within landscapes where a permanent water table impedes drainage, creating waterlogged soil conditions for prolonged periods. Mima mounds are named after the Mima Prairie in Thurston County, Washington.\n\nMima mounds occur outside the northwestern United States in three major regions west of the Mississippi River as illustrated by Cox and Washburn. First, they are found in a strip consisting of northwest Baja California, western and north central California, and south central Oregon where they are typically known as \"hogwallow mounds\". Within this strip and the area of San Diego, they are often an integral part of the local vernal pool landscape. Second, they are found in a roughly north-south strip of the Great Plains containing parts of north central New Mexico, and central Colorado, and central Wyoming where they are typically called \"prairie mounds\". Third, they are found in an area containing parts of east Texas, western Louisiana, southeast Oklahoma, and southern Missouri where they are commonly known as either \"pimple mounds\" or \"prairie mounds\". Finally, isolated patches of Mima mounds also are found in Iowa, eastern North Dakota, and northwest Minnesota.\n\nWithin Louisiana, Missouri, Oklahoma, and Texas, pimple mounds, also called locally \"prairie mounds\" and \"natural mounds\", consist of low, flattened, circular to oval, domelike, mounds composed of loose, sandy loam or loamy sand. Typically, these mounds consist entirely of a thickened loamy and sandy A and E horizons lying either on a more or less flat or slightly, but noticeably depressed, clayey B horizon. Pimple mounds range in diameter from 6 m to more than 45 m; in height 30 cm to greater than 1.2 m; and in density from several to greater than 425 mounds per hectare. Unlike the Mima mounds of Oregon and Washington, pimple mounds are not limited to the relatively flat and poorly drained surfaces, i.e. late Pleistocene coastal and fluvial terraces. They also occur in abundance of the slopes, summits and crests of hills created by the deep erosion and dissection of unconsolidated and unlithified early Pleistocene and middle Pleistocene, Pliocene, and older coastal plain sediments. Rarely, pimple mounds that occur on these hillslopes are elongated in an upslope-downslope direction.\n\nExcavations made into the Washington mounds show that underneath a blanket of prairie grass lies a mixture of loose sand, fine gravel, and decayed plants. Not all mima mound features have the same structure though. One mound in Washington had a very complex soil profile: A horizon is a black sandy loam (due to charcoal content from aboriginal burning on the prairies), B horizon is a gravelly sandy loam, C horizon is an extremely gravelly sand.\n\nVernal pools are shallow surficial depressions that seasonally fill with water during winter and spring rains and dry up during dry summer months. They get their name from the recognition of the seasonality of the habitat and the springtime flora associated with them. Vernal pools form where an impermeable or very slowly permeable layer underlies small and shallow depressions and creates a perched water table. The impermeable or very slowly permeable layer typically consists of either soil horizons such as duripans or claypans or bedrock in the form of volcanic mud or lava flows.\n\nWithin California, vernal pools are quite commonly associated with Mima Mounds. These Mima Mounds are typically located on stable landforms that are greater than 100,000 years old. These landforms are characterized by strongly developed soils that usually have a relatively impermeable layer (claypan or silcrete duripan) in the subsoil. This impermeable layer locally impedes drainage and creates perched water levels and causes the formation of vernal pools within the intermound depressions that are associated with Mima Mounds. Vernal pools are typically small, shallow, and complex ephemeral wetlands that only have internal drainage because they are hydrologically isolated from perennial inflow by a ring of Mima Mounds. Although the ponded water that fills vernal pools comes and goes throughout the year, it is present at least for a short time in most years. However, within California, the mound-depression microrelief associated with Mima Mounds is just one of a variety of geographic settings within which vernal pools occur. For example, in the Modoc Plateau region of California, numerous vernal pools are found on the surface of volcanic mudflows and basalt lava flows where Mima Mounds are completely absent.\n\nOne theory on the origin of Mima mounds is that they were created by small burrowing rodents such as pocket gophers (\"Thomomys talpoides\") of the endemic North American family \"Geomyidae\". Researchers in the 1940s found that Mima mounds tend to form in areas with poorly draining soils, so the \"Fossorial Rodent Hypothesis\" proposed that gophers build mounds as an evolutionary response to low water tables. It could be argued that gophers live in the mounds opportunistically but did not build them. Metal tracers implanted in a Mima mound field in San Diego demonstrated that gophers unexpectedly pushed soil upwards towards the center of the mounds instead of pushing the soil downward. This uphill soil transport contrasts with the typical gopher behavior of pushing soil downhill but can be overridden when soil is saturated. Consequently, gophers in mima mound fields seem to be aware of randomly distributed topographic highs and orient their burrowing accordingly in early mound creation stages. However, the mounds were already fully formed and the gophers may have just been maintaining them. Nevertheless, the fact that the surface area of a typical Mima mound is similar to the size of an individual gopher's home range is consistent with the theory they were constructed by the rodents. Results from the tracer study were incorporated into a numerical model that simulated the burrowing behavior of gophers. The advantage of modelling in this case is that (1) an initially flat surface can be specified, and (2) time can be sped up. In the computer simulations, mounds naturally emerged from randomly distributed topographic highs, and reached topographic steady state after several centuries of gopher activity, which could explain why nobody has ever witnessed the growth of one. Once the mound field reaches topographic maturity, the mounds feature more uniform spacing and hexagonal tessellation. Results indicated that formation of these mound fields are largely contributed by positive feedback loops which amplify small features to create large scale patterns, a common facet of self-organization. The slow modeled mound growth rates and their spatial distribution agreed with field observations. Although occupation of mounds by gophers does not by itself prove that gophers built the mounds since they could be living there opportunistically, to date, this is the strongest evidence for the origin of these enigmatic features.\n\nThe publication of this modelling study received attention from the international press.\n\nAnother major theory concerning the origin of pimple and prairie mounds argues that they are either coppice dunes or nebkhas formed by the accumulation of wind-blown sediments around clumps of vegetation. For example, based on grain-size data of and optically stimulated luminescence ages obtained from pimple mounds in the south-central United States, Seifert and others concluded that these mounds consisted of wind blown sediments that accumulated during prolonged late Holocene droughts. They suggest that although they superficially resemble the mima mounds of the northwestern United States, the pimple mounds of south-central America have a greatly different origin from them.\n\nAndrew Berg, a geologist with the U.S. Bureau of Mines in Spokane, proposed that Mima and pimple mounds were the result of very intense ground shaking resulting from major earthquakes. He formulated this hypothesis while building a dog house. As he hammered together sheets of plywood coated with volcanic ash, he noticed that the hammering vibrations caused the ash to heap into small mounds that looked a lot like miniature Mima mounds. From that observation, Berg hypothesized that vibrations from violent earthquakes could have formed the Mima mounds. According to Berg, the soil on the Mima Prairie is like volcanic ash, and the layer of rock below that is like a plank of wood. When seismic waves move through the hard ground and bump into faults, or large fractures in the ground, the waves bounce backward. Those ricocheted waves collide with other seismic waves from the quake, and between the collision points, the soil rises and forms mounds. Berg claims that Mima mounds occur only in seismically active areas—areas where the ground is unstable and many earthquakes occur. The area where the Washington Mima mounds are found experienced a major earthquake about 1,000 years ago.\n\nHowever, since this hypothesis has been proposed, there have been many large earthquakes throughout the world and none have been reported to have formed Mima mounds. In addition, Mima mounds have been gradually growing on the Carrizo Plain (California) since the 1980s when plowing of the fields was halted. These mounds have been forming in the absence of any large earthquakes. Therefore, there is no geological evidence supporting the 'earthquake' hypothesis.\n\nWhen clay is exposed to large amounts of water, the water collects between the clay minerals (which are flat planes). Due to the shape of the minerals, the water travels in between the compacted layer, thus \"swelling\" the clay-bed into mound-like features.\nSilts are also related with this geomorphologic feature; however, silt is coarser-grained sediment so the minerals do not \"hold\" water in the same way. Silt is more penetrable than clay is. Shrink/swell soils are most often related to landforms called \"hog wallows\" or \"gilgai\" that can look similar to mima mounds.\n\nIn 2017, Corina Tarnita and several of her colleagues published a paper in \"Nature\" which explained these and other related self-organised vegetation patterns by means of a general theory which integrates scale-dependent feedbacks and the activities of subterranean ecosystem engineers such as termites, ants, and rodents.\n\nThe phenomenon can be seen at the Mima Mounds Natural Area Preserve, a designated National Natural Landmark near Capitol State Forest in Thurston County, Washington. Mima mounds are also present at the Scatter Creek Unit, located in southern Thurston County, Washington.\n\n\nNotes\nFurther reading\n\n"}
{"id": "47926198", "url": "https://en.wikipedia.org/wiki?curid=47926198", "title": "Open Tree of Life", "text": "Open Tree of Life\n\nThe Open Tree of Life is an online phylogenetic tree of life – a collaborative effort, funded by the National Science Foundation. The first draft, including 2.3 million species, was released in September 2015. The Interactive graph allows the user to zoom in to taxonomic classifications, phylogenetic trees, and information about a node. Clicking on a species will return its source and reference taxonomy.\n\nThe project uses a supertree approach to generate a single phylogenetic tree (served at tree.opentreeoflife.org) from a comprehensive taxonomy and a curated set of published phylogenetic estimates.\n\nThe taxonomy is a combination of several large classifications produced by other projects; it is created using a software tool called \"smasher\"\n\nThe project was started in June 2012 with a three-year NSF award to researchers at ten universities. In 2015, a two-year supplemental award was made to researchers at three institutions.\n\n"}
{"id": "26875817", "url": "https://en.wikipedia.org/wiki?curid=26875817", "title": "Peak minerals", "text": "Peak minerals\n\nPeak minerals marks the point in time when the largest production of a mineral will occur in an area, with production declining in subsequent years. While most mineral resources will not be exhausted in the near future, global extraction and production is becoming more challenging. Miners have found ways over time to extract deeper and lower grade ores with lower production costs. More than anything else, declining average ore grades are indicative of ongoing technological shifts that have enabled inclusion of more 'complex' processing – in social and environmental terms \"as well as\" economic – and structural changes in the minerals exploration industry and these have been accompanied by significant increases in identified Mineral Reserves.\n\nThe concept of peak minerals offers a useful model for representing the changing impacts associated with processing declining resource qualities in the lead up to, and following, peak mineral production in a particular region within a certain time-frame.\n\nPeak minerals provides an analytical framework within which the economic, social and environmental trajectories of a particular mining industry can be explored in relation to the continuing (and often increasing) production of mineral resources. It focuses consideration on the change in costs and impacts associated with processing easily accessible, lower cost ores before peak production of an individual mine or group of mines for a given mineral. It outlines how the economy might respond as processing becomes characterised by higher costs as the peak is approached and passed. Issues associated with the concept of peak minerals include:\n\nGiurco et al. (2009) indicate that the debate about how to analytically describe resource depletion is ongoing. Traditionally, a fixed stock paradigm has been applied, but Tilton and Lagos (2007) suggest using an opportunity cost paradigm is better because the usable resource quantity is represented by price and the opportunity cost of using the resource. Unlike energy minerals such as coal or oil – or minerals used in a dissipative or metabolic fashion like phosphorus – most non-energy minerals and metals are unlikely to run out. Metals are inherently recyclable and more readily recoverable from end uses where the metal is used in a pure form and not transformed or dissipated; in addition, metal ore is accessible at a range of different grades. So, although metals are not facing exhaustion, they are becoming more challenging to obtain in the quantities that society demands, and the energy, environmental and social cost of acquiring them could constrain future increases in production and usage.\n\nGiven increasing global population and rapidly growing consumption (especially in China and India), frameworks for the analysis of resource depletion can assist in developing appropriate responses. The most popular contemporary focus for resource depletion is oil (or petroleum) resources. In 1956, oil geologist M. King Hubbert famously predicted that conventional oil production from the lower 48 (mainland) states of the United States would peak by 1970 and then enter a terminal decline. This model was accurate in predicting the peak (although the peak year was 1971). This phenomenon is now commonly called 'peak oil', with peak production curves known as Hubbert Curves.\n\nThe concept of peak minerals is an extrapolation and extension of Hubbert's model of peak oil. Although widely cited for his predictions of peak oil, Hubbert intended to explore an appropriate response to the finite supply of oil, and framed this work within the context of increasing global population and rapidly growing consumption of oil.\n\nIn establishing the peak oil model, Hubbert was primarily focused on arguing that a planned transition was required to ensure future energy services.\n\nWorld gold production has experienced multiple peaks due to new discoveries and new technologies. Many mineral resources have exhibited logistic Hubbert-type production trends in the past, but have transitioned to exponential growth during the last 10–15 years, precluding reliable estimates of reserves from within the framework of the logistic model.\n\nOnly limited substantive work is currently undertaken to examine how the concepts and assumptions of peak oil can be extrapolated so as to be applied to minerals in general. When extrapolating peak oil to account for peak minerals and then utilising this analytical 'peak framework' as a general model of resource exploitation, several factors must be taken into consideration:\nIn understanding how these factors are important for modelling peak minerals, it is important to consider assumptions concerning the modelling process, assumptions about production (particularly economic conditions), and the ability to make accurate estimates of resource quantity and quality and the potential of future exploration.\n\nPeak production poses a problem for resource rich countries like Australia, which have developed a comparative advantage in the global resources sector, which may diminish in the future. The costs of mining, once primarily reflected in economic terms, are increasingly being considered in social and environmental terms, although these are yet to meaningfully inform long-term decision-making in the sector. Such consideration is particularly important if the industry is seeking to operate in a socially, environmentally and economically sustainable manner into the next 30–50 years.\n\nIn 2008–09, minerals and fuel exports made up around 56% of Australia’s total exports. Consequently, minerals play a major role in Australia’s capacity to participate in international trade and contribute to the international strength of its currency. Whether this situation contributes to Australia’s economic wealth or weakens its economic position is contested. While those supporting Australia’s reliance on minerals cite the theory of comparative advantage, opponents suggest a reliance on resources leads to issues associated with 'Dutch disease' (a decline in other sectors of the economy associated with natural resource exploitation) and ultimately the hypothesised ‘resource curse’.\n\nContrary to the theory of the comparative advantage, many mineral resource-rich countries are often outperformed by resource-poor countries. This paradox, where natural resource abundance actually has a negative impact on the growth of the national economy is termed the resource curse. After an initial economic boost, brought on by the booming minerals economy, negative impacts linked to the boom surpass the positive, causing economic activity to fall below the pre-resource windfall level.\n\nThe economics of a commodity are generally determined by supply and demand. Mineral supply and demand will change dramatically as all costs (economic, technological, social and environmental) associated with production, processing and transportation of minerals increases with falling ore grades. These costs will ultimately influence the ability of companies to supply commodities, and the ability of consumers to purchase them. It is likely that social and environmental issues will increasingly drive economic costs associated with supply and demand patterns.\n\nAs neither overall stocks nor future markets are known, most economists normally do not consider physical scarcity as a good indicator for the availability of a resource for society. Economic scarcity has subsequently been introduced as a more valid approach to assess the supply of minerals. There are three commonly accepted measures for economic scarcity: the user costs associated with a resource, the real price of the resource, and the resource’s extraction costs. These measures have historically externalised impacts of a social or environmental nature – so might be considered inaccurate measures of economic scarcity given increased environmental or social scrutiny in the mining industry. Internalisation of these costs will contribute to economic scarcity by increasing the user costs, the real price of the resource, and its extraction costs.\n\nWhile the ability to supply a commodity determines its availability as has been demonstrated, demand for minerals can also influence their availability. How minerals are used, where they are distributed and how, trade barriers, downstream use industries, substitution and recycling can potentially influence the demand for minerals, and ultimately their availability. While economists are cognisant of the role of demand as an availability driver, historically they have not considered factors besides depletion as having a long-term impact on mineral availability.\n\nThere are a variety of indicators that show production is becoming more difficult and more expensive. Key environmental indicators that reflect increasingly expensive production are primarily associated with the decline in average ore grades of many minerals. This has consequences in mineral exploration, for mine depth, the energy intensity of mining, and the increasing quantity of waste rock.\n\nAdjusting to a higher energy intensity is challenging for the industry in light of peak oil and rising energy costs in a carbon constrained future.\n\nAlthough new mineral deposits are still being discovered, and reserves are increasing for some minerals, these are of lower quality and are less accessible.\n\nDifferent social issues must be addressed through time in relation to peak minerals at a national scale, and other issues manifest on the local scale.\n\nAs global mining companies seek to expand operations to access larger mining areas, competition with farmers for land and for scare water is becoming increasingly intense. Negative relationships with near neighbours influence companies' ability to establish and maintain a \"social license to operate\" within the community.\n\nAccess to identified resources is becoming harder as questions are asked about the benefit from the regional economic development mining is reputed to bring.\n\n\n"}
{"id": "171267", "url": "https://en.wikipedia.org/wiki?curid=171267", "title": "Psilotum", "text": "Psilotum\n\nPsilotum is a genus of fern-like vascular plants, commonly known as whisk ferns. It is one of two genera in the family Psilotaceae, the other being \"Tmesipteris\". Plants in these two genera were once thought to be descended from the earliest surviving vascular plants, but more recent phylogenies place them as basal ferns, as a sister group to Ophioglossales. They lack true roots and leaves, the stems being the organs containing conducting tissue. There are only two species in \"Psilotum\" and a hybrid between the two. They differ from those in \"Tmesipteris\" in having stems with many branches and a synangium with three lobes rather than two.\n\nWhisk ferns in the genus \"Psilotum\" lack true roots but are anchored by creeping rhizomes. The stems have many branches with paired enations, which look like small leaves but have no vascular tissue. Above these enations there are synangia formed by the fusion of three sporangia and which produce the spores. When mature, the synangia release yellow to whitish spores which develop into a gametophyte less than long. The gametophyte lives underground as a saprophyte, sometimes in a mycorrhizal association. When the gametophyte is mature, it produces both egg and sperm cells. The sperm cells swim using several flagella and when they reach an egg cell, unite with it to form the young sporophyte. A mature sporophyte may grow to a height of or more but lacks true leaves. The stem has a core of thick-walled protostele in its centre surrounded by an endodermis which regulates the flow of water and nutrients. The surface of the stem is covered with stomata which allow gas exchange with the surroundings.\n\nThe gametophyte of \"Psilotum\" is unusual in that it branches dichotomously, lives underground and possesses vascular tissue. The nutrition of the gametophyte appears to be myco-heterotrophic, assisted by endophytic fungi.\n\nThe genus \"Psilotum\" was first formally described in 1801 by Olof Swartz and the description was published in \"Journal fur die Botanik (Schrader)\". The name of the genus is from the Ancient Greek word \"psilos\" meaning \"bare\", \"smooth\" or \"bald\" referring to the lack of the usual plant organs, such as leaves.\n\nThere are two species, \"Psilotum nudum\" and \"Psilotum complanatum\", with a hybrid between them known, \"Psilotum\" × \"intermedium\" W. H. Wagner.\n\nThe distribution of \"Psilotum\" is tropical and subtropical, in the New World, Asia, and the Pacific, with a few isolated populations in south-west Europe. The highest latitudes known are in South Carolina, Cádiz province in Spain, and southern Japan for \"P. nudum\". In the U.S., \"P. nudum\" is found from Florida to Texas, and \"P. complanatum\" in Hawaii.\n\n\"Psilotum\" superficially resembles certain extinct early vascular plants, such as the rhyniophytes and the trimerophyte genus \"Psilophyton\". The unusual features of \"Psilotum\" that suggest an affinity with early vascular plants include dichotomously branching sporophytes, aerial stems arising from horizontal rhizomes, a simple vascular cylinder, homosporous and terminal eusporangia and a lack of roots. Unfortunately, no fossils of psilophytes are known to exist. A careful study of the morphology and anatomy suggests that whisk ferns are not closely related to rhyniophytes, and that the ancestral features present in living psilophytes represent a reduction from a more typical modern fern plant. Significant differences between \"Psilotum\" and the rhyniophytes and trimerophytes are that the development of its vascular strand is exarch, while it is centrarch in rhyniophytes and trimerophytes. The sporangia of \"Psilotum\" are trilocular synangia resulting from the fusion of three adjacent sporangia, and these are borne laterally on the axes. In the rhyniophytes and trimerophytes the sporangia were single and in a terminal position on branches.\n\nMolecular evidence strongly confirms that \"Psilotum\" is a fern and that psilophytes are sister to ophioglossoid ferns.\n"}
{"id": "748047", "url": "https://en.wikipedia.org/wiki?curid=748047", "title": "Radiant barrier", "text": "Radiant barrier\n\nA radiant barrier is a type of building product that reflects thermal radiation and reduces heat transfer. Since thermal energy is also transferred via conduction and convection, radiant barriers are often supplemented with thermal insulation products that slow down heat transfer via conduction or convection.\n\nRadiant barrier reflects heat radiation (radiant heat), preventing transfer from one side to another due to a reflective, low emittance surface. In building applications, this surface is typically a very thin, mirror-like aluminum foil. The foil may be coated for resistance to the elements or for abrasion resistance. The radiant barrier may be one or two sided. One sided radiant barrier may be attached to insulating materials, such as polyisocyanate, rigid foam, bubble insulation, or OSB. Additionally, reflective tape can be adhered to strips of radiant barrier to make it a contiguous vapor barrier, alternatively, radiant barrier can be perforated for vapor transmittance.\n\nAll materials give off, or emit, energy by thermal radiation as a result of their temperature. The amount of energy radiated depends on the surface temperature and a property called emissivity (also called \"emittance\"). Emissivity is expressed as a number between zero (0) and one (1) at a given wavelength. The higher the emissivity, the greater the emitted radiation at that wavelength. A related material property is reflectivity (also called \"reflectance\"). This is a measure of how much energy is reflected by a material at a given wavelength. Reflectivity is also expressed as a number between 0 and 1 (or a percentage between 0 and 100). At a given wavelength and angle of incidence the emissivity and reflectivity values sum to 1 by Kirchhoff's law.\n\nRadiant barrier materials must have low emissivity (usually 0.1 or less) at the wavelengths at which they are expected to function. For typical building materials, the wavelengths are in the mid- and long-infrared spectrum, in the range of 3-15 micrometres.\n\nRadiant barriers may or may not exhibit high visual reflectivity. While reflectivity and emissivity must sum to 1 at a given wavelength, reflectivity at one set of wavelengths (visible) and emissivity at a different set of wavelengths (thermal) do not necessarily sum to 1. Therefore, it is possible to create visibly dark colored surfaces with low thermal emissivity.\n\nTo perform properly, radiant barriers need to face open space (e.g., air or vacuum) through which there would otherwise be radiation.\n\nIn 1860, the French scientist Jean Claude Eugene Peclet experimented with the insulating effect of high and low emissive metals facing air spaces. Peclet experimented with a wide variety of metals ranging from tin to cast iron, and came to the conclusion that neither the color nor the visual reflectance were significant determining factors in the materials’ performance. Peclet calculated the reduction in BTUs for high and low emissive surfaces facing into various air spaces, discovering the benefits of a radiant barrier in reducing the transfer of heat.\n\nIn 1925, two German businessmen Schmidt and Dykerhoff filed for patents on reflective surfaces for use as building insulation because recent improvements in technology allowed low emissivity aluminum foil to be commercially viable. This became the launching pad for radiant barrier and reflective insulation around the world, and within the next 15 years, millions of square feet of radiant barrier were installed in the US alone.\nWithin 30 years, radiant barrier was making a name for itself, and was included in projects at MIT, Princeton, and Frank Sinatra’s residence in Palm Springs, California.\n\nFor the Apollo program, NASA helped develop a thin aluminum foil that reflected 95% of the radiant heat. A metalized film was used to protect spacecraft, equipment, and astronauts from thermal radiation or to retain heat in the extreme temperature fluctuations of space. The aluminum was vacuum-coated to a thin film and applied to the base of the Apollo landing vehicles. It was also used in numerous other NASA projects like the James Webb Space Telescope and Skylab. In the vacuum of outer space, where temperatures can range from heat transfer is only by radiation, so a radiant barrier is much more effective than it is on earth, where 5% to 45% of the heat transfer can still occur via convection and conduction, even when an effective radiant barrier is deployed. Radiant barrier is a Space Foundation Certified Space Technology(TM). Radiant barrier was inducted into the Space Technology Hall of Fame in 1996.\n\nSince the 1970s, sheets of metalized polyester called space blankets have been commercially available as a means to prevent hypothermia and other cold weather injuries. Because of their durability and light weight, these blankets are popular for survival and first aid applications. Swarms of people can be seen draped in reflective metalized film after a marathon, especially where the temperatures are particularly cold, like during the annual New York City Marathon which takes place in the fall.\n\nWindows glass can be coated to achieve low emissivity or “low-e”. Some windows use laminate polyester film where at least one layer has been metalized using a process called sputtering. Sputtering occurs when a metal, most often aluminum, is vaporized and the polyester film is passed through it. This process can be adjusted to control the amount of metal that ultimately coats the surface of the film.\n\nThese metalized films are applied to one or more surfaces of the glass to resist the transfer of radiant heat, yet the films are so thin that they allow visible light to pass through. Since the thin coatings are fragile and can be damaged when exposed to air and moisture, manufacturers typically use multiple pane windows. While films are typically applied to the glass during manufacturing, some films may be available for homeowners to apply themselves. Homeowner-applied window films are typically expected to last 10–15 years.\n\nWhen radiant solar energy strikes a roof, heating the roofing material (shingles, tiles or roofing sheets) and roof sheathing by conduction, it causes the underside of the roof surface and the roof framing to radiate heat downward through the roof space (attic / ceiling cavity) toward the attic floor / upper ceiling surface. When a radiant barrier is placed between the roofing material and the insulation on the attic floor, much of the heat radiated from the hot roof is reflected back toward the roof and the low emissivity of the underside of the radiant barrier means that very little radiant heat is emitted downwards. This makes the top surface of the insulation cooler than it would have been without a radiant barrier and thus reduces the amount of heat that moves through the insulation into the rooms below.\n\nThis is different from the cool roof strategy which reflects solar energy before it heats the roof, but both are means of reducing radiant heat. According to a study by Florida Solar Energy Center, a white tile or white metal cool roof can outperform traditional black shingle roof with a radiant barrier in the attic, but the black shingle roof with radiant barrier outperformed the red tile cool roof.\n\nFor installing a radiant barrier under a metal or tile roof, the radiant barrier may be applied directly over the roof sheathing. Then furring strips (1x4s) are applied over the radiant barrier before the metal or tile roof is applied. The furring strips ensure that the radiant barrier faces into a sufficient air space. If an air space is not present or is too small, heat may be able to conduct through the radiant barrier. Since the metal in the radiant barrier is highly conductive, the heat transfer would be by conduction and the heat would not be blocked. According to the US Department of Energy, “Reflective insulation and radiant barrier products must have an air space adjacent to the reflective material to be effective.”\n\nThe most common application for a radiant barrier is as a facing for attics. For a traditional shingle/tile/iron roof, radiant barriers may be applied over the rafters or trusses and under the roof decking. This application method has the radiant barrier sheets draped over the trusses of rafters, creating a small air space above with the radiant barrier facing into the entire interior attic space below. Reflective foil laminate (or sarking) is a product commonly used as the radiant barrier sheet.\n\nAnother method of applying radiant barrier to the roof in new construction is to use a radiant barrier that is pre-laminated to OSB panels or roof sheathing. Manufacturers of this installation method often tout the savings in labor costs in using a product that serves as roof decking and radiant barrier in one.\n\nTo apply radiant barrier in an existing attic, a radiant barrier may be stapled to the underside of the roof rafters. This method offers the same benefits as the draped method in that dual air spaces are provided. However, it is essential that the vents be allowed to remain open to prevent moisture from being trapped in the attic. In general, it is preferred to have the radiant barrier applied to the underside of the roof with an air space facing down to prevent the accumulation of dust, preventing the radiant barrier from conducting.\n\nThe final method of installing a radiant barrier in an attic is to lay it over the top of the insulation on the attic floor. While this method can be more effective in the winter there are a few potential concerns with this application, which the US Department of Energy and the Reflective Insulation Manufacturers Association International feel the need to address. First, a breathable radiant barrier should always be used here. This is usually achieved by small perforations in the radiant barrier foil. The vapor transmission rate of the radiant barrier should be at least 5 perms, as measured with ASTM E96, and the moisture in the insulation should be checked before installation. Second, the product should meet the required flame spread, which includes ASTM E84 with the ASTM E2599 method. Lastly, this method allows for dust to accumulate over the top surface of the radiant barrier, potentially reducing the efficiency over time.\n\nAccording to a 2010 study by the Building Envelope Research Program of the Oak Ridge National Laboratory, homes with air-conditioning duct work in the attic in the hottest climate zones, such as in the US Deep South, could benefit the most from radiant barrier interventions, with annual utility bill savings up to $150, whereas homes in milder climates, e.g., Baltimore, could see savings about half those of their southern neighbors. On the other hand, if there are no ducts or air handlers in the attic, the annual savings could be even much less, from about $12 in Miami to $5 in Baltimore. Nevertheless, a radiant barrier may still help to improve comfort and to reduce the peak air-conditioning load.\n\nOne common misconception regarding radiant barrier is that the heat reflecting off the radiant barrier back to the roof has the potential to increase the roof temperature and possibly damage the shingles. Performance testing by Florida Solar Energy Center demonstrated that the increase in temperature at the hottest part of the day was no more than about 5 degrees F. In fact, this study showed that radiant barrier had the potential to decrease the roof temperature once the sun went down because it prevented heat loss through the roof. RIMA International wrote a technical paper on the subject which included statements collected from large roofing manufacturers, and none said that radiant barrier would in any way affect the warranty of the shingles.\n\nWhen laying a radiant barrier over the insulation on the attic floor, it is possible for dust to accumulate on the top side. Many factors like dust particle size, dust composition and the amount of ventilation in the attic affect how dust accumulates and thus the ultimate performance of radiant barrier in the attic. A study by the Tennessee Valley Authority mechanically applied a small amount of dust over a radiant barrier and found no significant effect when testing for performance. However, TVA referenced a previous study which stated that it was possible for the radiant barrier to collect so much dust that its reflectivity could be decreased by nearly half.\nIt is not true that a double-sided radiant barrier on the attic floor is immune to the dust concern. The TVA study also tested a double-sided radiant barrier with black plastic draped on top to simulate heavy dust accumulation, as well as a single-sided radiant barrier with heavy kraft paper on the top. The test indicated that the radiant barrier was not performing, and the small air spaces created between the peaks of the insulation were not sufficient to block radiant heat.\n\nRadiant barrier may be used as a vented skin around the exterior of a wall. Furring strips are applied to the sheathing to create a vented air space between the radiant barrier and the siding, and vents are used at the top and bottom to allow convective heat to rise naturally to the attic. If brick is being used on the exterior, then a vented air space may already be present, and furring strips are not necessary. Wrapping a house with radiant barrier can result in a 10% to 20% reduction in the tonnage air conditioning system requirement, and save both energy and construction costs.\n\nReflective foil, bubble foil insulations, and radiant barriers are noted for their ability to reflect unwanted solar radiation in hot climates, when applied properly. Reflective foils are fabricated from aluminum foils with a variety of backings such as roofing paper, craft paper, plastic film, polyethylene bubbles, or cardboard. Reflective bubble foil is basically a plastic bubble wrap sheet with a reflective foil layer and belongs to a class of insulation products known as radiant foils. Reflective bubble/foil insulations are primarily radiant barriers, and reflective insulation systems work by reducing radiant heat gain. To be effective, the reflective surface must face an air space, also dust accumulation on the reflective surface will reduce its reflective capability. The radiant barrier should be installed in a manner to minimize dust accumulation on the reflective surface.\n\nRadiant barriers are more effective in hot climates than in cooler/cold climates (especially when cooling air ducts are located in the attic). When the sun heats a roof, it's primarily the sun's radiant energy that makes the roof hot. Much of this heat travels by conduction through the roofing materials to the attic side of the roof. The hot roof material then radiates its gained heat energy onto the cooler attic surfaces, including the air ducts and the attic floor. A radiant barrier reduces the radiant heat transfer from the underside of the roof to the other surfaces in the attic. Some studies show that radiant barriers can reduce cooling costs 5% to 10% when used in a warm, sunny climate. The reduced heat gain may even allow for a smaller air conditioning system. In cool climates, however, it's usually more cost-effective to install more thermal insulation than to add a radiant barrier.\n\nBoth the American Department of Energy (DOE, Energy Efficiency & Renewable Energy Department) and the Ministry of Natural Resources (NRCAN) state that these systems are not recommended for cold or very cold climates.\n\nCanada is considered to be a cold climate, so these products do not perform as promoted. Though they are often marketed as offering very high insulating values, there is no specific standard for radiant insulation products, so be wary of posted testimonials and manufacturers’ thermal performance claims. Research has shown that the insulation value of reflective bubble foil insulations and radiant barriers can vary from RSI 0 (R-0) to RSI 0.62 (R-3.5) per thickness of material. A study conducted by CMHC (Canada Mortgage & Housing Corporation) on four homes in Paris, ON found there that the performance of the bubble foil was similar to an uninsulated floor. It also performed a cost-benefit analysis and the cost-benefit ratio was $12 to $13 per cubic metre RSI.\n\nThe effective insulating value depends on the number of adjacent dead air spaces, layers of foil and where they are installed. If the foil is laminated to rigid foam insulation, the total insulating value is obtained by adding the RSI of the foam insulation to the RSI of the dead air space and the foil. If there is no air space or clear bubble layer, the RSI value of the film is zero.\n\n"}
{"id": "41114338", "url": "https://en.wikipedia.org/wiki?curid=41114338", "title": "Radiant heating and cooling system", "text": "Radiant heating and cooling system\n\nA radiant heating and cooling system refers to temperature-controlled surfaces that exchange heat with their surrounding environment through convection and radiation. By definition, in radiant heating and cooling systems, thermal radiation covers more than 50% of heat exchange within the space. \nHydronic radiant heating and cooling systems are water-based. It refers to panels or embedded building components (floors, ceilings or walls). Other types include air-based and electrical systems (which use electrical resistance for heating purpose mainly). Important portions of building surfaces are usually required for the radiant exchange.\n\nRadiant heating and cooling systems can be used in commercial, residential, education, and recreational buildings, museums, hospitals, and other type of buildings. The application depends on the type of radiant system (see below types of radiant systems), on climate conditions and on ventilation system used.\n\nHeat radiation is the energy in the form of electromagnetic waves emitted by a solid, liquid, or gas as a result of its temperature. \nIn buildings, the radiant heat flow between two internal surfaces (or a surface and a person) is influenced by the emissivity of the heat emitting surface and by the view factor between this surface and the receptive surface (object or person) in the room. \nThe heat transfer by radiation is proportional to the power of four of the absolute surface temperature.\n\nThe emissivity of a material (usually written ε or e) is the relative ability of its surface to emit energy by radiation. A black body has an emissivity of 1 and a perfect reflector has an emissivity of 0.\n\nIn radiative heat transfer, a view factor quantifies the relative importance of the radiation that leaves an object (person or surface) and strikes another one, considering the other surrounding objects. In enclosures, radiation leaving a surface is conserved, therefore, the sum of all view factors associated with a given object is equal to 1.\nIn the case of a room, the view factor of a radiant surface and a person depend on their relative positions. As a person is often changing position and as a room might be occupied by many persons at the same time, diagrams for omnidirectional person can be used.\n\nResponse time (τ95), aka time constant, is used to analyze the dynamic thermal performance of radiant systems. The response time for a radiant system is defined as the time it takes for the surface temperature of a radiant system to reach 95% of the difference between its final and initial values when a step change in control of the system is applied as input. It is mainly influenced by concrete thickness, pipe spacing, and to a less degree, concrete type. It is not affected by pipe diameter, room operative temperature, supply water temperature, and water flow regime. By using response time, radiant systems can be classified into fast response (τ95< 10 min, like RCP), medium response (1 h<τ95<9 h, like Type A, B, D, G) and slow response (9 h< τ95<19 h, like Type E and Type F). Additionally, floor and ceiling radiant systems have different response times due to different heat transfer coefficients with room thermal environment, and the pipe-embedded position.\n\nThe operative temperature is an indicator of thermal comfort which takes into account the effects of both convection and radiation. Operative temperature is defined as a uniform temperature of a radiantly black enclosure in which an occupant would exchange the same amount of heat by radiation plus convection as in the actual nonuniform environment.\n\nWith radiant systems, thermal comfort is achieved at warmer interior temp than all-air systems for cooling scenario, and at lower temperature than all-air systems for heating scenario.\nThus, radiant systems can helps to achieve energy savings in building operation while maintaining the wished comfort level.\n\nBased on a large study performed using Center for the Built Environment's Indoor environmental quality (IEQ) occupant survey to compare occupant satisfaction in radiant and all-air conditioned buildings, both systems create equal indoor environmental conditions, including acoustic satisfaction, with a tendency towards improved temperature satisfaction in radiant buildings. \n\nThe radiant temperature asymmetry is defined as the difference between the plane radiant temperature of the two opposite sides of a small plane element. As regards occupants within a building, thermal radiation field around the body may be non-uniform due to hot and cold surfaces and direct sunlight, bringing therefore local discomfort. The norm ISO 7730 and the ASHRAE 55 standard give the predicted percentage of dissatisfied occupants (PPD) as a function of the radiant temperature asymmetry and specify the acceptable limits. In general, people are more sensitive to asymmetric radiation caused by a warm ceiling than that caused by hot and cold vertical surfaces. The detailed calculation method of percentage dissatisfied due to a radiant temperature asymmetry is described in ISO 7730.\n\nWhile specific design requirements will depend on the type of radiant system, a few issues are common to most radiant systems.\n\n\nDepending on the position of the pipes in the building construction, hydronic radiant systems can be sorted into 4 main categories:\n\nThe norm ISO 11855-2\nfocuses on embedded water based surface heating and cooling systems and TABS. Depending on construction details, this norm distinguishes 7 different types of those systems (Types A to G)\n\n\nRadiant systems are associated with low-exergy systems. Low-exergy refers to the possibility to utilize ‘low quality energy’ (i.e. dispersed energy that has little ability to do useful work). Both heating and cooling can in principle be obtained at temperature levels that are close to the ambient environment. The low temperature difference requires that the heat transmission takes place over relative big surfaces as for example applied in ceilings or underfloor heating systems.\nRadiant systems using low temperature heating and high temperature cooling are typical example of low-exergy systems. \nEnergy sources such as geothermal (direct cooling / geothermal heat pump heating) and solar hot water are compatible with radiant systems. These sources can lead to important savings in terms of primary energy use for buildings.\n\nMap of buildings using hydronic radiant heating and cooling systems\n\n"}
{"id": "58097959", "url": "https://en.wikipedia.org/wiki?curid=58097959", "title": "Sahul (continent)", "text": "Sahul (continent)\n\nSahul was a prehistoric continent that consisted of Australia, New Guinea, Tasmania and Seram.\n\nSahul was partially submerged around 18,000 years ago.\n\nSahul and Sunda were points of early human migrations after leaving Africa.\n"}
{"id": "30247754", "url": "https://en.wikipedia.org/wiki?curid=30247754", "title": "Shebelinka gas field", "text": "Shebelinka gas field\n\nThe Shebelinka gas field is a Ukrainian natural gas field that was discovered in 1956. It began production in 1958 and produces natural gas and condensates. The total proven reserves of the Shebelinka gas field are around 18.4 trillion cubic feet (520×10m³).\n"}
{"id": "57687468", "url": "https://en.wikipedia.org/wiki?curid=57687468", "title": "Spiral ground heat exchanger", "text": "Spiral ground heat exchanger\n\nUsing of spiral ground heat exchangers in heat pump applications plays a very important role in supplying heating/cooling demands of the buildings cheaply and sustainable. These-days environmental pollution is one of the main concerns among governments and energy consumption of the buildings has a significant portion in it. One of the efficient ways in supplying heating and cooling demands of the buildings is using potential energy stored in ground which is clean and sustainable. These types of systems are called ground source heat pump (GSHP) systems which are used supplying space heating and cooling demands. The initial cost and efficiency of GSHP systems can be influenced by optimal design of ground heat exchangers (GHEs).\n\nOne of the most important GHEs that is used these days is \"spiral GHEs\". These types of GHEs are \"cheaper\" than others and also have some advantages. \nMany parameters influencing thermal performance of spiral GHEs such as external major diameter of borehole, spacing, distance between each turns of spiral GHEs, etc.When we have a large application area, one spiral GHE is not enough for supplying heating or cooling demands of the building. In this case more than one GHE is needed to be embedded into the ground. When we have more than one GHE in ground, optimization of distance between them become as an essential problem. Another important parameters which are highly affecting thermal performance of a GSHP system are thermal properties of ground. Before integrating a GSHP system to the building thermal properties of ground or soil should be determined.\n\nAir contamination is one of the significant environmental concerns in the world. Reaching solutions to environmental problems that we face today needs long term activities for sustainable development. Renewable energy sources seem to be one of the best and effective solutions. In recent decades, energy consumption for building has increased and many efforts are being done to investigate alternative energy sources for supplying heating and cooling demands of building. One of the best alternate ways is the use of ground energy, which is clean, green and sustainable. This energy can be utilized through GSHP system, which is well established in most of the European countries for space heating applications. GSHP systems contain two main parts, GHEs and heat pump.\n\nIn these systems, heat source is soil which stored huge amount of energy inside. When fluid/water stars flowing inside pipelines heat is transferred from fluid to the soil or from soil to the fluid due to the temperature difference between them. Many researches show indicate that, temperature of ground is constant specially after 6 meter depth. This means that, temperature in ground surface is not affecting temperature of soil after 6 meter depth.\n\nAs an example, assume that fluid is pumped into the ground at 40 degree Celsius and undisturbed ground temperature is 18 degree Celsius. As the time passes temperature of soil around spiral GHEs will be increased and then we will have less heat transfer between pumped fluid and soil due to the low temperature difference. In the case of deep GHEs, temperature of exhausted fluid is too high. In this case these types of GHEs can be used directly for supplying heating demands of the building.\n\nAs it was stated, heat is transferred from high to low temperature medium without needing any external devices.But in many cases heated is needed to be transferred reversely. Heat pumps are devices which receive heat from low temperature and deliver it to a higher temperature reservoir. Working procedure of refrigerators and heat pumps are same and only the purposes of their using are different.\nA heat pump can also extract heat from a heat source and reject heat to air and water at the higher temperature for heating. One of the important parameters for heat pumps is coefficient of performance (COP) of it. Definition of COP is desired output per desired input of the device.\n\nGround source heat pumps (GSHP) or geothermal heat pumps are heating/cooling devices which use ground as their heat source. GSHPs offer significant reductions of electrical energy use, cheap levels of maintenance requirements, and are environment friendly. A ground source heat pump is a heating and cooling system that transfers heat to or from the ground, using the ground as a heat sink in the summer and heat source in the winter. A ground source heat pump can be significantly more energy efficient than conventional air source heat pump. As it was discussed, GSHP systems consist of two main parts, heat pump and ground heat exchangers.\n\nGround heat exchangers use underground soil as a heat source. When water/fluid flows through pipes, heat is transferred from the water to the soil or from soil to the fluid depending on the temperature of fluid and its surrounding temperature. GHEs have some classifications such as deep, shallow, open loop, closed loop etc. Choosing correct types of GHEs is highly depending on geological properties of application area. For instance underground fluid flow or having a lake near application area could be very important advantages. Heat is transferred in the ground by two ways, convection and conduction.\n\nAs it was discussed several types of GHEs such as shallow and deep ones are existed. Deep GHEs are generally U-tube GHEs with common ranges of 50-200 meter. Shallow GHEs are divided to spiral, slinky, pond/lake and snails types. Among different types of shallow GHEs spiral ones show the best performance in the same application unit area which means that more amount of heat can be extracted or injected into the ground. This also means that more space area of the building could be heated or cooled by spiral GHEs. Spiral GHEs are favorable due to their high efficiencies and low initial costs. When spiral GHEs are connected to the heat pump system and total system (GSHP) is integrated to the buildings, heating/cooling demands of that specific residential building could be supplied easily by spiral GHEs. Many researches show that when nine numbers of spiral GHEs are connected to the heat pump system, heating demands of at least 200 square meter space area of a building could be supplied. Based on different parameters of spiral GHEs this value could be changed between 200-800 square meter space area. In Fig. 1 schematic view of series of spiral GHEs are shown.\n\nHeat transfer rate of GHEs is depending on several parameters such as thermal conductivity of soil or application area, thermal properties of fluid, peripheral area of GHEs and flow rate of fluid inside GHEs. When we have spiral GHEs it means that we have more peripheral area of heat exchanger and so we have more heat transfer. Or for pond/lake types GHEs, since we have fluid around heat exchangers this means that we will have more heat transfer and more temperature difference between inlet and outlet fluid. This issue is one the most important advantages of pond/lake GHEs.\n\nFig. 2 shows integration of a GSHP system to the building. As it is shown in this figure, heat is extracted from ground by spiral GHEs. After extraction process, fluid enters a controller to control inlet and out fluid temperature of the machine. After that fluid will enter heat pump system and supply required heating/cooling demands of the building. To calculate effective amount of space area that can be heated or cooled by GSHP system different parameters are needed. These parameters are amount of extracted/injected heat from/to ground by spiral GHEs, coefficient of performance (COP) of heat pump, heating and cooling loads of the building\n\nDifferent researches show that distance between spiral GHEs are highly affecting performance of GSHP system. It means that distance between them should be optimized well. By choosing low distance between them (less that 5 meter) there will be high performance loss in system and suitable amount of heat cannot be delivered or rejected from building.\n\nAs it is known, concept of extracting/injecting heat from/to ground is temperature different between two mediums. When there are more than one spiral GHEs embedded in ground it means that we have more thermal interactions in ground. When distance between heat exchangers is low (lower than 5 meter) thermal interactions between them is also too high. By increasing distance between them thermal interactions will be reduced and therefore we have low performance loss in system. Different researches show that at least 6/7 meter distance must be chosen between GHEs. For example, when number of spiral GHEs is nine we have 15%, 18%, 22% performance loss for 7, 6 and 5 meter distance between them respectively.\n\nIn ground source heat pump applications, optimization of distance between GHEs plays an important role in total performance of the system. This value also should be chosen based the availability of application area and also investors needs.\nIn Fig. 3 thermal interactions between nine mumbers of spiral GHEs are shown. As it is demonstrated as the time passes thermal interactions between spiral GHEs are increased.\n\nThe distance between each turns of the spiral and GHE major radius play a crucial role in the design process. These parameters are geometrical parameters which are influencing initial cost and thermal performance of GHEs. These properties are directly related to the vertical lengths required to construct the heat exchanger as well as amount of excavation. \nIt is seen that by increasing the Lp and D, HTR value of a GHE is improved. On the other hand, the total cost of embedding the GHE into ground becomes higher and higher by increasing the values of Lp and D. Therefore, it is important for designers and investors that they know the exact requirements of consumers.\n\nThe improvement of HTR value with longer and bigger exchangers results in increment of installation costs, which are the main drawback of GSHPs. During the designing of GHE fields, maximum heat load of the consumer is considered, and the minimum required borehole sizes (Lp and D) are chosen. On the other hand, trying to minimize the installation cost by decreasing the sizes of Lp and D alone, causes higher number of GHE which also increases the initial cost. Therefore, initial cost should be minimized by considering the whole system. Furthermore, not only the initial cost but also the operational costs should be considered during the optimization of the system overall costs. This requires the long term predictions of HTR value of a GHE.Results show that the average HTR values linearly increase with the value of D and Lp. These linear dependencies are due to the increment of peripheral surface area and heat capacity of ground per turn.\n\nIn this part, a building with 120 m2 heating/cooling area is considered. For heating and cooling of this building, GSHP system with vertical spiral GHE is chosen. As it is stated in different references, general required heat load for a standard building is approximately 80 W/m2. Therefore, total heat load of this building is 9600 W. Furthermore, COP value of a heat pump is assumed as 4.0. In this case, 7200 W heat is needed to pump from ground to building.\nTo predict the number of GHEs that are needed for heating and cooling applications, first of all long term performance of a single vertical spiral GHE should be estimated. Again by using COMSOL and based on previous validated data, HTR value of a single vertical spiral GHE is numerically predicted. The averaged HTR value is around 870 W for 1–6 months time interval.\n\nIn GHE designing procedure generally the most critical conditions are considered. The system is assumed to work 6 months non-stop in heating or cooling mode. For 6 month operation, COMSOL results show that averaged HTR of a single vertical spiral GHE is 870 W. By taking all the above arguments into consideration it is concluded that 9 vertical spiral GHE is needed for this building. For more than 5 GHEs, we assume that five GHEs graph can be used for performance loss prediction with an acceptable error. By looking at figures in relevant literature, ten meter distances between GHEs are chosen. For this distance, performance loss is less than 5%. Based on consumers utilization and designers goals this amount can of course be vary. The suggested configuration is shown in Fig. 4 for most critical conditions (6 months non-stop operation). In this configuration, performance loss is predicted less than 5%. \nSometimes the application area is limited and there is no enough space for placing GHEs. In that case, more performance losses can be accepted to minimize the application area. For nine vertical spiral GHEs, the following configuration can also be used. In corners and sides we can use three GHEs performance loss graph. \nFigure 5 demonstrates other configuration of nine spiral GHEs that can be used. \nNowadays keeping our environment clean is one the most important issues. There are still many waste heat and energies which are wasted into the environment such as, waste heat of gas turbines exhaust gasses or waste gasses in biomass systems. These waste heats can be used beneficially for supplying heating demands of the buildings. One way to use this energy usefully is to store it in ground and then re-extract it through spiral GHEs for providing heating demands of the buildings. Recent researches show that spiral GHEs could be one of the best choices. First waste heat could be stored in ground by spiral GHEs and then extracted by them. Ground has ability to store this energy in ground for a long period. Also results of different researches show that amount of space area that can be heated by GSHP system is multiplied by four.\n\nThis short article presents the analyses of various parameters on the efficiency of spiral GHE for GSHP systems. The most important parameters, which influence the performance of GSHPs, have been thoroughly analyzed, running long-term simulations and estimating the performance losses for each GHE configuration. \nThe results prove that Lp and D are one of the important parameters in the design of a GSHP. Although increasing these parameters can improve the efficiency of GSHP, a larger investment is needed for installation. Therefore, an optimum length should be found, which minimizes the total cost over the plant lifetime for an acceptable performance value.\n\nIn the large number of GHE, one of the most essential parameters that affect the system performance is distance between GHEs. The performance losses are less than 5% if the distance between GHE is more than 5 m, 7 m and 9 m for 2, 3 and 5 GHEs configuration. Therefore, spiral GHEs could be one good choice for storing environmental waste energy in the ground and then re-utilize it.\n"}
{"id": "17005656", "url": "https://en.wikipedia.org/wiki?curid=17005656", "title": "Subboreal", "text": "Subboreal\n\nThe Subboreal is a climatic period, immediately before the present one, of the Holocene. It lasted from 3710 to 450 BCE.\n\nThe composite scientific term \"Subboreal\", meaning \"below the Boreal,\" is derived from the Latin \"sub\" (below, under) and the Greek \"Βορέας\", from Boreas, the god of the North Wind. The word was first introduced in 1889 by Rutger Sernander to distinguish it from Axel Blytt's Boreal, which had been established in 1876.\n\nThe Subboreal followed the Atlantic and was followed by the Subatlantic. The Subboreal is equivalent to W. H. Zagwijn's pollen zones IVa and IVb and T. Litt's pollen zone VIII. In the pollen scheme of Fritz Theodor Overbeck, it occupies pollen zone X.\n\nIn paleoclimatology, it is divided into an Older Subboreal and a Younger Subboreal. Historically, the Subboreal is equivalent to most of the Neolithic and the entire Bronze Age, which started 4200 to 3800 years ago.\n\nThe Subboreal is usually defined as 3710 to 5660 years BP. The lower limit is flexible, as some authors prefer to use 4400 BCE, or 6350 BP in northwestern Poland, even 4830 BC, or 6780 BP, And others use 5000 calendar years, or 3050 BCE. The upper limit of the Subboreal and, therefore the beginning of the Subatlantic, is also flexible and can be attributed to 1170 to 830 BCE, but it is usually fixed at 450 BCE. In varve years, the Subboreal corrsponds to 5660 to 2750 years BP.\n\nThe boundary between the older and the younger Subboreal is considered to be 1350 BCE.\n\nThe climate was generally dryer and slightly cooler (by about 0.1 °C) than in the preceding Atlantic but still warmer than today. The temperatures were 0.7 °C higher than during the following Subatlantic. Consequently, in Scandinavia the lower limit of glaciers was 100 to 200 m higher than during the Subatlantic. On the whole, the oscillating temperatures slightly receded in the course of the Subboreal by about 0.3 °C.\n\nIn the Aegean, the beginning of the Subboreal was marked by a pronounced drought, centered around 5600 years BP. Of far greater importance as the coming to an end of the African Humid Period, reflected in the lakes of subtropical Africa (like Lake Chad) experiencing a rapid fall in their water levels. During the interval 6200 to 5000 years BP, drier conditions were in southern Mesopotamia, causing great demographic changes and probably instigating the end of Uruk.\n\nIn Germany, a drastic climatic cooling can be observed around 5000 varve years BP in the maars of the Eifel. In the preceding interval lasting from 8200 till 5000 varve years (Holocene Climatic Optimum), the July temperatures were on average still 1 °C higher. At the same time, the January temperatures were rising and the yearly precipitation increased.\n\nIn Northern Africa and in the Near East, the interval from 4700 to 4100 years BP had renewed and lasting dry conditions, as is indicated by lake level minima. Between 4500 and 4100 years BP, monsoonal precipitations weakened, a possible cause for the upheavals that led to the end of the Old Kingdom of Egypt.\n\nThe Levant shows a similar climatic evolution. The dry conditions prevailing in Mesopotamia around 4200 years BP probably resulted in the downfall of the Akkadian Empire.\n\nLevels of carbon dioxide had reached beginning of the Subboreal its Holocene minimal value of 260 ppm. During the Subboreal, it started rising and reached 293 ppm at the end of the period. As a comparison, today's value is over 400 ppm.\n\nIn Scandinavia, the Atlantic/Subboreal boundary shows a distinct vegetational change. Tat is less pronounced in Western Europe, but its typical mixed oak forest shows quite a fast decline in elm and linden. The decline in linden is not fully understood; it might be due to cooling or human interference. The decline in elm is most likely due to elm disease, caused by the ascomycete \"Ceratocystis ulmi\", but climatic changes and anthropogenic pressure on the forests certainly must be considered as well. The decline in elm, with a recession from 20 to 4%, as observed in Eifel maar pollen, has been dated in Central and Northern Europe as 4000 years BC, but it more likely was diachronous over the interval 4350 to 3780 BC.\n\nAnother important event was the immigration of European beech (\"Fagus sylvatica\") and hornbeam (\"Carpinus betulus\") from their retreats on the Balkan and south of the Apennines. This happened also diachronously: beech pollen are found for the first time in the interval 4340 to 3540 BC, hornbeam pollen somewhat later between 3400 and 2900 BC. With the start of the Younger Subboreal is the massive spreading of beech. The establishment of beech and hornbeam was accompanied by indicator plants for human settlements and agriculture like cereals and plantain (\"Plantago lanceolata\"), and hazel was receding.\n\nThe relatively-dry climate during the subboreal furthered the spreading of heath plants (Ericaceae).\n\nLike in the Atlantic, the global sea level kept on rising during the Subboreal but at a much slower rate. The increase amounted to about 1 m, which corresponds to a rate of 0.3 mm per year. At the end of the Subboreal, the sea level was about 1 m below the current value.\n\nIn the Baltic the Litorina Sea had already established itself before the onset of the Subboreal. During the Older Subboreal the second Litorina transgression raised the sea level to 1 m below the actual value. After an intermediate Post-litorine Regression the third Litorina transgression reached 60 cm below present and during the beginning Subatlantic, it reached today's value.\n\nIn the North Sea region, the Flandrian transgression of the Atlantic was followed by a slight regression or standstill at the beginning of the Subboreal.\n"}
{"id": "14932159", "url": "https://en.wikipedia.org/wiki?curid=14932159", "title": "Wernerian Natural History Society", "text": "Wernerian Natural History Society\n\nThe Wernerian Natural History Society (12 January 1808 – 16 April 1858), commonly abbreviated as the Wernerian Society, was a learned society interested in the broad field of natural history, and saw papers presented on various topics such as mineralogy, plants, insects, and scholarly expeditions. The Society was an offshoot of the Royal Society of Edinburgh, and from its beginnings it was a rather elite organization.\n\nThe Society was named after Abraham Gottlob Werner, a German geologist who was a creator of Neptunism, a theory of superposition based on a receding primordial ocean that had deposited all the rocks in the crust. At this time all rocks, including basalt, and crystalline substances were thought by some to be precipitated from solution.\n\nRobert Jameson, Regius Professor of Natural History at the University of Edinburgh, was the founder and life president of the Society. In 1800, he spent a year at the mining academy in Freiberg, Saxony, where he studied under Werner. The Society was founded on 12 January 1808, and the first meeting of the Society occurred on 2 March 1808. Between 1811 and 1839 eight volumes of \"Memoirs of the Wernerian Natural History Society\" appeared. More than twelve of Jameson's papers on geology and mineralogy were published in these volumes, and he also contributed some on zoology and botany. Proceedings after 1839 were published in Jameson's \"Edinburgh New Philosophical Journal\". The Society hosted many of the notable scientists of its day.\n\nThere were no meetings from 1850–1856, which coincided with the decline of Jameson himself. It was eventually decided to close the Society down and dispose of its assets, and it finally closed on 16 April 1858.\n\nMembers of the Wernerian Society were entitled to use the abbreviation M.W.S. after their name. \"Corresponding members\", based outside Edinburgh, used the designation C.M.W.S.\n\nFounding members, as of 12 January 1808:\n\n\n"}
{"id": "51499", "url": "https://en.wikipedia.org/wiki?curid=51499", "title": "Western Front (World War I)", "text": "Western Front (World War I)\n\nThe Western Front was the main theatre of war during the First World War. Following the outbreak of war in August 1914, the German Army opened the Western Front by invading Luxembourg and Belgium, then gaining military control of important industrial regions in France. The tide of the advance was dramatically turned with the Battle of the Marne. Following the Race to the Sea, both sides dug in along a meandering line of fortified trenches, stretching from the North Sea to the Swiss frontier with France, which changed little except during early 1917 and in 1918.\n\nBetween 1915 and 1917 there were several offensives along this front. The attacks employed massive artillery bombardments and massed infantry advances. Entrenchments, machine gun emplacements, barbed wire and artillery repeatedly inflicted severe casualties during attacks and counter-attacks and no significant advances were made. Among the most costly of these offensives were the Battle of Verdun, in 1916, with a combined 700,000 casualties (estimated), the Battle of the Somme, also in 1916, with more than a million casualties (estimated), and the Battle of Passchendaele (Third Battle of Ypres), in 1917, with 487,000 casualties (estimated).\n\nTo break the deadlock of trench warfare on the Western Front, both sides tried new military technology, including poison gas, aircraft and tanks. The adoption of better tactics and the cumulative weakening of the armies in the west led to the return of mobility in 1918. The German Spring Offensive of 1918 was made possible by the Treaty of Brest-Litovsk that ended the war of the Central Powers against Russia and Romania on the Eastern Front. Using short, intense \"hurricane\" bombardments and infiltration tactics, the German armies moved nearly to the west, the deepest advance by either side since 1914, but the result was indecisive.\n\nThe inexorable advance of the Allied armies during the second half of 1918 caused a sudden collapse of the German armies and persuaded the German commanders that defeat was inevitable. The German government surrendered in the Armistice of 11 November 1918, and the terms of peace were settled by the Treaty of Versailles in 1919.\n\nAt the outbreak of the First World War, the German Army, with seven field armies in the west and one in the east, executed a modified version of the Schlieffen Plan, moving quickly through neutral Belgium to attack France, and then turning southwards to encircle the French Army and trap it on the German border. The Western Front was the place where the most powerful military forces in Europe, the German and French armies, met and where the war was decided. Belgian neutrality had been guaranteed by Britain under the Treaty of London, 1839; this caused Britain to join the war at the expiration of its ultimatum at 11 pm GMT on 4 August. Armies under German generals Alexander von Kluck and Karl von Bülow attacked Belgium on 4 August 1914. Luxembourg had been occupied without opposition on 2 August. The first battle in Belgium was the Siege of Liège, which lasted from 5–16 August. Liège was well fortified and surprised the German Army under Bülow with its level of resistance. German heavy artillery was able to demolish the main forts within a few days. Following the fall of Liège, most of the Belgian field army retreated to Antwerp, leaving the garrison of Namur isolated, with the Belgian capital, Brussels, falling to the Germans on 20 August. Although the German army bypassed Antwerp, it remained a threat to their flank. Another siege followed at Namur, lasting from about 20–23 August.\n\nThe French deployed five armies on the frontier. The French Plan XVII was intended to bring about the capture of Alsace-Lorraine. On 7 August, the VII Corps attacked Alsace to capture Mulhouse and Colmar. The main offensive was launched on 14 August with the First and Second Armies attacking toward Sarrebourg-Morhange in Lorraine. In keeping with the Schlieffen Plan, the Germans withdrew slowly while inflicting severe losses upon the French. The French Third and Fourth Armies advanced toward the Saar River and attempted to capture Saarburg, attacking Briey and Neufchateau but were repulsed. The French VII Corps captured Mulhouse after a brief engagement on 7 August but German reserve forces engaged them in the Battle of Mulhouse and forced a French retreat.\n\nThe German Army swept through Belgium, executing civilians and razing villages. The application of \"collective responsibility\" against a civilian population further galvanised the allies. Newspapers condemned the German invasion, violence against civilians and destruction of property, which became known as the \"Rape of Belgium\". After marching through Belgium, Luxembourg and the Ardennes, the Germans advanced into northern France in late August, where they met the French Army, under Joseph Joffre, and the divisions of the British Expeditionary Force under Field Marshal Sir John French. A series of engagements known as the Battle of the Frontiers ensued, which included the Battle of Charleroi and the Battle of Mons. In the former battle the French Fifth Army was almost destroyed by the German 2nd and 3rd Armies and the latter delayed the German advance by a day. A general Allied retreat followed, resulting in more clashes at the Battle of Le Cateau, the Siege of Maubeuge and the Battle of St. Quentin (also called the First Battle of Guise).\n\nThe German Army came within of Paris but at the First Battle of the Marne (6–12 September), French and British troops were able to force a German retreat by exploiting a gap which appeared between the 1st and 2nd Armies, ending the German advance into France. The German Army retreated north of the Aisne River and dug in there, establishing the beginnings of a static western front that was to last for the next three years. Following this German retirement, the opposing forces made reciprocal outflanking manoeuvres, known as the Race for the Sea and quickly extended their trench systems from the Swiss frontier to the North Sea. The territory occupied by Germany held 64 percent of French pig-iron production, 24 percent of its steel manufacturing and 40 percent of the coal industry – dealing a serious blow to French industry.\n\nOn the Entente side (those countries opposing the German alliance), the final lines were occupied with the armies of each nation defending a part of the front. From the coast in the north, the primary forces were from Belgium, the British Empire and then France. Following the Battle of the Yser in October, the Belgian army controlled a length of West Flanders along the coast, known as the Yser Front, along the Yser river and the Yperlee canal, from Nieuwpoort to Boesinghe. Meanwhile, the British Expeditionary Force (BEF) occupied a position on the flank, having previous occupied a more central position.\n\nFrom 19 October until 22 November, the German forces made their final breakthrough attempt of 1914 during the First Battle of Ypres, which ended in a mutually-costly stalemate. After the battle, Erich von Falkenhayn judged that it was no longer possible for Germany to win the war by purely military means and on 18 November 1914 he called for a diplomatic solution. The Chancellor, Theobald von Bethmann-Hollweg; \"Generalfeldmarschall\" Paul von Hindenburg, commanding \"Ober Ost\" (Eastern Front high command); and Hindenburg's deputy, Erich Ludendorff, continued to believe that victory was achievable through decisive battles. During the Lodz offensive in Poland Falkenhayn hoped that the Russians would be made amenable to peace overtures. In his discussions with Bethmann-Hollweg, Falkenhayn viewed Germany and Russia as having no insoluble conflict and that the real enemies of Germany were France and Britain. A peace with only a few annexations of territory also seemed possible with France and that with Russia and France out of the war by negotiated settlements, Germany could concentrate on Britain and fight a long war with the resources of Europe at its disposal. Hindenburg and Ludendorff continued to believe that Russia could be defeated by a series of battles which cumulatively would have a decisive effect, after which Germany could finish off France and Britain.\n\nBetween the coast and the Vosges was a westward bulge in the trench line, named the Noyon salient for the captured French town at the maximum point of advance near Compiègne. Joffre's plan for 1915 was to attack the salient on both flanks to cut it off. The Fourth Army had attacked in Champagne from 20 December 1914 – 17 March 1915 but the French were not able to attack in Artois at the same time. The Tenth Army formed the northern attack force and was to attack eastwards into the Douai plain across a front between Loos and Arras. On 10 March, as part of the larger offensive in the Artois region, the British Army fought the Battle of Neuve Chapelle to capture Aubers Ridge. The assault was made by four divisions along a front. Preceded by a surprise bombardment lasting only 35 minutes, the initial assault made rapid progress and the village was captured within four hours. The advance then slowed because of supply and communication difficulties. The Germans brought up reserves and counter-attacked, forestalling the attempt to capture the ridge. Since the British had used about one-third of their supply of artillery ammunition, General Sir John French blamed the failure on the shortage of ammunition, despite the early success.\n\nAll sides had signed the Hague Conventions of 1899 and 1907, which prohibited the use of chemical weapons in warfare. In 1914, there had been small-scale attempts by both the French and Germans to use various tear gases, which were not strictly prohibited by the early treaties but which were also ineffective. The first use of more lethal chemical weapons was against the French near the Belgian town of Ypres.\n\nDespite the German plans to maintain the stalemate with the French and British, Albrecht, Duke of Württemberg, commander of the 4th Army planned an offensive at Ypres, site of the First Battle of Ypres in November 1914. The Second Battle of Ypres, April 1915, was intended to divert attention from offensives in the Eastern Front and disrupt Franco-British planning. After a two-day bombardment, the Germans released of of chlorine gas onto the battlefield. Though primarily a powerful irritant, it can asphyxiate in high concentrations or prolonged exposure. Being heavier than air, the gas crept across no man's land and drifted into the French trenches. The green-yellow cloud started killing some defenders and those in the rear fled in panic, creating an undefended gap in the Allied line. The Germans were unprepared for the level of their success and lacked sufficient reserves to exploit the opening. Canadian troops on the right drew back their left flank and halted the German advance. The gas attack was repeated two days later and caused a withdrawal of the Franco-British line but the opportunity had been lost.\n\nThe success of this attack would not be repeated, as the Allies countered by introducing gas masks and other countermeasures. An example of the success of these measures came a year later, on 27 April in the Gas attacks at Hulluch to the south of Ypres, where the 16th (Irish) Division withstood several German gas attacks. The British retaliated, developing their own chlorine gas and using it at the Battle of Loos in September 1915. Fickle winds and inexperience led to more British casualties from the gas than German. French, British and German forces all escalated the use of gas attacks through the rest of the war, developing the more deadly phosgene gas in 1915, then the infamous mustard gas in 1917, which could linger for days and could kill slowly and painfully. Countermeasures also improved and the stalemate continued.\n\nSpecialised aeroplanes for aerial combat were introduced in 1915. Aircraft were already in use for scouting and on 1 April, the French pilot Roland Garros became the first to shoot down an enemy aircraft by using a machine-gun that shot forward through the propeller blades. This was achieved by crudely reinforcing the blades to deflect bullets. Several weeks later Garros force-landed behind German lines. His aeroplane was captured and sent to Dutch engineer Anthony Fokker, who soon produced a significant improvement, the interrupter gear, in which the machine gun is synchronised with the propeller so it fires in the intervals when the blades of the propeller are out of the line of fire. This advance was quickly ushered into service, in the Fokker E.I (\"Eindecker\", or monoplane, Mark 1), the first single seat fighter aircraft to combine a reasonable maximum speed with an effective armament. Max Immelmann scored the first confirmed kill in an \"Eindecker\" on 1 August. Both sides developed improved weapons, engines, airframes and materials, until the end of the war. It also inaugurated the cult of the ace, the most famous being Manfred von Richthofen (the Red Baron). Contrary to the myth, anti-aircraft fire claimed more kills than fighters.\n\nThe final Entente offensive of the spring was the Second Battle of Artois, an offensive to capture Vimy Ridge and advance into the Douai plain. The French Tenth Army attacked on 9 May after a six-day bombardment and advanced to capture Vimy Ridge. German reinforcements counter-attacked and pushed the French back towards their starting points because French reserves had been held back and the success of the attack had come as a surprise. By 15 May the advance had been stopped, although the fighting continued until 18 June. In May the German Army captured a French document at La Ville-aux-Bois describing a new system of defence. Rather than relying on a heavily fortified front line, the defence was to be arranged in a series of echelons. The front line would be a thinly manned series of outposts, reinforced by a series of strongpoints and a sheltered reserve. If a slope was available, troops were deployed along the rear side for protection. The defence became fully integrated with command of artillery at the divisional level. Members of the German high command viewed this new scheme with some favour and it later became the basis of an elastic defence in depth doctrine against Entente attacks.\n\nDuring the autumn of 1915, the \"Fokker Scourge\" began to have an effect on the battlefront as Allied reconnaissance aircraft were nearly driven from the skies. These reconnaissance planes were used to direct gunnery and photograph enemy fortifications but now the Allies were nearly blinded by German fighters. However, the impact of German air superiority was diminished by their primarily defensive doctrine in which they tended to remain over their own lines, rather than fighting over Allied held territory.\n\nIn September 1915 the Entente allies launched another offensive, with the French Third Battle of Artois, Second Battle of Champagne and the British at Loos. The French had spent the summer preparing for this action, with the British assuming control of more of the front to release French troops for the attack. The bombardment, which had been carefully targeted by means of aerial photography, began on 22 September. The main French assault was launched on 25 September and, at first, made good progress in spite of surviving wire entanglements and machine gun posts. Rather than retreating, the Germans adopted a new defence-in-depth scheme that consisted of a series of defensive zones and positions with a depth of up to .\n\nOn 25 September, the British began the Battle of Loos, part of the Third Battle of Artois, which was meant to supplement the larger Champagne attack. The attack was preceded by a four-day artillery bombardment of 250,000 shells and a release of 5,100 cylinders of chlorine gas. The attack involved two corps in the main assault and two corps performing diversionary attacks at Ypres. The British suffered heavy losses, especially due to machine gun fire during the attack and made only limited gains before they ran out of shells. A renewal of the attack on 13 October fared little better. In December, French was replaced by General Douglas Haig as commander of the British forces.\n\nFalkenhayn believed that a breakthrough might no longer be possible and instead focused on forcing a French defeat by inflicting massive casualties. His new goal was to \"bleed France white\". As such, he adopted two new strategies. The first was the use of unrestricted submarine warfare to cut off Allied supplies arriving from overseas. The second would be attacks against the French army intended to inflict maximum casualties; Falkenhayn planned to attack a position from which the French could not retreat, for reasons of strategy and national pride and thus trap the French. The town of Verdun was chosen for this because it was an important stronghold, surrounded by a ring of forts, that lay near the German lines and because it guarded the direct route to Paris.\n\nFalkenhayn limited the size of the front to to concentrate artillery firepower and to prevent a breakthrough from a counter-offensive. He also kept tight control of the main reserve, feeding in just enough troops to keep the battle going. In preparation for their attack, the Germans had amassed a concentration of aircraft near the fortress. In the opening phase, they swept the air space of French aircraft, which allowed German artillery-observation aircraft and bombers to operate without interference. In May, the French countered by deploying \"escadrilles de chasse\" with superior Nieuport fighters and the air over Verdun turned into a battlefield as both sides fought for air superiority.\n\nThe Battle of Verdun began on 21 February 1916 after a nine-day delay due to snow and blizzards. After a massive eight-hour artillery bombardment, the Germans did not expect much resistance as they slowly advanced on Verdun and its forts. Sporadic French resistance was encountered. The Germans took Fort Douaumont and then reinforcements halted the German advance by 28 February.\n\nThe Germans turned their focus to Le Mort Homme on the west bank of the Meuse which blocked the route to French artillery emplacements, from which the French fired across the river. After some of the most intense fighting of the campaign, the hill was taken by the Germans in late May. After a change in French command at Verdun from the defensive-minded Philippe Pétain to the offensive-minded Robert Nivelle, the French attempted to re-capture Fort Douaumont on 22 May but were easily repulsed. The Germans captured Fort Vaux on 7 June and with the aid of diphosgene gas, came within of the last ridge before Verdun before being contained on 23 June.\n\nOver the summer, the French slowly advanced. With the development of the rolling barrage, the French recaptured Fort Vaux in November and by December 1916 they had pushed the Germans back from Fort Douaumont, in the process rotating 42 divisions through the battle. The Battle of Verdun—also known as the 'Mincing Machine of Verdun' or 'Meuse Mill'—became a symbol of French determination and self-sacrifice.\n\nIn the spring, Allied commanders had been concerned about the ability of the French Army to withstand the enormous losses at Verdun. The original plans for an attack around the River Somme were modified to let the British make the main effort. This would serve to relieve pressure on the French, as well as the Russians who had also suffered great losses. On 1 July, after a week of heavy rain, British divisions in Picardy began the Battle of the Somme with the Battle of Albert, supported by five French divisions on their right flank. The attack had been preceded by seven days of heavy artillery bombardment. The experienced French forces were successful in advancing but the British artillery cover had neither blasted away barbed wire, nor destroyed German trenches as effectively as was planned. They suffered the greatest number of casualties (killed, wounded and missing) in a single day in the history of the British Army, about 57,000.\n\nThe Verdun lesson learnt, the Allies' tactical aim became the achievement of air superiority and until September, German aircraft were swept from the skies over the Somme. The success of the Allied air offensive caused a reorganisation of the German air arm and both sides began using large formations of aircraft rather than relying on individual combat. After regrouping, the battle continued throughout July and August, with some success for the British despite the reinforcement of the German lines. By August, General Haig had concluded that a breakthrough was unlikely and instead, switched tactics to a series of small unit actions. The effect was to straighten out the front line, which was thought necessary in preparation for a massive artillery bombardment with a major push.\n\nThe final phase of the battle of the Somme saw the first use of the tank on the battlefield. The Allies prepared an attack that would involve 13 British and Imperial divisions and four French corps. The attack made early progress, advancing in places but the tanks had little effect due to their lack of numbers and mechanical unreliability. The final phase of the battle took place in October and early November, again producing limited gains with heavy loss of life. All told, the Somme battle had made penetrations of only and failed to reach the original objectives. The British had suffered about 420,000 casualties and the French around 200,000. It is estimated that the Germans lost 465,000, although this figure is controversial.\n\nThe Somme led directly to major new developments in infantry organisation and tactics; despite the terrible losses of 1 July, some divisions had managed to achieve their objectives with minimal casualties. In examining the reasons behind losses and achievements, once the British war economy produced sufficient equipment and weapons, the army made the platoon the basic tactical unit, similar to the French and German armies. At the time of the Somme, British senior commanders insisted that the company (120 men) was the smallest unit of manoeuvre; less than a year later, the section of ten men would be so.\n\nIn August 1916 the German leadership along the western front had changed as Falkenhayn resigned and was replaced by Hindenburg and Ludendorff. The new leaders soon recognised that the battles of Verdun and the Somme had depleted the offensive capabilities of the German Army. They decided that the German Army in the west would go over to the strategic defensive for most of 1917, while the Central powers would attack elsewhere.\n\nDuring the Somme battle and through the winter months, the Germans created a fortification behind the Noyon Salient that would be called the Hindenburg Line, using the defensive principles elaborated since the defensive battles of 1915, including the use of Eingreif divisions. This was intended to shorten the German front, freeing 10 divisions for other duties. This line of fortifications ran from Arras south to St Quentin and shortened the front by about . British long-range reconnaissance aircraft first spotted the construction of the Hindenburg Line in November 1916.\n\nThe Hindenburg Line was built between 2 and behind the German front line. On 25 February German forces began retreating to the line and the withdrawal was completed on 5 April, leaving behind a devastated territory to be occupied by the Allies. This withdrawal negated the French strategy of attacking both flanks of the Noyon salient, as it no longer existed. However, offensive advances by the British continued as the High Command claimed, with some justice, that this withdrawal resulted from the casualties the Germans received during the Battles of the Somme and Verdun, despite the Allies suffering greater losses.\n\nMeanwhile, on 6 April the United States declared war on Germany. In early 1915, following the sinking of the , Germany had stopped its unrestricted submarine warfare in the Atlantic because of concerns of drawing the United States into the conflict. With the growing discontent of the German public due to the food shortages, however, the government resumed unrestricted submarine warfare in February 1917. They had calculated that a successful submarine and warship siege of Britain would force that country out of the war within six months, while American forces would take a year to become a serious factor on the Western Front. The submarine and surface ships had a long period of success before Britain resorted to the convoy system, bringing a large reduction in shipping losses.\nBy 1917, the size of the British Army on the Western Front had grown to two-thirds the total numbers in the French forces. In April 1917 the BEF began the Battle of Arras. The Canadian Corps and the 5th Division, attacked German lines at Vimy Ridge, capturing the heights and the First Army to the south achieved the deepest advance since trench warfare began. Later attacks were confronted by German reinforcements defending the area using the lessons learned on the Somme in 1916. British attacks were contained and, according to Gary Sheffield, a greater rate of daily loss was inflicted on the British than in \"any other major battle\".\n\nDuring the winter of 1916–1917, German air tactics had been improved, a fighter training school was opened at Valenciennes and better aircraft with twin guns were introduced. The result was near disastrous losses for Allied air power, particularly for the British, Portuguese, Belgians and Australians who were struggling with outmoded aircraft, poor training and weak tactics. As a result, the Allied air successes over the Somme would not be repeated and heavy losses were inflicted by the Germans. During their attack at Arras, the British lost 316 air crews and the Canadians lost 114 compared to 44 lost by the Germans. This became known to the Royal Flying Corps as Bloody April.\n\nThe same month, the French Commander-in-Chief, General Robert Nivelle, ordered a new offensive against the German trenches, promising that it would end the war within 48 hours. The 16 April attack, dubbed the Nivelle Offensive (also known as the Second Battle of the Aisne, after the area where the offensive took place), would be 1.2 million men strong, preceded by a week-long artillery bombardment and accompanied by tanks. The offensive proceeded poorly as the French troops, with the help of two Russian brigades, had to negotiate rough, upward-sloping terrain in extremely bad weather. Planning had been dislocated by the voluntary German withdrawal to the Hindenburg Line. Secrecy had been compromised and German aircraft gained air superiority, making reconnaissance difficult and in places, the creeping barrage moved too fast for the French troops. Within a week the French suffered 120,000 casualties. Despite the casualties and his promise to halt the offensive if it did not produce a breakthrough, Nivelle ordered the attack to continue into May.\n\nOn 3 May the weary French 2nd Colonial Division, veterans of the Battle of Verdun, refused orders, arriving drunk and without their weapons. Lacking the means to punish an entire division, its officers did not immediately implement harsh measures against the mutineers. Mutinies occurred in 54 French divisions and 20,000 men deserted. Other Allied forces attacked but suffered massive casualties. Appeals to patriotism and duty followed, as did mass arrests and trials. The French soldiers returned to defend their trenches but refused to participate in further offensive action. On 15 May Nivelle was removed from command, replaced by Pétain who immediately stopped the offensive. The French would go on the defensive for the following months to avoid high casualties and to restore confidence in the French High Command, while the British assumed greater responsibility.\n\nOn 25 June the first US troops began to arrive in France, forming the American Expeditionary Force. However, the American units did not enter the trenches in divisional strength until October. The incoming troops required training and equipment before they could join in the effort, and for several months American units were relegated to support efforts. In spite of this, however, their presence provided a much-needed boost to Allied morale, with the promise of further reinforcements that could tip the manpower balance towards the Allies.\n\nIn June, the British launched an offensive in Flanders, in part to take pressure off the French armies on the Aisne, after the French part of the Nivelle Offensive failed to achieve the strategic victory that had been planned and French troops began to mutiny. The offensive began on 7 June, with a British attack on Messines Ridge, south of Ypres, to retake the ground lost in the First and Second battles in 1914. Since 1915 specialist Royal Engineer tunnelling companies had been digging tunnels under the ridge, and about of explosives had been planted in 21 mines under the German defences. Following several weeks of bombardment, the explosives in 19 of these mines were detonated, killing up to 7,000 German troops. The infantry advance that followed relied on three creeping barrages which the British infantry followed to capture the plateau and the east side of the ridge in one day. German counter-attacks were defeated and the southern flank of the Gheluvelt plateau was protected from German observation.\n\nOn 11 July 1917, during \"Unternehmen Strandfest\" (Operation Beachparty) at Nieuport on the coast, the Germans introduced a new weapon into the war when they fired a powerful blistering agent Sulfur mustard (Yellow Cross) gas. The artillery deployment allowed heavy concentrations of the gas to be used on selected targets. Mustard gas was persistent and could contaminate an area for days, denying it to the British, an additional demoralising factor. The Allies increased production of gas for chemical warfare but took until late 1918 to copy the Germans and begin using mustard gas.\n\nFrom 31 July to 10 November the Third Battle of Ypres included the First Battle of Passchendaele and culminated in the Second Battle of Passchendaele. The battle had the original aim of capturing the ridges east of Ypres then advancing to Roulers and Thourout to close the main rail line supplying the German garrisons on the Western front north of Ypres. If successful the northern armies were then to capture the German submarine bases on the Belgian coast. It was later restricted to advancing the British Army onto the ridges around Ypres, as the unusually wet weather slowed British progress. The Canadian Corps relieved the II ANZAC Corps and took the village of Passchendaele on 6 November, despite rain, mud and many casualties. The offensive was costly in manpower for both sides for relatively little gain of ground against determined German resistance but the ground captured was of great tactical importance. In the drier periods, the British advance was inexorable and during the unusually wet August and in the Autumn rains that began in early October, the Germans achieved only costly defensive successes, which led the German commanders in early October to begin preparations for a general retreat. Both sides lost a combined total of over a half million men during this offensive. The battle has become a byword among some British revisionist historians for bloody and futile slaughter, whilst the Germans called Passchendaele \"the greatest martyrdom of the war\".\n\nOn 20 November the British launched the first massed tank attack and the first attack using predicted artillery-fire (aiming artillery without firing the guns to obtain target data) at the Battle of Cambrai. The Allies attacked with 324 tanks (with one-third held in reserve) and twelve divisions, advancing behind a hurricane bombardment, against two German divisions. The machines carried fascines on their fronts to bridge trenches and the German tank traps. Special \"grapnel tanks\" towed hooks to pull away the German barbed wire. The attack was a great success for the British, who penetrated further in six hours than at the Third Ypres in four months, at a cost of only 4,000 British casualties. The advance produced an awkward salient and a surprise German counter-offensive began on 30 November, which drove back the British in the south and failed in the north. Despite the reversal, the attack was seen as a success by the Allies, proving that tanks could overcome trench defences. The Germans realised that the use of tanks by the Allies posed a new threat to any defensive strategy they might mount. The battle had also seen the first mass use of German \"stosstruppen\" on the Western front in the attack, who used infantry infiltration tactics to penetrate British defences, bypassing resistance and quickly advancing into the British rear.\n\nFollowing the successful Allied attack and penetration of the German defences at Cambrai, Ludendorff and Hindenburg determined that the only opportunity for German victory lay in a decisive attack along the Western front during the spring, before American manpower became overwhelming. On 3 March 1918, the Treaty of Brest-Litovsk was signed and Russia withdrew from the war. This would now have a dramatic effect on the conflict as 33 divisions were released from the Eastern Front for deployment to the west. The Germans occupied almost as much Russian territory under the provisions of the Treaty of Brest-Litovsk as they did in the Second World War but this considerably restricted their troop redeployment. The Germans achieved an advantage of 192 divisions in the west to the 178 Allied divisions, which allowed Germany to pull veteran units from the line and retrain them as \"sturmtruppen\" (40 infantry and 3 cavalry divisions were retained for German occupation duties in the east).\n\nThe Allies lacked unity of command and suffered from morale and manpower problems, the British and French armies were severely depleted and not in a position to attack in the first half of the year, while the majority of the newly arrived American troops were still training, with just six complete divisions in the line. Ludendorff decided on an offensive strategy beginning with a big attack against the British on the Somme, to separate them from the French and drive them back to the channel ports. The attack would combine the new storm troop tactics with over 700 aircraft, tanks and a carefully planned artillery barrage that would include gas attacks.\n\nOperation Michael, the first of the German Spring Offensives, very nearly succeeded in driving the Allied armies apart, advancing to within shelling distance of Paris for the first time since 1914. As a result of the battle, the Allies agreed on unity of command. General Ferdinand Foch was appointed commander of all Allied forces in France. The unified Allies were better able to respond to each of the German drives and the offensive turned into a battle of attrition. In May, the American divisions also began to play an increasing role, winning their first victory in the Battle of Cantigny. By summer, between 250,000 and 300,000 American soldiers were arriving every month. A total of 2.1 million American troops would be deployed on this front before the war came to an end. The rapidly increasing American presence served as a counter for the large numbers of redeployed German forces.\n\nIn July, Foch began the Second Battle of the Marne, a counter-offensive against the Marne salient which was eliminated by August. The Battle of Amiens began two days later, with Franco-British forces spearheaded by Australian and Canadian troops, along with 600 tanks and 800 aircraft. Hindenburg named 8 August as the \"Black Day of the German army\". The Italian 2nd Corps, commanded by General Alberico Albricci, also participated in the operations around Reims. German manpower had been severely depleted after four years of war and its economy and society were under great internal strain. The Allies fielded 216 divisions against 197 German divisions. The Hundred Days Offensive beginning in August proved the final straw and following this string of military defeats, German troops began to surrender in large numbers. As the Allied forces advanced, Prince Maximilian of Baden was appointed as Chancellor of Germany in October to negotiate an armistice. Ludendorff was forced out and fled to Sweden. The German retreat continued and the German Revolution put a new government in power. The Armistice of Compiègne was quickly signed, stopping hostilities on the Western Front on 11 November 1918, later known as Armistice Day. The German Imperial Monarchy collapsed when General Groener, the successor to Ludendorff, backed the moderate Social Democratic Government under Friedrich Ebert, to forestall a revolution like those in Russia the previous year.\n\nThe war along the Western Front led the German government and its allies to sue for peace in spite of German success elsewhere. As a result, the terms of the peace were dictated by France, Britain and the United States, during the 1919 Paris Peace Conference. The result was the Treaty of Versailles, signed in June 1919 by a delegation of the new German government. The terms of the treaty constrained Germany as an economic and military power. The Versailles treaty returned the border provinces of Alsace-Lorraine to France, thus limiting the coal required by German industry. The Saar, which formed the west bank of the Rhine, would be demilitarised and controlled by Britain and France, while the Kiel Canal opened to international traffic. The treaty also drastically reshaped Eastern Europe. It severely limited the German armed forces by restricting the size of the army to 100,000 and disallowing a navy or air force. The navy was sailed to Scapa Flow under the terms of surrender but was later scuttled as a reaction to the treaty.\n\nThe war in the trenches of the Western Front left tens of thousands of maimed soldiers and war widows. The unprecedented loss of life had a lasting effect on popular attitudes toward war, resulting later in an Allied reluctance to pursue an aggressive policy toward Adolf Hitler. Belgium suffered 30,000 civilian dead and France 40,000 (including 3,000 merchant sailors). The British lost civilian dead, were killed in air and naval attacks, were killed at sea and there were marine deaths. Another 62,000 Belgian, 107,000 British and 300,000 French civilians died due to war-related causes.\n\nGermany in 1919 was bankrupt, the people living in a state of semi-starvation and having no commerce with the remainder of the world. The Allies occupied the Rhine cities of Cologne, Koblenz and Mainz, with restoration dependent on payment of reparations. In Germany a Stab-in-the-back myth (\"Dolchstoßlegende\") was propagated by Hindenburg, Ludendorff and other defeated generals, that the defeat was not the fault of the 'good core' of the army but due to certain left-wing groups within Germany who signed a disastrous armistice; this would later be exploited by nationalists and the Nazi party propaganda to excuse the overthrow of the Weimar Republic in 1930 and the imposition of the Nazi dictatorship after March 1933.\n\nFrance lost more casualties relative to its population than any other great power and the industrial north-east of the country was devastated by the war. The provinces overrun by Germany had produced 40 percent of French coal and 58 percent of its steel output. Once it was clear that Germany was going to be defeated, Ludendorff had ordered the destruction of the mines in France and Belgium. His goal was to cripple the industries of Germany's main European rival. To prevent similar German attacks in the future, France later built a massive series of fortifications along the German border known as the Maginot Line.\n\n\nBooks\n\nJournals\n\n"}
{"id": "41819073", "url": "https://en.wikipedia.org/wiki?curid=41819073", "title": "William L. Chameides", "text": "William L. Chameides\n\nWilliam L. \"Bill\" Chameides (born November 21, 1949 in New York City) is an American atmospheric scientist who was the dean of Duke University's Nicholas School of the Environment from 2007 until July 1, 2014. He is an ISI highly cited researcher, as well as a member of the National Academy of Sciences and a fellow of the American Geophysical Union. In December 2013, Chameides announced that he would be stepping down from his position at Duke the following June, saying that he \"feels comfortable stepping down now given that he has accomplished many of his initial goals.\" He was replaced by Alan Townsend on July 1, but remains a member of the Nicholas School faculty.\n\nChameides, a native of New York City, originally wanted to be a lawyer at the time he went to college. However, he later changed his major, first to physics and then to atmospheric science, as a result of taking an undergraduate physics course. He received a bachelor's degree from Binghamton University in 1970, a Master of philosophy degree from Yale University in 1973, and a PhD in 1974, also from Yale. He then began postdoctoral research at the University of Michigan, where he conducted research with Ralph J. Cicerone. After completing this research, he spent 25 years at the Georgia Institute of Technology, where he eventually became the chairman of the department of atmospheric sciences. While an associate professor there, he was the editor-in-chief of the Journal of Geophysical Research from 1985 to 1989, and, five years later, was the lead author on a study in \"Science\" which reported that ozone from photochemical smog may significantly reduce global food production. In the study, Chameides et al. wrote that about 60% of smog is produced in North America, Europe, China, and Japan, which also produce about 60% of the global food supply. In 1998, Chameides was elected into the National Academy of Sciences, becoming the second Georgia Tech professor in the university's history to be so honored (the first was Mostafa El-Sayed in 1980). In 2005, he left Georgia Tech to become the chief scientist at the Environmental Defense Fund, a decision that Virginia Gewin referred to as \"unorthodox.\" Chameides said he made this decision because he wanted to \"...do more to advance the cause of good environmental stewardship.\" In 2011, Chameides was the vice-chair of a report issued by the United States National Research Council entitled \"America's Climate Choices.\"\n\nChameides has been active in highlighting the dangers of global warming, and has testified before the U.S. House of Representatives' Committee on Science, Space and Technology on the matter. In his testimony, delivered on April 25, 2013, he said that global warming is occurring, is primarily caused by human activity, and that it will have many negative effects on the United States, including, but not limited to, \"more intense and frequent heat waves, risks to coastal communities from sea level rise, greater drying of the arid Southwest, and increased public health risks.\" He currently runs a blog on the Nicholas School of the Environment website called \"The Green Grok,\" and has also blogged about global warming and other environmental issues on the Huffington Post and for Popular Science. Chameides has also written about the economics of climate change mitigation for both \"Science\" and for The Guardian. Additionally, after Joe Bastardi claimed on Fox News Channel that carbon dioxide can't cause global warming because of the first law of thermodynamics and Le Chatelier's principle, Media Matters for America contacted Chameides by email, who referred to Bastardi's claims as \"utter nonsense.\" He has also, along with Alan Leshner, criticized the North Carolina government's proposal of a law in response to a report stating that sea levels were projected to rise about 39 inches over the next century. In an article for the News & Observer, they accused the law's supporters of trying to \"disregard projections of sea level rise\" and trying to \"outlaw climate change.\" The law, which would prevent the government from using sea-level data based on anything except \"historical data\" for the next four years, was passed despite Chameides' objections. He has also criticized the cash for clunkers program by pointing out that, while one of its goals is to decrease CO emissions by promoting the purchase of more fuel-efficient cars, manufacturing new cars results in the production of carbon dioxide—he puts the figure at between 3 and 12 tons per car. He has also calculated that trading in a clunker that gets 18 mpg for a new car that gets 22 mpg would necessitate 5 and a half years of typical driving to offset the car's carbon footprint. Chameides has also written about other environmental issues, like chemicals used in certain consumer products. For example, when interviewed by the Taipei Times about the safety of optical brighteners in laundry detergent, he said that scientific studies on the topic have been \"inconclusive.\"\n"}
{"id": "25232357", "url": "https://en.wikipedia.org/wiki?curid=25232357", "title": "WindSim", "text": "WindSim\n\nWindSim is wind energy software that uses CFD to optimize wind turbine placement in onshore and offshore wind farms.\nWindSim is used worldwide by wind resource assessment professionals to help design more profitable wind farms.\n\nWindSim was first developed by Vector AS, a consulting firm, as an internal tool used to build the Norwegian Wind Atlas in cooperation with Norwegian Meteorological Institute .\nWindSim was productized for PC platforms in 2003.\nThe software developer and the software product are both named \"WindSim\".\n\n\nWindSim company website \nNorwegian Wind Atlas \n"}
{"id": "4860340", "url": "https://en.wikipedia.org/wiki?curid=4860340", "title": "Wind profiler", "text": "Wind profiler\n\nA wind profiler is a type of weather observing equipment that uses radar or sound waves (SODAR) to detect the wind speed and direction at various elevations above the ground. Readings are made at each kilometer above sea level, up to the extent of the troposphere (i.e., between 8 and 17 km above mean sea level). Above this level there is inadequate water vapor present to produce a radar \"bounce.\" The data synthesized from wind direction and speed is very useful to meteorological forecasting and timely reporting for flight planning. A twelve-hour history of data is available through NOAA websites.\n\nIn a typical implementation, the radar or sodar can sample along each of five beams: one is aimed vertically to measure vertical velocity, and four are tilted off vertical and oriented orthogonal to one another to measure the horizontal components of the air's motion. A profiler's ability to measure winds is based on the assumption that the turbulent eddies that induce scattering are carried along by the mean wind. The energy scattered by these eddies and received by the profiler is orders of magnitude smaller than the energy transmitted. However, if sufficient samples can be obtained, then the amplitude of the energy scattered by these eddies can be clearly identified above the background noise level, then the mean wind speed and direction within the volume being sampled can be determined. The radial components measured by the tilted beams are the vector sum of the horizontal motion of the air toward or away from the radar and any vertical motion present in the beam. Using appropriate trigonometry, the three-dimensional meteorological velocity components (u,v,w) and wind speed and wind direction are calculated from the radial velocities with corrections for vertical motions.\n\nPulse-Doppler radar wind profilers operate using electromagnetic (EM) signals to remotely sense winds aloft. The radar transmits an electromagnetic pulse along each of the antenna's pointing directions. A UHF profiler includes subsystems to control the radar's transmitter, receiver, signal processing, and Radio Acoustic Sounding System (RASS), if provided, as well as data telemetry and remote control.\n\nThe duration of the transmission determines the length of the pulse emitted by the antenna, which in turn corresponds to the volume of air illuminated (in electrical terms) by the radar beam. Small amounts of the transmitted energy are scattered back (referred to as backscattering) toward and received by the radar. Delays of fixed intervals are built into the data processing system so that the radar receives scattered energy from discrete altitudes, referred to as range gates. The Doppler frequency shift of the backscattered energy is determined, and then used to calculate the velocity\nof the air toward or away from the radar along each beam as a function of altitude. The source of the backscattered energy (radar “targets”) is small-scale turbulent fluctuations that induce irregularities in the radio refractive index of the atmosphere. The radar is most sensitive to scattering by turbulent eddies whose spatial scale is ½ the wavelength of the radar, or approximately 16 centimeters (cm) for a UHF profiler.\n\nA boundary-layer radar wind profiler can be configured to compute averaged wind profiles for periods ranging from a few minutes to an hour. Boundary-layer radar wind profilers are often configured to sample in more than one mode. For example, in a “low mode,” the pulse of energy transmitted by the profiler may be 60 m in length. The pulse length determines the depth of the column of air being sampled and thus the vertical resolution of the data. In a “high mode,” the pulse length is increased, usually to 100 m or greater. The longer pulse length means that more energy is being transmitted for each sample, which improves the signal-to-noise ratio (SNR) of the data. Using a longer pulse length increases the depth of the sample volume and thus decreases the vertical resolution in the data. The greater energy output of the high mode increases the maximum altitude to which the radar wind profiler can sample, but at the expense of coarser vertical resolution and an increase in the\naltitude at which the first winds are measured. When radar wind profilers are operated in multiple modes, the data are often combined into a single overlapping data set to simplify postprocessing and data validation procedures.\n\nAlternatively, a wind profiler may use sound waves to measure wind speed at various heights above the ground, and the thermodynamic structure of the lower layer of the atmosphere. These sodars can be divided in mono-static system using the same antenna for transmitting and receiving, and bi-static system using separate antennas. The difference between the two antenna systems determines whether atmospheric scattering is by temperature fluctuations (in mono-static systems), or by both temperature and wind velocity fluctuations (in bi-static systems).\n\nMono-static antenna systems can be divided further into two categories: those using multiple axis, individual antennas and those using a single phased array antenna. The multiple-axis systems generally use three individual antennas aimed in specific directions to steer the acoustic beam. One antenna is generally aimed vertically, and the other two are tilted slightly from the vertical at an orthogonal angle. Each of the individual antennas may use a single transducer focused into a parabolic reflector to form a parabolic loudspeaker, or an array of speaker drivers and horns (transducers) all transmitting in-phase to form a single beam. Both the tilt angle from the vertical and the azimuth angle of each antenna are fixed when the system is set up.\n\nThe vertical range of sodars is approximately 0.2 to 2 kilometers (km) and is a function of frequency, power output, atmospheric stability, turbulence, and, most importantly, the noise environment in which a sodar is operated. Operating frequencies range from less than 1000 Hz to over 4000 Hz, with power levels up to several hundred watts. Due to the attenuation characteristics of the atmosphere, high power, lower frequency sodars will generally produce greater height coverage. Some sodars can be operated in different modes to better match vertical resolution and range to the application. This is accomplished through a relaxation between pulse length and maximum altitude.\n\n"}
{"id": "28500519", "url": "https://en.wikipedia.org/wiki?curid=28500519", "title": "Yukon Interior dry forests", "text": "Yukon Interior dry forests\n\nThe Yukon Interior dry forests is a taiga ecoregion of Canada.\n\nThis ecoregion, which covers much of the southern Yukon and a very small portion of northwestern British Columbia, is mainly located on the Yukon Plateau, which consists of rolling hills, plateaus and deeply cut, broad valleys. The area has a dry subarctic climate, with summer temperatures averaging around , and winter temperatures averaging from to . Precipitation is low, averaging between 225–400 mm (8.9-15.7 in), with higher elevation areas and areas to the north-east receiving greater precipitation.\n\nBlack spruce (\"Picea mariana\") and white spruce (\"Picea glauca\") are the two most common trees, with lodgepole pine (\"Pinus contorta\" subsp. \"latifolia\") being found in very dry or burnt sites. South-facing, low elevation slopes are often characterised by grassland communities. Dwarf birch, willow and subalpine fir (\"Abies lasiocarpa\") are found the subalpine zone below the tree line, while the alpine zone above supports a sparse vegetative cover consisting of mountain avens, dwarf shrubs, forbs, grasses and lichen.\n\nMammals of this ecoregion include caribou (\"Rangifer tarandus\"), moose (\"Alces alces\"), mountain goat (\"Oreamnus americanus\"), Dall sheep (\"Ovis dalli\"), grizzly bear (\"Ursos arctos horriblus\"), black bear (\"Ursus americanus\"), gray wolf (\"Canis lupus\"), coyote (\"Canis latrans\"), beaver (\"Castor canadensis\") and hare. Avian species include the common raven (\"Corvus corax\"), ptarmigan and golden eagle (\"Aquila chryaetos\").\n\nThis ecoregion is well preserved, with an estimated 75% intact. Uplands are better preserved than valley bottoms, wherein most of the development in this ecoregion has taken place. There are no large protected areas in this ecoregion, but smaller areas are the Charlie Cole Creek Ecological Reserve and the Nisutlin River Delta National Wildlife Area. Timber harvesting, mining and a state-sponsored wolf kill are the greatest threats to the region's ecological integrity.\n\n"}
