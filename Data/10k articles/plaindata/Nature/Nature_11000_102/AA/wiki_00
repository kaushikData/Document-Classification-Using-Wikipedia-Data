{"id": "30388766", "url": "https://en.wikipedia.org/wiki?curid=30388766", "title": "Affleck–Dine mechanism", "text": "Affleck–Dine mechanism\n\nThe Affleck–Dine mechanism (AD mechanism) is a postulated mechanism for explaining baryogenesis during the primordial Universe immediately following the Big Bang. Thus, the AD mechanism may explain the asymmetry between matter and anti-matter in the current Universe. It was proposed in 1985 by Ian Affleck and Michael Dine of Princeton University.\n\nIn the supersymmetry theory of particle physics, ordinary quarks and leptons have scalar partners that carry baryon and lepton numbers. As the latter decay into fermions during the early Universe, the net baryon number that they carry can then form the currently observed excess of ordinary baryons. This occurs due to interactions of the scalars with the inflaton field, resulting in CP violations.\n\nThe AD mechanism must have occurred during or after the reheating event that followed cosmic inflation. This may explain why the net mass of normal matter and dark matter are apparently so close to each other, rather than being widely different.\n\n"}
{"id": "32302594", "url": "https://en.wikipedia.org/wiki?curid=32302594", "title": "Agrocarbon", "text": "Agrocarbon\n\nAgrocarbon is the international brand name of biochar products produced by 3Ragrocabon. 3Ragrocarbon is owned and operated by Terra Humanities LTD, a Swedish ecological-innovation technology and engineering company. 3RAgrocarbon utilizes patented 3R zero-emission Pyrolysis to create environmentally friendly bio-char and soil-nutrient enrichment products. The firm is headquartered in Hungary where its main production facility is located. The company is supported by, and partnered with the European Union on several projects focused on eco-safe agricultural and soil nutrient initiatives. The Agrocarbon is applied in all formulations, from stand alone biofertilizer to any combination as compost or soil activator. The refined and formulated Agrocarbon products are multi effect used for sustainable soil and carbon negative environmental and climate protection improvements. This includes economical food crop production and forest nursery, biological pest control, natural fertilization, soil moisture retention, restoration of soil biodiversity and natural balance.\n\nAgrocarbons are created using 3R slow pyrolysis. The process entails the input of high quality plant or animal biomass/waste in to a large horizontal rotating kiln. The kiln is specifically engineered for this process and is then heated between 450 °C - 850° with varying core temperatures. This causes the reductive thermal decomposition of the biomass input and creates Agrocarbons which can be utilized for a number of agricultural and industrial purposes ranging from soil enhancers to renewable energy. The 3R pyrolysis process is an original development and innovation of Terra Humana ltd. The development of this process is very substantial as most other pyrolysis methods are unable to operate efficiently on an industrial scale. 3R pyrolysis technology was proven and demonstrated at a technical readiness level of 8 and will soon reach level 9. The purpose of this process is to add value to otherwise wasted animal and plant byproducts.\n\nAll of the materials and gases used in the Agrocarbon development process are reused and recycled. Gases resulting from the 3R pyrolysis process are captured and chemically processed to create liquid bio-fuels. 3r Agrocarbon meets all environmental standards of the European Union and United States.\n\n3R agrocarbon is used as a soil nutrient enhancer to help recover phosphorus. Phosphorus is an element that is required for all organic life and is imperative to healthy plant growth. Most agricultural producers utilize phosphorus fertilizers, however these types of fertilizers are created using phosphate rocks which are non-renewable resources. 3R agrocarbon is rich in phosphorus due to its biomass inputs. In addition, the porous structure of the Agrocarbon bone-char is ideal for microbes beneficial to crop growth and can also be utilized to carry biological control agents.\n\n\n"}
{"id": "11208804", "url": "https://en.wikipedia.org/wiki?curid=11208804", "title": "Ann Zwinger", "text": "Ann Zwinger\n\nAnn Haymond Zwinger (1925–2014) was the author of many natural histories noted for detail and lyrical prose.\n\nAnn Haymond Zwinger was born March 12, 1925, in Muncie, Indiana, the daughter of William and Ann Haymond. While young, she lived along the White River. She studied art history and was awarded two degrees, an A.B. in Arts in 1946 by Wellesley College with the designation \"Wellesley College Scholar,\" now considered roughly equivalent to \"cum laude,\" and an A.M. in Fine Arts by Indiana University in 1950. She married Herman H. Zwinger, a pilot, in 1952. She taught Southwest Studies and English at Colorado College.\n\nShe died in Portland, Oregon on August 30, 2014.\n\nShe and co-author Beatrice Willard were finalists for the 1973 National Book Award in science for \"Land Above the Trees.\" In 1976, she received the John Burroughs Memorial Association Gold Medal for a distinguished contribution in natural history, \"Run, River, Run\". For the same book, she also received the Friends of American Writers Award for non-fiction.\n\n\n\n"}
{"id": "200716", "url": "https://en.wikipedia.org/wiki?curid=200716", "title": "Barycenter", "text": "Barycenter\n\nThe barycenter (or barycentre; from the Ancient Greek \"heavy\" + \"centre\") is the center of mass of two or more bodies that orbit each other and is the point about which the bodies orbit. It is an important concept in fields such as astronomy and astrophysics. The distance from a body's center of mass to the barycenter can be calculated as a two-body problem.\n\nIf one of two orbiting bodies is much more massive than the other and the bodies are relatively close to one another, the barycenter will typically be located within the more massive object. In this case, rather than the two bodies appearing to orbit a point between them, the less massive body will appear to orbit about the more massive body, while the more massive body might be observed to wobble slightly. This is the case for the Earth–Moon system, in which the barycenter is located on average from the Earth's center, 75% of the Earth's radius of . When the two bodies are of similar masses, the barycenter will generally be located between them and both bodies will follow an orbit around it. This is the case for Pluto and Charon, as well as for many binary asteroids and binary stars. When the less massive object is far away, the barycenter can be located outside the more massive object. This is the case for Jupiter and the Sun; despite the Sun being thousandfold more massive than Jupiter, their barycenter is slightly outside the Sun due to the relatively large distance between them.\n\nIn astronomy, barycentric coordinates are non-rotating coordinates with the origin at the barycenter of two or more bodies. The International Celestial Reference System is a barycentric coordinate system centered on the Solar System's barycenter.\n\nIn geometry, the term \"barycenter\" is synonymous with centroid, the geometric center of a geometric figure.\n\nThe barycenter is one of the foci of the elliptical orbit of each body. This is an important concept in the fields of astronomy and astrophysics. If \"a\" is the distance between the centers of the two bodies (the semi-major axis of the system), \"r\" is the semi-major axis of the primary's orbit around the barycenter, and is the semi-major axis of the secondary's orbit. When the barycenter is located \"within\" the more massive body, that body will appear to \"wobble\" rather than to follow a discernible orbit. In a simple two-body case, \"r\", the distance from the center of the primary to the barycenter is given by:\n\nwhere :\n\nThe following table sets out some examples from the Solar System. Figures are given rounded to three significant figures. The term primary–secondary is used to distinguish between involved participants; with the larger called \"the primary\", and the smaller called \"the secondary\".\n\nIf — which is true for the Sun and any planet — then the ratio approximates to:\n\nHence, the barycenter of the Sun–planet system will lie outside the Sun only if:\n\nThat is, where the planet is massive \"and\" far from the Sun.\n\nIf Jupiter had Mercury's orbit (), the Sun–Jupiter barycenter would be approximately 55,000 km from the center of the Sun (). But even if the Earth had Eris' orbit (), the Sun–Earth barycenter would still be within the Sun (just over 30,000 km from the center).\n\nTo calculate the actual motion of the Sun, you only need to consider the motions of the four giant planets (Jupiter, Saturn, Uranus, Neptune). The contributions of all other planets, dwarf planets, etc. are negligible. If the four giant planets were on a straight line on the same side of the Sun, the combined center of mass would lie about 1.17 solar radii or just over 810,000 km above the Sun's surface.\n\nThe calculations above are based on the mean distance between the bodies and yield the mean value \"r\". But all celestial orbits are elliptical, and the distance between the bodies varies between the apses, depending on the eccentricity, \"e\". Hence, the position of the barycenter varies too, and it is possible in some systems for the barycenter to be \"sometimes inside and sometimes outside\" the more massive body. This occurs where:\n\nNote that the Sun–Jupiter system, with \"e\" = 0.0484, just fails to qualify: .\n\nImages are representative (made by hand), not simulated.\n\nIn classical mechanics, this definition simplifies calculations and introduces no known problems. In general relativity, problems arise because, while it is possible, within reasonable approximations, to define the barycenter, the associated coordinate system does not fully reflect the inequality of clock rates at different locations. Brumberg explains how to set up barycentric coordinates in general relativity.\n\nThe coordinate systems involve a world-time, i.e. a global time coordinate that could be set up by telemetry. Individual clocks of similar construction will not agree with this standard, because they are subject to differing gravitational potentials or move at various velocities, so the world-time must be slaved to some ideal clock that is assumed to be very far from the whole self-gravitating system. This time standard is called Barycentric Coordinate Time, or TCB.\n\nBarycentric osculating orbital elements for some objects in the Solar System:\n\nFor objects at such high eccentricity, the Sun's barycentric coordinates are more stable than heliocentric coordinates.\n"}
{"id": "589225", "url": "https://en.wikipedia.org/wiki?curid=589225", "title": "Bimetallic strip", "text": "Bimetallic strip\n\nA bimetallic strip is used to convert a temperature change into mechanical displacement. The strip consists of two strips of different metals which expand at different rates as they are heated, usually steel and copper, or in some cases steel and brass. The strips are joined together throughout their length by riveting, brazing or welding. The different expansions force the flat strip to bend one way if heated, and in the opposite direction if cooled below its initial temperature. The metal with the higher coefficient of thermal expansion is on the outer side of the curve when the strip is heated and on the inner side when cooled.\n\nThe sideways displacement of the strip is much larger than the small lengthways expansion in either of the two metals. This effect is used in a range of mechanical and electrical devices. In some applications the bimetal strip is used in the flat form. In others, it is wrapped into a coil for compactness. The greater length of the coiled version gives improved sensitivity.\n\nThe earliest surviving bimetallic strip was made by the eighteenth-century clockmaker John Harrison who is generally credited with its invention. He made it for his third marine chronometer (H3) of 1759 to compensate for temperature-induced changes in the balance spring. It should not be confused with the bimetallic mechanism for correcting for thermal expansion in his gridiron pendulum. His earliest examples had two individual metal strips joined by rivets but he also invented the later technique of directly fusing molten brass onto a steel substrate. A strip of this type was fitted to his last timekeeper, H5. Harrison's invention is recognized in the memorial to him in Westminster Abbey, England.\n\nMechanical clock mechanisms are sensitive to temperature changes as each part has tiny tolerance and it leads to errors in time keeping. A bimetallic strip is used to compensate this phenomenon in the mechanism of some timepieces. The most common method is to use a bimetallic construction for the circular rim of the balance wheel. What it does is move a weight in a radial way looking at the circular plane down by the balance wheel, varying then, the momentum of inertia of the balance wheel. As the spring controlling the balance becomes weaker with the increasing temperature, the balance becomes smaller in diameter to decrease the momentum of inertia and keep the period of oscillation (and hence timekeeping) constant.\n\nNowadays this system is not used anymore since the appearance of low temperature coefficient alloys like nivarox, parachrom and many others depending on each brand.\n\nIn the regulation of heating and cooling, thermostats that operate over a wide range of temperatures are used. In these, one end of the bimetallic strip is mechanically fixed and attached to an electrical power source, while the other (moving) end carries an electrical contact. In adjustable thermostats another contact is positioned with a regulating knob or lever. The position so set controls the regulated temperature, called the \"set point\".\n\nSome thermostats use a mercury switch connected to both electrical leads. The angle of the entire mechanism is adjustable to control the set point of the thermostat.\n\nDepending upon the application, a higher temperature may open a contact (as in a heater control) or it may close a contact (as in a refrigerator or air conditioner).\n\nThe electrical contacts may control the power directly (as in a household iron) or indirectly, switching electrical power through a relay or the supply of natural gas or fuel oil through an electrically operated valve. In some natural gas heaters the power may be provided with a thermocouple that is heated by a pilot light (a small, continuously burning, flame). In devices without pilot lights for ignition (as in most modern gas clothes dryers and some natural gas heaters and decorative fireplaces) the power for the contacts is provided by reduced household electrical power that operates a relay controlling an electronic ignitor, either a resistance heater or an electrically powered spark generating device.\n\nA direct indicating dial thermometer really common in daily use devices (such as a patio thermometer or a meat thermometer) uses a bimetallic strip wrapped into a coil in its most used design. The coil changes the lineal movement of the metal expansion into a circular movement thanks to the helicoidal shape it draws. One end of the coil is fixed to the housing of the device as a fix point and the other drives an indicating needle inside a circular indicator. A bimetallic strip is also used in a recording thermometer. Breguet's thermometer consists of a tri-metallic helix in order to have a more accurate results.\n\nHeat engines are not the most efficient ones, and with the use of bimetallic strips the efficiency of the heat engines is even lower as there is no chamber to contain the heat. Moreover, the bimetallic strips cannot produce strength in its moves, the reason why is that in order to achieve reasonables bendings (movements) both metallic strips have to be thin to make the difference between the expansion noticeable. So the uses for metallic strips in heat engines are mostly in simple toys that have been built to demonstrate how the principle can be used to drive a heat engine.\n\nBimetal strips are used in miniature circuit breakers to protect circuits from excess current. A coil of wire is used to heat a bimetal strip, which bends and operates a linkage that unlatches a spring-operated contact. This interrupts the circuit and can be reset when the bimetal strip has cooled down.\n\nBimetal strips are also used in time-delay relays, lamp flashers, and fluorescent lamp starters. In some devices the current running directly through the bimetal strip is sufficient to heat it and operate contacts directly.\n\nCurvature of a bimetallic beam:\n\n\n\n"}
{"id": "2199750", "url": "https://en.wikipedia.org/wiki?curid=2199750", "title": "Calymmian", "text": "Calymmian\n\nThe Calymmian Period (from Greek κάλυμμα \"(kálymma)\", meaning \"cover\") is the first geologic period in the Mesoproterozoic Era and lasted from Mya to Mya (million years ago). Instead of being based on stratigraphy, these dates are defined chronometrically.\n\nThe period is characterised by expansion of existing platform covers, or by new platforms on recently cratonized basements.\n\nThe supercontinent Columbia broke up during the Calymmian some 1500 Mya.\n\n\n"}
{"id": "810090", "url": "https://en.wikipedia.org/wiki?curid=810090", "title": "Chota Nagpur Plateau", "text": "Chota Nagpur Plateau\n\nThe Chhota Nagpur Plateau is a plateau in eastern India, which covers much of Jharkhand state as well as adjacent parts of Odisha, West Bengal, Bihar and Chhattisgarh. The Indo-Gangetic plain lies to the north and east of the plateau, and the basin of the Mahanadi River lies to the south. The total area of the Chota Nagpur Plateau is approximately .\n\nThe name \"Nagpur\" is probably taken from Nagavanshis, who ruled in this part of the country. \"Chhota\" (=\"small\" in Hindi and Urdu) is also the name of a village in the outskirts of Ranchi, which has the remains of an old fort belonging to the Nagavanshis.\n\nThe Chhota Nagpur Plateau is a continental plateau—an extensive area of land thrust above the general land. The plateau has been formed by continental uplift from forces acting deep inside the earth. The Gondwana substrates attest to the plateau's ancient origin. It is part of the Deccan Plate, which broke free from the southern continent during the Cretaceous to embark on a 50-million-year journey that was violently interrupted by the northern Eurasian continent. The northeastern part of the Deccan Plateau, where this ecoregion sits, was the first area of contact with Eurasia.\n\nThe Chhota Nagpur Plateau consists of three steps. The highest step is in the western part of the plateau, where \"pats\", as a plateau is locally called, are above sea level. The highest point is . The next part contains larger portions of the old Ranchi and Hazaribagh districts and some parts of old Palamu district, before these were broken up into smaller administrative units. The general height is . The topography in undulating with prominent gneissic hills, often dome-like in outline. The lowest step of the plateau is at an average level of around . It covers the old Manbhum and Singhbhum districts. High hills are a striking part of this section - Parasnath Hills rise to a height of and Dalma Hills to . The large plateau is subdivided into several small plateaux or sub plateaux.\n\nThe western plateau with an average elevation of above mean sea level merges into the plateau of Surguja district of Chhattisgarh. The flat topped plateaux, locally known as \"pats\" are characterized by level surface and accordance of their summit levels shows they are part of one large plateau. Examples include Netarhat Pat, Jamira Pat, Khamar Pat, Rudni Pat etc. The area is also referred to as Western Ranchi Plateau. It is believed to be composed of Deccan lava.\n\nIt is the largest part of the Chhota Nagpur Plateau. The elevation of the plateau land in this part is about and gradually slopes down towards south-east into the hilly and undulating region of Singhbhum (earlier Singhbhum district or what is now Kolhan division). The plateau is highly dissected. Damodar River originates here and flows through a rift valley. To the north it is separated from the Hazaribagh plateau by the Damodar trough. To the west is a group of plateaux called \"pat\"\n\nThere are many waterfalls at the edges of Ranchi plateau where rivers coming from over the plateau surface form waterfalls when they descend through the precipitous escarpments of the plateau and enter the area of significantly lower height. The North Karo River has formed a high Pheruaghaugh Falls at the southern margin of Ranchi plateau. Such falls are called scarp falls. Hundru Falls (75 m) on Subarnarekha River near Ranchi, Dassam Falls (39.62 m) on Kanchi River, east of Ranchi, Sadni Falls (60 m) on Sankh River (Ranchi plateau) are examples of scarp falls. Sometimes waterfalls of various dimensions are formed when tributary streams join the master stream from great height forming hanging valleys. At Rajrappa (10 m), the Bhera river coming over from the Ranchi plateau hangs above the Damodar River at its point of confluence with the latter. The Jonha Falls (25.9 m) is another example of this category of falls. In fact the Gunga River hangs over its master stream, Raru River (to the east of Ranchi city) and forms the said falls.\n\nThe Hazaribagh plateau is often subdivided into two parts – the \"higher plateau\" and the \"lower plateau\". Here the higher plateau is referred to as Hazaribagh plateau and the lower plateau as Koderma plateau. The Hazaribagh plateau on which Hazaribagh town is built is about east by west and north by south with an average elevation of . The north-eastern and southern faces are mostly abrupt; but to the west it narrows and descends slowly in the neighbourhood.of Simaria and Jabra where it curves to the south and connects with the Ranchi Plateau through Tori pargana. It is generally separated from the Ranchi plateau by the Damodar trough.\n\nThe western portion of Hazaribagh plateau constitutes a broad watershed between the Damodar drainage on the south and the Lilajan and Mohana rivers on the north. The highest hills in this area are called after the villages of Kasiatu, Hesatu and Hudu, and rise fronting the south above the general level of the plateau. Further east along the southern face a long spur projects right up to the Damodar river where it ends in Aswa Pahar, elevation . At the south-eastern corner of the plateau is Jilinga Hill at . Mahabar Jarimo at and Barsot at stand in isolation to the east, and on the north-west edge of the plateau Sendraili at and Mahuda at are the most prominent features. Isolated on the plateau, in the neighbourhood of Hazaribagh town are four hills of which the highest Chendwar rises to . On all sides it has an exceedingly abrupt scarp, modified only on the south-east. In the south it falls almost sheer in a swoop of to the bed of Bokaro River, below Jilinga Hill. Seen from the north the edge of this plateau has the appearance of a range of hills, at the foot of which (on the Koderma plateau) runs the Grand Trunk Road and NH 2 (new NH19).\n\nThe Koderma plateau is also referred to as the Hazaribagh lower plateau or as the Chauparan-Koderma-Girighi sub-plateau.\n\nThe northern face of the Koderma plateau, elevated above the plains of Bihar, has the appearance of a range of hills, but in reality it is the edge of a plateau, from the level of the Gaya plain. Eastward this northern edge forms a well-defined watershed between heads of the tributaries of Gaya and those of the Barakar River, which traverses the Koderma and Giridih districts in an easterly direction. The slope of this plateau to the east is uniform and gentle and is continued past the river, which bears to the south-east, into the Santhal Parganas and gradually disappears in the lower plains of Bengal. The western boundary of the plateau is formed by the deep bed of the Lilajan River.The southern boundary consists of the face of the higher plateau, as far as its eastern extremity, where for some distance a low and undistinguished watershed runs eastward to the western spurs of Parasnath Hills. The drainage to the south of this low line passes by the Jamunia River to the Damodar.\n\nThe Damodar basin forms a trough between the Ranchi and Hazaribagh plateaux resulting from enormous fractures at their present edges, which caused the land between to sink to a great depth and incidentally preserved from denudation the Karanpura, Ramgarh and Bokaro coalfields. The northern boundary of the Damodar valley is steep as far as the south eastern corner of the Hazaribagh plateau. On the south of the trough the Damodar keeps close to the edge of the Ranchi plateau till it has passed Ramgarh, after which a turn to the north-east leaves on the right hand a wide and level valley on which the Subarnarekha begins to intrude, south of Gola till the Singhpur Hills divert it to the south. Further to the east the Damodar River passes tamely into the Manbhum sector of lowest step of the Chotanagpur plateau.\n\nThe Palamu division generally lies at a lower height than the surrounding areas of Chota Nagpur Plateau. On the east the Ranchi plateau intrudes into the division and the southern part of the division merges with the Pat region. On the west are the Surguja highlands of Chhattishgarh and Sonbhadra district of Uttar Pradesh. The Son River touches the north-western corner of the division and then forms the state boundary for about . The general system of the area is a series of parallel ranges of hills running east and west through which the North Koel River passes. The hills in the south are the highest in the area, and the picturesque and isolated cup-like Chhechhari valley is surrounded by lofty hills on every side. Lodh Falls drops from a height of from these hills, making it the highest waterfall on the Chota Nagpur Plateau. Netarhat and Pakripat plateaux are physiographically part of the Pat region.\n\nIn the lowest step of the Chota Nagpur Plateau, the Manbhum area covers the present Purulia district in West Bengal, and Dhanbad district and parts of Bokaro district in Jharkhand, and the Singhbhum area broadly covers Kolhan division of Jharkhand. The Manbhum area has a general elevation of about and it consists of undulating land with scattered hills – Baghmundi and Ajodhya range, Panchakot and the hills around Jhalda are the prominent ones. Adjacent Bankura district of West Bengal has been described as the “connecting link between the plains of Bengal on the east and Chota Nagpur plateau on the west.” The same could be said of the Asansol and Durgapur subdivisions of Bardhaman district.Recently a website launched for chotonagpur Manbhum-Singhbhum region specially with the mission to save wildlife and conserve the forest in plateau. Visit chotonagpurplateau.in for a detail information and articles.\n\nThe Singhbhum area contains much more hilly and broken country. The whole of the western part is a mass of hill ranges rising to in the south-west. Jamshedpur sits on an open plateau, above mean sea level, with a higher plateau to the south of it. The eastern part is mostly hilly, though near the borders of West Bengal it flattens out into an alluvial plain. In the Singhbhum area, there are hills alternating with valleys, steep mountains, deep forests on the mountain slopes, and, in the river basins, some stretches of comparatively level or undulating country. The centre of the area consists of an upland plateau enclosed by hill ranges. This strip, extending from the Subarnarekha River on the east to the Angarbira range to the west of Chaibasa, is a very fertile area. Saranda forest is reputed to have the best Sal forests in Asia.\n\nThe Chhota Nagpur Plateau has an attractive climate. For five to six months of the year, from October onward the days are sunny and bracing. The mean temperature in December is . The nights are cool and temperatures in winter may drop below freezing point in many places. In April and May the day temperature may cross but it is very dry and not sultry as in the adjacent plains. The rainy season (June to September) is pleasant. The Chota Nagpur Plateau receives an annual average rainfall of around , which is less than the rainforested areas of much of India and almost all of it in the monsoon months between June and August.\n\nThe \"Chhota Nagpur dry deciduous forests\", a tropical and subtropical dry broadleaf forests ecoregion, encompasses the plateau. The ecoregion has an area of , covering most of Jharkhand state and adjacent portions of Odisha, West Bengal, Bihar, Chhattisgarh, Uttar Pradesh, and Madhya Pradesh.\n\nThe ecoregion is drier than surrounding ones, including the Eastern Highlands moist deciduous forests that covers the Eastern Ghats and Satpura Range to the south, and the Lower Gangetic Plains moist deciduous forests in the lowlands to the east and north.\n\nThe plateau is covered with a variety of various habitats of which Sal forest is predominant. The plateau is home to the Palamau Tiger Reserve and other large blocks of natural habitat which are among the few remaining refuges left in India for large populations of tiger and Asian elephants.\n\nForests range from dry to wet and reach up to tall. The plateau is also swampy in some places and in other parts is covered with bamboo grasslands and shrubs such as \"Holarrhena\" and \"Dodonaea\". The flora of the plateau is distinct from the wetter parts of India that surround it and includes a number of endemic plants such as \"Aglaia haslettiana\" and endangered plant species including \"Madhuca longifolia\" and \"Butea monosperma.\"\n\nTigers, Asian elephants, four-horned antelope (\"Tetracerus quadricornis\"), blackbuck (\"Antilope cervicapra\"), chinkara (\"Gazella bennettii\"), dhole wild dog (\"Cuon alpinus\") and sloth bear (\"Melursus ursinus\") are some of the animals found here while birds include the threatened lesser florican (\"Eupodotis indica\"), Indian grey hornbill and other hornbills.\n\nMore than half of the natural forest on the plateau has been cleared for grazing land and the scale of the mining operations on the plateau is disturbing to the movement and therefore the survival of wildlife including elephants and tigers.\n\nAbout 6 percent of the ecoregion's area is within protected areas, comprising in 1997. The largest are Palamau Tiger Reserve and Sanjay National Park.\n\n\nChhota Nagpur plateau is a store house of minerals like mica, bauxite, copper, limestone, iron ore and coal. The Damodar valley is rich in coal and it is considered as the prime centre of coking coal in the country. Massive coal deposits are found in the central basin spreading over . The important coalfields in the basin are Jharia, Raniganj, West Bokaro, East Bokaro, Ramgarh, South Karanpura and North Karanpura.\n\n\n\n"}
{"id": "13506609", "url": "https://en.wikipedia.org/wiki?curid=13506609", "title": "Climatotherapy", "text": "Climatotherapy\n\nClimatotherapy refers to temporary or permanent relocation of a patient to a region with a climate more favourable to recovery from or management of a condition. Examples include:\n"}
{"id": "11063189", "url": "https://en.wikipedia.org/wiki?curid=11063189", "title": "Cooperative Institute for Meteorological Satellite Studies", "text": "Cooperative Institute for Meteorological Satellite Studies\n\nThe Cooperative Institute for Meteorological Satellite Studies (CIMSS) is a research institute where scientists study the use of data from geostationary and polar orbit weather satellites to improve forecasts of weather (including tropical cyclones and severe storms. CIMSS was formed through a Memorandum of Understanding between the University of Wisconsin–Madison, the National Oceanic and Atmospheric Administration (NOAA) and the National Aeronautics and Space Administration (NASA). CIMSS parent organization, the Space Science and Engineering Center (SSEC) is a primary developer and operator of environmental satellite technologies.\n\nIt is one of 16 NOAA Cooperative Institutes (CIs).\n\nCIMSS develops and successfully implements techniques and products for using geostationary and polar-orbiting weather satellite visible and thermal radiation observations to improve forecasts of severe storms, including tornadoes and tropical cyclones. CIMSS plays a major role in the transfer of new technology into operational practice. CIMSS scientists conduct research using passive remote sensing systems for meteorological and surface-based applications.\n\nCIMSS also plays a major role in instrument design and testing, and related software development for improved space-based measurements of the Earth's atmosphere. CIMSS is very active in national and international field programs, testing new instrumentation, data processing systems, and assessing the geophysical utility of measurements.\n\nCurrent research also focuses on the development and testing of computer-based analysis and forecast techniques that use observations from existing and planned spacecraft and ground-based weather observing systems as part of a national program to greatly improve weather forecast capabilities for the next decade. The optimal use of satellite data in climate and global change studies has become another essential part of the CIMSS mission.\n\nCIMSS serves as an international center for research on the interpretation and uses of operational and experimental satellite observations and remote sensing data acquired from aircraft and the ground. These data are applied to a wide variety of atmospheric and oceanographic studies and evaluated for their potential operational utility. The CIMSS international role is further strengthened through its visiting scientist program that hosts sabbaticals for several foreign scholars each year.\n\nThe CIMSS staff consists of over 100 associates, including administrative staff, principal investigators, federal employees stationed at CIMSS, scientific and programming staff, visiting scientists, and student hourly support. The federal employees consist of the 7 members of the NOAA Advanced Satellite Products Branch, 1 NOAA Office of Research and Applications employee, 1 NOAA Severe Storms Lab employee, and 1 NASA Langley Research Center employee.\n\n"}
{"id": "38812516", "url": "https://en.wikipedia.org/wiki?curid=38812516", "title": "Cruiser rules", "text": "Cruiser rules\n\nCruiser rules is a colloquial phrase referring to the conventions regarding the attacking of a merchant ship by an armed vessel. Here \"cruiser\" is meant in its original meaning of a ship sent on an independent mission such as commerce raiding. A cruiser in modern naval terminology refers to a type of ship rather than its mission. Cruiser rules govern when it is permissible to open fire on an unarmed ship and the treatment of the crews of captured vessels. During both world wars the question was raised of whether or not submarines were subject to cruiser rules. Initially submarines attempted to obey them, but abandoned them as the war progressed.\n\nThe essence of cruiser rules is that an unarmed vessel should not be attacked without warning. It can be fired on only if it repeatedly fails to stop when ordered to do so or resists being boarded by the attacking ship. The armed ship may only intend to search for contraband (such as war materials) when stopping a merchantman. If so, the ship may be allowed on its way, as it must be if it is flying the flag of a non-belligerent, after removal of any contraband. However, if it is intended to take the captured ship as a prize of war, or to destroy it, then adequate steps must be taken to ensure the safety of the crew. This would usually mean taking the crew on board and transporting them to a safe port. It is not usually acceptable to leave the crew in lifeboats. This can only be done if they can be expected to reach safety by themselves and have sufficient supplies and navigational equipment to do so.\n\nThe cruiser rules evolved during the 17th century when the issuing of letters of marque to privateers was at its peak. They were initially an understanding of the honourable way to behave rather than formal international agreements. A formal agreement between Great Britain and France at the end of the Crimean War was extended internationally at the Paris Declaration Respecting Maritime Law in 1856. It was signed by all maritime nations except the United States and Spain.\n\nA new international agreement was reached in 1909, the London Declaration concerning the Laws of Naval War. The participants in this treaty were the main European powers, the United States, and the Empire of Japan. Article 50 of this treaty was what was meant by \"cruiser rules\" during World War I. Initially, the treaty was respected. The first British merchant ship to be sunk by a German submarine was the SS \"Glitra\" in October 1914. The submarine, SM \"U-17\", allowed the \"Glitra's\" crew to board lifeboats first and then towed them to shore. Abiding by the cruiser rules in this way was particularly problematic for submarines. They did not have the room to take captured crew on board and towing lifeboats in this way prevented the submarine from diving. This put the submarine at considerable risk.\n\nGerman submarines were further endangered by the British anti-submarine Q-ships. These were merchant ships armed with hidden weapons. The idea was to tempt a submarine to surface and confront the Q-ship, then reveal the guns and open fire. In German eyes, this meant that all British ships were potentially a danger and they started to move away from the cruiser rules. At the beginning of 1915 Germany declared a war zone around the British Isles in retaliation for the British blockade of Germany. Henceforth, all neutral shipping within the declared zone was liable to attack without warning. This led to a series of notorious attacks on passenger ships with the loss of civilian lives, some of them American. These included RMS \"Lusitania\" in May 1915, SS \"Arabic\" in August 1915, and SS \"Sussex\" in March 1916. Fearing that American deaths would lead to the US entering the war, after each of these incidents Germany made a new pledge not to sink merchant ships until they had witnessed that life boats had been launched. However, these pledges were never honoured for long, if at all, and finally Germany announced unrestricted submarine warfare in February 1917. Germany believed that this strategy would win the war for them, but in reality it contributed to their defeat by causing the US to enter the war on the side of the Allies.\n\nDespite the experience of World War I, Britain initially expected that German submarines would fight under the cruiser rules in World War II. However, in September 1939 German submarine \"U-30\" sunk the British passenger liner SS \"Athenia\" apparently mistaking it for a military ship. Admiral Dönitz pressed for unrestricted submarine warfare. However, Dönitz was starved of resources until after the Battle of Britain in 1940 when it became clear that Britain could not be successfully invaded. After this, submarine attacks on British merchant shipping commenced in force in the Battle of the Atlantic.\n\n"}
{"id": "8587112", "url": "https://en.wikipedia.org/wiki?curid=8587112", "title": "David Lary", "text": "David Lary\n\nDavid Lary (born 7 December 1965) is an atmospheric scientist interested in applying computational and information systems to facilitate discovery and decision support in Earth system science. His main contributions have been to highlight the role of carbonaceous aerosols in atmospheric chemistry, heterogeneous bromine reactions, and to employ chemical data assimilation for satellite validation, and the use of machine learning for remote sensing applications. He is author of AutoChem, NASA release software that constitutes an automatic computer code generator and documentor for chemically reactive systems. It was designed primarily for modeling atmospheric chemistry, and in particular, for chemical data assimilation. He is author of more than 150 publications receiving more than 3,850 citations.\n\nAutoChem has won five NASA awards and has been used to perform long term chemical data assimilation of atmospheric chemistry and in the validation of observations from the NASA Aura satellite. It has been used in numerous peer reviewed articles.\nDavid Lary completed his education in the United Kingdom. He received a first class double honors BSc in physics and chemistry from King's College London (1987) with the Sambrooke Exhibition Prize in Natural Science, and a PhD in atmospheric chemistry from the University of Cambridge, Department of Chemistry while at Churchill College (1991). His thesis described the first chemical scheme for the ECMWF numerical weather prediction model. He then held post-doctoral research assistant and associate positions at the University of Cambridge until receiving a Royal Society research fellowship in 1996 (also at Cambridge). From 1998 to 2000 he held a joint position at Cambridge and the University of Tel-Aviv as a senior lecturer and Alon fellow. In 2001 he joined NASA/UMBC/GEST as the first distinguished Goddard fellow in earth science. Between 2001 and 2010 he was part of various branches at NASA Goddard Space Flight Center including the Global Modeling and Assimilation Office, the Atmospheric Chemistry and Dynamics Branch, the Software Integration and Visualization Office, and the Goddard Earth Sciences (GES) Data and Information Services Center (DISC).\n\nIn 2010 he moved to the William B. Hanson Center for Space Sciences as a professor of physics at the University of Texas at Dallas, where he has focused on the health effects of atmospheric particulates, and developing a fleet of unmanned aerial vehicles for a variety of agricultural, environmental, and meteorological applications. He is also adjunct Professor in Data Science and Machine Learning at Southern Methodist University, adjunct Professor at Baylor University Center for Astrophysics, Space Physics & Engineering Research, a\nScholar of the Institute for Integrative Health, adjunct Professor at the School of Public Health, University of North Texas Health Science Center, and the departments of Electrical Engineering and Geographic Information Systems at the University of Texas at Dallas.\n\n"}
{"id": "21018941", "url": "https://en.wikipedia.org/wiki?curid=21018941", "title": "Dirty thunderstorm", "text": "Dirty thunderstorm\n\nA dirty thunderstorm (also volcanic lightning, thunder volcano) is a weather phenomenon that is related to the production of lightning in a volcanic plume.\n\nThe earliest recorded observations of a dirty thunderstorm are from Pliny the Younger, describing the eruption of Mount Vesuvius in 79 AD. “There was a most intense darkness rendered more appalling by the fitful gleam of torches at intervals obscured by the transient blaze of lightning.” The first studies of volcanic lightning were also conducted at Mount Vesuvius by Professor Palmieri who observed the eruptions of 1858, 1861, 1868, and 1872 from the Vesuvius Observatory. These eruptions often included lightning activity.\n\nDirty thunderstorms earn their name from the ash, rock fragments, and other ejecta which collide during a volcanic eruption and generate static electricity within the volcanic plume. A study presented in the Bulletin of Volcanology stated that “27-35% of eruptions are accompanied by lightning, assuming one eruption per year per volcano.” This study also indicated that volcanic lightning has been observed with 212 eruptions from 80 different volcanoes.\n\nA famous image of the phenomenon was photographed by Carlos Gutierrez and occurred in Chile above the Chaiten Volcano. It circulated widely on the internet. Another notable image of this phenomenon is \"The Power of Nature\", taken by Mexican photographer Sergio Tapiro in Colima, Mexico, which won third place (Nature category) in the 2016 World Press Photo Contest. Other instances have been reported above Alaska's Mount Augustine volcano, Iceland's Eyjafjallajökull volcano and Mount Etna in Sicily, Italy.\n\nFrictional Charging\n\nTriboelectric charging from fragmentation of rocks near the vent and within the plume of a volcano during eruption offers another mechanism for electrical charging.\n\nA study in the journal \"Science\" indicated that electrical charges are generated when rock fragments, ash, and ice particles in a volcanic plume collide and produce static charges, just as ice particles collide in regular thunderstorms.\n\nVolcanic eruptions are sometimes accompanied by flashes of lightning. However, this lightning doesn’t descend from storm clouds in the sky. It is generated within the ash cloud spewing from the volcano, in a process called charge separation.\n\nAs the plume started going downwind, it seemed to have a life of its own and produced some 300 more or less normal [lightning bolts] ... The implication is that it has produced more charge than it started with. Otherwise [the plume] couldn't continue to make lightning.\n\n—Martin A. Uman, co-director of the University of Florida Lightning Research program\n\nWater Content\n\nLarge amounts of water are released as vapor during volcanic eruptions. A study performed by McNutt & Williams in 2010 found that the water content of volcanic plumes is much greater than the water content of thunderstorms. This study also found “there may be a threshold of water substance concentration required in a plume for lightning to occur (…) this appears to be a function of the large amount of water in the magma and not the smaller amount in the entrained air”.\n\nRadioactive Charging\n\nNaturally occurring radioisotopes within ejected rock particles may cause self-charging of volcanic plumes. During an eruption, a large amount of fragmented sub-surface rock is ejected into the atmosphere. In a study performed on ash particles from the Eyjafjallajökull and Grímsvötn eruptions, scientists found that radioisotopes were an unlikely source of self-charging in the Eyjafjallajökull plume. However, there was the potential for greater charging near the vent where the particle size is larger. A second study conducted on volcanic ash from the same eruptions found that both samples possessed a natural radioactivity above the background level.\n\nPlume Height\n\nThe height of the ash plume appears to be linked with the mechanism which generates the lightning. In taller ash plumes (7–12 km) it appears that the large concentrations of water vapor are contributing to lightning activity, while smaller ash plumes (1–4 km) appear to gain more of their electric charge from fragmentation of rocks near the vent of the volcano through charge separation. The atmospheric temperature also plays a role in the formation of lightning. Colder ambient temperatures will create greater amounts of ice inside of the plume thus leading to more friction and electrical activity.\n\nSeasonal Effects\n\nIn a study presented in the Bulletin of Volcanology in 2010, it was stated that “Seasonal effects show that more eruptions with lightning were reported in winter (bounded by the respective autumnal and vernal equinoxes) than in summer”. This finding indicates that the lightning activity is driven by magma-derived water rather than atmospheric.\n\nIt has been suggested that a by-product of volcanic lighting are lightning induced volcanic spherules, (LIVS). These small glass spherules are thought to be formed during high-temperatures processes such as cloud-to-ground lightning strikes. The temperature of a bolt of volcanic lightning can reach 30,000 °C. When this bolt contacts ash particles within the plume it will cause them to melt and then quickly solidify as they cool, forming orb shapes. The presence of volcanic spherules aids in providing evidence for the occurrence of volcanic lightning when the lightning itself was not observed directly.\n"}
{"id": "33548009", "url": "https://en.wikipedia.org/wiki?curid=33548009", "title": "Energy in Sudan", "text": "Energy in Sudan\n\nEnergy in Sudan describes energy and electricity production, consumption and imports in Sudan. Sudan is a net energy exporter. Primary energy use in Sudan was 179 kWh and 4 kWh per million persons in 2008.\n\nThe world share of energy production in Africa was 12 percent of oil and 7 percent of gas in 2009. In 2010, major energy producers in Africa were Algeria, Angola, Cameroon, Democratic Republic of the Congo, Equatorial Guinea, Gabon, Libya, Nigeria and Sudan.\n\nAccording to the OECD and the World Bank, the population growth of Sudan from 2004 to 2008 was 16.4 percent (in comparison to the world average of 5.3 percent, India at 5.6 percent and Nigeria at 17.6 percent).\n\nAccording to Transparency International, companies operating in Sudan (based on reporting in 2009) included four national companies: China National Petroleum Corporation (CNPC), Kuwait Petroleum Corporation (KPC), the Oil and Natural Gas Corporation (ONGC, the Indian NOC) and Petronas (\"Petroliam Nasional Berhad\"). Transparency International audited these companies' corporate reporting on anti-corruption programmes, organisational disclosure and country-level global disclosure (including in Sudan) until February 2011 to enhance the transparency and accountability of oil and gas revenues. The report \"Why transparency in the oil and gas sector matters\" states: \"Oil and gas producers transfer considerable funds to host governments – in the form of license fees, royalties, dividends, taxes and support for local communities. These financial inflows should contribute substantially to social and economic development, yet many resource-rich countries have been unable to transform resource wealth into wellbeing\". All African companies scored zero in reporting on anti-corruption programmes. In general, country-level disclosure in Sudan demonstrated opportunities for improvement.\n\n"}
{"id": "50482", "url": "https://en.wikipedia.org/wiki?curid=50482", "title": "Flood", "text": "Flood\n\nA flood is an overflow of water that submerges land that is usually dry. The European Union (EU) Floods Directive defines a flood as a covering by water of land not normally covered by water. In the sense of \"flowing water\", the word may also be applied to the inflow of the tide. Floods are an area of study of the discipline hydrology and are of significant concern in agriculture, civil engineering and public health. \n\nFlooding may occur as an overflow of water from water bodies, such as a river, lake, or ocean, in which the water overtops or breaks levees, resulting in some of that water escaping its usual boundaries, or it may occur due to an accumulation of rainwater on saturated ground in an areal flood. While the size of a lake or other body of water will vary with seasonal changes in precipitation and snow melt, these changes in size are unlikely to be considered significant unless they flood property or drown domestic animals.\n\nFloods can also occur in rivers when the flow rate exceeds the capacity of the river channel, particularly at bends or meanders in the waterway. Floods often cause damage to homes and businesses if they are in the natural flood plains of rivers. While riverine flood damage can be eliminated by moving away from rivers and other bodies of water, people have traditionally lived and worked by rivers because the land is usually flat and fertile and because rivers provide easy travel and access to commerce and industry.\n\nSome floods develop slowly, while others such as flash floods, can develop in just a few minutes and without visible signs of rain. Additionally, floods can be local, impacting a neighborhood or community, or very large, affecting entire river basins.\n\nThe word \"flood\" comes from the Old English \"flod\", a word common to Germanic languages (compare German \"Flut\", Dutch \"vloed\" from the same root as is seen in \"flow, float\"; also compare with Latin \"fluctus\", \"flumen\"). Deluge myths are mythical stories of a great flood sent by a deity or deities to destroy civilization as an act of divine retribution, and they are featured in the mythology of many cultures.\n\nFloods can happen on flat or low-lying areas when water is supplied by rainfall or snowmelt more rapidly than it can either infiltrate or run off. The excess accumulates in place, sometimes to hazardous depths. Surface soil can become saturated, which effectively stops infiltration, where the water table is shallow, such as a floodplain, or from intense rain from one or a series of storms. Infiltration also is slow to negligible through frozen ground, rock, concrete, paving, or roofs. Areal flooding begins in flat areas like floodplains and in local depressions not connected to a stream channel, because the velocity of overland flow depends on the surface slope. Endorheic basins may experience areal flooding during periods when precipitation exceeds evaporation.\n\nFloods occur in all types of river and stream channels, from the smallest ephemeral streams in humid zones to normally-dry channels in arid climates to the world's largest rivers. When overland flow occurs on tilled fields, it can result in a muddy flood where sediments are picked up by run off and carried as suspended matter or bed load. Localized flooding may be caused or exacerbated by drainage obstructions such as landslides, ice, debris, or beaver dams.\n\nSlow-rising floods most commonly occur in large rivers with large catchment areas. The increase in flow may be the result of sustained rainfall, rapid snow melt, monsoons, or tropical cyclones. However, large rivers may have rapid flooding events in areas with dry climate, since they may have large basins but small river channels and rainfall can be very intense in smaller areas of those basins.\n\nRapid flooding events, including flash floods, more often occur on smaller rivers, rivers with steep valleys, rivers that flow for much of their length over impermeable terrain, or normally-dry channels. The cause may be localized convective precipitation (intense thunderstorms) or sudden release from an upstream impoundment created behind a dam, landslide, or glacier. In one instance, a flash flood killed eight people enjoying the water on a Sunday afternoon at a popular waterfall in a narrow canyon. Without any observed rainfall, the flow rate increased from about in just one minute. Two larger floods occurred at the same site within a week, but no one was at the waterfall on those days. The deadly flood resulted from a thunderstorm over part of the drainage basin, where steep, bare rock slopes are common and the thin soil was already saturated.\n\nFlash floods are the most common flood type in normally-dry channels in arid zones, known as arroyos in the southwest United States and many other names elsewhere. In that setting, the first flood water to arrive is depleted as it wets the sandy stream bed. The leading edge of the flood thus advances more slowly than later and higher flows. As a result, the rising limb of the hydrograph becomes ever quicker as the flood moves downstream, until the flow rate is so great that the depletion by wetting soil becomes insignificant.\n\nFlooding in estuaries is commonly caused by a combination of sea tidal surges caused by winds and low barometric pressure, and they may be exacerbated by high upstream river flow.\n\nCoastal areas may be flooded by storm events at sea, resulting in waves over-topping defenses or in severe cases by tsunami or tropical cyclones. A storm surge, from either a tropical cyclone or an extratropical cyclone, falls within this category. Research from the NHC (National Hurricane Center) explains: \"Storm surge is an abnormal rise of water generated by a storm, over and above the predicted astronomical tides. Storm surge should not be confused with storm tide, which is defined as the water level rise due to the combination of storm surge and the astronomical tide. This rise in water level can cause extreme flooding in coastal areas particularly when storm surge coincides with normal high tide, resulting in storm tides reaching up to 20 feet or more in some cases.\"\n\nUrban flooding is the inundation of land or property in a built environment, particularly in more densely populated areas, caused by rainfall overwhelming the capacity of drainage systems, such as storm sewers. Although sometimes triggered by events such as flash flooding or snowmelt, urban flooding is a condition, characterized by its repetitive and systemic impacts on communities, that can happen regardless of whether or not affected communities are located within designated floodplains or near any body of water. Aside from potential overflow of rivers and lakes, snowmelt, stormwater or water released from damaged water mains may accumulate on property and in public rights-of-way, seep through building walls and floors, or backup into buildings through sewer pipes, toilets and sinks.\n\nIn urban areas, flood effects can be exacerbated by existing paved streets and roads, which increase the speed of flowing water.\n\nThe flood flow in urbanized areas constitutes a hazard to both the population and infrastructure. Some recent catastrophes include the inundations of Nîmes (France) in 1998 and Vaison-la-Romaine (France) in 1992, the flooding of New Orleans (USA) in 2005, and the flooding in Rockhampton, Bundaberg, Brisbane during the 2010–2011 summer in Queensland (Australia). Flood flows in urban environments have been studied relatively recently despite many centuries of flood events. Some recent research has considered the criteria for safe evacuation of individuals in flooded areas.\n\nCatastrophic riverine flooding is usually associated with major infrastructure failures such as the collapse of a dam, but they may also be caused by drainage channel modification from a landslide, earthquake or volcanic eruption. Examples include outburst floods and lahars. Tsunamis can cause catastrophic coastal flooding, most commonly resulting from undersea earthquakes.\n\nThe amount, location, and timing of water reaching a drainage channel from natural precipitation and controlled or uncontrolled reservoir releases determines the flow at downstream locations. Some precipitation evaporates, some slowly percolates through soil, some may be temporarily sequestered as snow or ice, and some may produce rapid runoff from surfaces including rock, pavement, roofs, and saturated or frozen ground. The fraction of incident precipitation promptly reaching a drainage channel has been observed from nil for light rain on dry, level ground to as high as 170 percent for warm rain on accumulated snow.\n\nMost precipitation records are based on a measured depth of water received within a fixed time interval. \"Frequency\" of a precipitation threshold of interest may be determined from the number of measurements exceeding that threshold value within the total time period for which observations are available. Individual data points are converted to \"intensity\" by dividing each measured depth by the period of time between observations. This intensity will be less than the actual peak intensity if the \"duration\" of the rainfall event was less than the fixed time interval for which measurements are reported. Convective precipitation events (thunderstorms) tend to produce shorter duration storm events than orographic precipitation. Duration, intensity, and frequency of rainfall events are important to flood prediction. Short duration precipitation is more significant to flooding within small drainage basins.\n\nThe most important upslope factor in determining flood magnitude is the land area of the watershed upstream of the area of interest. Rainfall intensity is the second most important factor for watersheds of less than approximately . The main channel slope is the second most important factor for larger watersheds. Channel slope and rainfall intensity become the third most important factors for small and large watersheds, respectively.\n\nTime of Concentration is the time required for runoff from the most distant point of the upstream drainage area to reach the point of the drainage channel controlling flooding of the area of interest. The time of concentration defines the critical duration of peak rainfall for the area of interest. The critical duration of intense rainfall might be only a few minutes for roof and parking lot drainage structures, while cumulative rainfall over several days would be critical for river basins.\n\nWater flowing downhill ultimately encounters downstream conditions slowing movement. The final limitation in coastal flooding lands is often the ocean or some coastal flooding bars which form natural lakes. In flooding low lands, elevation changes such as tidal fluctuations are significant determinants of coastal and estuarine flooding. Less predictable events like tsunamis and storm surges may also cause elevation changes in large bodies of water. Elevation of flowing water is controlled by the geometry of the flow channel and, especially, by depth of channel, speed of flow and amount of sediments in it Flow channel restrictions like bridges and canyons tend to control water elevation above the restriction. The actual control point for any given reach of the drainage may change with changing water elevation, so a closer point may control for lower water levels until a more distant point controls at higher water levels.\n\nEffective flood channel geometry may be changed by growth of vegetation, accumulation of ice or debris, or construction of bridges, buildings, or levees within the flood channel.\n\nExtreme flood events often result from coincidence such as unusually intense, warm rainfall melting heavy snow pack, producing channel obstructions from floating ice, and releasing small impoundments like beaver dams. Coincident events may cause extensive flooding to be more frequent than anticipated from simplistic statistical prediction models considering only precipitation runoff flowing within unobstructed drainage channels. Debris modification of channel geometry is common when heavy flows move uprooted woody vegetation and flood-damaged structures and vehicles, including boats and railway equipment. Recent field measurements during the 2010–11 Queensland floods showed that any criterion solely based upon the flow velocity, water depth or specific momentum cannot account for the hazards caused by velocity and water depth fluctuations. These considerations ignore further the risks associated with large debris entrained by the flow motion.\n\nSome researchers have mentioned the storage effect in urban areas with transportation corridors created by cut and fill. Culverted fills may be converted to impoundments if the culverts become blocked by debris, and flow may be diverted along streets. Several studies have looked into the flow patterns and redistribution in streets during storm events and the implication on flood modelling.\n\nThe primary effects of flooding include loss of life and damage to buildings and other structures, including bridges, sewerage systems, roadways, and canals.\n\nFloods also frequently damage power transmission and sometimes power generation, which then has knock-on effects caused by the loss of power. This includes loss of drinking water treatment and water supply, which may result in loss of drinking water or severe water contamination. It may also cause the loss of sewage disposal facilities. Lack of clean water combined with human sewage in the flood waters raises the risk of waterborne diseases, which can include typhoid, giardia, cryptosporidium, cholera and many other diseases depending upon the location of the flood.\n\nDamage to roads and transport infrastructure may make it difficult to mobilize aid to those affected or to provide emergency health treatment.\n\nFlood waters typically inundate farm land, making the land unworkable and preventing crops from being planted or harvested, which can lead to shortages of food both for humans and farm animals. Entire harvests for a country can be lost in extreme flood circumstances. Some tree species may not survive prolonged flooding of their root systems.\n\nEconomic hardship due to a temporary decline in tourism, rebuilding costs, or food shortages leading to price increases is a common after-effect of severe flooding. The impact on those affected may cause psychological damage to those affected, in particular where deaths, serious injuries and loss of property occur.\n\nUrban flooding can cause chronically wet houses, leading to the growth of indoor mold and resulting in adverse health effects, particularly respiratory symptoms. Urban flooding also has significant economic implications for affected neighborhoods. In the United States, industry experts estimate that wet basements can lower property values by 10–25 percent and are cited among the top reasons for not purchasing a home. According to the U.S. Federal Emergency Management Agency (FEMA), almost 40 percent of small businesses never reopen their doors following a flooding disaster. In the United States, insurance is available against flood damage to both homes and businesses.\n\nFloods (in particular more frequent or smaller floods) can also bring many benefits, such as recharging ground water, making soil more fertile and increasing nutrients in some soils. Flood waters provide much needed water resources in arid and semi-arid regions where precipitation can be very unevenly distributed throughout the year and kills pests in the farming land. Freshwater floods particularly play an important role in maintaining ecosystems in river corridors and are a key factor in maintaining floodplain biodiversity. Flooding can spread nutrients to lakes and rivers, which can lead to increased biomass and improved fisheries for a few years.\n\nFor some fish species, an inundated floodplain may form a highly suitable location for spawning with few predators and enhanced levels of nutrients or food. Fish, such as the weather fish, make use of floods in order to reach new habitats. Bird populations may also profit from the boost in food production caused by flooding.\n\nPeriodic flooding was essential to the well-being of ancient communities along the Tigris-Euphrates Rivers, the Nile River, the Indus River, the Ganges and the Yellow River among others. The viability of hydropower, a renewable source of energy, is also higher in flood prone regions.\n\nAt the most basic level, the best defense against floods is to seek higher ground for high-value uses while balancing the foreseeable risks with the benefits of occupying flood hazard zones. Critical community-safety facilities, such as hospitals, emergency-operations centers, and police, fire, and rescue services, should be built in areas least at risk of flooding. Structures, such as bridges, that must unavoidably be in flood hazard areas should be designed to withstand flooding. Areas most at risk for flooding could be put to valuable uses that could be abandoned temporarily as people retreat to safer areas when a flood is imminent.\n\nPlanning for flood safety involves many aspects of analysis and engineering, including:\n\nEach topic presents distinct yet related questions with varying scope and scale in time, space, and the people involved. Attempts to understand and manage the mechanisms at work in floodplains have been made for at least six millennia.\n\nIn the United States, the Association of State Floodplain Managers works to promote education, policies, and activities that mitigate current and future losses, costs, and human suffering caused by flooding and to protect the natural and beneficial functions of floodplains – all without causing adverse impacts. A portfolio of best practice examples for disaster mitigation in the United States is available from the Federal Emergency Management Agency.\n\nIn many countries around the world, waterways prone to floods are often carefully managed. Defenses such as detention basins, levees, bunds, reservoirs, and weirs are used to prevent waterways from overflowing their banks. When these defenses fail, emergency measures such as sandbags or portable inflatable tubes are often used to try to stem flooding. Coastal flooding has been addressed in portions of Europe and the Americas with coastal defenses, such as sea walls, beach nourishment, and barrier islands.\n\nIn the riparian zone near rivers and streams, erosion control measures can be taken to try to slow down or reverse the natural forces that cause many waterways to meander over long periods of time. Flood controls, such as dams, can be built and maintained over time to try to reduce the occurrence and severity of floods as well. In the United States, the U.S. Army Corps of Engineers maintains a network of such flood control dams.\n\nIn areas prone to urban flooding, one solution is the repair and expansion of man-made sewer systems and stormwater infrastructure. Another strategy is to reduce impervious surfaces in streets, parking lots and buildings through natural drainage channels, porous paving, and wetlands (collectively known as green infrastructure or sustainable urban drainage systems (SUDS)). Areas identified as flood-prone can be converted into parks and playgrounds that can tolerate occasional flooding. Ordinances can be adopted to require developers to retain stormwater on site and require buildings to be elevated, protected by floodwalls and levees, or designed to withstand temporary inundation. Property owners can also invest in solutions themselves, such as re-landscaping their property to take the flow of water away from their building and installing rain barrels, sump pumps, and check valves.\n\nA series of annual maximum flow rates in a stream reach can be analyzed statistically to estimate the 100-year flood and floods of other recurrence intervals there. Similar estimates from many sites in a hydrologically similar region can be related to measurable characteristics of each drainage basin to allow indirect estimation of flood recurrence intervals for stream reaches without sufficient data for direct analysis.\n\nPhysical process models of channel reaches are generally well understood and will calculate the depth and area of inundation for given channel conditions and a specified flow rate, such as for use in floodplain mapping and flood insurance. Conversely, given the observed inundation area of a recent flood and the channel conditions, a model can calculate the flow rate. Applied to various potential channel configurations and flow rates, a reach model can contribute to selecting an optimum design for a modified channel. Various reach models are available as of 2015, either 1D models (flood levels measured in the channel) or 2D models (variable flood depths measured across the extent of a floodplain). HEC-RAS, the Hydraulic Engineering Center model, is among the most popular software, if only because it is available free of charge. Other models such as TUFLOW combine 1D and 2D components to derive flood depths across both river channels and the entire floodplain.\n\nPhysical process models of complete drainage basins are even more complex. Although many processes are well understood at a point or for a small area, others are poorly understood at all scales, and process interactions under normal or extreme climatic conditions may be unknown. Basin models typically combine land-surface process components (to estimate how much rainfall or snowmelt reaches a channel) with a series of reach models. For example, a basin model can calculate the runoff hydrograph that might result from a 100-year storm, although the recurrence interval of a storm is rarely equal to that of the associated flood. Basin models are commonly used in flood forecasting and warning, as well as in analysis of the effects of land use change and climate change.\n\nAnticipating floods before they occur allows for precautions to be taken and people to be warned so that they can be prepared in advance for flooding conditions. For example, farmers can remove animals from low-lying areas and utility services can put in place emergency provisions to re-route services if needed. Emergency services can also make provisions to have enough resources available ahead of time to respond to emergencies as they occur. People can evacuate areas to be flooded.\n\nIn order to make the most accurate flood forecasts for waterways, it is best to have a long time-series of historical data that relates stream flows to measured past rainfall events. Coupling this historical information with real-time knowledge about volumetric capacity in catchment areas, such as spare capacity in reservoirs, ground-water levels, and the degree of saturation of area aquifers is also needed in order to make the most acrate flood forecasts.\n\nRadar estimates of rainfall and general weather forecasting techniques are also important components of good flood forecasting. In areas where good quality data is available, the intensity and height of a flood can be predicted with fairly good accuracy and plenty of lead time. The output of a flood forecast is typically a maximum expected water level and the likely time of its arrival at key locations along a waterway, and it also may allow for the computation of the likely statistical return period of a flood. In many developed countries, urban areas at risk of flooding are protected against a 100-year flood – that is a flood that has a probability of around 63% of occurring in any 100-year period of time.\n\nAccording to the U.S. National Weather Service (NWS) Northeast River Forecast Center (RFC) in Taunton, Massachusetts, a rule of thumb for flood forecasting in urban areas is that it takes at least of rainfall in around an hour's time in order to start significant ponding of water on impermeable surfaces. Many NWS RFCs routinely issue Flash Flood Guidance and Headwater Guidance, which indicate the general amount of rainfall that would need to fall in a short period of time in order to cause flash flooding or flooding on larger water basins.\n\nIn the United States, an integrated approach to real-time hydrologic computer modelling utilizes observed data from the U.S. Geological Survey (USGS), various cooperative observing networks, various automated weather sensors, the NOAA National Operational Hydrologic Remote Sensing Center (NOHRSC), various hydroelectric companies, etc. combined with quantitative precipitation forecasts (QPF) of expected rainfall and/or snow melt to generate daily or as-needed hydrologic forecasts. The NWS also cooperates with Environment Canada on hydrologic forecasts that affect both the USA and Canada, like in the area of the Saint Lawrence Seaway.\n\nThe Global Flood Monitoring System, \"GFMS,\" a computer tool which maps flood conditions worldwide, is available online. Users anywhere in the world can use GFMS to determine when floods may occur in their area. GFMS uses precipitation data from NASA's Earth observing satellites and the Global Precipitation Measurement satellite, \"GPM.\" Rainfall data from GPM is combined with a land surface model that incorporates vegetation cover, soil type, and terrain to determine how much water is soaking into the ground, and how much water is flowing into streamflow.\n\nUsers can view statistics for rainfall, streamflow, water depth, and flooding every 3 hours, at each 12 kilometer gridpoint on a global map. Forecasts for these parameters are 5 days into the future. Users can zoom in to see inundation maps (areas estimated to be covered with water) in 1 kilometer resolution.\n\nBelow is a list of the deadliest floods worldwide, showing events with death tolls at or above 100,000 individuals.\nFlood myths (great, civilization-destroying floods) are widespread in many cultures.\n\nFlood events in the form of divine retribution have also been described in religious text. As a prime example, the Genesis flood narrative plays a prominent role in Judaism, Christianity and Islam.\n\n\n"}
{"id": "34713249", "url": "https://en.wikipedia.org/wiki?curid=34713249", "title": "Forensic geophysics", "text": "Forensic geophysics\n\nForensic geophysics is a branch of forensic science and is the study, the search, the localization and the mapping of buried objects or elements beneath the soil or the water, using geophysics tools for legal purposes. There are various geophysical techniques for forensic investigations in which the targets are buried and have different dimensions (from weapons or metallic barrels to human burials and bunkers). Geophysical methods have the potential to aid the search and the recovery of these targets because they can non-destructively and rapidly investigate large areas where a suspect, illegal burial or, in general, a forensic target is hidden in the subsoil. When in the subsurface there is a contrast of physical properties between a target and the material in which it is buried, it is possible to individuate and define precisely the concealing place of the searched target. It is also possible to recognize evidences of human soil occupation or excavation, both recent and older. Forensic geophysics is an evolving technique that is gaining popularity and prestige in law enforcement.\n\nSearched for objects obviously include clandestine graves of murder victims, but also include unmarked burials in graveyards and cemeteries, weapons used in criminal activities and environmental crime illegally dumping material.\n\nThere are various near-surface geophysical techniques that can be utilised to detect a near-surface buried object, which should be site and case-specific. A thorough desk study (including historical maps), utility survey, site reconnaissance and control studies should be undertaken before trial geophysical surveys and then full geophysical surveys are undertaken in phased investigations. Note also other search techniques should be used to first to prioritise suspect areas, for example cadaver dogs or forensic geomorphologists.\n\nFor large-scale buried objects, seismic surveys may be appropriate but these have, at best, 2m vertical resolution so may not be ideal for certain targets, more typically they are used to detect bedrock below the surface (see.\n\nFor relatively quick site surveys, bulk ground electrical conductivity surveys can be collected which identifies areas of disturbance of different ground but these can suffer from a lack of resolution. This recent Black Death investigation in central London shows an example. shows a successful woodland search for a cold case in woodland in New Zealand.\n\nGround-penetrating radar (or GPR) has a typical maximum depth below ground level (bgl) of 10 m, depending upon the antennae frequencies used, typically 50 MHz to 1.2 Gz. The higher the frequency the smaller the object that can be resolved but also penetration depths decrease, so operators need to think carefully when choosing antennae frequencies and, ideally, undertake trial surveys using different antennae over a target at a known depth onsite. GPR is the most popularly used technique in forensic search, but is not suitable in certain soil types and environments, e.g. coastal (i.e. salt-rich) and clay-rich soils (lack of penetration). 2D profiles can be relatively quickly collected and, if time permits, successive profiles can be used to generate 3D datasets which may resolve more subtle targets (see ). Recent studies have used GPR to locate mass graves from the Spanish Civil War in mountainous and urban environments.\n\nElectrical resistivity methods can also detect objects, especially in clay-rich soil which would preclude the use of GPR. There are different equipment configurations, the dipole-dipole (fixed-offset) method is the most common which can traverse across an area, measuring resistivity variations at a set depth (typically 1-2x probe separations) which have been used in forensic searches. More slower methods are putting out many probes and collecting both spatially horizontally and vertically, called Electrical resistivity imaging (ERI). Multiple 2D profiles is termed electrical resistivity tomography (ERT).\n\nMagnetometry can detect buried metal (or indeed fired objects such as bricks or even where surface fires were) using simple total field magnetometers, through to fluxgate gradiometers and high-end alkali vapour gradiometers, depending upon accuracy (and cost) required (see ). Surface magnetic susceptibility has also shown recent promise for forensic search.\n\nWater-based searches are also becoming more common, with specialist marine magnetometers, side-scan sonar and other acoustic methods and even water-penetrating radar methods used to rapidly scan bottoms of ponds, lakes, rivers and near-shore depositional environments.\n\nThere has been recent efforts to undertake research over known buried and below-water surface simulated forensic targets in order to gain an insight into optimum search technique(s) and/or equipment configuration(s). Most commonly, this involved the burial porcine cadavers and long-term monitoring for soilwater, seasonal effects on electrical resistivity surveys, burial in walls and beneath concrete, and Long-Term monitoring in the UK, the US and Latin America. Finally there has been surveys in graveyards over graves of known ages to determine the geophysical responses of multi-geophysical techniques with increasing burial ages\n\n\n"}
{"id": "17528100", "url": "https://en.wikipedia.org/wiki?curid=17528100", "title": "Greasestock", "text": "Greasestock\n\nGreasestock is an American event held yearly in Yorktown Heights, New York. It is one of the largest alternative fuel, renewable energy, and low-energy green vehicle exhibitions in the United States. Exhibitors showcase a variety of alternative energy vehicles, as well as exhibits with a sustainable lifestyle theme. Although it is illegal in New York to power a vehicle with waste vegetable oil (the Federal Clean Air Act of 1990 forbids the use of non-compliant biodiesels such as raw vegetable oil; this is illegal nationwide, not just in New York), authorities in New York have stated they have no problem with the festival and have even participated in it.\n\nThe event was founded in 2003 by individuals who shared an interest in vegetable powered vehicles. It is held on the grounds of Peter Pratt's Inn, a historic landmark. According to Jon Pratt, founder of the event, while the original event hosted only eight people interested in discussing alternatives to gas, each successive incarnation of the Greasestock celebration has drawn more and more visitors from all over the United States who are interested in cheaper gas, cleaner energy, and helping the environment.\n\nThe green technologies demonstrated at the event include vegetable oil powered cars, biodiesel cars, solar powered cars, home heating alternatives, and organic farming exhibits. The event draws exhibitors from all over the United States, and included among the hundred or so vehicles on display is a vegetable powered garbage truck from Mamaroneck, New York, the first of its kind in New York, which saved the municipality around $10,000 per year on fuel costs.\n\nThe \"Veggie Van\", a project of V.O. Tech Inc. of Mahopac, New York and the Westchester County Government, is one of the exhibits at the Greasestock event. The van gets its fuel from a local restaurant, and the food filtered out of the vegetable oil is used as compost at the County's Hanover Farm in Yorktown, New York.\n\n"}
{"id": "1646838", "url": "https://en.wikipedia.org/wiki?curid=1646838", "title": "Grid energy storage", "text": "Grid energy storage\n\nGrid energy storage (also called large-scale energy storage) is a collection of methods used to store electrical energy on a large scale within an electrical power grid. Electrical energy is stored during times when production (especially from intermittent power plants such as renewable electricity sources such as wind power, tidal power, solar power) exceeds consumption, and returned to the grid when production falls below consumption.\n\n, the largest form of grid energy storage is dammed hydroelectricity, with both conventional hydroelectric generation as well as pumped storage. Alternatives include rail energy storage, where rail cars carrying 300 ton weights are moved up or down a 8-mile section of inclined rail track, storing or releasing energy as a result; or disused oil-well potential energy storage, where 100 ton weights are raised or lowered in a 12,000 ft deep decommissioned oil well.\n\nDevelopments in battery storage have enabled commercially viable projects to store energy during peak production and release during peak demand.\n\nAn alternative to grid storage is the use of peaking power plants to fill in demand gaps.\n\nThe stores are used – feeding power to the grids – at times when consumption that cannot be deferred or delayed exceeds production. In this way, electricity production need not be drastically scaled up and down to meet momentary consumption – instead, transmission from the combination of generators plus storage facilities is maintained at a more constant level.\n\nAn alternate and complementary approach to achieve the similar effect as grid energy storage is to use a smart grid communication infrastructure to enable Demand response. These technologies shift electricity consumption and electricity production from one time (when it's not useful) to another (when it's in demand).\n\nAny electrical power grid must match electricity production to consumption, both of which vary drastically over time. Any combination of energy storage and demand response has these advantages:\n\nEnergy derived from solar, tidal and wind sources inherently varies  – the amount of electricity produced varies with time of day, moon phase, season, and random factors such as the weather. Thus, renewables in the absence of storage present special challenges to electric utilities. While hooking up many separate wind sources can reduce the overall variability, solar is reliably not available at night, and tidal power shifts with the moon, so slack tides occur four times a day.\n\nHow much this affects any given utility varies significantly. In a summer peak utility, more solar can generally be absorbed and matched to demand. In winter peak utilities, to a lesser degree, wind correlates to heating demand and can be used to meet that demand. Depending on these factors, beyond about 20–40% of total generation, grid-connected intermittent sources such as solar power and wind turbines tend to require investment in grid interconnections, grid energy storage or demand side management.\n\nIn an electrical grid without energy storage, generation that relies on energy stored within fuels (coal, biomass, natural gas, nuclear) must be scaled up and down to match the rise and fall of electrical production from intermittent sources (see load following power plant). While hydroelectric and natural gas plants can be quickly scaled up or down to follow the wind, coal and nuclear plants take considerable time to respond to load. Utilities with less natural gas or hydroelectric generation are thus more reliant on demand management, grid interconnections or costly pumped storage.\n\nThe French consulting firm Yole Développement estimates the “stationary storage” market could be a $13.5 billion opportunity by 2023, compared with less than $1 billion in 2015.\n\nThe demand side can also store electricity from the grid, for example charging a battery electric vehicle stores energy for a vehicle and storage heaters, district heating storage or ice storage provide thermal storage for buildings. At present this storage serves only to shift consumption to the off-peak time of day, no electricity is returned to the grid.\n\nThe need for grid storage to provide peak power is reduced by demand side time of use pricing, one of the benefits of smart meters. At the household level, consumers may choose less expensive off-peak times for clothes washer/dryers, dishwashers, showers and cooking. As well commercial and industrial users will take advantage of cost savings by deferring some processes to off-peak times.\n\nRegional impacts from the unpredictable operation of wind power has created a new need for interactive demand response, where the utility communicates with the demand. Historically this was only done in cooperation with large industrial consumers, but now may be expanded to entire grids. For instance a few large scale projects in Europe link variations in wind power to change industrial food freezer loads, causing small variations in temperature. If communicated on a grid-wide scale, small changes to heating/cooling temperatures would instantly change consumption across the grid.\n\nA report released in December 2013 by the United States Department of Energy further describes the potential benefits of energy storage and demand side technologies to the electric grid: “Modernizing the electric system will help the nation meet the challenge of handling projected energy needs—including addressing climate change by integrating more energy from renewable sources and enhancing efficiency from non-renewable energy processes. Advances to the electric grid must maintain a robust and resilient electricity delivery system, and energy storage can play a significant role in meeting these challenges by improving the operating capabilities of the grid, lowering cost and ensuring high reliability, as well as deferring and reducing infrastructure investments. Finally, energy storage can be instrumental for emergency preparedness because of its ability to provide backup power as well as grid stabilization services.” The report was written by a core group of developers representing Office of Electricity Delivery and Energy Reliability, ARPA-E, Office of Science, Office of Energy Efficiency and Renewable Energy, Sandia National Laboratories, and Pacific Northwest National Laboratory; all of whom are engaged in the development of grid energy storage.\n\nEnergy storages are a valuable asset for the electrical grid. They can provide benefits and services such as load management, power quality and uninterruptable power supply to increase the efficiency and supply security. This becomes more and more important in regard to the energy transition and the need for a more efficient and sustainable energy system.\n\nNumerous energy storages (Pumped-storage hydroelectricity, Electric battery, Flywheel energy storage, Supercapacitor etc.) are available each different in technology and design. The potential they bring depend on how they are used and where they are deployed to. Not all energy storages operate equally well in all situations. For example, a pumped-hydro station is well suited for bulk load management applications due to their large capacities and power capabilities. However, suitable sitings are limited and their usefulness fades when dealing with localized power quality issues. On the other hand flywheels and capacitors are most effective in maintaining power quality but lack storage capacities to be used in larger applications. These constraints are a natural limitation to the storage’s applicability.\n\nSeveral studies have developed interest and investigated the suitability or selection of the optimal energy storage for certain applications. Literature surveys comprise the available information of the state-of-the-art and compare the storage’s uses based on current existing projects. Other studies take a step further in evaluating energy storages with each other and rank their fitness based on Multiple-criteria decision analysis. Another paper proposed an evaluation scheme through the investigation and modelling of storages as equivalent circuits. An indexing approach has also been suggested in a few studies, but is still in the novel stages . \n\nAnother grid energy storage method is to use off-peak or renewably generated electricity to compress air, which is usually stored in an old mine or some other kind of geological feature. When electricity demand is high, the compressed air is heated with a small amount of natural gas and then goes through turboexpanders to generate electricity.\n\nCompressed air storage is typically around 60–90% efficient\n\nAnother electricity storage method is to compress and cool air, turning it into liquid air, which can be stored, and expanded when needed, turning a turbine, generating electricity, with a storage efficiency of up to 70%.\n\n Battery storage was used in the early days of direct current electric power. Where AC grid power was not readily available, isolated lighting plants run by wind turbines or internal combustion engines provided lighting and power to small motors. The battery system could be used to run the load without starting the engine or when the wind was calm. A bank of lead-acid batteries in glass jars both supplied power to illuminate lamps, as well as to start an engine to recharge the batteries. Battery storage technology is typically around 80% to more than 90% efficient for newer lithium ion devices such as the Tesla Powerwall.\n\nBattery systems connected to large solid-state converters have been used to stabilize power distribution networks. Some grid batteries are co-located with renewable energy plants, either to smooth the power supplied by the intermittent wind or solar output, or to shift the power output into other hours of the day when the renewable plant cannot produce power directly (see Installation examples). These hybrid systems (generation + storage) can either alleviate the pressure on the grid when connecting renewable sources or be used to reach self-sufficiency and work \"off-the-grid\" (see Stand-alone power system).\n\nContrary to electric vehicle applications, batteries for stationary storage do not suffer from mass or volume constraints. However, due to the large amounts of energy and power implied, the cost per power or energy unit is crucial. The relevant metrics to assess the interest of a technology for grid-scale storage is the $/Wh (or $/W) rather than the Wh/kg (or W/kg). The electrochemical grid storage was made possible thanks to the development of the electric vehicle, that induced a fast decrease in the production costs of batteries below $300/kWh. By optimizing the production chain, major industrials aim to reach $150/kWh by the end of 2020. These batteries rely on a Li-Ion technology, which is suited for mobile applications (high cost, high density). Technologies optimized for the grid should focus on low cost and low density.\n\nSodium-Ion batteries are a cheap and sustainable alternative to Li-ion, because sodium is far more abundant and cheaper than lithium, but it has a lower power density. However, they are still on the early stages of their development.\n\nAutomotive-oriented technologies rely on solid electrodes, which feature a high energy density but require an expensive manufacturing process. Liquid electrodes represent a cheaper and less dense alternative as they do not need any processing.\n\nThese batteries are composed of two molten metal alloys separated by an electrolyte. They are simple to manufacture but require a temperature of several hundred degree Celsius to keep the alloy in a liquid state. This technology includes ZEBRA, Sodium-sulfur batteries and liquid metal. Sodium sulphur batteries are being used for grid storage in Japan and in the United States. The electrolyte is composed of solid beta alumina. The liquid metal battery, developed by the group of Pr. Sadoway, uses molten alloys of Magnesium and antimony separated by an electrically insulating molten salt. It is still in the prototyping phase.\n\nIn rechargeable flow batteries, the liquid electrodes are composed of transition metals in water at room temperature. They can be used as a rapid-response storage medium. Vanadium redox batteries is another flow battery. They are installed at Huxley Hill wind farm (Australia), Tomari Wind Hills at Hokkaidō (Japan), as well as in non-wind farm applications. A 12 MW·h flow battery was to be installed at the Sorne Hill wind farm (Ireland). These storage systems are designed to smooth out transient wind fluctuations. Hydrogen Bromide has been proposed for use in a utility-scale flow-type battery.\n\nFor example, in Puerto Rico a system with a capacity of 20 megawatts for 15 minutes (5 megawatt hour) stabilizes the frequency of electric power produced on the island. A 27 megawatt 15-minute (6.75 megawatt hour) nickel-cadmium battery bank was installed at Fairbanks Alaska in 2003 to stabilize voltage at the end of a long transmission line.\n\nIn 2016 a zinc-ion battery was proposed for use in grid storage applications.\n\nIn 2017 the California Public Utilities Commission installed 396 refrigerator-sized stacks of Tesla batteries at the Mira Loma substation in Ontario, California. The stacks are deployed in two modules of 10MW each (20MW in total), each capable of running for 4 hours, thus adding up to 80MWh of storage. The array is capable of powering 15,000 homes for over four hours.\n\nBYD proposes to use conventional consumer battery technologies such as lithium iron phosphate (LiFePO4) battery, connecting many batteries in parallel.\n\nThe largest grid storage batteries in the United States include the 31.5MW battery at Grand Ridge Power plant in Illinois and the 31.5 MW battery at Beech Ridge, West Virginia. Two batteries under construction in 2015 include the 400MWh (100MW for 4 hours) Southern California Edison project and the 52 MWh project on Kauai, Hawaii to entirely time shift a 13MW solar farm's output to the evening. Two batteries are in Fairbanks, Alaska (40 MW for 7 minutes using Ni-Cd cells), and in Notrees, Texas (36 MW for 40 minutes using lead-acid batteries). A 13 MWh battery made of used batteries from Daimler's Smart electric drive cars is being constructed in Lünen, Germany, with an expected second life of 10 years.\n\nIn 2015, a 221 MW battery storage was installed in the USA, with total capacity expected to reach 1.7 GW in 2020.\n\nThe UK had a 50 MW lithium ion grid-battery installed in Hertfordshire in 2018.\n\nIn November 2017 Tesla installed a 100 MW, 129 MWh battery system in South Australia. The Australian Energy Market Operator stated that this \"is both rapid and precise, compared to the service typically provided by a conventional synchronous generation unit\".\n\nCompanies are researching the possible use of electric vehicles to meet peak demand. A parked and plugged-in electric vehicle could sell the electricity from the battery during peak loads and charge either during night (at home) or during off-peak.\n\nPlug-in hybrid or electric cars could be used for their energy storage capabilities. Vehicle-to-grid technology can be employed, turning each vehicle with its 20 to 50 kWh battery pack into a distributed load-balancing device or emergency power source. This represents 2 to 5 days per vehicle of average household requirements of 10 kWh per day, assuming annual consumption of 3650 kWh. This quantity of energy is equivalent to between of range in such vehicles consuming 0.5 to 0.16 kWh per mile. These figures can be achieved even in home-made electric vehicle conversions. Some electric utilities plan to use old plug-in vehicle batteries (sometimes resulting in a giant battery) to store electricity However, a large disadvantage of using vehicle to grid energy storage would be if each storage cycle stressed the battery with one complete charge-discharge cycle. However, one major study showed that used intelligently, vehicle-to-grid storage actually improved the batteries longevity. Conventional (cobalt-based) lithium ion batteries break down with the number of cycles – newer li-ion batteries do not break down significantly with each cycle, and so have much longer lives. One approach is to reuse unreliable vehicle batteries in dedicated grid storage as they are expected to be good in this role for ten years . If such storage is done on a large scale it becomes much easier to guarantee replacement of a vehicle battery degraded in mobile use, as the old battery has value and immediate use.\n\nMechanical inertia is the basis of this storage method. When the electric power flows into the device, an electric motor accelerates a heavy rotating disc. The motor acts as a generator when the flow of power is reversed, slowing down the disc and producing electricity. Electricity is stored as the kinetic energy of the disc. Friction must be kept to a minimum to prolong the storage time. This is often achieved by placing the flywheel in a vacuum and using magnetic bearings, tending to make the method expensive. Greater flywheel speeds allow greater storage capacity but require strong materials such as steel or composite materials to resist the centrifugal forces. The ranges of power and energy storage technology that make this method economic, however, tends to make flywheels unsuitable for general power system application; they are probably best suited to load-leveling applications on railway power systems and for improving power quality in renewable energy systems such as the 20MW system in Ireland.\n\nApplications that use flywheel storage are those that require very high bursts of power for very short durations such as tokamak and laser experiments where a motor generator is spun up to operating speed and is partially slowed down during discharge.\n\nFlywheel storage is also currently used in the form of the Diesel rotary uninterruptible power supply to provide uninterruptible power supply systems (such as those in large datacenters) for ride-through power necessary during transfer – that is, the relatively brief amount of time between a loss of power to the mains and the warm-up of an alternate source, such as a diesel generator.\n\nThis potential solution has been implemented by EDA in the Azores on the islands of Graciosa and Flores. This system uses an 18 megawatt-second flywheel to improve power quality and thus allow increased renewable energy usage. As the description suggests, these systems are again designed to smooth out transient fluctuations in supply, and could never be used to cope with an outage exceeding a couple of days.\n\nPowercorp in Australia have been developing applications using wind turbines, flywheels and low load diesel (LLD) technology to maximize the wind input to small grids. A system installed in Coral Bay, Western Australia, uses wind turbines coupled with a flywheel based control system and LLDs. The flywheel technology enables the wind turbines to supply up to 95 percent of Coral Bay’s energy supply at times, with a total annual wind penetration of 45 percent.\n\nHydrogen is being developed as an electrical energy storage medium. Hydrogen is produced, then compressed or liquefied, cryogenicly stored at −252.882 °C, and then converted back to electrical energy or heat. Hydrogen can be used as a fuel for portable (vehicles) or stationary energy generation. Compared to pumped water storage and batteries, hydrogen has the advantage that it is a high energy density fuel.\n\nHydrogen can be produced either by reforming natural gas with steam or by the electrolysis of water into hydrogen and oxygen (see hydrogen production). Reforming natural gas produces carbon dioxide as a by-product. High temperature electrolysis and high pressure electrolysis are two techniques by which the efficiency of hydrogen production may be able to be increased. Hydrogen is then converted back to electricity in an internal combustion engine, or a fuel cell.\n\nThe AC-to-AC efficiency of hydrogen storage has been shown to be on the order of 20 to 45%, which imposes economic constraints. The price ratio between purchase and sale of electricity must be at least proportional to the efficiency in order for the system to be economic. Hydrogen fuel cells can respond quickly enough to correct rapid fluctuations in electricity demand or supply and regulate frequency. Whether hydrogen can use natural gas infrastructure depends on the network construction materials, standards in joints, and storage pressure.\n\nThe equipment necessary for hydrogen energy storage includes an electrolysis plant, hydrogen compressors or liquifiers, and storage tanks.\n\nBiohydrogen is a process being investigated for producing hydrogen using biomass.\n\nMicro combined heat and power (microCHP) can use hydrogen as a fuel.\n\nSome nuclear power plants may be able to benefit from a symbiosis with hydrogen production. High temperature (950 to 1,000 °C) gas cooled nuclear generation IV reactors have the potential to electrolyze hydrogen from water by thermochemical means using nuclear heat as in the sulfur-iodine cycle. The first commercial reactors are expected in 2030.\n\nA community based pilot program using wind turbines and hydrogen generators was started in 2007 in the remote community of Ramea, Newfoundland and Labrador. A similar project has been going on since 2004 in Utsira, a small Norwegian island municipality.\n\nUnderground hydrogen storage is the practice of hydrogen storage in underground caverns, salt domes and depleted oil and gas fields. Large quantities of gaseous hydrogen have been stored in underground caverns by Imperial Chemical Industries (ICI) for many years without any difficulties. The European project Hyunder indicated in 2013 that for the storage of wind and solar energy an additional 85 caverns are required as it cannot be covered by PHES and CAES systems.\n\nPower to gas is a technology which converts electrical power to a gas fuel. There are 2 methods, the first is to use the electricity for water splitting and inject the resulting hydrogen into the natural gas grid. The second less efficient method is used to convert carbon dioxide and water to methane, (see natural gas) using electrolysis and the Sabatier reaction. The excess power or off peak power generated by wind generators or solar arrays is then used for load balancing in the energy grid. Using the existing natural gas system for hydrogen, fuel cell maker Hydrogenics and natural gas distributor Enbridge have teamed up to develop such a power to gas system in Canada.\n\nPipeline storage of hydrogen where a natural gas network is used for the storage of hydrogen. Before switching to natural gas, the German gas networks were operated using towngas, which for the most part consisted of hydrogen. The storage capacity of the German natural gas network is more than 200,000 GW·h which is enough for several months of energy requirement. By comparison, the capacity of all German pumped storage power plants amounts to only about 40 GW·h. The transport of energy through a gas network is done with much less loss (<0.1%) than in a power network (8%). The use of the existing natural gas pipelines for hydrogen was studied by NaturalHy\n\nThe power-to-ammonia concept offers a carbon-free energy storage route with a diversified application palette. At times when there is surplus low-carbon power, it can be used to create ammonia fuel. Ammonia may be produced by splitting water into hydrogen and oxygen with electricity, then high temperature and pressure are used to combine nitrogen from the air with the hydrogen, creating ammonia. As a liquid it is similar to propane, unlike hydrogen alone, which is difficult to store as a gas under pressure or to cryogenically liquefy and store at −253 °C.\n\nJust like natural gas, the stored ammonia can be used as a thermal fuel for transportation and electricity generation or used in a fuel cell. A standard 60,000 m3 tank of liquid ammonia contains about 211 GWh of energy, equivalent to the annual production of roughly 30 wind turbines. Ammonia can be burned cleanly: water and nitrogen are released, but no CO2 and little or no nitrogen oxides. Ammonia has multiple uses besides being an energy carrier, it is the basis for the production of many chemicals, the most common use is for fertilizer. Given this flexibility of usage, and given that the infrastructure for the safe transport, distribution and usage of ammonia is already in place, it makes ammonia a good candidate to be a large-scale, non-carbon, energy carrier of the future.\n\nIn 2008 world pumped storage generating capacity was 104 GW, while other sources claim 127 GW, which comprises the vast majority of all types of grid electric storage – all other types combined are some hundreds of MW.\n\nIn many places, pumped storage hydroelectricity is used to even out the daily generating load, by pumping water to a high storage reservoir during off-peak hours and weekends, using the excess base-load capacity from coal or nuclear sources. During peak hours, this water can be used for hydroelectric generation, often as a high value rapid-response reserve to cover transient peaks in demand. Pumped storage recovers about 70% to 85% of the energy consumed, and is currently the most cost effective form of mass power storage. The chief problem with pumped storage is that it usually requires two nearby reservoirs at considerably different heights, and often requires considerable capital expenditure.\n\nPumped water systems have high dispatchability, meaning they can come on-line very quickly, typically within 15 seconds, which makes these systems very efficient at soaking up variability in electrical \"demand\" from consumers. There is over 90 GW of pumped storage in operation around the world, which is about 3% of \"instantaneous\" global generation capacity. Pumped water storage systems, such as the Dinorwig storage system in Britain, hold five or six hours of generating capacity, and are used to smooth out demand variations.\n\nAnother example is the 1836 MW Tianhuangping Pumped-Storage Hydro Plant in China, which has a reservoir capacity of eight million cubic meters (2.1 billion U.S. gallons or the volume of water over Niagara Falls in 25 minutes) with a vertical distance of 600 m (1970 feet). The reservoir can provide about 13 GW·h of stored gravitational potential energy (convertible to electricity at about 80% efficiency), or about 2% of China's daily electricity consumption.\n\nA new concept in pumped-storage is utilizing wind energy or solar power to pump water. Wind turbines or solar cells that direct drive water pumps for an energy storing wind or solar dam can make this a more efficient process but are limited. Such systems can only increase kinetic water volume during windy and daylight periods.\n\nHydroelectric dams with large reservoirs can also be operated to provide peak generation at times of peak demand. Water is stored in the reservoir during periods of low demand and released through the plant when demand is higher. The net effect is the same as pumped storage, but without the pumping loss. Depending on the reservoir capacity the plant can provide daily, weekly, or seasonal load following.\n\nMany existing hydroelectric dams are fairly old (for example, the Hoover Dam was built in the 1930s), and their original design predated the newer intermittent power sources such as wind and solar by decades. A hydroelectric dam originally built to provide baseload power will have its generators sized according to the average flow of water into the reservoir. Uprating such a dam with additional generators increases its peak power output capacity, thereby increasing its capacity to operate as a virtual grid energy storage unit. The United States Bureau of Reclamation reports an investment cost of $69 per kilowatt capacity to uprate an existing dam, compared to more than $400 per kilowatt for oil-fired peaking generators. While an uprated hydroelectric dam does not directly store excess energy from other generating units, it behaves equivalently by accumulating its own fuel – incoming river water – during periods of high output from other generating units. Functioning as a virtual grid storage unit in this way, the uprated dam is one of the most efficient forms of energy storage, because it has no pumping losses to fill its reservoir, only increased losses to evaporation and leakage.\n\nA dam which impounds a large reservoir can store and release a correspondingly large amount of energy, by controlling river outflow and raising or lowering its reservoir level a few meters. Limitations do apply to dam operation, their releases are commonly subject to government regulated water rights to limit downstream effect on rivers. For example, there are grid situations where baseload thermal plants, nuclear or wind turbines are already producing excess power at night, dams are still required to release enough water to maintain adequate river levels, whether electricity is generated or not. Conversely there's a limit to peak capacity, which if excessive could cause a river to flood for a few hours each day.\n\nSuperconducting magnetic energy storage (SMES) systems store energy in the magnetic field created by the flow of direct current in a superconducting coil which has been cryogenically cooled to a temperature below its superconducting critical temperature. A typical SMES system includes three parts: superconducting coil, power conditioning system and cryogenically cooled refrigerator. Once the superconducting coil is charged, the current will not decay and the magnetic energy can be stored indefinitely. The stored energy can be released back to the network by discharging the coil. The power conditioning system uses an inverter/rectifier to transform alternating current (AC) power to direct current or convert DC back to AC power. The inverter/rectifier accounts for about 2–3% energy loss in each direction. SMES loses the least amount of electricity in the energy storage process compared to other methods of storing energy. SMES systems are highly efficient; the round-trip efficiency is greater than 95%. The high cost of superconductors is the primary limitation for commercial use of this energy storage method.\n\nDue to the energy requirements of refrigeration, and the limits in the total energy able to be stored, SMES is currently used for short duration energy storage. Therefore, SMES is most commonly devoted to improving power quality. If SMES were to be used for utilities it would be a diurnal storage device, charged from base load power at night and meeting peak loads during the day.\n\nSuperconducting magnetic energy storage technical challenges are yet to be solved for it to become practical.\n\nIn Denmark the direct storage of electricity is perceived as too expensive for very large scale usage, albeit significant usage is made of existing Norwegian Hydro. Instead, the use of existing hot water storage tanks connected to district heating schemes, heated by either electrode boilers or heat pumps, is seen as a preferable approach. The stored heat is then transmitted to dwellings using district heating pipes.\n\nMolten salt is used to store heat collected by a solar power tower so that it can be used to generate electricity in bad weather or at night.\n\nOff-peak electricity can be used to make ice from water, and the ice can be stored. The stored ice can be used to cool the air in a large building which would have normally used electric AC, thereby shifting the electric load to off-peak hours. On other systems stored ice is used to cool the intake air of a gas turbine generator, thus increasing the on-peak generation capacity and the on-peak efficiency.\n\nA Pumped Heat Electricity Storage system uses a highly reversible heat engine/heat pump to pump heat between two storage vessels, heating one and cooling the other. The UK-based engineering company Isentropic that is developing the system claims a potential electricity-in to electricity-out round-trip efficiency of 72–80%.\n\nStoring energy by moving large solid masses such as on rail tracks has been considered.\n\nThe levelized cost of storing electricity depends highly on storage type and purpose; as subsecond-scale frequency regulation, minute/hour-scale peaker plants, or day/week-scale season storage.\n\nUsing battery storage is said to be U$0.12-0.17 per kWh.\n\nGenerally speaking, energy storage is economical when the marginal cost of electricity varies more than the costs of storing and retrieving the energy plus the price of energy lost in the process. For instance, assume a pumped-storage reservoir can pump to its upper reservoir a volume of water capable of producing 1,200 MW·h after all losses are factored in (evaporation and seeping in the reservoir, efficiency losses, etc.). If the marginal cost of electricity during off-peak times is $15 per MW·h, and the reservoir operates at 75% efficiency (i.e., 1,600 MW·h are consumed and 1,200 MW·h of energy are retrieved), then the total cost of filling the reservoir is $24,000. If all of the stored energy is sold the following day during peak hours for an average $40 per MW·h, then the reservoir will see revenues of $48,000 for the day, for a gross profit of $24,000.\n\nHowever, the marginal cost of electricity varies because of the varying operational and fuel costs of different classes of generators. At one extreme, base load power plants such as coal-fired power plants and nuclear power plants are low marginal cost generators, as they have high capital and maintenance costs but low fuel costs. At the other extreme, peaking power plants such as gas turbine natural gas plants burn expensive fuel but are cheaper to build, operate and maintain. To minimize the total operational cost of generating power, base load generators are dispatched most of the time, while peak power generators are dispatched only when necessary, generally when energy demand peaks. This is called \"economic dispatch\".\n\nDemand for electricity from the world's various grids varies over the course of the day and from season to season. For the most part, variation in electric demand is met by varying the amount of electrical energy supplied from primary sources. Increasingly, however, operators are storing lower-cost energy produced at night, then releasing it to the grid during the peak periods of the day when it is more valuable. In areas where hydroelectric dams exist, release can be delayed until demand is greater; this form of storage is common and can make use of existing reservoirs. This is not storing \"surplus\" energy produced elsewhere, but the net effect is the same – although without the efficiency losses. Renewable supplies with variable production, like wind and solar power, tend to increase the net variation in electric load, increasing the opportunity for grid energy storage.\n\nIt may be more economical to find an alternative market for unused electricity, rather than try and store it. High Voltage Direct Current allows for transmission of electricity, losing only 3% per 1000 km.\n\nThe United States Department of Energy's International Energy Storage Database provides a free list of grid energy storage projects, many of which show funding sources and amounts.\n\nThe demand for electricity from consumers and industry is constantly changing, broadly within the following categories:\n\nThere are currently three main methods for dealing with changing demand:\n\nThe problem with standby gas turbines is higher costs, expensive generating equipment is unused much of the time. Spinning reserve also comes at a cost, plants run below maximum output are usually less efficient. Grid energy storage is used to shift generation from times of peak load to off-peak hours. Power plants are able to run at their peak efficiency during nights and weekends.\n\nSupply-demand leveling strategies may be intended to reduce the cost of supplying peak power or to compensate for the intermittent generation of wind and solar power.\n\nIn order to keep the supply of electricity consistent and to deal with varying electrical loads it is necessary to decrease the difference between generation and demand. If this is done by changing loads it is referred to as demand side management (DSM). For decades, utilities have sold off-peak power to large consumers at lower rates, to encourage these users to shift their loads to off-peak hours, in the same way that telephone companies do with individual customers. Usually, these time-dependent prices are negotiated ahead of time. In an attempt to save more money, some utilities are experimenting with selling electricity at minute-by-minute spot prices, which allow those users with monitoring equipment to detect demand peaks as they happen, and shift demand to save both the user and the utility money. Demand side management can be manual or automatic and is not limited to large industrial customers. In residential and small business applications, for example, appliance control modules can reduce energy usage of water heaters, air conditioning units, refrigerators, and other devices during these periods by turning them off for some portion of the peak demand time or by reducing the power that they draw. Energy demand management includes more than reducing overall energy use or shifting loads to off-peak hours. A particularly effective method of energy demand management involves encouraging electric consumers to install more energy efficient equipment. For example, many utilities give rebates for the purchase of insulation, weatherstripping, and appliances and light bulbs that are energy efficient. Some utilities subsidize the purchase of geothermal heat pumps by their customers, to reduce electricity demand during the summer months by making air conditioning up to 70% more efficient, as well as to reduce the winter electricity demand compared to conventional air-sourced heat pumps or resistive heating. Companies with factories and large buildings can also install such products, but they can also buy energy efficient industrial equipment, like boilers, or use more efficient processes to produce products. Companies may get incentives like rebates or low interest loans from utilities or the government for the installation of energy efficient industrial equipment. Facilities may shift their demand by enlisting a third party to provide Energy Storage as a Service (ESaaS).\n\nThis is the area of greatest success for current energy storage technologies. Single-use and rechargeable batteries are ubiquitous, and provide power for devices with demands as varied as digital watches and cars. Advances in battery technology have generally been slow, however, with much of the advance in battery life that consumers see being attributable to efficient power management rather than increased storage capacity. Portable consumer electronics have benefited greatly from size and power reductions associated with Moore's law. Unfortunately, Moore's law does not apply to hauling people and freight; the underlying energy requirements for transportation remain much higher than for information and entertainment applications. Battery capacity has become an issue as pressure grows for alternatives to internal combustion engines in cars, trucks, buses, trains, ships, and aeroplanes. These uses require far more energy density (the amount of energy stored in a given volume or weight) than current battery technology can deliver. Liquid hydrocarbon fuel (such as gasoline/petrol and diesel), as well as alcohols (methanol, ethanol, and butanol) and lipids (straight vegetable oil, biodiesel) have much higher energy densities.\n\nThere are synthetic pathways for using electricity to reduce carbon dioxide and water to liquid hydrocarbon or alcohol fuels. These pathways begin with electrolysis of water to generate hydrogen, and then reducing carbon dioxide with excess hydrogen in variations of the reverse water gas shift reaction. Non-fossil sources of carbon dioxide include fermentation plants and sewage treatment plants. Converting electrical energy to carbon-based liquid fuel has potential to provide portable energy storage usable by the large existing stock of motor vehicles and other engine-driven equipment, without the difficulties of dealing with hydrogen or another exotic energy carrier. These synthetic pathways may attract attention in connection with attempts to improve energy security in nations that rely on imported petroleum, but have or can develop large sources of renewable or nuclear electricity, as well as to deal with possible future declines in the amount of petroleum available to import.\n\nBecause the transport sector uses the energy from petroleum very inefficiently, replacing petroleum with electricity for mobile energy will not require very large investments over many years.\n\nVirtually all devices that operate on electricity are adversely affected by the sudden removal of their power supply. Solutions such as UPS (uninterruptible power supplies) or backup generators are available, but these are expensive. Efficient methods of power storage would allow for devices to have a built-in backup for power cuts, and also reduce the impact of a failure in a generating station. Examples of this are currently available using fuel cells and flywheels.\n\n\n"}
{"id": "26032205", "url": "https://en.wikipedia.org/wiki?curid=26032205", "title": "Immersion baptism", "text": "Immersion baptism\n\nImmersion baptism (also known as baptism by immersion or baptism by submersion) is a method of baptism that is distinguished from baptism by affusion (pouring) and by aspersion (sprinkling), sometimes without specifying whether the immersion is total or partial, but very commonly with the indication that the person baptized is immersed completely. The term is also, though less commonly, applied exclusively to modes of baptism that involve only partial immersion (see Terminology, below)\n\nBaptism by immersion is understood by some to imply submersion of the whole body beneath the surface of the water.\n\nOthers speak of baptismal immersion as either complete or partial, and do not find it tautologous to describe a particular form of immersion baptism as \"full\" or \"total\".\n\nStill others use the term \"immersion baptism\" to mean a merely partial immersion by dipping the head in the water or by pouring water over the head of a person standing in a baptismal pool, and use instead for baptism that involves total immersion of the body beneath the water the term \"submersion baptism\".\n\nScholars generally agree that the early church baptized by immersion. It also used other forms. Immersion was probably the norm, but at various times and places immersion, whether full or partial, and also affusion were probably in use. Baptism of the sick or dying was usually by means other than even partial immersion and was still considered valid.\n\nSome writers speak of early Christians baptizing by total immersion (i.e., submerging the person being baptized), or say only that total immersion was preferred. Others speak of early Christians as baptizing either by submersion or by immersion. In one form of early Christian baptism, the candidate stood in water and water was poured over the upper body, and the Oxford Dictionary of the Christian Church says that at least from the 2nd century baptism was administered by a method \"whereby part of the candidate's body was submerged in the baptismal water which was poured over the remainder\".\n\nWilliam Sanford La Sor, Lothar Heiser, Jean-Charles Picard, Malka Ben Pechat, and Everett Ferguson agree that early Christian baptism was normally by total immersion. Sanford La Sor (1987) considers it likely that the archaeological evidence favours total immersion. Lothar Heiser (1986), likewise understands the literary and pictorial evidence to indicate total immersion. Jean-Charles Picard (1989), reaches the same conclusion, and so does Malka Ben Pechat (1989). The study by Everett Ferguson (2009) supports the view of La Sor, Heiser, Picard, and Pechat. Frank K. Flinn also says that the immersion was total, saying that the preference of the Early Church was total immersion in a stream or the sea or, if these were not available, in a fountain or bath-sized tank,\nCommenting on early church practice, other reference works speak of immersion without specifying whether it was total or partial. A recent Bible encyclopedia speaks of the \"consensus of scholarly opinion\" that the baptismal practice of John the Baptist and the apostles was by immersion. A standard Bible dictionary says that baptism was normally by immersion. Among other sources, Old says that immersion (though not the only form), was normally used, Grimes says \"There is little doubt that early Christian baptism was adult baptism by immersion.\", Howard Marshall says that immersion was the general rule, but affusion and even sprinkling were also practiced, since \"archaeological evidence supports the view that in some areas Christian baptism was administered by affusion\". His presentation of this view has been described by Porter and Cross as \"a compelling argument\". Laurie Guy says immersion was probably the norm, but that at various times and places full immersion, partial immersion and affusion were probably in use. Tischler says that total immersion seems to have been most commonly used. Stander and Louw argue that immersion was the prevailing practice of the Early Church. Grenz says that the New Testament does not state specifically what action the baptizer did to the person baptized, when both were in the water, but adds: \"Nevertheless, we conclude that of the three modes immersion carries the strongest case – exegetically, historically, and theologically. Therefore, under normal circumstances it ought to be the preferred, even the sole, practice of the church.\" Most scholars agree that immersion was the practice of the New Testament church.\n\nThe Oxford Dictionary of the Bible (2004) says \"Archaeological evidence from the early centuries shows that baptism was sometimes administered by submersion or immersion...but also by affusion from a vessel when water was poured on the candidate's head...\".\n\nThe Cambridge History of Christianity (2006) also concludes from the archaeological evidence that pouring water three times over the head was a frequent arrangement.\n\nRobin Jensen writes: \"Historians have sometimes assumed that baptism was usually accomplished by full immersion - or submersion - of the body (dunking). However, the archaeological and iconographic evidence is ambiguous on this point. Many - if not most - surviving baptismal fonts are too shallow to have allowed submersion. In addition, a significant number of depictions show baptismal water being poured over the candidate's head (affusion), either from a waterfall, an orb or some kind of liturgical vessel.\" \"Eerdman's Dictionary of the Bible\", also casts doubt on \"the usual assumption that all NT baptisms were by immersion\", stating that some early baptisteries were deep enough to stand in but not broad enough to lie down in, and mentioning that ancient representation of Christ at his baptism show him standing in waist-deep water. The immersion used by early Christians in baptizing \"need not have meant full submersion in the water\" and, while it may have been normal practice, it was not seen as a necessary mode of baptism, so that other modes also may have been used. Submersion, as opposed to partial immersion, may even have been a minority practice in early Christianity.\n\nThe \"Didache\" or \"Teaching of the Twelve Apostles\", an anonymous book of 16 short chapters, is probably the earliest known written instructions, outside of the Bible, for administering baptism. The first version of it was written . The second, with insertions and additions, was written . This work, rediscovered in the 19th century, provides a unique look at Christianity in the Apostolic Age. Its instructions on baptism are as follows:\nCommentaries, including those that distinguish immersion from submersion, typically understand that the Didache indicates a preference for baptizing by immersion. in \"living water\" (i.e., running water, seen as symbolic of life). Barclay observes that the Didache shows that baptism in the early church was by total immersion, if possible, Barton describes the immersion of the Didache as \"ideally by total immersion\", and Welch says it was by \"complete immersion\". In cases of insufficient water it permits pouring (affusion), which it differentiates from immersion, using the Greek word \"ekcheō\", (\"pour\", in the English translation) and not \"baptizō\" (\"baptize\", in the English translation), but which it still considers to be a form of baptism (\"baptisma\").\n\nMartin and Davids say the Didache envisages \"some form of immersion\", and the \"Oxford Dictionary of the Christian Church\" refers its readers to its entry on immersion, which it distinguishes from submersion and affusion.\n\nThe Didache gives \"the first explicit reference to baptism by pouring, although the New Testament does not exclude the possibility of this practice\" Brownson says that the Didache does not state whether pouring or immersion was recommended when using running water, and Sinclair B. Ferguson argues that the only mode that the Didache mentions is affusion. Lane says that \"it is probable that immersion was in fact the normal practice of baptism in the early church, but it was not regarded as an important issue\", and states that the Didache does not suggest that the pouring of water was any less valid than immersion.\n\nChristian theologians such as John Piper use several parts of the New Testament to support full immersion (submersion) as the intended symbol:\nPiper asserts that baptism refers to the physical lowering into the water and rising in faith in part because of the reflection of this symbol in which says \"having been buried with him in baptism and raised with him through your faith in the power of God, who raised him from the dead through the glory of the Father, so we too might walk in newness of life.\" Others hold that there is no evidence in the New Testament that any one mode of baptism was used.\n\nAs criticism of the claim that, in , which is the only reference in the New Testament to Christian baptism being administered in the open, the actions of \"going down into the water\" and \"coming up out of the water\" indicate that this baptism was by immersion, it is pointed out that \"going down into\" and \"coming up out of\" a river or a store of still water, actions there ascribed to both the baptizer and the baptized, do not necessarily involve immersion in the water. In the nineteenth century, anti-immersionist Rev. W. A. McKay wrote a polemic work against immersion baptism, arguing that it was a theological invention of the Roman Catholic Church. Differentiating between immersion and affusion, McKay held that βαπτίζω referred to affusion (which McKay understood as standing in water and having water poured over the head), as opposed to immersion. Challenging immersion baptism, he wrote:\n\nIn the same passage the act of baptizing is distinguished from the going down into the water: \"They both went down into the water, Philip and the eunuch, and he baptized him. And when they came up out of the water ...\"\nAs McKay and others also pointed out, the Greek preposition εἰς, here translated as \"into\", is the same as is used when Peter is told to go \"to\" the sea and take the first fish that came up () and in other passages where it obviously did not imply entry of the kind that submersion involves. In fact, in the same chapter 8 of the \"Acts of the Apostles\", the preposition εἰς appears 11 times, but only once is it commonly translated as \"into\"; in the other verses in which it appears it is best translated as \"to\". The same ambiguity pertains to the preposition ἐκ.\n\nJohn Calvin (1509–1564) wrote that \"it is evident that the term baptise means to immerse, and that this was the form used by the primitive Church\", but in the same context (\"Institutes of the Christian Religion\" IV, xv, 19), using the same verb \"immerse\", but indicating that it does not necessarily mean immersing \"wholly\", he also wrote: \"Whether the person who is baptised be wholly immersed, and whether thrice or once, or whether water be only poured or sprinkled upon him, is of no importance; Churches ought to be left at liberty in this respect, to act according to the difference of countries.\" Modern, professional lexicography defines βαπτίζω as dip, plunge or immerse, while giving examples of its use for merely partial immersion.\n\n and are two instances of New Testament uses of the verb \"baptizo\" outside the context of Christian baptism. One speaks of how the Pharisees do not eat unless they \"wash their hands\" thoroughly (\"nipto\", the ordinary word for washing something), and, after coming from the market place, do not eat unless they \"wash themselves\" (literally, \"baptize themselves\", passive or middle voice of \"baptizo\"). The other tells how a Pharisee, at whose house Jesus ate, \"was astonished to see that he did not first \"wash himself\" (literally, \"baptize himself\", aorist passive of \"baptizo\") before dinner\". Some commentaries claim that these two passages show that the word baptizo in the New Testament cannot be assumed to have the meaning \"immerse\".\n\nIn the first of the two passages, it is actually the hands that are specifically identified as \"washed\" (), not the entire persons, who are described as having (literally) \"baptized themselves\" – ). Zodhiates identifies the meaning of baptizo here as 'immerse', even if not totally (\"wash part of the body such as the hands\"). but the word is rendered \"wash themselves\" or \"purify themselves\", not \"baptize themselves\" or \"immerse themselves\", by modern English Bible translations, professional commentaries, and translation guides. For the same reason, the Liddell-Scott-Jones Greek–English Lexicon (1996) cites the other passage () as an instance of the use of the word baptizo to mean \"perform ablutions\", not \"baptize\", \"dip\", \"plunge\", \"immerse\", and the standard lexicon of Bauer and Danker treats it as an instance of a derived meaning, \"wash ceremonially for the purpose of purification\", distinct from the basic meaning (\"immerse\") of the verb \"baptizo\", in line with the view that cannot refer to a total immersion of the person. References to the cleaning of vessels which use baptizo also refer to immersion.\n\nThe burial symbolism of and is seen by some Christians as a reference not to the manner of baptism in water but to \"a spiritual death, burial, resurrection, and new life\".\n\nForms of baptismal immersion differ widely between Christian groups. In the view of many, baptismal immersion can be either complete or partial, and adjectives such as \"full\", \"total\", and \"partial\" serve to differentiate between immersion of the whole body or only of a part.\n\nThe Eastern Orthodox hold that baptism has always been by immersion and it is not proper to perform baptism by way of sprinkling of water. The immersion is done three times and is referred to as \"total\" or \"full\". Modern practice may vary within the Eastern Rite; Everett Ferguson cites Lothar Heiser as acknowledging: \"In the present practice of infant baptism in the Greek church the priest holds the child as far under the water as possible and scoops water over the head so as to be fully covered with water\", and the \"Oxford Dictionary of the Christian Church\" states that the rite \"whereby part of the candidate's body was submerged in the baptismal water which was poured over the remainder ... is still found in the Eastern Church\". Eastern Orthodox consider the form of baptism in which the person is placed in water as normative; only in exceptional circumstances, such as if a child is in imminent danger of death, may they baptize by affusion or, since there is always some moisture in air, perform \"air baptism\".\n\nBaptism by partial immersion, a mode of baptism that, according to the Oxford Dictionary of the Christian Church is still found in the Eastern Church, is also the form presented in the \"Key of Truth\", the text described as the manual of the old Armenian Baptists, which lays down that the person to be baptized \"shall come on his knees into the midst of the water\" and there make a profession of faith to \"the elect one\", who \"instantly takes the water into his hands, and ... shall directly or indirectly empty out the water over the head\".\n\nThe Syro Malabar Catholic sect of the Saint Thomas Christians, who trace their origin to Thomas the Apostle, have always practised pouring rather than any form of immersion.\nHowever the Malankara Church follows child immersion baptism in Baptismal font.\n\nAccording to the Catechism of the Catholic Church,\n\nComplete submersion of the person being baptized is the custom only among some denominations that have arisen in the second millennium, but, since the form of immersion that is in use in the Eastern Orthodox Church is referred to as \"full immersion\", that church is included in this section.\n\nBaptism is a second essential component of UPCI doctrine. Members of the UPCI affirm an indispensable need for baptism, citing John 3:5, Acts 2:38 and Matthew 28:19. They point to Matthew 3:13–16 as evidence that even Jesus himself was baptized. The UPCI mode of baptism is complete immersion in water, completed in the name of Jesus Christ for the remission of sins.\n\nAnabaptists (\"re-baptizers\") and Baptists promote adult baptism, or \"believer's baptism\". Baptism is seen as an act identifying one as having accepted Jesus Christ as Savior.\n\nAnabaptists were given that name because of performing baptism on people whom others, but not the Anabaptists, considered to be already baptized. They did not accept infant baptism as true baptism.\n\nAnabaptists perform baptisms indoors in a baptismal font or baptistry, a swimming pool, or a bathtub, or outdoors in a creek or river. Baptism memorializes the death, burial, and resurrection of Jesus. Baptism, they believe, does not accomplish anything in itself, but is an outward personal sign or testimony that the person's sins have already been washed away by the blood of Christ shed on the cross. It is considered a covenantal act, signifying entrance into the New Covenant of Christ.\n\nImmersion baptism, understood as demanding total submersion of the body, is required by Baptists, as enunciated in the 1689 Baptist Catechism: \"Baptism is rightly administered by immersion, or dipping the whole body of the person in water, in the name of the Father, and of the Son, and of the Holy Spirit\", indicating that the whole body must be immersed, not just the head.\n\nBaptism by submersion is practised by the Christian Church (Disciples of Christ), but most of them do not suggest rebaptism of those who have undergone a baptism of a different Christian tradition. Baptism in Churches of Christ, which also have roots in the Restoration Movement, is performed only by bodily immersion. This is based on their understanding of the meaning of the word \"baptizo\" as used in the New Testament, a belief that it more closely conforms to the death, burial and resurrection of Jesus, and that historically immersion was the mode used in the 1st century, and that pouring and sprinkling later emerged as secondary modes when immersion was not possible.\n\nSeventh-day Adventists believe that \"baptism symbolizes dying to self and coming alive in Jesus.\" Seventh-day Adventists teach that it symbolizes and declares a member's new faith in Christ and trust in His forgiveness. Buried in the water, the member arises to a new life in Jesus, empowered by the Holy Spirit. Adventists practice full immersion baptism.\n\nSabbath Rest Adventists adhere to full immersion in baptism as a symbol of the death of \"the old man\".\n\nMajor Protestant groups in which baptism by total or partial immersion is optional, although not typical, include Anglicans, Lutherans, Presbyterians, Methodists, and the Church of the Nazarene.\n\nLatter-day Saints’ written explanations concerning baptism stated: “…we are baptized by being lowered under water and raised back up by a person who has authority from God to do so. This action symbolizes Jesus Christ’s death, burial, and resurrection, and it also represents the end of our old lives and beginning a new life as His disciples.” \"Doctrine and Covenants\" 20:72-74) gives the authoritative declaration on mode:\n\nBaptism by immersion is the only way to be completely accepted as a member, either converted to or raised in the church; no other form is accepted in the LDS Church. Baptism for the dead is also performed in LDS temples, to provide this ordinance to those who did not have the opportunity in life, or were physically unable, \"post mortem\", for them to accept as they will. It is performed the same way there.\n\nThe Community of Christ, formerly known as the Reorganized Church of Jesus Christ of Latter Day Saints, practices baptism by submersion.\n\nComplete immersion of the person in water is considered necessary by Jehovah's Witnesses.\n\n\n\n"}
{"id": "35707126", "url": "https://en.wikipedia.org/wiki?curid=35707126", "title": "Ionospheric dynamo region", "text": "Ionospheric dynamo region\n\nIn the height region between about 85 and 200 km altitude on Earth, the ionospheric plasma is electrically conducting. Atmospheric tidal winds due to differential solar heating or due to gravitational lunar forcing move the ionospheric plasma against the geomagnetic field lines thus generating electric fields and currents just like a dynamo coil moving against magnetic field lines. That region is therefore called ionospheric dynamo region. \nThe magnetic manifestation of these electric currents on the ground can be observed during magnetospheric quiet conditions. They are called Sq-variations (S=solar; q=quiet) and L-variations (L=lunar) of the geomagnetic field.\nAdditional electric currents are generated by the varying magnetospheric electric convection field. These are the DP1-currents (the auroral electrojets) and the polar DP2-currents.\n\nRadioactive material from the ground and galactic cosmic rays ionize a small fraction of the atmospheric gas within the lower and middle atmosphere and make the gas electrically conducting. Electrons quickly attach to neutral particles forming negative ions. The positive ions are mostly singly charged. The electric conductivity depends on the mobility of the ions . That mobility is proportional to the reciprocal air density. Thus, the electric conductivity increases almost exponentially with altitude. The ions move with the neutral gas making the conductivity isotropic.\n\nAt heights between about 85 and 200 km however -the dynamo region-, solar X- and extreme ultraviolet radiation (XUV) is almost completely absorbed generating the ionospheric D-, E-, and F-layers. Here, the electrons are already bound to the geomagnetic field gyrating several times about these lines before they collide with the neutrals, while the positive ions still essentially move with the neutral gas. Thus, the electric conductivity becomes anisotropic. The conductivity parallel to an electric field E is called Pedersen conductivity. The conductivity orthogonal to E and the geomagnetic field B is the Hall conductivity. Ohmic losses and thus Joule heating occur when Pedersen currents flow. The component parallel to B still increases with altitude. Near the geomagnetic dip equator, a west-east directed electric field generates vertical Hall currents which cannot close. Therefore, a vertical polarization field builds up generating a horizontal Hall current which adds to the Pedersen current. Such enhancement is described by the Cowling conductivity. Pedersen and Hall conductivities reach maximum values near 120 to 140 km altitudes with numbers of about 1 mS/m during sunlit conditions. During the night, these numbers may decrease by a factor of ten or more. The values of these conductivities depend on local time, latitude, season and solar 11- year cycle. The height integrated conductivities become of the order of 50 S, or a total resistance of the dynamo region of about 1/50 = 0.02 Ohm during daytime conditions.\n\nIn the auroral regions which lie between about 15° and 20° geomagnetic co-latitude and the corresponding latitudes in the southern hemisphere, precipitating high energy particles from the magnetosphere ionize the neutral gas, in particular at heights around 110 to 120 km, and increase the electric conductivity substantially. During magnetospheric disturbed conditions, this conductivity enhancement becomes much larger, and the auroral regions move equatorward.\n\nAt heights above about 200 km, collisions between neutrals and plasma become rare so that both ions and electrons can only gyrate about the geomagnetic lines of force, or drift orthogonal to E and B. The parallel conductivity is so large that the geomagnetic lines of force become electric potential lines, and only electric fields orthogonal to B can exist (see magnetosphere).\n\nAtmospheric tides are global-scale waves excited by regular solar differential heating (thermal tides)or by the gravitational tidal force of the moon (gravitational tides). The atmosphere behaves like a huge waveguide closed at the bottom (the Earth's surface) and open to space at the top. In such a waveguide an infinite number of atmospheric wave modes can be excited. Because the waveguide is imperfect, however, only modes of lowest degree with large horizontal and vertical scales can develop sufficiently well so that they can be filtered out from the meteorological noise. They are solutions of the Laplace equation and are called Hough functions. These can be approximated by a sum of spherical harmonics.\n\nTwo kinds of wave modes exit: class 1 waves (sometimes called gravity waves), and class 2 waves (rotational waves). Class 2 waves owe their existence to the Coriolis effect and can only exist for periods larger than 12 hours. Tidal waves can be either internal (travelling waves ) with positive eigenvalues (or equivalent depth) which have finite vertical wavelengths and can transport wave energy upward, or external (evanescent waves) with negative eigenvalues and infinitely large vertical wavelengths meaning that their phases remain constant with altitude. These external wave modes cannot transport wave energy, and their amplitudes decrease exponentially with height outside their source regions. Each mode is characterized by four numbers: the zonal wave number n, positive for class 1 waves and negative for class 2 waves (their meridional structures becoming increasingly complex with increasing number n), a meridional wave number m, the eigenvalue, and the period, in our case one solar or lunar day, respectively. The modes are labeled as (m, n). Even numbers of n correspond to waves symmetric with respect to the equator, and odd numbers corresponding to antisymmetric waves.\n\nAt thermospheric heighs, dissipation of atmospheric waves becomes significant so that at above about 150 km altitude, all wave modes gradually become external waves, and the Hough functions degenerate to spherical harmonicss; e.g., mode (1, -2) develops to the spherical harmonic P(θ), mode (2, 2) becomes P(θ), with θ the co-latitude, etc.\n\nThe fundamental solar diurnal tidal mode which optimally matches the solar heat input configuration and thus is most strongly excited is the (1, -2) - mode. It depends on local time and travels westward with the Sun. It is an external mode of class 2. Its maximum pressure amplitude on the ground is about 60 hPa. Within the thermosphere, however, it becomes the predominant mode reaching temperature amplitudes at the exosphere of at least 140 K and horizontal winds of the order of 100 m/s and more increasing with geomagnetic activity. The largest solar semidiurnal wave is mode (2, 2) with maximum pressure amplitudes near the ground of 120  hPa. It is an internal class 1 wave. Its amplitude increases with altitude. Although its solar excitation is half of that of mode (1, -2), its amplitude on the ground is larger by a factor of two. This indicates the effect of suppression of external waves, in this case by a factor of four\n\nThe dominant migrating lunar tide is the (2, 2) mode depending on lunar local time. Its maximum pressure amplitude near the ground is 6 Pa, which is far below the meteorological noise. Therefore, it is not simple to detect such small signal. Because it is an internal waves, its amplitude increases with altitude reaching values at 100 km height two orders of magnitude larger than on the ground.\n\nMore than 100 geomagnetic observatories around the world measure regularly the variations of the earth's magnetic field. The daily variations during selected days of quiet geomagnetic activity are used to determine a monthly mean. From the horizontal component ΔH of such data, one can derive a corresponding overhead equivalent electric sheet current system at dynamo layer heights of strength\n\nJ = 2 ΔH/μ = 1.6 ΔH\n\nwith J (in milliAmpere/meter) the electric overhead sheet current, ΔH (in nanoTesla) the observed horizontal component of the geomagnetic variation, and μ the electric permeability of free space. One can determine the direction of the magnetic field with regard to the current by the simple rule of thumb: if the thumb of the right hand points into the direction of the current, the curved fingers give the direction of the associated magnetic field.\n\nOne has to take into account that this relationship is not unique. In general, electric currents within ionosphere and magnetosphere are three-dimensional, and an infinite number of current configurations fits to the geomagnetic variations observed on the ground. Magnetic measurement in space are necessary to obtain a realistic picture.\nFigure 1a shows current streamlines of an equivalent Sq current as seen from the sun at noon. This current configuration is fixed to the sun, while the earth rotates beneath it. A total current of about 140 kA flows within one daytime vortex. The rotating Sq current and the electrically conducting earth interior behave like a huge transformer with the dynamo region as the primary winding and the earth as the secondary winding. Since the Sq current varies with the basic period of one day, electric currents are induced within the earth's interior. The magnetic field of this secondary current is superimposed on the magnetic field of the primary Sq current. Methods to separate both components go back to Gauss. The amplitude of the secondary current is about 1/3 of that of the primary current and slightly shifted in phase. Figure 1b shows that secondary component. The ratio between sheet current and magnetic component given above has now simply the value one.\n\nThe Sq current depends on season. The summer vortex is intensified compared with the winter vortex and reaches into the winter hemisphere. A longitudinal dependence of the Sq current exists which is related to the inclined dipole component of the internal magnetic field, but probably also to nonmigrating tidal waves from below. In the course of the 11-year solar cycle, the amplitude of Sq increases by a factor of more than two from sunspot minimum to sunspot maximum. Two thirds of this increase may result from the enhancement of the electric conductivity with solar activity. The rest is probably due to the increase of the wind speed caused by the temperature increase with increasing solar activity. During the night, the electron density of the ionospheric E-layer diminishes much more strongly than that of the F-layer. Therefore, the height center of the dynamo region shifts upward.\n\nThe main driver of the Sq current is the external (1, -2) tidal wave mode. Due to its constant phase with altitude, is most efficient to drive coherent winds at dynamo layer height, while the currents generated by the internal modes interfere destructively at various heights. A Fourier analysis shows a semidiurnal component with an amplitude of 1 /2 of that of the diurnal component, phase shifted by 180°. This appears to be the result of nonlinear coupling between the product of the diurnally varying wind and the diurnally varying conductivity \nThe centers of the daytime vortices show a day to day variability. This can be attributed to the internal tidal modes which are sensitive to the varying meteorological conditions in the lower and in the middle atmosphere, in part also to solar activity. \nNear the dip equator (where the geomagnetic field lines are horizontal), a strong band of eastward flowing currents can be observed in a range of about 150 km from the equator\n. Such enhancement of the Sq current by a factor of about four is due to the Cowling conductivity. \nDuring a solar flare, bursts of solar radiation from the environment of an active sunspot reach the higher atmosphere, mainly at E- and D- layer heights, lasting at most for one hour. The electric conductivity increases, and the Sq current enhances on the daytime hemisphere. A small increase occurs, called geomagnetic solar flare effect or crochet. During a solar eclipse, the electric conductivity decreases in the shadow region, and the Sq current and thus the Sq variation diminishes by a few nT in that area. It is called a geomagnetic solar eclipse effect. Both events can be observed only during quiet geomagnetic activity.\nIn the aftermath of strong magnetospheric disturbances, a current system develops into a quasi \nanti-Sq-current. It is generated by Joule heating in the polar thermosphere. This current system is called Ddyn.\n\nIn order to determine quantitatively the dynamo action of the neutral wind U, one starts with the horizontal momentum equation of the wind together with an equation for the divergence of the wind. The momentum equation balances the inertial force, the Coriolis force, and the horizontal gradient of pressure p. In addition, the Ampere force Jx B couples the electric current density j to the wind and pressure system. The equivalent depth h (the eigenvalue of the tidal mode) determines the divergence of the wind. The electric current must obey Ohm's law. An electric polarization field E is generated by charge separation to enforce the condition of no sources and sinks of the current. Feedback between wind and electric current occurs via the Lorentz force Ux B. Usually, the electric conductivity tensor σ is considered as a given data set, and a height integrated conductivity tensor Σ and a height integrated sheet current J are applied.\n\nIn conventional models, the Ampere force is neglected. This means that gate B in Figure 2 is open. This is called a kinematic dynamo. Models with closed gate B are called hydromagnetic dynamos. The influence of the mutual coupling between wind and current can immediately be seen if one considers an infinitely large electric conductivity σ. In the kinematic model, the electric current would become infinitely large, while the wind amplitude remains constant. In the hydromagnetic model, the current reaches an upper limit, similar to a technical dynamo during short circuit, while the wind amplitude breaks down to a fraction of its original value. Charge separation acts like a self-impedance preventing the current to become infinitely large.\n\nLunar (L) currents are weaker by a factor of about 20 than the Sq currents. The dominant wind component to drive these currents is the (2, 2) tidal mode. The L current is similar in shape as the Sq current, with the difference that four vortices instead of two exit. In each vortex a total current of about 4 kA flows. The seasonal variation of L is similar to that of Sq. During sunlit hours, L is strongly enhanced, while it approaches zero during the night. Thus, the L current exhibits, in addition, a modulation depending on the lunar phase. The geomagnetic effect of the L-current can best been seen near the dip equator where the Cowling conductivity strongly enhances that current.\n\nInteraction between solar wind plasma and the polar geomagnetic field produces a global-scale magnetospheric electric convection field directed from dawn to dusk with a potential difference of about 15 kV during quiet magnetospheric conditions, increasing substantially during disturbed conditions. Charge separation takes place at the magnetopause. This area is connected with the ionospheric dynamo region via the first open geomagnetic field lines with one footpoint within the auroral regions. Thus, electric discharging currents can flow via field-aligned currents as two small bands within the auroral zone dynamo layer, on the daytime as well as on the nighttime hemisphere. These currents are called DP1 current or auroral electrojets. Their magnitudes are of the order of Mega Amperes. Ohmic losses and thus Joule heating of these currents is comparable with that due to the solar XUV heat input within middle and lower latitudes during quiet conditions and much larger during disturbed conditions. Therefore, it dominates ionospheric and thermospheric dynamics and causes ionospheric and thermospheric storms \n\nThe magnetospheric electric convection field drives a two cell current within the polar cup with their vortices situated on the morning and on the evening side. It is called DP2 current. That current exists already during quiet conditions (S) and becomes enhances during disturbed conditions. It is mainly composed of electric Hall currents.\n\nIf the azimuthal component of the interplanetary magnetic field (IMF) is directed toward dusk, the magnetospheric plasma is slowed down in the northern polar cap and accelerated in the southern polar cap. If the azimuthal component of the IMF is directed toward dawn, the situation is reversed. This devitation from co-rotation disappears at lower latitudes. The magnetic effect on the ground within the polar regions corresponds to an ionospheric Hall current at about 10 polar distance encircling the magnetic poles in a clockwise direction as seen by an observer standing on the ground during interplanetatry sectors with fields pointing away from the sun, and in a counterclockwise direction during toward-sector polarity \n\n"}
{"id": "5279371", "url": "https://en.wikipedia.org/wiki?curid=5279371", "title": "Jenny Greenteeth", "text": "Jenny Greenteeth\n\nJenny Greenteeth is a figure in English folklore. A river hag, similar to Peg Powler or a grindylow, she would pull children or the elderly into the water and drown them. The name is also used to describe pondweed or duckweed, which can form a continuous mat over the surface of a small body of water, making it misleading and potentially treacherous, especially to unwary children. With this meaning the name is common around Liverpool and southwest Lancashire.\n\nShe was often described as green-skinned, with long hair, and sharp teeth. She is called Jinny Greenteeth in Lancashire, but in Cheshire and Shropshire she is called Ginny Greenteeth, Jeannie Greenteeth or Wicked Jenny.\n\nShe is also described as lurking in the upper branches of trees at night, although this may be a folklorist's confusion with the northern English \"Jinny-hewlet\", a folk name for an owl.\n\nShe is likely to have been an invention to frighten children from dangerous waters, similar in nature to the Slavic Rusalka, the Kappa in Japanese mythology, or Australia's Bunyip. But one folklorists have seen in her a memory of sacrificial practices. A similar figure in Jamaican folklore is called the River Mumma (River Mother). She is said to live at the fountainhead of large rivers in Jamaica sitting on top of a rock, combing her long black hair with a gold comb. She usually appears at midday and she disappears if she observes anyone approaching. However, if an intruder sees her first and their eyes meet, terrible things will happen to the intruder. Relatively is in American folklore around Lake Erie, specifically in the urban legends of Erie, Pennsylvania in which sailors use a paranormal being to reason for the dangers and shipwrecks in the Erie Quadrangle (lake area around Erie County).Referred to as the Storm Hag (uncommonly also known as Jenny Greenteeth), is said to be a green skinned woman with teeth like a shark’s but green, as well as piercing yellow eyes who rests on the bottom of the Lake, off the coast of Presque Isle and sings a song whenever s ship approaches.\n\nWhen the sailors of the ship hear the song, the Storm Hag attacks the ship and crew with violent storms and waves, sinking and devouring them.\n\nJenny Greenteeth inspired the lake monster (Meg Mucklebones) in the 1985 Ridley Scott fantasy film \"Legend\".\n\nJenny Green Teeth (note the words are separated) is the title character in a book of short stories, \"Jenny Green Teeth and Other Short Stories\" by New Zealand-born English writer and Scholar Joel Hayward.\n\nJenny Green Teeth is also the main subject of a poem, \"Welsh Maiden\", by Joel Hayward in his collection, \"Lifeblood: A Book of Poems\" (2003); and is recalled by *John Heath-Stubbs in his poem ‘The Green Man’s Last Will and Testament’, lamenting the eclipse of \"the cruel nymphs/ Of the northern streams, Peg Powler of the Tees/ And Jenny Greenteeth of the Ribble\".\n\nIn the novelette \"Water Babies\" by Simon Brown, Jenny Greenteeth is mentioned as being one of many names for a child-snatching water-demon. In the novella, an Australian police officer investigates a series of drownings that turn out to be predatory attacks by a seal-like creature.\n\nShe appears in Terry Pratchett's \"The Wee Free Men\", attacking the main character Tiffany Aching and her brother Wentworth near a shallow stream.\n\nJenny Greenteeth appears in a number of \"Dungeons & Dragons\" (5th Edition) adventure modules, including: DDEX01-08 \"Tales Trees Tell\" (2014), DDAL04-01 \"Suits of the Mist\" (2016), DDAL04-06 \"The Ghost\" (2016), and DDAL04-14 \"The Dark Lord\" (2016).\n\nJenny Greenteeth appears in the story \"Something Borrowed\" and in the novel \"Summer Knight\", and is mentioned in the novel \"Proven Guilty\", all by Jim Butcher; and is also mentioned in \"Ink Exchange\" by Melissa Marr.\n\nShe also appears in the story \"Pretty Jennie Greenteeth\" by Leife Shallcross, which won the 2016 Aurealis Award for best young adult short story.\n\nIn the video game \"Remember Me\", there is a male version of the figure named Johnny Greenteeth.\n\n"}
{"id": "53076024", "url": "https://en.wikipedia.org/wiki?curid=53076024", "title": "Lake Palas Tuzla", "text": "Lake Palas Tuzla\n\nLake Palas Tuzla () is a saline lake in Kayseri Province, central Turkey. It is a registered natural protected area.\n\nThe lake is situated in the Palas neighborhood of Sarıoğlan district in Kayseri province. It is away from the city of Kayseri. The lake is at the center of a closed basin. Its elevation is above mean sea level. The surface area is . It is fed by five creeks. During the dry season, the lake basin is covered by a layer of salt. The lake is the site of a salt works.\n\nThe surrounding area of the lake is a wetland, which is located on bird migration routes and habitat of some endemic flora. The wetland was declared a natural protected area of first degree on June 26, 2009.\n"}
{"id": "2702745", "url": "https://en.wikipedia.org/wiki?curid=2702745", "title": "Lambda Tucanae", "text": "Lambda Tucanae\n\nThe Bayer designation Lambda Tucanae (λ Tuc, λ Tucanae) is shared by two star systems in the constellation Tucana.\n\n\nThey are separated by 0.23° in the sky.\n"}
{"id": "16407766", "url": "https://en.wikipedia.org/wiki?curid=16407766", "title": "List of Dungeons &amp; Dragons monsters (1974–76)", "text": "List of Dungeons &amp; Dragons monsters (1974–76)\n\nThis is the list of \"Dungeons & Dragons\" monsters from products published in 1974–1976, an important element of that role-playing game. This list only includes monsters from official \"Dungeons & Dragons\" supplements published by TSR, Inc., not those licensed or unlicensed third party products such as video games or unlicensed \"Dungeons & Dragons\" manuals. This list only includes the content from the original 1974 \"Dungeons & Dragons\" boxed set, the \"Greyhawk\" supplement (1974), the \"Blackmoor\" supplement (1975), and \"Eldritch Wizardry\" (1976).\n\nThe 1974 \"Dungeons & Dragons\" boxed set by Gary Gygax and Dave Arneson contained three booklets, including a list of monsters in the booklet \"Monsters & Treasure\". This booklet contained an index on pages 3–4 featuring statistics about how many creatures of each type of creature appeared per encounter, armor class, how many inches the creature could move on its turn, hit dice, % in lair, and treasure. Pages 5–20 followed with descriptions of each of the monsters, typically consisting of one or more paragraphs. Most of the monsters on this book did not feature an illustration. Also featured are descriptions of humans (bandits, brigands, berserkers, dervishes, nomads, buccaneers, cavemen, and mermen), horses, insects, and other small and large animals.\n\nThis first supplement to the Gygax/Arneson boxed set is by Gary Gygax and Rob Kuntz and was printed in 1974. Information is presented in a similar manner as that in the boxed set booklet. Page 33 contains an index of the monsters presented in the book, and pages 34–40 contain descriptions of each monster. Additions and corrections to Vampires and Elementals from the boxed set are included in this book on page 34.\n\nThis second supplement to the Gygax/Arneson boxed set is by Dave Arneson and was printed in 1975. Information is presented in a similar manner as that in the boxed set booklet. Page 14 contains an index of the monsters presented in the book, and pages 15–24 contain descriptions of each monster.\n\nThis third supplement to the Gygax/Arneson boxed set is by Gary Gygax and Brian Blume and was printed in 1976. Information is presented in a similar manner as that in the boxed set booklet. Page 27 contains an index of the monsters presented in the book, and pages 27–40 contain descriptions of each monster. Additions and corrections to several monsters from previous books in the series are included in this book from pages 27–29.\n\nThis fourth supplement to the Gygax/Arneson boxed set is by Robert Kuntz and James Ward and was printed in 1976.\n"}
{"id": "23657481", "url": "https://en.wikipedia.org/wiki?curid=23657481", "title": "List of black holes", "text": "List of black holes\n\nThis is a list of black holes (and stars considered probable candidates) organized by size (including black holes of undetermined mass); some items in this list are galaxies or star clusters that are believed to be organized around a black hole. Messier and New General Catalogue designations are given where possible.\n\n\n\n\n\n\n\nIn addition, the signal of several binary black holes merging into a single black hole and in so doing producing gravitational waves have been observed by the LIGO instrument. These are listed above in the section Black holes detected by gravitational wave signals.\n\nAs of 2014, there are 5 triple black hole systems known.\n\nlist \n\n"}
{"id": "25928200", "url": "https://en.wikipedia.org/wiki?curid=25928200", "title": "List of highest mountains of Austria", "text": "List of highest mountains of Austria\n\nThis page shows the highest mountains in Austria as well as the highest mountains in each mountain range and in each of the Austrian states. The heights are given in metres above the Adriatic Sea.\n\nThis table lists about 150 Austrian summits above 3150 m with a topographic prominence of at least 150 m and nine peaks of note with a slightly lower re-ascent. Only those mountains with a prominence of 150 m or more are ranked.\n\nThe ranges correspond to those listed for Austria in the AVE. (→ see diagram)\nIf the highest mountain in a range is not within Austrian national territory it is not shown in the list. (e.g.: Piz Linard (3,411m), highest mountain in the Silvretta) \n\n\n\n"}
{"id": "52918982", "url": "https://en.wikipedia.org/wiki?curid=52918982", "title": "List of microquasars", "text": "List of microquasars\n\nThis is a list of all known microquasars:\n\n"}
{"id": "28460340", "url": "https://en.wikipedia.org/wiki?curid=28460340", "title": "List of rivers of Ivory Coast", "text": "List of rivers of Ivory Coast\n\nFour major river systems follow meandering courses from north to south, draining into the Gulf of Guinea. From west to east these are the Cavally, Sassandra, Bandama, and Comoé — all relatively untamed rivers navigable only short distances inland from the coast. In the north, many smaller tributaries change to dry streambeds between rains.\n\nThe Cavally River has its headwaters in the Nimba Mountains in Guinea and forms the border between Ivory Coast and Liberia for over half its length. It crosses rolling land and rapids and is navigable for about fifty kilometers inland from its exit to the sea near Cape Palmas.\n\nThe Sassandra River Basin has its source in the high ground of the north, where the Tiemba River joins the Férédougouba River, which flows from the Guinea highlands. It is joined by the Bagbé, Bafing, Nzo, Lobo, and Davo rivers and winds through shifting sandbars to form a narrow estuary, which is navigable for about eighty kilometers inland from the port of Sassandra.\n\nThe Bandama River, often referred to as the Bandama Blanc, is the longest in the country, joining the Bandama Rouge (also known as the Marahoué), Solomougou, Kan, and Nzi rivers over its 800-kilometer course. This large river system drains most of central Ivory Coast before it flows into the Tagba Lagoon opposite Grand-Lahou. During rainy seasons, small craft navigate the Bandama for fifty or sixty kilometers inland.\n\nEasternmost of the main rivers, the Comoé, formed by the Leraba and Gomonaba, has its sources in the Sikasso Plateau of Burkina Faso. It flows within a narrow 700-kilometer basin and receives the Kongo, and Iringou tributaries before winding among the coastal sandbars and emptying into the Ebrié Lagoon near Grand-Bassam. The Comoé is navigable for vessels of light draft for about fifty kilometers to Alépé.\n\nLarge dams were built in the 1960s and 1970s to control the flow of major rivers to the south. These projects created reservoirs, now referred to as lakes bearing the names of the dams- -Buyo on the Sassandra, Kossou and Taabo on the Bandama, and Ayamé on the small Bia River in the southeast corner of the country. Lake Kossou is the largest of these, occupying more than 1,600 square kilometers in the center of the country.\n\nThis list is arranged by drainage basin, with respective tributaries indented under each larger stream's name.\n\n\n"}
{"id": "33910664", "url": "https://en.wikipedia.org/wiki?curid=33910664", "title": "Lithoprotection", "text": "Lithoprotection\n\nLithoprotection is a term introduced in 2001 by Armenian biologist Tigran Tadevosyan as the name to the phenomenon where rock cover of a habitat diversifies local wildlife.\n\nThe word \"lithoprotection\" originates from the Greek root \"lithos\", meaning \"stone\", and the Latin root \"protectus\", meaning \"to cover\".\n\nOriginally an existence of a lithoprotection had been proposed based on observations of increased plant and animal diversity in habitats with rock cover compared to habitats without rock cover in the same conditions of an arid climate and steep slopes (Safarian, 1960; Tadevosyan, 2001). To explain this phenomenon, the author compared the life forms of plants, and the body sizes and escape strategies of animals inhabiting habitats with and without rock cover (Tadevosyan, 2001 - 2002). His conclusion was that many groups of vascular plants, including trees, shrubs, succulents, ferns, and moss species, linked to rocky substrates because they require higher than an area's average soil humidity, sometimes being shaded by a solid piece of rock, as well as strong attachment to the substrate. On the other hand, many relatively large animals (including larger lizards, snakes, some mammals and many bird species) need habitats with a dense network of shelters created by crevices and spaces between rocks in order survive overheating and predation. Thus, due to its ability to prevent the evaporation of humidity from the soil, to catch the seeds of plants, to hold trees and shrubs securely to the ground and to serve as a network of efficient shelters for many animals and shade-loving plants, rock cover lithoprotection is being considered as a protective element of a habitat biodiversity.\n\nManaging lithoprotection may be used as a measure of managing wildlife. Removal of a lithoprotection by cleaning an area of stone and rocks is a common practice in an urban landscape development. Because a lack of lithoprotection is equal to decreased biodiversity, this practice can be considered one of the core mechanisms of biodiversity loss in urban landscapes. Adding lithoprotection in contrary typically attracts wildlife and helps diversify and maintain biodiversity of a landscape (Tadevosyan, 2001). Bright examples of artificial use of lithoprotection are attractive rockaries, Japanese rock garden and other elements of a garden design.\n"}
{"id": "45395581", "url": "https://en.wikipedia.org/wiki?curid=45395581", "title": "Mata Amarilla Formation", "text": "Mata Amarilla Formation\n\nThe Mata Amarilla formation is a stratigraphic division of Patagonia, Argentina, consisting of rocks deposited between the Albian and Santonian ages. The middle section was previously considered to be the Pari Aike Formation.\n\nThe middle section of the Mata Amarilla Formation has widely been regarded as Maastrichtian in age, but recent dating of a lava tuff layer shows that it dates back to 96.2 ± 0.7 Ma, during the Cenomanian.\n\n"}
{"id": "50702025", "url": "https://en.wikipedia.org/wiki?curid=50702025", "title": "Material passport", "text": "Material passport\n\nA material passport is a document consisting of all the materials that are included in a product or construction. It consist of a set of data describing defined characteristics of materials in products, which give them value for recovery, recycling and re-use.\n\nThe core idea behind the concept is that a material passport will contribute to a more \"circular economy\", in which materials are being recovered, recycled and/or re-used in an open traded material market. The concept of the 'material passport’ is currently being developed by multiple parties in mainly European countries. A possible second-hand material market or material-bank could become a reality in the future.\n\nSimilar concepts are being developed by several parties. Other names for the material passport are:\n\n\"According to United Nations estimates, construction accounts for some 50 percent of raw material consumption in Europe and 60 percent of waste\" \n\nAssuming that the earth is a closed system, this situation is objectively untenable. There is an urgent need to go about raw materials in a more intelligent way. A shift in the building sector would greatly benefit a situation towards needing less material, and using material more effectively, e.g. by ensuring a much longer and more useful life cycle.\nProponents of the material passport argue that it is a step towards this direction.\n\nThe material passport gives material an identity. By acknowledging that it exists, in a given form in a specific building, it ensures that material receives and keeps a value, e.g. through a possible re-use after the deconstruction of for example a building.\n\nLike a personal passport, the material passport allows the material to ‘travel’, or identifies the most useful future destination after it has served in a building. This could be in another building or in another product altogether. \n\nBy recognizing the individual materials in buildings, new ownership structures could be facilitated, that would enable more functions to be offered as a service. As we now can have lighting as a service, we could have other functions, such as \"shelter from elements\" as a service, instead of owning a roof.\n\nIn general, material passports create incentives for suppliers to produce and developers / managers / renovators to choose healthy, sustainable and circular materials/building products. They fit into a broader and growing movement that aims at developing circular building business models.\n\nThe material passport can be applied to every product or construction. There are different levels in which a product/construct can be discomposed:\n\nFor a building, a material passport could be a complete description of all products (staircase, window, furnace, …), components (iron beam, glass panel, …), and raw materials (wood, steel, …),that are present in the building. Ideally, this database is created during construction and subsequently continuously kept up to date. In case an existing building does not yet have a material passport, it can be created through various methods (e.g. plan analysis, digital 3D scanning).\n\nA material passport allows the owner of a product/construct to know exactly what it is made of. This is of importance at the end of its useful life, to enable the most effective re-use of the materials. It allows the owner to view a product/construct as a depot, inventory of valuable materials.\n\nFurthermore, the process of creating a material passport also shapes the design of the building. The easier the materials can be extracted and re-used, the better. This will lead to an increase of ‘recoverable’ or ‘reversible’ buildings, buildings that can be dis-assembled as easily as they were assembled.\n\nAnother possibility is that a material passport enables the owner to get a better overview of value of the product/construct. Besides the value of the location and of the space, it can now also improve the valuation of the materials used. A higher, or more accurate, valuation of product/construct can be made possible.\n\n\nThe first scientific publication about a material passport (2012) was written by Maayke Damen and is called \"A resources passport for a circular economy\". It provides a comprehensive overview of the advantages and disadvantages of a material passport for every actor in the supply chain. It includes an outline for the content of a material passport. \n\n\n"}
{"id": "15445422", "url": "https://en.wikipedia.org/wiki?curid=15445422", "title": "Monterey Accelerated Research System", "text": "Monterey Accelerated Research System\n\nMonterey Accelerated Research System (MARS) is a cabled-based observatory system below the surface of Monterey Bay, developed and managed by the Monterey Bay Aquarium Research Institute. The system, operational since November 10, 2008, incorporates a undersea cable that carries data and power to benthic instrument nodes, AUVs, and various benthic and moored instrumentation.\n\n\n"}
{"id": "13360851", "url": "https://en.wikipedia.org/wiki?curid=13360851", "title": "Nature center", "text": "Nature center\n\nA nature center (or nature centre) is an organization with a visitor center or interpretive center designed to educate people about nature and the environment. Usually located within a protected open space, nature centers often have trails through their property. Some are located within a state or city park, and some have special gardens or an arboretum. Their properties can be characterized as nature preserves and wildlife sanctuaries. Nature centers generally display small live animals, such as reptiles, rodents, insects, or fish. There are often museum exhibits and displays about natural history, or preserved mounted animals or nature dioramas. Nature centers are staffed by paid or volunteer naturalists and most offer educational programs to the general public, as well as summer camp, after-school and school group programs.\n\nSome nature centers allow free admission but collect voluntary donations in order to help offset expenses. They usually rely on support from dedicated volunteers.\n\nEnvironmental education centers differ from nature centers in that their museum exhibits and education programs are available mostly by appointment, although casual visitors may be allowed to walk on their grounds.\n\nSome city, state and national parks have facilities similar to nature centers, such as museum exhibits, dioramas and trails, and some offer park nature education programs, usually presented by a park ranger.\n\n"}
{"id": "3871315", "url": "https://en.wikipedia.org/wiki?curid=3871315", "title": "Osun-Osogbo", "text": "Osun-Osogbo\n\nOsun-Osogbo or Osun-Osogbo Sacred Grove is a sacred forest along the banks of the Osun river just outside the city of Osogbo, Osun State, Nigeria.\n\nThe Osun-Osogbo Grove is among the last of the sacred forests which usually adjoined the edges of most Yoruba cities before extensive urbanization. In recognition of its global significance and its cultural value, the Sacred Grove was inscribed as a UNESCO World Heritage Site in 2005.\n\nThe 1950s saw the desecration of the Osun-Osogbo Grove: shrines were neglected, priests abandoned the grove as customary responsibilities and sanctions weakened. Prohibited actions like fishing, hunting and felling of trees in the grove took place until an Austrian, Susanne Wenger, came and stopped the abuse going on in the grove.\n\nWith the encouragement of the Ataoja and the support of the local people, \"Wenger formed the New Sacred Art movement to challenge land speculators, repel poachers, protect shrines and begin the long process of bringing the sacred place back to life by establishing it, again, as the sacred heart of Osogbo.\"\n\nEvery year, the Osun-Osgogbo festival is celebrated in the month of August at the grove. Yearly, the festival attracts thousands of Osun worshippers, spectators and tourists from all walks of life.\n\nFor the people of Osogbo Land, August is a month of celebration, traditional cleansing of the city and cultural reunion of the people with their ancestors and founders of the Osogbo Kingdom.\n\nThe Osun-Osogbo Festival is a two-week-long programme. It starts with the traditional cleansing of the town called 'Iwopopo', which is followed in three days by the lighting of the 500-year-old sixteen-point lamp called 'Ina Olojumerindinlogun'. Then comes the 'Ibroriade', an assemblage of the crowns of the past ruler, Ataojas of Osogbo, for blessings. This event is led by the sitting Ataoja of Osogbo and the Arugba, Yeye Osun and a committee of priestesses.\n\nAesthetics, according to general usage, is the perception of beauty. It is the appreciation of the beauty in all parts of life, but as a field of study it is mostly concerned with the arts. The aesthetics of the Osun Osogbo festival includes drumming, dancing, musical performing, wearing of elaborate costumes, speaking of the Yoruba language, recitation of praise poetry and so on.\n\nThese elements make the festival what it is, and adding pomp and colour to the proceedings. The festival is of immense benefit to the tourism sector of Nigeria. It enables the community to sell its culture to tourist coming from both within the country and all over the world.\n\nThe Osun Osogbo festival also serves as a strong unifying factor in Osogboland, as irrespective of the different social, economic, religious and political convictions of the people, they all come together annually to celebrate the goddess.\n\n\n"}
{"id": "9686849", "url": "https://en.wikipedia.org/wiki?curid=9686849", "title": "Overnight capital cost (power generation)", "text": "Overnight capital cost (power generation)\n\nOvernight capital cost is a term used in the power generation industry to describe the cost of building a power plant overnight. The term is useful to compare the economic feasibility of building various plants. The overnight capital cost does not take into account financing costs or escalation, and hence is not an actual estimate of construction cost.\n\nInvestors in the energy industry typically look to the levelized cost of energy (LCOE) for comparing generation technologies (e.g. solar power, natural gas) in the long term, as it includes ongoing fuel, maintenance, and operation costs. The U.S. Department of Energy tracks and makes publicly available levelized cost of energy figures for competing technologies. These figures will vary substantially in other countries due to different energy policies and domestic energy sources.\n"}
{"id": "4407057", "url": "https://en.wikipedia.org/wiki?curid=4407057", "title": "PAN Parks", "text": "PAN Parks\n\nThe PAN Parks Foundation was a non-governmental organisation that aimed to protect Europe's wildernesses. The foundation filled for bankruptcy in May 2014 in The Netherlands, but was denied the status by the court and is currently in liquidation.\n\nThe PAN Parks Foundation was founded in 1998 by the World Wide Fund for Nature and the Dutch travel company Molecaten, with the aim of creating national parks in Europe, along the model of the Yellowstone and Yosemite National Parks in North America. The organisation aims to create a network of European wilderness areas where wilderness and high quality tourism facilities are balanced with environmental protection and sustainable local development. It attempts to achieve this through a process of auditing and verification, enabling it to certify parks owned by partners as meeting particular standards, combined with political advocacy on the local and European level.\n\n"}
{"id": "649356", "url": "https://en.wikipedia.org/wiki?curid=649356", "title": "Putty", "text": "Putty\n\nPutty is a material with high plasticity, similar in texture to clay or dough, typically used in domestic construction and repair as a sealant or filler. Painter's Putty is typically a linseed oil-based product used for filling holes, minor cracks and defacements in wood only. Putties can also be made intumescent, in which case they are used for firestopping as well as for padding of electrical outlet boxes in fire-resistance rated drywall assemblies. In the latter case, hydrates in the putty produce an endothermic reaction to mitigate heat transfer to the unexposed side.\n\nPutty has been used extensively in glazing for fixing and sealing panes of glass into wooden frames (or sashes), although its use is decreasing with the prevalence of PVC and metal window frames which use synthetic sealants such as silicone. Glazing putty is traditionally made by mixing a base of whiting (finely ground chalk) with linseed oil in various proportions. There are a number of synthetic alternatives such as polybutene based putties, where the polybutene is a low molecular weight oligomer replacing the linseed oil. Butyl rubber is also added to the mixture to provide some strength and flexibility.\n\nIn woodworking, water-based putties are more commonly used, as these emit very little odour, are more easily cleaned up and are compatible with water-based and latex sealers.\n\nPlumber's putty is waterproof, used to make watertight seals in plumbing.\n\nPratley's Putty is an adhesive used primarily for steel bonding.\n\nCertain types of putty also have use in the field of terminal ballistics, where the putty can accurately represent the average density of the human body. As such it can be used, for instance, to test the penetrative power of projectiles, or the stopping power of body armour.\n\nPlay putty, such as \"silly putty,\" is for children to play with, and often comes in plastic eggs.\n\n\n"}
{"id": "24655120", "url": "https://en.wikipedia.org/wiki?curid=24655120", "title": "Renewable Energy Directive 2009", "text": "Renewable Energy Directive 2009\n\nThe Renewable Energy Directive 2009/28/EC is a European Union directive which mandates levels of renewable energy use within the European Union. The directive was published on 23 April 2009 and amends and repeals the 2001 Directive on Electricity Production from Renewable Energy Sources 2001/77/EC. The directive requires that 20% of the energy consumed within the European Union is renewable. This target is pooled among the member states.\n\nEU leaders had already reached agreement in March 2007 that, in principle, 20% of the bloc's final energy consumption should be produced from renewable energy sources by 2020 as part of its drive to cut carbon dioxide emissions. This policy later became part of the EU2020 Energy Strategy dated 10 November 2010. The key objectives of the strategy are to reduce carbon dioxide emissions by 20%, to increase the share of renewable energy to 20%, and to achieve energy savings of 20% or more. The targets are mutually dependent.\n\nThe draft report on the directive was published by the European Commission in January 2008. Claude Turmes served as rapporteur on the draft.\n\nMembers states were obliged to notify the European Commission by 30 June 2010 of a National Renewable Energy Action Plan which sets out the road map of the trajectory. Member states also have to submit progress reports explaining their implementation of the directive and their progress towards their targets, as is required by article22 of the directive.\n\nA June 2015 report from the European Commission shows that EU countries are on track to meet the aggregate 20% goal.\n\nThe overall EU target for renewable energy use is 20% by the year 2020. Targets for renewable energy in each country vary from a minimum of 10% in Malta to 72% of total energy use in Iceland.\n\nThe current renewable energy directive ends in 2020. a new directive is being negotiated.\n\nDocuments leaked in late-2016 reveal that a confidential European Union impact assessment analyses four scenarios for paring back the 'priority dispatch' system afforded to renewable generation in many countries. The assessment concludes that removing priority dispatch could increase carbon emissions by 45million to 60million tonnes per annum or up to 10%, with the aim of making European energy generators more flexible and cost-competitive. Priority dispatch is mandated under the current EU renewable energy directive, although the United Kingdom, the Netherlands, and Sweden do not comply. Industry sources told \"The Guardian\" that it is \"highly likely\" that priority dispatch will be removed from the next EU directive, which takes effect from 2020. Sources also said that renewable generators would seek financial compensation if priority dispatch is eliminated. The WindEurope trade association reacted strongly to the news.\n\n\n"}
{"id": "4615500", "url": "https://en.wikipedia.org/wiki?curid=4615500", "title": "Sea glass", "text": "Sea glass\n\nSea glass and beach glass are similar but come from two different types of water. \"Sea glass\" is physically and chemically weathered glass found on beaches along bodies of salt water. These weathering processes produce natural frosted glass. \"Genuine sea glass\" can be collected as a hobby and is used for decoration, most commonly in jewelry. \"Beach glass\" comes from fresh water and in most cases has a different pH balance and a less frosted appearance than sea glass. Sea glass takes 20 to 40 years, and sometimes as much as 100 years, to acquire its characteristic texture and shape. Sometimes also colloquially referred to as \"Drift glass\" from the longshore drift process that forms the smooth edges. In practice, the two terms are used interchangeably.\n\nSea glass begins as normal shards of broken glass that are then persistently tumbled and ground until the sharp edges are smoothed and rounded. In this process, the glass loses its slick surface but gains a frosted appearance over many years.\nNaturally produced sea glass (\"genuine sea glass\") originates as pieces of glass from broken bottles, broken tableware, or even shipwrecks, which are rolled and tumbled in the ocean for years until all of their edges are rounded off, and the slickness of the glass has been worn to a frosted appearance.\n\nArtificially produced sea glass (sometimes called \"beach glass\"), although superficially similar to sea glass, nevertheless has clear differences in appearance. Having not actually originated from the sea, most connoisseurs will not consider artificial \"sea\" glass to actually be genuine sea glass, but rather simply tumbled glass, where pieces of modern-day glass are tossed into a rock tumbler or dipped in acid to produce the desired finish. Artificially-produced, the glass is much less expensive and is used for making jewelry, but is often passed off as real sea glass.\n\n Sea glass can be found all over the world, but the beaches of the northeast United States, Bermuda, Fort Bragg, California, Benicia, California, North Carolina, Scotland, the Isle of Man, northeast and northwest England, Mexico, Hawaii, Dominican Republic, Puerto Rico, Nova Scotia, Australia, Italy and southern Spain are famous for their bounty of sea glass, bottles, bottle lips and stoppers, art glass, marbles, and pottery shards. The best times to look are during spring tides (especially the perigean and proxigean tides) and during the first low tide after a storm.\n\nGlass from inland waterways such as the Chesapeake Bay and the Great Lakes is known as beach glass. It is similar to sea glass, but in the absence of wave rigor and oceanic saline, content is typically less weathered. Beach glass from inland regions often has prominently embossed designs or letters on it, which can make tracing its origin less challenging. The outer surface of beach glass shards may also be texturally varied, with one side frosty and the other shiny. This is most likely because they are pieces broken off from larger glass objects which are themselves still embedded in mud, silt or clay, slowly being exposed by wave action and erosion.\n\nThe color of sea glass is determined by its original source. Most sea glass comes from bottles, but it can also come from jars, plates, windows, windshields, ceramics or sea pottery.\n\nThe most common colors of sea glass are kelly green, brown, white, and clear. These colors predominantly come from glass bottles mostly used by companies that sell beer, juices, soft drinks, and other beverages. The clear or white glass comes from clear plates and glasses, windshields, windows, and assorted other sources.\n\nLess common colours include jade, amber (from bottles for whiskey, medicine, spirits, and early bleach bottles), golden amber or amberina (mostly used for spirit bottles), lime green (from soda bottles during the 1960s), forest green, and ice- or soft blue (from soda bottles, medicine bottles, ink bottles, and fruit jars from the late 19th and early 20th centuries, windows, and windshields). These colors are found about once for every 25 to 100 pieces of sea glass found.\n\nUncommon colors of sea glass include a type of green, which comes primarily from early to mid-1900s Coca-Cola, Dr Pepper, and RC Cola bottles as well as beer bottles. Soft green colors could come from bottles that were used for ink, fruit, and baking soda. These colors are found once in every 50 to 100 pieces.\n\nPurple sea glass is very uncommon, as is citron, opaque white (from milk bottles), cobalt and cornflower blue (from early Milk of Magnesia bottles, poison bottles, artwork, Bromo-Seltzer and Vicks VapoRub containers), and aqua (from Ball Mason jars and certain 19th century glass bottles). These colors are found once for every 200 to 1,000 pieces found.\n\nExtremely rare colors include gray, pink (often from Great Depression-era plates), teal (often from Mateus wine bottles), black (older, very dark olive green glass), yellow (often from 1930s Vaseline containers), turquoise (from tableware and art glass), red (often from old Schlitz bottles, car tail lights, dinnerware, or nautical lights, it is found once in about every 5,000 pieces), and orange (the least common type of sea glass, found once in about 10,000 pieces). These colors are found once for every 1,000 to 10,000 pieces collected. Some shards of black glass are quite old, originating from thick eighteenth-century gin, beer, and wine bottles.\n\nOld black glass that had iron slag added during production to increase strength and opaqueness were at times broken in shipment. These broken bottles were jettisoned at the beachside wharf upon landfall. They originally contained such things as wine, gin, whiskey, medicines, and liquids subject to light damage. The bottles that survived the long and often rough voyage were refilled once emptied of the original contents and recycled, sometimes for many decades. They were refilled with local spirits, herbal tinctures, extracts, and medicinals.\n\nAs the glass craft gained knowledge the bottles became thinner. Quality improved in glass formulations and purity, as well as strides in technical improvements in glass-making techniques all aid in placing a piece of sea glass in a loosely defined era. Along with knowledge of historical trading partners that visited the search locations, production sources can be deduced, but a single feature is not a reliable indicator of origin. Many similar features over a broad sample narrow the choices. Particular substances became associated with certain color, and shapes of bottles. These all help define country of origin. Medicines and liquor were often sold in green bottles; olive green was the most common color for gin, but brown was also used as well as cobalt blue. Whiskey was contained in green and brown bottles as well. The liquor bottles for sea shipment were oftentimes square so as to use space more efficiently in shipping crates. Poisons were almost always in blue bottles. Round, mallet, and squat cylinder black glass bottles were all made, and shape correlates to age of the bottles with some overlap. Correlations exist today: brown beer bottles, and of course the globally recognized shape of the Coca-Cola green glass bottle.\n\nIn the Caribbean, many nations traded, and pirates plundered them all. So glass from many producer countries can be found with examples going back to the 15th century. Old slave trading ports are good search locations. As are former colonial ports in the slave-molasses-rum triangle. All former colonial locations with sea trade routes, and the motherland shipping ports are prime search areas.\n\nFor instance, in Jamaica there are influxes of Spanish, African, English, American, East Indian, Chinese, and Jews (who incidentally arrived about the same time Christopher Columbus' son arrived to settle the island in 1510 and many later shipped with the Caribbean pirate crews) and other Europeans scattered throughout the historical record. The majority of black glass found on the island of Jamaica is English glass produced from the later 1600s until the 1800s in England. Sea glass arrived in Jamaica with the old world supply chain first established in the 15th century. The Spanish considered Santiago (Jamaica) a backwater to Cuba, and this allowed the English to relieve them of it in 1655 without much resistance. The first man-made glass most likely arrived with Christopher Columbus on his 2nd voyage when he claimed the island for Spain in 1494. He is reported to have landed at Dry Harbor, Discovery Bay on the north coast. Whether any of the glass on board became sea glass is part of the romance and wonder of beachcombing.\n\nBlack glass is often green or brown when held up to light, although it appears black to the unaided eye. Weathering and oxidation, together with UV light interacting with metallic oxides and chemicals in the glass and seawater are all factors affecting the color of sea glass over long exposure and time frames. In texture and color black sea glass resembles black beach rock, very much resembling the extrusive igneous rock basalt, or weathered black obsidian, a natural black volcanic glass. Gas bubbles are often trapped in old glass, impurities and irregularities in the original bottles were common and one indicator of age. Early examples were hand blown, later ones utilized a mold. Due to the inherent strength, you'll find larger size pieces of old black glass, including pieces that survived for centuries. Potential age of black beach glass found depends on your search location. Small pieces require a trained eye to spot. Texture is a good tool to employ along with color. Black glass is the rarest of all the sea/beach glasses due to age and difficulty in finding it. Although when you find an old, long used location it can produce a considerable amount of material.\n\nCollectors should be aware of the historical nature of the location, and the fact that gathering old items is considered cultural theft in some areas of the world. In other areas sea glass is just another piece of trash on the beach. Most sea glass' historical context is lost to the sea, but its mere presence in some areas is the heritage and legacy of the location. National treasures made of former trash exist, such as is Glass Beach at Fort Bragg CA. The glass at Ft. Bragg is mostly from the mid-20th century.\n\nLike collecting shells, fossils, or stones, combing shorelines for sea glass is a hobby many beach-goers and beachcombers enjoy. Hobbyists often fill decorative jars with their collections and take great pleasure in tracing a shard's provenance while artisans craft pieces of jewelry, stained glass and other decorative pieces from sea glass. Some collectors use their collections in creating works of art by putting them in cement or other adhesive to create a mosaic.\n\nIn North America, the hobby has the North American Sea Glass Association, which organizes a yearly conference and issues a newsletter.\n\nAuthentic sea and beach glass is becoming rarer and harder to find. More people are actively searching for it, and the shift to other materials such as plastic for containers has greatly reduced the number of glass containers dumped in the sea.\n\nThis scarcity has led to some artisans and crafters tumbling poorer pieces of sea glass shards to create what is called \"twice-tossed\" glass, while others create artificial sea glass, or \"craft glass\", from ordinary glass pieces using a rock tumbler. While such glass is chunkier than most true sea glass, lacks its romantic provenance, and differs in many technical ways (e.g., long-term exposure to water conditions creates an etched surface on the glass that cannot be duplicated artificially), it does meet the demand of crafters at a cheaper price and in a wider range of colors.\n\nA number of characteristics highlight the differences between artificial sea glass and natural sea glass, starting with the coloration and surface texture of each piece.\n\n"}
{"id": "19849627", "url": "https://en.wikipedia.org/wiki?curid=19849627", "title": "Shape waves", "text": "Shape waves\n\nShape waves are excitations propagating along Josephson vortices or fluxons. In the case of two-dimensional Josephson junctions (thick long Josephson junctions with an extra dimension) described by the 2D sine-Gordon equation, shape waves are distortions of a Josephson vortex line of an arbitrary profile. Shape waves have remarkable properties exhibiting Lorentz contraction and time dilation similar to that in special relativity. Position of the shape wave excitation on a Josephson vortex acts like a “minute-hand” showing the time in the rest-frame associated with the vortex. At some conditions, a moving vortex with the shape excitation can have less energy than the same vortex without it. \n"}
{"id": "2165732", "url": "https://en.wikipedia.org/wiki?curid=2165732", "title": "Snag (ecology)", "text": "Snag (ecology)\n\nIn forest ecology, a snag refers to a standing, dead or dying tree, often missing a top or most of the smaller branches. In freshwater ecology it refers to trees, branches, and other pieces of naturally occurring wood found sunken in rivers and streams; it is also known as coarse woody debris. When used in manufacturing, especially in Scandinavia, they are often called \"dead wood\" and in Finland \"kelo wood\".\n\nSnags are an important structural component in forest communities, making up 10–20% of all trees present in old-growth tropical, temperate, and boreal forests. Snags and downed coarse woody debris represent a large portion of the woody biomass in a healthy forest.\n\nIn temperate forests, snags provide critical habitat for more than 100 species of bird and mammal, and snags are often called 'wildlife trees' by foresters. Dead, decaying wood supports a rich community of decomposers like bacteria and fungi, insects, and other invertebrates. These organisms and their consumers, along with the structural complexity of cavities, hollows, and broken tops make snags important habitat for birds, bats, and small mammals, which in turn feed larger mammalian predators.\n\nSnags are optimal habitat for primary cavity nesters such as woodpeckers which create the majority of cavities used by secondary cavity users in forest ecosystems. Woodpeckers excavate cavities for more than 80 other species and the health of their populations relies on snags. Most snag-dependent birds and mammals are insectivorous and represent a major portion of the insectivorous forest fauna, and are important factors in controlling forest insect populations. There are many instances in which birds reduced outbreak populations of forest insects, such as woodpeckers affecting outbreaks of southern hardwood borers and Engelmann spruce beetles.\nSnag creation occurs naturally as trees die due to old age, disease, drought, or wildfire. A snag undergoes a series of changes from the time the tree dies until final collapse, and each stage in the decay process has particular value to certain wildlife species. Snag persistence depends on two factors, the size of the stem, and the durability of the wood of the species concerned. The snags of some large conifers, such as Giant Sequoia and Coast Redwood on the Pacific Coast of North America, and the Alerce of Patagonia, can remain intact for 100 years or more, becoming progressively shorter with age, while other snags with rapidly decaying wood, such as aspen and birch, break up and collapse in 2–10 years.\n\nSnag Forests, or Complex Early Seral Forests, are ecosystems that occupy potentially forested sites after a stand-replacement disturbance and before re-establishment of a closed-forest canopy. They are generated by natural disturbances such as wildfire or insect outbreaks that reset ecological succession processes and follow a pathway that is influenced by biological legacies (e.g., large live trees and snags downed logs, seed banks, resprout tissue, fungi, and other live and dead biomass) that were not removed during the initial disturbance.\n\nWater hunting birds like the osprey or kingfishers can be found near water, perched in a snag tree, or feeding upon their fish catch.\n\nIn freshwater ecology in Australia and the United States, the term snag is used to refer to the trees, branches and other pieces of naturally occurring wood found in a sunken form in rivers and streams. Such snags have been identified as being critical for shelter and as spawning sites for fish, and are one of the few hard substrates available for biofilm growth supporting aquatic invertebrates in lowland rivers flowing through alluvial flood plains. Snags are important as sites for biofilm growth and for shelter and feeding of aquatic invertebrates in both lowland and upland rivers and streams.In Australia, the role of freshwater snags has been largely ignored until recently, and more than one million snags have been removed from the Murray-Darling basin. Large tracts of the lowland reaches of the Murray-Darling system are now devoid of the snags that native fish like Murray cod require for shelter and breeding. The damage such wholesale snag removal has caused is clearly enormous, but is difficult to quantify (but see ). Most snags in these systems are river red gum snags. As the dense wood of river red gum is almost impervious to rot it is thought that some of the river red gum snags removed in past decades may have been several thousand years old.\n\nAlso known as \"deadheads\", partially submerged snags posed hazards to early riverboat navigation and commerce. If hit, snags punctured the wooden hulls used in the 19th century and early 20th century. Snags were, in fact, the most commonly encountered hazard, especially in the early years of steamboat travel. In the United States, the U.S. Army Corps of Engineers operated \"snagboats\" such as the \"W. T. Preston\" in the Puget Sound of Washington State and the \"Montgomery\" in the rivers of Alabama to pull out and clear snags. Starting in 1824, there were successful efforts to remove snags from the Mississippi and its tributaries. By 1835, a lieutenant reported to the Chief of Engineers that steamboat travel had become much safer, but by the mid-1840s the appropriations for snag removal dried up and snags re-accumulated until after the Civil War.\n\nIn Scandinavia and Finland snags, invariably pine trees, known in Finnish as \"kelo\" and in Swedish as \"torraka\", are collected for the production of different objects, from furniture to entire log houses. Commercial enterprises market them abroad as \"dead wood\" or in Finland as \"kelo wood\". They have been especially prized for their silver-grey weathered surface in the manufacture of vernacular or national romantic products. The suppliers of \"dead wood\" emphasise its age: the wood has developed with dehydration in the dry coldness of the subarctic zones, the tree having stopped growing after some 300–400 years, and the tree has remained upright for another few hundred years. \"Dead wood\" logs are easier to transport and handle than normal logs due to their lightness.\n\n"}
{"id": "34686580", "url": "https://en.wikipedia.org/wiki?curid=34686580", "title": "Table of years in the environment", "text": "Table of years in the environment\n\nThis is a table of articles relating to the environment by year. They relate to environmental law, conservation, environmentalism and environmental issues.\n\n1900 1901 1902 \n1903 1904 1905 \n1906 1907 1908 \n1909<br>\n1910 1911 1912 \n1913 1914 1915 \n1916 1917 1918 \n1919<br>\n1920 1921 1922 \n1923 1924 1925 \n1926 1927 1928 \n1929<br>\n1930 1931 1932 \n1933 1934 1935 \n1936 1937 1938 \n1939<br>\n1940 1941 1942 \n1943 1944 1945 \n1946 1947 1948 \n1949<br>\n1950 1951 1952 \n1953 1954 1955 \n1956 1957 1958 \n1959<br>\n1960 1961 1962 \n1963 1964 1965 \n1966 1967 1968 \n1969<br>\n1970 1971 1972 \n1973 1974 1975 \n1976 1977 1978 \n1979<br>\n1980 1981 1982 \n1983 1984 1985 \n1986 1987 1988 \n1989<br>\n1990 1991 1992 \n1993 1994 1995 \n1996 1997 1998 \n1999<br>\n\n2000 2001 2002 \n2003 2004 2005 \n2006 2007 2008 \n2009 <br>\n2010 2011 2012 \n2013 2014 2015 \n2016 \n\n"}
{"id": "47463", "url": "https://en.wikipedia.org/wiki?curid=47463", "title": "Thermosphere", "text": "Thermosphere\n\nThe thermosphere is the layer of the Earth's atmosphere directly above the mesosphere and below the exosphere. Within this layer of the atmosphere, ultraviolet radiation causes photoionization/photodissociation of molecules, creating ions in the ionosphere. Taking its name from the Greek θερμός (pronounced \"thermos\") meaning heat, the thermosphere begins at about 80 km (50 mi) above sea level. At these high altitudes, the residual atmospheric gases sort into strata according to molecular mass (see turbosphere). Thermospheric temperatures increase with altitude due to absorption of highly energetic solar radiation. Temperatures are highly dependent on solar activity, and can rise to 1,700 °C (3,100 °F) or more. Radiation causes the atmosphere particles in this layer to become electrically charged (see ionosphere), enabling radio waves to be refracted and thus be received beyond the horizon. In the exosphere, beginning at about 600 km (375 mi) above sea level, the atmosphere turns into space, although by the criteria set for the definition of the Kármán line, the thermosphere itself is part of space.\n\nThe highly diluted gas in this layer can reach during the day. Despite the high temperature, an observer or object will experience cold temperatures in the thermosphere, because the extremely low density of gas (practically a hard vacuum) is insufficient for the molecules to conduct heat. A normal thermometer will read significantly below , at least at night, because the energy lost by thermal radiation would exceed the energy acquired from the atmospheric gas by direct contact. In the anacoustic zone above , the density is so low that molecular interactions are too infrequent to permit the transmission of sound. \nThe dynamics of the thermosphere are dominated by atmospheric tides, which are driven by the very significant diurnal heating. Atmospheric waves dissipate above this level because of collisions between the neutral gas and the ionospheric plasma.\n\nThe International Space Station orbits the Earth within the middle of the thermosphere, between .\n\nIt is convenient to separate the atmospheric regions according to the two temperature minima at about 12 km altitude (the tropopause) and at about 85 km (the mesopause) (Figure 1). The thermosphere (or the upper atmosphere) is the height region above 85 km, while the region between the tropopause and the mesopause is the middle atmosphere (stratosphere and mesosphere) where absorption of solar UV radiation generates the temperature maximum near 45 km altitude and causes the ozone layer.\nThe density of the Earth's atmosphere decreases nearly exponentially with altitude. The total mass of the atmosphere is M = ρ H  ≃ 1 kg/cm within a column of one square centimeter above the ground (with ρ = 1.29 kg/m the atmospheric density on the ground at z = 0 m altitude, and H ≃ 8 km the average atmospheric scale height). 80% of that mass is concentrated within the troposphere. The mass of the thermosphere above about 85 km is only 0.002% of the total mass. Therefore, no significant energetic feedback from the thermosphere to the lower atmospheric regions can be expected.\n\nTurbulence causes the air within the lower atmospheric regions below the turbopause at about 110 km to be a mixture of gases that does not change its composition. Its mean molecular weight is 29 g/mol with molecular oxygen (O) and nitrogen (N) as the two dominant constituents. Above the turbopause, however, diffusive separation of the various constituents is significant, so that each constituent follows its own barometric height structure with a scale height inversely proportional to its molecular weight. The lighter constituents atomic oxygen (O), helium (He), and hydrogen (H) successively dominate above about 200 km altitude and vary with geographic location, time, and solar activity. The ratio\nN/O which is a measure of the electron density at the ionospheric F region is highly affected by these variations. These changes follow from the diffusion of the minor constituents through the major gas component during dynamic processes.\n\nThe thermosphere contains an appreciable concentration of elemental sodium located in a 10-km thick band that occurs at the edge of the mesosphere, 80 to 100 km above Earth's surface. The sodium has an average concentration of 400,000 atoms per cubic centimeter. This band is regularly replenished by sodium sublimating from incoming meteors. Astronomers have begun utilizing this sodium band to create \"guide stars\" as part of the optical correction process in producing ultra-sharp ground-based observations.\n\nThe thermospheric temperature can be determined from density observations as well as from direct satellite measurements. The temperature vs. altitude z in Fig. 1 can be simulated by the so-called Bates profile:\n\n(1) formula_1\n\nwith T the exospheric temperature above about 400 km altitude, \nT = 355 K, and z = 120 km reference temperature and height, and s an empirical parameter depending on T and decreasing with T. That formula is derived from a simple equation of heat conduction. One estimates a total heat input of q≃ 0.8 to 1.6 mW/m above z = 120 km altitude. In order to obtain equilibrium conditions, that heat input q above z is lost to the lower atmospheric regions by heat conduction.\n\nThe exospheric temperature T is a fair measurement of the solar XUV radiation. Since solar radio emission F at 10.7 cm wavelength is a good indicator of solar activity, one can apply the empirical formula for quiet magnetospheric conditions.\n\n(2) formula_2\n\nwith T in K, F in 10 W m Hz (the Covington index) a value of F averaged over several solar cycles. The Covington index varies typically between 70 and 250 during a solar cycle, and never drops below about 50. Thus, T varies between about 740 and 1350 K. During very quiet magnetospheric conditions, the still continuously flowing magnetospheric energy input contributes by about 250 K to the residual temperature of 500 K in eq.(2). The rest of 250 K in eq.(2) can be attributed to atmospheric waves generated within the troposphere and dissipated within the lower thermosphere.\n\nThe solar X-ray and extreme ultraviolet radiation (XUV) at wavelengths < 170 nm is almost completely absorbed within the thermosphere. This radiation causes the various ionospheric layers as well as a temperature increase at these heights (Figure 1).\nWhile the solar visible light (380 to 780 nm) is nearly constant with a variability of not more than about 0.1% of the solar constant, the solar XUV radiation is highly variable in time and space. For instance, X-ray bursts associated with solar flares can dramatically increase their intensity over preflare levels by many orders of magnitude over a time span of tens of minutes. In the extreme ultraviolet, the Lyman α line at 121.6 nm represents an important source of ionization and dissociation at ionospheric D layer heights. During quiet periods of solar activity, it alone contains more energy than the rest of the XUV spectrum. Quasi-periodic changes of the order of 100% or greater, with periods of 27 days and 11 years, belong to the prominent variations of solar XUV radiation. However, irregular fluctuations over all time scales are present all the time. During low solar activity, about half of the total energy input into the thermosphere is thought to be solar XUV radiation. Evidently, that solar XUV energy input occurs only during daytime conditions, maximizing at the equator during equinox.\n\nA second source of energy input into the thermosphere is solar wind energy which is transferred to the magnetosphere by mechanisms that are not well understood. One possible way to transfer energy is via a hydrodynamic dynamo process. Solar wind particles penetrate into the polar regions of the magnetosphere where the geomagnetic field lines are essentially vertically directed. An electric field is generated, directed from dawn to dusk. Along the last closed geomagnetic field lines with their footpoints within the auroral zones, field aligned electric currents can flow into the ionospheric dynamo region where they are closed by electric Pedersen and Hall currents. Ohmic losses of the Pedersen currents heat the lower thermosphere (see e.g., Magnetospheric electric convection field). In addition, penetration of high energetic particles from the magnetosphere into the auroral regions enhance drastically the electric conductivity, further increasing the electric currents and thus Joule heating. During quiet magnetospheric activity, the magnetosphere contributes perhaps by a quarter to the thermosphere's energy budget. This is about 250 K of the exospheric temperature in eq.(2). During very large activity, however, this heat input can increase substantially, by a factor of four or more. That solar wind input occurs mainly in the auroral regions during both day and night.\n\nTwo kinds of large-scale atmospheric waves within the lower atmosphere exist: internal waves with finite vertical wavelengths which can transport wave energy upward; and external waves with infinitely large wavelengths which cannot transport wave energy. Atmospheric gravity waves and most of the atmospheric tides generated within the troposphere belong to the internal waves. Their density amplitudes increase exponentially with height, so that at the mesopause these waves become turbulent and their energy is dissipated (similar to breaking of ocean waves at the coast), thus contributing to the heating of the thermosphere by about 250 K in eq.(2). On the other hand, the fundamental diurnal tide labelled (1, −2) which is most efficiently excited by solar irradiance is an external wave and plays only a marginal role within lower and middle atmosphere. However, at thermospheric altitudes, it becomes the predominant wave. It drives the electric Sq-current within the ionospheric dynamo region between about 100 and 200 km height.\n\nHeating, predominately by tidal waves, occurs mainly at lower and middle latitudes. The variability of this heating depends on the meteorological conditions within troposphere and middle atmosphere, and may not exceed about 50%.\n\nWithin the thermosphere above about 150 km height, all atmospheric waves successively become external waves, and no significant vertical wave structure is visible. The atmospheric wave modes degenerate to the spherical functions P with m a meridional wave number and n the zonal wave number (m = 0: zonal mean flow; m = 1: diurnal tides; m = 2: semidiurnal tides; etc.). The thermosphere becomes a damped oscillator system with low-pass filter characteristics. This means that smaller-scale waves (greater numbers of (n,m)) and higher frequencies are suppressed in favor of large-scale waves and lower frequencies. If one considers very quiet magnetospheric disturbances and a constant mean exospheric temperature (averaged over the sphere), the observed temporal and spatial distribution of the exospheric temperature distribution can be described by a sum of spheric functions:\n\n(3) formula_3\n\nHere, it is φ latitude, λ longitude, and t time, ω the angular frequency of one year, ω the angular frequency of one solar day, and τ = ωt + λ the local time. t = June 21 is the date of northern summer solstice, and τ = 15:00 is the local time of maximum diurnal temperature.\n\nThe first term in (3) on the right is the global mean of the exospheric temperature (of the order of 1000 K). The second term [with P = 0.5(3 sin(φ)−1)] represents the heat surplus at lower latitudes and a corresponding heat deficit at higher latitudes (Fig. 2a). A thermal wind system develops with wind toward the poles in the upper level and wind away from the poles in the lower level. The coefficient ΔT ≈ 0.004 is small because Joule heating in the aurora regions compensates that heat surplus even during quiet magnetospheric conditions. During disturbed conditions, however, that term becomes dominant, changing sign so that now heat surplus is transported from the poles to the equator. The third term (with P = sin φ) represents heat surplus on the summer hemisphere and is responsible for the transport of excess heat from the summer into the winter hemisphere (Fig. 2b). Its relative amplitude is of the order ΔT ≃ 0.13. The fourth term (with P(φ) = cos φ) is the dominant diurnal wave (the tidal mode (1,−2)). It is responsible for the transport of excess heat from the daytime hemisphere into the nighttime hemisphere (Fig. 2d). Its relative amplitude is ΔT≃ 0.15, thus on the order of 150 K. Additional terms (e.g., semiannual, semidiurnal terms and higher order terms) must be added to eq.(3). However, they are of minor importance. Corresponding sums can be developed for density, pressure, and the various gas constituents.\n\nIn contrast to solar XUV radiation, magnetospheric disturbances, indicated on the ground by geomagnetic variations, show an unpredictable impulsive character, from short periodic disturbances of the order of hours to long-standing giant storms of several days' duration. The reaction of the thermosphere to a large magnetospheric storm is called thermospheric storm. Since the heat input into the thermosphere occurs at high latitudes (mainly into the auroral regions), the heat transport represented by the term P in eq.(3) is reversed. In addition, due to the impulsive form of the disturbance, higher-order terms are generated which, however, possess short decay times and thus quickly disappear. The sum of these modes determines the \"travel time\" of the disturbance to the lower latitudes, and thus the response time of the thermosphere with respect to the magnetospheric disturbance. Important for the development of an ionospheric storm is the increase of the ratio N/O during a thermospheric storm at middle and higher latitude. An increase of N increases the loss process of the ionospheric plasma and causes therefore a decrease of the electron density within the ionospheric F-layer (negative ionospheric storm).\n"}
{"id": "72726", "url": "https://en.wikipedia.org/wiki?curid=72726", "title": "Thrust fault", "text": "Thrust fault\n\nA thrust fault is a break in the Earth's crust, across which older rocks are pushed above younger rocks.\n\nA thrust fault is a type of reverse fault that has a dip of 45 degrees or less.\n\nIf the angle of the fault plane is lower (often less than 15 degrees from the horizontal) and the displacement of the overlying block is large (often in the kilometer range) the fault is called an \"overthrust\" or \"overthrust fault\". Erosion can remove part of the overlying block, creating a \"fenster\" (or \"window\") – when the underlying block is exposed only in a relatively small area. When erosion removes most of the overlying block, leaving island-like remnants resting on the lower block, the remnants are called \"klippen\" (singular \"klippe\").\n\nIf the fault plane terminates before it reaches the Earth's surface, it is referred to as a \"blind thrust\" fault. Because of the lack of surface evidence, blind thrust faults are difficult to detect until they rupture. The destructive 1994 quake in Northridge, California, was caused by a previously undiscovered blind thrust fault.\n\nBecause of their low dip, thrusts are also difficult to appreciate in mapping, where lithological offsets are generally subtle and stratigraphic repetition is difficult to detect, especially in peneplain areas.\n\nThrust faults, particularly those involved in thin-skinned style of deformation, have a so-called \"ramp-flat\" geometry. Thrusts mostly propagate along zones of weakness within a sedimentary sequence, such as mudstones or salt layers, these parts of the thrust are called \"decollements\". If the effectiveness of the decollement becomes reduced, the thrust will tend to cut up the section to a higher stratigraphic level until it reaches another effective decollement where it can continue as bedding parallel flat. The part of the thrust linking the two flats is known as a \"ramp\" and typically forms at an angle of about 15°-30° to the bedding. Continued displacement on a thrust over a ramp produces a characteristic fold geometry known as a \"ramp anticline\" or, more generally, as a \"fault-bend fold\".\n\nFault-propagation folds form at the tip of a thrust fault where propagation along the decollement has ceased but displacement on the thrust behind the fault tip is continuing. The continuing displacement is accommodated by formation of an asymmetric anticline-syncline fold pair. As displacement continues the thrust tip starts to propagate along the axis of the syncline. Such structures are also known as \"tip-line folds\". Eventually the propagating thrust tip may reach another effective decollement layer and a composite fold structure will develop with characteristics of both fault-bend and fault-propagation folds.\n\nDuplexes occur where there are two decollement levels close to each other within a sedimentary sequence, such as the top and base of a relatively strong sandstone layer bounded by two relatively weak mudstone layers. When a thrust that has propagated along the lower detachment, known as the \"floor thrust\", cuts up to the upper detachment, known as the \"roof thrust\", it forms a ramp within the stronger layer. With continued displacement on the thrust, higher stresses are developed in the footwall of the ramp due to the bend on the fault. This may cause renewed propagation along the floor thrust until it again cuts up to join the roof thrust. Further displacement then takes place via the newly created ramp. This process may repeat many times, forming a series of fault bounded thrust slices known as \"imbricates\" or horses, each with the geometry of a fault-bend fold of small displacement. The final result is typically a lozenge shaped duplex.\n\nMost duplexes have only small displacements on the bounding faults between the horses and these dip away from the foreland. Occasionally the displacement on the individual horses is greater, such that each horse lies more or less vertically above the other, this is known as an \"antiformal stack\" or \"imbricate stack\". If the individual displacements are greater still, then the horses have a foreland dip.\n\nDuplexing is a very efficient mechanism of accommodating shortening of the crust by thickening the section rather than by folding and deformation.\n\nLarge overthrust faults occur in areas that have undergone great compressional forces.\n\nThese conditions exist in the orogenic belts that result from either two continental tectonic collisions or from subduction zone accretion.\n\nThe resultant compressional forces produce mountain ranges. The Himalayas, the Alps, and the Appalachians are prominent examples of compressional orogenies with numerous overthrust faults.\n\nThrust faults occur in the foreland basin which occur marginal to orogenic belts. Here, compression does not result in appreciable mountain building, which is mostly accommodated by folding and stacking of thrusts. Instead thrust faults generally cause a thickening of the stratigraphic section.\n\nForeland basin thrusts also usually observe the ramp-flat geometry, with thrusts propagating within units at a very low angle \"flats\" (at 1–5 degrees) and then moving up-section in steeper ramps (at 5–20 degrees) where they offset stratigraphic units. Identifying ramps where they occur within units is usually problematic.\n\nThrusts and duplexes are also found in accretionary wedges in the ocean trench margin of subduction zones, where oceanic sediments are scraped off the subducted plate and accumulate. Here, the accretionary wedge must thicken by up to 200% and this is achieved by stacking thrust fault upon thrust fault in a melange of disrupted rock, often with chaotic folding. Here, ramp flat geometries are not usually observed because the compressional force is at a steep angle to the sedimentary layering.\n\nThrust faults were unrecognised until the work of Arnold Escher von der Linth, Albert Heim and Marcel Alexandre Bertrand in the Alps working on the Glarus Thrust; Charles Lapworth, Ben Peach and John Horne working on parts of the Moine Thrust Scotland; Alfred Elis Törnebohm in the Scandinavian Caledonides and R. G. McConnell in the Canadian Rockies. The realisation that older strata could, via faulting, be found above younger strata, was arrived at more or less independently by geologists in all these areas during the 1880s. Geikie in 1884 coined the term \"thrust-plane\" to describe this special set of faults. He wrote:\nBy a system of reversed faults, a group of strata is made to cover a great breadth of ground and actually to overlie higher members of the same series. The most extraordinary dislocations, however, are those to which for distinction we have given the name of Thrust-planes. They are strictly reversed faults, but with so low a hade that the rocks on their upthrown side have been, as it were, pushed horizontally forward.\n\n"}
{"id": "473167", "url": "https://en.wikipedia.org/wiki?curid=473167", "title": "Traveling-wave tube", "text": "Traveling-wave tube\n\nA traveling-wave tube (TWT, pronounced \"twit\") or traveling-wave tube amplifier (TWTA, pronounced \"tweeta\") is a specialized vacuum tube that is used in electronics to amplify radio frequency (RF) signals in the microwave range. The TWT belongs to a category of \"linear beam\" tubes, such as the klystron, in which the radio wave is amplified by absorbing power from a beam of electrons as it passes down the tube. Although there are various types of TWT, two major categories are:\nA major advantage of the TWT over some other microwave tubes is its ability to amplify a wide range of frequencies, a wide bandwidth. The bandwidth of the helix TWT can be as high as two octaves, while the cavity versions have bandwidths of 10–20%. Operating frequencies range from 300 MHz to 50 GHz. The power gain of the tube is on the order of 40 to 70 decibels, and output power ranges from a few watts to megawatts.\n\nTWTs account for over 50% of the sales volume of all microwave vacuum tubes. They are widely used as the power amplifiers and oscillators in radar systems, communication satellite and spacecraft transmitters, and electronic warfare systems.\n\nThe TWT is an elongated vacuum tube with an electron gun (a heated cathode that emits electrons) at one end. A voltage applied across the cathode and anode accelerates the electrons towards the far end of the tube, and an external magnetic field around the tube focuses the electrons into a beam. At the other end of the tube the electrons strike the \"collector\", which returns them to the circuit.\n\nWrapped around the inside of the tube, just outside the beam path, is a helix of wire, typically oxygen-free copper. The RF signal to be amplified is fed into the helix at a point near the emitter end of the tube. The signal is normally fed into the helix via a waveguide or electromagnetic coil placed at one end, forming a one-way signal path, a directional coupler.\n\nBy controlling the accelerating voltage, the speed of the electrons flowing down the tube is set to be similar to the speed of the RF signal running down the helix. The signal in the wire causes a magnetic field to be induced in the center of the helix, where the electrons are flowing. Depending on the phase of the signal, the electrons will either be sped up or slowed down as they pass the windings. This causes the electron beam to \"bunch up\", known technically as \"velocity modulation\". The resulting pattern of electron density in the beam is an analog of the original RF signal.\n\nBecause the beam is passing the helix as it travels, and that signal varies, it causes induction in the helix, amplifying the original signal. By the time it reaches the other end of the tube, this process has had time to deposit considerable energy back into the helix. A second directional coupler, positioned near the collector, receives an amplified version of the input signal from the far end of the RF circuit. Attenuators placed along the RF circuit prevent the reflected wave from traveling back to the cathode.\n\nHigher powered helix TWTs usually contain beryllium oxide ceramic as both a helix support rod and in some cases, as an electron collector for the TWT because of its special electrical, mechanical, and thermal properties.\n\nThere are a number of RF amplifier tubes that operate in a similar fashion to the TWT, known collectively as velocity-modulated tubes. The best known example is the klystron. All of these tubes use the same basic \"bunching\" of electrons to provide the amplification process, and differ largely in what process causes the velocity modulation to occur.\n\nIn the klystron, the electron beam passes through a hole in a resonant cavity which is connected to the source RF signal. The signal at the instant the electrons pass through the hole causes them to be accelerated (or decelerated). The electrons enter a \"drift tube\" in which faster electrons overtake the slower ones, creating the bunches, after which the electrons pass through another resonant cavity from which the output power is taken. Since the velocity sorting process takes time, the drift tube must often be several feet long.\n\nIn comparison, in the TWT the acceleration is caused by the interactions with the helix along the entire length of the tube. This allows the TWT to have a very low noise output, a major advantage of the design. More usefully, this process is much less sensitive to the physical arrangement of the tube, which allows the TWT to operate over a wider variety of frequencies. TWT's are generally at an advantage when low noise and frequency variability are useful.\n\nHelix TWTs are limited in peak RF power by the current handling (and therefore thickness) of the helix wire. As power level increases, the wire can overheat and cause the helix geometry to warp. Wire thickness can be increased to improve matters, but if the wire is too thick it becomes impossible to obtain the required helix pitch for proper operation. Typically helix TWTs achieve less than 2.5 kW output power.\n\nThe coupled-cavity TWT overcomes this limit by replacing the helix with a series of coupled cavities arranged axially along the beam. This structure provides a helical waveguide, and hence amplification can occur via velocity modulation. Helical waveguides have very nonlinear dispersion and thus are only narrowband (but wider than klystron). A coupled-cavity TWT can achieve 60 kW output power.\n\nOperation is similar to that of a klystron, except that coupled-cavity TWTs are designed with attenuation between the slow-wave structure instead of a drift tube. The slow-wave structure gives the TWT its wide bandwidth. A free electron laser allows higher frequencies.\n\nA TWT integrated with a regulated power supply and protection circuits is referred to as a traveling-wave-tube amplifier (abbreviated TWTA and often pronounced \"TWEET-uh\"). It is used to produce high-power radio frequency signals. The bandwidth of a broadband TWTA can be as high as one octave, although tuned (narrowband) versions exist; operating frequencies range from 300 MHz to 50 GHz.\n\nA TWTA consists of a traveling-wave tube coupled with its protection circuits (as in klystron) and regulated power supply electronic power conditioner (EPC), which may be supplied and integrated by a different manufacturer. The main difference between most power supplies and those for vacuum tubes is that efficient vacuum tubes have depressed collectors to recycle kinetic energy of the electrons, so the secondary winding of the power supply needs up to 6 taps of which the helix voltage needs precise regulation. The subsequent addition of a linearizer (as for inductive output tube) can, by complementary compensation, improve the gain compression and other characteristics of the TWTA; this combination is called a linearized TWTA (LTWTA, \"EL-tweet-uh\").\n\nBroadband TWTAs generally use a helix TWT and achieve less than 2.5 kW output power. TWTAs using a coupled cavity TWT can achieve 15 kW output power, but at the expense of bandwidth.\n\nThe original design and prototype of the TWT was done by Andrei \"Andy\" Haeff c. 1931 while he was working as a doctoral student at the Kellogg Radiation Laboratory at Caltech. His original patent, \"Device for and Method of Controlling High Frequency Currents\", was filed in 1933 and granted in 1936.\n\nThe invention of the TWT is often attributed to Rudolf Kompfner in 1942–1943. In addition, Nils Lindenblad, working at RCA (Radio Corporation of America) in the USA also filed a patent for a device in May 1940 that was remarkably similar to Kompfner's TWT. Both of these devices were improvements over Haeff's original design as they both used the then newly invented precision electron gun as the source of the electron beam and they both directed the beam down the center of the helix instead of outside of it. These configuration changes resulted in much greater wave amplification than Haeff's design as they relied on the physical principles of velocity modulation and electron bunching. Kompfner developed his TWT in a British Admiralty radar laboratory during World War II. His first sketch of his TWT is dated November 12, 1942, and he built his first TWT in early 1943.\nThe TWT was later refined by Kompfner, John R. Pierce, and Lester M. Field at Bell Labs. Note that Kompfner's US patent, granted in 1953, does cite Haeff's previous work.\n\nBy the 1950s, after further development at the Electron Tube Laboratory at Hughes Aircraft Company in Culver City, California, TWTs went into production there, and by the 1960s TWTs were also produced by such companies as the English Electric Valve Company, followed by Ferranti in the 1970s.\n\nOn July 10, 1962, the first communications satellite, Telstar 1, was launched with a 2 W, 4 GHz RCA-designed TWT transponder used for transmitting RF signals to Earth stations. Syncom 2 was successfully launched into geosynchronous orbit on July 26, 1963 with two 2 W, 1850 MHz Hughes-designed TWT transponders — one active and one spare.\n\nTWTAs are commonly used as amplifiers in satellite transponders, where the input signal is very weak and the output needs to be high power.\n\nA TWTA whose output drives an antenna is a type of transmitter. TWTA transmitters are used extensively in radar, particularly in airborne fire-control radar systems, and in electronic warfare and self-protection systems. In such applications, a control grid is typically introduced between the TWT's electron gun and slow-wave structure to allow pulsed operation. The circuit that drives the control grid is usually referred to as a grid modulator.\n\nAnother major use of TWTAs is for the electromagnetic compatibility (EMC) testing industry for immunity testing of electronic devices.\n\nTWTAs can often be found in older (pre-1995) aviation SSR microwave transponders.\n\n\n\n"}
{"id": "8282374", "url": "https://en.wikipedia.org/wiki?curid=8282374", "title": "Tropical cyclone", "text": "Tropical cyclone\n\nA tropical cyclone is a rapidly rotating storm system characterized by a low-pressure center, a closed low-level atmospheric circulation, strong winds, and a spiral arrangement of thunderstorms that produce heavy rain. Depending on its location and strength, a tropical cyclone is referred to by different names, including hurricane (), typhoon (), tropical storm, cyclonic storm, tropical depression, and simply cyclone. A hurricane is a tropical cyclone that occurs in the Atlantic Ocean and northeastern Pacific Ocean, and a typhoon occurs in the northwestern Pacific Ocean; while in the south Pacific or Indian Ocean, comparable storms are referred to simply as \"tropical cyclones\" or \"severe cyclonic storms\".\n\n\"Tropical\" refers to the geographical origin of these systems, which form almost exclusively over tropical seas. \"Cyclone\" refers to their winds moving in a circle, whirling round their central clear eye, with their winds blowing counterclockwise in the Northern Hemisphere and clockwise in the Southern Hemisphere. The opposite direction of circulation is due to the Coriolis effect. Tropical cyclones typically form over large bodies of relatively warm water. They derive their energy through the evaporation of water from the ocean surface, which ultimately recondenses into clouds and rain when moist air rises and cools to saturation. This energy source differs from that of mid-latitude cyclonic storms, such as nor'easters and European windstorms, which are fueled primarily by horizontal temperature contrasts. Tropical cyclones are typically between in diameter.\n\nThe strong rotating winds of a tropical cyclone are a result of the conservation of angular momentum imparted by the Earth's rotation as air flows inwards toward the axis of rotation. As a result, they rarely form within 5° of the equator. Tropical cyclones are almost unknown in the South Atlantic due to a consistently strong wind shear and a weak Intertropical Convergence Zone. Also, the African easterly jet and areas of atmospheric instability which give rise to cyclones in the Atlantic Ocean and Caribbean Sea, along with the Asian monsoon and Western Pacific Warm Pool, are features of the Northern Hemisphere and Australia.\n\nCoastal regions are particularly vulnerable to the impact of a tropical cyclone, compared to inland regions. The primary energy source for these storms is warm ocean waters, therefore these forms are typically strongest when over or near water, and weaken quite rapidly over land. Coastal damage may be caused by strong winds and rain, high waves (due to winds), storm surges (due to wind and severe pressure changes), and the potential of spawning tornadoes. Tropical cyclones also draw in air from a large area—which can be a vast area for the most severe cyclones—and concentrate the precipitation of the water content in that air (made up from atmospheric moisture and moisture evaporated from water) into a much smaller area. This continual replacement of moisture-bearing air by new moisture-bearing air after its moisture has fallen as rain, which may cause extremely heavy rain and river flooding up to from the coastline, far beyond the amount of water that the local atmosphere holds at any one time.\n\nThough their effects on human populations are often devastating, tropical cyclones can relieve drought conditions. They also carry heat energy away from the tropics and transport it toward temperate latitudes, which may play an important role in modulating regional and global climate.\n\nTropical cyclones are areas of relatively low pressure in the troposphere, with the largest pressure perturbations occurring at low altitudes near the surface. On Earth, the pressures recorded at the centers of tropical cyclones are among the lowest ever observed at sea level. The environment near the center of tropical cyclones is warmer than the surroundings at all altitudes, thus they are characterized as \"warm core\" systems.\n\nThe near-surface wind field of a tropical cyclone is characterized by air rotating rapidly around a center of circulation while also flowing radially inwards. At the outer edge of the storm, air may be nearly calm; however, due to the Earth’s rotation, the air has non-zero absolute angular momentum. As air flows radially inward, it begins to rotate cyclonically (counter-clockwise in the Northern Hemisphere, and clockwise in the Southern Hemisphere) in order to conserve angular momentum. At an inner radius, air begins to ascend to the top of the troposphere. This radius is typically coincident with the inner radius of the eyewall, and has the strongest near-surface winds of the storm; consequently, it is known as the \"radius of maximum winds\". Once aloft, air flows away from the storm's center, producing a shield of cirrus clouds.\n\nThe previously mentioned processes result in a wind field that is nearly axisymmetric: Wind speeds are low at the center, increase rapidly moving outwards to the radius of maximum winds, and then decay more gradually with radius to large radii. However, the wind field often exhibits additional spatial and temporal variability due to the effects of localized processes, such as thunderstorm activity and horizontal flow instabilities. In the vertical direction, winds are strongest near the surface and decay with height within the troposphere.\n\nAt the center of a mature tropical cyclone, air sinks rather than rises. For a sufficiently strong storm, air may sink over a layer deep enough to suppress cloud formation, thereby creating a clear \"eye\". Weather in the eye is normally calm and free of clouds, although the sea may be extremely violent. The eye is normally circular in shape, and is typically in diameter, though eyes as small as and as large as have been observed.\n\nThe cloudy outer edge of the eye is called the \"eyewall\". The eyewall typically expands outward with height, resembling an arena football stadium; this phenomenon is sometimes referred to as the \"stadium effect\". The eyewall is where the greatest wind speeds are found, air rises most rapidly, clouds reach to their highest altitude, and precipitation is the heaviest. The heaviest wind damage occurs where a tropical cyclone's eyewall passes over land.\n\nIn a weaker storm, the eye may be obscured by the central dense overcast, which is the upper-level cirrus shield that is associated with a concentrated area of strong thunderstorm activity near the center of a tropical cyclone.\n\nThe eyewall may vary over time in the form of eyewall replacement cycles, particularly in intense tropical cyclones. Outer rainbands can organize into an outer ring of thunderstorms that slowly moves inward, which is believed to rob the primary eyewall of moisture and angular momentum. When the primary eyewall weakens, the tropical cyclone weakens temporarily. The outer eyewall eventually replaces the primary one at the end of the cycle, at which time the storm may return to its original intensity.\n\nOn occasion, tropical cyclones may undergo a process known as rapid deepening, a period in which the minimum sea-level pressure of a tropical cyclone decreases by 42mb in a 24-hour period. In order for rapid deepening to occur, several conditions must be in place. Water temperatures must be extremely high (near or above 30 °C, 86 °F), and water of this temperature must be sufficiently deep such that waves do not upwell cooler waters to the surface. Wind shear must be low; when wind shear is high, the convection and circulation in the cyclone will be disrupted. Usually, an anticyclone in the upper layers of the troposphere above the storm must be present as well—for extremely low surface pressures to develop, air must be rising very rapidly in the eyewall of the storm, and an upper-level anticyclone helps channel this air away from the cyclone efficiently.\n\nThere are a variety of metrics commonly used to measure storm size. The most common metrics include the radius of maximum wind, the radius of 34-knot wind (i.e. gale force), the radius of outermost closed isobar (ROCI), and the radius of vanishing wind. An additional metric is the radius at which the cyclone's relative vorticity field decreases to 1×10 s.\n\nOn Earth, tropical cyclones span a large range of sizes, from as measured by the radius of vanishing wind. They are largest on average in the northwest Pacific Ocean basin and smallest in the northeastern Pacific Ocean basin. If the radius of outermost closed isobar is less than two degrees of latitude (), then the cyclone is \"very small\" or a \"midget\". A radius of 3–6 latitude degrees () is considered \"average sized\". \"Very large\" tropical cyclones have a radius of greater than 8 degrees (). Observations indicate that size is only weakly correlated to variables such as storm intensity (i.e. maximum wind speed), radius of maximum wind, latitude, and maximum potential intensity.\n\nSize plays an important role in modulating damage caused by a storm. All else equal, a larger storm will impact a larger area for a longer period of time. Additionally, a larger near-surface wind field can generate higher storm surge due to the combination of longer wind fetch, longer duration, and enhanced wave setup.\n\nThe upper circulation of strong hurricanes extends into the tropopause of the atmosphere, which at low latitudes is .\n\nThe three-dimensional wind field in a tropical cyclone can be separated into two components: a \"primary circulation\" and a \"secondary circulation\". The primary circulation is the rotational part of the flow; it is purely circular. The secondary circulation is the overturning (in-up-out-down) part of the flow; it is in the radial and vertical directions. The primary circulation is larger in magnitude, dominating the surface wind field, and is responsible for the majority of the damage a storm causes, while the secondary circulation is slower but governs the energetics of the storm.\n\nA tropical cyclone's primary energy source is heat from the evaporation of water from the surface of a warm ocean, previously heated by sunshine. The energetics of the system may be idealized as an atmospheric Carnot heat engine. First, inflowing air near the surface acquires heat primarily via evaporation of water (i.e. latent heat) at the temperature of the warm ocean surface (during evaporation, the ocean cools and the air warms). Second, the warmed air rises and cools within the eyewall while conserving total heat content (latent heat is simply converted to sensible heat during condensation). Third, air outflows and loses heat via infrared radiation to space at the temperature of the cold tropopause. Finally, air subsides and warms at the outer edge of the storm while conserving total heat content. The first and third legs are nearly isothermal, while the second and fourth legs are nearly isentropic. This in-up-out-down overturning flow is known as the secondary circulation. The Carnot perspective provides an upper bound on the maximum wind speed that a storm can attain.\n\nScientists estimate that a tropical cyclone releases heat energy at the rate of 50 to 200 exajoules (10 J) per day, equivalent to about 1 PW (10 watt). This rate of energy release is equivalent to 70 times the world energy consumption of humans and 200 times the worldwide electrical generating capacity, or to exploding a 10-megaton nuclear bomb every 20 minutes.\n\nThe primary rotating flow in a tropical cyclone results from the conservation of angular momentum by the secondary circulation. Absolute angular momentum on a rotating planet formula_1 is given by\n\nwhere formula_3 is the Coriolis parameter, formula_4 is the azimuthal (i.e. rotating) wind speed, and formula_5 is the radius to the axis of rotation. The first term on the right hand side is the component of planetary angular momentum that projects onto the local vertical (i.e. the axis of rotation). The second term on the right hand side is the relative angular momentum of the circulation itself with respect to the axis of rotation. Because the planetary angular momentum term vanishes at the equator (where formula_6 ), tropical cyclones rarely form within 5° of the equator.\n\nAs air flows radially inward at low levels, it begins to rotate cyclonically in order to conserve angular momentum. Similarly, as rapidly rotating air flows radially outward near the tropopause, its cyclonic rotation decreases and ultimately changes sign at large enough radius, resulting in an upper-level anti-cyclone. The result is a vertical structure characterized by a strong cyclone at low levels and a strong anti-cyclone near the tropopause; from thermal wind balance, this corresponds to a system that is warmer at its center than in the surrounding environment at all altitudes (i.e. \"warm-core\"). From hydrostatic balance, the warm core translates to lower pressure at the center at all altitudes, with the maximum pressure drop located at the surface.\n\nDue to surface friction, the inflow only partially conserves angular momentum. Thus, the sea surface lower boundary acts as both a source (evaporation) and sink (friction) of energy for the system. This fact leads to the existence of a theoretical upper bound on the strongest wind speed that a tropical cyclone can attain. Because evaporation increases linearly with wind speed (just as climbing out of a pool feels much colder on a windy day), there is a positive feedback on energy input into the system known as the Wind-Induced Surface Heat Exchange (WISHE) feedback. This feedback is offset when frictional dissipation, which increases with the cube of the wind speed, becomes sufficiently large. This upper bound is called the \"maximum potential intensity\", formula_7, and is given by\n\nwhere formula_9 is the temperature of the sea surface, formula_10 is the temperature of the outflow ([K]), formula_11 is the enthalpy difference between the surface and the overlying air ([J/kg]), and formula_12 and formula_13 are the surface exchange coefficients (dimensionless) of enthalpy and momentum, respectively. The surface-air enthalpy difference is taken as formula_14, where formula_15 is the saturation enthalpy of air at sea surface temperature and sea-level pressure and formula_16 is the enthalpy of boundary layer air overlying the surface.\n\nThe maximum potential intensity is predominantly a function of the background environment alone (i.e. without a tropical cyclone), and thus this quantity can be used to determine which regions on Earth can support tropical cyclones of a given intensity, and how these regions may evolve in time. Specifically, the maximum potential intensity has three components, but its variability in space and time is due predominantly to the variability in the surface-air enthalpy difference component formula_11.\n\nA tropical cyclone may be viewed as a heat engine that converts input heat energy from the surface into mechanical energy that can be used to do mechanical work against surface friction. At equilibrium, the rate of net energy production in the system must equal the rate of energy loss due to frictional dissipation at the surface, i.e.\n\nThe rate of energy loss per unit surface area from surface friction, formula_19, is given by\n\nwhere formula_21 is the density of near-surface air ([kg/m]) and formula_22 is the near surface wind speed ([m/s]).\n\nThe rate of energy production per unit surface area, formula_23 is given by\n\nwhere formula_25 is the heat engine efficiency and formula_26 is the total rate of heat input into the system per unit surface area. Given that a tropical cyclone may be idealized as a , the Carnot heat engine efficiency is given by\n\nHeat (enthalpy) per unit mass is given by\n\nwhere formula_29 is the heat capacity of air, formula_30 is air temperature, formula_31 is the latent heat of vaporization, and formula_32 is the concentration of water vapor. The first component corresponds to sensible heat and the second to latent heat.\n\nThere are two sources of heat input. The dominant source is the input of heat at the surface, primarily due to evaporation. The bulk aerodynamic formula for the rate of heat input per unit area at the surface, formula_33, is given by\n\nwhere formula_14 represents the enthalpy difference between the ocean surface and the overlying air. The second source is the internal sensible heat generated from frictional dissipation (equal to formula_19), which occurs near the surface within the tropical cyclone and is recycled to the system.\n\nThus, the total rate of net energy production per unit surface area is given by\n\nSetting formula_18 and taking formula_40 (i.e. the rotational wind speed is dominant) leads to the solution for formula_7 given above. This derivation assumes that total energy input and loss within the system can be approximated by their values at the radius of maximum wind. The inclusion of formula_42 acts to multiply the total heat input rate by the factor formula_43. Mathematically, this has the effect of replacing formula_9 with formula_10 in the denominator of the Carnot efficiency.\n\nAn alternative definition for the maximum potential intensity, which is mathematically equivalent to the above formulation, is\n\nwhere CAPE stands for the Convective Available Potential Energy, formula_47 is the CAPE of an air parcel lifted from saturation at sea level in reference to the environmental sounding, formula_48 is the CAPE of the boundary layer air, and both quantities are calculated at the radius of maximum wind.\n\nOn Earth, a characteristic temperature for formula_9 is 300 K and for formula_10 is 200 K, corresponding to a Carnot efficiency of formula_51. The ratio of the surface exchange coefficients, formula_52, is typically taken to be 1. However, observations suggest that the drag coefficient formula_13 varies with wind speed and may decrease at high wind speeds within the boundary layer of a mature hurricane. Additionally, formula_12 may vary at high wind speeds due to the effect of sea spray on evaporation within the boundary layer.\n\nA characteristic value of the maximum potential intensity, formula_7, is . However, this quantity varies significantly across space and time, particularly within the seasonal cycle, spanning a range of . This variability is primarily due to variability in the surface enthalpy disequilibrium ( formula_11 ) as well as in the thermodynamic structure of the troposphere, which are controlled by the large-scale dynamics of the tropical climate. These processes are modulated by factors including the sea surface temperature (and underlying ocean dynamics), background near-surface wind speed, and the vertical structure of atmospheric radiative heating. The nature of this modulation is complex, particularly on climate time-scales (decades or longer). On shorter time-scales, variability in the maximum potential intensity is commonly linked to sea surface temperature perturbations from the tropical mean, as regions with relatively warm water have thermodynamic states much more capable of sustaining a tropical cyclone than regions with relatively cold water. However, this relationship is indirect via the large-scale dynamics of the tropics; the direct influence of the absolute sea surface temperature on formula_7 is weak in comparison.\n\nThe passage of a tropical cyclone over the ocean causes the upper layers of the ocean to cool substantially, which can influence subsequent cyclone development. This cooling is primarily caused by wind-driven mixing of cold water from deeper in the ocean with the warm surface waters. This effect results in a negative feedback process that can inhibit further development or lead to weakening. Additional cooling may come in the form of cold water from falling raindrops (this is because the atmosphere is cooler at higher altitudes). Cloud cover may also play a role in cooling the ocean, by shielding the ocean surface from direct sunlight before and slightly after the storm passage. All these effects can combine to produce a dramatic drop in sea surface temperature over a large area in just a few days. Coversely, the mixing of the sea can result in heat being inserted in deeper waters, with potential effects on global climate.\n\nThere are six Regional Specialized Meteorological Centers (RSMCs) worldwide. These organizations are designated by the World Meteorological Organization and are responsible for tracking and issuing bulletins, warnings, and advisories about tropical cyclones in their designated areas of responsibility. In addition, there are six Tropical Cyclone Warning Centers (TCWCs) that provide information to smaller regions.\n\nThe RSMCs and TCWCs are not the only organizations that provide information about tropical cyclones to the public. The Joint Typhoon Warning Center (JTWC) issues advisories in all basins except the Northern Atlantic for the purposes of the United States Government. The Philippine Atmospheric, Geophysical and Astronomical Services Administration (PAGASA) issues advisories and names for tropical cyclones that approach the Philippines in the Northwestern Pacific to protect the life and property of its citizens. The Canadian Hurricane Center (CHC) issues advisories on hurricanes and their remnants for Canadian citizens when they affect Canada.\n\nOn March 26, 2004, Hurricane Catarina became the first recorded South Atlantic cyclone, striking southern Brazil with winds equivalent to Category 2 on the Saffir-Simpson Hurricane Scale. As the cyclone formed outside the authority of another warning center, Brazilian meteorologists initially treated the system as an extratropical cyclone, but later on classified it as tropical.\n\nWorldwide, tropical cyclone activity peaks in late summer, when the difference between temperatures aloft and sea surface temperatures is the greatest. However, each particular basin has its own seasonal patterns. On a worldwide scale, May is the least active month, while September is the most active month. November is the only month in which all the tropical cyclone basins are active.\n\nIn the Northern Atlantic Ocean, a distinct cyclone season occurs from June 1 to November 30, sharply peaking from late August through September. The statistical peak of the Atlantic hurricane season is September 10. The Northeast Pacific Ocean has a broader period of activity, but in a similar time frame to the Atlantic. The Northwest Pacific sees tropical cyclones year-round, with a minimum in February and March and a peak in early September. In the North Indian basin, storms are most common from April to December, with peaks in May and November. In the Southern Hemisphere, the tropical cyclone year begins on July 1 and runs all year-round encompassing the tropical cyclone seasons, which run from November 1 until the end of April, with peaks in mid-February to early March.\n\nThe formation of tropical cyclones is the topic of extensive ongoing research and is still not fully understood. While six factors appear to be generally necessary, tropical cyclones may occasionally form without meeting all of the following conditions. In most situations, water temperatures of at least are needed down to a depth of at least ; waters of this temperature cause the overlying atmosphere to be unstable enough to sustain convection and thunderstorms. For tropical transitioning cyclones (i.e. Hurricane Ophelia (2017)) a water temperature of at least has been suggested.\n\nAnother factor is rapid cooling with height, which allows the release of the heat of condensation that powers a tropical cyclone. High humidity is needed, especially in the lower-to-mid troposphere; when there is a great deal of moisture in the atmosphere, conditions are more favorable for disturbances to develop. Low amounts of wind shear are needed, as high shear is disruptive to the storm's circulation. Tropical cyclones generally need to form more than or five degrees of latitude away from the equator, allowing the Coriolis effect to deflect winds blowing towards the low pressure center and creating a circulation. Lastly, a formative tropical cyclone needs a preexisting system of disturbed weather. Tropical cyclones will not form spontaneously. Low-latitude and low-level westerly wind bursts associated with the Madden–Julian oscillation can create favorable conditions for tropical cyclogenesis by initiating tropical disturbances.\n\nMost tropical cyclones form in a worldwide band of thunderstorm activity near the equator, referred to as the Intertropical Front (ITF), the Intertropical Convergence Zone (ITCZ), or the monsoon trough. Another important source of atmospheric instability is found in tropical waves, which contribute to the development of about 85% of intense tropical cyclones in the Atlantic Ocean and become most of the tropical cyclones in the Eastern Pacific. The majority forms between 10 and 30 degrees of latitude away of the equator, and 87% forms no farther away than 20 degrees north or south. Because the Coriolis effect initiates and maintains their rotation, tropical cyclones rarely form or move within 5 degrees of the equator, where the effect is weakest. However, it is still possible for tropical systems to form within this boundary as Tropical Storm Vamei and Cyclone Agni did in 2001 and 2004, respectively.\n\nThe movement of a tropical cyclone (i.e. its \"track\") is typically approximated as the sum of two terms: \"steering\" by the background environmental wind and \"beta drift\".\n\nEnvironmental steering is the dominant term. Conceptually, it represents the movement of the storm due to prevailing winds and other wider environmental conditions, similar to \"leaves carried along by a stream\". Physically, the winds, or flow field, in the vicinity of a tropical cyclone may be treated as having two parts: the flow associated with the storm itself, and the large-scale background flow of the environment in which the storm takes place. In this way, tropical cyclone motion may be represented to first-order simply as advection of the storm by the local environmental flow. This environmental flow is termed the \"steering flow\".\n\nClimatologically, tropical cyclones are steered primarily westward by the east-to-west trade winds on the equatorial side of the subtropical ridge—a persistent high-pressure area over the world's subtropical oceans. In the tropical North Atlantic and Northeast Pacific oceans, the trade winds steer tropical easterly waves westward from the African coast toward the Caribbean Sea, North America, and ultimately into the central Pacific Ocean before the waves dampen out. These waves are the precursors to many tropical cyclones within this region. In contrast, in the Indian Ocean and Western Pacific in both hemispheres, tropical cyclogenesis is influenced less by tropical easterly waves and more by the seasonal movement of the Inter-tropical Convergence Zone and the monsoon trough. Additionally, tropical cyclone motion can be influenced by transient weather systems, such as extratropical cyclones.\n\nIn addition to environmental steering, a tropical cyclone will tend to drift slowly poleward and westward, a motion known as \"beta drift\". This motion is due to the superposition of a vortex, such as a tropical cyclone, onto an environment in which the Coriolis force varies with latitude, such as on a sphere or beta plane. It is induced indirectly by the storm itself, the result of a feedback between the cyclonic flow of the storm and its environment.\n\nPhysically, the cyclonic circulation of the storm advects environmental air poleward east of center and equatorial west of center. Because air must conserve its angular momentum, this flow configuration induces a cyclonic gyre equatorward and westward of the storm center and an anticyclonic gyre poleward and eastward of the storm center. The combined flow of these gyres acts to advect the storm slowly poleward and westward. This effect occurs even if there is zero environmental flow.\n\nA third component of motion that occurs relatively infrequently involves the interaction of multiple tropical cyclones. When two cyclones approach one another, their centers will begin orbiting cyclonically about a point between the two systems. Depending on their separation distance and strength, the two vortices may simply orbit around one another or else may spiral into the center point and merge. When the two vortices are of unequal size, the larger vortex will tend to dominate the interaction, and the smaller vortex will orbit around it. This phenomenon is called the Fujiwhara effect, after Sakuhei Fujiwhara.\n\nThough a tropical cyclone typically moves from east to west in the tropics, its track may shift poleward and eastward either as it moves west of the subtropical ridge axis or else if it interacts with the mid-latitude flow, such as the jet stream or an extratropical cyclone. This motion, termed \"recurvature\", commonly occurs near the western edge of the major ocean basins, where the jet stream typically has a poleward component and extratropical cyclones are common. An example of tropical cyclone recurvature was Typhoon Ioke in 2006.\n\nThe landfall of a tropical cyclone occurs when a storm's surface center moves over a coastline. Storm conditions may be experienced on the coast and inland hours before landfall; in fact, a tropical cyclone can launch its strongest winds over land, yet not make landfall. NOAA uses the term \"direct hit\" to describe when a location (on the left side of the eye) falls within the radius of maximum winds (or twice that radius if on the right side), whether or not the hurricane's eye made landfall.\n\nMost tropical cyclones form on the side of the subtropical ridge closer to the equator, then move poleward past the ridge axis before recurving into the main belt of the Westerlies. When the subtropical ridge position shifts due to El Niño, so will the preferred tropical cyclone tracks. Areas west of Japan and Korea tend to experience much fewer September–November tropical cyclone impacts during El Niño and neutral years. During El Niño years, the break in the subtropical ridge tends to lie near 130°E which would favor the Japanese archipelago. During El Niño years, Guam's chance of a tropical cyclone impact is one-third more likely than of the long-term average. The tropical Atlantic Ocean experiences depressed activity due to increased vertical wind shear across the region during El Niño years. During La Niña years, the formation of tropical cyclones, along with the subtropical ridge position, shifts westward across the western Pacific Ocean, which increases the landfall threat to China and much greater intensity in the Philippines.\n\nA tropical cyclone can cease to have tropical characteristics in several different ways. One such way is if it moves over land, thus depriving it of the warm water it needs to power itself, quickly losing strength. Most strong storms lose their strength very rapidly after landfall and become disorganized areas of low pressure within a day or two, or evolve into extratropical cyclones. There is a chance a tropical cyclone could regenerate if it managed to get back over open warm water, such as with Hurricane Ivan. If it remains over mountains for even a short time, weakening will accelerate. Many storm fatalities occur in mountainous terrain, when diminishing cyclones unleash their moisture as torrential rainfall. This rainfall may lead to deadly floods and mudslides, as was the case with Hurricane Mitch around Honduras in October 1998. Without warm surface water, the storm cannot survive.\n\nA tropical cyclone can dissipate when it moves over waters significantly below . This will cause the storm to lose its tropical characteristics, such as a warm core with thunderstorms near the center, and become a remnant low-pressure area. These remnant systems may persist for up to several days before losing their identity. This dissipation mechanism is most common in the eastern North Pacific. Weakening or dissipation can occur if it experiences vertical wind shear, causing the convection and heat engine to move away from the center; this normally ceases development of a tropical cyclone. In addition, its interaction with the main belt of the Westerlies, by means of merging with a nearby frontal zone, can cause tropical cyclones to evolve into extratropical cyclones. This transition can take 1–3 days. Even after a tropical cyclone is said to be extratropical or dissipated, it can still have tropical storm force (or occasionally hurricane/typhoon force) winds and drop several inches of rainfall. In the Pacific Ocean and Atlantic Ocean, such tropical-derived cyclones of higher latitudes can be violent and may occasionally remain at hurricane or typhoon-force wind speeds when they reach the west coast of North America. These phenomena can also affect Europe, where they are known as \"European windstorms\"; Hurricane Iris's extratropical remnants are an example of such a windstorm from 1995. A cyclone can also merge with another area of low pressure, becoming a larger area of low pressure. This can strengthen the resultant system, although it may no longer be a tropical cyclone. Studies in the 2000s have given rise to the hypothesis that large amounts of dust reduce the strength of tropical cyclones.\n\nIn the 1960s and 1970s, the United States government attempted to weaken hurricanes through Project Stormfury by seeding selected storms with silver iodide. It was thought that the seeding would cause supercooled water in the outer rainbands to freeze, causing the inner eyewall to collapse and thus reducing the winds. The winds of Hurricane Debbie—a hurricane seeded in Project Stormfury—dropped as much as 31%, but Debbie regained its strength after each of two seeding forays. In an earlier episode in 1947, disaster struck when a hurricane east of Jacksonville, Florida promptly changed its course after being seeded, and smashed into Savannah, Georgia. Because there was so much uncertainty about the behavior of these storms, the federal government would not approve seeding operations unless the hurricane had a less than 10% chance of making landfall within 48 hours, greatly reducing the number of possible test storms. The project was dropped after it was discovered that eyewall replacement cycles occur naturally in strong hurricanes, casting doubt on the result of the earlier attempts. Today, it is known that silver iodide seeding is not likely to have an effect because the amount of supercooled water in the rainbands of a tropical cyclone is too low.\n\nOther approaches have been suggested over time, including cooling the water under a tropical cyclone by towing icebergs into the tropical oceans. Other ideas range from covering the ocean in a substance that inhibits evaporation, dropping large quantities of ice into the eye at very early stages of development (so that the latent heat is absorbed by the ice, instead of being converted to kinetic energy that would feed the positive feedback loop), or blasting the cyclone apart with nuclear weapons. Project Cirrus even involved throwing dry ice on a cyclone. These approaches all suffer from one flaw above many others: tropical cyclones are simply too large and long-lived for any of the weakening techniques to be practical.\n\nTropical cyclones out at sea cause large waves, heavy rain, flood and high winds, disrupting international shipping and, at times, causing shipwrecks. Tropical cyclones stir up water, leaving a cool wake behind them, which causes the region to be less favorable for subsequent tropical cyclones. On land, strong winds can damage or destroy vehicles, buildings, bridges, and other outside objects, turning loose debris into deadly flying projectiles. The storm surge, or the increase in sea level due to the cyclone, is typically the worst effect from landfalling tropical cyclones, historically resulting in 90% of tropical cyclone deaths.\nThe broad rotation of a landfalling tropical cyclone, and vertical wind shear at its periphery, spawns tornadoes. Tornadoes can also be spawned as a result of eyewall mesovortices, which persist until landfall.\n\nOver the past two centuries, tropical cyclones have been responsible for the deaths of about 1.9 million people worldwide. Large areas of standing water caused by flooding lead to infection, as well as contributing to mosquito-borne illnesses. Crowded evacuees in shelters increase the risk of disease propagation. Tropical cyclones significantly interrupt infrastructure, leading to power outages, bridge destruction, and the hampering of reconstruction efforts. On average, the Gulf and east coasts of the United States suffer approximately US $5 billion (1995 US $) in cyclone damage every year. The majority (83%) of tropical cyclone damage is caused by severe hurricanes, category 3 or greater. However, category 3 or greater hurricanes only account for about one-fifth of cyclones that make landfall every year.\n\nAlthough cyclones take an enormous toll in lives and personal property, they may be important factors in the precipitation regimes of places they impact, as they may bring much-needed precipitation to otherwise dry regions. Tropical cyclones also help maintain the global heat balance by moving warm, moist tropical air to the middle latitudes and polar regions, and by regulating the thermohaline circulation through upwelling. The storm surge and winds of hurricanes may be destructive to human-made structures, but they also stir up the waters of coastal estuaries, which are typically important fish breeding locales. Tropical cyclone destruction spurs redevelopment, greatly increasing local property values.\n\nWhen hurricanes surge upon shore from the ocean, salt is introduced to many freshwater areas and raises the salinity levels too high for some habitats to withstand. Some are able to cope with the salt and recycle it back into the ocean, but others can not release the extra surface water quickly enough or do not have a large enough freshwater source to replace it. Because of this, some species of plants and vegetation die due to the excess salt. In addition, hurricanes can carry toxins and acids onto shore when they make landfall. The flood water can pick up the toxins from different spills and contaminate the land that it passes over. The toxins are very harmful to the people and animals in the area, as well as the environment around them. The flooding water can also spark many dangerous oil spills.\n\nHurricane preparedness encompasses the actions and planning taken before a tropical cyclone strikes to mitigate damage and injury from the storm. Knowledge of tropical cyclone impacts on an area help plan for future possibilities. Preparedness may involve preparations made by individuals as well as centralized efforts by governments or other organizations. Tracking storms during the tropical cyclone season helps individuals know current threats. Regional Specialized Meteorological Centers and Tropical Cyclone Warning Centers provide current information and forecasts to help individuals make the best decision possible.\n\nHurricane response is the disaster response after a hurricane. Activities performed by hurricane responders include assessment, restoration, and demolition of buildings; removal of debris and waste; repairs to land-based and maritime infrastructure; and public health services including search and rescue operations. Hurricane response requires coordination between federal, tribal, state, local, and private entities. According to the National Voluntary Organizations Active in Disaster, potential response volunteers should affiliate with established organizations and should not self-deploy, so that proper training and support can be provided to mitigate the danger and stress of response work.\n\nHurricane responders face many hazards. Hurricane responders may be exposed to chemical and biological contaminants including stored chemicals, sewage, human remains, and mold growth encouraged by flooding, as well as asbestos and lead that may be present in older buildings. Common injuries arise from falls from heights, such as from a ladder or from level surfaces; from electrocution in flooded areas, including from backfeed from portable generators; or from motor vehicle accidents. Long and irregular shifts may lead to sleep deprivation and fatigue, increasing the risk of injuries, and workers may experience mental stress associated with a traumatic incident. Additionally, heat stress is a concern as workers are often exposed to hot and humid temperatures, wear protective clothing and equipment, and have physically difficult tasks.\n\nIntense tropical cyclones pose a particular observation challenge, as they are a dangerous oceanic phenomenon, and weather stations, being relatively sparse, are rarely available on the site of the storm itself. In general, surface observations are available only if the storm is passing over an island or a coastal area, or if there is a nearby ship. Real-time measurements are usually taken in the periphery of the cyclone, where conditions are less catastrophic and its true strength cannot be evaluated. For this reason, there are teams of meteorologists that move into the path of tropical cyclones to help evaluate their strength at the point of landfall.\n\nTropical cyclones far from land are tracked by weather satellites capturing visible and infrared images from space, usually at half-hour to quarter-hour intervals. As a storm approaches land, it can be observed by land-based Doppler weather radar. Radar plays a crucial role around landfall by showing a storm's location and intensity every several minutes.\n\nIn situ measurements, in real-time, can be taken by sending specially equipped reconnaissance flights into the cyclone. In the Atlantic basin, these flights are regularly flown by United States government hurricane hunters. The aircraft used are WC-130 Hercules and WP-3D Orions, both four-engine turboprop cargo aircraft. These aircraft fly directly into the cyclone and take direct and remote-sensing measurements. The aircraft also launch GPS dropsondes inside the cyclone. These sondes measure temperature, humidity, pressure, and especially winds between flight level and the ocean's surface. A new era in hurricane observation began when a remotely piloted Aerosonde, a small drone aircraft, was flown through Tropical Storm Ophelia as it passed Virginia's Eastern Shore during the 2005 hurricane season. A similar mission was also completed successfully in the western Pacific Ocean. This demonstrated a new way to probe the storms at low altitudes that human pilots seldom dare.\n\nBecause of the forces that affect tropical cyclone tracks, accurate track predictions depend on determining the position and strength of high- and low-pressure areas, and predicting how those areas will change during the life of a tropical system. The deep layer mean flow, or average wind through the depth of the troposphere, is considered the best tool in determining track direction and speed. If storms are significantly sheared, use of wind speed measurements at a lower altitude, such as at the 70 kPa pressure surface ( above sea level) will produce better predictions. Tropical forecasters also consider smoothing out short-term wobbles of the storm as it allows them to determine a more accurate long-term trajectory. High-speed computers and sophisticated simulation software allow forecasters to produce computer models that predict tropical cyclone tracks based on the future position and strength of high- and low-pressure systems. Combining forecast models with increased understanding of the forces that act on tropical cyclones, as well as with a wealth of data from Earth-orbiting satellites and other sensors, scientists have increased the accuracy of track forecasts over recent decades. However, scientists are not as skillful at predicting the intensity of tropical cyclones. The lack of improvement in intensity forecasting is attributed to the complexity of tropical systems and an incomplete understanding of factors that affect their development. New tropical cyclone position and forecast information is available at least every twelve hours in the Southern Hemisphere and at least every six hours in the Northern Hemisphere from Regional Specialized Meteorological Centers and Tropical Cyclone Warning Centers.\n\nTropical cyclones are classified into three main groups, based on intensity: tropical depressions, tropical storms, and a third group of more intense storms, whose name depends on the region. For example, if a tropical storm in the Northwestern Pacific reaches hurricane-strength winds on the Beaufort scale, it is referred to as a \"typhoon\"; if a tropical storm passes the same benchmark in the Northeast Pacific Basin, or in the North Atlantic, it is called a \"hurricane\". Neither \"hurricane\" nor \"typhoon\" is used in either the Southern Hemisphere or the Indian Ocean. In these basins, storms of a tropical nature are referred to as either tropical cyclones, severe tropical cyclones or very intense tropical cyclones.\n\nAs indicated in the table below, each basin uses a separate system of terminology, which can make comparisons between different basins difficult. In the Pacific Ocean, hurricanes from the Central North Pacific sometimes cross the 180th meridian into the Northwest Pacific, becoming typhoons (such as Hurricane/Typhoon Ioke in 2006); on rare occasions, the reverse will occur. It should also be noted that typhoons with 1-minute sustained winds greater than 67 metres per second (m/s), over , are called \"Super Typhoons\" by the Joint Typhoon Warning Center.\n\nAlthough not strictly tropical (or even subtropical) cyclonic systems like depressions, storms and hurricanes/​typhoons, the National Hurricane Center, Central Pacific Hurricane Center, and Joint Typhoon Warning Center also monitor and designate certain areas of interest (also called invests) for potential cyclonic activity, sometimes classifying them as potential tropical cyclones as an unofficial fourth category.\n\nIf a tropical disturbance (not yet developed into a tropical depression) is capable of producing tropical storm or hurricane conditions on land within 48 hours, then it will be classified by the NHC, CPHC, or JTWC as a potential tropical cyclone (PTC).\n\nA tropical depression or tropical low is a tropical disturbance that has a clearly defined surface circulation with maximum sustained winds of less than . Within the Southern Hemisphere, the depression can have gale force or stronger winds in one or more quadrants, but not near the centre.\n\n A tropical storm is an organized system of strong thunderstorms with a defined surface circulation and maximum sustained winds between and . At this point, the distinctive cyclonic shape starts to develop, although an eye is not usually present. Government weather services first assign names to systems that reach this intensity (thus the term \"named storm\"). Although tropical storms are less intense than a hurricane they can produce significant damage. The shear force of winds can blow off shingles, and air borne objects can cause damage to power lines, roofing and siding. More dangerous is the heavy rainfall causing inland flooding.\n\nA hurricane or typhoon (sometimes simply referred to as a tropical cyclone, as opposed to a depression or storm) is a system with sustained winds of at least . A cyclone of this intensity tends to develop an eye, an area of relative calm (and lowest atmospheric pressure) at the center of circulation. The eye is often visible in satellite images as a small, circular, cloud-free spot. Surrounding the eye is the eyewall, an area about to wide in which the strongest thunderstorms and winds circulate around the storm's center. Maximum sustained winds in the strongest tropical cyclones have been estimated at about .\n\nThe word \"typhoon\", which is used today in the Northwest Pacific, may be derived from Arabic \"ţūfān\" (طوفان) (similar in Hindustani and Persian), or Greek \"Typhon\" (Τυφών) (a monster from Greek mythology associated with storms) or Chinese \"táifēng\" (Simplified Chinese: 台风, Traditional Chinese: 颱風) (fēng = wind).\n\nThe word \"hurricane\", used in the North Atlantic and Northeast Pacific, is derived from \"huracán\", the Spanish word for the Carib/Taino storm god, Juracán. This god is believed by scholars to have been at least partially derived from the Mayan creator god, Huracan. Huracan was believed by the Maya to have created dry land out of the turbulent waters. The god was also credited with later destroying the \"wooden people\", the precursors to the \"maize people\", with an immense storm and flood. Huracan is also the source of the word \"orcan\", another word for a particularly strong European windstorm.\n\nAlmost all tropical and subtropical cyclones, even if not developed enough to be formally assigned names, are assigned numbers by the warning centers tasked with monitoring them.\n\nFor example, systems (tropical, subtropical, or even potential tropical) forming in the North Atlantic and North Pacific basins (in an official manner), as well as those originating in other areas (in an unofficial basis) serving United States government interests (both civilian and military), are assigned \"tropical cyclone numbers\" (or \"TC numbers\" for short) by the National Hurricane Center, the Central Pacific Hurricane Center, and the Joint Typhoon Warning Center. A TC number is a two-digit number (starting each year/season with \"01\" and running upwards) followed by (except for the North Atlantic basin) a suffix letter corresponding to the basin of origin (like \"E\" for Eastern Pacific, \"C\" for Central Pacific, and \"W\" for Western Pacific); the number is often spelled-out in English (like \"ONE\", sometimes appended with a hyphen and the basin suffix as in \"ONE-E\") for the purpose of generating a placeholder name for a (sub)tropical depression or an otherwise yet-unnamed storm (especially in JTWC-tracked areas where differences in wind scales result in one agency upgrading the system to a storm without the other following suit).\n\nExamples of TC numbers are \"PTC 08\" for a North Atlantic potential tropical cyclone \"EIGHT\", \"TD 21E\" for an East Pacific tropical depression \"TWENTYONE-E\", and \"SD 03C\" for a Central Pacific subtropical depression \"THREE-C\". TC numbers are hard-limited to a maximum of \"49\" by the Automated Tropical Cyclone Forecasting System; however, the NHC and the CPHC usually limit this to \"30\"; also, TC numbers are not recycled until the next year/season. This numbering system is similar to that used for invests, except invests are numbered from \"90\" to \"99\" (which are rotated and recycled within the same year/season) and explicitly specify suffix \"L\" for North Atlantic systems.\n\nThe practice of using names to identify tropical cyclones goes back many years, with systems named after places or things they hit before the formal start of naming. The system currently used provides positive identification of severe weather systems in a brief form, that is readily understood and recognized by the public. The credit for the first usage of personal names for weather systems is generally given to the Queensland Government Meteorologist Clement Wragge who named systems between 1887 and 1907. This system of naming weather systems subsequently fell into disuse for several years after Wragge retired, until it was revived in the latter part of World War II for the Western Pacific. Formal naming schemes have subsequently been introduced for the North and South Atlantic, Eastern, Central, Western and Southern Pacific basins as well as the Australian region and Indian Ocean.\n\nAt present tropical cyclones are officially named by one of eleven meteorological services and retain their names throughout their lifetimes to provide ease of communication between forecasters and the general public regarding forecasts, watches, and warnings. Since the systems can last a week or longer and more than one can be occurring in the same basin at the same time, the names are thought to reduce the confusion about what storm is being described. Names are assigned in order from predetermined lists with one, three, or ten-minute sustained wind speeds of more than depending on which basin it originates. However, standards vary from basin to basin with some tropical depressions named in the Western Pacific, while tropical cyclones have to have a significant amount of gale-force winds occurring around the center before they are named within the Southern Hemisphere. The names of significant tropical cyclones in the North Atlantic Ocean, Pacific Ocean, and Australian region are retired from the naming lists and replaced with another name.\n\nTropical cyclones that cause extreme destruction are rare, although when they occur, they can cause great amounts of damage or thousands of fatalities. \n\nThe 1970 Bhola cyclone is considered to be the deadliest tropical cyclone on record, which killed around 300,000 people, after striking the densely populated Ganges Delta region of Bangladesh on November 13, 1970. Its powerful storm surge was responsible for the high death toll. The North Indian cyclone basin has historically been the deadliest basin. Elsewhere, Typhoon Nina killed nearly 100,000 in China in 1975 due to a 100-year flood that caused 62 dams including the Banqiao Dam to fail. The Great Hurricane of 1780 is the deadliest North Atlantic hurricane on record, killing about 22,000 people in the Lesser Antilles. A tropical cyclone does not need to be particularly strong to cause memorable damage, primarily if the deaths are from rainfall or mudslides. Tropical Storm Thelma in November 1991 killed thousands in the Philippines, although the strongest typhoon to ever make landfall on record was Typhoon Haiyan in November 2013, causing widespread devastation in Eastern Visayas, and killing at least 6,300 people in the Philippines alone. In 1982, the unnamed tropical depression that eventually became Hurricane Paul killed around 1,000 people in Central America.\n\nHurricane Harvey and Hurricane Katrina are estimated to be the costliest tropical cyclones to impact the United States mainland, each causing damage estimated at $125 billion. Harvey killed at least 90 people in August 2017 after making landfall in Texas as a low-end Category 4 hurricane. Hurricane Katrina is estimated as the second-costliest tropical cyclone worldwide, causing $81.2 billion in property damage (2008 USD) alone, with overall damage estimates exceeding $100 billion (2005 USD). Katrina killed at least 1,836 people after striking Louisiana and Mississippi as a major hurricane in August 2005. Hurricane Maria is the third most destructive tropical cyclone in U.S history, with damage totaling $91.61 billion (2017 USD), and with damage costs at $68.7 billion (2012 USD), Hurricane Sandy is the fourth most destructive tropical cyclone in U.S history. The Galveston Hurricane of 1900 is the deadliest natural disaster in the United States, killing an estimated 6,000 to 12,000 people in Galveston, Texas. Hurricane Mitch caused more than 10,000 fatalities in Central America, making it the second deadliest Atlantic hurricane in history. Hurricane Iniki in 1992 was the most powerful storm to strike Hawaii in recorded history, hitting Kauai as a Category 4 hurricane, killing six people, and causing U.S. $3 billion in damage. Other destructive Eastern Pacific hurricanes include Pauline and Kenna, both causing severe damage after striking Mexico as major hurricanes. In March 2004, Cyclone Gafilo struck northeastern Madagascar as a powerful cyclone, killing 74, affecting more than 200,000, and becoming the worst cyclone to affect the nation for more than 20 years.\nThe most intense storm on record was Typhoon Tip in the northwestern Pacific Ocean in 1979, which reached a minimum pressure of and maximum sustained wind speeds of or . The highest maximum sustained wind speed ever recorded was or in Hurricane Patricia in 2015, which is the most intense cyclone ever recorded in the Western Hemisphere. Typhoon Nancy in 1961 also had recorded wind speeds of or , but recent research indicates that wind speeds from the 1940s to the 1960s were gauged too high, and this is no longer considered the storm with the highest wind speeds on record. Likewise, a surface-level gust caused by Typhoon Paka on Guam in late 1997 was recorded at or . Had it been confirmed, it would be the strongest non-tornadic wind ever recorded on the Earth's surface, but the reading had to be discarded since the anemometer was damaged by the storm.\nThe World Meteorological Organization established Barrow Island (Queensland) as the location of the highest non-tornado related wind gust at on April 10, 1996 during Severe Tropical Cyclone Olivia.\n\nIn addition to being the most intense tropical cyclone on record based on pressure, Tip was the largest cyclone on record, with tropical storm-force winds in diameter. The smallest storm on record, Tropical Storm Marco, formed during October 2008 and making landfall in Veracruz. Marco generated tropical storm-force winds only in diameter.\n\nHurricane John is the longest-lasting tropical cyclone on record, lasting 31 days in 1994. Before the advent of satellite imagery in 1961, however, many tropical cyclones were underestimated in their durations. John is also the longest-tracked tropical cyclone in the Northern Hemisphere on record, which had a path of . Cyclone Rewa of the 1993–94 South Pacific and Australian region cyclone seasons had one of the longest tracks observed within the Southern Hemisphere, traveling a distance of over during December 1993 and January 1994.\n\nWhile the number of storms in the Atlantic has increased since 1995, there is no obvious global trend; the annual number of tropical cyclones worldwide remains about 87 ± 10 (Between 77 and 97 tropical cyclones annually). However, the ability of climatologists to make long-term data analysis in certain basins is limited by the lack of reliable historical data in some basins, primarily in the Southern Hemisphere, while noting that a significant downward trend in tropical cyclone numbers has been identified for the region near Australia (based on high quality data and accounting for the influence of the El Niño-Southern Oscillation). In spite of that, there is some evidence that the intensity of hurricanes is increasing. Kerry Emanuel stated, \"Records of hurricane activity worldwide show an upswing of both the maximum wind speed in and the duration of hurricanes. The energy released by the average hurricane (again considering all hurricanes worldwide) seems to have increased by around 70% in the past 30 years or so, corresponding to about a 15% increase in the maximum wind speed and a 60% increase in storm lifetime.\"\n\nAtlantic storms are becoming more destructive financially, as evidenced by the fact that five of the ten most expensive storms in United States history have occurred since 1990. According to the World Meteorological Organization, \"recent increase in societal impact from tropical cyclones has been caused largely by rising concentrations of population and infrastructure in coastal regions.\" Political scientist Pielke \"et al.\" (2008) normalized mainland US hurricane damage from 1900–2005 to 2005 values and found no remaining trend of increasing absolute damage. The 1970s and 1980s were notable because of the extremely low amounts of damage compared to other decades. The decade 1996–2005 was the second most damaging among the past 11 decades, with only the decade 1926–1935 surpassing its costs.\n\nOften in part because of the threat of hurricanes, many coastal regions had sparse population between major ports until the advent of automobile tourism; therefore, the most severe portions of hurricanes striking the coast may have gone unmeasured in some instances. The combined effects of ship destruction and remote landfall severely limit the number of intense hurricanes in the official record before the era of hurricane reconnaissance aircraft and satellite meteorology. Although the record shows a distinct increase in the number and strength of intense hurricanes, therefore, experts regard the early data as suspect.\n\nThe number and strength of Atlantic hurricanes may undergo a 50–70 year cycle, also known as the Atlantic Multidecadal Oscillation. Nyberg \"et al.\" reconstructed Atlantic major hurricane activity back to the early 18th century and found five periods averaging 3–5 major hurricanes per year and lasting 40–60 years, and six other averaging 1.5–2.5 major hurricanes per year and lasting 10–20 years. These periods are associated with the Atlantic multidecadal oscillation. Throughout, a decadal oscillation related to solar irradiance was responsible for enhancing/dampening the number of major hurricanes by 1–2 per year.\n\nAlthough more common since 1995, few above-normal hurricane seasons occurred during 1970–94. Destructive hurricanes struck frequently from 1926 to 1960, including many major New England hurricanes. Twenty-one Atlantic tropical storms formed in 1933, a record only recently exceeded in 2005, which saw 28 storms. Tropical hurricanes occurred infrequently during the seasons of 1900–25; however, many intense storms formed during 1870–99. During the 1887 season, 19 tropical storms formed, of which a record 4 occurred after November 1 and 11 strengthened into hurricanes. Few hurricanes occurred in the 1840s to 1860s; however, many struck in the early 19th century, including an 1821 storm that made a direct hit on New York City. Some historical weather experts say these storms may have been as high as Category 4 in strength.\n\nThese active hurricane seasons predated satellite coverage of the Atlantic basin. Before the satellite era began in 1960, tropical storms or hurricanes went undetected unless a reconnaissance aircraft encountered one, a ship reported a voyage through the storm, or a storm hit land in a populated area.\n\nProxy records based on paleotempestological research have revealed that major hurricane activity along the Gulf of Mexico coast varies on timescales of centuries to millennia. Few major hurricanes struck the Gulf coast during 3000–1400 BC and again during the most recent millennium. These quiescent intervals were separated by a hyperactive period during 1400 BC and 1000 AD, when the Gulf coast was struck frequently by catastrophic hurricanes and their landfall probabilities increased by 3–5 times. This millennial-scale variability has been attributed to long-term shifts in the position of the Azores High, which may also be linked to changes in the strength of the North Atlantic oscillation.\n\nAccording to the Azores High hypothesis, an anti-phase pattern is expected to exist between the Gulf of Mexico coast and the Atlantic coast. During the quiescent periods, a more northeasterly position of the Azores High would result in more hurricanes being steered towards the Atlantic coast. During the hyperactive period, more hurricanes were steered towards the Gulf coast as the Azores High was shifted to a more southwesterly position near the Caribbean. Such a displacement of the Azores High is consistent with paleoclimatic evidence that shows an abrupt onset of a drier climate in Haiti around 3200 C years BP, and a change towards more humid conditions in the Great Plains during the late-Holocene as more moisture was pumped up the Mississippi Valley through the Gulf coast. Preliminary data from the northern Atlantic coast seem to support the Azores High hypothesis. A 3000-year proxy record from a coastal lake in Cape Cod suggests that hurricane activity increased significantly during the past 500–1000 years, just as the Gulf coast was amid a quiescent period of the last millennium.\n\nThe 2007 IPCC report noted many observed changes in the climate, including atmospheric composition, global average temperatures, ocean conditions, and others. The report concluded the observed increase in tropical cyclone intensity is larger than climate models predict. In addition, the report considered that it is likely that storm intensity will continue to increase through the 21st century, and declared it more likely than not that there has been some human contribution to the increases in tropical cyclone intensity.\n\nP.J. Webster and others published in 2005 an article in \"Science\" examining the \"changes in tropical cyclone number, duration, and intensity\" over the past 35 years, the period when satellite data has been available. Their main finding was although the number of cyclones decreased throughout the planet excluding the north Atlantic Ocean, there was a great increase in the number and proportion of very strong cyclones.\n\nAccording to 2006 studies by the National Oceanic and Atmospheric Administration, \"the strongest hurricanes in the present climate may be upstaged by even more intense hurricanes over the next century as the earth's climate is warmed by increasing levels of greenhouse gases in the atmosphere\".\n\nStudies published since 2008, by Kerry Emanuel from MIT, indicate that global warming is likely to increase the intensity but decrease the frequency of hurricane and cyclone activity. In an article in \"Nature\", Kerry Emanuel stated that potential hurricane destructiveness, a measure combining hurricane strength, duration, and frequency, \"is highly correlated with tropical sea surface temperature, reflecting well-documented climate signals, including multidecadal oscillations in the North Atlantic and North Pacific, and global warming\". Emanuel predicted \"a substantial increase in hurricane-related losses in the twenty-first century\".\n\nResearch reported in the September 3, 2008 issue of \"Nature\" found that the strongest tropical cyclones are getting stronger, in particular over the North Atlantic and Indian oceans. Wind speeds for the strongest tropical storms increased from an average of in 1981 to in 2006, while the ocean temperature, averaged globally over all the regions where tropical cyclones form, increased from to during this period.\n\nA 2017 study looked at compounding effects from floods, storm surge, and terrestrial flooding (rivers), and projects an increase due to global warming.\n\nIn addition to tropical cyclones, there are two other classes of cyclones within the spectrum of cyclone types. These kinds of cyclones, known as extratropical cyclones and subtropical cyclones, can be stages a tropical cyclone passes through during its formation or dissipation. An \"extratropical cyclone\" is a storm that derives energy from horizontal temperature differences, which are typical in higher latitudes. A tropical cyclone can become extratropical as it moves toward higher latitudes if its energy source changes from heat released by condensation to differences in temperature between air masses; although not as frequently, an extratropical cyclone can transform into a subtropical storm, and from there into a tropical cyclone. From space, extratropical storms have a characteristic \"comma-shaped\" cloud pattern. Extratropical cyclones can also be dangerous when their low-pressure centers cause powerful winds and high seas.\n\nA \"subtropical cyclone\" is a weather system that has some characteristics of a tropical cyclone and some characteristics of an extratropical cyclone. They can form in a wide band of latitudes, from the equator to 50°. Although subtropical storms rarely have hurricane-force winds, they may become tropical in nature as their cores warm. From an operational standpoint, a tropical cyclone is usually not considered to become subtropical during its extratropical transition.\n\nIn popular culture, tropical cyclones have made several appearances in different types of media, including films, books, television, music, and electronic games. These media often portray tropical cyclones that are either entirely fictional or based on real events. For example, George Rippey Stewart's \"Storm\", a best-seller published in 1941, is thought to have influenced meteorologists on their decision to assign female names to Pacific tropical cyclones. Another example is the hurricane in \"The Perfect Storm\", which describes the sinking of the \"Andrea Gail\" by the 1991 Perfect Storm. Hypothetical hurricanes have been featured in parts of the plots of series such as \"The Simpsons\", \"Invasion\", \"Family Guy\", \"Seinfeld\", \"Dawson's Creek\", \"Burn Notice\" and \"\". The 2004 film \"The Day After Tomorrow\" includes several mentions of actual tropical cyclones and features fantastical \"hurricane-like\", albeit non-tropical, Arctic storms.\n\nForecasting and preparation\n\nTropical cyclone seasons\n\nRegional Specialized Meteorological Centers\n\nTropical Cyclone Warning Centers\n\nReference\n"}
{"id": "46333271", "url": "https://en.wikipedia.org/wiki?curid=46333271", "title": "Vertebrate land invasion", "text": "Vertebrate land invasion\n\nThe aquatic to terrestrial transition of vertebrate organisms occurred in the late Devonian era and was an important step in the evolutionary history of modern land vertebrates. The transition allowed animals to escape competitive pressure from the water and explore niche opportunities on land. Fossils from this period have allowed scientists to identify some of the species that existed during this transition, such as Tiktaalik and Acanthostega. Many of these species were also the first to develop adaptations suited to terrestrial over aquatic life, such as neck mobility and hindlimb locomotion.\n\nThe late Devonian vertebrate transition was preceded by the plant and invertebrate terrestrial invasion. These invasions allowed for the appropriate niche development that would ultimately facilitate the vertebrate invasion. While the late Devonian event was the first land invasion by vertebrate organisms, aquatic species have continued to develop adaptations suited to terrestrial life (and vice versa) from the late Devonian to the Holocene.\n\nThe vertebrate species that were important to the initial water to land transition can be qualified as being in one of five groups: Sarcopterygian fishes, prototetrapods, aquatic tetrapods, true tetrapods, and terrestrial tetrapods. Many morphological changes occurred throughout this transition. Mechanical support structures changed from fins to limbs, the method of locomotion changed from swimming to walking, respiratory structures changed from gills to lungs, feeding mechanisms changed from suction feeding to biting, and mode of reproduction changed from larval development to metamorphosis.\n\nLungfish appeared approximately 400 million years ago. It is a species that endured rapid evolution during the Devonian era, which became known as the dipnoan renaissance. The Acanthostega species, known as the fish with legs, is considered a tetrapod by structural findings but is postulated to have perhaps never left the aquatic environment. Its legs are not well-suited to support its weight. The bones of its forearm, the radius and ulna, are very thin at the wrist and also unable to support it on land. It also lacks a sacrum and strong ligaments at the hip, which would be integral to supporting the animal against gravity. In this sense, the species is considered a tetrapod but not one that has adapted well enough to walk on land. Furthermore, its gill bars have a supportive brace characterized for use as an underwater ear because it can pick up noise vibrations through the water. Tetrapods that adapted to terrestrial living adapted these gill bones to pick up sounds through air, and they later became the middle ear bones seen in mammalian tetrapods. Ichthyostega, on the other hand, is considered to be a fully terrestrial tetrapod that perhaps depended on water for its aquatic young. Comparisons between the skeletal features of Acanthostega and Ichthyostega reveal that they had different habits. Acanthostega is likely exclusive to an aquatic environment, while Ichthyostega is progressed in the aquatic to terrestrial transition by living dominantly on the shores.\n\nAn evolutionary timeline of the late Devonian vertebrate terrestrial invasion demonstrates the changes that took place. A group of fish from the Givetian stage began developing limbs, and eventually evolved into aquatic tetrapods in the Famennian stage. Pederpes, Westlothiana, Protogyrinus, and Crassigyrinus descended from these species into the carboniferous period and were the first land vertebrates.\n\nA particularly important transitional species is one known as Tiktaalik. It has a fin, but the fin has bones within it that are similar to mammalian tetrapods. It has an upper arm bone, a lower arm bone, forearm bones, a wrist, and fingerlike projections. Essentially, it is a fin that can support the animal. Similarly, it also has a neck that allows independent head movement from the body. Its ribs are also able to support the body in gravity. Its skeletal features exhibit its ability as a fish that can live in shallow water and also venture onto land.\n\nIt took many millions of years for vertebrates to transition out of water onto land. During this time, both the competitive pressures that would push species out of the water and the niche occupation incentives that would pull species onto land were slowly building. The culmination of these driving factors are what ultimately facilitated the vertebrate transition.\n\nScientists believe that a long period of time where biotic and abiotic factors in the aquatic environment were unfavourable to certain aquatic organisms is what pushed their transition to shallower waters. Some of these push factors are environmental hypoxia, unfavourable aquatic temperatures, and increased salinity. Other constantly present factors such as predation, competition, waterborne diseases and parasites also contributed to the transition.\n\nA theory put forth by Joseph Barrell possibly helps explain what may have initiated these push factors to become relevant in the late Devonian. The extensive oxidized sediments that were present in Europe and North America during the late Devonian are evidence of severe droughts during this time. These droughts would cause small ponds and lakes to dry out, forcing certain aquatic organisms to move on land to find other bodies of water. Natural selection on these organisms eventually led to the evolution of the first terrestrial vertebrates.\n\nThe pull factors were secondary to the push factors, and only became significant once the pressures to leave the aquatic environment became significant. These were largely the niches and opportunities that were available for exploitation in the terrestrial environment, and include higher environmental oxygen partial pressures, favourable temperatures, and the lack of competitors and predators on land. The plants and invertebrates that had preceded the vertebrate invasion also provided opportunities in the form of abundant prey and lack of predators.\n\nThere were many challenges that the first land vertebrates faced. These challenges allowed for rapid natural selection and niche domination, resulting in an adaptive radiation that produced many different vertebrate land species in a relatively short period of time.\n\nDepending on the water depth at which a species lives, the visual perception of many aquatic species is better suited to darker environments than those on land. Similarly, hearing in aquatic organisms is better optimized for sounds underwater, where the speed and amplitude of sound is greater than in air.\n\nHomeostasis was almost definitely a challenge for land invading vertebrates. Gas exchange and water balance are highly different in water and in air. Homeostasis mechanisms suitable for a terrestrial environment may have been necessary to develop before these organisms invaded land.\n\nThe primary anatomical barrier is the development of lungs for proper gas exchange, however other anatomical barriers also exist. The stressors of the musculoskeletal system are different in air than they are in water, and the muscles and bones must be strong enough to withstand the increased effects of gravity on land.\n\nMany behaviours, such as reproduction, are specifically optimized to a wet environment. Navigation and locomotion are also highly different in aquatic environments compared to terrestrial environments.\n\nThe ancestral species of tetrapods that lived entirely in water had tall and narrow skulls with eyes facing sideways and forwards to maximize visibility for predators and prey in the aquatic environment. As the ancestors of early tetrapods started inhabiting shallower waters, these species had flatter skulls with eyes at the tops of their heads, which made it possible to spot food above them. Once the tetrapods transitioned onto land, the lineages evolved to have tall and narrow skulls with eyes facing sideways and forwards again. This allowed them to navigate through the terrestrial environment and look for predators and prey.\n\nFish do not have necks, so the head is directly connected to the shoulders. In contrast, land animals use necks to move their heads so they can look down to see the food on the ground. The greater the mobility of the neck, the more visibility the land animal has. As lineages moved from completely aquatic environments to shallower waters and land, they gradually evolved vertebral columns that increased neck mobility. The first neck vertebra that evolved permitted the animals to have flexion and extension of the head so that they can see up and down. The second neck vertebra evolved to allow rotation of the neck for moving the head left and right. As tetrapod species continued to evolve on land, adaptations included seven or more vertebrae, allowing increasing neck mobility.\n\nThe sacrum connects the pelvis and hindlimbs and is useful for motion on land. The aquatic ancestors of tetrapods did not have a sacrum, so it was speculated to have evolved for locomotive function exclusive to terrestrial environments. However, the Acanthostega species is one of the earliest lineages to have a sacrum, even though it is a fully aquatic species. Once species moved onto land, the trait was adapted for terrestrial locomotion support, which is evidenced by additional vertebrae fusing similarly to permit additional support. This is an example of exaptation, where a trait performs a function that did not arise through natural selection for its current use.\n\nAs the lineages evolved to adapt to terrestrial environments, many lost traits that were better suited for the aquatic environment. Many lost their gills, which were only useful for obtaining oxygen in water. Their tail fins became smaller. They lost the lateral line system, a network of canals along the skull and jaw that are sensitive to vibration, which does not work outside of an aquatic environment.\n\nFor successful land invasion, the species had several pre-adaptations like air-breathing and limb-based locomotion. Aspects such as reproduction and swallowing, however, have bound these species to the aquatic environment. These pre-adaptations have allowed vertebrates to venture onto land hundreds of times, but were not able to accomplish the same degree of prolific radiation into diverse terrestrial species. To understand the potential of future invasions, studies must evaluate the models of evolutionary steps taken in past invasions. The commonalities to current and future invasions may then be elucidated to predict the effects of environmental changes.\n"}
{"id": "1572073", "url": "https://en.wikipedia.org/wiki?curid=1572073", "title": "Volcanic plug", "text": "Volcanic plug\n\nA volcanic plug, also called a volcanic neck or lava neck, is a volcanic object created when magma hardens within a vent on an active volcano. When present, a plug can cause an extreme build-up of pressure if rising volatile-charged magma is trapped beneath it, and this can sometimes lead to an explosive eruption. Glacial erosion can lead to exposure of the plug on one side, while a long slope of material remains on the opposite side. Such landforms are called crag and tail. If a plug is preserved, erosion may remove the surrounding rock while the erosion-resistant plug remains, producing a distinctive upstanding landform.\n\nAn example of two volcanic plugs can be found at the Pitons, in Saint Lucia, as they rise abruptly out of the eastern Caribbean Sea.\n\nNear the village of Rhumsiki in the Far North Province of Cameroon, Kapsiki Peak is an example of a volcanic plug and is one of the most photographed parts of the Mandara Mountains. Spectacular volcanic plugs are present in the center of La Gomera island in the Canary Islands archipelago, within the Garajonay National Park.\n\nBorgarvirki is a volcanic plug located in north Iceland.\n\nA volcanic plug is situated in the town of Motta Sant'Anastasia in Italy.\n\nSaint Michel d'Aiguilhe chapel, whose construction started in 969, near Le Puy-en-Velay in France. The volcanic plug rises about above the surroundings. Another building on a volcanic plug is the 14th century Trosky Castle in the Czech Republic.\n\nStrombolicchio, the northernmost of the Aeolian Islands, and Rockall, a small, uninhabited, remote islet in the North Atlantic Ocean, are also volcanic plugs.\n\nIn the United Kingdom, two examples of a building on a volcanic plug are the Castle Rock in Edinburgh, Scotland, and Deganwy Castle, Wales. The Law, Dundee, Ailsa Craig, Bass Rock, North Berwick Law and Dumgoyne hill are other examples of volcanic plugs located in Scotland. There are over 30 volcanic plugs in Northern Ireland, including Slemish in Ballymena, Tievebulliagh, Scawt Hill, Carrickarede, Scrabo and Slieve Gallion.\n\nThere are several volcanic plugs in the United States, including Morro Rock in California and Shiprock in New Mexico. Devils Tower in Wyoming and Little Devils Postpile in Yosemite National Park, California, are also believed to be volcanic plugs by many geologists.\n\nIn Canada, the Northern Cordilleran Volcanic Province gives rise to several confirmed and suspected plugs. Chief among these is Castle Rock, located in British Columbia, which last erupted during the Pleistocene.\n\nThe southern coast of Saint Lucia is dominated by the iconic Pitons, a UNESCO World Heritage site. The twin peaks, Gros Piton and Petit Piton, steeply rise more than above the Caribbean.\n\nThere are several volcanic plugs in the North Island of New Zealand, including:\n\n\nIn New Zealand's South Island, Ōnawe Peninsula on Banks Peninsula is a prominent volcanic plug, and erosion of Saddle Hill near Dunedin has also revealed a plug. Dunedin's Mount Cargill displays two plugs: its main summit and the subsidiary summit of Buttar's Peak.\n\nIn Australia, the Nut and Table Cape in Tasmania are further examples along with Mount Warning in New South Wales.\n"}
{"id": "342086", "url": "https://en.wikipedia.org/wiki?curid=342086", "title": "War of the currents", "text": "War of the currents\n\nThe war of the currents (sometimes called battle of the currents) was a series of events surrounding the introduction of competing electric power transmission systems in the late 1880s and early 1890s. It included commercial competition, a debate over electrical safety, and a media/propaganda campaign that grew out of it, with the main players being the direct current (DC)–based Edison Electric Light Company and the alternating current (AC)–based Westinghouse Electric Company. It took place during the introduction and rapid expansion of the alternating current standard (already in use and advocated by several US and European companies) and its eventual adoption over the direct current distribution system. Three aspects have been conflated into the \"war\": open competition involving large electric companies and a format war involving their developing systems, a general fear in the public's mind of death by accidental electrocution from high voltage AC leading to a debate over its safety and regulation, and the debate and behind-the-scene maneuvers associated with the introduction of the electric chair.\n\nThe introduction of large scale outdoor arc lighting systems in the mid- to late-1870s, some of them powered by high-voltage alternating current, was followed in 1882 by Thomas Edison's low voltage DC electric distribution \"utility\" designed for indoor business and residential use as an alternative to gas and oil-based lighting. In 1886 George Westinghouse began building an alternating current system that used a transformer to step up voltage for long-distance transmission and then stepped it back down for indoor lighting, a more efficient and less expensive system that directly competed for the market the Edison system was designed to serve. As many other electric companies joined in and the use of AC spread rapidly, Edison's company made claims in early 1888 that alternating current was hazardous and inferior to the patented direct current system.\n\nIn the spring of 1888, a media furor arose over a series of deaths caused by pole-mounted high-voltage AC lines in New York City and around the country, attributed to the greed and callousness of the local AC-based lighting companies. In June of that year a New York electrical engineer named Harold P. Brown came to prominence as an opponent of the use of alternating current, claiming the AC-based lighting companies were putting the public at risk using high-voltages and installing it in a slipshod manner. Brown's campaign immediately gained the assistance of Edison and his company, aiding Brown in his public electrocution of animals with AC, trying to claim that AC was more dangerous than DC. Historians noted, and documents from the period seem to show, that there grew to be collusion between the Edison company and Brown in their parallel attempts to limit the use of AC: assisting Brown's attempt to push through legislation to control and severely limit AC installations and voltages (to the point of making it an ineffective power delivery system), providing technical assistance in Brown's tests to show AC would be the best current to power the new electric chair, and colluding with Brown and Westinghouse's chief AC rival, the Thomson-Houston Electric Company, to make sure the first electric chair was powered by a Westinghouse AC generator.\n\nThis was a period of industrial consolidation and by 1890 over a dozen electric companies had merged down to three; Edison (now Edison General Electric), Thomson-Houston, and Westinghouse. By the early 1890s the latter two were generating profits far in advance of the DC-based Edison company. During this period Thomas Edison left the electric power business and the company he founded was beginning to add AC technology to its system. Edison Electric's institutional opposition to alternating current came to an end in 1892 when they merged with what had become their biggest competitor, the Thomson-Houston company, a merger that put the managers of Thomson-Houston in control of the new company, now called General Electric. The merger of the Edison company (along with its strong lighting patents) with Thomson-Houston (and its AC patents) created a company that now controlled three quarters of the US electrical business. Westinghouse won the bid to supply electrical power for the World's Columbian Exposition in 1893 and won the first contract at Niagara Falls later that year. Their lead in the field quickly diminished with later contracts being split with General Electric.\n\nThere were several technical factors that drove the adoption of alternating-current over direct-current. The direct-current system generated and distributed electrical power at the same voltage as used by the customer's lamps and motors. This required the use of large, costly distribution wires and forced generating plants to be near the loads. With the development of a practical transformer, alternating-current power could be sent long distances over relatively small wires at a conveniently high voltage, then reduced in voltage to that used by a customer. Alternating-current generating stations could be larger, more efficient, and the distribution wires were relatively less costly.\n\nThe lower cost of AC power distribution prevailed, though DC systems persisted in some urban areas throughout the 20th century. While DC power is not used generally for the transmission of energy from power plants into homes as Edison and others intended, it is still common when distances are small or when isolation between different AC systems is needed, known as High-Voltage DC (HVDC). Low voltage DC is used widely in modern electronic devices, including computers, telephones, and automotive systems; in contrast, most electric motors are powered by line voltage AC.\n\nThe war of the currents grew out of the development of two lighting systems; arc lighting running on alternating current and incandescent lighting running on direct current. Both were supplanting gas lighting systems, with arc lighting taking over large area/street lighting, and incandescent lighting replacing gas for business and residential lighting.\n\nThe first type of widely used electric light was the arc lamp. These lamps had been around for most of the 19th century but by the late 1870s were beginning to be installed in cities in large scale systems powered by central generating plants. Arc lighting systems were extremely brilliant and capable of lighting whole streets, factory yards, or the interior of large buildings. They needed high voltages (above 3,000 volts) and some ran better on alternating current. Alternating current had been under development for a while in Europe with contributions being made to the field by Guillaume Duchenne (1850s), the dynamo work of Zénobe Gramme, Ganz Works (1870s), Sebastian Ziani de Ferranti (1880s), Lucien Gaulard, and Galileo Ferraris. The high voltages allowed a central generating station to supply a large area, up to long circuits since the capacity of a wire is proportional to the square of the current traveling on it, each doubling of the voltage allowed the same size cable to transmit the same amount of power four times the distance. 1880 saw the installation of large-scale arc lighting systems in several US cities including a central station set up by the Brush Electric Company in December 1880 to supply a length of Broadway in New York City with a 3,500–volt demonstration arc lighting system. The disadvantages of arc lighting were: it was maintenance intensive, buzzed, flickered, constituted a fire hazard, was really only suitable for outdoor lighting, and, at the high voltages used, was dangerous to work with.\n\nIn 1878 inventor Thomas Edison saw a market for a system that could bring electric lighting directly into a customer's business or home, a niche not served by arc lighting systems. By 1882 the investor-owned utility Edison Illuminating Company was established in New York City. Edison designed his \"utility\" to compete with the then established gas lighting utilities, basing it on a relatively low 110 volt direct current supply to power a high resistance incandescent lamp he had invented for the system. Edison direct current systems would be sold to cities throughout the United States, making it a standard with Edison controlling all technical development and holding all the key patents. Direct current worked well with incandescent lamps, which were the principal load of the day. Direct-current systems could be directly used with storage batteries, providing valuable load-leveling and backup power during interruptions of generator operation. Direct-current generators could be easily paralleled, allowing economical operation by using smaller machines during periods of light load and improving reliability. Edison had invented a meter to allow customers to be billed for energy proportional to consumption, but this meter worked only with direct current. Direct current also worked well with electric motors, an advantage DC held throughout the 1880s. The primary drawback with the Edison direct current system was that it ran at 110 volts from generation to its final destination giving it a relatively short useful transmission range: to keep the size of the expensive copper conductors down generating plants had to be situated in the middle of population centers and could only supply customers less than a mile from the plant.\n\nStarting in the 1880s alternating current gained its key advantage over direct current with the development of functional transformers that allowed the voltage to be \"stepped up\" to much higher transmission voltages and then dropped down to a lower end user voltage for business and residential use. Using induction coils to transfer power between electrical circuits had been around for 40 years with Pavel Yablochkov using them in his lighting system in 1876 and Lucien Gaulard and John Dixon Gibbs using the principle to create a \"step down\" transformer in 1882, but the design was not very efficient. A prototype of the high efficiency, closed core shunt connection transformer was made by the Hungarian \"Z.B.D.\" team (composed of Károly Zipernowsky, Ottó Bláthy and Miksa Déri) at Ganz Works in 1884. The new Z.B.D. transformers were 3.4 times more efficient than the open core bipolar devices of Gaulard and Gibbs. Transformers in use today are designed based on principles discovered by the three engineers. Their patents included another major related innovation: the use of parallel connected (as opposed to series connected) power distribution. Ottó Bláthy also invented the first AC electricity meter. The reliability of this type of AC technology received impetus after the Ganz Works electrified Rome, a large metropolis, in 1886.\n\nIn North America the inventor and entrepreneur George Westinghouse entered the electric lighting business in 1884 when he started to develop a DC system and hired William Stanley, Jr. to work on it. Westinghouse became aware of the new European transformer based AC systems in 1885 when he read about them in the UK technical journal \"Engineering\". He grasped that AC combined with transformers meant greater economies of scale could be achieved with large centralized power plants transmitting stepped up voltage very long distances to be used in arc lighting as well lower voltage home and commercial incandescent lighting supplied via a \"step down\" transformer at the other end. Westinghouse saw a way to build a truly competitive system instead of simply building another barely competitive DC lighting system using patents just different enough to get around the Edison patents. The Edison DC system of centralized DC plants with their short transmission range also meant there was a patchwork of un-supplied customers between Edison's plants that Westinghouse could easily supply with AC power. \n\nWestinghouse purchased the US patents rights to the Gaulard-Gibbs transformer and imported several of those as well as Siemens AC generators to begin experimenting with an AC-based lighting system in Pittsburgh. William Stanley used the Gaulard-Gibbs design and designs from the ZBD Transformer to develop the first practical transformer. The Westinghouse Electric Company was formed at the beginning of 1886. In March 1886 Stanley, with Westinghouse's backing, installed the first multiple-voltage AC power system, a demonstration incandescent lighting system, in Great Barrington, Massachusetts. Expanded to the point where it could light 23 businesses along main street with very little power loss over 4000 feet, the system used transformers to step 500 AC volts at the street down to 100 volts to power incandescent lamps at each location. By fall of 1886 Westinghouse, Stanley, and Oliver B. Shallenberger had built the first commercial AC power system in the US in Buffalo, New York.\n\nBy the end of 1887 Westinghouse had 68 alternating current power stations to Edison's 121 DC-based stations. To make matters worse for Edison, the Thomson-Houston Electric Company of Lynn, Massachusetts (another competitor offering AC- and DC-based systems) had built 22 power stations. Thomson-Houston was expanding their business while trying to avoid patent conflicts with Westinghouse, arranging deals such as coming to agreements over lighting company territory, paying a royalty to use the Stanley AC transformer patent, and allowing Westinghouse to use their Sawyer-Man incandescent bulb patent. Besides Thomson-Houston and Brush there were other competitors at the time included the United States Illuminating Company and the Waterhouse Electric Light Company. All of the companies had their own electric power systems, arc lighting systems, and even incandescent lamp designs for domestic lighting, leading to constant lawsuits and patent battles between themselves and with Edison.\n\nElihu Thomson of Thomson-Houston was concerned about AC safety and put a great deal of effort into developing a lightning arrestor for high-tension power lines as well as a magnetic blowout switch that could shut the system down in a power surge, a safety feature the Westinghouse system did not have. Thomson also worried about what would happen with the equipment after they sold it, assuming customers would follow a risky practice of installing as many lights and generators as they could get away with. He also thought the idea of using AC lighting in residential homes was too dangerous and had the company hold back on that type of installations until a safer transformer could be developed.\n\nDue to the hazards presented by high voltage electrical lines most European cities and the city of Chicago in the US required them to be buried underground. The City of New York did not require burying and had little in the way of regulation so by the end of 1887 the mishmash of overhead wires for telephone, telegraph, fire and burglar alarm systems in Manhattan were now mixed with haphazardly strung AC lighting system wires carrying up to 6000 volts. Insulation on power lines was rudimentary, with one electrician referring to it as having as much value \"as a molasses covered rag\", and exposure to the elements was eroding it over time. A third of the wires were simply abandoned by defunct companies and slowly deteriorating, causing damage to, and shorting out the other lines. Besides being an eyesore, New Yorkers were annoyed when a large March 1888 snowstorm (the Great Blizzard of 1888) tore down a large number of the lines, cutting off utilities in the city. This spurred on the idea of having these lines moved underground but it was stopped by a court injunction obtained by Western Union. Legislation to give all the utilities 90 days to move their lines into underground conduits supplied by the city was slowly making its way through the government but that was also being fought in court by the United States Illuminating Company, who claimed their AC lines were perfectly safe.\n\nAs AC systems continued to spread into territories covered by DC systems, with the companies seeming to impinge on Edison patents including incandescent lighting, things got worse for the company. The price of copper was rising, adding to the expense of Edison's low voltage DC system, which required much heavier copper wires than higher voltage AC systems. Thomas Edison's own colleagues and engineers were trying to get him to consider AC. Edison's sales force was continually losing bids in municipalities that opted for cheaper AC systems and Edison Electric Illuminating Company president Edward Hibberd Johnson pointed out that if the company stuck with an all DC system it would not be able to do business in small towns and even mid-sized cities. Edison Electric had a patent option on the ZBD transformer, and a confidential in house report recommended that the company go AC, but Thomas Edison was against the idea.\n\nAfter Westinghouse installed his first large scale system Edison wrote in a November 1886 private letter to Edward Johnson, \"\"Just as certain as death Westinghouse will kill a customer within six months after he puts in a system of any size, He has got a new thing and it will require a great deal of experimenting to get it working practically.\" Edison seemed to hold a view that the very high voltage used in AC systems was too dangerous and that it would take many years to develop a safe and workable system. Safety and avoiding the bad press of killing a customer had been one of the goals in designing his DC system and he worried that a death caused by a mis-installed AC system could hold back the use of electricity in general, Edison's understanding of how AC systems worked seemed to be extensive. He noted what he saw as inefficiencies and that, combined with the capital costs in trying to finance very large generating plants, led him to believe there would be very little cost savings in an AC venture. Edison was also of the opinion that DC was a superior system (a fact that he was sure the public would come to recognize) and inferior AC technology was being used by other companies as a way to get around his DC patents.\n\nIn February 1888 Edison Electric president Edward Johnson published an 84-page pamphlet titled \"A Warning from the Edison Electric Light Company\"\" and sent it to newspapers and to companies that had purchased or were planning to purchase electrical equipment from Edison competitors, including Westinghouse and Thomson Houston, stating that the competitors were infringing on Edison's incandescent light and other electrical patents. It warned that purchasers could find themselves on the losing side of a court case if those patents were upheld. The pamphlet also emphasized the safety and efficiency of direct current, with the claim DC had not caused a single death, and included newspaper stories of accidental electrocutions caused by alternating current.\n\nAs arc lighting systems spread so did stories of how the high voltages involved were killing people, usually unwary linemen, a strange new phenomenon that seemed to instantaneously strike a victim dead. One such story in 1881 of drunken dock worker dying after he grabbed a large electric dynamo led Buffalo, New York dentist Alfred P. Southwick to seek some application for the curious phenomenon. He worked with local physician George E. Fell and the Buffalo ASPCA, electrocuting hundreds of stray dogs, to come up with a method to euthanize animals via electricity. Southwick's 1882 and 1883 articles on how electrocution could be a replacement for hanging, using a restraint similar to a dental chair (an electric chair) caught the attention of New York State politicians who, following a series of botched hangings, were desperately seeking an alternative. An 1886 commission appointed by New York governor David B. Hill, which including Southwick, recommended in 1888 that executions be carried out by electricity using the electric chair.\n\nThere were early indications that this new form of execution would become mixed up with the war of currents. As part of their fact-finding, the commission sent out surveys to hundreds of experts on law and medicine, seeking their opinions, as well as contacting electrical experts, including Elihu Thomson and Thomas Edison. In late 1887, when death penalty commission member Southwick contacted Edison, the inventor stated he was against capital punishment and wanted nothing to do with the matter. After further prompting, Edison hit out at his chief electric power competitor, George Westinghouse, in what may have been the opening salvo in the war of currents, stating in a December 1887 letter to Southwick that it would be best to use current generated by \"'alternating machines,' manufactured principally in this country by Geo. Westinghouse\". Soon after the execution by electricity bill passed in June 1888, Edison was asked by a New York government official what means would be the best way to implement the state's new form of execution. \"Hire out your criminals as linemen to the New York electric lighting companies\" was Edison's tongue in cheek answer.\n\nAs the number of deaths attributed to high voltage lighting around the country continued to mount, a cluster of deaths in New York City in the spring of 1888 related to AC arc lighting set off a media frenzy against the \"deadly arc-lighting current\" and the seemingly callous lighting companies that used it. These deaths included a 15-year-old boy killed on April 15 by a broken telegraph line that had energized with alternating current from a United States Illuminating Company line, a clerk killed two weeks later by an AC line, and a Brush Electric Company lineman killed in May by the AC line he was cutting. The press in New York seemed to switch overnight from stories about electric lights vs gas lighting to \"death by wire\" incidents, with each new report seeming to fan public resentment against high voltage AC and the dangerously tangled overhead electrical wires in the city.\n\nAt this point an electrical engineer named Harold P. Brown, who at that time seemed to have no connection to the Edison company, sent a June 5, 1888 letter to the editor of the \"New York Post\" claiming the root of the problem was the alternating current (AC) system being used. Brown argued that the AC system was inherently dangerous and \"damnable\" and asked why the \"public must submit to constant danger from sudden death\" just so utilities could use a cheaper AC system.\n\nAt the beginning of attacks on AC, Westinghouse, in a June 7, 1888 letter, tried to defuse the situation. He invited Edison to visit him in Pittsburgh and said \"I believe there has been a systemic attempt on the part of some people to do a great deal of mischief and create as great a difference as possible between the Edison Company and The Westinghouse Electric Co., when there ought to be an entirely different condition of affairs\". Edison thanked him but said \"My laboratory work consumes the whole of my time\".\n\nOn June 8 Brown was lobbying in person before the New York Board of Electrical Control, asking that his letter to the paper be read into the meeting's record and demanding severe regulations on AC including limiting power to 300 volts, a level that would make AC next to useless for transmission. There were many rebuttals to Browns claims in the newspapers and letters to the board, with people pointing out he was showing no scientific evidence that AC was more dangerous than DC. Westinghouse pointed out in letters to various newspapers the number of fires caused by DC equipment and suggested that Brown was obviously being controlled by Edison, something Brown continually denied.\n\nA July edition of \"The Electrical Journal\" covered Brown's appearance before the New York Board of Electrical Control and the debate in technical societies over the merits of DC and AC and noted that:\n\nAt a July meeting Board of Electrical Control, Brown's criticisms of AC and even his knowledge of electricity was challenged by other electrical engineers, some of whom worked for Westinghouse. At this meeting, supporters of AC provided anecdotal stories from electricians on how they had survived shocks from AC at voltages up to 1000 volts and argued that DC was the more dangerous of the two.\n\nBrown, determined to prove alternating current was more dangerous than direct current, at some point contacted Thomas Edison to see if he could make use of equipment to conduct experiments. Edison immediately offered to assist Brown in his crusade against AC companies. Before long, Brown was loaned space and equipment at Edison's West Orange, New Jersey laboratory, as well as laboratory assistant Arthur Kennelly.\n\nBrown paid local children to collect stray dogs off the street for his experiments with direct and alternating current. After much experimentation killing a series of dogs, Brown held a public demonstration on July 30 in a lecture room at Columbia College. With many participants shouting for the demonstration to stop and others walking out, Brown subjected a caged dog to several shocks with increasing levels of direct current up to 1000 volts, which the dog survived. Brown then applied 330 volts of alternating current which killed the dog. Four days later he held a second demonstration to answer critics' claims that the DC probably weakened the dog before it died. In this second demonstration, three dogs were killed in quick succession with 300 volts of AC. Brown wrote to a college that he was sure this demonstration would get the New York Board of Electrical Control to limit AC installations to 300 volts. Brown's campaign to restrict AC to 300 volts went nowhere but legislation did come close to passing in Ohio and Virginia.\n\nWhat brought Brown to the forefront of the debate over AC and his motives remain unclear, but historians note there grew to be some form of collusion between the Edison company and Brown. Edison records seem to show it was Edison Electric Light treasurer Francis S. Hastings who came up with the idea of using Brown and several New York physicians to attack Westinghouse and the other AC companies in retaliation for what Hastings thought were unscrupulous bids by Westinghouse for lighting contracts in Denver and Minneapolis. Hasting brought Brown and Edison together and was in continual contact with Brown. Edison Electric seemed to be footing the bill for some of Brown's publications on the dangers of AC. In addition, Thomas Edison himself sent a letter to the city government of Scranton, Pennsylvania recommending Brown as an expert on the dangers of AC. Some of this collusion was exposed in letters stolen from Brown's office and published in August 1889.\n\nDuring this period Westinghouse continued to pour money and engineering resources into the goal of building a completely integrated AC system. To gain control of the Sawyer-Man lamp patents he bought Consolidated Electric Light in 1887. He bought the Waterhouse Electric Light Company in 1888 and the United States Illuminating Company in 1890, giving Westinghouse their own arc lighting systems as well as control over all the major incandescent lamp patents not controlled by Edison. In April 1888 Westinghouse engineer Oliver B. Shallenberger developed an induction meter that used a rotating magnetic field for measuring alternating current giving the company a way to calculate how much electricity a customer used. In July 1888 Westinghouse paid a substantial amount to license Nikola Tesla's US patents for a poly-phase AC induction motor and obtained a patent option on Galileo Ferraris' induction motor design. Although the acquisition of a feasible AC motor gave Westinghouse a key patent in building a completely integrated AC system, the general shortage of cash the company was going through by 1890 meant development had to be put on hold for a while. The difficulties of obtaining funding for such a capital intensive business was becoming a serious problem for the company and 1890 saw the first of several attempts by investor J. P. Morgan to take over Westinghouse Electric.\n\nThomson-Houston was continuing to expand, buying seven smaller electric companies including a purchase of the Brush Electric Company in 1889. By 1890 Thomson-Houston controlled the majority of the arc lighting systems in the US and a collection of its own US AC patents. Several of the business deals between Thomson-Houston and Westinghouse fell apart and in April 1888 a judge rolled back part of Westinghouse's original Gaulard Gibbs patent, stating it only covered transformers linked in series.\n\nWith the help of the financier Henry Villard the Edison group of companies also went through a series of mergers: \"Edison Lamp Company\", a lamp manufacturer in East Newark, New Jersey; \"Edison Machine Works\", a manufacturer of dynamos and large electric motors in Schenectady, New York; \"Bergmann & Company\", a manufacturer of electric lighting fixtures, sockets, and other electric lighting devices; and \"Edison Electric Light Company\", the patent-holding company and the financial arm backed by J.P. Morgan and the Vanderbilt family for Edison's lighting experiments, merged. The new company, \"Edison General Electric Company\", was formed in January 1889 with the help of Drexel, Morgan & Co. and Grosvenor Lowrey with Villard as president. It later included the Sprague Electric Railway & Motor Company.\n\nThrough the fall of 1888 a battle of words with Brown specifically attacking Westinghouse continued to escalate. In November George Westinghouse challenged Brown's assertion in the pages of the \"Electrical Engineer\" that the Westinghouse AC systems had caused 30 deaths. The magazine investigated the claim and found at most only two of the deaths could be attributed to Westinghouse installations.\n\nAlthough New York had a criminal procedure code that specified electrocution via an electric chair, it did not spell out the type of electricity, the amount of current, or its method of supply, since these were still relative unknowns. The New York Medico-Legal Society, an informal society composed of doctors and lawyers, was given the task of working out the details and in late 1888 through early 1889 conducted a series of animal experiments on voltage amounts, electrode design and placement, and skin conductivity. During this time they sought the advice of Harold Brown as a consultant. This ended up expanding the war of currents into the development of the chair and the general debate over capital punishment in the US.\n\nAfter the Medico-Legal Society formed their committee in September 1888 chairman Frederick Peterson, who had been an assistant at Brown's July 1888 public electrocution of dogs with AC at Columbia College, had the results of those experiments submitted to the committee. The claims that AC was more deadly than DC and was the best current to use was questioned with some committee members, pointing out that Brown's experiments were not scientifically carried out and were on animals smaller than a human being. At their November meeting the committee recommended 3000 volts although the type of electricity, direct current or alternating current, was not determined. \n\nIn order to more conclusively prove to the committee that AC was more deadly than DC, Brown contacted Edison Electric Light treasurer Francis S. Hastings to arrange the use of the West Orange laboratory. There on December 5, 1888 Brown set up an experiment with members of the press, members of the Medico-Legal Society, the chairman of the death penalty commission, and Thomas Edison looking on. Brown used alternating current for all of his tests on animals larger than a human, including 4 calves and a lame horse, all dispatched with 750 volts of AC. Based on these results the Medico-Legal Society's December meeting recommended the use of 1000–1500 volts of alternating current for executions and newspapers noted the AC used was half the voltage used in the power lines over the streets of American cities.\n\nWestinghouse criticized these tests as a skewed self-serving demonstration designed to be a direct attack on alternating current. On December 13 in a letter to the \"New York Times\", Westinghouse spelled out where Brown's experiments were wrong and claimed again that Brown was being employed by the Edison company. Brown's December 18 letter refuted the claims and Brown even challenged Westinghouse to an electrical duel, with Brown agreeing to be shocked by ever-increasing amounts of DC power if Westinghouse submitted himself to the same amount of increasing AC power, first to quit loses. Westinghouse declined the offer.\n\nIn March 1889 when members of the Medico-Legal Society embarked on another series of tests to work out the details of electrode composition and placement they turned to Brown for technical assistance. Edison treasurer Hastings tried unsuccessfully to obtain a Westinghouse AC generator for the test. They ended up using Edison's West Orange laboratory for the animal tests.\n\nAlso in March, Superintendent of Prisons Austin Lathrop asked Brown if he could supply the equipment needed for the executions as well as design the electric chair. Brown turned down the job of designing the chair but did agree to fulfill the contract to supply the necessary electrical equipment. The state refused to pay up front, and Brown apparently turned to Edison Electric as well as Thomson-Houston Electric Company to help obtaining the equipment. This became another behind-the-scenes maneuver to acquire Westinghouse AC generators to supply the current, apparently with the help of the Edison company and Westinghouse's chief AC rival, Thomson-Houston. Thomson-Houston arranged to acquire three Westinghouse AC generators by replacing them with new Thomson-Houston AC generators and Edison Electric probably put up the money to acquire them for Brown. Thomson-Houston president Charles Coffin had at least two reasons for obtaining the Westinghouse generators; he did not want his company's equipment to be associated with the death penalty and he wanted to use one to prove a point, paying Brown to set up a public efficiency test to show that Westinghouse's sales claim of manufacturing 50% more efficient generators was false.\n\nThat spring Brown published \"The Comparative Danger to Life of the Alternating and Continuous Electrical Current\" detailing the animal experiments done at Edison's lab and claiming they showed AC was far deadlier then DC. This 61-page professionally printed booklet (probably paid for by the Edison company) was sent to government officials, newspapers, and businessmen in towns with populations greater than 5000 inhabitants.\n\nIn May 1889 when New York had its first criminal sentenced to be executed in the electric chair, a street merchant named William Kemmler, there was a great deal of discussion in the editorial column of the \"New York Times\" as to what to call the then-new form of execution. The term \"\"Westinghouse\"d\" was put forward as well as \"\"Gerry\"cide\" (after death penalty commission head Elbridge Gerry), and \"\"Brown\"ed\". \"The Times\" hated the word that was eventually adopted, electrocution, describing it as being pushed forward by \"pretentious ignoramuses\". One of Edison's lawyers wrote to his colleague expressing an opinion that Edison's preference for \"dynamort\", \"ampermort\" and \"electromort\" were not good terms but thought \"Westinghouse\"d was the best choice.\"\n\nWilliam Kemmler was sentenced to die in the electric chair around June 24, 1889, but before the sentence could be carried out an appeal was filed on the grounds that it constituted cruel and unusual punishment under the US Constitution. It became obvious to the press and everyone involved that the politically connected (and expensive) lawyer who filed the appeal, William Bourke Cockran, had no connection to the case but did have connection to the Westinghouse company, obviously paying for his services.\n\nDuring fact-finding hearings held around the state beginning on July 9 in New York City, Cockran used his considerable skills as a cross-examiner and orator to attack Brown, Edison, and their supporters. His strategy was to show that Brown had falsified his test on the killing power of AC and to prove that electricity would not cause certain death and simply lead to torturing the condemned. In cross examination he questioned Brown's lack of credentials in the electrical field and brought up possible collusion between Brown and Edison, which Brown again denied. Many witnesses were called by both sides to give firsthand anecdotal accounts about encounters with electricity and evidence was given by medical professionals on the human body's nervous system and the electrical conductivity of skin. Brown was accused of fudging his tests on animals, hiding the fact that he was using lower current DC and high-current AC. When the hearing convened for a day at Edison's West Orange lab to witness demonstrations of skin resistance to electricity, Brown almost got in a fight with a Westinghouse representative, accusing him of being in the Edison laboratory to conduct industrial espionage. Newspapers noted the often contradictory testimony was raising public doubts about the electrocution law but after Edison took the stand many accepted assurances from the \"wizard of Menlo Park\" that 1000 volts of AC would easily kill any man.\n\nAfter the gathered testimony was submitted and the two sides presented their case, Judge Edwin Day ruled against Kemmler's appeal on October 9 and US Supreme Court denied Kemmler's appeal on May 23, 1890.\n\nWhen the chair was first used, on August 6, 1890, the technicians on hand misjudged the voltage needed to kill William Kemmler. After the first jolt of electricity Kemmler was found to be still breathing. The procedure had to be repeated and a reporter on hand described it as \"an awful spectacle, far worse than hanging.\" George Westinghouse commented: \"They would have done better using an axe.\"\n\nOn August 25, 1889 the New York Sun ran a story headlined:\nThe story was based on 45 letters stolen from Brown's office that spelled out Brown's collusion with Thomson-Houston and Edison Electric. The majority of the letters were correspondence between Brown and Thomson-Houston on the topic of acquiring the three Westinghouse generators for the state of New York as well as using one of them in an efficiency test. They also showed that Brown had received $5,000 from Edison Electric to purchase the surplus Westinghouse generators from Thomson-Houston. Further Edison involvement was contained in letters from Edison treasurer Hastings asking Brown to send anti-AC pamphlets to all the legislators in the state of Missouri (at the company's expense), Brown requesting that a letter of recommendation from Thomas Edison be sent to Scranton, PA, as well as Edison and Arthur Kennelly coaching Brown in his upcoming testimony in the Kemmler appeal trial.\n\nBrown was not slowed down by this revelation and characterized his efforts to expose Westinghouse as the same as going after a grocer who sells poison and calls it sugar.\n\n1889 saw another round of deaths attributed to alternating current including a lineman in Buffalo, New York, four linemen in New York City, and a New York fruit merchant who was killed when the display he was using came in contact with an overhead line. NYC Mayor Hugh J. Grant, in a meeting with the Board of Electrical Control and the AC electric companies, rejected the claims that the AC lines were perfectly safe saying \"we get news of all who touch them through the coroners office\". On October 11, 1889, John Feeks, a Western Union lineman, was high up in the tangle of overhead electrical wires working on what were supposed to be low-voltage telegraph lines in a busy Manhattan district. As the lunchtime crowd below looked on he grabbed a nearby line that, unknown to him, had been shorted many blocks away with a high-voltage AC line. The jolt entered through his bare right hand and exited his left steel studded climbing boot. Feeks was killed almost instantly, his body falling into the tangle of wire, sparking, burning, and smoldering for the better part of an hour while a horrified crowd of thousands gathered below. The source of the power that killed Feeks was not determined although United States Illuminating Company lines ran nearby.\n\nFeeks' public death sparked a new round of people fearing the electric lines over their heads in what has been called the \"Electric Wire Panic\". The blame seemed to settle on Westinghouse since, having bought many of the lighting companies involved, people assumed Feeks' death was the fault of a Westinghouse subsidiary. Newspapers joined into the public outcry following Feeks' death, pointing out men's lives \"were cheaper to this monopoly than insulated wires\" and calling for the executives of AC companies to be charged with manslaughter. The October 13, 1889, New Orleans \"Times-Picayune\" noted “\"Death does not stop at the door, but comes right into the house, and perhaps as you are closing a door or turning on the gas you are killed\".\" Harold Brown's reputation was rehabilitated almost overnight with newspapers and magazines seeking his opinion and reporters following him around New York City where he measured how much current was leaking from AC power lines.\nAt the peak of the war of currents, Edison himself joined the public debate for the first time, denounced AC current in a November 1889 article in the \"North American Review\" titled \"The Dangers of Electric Lighting\". Edison put forward the view that burying the high-voltage lines was not a solution, and would simply move the deaths underground and be a \"constant menace\" that could short with other lines threatening people's homes and lives. He stated the only way to make AC safe was to limit its voltage and vowed Edison Electric would never adopt AC as long as he was in charge.\n\nGeorge Westinghouse was suddenly put in the role of a \"villain\" trying to defend pole-mounted AC installations that he knew were unsafe and fumbled at reporters' questions trying to point out all the other things in a large city that were more dangerous. The next month he did better in his response printed in the \"North American Review\", pointing out that his AC/transformer system actually used lower household voltages than the Edison DC system. He also pointed out 87 deaths in one year caused by street cars and gas lighting vs only 5 accidental electrocutions and no in-home deaths attributed to AC current.\n\nThe crowd that watched Feeks contained many New York aldermen due to the site of the accident being near the New York government offices and the horrifying affair galvanized them into the action of passing the law on moving utilities underground. The electric companies involved obtained an injunction preventing their lines from being cut down immediately but shut down most of their lighting until the situation was settled, plunging many New York streets into darkness. The legislation ordering the cutting down of all of the utility lines was finally upheld by the New York Supreme Court in December. The AC lines were cut down keeping many New York City streets in darkness for the rest of the winter since little had been done by the overpaid Tammany Hall city supervisors who were supposed to see to building the underground \"subways\" to house them.\n\nEven with the Westinghouse propaganda losses, the war of currents itself was winding down with direct current on the losing side. This was due in part to Thomas Edison himself leaving the electric power business. Edison was becoming marginalized in his own company having lost majority control in the 1889 merger that formed Edison General Electric. In 1890 he told president Henry Villard he thought it was time to retire from the lighting business and moved on to an iron ore refining project that preoccupied his time. Edison's dogmatic anti-AC values were no longer controlling the company. By 1889 Edison's Electric's own subsidiaries were lobbying to add AC power transmission to their systems and in October 1890 Edison Machine Works began developing AC-based equipment.\n\nWith Thomas Edison no longer involved with Edison General Electric, the war of currents came to a close with a financial merger. Edison president Henry Villard, who had engineered the merger that formed Edison General Electric, was continually working on the idea of merging that company with Thomson-Houston or Westinghouse. He saw a real opportunity in 1891. The market was in a general downturn causing cash shortages for all the companies concerned and Villard was in talks with Thomson-Houston, which was now Edison General Electric's biggest competitor. Thomson-Houston had a habit of saving money on development by buying, or sometimes stealing, patents. Patent conflicts were stymieing the growth of both companies and the idea of saving on some 60 ongoing lawsuits as well as saving on profit losses of trying to undercut each other by selling generating plants below cost pushed forward the idea of this merger in financial circles. Edison hated the idea and tried to hold it off but Villard thought his company, now winning its incandescent light patent lawsuits in the courts, was in a position to dictate the terms of any merger. As a committee of financiers, which included J.P. Morgan, worked on the deal in early 1892 things went against Villard. In Morgan's view Thomson-Houston looked on the books to be the stronger of the two companies and engineered a behind the scenes deal announced on April 15, 1892, that put the management of Thomson-Houston in control of the new company, now called General Electric (dropping Edison's name). Thomas Edison was not aware of the deal until the day before it happened.\n\nThe fifteen electric companies that existed 5 years before had merged down to two; General Electric and Westinghouse. The war of currents came to an end and this merger of the Edison company, along with its lighting patents, and the Thomson-Houston, with its AC patents, created a company that controlled three quarters of the US electrical business. From this point on General Electric and Westinghouse were both marketing alternating current systems. Edison put on a brave face noting to the media how his stock had gained value in the deal but privately he was bitter that his company and all of his patents had been turned over to the competition.\n\nEven though the institutional war of currents had ended in a financial merger the technical difference between direct and alternating current systems followed a much longer technical merger. Due to innovation in the US and Europe, alternating current's economy of scale with very large generating plants linked to loads via long distance transmission was slowly being combined with the ability to link it up with all of the existing systems that needed to be supplied. These included single phase AC systems, poly-phase AC systems, low voltage incandescent lighting, high voltage arc lighting, and existing DC motors in factories and street cars. In the engineered \"universal system\" these technological differences were temporarily being bridged via the development of rotary converters and motor–generators that allowed the large number of legacy systems to be connected to the AC grid. These stopgaps were slowly replaced as older systems were retired or upgraded.\n\nIn May 1892 Westinghouse Electric managed to underbid General Electric on the contract to electrify the World's Columbian Exposition in Chicago and, although they made no profit, their demonstration of a safe and effective highly flexible universal alternating current system powering all of the disparate electrical systems at the Exposition led to them winning the bid at the end of that year to build an AC power station at Niagara Falls. General Electric was awarded contracts to build AC transmission lines and transformers in that project and further bids at Niagara were split with GE who were quickly catching up in the AC field due partly to Charles Proteus Steinmetz, a Prussian mathematician who was the first person to fully understand AC power from a solid mathematical standpoint. General Electric hired many talented new engineers to improve its design of transformers, generators, motors and other apparatus.\n\nPatent lawsuits were still hampering both companies and bleeding off cash, so in 1896, J. P. Morgan engineered a patent sharing agreement between the two companies that remained in force for 11 years.\n\nIn 1897 Edison sold his remaining stock in Edison Electric Illuminating of New York to finance his iron ore refining prototype plant. In 1908 Edison said to George Stanley, son of AC transformer inventor William Stanley, Jr., \"Tell your father I was wrong\", probably admitting he had underestimated the developmental potential of alternating current.\n\nThe International Electro-Technical Exhibition of 1891, in Frankfurt, Germany, featured the long distance transmission of high-power, three-phase electric current. It was held between 16 May and 19 October on the disused site of the three former “Westbahnhöfe” (Western Railway Stations) in Frankfurt am Main. The exhibition featured the first long distance transmission of high-power, three-phase electric current, which was generated 175 km away at Lauffen am Neckar. It successfully operated motors and lights at the fair. When the exhibition closed, the power station at Lauffen continued in operation, providing electricity for the administrative capital, Heilbronn, making it the first place to be equipped with three-phase AC power. Many corporate technical representatives (including E.W. Rice of Thomson-Houston Electric Company (what became General Electric)) attended. The technical advisers and representatives were impressed. As a result of the successful field trial, three-phase current, as far as Germany was concerned, became the most economical means of transmitting electrical energy.\n\nIn Europe, Siemens and Halske became the dominant force. Three phase 60 Hz at 120 volts became the dominant system in North America while 220-240 volts at 50 Hz became the standard in Europe.\n\nAlternating current power transmission networks today provide redundant paths and lines for power routing from any power plant to any load center, based on the economics of the transmission path, the cost of power, and the importance of keeping a particular load center powered at all times. High voltage power transmission allows generators (such as hydroelectric sites) to be located far from the loads.\n\nIn 1882, the German Miesbach–Munich Power Transmission used 2kV DC over a distance of . In 1889, the first long distance transmission of DC electricity in the United States was switched on at Willamette Falls Station, in Oregon City, Oregon. In 1890, a flood destroyed the power station. This unfortunate event paved the way for the first long distance transmission of AC electricity in the world when Willamette Falls Electric company installed experimental AC generators from Westinghouse in 1890. That same year, the Niagara Falls Power Company (NFPC) and its subsidiary Cataract Company formed the International Niagara Commission composed of experts, to analyze proposals to harness Niagara Falls to generate electricity. The commission was led by Sir William Thomson (later Lord Kelvin) and included Eleuthère Mascart from France, William Unwin from England, Coleman Sellers from the US, and Théodore Turrettini from Switzerland. It was backed by entrepreneurs such as J. P. Morgan, Lord Rothschild, and John Jacob Astor IV. Among 19 proposals, they even briefly considered compressed air as a power transmission medium, but preferred electricity. They could not decide which method would be best overall.\n\nBy 1893 the NFPC had rejected the remaining proposals from a half dozen companies and awarded the generating contract to Westinghouse with further transmission lines and transformer contracts awarded to General Electric. Work began in 1893 on the Niagara Falls generation project: 5,000 horsepower (3,700 kW) was to be generated and transmitted as alternating current, at a frequency of 25 Hz to minimize impedance losses in transmission (changed to 60 Hz in the 1950s).\n\nSome doubted that the system would generate enough electricity to power industry in Buffalo, New York. The inventor Nikola Tesla was sure it would work, saying that Niagara Falls could power the entire eastern United States. None of the previous polyphase alternating current transmission demonstration projects were on the scale of power available from Niagara:\n\nWestinghouse also had to develop a system that could be converted to all the needed power standards including single phase and polyphase AC and DC for street cars and factory motors. Westinghouse's initial customer for the hydroelectric generators at the Edward Dean Adams Station at Niagara in 1895 were the plants of the Pittsburgh Reduction Company which needed large quantities of cheap electricity for smelting aluminum. On November 16, 1896, electrical power transmitted to Buffalo began powering its street cars. The generators were built by Westinghouse Electric Corporation with generators that bore Tesla's name. The scale of the project soon had General Electric contributing equally, building transmission equipment, plants, and generators.\n\nEdison's DC distribution system consisted of generating plants feeding heavy distribution conductors, with customer loads (lighting and motors) tapped off them. The system operated at the same voltage level throughout; for example, 100 volt lamps at the customer's location were connected to a generator supplying 110 volts, to allow for some voltage drop in the wires between the generator and load. The voltage level was chosen for convenience in lamp manufacture; high-resistance carbon filament lamps could be constructed to withstand 100 volts, and to provide lighting performance economically competitive with gas lighting. At the time it was felt that 100 volts was not likely to present a severe hazard of fatal electric shock.\n\nTo save on the cost of copper conductors, a three-wire distribution system was used. The three wires were at +110 volts, 0 volts and −110 volts relative potential. 100-volt lamps could be operated between either the +110 or −110 volt legs of the system and the 0-volt \"neutral\" conductor, which carried only the unbalanced current between the + and − sources. The resulting three-wire system used less copper wire for a given quantity of electric power transmitted, while still maintaining (relatively) low voltages. Even with this innovation, the voltage drop due to the resistance of the system conductors was so high that generating plants had to be located within a mile (1–2 km) or so of the load. Higher voltages could not so easily be used with the DC system because there was no efficient low-cost technology to reduce a high transmission voltage to a low utilization voltage.\nIn the alternating current system, transformers were used between the (relatively) high voltage distribution system and the customer loads. Lamps and small motors could still be operated at some convenient low voltage, but the transformers allowed power to be transmitted at much higher voltages, say, ten times that of the loads. For a given quantity of power transmitted, the wire cross-sectional area was inversely proportional to the voltage used. Alternatively, the allowable length of a circuit, given a wire size and allowable voltage drop, increased approximately as the square of the distribution voltage. This meant that fewer, larger generating plants could serve the load in a given area. Large loads, such as industrial motors or converters for electric railway power, could be served by the same distribution network that fed lighting, by using a transformer with a suitable secondary voltage.\n\nEdison's response to the limitations of direct current was to generate power close to where it was consumed (today called distributed generation) and install large conductors to handle the growing demand for electricity, but this solution proved to be costly (especially for rural areas which could not afford to build a local station or to pay for large amounts of very thick copper wire), impractical (including inefficient voltage conversion) and unmanageable.\n\nDirect current could not easily be converted to higher or lower voltages. This meant that separate electrical lines had to be installed to supply power to appliances that used different voltages, for example, lighting and electric motors. This required more wires to lay and maintain, wasting money and introducing unnecessary hazards.\n\nLow-frequency (50–60 Hz) alternating currents can be more dangerous than similar levels of DC since the alternating fluctuations can cause the heart to lose coordination, inducing ventricular fibrillation, a deadly loss of heart rhythm that must be corrected immediately. Any practical distribution system will use voltage levels sufficient for a dangerous amount of current to flow, whether it uses alternating or direct current. As precautions against electrocution are similar for both AC and DC, the technical and economic advantages of AC power transmission outweighed this theoretical risk, and it was eventually adopted as the standard worldwide.\nThe advantage of AC for distributing power over a distance is due to the ease of changing voltages using a transformer. Available power is the product of current × voltage at the load. For a given amount of power, a low voltage requires a higher current and a higher voltage requires a lower current. Since metal conducting wires have an almost fixed electrical resistance, some power will be wasted as heat in the wires. This power loss is given by Joule's laws and is proportional to the square of the current. Thus, if the overall transmitted power is the same, and given the constraints of practical conductor sizes, high-current, low-voltage transmissions will suffer a much greater power loss than low-current, high-voltage ones. This holds whether DC or AC is used.\n\nConverting DC power from one voltage to another required a large spinning rotary converter or motor-generator set, which was difficult, expensive, inefficient, and required maintenance, whereas with AC the voltage can be changed with simple and efficient transformers that have no moving parts and require very little maintenance. This was the key to the success of the AC system. Modern transmission grids regularly use AC voltages up to 765,000 volts. Power electronic devices such as the mercury-arc valve and thyristor made high-voltage direct current transmission practical by improving the reliability and efficiency of conversion between alternating and direct current, but such technology only became possible on an industrial scale starting in the 1960s.\n\nAlternating-current transmission lines have losses that do not occur with direct current. Due to the skin effect, a conductor will have a higher resistance to alternating current than to direct current; the effect is measurable and of practical significance for large conductors carrying thousands of amperes. The increased resistance due to the skin effect can be offset by changing the shape of conductors from a solid core to a braid of many small (isolated) wires. Total losses in systems using high-voltage transmission and transformers to reduce (or increase) the voltage are very much lower than DC transmission at working voltage.\n\nSome cities continued to use DC well into the 20th century. For example, central Helsinki had a DC network until the late 1940s, and Stockholm lost its dwindling DC network as late as the 1970s. A mercury-arc valve rectifier station could convert AC to DC where networks were still used. Parts of Boston, Massachusetts, along Beacon Street and Commonwealth Avenue still used 110 volts DC in the 1960s, causing the destruction of many small appliances (typically hair dryers and phonographs) used by Boston University students, who ignored warnings about the electricity supply. New York City's electric utility company, Consolidated Edison, continued to supply direct current to customers who had adopted it early in the twentieth century, mainly for elevators. The New Yorker Hotel, constructed in 1929, had a large direct-current power plant and did not convert fully to alternating-current service until well into the 1960s. This was the building in which AC pioneer Nikola Tesla spent his last years, and where he died in 1943. New York City's Broadway theaters continued to use DC services until 1975, requiring the use of outmoded manual resistance dimmer boards operated by several stagehands. This practice ended when the musical \"A Chorus Line\" introduced computerized lighting control and thyristor (SCR) dimmers to Broadway, and New York theaters were finally converted to AC. In January 1998, Consolidated Edison started to eliminate DC service. At that time there were 4,600 DC customers. By 2006, there were only 60 customers using DC service, and on November 14, 2007, the last direct-current distribution by Con Edison was shut down. Customers still using DC were provided with on-site AC to DC rectifiers. Pacific Gas and Electric Company still provided DC power to some locations in San Francisco through at least 2012, primarily for elevators, supplied by close to 200 rectifiers each providing power for 7-10 customers.\n\nThe last DC commercial power distribution system in the United States was decommissioned in 2007. The Central Electricity Generating Board in the UK continued to maintain a 200 volt DC generating station at Bankside Power Station on the River Thames in London as late as 1981. It exclusively powered DC printing machinery in Fleet Street, then the heart of the UK's newspaper industry. It was decommissioned later in 1981 when the newspaper industry moved into the developing docklands area further down the river (using modern AC-powered equipment). The building was converted into an art gallery, the Tate Modern.\n\nElectric railways that use a third-rail system exclusively employ DC power between 110 and 1500 volts. Railways with overhead catenary lines use various power schemes including both high-voltage AC and low voltage DC. Low voltage in this context is 5 kV or below.\n\nHigh-voltage direct current (HVDC) systems are used for bulk transmission of energy from distant generating stations or for interconnection of separate alternating-current systems. These HVDC systems use electronic devices like mercury-arc valves, thyristors or IGBTs that were unavailable during the war of currents era. Power is still converted to and from alternating current at each side of the modern HVDC link. The advantages of HVDC over AC systems for bulk transmission include higher power ratings for a given line (important since installing new lines and even upgrading old ones is extremely expensive) and better control of power flows, especially in transient and emergency conditions that can often lead to blackouts. Many modern plants now use HVDC as an alternative to AC systems for long distance, high load transmission, especially in developing countries such as Russia, China, India and Brazil. One of the principal advantages is the ability to transfer power between two AC systems that are not in synchronized phase. (See List of HVDC projects for more details.)\n\nDC power distribution is still common when distances are small, and especially when energy storage or conversion uses batteries or fuel cells. These applications include:\n\nIn these applications, direct current may be used directly or converted to alternating current using power electronic devices. In the future, this may provide a way to supply energy to a grid from distributed sources. For example, hybrid vehicle owners may rent the capacity of their vehicle's batteries for load-levelling purposes by the local electrical utility company.\n\n\n\n"}
{"id": "4380391", "url": "https://en.wikipedia.org/wiki?curid=4380391", "title": "World record", "text": "World record\n\nA world record is usually the best global performance ever recorded and officially verified in a specific skill or sport. The book \"Guinness World Records\" collates and publishes notable records of all types, from first and best to worst human achievements, to extremes in the natural world and beyond.\n\nIn the United States the form World's Record was formerly more common. The term World Best was also briefly in use. The latter term is still used in athletics events, including track and field and road running to describe good and bad performances not recognized as an official world record: either because it is not an event where the IAAF tracks the record (e.g. the 150 m run or individual events in a decathlon), or because it does not fulfil other rigorous criteria of an otherwise qualifying event (e.g. the Great North Run half-marathon, which has an excessive downhill gradient). The term is also used in video game speedrunning when someone achieves the fastest possible time for the game and category.\n\nMalaysia is one country where world record-breaking has become something of a national fad. In India, the setting and breaking of records is popular: world record registrars based in India are Limca Book of Records, Unique World Records, World Records India, and Asia Book of Records.\n\nSome sports have world records recognised by their respective sports governing bodies:\n\n\n"}
{"id": "13873874", "url": "https://en.wikipedia.org/wiki?curid=13873874", "title": "Zeta potential titration", "text": "Zeta potential titration\n\nZeta potential titration is a titration of heterogeneous systems, for example colloids and emulsions. Solids in such systems have very high surface area. This type of titration is used to study the zeta potential of these surfaces under different conditions.\n\nThe iso-electric point is one such property. The iso-electric point is the pH value at which the zeta potential is approximately zero. At a pH near the iso-electric point (± 2 pH units), colloids are usually unstable; the particles tend to coagulate or flocculate. Such titrations use acids or bases as titration reagents. Tables of iso-electric points for different materials are available. The attached figure illustrates results of such titrations for concentrated dispersions of alumina (4% v/v) and rutile (7% v/v). It is seen that iso-electric point of alumina is around pH 9.3, whereas for rutile it is around pH 4. Alumina is unstable in the pH range from 7 to 11. Rutile is unstable in the pH range from 2 to 6.\n\nAnother purpose of this titration is determination of the optimum dose of surfactant for achieving stabilization or flocculation of a heterogeneous system. Examples can be found in the book by Dukhin and Goetz.\n\nIn a zeta-potential titration, the Zeta potential is the indicator. Measurement of the zeta potential can be performed using microelectrophoresis, or electrophoretic light scattering, or electroacoustic phenomena. The last method makes possible to perform titrations in concentrated systems, with no dilution. The book by Dukhin and Goetz provides a detailed description of such titrations.\n\n"}
