{"id": "49030874", "url": "https://en.wikipedia.org/wiki?curid=49030874", "title": "2015–16 Kuantan bauxite disaster", "text": "2015–16 Kuantan bauxite disaster\n\nThe 2015–16 Kuantan Bauxite Disaster is an ecological disaster which occurred from 2015 to 2016 onwards in Kuantan District, Pahang, Malaysia. The bauxite mining was blamed for turning the waters and seas red near Kuantan.\n\nMalaysia annual output of bauxite ore has increased from over 200,000 tonnes in 2013 to nearly 20 million tonnes in 2015, becoming the world's top producer and accounting for nearly half of the supply to China's massive aluminium industry. Kuantan, the capital of Pahang, which has a largely unregulated bauxite mining industry ramped up bauxite output starting in 2014 to fill a supply gap after neighboring Indonesia banned bauxite exports in January the same year and to meet demand from China. In July 2015, the Pahang government revoked the licences of 34 contractors to curb rampant bauxite mining in the area, due to concerns over rising pollution from the activity and only 11 operators would be allowed to operate.\n\nAs of December 2016, the Kuantan Port Consortium (KPC) has invested an amount of RM30 million to clean the bauxite waste at Kuantan harbour area.\n"}
{"id": "7575129", "url": "https://en.wikipedia.org/wiki?curid=7575129", "title": "Adam Sedgwick", "text": "Adam Sedgwick\n\nAdam Sedgwick (; 22 March 1785 – 27 January 1873) was a British geologist and priest, one of the founders of modern geology. He proposed the Cambrian and Devonian period of the geological timescale. Based on work which he did on Welsh rock strata, he proposed the Cambrian period in 1835, in a joint publication in which Roderick Murchison also proposed the Silurian period. Later in 1840, to resolve what later became known as the Great Devonian Controversy about rocks near the boundary between the Silurian and Carboniferous periods, he and Murchison proposed the Devonian period.\n\nThough he had guided the young Charles Darwin in his early study of geology and continued to be on friendly terms, Sedgwick was an opponent of Darwin's theory of evolution by means of natural selection.\n\nSedgwick was born in Dent, Yorkshire, the third child of an Anglican vicar. He was educated at Sedbergh School and Trinity College, Cambridge.\n\nHe studied mathematics and theology, and obtained his BA (5th Wrangler) from the University of Cambridge in 1808 and his MA in 1811. On July 20, 1817 he was ordained a deacon, then a year later he was ordained as a priest. His academic mentors at Cambridge were Thomas Jones and John Dawson. He became a Fellow of Trinity College, Cambridge and Woodwardian Professor of Geology at Cambridge from 1818, holding the chair until his death in 1873. His biography in the Cambridge Alumni database says that upon his acceptance of the position, reverend Sedgwick had no working knowledge of geology. An 1851 portrait of Sedgwick by William Boxall hangs in Trinity's collection.\n\nSedgwick studied the geology of the British Isles and Europe. He founded the system for the classification of Cambrian rocks and with Roderick Murchison worked out the order of the Carboniferous and underlying Devonian strata. These studies were mostly carried out in the 1830s. The investigations into the Devonian meant that Sedgwick was involved with Murchison in a vigorous debate with Henry De la Beche, in what became known as the great Devonian controversy.\n\nSedgwick investigated the phenomena of metamorphism and concretion, and was the first to distinguish clearly between stratification, jointing, and slaty cleavage. He was elected to Fellow of the Royal Society on 1 February 1821. In 1844, he was elected a Foreign Honorary Member of the American Academy of Arts and Sciences.\n\nAs a co-trustee of the will of Ann Sill, an owner of slaves in plantations in Jamaica, in 1835 Sedgwick was awarded half of £3783 in compensation for 174 slaves, following the abolition of slavery by the British government. A liberal Whig in politics, Sedgwick had long been a passionate supporter of abolition. \n\nThe Church of England, by no means a fundamentalist or evangelical church, encloses a wide range of beliefs. During Sedgwick's life there developed something of a chasm between the conservative high church believers and the liberal wing. After simmering for some years, the publication of \"Essays and Reviews\" by liberal churchmen in 1860 pinpointed the differences. In all this, Sedgwick, whose science and faith were intertwined in a natural theology, was definitely on the conservative side, and extremely outspoken about it. He told the February 1830 meeting of the Geological Society of London:\n\nAs a geologist in the mid-1820s he supported William Buckland's interpretation of certain superficial deposits, particularly loose rocks and gravel, as \"diluvium\" relating to worldwide floods, and in 1825 he published two papers identifying these as due to a \"great irregular inundation\" from the \"waters of a general deluge\", Noah's flood. Sedgwick's subsequent investigations and discussions with continental geologists persuaded him that this was problematic. In early 1827, after spending several weeks in Paris, he visited geological features in the Scottish Highlands with Roderick Murchison. He later wrote \"If I have been converted in part from the diluvian theory...it was...by my own gradual improved experience, and by communicating with those about me. Perhaps I may date my change of mind (at least in part) from our journey in the Highlands, where there are so many indications of local diluvial operations... Humboldt ridiculed [the doctrine] beyond measure when I met him in Paris. Prévost lectured against it.\" In response to Charles Lyell's 1830 publication, \"Principles of Geology,\" which is known for promoting uniformitarian geology Sedgwick talked of floods at various dates, then on 18 February 1831 when retiring from the Presidency of the Geological Society he recanted his former belief in Buckland's theory.\n\nHe strongly believed that species of organisms originated in a succession of Divine creative acts throughout the long expanse of history. Any form of development that denied a direct creative action smacked as materialistic and amoral. For Sedgwick, moral truths (the obtainment of which separates man from beast) were to be distinguished from physical truths, and to combine these or blur them together could only lead to disastrous consequences. In fact, one's own hope for immortality may ultimately rest on it.\n\nHe stated in 1830 that Scriptural geologists proposed \"a deformed progeny of heretical and fantastical conclusions, by which sober philosophy has been put to open shame, and sometimes even the charities of life have been exposed to violation.\" In 1834 he continued, \"They have committed the folly and SIN of dogmatizing,\" having \"sinned against plain sense,\" and \"of writing mischievous nonsense,\" \"Their eyes cannot bear to look upon\" truth and suppose an \"ignorant and dishonest\" theory. They show \"bigotry and ignorance,\" of nature's laws and natural phenomena. Henry Cole then responded in 1834 in a 136-page \"letter,\" \"Popular Geology Subversive of Divine Revelation\". He referred to Sedgwick's ideas as \"unscriptural and anti-Christian,\" \"scripture-defying\", \"revelation-subverting,\" and \"baseless speculations and self-contradictions,\" which were \"impious and infidel\". \n\nWhile he became increasingly Evangelical with age, he strongly supported advances in geology against conservative churchmen. At the September 1844 British Association for the Advancement of Science meeting at York he achieved national celebrity for his reply defending modern geology against an attack by the Dean of York, the Reverend William Cockburn, who described it as unscriptural. The entire chapter house of the cathedral refused to sit down with Sedgwick, and he was opposed by conservative papers including \"The Times\", but his courage was hailed by the full spectrum of the liberal press, and the confrontation was a key moment in the battle over relations between Scripture and science.\n\nWhen Robert Chambers anonymously published his own theory of universal evolutionism as his \"development hypothesis\" in the book \"Vestiges of the Natural History of Creation\" published in October 1844 to immediate popular success, Sedgwick's many friends urged him to respond. Like other eminent scientists he initially ignored the book, but the subject kept recurring and he then read it carefully and made a withering attack on the book in the July 1845 edition of the \"Edinburgh Review\". \"Vestiges\" \"comes before [its readers] with a bright, polished, and many-coloured surface, and the serpent coils a false philosophy, and asks them to stretch out their hands and pluck the forbidden fruit\", he wrote in his review. Accepting the arguments in \"Vestiges\" was akin to falling from grace and away from God's favour.\n\nHe lashed out at the book in a letter to Charles Lyell, bemoaning the consequences of it conclusions. \"...If the book be true, the labours of sober induction are in vain; religion is a lie; human law is a mass of folly, and a base injustice; morality is moonshine; our labours for the black people of Africa were works of madmen; and man and woman are only better beasts!\" Later, Sedgwick added a long preface to the 5th edition of his \"Discourse on the Studies of the University of Cambridge\" (1850), including a lengthy attack on \"Vestiges\" and theories of development in general.\n\nCharles Darwin was one of his geology students in 1831, and accompanied him on a field trip to Wales that summer. The two kept up a correspondence while Darwin was on the \"Beagle\" expedition, and afterwards. However, Sedgwick never accepted the case for evolution made in \"On the Origin of Species\" in 1859 any more than he did that in \"Vestiges\" in 1844. In response to receiving and reading Darwin's book, he wrote to Darwin saying:\n\nSedgwick regarded natural selection as\n\nHe emphasised his distinction between the moral and physical aspects of life, \"There is a moral or metaphysical part of nature as well as a physical. A man who denies this is deep in the mire of folly\". If humanity broke this distinction it \"would suffer a damage that might brutalize it—& sink the human race into a lower grade of degradation than any into which it has fallen since its written records tell us of its history\".\n\nIn a letter to another correspondent, Sedgwick was even harsher on Darwin's book, calling it \"utterly false\" and writing that \"It repudiates all reasoning from final causes; and seems to shut the door on any view (however feeble) of the God of Nature as manifested in His works. From first to last it is a dish of rank materialism cleverly cooked and served up\".\n\nDespite this difference of opinion, the two men remained friendly until Sedgwick's death. In contrast to Sedgwick, liberal church members (who included biologists such as George Rolleston, William Henry Flower and William Kitchen Parker) were usually comfortable with evolution.\n\nThe Sedgwick Club, the oldest student-run geological society in the world, was set up in honour of him in 1880. \n\nOn the death of Sedgwick it was decided that his memorial should take the form of a new and larger museum. Hitherto the geological collections had been placed in the Woodwardian Museum in Cockerell's Building. Through the energy of Professor T. McK. Hughes (successor to Sedgwick) the new building, termed the Sedgwick Museum, was completed and opened in 1903.\n\nIn 1865, the University of Cambridge received from A. A. Van Sittart the sum of 500 pounds sterling \"for the purpose of encouraging the study of geology among the resident members of the university, and in honour of the Rev. Adam Sedgwick\". Thus was founded the Sedgwick Prize to be given every third year for the best essay on some geological subject. The first Sedgwick Prize was awarded in 1873.\n\nTo celebrate the bicentenary of Sedgwick's birth a geological trail was created near Dent, the village where he was born. The Sedgwick Trail follows the River Clough, highlighting rock features and exploring the Dent Fault.\n\n\n"}
{"id": "2445103", "url": "https://en.wikipedia.org/wiki?curid=2445103", "title": "Adaro (mythology)", "text": "Adaro (mythology)\n\nThe Adaro were malevolent merman-like sea spirits found in the mythology of the Solomon Islands.\n\nAdaro is a unique creature that lived in the Pacific Ocean. The Adaro is very dangerous. Said to arise from the wicked part of a person's spirit, which is divided between the Aunga (good) which dies, and the Adaro (evil) which stays as a ghost. An adaro is described as a man with gills behind his ears, tail fins for feet, a horn like a shark's dorsal fin, and a swordfish or sawfish-like spear growing out of his head. They may also travel in Jaratep or Daretep waterspouts and along rainbows and are said to kill unwary fishermen by firing flying fish at them.\n"}
{"id": "26662554", "url": "https://en.wikipedia.org/wiki?curid=26662554", "title": "Aureole effect", "text": "Aureole effect\n\nThe aureole effect or water aureole is an optical phenomenon similar to Heiligenschein, creating sparkling light and dark rays radiating from the shadow of the viewer's head. This effect is seen only over a rippling water surface. The waves act as lenses to focus and defocus sunlight: focused sunlight produces the lighter rays, while defocused sunlight produces the darker rays. Suspended particles in the water help make the aureole effect more pronounced. The effect extends a greater angular distance from the viewer's shadow when the viewer is higher above the water, and can sometimes be seen from a plane.\n\nAlthough the focused (light) ray cones are actually more or less parallel to each other, the rays from the aureole effect appear to be radiating from the shadow of the viewer’s head due to perspective effects. The viewer's line of sight is parallel and lies within the cones, so from the viewer's perspective the rays seem to be radiating from the antisolar point, within the viewer's shadow.\n\nAs in similar antisolar optical effects (such as a glory or Heiligenschein), each observer will see an aureole effect radiating only from their own head’s shadow. Similarly, if a photographer holds their camera at arm's length, the aureole effect appearing in the picture will be seen radiating from the shadow of the camera, although the photographer would still see it around their head's shadow while taking the picture. This happens because the aureole effect always appears directly opposite the sun, centered at the antisolar point. The antisolar point itself is located within the shadow of the viewer, whatever this is: the eyes of the viewer or the camera's lens. As a matter of fact, when aureole effects are photographed from a plane, it is possible to tell where the photographer was seated.\n\n"}
{"id": "38348972", "url": "https://en.wikipedia.org/wiki?curid=38348972", "title": "Bibliography of ecology", "text": "Bibliography of ecology\n\nThis is a bibliography of ecology.\n\n\n\n"}
{"id": "482912", "url": "https://en.wikipedia.org/wiki?curid=482912", "title": "Bingham plastic", "text": "Bingham plastic\n\nA Bingham plastic is a viscoplastic material that behaves as a rigid body at low stresses but flows as a viscous fluid at high stress. It is named after Eugene C. Bingham who proposed its mathematical form.\n\nIt is used as a common mathematical model of mud flow in drilling engineering, and in the handling of slurries. A common example is toothpaste, which will not be extruded until a certain pressure is applied to the tube. It is then pushed out as a solid plug.\n\nFigure 1 shows a graph of the behaviour of an ordinary viscous (or Newtonian) fluid in red, for example in a pipe. If the pressure at one end of a pipe is increased this produces a stress on the fluid tending to make it move (called the shear stress) and the volumetric flow rate increases proportionally. However, for a Bingham Plastic fluid (in blue), stress can be applied but it will not flow until a certain value, the yield stress, is reached. Beyond this point the flow rate increases steadily with increasing shear stress. This is roughly the way in which Bingham presented his observation, in an experimental study of paints. These properties allow a Bingham plastic to have a textured surface with peaks and ridges instead of a featureless surface like a Newtonian fluid. \n\nFigure 2 shows the way in which it is normally presented currently. The graph shows shear stress on the vertical axis and shear rate on the horizontal one. (Volumetric flow rate depends on the size of the pipe, shear rate is a measure of how the velocity changes with distance. It is proportional to flow rate, but does not depend on pipe size.) As before, the Newtonian fluid flows and gives a shear rate for any finite value of shear stress. However, the Bingham plastic again does not exhibit any shear rate (no flow and thus no velocity) until a certain stress is achieved. For the Newtonian fluid the slope of this line is the viscosity, which is the only parameter needed to describe its flow. By contrast, the Bingham plastic requires two parameters, the yield stress and the slope of the line, known as the plastic viscosity.\n\nThe physical reason for this behaviour is that the liquid contains particles (such as clay) or large molecules (such as polymers) which have some kind of interaction, creating a weak solid structure, formerly known as a false body, and a certain amount of stress is required to break this structure. Once the structure has been broken, the particles move with the liquid under viscous forces. If the stress is removed, the particles associate again.\n\nThe material is an elastic solid for shear stress formula_1, less than a critical value formula_2. Once the critical shear stress (or \"yield stress\") is exceeded, the material flows in such a way that the shear rate, ∂\"u\"/∂\"y\" (as defined in the article on viscosity), is directly proportional to the amount by which the applied shear stress exceeds the yield stress:\n\nIn fluid flow, it is a common problem to calculate the pressure drop in an established piping network. Once the friction factor, \"f\", is known, it becomes easier to handle different pipe-flow problems, viz. calculating the pressure drop for evaluating pumping costs or to find the flow-rate in a piping network for a given pressure drop. It is usually extremely difficult to arrive at exact analytical solution to calculate the friction factor associated with flow of non-Newtonian fluids and therefore explicit approximations are used to calculate it. Once the friction factor has been calculated the pressure drop can be easily determined for a given flow by the Darcy–Weisbach equation:\n\nwhere:\n\nAn exact description of friction loss for Bingham plastics in fully developed laminar pipe flow was first published by Buckingham. His expression, the \"Buckingham–Reiner\" equation, can be written in a dimensionless form as follows:\n\nwhere:\n\nThe Reynolds number and the Hedstrom number are respectively defined as:\n\nwhere:\n\nDarby and Melson developed an empirical expression \nthat was then refined, and is given by:\n\nwhere:\n\nNote: Darby and Melson's expression is for a Fanning friction factor, and needs to be multiplied by 4 to be used in the friction loss equations located elsewhere on this page.\n\nAlthough an exact analytical solution of the Buckingham–Reiner equation can be obtained because it is a fourth order polynomial equation in \"f\", due to complexity of the solution it is rarely employed. Therefore, researchers have tried to develop explicit approximations for the Buckingham–Reiner equation.\n\nThe Swamee–Aggarwal equation is used to solve directly for the Darcy–Weisbach friction factor \"f\" for laminar flow of Bingham plastic fluids. It is an approximation of the implicit \"Buckingham–Reiner\" equation, but the discrepancy from experimental data is well within the accuracy of the data.\nThe Swamee–Aggarwal equation is given by:\n\nDanish \"et al.\" have provided an explicit procedure to calculate the friction factor \"f\" by using the Adomian decomposition method. The friction factor containing two terms through this method is given as:\n\nwhere\n\nand\n\nIn 1981, Darby and Melson, using the approach of Churchill and of Churchill and Usagi, developed an expression to get a single friction factor equation valid for all flow regimes:\n\nwhere:\n\nBoth Swamee–Aggarwal equation and the Darby–Melson equation can be combined to give an explicit equation for determining the friction factor of Bingham plastic fluids in any regime. Relative roughness is not a parameter in any of the equations because the friction factor of Bingham plastic fluids is not sensitive to pipe roughness.\n\n"}
{"id": "45086", "url": "https://en.wikipedia.org/wiki?curid=45086", "title": "Biodiversity", "text": "Biodiversity\n\nBiodiversity generally refers to the variety and variability of life on Earth. According to the United Nations Environment Programme (UNEP), biodiversity typically measures variation at the genetic, species, and ecosystem level. Terrestrial biodiversity tends to be greater near the equator, which seems to be the result of the warm climate and high primary productivity. Biodiversity is not distributed evenly on Earth, and is richest in the tropics. These tropical forest ecosystems cover less than 10 percent of earth's surface, and contain about 90 percent of the world's species. Marine biodiversity tends to be highest along coasts in the Western Pacific, where sea surface temperature is highest, and in the mid-latitudinal band in all oceans. There are latitudinal gradients in species diversity. Biodiversity generally tends to cluster in hotspots, and has been increasing through time, but will be likely to slow in the future.\n\nRapid environmental changes typically cause mass extinctions. More than 99.9 percent of all species that ever lived on Earth, amounting to over five billion species, are estimated to be extinct. Estimates on the number of Earth's current species range from 10 million to 14 million, of which about 1.2 million have been documented and over 86 percent have not yet been described. More recently, in May 2016, scientists reported that 1 trillion species are estimated to be on Earth currently with only one-thousandth of one percent described. The total amount of related DNA base pairs on Earth is estimated at 5.0 x 10 and weighs 50 billion tonnes. In comparison, the total mass of the biosphere has been estimated to be as much as 4 TtC (trillion tons of carbon). In July 2016, scientists reported identifying a set of 355 genes from the Last Universal Common Ancestor (LUCA) of all organisms living on Earth.\n\nThe age of the Earth is about 4.54 billion years. The earliest undisputed evidence of life on Earth dates at least from 3.5 billion years ago, during the Eoarchean Era after a geological crust started to solidify following the earlier molten Hadean Eon. There are microbial mat fossils found in 3.48 billion-year-old sandstone discovered in Western Australia. Other early physical evidence of a biogenic substance is graphite in 3.7 billion-year-old meta-sedimentary rocks discovered in Western Greenland. More recently, in 2015, \"remains of biotic life\" were found in 4.1 billion-year-old rocks in Western Australia. According to one of the researchers, \"If life arose relatively quickly on Earth .. then it could be common in the universe.\"\n\nSince life began on Earth, five major mass extinctions and several minor events have led to large and sudden drops in biodiversity. The Phanerozoic eon (the last 540 million years) marked a rapid growth in biodiversity via the Cambrian explosion—a period during which the majority of multicellular phyla first appeared. The next 400 million years included repeated, massive biodiversity losses classified as mass extinction events. In the Carboniferous, rainforest collapse led to a great loss of plant and animal life. The Permian–Triassic extinction event, 251 million years ago, was the worst; vertebrate recovery took 30 million years. The most recent, the Cretaceous–Paleogene extinction event, occurred 65 million years ago and has often attracted more attention than others because it resulted in the extinction of the dinosaurs.\n\nThe period since the emergence of humans has displayed an ongoing biodiversity reduction and an accompanying loss of genetic diversity. Named the Holocene extinction, the reduction is caused primarily by human impacts, particularly habitat destruction. Conversely, biodiversity positively impacts human health in a number of ways, although a few negative effects are studied.\n\nThe United Nations designated 2011–2020 as the United Nations Decade on Biodiversity.\n\nThe term \"biological diversity\" was used first by wildlife scientist and conservationist Raymond F. Dasmann in the year 1968 lay book \"A Different Kind of Country\" advocating conservation. The term was widely adopted only after more than a decade, when in the 1980s it came into common usage in science and environmental policy. Thomas Lovejoy, in the foreword to the book \"Conservation Biology\", introduced the term to the scientific community. Until then the term \"natural diversity\" was common, introduced by The Science Division of The Nature Conservancy in an important 1975 study, \"The Preservation of Natural Diversity.\" By the early 1980s TNC's Science program and its head, Robert E. Jenkins, Lovejoy and other leading conservation scientists at the time in America advocated the use of the term \"biological diversity\".\n\nThe term's contracted form \"biodiversity\" may have been coined by W.G. Rosen in 1985 while planning the 1986 \"National Forum on Biological Diversity\" organized by the National Research Council (NRC). It first appeared in a publication in 1988 when sociobiologist E. O. Wilson used it as the title of the proceedings of that forum.\n\nSince this period the term has achieved widespread use among biologists, environmentalists, political leaders and concerned citizens.\n\nA similar term in the United States is \"natural heritage.\" It pre-dates the others and is more accepted by the wider audience interested in conservation. Broader than biodiversity, it includes geology and landforms.\n\n\"Biodiversity\" is most commonly used to replace the more clearly defined and long established terms, species diversity and species richness. Biologists most often define biodiversity as the \"totality of genes, species and ecosystems of a region\". An advantage of this definition is that it seems to describe most circumstances and presents a unified view of the traditional types of biological variety previously identified:\n\n\nThis multilevel construct is consistent with Datman and Lovejoy. An explicit definition consistent with this interpretation was first given in a paper by Bruce A. Wilcox commissioned by the International Union for the Conservation of Nature and Natural Resources (IUCN) for the 1982 World National Parks Conference. Wilcox's definition was \"Biological diversity is the variety of life forms...at all levels of biological systems (i.e., molecular, organismic, population, species and ecosystem)...\".\nThe 1992 United Nations Earth Summit defined \"biological diversity\" as \"the variability among living organisms from all sources, including, 'inter alia', terrestrial, marine and other aquatic ecosystems and the ecological complexes of which they are part: this includes diversity within species, between species and of ecosystems\". This definition is used in the United Nations Convention on Biological Diversity.\n\nOne textbook's definition is \"variation of life at all levels of biological organization\".\n\nBiodiversity can be defined genetically as the diversity of alleles, genes and organisms. They study processes such as mutation and gene transfer that drive evolution.\n\nMeasuring diversity at one level in a group of organisms may not precisely correspond to diversity at other levels. However, tetrapod (terrestrial vertebrates) taxonomic and ecological diversity shows a very close correlation.\n\nBiodiversity is not evenly distributed, rather it varies greatly across the globe as well as within regions. Among other factors, the diversity of all living things (biota) depends on temperature, precipitation, altitude, soils, geography and the presence of other species. The study of the spatial distribution of organisms, species and ecosystems, is the science of biogeography.\n\nDiversity consistently measures higher in the tropics and in other localized regions such as the Cape Floristic Region and lower in polar regions generally. Rain forests that have had wet climates for a long time, such as Yasuní National Park in Ecuador, have particularly high biodiversity.\n\nTerrestrial biodiversity is thought to be up to 25 times greater than ocean biodiversity. A new method used in 2011, put the total number of species on Earth at 8.7 million, of which 2.1 million were estimated to live in the ocean. However, this estimate seems to under-represent the diversity of microorganisms.\n\nGenerally, there is an increase in biodiversity from the poles to the tropics. Thus localities at lower latitudes have more species than localities at higher latitudes. This is often referred to as the latitudinal gradient in species diversity. Several ecological mechanisms may contribute to the gradient, but the ultimate factor behind many of them is the greater mean temperature at the equator compared to that of the poles.\n\nEven though terrestrial biodiversity declines from the equator to the poles, some studies claim that this characteristic is unverified in aquatic ecosystems, especially in marine ecosystems. The latitudinal distribution of parasites does not appear to follow this rule.\n\nIn 2016, an alternative hypothesis (\"the fractal biodiversity\") was proposed to explain the biodiversity latitudinal gradient. In this study, the species pool size and the fractal nature of ecosystems were combined to clarify some general patterns of this gradient. This hypothesis considers temperature, moisture, and net primary production (NPP) as the main variables of an ecosystem niche and as the axis of the ecological hypervolume. In this way, it is possible to build fractal hypervolumes, whose fractal dimension rises up to three moving towards the equator.\n\nA biodiversity hotspot is a region with a high level of endemic species that have experienced great habitat loss. The term hotspot was introduced in 1988 by Norman Myers. While hotspots are spread all over the world, the majority are forest areas and most are located in the tropics.\n\nBrazil's Atlantic Forest is considered one such hotspot, containing roughly 20,000 plant species, 1,350 vertebrates and millions of insects, about half of which occur nowhere else. The island of Madagascar and India are also particularly notable. Colombia is characterized by high biodiversity, with the highest rate of species by area unit worldwide and it has the largest number of endemics (species that are not found naturally anywhere else) of any country. About 10% of the species of the Earth can be found in Colombia, including over 1,900 species of bird, more than in Europe and North America combined, Colombia has 10% of the world's mammals species, 14% of the amphibian species and 18% of the bird species of the world. Madagascar dry deciduous forests and lowland rainforests possess a high ratio of endemism. Since the island separated from mainland Africa 66 million years ago, many species and ecosystems have evolved independently. Indonesia's 17,000 islands cover and contain 10% of the world's flowering plants, 12% of mammals and 17% of reptiles, amphibians and birds—along with nearly 240 million people. Many regions of high biodiversity and/or endemism arise from specialized habitats which require unusual adaptations, for example, alpine environments in high mountains, or Northern European peat bogs.\n\nAccurately measuring differences in biodiversity can be difficult. Selection bias amongst researchers may contribute to biased empirical research for modern estimates of biodiversity. In 1768, Rev. Gilbert White succinctly observed of his Selborne, Hampshire \"\"all nature is so full, that that district produces the most variety which is the most examined.\"\"\n\nBiodiversity is the result of 3.5 billion years of evolution. The origin of life has not been definitely established by science, however some evidence suggests that life may already have been well-established only a few hundred million years after the formation of the Earth. Until approximately 600 million years ago, all life consisted of microorganisms – archaea, bacteria, and single-celled protozoans and protists.\n\nThe history of biodiversity during the Phanerozoic (the last 540 million years), starts with rapid growth during the Cambrian explosion—a period during which nearly every phylum of multicellular organisms first appeared. Over the next 400 million years or so, invertebrate diversity showed little overall trend and vertebrate diversity shows an overall exponential trend. This dramatic rise in diversity was marked by periodic, massive losses of diversity classified as mass extinction events. A significant loss occurred when rainforests collapsed in the carboniferous. The worst was the Permian-Triassic extinction event, 251 million years ago. Vertebrates took 30 million years to recover from this event.\n\nThe fossil record suggests that the last few million years featured the greatest biodiversity in history. However, not all scientists support this view, since there is uncertainty as to how strongly the fossil record is biased by the greater availability and preservation of recent geologic sections. Some scientists believe that corrected for sampling artifacts, modern biodiversity may not be much different from biodiversity 300 million years ago., whereas others consider the fossil record reasonably reflective of the diversification of life. Estimates of the present global macroscopic species diversity vary from 2 million to 100 million, with a best estimate of somewhere near 9 million, the vast majority arthropods. Diversity appears to increase continually in the absence of natural selection.\n\nThe existence of a \"global carrying capacity\", limiting the amount of life that can live at once, is debated, as is the question of whether such a limit would also cap the number of species. While records of life in the sea shows a logistic pattern of growth, life on land (insects, plants and tetrapods) shows an exponential rise in diversity. As one author states, \"Tetrapods have not yet invaded 64 per cent of potentially habitable modes and it could be that without human influence the ecological and taxonomic diversity of tetrapods would continue to increase in an exponential fashion until most or all of the available ecospace is filled.\"\n\nIt also appears that the diversity continue to increase over time, especially after mass extinctions.\n\nOn the other hand, changes through the Phanerozoic correlate much better with the hyperbolic model (widely used in population biology, demography and macrosociology, as well as fossil biodiversity) than with exponential and logistic models. The latter models imply that changes in diversity are guided by a first-order positive feedback (more ancestors, more descendants) and/or a negative feedback arising from resource limitation. Hyperbolic model implies a second-order positive feedback. The hyperbolic pattern of the world population growth arises from a second-order positive feedback between the population size and the rate of technological growth. The hyperbolic character of biodiversity growth can be similarly accounted for by a feedback between diversity and community structure complexity. The similarity between the curves of biodiversity and human population probably comes from the fact that both are derived from the interference of the hyperbolic trend with cyclical and stochastic dynamics.\n\nMost biologists agree however that the period since human emergence is part of a new mass extinction, named the Holocene extinction event, caused primarily by the impact humans are having on the environment. It has been argued that the present rate of extinction is sufficient to eliminate most species on the planet Earth within 100 years.\n\nIn 2011, in his \"Biodiversity-related Niches Differentiation Theory\", Roberto Cazzolla Gatti proposed that species themselves are the architects of biodiversity, by proportionally increasing the number of potentially available niches in a given ecosystem. This study led to the idea that biodiversity is autocatalytic. An ecosystem of interdependent species can be, therefore, considered as an emergent autocatalytic set (a self-sustaining network of mutually \"catalytic\" entities), where one (group of) species enables the existence of (i.e., creates niches for) other species. This view offers a possible answer to the fundamental question of why so many species can coexist in the same ecosystem.\n\nNew species are regularly discovered (on average between 5–10,000 new species each year, most of them insects) and many, though discovered, are not yet classified (estimates are that nearly 90% of all arthropods are not yet classified). Most of the terrestrial diversity is found in tropical forests and in general, land has more species than the ocean; some 8.7 million species may exists on Earth, of which some 2.1 million live in the ocean.\n\n\"Ecosystem services are the suite of benefits that ecosystems provide to humanity.\" The natural species, or biota, are the caretakers of all ecosystems. It is as if the natural world is an enormous bank account of capital assets capable of paying life sustaining dividends indefinitely, but only if the capital is maintained.\n\nThese services come in three flavors:\n\nThere have been many claims about biodiversity's effect on these ecosystem services, especially provisioning and regulating services. After an exhaustive survey through peer-reviewed literature to evaluate 36 different claims about biodiversity's effect on ecosystem services, 14 of those claims have been validated, 6 demonstrate mixed support or are unsupported, 3 are incorrect and 13 lack enough evidence to draw definitive conclusions.\n\nGreater species diversity \n\nGreater species diversity\n\n\n\n\n\n\n\nOther sources have reported somewhat conflicting results and in 1997 Robert Costanza and his colleagues reported the estimated global value of ecosystem services (not captured in traditional markets) at an average of $33 trillion annually.\n\nSince the stone age, species loss has accelerated above the average basal rate, driven by human activity. Estimates of species losses are at a rate 100-10,000 times as fast as is typical in the fossil record.\nBiodiversity also affords many non-material benefits including spiritual and aesthetic values, knowledge systems and education.\n\nAgricultural diversity can be divided into two categories: intraspecific diversity, which includes the genetic variety within a single species, like the potato (\"Solanum tuberosum\") that is composed of many different forms and types (e.g. in the U.S. they might compare russet potatoes with new potatoes or purple potatoes, all different, but all part of the same species, \"S. tuberosum\").\n\nThe other category of agricultural diversity is called interspecific diversity and refers to the number and types of different species. Thinking about this diversity we might note that many small vegetable farmers grow many different crops like potatoes and also carrots, peppers, lettuce etc.\n\nAgricultural diversity can also be divided by whether it is ‘planned’ diversity or ‘associated’ diversity. This is a functional classification that we impose and not an intrinsic feature of life or diversity. Planned diversity includes the crops which a farmer has encouraged, planted or raised (e.g. crops, covers, symbionts and livestock, among others), which can be contrasted with the associated diversity that arrives among the crops, uninvited (e.g. herbivores, weed species and pathogens, among others).\n\nThe control of associated biodiversity is one of the great agricultural challenges that farmers face. On monoculture farms, the approach is generally to eradicate associated diversity using a suite of biologically destructive pesticides, mechanized tools and transgenic engineering techniques, then to rotate crops. Although some polyculture farmers use the same techniques, they also employ integrated pest management strategies as well as strategies that are more labor-intensive, but generally less dependent on capital, biotechnology and energy.\n\nInterspecific crop diversity is, in part, responsible for offering variety in what we eat. Intraspecific diversity, the variety of alleles within a single species, also offers us choice in our diets. If a crop fails in a monoculture, we rely on agricultural diversity to replant the land with something new. If a wheat crop is destroyed by a pest we may plant a hardier variety of wheat the next year, relying on intraspecific diversity. We may forgo wheat production in that area and plant a different species altogether, relying on interspecific diversity. Even an agricultural society which primarily grows monocultures, relies on biodiversity at some point.\n\nMonoculture was a contributing factor to several agricultural disasters, including the European wine industry collapse in the late 19th century and the US southern corn leaf blight epidemic of 1970.\n\nAlthough about 80 percent of humans' food supply comes from just 20 kinds of plants, humans use at least 40,000 species. Many people depend on these species for food, shelter and clothing. Earth's surviving biodiversity provides resources for increasing the range of food and other products suitable for human use, although the present extinction rate shrinks that potential.\n\nBiodiversity's relevance to human health is becoming an international political issue, as scientific evidence builds on the global health implications of biodiversity loss.\n\nThe growing demand and lack of drinkable water on the planet presents an additional challenge to the future of human health. Partly, the problem lies in the success of water suppliers to increase supplies and failure of groups promoting preservation of water resources. While the distribution of clean water increases, in some parts of the world it remains unequal. According to the World Health Organisation (2018) only 71% of the global population used a safely managed drinking-water service.\n\nSome of the health issues influenced by biodiversity include dietary health and nutrition security, infectious disease, medical science and medicinal resources, social and psychological health. Biodiversity is also known to have an important role in reducing disaster risk and in post-disaster relief and recovery efforts.\n\nBiodiversity provides critical support for drug discovery and the availability of medicinal resources. A significant proportion of drugs are derived, directly or indirectly, from biological sources: at least 50% of the pharmaceutical compounds on the US market are derived from plants, animals and micro-organisms, while about 80% of the world population depends on medicines from nature (used in either modern or traditional medical practice) for primary healthcare. Only a tiny fraction of wild species has been investigated for medical potential. Biodiversity has been critical to advances throughout the field of bionics. Evidence from market analysis and biodiversity science indicates that the decline in output from the pharmaceutical sector since the mid-1980s can be attributed to a move away from natural product exploration (\"bioprospecting\") in favor of genomics and synthetic chemistry, indeed claims about the value of undiscovered pharmaceuticals may not provide enough incentive for companies in free markets to search for them because of the high cost of development; meanwhile, natural products have a long history of supporting significant economic and health innovation. Marine ecosystems are particularly important, although inappropriate bioprospecting can increase biodiversity loss, as well as violating the laws of the communities and states from which the resources are taken.\n\nMany industrial materials derive directly from biological sources. These include building materials, fibers, dyes, rubber and oil. Biodiversity is also important to the security of resources such as water, timber, paper, fiber and food. As a result, biodiversity loss is a significant risk factor in business development and a threat to long term economic sustainability.\n\nBiodiversity enriches leisure activities such as hiking, birdwatching or natural history study. Biodiversity inspires musicians, painters, sculptors, writers and other artists. Many cultures view themselves as an integral part of the natural world which requires them to respect other living organisms.\n\nPopular activities such as gardening, fishkeeping and specimen collecting strongly depend on biodiversity. The number of species involved in such pursuits is in the tens of thousands, though the majority do not enter commerce.\n\nThe relationships between the original natural areas of these often exotic animals and plants and commercial collectors, suppliers, breeders, propagators and those who promote their understanding and enjoyment are complex and poorly understood. The general public responds well to exposure to rare and unusual organisms, reflecting their inherent value.\n\nPhilosophically it could be argued that biodiversity has intrinsic aesthetic and spiritual value to mankind \"in and of itself\". This idea can be used as a counterweight to the notion that tropical forests and other ecological realms are only worthy of conservation because of the services they provide.\n\nBiodiversity supports many ecosystem services:\n\n\"There is now unequivocal evidence that biodiversity loss reduces the efficiency by which ecological communities capture biologically essential resources, produce biomass, decompose and recycle biologically essential nutrients... There is mounting evidence that biodiversity increases the stability of ecosystem functions through time... Diverse communities are more productive because they contain key species that have a large influence on productivity and differences in functional traits among organisms increase total resource capture... The impacts of diversity loss on ecological processes might be sufficiently large to rival the impacts of many other global drivers of environmental change... Maintaining multiple ecosystem processes at multiple places and times requires higher levels of biodiversity than does a single process at a single place and time.\"\n\nIt plays a part in regulating the chemistry of our atmosphere and water supply. Biodiversity is directly involved in water purification, recycling nutrients and providing fertile soils. Experiments with controlled environments have shown that humans cannot easily build ecosystems to support human needs; for example insect pollination cannot be mimicked, though there have been attempts to create artificial pollinators using unmanned aerial vehicles. The economic activity of pollination alone represented between $2.1-14.6 billions in 2003.\n\nAccording to Mora and colleagues, the total number of terrestrial species is estimated to be around 8.7 million while the number of oceanic species is much lower, estimated at 2.2 million. The authors note that these estimates are strongest for eukaryotic organisms and likely represent the lower bound of prokaryote diversity. Other estimates include:\n\nSince the rate of extinction has increased, many extant species may become extinct before they are described. Not surprisingly, in the animalia the most studied groups are birds and mammals, whereas fishes and arthropods are the least studied animals groups.\n\nDuring the last century, decreases in biodiversity have been increasingly observed. In 2007, German Federal Environment Minister Sigmar Gabriel cited estimates that up to 30% of all species will be extinct by 2050. Of these, about one eighth of known plant species are threatened with extinction. Estimates reach as high as 140,000 species per year (based on Species-area theory). This figure indicates unsustainable ecological practices, because few species emerge each year. Almost all scientists acknowledge that the rate of species loss is greater now than at any time in human history, with extinctions occurring at rates hundreds of times higher than background extinction rates. As of 2012, some studies suggest that 25% of all mammal species could be extinct in 20 years.\n\nIn absolute terms, the planet has lost 58% of its biodiversity since 1970 according to a 2016 study by the World Wildlife Fund. The Living Planet Report 2014 claims that \"the number of mammals, birds, reptiles, amphibians and fish across the globe is, on average, about half the size it was 40 years ago\". Of that number, 39% accounts for the terrestrial wildlife gone, 39% for the marine wildlife gone and 76% for the freshwater wildlife gone. Biodiversity took the biggest hit in Latin America, plummeting 83 percent. High-income countries showed a 10% increase in biodiversity, which was canceled out by a loss in low-income countries. This is despite the fact that high-income countries use five times the ecological resources of low-income countries, which was explained as a result of process whereby wealthy nations are outsourcing resource depletion to poorer nations, which are suffering the greatest ecosystem losses.\n\nA 2017 study published in \"PLOS One\" found that the biomass of insect life in Germany had declined by three-quarters in the last 25 years. Dave Goulson of Sussex University stated that their study suggested that humans \"appear to be making vast tracts of land inhospitable to most forms of life, and are currently on course for ecological Armageddon. If we lose the insects then everything is going to collapse.\"\n\nIn 2006, many species were formally classified as rare or endangered or threatened; moreover, scientists have estimated that millions more species are at risk which have not been formally recognized. About 40 percent of the 40,177 species assessed using the IUCN Red List criteria are now listed as threatened with extinction—a total of 16,119.\n\nJared Diamond describes an \"Evil Quartet\" of habitat destruction, overkill, introduced species and secondary extinctions. Edward O. Wilson prefers the acronym HIPPO, standing for Habitat destruction, Invasive species, Pollution, human over-Population and Over-harvesting. The most authoritative classification in use today is IUCN's Classification of Direct Threats which has been adopted by major international conservation organizations such as the US Nature Conservancy, the World Wildlife Fund, Conservation International and BirdLife International.\n\nHabitat destruction has played a key role in extinctions, especially in relation to tropical forest destruction. Factors contributing to habitat loss include: overconsumption, overpopulation, land use change, deforestation, pollution (air pollution, water pollution, soil contamination) and global warming or climate change.\n\nHabitat size and numbers of species are systematically related. Physically larger species and those living at lower latitudes or in forests or oceans are more sensitive to reduction in habitat area. Conversion to \"trivial\" standardized ecosystems (e.g., monoculture following deforestation) effectively destroys habitat for the more diverse species that preceded the conversion. Even the simplest forms of agriculture affect diversity - through clearing/draining land, discouraging weeds and \"pests\", and encouraging just a limited set of domesticated plant and animal species. In some countries lack of property rights or lax law/regulatory enforcement necessarily leads to biodiversity loss (degradation costs having to be supported by the community).\n\nA 2007 study conducted by the National Science Foundation found that biodiversity and genetic diversity are codependent—that diversity among species requires diversity within a species and \"vice versa\". \"If any one type is removed from the system, the cycle can break down and the community becomes dominated by a single species.\"\n, the most threatened ecosystems occur in fresh water, according to the Millennium Ecosystem Assessment 2005, which was confirmed by the \"Freshwater Animal Diversity Assessment\" organised by the biodiversity platform and the French \"Institut de recherche pour le développement\" (MNHNP).\n\nCo-extinctions are a form of habitat destruction. Co-extinction occurs when the extinction or decline in one species accompanies similar processes in another, such as in plants and beetles.\n\nBarriers such as large rivers, seas, oceans, mountains and deserts encourage diversity by enabling independent evolution on either side of the barrier, via the process of allopatric speciation. The term invasive species is applied to species that breach the natural barriers that would normally keep them constrained. Without barriers, such species occupy new territory, often supplanting native species by occupying their niches, or by using resources that would normally sustain native species.\n\nThe number of species invasions has been on the rise at least since the beginning of the 1900s. Species are increasingly being moved by humans (on purpose and accidentally). In some cases the invaders are causing drastic changes and damage to their new habitats (e.g.: zebra mussels and the emerald ash borer in the Great Lakes region and the lion fish along the North American Atlantic coast). Some evidence suggests that invasive species are competitive in their new habitats because they are subject to less pathogen disturbance. Others report confounding evidence that occasionally suggest that species-rich communities harbor many native and exotic species simultaneously while some say that diverse ecosystems are more resilient and resist invasive plants and animals. An important question is, \"do invasive species cause extinctions?\" Many studies cite effects of invasive species on natives, but not extinctions. Invasive species seem to increase local (i.e.: alpha diversity) diversity, which decreases turnover of diversity (i.e.: beta diversity). Overall gamma diversity may be lowered because species are going extinct because of other causes, but even some of the most insidious invaders (e.g.: Dutch elm disease, emerald ash borer, chestnut blight in North America) have not caused their host species to become extinct. Extirpation, population decline and homogenization of regional biodiversity are much more common. Human activities have frequently been the cause of invasive species circumventing their barriers, by introducing them for food and other purposes. Human activities therefore allow species to migrate to new areas (and thus become invasive) occurred on time scales much shorter than historically have been required for a species to extend its range.\n\nNot all introduced species are invasive, nor all invasive species deliberately introduced. In cases such as the zebra mussel, invasion of US waterways was unintentional. In other cases, such as mongooses in Hawaii, the introduction is deliberate but ineffective (nocturnal rats were not vulnerable to the diurnal mongoose). In other cases, such as oil palms in Indonesia and Malaysia, the introduction produces substantial economic benefits, but the benefits are accompanied by costly unintended consequences.\n\nFinally, an introduced species may unintentionally injure a species that depends on the species it replaces. In Belgium, Prunus spinosa from Eastern Europe leafs much sooner than its West European counterparts, disrupting the feeding habits of the \"Thecla betulae\" butterfly (which feeds on the leaves). Introducing new species often leaves endemic and other local species unable to compete with the exotic species and unable to survive. The exotic organisms may be predators, parasites, or may simply outcompete indigenous species for nutrients, water and light.\n\nAt present, several countries have already imported so many exotic species, particularly agricultural and ornamental plants, that their own indigenous fauna/flora may be outnumbered. For example, the introduction of kudzu from Southeast Asia to Canada and the United States has threatened biodiversity in certain areas.\n\nEndemic species can be threatened with extinction through the process of genetic pollution, i.e. uncontrolled hybridization, introgression and genetic swamping. Genetic pollution leads to homogenization or replacement of local genomes as a result of either a numerical and/or fitness advantage of an introduced species.\nHybridization and introgression are side-effects of introduction and invasion. These phenomena can be especially detrimental to rare species that come into contact with more abundant ones. The abundant species can interbreed with the rare species, swamping its gene pool. This problem is not always apparent from morphological (outward appearance) observations alone. Some degree of gene flow is normal adaptation and not all gene and genotype constellations can be preserved. However, hybridization with or without introgression may, nevertheless, threaten a rare species' existence.\n\nOverexploitation occurs when a resource is consumed at an unsustainable rate. This occurs on land in the form of overhunting, excessive logging, poor soil conservation in agriculture and the illegal wildlife trade.\n\nAbout 25% of world fisheries are now overfished to the point where their current biomass is less than the level that maximizes their sustainable yield.\n\nThe overkill hypothesis, a pattern of large animal extinctions connected with human migration patterns, can be used explain why megafaunal extinctions can occur within a relatively short time period.\n\nIn agriculture and animal husbandry, the Green Revolution popularized the use of conventional hybridization to increase yield. Often hybridized breeds originated in developed countries and were further hybridized with local varieties in the developing world to create high yield strains resistant to local climate and diseases. Local governments and industry have been pushing hybridization. Formerly huge gene pools of various wild and indigenous breeds have collapsed causing widespread genetic erosion and genetic pollution. This has resulted in loss of genetic diversity and biodiversity as a whole.\n\nGenetically modified organisms contain genetic material that is altered through genetic engineering. Genetically modified crops have become a common source for genetic pollution in not only wild varieties, but also in domesticated varieties derived from classical hybridization.\n\nGenetic erosion and genetic pollution have the potential to destroy unique genotypes, threatening future access to food security. A decrease in genetic diversity weakens the ability of crops and livestock to be hybridized to resist disease and survive changes in climate.\n\nGlobal warming is also considered to be a major potential threat to global biodiversity in the future. For example, coral reefs - which are biodiversity hotspots - will be lost within the century if global warming continues at the current trend.\n\nClimate change has seen many claims about potential to affect biodiversity but evidence supporting the statement is tenuous. Increasing atmospheric carbon dioxide certainly affects plant morphology and is acidifying oceans, and temperature affects species ranges, phenology, and weather, but the major impacts that have been predicted are still just \"potential\" impacts. We have not documented major extinctions yet, even as climate change drastically alters the biology of many species.\n\nIn 2004, an international collaborative study on four continents estimated that 10 percent of species would become extinct by 2050 because of global warming. \"We need to limit climate change or we wind up with a lot of species in trouble, possibly extinct,\" said Dr. Lee Hannah, a co-author of the paper and chief climate change biologist at the Center for Applied Biodiversity Science at Conservation International.\n\nA recent study predicts that up to 35% of the world terrestrial carnivores and ungulates will be at higher risk of extinction by 2050 because of the joint effects of predicted climate and land-use change under business-as-usual human development scenarios.\n\nThe world’s population numbered nearly 7.6 billion as of mid-2017 (which is approximately one billion more inhabitants compared to 2005) and is forecast to reach 11.1 billion in 2100. Sir David King, former chief scientific adviser to the UK government, told a parliamentary inquiry: \"It is self-evident that the massive growth in the human population through the 20th century has had more impact on biodiversity than any other single factor.\" At least until the middle of the 21st century, worldwide losses of pristine biodiverse land will probably depend much on the worldwide human birth rate. Biologists such as Paul R. Ehrlich and Stuart Pimm have noted that human population growth and overconsumption are the main drivers of species extinction.\n\nAccording to a 2014 study by the World Wildlife Fund, the global human population already exceeds planet's biocapacity - it would take the equivalent of 1.5 Earths of biocapacity to meet our current demands. The report further points that if everyone on the planet had the Footprint of the average resident of Qatar, we would need 4.8 Earths and if we lived the lifestyle of a typical resident of the USA, we would need 3.9 Earths.\n\nRates of decline in biodiversity in this sixth mass extinction match or exceed rates of loss in the five previous mass extinction events in the fossil record. Loss of biodiversity results in the loss of natural capital that supplies ecosystem goods and services. From the perspective of the method known as Natural Economy the economic value of 17 ecosystem services for Earth's biosphere (calculated in 1997) has an estimated value of US$33 trillion (3.3x10) per year.\n\nConservation biology matured in the mid-20th century as ecologists, naturalists and other scientists began to research and address issues pertaining to global biodiversity declines.\n\nThe conservation ethic advocates management of natural resources for the purpose of sustaining biodiversity in species, ecosystems, the evolutionary process and human culture and society.\n\nConservation biology is reforming around strategic plans to protect biodiversity. Preserving global biodiversity is a priority in strategic conservation plans that are designed to engage public policy and concerns affecting local, regional and global scales of communities, ecosystems and cultures. Action plans identify ways of sustaining human well-being, employing natural capital, market capital and ecosystem services.\n\nIn the EU Directive 1999/22/EC zoos are described as having a role in the preservation of the biodiversity of wildlife animals by conducting research or participation in breeding programs.\n\nRemoval of exotic species will allow the species that they have negatively impacted to recover their ecological niches. Exotic species that have become pests can be identified taxonomically (e.g., with Digital Automated Identification SYstem (DAISY), using the barcode of life). Removal is practical only given large groups of individuals due to the economic cost.\n\nAs sustainable populations of the remaining native species in an area become assured, \"missing\" species that are candidates for reintroduction can be identified using databases such as the \"Encyclopedia of Life\" and the Global Biodiversity Information Facility.\n\n\nProtected areas are meant for affording protection to wild animals and their habitat which also includes forest reserves and biosphere reserves. Protected areas have been set up all over the world with the specific aim of protecting and conserving plants and animals. Some scientists have called on the global community to designate as protected areas 30 percent of the planet by 2030, and 50 percent by 2050, in order to mitigate biodiversity loss from anthropogenic causes.\n\nNational park and nature reserve is the area selected by governments or private organizations for special protection against damage or degradation with the objective of biodiversity and landscape conservation. National parks are usually owned and managed by national or state governments. A limit is placed on the number of visitors permitted to enter certain fragile areas. Designated trails or roads are created. The visitors are allowed to enter only for study, cultural and recreation purposes. Forestry operations, grazing of animals and hunting of animals are regulated. Exploitation of habitat or wildlife is banned.\n\nWildlife sanctuaries aim only at conservation of species and have the following features:\n\n\nThe forests play a vital role in harbouring more than 45,000 floral and 81,000 faunal species of which 5150 floral and 1837 faunal species are endemic. Plant and animal species confined to a specific geographical area are called endemic species. In reserved forests, rights to activities like hunting and grazing are sometimes given to communities living on the fringes of the forest, who sustain their livelihood partially or wholly from forest resources or products. The unclassed forests covers 6.4 percent of the total forest area and they are marked by the following characteristics:\n\n\n\nIn zoological parks or zoos, live animals are kept for public recreation, education and conservation purposes. Modern zoos offer veterinary facilities, provide opportunities for threatened species to breed in captivity and usually build environments that simulate the native habitats of the animals in their care. Zoos play a major role in creating awareness about the need to conserve nature.\n\nIn botanical gardens, plants are grown and displayed primarily for scientific and educational purposes. They consist of a collection of living plants, grown outdoors or under glass in greenhouses and conservatories. In addition, a botanical garden may include a collection of dried plants or herbarium and such facilities as lecture rooms, laboratories, libraries, museums and experimental or research plantings.\n\nFocusing on limited areas of higher potential biodiversity promises greater immediate return on investment than spreading resources evenly or focusing on areas of little diversity but greater interest in biodiversity.\n\nA second strategy focuses on areas that retain most of their original diversity, which typically require little or no restoration. These are typically non-urbanized, non-agricultural areas. Tropical areas often fit both criteria, given their natively high diversity and relative lack of development.\n\n\nGlobal agreements such as the Convention on Biological Diversity, give \"sovereign national rights over biological resources\" (not property). The agreements commit countries to \"conserve biodiversity\", \"develop resources for sustainability\" and \"share the benefits\" resulting from their use. Biodiverse countries that allow bioprospecting or collection of natural products, expect a share of the benefits rather than allowing the individual or institution that discovers/exploits the resource to capture them privately. Bioprospecting can become a type of biopiracy when such principles are not respected.\n\nSovereignty principles can rely upon what is better known as Access and Benefit Sharing Agreements (ABAs). The Convention on Biodiversity implies informed consent between the source country and the collector, to establish which resource will be used and for what and to settle on a fair agreement on benefit sharing.\n\nBiodiversity is taken into account in some political and judicial decisions:\n\nUniform approval for use of biodiversity as a legal standard has not been achieved, however. Bosselman argues that biodiversity should not be used as a legal standard, claiming that the remaining areas of scientific uncertainty cause unacceptable administrative waste and increase litigation without promoting preservation goals.\n\nIndia passed the Biological Diversity Act in 2002 for the conservation of biological diversity in India. The Act also provides mechanisms for equitable sharing of benefits from the use of traditional biological resources and knowledge.\n\nLess than 1% of all species that have been described have been studied beyond simply noting their existence. The vast majority of Earth's species are microbial. Contemporary biodiversity physics is \"firmly fixated on the visible [macroscopic] world\". For example, microbial life is metabolically and environmentally more diverse than multicellular life (see e.g., extremophile). \"On the tree of life, based on analyses of small-subunit ribosomal RNA, visible life consists of barely noticeable twigs. The inverse relationship of size and population recurs higher on the evolutionary ladder—to a first approximation, all multicellular species on Earth are insects\". Insect extinction rates are high—supporting the Holocene extinction hypothesis.\n\nThe number of morphological attributes that can be scored for diversity study is generally limited and prone to environmental influences; thereby reducing the fine resolution required to ascertain the phylogenetic relationships. DNA based markers- microsatellites otherwise known as \"simple sequence repeats\" (SSR) were therefore used for the diversity studies of certain species and their wild relatives.\n\nIn the case of cowpea, a study conducted to assess the level of genetic diversity in cowpea germplasm and related wide species, where the relatedness among various taxa were compared, primers useful for classification of taxa identified, and the origin and phylogeny of cultivated cowpea classified show that SSR markers are useful in validating with species classification and revealing the center of diversity.\n\n\n\n\n\n"}
{"id": "35011503", "url": "https://en.wikipedia.org/wiki?curid=35011503", "title": "Book of Indian Birds", "text": "Book of Indian Birds\n\nThe Book of Indian Birds by Salim Ali is a landmark book on Indian ornithology, which helped spark popular interest in the birds of India. First published in 1941, it is currently in its 13th edition. Most of the older books on Indian birds were meant for identification using specimens in the hand and the only other field guide, Hugh Whistler's \"Popular Handbook Of Indian Birds\" was published in London, was getting out of date and not readily available in India. The book of Indian birds provided a popular bird-guide in a low-cost edition. \n\nThe J. Paul Getty Award for Conservation Leadership, while awarding Salim Ali in 1976, said in its citation:\n\nAmong the early converts of the book was Jawaharlal Nehru, who was to become India's first prime minister. While in jail under the British Raj, he gave a copy to his 25-year-old daughter Indira Gandhi.\n\nThe second edition of the book was published in 1942, subsequent editions being in 1944, 1945, 1961, 1964, 1968, 1972, 1977, 1979 (11th edition with reprints in 1984, 1986, 1988, 1990 and 1992), 1996 (12th edition) and 2002 (13th edition).\n\n"}
{"id": "48231632", "url": "https://en.wikipedia.org/wiki?curid=48231632", "title": "British Mid-Ocean Ridge Initiative", "text": "British Mid-Ocean Ridge Initiative\n\nThe British Mid-Ocean Ridge Initiative (the BRIDGE Programme) was a multidisciplinary scientific investigation of the creation of the Earth’s crust in the deep oceans. It was funded by the UK’s Natural Environment Research Council (NERC) from 1993 to 1999.\n\nMid-Ocean ridges are active volcanic mountain ranges snaking through the depths of the Earth’s oceans. They occur where the edges of the Earth’s tectonic plates are separating, allowing mantle rock to rise to the seafloor and harden, creating new crust. The addition of this crust can cause ocean basins to widen perpendicular to the ridge. This seafloor spreading is the engine of continental drift. At intervals along the mid-ocean ridges super-heated mineral-rich fluids are vented from the seabed. These hydrothermal vents are populated by animal and bacterial species not found elsewhere on Earth.\n\nBRIDGE investigated the geological setting of the ridge, the geochemistry of vent fluids, and ways in which biological communities survive in this apparently hostile environment. To achieve this the programme developed novel deep-ocean technologies for deployment from surface ships and manned submersibles. It also conducted experimental research into the mechanical and chemical nature of the rocks and underlying crust in these active volcanic regions. The scale of the investigation ranged from extensive regional studies mapping unexplored seafloor to microscopic and chemical analyses at individual vent sites. To achieve the programme’s objectives work was focused at five contrasting locations: the Mid-Atlantic Ridge at 24–30°N; the Mid-Atlantic Ridge at 36–39°N; Iceland and the Reykjanes Ridge to its south west; the Scotia back-arc basin (SW Atlantic); and the Lau basin (SW Pacific). Intensive localised studies were made within these areas.\n\nThe idea for a British mid-ocean ridge research programme was developed by Professors Joe Cann of Leeds University and Roger Searle of Durham University after they attended a meeting in Oregon in 1987 where the idea for a US mid-ocean ridge research programme (the RIDGE Program) was being developed. \nIn the UK researchers in many disciplines were already studying mid-ocean ridges but it was felt this research could be better integrated to produce new multidisciplinary approaches yielding results of wider significance.\n\nThe ‘BRIDGE’ branding of research commenced before research council funding was sought for a formal programme. BRIDGE was mentioned by name in \"The Independent\" newspaper in February 1989. By this time the community of researchers in this field were referring to themselves as the BRIDGE Consortium. Deep-ocean science cruises were being identified as BRIDGE cruises by 1990. The first BRIDGE newsletter appeared in 1991.\n\nOnce the idea of BRIDGE was in place an application for funding was made to the Natural Environment Research Council. This was successful and full funding commenced in 1993 for a programme that would run until 1999. The final budget was £13M.\n\n\nThis last aim was achieved directly and by participation in the international InterRidge network.\n\n\nFrom the wide range of scientific problems that could be addressed by mid-ocean ridge research, BRIDGE identified six that were of most relevance to UK research.\n\n\nThe scientific aims and objectives of the programme were directed by an international steering committee which met twice a year. The programme held a series of annual funding rounds to which scientists and engineers in the field submitted research proposals. Following a peer-review assessment of each proposal by independent referees the steering committee ranked the most highly rated proposals on their scientific merit and contribution to the programme’s objectives. This short-list was then recommended to NERC for funding.\n\nFrom 1993 to 1995 programme management (day-to-day administration and budget oversight) was undertaken by NERC head office in Swindon. A separate Science Coordinator role (incorporating, among other duties, responsibility for expanding the BRIDGE Consortium, organising national conferences and publishing the newsletter) was based at Leeds University where the BRIDGE Chief Scientist, Joe Cann, was chairman of Earth Sciences.\n\nIn 1995 NERC began contracting out programme management for their large programmes. BRIDGE programme management absorbed the science coordination role and a new programme manager was appointed, based at Leeds University. The Leeds BRIDGE office was the programme hub until the end of March 1999 after which the conclusion of the programme was administered by NERC.\n\nBRIDGE funded 44 research projects: 4 multidisciplinary; 15 geology; 6 biology; 11 studies of the hydrothermal environment at vent fields (9 of the ocean floor and 2 of the overlying water column); and 8 engineering projects to develop the required technologies. More than 200 scientists in 28 research centres around the UK contributed to this programme. There were 26 BRIDGE deep-ocean research cruises to the North Atlantic, SW Atlantic, SE Pacific, SW Pacific and Indian oceans, 18 of which were directly funded by the programme.\n\nTo discuss and publicise the programme’s results BRIDGE organised its own science conferences at Durham University (1991), the Institute of Oceanographic Sciences Deacon Laboratory (IOSDL), Wormley (1992), Leeds University (1993), Oxford University (1994), the Geological Society of London (1994, 1995 and 1997), Cambridge University (1996), Southampton Oceanography Centre (1997) and Bristol University (1998). In addition BRIDGE science was reported at other meetings nationally and internationally, for example: at the Royal Society meeting \"Mid-Ocean Ridges: Dynamics of Processes Associated with Creation of New Ocean Crust\" (1996), the 1996 British Association for the Advancement of Science annual science festival at Birmingham in a BRIDGE session entitled \"Abyssal Inferno: Seafloor volcanoes, hot vents and exotic life at the mid-ocean ridges\", at \"Geoscience 98\", Keele University (1998), at the meeting \"Technology for Deep-Sea Geological Investigations\" at the Geological Society of London (1998) and at meetings of the American Geophysical Union.\n\nThree of the BRIDGE conferences resulted in books published by the Geological Society of London, presenting in greater detail the science reported at the meetings.\n\nThroughout the programme rapid publication of results was effected through \"The BRIDGE Newsletter\". In style this was an academic journal (but without peer review) comprising BRIDGE science results together with conference announcements, meeting reports, cruise reports, updates from the mid-ocean ridge programmes of other nations and general news items of relevance to this field of research. It was published twice a year in spring and autumn. The first issue of eight stapled sheets appeared in August 1991 but after NERC funding commenced it was commercially printed and bound. By issue 10, in April 1996, it had grown to 100 pages and was being distributed to more than 600 researchers and interested parties in 20 countries. The last newsletter, No. 17, was produced in autumn 1999 as a magazine called \"The Fiery Deep, Exploring a New Earth\" summarising the programme and its results to that time. On 16 November 1999 at the Natural History Museum, London these results were presented to invited guests at a formal end of programme meeting.\n\nAs the programme ended, Joe Cann reported, \"As a result of the BRIDGE initiative, several groups of UK scientists are at the forefront of international research in mid-ocean ridge science. The areas of expertise of these scientists range from marine geophysics and geodynamics, physical and chemical oceanography, to marine biology.\" \"Every area had success. Here are a few examples. We found new pools of molten rock below the ocean floor where none was expected. We discovered large fields of hot springs, where the wisdom of the time said there should be none. We followed the strange lifecycle of the blind shrimp that live around hot springs in the Atlantic. We made sonar images of the first of a family of enormous faults that slice through the ocean floor, bringing deep rock to the surface. We showed how the flow of one of the big, hot spring fields was affected by scientific drilling. We traced the relationships between animals in hot spring communities up and down the Atlantic. We built new instruments, too, that can operate in these hostile regions\".\n\nIn addition to the results of the researches, which are still quoted, the BRIDGE Programme left an interdisciplinary community of deep-ocean scientists with a proven track record of collaboration and new equipment for working at depths of over 3,500 metres.\n\nBRIDGE had purchased for the UK research fleet a Simrad multibeam echosounder for mapping the seafloor from a surface ship. To increase detail in any geographical areas of interest it also funded upgrades to the existing UK Towed Ocean Bottom Instrument (TOBI), which made 3D images of the seabed as it was towed 300m above the ocean floor. TOBI was modified to increase its resolution, to add a gyrocompass and to add a three component magnetometer for measuring the magnetic field of the seafloor rock over which it was towed.\n\nThe BRIDGE Towed instrument (BRIDGET), was developed for hunting and studying the plumes of warm, mineral rich fluids rising into the water column from vent fields. This “hot-spring sniffer” was towed at depth behind a ship in areas where vent fields were suspected to occur and fed geochemical data back to the ship in real time.\n\nOnce fields had been detected the fluids venting from the sea-floor could be studied directly using the MEDUSA instrument. Deployed by a deep submergence vehicle, this could be placed over individual vents for extended time periods to record the characteristics of the fluids as they emerge. At the BRIDGE programme’s close six MEDUSA instruments had been built with BRIDGE funding, three more were constructed for the Geological Survey of Japan, and the next generation was being developed for various US agencies including NASA.\n\nFor examining the rock of the mid-ocean ridge a new deep ocean drill, the BRIDGE Drill, was developed which marked the core as it drilled. The marking of the core allowed the original north-south orientation of the core to be known after it had been removed. This permitted the magnetic alignment of the rock from which the sample was taken to be determined, providing information on sea-bed movements that had taken place after the rock had formed.\n\nFor study of the dispersal of animals found at the vent fields, the biologists developed a Planktonic Larval Sampler for Molecular Analysis (PLASMA). This was designed to take samples of water to catch the dispersing larvae of animals living around the vents. PLASMA could be left on the sea-bed in the vicinity of a vent field for up to a year if required, sampling at programmed intervals and preserving any larvae for DNA analysis after the recovery of the equipment.\n\nBRIDGE collected and compiled: multibeam bathymetry, sonar imagery, seismic data, electromagnetic data, gravimetry, petrology (including rock sections, cores, sediments and analytical data), chemical and physical oceanography (samples and analytical data), macro- and microbiology (specimens, film and analytical data); numerical models and audiovisual records. For the benefit of future researchers a BRIDGE data archive was lodged with the UK’s National Oceanography Centre at Southampton.\n\n"}
{"id": "23809814", "url": "https://en.wikipedia.org/wiki?curid=23809814", "title": "Chaman Fault", "text": "Chaman Fault\n\nThe Chaman Fault is a major, active geological fault in Pakistan and Afghanistan that runs for over 850 km. Tectonically, it is actually a system of related geologic faults that separates the Eurasian Plate from the Indo-Australian Plate. It is a terrestrial, primarily transform, left-lateral strike-slip fault. The slippage rate along the Chaman fault system as the Indo-Australian Plate moves northward (relative to the Eurasian Plate) has been estimated at 10 mm/yr or more. In addition to its primary transform aspect, the Chaman fault system has a compressional component as the Indian Plate is colliding with the Eurasian Plate. This type of plate boundary is sometimes called a transpressional boundary.\n\nFrom the south, the Chaman fault starts at the triple junction where the Arabian Plate, the Eurasian Plate and the Indo-Australian Plate meet, which is just off the Makran Coast of Pakistan. The fault tracks northeast across Balochistan and then north-northeast into Afghanistan, runs just to the west of Kabul, and then northeastward across the right-lateral-slip Herat fault, up to where it merges with the Pamir fault system north of the 38° parallel. The Ghazaband and Ornach-Nal faults are often included as part of the Chaman fault system. South of the triple junction, where the fault zone lies undersea and extends southwest to approximately 10°N 57°E, it is known as the Owen Fracture Zone.\n\nWhile there is general agreement that the fault is slipping at a rate of at least 10 mm/yr, there is a report of volcanic rocks in Pakistan dated to 2 m.y. BP which have been offset such as to indicate a slip rate of 25–35 mm/yr. Offsets have been described throughout the fault in Pakistan that are young enough that “only the alluvium of the bottom of active dry washes is not displaced.”\n\nThe parallel mountain ranges of eastern Balochistan, (east to west) the Kirthar Mountains, the Khude Mountains, the Zarro Mountains, the Pab Mountains and the Mor Mountains, are a result of the compressional plate boundary and are aligned parallel to the Chaman fault movement. The fault itself is west of these ranges.\n\n\n\n"}
{"id": "40087634", "url": "https://en.wikipedia.org/wiki?curid=40087634", "title": "Chaotian (geology)", "text": "Chaotian (geology)\n\nIn the geologic record the Chaotian eon or era is unofficially proposed to denote the time preceding the solidification of the Earth's crust and the formation of Earth's moon, it is the earliest era within the eon of Hadean. It lasted 196 million years, at to the beginning of the Zirconian era, . It is named after Chaos, the primeval void in Greek mythology. The Chaotian sets in with the emergence of Earth at 4.6 billion years ago. Its upper limit and thus the transition to the Zirconian era is defined by the occurrence of the first conservation capable mineral. These are zircon, the oldest mineral in the Jack Hills of Narryer Gneiss Terrane in Western Australia (Yilgarn craton) found and were dated 4,404 ± 8 million years ago \n\nAccording to first proposal, it precedes the Hadean eon and is the earliest eon in Earth's history as a planet. The end of the Chaotian was marked by the hypothetical collision of the proto-Earth and a planet-sized body named Theia, leading to the formation of the Moon.\n\nAlternatively it is defined as the first era of the Hadean eon, before the formation of the first crust on the Earth. As of 2012, it is considered to be part of a proposed revision of the Precambrian time scale.\n\nThe term 'Chaotian' is derived from mythological chaos, reflecting the chaotic conditions thought to have existed at the start of the formation of the Earth.\n\nInformation about this earliest period can therefore only be obtained indirectly, for example by astrophysical considerations and geochemical survey of meteorite material and lunar rock.\n\nThis geological era designation was proposed by NASA scientists at the Ames Research Center in 2010 to formalize terminology in the earliest stages of Earth's history.\n\nThe NASA proposal divides the Chaotian into the Eochaotian (4.7-4.65 Gya) and Neochaotian (4.65-4.6 Gya) eras, which are in turn proposed to be divided into the Nephelean (4.7-4.68 Gya) and Erebrean (4.68-4.65 Gya), and Hyperitian (4.65-4.62 Gya) and Titanomachean (4.62-4.6 Gya) periods, respectively. , this has been adopted by the IUGS.\n\nThe exact date of the birth of our solar system (and the earth) is still controversial. Calcium-aluminium-rich inclusions (CAI), the first condensate from the planetary orbit, were dated to 4,567.30 ± 0.16 million years ago by the uranium-lead method.\n\nManganese-chromium dating of chondrules revealed an age of 4.571 billion years ago. In 1979 dating of the Mundrabilla iron meteorite of Western Australia using the argon method determined an age of 4,570 ± 60 million years ago.\n\nThe proposed collision with Theia, leading to the formation of the Moon, is estimated to have occurred 4.527 billion years ago.\n\nOutgassing and differentiation of the earth in an iron-rich core and mantle may have been completed 4.45 billion years ago.\n"}
{"id": "38861011", "url": "https://en.wikipedia.org/wiki?curid=38861011", "title": "Crepe rubber", "text": "Crepe rubber\n\nCrepe rubber is coagulated latex that is rolled out in crinkled sheets, commonly used to make soles for shoes and boots but is also a raw material for further processed rubber products.\n\nColloidal latex is first mixed with formic acid to cause it to coagulate. The coagulum is processed in a \"creping battery\", a series of machines that crush, press and roll the coagula. The sheets are hung in a heated drying shed and then sorted by grade and packed for shipping.\n\nThere are several types and grades of rubber crepe, mainly distinguished by the grade and pre-processing of the latex used in their manufacture.\n"}
{"id": "1843913", "url": "https://en.wikipedia.org/wiki?curid=1843913", "title": "Desert ecology", "text": "Desert ecology\n\nDesert ecology is the study of interactions between both biotic and abiotic components of desert environments. A desert ecosystem is defined by interactions between organism populations, the climate in which they live, and any other non-living influences on the habitat. Deserts are arid regions which are generally associated with warm temperatures, however cold deserts also exist. Deserts can be found on every continent, with the largest being located in Antarctica, the Arctic, Northern Africa, and the Middle East.\n\nDeserts experience a wide range of temperatures and weather conditions. They can be classified into four types: hot, semiarid, coastal, and cold. Hot deserts have warm temperatures at all times of the year, and low annual precipitation. The lack of humidity in these deserts results in high temperatures during the daytime and large amounts of heat loss at night. Although the average annual temperature in hot deserts is 20 °C to 25 °C, extreme weather conditions can lead to temperatures from -18 °C to 49 °C. Rainfall generally occurs in concentrated bursts, followed by long periods of no precipitation. Semiarid deserts experience similar conditions to hot deserts, however the maximum and minimum temperatures tend to be less extreme, ranging from 10 °C to 38 °C. Coastal deserts are cooler than hot and semiarid deserts, with average summer temperatures between 13 °C and 24 °C. They also feature higher total rainfall values. Cold deserts are similar in temperature to coastal deserts, however they receive more annual precipitation, which much of it in the form of snow. Deserts are most notable for their dry climates resulting from rain-blocking mountain ranges and remoteness from oceanic moisture. Deserts occupy one-fifth of the Earth's land surface and occur in two belts: between 15° and 35° latitude in both the southern and northern hemispheres. These bands are associated with the high solar intensities that all areas in the tropics receive, and at the same time with dry air brought down by the descending arms of Hadley and Ferell atmospheric circulation cells.\n\nDeserts generally occur because of global wind patterns or rain shadows. Dry winds not only hold little moisture for these areas, but also tend to evaporate any water present. Rain shadows do not occur because of general wind patterns but rather, due to the wind passing over a mountain range. As air rises and cools, its relative humidity increases and some or most moisture rains out, leaving little to no water vapour to form precipitation on the other side of the mountain range.\n\nMany desert ecosystems are limited by available water levels, rather than rates of radiation or temperature. Water flow in these ecosystems can be thought of as similar to energy flow; in fact, it is often useful to look at water and energy flow together when studying desert ecosystems and ecology.\n\nWater availability in deserts may also be hindered by loose sediments. Dust clouds commonly form in windy, arid climates. Scientists have previously theorised that desert dust clouds would enhance rainfall, however some studies have shown that precipitation is actually inhibited by this phenomenon by absorbing moisture from the atmosphere. This absorption can result in a positive feedback loop, which leads to further desertification.\n\nDesert landscapes can contain a wide variety of geological features, such as oases, rock outcrops, dunes, and mountains. Dunes are structures formed by wind moving sediments into mounds. Desert dunes are generally classified based on their orientation relative to wind directly. Possibly the most recognisable dune type are transverse dunes, whose crests are transverse to the wind direction. Many dunes are considered to be active, meaning that they travel and change over time due to the influence of the wind. However, some dunes can be anchored in place by vegetation or topography, preventing their movement. Some dunes may also be referred to as sticky, which occurs when the individual grains of sand become cemented together. Sticky dunes tend to be more stable and resistant to wind reworking than loose dunes.\n\nAnalysis of geological features in desert environments can reveal a lot about the history of the area. Through observation and identification of rock deposits, geologists are able to interpret the order of events that occurred during desert formation. For example, research conducted on the surface geology of the Namib Desert allowed geologists to interpret ancient movements of the Kuiseb River based on rock ages and features identified in the area.\n\nDeserts support diverse communities of plant and animals that have evolved resistance to, and methods of circumventing of, the extreme temperatures and arid conditions. For example, desert grasslands are more humid and slightly cooler than its surrounding ecosystems. Many animals get their energy by eating plants, but desert plants give up the fruit of their production very reluctantly. To avoid intense temperatures, the majority of small desert mammals are nocturnal, and dig burrows in which they live. These burrows help maintain to prevent overheating and resulting loss of water, and to maintain the mammal's optimal temperature. Desert ecology is characterized by dry, alkaline soils, low net production and opportunistic feeding patterns by herbivores and carnivores. Other organisms' survival tactics are physiologically based. Such tactics include the completion of life cycles ahead of anticipated drought seasons, and storing water with the help of specialized organs.\n\nDesert climates are particularly demanding on endothermic organisms. In environments where the external temperature is less than their body temperature, most endotherms are able to balance heat production and heat loss to maintain a comfortable temperature. However, in deserts where air and ground temperatures exceed body temperature, endotherms must be able to dissipate the large amounts of heat being absorbed in these environments. In order to cope with extreme conditions, desert endotherms have adapted through the means of avoidance, relaxation of homeostasis, and specializations. Nocturnal desert rodents, like the kangaroo rat, will spend the daytime in cool burrows deep underground, and emerge at night to seek food. Birds are much more mobile than ground-dwelling endotherms, and can therefore avoid heat-induced dehydration by flying between water sources. To prevent overheating, the body temperatures of many desert mammals have adapted to be much higher than non-desert mammals. Camels, for example, can maintain body temperatures that are about equal to typical desert air temperatures. This adaptations allows camels to retain large amounts of water for extended periods of time. Other examples of higher body temperature in desert mammals include the diurnal antelope ground squirrel, and the oryx. Certain desert endotherms have evolved very specific and unique characteristics to combat dehydration. Male sandgrouse have specialized belly feathers that are able to trap and carry water. This allows the sandgrouse to provide a source of hydration for their chicks, who do not yet have the ability to fly to water sources themselves.\n\nAlthough deserts have severe climates, some plants still manage to grow. Plants that can survive in arid deserts are called xerophytes, meaning they are able to survive long dry periods. Such plants may close their stomata the daytime and open them at night, at the time when a plant may load carbon dioxide while, thanks to lower temperatures, losing less water to evaporation.\n\nAdaptations in xerophytes include resistance to heat and water loss, increased water storage capabilities, and reduced surface area of leaves. One of the most common families of desert plants are the cacti, which are covered in sharp spines or bristles for defence. The bristles on certain cacti also have the ability to reflect sunlight, such as those of the old man cactus. Certain xerophytes, like oleander, feature stomata that are recessed as a form of protection against hot, dry desert winds, which allows the leaves to retain water more effectively. Another unique adaptation can be found in xerophytes like ocotillo, which are \"leafless during most of the year, thereby avoiding excessive water loss\".\n\nThe harsh climate of most desert regions is a major obstacle in conducting research into these ecosystems. In the environments requiring special adaptations to survive, it is often difficult or even impossible for researchers to spend extended periods of time investigating the ecology of such regions. To overcome the limitations imposed by desert climates, some scientists have used technological advancements in the area of remote sensing and robotics. One such experiment, conducted in 1997, had a specialised robot named Nomad travel through a portion of the Atacama Desert. During this expedition, Nomad travelled over 200 kilometres and provided the researchers with many photographs of sites visited along its path.\n\n"}
{"id": "14612089", "url": "https://en.wikipedia.org/wiki?curid=14612089", "title": "Desertec", "text": "Desertec\n\nDESERTEC was a large-scale project supported by a foundation of the same name and the consortium Dii (Desertec industrial initiative) created in Germany as a limited liability company (GmbH). The project aimed at creating a global renewable energy plan based on the concept of harnessing sustainable power from sites where renewable sources of energy are more abundant and transferring it through high-voltage direct current transmission to consumption centers. All kinds of renewable energy sources are envisioned, but the sun-rich deserts of the world play a special role.\n\nThere are some parallels between Desertec and the Atlantropa project plan in the 1920s. Atlantropa aimed to integrate Europe and Northern Africa and its electricity grid based on a giant hydro power station at Gibraltar. The industry platform Dii is continuing to pave the way for renewables and grid integration on a very pragmatic basis.\n\nDESERTEC was developed by the Trans-Mediterranean Renewable Energy Cooperation (TREC), a voluntary organization founded in 2003 by the Club of Rome and the National Energy Research Center Jordan, made up of scientists and experts from across Europe, the Middle East and North Africa (EU-MENA). \nIt is from this network that the DESERTEC Foundation later emerged as a non-profit organization tasked with promoting the DESERTEC solution around the world. Founding members of the foundation are the German Association of the Club of Rome, members of the network of scientists TREC as well as committed private supporters and long-time promoters of the DESERTEC idea. \nIn 2009, the DESERTEC Foundation founded the Munich-based industrial initiative 'Dii GmbH' together with partners from the industrial and finance sectors. Its task is to accelerate the implementation of the DESERTEC Concept in the focus region EU-MENA.\n\nThe scientific studies done by the German Aerospace Center (DLR) between 2004 and 2007 demonstrated that the desert sun could meet rising power demand in the MENA region while also helping to power Europe, reduce carbon emissions across the EU-MENA region and power desalination plants to provide freshwater to the MENA region. Dii GmbH published a further study called Desert Power 2050 in June 2012. It found that the MENA region would be able to meet its needs for power with renewable energy, while exporting its excess power to create an export industry with an annual volume of more than €60 billion. Meanwhile, by importing desert power, Europe could save around €30/MWh.\nBy taking into account land and water use, DESERTEC intends to offer an integrated and comprehensive solution to food and water shortages.\n\nAtlantropa was a gigantic engineering project devised by Herman Sörgel in the 1920s and promulgated by him until his death in 1952. Its central feature was a hydroelectric dam to be built across the Strait of Gibraltar, which would have provided enormous amounts of hydroelectricity for the whole Mediterranean region and lowered the surface of the Mediterranean Sea by up to , opening up large new lands for settlement. The project reached great popularity in the late 1920s and early 1930s, and for a short period again, in the late 1940s and early 1950s, but was forgotten after Sörgel's death 1952.\n\nThe overall technocratic approach to strengthen political unity across the Mediterranean Sea via a large-scale infrastructure project has some parallels to Desertec, as well some technical aspects as the idea of fostering a cross-border electricity grid in the region. A major difference lies in the stepwise approach taken by Desertec, which took previous failures of large-scale technical visions into account. Atlantropa relied on a single large-scale infrastructure and was never, despite its longstanding popularity, close to technical realization. Desertec was led by a similar macro-engineering vision but attempted to start via small-scale steps, which at least partially succeeded.\n\nThe DESERTEC concept was originated with Dr Gerhard Knies, a German particle physicist and founder of the Trans-Mediterranean Renewable Energy Cooperation (TREC) network of researchers. In 1986, in the wake of the Chernobyl nuclear accident, he was searching for a potential alternative source of clean energy and arrived at the following remarkable conclusion: in just six hours, the world's deserts receive more energy from the sun than humankind consumes in a year. The DESERTEC concept was developed further by TREC – an international network of scientists, experts and politicians from the field of renewable energies – founded in 2003 by the Club of Rome and the National Energy Research Center Jordan. One of the most famous members was Prince Hassan bin Talal of Jordan. In 2009, TREC emerged to the non-profit DESERTEC Foundation.\n\nThe DESERTEC Foundation was founded on 20 January 2009 with the aim of promoting the implementation of the DESERTEC Concept for clean power from deserts all over the world. It is a non-profit organisation based in Hamburg. The founding members were the German Association of the Club of Rome, members of the TREC network of scientists as well as committed private supporters and long-time promoters of the DESERTEC idea.\n\nThe DESERTEC Foundation has two directors: Andreas Huber and Manfred Bohnen. Roland Berger is DESERTEC Curator.\n\nThe foundation's missions is to accelerate the implementation of the DESERTEC Concept by:\n\n\nTo help accelerate the implementation of the DESERTEC idea in EU-MENA, the non-profit DESERTEC Foundation and a group of 12 European companies led by Munich Re founded an industrial initiative called Dii GmbH in Munich on 30 October 2009. The other companies included Deutsche Bank, E.ON, RWE, Abengoa. \nLike the DESERTEC Foundation, Dii GmbH did not intend to build power plants itself. Instead it focused on four core objectives in EU-MENA:\n\nDii GmbH aimed to create a positive investment climate for renewable energies and interconnected power grid in North Africa and the Middle East by encouraging the necessary technological, economic, political and market frameworks. This included the development of a long-term implementation perspective called Desert Power 2050 with guidance on investment and funding. Dii GmbH has initiated selected reference projects to demonstrate overall feasibility and reduce system overall costs.\n\nOn 24 November 2011, a memorandum of understanding (MoU) was signed between the Medgrid consortium and Dii to study, design and promote an interconnected electrical grid linking DESERTEC and the Medgrid projects. The Medgrid together with DESERTEC would serve as the backbone of the European super grid and the benefits of investing in HVDC technology are being assessed to reach the final goal – the supersmart grid. The activities of Dii and Medgrid were covered by the Mediterranean Solar Plan (MSP), a political initiative within the framework of the Union for the Mediterranean (UfM).\n\nThe company was formed by the DESERTEC foundation and a consortium of worldwide companies.\n\nAs of March 2014, Dii consisted of 20 shareholders (listed below) and 17 associate partners.\nManaging Director of Dii GmbH has been Paul van Son, a senior international energy manager. End of 2014, most shareholders did not extend their contract with Dii after it had accomplished its initial mission. RWE, State Grid Corporation of China and ACWA Power stayed on board and a number of partner companies to drive the new mission of Dii: \"To facilitate the rapid deployment of utility-scale renewable energy projects in desert areas, and to integrate them in the interconnected power systems\"\n\nDESERTEC is a global renewable energy solution based on harnessing sustainable power from the sites where renewable sources of energy are at their most abundant. These sites can be used thanks to low-loss High-Voltage Direct Current transmission. All kinds of renewables will be used in the DESERTEC Concept, but the sun-rich deserts of the world play a special role.\nThe original and first region for the assessment and application of this concept is the EU-MENA region (European Union, Middle East and Northern Africa). The DESERTEC organisations promote the generation of electricity in North Africa, the Middle East and Europe using renewable sources, such as solar power plants, wind parks, and develop a Euro-Mediterranean electricity network, primarily made up of high voltage direct current (HVDC) transmission cables. Despite its name, DESERTEC's proposal would see most of the power plants located outside of the Sahara Desert itself but rather in the surrounding areas, in the more accessible North and South steppes and woodlands, as well as the relatively moist Atlantic Coastal Desert. Under the DESERTEC proposal, concentrating solar power systems, photovoltaic systems and wind parks would be spread over the wide desert regions in North Africa like the Sahara Desert and all its subdivisions. The generated electricity would be transmitted to European and African countries by a super grid of high-voltage direct current cables. It would provide a considerable part of the electricity demand of the MENA countries and furthermore provide continental Europe with 15% of its electricity needs. Exported desert power would complement Europe's transition to renewables which would be based primarily on harnessing domestic sources of energy that would increase its energy independence. According to a scenario by the German Aerospace Center (DLR), by 2050, investments into solar plants and transmission lines would be total €400 billion. An exact proposal how to realize this scenario, including technical and financial requirements, will be designed by 2012/2013 (see Desert Power 2050).\n\nIn March 2012, the DESERTEC Foundation started working in a further focus region. A year after the nuclear disaster in Fukushima, the DESERTEC Foundation and the Japan Renewable Energy Foundation (JREF) have signed a MoU. They will exchange knowledge and know-how, and coordinate their work together to develop suitable framework conditions for the deployment of renewables and to establish transnational cooperation in Greater East Asia. The aim is to accelerate the deployment of renewable energy in Asia to provide secure and sustainable alternatives to fossil and nuclear power. As a part of its mission, JREF promotes the Asia Super Grid Initiative to facilitate an electricity system based fully on renewable energy. The DESERTEC Foundation sees such a grid as an important step towards the implementation of DESERTEC in Greater East Asia and has already conducted a feasibility study on potential grid corridors to make best use of the region’s desert sun.\n\nThe DESERTEC Concept was developed by an international network of politicians, academics and economists, called TREC. The research institutes for renewable sources of the governments of Morocco (CDER), Algeria (NEAL), Libya (CSES), Egypt (NREA), Jordan (NERC) and Yemen (Universities of Sana'a and Aden) as well as the German Aerospace Center (DLR) made significant contributions towards the development of the DESERTEC Concept. The basic studies relating to DESERTEC were led by DLR scientist Dr. Franz Trieb working for the Institute for Technical Thermodynamics at the DLR. The three studies were funded by the German Federal Ministry for the Environment, Nature Conservation, and Nuclear Safety (BMU). The studies, conducted between 2004 and 2007, evaluated the following as shown in the table below;\n\nThe studies concluded that the extremely high solar radiation in the deserts of North Africa and the Middle East outweighs the 10–15% transmission losses between the desert regions and Europe. This means that solar thermal power plants in the desert regions are more economical than the same kinds of plants in southern Europe. The German Aerospace Center has calculated that if solar thermal power plants were to be constructed in large numbers in the coming years, the estimated cost of electricity would come down from 0.09–0.22 euro/kWh to about 0.04–0.05 euro/kWh.\n\nThe Sahara Desert was chosen as an ideal location for solar farms as it is exposed to bright sunshine nearly all the time, roughly between 80% and 97% of the daylight hours in the best cases. This is the sunniest year-round area on the planet. In the world's largest hot desert, there is an extremely vast area, covering almost the whole desert, that receives more than 3,600 h of yearly sunshine. There is also a very large area in excess of 4,000 h of sunshine annually. The highest solar radiation received on the planet is in the Sahara Desert, under the Tropic of Cancer. This results from a general, strong lack of cloud cover year-round and a geographical position under the tropics.\n\nThe annual average insolation, which represents the total amount of solar radiation energy received on a given area and on a giver period, is about 2,500 kWh/(m² year) over the region and this number can soar up to almost 3,000 kWh/(m² year) in the best cases. The weather features of the Sahara Desert, especially the insolation, have a pronounced nature. The annual electricity production reaches 1,300,000 TWh at maximum in this sun-drenched area if the whole desert is covered in solar panels.\nThe desert is also extremely vast covering about some 9,000,000 km² (3,474,920 sq mi), being almost as large as China or the United States and is sparsely populated, making it possible to set up large solar farms without a negative impact on inhabitants of the region, too. Lastly, sand deserts can provide silicon, a raw material that is essential in the production of solar panels.\n\nThe great African desert is relatively cloud-free all year long but it's important to note the harsh, desert climate also has some negative features such as extreme heat and sometimes dust or sand-laden winds which frequently blow over the desert and can even result in severe duststorms or sandstorms. Both phenomenons reduce the solar electricity productivity and the efficiency of the solar panels.\n\nDii announced it will introduce a roll-out-plan in late 2012 which includes concrete recommendations how to enable investments in renewable energy and interconnected power grids. Dii claims working with all key stakeholders from the international scientific and business communities as well as policy-makers and civil society to enable two or three concrete reference projects to demonstrate the feasibility of the long-term vision.\nDii developed a strategic framework for a fully integrated and decarbonized power system based on renewable energies for the entire North Africa, Middle East and Europe (EUMENA) region in 2050. Therefore, Dii researched from the viewpoint of technology and geography what is the optimal mix of renewable energies to provide the EUMENA region with sustainable energy. In July 2012 Dii presented the first part of its study “Desert Power 2050 – Perspectives on a Sustainable Power System for EUMENA.\n\nKey Findings\n\"Desert Power 2050\" demonstrates that the abundance of sun and wind in the EUMENA region will enable the creation of a joint power network that will entail more than 90 percent renewables. According to the study such a joint power network involving North Africa, the Middle East and Europe (EUMENA) offers clear benefits to all involved. The nations of the Middle East and North Africa (MENA) could meet their expanding needs for power with renewable energy, while developing an export industry from their excess power with could reach an annual volume worth more than 60 billion euros, according to the study results. By importing up to 20 percent of its power from the deserts, Europe could save up to 30 euros for each megawatt hour of desert power.\n\nThe north and south would become the powerhouses of this joint network, supported by wind and hydropower in Scandinavia, as well as wind and solar energy in the MENA region. Supply and demand would complement one other – both regionally and seasonally – according to the findings of \"Desert Power 2050\". With its constant supply of wind and solar energy throughout the year, the MENA region can cover Europe's energy needs without the latter having to build costly excess capacities. A further benefit of the power network is the enhanced security of supply to all nations concerned. A renewables based network would lead to mutual reliance among the countries involved, complemented by inexpensive imports from the south and the north.\n\nMethodology\n\"Desert Power 2050\" presents the full perspective of the EUMENA region, which includes, for instance, the growing consumption of power in the MENA states. The power requirements of the MENA states are likely to more than quadruple by 2050, totalling more than 3000 terawatt hours. Unlike in Europe, the population will also grow considerably by the middle of the century, thus heightening the demand for new jobs. \nAnalysing the design of a power system built to include more than 90% renewables 40 years into the future is necessarily subject to major uncertainties on a range of assumptions. To address these uncertainties Dii analysed so-called sensitivities, or perspectives, to show how the results react to changed parameters. Dii has analysed a total of 18 perspectives on the EUMENA power supply in 2050. They cover a wide range of major impact factors on the attractiveness of power system integration. The main message of the study: grid integration across the Mediterranean is valuable under all foreseeable circumstances.\n\nSecond Phase\nDesert energy could be a stimulus for growth and make an important contribution when it comes to coping with the social and economic challenges in North Africa and the Middle East. Dii announced that a second phase of Desert Power 2050, \"Getting Started\", will examine this topic in greater depth in the next few months, with discussions including political, scientific and industrial stakeholders. The objective is to formulate recommendations for the regulatory steps required in the years to come.\n\nMore energy falls on the world's deserts in six hours than the world consumes in a year, and the Saharan desert is virtually uninhabited and is close to Europe. Supporters say that the project will keep Europe \"at the forefront of the fight against climate change and help North African and European economies to grow within greenhouse gas emission limits\". DESERTEC officials say the project could one day deliver 15 percent of Europe's electricity and a considerable part of MENA's electricity demand. According to the DESERTEC Foundation, the project has strong job creation potential and could improve the stability in the region. According to the report by Wuppertal Institute for Climate, Environment and Energy and the Club of Rome, the project could create 240,000 German jobs and generate €2 trillion worth of electricity by 2050.\n\nConcentrated solar power (also called concentrating solar power and CSP) systems use mirrors or lenses to concentrate a large area of sunlight, or solar thermal energy, onto a small area. Electrical power is produced when the concentrated light is converted to heat, which drives a heat engine (usually a steam turbine) connected to an electrical power generator. Molten salt can be employed as a thermal energy storage method to retain thermal energy collected by a solar tower or solar trough so that it can be used to generate electricity in bad weather or at night. Since solar fields feed their heat energy into a conventional generation unit with a steam turbine, they can be combined without any problem with fossil fuel hybrid power plants. This hybridization secures energy supply also in unfavourable weather and at night without the need of accelerating costly compensatory plants. A technical challenge is the cooling which is necessary for every heating power system. Dii is therefore reliant either on an adequate water supply, coastal facilities or improved cooling technology.\n\nDii also considers photovoltaics (PV) as a technology suitable for desert power plants. Photovoltaics is a method of generating electrical power by converting solar radiation into direct current electricity using semiconductors. Photovoltaic power generation employs solar panels composed of a number of solar cells containing a photovoltaic material. Materials presently used for photovoltaics include monocrystalline silicon, polycrystalline silicon, amorphous silicon, cadmium telluride, and copper indium gallium selenide/sulfide. Driven by advances in technology and increases in manufacturing scale and sophistication, the cost of photovoltaics has declined steadily since the first solar cells were manufactured.\n\nIn 2010, First Solar, a producer of thin film solar panels, joined Dii as associated partner. The US based company already has experience with huge PV installations, and has constructed the 550 megawatt Desert Sunlight Solar Farm and Topaz Solar Farm in California, which are the biggest two PV installations of the world.\n\nAs also parts of the desert regions in the Middle East and North Africa (MENA) come with high wind potential, Dii is examining in which geographic regions the installation of wind farms is suitable. Wind turbines produce electricity by wind turning the blades, which spin a shaft, which connects to a generator which produces electricity. The Sahara Desert is one of the windiest areas on the planet, especially on the western coast where lies the Atlantic coastal desert along Western Sahara and Mauritania. The annual average wind speed at the ground greatly exceeds 5 m/s in most of the desert, and even approach 8 m/s or 9 m/s along the western ocean coast. It's important to note that wind speed increases with height. The regularity and the constancy of winds in arid regions are major assets for wind energy, too. The winds blow nearly constantly over the desert and there are generally no windless days during throughout the year. Therefore, the desert of North Africa is also an ideal location to install large-scale wind parks and wind turbines with very good productivity.\n\nTo export renewable energy produced in the MENA desert region, a high-voltage direct current (HVDC) electric power transmission system is needed. High Voltage DC (HVDC) technology is a proven and economical method of power transmission over very long distances and also a trusted method to connect asynchronous grids or grids of different frequencies. With HVDC energy can also be transported in both directions. For long-distance transmission HVDC suffers lower electrical losses than alternating current (AC) transmission. Because of the higher solar radiation in MENA, the production of energy, even with the included transmissions losses, is still advantageous over the production in South Europe.\n\nAlso very long distance projects have already been realized with technological cooperation from ABB and Siemens – both shareholders of Dii; namely the 800 kV HVDC Xiangjiaba-Shanghai transmission system, which was commissioned by State Grid Corporation of China (SGCC) in June 2010. The HVDC link is the most powerful and longest transmission of its kind to be implemented anywhere in the world; and at the time of commissioning, transmitted 6,400 MW of power over a distance of nearly 2,000 kilometres. This is longer than would be needed to link MENA and Europe. Siemens Energy has equipped the sending converter station Fulong for this link with ten DC converter transformers, including five rated at 800 kV.\n\nThe second HVDC project which is also for SGCC with cooperation from ABB, is a new HVDC link of 3,000 MW over 920 kilometres from Hulunbeir, in Inner Mongolia, to Shenyang in the province of Liaoning in the North-Eastern part of China in 2010. Another project scheduled for 2014 commissioning – is the construction of an ±800 kV North-East UHVDC link from the North-Eastern and Eastern region of India to the city of Agra across a distance of 1,728 kilometres.\n\nAnother project of this type is the Rio Madeira HVDC system a HVDC link of .\n\nThe Sahara Desert covers huge parts of Algeria, Chad, Egypt, Libya, Mali, Mauritania, Morocco, Niger, Western Sahara, Sudan and Tunisia. It is one of three distinct physiographic provinces of the African massive physiographic division.\n\nThe first solar and wind power projects in North Africa have already begun. Algeria initiated a unique project in 2011 dealing with Hybrid power generation which combines a 25 MW concentrating solar power array in conjunction with a 130 MW combined cycle gas turbine plant Hassi R'Mel integrated solar combined cycle power station.\n\nOther countries like Morocco have set up ambitious plans on the implementation of renewable energy. The Ouarzazate solar power station in Morocco for example, with the capacity of 500 MW, will be one of the largest concentrated solar plants in the world.\n\nIn 2011, the DESERTEC Foundation started to evaluate projects that could serve as models for the implementation of DESERTEC according to its sustainability criteria. The first of these is the TuNur solar power plant in Tunisia that is planned to have 2 GW of capacity. Creating up to 20,000 direct and indirect local jobs, its plants include dry-cooling systems that reduce water usage by up to 90%. Construction is planned to begin in 2014, and export power to Italy by 2016. A video on YouTube is explaining this project.\n\nTalks with the Moroccan government had been successful and the Dii confirmed their first reference project would be in Morocco. As a partner in a beginning partnership between Europe and MENA Morocco is especially well-suited since a grid connection from Morocco via Gibraltar to Spain already exists. Also the Moroccan government enacted a program to support renewable energies. In June 2011, Dii signed a Memorandum of Understanding with the Moroccan Agency for Solar Energy (MASEN). MASEN will act as a project developer and will be responsible for all important project steps in Morocco. Dii will promote the project and its financing in the European Union in Brussels as well as in national governments. This reference project, with a total capacity of 500 MW, will be a combination of concentrated solar power plants (400 MW) and photovoltaics (100 MW). The first available power from the joint Dii/MASEN project could be fed into the Moroccan and Spanish grids between 2014 and 2016, depending on the selected technology and market conditions. Based on the current estimate the total costs are €2 billion.\n\nIn April 2010, Dii emphasized that the power plant won’t be installed in the region of Western Sahara which is administered by Morocco. An official spokesperson of Dii made the following confirmation: \"Our reference projects will not be located in the region. When looking for project sites, DESERTEC Industrial Initiative will also take political, ecological or cultural issues into consideration. This procedure is in line with the funding policies of international development banks.\"\n\nIn Tunisia, STEG Énergies Renouvelables, a subsidiary of the Tunisian state utility company STEG, and Dii are currently working on a pre-feasibility study. The study focuses on substantial solar and wind energy projects in Tunisia. Research will address the technical and regulatory conditions for the supply of energy in local networks for the export of power to neighbouring countries as well as Europe. Besides financing of the project will be analysed.\n\nAlgeria, which offers excellent conditions for renewable energy, is considered as a potential location for a further reference project. In December 2011, the Algerian energy supplier Sonelgaz and Dii signed a Memorandum of Understanding on their future collaboration in the presence of EU Energy Commissioner Günther Oettinger and the Algerian Minister for Energy and Mining Youcef Yousfi. The focus of this cooperation will be the strengthening and the exchange of technical expertise, joint efforts in market development and the progress of renewable energy in Algeria as well as in foreign countries.\n\nSince the Euro-Mediterranean projects, Medgrid and DESERTEC are both attempting to generate solar energy from deserts and complement each other, a MoU was signed on 24 November 2011 between Medgrid and Dii to study, design and promote an interconnected electrical grid linking both projects. The plan is to build five interconnections at a cost of around 5 billion euros ($6.7 billion), including between Tunisia and Italy. The activities of Dii and Medgrid are covered by the Mediterranean Solar Plan (MSP), a political initiative within the framework of the Union for the Mediterranean (UfM).\n\nIn March 2012 Dii, Medgrid, Friends of the supergrid and Renewables Grid Initiative signed a joint declaration to support the effective and complete integration, in a single electricity market, of renewable energy from both large-scale and decentralised sources, which shall not be played out against each other in Europe and in its neighbouring regions.\n\nSome experts – such as Professor Tony Day, director of the Centre for Efficient and Renewable Energy in Building at London South Bank University, Henry Wilkinson of Janusian Security Risk Management, and Wolfram Lacher of Control Risks consultancy – are concerned about political obstacles to the project. Generating so much of the electricity consumed in Europe and in Africa would create a political dependency on North African countries which had corruption before Arab Spring and a lack of cross-border coordination. Moreover, DESERTEC would require extensive economic and political cooperation between Algeria and Morocco, which is at risk as the border between the two countries is closed due to a disagreement over the Western Sahara, Inram Kada by EUMENA, is responsible for expediting the project. Cooperation between the states of Europe and the states of the Middle East and North Africa is also certain to be challenging. Large scale cooperation necessary between the EU and the North African nations the project may be delayed due to bureaucratic red tape and other factors such as expropriation of assets.\n\nThere are also concerns that the water requirement for the solar plant to clean dust off panels and for turbine coolant may be detrimental to local populations in terms of the demand it will place on the local water supply. An EU innovation supported project however resulted in the development of a silicone based film with a nano-dentrite structure on it. The film is fused on top of the solar panels and the nano-dentrite structure makes that sand, water, salt, bacteria, molds, etc. can't attach to the photovoltaic panels.\nOpposed to this, studies point out the generation of fresh water by the solar thermal plants. Furthermore, no significant amount of water is needed for cleaning and cooling, since alternative technologies can be used (dry cleaning, dry cooling). However, dry cooling is more expensive, technologically challenging and less efficient than the water cooling currently planned. Plans for water desalination for cooling purposes are not part of the DESERTEC business plan or cost estimates as proposed.\n\nThe late Hermann Scheer (Eurosolar) pointed out that the doubled solar radiation in the Sahara can not be the only criterion especially with its continuous trade winds there .\n\nTransmitting energy over long distances has been criticized, with questions raised over the cost of cabling compared to energy generation, and over electricity losses. However, the study and current operating technology show that electricity losses using high-voltage direct current transmission amount to only 3% per 1,000 km (10% per 3,000 km).\n\nInvestment may be required within Europe in a \"supergrid\". In response, one proposal is to cascade power between neighbouring states so that states draw on the power generation of neighbouring states rather than from distant desert sites.\n\nOne key question will be the cultural aspect, as Middle Eastern and African nations may need assurance that they will own the project rather than it being imposed from Europe.\n\n"}
{"id": "6885984", "url": "https://en.wikipedia.org/wiki?curid=6885984", "title": "East Kolkata Wetlands", "text": "East Kolkata Wetlands\n\nThe East Calcutta Wetlands, (22 0 27’ N 88 0 27’ E), are a complex of natural and human-made wetlands lying east of the city of Calcutta (Kolkata), of West Bengal in India. The wetlands cover 125 square kilometers, and include salt marshes and salt meadows, as well as sewage farms and settling ponds. The wetlands are used to treat Kolkata's sewage, and the nutrients contained in the waste water sustaining fish farms and agriculture.\n\nThe name East Calcutta Wetlands was coined by Dr. Dhrubajyoti Ghosh, Special Advisor(Agricultural Ecosystems), Commission on Ecosystem Management,IUCN, who reached this incredible but neglected part of the city searching the answer to a question: What exactly happens to the city sewage? These natural water bodies which were known just as fisheries provided the answer. Devised by local fishermen and farmers, these wetlands served, in effect, as the natural sewage treatment plant for the city. The East Kolkata Wetlands host the largest sewage fed aquaculture in the world. . After the decision to extend Salt Lake City by converting more wetlands in the area, a Public Interest Litigation (PIL) by CSOs. saved the Wetlands by a landmark judgement of Justice Umesh Chandra Banerjee of Calcutta High Court.\n\nThe East Calcutta Wetlands were designated a \"wetland of international importance\" under the Ramsar Convention on 19 August 2002.\n\nThere are about 100 plant species, which have been recorded in and around the East Calcutta Wetlands. These include\nsagittaria montividensis, cryptocoryene ciliata, cyperus spp., crostichum aureum, lpomoea aquatica, etc. The sunderbans used to extend up to patuli in the 1950s.\n\nSeveral kinds of water hyacinths grow across these wetlands. Local farmers and fisher folk use water hyacinth to create a buffer between land and water to minimize erosion.\n\nThe area is also home to large numbers of coconut and betel nut trees. Many varieties of vegetables are farmed here, including cauliflower, eggplant, pumpkin, sunflower and sacred basil. Tracts of land are dedicated to paddy cultivation as well.\n\nNumerous species of fish are farmed in the sewage fed ponds called bheris in the East Kolkata wetlands. These include silver carp, tilapia,\nThe area is also home to marsh mongoose and small Indian mongoose. Palm Civet and Small Indian Civet are significant in and around East Calcutta Wetlands. Approximately 20 mammals are reported from this region. Snakes found in the East Calcutta Wetland include Checkered keel back (Xenochrophis piscator), Smooth water snake (Enhydris enhydris), Buff striped keel back (Amphiesma stolata), and Bronze back tree snake (Tendrelaphis pristis) It is the Type locality of a mammalian species, called Salt Lake Marsh Mongoose. Over 40 species of birds can be spotted at the wetlands. The process of urbanisation however, is leading to the disappearance of many bird species from the area.(Ghosh,A,K, 2004)\n\nKolkata is an example of how natural wetlands are sometimes being utilized in developing countries. Using the purification capacity of wetlands, the Indian city of Kolkata (Calcutta) has pioneered a system of sewage disposal. Built to house one million people, Kolkata is now home to over 10 million, many living in slums. But the 8,000-hectare East Kolkata Wetlands Ramsar Site, a patchwork of tree-fringed canals, vegetable plots, rice paddies and fish ponds – and the 20,000 people that work in them – daily transform one-third of the city's sewage and most of its domestic refuse into a rich harvest of fish and fresh vegetables. For example, the Mudially Fishermen's Cooperative Society is a collective of 300 families that lease 70 hectares into which wastewater from the city is released. Through a series of natural treatment processes – including the use of Eichhornia crassipes and other plants for absorbing oil, grease and heavy metals – the Cooperative has turned the area into a thriving fish farm and nature park.\n\nRecently illegal landfills are on the rise and the wetlands are being slowly assimilated in the stream city. This unprecedented land development and urbanization are creating concerns about the impact on the environment. This is because the wetlands serve as a natural sponge absorbing excess rainfall and doing its bit to reduce pollution.\nWetlands are under threat due to exponential expansion of real-estate projects in eastern Kolkata especially in the Salt Lake and Rajarhat sectors.\n\nMicrobial Diversity is an integral part of biodiversity which includes bacteria, archaea, fungi, algae, protozoa and protists (Ghosh, A., 2007). East Kolkata Wetland shows an immense diversity of flora and fauna both at the macro and micro level. Microbial richness of a region is its unseen asset that needs to be explored and conserved. Soil samples collected from ECW shows the presence of various new strains of microbes which are not only ecologically important but also have commercial value (Ghosh, A., 2007). These include Actinobacteria which are responsible for the degradation of nitrophenol, nitroaromatic compounds, pesticides and herbicides; Proteobacteria related to the bioremediation of heavy metals, degradation and recycling of woody tissues of plants, oil contaminated soil and toxic compounds and nitrogen fixation along with the cyanobacters; other bacteria playing important roles in metal accumulation, oil degradation, antimicrobial compound production, enzyme production etc. (Ghosh, A., 2007).\n\nThe East Kolkata Wetlands (Conservation and Management) Act, 2006 represents an important landmark as it paved way for establishment of the East Kolkata Wetlands Management Authority (EKWMA) for conservation and management of the EKW. The EKWMA is constituted under Section 3 of the Act, 2006. In 2017 Section 3(2) of the Act, 2006 has been amended and the composition of the EKWMA has been changed. The EKWMA is a thirteen (13) member body with the Chief Secretary, Government of West Bengal, Secretaries of different Departments of State Govt. as well as four experts nominated by the State Govt. under the chairmanship of Minister-in-Charge, Department of Environment, Government of West Bengal.\nThe EKWMA is guided by the East Kolkata Wetlands (Conservation and Management) Act, 2006, the East Kolkata Wetlands (Conservation and Management) Rules, 2006, and the Wetlands (Conservation and Management) Rules, 2017.\n\n1. Urban Wastewater: Livelihoods, Health and Environmental Impacts in India: The Case of the East Calcutta Wetlands by Gautam Gupta, Jadavpur University (see: www.iwmi.cgiar.org/ ... /Urban%20Wastewater%20WS_Kolkata.pdf ).\n\n2. S.Ray Choudhury, A. R. Thakur. Microbial Genetic Resource Mapping of East Kolkata Wetland. Current Science, Vol. 91, No. 2, 2006 25 July.\n\n3. Ghosh, A., Maity, B., Chakrabarti, K., Chattopadhyay, D. (2007). Bacterial diversity of East Calcutta Wet land area: possible identification of potential bacterial population for different biotechnological uses. Microb Ecol. Oct;54(3):452-9.\n\nMaiti, P and Banerjee, S. (1999). Heavy metal in wastewater ponds in and around Calcutta and their effect on Mammalian System contaminated through fish raised in wastewater improvement, Annual report (1998–1999), Department of Zoology, University of Calcutta.\n\nBarbier, E.B., Acreman, M., and Knowler, D. (1997). Economic valuation of wetlands - A\nguide for policy makers and planners, Ramsar Copnvention Bureau, Gland, Switzerland.\n\nBiswas, K. P. (1927). 'Flora of the Salt Lakes, Calcutta', Journal of Department of Science, University of Calcutta, vol. 8.\n\nBose, B.C. (1944). 'Calcutta sewage-fisheries culture', Proc. Natl. Inst. Sci. India, 10.\n\nBrown, L. R. (2001). Eco-Economy-Building an Economy for the Earth, Earthscan, London.\n\nChakraborty, S. (1970). 'Some consideration on the evolution of physiography of Bengal', in A. B. Chatterjee, A. Gupta, and P. K. Mukhopadhyay (eds.), West Bengal, Geographical Institute, Presidency College, Firma K. L. Mukhopadhyay, Calcutta, India.\n\nClarke, W. (1865). 'Report of the project of The Salt Lake Reclamation & Irrigation Company Limited', in Selections from the Records of the Bengal Government (containing papers from 1865–1904), Calcutta, India.\n\nCMG (1945). 'Some facts about Calcutta drainage', in A. Home (ed.), The Calcutta Municipal Gazette: Official organ of the Corporation, Central Municipal Office, Calcutta, India, 42(7).\n\n______ (1964). 'Reclamation of Salt Lakes - Dr. B. C. Roy's dream' in A. Home (ed.), The Calcutta Municipal Gazette: Official organ of the Corporation of Calcutta, Central Municipal Office, Calcutta, India, 81(6&7). \nCMW&SA (1996). Sustaining Calcutta, Present Status Report of the Urban People's Environment, Calcutta Metropolitan Water and Sanitation Authority, Kolkata.\n\n____ (1997). Base line document for management action plan, East Calcutta Wetlands and Waste Recycling Region, Calcutta Metropolitan Water and Sanitation Authority, Kolkata.\n\nCook, C.D.K. (1996). Aquatic and Wetlands Plants of India. Oxford University Press.\n\nCostanza, R., d'Agre, R.,m Groot, R. de, Farber, S., Grasso, M., Hannon, B., Limbujrg, K., Naeem, S.O., Neill, R. V., Paruelo, J., Raskin, R. G., Sutton P., and Belt, M. van den (1997).'The Value of the World's Ecosystem Services and Natural Capital', Nature, vol. 387.\n\nDasgupta, R. (1973). 'Contribution of botany of a portion of Salt Lakes, West Bengal', Ind.Mus. Bull., vol. 1.\n\nDavid, A. (1959). 'Effect of Calcutta sewage upon the fisheries of the Kulti estuary and connected cultivable fisheries', Journal of Asiatic Society (Bengal), vol. 1, No. 4.\n\nDe, M., Bhunia, S., and Sengupta, T. (1989). 'A preliminary account of major wetland fauna of Calcutta and surroundings', Ecology, 3(9).\n\nDeb, S. C., and Santra, S. C. (1996). 'Bio-accumulation of metals in sewage fed aquatic system - a case study from Calcutta (India)', International Journal of Environmental Studies.\n\nDeb, S. C., Das, K. K., and Santra, S. C. (1996). 'Studies on the productivity of sewage-fed ecosystem', Journal of Environmental Protection, 12.\n\nDEC a (1945). 'History of the Gangetic Delta, Appendix 1 a', Report of the committee to inquire into the drainage conditions of Calcutta and adjoining area, Drainage Enquiry Committee, Government of Bengalk, Calcutta, India.\n\n____b (1945). 'Draionage (rural) of the area falling with the outer zone, which is to be investigated by the Calcutta Drainage Committee, Appendix IX', Reports of the committee to inquire into the drainage condition of Calcutta and adjoining area, Drainage Enquiry Committee, Government of Bengal, Calcutta, India.\n\nDOE (1999). Development and Management of the Calcutta canal systems and wetlands, Report of the committee constituted by the Department of Environment, Government of West Bengal.\n\n____ (2001). Report of the committee to look into all aspects of the existing and permissible land uses in the East Kolkata Wetland Area, Department of Environment, Government of West Bengal. \n_____ (2004)Report of the committee for formulation of the guidelines for preparation of management plan of East Kolkata Wetland, Department of Environment, Government of West Bengal.\nDepartment of Fisheries (1983). Report on study of heavy metal in sewage-fed fisheries,\nDepartment of Fisheries, Government of West Bengal.\n\nDouglas, J. S. (1972). Beginner's guide to applied ecology, Pelham Books, London.\n\nEkins, P. (1972). Beginner's guide to applied ecology, Pelham Books, London.\n\nEkins, P. (1992). A New World Order, Grassroot Movement for global change, Routledge,\nLondon.\n\nFarber, S., and Costanza, R. (1987). 'The economic value of wetland systems', Journal of Environmental Management, 24.\n\nGhosh,A.K(.1990 ).Biological Resources of East cAlcutta Wetlands.Indian J Landscape System and Ecological Studies,13 (1):10 - 23\n\nGhosh,A.K and Chakrabarty, Satyesh(1997). Management of East CAlcutta Wetlands and Canal Systems. A Report, CEMSAP,Dept.Environment Govt.West Bengal, \n1-188.\n\nGhosh,A.K. and Shreela Chakrabarti (1999).Human Interventions and Changing Status. Sci. Cult., 65:36 -38.\nGhosh,A.K.(2004). Avian Diversity of East Calcutta Wetlands. Environ,9 (1):8 - 13.\nGhosh, S. K., and Ghosh, D. (2003). Rehabilitating Biodiversity: A community-based initiativew in the East Calcutta Wetlands, A Communiqué published through WWF-India (W.B.S.O.), in collaboration with British Council Division, Kolkata.\n\nGhosh, D., and Furedy, C. (1984). 'Resource Conserving Traditions and Waste Disposal: The Garbage Farms and Sewage-fed Fisheries of Calcutta', Conservation and Recycling, Vol. 7, No. 2-4.\n\nGhosh, D., and Sen, S. (1987). 'Ecological History of Calcutta's Wetland Conservation', Environmental Conservation, vol. 14(3).\n\n______(1992). 'Developing Waterlogged Areas for Urban Fishery and Waterfront Recreation Project', AMBIO, Journal of the Royal Swedish Academy of Sciences, vol. 21, No. 2.\n\nGhosh, D. (1978). Ecological Study of Some selected Urban and Semi-urban Centers of West Bengal and suggesting certain controls of the Ecosystem, Ph. D. thesis, University of Calcutta.\n\n____ (1983). Sewage treatment fisheries in East Calcutta Wetlands, mimeographed, (not available for checking), Reports to the Department of Fisheries, Government of West Bengal, Calcutta, India.\n\n____ (1992). 'The ecologically handicapped', The Statesman, 12 March.\n\n____ (1996). Turning around: for a community based technology. Calcutta Environment improvement, CMW&SA.\n\n(1999). 'Rebellion of Nature and Need for a Global Convention on Consumption Imbalance', Journal of Indian Anthropological Society, 34.\n\n____(2001). 'Empowering the Ecologically Handicapped; in V. G. Martin and M. A. Parthasarathy (eds.), Wilderness and Humanity: the Global Issues, Flcrum Publishing, Golden, Colorado.\n\nGhosh, S. K. (2002). Reclamation and enhancement of biodiversity of the East Calcutta Wetlands, Project report prepared for British Council, Calcutta, implemented through WWF- India, West Bengal State Office.\n\n____ (2002) Wetland Ecosystem, West Bengal State Biodiversity Strategy and Action Plan, National Biodiversity Strategy and Action Plan, Department of Environment, Government of West Bengal and Ramkrishna Mission Narendrapur, West Bengal, India, executed by Ministry of Environment and Forest, Government of India, technical implementation by Kalpavrissh and administrative co-ordination by Biotech Consortium, India Ltd., funded by Global Environmental Facility through UNDP.\n\nGhosh, S. K., and Ghosh D. (2003). Community based rehabilitation of wetlands in West Bengal, India, S. B. Ray et al. (ed.), Contemporary Studies in Natural Resource Management in India, Forest Studies Series, Inter-India Publication, New Delhi.\n\nGhosh, S./ K., and Mitra, A. (1997). Flora and Fauna of East Calcutta Wetlands, Project report of Creative Research Group, East Calcutta Wetlands and Waste Recycling (Primary data), Environmental Improvement Programme, Calcutta Metropolitan Water and Sanitation Authority.\n\nGhosh, S. K., and Santra, S.C. (1996). 'Domestic and Municipal Wastewater Treatment by Some Common Tropical Aquatic Macrophytes', Indian Biologist. vol 28 (1).\n\nGhosh, A., Maity, B., Chakrabarti, K., Chattopadhyay, D. (2007). Bacterial diversity of East Calcutta Wet land area: possible identification of potential bacterial population for different biotechnological uses. Microb Ecol. Oct;54(3):452-9.\n\n____ (1997). 'Economic benefits of wetland vegetation for rural population in West Bengal India', in W. Giesen (ed), Wetland Biodiversity and Development, proceedings of workshop of the International Conference on Wetland and Development, held in Kuala Lumpur, Malaysia, 9–13 October 1995. Wetlands International Kuala Lumpur.\n\nGood R.E., Whigham, D. F., and Simpson, R.L.(1978). (eds.) Freshwater Wetlands, Ecological Processes and Management Potential, Academic Press, New York.\n\nHolling, C. S., Schindler, D.W., Walker, B.W., and Roughgarden, J.(1955). 'Biodiversity in the functioning of the ecosystem, an ecological synthesis', in Parings \"et al.\" (eds.), Biodiversity Loss, Economic and Ecological issues, Cambridge University Press.\n\nInstit;ute for Wetland Management and Ecological Design (1997). A study on the status of sewage of Calcutta as carrier of pollutants, nutrients and sediments, Report submitted to the West Bengal Pollution Control Board, Calcutta.\n\nISI (2001). Report on Environmental Conservation and Valuation of East Calcutta Wetlands 999-2000, World Bank aided 'India Environmental Capacity Building' Technical Assistance Project.\n\nIrrigation and Waterways Directorate (1959). Final Report of the West Bengal Flood Enquiry ommittee, Government of West Bengal, Irrigation and Waterways Department, Calcutta.\n\nJana, B. B., Banerjee, R. D., Guterstam, B., and Heeb, J. (2000). (eds.) Waste recycling and resource management in the developing world, University of Kalyani.\n\nKolstad, C. D., and Guzman, R. (1999). 'Information and the divergence between willingness- to-pay’,Environ.Econ.Mgmt.\n\nKormondy, E.J.(1974). Concepts of Ecology, Prentice Hall of India, New Delhi.\n\nLarson, J.S. (1976). (ed.). Models for Assessment of Freshwater Wetlands, Water Resources Research Centre, University of Massachusetts, Amherst, USA. publication no. 32, completion report FY 76-5.\n\nMaltby, E.(1986). Waterlogged Wealth, International Institute of Environment and Development, Earthscan, London.\n\nMisra, A.(1993). Aj Bhi Khare Hai Talab, Paryavayaran Kaksh, Gandhi Santi Pratisthan,New Delhi.\n\nMitchell, B.(1979). Geography and Resource Analysis, Longman, London.\n\nMitsch. W.J., and Gosselink, J. G. (1986). Wetlands, Van Nostrand Reinhold Company, Newyork\n\nMonkhouse, F.J., and Wilkinson, H.R.(1976). Maps and Diagrams: Their Compilation and \nConstruction, Methuen & Co. Ltd., London\n\nMukherjee, D.P., Kumar, B., and Saha, R.(2005) Performance of Sewage - Ponds in Treating Wastewater (unpublished report), Central Pollution Control Board, Eastern Regional Office, Kolkata.\n\nNBSAP(2002). National Biodiversity Strategy and Action Plan, West Bengal State Biodiversity Strategy and Action Plan, Department of Environment, Government of West Bengal and Ramkrishna Mission Narendrapur, West Bengal, India, executed by Ministry of Environment and Forest, Government of India, technical implementation by Kalpavriksh and administrative co-ordination by Biotech Consortium, India Ltd., funded by Global Environmental Facility through UNDP.\n\nPal, D., and Dasgupta, C. K. (1988). 'Interaction with fish and human pathogens', proceedings of National Symposium on 'Fish and Their Environment, Trivandrum.\n\nPearce D. W., and Turner R.K.(1990). Economics of natural resources and the environment, Johns Hopkins University Press, Baltimore.\n\nSachs, W. (2001). Planet Dialectics: Explorations in Environment and Development, Zed\nBooks, London.\n\nSarkar, R. (2002). Valuing the ecosystem benefits of treatment of manmade wetlands using conventional economic indicators - a case study of the East Calcutta Wetlands, Occasional Papers no. 01/2002, Department of Business Management, University of Calcutta.\n\nSchuyt, K., and Brander, L. (2004). 'The Economic Values of World's Wetlands', Living\nWaters, Conserving the source of life, WWF, Gland/Amsterdam.\n\nScott, D. A. (1989) (ed.). A Directory of Asian Wetlands. IUCN, Gland, Switzerland, and Cambridge, U.K.\n\nSewell, R. B.(1934). A study of the fauna of the Salt Lake, Calcutta. Record of the Indian Museum. 36.\nStewart. D. (1836). 'Report on the project of The Salt Lake Reclamation & Irrigation Company Limited', in Selection from the records of the Bengal Government, (containing papers from 1985 to 1964), Government of West Bengal, Calcutta, India.\n\nThomas, R. W., and Huggett, R. J. (1980), Modeling in Geography: A Mathematical Approach, Harper & Row, London.\n\nTrisal, C. L., and Zutshi, D. P. (1985). 'Ecology and Management of Wetland Ecosystems in India', Paper presented at the Regional Meeting of the National MAB Committee of Central and South Asian Countries, New Delhi.\n\nTurner, R. K., and Bateman, I. J. (1995). 'Wetland Valuation: three case studies', in Perring \"et al.\" (eds.), Biodiversity loss, economic and ecological issues, Cambridge University Press.\n\nUNESCO (2000). Science for the twenty-first century, a new commitment, World Conference on Science.\n\nUnited Nations Development Programme (1998), Human Development Report 1998, Oxford University Press, New York.\n\nWCED (1987). Our Common Future, World Comkmission on Environment and Development Oxford University Press, Oxford.\n\nWorld Wide Fund for Nature (1993). Directory of Indian Wetlands.\n\n"}
{"id": "1789762", "url": "https://en.wikipedia.org/wiki?curid=1789762", "title": "Ecosynthesis", "text": "Ecosynthesis\n\nEcosynthesis is the use of introduced species to fill niches in a disrupted environment, with the aim of increasing the speed of ecological restoration. This decreases the amount of physical damage done in a disrupted landscape. An example is using willow in a stream corridor for sediment and phosphorus capture. It aims to aid ecological restoration which, is the practice of renewing and restoring degraded, damaged, or destroyed ecosystems and habitats in the environment by active human intervention and action. Humans ecosynthesis to make environments more suitable for life, through restoration ecology (introduced species, vegetation mapping, habitat enhancement, remediation and mitigation.) \n\n A controversial science of applying ecosynthesis to other planets to make them habitable like Earth. This is a futuristic science that has almost no hold on society today due to its large manufacturing costs and level of technology available.\n\nEcological restoration aims to recreate, initiate, or accelerate the recovery of an ecosystem that has been disturbed.\n\nRevegetation: the establishment of vegetation on sites where it has been previously lost, often with erosion control as the primary goal.\n\nHabitat enhancement: the process of increasing the suitability of a site as habitat for some desired species.\n\nRemediation: improving an existing ecosystem or creating a new one with the aim of replacing another that has deteriorated or been destroyed.\n\nMitigation: legally mandated remediation for loss of protected species or an ecosystem.\n\nThrough restoration ecology humans can help ecosystems that we have either caused harm to or disturbed be brought back to functional state.\n\n A clear example of humans ecosynthesiszing would be through the introduction of a species to cause a trophic cascade, which is the result of indirect effects between nonadjacent trophic levels in a food chain or food web, such as the top predator in a food chain and a plant. The most famous example of a trophic cascade is that of the introduction of wolves to YellowStone National Park, which had extradionary effects to the ecosystem. Yellowstone National Park had a massive population of elk because they had no predators, which caused the local aspen population and other vegetation to significantly decrease in population size. However, the introduction of wolves controlled the elk population and indirectly affected the aspen and other vegetation. This brought the ecosystem back to a sustainable life.\n\n"}
{"id": "355438", "url": "https://en.wikipedia.org/wiki?curid=355438", "title": "Eight-thousander", "text": "Eight-thousander\n\nThe International Mountaineering and Climbing Federation or UIAA recognise eight-thousanders as the 14 mountains that are more than in height above sea level, and are considered to be sufficiently independent from neighbouring peaks. However, there is no precise definition of the criteria used to assess independence and since 2012, the UIAA has been involved in a process to consider whether the list should be expanded to 20 mountains. All eight-thousanders are located in the Himalayan and Karakoram mountain ranges in Asia, and their summits are in the death zone.\n\nThe first recorded attempt on an eight-thousander was when Albert F. Mummery and J. Norman Collie tried to climb Pakistan's Nanga Parbat in 1895. The attempt was unsuccessful when Mummery and two Gurkhas, Ragobir, and Goman Singh, were killed by an avalanche.\n\nThe first recorded successful ascent of an eight-thousander was by the French Maurice Herzog and Louis Lachenal, who on the 1950 French Annapurna expedition reached the summit of Annapurna on 3 June 1950. The first winter ascent of an eight-thousander was done by a Polish team led by Andrzej Zawada on Mount Everest. Two\nclimbers Leszek Cichy and Krzysztof Wielicki reached the summit on 17 February 1980.\n\nThe first person to climb all 14 eight-thousanders was the Italian Reinhold Messner, on 16 October 1986. In 1987, Polish climber Jerzy Kukuczka became the second person to accomplish this feat. Kukuczka is also the man who established the most new routes (9) on the main eight-thousanders. Messner summited each of the 14 peaks without the aid of supplemental oxygen. This feat was not repeated until nine years later by the Swiss Erhard Loretan in 1995. Phurba Tashi of Nepal has completed the most climbs of the eight-thousanders, with 30 ascents between 1998 and 2011. Juanito Oiarzabal has completed the second most, with a total of 25 ascents between 1985 and 2011. \n\nThe Italian Simone Moro made the most first winter ascents of eight-thousanders (4); Jerzy Kukuczka made four winter ascents as well, but one was a repetition. , K2 remains the only eight-thousander that has never been summited in the winter.\n\nIn 2010, Spanish climber Edurne Pasaban, became the first woman to summit all 14 eight-thousanders with no disputed climbing. In August 2011, Austrian climber Gerlinde Kaltenbrunner became the first woman to climb the 14 eight-thousanders without the use of supplementary oxygen.\n\nThe first couple and team who summited all 14 eight-thousanders together were the Italians Nives Meroi (second woman without supplementary oxygen), and her husband in 2017. They climbed in alpine style, without the use of supplementary oxygen.\n\n, the country with the most climbers to have climbed all 14 eight-thousanders is Italy with seven climbers, followed by Spain with six climbers, and South Korea with five climbers. Kazakhstan and Poland have three climbers each that completed the \"Crown of the Himalaya\".\n\nIn 2012, to relieve capacity pressure, and develop climbing tourism, Nepal lobbied the International Climbing and Mountaineering Federation (or UIAA) to reclassify 5 summits (2 on Lhotse and 3 on Kanchenjunga), as standalone eight-thousanders, while Pakistan lobbied for a sixth summit (on Broad Peak) In 2012, the UIAA set up a project group to consider the proposals called the \"AGURA Project\". The 6 proposed summits for reclassification are subsidiary-summits of existing eight-thousanders, but which are also themselves above 8,000 metres and have a prominence above 60 metres.\n\nThe proposed 6 new eight-thousander peaks would not meet the wider UIAA criteria of 600 meters of elevation change between standalone peaks, called topographic prominence, as used by the UIAA elsewhere for major mountains (the lowest prominence of the existing list of 14 eight-thousanders is Lothse, at 610 metres). For example, only Broad Peak Central, with a topographic prominence of 181 meters, would even meet the 150–metre prominence threshold to be a British Isles Marilyn. However, the appeal noted the UIAA's 1994 reclassification of Alpine 4,000 metre peaks where a prominence threshold of 30–metres was used, amongst other criteria; the logic being that if 30–metres worked for 4,000 metres summits, then 60–metres should work for 8,000m summits. \n\n, there has been no conclusion by the UIAA and the proposals appear to have been set aside.\n\nThere is no single undisputed source for verified Himalayan ascents, however, Elizabeth Hawley's \"The Himalayan Database\", is considered as an important source for the \"Nepalese Himalayas\". Online ascent databases pay close regard to \"The Himalayan Database\", including the website \"AdventureStats.com\", and the \" List\". Various mountaineering journals (including the Alpine Journal and the American Alpine Journal), maintain extensive records and archives but do not always opine on ascents. \n\nThe \"No O\" column lists people who have climbed all 14 eight-thousanders without any bottled, or supplementary, oxygen.\nClaims in which not enough evidence was provided to verify the ascents of all 14 peaks. The disputed ascent in each claim is shown in parentheses. In most cases, the influential Himalayan chronicler Elizabeth Hawley, is an important source regarding the fact-base of the dispute. Her well-regarded \"The Himalayan Database\" is the source for other online Himalayan ascent databases (e.g. AdventureStats.com).\n\nCho Oyu is a recurrent problem peak as it is a small hump circa 30 mins into the summit plateau, and the main proxy - \"Did you see Everest\" - requires clear weather. Shishapangma is another problem peak because of its dual summits, which despite being close in height, are up to two hours climbing time apart. Elizabeth Hawley famously got Ed Viesturs to re-climb the main summit of Shishapangma.\n\n\n"}
{"id": "2170017", "url": "https://en.wikipedia.org/wiki?curid=2170017", "title": "Energy Policy Act of 2005", "text": "Energy Policy Act of 2005\n\nThe Energy Policy Act of 2005 () is a bill passed by the United States Congress on July 29, 2005, and signed into law by President George W. Bush on August 8, 2005, at Sandia National Laboratories in Albuquerque, New Mexico. The act, described by proponents as an attempt to combat growing energy problems, changed US energy policy by providing tax incentives and loan guarantees for energy production of various types.\n\nThe Public Utility Holding Company Act of 1935 was repealed, effective February 2006, by the passing of this act.\n\n\nIn Congressional bills, an \"authorization\" of a discretionary program is a permission to spend money IF money has been appropriated; while an \"appropriation\" is the provision of funds so it can be spent. The authorizations above will not get carried out if money is never appropriated for them.\n\n\nThe bill amends the Uniform Time Act of 1966 by changing the start and end dates of daylight saving time, beginning in 2007. Clocks were set ahead one hour on the second Sunday of March (March 11, 2007) instead of on the first Sunday of April (April 1, 2007). Clocks were set back one hour on the first Sunday in November (November 4, 2007), rather than on the last Sunday of October (October 28, 2007). This had the net effect of slightly lengthening the duration of daylight saving time.\n\nLobbyists for this provision included the Sporting Goods Manufacturers Association, the National Association of Convenience Stores, and the National Retinitis Pigmentosa Foundation Fighting Blindness.\n\nLobbyists against this provision included the U.S. Conference of Catholic Bishops, the United Synagogue of Conservative Judaism, the National Parent-Teacher Association, the Calendaring and Scheduling Consortium, the Edison Electric Institute, and the Air Transport Association. This section of the act is controversial; some have questioned whether daylight saving results in net energy savings.\n\nThe Act created the Energy Efficient Commercial Buildings Tax Deduction, a special financial incentive designed to reduce the initial cost of investing in energy-efficient building systems via an accelerated tax deduction under section §179D of the Internal Revenue Code (IRC) Many building owners are unaware that the [Policy Act of 2005] includes a tax deduction (§179D) for investments in \"energy efficient commercial building property\" designed to significantly reduce the heating, cooling, water heating and interior lighting cost of new or existing commercial buildings placed into service between January 1, 2006 and December 31, 2013.\n§179D includes full and partial tax deductions for investments in energy efficient commercial building that are designed to increase the efficiency of energy-consuming functions. Up to $.60 for lighting, $.60 for HVAC and $.60 for building envelope, creating a potential deduction of $1.80 per sq/ft. Interior lighting may also be improved using the Interim Lighting Rule, which provides a simplified process to earn the Deduction, capped at $0.30-$0.60/square foot. Improvements are compared to a baseline of ASHRAE 2001 standards.\n\nTo obtain these benefits the facilities/energy division of a business, its tax department, and a firm specializing in EPAct 179D deductions needed to cooperate. IRS mandated software had to be used and an independent 3rd party had to certify the qualification. For municipal buildings, benefits were passed through to the primary designers/architects in an attempt to encourage innovative municipal design.\n\nThe Commercial Buildings Tax Deduction expiration date had been extended twice, last by the Energy Improvement and Extension Act of 2008. With this extension, the CBTD could be claimed for qualifying projects completed before January 1, 2014.\n\nThe commercial building tax deductions could be used to improve the payback period of a prospective energy improvement investment. The deductions could be combined by participating in demand response programs where building owners agree to curtail usage at peak times for a premium. The most common qualifying projects were in the area of lighting.\n\nSummary of Energy Savings Percentages Provided by IRS Guidance\n\nPercentages permitted under Notice 2006-52\n\nPercentages permitted under Notice 2008-40\n\nPercentages permitted under Notice 2012-22\nEffective date of Notice 2012-22 – December 31, 2013; if §179D is extended beyond December 31, 2013, is also effective (except as otherwise provided in an amendment of §179D or the guidance thereunder) during the period of the extension.\n\nThe Congressional Budget Office review of the conference version of the bill estimated the Act will increase direct spending by $2.2 billion over the 2006-2010 period, and by $1.6 billion over the 2006-2015 period. The CBO did not attempt to estimate additional effects on discretionary spending.\nThe CBO and the Joint Committee on Taxation estimated that the legislation would reduce revenues by $7.9 billion over the 2005-2010 period and by $12.3 billion over the 2005-2015 period.\n\nThe collective reduction in national consumption of energy (gas and electricity) is significant for home heating. The Act provided gible financial incentives (tax credits) for average homeowners to make environmentally positive changes to their homes. It made improvements to home energy use more affordable for walls, doors, windows, roofs, water heaters, etc. Consumer spending, and hence the national economy, was abetted. Industry grew for manufacture of these environmentally positive improvements. These positive improvements have been near and long-term in effect.\n\nThe collective reduction in national consumption of oil is significant for automotive vehicles. The Act provided tangible financial incentives (tax credits) for operators of hybrid vehicles. It helped fuel competition among auto makers to meet rising demands for fuel-efficient vehicles. Consumer spending, and hence the national economy, was abetted. Dependence on imported oil was reduced. The national trade deficit was improved. Industry grew for manufacture of these environmentally positive improvements. These positive improvements have been near and long-term in effect.\n\n\nThe Act was voted on and passed twice by the United States Senate, once prior to conference committee, and once after. In both cases, there were numerous senators who voted against the bill. John McCain, the Republican Party nominee for President of the United States in the 2008 election voted against the bill. Democrat Barack Obama, President of the United States from January 2009 to January 2017, voted in favor of the bill.\n\n\nTo remove from 18 CFR Part 366.1 the definitions of “electric utility company” and exempt wholesale generator (EWG),that an EWG is not an electric utility company.\n\nJune 28, 2005, 10:00 a.m. Yeas - 85, Nays - 12\n\nThe bill's conference committee included 14 Senators and 51 House members. The senators on the committee were: Republicans Domenici, Craig, Thomas, Alexander, Murkowski, Burr, Grassley and Democrats Bingaman, Akaka, Dorgan, Wyden, Johnson, and Baucus.\n\nJuly 29, 2005, 12:50 p.m. Yeas - 74, Nays - 26\n\n\n\n\n"}
{"id": "19325288", "url": "https://en.wikipedia.org/wiki?curid=19325288", "title": "Expedition Global Eagle", "text": "Expedition Global Eagle\n\nExpedition Global Eagle was the first attempt in history to circumnavigate the globe using an autogyro. The flight was attempted in 2004 by Warrant Officer Barry Jones using an open-cockpit autogyro which he named \"Global Eagle\". The purpose of the mission was twofold; to set the world record and to raise funds for three charities; the Dyslexia Institute, the NSPCC and the British Red Cross. Jones embarked on his mission on 26 April 2004 from the Museum of Army Flying, Middle Wallop, Hampshire with an honorary military helicopter armada accompanying him during the send off. The expedition was supported by a team of soldiers based at Dishforth, North Yorkshire, United Kingdom. \n\nThe expedition encountered difficulties while flying through Europe, including the Alps and the Middle East and it was downgraded due to flight delays before finally being abandoned after it landed in Guwahati, India around the onset of monsoon season. Officer Jones had set up a website where he described the details of each stage of his trip. The images of the flight as well as details of each stage of the flight were regularly uploaded on the website.\n\nThe autogyro is the last remaining type of aircraft which has not a yet been used to circumnavigate the globe. The expedition was the first attempt ever to fly an autogyro around the Earth, a trip of about 25,000 miles (40,000 km). In February 2003, a year before the circumnavigation attempt, the \"Global Eagle\" broke the world range record by flying non-stop from Culdrose in Cornwall to Wick in Scotland, a total of 580 miles (928 km), after a flight lasting 7 hours and 23 minutes, breaking the old record of either 543.27 statute miles (874.32 km) or 869.23 km (540.11 mi) held by Wing Commander Ken Wallis. During the record-breaking trip Jones drifted toward North America due to a broken radio, before finally correcting the course. While flying over Wales he had to fly over the clouds in the open cockpit of the autogyro. He also flew for approximately 50 miles (80 km) over the North Sea which, as he acknowledged during an interview, was a dangerous course due to the fact the autogyro only had one engine and therefore in case it cut-off there would be no alternative solution but to crash land in the water. The average flight altitude was 4,000 ft and the average speed was 70 mph. In 2004, Jones, a Lynx helicopter pilot, was one of 75 licensed autogyro pilots in the UK. Later in 2003 the original Eagle suffered an accident during landing.\n\nThe circumnavigation attempt trip commenced on 26 April 2004 under the patronage of General Sir Michael Walker, Chief of Defence Staff of the British Armed Forces and it was supposed to last about three and a half months. The autogyro flight was to have touched down in twenty five countries with frequent landings due to the limited range of the autogyro. The \"Global Eagle\"'s route was planned to approximately follow the flightpath taken by Brian Milton in 1998 when he became the first person to circumnavigate the globe using a microlight. Due to technical difficulties with the original gyrocopter the gyro at launch was supplied by the Italian manufacturer \"Magni Gyro\" which also supplied spare parts and technical assistance to the expedition. The new gyro featured a new colour scheme and the eagle logo was not used. \nThe expedition successfully completed the European leg of the journey which included Oostende, Belgium, Friedrichshafen, Germany, the Alps, Bolzano, Trento, Casaleggio Novara, the location of the Magni airfield in Italy, Forlì, Pescara, Bari, Italy, Corfu, Athens, Mykonos, Kos and Rhodes, in Greece and the military base of Akrotiri in Cyprus, albeit with delays. The journey through the Alps was very difficult because Jones had to climb to 10,000 feet over the Alps where it was extremely windy. Jones also had to make a forced landing in the Italian Alps. Also the trip from Athens to Akrotiri over the Mediterranean proved very frightening because flying over the water, with only a few ships below, meant that it would be very difficult to get any help in case of a mishap. From Akrotiri, Global Eagle went to Amman, Jordan, Turaif, Saudi Arabia, Arar, Hafr Al-Batin, Qaisumah, Jubail, Bahrain, Abu Dhabi in the Arab Emirates and Muscat, Oman. \nOver Jordan and Saudi Arabia, Jones had to fly over the desert for hours and to battle sandstorms and hot weather at low altitudes while at higher altitudes it would become very cold. Further, communication was impossible while flying over the desert because of a lack of ground stations and the atmospheric conditions prevailing in the area. While there, Jones had to communicate with commercial aircraft flying over the desert which would then relay his messages to the ground stations at the local airports. A Saudia flight helped him approach and land at Turaif. \n\nSubsequently Jones arrived at Abu Dhabi and from there he flew to Muscat, Oman and then Gawadar and Ormara in Pakistan and, finally, Karachi. The flight from Muscat to Karachi broke the record for the longest flight over water by an autogyro and lasted six hours. \n\nFrom Karachi, Jones flew the autogyro to India where it navigated to Ahmedabad, Udaipur, Jaipur, New Delhi, Bareilly, Gorakhpur, Patna and Baghdogra, sometimes through sandstorms, and finally landed during monsoon rains in the army base of Guwahati. The delays incurred in the European leg of the journey proved damaging to the effort because Jones was forced to arrive in India during monsoon season. The monsoons he encountered were the worst in 20 years with 100 people killed in the Guwahati area alone. The technical difficulties encountered in India, due to the weather, led to the downgrading of the purpose of the mission from circumnavigating the globe to flying to Australia.\n\nJones and his support team then returned to the UK in order to revise the plans for the truncated trip to Australia. Upon returning to India they discovered that the gyro while at the army base of Guwahati had spent time submerged in water. The transponder, the radio and the rest of the instruments were damaged. As well, the flying controls and the cables needed to be replaced. The damage totalled £10,000 and could not be raised on time. In addition the team members, being soldiers, had to return to active duty by early 2005. This led to the attempt being abandoned. The trip when cancelled had covered between 6,550–7,500 miles (10,480–12,100 km) and had lasted for four months due to the delays experienced, averaging approximately 350 miles per flight day. Even though the expedition did not succeed in its goal of circumnavigating the globe it demonstrated the wide range of conditions under which a light, open-cockpit autogyro could operate.\n\nThe detailed route, times and other details of the completed trip were as follows:\n\n\nThis is the list of the unfinished part of the expedition:\n\n\nThe Magni gyrocopter model: Magni VPM M16 (Similar to the Magni M16 - 2000) used in the expedition had the following specifications:\n\n\n"}
{"id": "8158750", "url": "https://en.wikipedia.org/wiki?curid=8158750", "title": "Flora of the Chatham Islands", "text": "Flora of the Chatham Islands\n\nThe flora of the Chatham Islands consists of around 388 terrestrial plant species, of which 47 are endemic. The Chatham Islands make up the Chatham floristic province of the Neozeylandic Region of the Antarctic Kingdom.\n\nThe flora of the Chatham Islands include:\n\n"}
{"id": "426729", "url": "https://en.wikipedia.org/wiki?curid=426729", "title": "Hypercane", "text": "Hypercane\n\nA hypercane is a hypothetical class of extreme tropical cyclone that could form if ocean temperatures reached , which is warmer than the warmest ocean temperature ever recorded. Such an increase could be caused by a large asteroid or comet impact, a large supervolcanic eruption, or extensive global warming. There is some speculation that a series of hypercanes resulting from an impact by a large asteroid or comet contributed to the demise of the non-avian dinosaurs. The hypothesis was created by Kerry Emanuel of MIT, who also coined the term.\n\nIn order to form a hypercane, according to Emanuel's hypothetical model, the ocean temperature would have to be at least 49 °C (120 °F). A critical difference between a hypercane and present-day hurricanes, is that a hypercane would extend into the upper stratosphere, whereas present-day hurricanes extend into only the lower stratosphere.\n\nHypercanes would have wind speeds of over , and would also have a central pressure of less than (700 millibars), giving them an enormous lifespan. For comparison, the largest and most intense storm on record was 1979's Typhoon Tip, with a wind speed of and central pressure of (870 millibars). Such a storm would be eight times more powerful than the strongest storms yet recorded.\n\nThe extreme conditions needed to create a hypercane could conceivably produce a system up to the size of North America, creating storm surges of and an eye nearly across. The waters could remain hot enough for weeks, allowing more hypercanes to be formed. A hypercane's clouds would reach to into the stratosphere. Such an intense storm would also damage the Earth's ozone. Water molecules in the stratosphere would react with ozone to accelerate decay into O and reduce absorption of ultraviolet light.\n\nA hurricane functions as a Carnot heat engine powered by the temperature difference between the sea and the uppermost layer of the troposphere. As air is drawn in towards the eye it acquires latent heat from evaporating sea-water, which is then released as sensible heat during the rise inside the eyewall and radiated away at the top of the storm system. The energy input is balanced by energy dissipation in a turbulent boundary layer close to the surface, which leads to an energy balance equilibrium. \n\nHowever, in Emanuel's model, if the temperature difference between the sea and the top of the troposphere is too large, there is no solution to the equilibrium equation. As more air is drawn in, the released heat reduces the central pressure further, drawing in more heat in a runaway positive feedback. The actual limit to hypercane intensity depends on other energy dissipation factors that are uncertain: whether inflow ceases to be isothermal, whether shock waves would form in the outflow around the eye, or whether turbulent breakdown of the vortex happens.\n\n"}
{"id": "2349852", "url": "https://en.wikipedia.org/wiki?curid=2349852", "title": "Index of radiation articles", "text": "Index of radiation articles\n\n\n"}
{"id": "1527600", "url": "https://en.wikipedia.org/wiki?curid=1527600", "title": "Kick 'em Jenny", "text": "Kick 'em Jenny\n\nKick 'em Jenny (also: Kick-'em-Jenny or Mt. Kick-'Em-Jenny) is an active submarine volcano or seamount on the Caribbean Sea floor, located north of the island of Grenada and about west of Ronde Island in the Grenadines. Kick-'em-Jenny rises above the sea floor on the steep inner western slope of the Lesser Antilles ridge. The South American tectonic plate is subducting the Caribbean tectonic plate to the east of this ridge and under the Lesser Antilles island arc.\n\nThe volcano was unknown before 1939, although \"Kick 'em Jenny\" appeared on earlier maps as either the name of a small island now called Diamond Rock (or Île Diamante), or the name of the strait between Grenada and Ronde Island (or Île de Ronde). The name itself may be a reference to the waters sometimes being extremely rough.\n\nThe first record of the volcano was in 1939, although it must have erupted many times before that date. On 23–24 July 1939 an eruption broke the sea surface, sending a cloud of steam and debris into the air and generating a series of tsunamis around two metres high when they reached the coastlines of northern Grenada and the southern Grenadines. A small tsunami also reached the West coast of nearby Barbados, where 'a sea-wave' suddenly washed over a coastal road, most likely at Paynes Bay.\n\nThe volcano has erupted on at least twelve occasions between 1939 and 2001 (the last being on December 4, 2001), although none of the eruptions have been as large as the 1939 one, and most were only detected by seismographs. The larger eruptions have also been heard underwater or on land close to the volcano as a deep rumbling sound.\n\nA submersible survey in 2003 detected a crater with active fumaroles releasing cold and hot gas bubbles. Samples of fresh olivine basalt were collected. An arc shaped collapse structure appears on the west flank and was the apparent source of a submarine debris avalanche extending 15 km down the ridge slope to the west toward the Grenada Basin. The Global Volcanism Program reports the summit to be below the sea surface.\n\nSigns of elevated seismicity began July 11, 2015, and on July 23 a strong continuous signal was recorded by instruments observing Kick 'em Jenny, prompting authorities to raise the alert level to orange, which is the second-highest level. The following day, July 24, at 0200 an hour long explosion event was recorded, scientists from the University of the West Indies Seismic Research Centre observed nothing out of the ordinary at the surface above the volcano during an overflight on 25 July, and by 1800 no activity was recorded. On 26 July the Alert Level was lowered to Yellow. This has now been changed. On March 12, 2018 the warning has once again been raised to Orange.\n\nThe volcano is on the shipping route from St Vincent to Grenada. There is a maritime exclusion zone monitored by the Seismic Research Centre of the University of the West Indies, Trinidad and this is normally at 1.5 km from the centre of the volcano. Bubbles of volcanic gases can lower water density, creating a sinking hazard. This is marked on marine charts. During periods of high level seismic activity this is increased to 5 km.\n\n"}
{"id": "11965281", "url": "https://en.wikipedia.org/wiki?curid=11965281", "title": "Lau event", "text": "Lau event\n\nThe Lau event was the last of three relatively minor mass extinctions (the Ireviken, Mulde, and Lau events ) during the Silurian period. It had a major effect on the conodont fauna, but barely scathed the graptolites. It coincided with a global low point in sea level, is closely followed by an excursion in geochemical isotopes in the ensuing late Ludfordian faunal stage and a change in depositional regime.\n\nThe Lau event started at the beginning of the late Ludfordian, a subdivision of the Ludlow stage, about . Its strata are best exposed in Gotland, Sweden, taking its name from the parish of Lau. Its base is set at the first extinction datum, in the Eke beds, and despite a scarcity of data, it is apparent that most major groups suffered an increase in extinction rate during the event; major changes are observed worldwide at correlated rocks, with a \"crisis\" observed in populations of conodonts and graptolites. More precisely, conodonts suffered in the Lau event, and graptolites in the subsequent isotopic excursion. Local extinctions may have played a role in many places, especially the increasingly enclosed Welsh basin; the event's relatively high severity rating of 6.2 does not change the fact that many life-forms became re-established shortly after the event, presumably surviving in refuge or in environments that have not been preserved in the geological record. Although life persisted after the event, community structures were permanently altered and many lifeforms failed to regain the niches they had occupied before the event.\n\nA peak in , accompanied by fluctuations in other isotope concentrations, is often associated with mass extinctions. Some workers have attempted to explain this event in terms of climate or sea level change – perhaps arising due to a build-up of glaciers; however, such factors alone do not appear to be sufficient to explain the events. An alternative hypothesis is that changes in ocean mixing were responsible. An increase in density is required to make water downwell; the cause of this densification may have changed from hypersalinity (due to ice formation and evaporation) to temperature (due to water cooling).\n\nThe curve slightly lags conodont extinctions, hence the two events may not represent the same thing. Therefore, the term \"Lau event\" is used only for the extinction, not the following isotopic activity, which is named after the time period in which it occurred.\n\nLoydell suggests many causes of the isotopic excursion, including increased carbon burial, increased carbonate weathering, changes in atmospheric and oceanic interactions, changes in primary production, and changes in humidity or aridity. He uses a correlation between the events and glacially induced global sea level change to suggest that carbonate weathering is the major player, with other factors playing a less significant role.\n\nProfound sedimentary changes occurred at the beginning of the Lau event; these are probably associated with the onset of sea level rise, which continued through the event, reaching a high point at the time of deposition of the Burgsvik beds, after the event.\n\nThese changes appear to display anachronism, marked by an increase in erosional surfaces and the return of flat-pebbled conglomerates in the Eke beds. This is further evidence of a major blow to ecosystems of the time – such deposits can only form in conditions similar to those of the early Cambrian period, when life as we know it was only just becoming established. Indeed, stromatolites, which rarely form in the presence of abundant higher life forms, are observed during the Lau event and, occasionally, in the overlying Burgsvik beds; microbial colonies of \"Rothpletzella\" and \"Wetheredella\" become abundant. This suite of characteristics is common to the larger end-Ordovician and end-Permian extinctions.\n\n"}
{"id": "33558266", "url": "https://en.wikipedia.org/wiki?curid=33558266", "title": "Lepidoptera Indica", "text": "Lepidoptera Indica\n\nLepidoptera Indica was a 10 volume work on the butterflies of the Indian region that was begun in 1890 and completed in 1913. It was published by Lovell Reeve and Co. of London. It has been considered the \"magnum opus\" of its author, Frederic Moore, assistant curator at the museum of the East India Company. Frederic Moore described a number of new species through this publication. Moore was a splitter, often creating new genera for species and new species for what are now treated as subspecies.\n\nThe series was based on a large collection of butterflies that were under the care of the curator of the Asiatic Museum, Dr Thomas Horsfield. The museum was closed in 1879 and the collection was transferred to the British Museum. Moore in his preface defined the Indian region as being roughly bounded by the Himalayan mountains in the north, Suleiman and Hala mountains in the northwest, Ceylon to the South and Burma in the East and including the Andaman and Nicobar Islands. Moore however died before the work could be completed and the work was then taken over by Colonel Charles Swinhoe, brother of Robert Swinhoe and one of the founders of the Bombay Natural History Society.\n\nThe butterfly collections that were described in this work, many of which were previously undescribed had been made by additions from numerous correspondents and collectors in the Civil and Military services from across India. The first catalogue of these collections was prepared by Arthur Grote in 1857-1859. Grote also took notes on the life histories, making use of a Munshi Zynulabdin, a local artist. Moore also noted the contributions of Sir Walter Elliot from the Madras region, S. Nevill(e) Ward for notes from the Malabar coast, W. S. Atkinson, A E Russell, Colonel A M Lang (Oudh, Kashmir, Simla), Captain T. Hutton (Mussoorie), Captain H. L. de la Chaumette (Lucknow), C. Horne, Dr Francis Day, W. Forsyth Hunter, Major J. Le Mesurier, Major-Gen. G. Ramsay, Lt.-Col. H H G Godwin-Austin (sic), Captain R. Bayne Reed, W. B Farr, G H Wilkinson, Dr. A. Leith, Dr. J. Shortt, Capt. H B Hellard, W C Hewitson for material from the Atkinson collection, J. O Westwood for material collected by R Hunter at Saugor, J. Anderson, James Wood-Mason, Rev. J H Hocking, Mrs. F. A. de Roepstorff (for collections from the Andaman and Nicobar Islands made by her husband, superintendent of the Danish settlement at Camorta who was killed in 1883), A. Lindsay, G.A.J. Rothney, Colonel C. Swinhoe, Major J W Yerbury, Lionel de Niceville, H J Elwes, Hon. L. W. de Rothschild, G F Hampson (Nilgiris). Moore also thanks W F Kirby and A G Butler of the British Museum for nomenclature and bibliographic help. He also thanks private collectors including W L Distant, Godman and Salvin, H. Druce, Henley Grose-Smith and J J Weir, J H Leech and P. Crowley.\n\nThe delineation and lithography for the first few volumes was done mainly by Moore's son F. C. Moore but later made use of other artists including John Nugent Fitch and E. C. Knight.\n\nMoore died in 1907 and from volume 7 the work was taken over by Colonel Charles Swinhoe\nBy the 8th volume, there were photographs of the genitalia of some species.\n\nMoore was a splitter and did not follow the concept of subspecies for polytypic forms. Moore often created genera for single species. His approach was however dropped after Swinhoe took over.\n\n"}
{"id": "16642294", "url": "https://en.wikipedia.org/wiki?curid=16642294", "title": "List of Superfund sites in New Hampshire", "text": "List of Superfund sites in New Hampshire\n\nThis is a list of Superfund sites in New Hampshire designated under the Comprehensive Environmental Response, Compensation, and Liability Act (CERCLA) environmental law. The CERCLA federal law of 1980 authorized the United States Environmental Protection Agency (EPA) to create a list of polluted locations requiring a long-term response to clean up hazardous material contaminations. These locations are known as Superfund sites, and are placed on the National Priorities List (NPL). \n\nThe NPL guides the EPA in \"determining which sites warrant further investigation\" for environmental remediation. As of November 29, 2010, there were 20 Superfund sites on the National Priorities List in New Hampshire. One additional site is currently proposed for entry on the list. No sites have been cleaned up and removed from the list.\n\n\n"}
{"id": "24144971", "url": "https://en.wikipedia.org/wiki?curid=24144971", "title": "List of cancelled nuclear reactors in the United States", "text": "List of cancelled nuclear reactors in the United States\n\nThis is a list of cancelled nuclear reactors in the United States.\n\nThe late 1960s and early 1970s saw a rapid growth in the development of nuclear power in the United States. By 1976, however, many nuclear plant proposals were no longer viable due to a slower rate of growth in electricity demand, significant cost and time overruns, and more complex regulatory requirements. Also, there was considerable public opposition to nuclear power in the USA by this time, which contributed to delays in licensing planned nuclear power stations, and further increased costs.\n\nBy the end of the 1970s it became clear that nuclear power would not grow nearly as dramatically as once believed. This was particularly galvanized by the Three Mile Island accident in 1979. Eventually, more than 120 reactor orders were ultimately cancelled and the construction of new reactors ground to a halt. Al Gore has commented on the historical record and reliability of nuclear power in the United States:\n\nOf the 253 nuclear power reactors originally ordered in the United States from 1953 to 2008, 48 percent were cancelled, 11 percent were prematurely shut down, 14 percent experienced at least a one-year-or-more outage, and 27 percent are operating without having a year-plus outage. Thus, only about one fourth of those ordered, or about half of those completed, are still operating and have proved relatively reliable.\n\nA cover story in the February 11, 1985, issue of \"Forbes magazine\" commented on the overall management of the nuclear power program in the United States:\n\nThe failure of the U.S. nuclear power program ranks as the largest managerial disaster in business history, a disaster on a monumental scale ... only the blind, or the biased, can now think that the money has been well spent. It is a defeat for the U.S. consumer and for the competitiveness of U.S. industry, for the utilities that undertook the program and for the private enterprise system that made it possible.\n\nDuring the 2000s, aging infrastructure, growing power use and fears of global climate change all prompted what was then called the \"nuclear renaissance\". Engineering companies noted that the commissioning process was a major barrier to further construction, and changes to the system were carried out by the US Nuclear Regulatory Commission as part of the Energy Policy Act of 2005, along with new tax incentives and loan guarantees. As many as 30 new reactors were planned by 2009.\n\n, only two new reactors are still under construction, both at Vogtle. The project has announced significant delays and budget overruns. Most of the other new builds and the equally extensive list of upgrades to existing reactors have been shelved. The majority of this change in fortunes is due to the rapidly falling prices of natural gas.\n\n\n"}
{"id": "53700592", "url": "https://en.wikipedia.org/wiki?curid=53700592", "title": "List of ice cores", "text": "List of ice cores\n\nThis is a list of ice cores drilled for scientific purposes. Note that many of these locations are on moving ice sheets, and the latitude and longitude given is as of the date of drilling. \n\n\n"}
{"id": "29344894", "url": "https://en.wikipedia.org/wiki?curid=29344894", "title": "List of nature parks of Turkey", "text": "List of nature parks of Turkey\n\nThis is a list of nature parks of Turkey. As of October 2010, there are twelve nature reserves located at the coastal regions of the country. These protected areas are administered by the Directorate-General of Nature Protection and National Parks () of the Ministry of Environment and Forest.\n\n\n"}
{"id": "14355155", "url": "https://en.wikipedia.org/wiki?curid=14355155", "title": "List of rodents", "text": "List of rodents\n\nRodents are animals that gnaw with two continuously growing incisors. Forty percent of mammal species are rodents, and they inhabit every continent except Antarctica.\n\nThis list contains 2,276 species in 489 genera in the order Rodentia.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "19605", "url": "https://en.wikipedia.org/wiki?curid=19605", "title": "Main sequence", "text": "Main sequence\n\nIn astronomy, the main sequence is a continuous and distinctive band of stars that appears on plots of stellar color versus brightness. These color-magnitude plots are known as Hertzsprung–Russell diagrams after their co-developers, Ejnar Hertzsprung and Henry Norris Russell. Stars on this band are known as main-sequence stars or dwarf stars. These are the most numerous true stars in the universe, and include the Earth's Sun.\n\nAfter condensation and ignition of a star, it generates thermal energy in its dense core region through nuclear fusion of hydrogen into helium. During this stage of the star's lifetime, it is located on the main sequence at a position determined primarily by its mass, but also based upon its chemical composition and age. The cores of main-sequence stars are in hydrostatic equilibrium, where outward thermal pressure from the hot core is balanced by the inward pressure of gravitational collapse from the overlying layers. The strong dependence of the rate of energy generation on temperature and pressure helps to sustain this balance. Energy generated at the core makes its way to the surface and is radiated away at the photosphere. The energy is carried by either radiation or convection, with the latter occurring in regions with steeper temperature gradients, higher opacity or both.\n\nThe main sequence is sometimes divided into upper and lower parts, based on the dominant process that a star uses to generate energy. Stars below about 1.5 times the mass of the Sun () primarily fuse hydrogen atoms together in a series of stages to form helium, a sequence called the proton–proton chain. Above this mass, in the upper main sequence, the nuclear fusion process mainly uses atoms of carbon, nitrogen and oxygen as intermediaries in the CNO cycle that produces helium from hydrogen atoms. Main-sequence stars with more than two solar masses undergo convection in their core regions, which acts to stir up the newly created helium and maintain the proportion of fuel needed for fusion to occur. Below this mass, stars have cores that are entirely radiative with convective zones near the surface. With decreasing stellar mass, the proportion of the star forming a convective envelope steadily increases. Main-sequence stars below undergo convection throughout their mass. When core convection does not occur, a helium-rich core develops surrounded by an outer layer of hydrogen.\n\nIn general, the more massive a star is, the shorter its lifespan on the main sequence. After the hydrogen fuel at the core has been consumed, the star evolves away from the main sequence on the HR diagram, into a supergiant, red giant, or directly to a white dwarf.\n\nIn the early part of the 20th century, information about the types and distances of stars became more readily available. The spectra of stars were shown to have distinctive features, which allowed them to be categorized. Annie Jump Cannon and Edward C. Pickering at Harvard College Observatory developed a method of categorization that became known as the Harvard Classification Scheme, published in the \"Harvard Annals\" in 1901.\n\nIn Potsdam in 1906, the Danish astronomer Ejnar Hertzsprung noticed that the reddest stars—classified as K and M in the Harvard scheme—could be divided into two distinct groups. These stars are either much brighter than the Sun, or much fainter. To distinguish these groups, he called them \"giant\" and \"dwarf\" stars. The following year he began studying star clusters; large groupings of stars that are co-located at approximately the same distance. He published the first plots of color versus luminosity for these stars. These plots showed a prominent and continuous sequence of stars, which he named the Main Sequence.\n\nAt Princeton University, Henry Norris Russell was following a similar course of research. He was studying the relationship between the spectral classification of stars and their actual brightness as corrected for distance—their absolute magnitude. For this purpose he used a set of stars that had reliable parallaxes and many of which had been categorized at Harvard. When he plotted the spectral types of these stars against their absolute magnitude, he found that dwarf stars followed a distinct relationship. This allowed the real brightness of a dwarf star to be predicted with reasonable accuracy.\n\nOf the red stars observed by Hertzsprung, the dwarf stars also followed the spectra-luminosity relationship discovered by Russell. However, the giant stars are much brighter than dwarfs and so do not follow the same relationship. Russell proposed that the \"giant stars must have low density or great surface-brightness, and the reverse is true of dwarf stars\". The same curve also showed that there were very few faint white stars.\n\nIn 1933, Bengt Strömgren introduced the term Hertzsprung–Russell diagram to denote a luminosity-spectral class diagram. This name reflected the parallel development of this technique by both Hertzsprung and Russell earlier in the century.\n\nAs evolutionary models of stars were developed during the 1930s, it was shown that, for stars of a uniform chemical composition, a relationship exists between a star's mass and its luminosity and radius. That is, for a given mass and composition, there is a unique solution for determining the star's radius and luminosity. This became known as the Vogt-Russell theorem; named after Heinrich Vogt and Henry Norris Russell. By this theorem, when a star's chemical composition and its position on the main sequence is known, so too is the star's mass and radius. (However, it was subsequently discovered that the theorem breaks down somewhat for stars of non-uniform composition.)\n\nA refined scheme for stellar classification was published in 1943 by William Wilson Morgan and Philip Childs Keenan. The MK classification assigned each star a spectral type—based on the Harvard classification—and a luminosity class. The Harvard classification had been developed by assigning a different letter to each star based on the strength of the hydrogen spectral line, before the relationship between spectra and temperature was known. When ordered by temperature and when duplicate classes were removed, the spectral types of stars followed, in order of decreasing temperature with colors ranging from blue to red, the sequence O, B, A, F, G, K and M. (A popular mnemonic for memorizing this sequence of stellar classes is \"Oh Be A Fine Girl/Guy, Kiss Me\".) The luminosity class ranged from I to V, in order of decreasing luminosity. Stars of luminosity class V belonged to the main sequence.\n\nIn April 2018, astronomers reported the detection of the most distant \"ordinary\" (i.e., main sequence) star, named Icarus (formally, MACS J1149 Lensed Star 1), at 9 billion light-years away from Earth.\n\nWhen a protostar is formed from the collapse of a giant molecular cloud of gas and dust in the local interstellar medium, the initial composition is homogeneous throughout, consisting of about 70% hydrogen, 28% helium and trace amounts of other elements, by mass. The initial mass of the star depends on the local conditions within the cloud. (The mass distribution of newly formed stars is described empirically by the initial mass function.) During the initial collapse, this pre-main-sequence star generates energy through gravitational contraction. Upon reaching a suitable density, energy generation is begun at the core using an exothermic nuclear fusion process that converts hydrogen into helium.\n\nWhen nuclear fusion of hydrogen becomes the dominant energy production process and the excess energy gained from gravitational contraction has been lost, the star lies along a curve on the Hertzsprung–Russell diagram (or HR diagram) called the standard main sequence. Astronomers will sometimes refer to this stage as \"zero age main sequence\", or ZAMS. The ZAMS curve can be calculated using computer models of stellar properties at the point when stars begin hydrogen fusion. From this point, the brightness and surface temperature of stars typically increase with age.\n\nA star remains near its initial position on the main sequence until a significant amount of hydrogen in the core has been consumed, then begins to evolve into a more luminous star. (On the HR diagram, the evolving star moves up and to the right of the main sequence.) Thus the main sequence represents the primary hydrogen-burning stage of a star's lifetime.\n\nThe majority of stars on a typical HR diagram lie along the main-sequence curve. This line is pronounced because both the spectral type and the luminosity depend only on a star's mass, at least to zeroth-order approximation, as long as it is fusing hydrogen at its core—and that is what almost all stars spend most of their \"active\" lives doing.\n\nThe temperature of a star determines its spectral type via its effect on the physical properties of plasma in its photosphere. A star's energy emission as a function of wavelength is influenced by both its temperature and composition. A key indicator of this energy distribution is given by the color index, \"B\" − \"V\", which measures the star's magnitude in blue (\"B\") and green-yellow (\"V\") light by means of filters. This difference in magnitude provides a measure of a star's temperature.\n\nMain-sequence stars are called dwarf stars, but this terminology is partly historical and can be somewhat confusing. For the cooler stars, dwarfs such as red dwarfs, orange dwarfs, and yellow dwarfs are indeed much smaller and dimmer than other stars of those colors. However, for hotter blue and white stars, the size and brightness difference between so-called \"dwarf\" stars that are on the main sequence and the so-called \"giant\" stars that are not becomes smaller; for the hottest stars it is not directly observable. For those stars the terms \"dwarf\" and \"giant\" refer to differences in spectral lines which indicate if a star is on the main sequence or off it. Nevertheless, very hot main-sequence stars are still sometimes called dwarfs, even though they have roughly the same size and brightness as the \"giant\" stars of that temperature.\n\nThe common use of \"dwarf\" to mean main sequence is confusing in another way, because there are dwarf stars which are not main-sequence stars. For example, a white dwarf is the dead core of a star that is left after the star has shed its outer layers, that is much smaller than a main-sequence star, roughly the size of Earth. These represent the final evolutionary stage of many main-sequence stars.\n\nBy treating the star as an idealized energy radiator known as a black body, the luminosity \"L\" and radius \"R\" can be related to the effective temperature \"T\" by the Stefan–Boltzmann law:\n\nwhere \"σ\" is the Stefan–Boltzmann constant. As the position of a star on the HR diagram shows its approximate luminosity, this relation can be used to estimate its radius.\n\nThe mass, radius and luminosity of a star are closely interlinked, and their respective values can be approximated by three relations. First is the Stefan–Boltzmann law, which relates the luminosity \"L\", the radius \"R\" and the surface temperature \"T\". Second is the mass–luminosity relation, which relates the luminosity \"L\" and the mass \"M\". Finally, the relationship between \"M\" and \"R\" is close to linear. The ratio of \"M\" to \"R\" increases by a factor of only three over 2.5 orders of magnitude of \"M\". This relation is roughly proportional to the star's inner temperature \"T\", and its extremely slow increase reflects the fact that the rate of energy generation in the core strongly depends on this temperature, whereas it has to fit the mass–luminosity relation. Thus, a too high or too low temperature will result in stellar instability.\n\nA better approximation is to take \"ε\" = \"L/M\", the energy generation rate per unit mass, as ε is proportional to \"T\", where \"T\" is the core temperature. This is suitable for stars at least as massive as the Sun, exhibiting the CNO cycle, and gives the better fit \"R\" ∝ \"M\".\n\nThe table below shows typical values for stars along the main sequence. The values of luminosity (\"L\"), radius (\"R\") and mass (\"M\") are relative to the Sun—a dwarf star with a spectral classification of G2 V. The actual values for a star may vary by as much as 20–30% from the values listed below.\n\nAll main-sequence stars have a core region where energy is generated by nuclear fusion. The temperature and density of this core are at the levels necessary to sustain the energy production that will support the remainder of the star. A reduction of energy production would cause the overlaying mass to compress the core, resulting in an increase in the fusion rate because of higher temperature and pressure. Likewise an increase in energy production would cause the star to expand, lowering the pressure at the core. Thus the star forms a self-regulating system in hydrostatic equilibrium that is stable over the course of its main sequence lifetime.\n\nMain-sequence stars employ two types of hydrogen fusion processes, and the rate of energy generation from each type depends on the temperature in the core region. Astronomers divide the main sequence into upper and lower parts, based on which of the two is the dominant fusion process. In the lower main sequence, energy is primarily generated as the result of the proton-proton chain, which directly fuses hydrogen together in a series of stages to produce helium. Stars in the upper main sequence have sufficiently high core temperatures to efficiently use the CNO cycle. (See the chart.) This process uses atoms of carbon, nitrogen and oxygen as intermediaries in the process of fusing hydrogen into helium.\n\nAt a stellar core temperature of 18 million Kelvin, the PP process and CNO cycle are equally efficient, and each type generates half of the star's net luminosity. As this is the core temperature of a star with about 1.5 , the upper main sequence consists of stars above this mass. Thus, roughly speaking, stars of spectral class F or cooler belong to the lower main sequence, while A-type stars or hotter are upper main-sequence stars. The transition in primary energy production from one form to the other spans a range difference of less than a single solar mass. In the Sun, a one solar-mass star, only 1.5% of the energy is generated by the CNO cycle. By contrast, stars with 1.8 or above generate almost their entire energy output through the CNO cycle.\n\nThe observed upper limit for a main-sequence star is 120–200 . The theoretical explanation for this limit is that stars above this mass can not radiate energy fast enough to remain stable, so any additional mass will be ejected in a series of pulsations until the star reaches a stable limit. The lower limit for sustained proton–proton nuclear fusion is about 0.08 or 80 times the mass of Jupiter. Below this threshold are sub-stellar objects that can not sustain hydrogen fusion, known as brown dwarfs.\n\nBecause there is a temperature difference between the core and the surface, or photosphere, energy is transported outward. The two modes for transporting this energy are radiation and convection. A radiation zone, where energy is transported by radiation, is stable against convection and there is very little mixing of the plasma. By contrast, in a convection zone the energy is transported by bulk movement of plasma, with hotter material rising and cooler material descending. Convection is a more efficient mode for carrying energy than radiation, but it will only occur under conditions that create a steep temperature gradient.\n\nIn massive stars (above 10 ) the rate of energy generation by the CNO cycle is very sensitive to temperature, so the fusion is highly concentrated at the core. Consequently, there is a high temperature gradient in the core region, which results in a convection zone for more efficient energy transport. This mixing of material around the core removes the helium ash from the hydrogen-burning region, allowing more of the hydrogen in the star to be consumed during the main-sequence lifetime. The outer regions of a massive star transport energy by radiation, with little or no convection.\n\nIntermediate-mass stars such as Sirius may transport energy primarily by radiation, with a small core convection region. Medium-sized, low-mass stars like the Sun have a core region that is stable against convection, with a convection zone near the surface that mixes the outer layers. This results in a steady buildup of a helium-rich core, surrounded by a hydrogen-rich outer region. By contrast, cool, very low-mass stars (below 0.4 ) are convective throughout. Thus the helium produced at the core is distributed across the star, producing a relatively uniform atmosphere and a proportionately longer main sequence lifespan.\n\nAs non-fusing helium ash accumulates in the core of a main-sequence star, the reduction in the abundance of hydrogen per unit mass results in a gradual lowering of the fusion rate within that mass. Since it is the outflow of fusion-supplied energy that supports the higher layers of the star, the core is compressed, producing higher temperatures and pressures. Both factors increase the rate of fusion thus moving the equilibrium towards a smaller, denser, hotter core producing more energy whose increased outflow pushes the higher layers further out. Thus there is a steady increase in the luminosity and radius of the star over time. For example, the luminosity of the early Sun was only about 70% of its current value. As a star ages this luminosity increase changes its position on the HR diagram. This effect results in a broadening of the main sequence band because stars are observed at random stages in their lifetime. That is, the main sequence band develops a thickness on the HR diagram; it is not simply a narrow line.\n\nOther factors that broaden the main sequence band on the HR diagram include uncertainty in the distance to stars and the presence of unresolved binary stars that can alter the observed stellar parameters. However, even perfect observation would show a fuzzy main sequence because mass is not the only parameter that affects a star's color and luminosity. Variations in chemical composition caused by the initial abundances, the star's evolutionary status, interaction with a close companion, rapid rotation, or a magnetic field can all slightly change a main-sequence star's HR diagram position, to name just a few factors. As an example, there are metal-poor stars (with a very low abundance of elements with higher atomic numbers than helium) that lie just below the main sequence and are known as subdwarfs. These stars are fusing hydrogen in their cores and so they mark the lower edge of main sequence fuzziness caused by variance in chemical composition.\n\nA nearly vertical region of the HR diagram, known as the instability strip, is occupied by pulsating variable stars known as Cepheid variables. These stars vary in magnitude at regular intervals, giving them a pulsating appearance. The strip intersects the upper part of the main sequence in the region of class \"A\" and \"F\" stars, which are between one and two solar masses. Pulsating stars in this part of the instability strip that intersects the upper part of the main sequence are called Delta Scuti variables. Main-sequence stars in this region experience only small changes in magnitude and so this variation is difficult to detect. Other classes of unstable main-sequence stars, like Beta Cephei variables, are unrelated to this instability strip.\n\nThe total amount of energy that a star can generate through nuclear fusion of hydrogen is limited by the amount of hydrogen fuel that can be consumed at the core. For a star in equilibrium, the energy generated at the core must be at least equal to the energy radiated at the surface. Since the luminosity gives the amount of energy radiated per unit time, the total life span can be estimated, to first approximation, as the total energy produced divided by the star's luminosity.\n\nFor a star with at least 0.5 , when the hydrogen supply in its core is exhausted and it expands to become a red giant, it can start to fuse helium atoms to form carbon. The energy output of the helium fusion process per unit mass is only about a tenth the energy output of the hydrogen process, and the luminosity of the star increases. This results in a much shorter length of time in this stage compared to the main sequence lifetime. (For example, the Sun is predicted to spend burning helium, compared to about 12 billion years burning hydrogen.) Thus, about 90% of the observed stars above 0.5 will be on the main sequence. On average, main-sequence stars are known to follow an empirical mass-luminosity relationship. The luminosity (\"L\") of the star is roughly proportional to the total mass (\"M\") as the following power law:\n\nThis relationship applies to main-sequence stars in the range 0.1–50 .\n\nThe amount of fuel available for nuclear fusion is proportional to the mass of the star. Thus, the lifetime of a star on the main sequence can be estimated by comparing it to solar evolutionary models. The Sun has been a main-sequence star for about 4.5 billion years and it will become a red giant in 6.5 billion years, for a total main sequence lifetime of roughly 10 years. Hence:\n\nwhere \"M\" and \"L\" are the mass and luminosity of the star, respectively, formula_4 is a solar mass, formula_5 is the solar luminosity and formula_6 is the star's estimated main sequence lifetime.\n\nAlthough more massive stars have more fuel to burn and might intuitively be expected to last longer, they also radiate a proportionately greater amount with increased mass. This is required by the stellar equation of state; for a massive star to maintain equilibrium, the outward pressure of radiated energy generated in the core not only must but \"will\" rise to match the titanic inward gravitational pressure of its envelope. Thus, the most massive stars may remain on the main sequence for only a few million years, while stars with less than a tenth of a solar mass may last for over a trillion years.\n\nThe exact mass-luminosity relationship depends on how efficiently energy can be transported from the core to the surface. A higher opacity has an insulating effect that retains more energy at the core, so the star does not need to produce as much energy to remain in hydrostatic equilibrium. By contrast, a lower opacity means energy escapes more rapidly and the star must burn more fuel to remain in equilibrium. Note, however, that a sufficiently high opacity can result in energy transport via convection, which changes the conditions needed to remain in equilibrium.\n\nIn high-mass main-sequence stars, the opacity is dominated by electron scattering, which is nearly constant with increasing temperature. Thus the luminosity only increases as the cube of the star's mass. For stars below 10 , the opacity becomes dependent on temperature, resulting in the luminosity varying approximately as the fourth power of the star's mass. For very low-mass stars, molecules in the atmosphere also contribute to the opacity. Below about 0.5 , the luminosity of the star varies as the mass to the power of 2.3, producing a flattening of the slope on a graph of mass versus luminosity. Even these refinements are only an approximation, however, and the mass-luminosity relation can vary depending on a star's composition.\n\nWhen a main-sequence star has consumed the hydrogen at its core, the loss of energy generation causes its gravitational collapse to resume and the star evolves off the main sequence. The path which the star follows across the HR diagram is called an evolutionary track.\n\nStars with less than , are predicted to directly become white dwarfs when energy generation by nuclear fusion of hydrogen at their core comes to a halt although no stars are old enough for this to have occurred.\nIn stars more massive than , the hydrogen surrounding the helium core reaches sufficient temperature and pressure to undergo fusion, forming a hydrogen-burning shell and causing the outer layers of the star to expand and cool. The stage as these stars move away from the main sequence is known as the subgiant branch; it is relatively brief and appears as a gap in the evolutionary track since few stars are observed at that point.\n\nWhen the helium core of low-mass stars becomes degenerate, or the outer layers of intermediate-mass stars cool sufficiently to become opaque, their hydrogen shells increase in temperature and the stars start to become more luminous. This is known as the red giant branch; it is a relatively long-lived stage and it appears prominently in H-R diagrams. These stars will eventually end their lives as white dwarfs.\n\nThe most massive stars do not become red giants, instead their cores quickly become hot enough to fuse helium and eventually heavier elements and they are known as supergiants. They follow approximately horizontal evolutionary tracks from the main sequence across the top of the H-R diagram. Supergiants are relatively rare and do not show prominently on most H-R diagrams. Their cores will eventually collapse, usually leading to a supernova and leaving behind either a neutron star or black hole.\n\nWhen a cluster of stars is formed at about the same time, the main sequence lifespan of these stars will depend on their individual masses. The most massive stars will leave the main sequence first, followed in sequence by stars of ever lower masses. The position where stars in the cluster are leaving the main sequence is known as the turnoff point. By knowing the main sequence lifespan of stars at this point, it becomes possible to estimate the age of the cluster.\n\n\n"}
{"id": "16738000", "url": "https://en.wikipedia.org/wiki?curid=16738000", "title": "Marcellus Formation", "text": "Marcellus Formation\n\nThe Marcellus Formation (also classified as the Marcellus Subgroup of the Hamilton Group, Marcellus Member of the Romney Formation, or simply the Marcellus Shale) is a Middle Devonian age unit of sedimentary rock found in eastern North America. Named for a distinctive outcrop near the village of Marcellus, New York, in the United States,\nit extends throughout much of the Appalachian Basin. \n\nThe unit name usage by the U.S. Geological Survey (USGS) includes \"Marcellus Shale\" and \"Marcellus Formation\". The unit was first described and named as the \"Marcellus shales\" by J. Hall in 1839.\n\nStratigraphically, the Marcellus is the lowest unit of the Devonian age Hamilton Group, and is divided into several sub-units. Although black shale is the dominant lithology, it also contains lighter shales and interbedded limestone layers due to sea level variation during its deposition almost . The black shale was deposited in relatively deep water devoid of oxygen, and is only sparsely fossiliferous. Most fossils are contained in the limestone members, and the fossil record in these layers provides important paleontological insights on faunal turnovers. \n\nThe shale contains largely untapped natural gas reserves, and its proximity to the high-demand markets along the East Coast of the United States makes it an attractive target for energy development and export.\nThe black shales also contain iron ore that was used in the early economic development of the region, and uranium and pyrite which are environmental hazards. The fissile shales are also easily eroded, presenting additional civil and environmental engineering challenges.\n\nThe Marcellus Formation is a black shale that may contain limestone beds and concentrations of iron pyrite (FeS) and siderite (FeCO).\nIts sedimentary structure, or bedding, is moderately well developed. Like most shales, it tends to split easily along the bedding plane, a property known as fissility.\nLighter colored shales in the upper portion of the formation tend to split into small thin-edged fragments after exposure.\nThese fragments may have rust stains from exposure of pyrite to air, and tiny gypsum (CaSO·2HO) crystals from the reaction between pyrite and limestone particles.\nFresh exposures of the pyriteiferous shale may develop the secondary mineralization of orange limonite (FeO(OH)·nHO), and the pale yellow efflorescence or bloom of sulfur, associated with acid rock drainage.\nPyrite is especially abundant near the base,\nand the upper contacts of limestones, but framboidal microcrystals and euhedral crystals of pyrite occur throughout the organic-rich deposits.\nThe Marcellus also contains uranium,\nand the radioactive decay of the uranium-238 (U) makes it a source rock for radioactive radon gas (Rn).\n\nMeasured total organic content of the Marcellus Formation ranges from less than 1% in eastern New York, to over 11% in the central part of the state,\nand the shale may contain enough carbon to support combustion.\nThe more organic-rich black shales can be bituminous, but are too old to contain bituminous coal formed from land plants.\nIn petroleum geology, these black shales are an important source rock that filled conventional petroleum reservoirs in overlying formations, are an unconventional shale gas reservoir, and are an impermeable seal that traps underlying conventional natural gas reservoirs.\nTo the west the formation may produce liquid petroleum; further north heating during deeper burial more than 240 million years ago cracked this oil into gas.\n\nThe Marcellus is found throughout the Allegheny Plateau region of the northern Appalachian Basin of North America. In the United States, the Marcellus shale runs across the Southern Tier and Finger Lakes regions of New York, in northern and western Pennsylvania, eastern Ohio, through western Maryland, and throughout most of West Virginia extending across the state line into extreme western Virginia. The Marcellus bedrock in eastern Pennsylvania extends across the Delaware River into extreme western New Jersey. It also exists in the subsurface of a small portion of Kentucky and Tennessee. Below Lake Erie, it can be found crossing the border into Canada, where it stretches between Port Stanley and Long Point to St. Thomas in southern Ontario.\n\nThe Marcellus appears in outcrops along the northern margin of the formation in central New York. There, the two joint planes in the Marcellus are nearly at right angles, each making cracks in the formation that run perpendicular to the bedding plane, which lies almost level.\nThese joints form smooth nearly vertical cliffs, and the intersecting joint planes form projecting corners in the rock faces.\nOnce exposed, the weathered faces lose most of their organic carbon,\nturning from black or dark gray to a lighter shade of gray.\n\nOutcrops of the Marcellus can contain very small beds that resemble coal. The New York outcrops, and others further south in Pennsylvania and New Jersey, were extensively excavated in the early 19th century, sometimes at great expense, in the false hope of finding minable coal seams.\nIn Perry County, Pennsylvania along the Juniata River the false coal beds become up to thick, but they did not produce a valuable fuel, despite the considerable effort expended to mine it from the surrounding hills.\nSeaweed and marine plants probably formed the false coal. True coal is formed from terrestrial plants, which only began to appear in Marcellus and later fossils.\n\nClose proximity to the surface of Marcellus bedrock south of the New York outcrops makes an east-west band running through the city of Syracuse a high-risk area for radon as an indoor air pollutant. From the surface exposures along the northern and eastern margins, the formation descends to depths of over below the surface in southern Pennsylvania.\n\nUpturned beds are exposed in sections of the folded Ridge-and-Valley Appalachians, including exposures on the flanks and axis of the Broad Top Synclinorium in south central Pennsylvania. Exposed beds are nearly horizontal on the Allegheny Plateau, but upturned to form slightly overturned beds found along the Allegheny Front.\nFrom Wind Gap, Pennsylvania heading south, the dip of the beds steepens, becoming vertical at Bowmanstown on the Lehigh River. Nearby, in the Lehigh Gap area of Pennsylvania, the Marcellus is extensively faulted,\nand the beds are steeply overturned, with a reverse dip angle of up to 40° south.\n\nThe Marcellus Shale and the fine-grained shales near the middle of the Mahantango Formation are classified by geologists as slope-formers.\nMarcellus and Mahantango shale beds dipping at 60° to 75° to the west form the west facing slopes of Tonoloway Ridge on the west flank of the Cacapon Mountain anticline in the Eastern Panhandle of West Virginia.\nOn the eastern limb of this anticline, beds of these shales dipping to the east at a shallower angle also form the steep slopes on the east side of Warm Springs Ridge.\n\nThe Marcellus is easily eroded, and is also found underlying low areas between some Appalachian ridges, forming linear valleys of moderate relief. These bedrock surfaces are typically covered with colluvium from erosion of stratigraphically higher and more erosion-resistant strata that form the surrounding higher ground. The soils formed from the Marcellus and the overlying Hamilton shales are deep, free of stones, and well suited for agriculture.\nSampling of soil formed on the Marcellus bedrock showed the dominant mineralogy consisted of quartz, illite, montmorillonite, muscovite, and biotite, with phases of todorokite and trona appearing at depths closer to the bedrock.\n\nUpturned beds of the soft shale also capture streams and rivers with relatively straight segments in strike valleys such as the Aquashicola Creek and McMichael Creek at the foot of The Poconos,\nand the long, straight section of the Lost River in West Virginia.\nBelow Port Jervis, New York, the Walpack Ridge deflects the Delaware River into the Minisink Valley, where it follows the southwest strike of the eroded Marcellus beds along the Pennsylvania – New Jersey state line for to the end of the ridge at Walpack Bend in the Delaware Water Gap National Recreation Area.\nThe Minisink is a buried valley where the Delaware flows in a bed of glacial till that buried the eroded Marcellus bedrock during the last glacial period. This buried valley continues along the strike of the Marcellus southwest from the bend through Stroudsburg, Pennsylvania, and northeast from Port Jervis toward the Hudson River,\nalong the route of the Delaware and Hudson Canal.\n\nEarly in the Acadian orogeny, as the Acadian Mountains were rising up, the black and gray shales of the Hamilton Group began accumulating as erosion of the mountains deposited terrigenous sediments from the land into the sea.\nThe Marcellus Shale was formed from the very first deposits in a relatively deep, sediment- and oxygen-starved (anoxic), trough that formed parallel to the mountain chain.\nThese clastic fragments of rock were carried in braided streams to the ancient Catskill Delta, a river delta probably similar to the present day Niger Delta of Africa.\n\nSmaller particles remained suspended longer in this epeiric sea, flowing offshore as turbidites in a slow but persistent underwater avalanche. They finally came to rest at the bottom of the Acadian foredeep in the Appalachian Basin, hundreds of meters from shore, at depths that may have been or more beneath the surface.\nAlternatively, the basin may have been as shallow as or less, if the warm water was sufficiently stratified so that oxygen rich surface water did not mix with the anoxic bottom water.\nThe Marcellus deposition produced a \"transgressive\" black shale,\nbecause it was deposited in deepening conditions when the basin floor dropped as the mountains rose up.\n\nThe dark shale facies of the Marcellus were formed from flysch, a fine mud deposited in deep water; the deepening sea that deposited the Marcellus cut off the supply of carbonates that form limestone and the fine-grained flysch sediments buried the Onondaga limestone beds.\n\nOrganic matter, probably dominated by plankton, also settled to the bottom, but the normal aerobic decay process was inhibited in the anaerobic environment thereby preserving the organic carbon.\n\nUranium was also incorporated in these organic muds syndepositionally,\nmeaning it was deposited at the same time, rather than being introduced to the formation later.\nThe organic matter scavenged trace elements from the seawater,\nincluding the redox-sensitive elements uranium, rhenium, molybdenum, osmium, chromium, and selenium.\n\nThe Marcellus was deposited during the development of land plants, when atmospheric oxygen was increasing, resulting in a reduction of carbon dioxide in the atmosphere, and the seawater where it was deposited.\nNamed members of the Marcellus reflect two composite depositional sequences,\nwith a general coarsening upward cycle that continues into the base of the overlying Mahantango Formation.\nThe interbedding of lighter shale and limestone members is attributed to relatively short-term oscillations in basin depth.\nLater deep water depositional sequences formed the overlying Brallier Formation and Harrell Formation.\n\nOn the geological timescale, the Marcellus occurs in the Middle Devonian epoch, of the Devonian period, in the Paleozoic era, of the Phanerozoic eon. Radiometric dating of a Marcellus sample from Pennsylvania placed its age at 384 million years old, and a sample from the bentonite at the top of the Onondaga at 390 ± 0.5 million years old.\n\nRelative age dating of the Marcellus places its formation in the Cazenovia subdivision of the Givetian faunal stage, or 391.9 to 383.7 million years ago (Ma).\nThe Union Springs member, at the base of the Marcellus in New York, has been dated to the end of the Eifelian, the stage which immediately preceded the Givetian.\nAnoxic dark shales in the formation mark the Kačák Event,\na late-Eifelian-stage marine anoxic event also associated with an extinction event. In 2012, Read and Erikson also depicted the formation as Eifelian.\n\nIn the first Pennsylvania Geological Survey, begun in 1836, Henry Darwin Rogers classified the Marcellus as the \"Cadent Lower Black Slate\" which he numbered \"No. VIII b.\" In the first New York State Geological Survey, also begun that year, James Hall established the term \"Marcellus Shale\" in his 1839 report titled \"Marcellus Shales in Seneca County.\" Professor Hall also argued in 1839 against formulating geological names based on observed characteristics that may vary from place to place or need revision in the future, and in favor of location-based nomenclature where \"the rock or group will receive its name from the place where it is best developed.\" His arguments proved persuasive, and the location-based name for this, and many of the other group names he published based on exposures in New York, were adopted in the second Pennsylvania survey, and are now widely accepted.\n\nIn the first New York survey, the Marcellus shale was placed below the Hamilton Group at the base of the Erie division of the New York system, but this taxonomy is obsolete.\n\nIn current practice, the Marcellus Formation (abbr. Dm or Dms) is classified as the basal unit of the Hamilton Group (Dh), lying beneath the Mahantango Formation (Dmh) member of this group in Pennsylvania and Maryland. In New York, the Mahantango, also of Middle Devonian age, is further divided. There the Marcellus is separated from the overlying Skaneateles Formation, a more clastic and fossiliferous dark shale, by the thin Stafford or Mottville Limestone bed.\n\nIn West Virginia, the Marcellus may be separated from the brown shales of the Mahantango by occasional sandstone beds and concretions,\nor it may lie directly below the younger Late Devonian Harrel Formation (or its lateral equivalents) because of a disconformity, which represents a gap in the geological record due to a period of erosion or non-deposition.\nIn eastern Ohio the Hamilton Group also lies disconformably beneath the Rhinestreet Shale Member of the West Falls Formation, another transgressive black shale tongue with similar characteristics to the Marcellus.\n\nThe Marcellus shale is typically found deposited on the limestone of the Onondaga Formation (Don), which extends down to the end of the Early Devonian period. The contact between them may be sharp, gradational, or erosional. In southwestern Ontario, Canada, north of Lake Erie, the Marcellus overlies the Dundee Formation, a lateral equivalent of the Onondaga.\nIn Pennsylvania, the Marcellus forms a sharp conformable contact with the Onondaga's Selinsgrove Limestone member.\nA thin pyrite-carbonate bed is also found at the base of the Marcellus black shale in the exposures of south central Pennsylvania, above a thin calcerous green shale bed, which lies upon the Onondaga limestone.\n\nIn eastern New York, the contact between the Marcellus and Onondaga (where present) is gradational.\nIn western New York, the Union Springs member of the Marcellus conformably overlies the Seneca member of the Onondaga Limestone,\nor the stratigraphically higher Cherry Valley Limestone member may rest directly and unconformably upon the Onondaga in the absence of the Union Springs shale.\nThe local disappearance of units of the Onondaga suggests that its upper contact with the Marcellus can be erosional.\nIn Erie County in western New York, both the upper and lower contact of the Marcellus are eroded away.\n\nIn eastern West Virginia the Marcellus overlies the Onesquethaw Group, consisting of the dark gray or green, calcitic, mostly nonfissil Needmore Shale, which grades westward into the Huntersville Chert.\nTo the south and west, the Hamilton Group grades laterally into the Millboro Shale formation in southern West Virginia and Virginia,\nwhich grades into the lower part of the Chattanooga Shale of Tennessee.\n\nThe Milboro is gradational with the underlying Needmore Formation shale.\nSouth of the Mason-Dixon line, due to the difficulty in differentiating the Millboro and Needmore shales with the limited exposures available,\nand initial uncertainty in correlation with the New York survey, they were mapped as the Romney Formation, a unit containing all the Middle Devonian strata,\nnamed for an exposure at Romney, West Virginia.\nThe correlations were established by 1916 through tracing the New York exposures across Pennsylvania and Maryland into West Virginia, so under the principle of scientific priority,\nthe Romney classification is now obsolete; but its Marcellus and underlying Needmore shale members are still found grouped in an undifferentiated map unit (Dmn).\n\nTioga metabentonite or K-bentonite–stratigraphic unit about thick that consists of several discrete, relatively thin volcanic ash falls–is also included at the base of the Marcellus in eastern Pennsylvania.\nIn 1843 it was described without being named by Hall,\nand more than 100 years passed before it was eventually named for the natural gas field in Tioga County, Pennsylvania,\nwhere it was encountered when drilling gas wells. It is a regional stratigraphic marker,\nused by geologists to identify the Marcellus,\nand correlate laterally equivalent strata. Difficulty in correctly identifying the more than 80 different ash falls during the Devonian period, collected in 15 or more beds, has also led to many miscorrelations.\n\nFrom Virginia to New York the Tioga is widely distributed, running across the central and northern parts of the Appalachian basin,\nan areal extent exceeding .\nExplosive eruptions associated with the Acadian orogeny originating near present-day central Virginia released the ash into the atmosphere.\nIt was dispersed across the Appalachian, Michigan, and Illinois Basins by the southern trade winds, because this area was in the southern hemisphere during the Devonian period.\nThe volcanic origin of the ash is evidenced by its distinctive mineralogy–the ash was deposited directly upon the water, so its angular quartz grains differ from the clastic sediments rounded through the erosion process that carries them to the sea. As the volcanic ash settled to the bottom, it was admixed with these terrigenous components, producing a distinctive lithology in the sedimentary rock.\n\nThe Tioga may appear in the formation as a gray, brown, black, or olive bed, or parting,\nconsisting of coarse crystal tuff or tuffaceous shale,\nthinly laminated, with sand-sized mica flakes.\nThe Tioga ash bed zone consists of eight ash beds labeled according to their stratigraphic order from A (oldest) to H (youngest),\nand another bed known as the Tioga middle coarse zone.\nIts basal beds are found within the uppermost beds of the Onondaga Limestone or Needmore Shale, and the uppermost ash bed within the lowermost part of the Marcellus or Millboro Shale.\nIn western New York state, the Tioga Ash Bed B marks the boundary between the Moorehouse and Seneca Members of the Onondaga Formation,\nbut in the central part of the state, and the southern part of the basin, the ash beds are actually in the Marcellus.\nThis indicates that deposition of the Marcellus there began earlier,\nsince the ash beds represent a single epoch in geologic time.\n\nMaximum thickness of the Marcellus ranges from in New Jersey, to in Canada.\nIn West Virginia, the Marcellus Formation is as much as thick.\nIn extreme eastern Pennsylvania, it is thick,\nthinning to the west, becoming only thick along the Ohio River, and only a few feet in Licking County, Ohio.\nThe thinning, or stratigraphic convergence, from east to west is caused by decreasing grain size in the clastic deposits, which entered the basin from the east.\nThe beds finally \"pinch out\" westward because deposition was limited by the Cincinnati Arch,\nthe bulge that formed the west shore of the basin. Where the formation is relatively thick, it is divided into several members, and as the formation continues to thicken to the east, these members are further divided. Some workers chose to classify the Marcellus as a subgroup, and classify some of the members as separate formations.\n\nA local Purcell limestone member, of inter-bedded calcitic shale and limestone,\ndivides the Marcellus in eastern Pennsylvania.\nThe Purcell is stratigraphically equivalent to the Cherry Valley Limestone member in New York,\na bioclastic packstone, consisting of skeletal limestones, with shaly intervals between its lower massive limestone layer, thick nodular limestone/marlstone, and upper limestone layer.\nOther named members include the Bakoven Shale, Cardiff Shale, Chittenango shale, Solsville sandstone, Union Springs shale and limestone, and Stony Hollow shale and limestone. The Union Springs, Cherry Valley, and Oatka Creek merge beneath Lake Erie, into the Bell Shale, Rockport Quarry Limestone, and Arkona Shale of Ontario.\n\nThe Union Springs is an organic-rich, pyritiferous, thinly bedded, blackish gray to black shale with mudstone concretionary layers, and thin silt bands at the bottom. To the east, it becomes the Bakoven Member, a darker, less organic shale with fewer limestone layers. To the west the Union Springs beds thin, with its upper limestones merging with the overlying calcareous Cherry Valley Member. A regional unconformity appears in western New York, as the Union Springs lenses in and out, and then reappears in northwest Pennsylvania and northeast Ohio between the Onondaga and Cherry Valley.\n\nIn Western and central New York, the uppermost member is the dark grey to black organic-rich Oatka Creek shale. Unlike the other Devonian shales in this region, the gray shale at the top of the Oatka Creek thickens gradually to the west, as well as the east,\nwhere it divides into the Cardiff member lying above the Chittenango member in central New York.\nOrganic-rich, fissile, sooty black shales make up the Chittenango Member.\nAt the base of the Chittenango,\nabove the Bierne Member shale,\nlies the Halihan Hill Bed, a highly bioturbated bioclastic limestone.\n\nFurther east, the homogeneous Cardiff divides into the Bridgewater, Solsville, and Pecksport shale members, from base to top. The Bridgewater is a fissile dark silty shale with relatively rare fossils. A thin concretionary zone lies above, then the Solsville grades from a gray calcareous shale, to sandy siltstones and fine sandstones at the top, with the gray shale of the Pecksport shale and siltsone overlying it.\n\nIn south central Pennsylvania, the Marcellus is mapped with three members, from top to base: The Mahanoy Member (Dmm), a dark gray to grayish black silty shale and siltstone; the Turkey Ridge Member (Dmt), an olive to dark-gray fine to medium grained sandstone; and the Shamokin Member (Dms), a dark gray to grayish black fissile carbonaceous shale that is calcareous in places near the base. The Turkey Ridge is commonly mapped in the Mahantango Formation, or included in the Montebello Formation (Dmot), and only the Shamokin correlates with the Marcellus on adjacent map sheets. In extreme eastern Pennsylvania, the Broadhead Creek member, a dark gray silty shale with dark gray shaly limestone concretions, appears above the Stony Hollow and Union Springs, in a layer up to thick.\n\nThere are relatively sparse inclusions of fossilized marine fauna found in the Marcellus, but these fossils are still important to paleontology. For example, the Marcellus contains the oldest known diverse collection of thin-shelled mollusks still having well preserved shell microstructure.\nIt is also where goniatites, an extinct shelled swimmer similar to a squid, make their first appearance in the fossil record.\nLife on land also enters the fossil record in the Marcellus, with the trunks of branchless conifer trees that floated out to sea to be preserved in the black shale.\n\nMarcellus fossils include specimens of the large clam-like brachiopod Spinocyrtia.\nExternal molds of crinoids, plant-like animals related to starfish also known as \"sea lilies,\"\nare found in the formation,\nwith the molds partially filled with limonite; brachiopod and bivalve (clam) molds have also been found in the shale.\nSmall conical tentaculitids are commonly found in the Chittenango Member.\nThe Halihan Hill bed contains styliolinids and macrofauna including brachiopods, coral-like bryozoans, small bivalves and gastropods (snails),\nincorporated after the faunal turnover when Emsian and Eifelian Schoharie/Onondaga fauna were replaced by the Givetian Hamilton fauna.\n\nThe Solsville member contains well preserved bivalves, gastropods, and brachiopods. These shellfish lived in the benthic zone at the bottom of marginal marine to open marine environments that existed west of the ancient Catskill Delta.\nThe fossil record in this member shows the base was dominated by deposit feeders, while the upper layers were dominated by filter feeders. This can be correlated to the lithology: the finer sediments of the shales at the base of this member would contain abundant adherent organic matter for deposit feeders, but would tend to foul the gills of filter feeders when suspended; the coarser sediments of the sandstones at the top would have contained less organic matter to support deposit feeders.\nBelow the Solsville, at the base of the Otsego in eastern New York, a coral bed is found; another coral bed can be seen at the top of the Marcellus near Berne, New York.\n\nA diverse, eel-like conodont fauna occurs in the limestone of the Cherry Valley Member,\nwhich is also known for its rich nautiloid and goniatite cephalopod fauna.\nOriginally named the \"Goniatite Limestone\",\nit produces their fossilized remains with shells that can be larger than across.\nIt also contains the \"Cephalopod Graveyard\" in the Schoharie Valley of eastern New York, an unusual accumulation of abundant coiled and straight shells of several types of large adult cephalopods. This bed lacks juvenile fossils, indicating that if their behavior was similar to modern squid, this may have been an area where these Devonian cephalopods reproduced and died.\nThis stratigraphic interval also provides an excellent example of incursion epiboles, which are sudden appearances and disappearances of fossil taxa in relatively thin sections of the rock unit. In the Cherry Valley, the taxa do not reoccur; instead each thin concretionary limestone bed contains different species of goniatites.\nThe Cherry Valley and Union Springs also contain well-preserved anarcestida.\n\nThe Marcellus natural gas trend, which encompasses 104,000 square miles and stretches across Pennsylvania and West Virginia, and into southeast Ohio and upstate New York, is the largest source of natural gas in the United States, and production was still growing rapidly in 2013. The Marcellus is an example of shale gas, natural gas trapped in low-permeability shale, and requires the well completion method of hydraulic fracturing to allow the gas to flow to the well bore. The surge in drilling activity in the Marcellus Shale since 2008 has generated both economic benefits and environmental concerns—and thus, considerable controversy.\n\nAt the base of the Marcellus, in the pyrite-carbonate bed between the carbonaceous black shale and a green calcareous shale bed, pyrite, carbonate, and groundwater reacted to form gossan iron oxide and gypsum.\nAs far as the ground water necessary for the conversion could penetrate, the pyrite-carbonate was converted to a usable brown hematite iron ore along the outcrops and near the bedrock surface.\nThe Marcellus iron ore was actively mined in south Central Pennsylvania from its discovery in the late 18th century, until it was supplanted by the rich ore beds of the Iron Range of Minnesota in the early 20th century.\nThe ore was easily located and worked from shallow pits and shafts, but once the usable upper deposits were removed, or if a mine shaft entered the bed too far below the surface, only unusable unconverted pyritic deposits were found.\n\nHematite ore was converted to pig iron in charcoal-fired stone blast furnaces that were constructed throughout the Juniata River region near the workable ore deposits from the Marcellus and other formations.\nIron products from this area, known as \"Juniata Iron,\" were produced during the period between the American Revolution and the American Civil War. These blast furnaces were important to the economy of the region at the time,\nbut the cold blast stone furnaces typically employed were inefficient, and consumed significant amounts of timber from the nearby hardwood forests, which ultimately led to their demise. A typical furnace used of hematite ore and of charcoal to produce of pig iron, and could produce several thousand pounds per day, which required logging more than of forest daily.\n\nThe ore from the Marcellus varied in thickness, becoming unworkably thin, and even disappearing altogether in places between the workable beds. The quality of the ore also varied, and it was not always profitable to smelt, as several furnaces built near iron ore mines in the Marcellus were abandoned before the ore and timber resources used to fuel them became scarce.\n\nOre found interbedded in the black slaty shale contained a relatively high proportion of carbon which was burned in the furnace, and sulfur, which produced a usable but \"red-short\" iron.\nRed-short iron has the undesirable properties of oxidizing more easily, and a tendency to crack, especially when heated to a red-hot state.\nIn some locations in Pennsylvania the quality of the ore was quite good, with relatively deep veins containing 45% iron, and very low sulfur.\nIn Virginia, the Marcellus ore occasionally contained zinc, which produced a characteristic green flame in the furnace as it was consumed, but deposited a hard mass of impure zinc oxide known as cadmia, which built up over time near the top of the flue, and had to be removed periodically to keep it unobstructed.\n\nDrainage that reacted with the pyrite inclusions also deposited a form of bog iron near several outcrops of the Marcellus. In the 19th century, iron ore from these deposits was used as a mineral paint pigment. After being heated in a kiln and finely ground, it was mixed with linseed oil, and used to paint exterior wood on barns, covered bridges, and railroad cars. In addition to the bog iron, at several sloped locations in eastern Pennsylvania brown hematite was found lying on the Marcellus bedrock buried beneath the soil. These deposits were also excavated and used for mineral paint during that time. A bed of hematite paint ore is also found almost directly below the Marcellus, but it is actually part of the underlying Oriskany Formation.\n\nIron rich \"ferruginous waters\" emanating from chalybeate springs near the base of the Marcellus in Bedford, Pennsylvania were believed to have healing powers by Native Americans. The Bedford Springs Hotel was a mineral spa built in 1802 around a series of mineral springs, including one of these, its \"iron spring\". The Chalybeate Springs Hotel, built nearby in 1851 around three other mineral springs including another chalybeate spring, became a \"resort for invalids\". The iron-rich waters were prescribed for anemia and related complications. Both of these mineral springs contain iron in the form of dissolved iron carbonate, which gives these waters a \"slightly inky taste\".\n\nThe Marcellus has also been used locally for shale aggregate and common fill, although the pyritic shales are not suitable for this purpose because of acid rock drainage and volumetric expansion.\nIn the 19th century, this shale was used for walkways and roadways,\nand was considered superior \"road metal\" because the fine grained fragments packed together tightly, yet drained well after a rain.\n\nThe dark slaty shales may have the necessary cleavage and hardness to be worked, and were quarried for low grade roofing slate in eastern Pennsylvania during the 19th century. The slates from the Marcellus were inferior to the Martinsburg Formation slate quarried further south, and most quarries were abandoned, with the last significant operation in Lancaster County. The Marcellus black slate was also quarried in Monroe County, Pennsylvania, for school slates used by students in 19th-century rural schools.\n\nCarbonaceous shales, such as the Marcellus, are a possible target for carbon capture and storage for mitigation of global warming. Because carbon adsorbs carbon dioxide (CO) at a greater rate than methane (CH), carbon dioxide injected into the formation for geological sequestration could also be used to recover additional natural gas in a process analogous to enhanced coal bed methane recovery, but the practical value of this theoretical technique is not yet known. Scientists believe that adsorption would allow sequestration at shallower depths than absorption in deep saline formations, which must be at least below the surface to maintain liquid CO in a supercritical state.\n\nExposures from cut and fill road construction in Virginia and Pennsylvania have resulted in localized acid rock drainage due to oxidation of the pyrite inclusions.\nThe newly exposed shale on the cut face weathers rapidly, allowing air and water into the unexcavated rock, resulting in acidic surface runoff after precipitation events.\nAcidic runoff disrupts aquatic ecosystems, and highly acidic soil contaminated by this runoff will not support vegetation, which is unsightly, and can lead to problems with soil erosion.\n\nNatural decomposition of the shale into smaller fragments can affect slope stability, necessitating shallower slopes that require more material be disturbed in cut and fill work, exacerbating the acid rock drainage problem. The cut material cannot be used as fill beneath roads and structures due to volumetric expansion, compounding the problem. The Tioga ash beds contain bentonite clay which presents a landslide hazard in the unexcavated rock as well.\n\nDamage to structures constructed on fill consisting of pyritic Marcellus shale has been caused by expansion from sulfuric acid (HSO) runoff reacting with the calcite (CaCO) in the shale to produce gypsum (CaSO), which has double the molar volume.\nOther sulfate minerals that can be produced by reactions with pyrite include anhydrite, melanterite, rozenite, jarosite, and alunite.\nThe reactions have generated a heave pressure on the order of 500 kPa (10,000 pounds per square foot), but may be able to generate four times this pressure   enough to heave foundations in a 5-story building.\nLimestone, which is used to neutralize the acid drainage, can actually exacerbate the expansion problem by promoting sulfate–sulfate reactions that form the minerals thaumasite and ettringite, which have even higher molar volumes.\n\nDrilling boreholes through the Hamilton Group shales in the subsurface can be problematic. The Marcellus has a relatively low density, and these shales may not be chemically compatible with some drilling fluids. The shale is relatively fragile, and may fracture under pressure, causing a problem in circulating the drilling fluid back up through the borehole known as lost circulation. The formation may also be under-pressurized, further complicating the drilling process.\n\n\n"}
{"id": "9450429", "url": "https://en.wikipedia.org/wiki?curid=9450429", "title": "Meltwater management", "text": "Meltwater management\n\nMeltwater management is a runoff management system designed to deal with runoff caused by the melting of snow in colder climates. They were designed when it became apparent that using the Best management practice (BMP) for rainfall runoff was not working.\n\nSnow filtration is a system to deal with left-over snow in an environmentally friendly way. Removing snow from roads is done to ensure road safety. The conventional method is by removing the snow and emptying it into rivers. However, since salt is used to melt ice on the roads, the salt also enters the rivers with the dumped snow. This increases the salt content of the rivers which can have impacts on aquatic and marine ecosystems. The practice of removing snow from streets and off-loading it into rivers affects the aquatic ecosystems. \n\nIn spring as snow melts, the salt used on roads as a deicer is left behind, which can be environmentally damaging as it creates a hypertonic environment for plants.\n\nThe snow runoff is collected and brought to a collection center. It is directed through an oil and grit separator which also monitors the consistency of pollutants in the runoff. The oil and grit separator causes electrochemical and biological processes, which bind heavy metals and nutrients to the sediment by means of reduction-oxidation reactions. At the same time, this reduces the amount of chlorine present in the runoff. \n\nThe runoff then flows into a catch basin, where plants use the salt ions (e.g. K, P, N) for their growth.The runoff is further filtered by the plants, leaving only clean water to go back into streams and rivers.\n"}
{"id": "28404157", "url": "https://en.wikipedia.org/wiki?curid=28404157", "title": "Ministry of Mines and Energy (Colombia)", "text": "Ministry of Mines and Energy (Colombia)\n\nThe Ministry of Mines and Energy () is the national executive ministry of the Government of Colombia that oversees the regulation of the mining and mineral industry and the electricity sector in Colombia, it is similar in its duties to other energy ministries of other countries.\n"}
{"id": "29262193", "url": "https://en.wikipedia.org/wiki?curid=29262193", "title": "National Balloon Facility", "text": "National Balloon Facility\n\nThe National Balloon Facility, also known as the TIFR Balloon Facility, is a stratospheric-balloon launch base under the joint management of the Tata Institute of Fundamental Research (TIFR) and the Indian Space Research Organisation (ISRO). This institute is located at Hyderabad near Electronics Corporation of India Limited (ECIL). Being the only major balloon facility in the world close to the geomagnetic equator, this facility is extensively used by researchers from all over the world.\n\nBallooning activity in India was prompted by research into atmospheric exploration and cosmic ray studies. During the 1940s, Dr. Homi J. Bhabha of TIFR started sending balloons up to an altitude of 25 km to study secondary cosmic radiation. Due to unique geographic reasons Hyderabad was selected for stratospheric balloon launches. After various diplomatic meetings arrangements were made by US and Indian organisations to jointly launch balloons from Hyderabad. The first launch had support of Air Force Cambridge Research Laboratory (AFCRL) and TIFR.\n\nThe launches were carried out from two locations. US balloons were launched from Begumpet Airport and Indian balloons from Osmania University Campus. The first balloon was launched on 2 February 1961. During the first stage of the facility's operation, forty balloons were launched.\n\nAfter five years all the facilities were consolidated at Osmania University and under management of National Center for Atmospheric Research & TIFR and around 170 balloons were launched.\n\nAfter a few years with increasing demands for balloon flights, a relatively low population density and proximity to the geomagnetic equator, a permanent site at Hyderabad near ECIL was selected. However, presently due to rapid development this facility is today surrounded by several population settlements: by the Cherlapally Central Jail in the east; Kamala Nagar in the west; and Electronics Corporation of India Ltd. in the south.\n\nThe campus contains a complete balloon manufacturing facility. The institute conducts balloon-based scientific experiments in X-ray and Infrared Astronomy, and in Aeronomy.\nBalloons from this facility are launched twice in a year, i.e. in summer and winter. NBF also regularly monitors and analyses local weather at tropospheric and stratospheric altitudes, required for making decisions about balloon launches.\n\nUnder the High Altitude Balloon Development Project, a 61,000m³ indigenously developed balloon penetrated into the Mesosphere on 7 January 2014, for the first time in India. Until that time, only balloons from USA and Japan had accomplished this feat.\n\n"}
{"id": "56136980", "url": "https://en.wikipedia.org/wiki?curid=56136980", "title": "Natural History Museum of Armenia", "text": "Natural History Museum of Armenia\n\nThe Natural History Museum of Armenia represents Armenia's unique nature and its diversity. The museum was reopened in 2004.\n\nDuring the Soviet period, an anti-religion museum was opened in 1952 which was later transformed into a museum of natural sciences and in 1960 renamed as The Natural History Museum of Armenia.\nIn 1999 the museum was relocated in another building. From 2000 to 2004 with the patronage of Bella and Levon Aharonyan, two Iranian-Armenian benefactors, the building of the museum was reconstructed and opened to the public with its new exhibition.\n\nThe museum is home to over 6000 objects, only one-third of which are temporarily or primarily put on display. In four rooms, rare types of flora and fauna are exhibited. The museum also presents the personal collection of the Aharonyan family.\n\nThe objects in the museum represent the diversity of nature in Armenia. The main exhibition contains photos related to zoology, botany, geology and plastic arts. replicas of mammals, reptiles, birds, amphibious animals, fish, insects, and arachnid as well as skulls, skins, shells, pieces of trees, samples of herbarium, animal or plant fossils, useful extracted materials, oil paintings, drawings etc.\n\nThe museum holds exhibitions and occasional events aiming to increase environmental awareness and active participation in offering solutions for environmental problems. Many of the animals in the museum are registered in the IUCN Red List of Threatened Species.\n\nSince February 27, 2003, The Natural History Museum of Armenia operates according to the constitution of the republic of Armenia. The goal of the museum, аs a non-commercial organization, is to raise awareness and knowledge regarding environmental issues and positively influence the accomplishment of environmental programs.\n\nThe organization fulfills various types of enterprises such as the following:\n\nThe museum has active branches in Shirak (Gyumri) and Ararat (Qukasav) regions.\n\n\n"}
{"id": "18935319", "url": "https://en.wikipedia.org/wiki?curid=18935319", "title": "Ocean turbidity", "text": "Ocean turbidity\n\nOcean turbidity is a measure of the amount of cloudiness or haziness in sea water caused by individual particles that are too small to be seen without magnification. Highly turbid ocean waters are those with a large number of scattering particulates in them. In both highly absorbing and highly scattering waters, visibility into the water is reduced. The highly scattering (turbid) water still reflects a lot of light while the highly absorbing water, such as a blackwater river or lake, is very dark. The scattering particles that cause the water to be turbid can be composed of many things, including sediments and phytoplankton.\n\nThere are a number of ways to measure ocean turbidity, including autonomous remote vehicles, shipcasts and satellites.\n\nFrom a satellite, a proxy measurement of the water turbidity can be made by examining the amount of reflectance in the visible region of the electromagnetic spectrum. For the Advanced Very High Resolution Radiometer (AVHRR), the logical choice is band 1, covering wavelengths 580 to 680 nanometres, the orange and red. In order to make derived products that are comparable over time and space, an atmospheric correction is required. To do this, the effects of Rayleigh scattering are calculated based on the satellite viewing angle and the solar zenith angle and then subtracted from the band 1 radiance. For an aerosol correction, band 2 in the near infrared is used. It is first corrected for Rayleigh scattering and then subtracted from the Rayleigh corrected band 1. The Rayleigh corrected band 2 is assumed to be aerosol radiance because no return signal from water in the near infrared is expected since water is highly absorbing at those wavelengths. Because bands 1 and 2 are relatively close on the electromagnetic spectrum, we can reasonably assume their aerosol radiances are the same.\n\nIn these images the turbidity is quantified as the percent reflected light emerging from the water column in a range of 0 to 8 percent. The reflectance percentage can be correlated to attenuation, Secchi disk depth or total suspended solids although the exact relationship will vary regionally and depends on the optical properties of the water. For example, in Florida Bay, 10% reflectance corresponds to a sediment concentration of 30 milligram/litre and a Secchi depth of 0.5 metre. These relationships are approximately linear so that 5% reflectance would correspond to a sediment concentration of approximately 15 milligram/litre and a Secchi depth of 1 metre. In the Mississippi River plume regions these same reflectance values would represent sediment concentrations that are about ten times or more higher.\n\nAs one would expect, the majority of these images reveal large increases in turbidity in the regions where a hurricane has made landfall. The increases are primarily due to sediments that have been resuspended from the shallow bottom regions. In areas near shore some of the signal may also be due to sediments eroded from beaches as well as from sediment laden river plumes. In some cases a post-hurricane phytoplankton bloom due to increased nutrient availability may perhaps be detectable.\n\nThe examination of the turbidity after the passing of a hurricane can have potentially many uses for coastal resource management including:\n\n\nWith regard to these uses, determining the regions of high turbidity will allow managers to best decide on response strategies as well as help ensure that post-hurricane resources are most effectively utilized.\n\nOnly a small fraction of the light incident on the ocean will be reflected and received by the satellite. The probability for a photon to reflect and exit the ocean decreases exponentially with length of its path through the water because the ocean is an absorbing medium. The more ocean a photon must travel through, the greater its chances of being absorbed by something. After absorption, it will eventually become part of the ocean's heat reservoir. The absorption and scattering characteristics of a water body determine the rate of vertical light attenuation and set a limit to the depths contributing to a satellite signal. A reasonable rule of thumb is that 90 percent of the signal coming from the water that is seen by the satellite is from the first attenuation length. How deep this is depends on the absorption and scattering properties of both the water itself and other constituents in the water. For wavelengths in the near infrared and longer, the penetration depth varies from a metre to a few micrometres. For band 1, the penetration depth will usually be between 1 and 10 metres. If the water has a large turbidity spike below 10 metres, the spike is unlikely to be seen by a satellite.\n\nFor very shallow clear water there is a good chance the bottom may be seen. For example, in the Bahamas, the water is quite clear and only a few metres deep, resulting in an apparent high turbidity because the bottom reflects a lot of the band 1 light. For areas with consistently high turbidity signals, particularly areas with relatively clear water, part of the signal may be due to bottom reflection. Normally this will not be a problem with a post-hurricane turbidity image since the storm easily resuspends enough sediment such that bottom reflection is negligible.\n\nClouds are also problematic for the interpretation of satellite derived turbidity. Cloud removal algorithms perform a satisfactory job for pixels that are fully cloudy. Partially cloudy pixels are much harder to identify and typically result in false high turbidity estimates. High turbidity values near clouds are suspect.\n\n\n\"Note: The information in this page has been incorporated from NOAA, allowable under United States fair use laws. Original source of the information is at http://www.csc.noaa.gov/crs/cohab/hurricane/turbid.htm\"</small>\n"}
{"id": "49686904", "url": "https://en.wikipedia.org/wiki?curid=49686904", "title": "Our World In Data", "text": "Our World In Data\n\nOur World In Data (OWID) is an online publication that presents empirical research and data that show how living conditions around the world are changing. The web publication on global development uses interactive data visualisations (charts and maps) to present the research findings on development that explain the causes and consequences of the observed changes. The aim is to show how the world is changing and why.\n\nThe publication is developed at the University of Oxford and authored by social historian and development economist Max Roser. It covers a wide range of topics across many academic disciplines: Trends in health, food provision, the growth and distribution of incomes, violence, rights, wars, culture, energy use, education, and environmental changes are empirically analysed and visualised in this web publication.\n\nCovering all of these aspects in one resource makes it possible to understand how the observed long-run trends are interlinked. The research on global development is presented to the audience of interested readers, journalists, academics, and policy people. The articles cross-reference each other to make it possible for the reader to learn about the drivers of the observed long-run trends. For each topic the quality of the data is discussed and, by pointing the visitor to the sources, this website works as a database of databases – a meta-database.\n\nOur World In Data is made available as a public good: \nOur World in Data is currently financed entirely through small individual donations from readers of the publication. The website is used widely in the media. Authors like John Green and Steven Berlin Johnson use it for their work. It is used in teaching by economists, historians, and international development experts.\n\nTina Rosenberg emphasised in \"The New York Times\" that Our World In Data presents a “big picture that’s an important counterpoint to the constant barrage of negative world news”. Steven Pinker placed Roser’s Our World In Data on his list of his personal “cultural highlights” and explained in his article on 'the most interesting recent scientific news' why he considers Our World In Data so very important.\n"}
{"id": "50152014", "url": "https://en.wikipedia.org/wiki?curid=50152014", "title": "Petroleum industry in Syria", "text": "Petroleum industry in Syria\n\nThe petroleum industry in Syria forms a major part of the economy of Syria.\nAccording to the International Monetary Fund, before the Syrian Civil War, oil sales for 2010 were projected to generate $3.2 billion for the Syrian government and accounted for 25.1% of the state's revenue.\n\nSyria is a relatively small oil producer, that accounted for just 0.5% of the global production in 2010, falling to less than 0.05% by 2016. Although Syria is not a major oil exporter by Middle Eastern standards, oil is a major component of the Syrian economy. Syria's oil sector has been hit by the Civil War and international sanctions imposed on Syria.\n\nSyria’s two biggest oil companies are the Syrian Petroleum Company (SPC), which is owned by the Syrian Ministry of Petroleum and Mineral Resources, and Al-Furat Petroleum Company, which is 50% owned by SPC and the other 50% foreign owned. The joint venture currently includes SPC with 50%, Anglo-Dutch Shell with 32%, and the remainder held by Himalaya Energy Syria, a consortium of China National Petroleum Company and India's Oil and Natural Gas Corporation. As of December 2011 Shell has suspended operations in Syria due to EU sanctions. Another important consortium is Deir Ez Zor Petroleum Company, owned by SPC and France's Total. Total suspended operations in the country from December 2011. A more recent entrant is the United Kingdom's Gulfsands Petroleum. As of February 2012, Gulfsands has suspended its Syrian operations due to EU sanctions.\n\nSyria is the only significant crude oil producing country in the Eastern Mediterranean region, which includes Jordan, Lebanon, Israel and the Palestinian territories. According to the \"Oil and Gas Journal\", Syria had of petroleum reserves as of 1 January 2010. Syria's known oil reserves are mainly in the eastern part of the country in the Deir ez-Zor Governorate near its border with Iraq and along the Euphrates River, and a number of smaller fields are located in the center of the country. In 2010, Syria produced around per day of crude oil. Oil production has stabilized after falling for a number of years, and is poised to turn around as new fields come online. In 2008, Syria produced of natural gas, and two years later in 2010, it increased production to . While much of its oil is exported to Europe, Syria's natural gas is used in reinjection for enhanced oil recovery and for domestic electricity generation.\n\nAlthough Syria produces relatively modest quantities of oil and gas, its location is strategic in terms of regional security and prospective energy transit routes. The first section of the Arab Gas Pipeline to enter Syria, totalling through Syria, starts at the Jordan–Syria border in the south, connecting to Tishreen and Deir Ali power stations. This was subsequently extended to connect to Deir Ammar power station in Lebanon and Baniyas in Syria. A further section was planned to continue up to the Turkish border, but the ongoing civil war is expected to further delay this connection.\n\nSyrian oil exploration first began in 1933 during the French Mandate by the Iraq Petroleum Company (a consortium made up of Shell, BP, Exxon-Mobil, Total, and Gulbekian), which after unsuccessfully drilling eleven wells, gave up hope of finding any oil in Syria. In 1949, James W. Menhall (1881-1974), an independent oil producer from Illinois, United States, was approached by Syrian President Shukri al-Quwatli to come to Syria to prospect for oil. His James W. Menhall Drilling Company was awarded a concession on 17 May 1955 by unanimous vote of the Syrian parliament and started drilling in April 1956. In September 1956, Menhall discovered the Karatchok oil field with over 1 billion barrels of reserves. Six consecutive wells were drilled, each producing in excess of . Menhall was just commencing preparations to drill the adjoining Rumeilan oil field (with over 500 million barrels of reserves) when, on 5 October 1958, President Nasser of Egypt, as President of the United Arab Republic, expropriated Menhall's concession and confiscated his equipment and three drilling rigs without compensation. The Syrian government, under the Baath Party leadership, has since refused to consider any compensation. The Syrian oil industry took off in 1968, when the Karatchok oil field began production after a pipeline connecting it to the Homs refinery was completed, although Syria did not begin exporting oil until the mid-1980s.\n\nIn 2009, Syria's net petroleum exports were estimated to be . All oil exports are marketed by Sytrol, Syria's state oil marketing firm, which sells most of its volumes under 12-month contracts. Before the imposition of EU sanctions, Syrian crude oil exports went mostly to the European Union, in particular Germany, Italy, and France, totaling an estimated in 2009, according to Eurostat. In 2010, the European Union as a whole spent $4.1 billion on Syrian oil imports.\n\nLocal exporters of oil in Syria include the Altoun Group in Maraba, Damascus, Syria, formed in 1968. It is owned by Salim Altoun, who has been on an EU sanctions list since May 2012.\n\nThe Syrian Civil War broke out in 2011 and is ongoing. In the same year, Syria’s major trading partners imposed international sanctions on Syrian oil and financial system, devastating the Syrian oil industry, adding to the devastation of the civil war.\n\nBesides the Civil War and the imposition of international sanctions, Syria's oil sector faces a number of challenges, including a decline in output and production resulting from technological problems and a depletion of oil reserves. Syria's rate of oil production has decreased steadily from a peak of close to in 1995 down to approximately in 2010. Meanwhile, consumption is rising, which means that Syria could become a net oil importer within a decade. To counteract this problem, Syria intensified oil exploration efforts.\n\nSyria's upstream oil production and development has traditionally been the mandate of the Syrian Petroleum Company (SPC), owned by the Syrian Ministry of Petroleum and Mineral Resources. SPC has undertaken efforts to reverse the trend toward declining oil production and exports by increasing oil exploration and production in partnership with foreign oil companies. The SPC directly controls about half of the country's oil production and takes a 50% stake in development work with foreign partners.\n\nForeign investment is vital for improving production levels; however, following the outbreak of the Syrian Civil War, many countries important to Syria's oil exports have imposed sanctions, which legally prevents Western companies from working in the country.\n\nAl-Furat Petroleum Company is a joint venture established in 1985. It is currently 50% owned by SPC and the other 50% is foreign owned: Anglo-Dutch Shell with 32%, and the remainder held by Himalaya Energy Syria, a consortium of China National Petroleum Company and India's Oil and Natural Gas Corporation. The company produced an average . As of December 2011 Shell has suspended operations in Syria due to EU sanctions.\n\nDeir Ez Zor Petroleum Company is a consortium owned by SPC and France's Total. Total formed its Syrian operation in 1988 and in 2008 renewed its partnership sharing agreement with Syria. Prior to the Syrian Civil War, the company produced an average from its fields around Deir ez-Zour.\n\nA more recent entrant is the United Kingdom's Gulfsands Petroleum. At the beginning of 2011, its oil fields produced . As of February 2012, Gulfsands has suspended its Syrian operations due to EU sanctions. Gulfsands dealings in Syria were also hit by EU sanctions due to Syrian tycoon Rami Makhlouf, a cousin of the Syrian president, holding 5.7% of the company, with this stake being suspended in August 2011 due to specific sanctions relating to Makhlouf. Other international players include or included: Canada's Suncor, Poland's Kulczyk, US-Egyptian firm IPR, Croatia's INA, Russia's Stroytransgas and Soyuzneftegaz, and Triton Singapore.\n\nAnother option to handle the increase in domestic oil consumption besides increasing oil production is to switch power stations from oil-fired to natural gas-fired. Proven natural gas reserves, approximately three-quarters of which are owned by the Syrian Petroleum Company, were estimated at 2.6 trillion cubic meters (9.1 trillion cubic feet) in 2010. A major challenge for the natural gas industry is logistics: reserves are located mainly in northeastern Syria, whereas the population is concentrated in the west and south.\n\n"}
{"id": "15859865", "url": "https://en.wikipedia.org/wiki?curid=15859865", "title": "R. D. Lawrence", "text": "R. D. Lawrence\n\nRonald Douglas Lawrence (September 12, 1921 – November 27, 2003) was a Canadian naturalist and wildlife author. He was an expert on the wildlife of Canada, on which he wrote more than thirty books, which have been published in 14 languages.\n\nLawrence was born in 1921 on a British passenger ship in the Bay of Biscay off the coast of Spain. His mother was Spanish and his father, a journalist, was British. He was one of five children. As a child in northern Spain, Lawrence developed an early interest in nature. In his autobiography, he described his young self as a \"happy loner engrossed in the natural world ... I cannot recall a single day when I was bored\". \n\nAt 14 years, Lawrence lied about his age so that he could join the Republicans in the Spanish Civil War, and killed a man on his first night to save himself. He served for two years, but finding himself outnumbered in the Pyrenees, fled to France in 1938. Shortly after he traveled to Britain to rejoin his family, and during the summer of 1939 he worked on the \"Queen Mary\". With the arrival of World War II, he enlisted with the British and spent another five years at war, beginning in September 1939. He was a tank gunner in Dunkirk and North Africa, and participated in D-Day at Normandy, where he was seriously injured in August 1944. His leg was full of shrapnel, but he insisted that his leg not be amputated; instead, doctors removed 37 of 39 pieces of shrapnel, and Lawrence was determined to exercise the limb until he no longer walked with a limp. His war experiences scarred him, and are recounted in his 1994 autobiography, which he wrote at the encouragement of his publisher.\n\nAfter the war, he studied biology at Cambridge University for four years, but did not complete his degree. He found the environment stifling, with snobbish students and a pedagogy that ignored the first-hand experience of nature. Following Cambridge, he went back to Spain, where he worked as a journalist and novelist and married a British woman. They had a son in 1953 after returning to Britain.\n\nLawrence moved to Canada alone in June 1954, later reflecting that \"it was as if I had come home after a long absence\". Living in Toronto, he became a reporter for the \"Toronto Star\". Within six months he drove west to Rainy River, Ontario and purchased 100 acres of land for a homestead. His wife and son joined him in July 1955. For three years he made a living cutting timber from Crown land and selling it to mills. He and his wife had a second child, but his wife did not like their lifestyle in Canada, so she took the children to England and filed for divorce. Lawrence was left in the company of his four dogs, one part wolf, which he developed into a sled dog team. During this period Lawrence also sold fur pelts, but he came to view trapping as cruel, and stopped. \n\nIn 1957 Lawrence left the area and headed west, embarking on months-long wilderness excursions, which he supported by working for newspapers and living frugally. In southeastern British Columbia he tracked a cougar for nine months, and in Ontario he observed a beaver colony for six months. Lawrence was extremely dedicated to wildlife observation, and believed that his close but unobtrusive study provided new insight into animal behavior. He said, for example, that he witnessed starving rabbits commit suicide by running their heads into trees, and saw herbivores, like beavers, consume flesh. During this period he met Joan, his second wife, in Winnipeg, with whom he raised orphaned animals. By this time Lawrence had rescued moose calves, bear cubs, and wolves. Joan died in 1969. His decades of wilderness adventure and study were the material of his nature writing.\n\nLawrence returned to Ontario and married Sharon, his third wife. They settled on a 100-acre property—his last home, called \"Wolf Hollow\"—in the Haliburton Highlands. He and Sharon rehabilitated wildlife, and Lawrence now lectured and supervised graduate students. According to his wife, \"Ron mentored hundreds of young people ... so they in turn could continue to educate and influence younger generations\".\n\nThe naturalist acknowledged Henry David Thoreau as a great influence in his life: \"At fourteen I read \"Walden\" and was deeply impressed by one of Thoreau's sentences: 'In wildness is the preservation of the world.'\" He was concerned and angry about humankind's treatment of nature, but generally kept his opinions out of his writing; he opposed clear-cut logging and the organized hunts of bears and wolves organized to control deer populations. He loved wolves and considered them the \"ultimate stabilizers\" in their ecosystems. In his memoir (\"The Green Trees Beyond\", 1994), he wrote of his preference for nature over civilization, and explained how (per one reviewer) \"he was taught to love for the first time by a wolf\". He reared a number of abandoned wolves: in his memoir, he wrote, \"We cannot make up our minds whether we are a family of four people or a pack of four wolves ... The wolves, of course, have no doubt about the matter. They see us as a pack, Sharon fulfilling the role of much loved materfamilias, myself as the pack leader.\"\n\nLawrence finished his last book in 1997, and had four in progress at the time of his death. \"Cry Wild\" (1970), a book about wolves, is Lawrence's most popular; a 1991 reprint in the United States sold 1.5 million copies in three months. Lawrence kept a low profile, which may explain his relative lack of fame in Canada, but his writing brought him much attention: he and Sharon received six thousand visitors to their Haliburton homestead over 12 years. Lawrence died of Alzheimer's disease on November 27, 2003, in Haliburton County, Ontario.\n\nThe following is a partial list of Lawrence's published books.\n\n"}
{"id": "9479013", "url": "https://en.wikipedia.org/wiki?curid=9479013", "title": "Raet", "text": "Raet\n\nRaet is the largest terminal moraine in Scandinavia. It was formed during the end of the last glacial period, 12,800–11,500 years ago, in one of the latest advances of the glaciers.\n\nAs the glacier retreated towards the end of this period, it could stop for hundreds of years before moving forward again. Material that the glacier brought from the inland, such as particles of rocks, gravel, sand and clay, gathered where the edge of the ice laid still for some time. Such deposits, which are partly deposited on land and partly in ocean, are called \"terminal moraine\".\n\nAs the ice pulled back and the land rose, these deposits were left behind in the landscape. Later these deposits have been affected by sea and precipitation so that the roughest material remains on the surface. If you dig yourself through a terminal moraine, it is the variation in the size of the particles that are striking; it consists of unsorted material.\n\nSeveral steps of terminal moraines or ra-steps in Norway can be identified. The oldest are located in the sea and along the sea, the youngest in the mountain valleys.\n\nFrom ancient times, parts of Raet have been important as a route in Norway, especially in Vestfold and Østfold.\n\nThe word \"ra\" is of Old Norwegian origin and means a ridge of gravel. The word is used in Norwegian as a general term about terminal moraine.\n\nThis structure goes a long way over land and elsewhere under water.\n\nIn Vestfold and Østfold, this geological formation is very visible and well-known. The Ra step was deposited for about 10,600 years ago around a glacier covering most of Scandinavia and had its center above what is today Bottenviken.\n\n\"Raet\" as the name of this long ridge through Vestfold is very old; in Horten municipality lies a farm called Ra. The word is used as farm name in many places in Norway, this is farms located on terminal moraines.\n\nToday, knowledge of moraines and the ice age is taken as granted, but the knowledge about this is only about 150 years old. Geology research on moraines and the Ice Age took place in the mid-1800s, and it took a long time before an understanding of how the ra-step forms a coherent structure around the ice with Bottenviken as the center.\n\nThe Ra-step can be followed from Finland through Sweden to Northern Norway, some places in the sea, elsewhere far inland. From Sweden the ra-step can be followed into Østfold at Halden - Moss, it crosses Oslofjorden at Jeløya and reaches Horten, continues through Vestfold, follows highway 19 to Gulli, then old E18 to Larvik, through Bøkeskogen, over the Farris eidet and continues beyond Brunlanes, extends into the sea and is visible above sea level as islands along Telemark and Aust-Agder: Mølen, Jomfruland, Merdø and some other small islands.\n\nEast of Fevik in Grimstad, Raet goes on land and continues west to Rogaland and along Western Norway, Trøndelag and Northern Norway.\nIn Trøndelag, the ra-step is called Tautra because the ice edge crossed the Trondheimsfjord at Tautra.\n\nRaet dams up several lakes. In Østfold Vannsjø is among the seas that are created in this way, and in Vestfold this applies to Borrevannet, Goksjø and Farris as well as the lake Rore in Grimstad.\n\nThere is also a somewhat older maternity disposition a little beyond the big Raet, or Ra-stage, and this is often called \"Ytre Raet\". This older stage is 250 years older than the main stage.\nThe geographic distance between Raet and Ytre Raet varies, and the distance is greatest through Sandefjord, Nøtterøy and Tønsberg where Ytre Raet crosses the northern part of Nøtterøy and continues in Slagen, Ringshaug and Skallevold. In Brunlanes and at Fredrikstad and Halden the distance is shorter between the two ra-steps.\n\nYtre Raet contributes to large, good agricultural areas in the municipalities of Sandefjord, Nøtterøy and Tønsberg.\n\nRaet is used as place name for large parts of the ra through Østfold and Vestfold. \n\nRaet was in old age a convenient route, because there were so few watercourses to cross. When Bishop Jens Nilsson went on visits to Østfold and Bohuslän in the late 16th century, he traveled along Raet because it was easily accessible. Still E6 in Østfold and E18 in Vestfold follow Raet for long distances.\n\nRaet also attracted settlements, so that the oldest, largest and best farms are still there.\nSimilarly tumuli in Iron Age often was located along Raet.\n\nThe top of the moraine has been used as road from prehistoric times, and in some places \"Raveien \" is the name of the road.\n\nE18 long followed this old trace in Vestfold from Sem to Farriseidet.\nNow the road in the main traces follows a trace inside the raid.\n\nIn Østfold, E6 follows Raet from Halden to Moss.\n\nThe place Helgeroa in Vestfold, near the Mølen, where Raet descended on the seabed, became a central place for a period of communication precisely because the location lay there the natural road in the direction of the east through Vestfold.\n\nSeveral areas of raet are protected as national parks, landscaping areas and nature reserves.\n\nTwo national parks are on the Raet: Jomfruland National Park in Telemark and Raet National Park in Aust-Agder. In Vestfold Mølen and Bøkeskogen in Larvik and Bøkemoa in the Sandefjord protected.\n\nGea Norvegica Geopark has as one of its tasks to convey knowledge about the importance of geology, and has information boards, among other things, on the Mølen in Brunlanes in Larvik municipality.\n\nIn Eastern Norway, the different stages of ice melting are clearly marked in the landscape with endemorins in several stages.\n\nThe provisions in Eastern Norway are best investigated, and the moral steps in Eastern Norway have therefore become a reference framework in the national context.\n\nMostly, it has been so that the ice has moved forward at the same time in a larger area.\n\nThe oldest ra-step to be seen in Oslofjord is called Tjøme-Hvaler-step and is approximately 11,200 years old.\n\nThe next step is called \" Ytre Raet \" and is approximately 10,850 years old, while the main stage, \"Ra-Step\", is approx. 10,600 years old.\n\nYounger is the Ås-Ski-step, which forms the threshold for Inner Oslofjord. This is about 10,200–10,400 years old. Large sand areas are deposited as the ice lay at Drøbak-Svelvik.\n\nThe next stop for the ice edge is called the Aker-step in the Groruddalen where the deposits demolishes Maridalsvannet, Bogstadvannet and Sognsvann. In Sylling, the acronym dams up Holsfjorden. The Aker step is approx. 9,800 years old.\n\nOn Romerike there are several steps of which Gardermoen or Hauerseter is the most marked with large sandboxes. This is about 9,600 years old.\n\nThen the Minnesund-step is the last before the ice melted. This is about 9,500 years old.\n\n"}
{"id": "34975333", "url": "https://en.wikipedia.org/wiki?curid=34975333", "title": "Rolling cone motion", "text": "Rolling cone motion\n\nRolling cone motion is the rolling motion generated by a cone rolling over another cone. In rolling cone motion, at least one of the cones is convex, while the other cone may be either convex, or concave, or a flat surface (a flat surface can be regarded as a special case of a cone whose apex angle equals formula_1). The distinguishing characteristic of a rolling cone, in relation to other axially symmetrical rollers (cylinder, sphere, round disk), is that while rolling on a flat surface, the cone's center of gravity performs a circular motion rather than a linear one. Another unique characteristic is that one of its points (its apex) is at rest throughout the entire motion.\n\nThe motion of a rolling cone can be described as a combination of a rotational motion of the cone around its axis of symmetry, and a rotary motion of its axis around the axis of symmetry of the stationary cone. The ratio between the angular velocities of these two motions is given by:\nwhere formula_3 and formula_4 are the half apex angles of the stationary cone and the rolling cone, respectively, formula_5 is the angular velocity of the rolling cone's axis of symmetry around the axis of symmetry of the stationary cone, and formula_6 is the angular velocity of the rolling cone around its own axis of symmetry. In the special case of a cone rolling on a flat surface, this ratio becomes formula_7, where formula_3 is the cone's half apex angle. For example, a cone having an apex angle of 120 degrees, while being rolled on a flat surface, will perform exactly two full rotations around its axis of symmetry before returning to its original position.\n\nOne of the most practical applications of rolling cones is the use of tapered roller bearings in rotating devices. Tapered bearings can bear higher loads than ball bearings in both radial and axial directions, and therefore are more frequently used as wheel bearings in most wheeled land vehicles.\n\nIn Conveyor systems, conical rollers are sometimes used when there's a need to create a curved path. A common example is belt conveyors in airport terminals where there's a need to move the luggage in loops.\n\nIn the 18th and 19th century rolling cone motion was used in the process of olive oil extraction. The olives were put in a large circular basin and heavy metal cones were rolled upon them. The fact that a cone can roll in circles without sliding made it more efficient to use conical roller millstones.\n\n\n"}
{"id": "34627542", "url": "https://en.wikipedia.org/wiki?curid=34627542", "title": "Sea sorrel", "text": "Sea sorrel\n\nSea sorrel or in French can refer to different seaweeds:\n\n\n"}
{"id": "19231038", "url": "https://en.wikipedia.org/wiki?curid=19231038", "title": "Soil guideline value", "text": "Soil guideline value\n\nSoil Guideline Values (SGVs) are figures which are used in non-statutory technical guidance for assessors carrying out risk assessments to determine whether land is considered ‘contaminated’ under United Kingdom law, that is “land which appears to… be in such a condition, by reason of substances in, on or under the land, that (a) significant harm is being caused or there is a significant possibility of such harm being caused…” \n\nThis guidance stipulates three stages in such risk assessments:\n\n\nSoil Guideline Values are used in the second stage, GQRA, to determine whether harm caused by long-term exposure to a given soil concentration of chemicals \"may\" present an unacceptable risk to \"human\" health in some \"generic\" land-use scenario. The SGVs are therefore conservative estimates for a given scenario. Exceedance of a SGV does not confirm that there is a ‘significant possibility of significant harm’, merely that the possibility exists and therefore more detailed, site-specific investigation of contaminants present, pathways and receptors is required.\n\nSGVs are derived by the Environment Agency using the ‘CLEA’ model (which can also be customised or used for DQRA). However, there are no currently valid SGVs for many important and common elements, such as copper, zinc, lead or chromium. The only body mandated to produce these values is the Environment Agency, following extensive consultation with other government departments.\n\nIn lieu of such figures, equivalent values known as “Generic Assessment Criteria” (GAC) may be calculated by any individual or organisation, starting from toxicity and relevant data and using the CLEA model, just as the Environment Agency calculates SGVs. SGVs are in fact a GAC derived and published by the environment agency. For example, a range of GACs for more complex chemicals has been calculated and been made publicly available by charity CL:AIRE, with input from a range of authoritative sources including the Environment Agency. That said, GACs are not always openly published, as deriving them is a complex matter and thus these values do hold commercial value for consultancies who have undertaken such calculations.\n\nThere are three generic land-use scenarios for which SGVs are published, and most derivations of GACs also follow these scenarios:\n\n\nSGVs and GACs derived from these scenarios may only be used if the scenarios apply exactly or otherwise it is clearly demonstrated that the scenarios are more conservative than the real-world situation. For example, in the case of a playing field, it could be argued that the residential scenario is suitably conservative:\n\n\n\n"}
{"id": "2255170", "url": "https://en.wikipedia.org/wiki?curid=2255170", "title": "Steve Madge", "text": "Steve Madge\n\nSteve Madge is a birder, author, and bird tour leader, based in Cornwall, England.\n\nHe is a former member of the British Birds Rarities Committee. He is president of the Cornwall Birdwatching and Preservation Society.\n\nHe has written three volumes in the Helm Identification Guides series - on \"Wildfowl\", \"Crows and Jays\" and \"Pheasants, Partridges & Grouse\", and is co-author with Mark Beaman of The Handbook of Bird Identification.\n"}
{"id": "24728109", "url": "https://en.wikipedia.org/wiki?curid=24728109", "title": "Superflare", "text": "Superflare\n\nSuperflares are very strong explosions observed on stars with energies up to ten thousand times that of typical solar flares. The stars in this class satisfy conditions which should make them solar analogues, and would be expected to be stable over very long time scales.\nThe original nine candidates were detected by a variety of methods. No systematic study was possible until the launch of the Kepler satellite, which monitored a very large number of solar-type stars with very high accuracy for an extended period. This showed that a small proportion of stars had violent outbursts, up to 10,000 times as powerful as the strongest flares known on the Sun. In many cases there were multiple events on the same star. Younger stars were more likely to flare than old ones, but strong events were seen on stars as old as the Sun.\n\nThe flares were initially explained by postulating giant planets in very close orbits, such that the magnetic fields of the star and planet were linked. The orbit of the planet would warp the field lines until the instability released magnetic field energy as a flare. However, no such planet has showed up as a Kepler transit and this theory has been abandoned.\n\nAll superflare stars show quasi-periodic brightness variations interpreted as very large starspots carried round by rotation. Spectroscopic studies found spectral lines that were clear indicators of chromospheric activity associated with strong and extensive magnetic fields. This suggests that superflares only differ in scale from solar flares.\n\nAttempts have been made to detect past solar superflares from nitrate concentrations in polar ice, from historical observations of auroras, and from those radioactive isotopes that can be produced by solar energetic particles. Although two promising events have been found in the carbon-14 records in tree rings, it is not possible to associate them definitely with a superflare event.\n\nSolar superflares would have drastic effects, especially if they occurred as multiple events. Since they can occur on stars of the same age, mass and composition as the Sun this cannot be ruled out. However, solar-type superflare stars are very rare and are magnetically much more active than the Sun; if solar superflares do occur, it may be in well-defined episodes that occupy a small fraction of its time.\n\nA superflare star is not the same as a flare star, which usually refers to a very late spectral type red dwarf. The term is restricted to large transient events on stars that satisfy the following conditions:\nEssentially such stars may be regarded as solar analogues.\nOriginally nine superflare stars were found, some of them similar to the Sun.\n\nThe original paper identified nine candidate objects from a literature search:\nType gives the spectral classification including spectral type and luminosity class.\n\nV (mag) means the normal apparent visual magnitude of the star.\n\nEW(He) is the equivalent width of the 5875.6Å He I D3 line seen in emission.\n\nThe observations vary for each object. Some are X-ray measurements, others are visual, photographic, spectroscopic or photometric. The energies for the events vary from 2 × 10 to 2 × 10 ergs.\n\nThe Kepler spacecraft is a space observatory designed to find planets by the method of transits. A photometer continually monitors the brightness of 150,000 stars in a fixed area of the sky (in the constellations of Cygnus, Lyra and Draco) to detect changes in brightness caused by planets passing in front of the stellar disc. More than 90,000 are G-type stars (similar to the Sun) on or near the main sequence. The observed area corresponds to about 0.25% of the entire sky. The photometer is sensitive to wavelengths of 400–865 nm: the entire visible spectrum and part of the infrared. The photometric accuracy achieved by Kepler is typically 0.01% (0.1 mmag) for 30 minute integration times of 12th magnitude stars.\n\nThe high accuracy, the large number of stars observed and the long period of observation make Kepler ideal for detecting superflares. Studies published in 2012 and 2013 involved 83,000 stars over a period of 500 days (much of the data analysis was carried out with the help of five first-year undergraduates). The stars were selected from the Kepler Input Catalog to have T, the effective temperature, between 5100 and 6000K (the solar value is 5750K) to find stars of similar spectral class to the Sun, and the surface gravity log g > 4.0 to eliminate sub-giants and giants. The spectral classes range from F8 to G8. The integration time was 30 min in the original study. 1547 superflares were found on 279 solar-type stars.The most intense events increased the brightness of the stars by 30% and had an energy of 10 ergs. White-light flares on the Sun change the brightness by about 0.01%, and the strongest flares have a visible-light energy of about 10 ergs. (All energies quoted are in the optical bandpass and so are lower limits since some energy is emitted at other wavelengths.) Most events were much less energetic than this: flare amplitudes below 0.1% of the stellar value and energies of 2 × 10 ergs were detectable with the 30 minute integration. The flares had a rapid rise followed by an exponential decay on a time scale of 1–3 hours. The most powerful events corresponded to energies ten thousand greater than the largest flares observed on the Sun. Some stars flared very frequently: one star showed 57 events in 500 days, a rate of one every nine days. For the statistics of flares, the number of flares decreased with energy E roughly as E, a similar behaviour to solar flares. The duration of the flare increased with its energy, again in accordance with the solar behaviour.\n\nSome Kepler data is taken at one minute sampling, though inevitably with lower accuracy. Using this data, on a smaller sample of stars, reveals flares that are too brief for reliable detection with 30-min integrations, allowing detection of events as low as 10 ergs, comparable with the brightest flares on the Sun. The occurrence frequency as a function of energy remains a power law E when extended to lower energies, with n around 1.5. At this time resolution some superflares show multiple peaks with separations of 100 to 1000 seconds, again comparable to the pulsations in solar flares. The star KIC 9655129 showed two periods, of 78 and 32 minutes, suggesting magnetohydrodynamic oscillations in the flaring region. These observations suggest that superflares are different only in scale and not in type to solar flares.\n\nSuperflare stars show a quasi-periodic brightness variation, which is interpreted as evidence of starspots carried around by solar rotation. This allows an estimate of the rotation period of the star; values range from less than one day up to tens of days (the value for the Sun is 25 days). On the Sun, radiometer monitoring from satellites shows that large sunspots can reduce the brightness by up to 0.2%. In superflare stars the most common brightness variations are 1-2%, though they can be as great as 7-8%, suggesting that the area of the starspots can be very much larger than anything found on the Sun. In some cases the brightness variations can be modelled by only one or two large starspots, though not all cases are so simple. The starspots could be groups of smaller spots or single giant spots.\n\nFlares are more common in stars with short periods. However, the energy of the largest flares is not related to the period of rotation. Stars with larger variations also have much more frequent flares; there is as well a tendency for them to have more energetic flares. Large variations can be found on even the most slowly rotating stars: one star had a rotation period of 22.7 days and variations implying spot coverage of 2.5% of the surface, over ten times greater than the maximum solar value. By estimating the size of the starspots from the amplitude variation, and assuming solar values for the magnetic fields in the spots (1000 G), it is possible to estimate the energy available: in all cases there is enough energy in the field to power even the largest flares observed. This suggests that superflares and solar flares have essentially the same mechanism.\n\nIn order to determine whether superflares can occur on the Sun, it is important to narrow the definition of Sun-like stars. When the temperature range is divided into stars with T above and below 5600K (early and late G-type stars), stars of lower temperature are about twice as likely to show superflare activity as those in the solar range and those that do so have more flares: the occurrence frequency of flares (number per star per year) is about five times as great in the late-type stars. It is well known that both the rotation rate and the magnetic activity of a star decrease with age in G-type stars. When flare stars are divided into fast and slow rotators, using the rotation period estimated from brightness variations, there is a general tendency for the fastest-rotating (and presumably youngest) stars to show a greater probability of activity: in particular, stars rotating in less than 10 days are 20-30 times more likely to have activity. Nevertheless, 44 superflares were found on 19 stars with similar temperatures to the Sun and periods greater than 10 days (out of 14000 such stars examined); four superflares with energies in the range 1-5 × 10 ergs were detected on stars rotating more slowly than the Sun (of about 5000 in the sample). The distribution of flares with energy has the same shape for all classes of star: although Sun-like stars are less likely to flare, they have the same proportion of very energetic flares as younger and cooler stars.\n\nKepler data have also been used to search for flares on stars of later spectral types than G. A sample of 23,253 stars with effective temperature T less than 5150K and surface gravity log g > 4.2, corresponding to main sequence stars later than K0V, was examined for flares over a time period of 33.5 days. 373 stars were identified as having obvious flares. Some stars had only one flare, while others showed as many as fifteen. The strongest events increased the brightness of the star by 7-8%. This is not radically different from the peak brightness of flares on G-type stars; however, since K and M stars are less luminous than type G, this suggests that flares on these stars are less energetic. Comparing the two classes of stars studied, it seems that M stars flare more frequently than K stars but the duration of each flare tends to be shorter. It is not possible to draw any conclusions about the relative proportion of G and K type stars showing superflares, or about the frequency of flares on those stars that do show such activity, since the flare detection algorithms and criteria in the two studies are quite different.\n\nMost (though not all) of the K and M stars show the same quasi-periodic brightness variations as the G stars. There is a tendency for more energetic flares to occur on more variable stars; however flare frequency is only weakly related to variability.\n\nWhen superflares were originally discovered on solar-type stars it was suggested that these eruptions may be produced by the interaction of the star's magnetic field with the magnetic field of a gas-giant planet orbiting so close to the primary that the magnetic fields were linked. Rotation or orbital motion would wind up the magnetic fields until a reconfiguration of the fields would cause an explosive release of energy. The RS Canum Venaticorum variables are close binaries, with orbital periods between 1 and 14 days, in which the primary is an F- or G-type main sequence star, and with strong chromospheric activity at all orbital phases. These systems have brightness variations attributed to large starspots on the primary; some show large flares thought to be caused by magnetic reconnection. The companion is close enough to spin up the star by tidal interactions.\n\nA gas giant however would not be massive enough to do this, leaving the various measurable properties of the star (rotation speed, chromospheric activity) unchanged. If the giant and the primary were close enough for the magnetic fields to be linked, the orbit of the planet would wrap the field lines until the configuration became unstable followed by a violent release of energy in the form of a flare. Kepler discovered a number of closely orbiting gas giants, known as hot Jupiters; studies of two such systems showed periodic variations of the chromospheric activity of the primary synchronised to the period of the companion.\n\nNot all planetary transits can be detected by Kepler, since the planetary orbit may be out of the line of sight to Earth. However, the hot Jupiters orbit so close to the primary that the chance of a transit is about 10%. If superflares were caused by close planets the 279 flare stars discovered should have about 28 transiting companions; none of them actually showed evidence of transits, effectively excluding this explanation.\n\nSpectroscopic studies of superflares allow their properties to be determined in more detail, in the hope of detecting the cause of the flares. The first studies were made using the high dispersion spectrograph on the Subaru telescope in Hawaii. Some 50 apparently solar-type stars, known from the Kepler observations to show superflare activity, have been examined in detail. Of these, only 16 showed evidence of being visual or spectroscopic binaries; these were excluded since close binaries are frequently active, while in the case of visual binaries there is the chance of activity taking place on the companion. Spectroscopy allows accurate determinations of the effective temperature, the surface gravity and the abundance of elements beyond helium ('metallicity'); most of the 34 single stars proved to be main sequence stars of spectral type G and similar composition to the Sun. Since properties such as temperature and surface gravity change over the lifetime of a star, stellar evolution theory allows an estimate of the age of a star: in most cases the age appeared to be above several hundred million years. This is important since very young stars are known to be much more active. Nine of the stars conformed to the narrower definition of solar-type given above, with temperatures greater than 5600K and rotation periods longer than 10 days; some had periods above 20 or even 30 days. Only five of the 34 could be described as fast rotators.\n\nObservations from LAMOST have been used to measure chromospheric activity of 5,648 solar-like stars in the Kepler field, including 48 superflare stars. These observations show that superflare stars are generally characterized by larger chromospheric emissions than other stars, including the Sun. However, superflare stars with activity levels lower than, or comparable to, the Sun do exist, suggesting that solar flares and superflares most likely share the same origin. The very large ensemble of solar-like stars included in this study enables detailed and robust estimates of the relation between chromospheric activity and the occurrence of superflares.\n\nAll the stars showed the quasi-periodic brightness variations, ranging from 0.1% to nearly 10%, interpreted as the rotation of large starspots. When large spots exist on a star, the activity level of the chromosphere becomes high; in particular, large chromospheric plages form around sunspot groups. The intensities of certain solar and stellar lines generated in the chromosphere, particularly the lines of ionised calcium (Ca II) and the Hα line of hydrogen, are known to be indicators of magnetic activity. Observations of the Ca lines in stars of similar age to the Sun even show cyclic variations reminiscent of the 11 year solar cycle. By observing certain infrared lines of Ca II for the 34 superflare stars it was possible to estimate their chromospheric activity. Measurements of the same lines at points within an active region on the Sun, together with simultaneous measurements of the local magnetic field, show that there is a general relation between field and activity.\n\nAlthough the stars show a clear correlation between rotational speed and activity, this does not exclude activity on slowly rotating stars: even stars as slow as the Sun can have high activity. All the superflare stars observed had more activity than the Sun, implying larger magnetic fields. There is also a correlation between the activity of a star and its brightness variations (and therefore the starspot coverage): all stars with large amplitude variations showed high activity.\n\nKnowing the approximate area covered by starspots from the size of the variations, and the field strength estimated from the chromospheric activity, allows an estimate of the total energy stored in the magnetic field; in all cases there was enough energy stored in the field to account for even the largest superflares. Both the photometric and the spectroscopic observations are consistent with the theory that superflares are different only in scale from solar flares, and can be accounted for by the release of magnetic energy in active regions very much larger than those on the Sun. Nevertheless, these regions can appear on stars with masses, temperatures, compositions, rotation speeds and ages similar to the Sun.\n\nSee also Chromosphere#On other stars\n\nSince stars apparently identical to the Sun can produce superflares it is natural to ask if the Sun itself can do so, and to try to find evidence that it has done in the past. Large flares are invariably accompanied by energetic particles, and these particles produce effects if they reach the earth. The Carrington event of 1859, the largest flare of which we have direct observation, produced global auroral displays extending close to the equator. Energetic particles can produce chemical changes in the atmosphere, which can be permanently recorded in the polar ice. Fast protons generate distinctive isotopes, particularly carbon-14, which can be taken up and preserved by living creatures.\n\nWhen solar energetic particles reach the Earth's atmosphere they cause ionisation that creates nitric oxide (NO) and other reactive nitrogen species, which then precipitate out in the form of nitrates. Since all energetic particles are deflected to a greater or lesser extent by the geomagnetic field, they enter preferentially at the polar latitudes; since high latitudes also contain permanent ice, it is natural to look for the nitrate signature of particle events in ice cores. A study of a Greenland ice core extending back to 1561 AD achieved resolutions of 10 or 20 samples a year, allowing in principle the detection of single events. Precise dates (within one or two years) can be achieved by counting annual layers in the cores, checked by identification of deposits associated with known volcanic eruptions. The core contained an annual variation of nitrate concentration, accompanied by a number of 'spikes' of different amplitudes. The strongest of these in the entire record was dated to within a few weeks of the Carrington event of 1859. However, other events can produce nitrate spikes, including biomass burning which also produces enhanced ammonium concentrations. An examination of fourteen ice cores from Antarctic and Arctic regions showed large nitrate spikes: however, none of them were dated to 1859 (the closest was 1863). All such spikes were associated with ammonium and other chemical indicators of combustion. There is no evidence that nitrate concentrations can be used as indicators of historic solar activity.\n\nWhen energetic protons enter the atmosphere they create isotopes by reactions with the major components; the most important of these is carbon-14 (C), which is created when secondary neutrons react with nitrogen. C, which has a half-life of 5,730 years, reacts with oxygen to form carbon dioxide which is taken up by plants; dating wood by its C content is the basis of radiocarbon dating. If wood of known age is available the process can be reversed. Measuring the C content and using the half-life allows estimation of the content when the wood was formed. The growth rings of trees show patterns, caused by various environmental factors: dendrochronology uses these growth rings of trees, compared across overlapping sequences, to establish accurate dates. Applying this method shows that atmospheric C does indeed vary with time, due to solar activity. This is the basis of the carbon dating calibration curve. Clearly, it can also be used to detect any peaks in production caused by solar flares, if those flares create enough energetic particles to produce a measurable increase in C.\n\nAn examination of the calibration curve, which has a time resolution of five years, showed three intervals in the last 3,000 years in which C increased significantly. On the basis of this two Japanese cedar trees were examined with a resolution of a single year, and showed an increase of 1.2% in AD 774, some twenty times larger than anything expected from the normal solar variation. This peak steadily diminished over the next few years. The result was confirmed by studies of German oak, bristlecone pine from California, Siberian larch, and Kauri wood from New Zealand. All determinations agreed on both the time and amplitude of the effect. In addition, measurements of coral skeletons from the South China Sea showed substantial variations in C over a few months around the same time; however, the date could only be established to within a period of ±14 years around 783 AD.\n\nCarbon-14 is not the only isotope that can be produced by energetic particles. Beryllium-10 (Be) is also formed from nitrogen and oxygen, and deposited in polar ice. However, Be deposition can be strongly related to local weather and shows extreme geographic variability; it is also more difficult to assign dates. Nevertheless, a Be increase during the 770s was found in an ice core from the Antarctic, though the signal was less striking because of the lower time resolution (several years); another smaller increase was seen in Greenland. When data from two sites in North Greenland and one in the West Antarctic, all taken with a one-year resolution, were compared they all showed a strong signal: the time profile also matched well with the C results (within the uncertainty of dating for the Be data). Chlorine-36 (Cl) can be produced from argon and deposited in polar ice; because argon is a minor atmospheric constituent the abundance is low. The same ice cores which showed Be also provided increases of Cl, though with a resolution of five years a detailed match was impossible.\n\nA second event in AD 993/4 has also been found from C in tree rings, but at a lower intensity. This event also produced measurable increases in Be and Cl in Greenland ice cores.\n\nIf these events are presumed to be produced by fast particles from large flares, it is not easy to estimate the particle energy in the flare or compare it with known events. The Carrington event does not appear in the C record, and neither did any other large particle event that has been directly observed. The flux of particles must be estimated by calculating production rates of radiocarbon, and then modelling the behaviour of the CO once it has entered the carbon cycle; the fraction of the created radiocarbon taken up by trees depends to some extent on that cycle. As an extra complication, the cosmogenic isotopes are preferentially created by energetic protons (several hundred MeV). The energetic particle spectrum of a solar flare varies considerably between events; one with a 'hard' spectrum, with more high-energy protons, will be more efficient at producing a C increase. The most powerful flare which also had a hard spectrum that has been observed instrumentally took place in February 1956 (the beginning of nuclear testing obscures any possible effects in the C record); it has been estimated that if a single flare were responsible for the AD 774/5 event it would need to be 25-50 times more powerful than this. A sunspot group may produce several flares over its lifetime, and the effects of such a sequence would be aggregated over the one-year period covered by a single C measurement; however, the total effect would still be ten times greater than anything observed in a similar period in modern times.\n\nSolar flares are not the only possibility for producing the cosmogenic isotopes. A long or short gamma-ray burst has been proposed as being consistent with all the details of the AD 774/5 event if it was sufficiently close. However, as known presently, this explanation is very unlikely.\n\nA number of attempts have been made to find additional evidence supporting the superflare interpretation of the isotope peak around AD 774/5 by studying historical records. The Carrington event produced auroral displays as far south as Caribbean and Hawaii, corresponding to geomagnetic latitude of about 22°; if the event of 774/5 corresponded to an even more energetic flare there should have been a global auroral event.\n\nUsoskin et al. cited references to aurorae in Chinese chronicles for AD 770 (twice), 773 and 775. They also quote a “red cross” in the sky in AD 773/4 from the Anglo-Saxon Chronicle; “inflamed shields” or “shields burning with a red colour” seen in the sky over Germany in AD 776 recorded in the Royal Frankish Annals; “fire in heaven” seen in Ireland in AD 772; and an apparition in Germany in AD 773 interpreted as riders on white horses. Even if the dates do not precisely conform to the C increase this might suggest a period of high solar activity. Zhou et al. add further details from the Chinese chronicles. On a date which they give as 17 January AD 775, there were more than ten bands of white lights “like the spread silk” stretching across eight Chinese constellations; the display lasted for several hours. The observations, made during the Tang dynasty, were made from the capital Xian; although geomagnetic latitudes change over time, this would correspond to the lower twenties.\n\nThere are a number of difficulties involved when trying to link the C results to historical chronicles. Tree ring dates may be in error because there is no discernible ring for a year (unusually cold weather), or two rings (a second growth during a warm autumn). If the cold weather were global, following a large volcanic eruption, it is conceivable that the effects could also be global: the apparent C date may not always match the chronicles.\n\nFor the isotope peak in AD 993/994 studied by Hayakawa et al. surveyed contemporary historical documents show clustering auroral observations in late 992, while their relationship with the isotope peak is still under discussion.\n\nSuperflares seem to be associated with a general high level of magnetic activity. As well as looking for individual events, it is possible to examine the isotope records to find the activity level in the past and identify periods when it may have been much higher than now. Lunar rocks provide a record unaffected by geomagnetic shielding and transport processes. Both cosmic rays and solar particle events can create isotopes in rocks, and both are affected by solar activity. The cosmic rays are much more energetic and penetrate more deeply, and can be distinguished from the solar particles which affect the outer layers. Several different radioisotopes can be produced with very different half-lives; the concentration of each may be regarded as representing an average of particle flux over its half-life. Since fluxes must be converted into isotope concentrations by simulations there is a certain model-dependence here. The data are consistent with the view that the flux of energetic solar particles with energies above a few tens of MeV has not changed over periods ranging from five thousand to five million years. Of course, a period of intense activity over a time scale short with respect to the half-life would not be detected.\n\nC measurements, even with low time resolution, can indicate the state of solar activity over the last 11,000 years until about 1900. Although radiocarbon dating has been applied as far back as 50,000 years, during the deglaciations at the start of the Holocene the biosphere and its carbon uptake changed dramatically making estimation before this impractical; after about 1900 the Suess effect makes interpretation difficult. Be concentrations in stratified polar ice cores provide an independent measure of activity. Both measures agree reasonably with each other and with the Zurich sunspot number of the last two centuries. As an additional check, it is possible to recover the isotope Titanium-44 (Ti) from meteorites; this provides a measurement of activity that is not affected by changes in transport process or the geomagnetic field. Although it is limited to about the last two centuries, it is consistent with all but one of the C and Be reconstructions and confirms their validity. The energetic flare events discussed above are rare; on long time scales (significantly more than a year), the radiogenic particle flux is dominated by cosmic rays. The inner solar system is shielded by the general magnetic field of the sun, which is strongly dependent on the time within a cycle and the strength of the cycle. The result is that times of powerful activity show up as \"decreases\" in the concentrations of all these isotopes. Because cosmic rays are also influenced by the geomagnetic field, difficulties in reconstructing this field set a limit to the accuracy of the reconstructions.\n\nThe C reconstruction of activity over the last 11,000 years shows no period significantly higher than the present; in fact, the general level of activity in the second half of the 20th century was the highest since 9000 BC. In particular, the activity in the period around the AD 774 C event (averaged over decades) was somewhat lower than the long-term average, while the AD 993 event coincided with a small minimum. A more detailed scrutiny of the period AD 731 to 825, combining several C datasets of one- and two-year resolution with auroral and sunspot accounts does show a general increase in solar activity (from a low level) after about AD 733, reaching its highest level after 757 and remaining high in the 760s and 770s; there were several aurorae around this time, and even a low-latitude aurora in China.\n\nThe effect of the sort of superflare apparently found on the original nine candidate stars would be catastrophic for the Earth and would leave traces on the Solar System; the event on S Fornacis for example involved an increase in the stars' luminosity by a factor of about twenty. Thomas Gold suggested that the glaze on the top surface of certain lunar rocks might be caused by a solar outburst involving a luminosity increase of over a hundred times for 10 to 100 seconds at some time in the last 30,000 years. Apart from the terrestrial effects, this would cause local ice melting followed by refreezing as far out as the moons of Jupiter. There is no evidence of superflares on this scale having occurred in the Solar System.. \n\nEven for much smaller superflares, at the lower end of the Kepler range, the effects would be serious. In 1859 the Carrington event caused failures in the telegraph system in Europe and North America. Possible consequences today would include:\nIt is evident that superflares often repeat rather than occurring as isolated events. The NO and other odd nitrogens created by flare particles catalyse the destruction of ozone without being consumed themselves, and have a long lifetime in the stratosphere. Flares at a frequency of one a year or even less would have a cumulative effect; the destruction of the ozone layer could be permanent and lead to at least a low-level extinction event.\n\nSuperflares have also been suggested as a solution to the Faint young Sun paradox.\n\nSince superflares can occur on stars apparently equivalent in every way to the Sun, it is natural to ask if they can occur on the Sun itself. An estimate based on the original Kepler photometric studies suggested a frequency on solar-type stars (early G-type and rotation period more than 10 days) of once every 800 years for an energy of 10 erg and every 5000 years at 10 erg. One-minute sampling provided statistics for less energetic flares and gave a frequency of one flare of energy 10 erg every 5–600 years for a star rotating as slowly as the Sun; this would be rated as X100 on the solar flare scale. This is based on a straightforward comparison of the number of stars studied with the number of flares observed. An extrapolation of the empirical statistics for solar flares to an energy of 10 erg suggests a frequency of one in 10,000 years.\n\nHowever, this does not match the known properties of superflare stars. Such stars are extremely rare in the Kepler data; one study showed only 279 such stars in 31,457 studied, a proportion below 1%; for older stars this fell to 0.25%. Also, about half of the stars which were active showed repeating flares: one had as many as 57 events in 500 days. Concentrating on solar-type stars, the most active averaged one flare every 100 days; the frequency of superﬂare occurrence in the most active Sun-like stars is 1000 times larger than that of the general average for such stars. This suggests that such behaviour is not present throughout a star's lifetime, but is confined to episodes of extraordinary activity. This is also suggested by the clear relation between the magnetic activity of a star and its superflare activity; in particular, superflare stars are much more active (based on starspot area) than the Sun.\n\nThere is no evidence for any flare greater than the Carrington event (about 10 erg, or 1/10,000 of the largest superflares) in the last 200 years. Although larger events from the C record ca. 775 AD is unambiguously identified as a solar event, its association to the flare energy is unclear, and it is unlikely to exceed 10 erg. \n\nThe more energetic superflares seem to be ruled out by energetic considerations for our sun, which suggest it is not capable of a flare of more than 10 ergs. A calculation of the free energy in magnetic fields in active regions that could be released as flares gives a lower upper bound of around 3×10 erg suggesting the most energetic a super flare can be is three times that of the Carrington event.\n\nSome stars have a magnetic field 5 times that of Earth and rotate much faster and these could theoretically have a flare of up to 10 ergs. This could explain some superflares at the lower end of the range. To go higher than this may require an anti-solar rotation curve - one in which the polar regions rotate faster than the equatorial regions.\n\n"}
{"id": "2954500", "url": "https://en.wikipedia.org/wiki?curid=2954500", "title": "Supreme War Council", "text": "Supreme War Council\n\nThe Supreme War Council was a central command that coordinated Allied military strategy during World War I. It was founded in 1917, and was based in Versailles. The council served as a forum for preliminary discussions of potential armistice terms and peace treaty settlement conditions.\n\nBritish Prime Minister David Lloyd George had grave concerns regarding the strategy of Sir William Robertson, Chief of the Imperial General Staff, and Sir Douglas Haig, the Commander in Chief of the British Expeditionary Force, in response to the Allied losses at the Somme and at Battle of Passchendaele. \n\nFollowing the Italian defeat at the Battle of Caporetto, in which the Germans and Austro-Hungarians surprised the Italian forces, Lloyd George proposed the formation of the Supreme War Council at the Rapallo Conference of 5–7 November 1917.\n\nJapan and Russia were not to be included, and the Italians and French, worried that Salonika might be evacuated, wanted it confined to the Western Front.\n\nEach Allied nation would also appoint a senior military officer as Permanent Military Representative. The French PMR was Ferdinand Foch, later replaced by Maxime Weygand and Joseph Joffre. The British were represented by Sir Henry Hughes Wilson. Italy was represented by Luigi Cadorna.\n\nThe USA, which was \"Associated\" with the Allies, was not involved with the political structures, but sent a Permanent Military Representative, Tasker H. Bliss.\n\nWilson and his staff conducted numerous research projects into offensives against Turkey, culminating in the \"Joint Note 12\".\n\nAt the Supreme War Council (30 January- 1 February) Hankey recorded that the national Commanders-in-Chief, national Chiefs of Staff and PMRs “all gave different advice” creating “a worse state of chaos than I have ever known in all my wide experience”. Lloyd George, whose main goal was to thwart Robertson, blocked a suggestion by Foch (French Chief of Staff) that the proposed Allied Reserve be controlled by the national Chiefs of Staff, possibly prompted by the notes Wilson was passing him across the table. It was eventually agreed that Foch should command the Reserve, with Wilson as his deputy.\n\nThe Allied Reserve eventually slipped from the agenda as the Commanders-in-Chief, Haig and Pétain refused to hand over sufficient troops. Early in 1918 Wilson replaced Robertson as CIGS and at the end of March Foch became Allied Generalissimo. After April 1918 all Allied troops on the Western Front were placed under the command of the , a multi-national general staff that developed from the Supreme War Council. The was on similar lines to the and came under General Ferdinand Foch, who had overall command of all Allied troops. Without its two main personalities the military machinery at Versailles became less important.\n\nThis meeting was held four days after the ratification of the Treaty of Versailles. Lloyd George proposed dropping the blockade of the Russian Socialist Federal Soviet Republic by starting negotiations with the \"Russian people\" in the form of the centrosoyuz, which at that time was not controlled by the Bolsheviks. This was agreed, with a communique from the Council being published on 16 January. In the event, the negotiations soon became simply between the United Kingdom and a bolshevised centrosoyuz, leading to the Anglo-Soviet Trade Agreement.\n\nThis was attended by German delegates to discuss war reparations.\nRelated documents:\n\n"}
{"id": "41725758", "url": "https://en.wikipedia.org/wiki?curid=41725758", "title": "Sydenham River (Ontario)", "text": "Sydenham River (Ontario)\n\nThere are two Sydenham Rivers in Ontario.\n\n"}
{"id": "46783777", "url": "https://en.wikipedia.org/wiki?curid=46783777", "title": "War tax due stamp", "text": "War tax due stamp\n\nWar tax due stamp is a kind of war tax and postage due stamps that was used for mail when the war tax has not been paid by the sender. They were issued in Romania between 1915 and 1921.\n\nDuring war the sender of a letter paid a war tax. Then they placed a war tax stamp on the letter showing the tax was paid. If the sender did not do that, a war tax due stamp was applied by the postal service. When the letter was delivered the person receiving the letter paid double the normal rate.\n\n"}
{"id": "11543643", "url": "https://en.wikipedia.org/wiki?curid=11543643", "title": "Wildflowers of New England", "text": "Wildflowers of New England\n\nMany species of wildflowers are native to New England. There are four important community types which show considerable diversity and blending across this United States physiographic region. These are: alpine, coniferous forests, northern hardwood forests, and wetlands. Wetlands may be further subdivided into bogs, swamps, and bottomlands. \nThis article lists some of these Wildflowers of New England and references sites for further research.\n\nHabitat Loss and Invasive Species are major threats to the wildflowers of this region.\nThese invasive species include Purple Loosestrife, Garlic Mustard and Multiflora Rose.\n\n\n\n"}
